title
Parsing as Reduction

abstract
We reduce phrase-based parsing to dependency parsing. Our reduction is grounded on a new intermediate representation, "head-ordered dependency trees," shown to be isomorphic to constituent trees. By encoding order information in the dependency labels, we show that any off-theshelf, trainable dependency parser can be used to produce constituents. When this parser is non-projective, we can perform discontinuous parsing in a very natural manner. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with strong baselines, such as the Berkeley parser for English and the best non-reranking system in the SPMRL-2014 shared task. Results are particularly striking for discontinuous parsing of German, where we surpass the current state of the art by a wide margin. * This research was carried out during an internship at Priberam Labs.

Introduction Constituent parsing is a central problem in NLP-one at which statistical models trained on treebanks have excelled  (Charniak, 1996; Klein and Manning, 2003; Petrov and Klein, 2007) . However, most existing parsers are slow, since they need to deal with a heavy grammar constant. Dependency parsers are generally faster, but less informative, since they do not produce constituents, which are often required by downstream applications  (Johansson and Nugues, 2008; Wu et al., 2009; Berg-Kirkpatrick et al., 2011; Elming et al., 2013) . How to get the best of both worlds? Coarse-to-fine decoding  (Charniak and Johnson, 2005)  and shift-reduce parsing  (Sagae and Lavie, 2005; Zhu et al., 2013)  were a step forward to accelerate constituent parsing, but typical runtimes still lag those of dependency parsers. This is only made worse if discontinuous constituents are allowed-such discontinuities are convenient to represent wh-movement, scrambling, extraposition, and other linguistic phenomena common in free word order languages. While non-projective dependency parsers, which are able to model such phenomena, have been widely developed in the last decade  (Nivre et al., 2007; McDonald et al., 2006; Martins et al., 2013) , discontinuous constituent parsing is still taking its first steps  (Maier and S?gaard, 2008; Kallmeyer and Maier, 2013) . In this paper, we show that an off-the-shelf, trainable, dependency parser is enough to build a highly-competitive constituent parser. This (surprising) result is based on a reduction 1 of constituent to dependency parsing, followed by a simple post-processing procedure to recover unaries. Unlike other constituent parsers, ours does not require estimating a grammar, nor binarizing the treebank. Moreover, when the dependency parser is non-projective, our method can perform discontinuous constituent parsing in a very natural way. Key to our approach is the notion of headordered dependency trees (shown in Figure  1 ): by endowing dependency trees with this additional layer of structure, we show that they become isomorphic to constituent trees. We encode this structure as part of the dependency labels, enabling a dependency-to-constituent conversion. A related conversion was attempted by  Hall and Nivre (2008)  to parse German, but their complex encoding scheme blows up the number of arc labels, affecting the final parser's quality. By contrast, our light encoding achieves a 10-fold decrease in the label alphabet, leading to more accurate parsing. While simple, our reduction-based parsers are on par with the Berkeley parser for English  (Petrov and Klein, 2007) , and with the best single system in the recent SPMRL shared task  (Seddah et al., 2014) , for eight morphologically rich languages. For discontinuous parsing, we surpass the current state of the art by a wide margin on two German datasets (TIGER and NEGRA), while achieving fast parsing speeds. We provide a free distribution of our parsers along with this paper, as part of the TurboParser toolkit. 2 

 Background We start by reviewing constituent and dependency representations, and setting up the notation. Following Kong and Smith (2014), we use c-/d-prefixes for convenience (e.g., we write c-parser for constituent parser and d-tree for dependency tree). 

 Constituent Trees Constituent-based representations are commonly seen as derivations according to a context-free grammar (CFG). Here, we focus on properties of the c-trees, rather than of the grammars used to generate them. We consider a broad scenario that permits c-trees with discontinuities, such as the ones derived with linear context-free rewriting systems (LCFRS;  Vijay-Shanker et al. (1987) ). We also assume that the c-trees are lexicalized. Formally, let w 1 w 2 . . . w L be a sentence, where w i denotes the word in the ith position. A ctree is a rooted tree whose leaves are the words {w i } L i=1 , and whose internal nodes (constituents) are represented as a tuple Z, h, I , where Z is a non-terminal symbol, h ? {1, . . . , L} indicates the lexical head, and I ? {1, . . . , L} is the node's yield. Each word's parent is a pre-terminal unary node of the form p i , i, {i} , where p i denotes the word's part-of-speech (POS) tag. The yields and lexical heads are defined so that for every constituent Z, h, I with children  and (ii)  there is a unique k such that h = m k . This kth node (called the head-child node) is commonly chosen applying a handwritten set of head rules  (Collins, 1999; Yamada and Matsumoto, 2003) . { X k , m k , J k } K k=1 , (i) we have I = K k=1 J k ; A c-tree is continuous if all nodes Z, h, I have a contiguous yield I, and discontinuous otherwise. Trees derived by a CFG are always continuous; those derived by a LCFRS may have discontinuities, the yield of a node being a union of spans, possibly with gaps in the middle. Figure  1  shows an example of a continuous and a discontinuous c-tree. Discontinuous c-trees have crossing branches, if the leaves are drawn in left-to-right surface order. An internal node which is not a preterminal is called a proper node. A node is called unary if it has exactly one child. A c-tree without unary proper nodes is called unaryless. If all proper nodes have exactly two children then it is called a binary c-tree. Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form. Prior work. There has been a long string of work in statistical c-parsing, shifting from simple models  (Charniak, 1996)  to more sophisticated ones using structural annotation  (Johnson, 1998; Klein and Manning, 2003) , latent grammars  (Matsuzaki et al., 2005; Petrov and Klein, 2007) , and lexicalization  (Eisner, 1996; Collins, 1999 ). An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy  (Charniak and Johnson, 2005; Huang, 2008; Bj?rkelund et al., 2014) . Discontinuous c-parsing is considered a much harder problem, involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars, with treebankderived c-parsers exhibiting near-exponential runtime (Kallmeyer and Maier, 2013, Figure  27 ). To speed up decoding, prior work has considered restrictons, such as bounding the fan-out  (Maier et al., 2012)  and requiring well-nestedness (Kuhlmann and  Nivre, 2006; G?mez-Rodr?guez et al., 2010) . Other approaches eliminate the discontinuities via tree transformations  (Boyd, 2007; K?bler et al., 2008) , sometimes as a pruning step in a coarse-to-fine parsing approach (van Cranenburgh and Bod, 2013). However, reported runtimes are still superior to 10 seconds per sentence, which is not practical. Recently,  Versley (2014a)  proposed an easy-first approach that leads to considerable speed-ups, but is less accurate. In this paper, we design fast discontinuous c-parsers that outperform all the ones above by a wide margin, with similar runtimes as Versley (2014a). 

 Dependency Trees In this paper, we use d-parsers as a black box to parse constituents. Given a sentence w 1 . . . w L , a d-tree is a directed tree spanning all the words in the sentence.  3   h, m, , expressing a typed dependency relation between the head word w h and the modifier w m . A d-tree is projective if for every arc h, m, there is a directed path from h to all words that lie between h and m in the surface string  (Kahane et al., 1998) . Projective d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes  (Gaifman, 1965) . However, this relation is many-to-one: as shown in Figure  2 , several c-trees may project onto the same d-tree, differing on their flatness and on left or right-branching decisions. In the next section, we introduce the concept of head-ordered d-trees and express one-to-one mappings between these two representations. Prior work. There has been a considerable amount of work developing rich-feature d-parsers. While projective d-parsers can use dynamic programming  (Eisner and Satta, 1999 ; Koo and consider an extra root symbol, as often done in the literature. Collins, 2010), non-projective d-parsers typically rely on approximate decoders, since the underlying problem is NP-hard beyond arc-factored models  (McDonald and Satta, 2007 ). An alternative are transition-based d-parsers  (Nivre et al., 2006; Zhang and Nivre, 2011) , which achieve observed linear time. Since d-parsing algorithms do not have a grammar constant, typical implementations are significantly faster than c-parsers  (Rush and Petrov, 2012; Martins et al., 2013) . The key contribution of this paper is to reduce c-parsing to dparsing, allowing to bring these runtimes closer. 

 Head-Ordered Dependency Trees We next endow d-trees with another layer of structure, namely order information. In this framework, not all modifiers of a head are "born equal." Instead, their attachment to the head occurs as a sequence of "events," which reflect the head's preference for attaching some modifiers before others. As we will see, this additional structure will undo the ambiguity expressed in Figure  2 . 

 Strictly Ordered Dependency Trees Let us start with the simpler case where the attachment order is strict. For each head word h with modifiers M h = {m 1 , . . . , m K }, we endow M h with a strict order relation ? h , so we can organize all the modifiers of h as a chain, m i 1 ? h m i 2 ? h . . . ? h m i K . We regard this chain as reflecting the order by which words are attached (i.e., if m i ? h m j this means that "m i is attached  to h before m j "). We represent this graphically by decorating d-arcs with indices (#1, #2, . . .) to denote the order of events, as we do in Figure  1 . A d-tree endowed with a strict order for each head is called a strictly ordered d-tree. We establish below a correspondence between strictly ordered d-trees and binary c-trees. Before doing so, we need a few more definitions about c-trees. For each word position h ? {1, . . . , L}, we define ?(h) as the node higher in the c-tree whose lexical head is h. We call the path from ?(h) down to the pre-terminal p h the spine of h. We may regard a c-tree as a set of L spines, one per word, which attach to each other to form a tree  (Carreras et al., 2008) . We then have the following Proposition 1. Binary c-trees and strictly-ordered d-trees are isomorphic, i.e., there is a one-to-one correspondence between the two sets, where the number of symbols is preserved. Proof. We use the construction in Figure  3 . A formal proof is given as supplementary material. 

 Weakly Ordered Dependency Trees Next, we relax the strict order assumption, restricting the modifier sets M h = {m 1 , . . . , m K } to be only weakly ordered. This means that we can partition the K modifiers into J equivalence classes, M h = J j=1 M j h , and define a strict order ? h on the quotient set: M 1 h ? h . . . ? h M J h . Intuitively, there is still a sequence of events (1 to J), but now at each event j it may happen that multiple modifiers (the ones in the equivalence set M j h ) are si- for every u := X, m, J which is a child of v do 5: if m = h then 6: Add to D an arc h, m, Z , and put it in M j(h) h . 

 7: end if 8: end for 9: Set j(h) := j(h) + 1. 10: end for multaneously attached to h. A weakly ordered d-tree is a d-tree endowed with a weak order for each head and such that any pair m, m in the same equivalence class (written m ? h m ) receive the same dependency label . We now show that Proposition 1 can be generalized to weakly ordered d-trees. Proposition 2. Unaryless c-trees and weaklyordered d-trees are isomorphic. Proof. This is a simple extension of Proposition 1. The construction is the same as in Figure  3 , but now we can collapse some of the nodes in the linked list, originating multiple modifiers attaching to the same position of the spine-this is only possible for sibling arcs with the same index and arc label. Note, however, that if we start with a c-tree with unary nodes and apply the inverse procedure to obtain a d-tree, the unary nodes will be lost, since they do not involve attachment of modifiers. In a chain of unary nodes, only the last node is recovered in the inverse transformation. We emphasize that Propositions 1-2 hold without blowing up the number of symbols. That is, the dependency label alphabet is exactly the same as the set of phrasal symbols in the constituent representations. Algorithms 1-2 convert back and forth between the two formalisms, performing the construction of Figure  3 . Both algorithms run in linear time with respect to the size of the sentence. 

 Continuous and Projective Trees What about the more restricted class of projective d-trees? Can we find an equivalence relation with continuous c-trees? In this section, we give a precise answer to this question. It turns out that we need an additional property, illustrated in Figure  4 . We say that ? h has the nesting property iff closer words in the same direction are always attached first, i.e., iff h < m i < m j or h > m i > Algorithm 2 Conversion from d-tree to c-tree Input: head-ordered d-tree D. Output: c-tree C. 1: Nodes := GETPOSTORDERTRAVERSAL(D). 2: for h ? Nodes do 3: Create v := p h , h, {h} and set ?(h) := v. 4: Sort M h (D), yielding M 1 h ? h M 2 h ? h . . . ? h M J h . 5: for j = 1, . . . , J do 6: Let Z be the label in end for 12: end for m j implies that either m i ? h m j or m i ? h m j . A weakly-ordered d-tree which is projective and whose orders ? h have the nesting property for every h is called a nested-weakly ordered projective d-tree. We then have the following result. Proposition 3. Continuous unaryless c-trees and nested-weakly ordered projective d-trees are isomorphic. { h, m, Z | m ? M j h }. 7: Obtain c-nodes ?(h) = X, Proof. See the supplementary material. Together, Propositions 1-3 have as corollary that nested-strictly ordered projective d-trees are in a one-to-one correspondence with binary continuous c-trees. The intuition is simple: if ? h has the nesting property, then, at each point in time, all one needs to decide about the next event is whether to attach the closest available modifier on the left or on the right. This corresponds to choosing between left-branching or right-branching in a ctree. While this is potentially interesting for most continuous c-parsers, which work with binarized c-trees when running the CKY algorithm, our cparsers (to be described in ?4) do not require any binarization since they work with weakly-ordered d-trees, using Proposition 2. 

 Reduction-Based Constituent Parsers We now invoke the equivalence results established in ?3 to build c-parsers when only a trainable dparser is available. Given a c-treebank provided as input, our procedure is outlined as follows: 1. Convert the c-treebank to dependencies (Algorithm 1). 2. Train a labeled d-parser on this treebank. 3. For each test sentence, run the labeled d-parser and convert the predicted d-tree into a c-tree without unary nodes (Algorithm 2). 4. Do post-processing to recover unaries. The next subsections describe each of these steps in detail. Along the way, we illustrate with experiments using the English Penn Treebank  (Marcus et al., 1993) , which we lexicalized by applying the head rules of Collins (1999). 4 

 Dependency Encoding The first step is to convert the c-treebank to headordered dependencies, which we do using Algorithm 1. If the original treebank has discontinuous c-trees, we end up with non-projective d-trees or with violations of the nested property, as established in Proposition 3. We handle this gracefully by training a non-projective d-parser in the subsequent stage (see ?4.2). Note also that this conversion drops the unary nodes (a consequence of Proposition 2). These nodes will be recovered in the last stage, as described in ?4.4. Since in this paper we are assuming that only an off-the-shelf d-parser is available, we need to convert head-ordered d-trees to plain d-trees. We do so by encoding the order information in the dependency labels. We tried two different strategies. The first one, direct encoding, just appends suffixes #1, #2, etc., as in Figure  1 . A disadvantage is that the number of labels grows unbounded with the treebank size, as we may encounter complex substructures where the event sequences are long. The second strategy is a delta-encoding scheme where, rather than writing the absolute indices in the dependency label, we write the differences between consecutive ones.  5  We used this strategy for the continuous treebanks only, whose d-trees are guaranteed to satisfy the nested property. For comparison, we also implemented a replication of the encoding proposed by Hall and Nivre (  2008 ), which we call H&N-encoding. This strategy concatenates all the c-nodes' symbols in the modifier's spine with the attachment position in the head's spine (e.g., in Figure  3 , if the modifier m 2 has a spine with nodes X 1 , X 2 , X 3 , the generated d-label would be X 1 |X 2 |X 3 #2; our direct encoding scheme generates Z 2 #2 instead). Since their strategy encodes the entire spines into com-plex arc labels, many such labels will be generated, leading to slower runtimes and poorer generalization, as we will see. For the training portion of the English PTB, which has 27 non-terminal symbols, the direct encoding strategy yields 75 labels, while delta encoding yields 69 labels (2.6 indices per symbol). By contrast, the H&N-encoding procedure yields 731 labels, more than 10 times as many. We later show (in Tables  1-2 ) that delta-encoding leads to a slightly higher c-parsing accuracy than direct encoding, and that both strategies are considerably more accurate than H&N-encoding. 

 Training the Labeled Dependency Parser The next step is to train a labeled d-parser on the converted treebank. If we are doing continuous cparsing, we train a projective d-parser; otherwise we train a non-projective one. In our experiments, we found it advantageous to perform labeled d-parsing in two stages, as done by  McDonald et al. (2006) : first, train an unlabeled d-parser; then, train a dependency labeler.  6  Table  1  compares this approach against a oneshot strategy, experimenting various off-theshelf d-parsers: MaltParser  (Nivre et al., 2007) , MSTParser  (McDonald et al., 2005) , ZPar  Nivre, 2011), and TurboParser (Martins et al., 2013) , all with the default settings. For Tur-boParser, we used basic, standard and full models. Our separate d-labeler receives as input a backbone d-structure and predicts a label for each arc. For each head h, we predict the modifiers' labels using a simple sequence model, with features of the form ?(h, m, ) and ?(h, m, m , , ), where m and m are two consecutive modifiers (possibly on opposite sides of the head) and and are their labels. We use the same arc label features ?(h, m, ) as TurboParser. For ?(h, m, m , , ), we use the POS triplet p h , p m , p m , plus unilexical features where each of the three POS is replaced by the word form. Both features are conjoined with the label pair and . Decoding under this model can be done by running the Viterbi algorithm independently for each head. The runtime is almost negligible compared with the time to parse: it took 2.1 seconds to process PTB ?22, a fraction of about 5% of the total runtime. 

 Decoding into Unaryless Constituents After training the labeled d-parser, we can run it on the test data. Then, we need to convert the predicted d-tree into a c-tree without unaries. To accomplish this step, we first need to recover, for each head h, the weak order of its modifiers M h . We do this by looking at the predicted dependency labels, extracting the event indices j, and using them to build and sort the equivalent classes { M j h } J j=1 . If two modifiers have the same index j, we force them to have consistent labels (by always choosing the label of the modifier which is the closest to the head). For continuous c-parsing, we also decrease the index j of the modifier closer to the head as much as necessary to make sure that the nesting property holds. In PTB ?22, these corrections were necessary only for 0.6% of the tokens. Having done this, we use Algorithm 2 to obtain a predicted c-tree without unary nodes. 

 Recovery of Unary Nodes The last stage is to recover the unary nodes. Given a unaryless c-tree as input, we predict unaries by running independent classifiers at each node in the tree (a simple unstructured task). Each class is either NULL (in which case no unary node is appended to the current node) or a concatenation of unary node labels (e.g., S->ADJP for a node JJ). We obtained 64 classes by processing the training sections of the PTB, the fraction of unary nodes being about 11% of the total number of nodes. To reduce complexity, for each node symbol we only consider classes that have been observed with that symbol in the training data. In PTB ?22, this yields an average of 9.9 candidates per node occurrence. The classifiers are trained on the original ctreebank, stripping off unary nodes and trained to recover those nodes. We used the following features (conjoined with the class and with a flag indicating if the node is a pre-terminal): ? The production rules above and beneath the node (e.g., S->NP VP and NP->DT NN); ? The node's label, alone and conjoined with the parent's label or the left/right sibling's label; ? The leftmost and rightmost word/lemma/POS tag/morpho-syntactic tags in the node's yield; ? If the left/right node is a pre-terminal, the word/lemma/morpho-syntactic tags beneath. This is a relatively easy task: when gold unaryless c-trees are provided as input, we obtain an EVALB F 1 -score of 99.43%. This large figure is due to the small amount of unary nodes, making this module have less impact on the final parser than the d-parser. Being a lightweight unstructured task, this step took only 0.7 seconds to run on PTB ?22, a tiny fraction (less than 2%) of the total runtime. Table  1  shows the accuracies obtained with the d-parser followed by the unary predictor. Since two-stage TP-Full with delta-encoding is the best strategy, we use this configuration in the sequel. To further explore the impact of delta encoding, we report in Table  2  the scores obtained by direct and delta encodings on eight other treebanks (see ?5.2 for details on these datasets). With the exception of German, in all cases the delta encoding yielded better EVALB F 1 -scores with fewer labels. 

 Experiments To evaluate the performance of our reductionbased parsers, we conduct experiments in a variety Parser LR LP F1 #Toks/s.  Charniak (2000)  89.5 89.9 89.5 -Klein and  Manning (2003)  85.3 86.5 85.9 143  Petrov and Klein (2007)  90 of treebanks, both continuous and discontinuous. 

 Results on the English PTB Table  3  shows the accuracies and speeds achieved by our system on the English PTB ?23, in comparison to state-of-the-art c-parsers. We can see that our simple reduction-based c-parser surpasses the three Stanford parsers  (Klein and Manning, 2003; Socher et al., 2013, and Stanford Shift-Reduce) , and is on par with the Berkeley parser  (Petrov and Klein, 2007) , while being more than 5 times faster. The best supervised competitor is the recent shift-reduce parser of  Zhu et al. (2013) , which achieves similar, but slightly better, accuracy and speed. Our technique has the advantage of being flexible: since the time for d-parsing is the dominating factor (see ?4.4), plugging a faster d-parser automatically yields a faster c-parser. While reranking and semi-supervised systems achieve higher accuracies, this aspect is orthogonal, since the same techniques can be applied to our parser. 

 Results on the SPMRL Datasets We experimented with datasets for eight languages, from the SPMRL14 shared task  (Seddah et al., 2014) . We used the official training, development and test sets with the provided predicted POS tags. For French and German, we used the lexicalization rules detailed in  Dybro-Johansen (2004)  and  Rehbein (2009) , respectively. For Basque, Hungarian and Korean, we always took the rightmost modifier as head-child node. For Hebrew and Polish we used the leftmost modifier instead. For Swedish we induced head rules from the provided dependency treebank, as described in  Versley (2014b) . These choices were based on dev-set experiments. Table  4  shows the results. For all languages ex-cept French, our system outperforms the Berkeley parser  (Petrov and Klein, 2007) , with or without prescribed POS tags. Our average F 1 -scores are superior to the best non-reranking system participating in the shared task  (Crabb? and Seddah, 2014)  and to the c-parser of  Hall et al. (2014) , achieving the best results for 4 out of 8 languages. 

 Results on the Discontinuous Treebanks Finally, we experimented on two widely-used discontinuous German treebanks: TIGER  (Brants et al., 2002)  and NEGRA  (Skut et al., 1997) . For the former, we used two different splits: TIGER-SPMRL, provided in the SPMRL14 shared task; and TIGER-H&N, used by  Hall and Nivre (2008) . For NEGRA, we used the standard splits. In these experiments, we skipped the unary recovery stage, since very few unary nodes exist in the data.  7  We ran TurboTagger to predict POS tags for TIGER-H&N and NEGRA, while in TIGER-SPMRL we used the predicted POS tags provided in the shared task. All treebanks were lexicalized using the head-rule sets of  Rehbein (2009) . For comparison to related work, sentence length cut-offs of 30, 40 and 70 were applied during the evaluation. Table  5  shows the results. We observe that our approach outperforms all the competitors considerably, achieving state-of-the-art accuracies for both datasets. The best competitor, van  Cranenburgh and Bod (2013) , is more than 3 points behind, both in TIGER-H&N and in NEGRA. Our reduction-based parsers are also much faster: van Cranenburgh and Bod (2013) report 3 hours to parse NEGRA with L ? 40. Our system parses all NEGRA sentences (regardless of length) in 27.1 seconds in a single core, which corresponds to a rate of 618 tokens per second. This approaches the speed of the easy-first system of Versley (2014a), who reports runtimes in the range 670-920 tokens per second, but is much less accurate. 

 Related Work Conversions between constituents and dependencies have been considered by De  Marneffe et al. (2006)  in one direction, and by  Collins et al. (1999)  and  Xia and Palmer (2001)  in the other, toward multi-representational treebanks  (Xia et al., 2008) . This prior work aimed at linguistically sound conversions, involving grammar-specific transformation rules to handle the kind of ambiguities expressed in Figure  2 . Our work differs in that we are not concerned about the linguistic plausibility of our conversions, but only with the formal aspects that underlie the two representations. The work most related to ours is Hall and  Nivre (2008) , who also convert dependencies to constituents to prototype a c-parser for German. Their encoding strategy is compared to ours in ?4.1: they encode the entire spines into the dependency labels, which become rather complex and numerous. A similar strategy has been used by Versley (2014a) for discontinuous c-parsing. Both are largely outperformed by our system, as shown in ?5.3. The crucial difference is that we encode only the top node's label and its position in the spinebesides being a much lighter representation, ours has an interpretation as a weak ordering, leading to the isomorphisms expressed in Propositions 1-3. Joint constituent and dependency parsing have been tackled by  Carreras et al. (2008)  and  Rush et al. (2010) , but the resulting parsers, while accurate, are more expensive than a single c-parser. Very recently,  Kong et al. (2015)  proposed a much cheaper pipeline in which d-parsing is performed first, followed by a c-parser constrained to be con-   Petrov and Klein (2007)  using the predicted POS tags provided by the organizers. Crabb? and  Seddah (2014)  is the best non-reranking system in the shared task, and  Bj?rkelund et al. (2014)  the ensemble and reranking-based system which won the official task. We report their published scores. sistent with the predicted d-structure. Our work differs in which we do not need to run a c-parser in the second stage-instead, the d-parser already stores constituent information in the arc labels, and the only necessary post-processing is to recover unary nodes. Another advantage of our method is that it can be readily used for discontinuous parsing, while their constrained CKY algorithm can only produce continuous parses. 

 Conclusion We proposed a reduction technique that allows to implement a c-parser when only a d-parser is given. The technique is applicable to any d-parser, regardless of its nature or kind. This reduction was accomplished by endowing d-trees with a weak order relation, and showing that the resulting class of head-ordered d-trees is isomorphic to constituent trees. We showed empirically that the our reduction leads to highly-competitive c-parsers for English and for eight morphologically rich languages; and that it outperforms the current state of the art in discontinuous parsing of German. Figure 3 : 3 Figure 3: Transformation of a strictly-ordered d-tree into a binary c-tree. Each node is split into a linked list forming a spine, to which modifiers are attached in order. 
