title
Easy-First POS Tagging and Dependency Parsing with Beam Search

abstract
In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of "early-update" to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance.

Introduction The easy-first dependency parsing algorithm  (Goldberg and Elhadad, 2010)  is attractive due to its good accuracy, fast speed and simplicity. The easy-first parser has been applied to many applications  (Seeker et al., 2012; S?ggard and Wulff, 2012) . By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted  (Goldberg and Elhadad, 2010) . To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning  (Collins, 2002)  enable the use of non-local features that are helpful to improve parsing performance without overfitting  (Zhang and Nivre, 2012) . Due to these advantages, beam search and global learning has been applied to many NLP tasks  (Collins and Roark 2004; Zhang and Clark, 2007) . However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one 1 . The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences. For algorithms which do not exhibit spurious ambiguity, "early update"  (Collins and Roark 2004)  is always valid: at the k-th step when the single correct action sequence falls off the beam, 1 As shown by  (Huang et al., 2012) , only valid update guarantees the convergence of any perceptron-based training. Invalid update may lead to bad learning or even make the learning not converge at all. its model score must be lower than those still in the beam (as illustrated in figure  1 , also see the proof in  (Huang et al., 2012) ). While for easyfirst dependency parsing, there could be multiple action sequences that yield the gold result (C 1 and C 2 in figure  1 ). When all correct sequences fall off the beam, some may indeed have higher model score than those still in the beam (C 2 in figure  1 ), causing invalid update. For the purpose of valid update, we present a simple solution which is based on early update. The basic idea is to use one of the correct action sequences that were pruned right at the k-th step (C 1 in figure  1 ) for parameter update. The proposed solution is general and can also be applied to other algorithms that exhibit spurious ambiguity, such as easy-first POS tagging  (Ma et al., 2012)  and transition-based dependency parsing with dynamic oracle  (Goldberg and Nivre, 2012) . In this paper, we report experimental results on both easy-first dependency parsing and POS tagging  (Ma et al., 2012) . We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date 2 for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers  (Huang and Sagae, 2010; Zhang and Nivre, 2011) . On PTB, we also achieve good results that are comparable to the state-of-the-art. 

 Easy-first dependency parsing The easy-first dependency parsing algorithm  (Goldberg and Elhadad, 2010)  builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p 1 ,?, p r . p i is initialized with the i-th word of the input sentence. Action LEFT(i)/RIGHT(i) attaches p i to its left/right neighbor and then removes p i from the sub-tree list. The algorithm proceeds until only one sub-tree left which is the dependency tree of the input sentence (see the example in figure  2 ). Each step, the algorithm chooses the highest score action to perform according to the linear model: ( ) ( ) Here, is the weight vector and is the feature representation. In particular, ( ( ) ( )) denotes features extracted from p i . The parsing algorithm is greedy which explores a tiny fraction of the search space. Once an incorrect action is selected, it can never yield the correct dependency tree. To enlarge the search space, we introduce the beam-search extension in the next section. 

 Easy-first with beam search In this section, we introduce easy-first with beam search in our own notations that will be used throughout the rest of this paper. For a sentence x of n words, let be the action (sub-)sequence that can be applied, in sequence, to x and the result sub-tree list is denoted by ( ) For example, suppose x is "I am valid" and y is [RIGHT(1)], then y(x) yields figure 2(b). Let to be LEFT(i)/RIGHT(i) actions where 1 . Thus, the set of all possible one-action extension of is: ( ) ( ) Here, ' ' means insert to the end of . Following  (Huang et al., 2012) , in order to formalize beam search, we also use the ( ) operation which returns the top s action sequences in according to ( ). Here, denotes a set of action sequences, ( ) denotes the sum of feature vectors of each action in Pseudo-code of easy-first with beam search is shown in algorithm 1. Beam search grows s (beam width) action sequences in parallel using a // top correct extension from the beam 1 2 for 1 1 do 3 ? ( ) 4 ( )  5 if // ). At each step, the sequences in are expanded in all possible ways and then is filled up with the top s newly expanded sequences (line 2 ~ line 3). Finally, it returns the dependency tree built by the top action sequence in . 

 Training To learn the weight vector , we use the perceptron-based global learning 3  (Collins, 2002)  which updates by rewarding the feature weights fired in the correct action sequence and punish those fired in the predicted incorrect action sequence. Current work  (Huang et al., 2012)  rigorously explained that only valid update ensures convergence of any perceptron variants. They also justified that the popular "early update"  (Collins and Roark, 2004 ) is valid for the systems that do not exhibit spurious ambiguity 4 . However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even "early update" could fail to ensure validity of update (see the example in figure  1 ). For validity of update, we propose a simple solution which is based on "early update" and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following  (Zhang and Nivre, 2012) , we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions.  4  As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers  (Nivre et al., 2003; Huang and Sagae 2010; Zhang and Clark 2008)  ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of  (Goldberg and Elhadad, 2010)  for p in p i-1 , p i , p i+1 w p -vl p , w p -vr p , t p -vl p , t p -vr p , tlc p , trc p , wlc p , wlc p for p in p i-2 , p i-1 , p i , p i+1 , p i+2 t p -tlc p , t p -trc p , t p-tlc p -trc p for p, q, r in (p i-2 , p i-1 , p i ), (p i-1 , p i+1 , p i ), (p i+1 , p i+2 ,p i ) t p -t q -t r , t p -t q -w r for p, q in (p i-1 , p i ) t p -tlc p -t q , t p -trc p -t q , , t p -tlc p -w q , , t p -trc p -w q , t p -w q -tlc q , t p -w q -trc q Table 1: Feature templates for English dependency parsing. w p denotes the head word of p, t p denotes the POS tag of w p . vl p /vr p denotes the number p's of left/right child. lc p /rc p denotes p's leftmost/rightmost child. p i denotes partial tree being considered. pruned right at the step when all correct sequence falls off the beam (as C 1 in figure  1 ). Algorithm 2 shows the pseudo-code of the training procedure over one training sample ( ), a sentence-tree pair. Here we assume to be the set of all correct action sequences/subsequences. At step k, the algorithm constructs a correct action sequence ? of length k by extending those in (line 3). It also checks whether no longer contains any correct sequence. If so, ? together with are used for parameter update (line 5 ~ line 6). It can be easily verified that each update in line 6 is valid. Note that both 'TOPC' and the operation in line 5 use to check whether an action sequence y is correct or not. This can be efficiently implemented (without explicitly enumerating ) by checking if each LEFT(i)/RIGHT(i) in y are compatible with ( ): p i already collected all its dependents according to t; p i is attached to the correct neighbor suggested by t. 

 Experiments For English, we use PTB as our data set. We use the standard split for dependency parsing and the split used by  (Ratnaparkhi, 1996)  for POS tagging. Penn2Malt 5 is used to convert the bracketed structure into dependencies. For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing. For Chinese, we use CTB 5.1 and the split suggested by  (Duan et al., 2007)  for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of  (Zhang and Clark 2008)  to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. Features used in English dependency parsing are listed in table 1. Besides the features in  (Goldberg and Elhadad, 2010) , we also include some trigram features and valency features which are useful for transition-based dependency parsing  (Zhang and Nivre, 2011) . For English POS tagging, we use the same features as in  (Shen et al., 2007) . For Chinese POS tagging and dependency parsing, we use the same features as  (Ma et al., 2012) . All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 

 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table  2 and table 3 . We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. While tagging accuracy on English only slightly improved. This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. Table  2  and table 3 also show that the speed of both tagging and dependency parsing drops linearly with the growth of beam width. 

 Final results Tagging results on the test set together with some previous results are listed in table 4. Dependency parsing results on CTB and PTB are listed in table 5 and table 6, respectively. On CTB, tagging accuracy of our greedy baseline is already comparable to the state-of-the-art. As the beam size grows to 5, tagging accuracy increases to 94.01% which is 2.3% error reduction. This is also the best tagging accuracy comparing with previous single tagging models (For limited space, we do not list the performance of joint tagging-parsing models). Parsing performances on both PTB and CTB are significantly improved with a relatively small beam width (s = 8). In particular, we achieve 86.33% uas on CTB which is 1.54% uas improvement over the greedy baseline parser. Moreover, the performance is better than the best transition-based parser  (Zhang and Nivre, 2011)  which adopts a much larger beam width (s = 64). 

 Conclusion and related work This work directly extends  (Goldberg and Elhadad, 2010)  with beam search and global learning. We show that both the easy-first POS tagger and dependency parser can be significantly impr- PTB CTB  (Collins, 2002 ) 97.11 (Hatori et al., 2012  93.82  (Shen et al., 2007 ) 97.33 (Li et al., 2012  93.88  (Huang et al., 2012)    oved using beam search and global learning. This work can also be considered as applying  (Huang et al., 2012)  to the systems that exhibit spurious ambiguity. One future direction might be to apply the training method to transitionbased parsers with dynamic oracle  (Goldberg and Nivre, 2012)  and potentially further advance performances of state-of-the-art transition-based parsers.  Shen et al., (2007)  and  (Shen and Joshi, 2008)  also proposed bi-directional sequential classification with beam search for POS tagging and LTAG dependency parsing, respectively. The main difference is that their training method aims to learn a classifier which distinguishes between each local action while our training method aims to distinguish between action sequences. Our method can also be applied to their framework. Figure 1 : 1 Figure 1: Example of cases without/with spurious ambiguity. The 3 ? 1 table denotes a beam. "C/P" denotes correct/predicted action sequence. The numbers following C/P are model scores. 
