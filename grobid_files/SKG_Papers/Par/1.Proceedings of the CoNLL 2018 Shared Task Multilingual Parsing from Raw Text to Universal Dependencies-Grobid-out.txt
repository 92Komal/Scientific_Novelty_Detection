title


abstract


Introduction This volume contains papers describing systems submitted to the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, and two overview papers: one summarizing the main task, its features, evaluation methodology for the main and additional metrics, and some interesting observations about the submitted systems and the task as a whole; the other overview paper discusses a complementary task focusing on Extrinsic Parser Evaluation (EPE). This Shared Task (http://universaldependencies.org/conll18/) is an extension of the CoNLL 2017 Shared Task, with certain important differences. Like in the previous year, the data come from the Universal Dependencies project (http://universaldependencies.org), which provides annotated treebanks for a large number of languages using the same annotation scheme. The number of treebanks in the task (  82 ) is similar to the previous year (81), but some treebanks from the 2017 task were not included in the present task, and some new treebanks were added instead. The datasets are samples from 57 different languages (all languages from the previous year, and eight new languages). In comparison to 2017, there were more low-resource languages with extremely little training data, calling for cross-lingual transfer techniques. Unlike 2017, none of the low-resource languages were "surprise languages". Participants had to process all the test sets. The TIRA platform has been used for evaluation, as was the case already for the CoNLL 2015, 2016 and 2017 Shared Tasks, meaning that participants had to provide their code on a designated virtual machine to be run by the organizers to produce official results. However, test data have been published after the official evaluation period, and participants could run their systems at home to produce additional results they were allowed to include in the system description papers. The systems were ranked according to three main evaluation metrics -LAS (Labeled Attachment Score), MLAS (Morphology-aware Labeled Attachment Score), and BLEX (BiLEXical Dependency Score). Like last year, participating systems minimally had to find labeled syntactic dependencies between words. In addition, this year's task featured new metrics that also scored a system's capacity to predict a morphological analysis of each word, including a part-of-speech tag, morphological features, and a lemma. Regardless of metric, the assumption was that the input should be raw text, with no goldstandard word or sentence segmentation, and no gold-standard morphological annotation. However, for teams who wanted to concentrate on one or more subtasks, segmentation and morphology predicted by a baseline system was made available. The complementary EPE task seeks to provide better estimates of the relative utility of different parsers for a variety of downstream applications that depend centrally on the analysis of grammatical structure, viz. biomedical event extraction, negation resolution, and fine-grained opinion analysis. EPE 2018 was organized as an optional add-on exercise to the core task: the submitted systems were applied to extra texts, about 1.1 million tokens of English. It is interesting to see to what degree different intrinsic evaluation metrics from the core task correlate with end-to-end EPE results, and comparison to earlier EPE campaigns-with other types of dependency representations and additional sources of training data-further helps to put the core task into perspective. A total of 25 systems ran successfully and have been ranked (http://universaldependencies. org/conll18/results.html); 17 teams submitted parser outputs for the EPE test set. While there are clear overall winners in each of the evaluation metrics, we would like to thank all participants for working hard on their submissions and adapting their systems not only to the datasets available, but also to the evaluation platform. We would like to thank all of them for their effort, since it is the participants who are the core of any shared task's success. We would like to thank the CoNLL organizers for their support and the reviewers for helping to improve the submitted system papers. Special thanks go to Martin Potthast of the TIRA platform for handling such a large number of systems, running often for several hours each, and for being very responsive and helpful to us and all system participants, round the clock during the evaluation phase and beyond. We also thank to the 200+ people working on the Universal Dependencies project during the past four years, without whom there would be no data. Daniel Zeman, Jan  Haji?, Martin Popel, Milan Straka, Joakim Nivre, Filip Ginter and Slav Petrov,    the organizers of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies Stephan Oepen, main organizer of the EPE 2018 task Prague, September 2018
