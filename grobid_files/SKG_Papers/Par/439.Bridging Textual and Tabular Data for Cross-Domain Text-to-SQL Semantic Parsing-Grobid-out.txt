title
Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing

abstract
We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by BERT with minimal subsequent layers and the text-DB contextualization is realized via the fine-tuned deep attention in BERT. Combined with a pointergenerator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on the well-studied Spider benchmark (65.5% dev, 59.2% test), despite being much simpler than most recently proposed models for this task. Our analysis shows that BRIDGE effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-DB related tasks. Our implementation is available at https://github.com/ salesforce/TabularSemanticParsing.

Introduction Text-to-SQL semantic parsing addresses the problem of mapping natural language utterances to executable relational DB queries. Early work in this area focus on training and testing the semantic parser on a single DB  (Hemphill et al., 1990; Dahl et al., 1994; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016) . However, DBs are widely used in many domains and developing a semantic parser for each individual DB is unlikely to scale in practice. More recently, large-scale datasets consisting of hundreds of DBs and the corresponding question-SQL pairs have been released   Figure  1 : Two questions from the Spider dataset with similar intent resulted in completely different SQL logical forms on two DBs. In cross-DB text-to-SQL semantic parsing, the interpretation of a natural language question is strictly grounded in the underlying relational DB schema. well across different DBs  (Guo et al., 2019; Bogin et al., 2019b; Wang et al., 2019; Suhr et al., 2020; Choi et al., 2020) . The setup is challenging as it requires the model to interpret a question conditioned on a relational DB unseen during training and accurately express the question intent via SQL logic. Consider the two examples shown in Figure  1 , both questions have the intent to count, but the corresponding SQL queries are drastically different due to differences in the target DB schema. As a result, cross-DB text-to-SQL semantic parsers cannot trivially memorize seen SQL patterns, but instead has to accurately model the natural language question, the target DB structure, and the contextualization of both. State-of-the-art cross-DB text-to-SQL semantic parsers adopt the following design principles to address the aforementioned challenges. First, the question and schema representation should be contextualized with each other  (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Yin et al., 2020) . Second, large-scale pre-trained language models (LMs) such as BERT  (Devlin et al., 2019)  and RoBERTa  (Liu et al., 2019c)  can significantly boost parsing accuracy by providing better representations of text and capturing long-term dependencies. Third, under data privacy constraints, leveraging available DB content can resolve ambiguities in the DB schema  (Bogin et al., 2019b; Wang et al., 2019; Yin et al., 2020 ). Consider the second example in Figure  1 , knowing "PLVDB" is a value of the field Journal.Name helps the model to generate the WHERE condition. We present BRIDGE, a powerful sequential text-DB encoding framework assembling the three design principles mentioned above. BRIDGE represents the relational DB schema as a tagged sequence concatenated to the question. Different from previous work which proposed specialpurpose layers for modeling the DB schema  (Bogin et al., 2019a,b; Choi et al., 2020)  and cross text-DB linking  (Guo et al., 2019; Wang et al., 2019) , BRIDGE encodes the tagged hybrid sequence with BERT and lightweight subsequent layers -two single-layer bi-directional LSTMs  (Hochreiter and Schmidhuber, 1997) . Each schema component (table or field) is simply represented using the hidden state of its special token in the hybrid sequence. To better align the schema components with the question, BRIDGE augments the hybrid sequence with anchor texts, which are automatically extracted DB cell values mentioned in the question. Anchor texts are appended to their corresponding fields in the hybrid sequence (Figure  2 ). The text-DB alignment is then implicitly achieved via fine-tuned BERT attention between overlapped lexical tokens. Combined with a pointer-generator decoder  (See et al., 2017)  and schema-consistency driven search space pruning, BRIDGE performs competitively on the well studied Spider benchmark (Structure Acc: 65.6% dev, 59.2% test, top-4 rank; Execution Acc: 59.9% test, top-1 rank), outperforming most of recently proposed models with more sophisticated neural architectures. Our analysis shows that when applied to Spider, the BERT-encoded hybrid representation can effectively capture useful cross-modal dependencies and the anchor text augmentation resulted in significant performance improvement. 

 Model In this section, we present the BRIDGE model that combines a BERT-based encoder with a sequential pointer-generator to perform end-to-end cross-DB text-to-SQL semantic parsing. 

 Problem Definition We formally defined the cross-DB text-to-SQL task as the following. Given a natural language question Q and the schema S = T , C for a relational database, the parser needs to generate the corresponding SQL query Y. The schema consists of tables T = {t 1 , . . . , t N } and fields C = {c 11 , . . . , c 1|T 1 | , . . . , c n1 , . . . , c N|T N | }. Each table t i and each field c i j has a textual name. Some fields are primary keys, used for uniquely indexing eachEar data record, and some are foreign keys, used to reference a primary key in a different table. In addition, each field has a data type, ? ? {number, text, time, boolean, etc.}. Most existing solutions for this task do not consider DB content  (Zhong et al., 2017; . Recent approaches show accessing DB content significantly improves system performance  (Liang et al., 2018; Wang et al., 2019; Yin et al., 2020) . We consider the setting adopted by  Wang et al. (2019)  where the model has access to the value set of each field instead of full DB content. For example, the field Property_Type_Code in Figure  2  can take one of the following values: {"Apartment", "Field", "House", "Shop", "Other"}. We call such value sets picklists. This setting protects individual data record and sensitive fields such as user IDs or credit numbers can be hidden. 

 Question-Schema Serialization and Encoding As shown in Figure  2 , we represent each table with its table name followed by its fields. Each table name is preceded by the special token  [T]  and each field name is preceded by  [C] . The representations of multiple tables are concatenated to form a serialization of the schema, which is surrounded by two  [SEP]  tokens and concatenated to the question. Finally, following the input format of BERT, the question is preceded by  [CLS]  to form the hybrid question-schema serialization X =[CLS], Q, [SEP], [T], t 1 , [C], c 11 . . . , c 1|T 1 | , [T], t 2 , [C], c 21 , . . . , [C], c N|T N | , [SEP]. X is encoded with BERT, followed by a bidirectional LSTM to form the base encoding h X ? R |X|?n . The question segment of h X is passed through another bi-LSTM to obtain the question encoding h Q ? R |Q|?n . Each table/field is represented using the slice of h X corresponding to its special token [T]/[C]. Meta-data Features We train dense look-up features to represent meta-data of the schema. This includes whether a field is a primary key ( f pri ? R 2?n ), whether the field appears in a foreign key pair ( f for ? R 2?n ) and the data type of the field ( f type ? R |?|?n ). These meta-data features are fused with the base encoding of the schema component via a projection layer g to obtain the following encoding output: h t i S = g([h p X ; 0; 0; 0]), (1) h c i j S = g([h q X ; f u pri ; f v for ; f w type ]) (2) = ReLU(W g [h m X ; f u pri ; f v for ; f w type ] + b g ) h S = [h t 1 , . . . , h t |T | , h c 11 , . . . , h c N|T N | ] ? R |S|?n , ( 3 ) where p is the index of [T] associated with table t i in X and q is the index of [C] associated with field c i j in X. u, v and w are feature indices indicating the properties of c i j . [h m X ; f u pri ; f v for ; f w type ] ? R 4n is the concatenation of the four vectors. The meta-data features are specific to fields and the table representations are fused with place-holder 0 vectors. 

 Bridging Modeling only the table/field names and their relations is not always enough to capture the semantics of the schema and its dependencies with the question. Consider the example in Figure  2 , Property_-Type_Code is a general expression not explicitly mentioned in the question and without access to the set of possible field values, it is difficult to associate "houses" and "apartments" with it. To resolve this problem, we make use of anchor text to link value mentions in the question with the corresponding DB fields. We perform fuzzy string match between Q and the picklist of each field in the DB. The matched field values (anchor texts) are inserted into the question-schema representation X, succeeding the corresponding field names and separated by the special token  [V] . If multiple values were matched for one field, we concatenate all of them in matching order (Figure  2 ). If a question mention is matched with values in multiple fields. We add all matches and let the model learn to resolve ambiguity 1 . The anchor texts provide additional lexical clues for BERT to identify the corresponding mention in Q. And we name this mechanism "bridging". 

 Decoder We use an LSTM-based pointer-generator  (See et al., 2017)  with multi-head attention  (Vaswani et al., 2017)  as the decoder. The decoder starts from the final state of the question encoder. At each step, the decoder performs one of the following actions: generating a token from the vocabulary V, copying a token from the question Q or copying a schema component from S. Mathematically, at each step t, given the decoder state s t and the encoder representation [h Q ; h S ] ? R (|Q|+|S|)?n , we compute the multi-head attention as defined in  Vaswani et al. (2017) : e (h) t j = s t W (h) U (h j W (h) V ) ? n/H ; ? (h) t j = softmax j e (h) t j (4) z (h) t = |Q|+|S| j=1 ? (h) t j (h j W (h) V ); z t = z (1) t ; ? ? ? ; z (H) t , (5) where h ? [1, . . . , H] is the head number and H is the total number of heads. The scalar probability of generating from V and the output distribution are p t gen = sigmoid(s t W s gen + z t W z gen + b gen ) (6) p t out = p t gen P V (y t ) + (1 ? p t gen ) j: X j =y t ? (H) t j , (7) where P V (y t ) is the softmax LSTM output distribution and X is the length-(|Q| + |S|) sequence that consists of only the question words and special tokens [T] and [C] from X. We use the attention weights of the last head to compute the pointing distribution 2 . We extend the input state to the LSTM decoder using selective read proposed by  Gu et al. (2016) . 

 Show names of properties that are either houses or apartments 

 Property id 

 Property type code Property name 

 Date on market Date sold ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 

 Property type code Property type description ? ? ? ? ? ? ? ? Reference Property Types The technical details of this extension can be found in ?A.2. 

 Schema-Consistency Guided Decoding We propose a simple pruning strategy for sequence decoders, based on the fact that the DB fields appeared in each SQL clause must only come from the tables in the FROM clause. 

 Generating SQL Clauses in Execution Order To this end we rearrange the clauses of each SQL query in the training set into the standard DB execution order  (Rob and Coronel, 1995)  shown in table  1 . For example, the SQL SELECT COUNT(*) FROM Properties is converted to FROM Properties SELECT COUNT(*) 3 . We can show that all SQL queries with clauses in execution order satisfy the following lemma Lemma 1 Let Y exec be a SQL query with clauses arranged in execution order, then any table field in Y exec must appear after the table. As a result, we adopt a binary attention mask ? ?(H) t = ? (H) t ? ? (8) which initially has entries corresponding to all fields set to 0. Once a table t i is decoded, we set all entries in ? corresponding to {c i1 , . . . , c i|T i | } to 1. This allows the decoder to only search in the space specified by the condition in Lemma 1 with little overhead in decoding speed.  3  More complex examples can be found in Table  A1 . Written: SELECT FROM WHERE GROUPBY HAVING ORDERBY LIMIT Exec: FROM WHERE GROUPBY HAVING SELECT ORDERBY LIMIT  

 Related Work Text-to-SQL Semantic Parsing Recently the field has witnessed a re-surge of interest for textto-SQL semantic parsing  (Androutsopoulos et al., 1995) , by virtue of the newly released large-scale datasets  (Zhong et al., 2017;  and matured neural network modeling tools  (Vaswani et al., 2017; Shaw et al., 2018; Devlin et al., 2019) . While existing models have surpassed human performance on benchmarks consisting of single-table and simple SQL queries  (Hwang et al., 2019; He et al., 2019a) , ample space of improvement still remains for the Spider benchmark which consists of relational DBs and complex SQL queries 4 . Recent architectures proposed for this problem show increasing complexity in both the encoder and the decoder  (Guo et al., 2019; Wang et al., 2019; Choi et al., 2020) .  Bogin et al. (2019a,b)  proposed to encode relational DB schema as a graph and also use the graph structure to guide decoding.  Guo et al. (2019)  proposes schema-linking and SemQL, an intermediate SQL representation customized for questions in the Spider dataset which was synthesized via a tree-based decoder.  Wang et al. (2019)  proposes RAT-SQL, a unified graph encoding mechanism which effectively covers relations in the schema graph and its linking with the question. The overall architecture of RAT-SQL is deep, consisting of 8 relational self-attention layers on top of BERT-large. In comparison, BRIDGE uses BERT combined with minimal subsequent layers. It uses a simple sequence decoder with search space-pruning heuristics and applies little abstraction to the SQL surface form. Its encoding architecture took inspiration from the table-aware BERT encoder proposed by  Hwang et al. (2019) , which is very effective for WikiSQL but has not been successful adapted to Spider.  Yavuz et al. (2018)  uses question-value matches to achieve high-precision condition predictions on WikiSQL.  Shaw et al. (   2019 ) also shows that value information is critical to the cross-DB semantic parsing tasks, yet the paper reported negative results augmenting an GNN encoder with BERT and the overall model performance is much below state-of-the-art. While previous work such as  (Guo et al., 2019; Wang et al., 2019; Yin et al., 2020)  use feature embeddings or relational attention layers to explicitly model schema linking, BRIDGE models the linking implicitly with BERT and lexical anchors. An earlier version of this model is implemented within the Photon NLIDB model  (Zeng et al., 2020) , with up to one anchor text per field and an inferior anchor text matching algorithm. 4 Experiment Setup 

 Joint Text- 

 Dataset We evaluate BRIDGE using Spider , a large-scale, human annotated, crossdatabase text-to-SQL benchmark 5 . Table  2  shows the statistics of its train/dev/test splits. The test set is hidden. We run hyperparameter search and analysis on the dev set and report the test set performance only using our best approach. 

 Evaluation Metrics We report the official evaluation metrics proposed by the Spider team. Exact Set Match (E-SM) This metrics evaluates the structural correctness of the predicted SQL by checking the orderless set match of each SQL clause in the predicted query w.r.t. the ground truth. It ignores errors in the predicted values. Execution Accuracy (EA) This metrics checks if the predicted SQL is executable on the target DB and if the execution results of match those of the ground truth. It is a performance upper bound as two SQL queries with different semantics can execute to the same results on a DB. 

 Implementation Details Anchor Text Selection Given a DB, we compute the pickist of each field using the official DB files. We designed a fuzzy matching algorithm to match a question to possible value mentions in the DB (described in detail in ?A.3). We include up to k matches per field, and break ties by taking the longer match. We exclude all number matches as Data Repair The original Spider dataset contains errors in both the example files and database files. We manually corrected some errors in the train and dev examples. For comparison with others in ?5.1, we report metrics using the official dev/test sets. For our own ablation study and analysis, we report metrics using the corrected dev files. We also use a high-precision heuristics to identify missing foreign key pairs in the databases and combine them with the released ones during training and inference: if two fields of different tables have identical name and one of them is a primary key, we count them as a foreign key pair  6  . Training We train our model using cross-entropy loss. We use Adam-SGD (Kingma and Ba, 2015) with default parameters and a mini-batch size of 32. We use the uncased BERT-base model from the Huggingface's transformer library  (Wolf et al., 2019) . We set all LSTMs to 1-layer and set the hidden state dimension n = 512. We train a maximum of 50,000 steps and set the learning rate to 5e ? 4 in the first 5,000 iterations and linearly shrink it to 0. We fine-tune BERT with a fine-tuning rate linearly increasing from 3e ? 5 to 8e ? 5 in the first 5,000 iterations and linearly decaying to 0. We randomly permute the   57.6 53.4 GNN + Bertrand-DR  (Kelkar et al., 2020)  57.9 54.6 IRNet + BERT  (Guo et al., 2019)  61.9 54.7 RAT-SQL v2 ?  (Wang et al., 2019)  62.7 57.2 RYANSQL + BERT L  (Choi et al., 2020)  66  Decoding The decoder uses a generation vocabulary consisting of 70 SQL keywords and reserved tokens, plus the 10 digits to generate numbers not explicitly mentioned in the question (e.g. "first", "second", "youngest" etc.). We use a beam size of 256 for leaderboard evaluation. All other experiments uses a beam size of 16. We use schemaconsistency guided decoding during inference only. It cannot guarantee schema consistency 7 and we run a static SQL correctness check on the beam search output to eliminate predictions that are either syntactically incorrect or violates schema consistency 8 If no predictions in the beam satisfy the two criteria, we output a default SQL query which count the number of entries in the first table. 

 Results 

 End-to-end Performance Evaluation Table  3  shows the E-SM accuracy of BRIDGE compared to other approaches ranking at the top of the Spider leaderboard. BRIDGE per-7 Consider the example SQL query shown in Table A2 which satisfies the condition of Lemma 1, the table VOTING_-RECORD only appears in the first sub-query, and the field VOTING_RECORD.PRESIDENT_Vote in the second sub-query is out of scope. 8 Prior work such as  performs the more aggressive execution-guided decoding. However, it is difficult to apply this approach to complex SQL queries  (Zhong et al., 2017) . We build a static SQL analyzer on top of the Mozilla SQL Parser (https://github.com/mozilla/ moz-sql-parser). Our static checking approach handles complex SQL queries and avoids DB execution overhead. forms very competitively, significantly outperforming most of recently proposed architectures with more complicated, task-specific layers (Global-GNN, EditSQL+BERT, IRNet+BERT, RAT-SQL v2, RYANSQL+BERT L ). We find changing k from 1 to 2 yield marginal performance improvement since only 77 SQL queries in the dev set contains more than one textual values (Figure  3 ). In addition, BRIDGE generates executable SQL queries by copying values from the input question while most existing models do not. As of June 1st, 2020, BRIDGE ranks top-1 on the Spider leaderboard by execution accuracy. The two approaches significantly better than BRIDGE by E-SM are RYANSQL v2+BERT L and RAT-SQL v3+BERT L . We further look at the performance comparison with RAT-SQL v3+BERT L across different difficulty level in Table  4 . Both model achieves > 80% E-SM accuracy in the easy category, but BRIDGE shows more significant overfitting. BRIDGE also underperforms RAT-SQL v3+BERT L in the other three categories, with considerable gaps in medium and hard. As descirbed in ?3, RAT-SQL v3 uses very different encoder and decoder architectures compared to BRIDGE and it is difficult to conduct a direct comparison without a model ablation 9 . We hypothesize that the most critical difference that leads to the performance gap is in their encoding schemes. RAT-SQL v3 explicitly models the question-schemavalue matching via a graph and the matching condition (full-word match, partial match, etc.) are used to label the graph edge. BRIDGE represents the same information in a tagged sequence and uses fine-tuned BERT to implicitly obtain such mapping. While the anchor text selection algorithm ( ?4.3) has taken into account string variations, BERT may not be able to capture the linking when string variations exist -it has not seen tabular input during pre-training. The tokenization scheme adopted by BERT and other pre-trained LMs (e.g. GPT-2) cannot effectively capture partial string matches in a novel input (e.g. "cats" and "cat" are two different words in the vocabularies of BERT and GPT-2). We think recent works on text-table joint pretraining have the potential to overcome this problem  (Yin et al., 2020; Herzig et al., 2020) . RAT-SQL v3 uses BERT LARGE which has a significantly larger number of parameters than 9 RAT-SQL v3 entered the leaderboard within a month of EMNLP deadline and hasn't released its source code. 

 Model Easy  BRIDGE. While we hypothetically attribute some of the performance gap to the difference in model sizes, preliminary experiments of BRIDGE + BERT LARGE offers only a small amount of improvement (66.9 ? 67.9 on the cleaned dev set). 

 Ablation Study We perform a thorough ablation study to show the contribution of each BRIDGE sub-component (Table  5 ). Overall, all sub-components significantly contributed to the model performance. The decoding search space pruning strategies we introduced (including generation in execution order, schema-consistency guided decoding and static SQL correctness check) are effective, with absolute E-SM improvements ranging from 0.6% to 2.6%. However, encoding techniques for bridging textual and tabular input contribute more. Especially, adding anchor texts results in an absolute E-SM improvement of 3%. A further comparison between BRIDGE with and without anchor texts (Table  A3 ) shows that anchor text augmentation improves the model performance at all hardness levels, especially in the hard and extra-hard categories. Shuffling and randomly dropping non-ground-truth tables during training also significantly helps our ap- proach, as it increases the diversity of DB schema seen by the model and reduces overfitting to a particular table arrangement. Moreover, BERT is critical to the performance of BRIDGE, magnifying performance of the base model by more than three folds. This is considerably larger than the improvement prior approaches have obtained from adding BERT. Consider the performances of RAT-SQL v2 and RAT-SQL v2+BERT L in Table  3 , the improvement with BERT L is 7%. This shows that simply adding BERT to existing approaches results in significant redundancy in the model architecture. We perform a qualitative attention analysis in ?A.6 to show that after fine-tuning, the BERT layers effectively capture the linking between question mentions and the anchor texts, as well as the relational DB structures. 

 Error Analysis We randomly sampled 50 dev set examples for which the best BRIDGE model failed to produce a prediction that matches the ground truth and manually categorized the errors. Each example is assigned to only the category it fits most. Error Types Figure  4  shows the number of examples in each category. 24% of the examined predictions are false negatives. Among them, 7 are semantically equivalent to the ground truths; 4 contain GROUP BY keys different but equivalent to those of the ground truth (e.g. GROUY BY car_models.name vs. GROUP BY car_models.id); 1 has the wrong ground truth annotation. Among the true negatives, 11 have SQL structures completely deviated from the ground truth. 22 have errors that can be pinpointed to specific clauses: FROM (8), WHERE (7), SELECT (5), GROUP BY (1), ORDER BY (1). 4 have errors in the operators: 3 in the aggregation operator and 1 in the comparison operator. 1 example has non-grammatical natural language question. Error Causes A prominent cause of errors for BRIDGE is irregular design and naming in the DB schema. Table  6  shows 3 examples where BRIDGE made a wrong prediction from the medium hardness level in the dev set. In the second example, the DB contains a field named "hand" which stores information that indicates whether a tennis player is right-handed or left-handed. While "hand" is already a rarely seen field name (comparing to "name", "address" etc.), the problem is worsened by the fact that the field values are acronyms which bypassed the anchor text match. Similarly, in the third example, BRIDGE fails to detect that "highschooler", normally written as "high schooler" is a synonym of student. Occasionally, however, BRIDGE still makes mistakes w.r.t. schema components explicitly mentioned in the question, as shown by the first example. Addressing such error cases could further improve its performance.  6  shows examples of errors made by BRIDGE on the Spider dev set, all selected from the medium hardness level. The first example represents a type of errors that have a surprisingly high occurrence in the dev set. In this case the input question is unambiguous but the model simply missed seemingly obvious information. In the shown example while "released years" were explicitly mentioned in the question, the model still predicts the "Age" field instead, which is related to the tail of the question. The second example illustrates a DB with a rare relation "left-handed" represented with an obscure table name "hand". Interpreting this column requires background knowledge about the table. The example is made even harder given that the corresponding value "left" is denoted with only the first letter "L" in the table. The third example shows a complex case where the graph structure of the DB is critical for understanding the question. Here instead of predicting the table storing all student records, BRIDGE predicted the table storing the "friendship" relationship among students. 

 Sample Error Cases Table 

 Performance by Database We further compute the E-SM accuracy of BRIDGE over different DBs in the Spider dev set. Figure  5  shows drastic performance differences across DBs. While BRIDGE achieves near perfect score on some, the performance is only 30%-40% on the others. The performance does not always negatively correlates with the schema size.  We hypothesize that the model scores better on DB schema similar to those seen during training and better characterization of the "similarity" here could help transfer learning. 

 Discussion Anchor Selection BRIDGE adopts simple string matching for anchor text selection. In our experiments, improving anchor text selection accuracy significantly improves the end-to-end accuracy. Extending anchor text matching to cases beyond simple string match (e.g. "LA"?"Los Angeles") is a future direction. Furthermore, this step can be learned either independently or jointly with the textto-SQL objective. Currently BRIDGE ignores number mentions. We may introduce features which indicate a specific number in the question falls within the value range of a specific column. 

 Input Size As BRIDGE serializes all inputs into a sequence with special tags, a fair concern is that the input would be too long for large relational DBs. We believe this can be addressed with recent architecture advancements in transformers  (Beltagy et al., 2020) , which have scaled up the attention mechanism to model very long sequences. Relation Encoding BRIDGE fuses DB schema meta data features to each individual table field representations. This mechanism is not as strong as directly modeling the original graph structure. It works well in Spider, where the foreign key pairs often have exactly the same names. We consider regularizing specific attention heads to capture DB connections  (Strubell et al., 2018)  a promising way to model the graph structure of relational DBs within the BRIDGE framework without introducing (a lot of) additional parameters. 

 Conclusion We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language question and relational DBs in cross-DB semantic parsing. BRIDGE serializes the question and DB schema into a tagged sequence and maximally utilizes pre-trained LMs such as BERT to capture the linking between text mentions and the DB schema components. It uses anchor texts to further improve the alignment between the two crossmodal inputs. Combined with a simple sequential pointer-generator decoder with schema-consistency driven search space pruning, BRIDGE attained state-of-the-art performance on Spider. In the future, we plan to study the application of BRIDGE and its extensions to other text-table related tasks such as fact checking and weakly supervised semantic parsing.       SELECTFigure 2 : 2 Figure2: The BRIDGE encoder. The two phrases "houses" and "apartments" in the input question both matched to two DB fields. The matched values are appended to the corresponding field names in the hybrid sequence. 
