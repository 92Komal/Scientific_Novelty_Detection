title
Discourse Parsing with Attention-based Hierarchical Neural Networks

abstract
RST-style document-level discourse parsing remains a difficult task and efficient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.

Introduction A document is formed by a series of coherent text units. Document-level discourse parsing is a task to identify the relations between the text units and to determine the structure of the whole document the text units form. Rhetorical Structure Theory (RST)  (Mann and Thompson, 1988)  is one of the most influential discourse theories. According to RST, the discourse structure of a document can be represented by a Discourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks including automatic document summarization  (Louis et al., 2010; Marcu, 2000) , question-answering  (Verberne et al., 2007)  and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art ap-proaches heavily rely on manual feature engineering  (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014) . While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of  Li et al. (2014a) .  Li et al. (2014a)  propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by  Socher et al. (2013) . As many documents contain more than a hundred EDUs which form quite a long sequence, those weaknesses may lead to inferior results on this task. In this paper, we propose to use a hierarchical bidirectional Long Short-Term Memory (bi-LSTM) network to learn representations of text spans. Comparing with vanilla recursive/recurrent neural networks, LSTM-based networks can store information for a long period of time and don't suffer from gradient vanishing problem. We apply a hierarchical bi-LSTM network because the way words form an EDU and EDUs form a text span is different and thus they should be modeled separately and hierarchically. On top of that, we apply attention mechanism to attend over all EDUs to pick up prominent semantic information of a text span. Besides, we use tensor-based transformation function to model complicated feature interactions and thus it can produce combinatorial features. We summarize contributions of our work as follows: ? We propose to use a hierarchical bidirectional LSTM network to learn the compositional semantic representations of text spans, which naturally matches and models the intrinsic hierarchical structure of text spans. ? We extend our hierarchical bi-LSTM network with attention mechanism to allow the network to focus on the parts of input containing prominent semantic information for the compositional representations of text spans and thus alleviate the problem caused by the limited memory of LSTM for long text spans. ? We adopt a tensor-based transformation function to allow explicit feature interactions and apply tensor factorization to reduce the parameters and computations. ? We use two level caches to intensively accelerate our probabilistic CKY-like parsing process. The rest of this paper is organized as follows: Section 2 gives the details of our parsing model. Section 3 describes our parsing algorithm. Section 4 gives our training criterion. Section 5 reports the experimental results of our approach. Section 6 introduces the related work. Conclusions are given in section 7. 

 Parsing Model Given two successive text spans, our parsing model evaluates the probability to combine them into a larger span, identifies which one is the nucleus and determines what is the relation between them. As with the work of Ji and Eisenstein (2014), we set three classifiers which share the same features as input to deal with those problems. The whole parsing model is shown in Figure  1 . Three classifiers are on the top. The semantic representations of the two given text spans which come from the output of attention-based hierarchical bi-LSTM network with tensor-based transformation function is the main part of input to the classifiers. Additionally, following the previous practice of  Li et al. (2014a) , a small set of handcrafted features is introduced to enhance the model.  

 Hierarchical Bi-LSTM Network for Text Span Representations Long Short-Term Memory (LSTM) networks have been successfully applied to a wide range of NLP tasks for the ability to handle long-term dependencies and to mitigate the curse of gradient vanishing  (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2014; Rockt?schel et al., 2015; Hermann et al., 2015) . A basic LSTM can be described as follows. A sequence {x 1 , x 2 , ..., x n } is given as input. At each time-step, the LSTM computation unit takes in one token x t as input and it keeps some information in a cell state C t and gives an output h t . They are calculated in this way: i t = ?(W i [h t?1 ; x t ] + b i ) (1) f t = ?(W f [h t?1 ; x t ] + b f ) (2) Ct = tanh(W C [h t?1 ; x t ] + b C ) (3) C t = f t C t?1 + i t Ct (4) o t = ?(W o [h t?1 ; x t ] + b o ) (5) h t = o t tanh(C t ) (6) where W i , b i , W f , b f , W c , b C , W o , b o are LSTM pa- rameters, denotes element-wise product and ? denotes sigmoid function. The output at the last token, i.e., h n is taken as the representation of the whole sequence. Since an EDU is a sequence of words, we derive the representation of an EDU from the sequence constituted by concatenation of word embeddings and the POS tag embeddings of the words as Figure  2  shows. Previous work on discourse parsing tends to extract some features from the beginning and end of text units partly because discourse clues such as discourse markers(e.g., because, though) are often situated at the beginning or end of text units  (Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li et al., 2014a; Li et al., 2014b; Heilman and Sagae, 2015) . Considering the last few tokens of a sequence normally have more influence on the representation of the whole sequence learnt with LSTM because they get through less times of forget gate from the LSTM computation unit, to effectively capture the information from both beginning and end of an EDU, we use bidirectional LSTM to learn the representation of an EDU. In other words, one LSTM takes the word sequence in forward order as input, the other takes the word sequence in reversed order as input. The representation of a sequence is the concatenation of the two vector representations calculated by the two LSTMs. Since a text span is a sequence of EDUs, its meaning can be computed from the meanings of the EDUs. So we use another bi-LSTM to derive the compositional semantic representation of a text span from the EDUs it contains. The two bi-LSTM networks form a hierarchical structure as Figure  1  shows. 

 Attention The representation of a sequence computed by bi-LSTMs is always a vector with fixed dimension despite the length of the sequence. Thus when dealing with a text span with hundreds of EDUs, bi-LSTM may not be enough to capture the whole semantic information with its limited output vector dimension. Attention mechanism can attend over the output at every EDU with global context and pick up prominent semantic information and drop the subordinate information for the compositional representation of the span, so we employ attention mechanism to alleviate the problem caused by the limited memory of LSTM networks. The attention mechanism is inspired by the work of  Rockt?schel et al. (2015) . Our attention-based bi-LSTM network is shown in Figure  3 . We combine the last outputs of the span level bi-LSTM to be h s = [ ? ? h en , ? ? h e 1 ]. We also combine the outputs of the two LSTM at every EDU of the span: h t = [ ? ? h t , ? ? h t ] and thus get a matrix H = [h 1 ; h 2 ; ...; h n ] T . Taking H ? R d?n and h s ? R d as inputs, we get a vector ? ? R n standing for weights of EDUs to the text span and use it to get a weighted representation of the span r ? R d : M = tanh(W y H + W l h s ? e n ) (7) ? = sof tmax(w T ? M ) (8) r = H? (9) where ? denotes Cartesian product , M ? R k?n , e n is a n dimensional vector of all 1s and we use the Cartesian product W l h s ? e n to repeat the result of W l h s n times in column to form a matrix and W y ? R k?d , W l , ? R k?d , w ? ? R k are parameters. We synthesize the information of r and h s to get the final representation of the span: w h = ?(W hr r + W hh h s ) (10) h = w h h s + (1 ? w h ) r (11) where W hr , W hh ? R d?d are parameters, w h ? R d is a computed vector representing the element-wise weight of h s and the element-wise weighted summation h ? R d is the final representation of the text span computed by the attention-based bidirectional LSTM network.  

 Classifiers We concatenate the representations of the two given spans: h = [h s1 , h s2 ] and feed h into a full connection hidden layer to obtain a higher level representation v which is the input to the three classifiers: v = Relu(W h [h s1 , h s2 ] + b h ) (12) For each classifier, we firstly transform v ? R l into a hidden layer: v sp = Relu(W hs v + b hs ) (13) v nu = Relu(W hn v + b hn ) (14) v rel = Relu(W hr v + b hr ) (15) where W hs , W hn , W hr ? R h?l are transformation matrices and b hs , b hn , b hr ? R h are bias vectors. Then we feed these vectors into the respective output layer: y sp = ?(w s v sp + b s ) (16) y nu = sof tmax(W n v nu + b n ) ( 17 ) y rel = sof tmax(W r v rel + b r ) (18) where w s ? R h , b s ? R, W n ? R 3?h , W n ? R 3?h , b n ? R 3 , W r ? R nr?h , b n ? R nr are pa- rameters and n r is the number of different discourse relations. The first classifier is a binary classifier which outputs the probability the two spans should be combined. The second classifier is a multiclass classifier which identifies the nucleus to be span 1, span 2 or both. The third classifier is also a multiclass classifier which determines the relation between the two spans. 

 Tensor-based Transformation Tensor-based transformation function has been successfully utilized in many tasks to allow complicated interaction between features  (Sutskever et al., 2009; Socher et al., 2013; Pei et al., 2014) . Based on the intuition that allowing complicated interaction between the features of the two spans may help to identify how they are related, we adopt tensor-based transformation function to strengthen our model. A tensor-based transformation function on x ? R d 1 is as follows: y = W x + x T T [1:d 2 ] x + b ( 19 ) y i = j W ij x j + j,k T [i] j,k x j x k + b i (20) where y ? R d 2 is the output vector, y i ? R is the ith element of y, W ? R d 2 ?d 1 is the transformation matrix, T [1:d 2 ] ? R d 1 ?d 1 ?d 2 is a 3rd-order transformation tensor. A normal transformation function in neural network models only has the first term W x with the bias term. It means for normal transformation function each unit of the output vector is the weighted summation of the input vector and this only allows additive interaction between the units of the input vector. With the tensor multiplication term, each unit of the output vector is augmented with the weighted summation of the multiplication of the input vector units and thus we incorporate multiplicative interaction between the units of the input vector. Inevitably, the incorporation of tensor leads to side effects which include the increase in parameter number and computational complexity. To remedy this, we adopt tensor factorization in the same way as  Pei et al. (2014) : we use two low rank matrices to approximate each tensor slice T [i] ? R d 1 ?d 1 : We apply the factorized tensor-based transformation function to the combined text span representation h = [h s1 , h s2 ] to make the features of the two spans explicitly interact with each other: v = Relu(W h [h s1 , h s2 ] + [h s1 , h s2 ] T P [1:d] h Q [1:d] h [h s1 , h s2 ] + b h ) (22) Comparing with Eq. 12, the transformation function is added with a tensor term. 

 Handcrafted Features Most previously proposed state-of-the-art systems heavily rely on handcrafted features  (Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2013; Ji and Eisenstein, 2014; Heilman and Sagae, 2015) .  Li et al. (2014a)  show that some basic features are still necessary to get a satisfactory result for their recursive deep model. Following their practice, we adopt minimal basic features which are utilized by most systems to further strengthen our model. We list these features in Table  1 . We apply the factorized tensor-based transformation function to Word/POS features to allow more complicated interaction between them. 

 Parsing Algorithm In this section, we describe our parsing algorithm which utilizes the parsing model to produce the global optimal DT for a segmented document. 

 Probabilistic CKY-like Algorithm We adopt a probabilistic CKY-like bottom-up algorithm which is also adopted in  (Joty et al., 2013; Li et al., 2014a)  to produce a DT for a document. This parsing algorithm is a dynamic programming algorithm and produces the global optimal DT with our parsing model. Given a text span which is constituted by [e i , e i+1 , ..., e j ] and the possible subtrees of [e i , e i+1 , ..., e k ] and [e k+1 , e k+2 , ..., e j ] for all k ? {i, i+1, ..., j?1} with their probabilities, we choose k and combine the corresponding subtrees to form a combined DT with the following recurrence formula: k = arg max k {P sp (i, k, j)P i,k P k+1,j } (23) where P i,k and P k+1,j are the probabilities of the most probable subtrees of [e i , e i+1 , ..., e k ] and [e k+1 , e k+2 , ..., e j ] respectively, P sp (i, k, j) is the probability which is predicted by our parsing model to combine those two subtrees to form a DT. The probability of the most probable DT of [e i , e i+1 , ..., e j ] is: P i,j = max k {P sp (i, k, j)P i,k P k+1,j } (24) 

 Parsing Acceleration Computational complexity of the original probabilistic CKY-like algorithm is O(n 3 ) where n is the number of EDUs of the document. But in this work, given each pair of text spans, we compute the representations of them with hierarchical bi-LSTM network at the expense of an additional O(n) computations. So the computational complexity of our parser becomes O(n 4 ) and it is unacceptable for long documents. However, most computations are duplicated, so we use two level caches to drastically accelerate parsing. Firstly, we cache the outputs of the EDU level bi-LSTM which are the semantic representations of EDUs. As for the forward span level LSTM, after we get the semantic representation of a span, we cache it too and use it to compute the representation of an extended span. For example, after we get the representation of span constituted by [e 1 , e 2 , e 3 ], we take it with semantic representation of e 4 to compute the representation of the span constituted by [e 1 , e 2 , e 3 , e 4 ] in one LSTM computation step. For the backward span level LSTM, we do it the same way just in reversed order. Thus we decrease the computational complexity of computing the semantic representations for all possible span pairs which is the most time-consuming part of the original parsing process from O(n 4 ) to O(n 2 ). Secondly, it can be seen that before we apply Relu to the tensor-based transformation function, many calculations from the two spans which include a large part of tensor multiplication are independent. The multiplication between the elements of the representations of the two spans caused by the tensors and the element-wise non-linear activation function Relu terminate the independence between them. So we can further cache the independent calculation results before Relu operation for each span. Thus we decrease the computational complexity of a large part of tensor-based transformation from O(n 3 ) to Word/POS Features One-hot representation of the first two words and of the last word of each span. One-hot representation of POS tags of the first two words and of the last word of each span. Shallow Features Number of EDUs of each span. Number of words of each span. Predicted relations of the two subtrees' roots. Whether each span is included in one sentence. Whether both spans are included in one sentence. O(n 2 ) which is the second time-consuming part of the original parsing process. The remaining O(n 3 ) computations include a little part of tensor-based transformation computations, Relu operation and the computations from the three classifiers. These computations take up only a little part of the original parsing model computations and thus we greatly accelerate our parsing process. 

 Max-Margin Training We use Max-Margin criterion for our model training. We try to learn a function that maps: X ? Y , where X is the set of documents and Y is the set of possible DTs. We define the loss function for predicting a DT ?i given the correct DT y i as: (y i , ?i ) = r? ?i ?1{r ? y i } ( 25 ) where r is a span specified with nucleus and relation in the predicted DT, ? is a hyperparameter referred to as discount parameter and 1 is indicator function. We expect the probability of the correct DT to be a larger up to a margin to other possible DTs: P rob(x, y i ) ? P rob(x i , ?i ) + (y i , ?i ) (26) The objective function for m training examples is as follows: J(?) = 1 m m i=1 l i (?), where (27) l i (?) = max ?i (P rob(x i , ?i ) + (y i , ?i )) ?P rob(x i , y i ) ( 28 ) where ? denotes all the parameters including our neural network parameters and all embeddings. The probabilities of the correct DTs increase and the probabilities of the most probable incorrect DTs decrease during training. We adopt Adadelta (Zeiler, 2012) with mini-batch to minimize the objective function and set the initial learning rate to be 0.012. 

 Experiments We evaluate our model on RST Discourse Treebank 1 (RST-DT)  (Carlson et al., 2003) . It is partitioned into a set of 347 documents for training and a set of 38 documents for test. Non-binary relations are converted into a cascade of right-branching binary relations. The standard metrics of RST-style discourse parsing evaluation include blank tree structure referred to as span (S), tree structure with nuclearity (N) indication and tree structure with rhetorical relation (R) indication. Following other RSTstyle discourse parsing systems, we evaluate the relation metric in 18 coarse-grained relation classes. Since our work focus does not include EDU segmentation, we evaluate our system with gold-standard EDU segmentation and we apply the same setting on this to other discourse parsing systems for fair comparison. 

 Experimental Setup The dimension of word embeddings is set to be 50 and the dimension of POS embeddings is set to be 10. We pre-trained the word embeddings with GloVe  (Pennington et al., 2014)  on English Gigaword 2 and we fine-tune them during training. Considering some words are pretrained by GloVe but don't appear in the RST-DT training set, we want to use their embeddings if they appear in test set. Following  Kiros et al. (2015) , we expand our vocabulary with those words using a matrix W ? R 50?50 that maps word embeddings from the pre-trained word embedding space to the fine-tuned word embedding space. The objective function for training the matrix W is as follows: min W,b ||V tuned ? V pretrained W ? b|| 2 2 (29) where V tuned , V pretrained ? R |V |?50 contain finetuned and pre-trained embeddings of words appearing in training set respectively, |V | is the size of RST-DT training set vocabulary and b is the bias term also to be trained. We lemmatize all the words appeared and represent all numbers with a special token. We use Stanford CoreNLP toolkit  to preprocess the text including lemmatization, POS tagging etc. We use Theano library  (Bergstra et al., 2010)  to implement our parsing model. We randomly initialize all parameters within (-0.012, 0.012) except word embeddings. We adopt dropout strategy  (Hinton et al., 2012)  to avoid overfitting and we set the dropout rate to be 0.3. 

 Results and Analysis To show the effectiveness of the components incorporated into our model, we firstly test the performance of the basic hierarchical bidirectional LSTM network without attention mechanism (ATT), tensor-based transformation (TE) and handcrafted features (HF). Then we add them successively. The results are shown in Table  2 . The performance is improved by adding each component to our basic model and that shows the effectiveness of attention mechanism and tensor-based transformation function. Even without handcrafted features, the performance is still competitive. It indicates that the semantic representations of text spans produced by our attention-based hierarchical bi-LSTM network are effective and the handcrafted features are complementary to semantic representations produced by the network. We also experiment without mapping the OOV word embeddings and use the same embedding for all OOV words. The result is shown in  3. Without mapping the OOV word embeddings the performance decreases slightly, which demonstrates that the relation between pre-trained embedding space and the fine-tuned embedding space can be learnt and it is beneficial to train a matrix to transform OOV word embeddings from the pre-trained embedding space to the fine-tuned embedding space. We compare our system with other state-of-the-art systems including  (Joty et al., 2013; Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014a; Li et al., 2014b; Heilman and Sagae, 2015) . Systems proposed by  Joty et al. (2013) ,  Heilman (2015)  and  Feng and Hirst (2014)  are all based on variants of CRFs.  Ji and Eisenstein (2014)  use a projection matrix acting on one-hot representations of features to learn representations of text spans and build Support Vector Machine (SVM) classifier on them.  Li et al. (2014b)  adopt dependency parsing methods to deal with this task. These systems are all based on handcrafted features.  Li et al. (2014a)  adopt a recursive deep model and use some basic handcrafted features to improve their performances which has been stated before. Table  4  shows the performance for our system and those systems. EDU30 and focuses more on EDU32 which is reasonable according to our analysis above. 

 Related Work Two most prevalent discourse parsing treebanks are RST Discourse Treebank (RST-DT)  (Carlson et al., 2003)  and Penn Discourse TreeBank (PDTB)  (Prasad et al., 2008) . We evaluate our system on RST-DT which is annotated in the framework of Rhetorical Structure Theory  (Mann and Thompson, 1988) . It consists of 385 Wall Street Journal articles and is partitioned into a set of 347 documents for training and a set of 38 documents for test. 110 fine-grained and 18 coarse-grained relations are defined on RST-DT. Parsing algorithms published on RST-DT can mainly be categorized as shift-reduce parsers and probabilistic CKY-like parsers. Shiftreduce parsers are widely used for their efficiency and effectiveness and probabilistic CKY-like parsers lead to the global optimal result for the parsing models. State-of-the-art systems belonging to shiftreduce parsers include  (Heilman and Sagae, 2015; Ji and Eisenstein, 2014) . Those belonging to probabilistic CKY-like parsers include  (Joty et al., 2013; Li et al., 2014a) . Besides, Feng and Hirst (2014) adopt a greedy bottom-up approach as their parsing algorithm. Lexical, syntactic, structural and semantic features are extracted in these systems. SVM and variants of Conditional Random Fields (CRFs) are mostly used in these models.  Li et al. (2014b)  distinctively propose to use dependency structure to represent the relations between EDUs. Recursive deep model proposed by  Li et al. (2014a)  has been the only proposed deep learning model on RST-DT. Incorporating attention mechanism into RNN (e.g., LSTM, GRU) has been shown to learn better representation by attending over the output vectors and picking up important information from relevant positions of a sequence and this approach has been utilized in many tasks including neural machine translation  (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Hermann et al., 2015) , text entailment recognition  (Rockt?schel et al., 2015)  etc. Some work also uses tensor-based transformation function to make stronger interaction between features and learn combinatorial features and they get performance boost in their tasks  (Sutskever et al., 2009; Socher et al., 2013; Pei et al., 2014) . 

 Conclusion In this paper, we propose an attention-based hierarchical neural network for discourse parsing. Our attention-based hierarchical bi-LSTM network produces effective compositional semantic representations of text spans. We adopt tensor-based transformation function to allow complicated interaction between features. Our two level caches accelerate parsing process significantly and thus make it practical. Our proposed system achieves comparable results to state-of-the-art systems. We will try extending attention mechanism to obtain the representation of a text span by referring to another text span at minimal additional cost. Figure 1 : 1 Figure 1: Schematic structure of our parsing model. 
