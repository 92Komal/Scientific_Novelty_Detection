title
Transition-Based Dependency Parsing with Heuristic Backtracking

abstract
We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking. This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search. This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions. When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese.

Introduction Transition-based parsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence. In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions. Recent models include the greedy Stack-LSTM model of  Dyer et al. (2015)  and the globally normalized feed-forward networks of  Andor et al. (2016) . These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One common search technique is beam search.  (Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015; Weiss et al., 2015; Yazdani and Henderson, 2015)  In beamsearch, a fixed number of candidate transition sequences are generated, and the highest-scoring sequence is chosen as the answer. One downside to beam search is that it often results in a significant amount of wasted predictions. A constant number of beams are explored at all points throughout the sentence, leading to some unnecessary exploration towards the beginning of the sentence, and potentially insufficient exploration towards the end. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam. Another approach to solving this issue is given by  Choi and McCallum (2013) . They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams. Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up. In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function  (Pearl, 1984) : a function which estimates the future cost of taking a particular decision. When paired with a good heuristic, heuristic backtracking maintains the property of reducing wasted predictions, but allows us to more fully explore the space of possible transition sequences (as compared to selectional branching). In this paper, we use a heuristic based on the confidence of transition predictions. We also introduce a new optimization: heuristic backtracking with cutoff. Since heuristic backtracking produces results incrementally, it is possible to stop the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining. We compare the performance of these various decoding algorithms with the Stack-LSTM parser  (Dyer et al., 2015) , and achieve slightly higher accuracy than beam search, in significantly less time. 

 Transition-Based Parsing With Stack-LSTM Our starting point is the model described by  Dyer et al. (2015) .  1  The parser implements the arc-standard algorithm  (Nivre, 2004)  and it therefore makes use of a stack and a buffer. In  (Dyer et al., 2015) , the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM. The three encoded sequences form the parser state p t defined as follows, p t = max {0, W[s t ; b t ; a t ] + d} , (1) where W is a learned parameter matrix, b t , s t and a t are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term. The output p t (after a component-wise rectified linear unit (ReLU) nonlinearity  (Glorot et al., 2011) ) is then used to compute the probability of the parser action at time t as: p(z t | p t ) = exp g zt p t + q zt z ?A(S,B) exp g z p t + q z , (2) where g z is a column vector representing the (output) embedding of the parser action z, and q z is a bias term for action z. The set A(S, B) represents  1  We refer to the original work for details. the valid transition actions that may be taken in the current state. The objective function is: L ? (w, z) = |z| t=1 log p(z t | p t ) (3) where z refers to parse transitions. 

 Heuristic Backtracking Using the Stack-LSTM parsing model of  Dyer et al. (2015)  to predict each decision greedily yields very high accuracy; however, it can only explore one path, and it therefore can be improved by conducting a larger search over the space of possible parses. To do this, we introduce a new algorithm, heuristic backtracking. We also introduce a novel cutoff approach to further increase speed. 

 Decoding Strategy We model the space of possible parses as a tree, where each node represents a certain parse state (with complete values for stack, buffer, and action history). Transitions connect nodes of the tree, and leaves of the tree represent final states. During the first iteration, we start at the root of the tree, and greedily parse until we reach a leaf. That is, for each node, we use the Stack-LSTM model to calculate scores for each transition (as described in Section 2), and then execute the highest-scoring transition, generating a child node upon which we repeat the procedure. Additionally, we save an ordered list of the transition scores, and calculate the confidence of the node (as described in Section 3.2). When we reach the leaf node, we backtrack to the location that is most likely to fix a mistake. To find this, we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2). We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored. At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf. Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer. Just as in previous studies  (Collins and Roark, 2004) , we use the  n 1 1 n 1 2 n 2 2 n 3 2 n 4 2 n 1 3 n 2 3 n 3 3 n 4 3 n 1 4 n 2 4 n 3 4 n 4 4 n 1 l n 2 l n 3 l n 4 l . . . . . . . . . . . . (a) Beam Search n 1 1 n 1 2 n 2 2 n 3 2 n 4 2 n 1 3 n 2 3 n 1 4 n 2 4 n 3 4 n 1 l n 2 l n 3 l n 4 l . . . . . . . . . (b) Dynamic Beam Search n 1 1 n 1 2 n 2 2 n 1 3 n 2 3 n 3 3 n 1 4 n 2 4 n 3 4 n 4 4 n 1 l n 2 l n 3 l n 4 l . . . . . . . . . . . . (c) Selectional n 1 1 n 1 2 n 2 2 n 1 3 n 2 3 n 1 4 n 2 

 Calculating Error Likelihood Let n indicate a node, which consists of a state, a buffer, and an action history. We may refer to a specific node as n j i , which means it has i actions in its action history and it is part of the history of the jth leaf (and possibly subsequent leaves). Let the function T (n) represent a sorted vector containing all possible transitions from n, and S(n) represent a sorted vector containing the scores of all of these transitions, in terms of log probabilities of each score. We can index the scores in order of value, so T 1 (n) is the highest-scoring transition and S 1 (n) is its score, T 2 (n) is the second-highest-scoring transition, etc. Here, let u n indicate the ranking of the transition leading to the first unexplored child of a node n. Also, let V (n) represent the total score of all nodes in the history of n, i.e. the sum of all the scores of individual transitions that allowed us to get to n. To calculate the confidence of an individual node, Choi and McCallum (2013) simply found the score margin, or difference in probability between the topscoring transition and the second-highest scoring transition: C(n) = S 1 (n) ? S 2 (n). In selectional branching, the only states for which the confidence was relevant were the states in the first greedy parse, i.e. states n 1 i for all i. For heuristic backtracking, we wish to generalize this to any state n j i for all i and j. We do this in the following way: H(n j i ) = (V (n 1 i ) ? V (n j i )) + (S (u n j i )?1 (n j i ) + S (u n j i ) (n j i )) (4) Intuitively, this formula means that the node that will be explored first is the node that will yield a parse that scores as close to the greedy choice as possible. The first term ensures that it has a history of good choices, and the second term ensures that the new child node being explored will be nearly as good as the prior child. 

 Number of Predictions As discussed earlier, we use number of predictions made by the model as a proxy for the speed; execution speed may vary based on system and algorithmic implementation, but prediction count gives a good estimate of the overall work done by the algorithm. Consider a sentence of length l, which requires at most 2l transitions with the greedy decoder  (Nivre, 2004) . The number of predictions required for heuristic backtracking for b leaves is guaranteed to be less than or equal to a beam search with b beams. When doing a beam search, the first transition will require 1 prediction, and then every subsequent transition will require 1 prediction per beam, or b predictions. This results in a total of b(2l ? 1) + 1 predictions. When doing heuristic backtracking, the first greedy search will require 2l predictions. Every subsequent prediction will require a number of predictions dependent on the target of the backtrack: backtracking to n j i will require 2l ? (i + 1) predictions. Note that 0 < i < 2l. Thus, each backtrack will require at maximum 2l ? 1 predictions. Therefore, the maximum total amount of predictions is 2l + (b ? 1)(2l ? 1) = b(2l ? 1) + 1. However, note that on average, there are significantly fewer. Assuming that all parts of a sentence have approximately equal score distributions, the average backtrack will be where i = l, and reduce predictions by 50%. An intuitive understanding of this difference can be gained by viewing the graphs of various decoding methods in Figure  1 . Beam search has many nodes which never yield children that reach an end-state; dynamic beam search has fewer, but still several. Selectional branching has none, but suffers from the restriction that every parse candidate can be no more than one decision away from the greedy parse. With heuristic backtracking, there is no such restriction, but yet every node explored is directly useful for generating a candidate parse. 

 Early Cutoff Another inefficiency inherent to beam search is the fact that all b beams are always fully explored. Since the beams are calculated in parallel, this is inevitable. However, with heuristic backtracking, the beams are calculated incrementally; this gives us the opportunity to cut off our search at any point. In order to leverage this into more efficient parsing, we constructed a second Stack-LSTM model, which we call the cutoff model. The cutoff model uses a single Stack-LSTM 2 that takes as input the sequence of parser states (see Eq 1), and outputs a boolean variable predicting whether the entire parse is correct or incorrect. To train the cutoff model, we used stochastic gradient descent over the set. For each training example, we first parse it greedily using the Stack-LSTM parser. Then, for as long as the parse has at least one mistake, we pass it to the cutoff model as a negative training example. Once the parse is completely correct, we pass it to the cutoff model as a positive training example. The loss function that we 2 2 layers and 300 dimensions. use is: L ? = ? log p(t | s) (5) where s is the LSTM encoded vector and t is the truth (parse correct/incorrect). When decoding using early cutoff, we follow the exact same procedure as for normal heuristic backtracking, but after every candidate parse is generated, we use it as input to our cutoff model. When our cutoff model returns our selection as correct, we stop backtracking and return it as the answer. If we make b attempts without finding a correct parse, we follow the same procedure as before. 

 Experiments and Results To test the effectiveness of heuristic backtracking, we compare it with other decoding techniques: greedy, beam search, 3 , dynamic beam search (Mejia-Lavalle and Ramos, 2013), and selectional branching  (Choi and McCallum, 2013) . We then try heuristic backtracking (see Section 3.1), and heuristic backtracking with cutoff (see Section 3.4). Note that beam search was not used for early-update training  (Collins and Roark, 2004) . We use the same greedy training strategy for all models, and we only change the decoding strategy. We tested the performance of these algorithms on the English SD and Chinese CTB.  4  A single model was trained using the techniques described in Section 2, and used as the transition model for all decoding algorithms. Each decoding technique was tested with varying numbers of beams; as b increased, both the predictions per sentence and accuracy trended upwards. The results are summarized in Table  1 .  5  Note that we report results for only the highestaccuracy b (in the development set) for each. We also report the results of the cutoff model in Table  2 . The same greedily-trained model as above was used to generate candidate parses and confidence estimates for each transition, and then the cutoff model was trained to use these confidence esti- mates to discriminate between correctly-parsed and incorrectly-parsed sentences. 

 Discussion In Table  1  we see that in both English and Chinese, the best heuristic backtracking performs approximately as well as the best beam search, while making less than half the predictions. This supports our hypothesis that heuristic backtracking can perform at the same level as beam search, but with increased efficiency. Dynamic beam search also performed as well as full beam search, despite demonstrating a reduction in predictions on par with that of heuristic backtracking. Since the implementation of dynamic beam search is very straightforward for systems which have already implemented beam search, we believe this will prove to be a useful finding. Heuristic backtracking with cutoff outperformed greedy decoding, and reduced transitions by an additional 50%. However, it increased accuracy slightly less than full heuristic backtracking. We believe this difference could be mitigated with an improved cutoff model; as can be seen in Table  2 , the cutoff model was only able to discriminate between correct and incorrect parses around 75% of the time. Also, note that while predictions per sentence were low, the overall runtime was increased due to running the cutoff LSTM multiple times per sentence. 

 Language Cutoff Accuracy English 72.43% Chinese 75.18%  

 Related Work Heuristic backtracking is most similar to the work of  Choi and McCallum (2013) , but is distinguished from theirs by allowing new beams to be initialized from any point in the parse, rather than only from points in the initial greedy parse. Heuristic backtracking also bears similarity to greedy-best-firstsearch  (Pearl, 1984) , but is unique in that it guarantees that b candidate solutions will be found within b(2l ? 1) + 1 predictions. Our work also relates to beam-search parsers  (Zhang and Clark, 2008, inter alia) . 

 Conclusions We have introduced a novel decoding algorithm, called heuristic backtracking, and presented evidence that it performs at the same level as beam search for decoding, while being significantly more efficient. We have demonstrated this for both English and Chinese, using a parser with strong results with a greedy decoder. We expect that heuristic backtracking could be applied to any other transition-based parser with similar benefits. We plan on experimenting with various heuristics and cutoff models, such as adapting the attentionbased models of  Bahdanau et al. (2014)  to act as a guide for both the heuristic search and cutoff. Figure 1 : 1 Figure 1: Visualization of various decoding algorithms 

 Table 1 : 1 UAS and LAS of various decoding methods. Pred/Sent refers to number of predictions made by the Stack-LSTM per sentence. Decoding English Pred/Sent UAS LAS Greedy -Dyer et al. 47.92 93.04% 90.87% Beam Search 542.09 93.32% 91.19% Dynamic Beam Search 339.42 93.32% 91.19% Sel. Branching 59.66 93.24% 91.12% Heur. Backtr. 198.03 93.30% 91.18% Heur. Backtr. w/ Cutoff 108.32 93.27% 91.16% Decoding Chinese Pred/Sent UAS LAS Greedy -Dyer et al. 53.79 87.31% 85.88% Beam Search 815.65 87.62% 86.17% Dynamic Beam Search 282.32 87.62% 86.17% Sel. Branching 91.51 87.53% 86.08% Heur. Backtr. 352.30 87.61% 86.16% Heur. Backtr. w/ Cutoff 162.37 87.60% 86.15% 

 Table 2 : 2 Test-set accuracy of cutoff model on English and Chinese. 

			 Greedy and beam-search were already explored by Dyer et al. (2015)  4 Using the exact same settings as Dyer et al. (2015)  with pretrained embeddings and part-of-speech tags.5  The development sets are used to set the model parameters; results on the development sets are similar to the ones obtained in the test sets.
