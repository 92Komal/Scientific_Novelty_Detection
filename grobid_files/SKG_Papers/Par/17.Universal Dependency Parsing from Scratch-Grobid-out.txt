title
Universal Dependency Parsing from Scratch

abstract
This paper describes Stanford's system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline system that takes raw text as input, and performs all tasks required by the shared task, ranging from tokenization and sentence segmentation, to POS tagging and dependency parsing. Our single system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies.

Introduction Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling  (Marcheggiani and Titov, 2017) , relation extraction  (Zhang et al., 2018) , and machine translation  (Chen et al., 2017) . However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers  (Zeman et al., 2017) . In reality, however, these upstream systems are still far from perfect. To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task  (Zeman et al., 2018) .  1  Harnessing the power of neural systems, this pipeline achieves competitive performance in each of the inter-linked stages: tokenization, sentence and word segmentation, partof-speech (POS)/morphological features (UFeats) tagging, lemmatization, and finally, dependency parsing. Our main contributions include: ? New methods for combining symbolic statistical knowledge with flexible, powerful neural systems to improve robustness; ? A biaffine classifier for joint POS/UFeats prediction that improves prediction consistency; ? A lemmatizer enhanced with an edit classifier that improves the robustness of a sequenceto-sequence model on rare sequences; and ? Extensions to our parser from  (Dozat et al., 2017)  to model linearization. Our system achieves competitive performance on big treebanks. After fixing an unfortunate bug, the corrected system would have placed the 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on low-resource treebank categories on all metrics by a large margin. We perform extensive ablation studies to demonstrate the effectiveness of our novel methods, and highlight future directions to improve the system. 2 

 System Description In this section, we present detailed descriptions for each component of our neural pipeline system, namely the tokenizer, the POS/UFeats tagger, the lemmatizer, and finally the dependency parser. 161 

 Tokenizer To prepare sentences in the form of a list of words for downstream processing, the tokenizer component reads raw text and outputs sentences in the CoNLL-U format. This is achieved with two subsystems: one for joint tokenization and sentence segmentation, and the other for splitting multiword tokens into syntactic words. Tokenization and sentence segmentation. We treat joint tokenization and sentence segmentation as a unit-level sequence tagging problem. For most languages, a unit of text is a single character; however, in Vietnamese orthography, the most natural units of text are single syllables.  3  We assign one out of five tags to each of these units: end of token (EOT), end of sentence (EOS), multi-word token (MWT), multi-word end of sentence (MWS), and other (OTHER). We use bidirectional LSTMs (BiLSTMs) as the base model to make unit-level predictions. At each unit, the model predicts hierarchically: it first decides whether a given unit is at the end of a token with a score s  (tok)  , then classifies token endings into finer-grained categories with two independent binary classifiers: one for sentence ending s (sent) , and one for MWT s  (MWT)  . Since sentence boundaries and MWTs usually require a larger context to determine (e.g., periods following abbreviations or the ambiguous word "des" in French), we incorporate token-level information into a two-layer BiLSTM as follows (see also Figure  1 ). The first layer BiLSTM operates directly on raw units, and makes an initial prediction over the categories. To help capture local unit patterns more easily, we also combine the firstlayer BiLSTM with 1-D convolutional networks, by using a one hidden layer convolutional network (CNN) with ReLU nolinearity at its first layer, giving an effect a little like a residual connection  (He et al., 2016) . The output of the CNN is simply added to the concatenated hidden states of the Bi-LSTM for downstream computation: h RNN 1 = [ ? ? h 1 , ? ? h 1 ] = BiLSTM 1 (x), (1) h CNN 1 = CNN(x), (2) h 1 = h RNN 1 + h CNN 1 , (3) [s (tok) 1 , s (sent) 1 , s (MWT) 1 ] = W 1 h 1 , (4) where x is the input character representations, BiLSTM 1 Step t Input Unit t 1-D CNN ! ",$ (&'()) ! ",$ (+,-) ! ",$ ()./) ! 0,$ (&'()) ! 0,$ (+,-) ! 0,$ ()./) ! $ (&'()) ! $ (+,-) ! $ ()./) ? + + + Input First layer 

 First layer prediction & gating 

 Second layer Final prediction 

 Second layer prediction BiLSTM 1 Step t Figure  1 : Illustration of the tokenizer/sentence segmenter model. Components in blue represent the gating mechanism between the two layers. and W 1 contains the weights and biases for a linear classifier.  4  For each unit, we concatenate its trainable embedding with a four-dimensional binary feature vector as input, each dimension corresponding to one of the following feature functions: (1) does the unit start with whitespace; (2) does it start with a capitalized letter; (3) is the unit fully capitalized; and (4) is it purely numerical. To incorporate token-level information at the second layer, we use a gating mechanism to suppress representations at non-token boundaries before propagating hidden states upward: g 1 = h 1 ?(s (tok) 1 ) (5) h 2 = [ ? ? h 2 , ? ? h 2 ] = BiLSTM 2 (g 1 ), (6) [s (tok) 2 , s (sent) 2 , s (MWT) 2 ] = W 2 h 2 , (7) where is an element-wise product broadcast over all dimensions of h 1 for each unit. This can be viewed as a simpler alternative to multiresolution RNNs  (Serban et al., 2017) , where the first-layer BiLSTM operates at the unit level, and the second layer operates at the token level. Unlike multi-resolution RNNs, this formulation is end-toend differentiable, and can more easily leverage efficient off-the-shelf RNN implementations. To combine predictions from both layers of the BiLSTM, we simply sum the scores to obtain s (X) = s (X) 1 +s (X) 2 , where X ? {tok, sent, MWT}. The final probability over the tags is then p EOT = p +? p EOS = p ++? , (8) p MWT = p +?+ p MWS = p +++ , (9) where p ? = ?(?s  (tok)  )?(?s (sent) )?(?s  (MWT)  ), 162 and ?(?) is the logistic sigmoid function. p OTHER is simply ?(?s  (tok)  ). The model is trained to minimize the standard cross entropy loss. Multi-word Token Expansion. The tokenizer/ sentence segmenter produces a collection of sentences, each being a list of tokens, some of which are labeled as multi-word tokens (MWTs). We must expand these MWTs into the underlying syntactic words they correspond to (e.g., "im" to "in dem" in German), in order for downstream systems to process them properly. To achieve this, we take a hybrid approach to combine symbolic statistical knowledge with the power of neural systems. The symbolic statistical side is a frequency lexicon. Many languages, like German, have only a handful of rules for expanding a few MWTs. We leverage this information by simply counting the number of times a MWT is expanded into different sequences of words in the training set, and retaining the most frequent expansion in a dictionary to use at test time. When building this dictionary, we lowercase all words in the expansions to improve robustness. However, this approach would fail for languages with rich clitics, a large set of unique MWTs, and/or complex rules for MWT expansion, such as Arabic and Hebrew. We capture this by introducing a powerful neural system. Specifically, we train a sequence-to-sequence model using a BiLSTM encoder with an attention mechanism  (Bahdanau et al., 2015)  in the form of a multi-layer perceptron (MLP). Formally, the input multi-word token is represented by a sequence of characters x 1 , . . . , x I , and the output syntactic words are represented similarly as a sequence of characters y 1 , . . . , y J , where the words are separated by space characters. Inputs to the RNNs are encoded by a shared matrix of character embeddings E. Once the encoder hidden states h enc are obtained with a single-layer BiLSTM, each decoder step is unrolled as follows: h dec j = LSTM dec (E y j?1 , h dec j?1 ), (10) ? ij ? exp(u ? tanh(W ? [h dec j , h enc i ])), (11) c j = i ? ij h enc i , (12) P (y j = w|y <j ) ? u w tanh(W [h dec j , c j ]). (13) Here, w is a character index in the output vocabulary, y 0 a special start-of-sequence symbol in the vocabulary, and h dec 0 the concatenation of the last hidden states of each direction of the encoder. To bring the symbolic and neural systems together, we train them separately and use the following protocol during evaluation: for each MWT, we first look it up in the dictionary, and return the expansion recorded there if one can be found. If this fails, we retry by lowercasing the incoming token. If that fails again, we resort to the neural system to predict the final expansion. This allows us to not only account for languages with flexible MWTs patterns (Arabic and Hebrew), but also leverage the training set statistics to cover both languages with simpler MWT rules, and MWTs in the flexible languages seen in the training set without fail. This results in a high-performance, robust system for multi-word token expansion. 

 POS/UFeats Tagger Our tagger follows closely that of  (Dozat et al., 2017) , with a few extensions. As in that work, the core of the tagger is a highway BiLSTM  (Srivastava et al., 2015)  with inputs coming from the concatenation of three sources: (1) a pretrained word embedding, from the word2vec embeddings provided with the task when available  (Mikolov et al., 2013) , and from fastText embeddings otherwise  (Bojanowski et al., 2017) ; (2) a trainable frequent word embedding, for all words that occurred at least seven times in the training set; and (3) a character-level embedding, generated from a unidirectional LSTM over characters in each word. UPOS is predicted by first transforming each word's BiLSTM state with a fully-connected (FC) layer, then applying an affine classifier: h i = BiLSTM (tag) i (x 1 , . . . , x n ), (14) v (u) i = FC (u) (h i ), (15) P y (u) ik |X = softmax k W (u) v (u) i . ( 16 ) To predict XPOS, we similarly start with transforming the BiLSTM states with an FC layer. In order to further ensure consistency between the different tagsets (e.g., to avoid a VERB UPOS with an NN XPOS), we use a biaffine classifier, conditioned on a word's XPOS state as well as an embedding for its gold (at training time) or predicted (at inference time) UPOS tag y (u) i * : v (x) i = FC (x) (h i ), (17) s (x) i = [E (u) y (u) i * , 1] U (x) [v (x) i , 1], (18) P y (x) ik |y (u) i * , X = softmax k s (x) i . ( 19 ) UFeats is predicted analogously with separate parameters for each individual UFeat tag. The tagger is also trained to minimize the cross entropy loss. Some languages have composite XPOS tags, yielding a very large XPOS tag space (e.g., Arabic and Czech). For these languages, the biaffine classifier requires a prohibitively large weight tensor U  (x)  . For languages that use XPOS tagsets with a fixed number of characters, we classify each character of the XPOS tag in the same way we classify each UFeat. For the rest, instead of taking the biaffine approach, we simply share the FC layer between all three affine classifiers, hoping that the learned features for one will be used by another. 

 Lemmatizer For the lemmatizer, we take a very similar approach to that of the multi-word token expansion component introduced in Section 2.1 with two key distinctions customized to lemmatization. First, we build two dictionaries from the training set, one from a (word, UPOS) pair to the lemma, and the other from the word itself to the lemma. During evaluation, the predicted UPOS is used. When the UPOS-augmented dictionary fails, we fall back to the word-only dictionary before resorting to the neural system. In looking up both dictionaries, the word is never lowercased, because case information is more relevant in lemmatization than in MWT expansion. Second, we enhance the neural system with an edit classifier that shortcuts the prediction process to accommodate rare, long words, on which the decoder is more likely to flounder. The concatenated encoder final states are put through an FC layer with ReLU nonlinearity and fed into a 3way classifier, which predicts whether the lemma is (1) exactly identical to the word (e.g., URLs and emails), (2) the lowercased version of the word (e.g., capitalized rare words in English that are not proper nouns), or (3) in need of the sequence-tosequence model to make more complex edits to the character sequence. During training time, we assign the labels to each word-lemma pair greedily in the order of identical, lowercase, and sequence decoder, and train the classifier jointly with the sequence-to-sequence lemmatizer. At evaluation time, predictions are made sequentially, i.e., the classifier first determines whether any shortcut can be taken, before the sequence decoder model is used if needed. 

 Dependency Parser The dependency parser also follows that of  (Dozat et al., 2017)  with a few augmentations. The highway BiLSTM takes as input pretrained word embeddings, frequent word and lemma embeddings, character-level word embeddings, summed XPOS and UPOS embeddings, and summed UFeats embeddings. In  (Dozat et al., 2017) , unlabeled attachments are predicted by scoring each word i and its potential heads with a biaffine transformation h t = BiLSTM (parse) t (x 1 , . . . , x n ), (20) v (ed) i , v (eh) j = FC (ed) (h i ), FC (eh) (h j ), (21) s (e) ij = [v (eh) j , 1] U (e) [v (ed) i , 1], (22) = Deep-Biaff (e) (h i , h j ), (23) P y (e) ij |X = softmax j s (e) i , (24) where v  (ed)  i is word i's edge-dependent representation and v (eh) i its edge-head representation. This approach, however, does not explicitly take into consideration relative locations of heads and dependents during prediction; instead, such predictive location information must be implicitly learned by the BiLSTM. Ideally, we would like the model to explicitly condition on (i ? j), namely the dependent i and its potential head j's location relative to each other, in modeling p(y ij ).  5  Here, we motivate one way to build this into the model. First we factorize the relative location of word i and head j into their linear order and the distance between them, i.e., P (y ij |sgn(i ? j), abs(i ? j)), where sgn(?) is the sign function. Applying Bayes' rule and assuming conditional independence, we arrive at the following P (y ij |sgn(i ? j), abs(i ? j)) ? (25) P (y ij )P (sgn(i ? j)|y ij )P (abs(i ? j)|y ij ). In a language where heads always follow their dependents, P (sgn(i ? j) = 1|y ij ) would be extremely low, heavily penalizing rightward attachments. Similarly, in a language where dependencies are always short, P (abs(i?j) 0|y ij ) would be extremely low, penalizing longer edges. P (y ij ) can remain the same as computed in Eq. (  24 ). P (sgn(i ? j)|y ij ) can be computed similarly with a deep biaffine scorer (cf. Eqs. (  20 )-(  23 )) over the recurrent states. This results in the score of j preceding i; flipping the sign wherever i precedes j turns this into the log odds of the ob-served linearization. Applying the sigmoid function then turns it into a probability: s (l) ij = Deep-Biaff (l) (h i , h j ), (26) s (l) ij = sgn(i ? j)s (l) ij , (27) P (sgn(i ? j)|y ij ) = ? s (l) ij . (28) This can be effortlessly incorporated into the edge score by adding in the log of this probability ? log(1 + exp(?s (l) ij )). Error is not backpropagated to this submodule through the final attachment loss; instead, it is trained with its own cross entropy, with error only computed on gold edges. This ensures that the model learns the conditional probability given a true edge, rather than just learning to predict the linear order of two words. For P (abs(i ? j)|y ij ), we use another deep biaffine scorer to generate a distance score. Distances are always no less than 1, so we apply 1 + softplus to predict the distance between i and j when there's an edge between them: s (d) ij = Deep-Biaff (d) (h i , h j ), (29) s (d ) ij = 1 + softplus s (d) ij . (30) where softplus(x) = log(1 + exp(x)). The distribution of edge lengths in the treebanks roughly follows a Zipfian distribution, to which the Cauchy distribution is closely related, only the latter is more stable for values at or near zero. Thus, rather than modeling the probability of an arc's length, we can use the Cauchy distribution to model the probability of an arc's error in predicted length, namely how likely it is for the predicted distance and the true distance to have a difference of ? (d) ij : Zipf(k; ?, ?) ? (k ? /?) ?1 , (31) Cauchy(x; ?) ? (1 + x 2 /?) ?1 (32) ? (d) ij = abs(i ? j) ? s (d) ij , (33) P (abs(i ? j)|y ij ) ? (1 + ? 2(d) ij /2) ?1 . ( 34 ) When the difference ? (d) ij is small or zero, there will be effectively no penalty; but when the model expects a significantly longer or shorter arc than the observed distance between i and j, it is discouraged from assigning an edge between them. As with the linear order probability, the log of the distance probability is added to the edge score, and trained with its own cross-entropy on gold edges. 6 6 Note that the penalty assigned to the edge score in this way is proportional to ln ? (d) ij for high ? (d) ij ; using a Gamma At inference time, the Chu-Liu/Edmonds algorithm  (Chu and Liu, 1965; Edmonds, 1967)  is used to ensure a maximum spanning tree. Dependency relations are assigned to gold (at training time) or predicted (at inference time) edges y (e) i * using another deep biaffine classifier, following  (Dozat et al., 2017)  with no augmentations: s (r) i = Deep-Biaff (r) h i , h y (e) i * , (35) P y (r) ik |y (e) i * = softmax k s (r) i . (36) 

 Training Details Except where otherwise stated, our system is a pipeline: given a document of raw text, the tokenizer/sentence segmenter/MWT expander first splits it into sentences of syntactic words; the tagger then assigns UPOS, XPOS and UFeat tags to each word; the lemmatizer takes the predicted word and UPOS tag and outputs a lemma; finally, the parser takes all annotations as input and predicts the head and dependency label for each word. All components are trained with early stopping on the dev set when applicable. When a dev set is unavailable, we split the training set into an approximately 7-to-1 split for training and development. All components (except the dependency parser) are trained and evaluated on the development set assuming all related components had oracle implementations. This means the tokenizer/sentence segmenter assumes all correctly predicted MWTs will be correctly expanded, the MWT expander assumes gold word segmentation, and all downstream tasks assume gold word segmentation, along with gold annotations of all prerequisite tasks. The dependency parser is trained with predicted tags and morphological features from the POS/UFeats tagger. Treebanks without training data. For treebanks without training data, we adopt a heuristic approach for finding replacements. Where a larger treebank in the same language is available (i.e., all PUD treebanks and Japanese-Modern), we used the models from the largest treebank available in that language. Where treebanks in related languages are available (as determined by language families from Wikipedia), we use models from the largest treebank in that related language. We or Poisson distribution to model the distance directly, or using a normal distribution instead of Cauchy, respectively, assigns penalties roughly proportional to ? ended up choosing the models from English-EWT for Naija (an English-based pidgin), Irish-IDT for Breton (both are Celtic), and Norwegian-Nynorsk for Faroese (both are West Scandinavian). For Thai, since it uses a different script from all other languages, we use UDPipe 1.2 for all components. Hyperparameters. The tokenizer/sentence segmenter uses BiLSTMs with 64d hidden states in each direction and takes 32d character embeddings as input. During training, we employ dropout to the input embeddings and hidden states at each layer with p = .33. We also randomly replace the input unit with a special <UNK> unit with p = .33, which would be used in place of any unseen input at test time. We add noise to the gating mechanism in Eq. (  6 ) by randomly setting the gates to 1 with p = .02 and setting its temperature to 2 to make the model more robust to tokenization errors at test time. Optimization is performed with Adam (Kingma and Ba, 2015) with an initial learning rate of .002 for up to 20,000 steps, and whenever dev performance deteriorates, as is evaluated every 200 steps after the 2,000 th step, the learning rate is multiplied by .999. For the convolutional component we use filter sizes of 1 and 9, and for each filter size we use 64 channels (same as one direction in the BiLSTM). The convolutional outputs are concatenated in the hidden layer, before an affine transform is applied to serve as a residual connection for the BiLSTM. For the MWT expander, we use BiLSTMs with 256d hidden states in each direction as the encoder, a 512d LSTM decoder, 64d character embeddings as input, and dropout rate p = .5 for the inputs and hidden states. Models are trained up to 100 epochs with the standard Adam hyperparameters, and the learning rate is annealed similarly every epoch after the 15 th epoch by a factor of 0.9. Beam search of beam size 8 is employed in evaluation. The lemmatizer uses BiLSTMs with 100d hidden states in each direction of the encoder, 50d character embeddings as input, and dropout rate p = .5 for the inputs and hidden states. The decoder is an LSTM with 200d hidden states. During training we jointly minimize (with equal weights) the cross-entropy loss of the edit classifier and the negative log-likelihood loss of the seq2seq lemmatizer. Models are trained up to 60 epochs with standard Adam hyperparameters. The tagger and parser share most of their hyperparameters. We use 75d uncased frequent word and lemma embeddings, and 50d POS tag and UFeat embeddings. Pretrained embeddings and character-based word representations are both transformed to be 125d. During training, all embeddings are randomly replaced with a <drop> symbol with p = .33. We use 2-layer 200d BiL-STMs for the tagger and 3-layer 400d BiLSTMs for the parser. We employ dropout in all feedforward connections with p = .5 and all recurrent connections  (Gal and Ghahramani, 2016)  with p = .25 (except p = .5 in the tagger BiLSTM). All classifiers use 400d FC layers (except 100d for UFeats) with the ReLU nonlinearity. We train the systems with Adam (? = .003, ? 1 = .9, ? 2 = .95) until dev accuracy decreases, at which point we switch to AMSGrad  (Reddi et al., 2018)  until 3,000 steps pass with no dev accuracy increases. 

 Results The main results are shown in Table  1 . As can be seen from the table, our system achieves competitive performance on nearly all of the metrics when macro-averaged over all treebanks. Moreover, it achieves the top performance on several metrics when evaluated only on big treebanks, showing that our systems can effectively leverage statistical patterns in the data. Where it is not the top performing system, our system also achieved competitive results on each of the metrics on these treebanks. This is encouraging considering that our system is comprised of single-system components, whereas some of the best performing teams used ensembles (e.g., HIT-SCIR  (Che et al., 2018) ). When taking a closer look, we find that our UFeats classifier is very accurate on these treebanks as well. Not only did it achieve the top performance on UFeats F 1 , but also it helped the parser achieve top MLAS as well on big treebanks, even when the parser is not the best-performing as evaluated by other metrics. We also note the contribution from our consistency modeling in the POS tagger/UFeats classifier: in both settings the individual metrics (UPOS, XPOS, and UFeats) achieve a lower advantage margin over the reference systems when compared to the AllTags metric, showing that these reference systems, though sometimes more accurate on each individual task, are not as consistent as our system overall. The biggest disparity between the all-treebanks and big-treebanks results comes from sentence segmentation. After inspecting the results on smaller treebanks and double-checking our implementation, we noticed issues with how we processed data in the tokenizer that negatively impacted generalization on these treebanks. 7 This is devastating for these treebanks, as all downstream components process words at the sentence level. We fixed this issue, and trained new tokenizers with all hyperparameters identical to our system at submission. We further built an unofficial evaluation pipeline, which we verified achieves the same evaluation results as the official system, and eval-7 Specifically, our tokenizer was originally designed to be aware of newlines (\n) in double newline-separated paragraphs, but we accidentally prepared training and dev sets for low resource treebanks by putting each sentence on its own line in the text file. This resulted in the sentence segmenter overfitting to relying on newlines. In later experiments, we replaced all in-paragraph whitespaces with space characters. uated our entire pipeline by only replacing the tokenizer. As is shown in Table  1 , the resulting system (Stanford+) is much more accurate overall, and we would have ranked 2 nd , 1 st , and 3 rd on the official evaluation metrics LAS, MLAS, and BLEX, respectively. 8 On big treebanks, all metrics changed within only 0.02% F 1 and are thus not included. On small treebanks, however, this effect is more pronounced: as is shown in Table  2 , our corrected system outperforms all submission systems on all official evaluation metrics on all low-resource treebanks by a large margin. 

 Analysis In this section, we perform ablation studies on the new approaches we proposed for each component, and the contribution of each component to the final pipeline. For each component, we assume access to an oracle for all other components in the analysis, and show their efficacy on the dev sets. 9 For the ablations on the pipeline, we report macroaveraged F 1 on the test set.  Tokenizer. We perform ablation studies on the less standard components in the tokenizer, namely the gating mechanism in Eq. (  6 ) (gating), the convolutional residual connections (conv), and the seq2seq model in the MWT expander (seq2seq), on all 61 big treebanks. As can be seen in Table 3, all but the gating mechanism make noticeable differences in macro F 1 . When taking a closer look, we find that both gating and conv show a mixed contribution to each treebank, and we could have improved overall performance further through treebank-level component selection. One surprising discovery is that conv greatly helps identify MWTs in Hebrew (+34.89 Words F 1 ) and sentence breaks in Ancient Greek-PROIEL (+18.77 Sents F 1 ). In the case of seq2seq, although the overall macro difference is small, it helps with the word segmentation performance on all treebanks where it makes any meaningful difference, most notably +10.08 on Hebrew and +4.19 on Arabic in Words F 1 (see also Figure  2 ). Finally, we note that dropout plays an important role in safeguarding the tokenizer from overfitting. POS/UFeats Tagger. The main novelty in our tagger is the explicit conditioning of XPOS and UFeats predictions on the UPOS prediction. We compare this against a tagger that simply shares the hidden features between the UPOS, XPOS, and UFeats classifiers.  , where p c (X) is the accuracy of X. This quantifies (in nats) how much more likely it is to get all tags right than we would expect given their individual accuracies, if they were independent. As can be seen in Table  4 , the added parameters do not affect UPOS performance significantly, but do help improve XPOS and UFeats prediction. Moreover, the biaffine classifier is markedly more consistent than the affine one with shared representations. Lemmatizer. We perform ablation studies on three individual components in our lemmatizer: the edit classifier (edit), the sequence-to-sequence module (seq2seq) and the dictionaries (dictionaries). As shown in Table  5 , we find that our lemmatizer with all components achieves the best overall performance. Specifically, adding the neural components (i.e., edit & seq2seq) drastically improves overall lemmatization performance over a simple dictionary-based approach (+6.77 F 1 ), and the gains are consistent over different treebank groups. While adding the edit classifier slightly decreases the F 1 score on small treebanks, it improves the performance on lowresource languages substantially (+0.91 F 1 ), and therefore leads to an overall gain of 0.11 F 1 . Tree-   To further understand the behavior of the edit classifier, for each treebank we present the ratio of all predicted edit types on dev set words in Figure  3 . We find that the behavior of the edit classifier aligns well with linguistic knowledge. For example, while Ancient Greek, Arabic and Korean require a lot of complex edits in lemmatization, the vast majority of operations in Chinese and Japanese are simple identity mappings. Dependency Parser. The main innovation for the parsing module is terms that model locations of a dependent word relative to possible head words in the sentence. Here we examine the impact of these terms, namely linearization (Eq. (  28 )) and distance (Eq. (  34 )). For this analysis, we exclude six treebanks with very small dev sets. As can be seen in Table  6 , both terms contribute significantly to the final parser performance, with the distance term contributing slightly more. Pipeline Ablation. We analyze the contribution of each pipeline component by incrementally replacing them with gold annotations and observing performance change. As shown in Figure  4 , most downstream systems benefit moderately from gold sentence and word segmentation, while the parser largely only benefits from improved POS/UFeats tagger performance (aside from BLEX, which is directly related to lemmatization performance and benefits notably). Finally, we note that the parser still is far from perfect even given gold annotations from all upstream tasks, but our components in the pipeline are very effective at closing the gap between predicted and gold annotations. 

 Conclusion & Future Directions In this paper, we presented Stanford's submission to the CoNLL 2018 UD Shared Task. Our submission consists of neural components for each stage of a pipeline from raw text to dependency parses. The final system was very competitive on big treebanks; after fixing our preprocessing bug, it would have outperformed all official systems on all metrics for low-resource treebank categories. One of the greatest opportunities for further gains is through the use of context-sensitive word embeddings, such as ELMo  (Peters et al., 2018)  and ULMfit  (Howard and Ruder, 2018) . Although this requires a large resource investment, HIT-SCIR  (Che et al., 2018)  has shown solid improvements from incorporating these embeddings. (d) ij , ln ?(? (d) ij ), and ? 2(d) ij .Thus, the Cauchy is more numerically stable during training. 
