title
Coarse-to-Fine Decoding for Neural Semantic Parsing

abstract
Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.

Introduction Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks  (Bahdanau et al., 2015; Vinyals et al., 2015)  has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem  (Jia and Liang, 2016; Dong and Lapata, 2016; . The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders  (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017) , decoders constrained by a grammar model  (Xiao et al., 2016; Yin and Neubig, 2017; , or modular decoders which use syntax to dynamically compose various submodels  (Rabinovich et al., 2017) . In this work, we propose to decompose the decoding process into two stages. The first decoder focuses on predicting a rough sketch of the meaning representation, which omits low-level details, such as arguments and variable names. Example sketches for various meaning representations are shown in Table  1 . Then, a second decoder fills in missing details by conditioning on the natural language input and the sketch itself. Specifically, the sketch constrains the generation process and is encoded into vectors to guide decoding. We argue that there are at least three advantages to the proposed approach. Firstly, the decomposition disentangles high-level from low-level semantic information, which enables the decoders to model meaning at different levels of granularity. As shown in Table  1 , sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go. Secondly, the model can explicitly share knowledge of coarse structures for the examples that have the same sketch (i.e., basic meaning), even though their actual meaning representations are different (e.g., due to different details). Thirdly, after generating the sketch, the decoder knows what the basic meaning of the utterance looks like, and the model can use it as global context to improve the prediction of the final details. Our framework is flexible and not restricted to specific tasks or any particular model. We conduct experiments on four datasets representative of various semantic parsing tasks ranging from logical form parsing, to code generation, and SQL query generation. We adapt our architecture to these tasks and present several ways to obtain sketches from their respective meaning representations. Experimental results show that our framework achieves competitive performance compared 

 Dataset Length Example GEO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all flights from dallas before 10am y : (lambda $0 e (and (flight $0) (from $0 dallas:ci) (< (departure time $0) 1000:ti))) a : (lambda#2 (and flight@1 from@2 (< departure time@1 ? ) ) ) DJANGO 14.4 8.7 8.0 x : if length of bits is lesser than integer 3 or second element of bits is not equal to string 'as' , y : if len(bits) < 3 or bits  

 Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations  (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015) . These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results  (Dong and Lapata, 2016; Jia and Liang, 2016; , eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation  Jia and Liang, 2016) , transfer learning  (Fan et al., 2017) , sharing parameters for multiple languages or meaning representations  (Susanto and Lu, 2017; Herzig and Berant, 2017) , and utilizing user feedback signals  (Iyer et al., 2017) . There are also efforts to develop structured decoders that make use of the syntax of meaning representations.  Dong and Lapata (2016)  and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion.  Xiao et al. (2016)  and  employ the grammar to constrain the decoding process.  Cheng et al. (2017)  use a transition system to generate variable-free queries.  Yin and Neubig (2017)  design a grammar model for the generation of abstract syntax trees  (Aho et al., 2007)  in depth-first, left-to-right order.  Rabinovich et al. (2017)  propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The flexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-fine methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing  (Charniak et al., 2006; Petrov, 2011) .  and  Zhang et al. (2017)  use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-fine inference for lexical induction, sketches in our case are abstractions of the final meaning representation. The idea of using sketches as intermediate representations has also been explored in the field of program synthesis  (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017) .  Yaghmazadeh et al. (2017)  use SEMPRE  (Berant et al., 2013)  to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. 

 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. We first generate the meaning sketch a for natural language input x. Then, a fine meaning decoder fills in the missing details (shown in red) of meaning representation y. The coarse structure a is used to guide and constrain the output decoding. Let x = x 1 ? ? ? x |x| denote a natural language expression, and y = y 1 ? ? ? y |y| its meaning representation. We wish to estimate p (y|x), the conditional probability of meaning representation y given input x. We decompose p (y|x) into a twostage generation process: p (y|x) = p (y|x, a) p (a|x) (1) where a = a 1 ? ? ? a |a| is an abstract sketch representing the meaning of y. We defer detailed description of how sketches are extracted to Section 4. Suffice it to say that the extraction amounts to stripping off arguments and variable names in logical forms, schema specific information in SQL queries, and substituting tokens with types in source code (see Table  1 ). As shown in Figure  1 , we first predict sketch a for input x, and then fill in missing details to generate the final meaning representation y by conditioning on both x and a. The sketch is encoded into vectors which in turn guide and constrain the decoding of y. We view the input expression x, the meaning representation y, and its sketch a as sequences. The generation probabilities are factorized as: p (a|x) = |a| t=1 p (a t |a <t , x) (2) p (y|x, a) = |y| t=1 p (y t |y <t , x, a) (3) where a <t = a 1 ? ? ? a t?1 , and y <t = y 1 ? ? ? y t?1 . In the following, we will explain how p (a|x) and p (y|x, a) are estimated. 

 Sketch Generation An encoder is used to encode the natural language input x into vector representations. Then, a decoder learns to compute p (a|x) and generate the sketch a conditioned on the encoding vectors. Input Encoder Every input word is mapped to a vector via x t = W x o (x t ), where W x ? R n?|Vx| is an embedding matrix, |V x | is the vocabulary size, and o (x t ) a one-hot vector. We use a bi-directional recurrent neural network with long short-term memory units (LSTM, Hochreiter and Schmidhuber 1997) as the input encoder. The encoder recursively computes the hidden vectors at the t-th time step via: ? ? e t = f LSTM ? ? e t?1 , x t , t = 1, ? ? ? , |x| (4) ? ? e t = f LSTM ? ? e t+1 , x t , t = |x|, ? ? ? , 1 (5) e t = [ ? ? e t , ? ? e t ] (6) where [?, ?] denotes vector concatenation, e t ? R n , and f LSTM is the LSTM function. Coarse Meaning Decoder The decoder's hidden vector at the t-th time step is computed by d t = f LSTM (d t?1 , a t?1 ) , where a t?1 ? R n is the embedding of the previously predicted token. The hidden states of the first time step in the decoder are initialized by the concatenated encoding vectors d 0 = [ ? ? e |x| , ? ? e 1 ]. Additionally, we use an attention mechanism  (Luong et al., 2015)  to learn soft alignments. We compute the attention score for the current time step t of the decoder, with the k-th hidden state in the encoder as: s t,k = exp{d t ? e k }/Z t (7) where Z t = |x| j=1 exp{d t ? e j } is a normalization term. Then we compute p (a t |a <t , x) via: e d t = |x| k=1 s t,k e k ( 8 ) d att t = tanh W 1 d t + W 2 e d t (9) p (a t |a <t , x) = softmax at W o d att t + b o (10) where W 1 , W 2 ? R n?n , W o ? R |Va|?n , and b o ? R |Va| are parameters. Generation terminates once an end-of-sequence token "</s>" is emitted. 

 Meaning Representation Generation Meaning representations are predicted by conditioning on the input x and the generated sketch a. The model uses the encoder-decoder architecture to compute p (y|x, a), and decorates the sketch a with details to generate the final output. Sketch Encoder As shown in Figure  1 , a bidirectional LSTM encoder maps the sketch sequence a into vectors {v k } |a| k=1 as in Equation (  6 ), where v k denotes the vector of the k-th time step. 

 Fine Meaning Decoder The final decoder is based on recurrent neural networks with an attention mechanism, and shares the input encoder described in Section 3.1. The decoder's hidden states {h t } |y| t=1 are computed via: i t = v k y t?1 is determined by a k y t?1 otherwise (11) h t = f LSTM (h t?1 , i t ) where h 0 = [ ? ? e |x| , ? ? e 1 ] , and y t?1 is the embedding of the previously predicted token. Apart from using the embeddings of previous tokens, the decoder is also fed with {v k } |a| k=1 . If y t?1 is determined by a k in the sketch (i.e., there is a one-toone alignment between y t?1 and a k ), we use the corresponding token's vector v k as input to the next time step. The sketch constrains the decoding output. If the output token y t is already in the sketch, we force y t to conform to the sketch. In some cases, sketch tokens will indicate what information is missing (e.g., in Figure  1 , token "flight@1" indicates that an argument is missing for the predicate "flight"). In other cases, sketch tokens will not reveal the number of missing tokens (e.g., "STRING" in DJANGO) but the decoder's output will indicate whether missing details have been generated (e.g., if the decoder emits a closing quote token for "STRING"). Moreover, type information in sketches can be used to constrain generation. In Table  1 , sketch token "NUMBER" specifies that a numeric token should be emitted. For the missing details, we use the hidden vector h t to compute p (y t |y <t , x, a), analogously to Equations (  7 )-(10). 

 Training and Inference The model's training objective is to maximize the log likelihood of the generated meaning representations given natural language expressions: max (x,a,y)?D log p (y|x, a) + log p (a|x) where D represents training pairs. At test time, the prediction for input x is obtained via ? = arg max a p (a |x) and ? = arg max y p (y |x, ?), where a and y represent coarse-and fine-grained meaning candidates. Because probabilities p (a|x) and p (y|x, a) are factorized as shown in Equations (  2 )-(  3 ), we can obtain best results approximately by using greedy search to generate tokens one by one, rather than iterating over all candidates. 

 Semantic Parsing Tasks In order to show that our framework applies across domains and meaning representations, we developed models for three tasks, namely parsing natural language to logical form, to Python source code, and to SQL query. For each of these tasks we describe the datasets we used, how sketches were extracted, and specify model details over and above the architecture presented in Section 3. 

 Natural Language to Logical Form For our first task we used two benchmark datasets, namely GEO (880 language queries to a database of U.S. geography) and ATIS (5, 410 queries to a flight booking system). Examples are shown in Table  1  (see the first and second block). We used standard splits for both datasets: 600 training and 280 test instances for GEO  (Zettlemoyer and Collins, 2005) ; 4, 480 training, 480 development, and 450 test examples for ATIS. Meaning representations in these datasets are based on ?-calculus  (Kwiatkowski et al., 2011) . We use brackets to linearize the hierarchical structure. The first element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments. Algorithm 1 shows the pseudocode used to extract sketches from ?-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol "@" to denote the number of missing arguments in a predicate. For example, we extract "from@2" from the expression "(from $0 dallas:ci)" which indicates that the predicate "from" has two arguments. We use "?" as a placeholder in cases where only partial argument information can be omitted. We also omit variable information defined by the lambda operator and quantifiers (e.g., exists, count, and argmax). We use the symbol "#" to denote the number of omitted tokens. For the example in Figure  1 , "lambda $0 e" is reduced to "lambda#2". The meaning representations of these two datasets are highly compositional, which motivates us to utilize the hierarchical structure of ?-calculus. A similar idea is also explored in the tree decoders proposed in  Dong and Lapata (2016)  and  Yin and Neubig (2017)  where parent hidden states are fed to the input gate of the LSTM units. On the contrary, parent hidden states serve as input to the softmax classifiers of both fine and coarse meaning decoders. Parent Feeding Taking the meaning sketch "(and flight@1 from@2)" as an example, the parent of "from@2" is "(and". Let p t denote the parent of the t-th time step in the decoder. Compared with Equation (10), we use the vector d att t and the hidden state of its parent d pt to compute the prob-ability p (a t |a <t , x) via: p (a t |a <t , x) = softmax at W o [d att t , d pt ] + b o where [?, ?] denotes vector concatenation. The parent feeding is used for both decoding stages. 

 Natural Language to Source Code Our second semantic parsing task used DJANGO  (Oda et al., 2015) , a dataset built upon the Python code of the Django library. The dataset contains lines of code paired with natural language expressions (see the third block in Table  1 ) and exhibits a variety of use cases, such as iteration, exception handling, and string manipulation. The original split has 16, 000 training, 1, 000 development, and 1, 805 test instances. We used the built-in lexical scanner of Python 1 to tokenize the code and obtain token types. Sketches were extracted by substituting the original tokens with their token types, except delimiters (e.g., "[", and ":"), operators (e.g., "+", and "*"), and built-in keywords (e.g., "True", and "while"). For instance, the expression "if s[:4].lower() == 'http':" becomes "if NAME [ : NUMBER ] . NAME ( ) == STRING :", with details about names, values, and strings being omitted. DJANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism  (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016) , which allows the fine meaning decoder (Section 3.2) to directly copy tokens from the natural language input. Copying Mechanism Recall that we use a softmax classifier to predict the probability distribution p (y t |y <t , x, a) over the pre-defined vocabulary. We also learn a copying gate g t ? [0, 1] to decide whether y t should be copied from the input or generated from the vocabulary. We compute the modified output distribution via: g t = sigmoid(w g ? h t + b g ) p (y t |y <t , x, a) = (1 ? g t )p (y t |y <t , x, a) + 1 [yt / ?Vy] g t k:x k =yt s t,k where w g ? R n and b g ? R are parameters, and the indicator function 1 [yt / ?Vy] is 1 only if y t is not in the target vocabulary V y ; the attention score s t,k (see Equation (  7 )) measures how likely it is to copy y t from the input word x k . 

 Natural Language to SQL The WIKISQL  (Zhong et al., 2017)  dataset contains 80, 654 examples of questions and SQL queries distributed across 24, 241 tables from Wikipedia. The goal is to generate the correct SQL query for a natural language question and table schema (i.e., table column names), without using the content values of tables (see the last block in Table  1  for an example). The dataset is partitioned into a training set (70%), a development set (10%), and a test set (20%). Each table is present in one split to ensure generalization to unseen tables. WIKISQL queries follow the format "SELECT agg op agg col WHERE (cond col cond op cond) AND ...", which is a subset of the SQL syntax. SELECT identifies the column that is to be included in the results after applying the aggregation operator agg op 2 to column agg col. WHERE can have zero or multiple conditions, which means that column cond col must satisfy the constraints expressed by the operator cond op 3 and the condition value cond. Sketches for SQL queries are simply the (sorted) sequences of condition operators cond op in WHERE clauses. For example, in Table  1 , sketch "WHERE > AND =" has two condition operators, namely ">" and "=". The generation of SQL queries differs from our previous semantic parsing tasks, in that the table schema serves as input in addition to natural language. We therefore modify our input encoder in order to render it table-aware, so to speak. Furthermore, due to the formulaic nature of the SQL query, we only use our decoder to generate the WHERE clause (with the help of sketches). The SELECT clause has a fixed number of slots (i.e., aggregation operator agg op and column agg col), which we straightforwardly predict with softmax classifiers (conditioned on the input). We briefly explain how these components are modeled below. as " c 1,1 ? ? ? c 1,|c 1 | ? ? ? c M,1 ? ? ? c M,|c M | ", where the k-th column ("c k,1 ? ? ? c k,|c k | ") has |c k | words. As shown in Figure  2   ? = [ ? ? ? |x| , ? ? ? 1 ] (12) analogously to Equations (  4 )-(  6 ). SELECT Clause We feed the question vector ? into a softmax classifier to obtain the aggregation operator agg op. If agg col is the k-th table column, its probability is computed via: WHERE Clause We first generate sketches whose details are subsequently decorated by the fine meaning decoder described in Section 3.2. As the number of sketches in the training set is small (35 in total), we model sketch generation as a classification problem. We treat each sketch a as a category, and use a softmax classifier to compute p (a|x): ?(x) = w 3 ? tanh (W 4 x + b 4 ) (13) p (agg col = k|x) ? exp{?([?, c k ])} (14) where M j=1 p (agg col = j|x) = 1, ?(?) is a scoring network, and W 4 ? R 2n?m , w 3 , b 4 ? R m are parameters. p (a|x) = softmax a (W a ? + b a ) where W a ? R |Va|?n , b a ? R |Va| are parameters, and ? is the table-aware input representation defined in Equation (  12 ). Once the sketch is predicted, we know the condition operators and number of conditions in the WHERE clause which follows the format "WHERE (cond op cond col cond) AND ...". As shown in Figure  3 , our generation task now amounts to populating the sketch with condition columns cond col and their values cond. Let {h t } |y| t=1 denote the LSTM hidden states of the fine meaning decoder, and {h att t } |y| t=1 the vectors obtained by the attention mechanism as in Equation (  9 ). The condition column cond col yt is selected from the table's headers. For the k-th column in the table, we compute p (cond col yt = k|y <t , x, a) as in Equation (  14 ), but use different parameters and compute the score via ?([h att t , c k ]). If the k-th table column is selected, we use c k for the input of the next LSTM unit in the decoder. Condition values are typically mentioned in the input questions. These values are often phrases with multiple tokens (e.g., Mikhail Snitko in Table  1 ). We therefore propose to select a text span from input x for each condition value cond yt rather than copying tokens one by one. Let x l ? ? ? x r denote the text span from which cond yt is copied. We factorize its probability as: p (cond yt = x l ? ? ? x r |y <t , x, a) = p l L yt |y <t , x, a p r R yt |y <t , x, a, l L yt p l L yt |y <t , x, a ? exp{?([h att t , ?l ])} p r R yt |y <t , x, a, l L yt ? exp{?([h att t , ?l , ?r ])} where l L yt / r R yt represents the first/last copying index of cond yt is l/r, the probabilities are normalized to 1, and ?(?) is the scoring network defined in Equation (  13 ). Notice that we use different parameters for the scoring networks ?(?). The copied span is represented by the concatenated vector [? l , ?r ], which is fed into a one-layer neural network and then used as the input to the next LSTM unit in the decoder. 

 Experiments We present results on the three semantic parsing tasks discussed in Section 4. Our implementation and pretrained models are available at https:// github.com/donglixp/coarse2fine. 

 Experimental Setup Preprocessing For GEO and ATIS, we used the preprocessed versions provided by  Dong and Lapata (2016) , where natural language expressions are lowercased and stemmed with NLTK  (Bird et al., 2009) , and entity mentions are replaced by numbered markers. We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact. We employed the preprocessed DJANGO data provided by  Yin and Neubig (2017) , where input expressions are tokenized by NLTK, and quoted strings in the input are replaced with place holders. WIK-ISQL was preprocessed by the script provided by  Zhong et al. (2017) , where inputs were lowercased and tokenized by Stanford CoreNLP . Configuration Model hyperparameters were cross-validated on the training set for GEO, and were validated on the development split for the other datasets. Dimensions of hidden vectors and word embeddings were selected from {250, 300} and {150, 200, 250, 300}, respectively. The dropout rate was selected from {0.3, 0.5}. Label smoothing  (Szegedy et al., 2016)  was employed for GEO and ATIS. The smoothing parameter was set to 0.1. For WIKISQL, the hidden size of ?(?) 

 Method GEO ATIS ZC07  (Zettlemoyer and Collins, 2007)  86.1 84.6 UBL  (Kwiatkowksi et al., 2010)  87.9 71.4 FUBL  (Kwiatkowski et al., 2011)  88.6 82.8 GUSP++  (Poon, 2013)  -83.5 KCAZ13  (Kwiatkowski et al., 2013)  89.0 -DCS+L  87.9 -TISP  (Zhao and Huang, 2015)  88.9 84.2 SEQ2SEQ  (Dong and Lapata, 2016)  84.6 84.2 SEQ2TREE  (Dong and Lapata, 2016)  87.1 84.6 ASN  (Rabinovich et al., 2017)  85.7 85.3 ASN+SUPATT  (Rabinovich et al., 2017)   Evaluation We use accuracy as the evaluation metric, i.e., the percentage of the examples that are correctly parsed to their gold standard meaning representations. For WIKISQL, we also execute generated SQL queries on their corresponding tables, and report the execution accuracy which is defined as the proportion of correct answers. 

 Results and Analysis We compare our model (COARSE2FINE) against several previously published systems as well as various baselines. Specifically, we report results with a model which decodes meaning representations in one stage (ONESTAGE) without leveraging sketches. We also report the results of several ablation models, i.e., without a sketch encoder and without a table-aware input encoder.   ) . Again we observe that the sketch encoder is beneficial and that there is an 8.9 point difference in accuracy between COARSE2FINE and the oracle. Results on WIKISQL are shown in Table  4 . Our model is superior to ONESTAGE as well as to previous best performing systems. COARSE2FINE's accuracies on aggregation agg op and agg col are 90.2% and 92.0%, respectively, which is comparable to SQLNET  (Xu et al., 2017) . So the most gain is obtained by the improved decoder of the WHERE clause. We also find that a tableaware input encoder is critical for doing well on this task, since the same question might lead to different SQL queries depending on the table schemas. Consider the question "how many presidents are graduated from A ". The SQL query over table " President College " is "SELECT We also examine the predicted sketches themselves in Table  5 . We compare sketches generated by COARSE2FINE against ONESTAGE. The latter model generates meaning representations without an intermediate sketch generation stage. Nevertheless, we can extract sketches from the output of ONESTAGE following the procedures described in Section 4. Sketches produced by COARSE2FINE are more accurate across the board. This is not surprising because our model is trained explicitly to generate compact meaning sketches. Taken together (Tables  2-4 ), our results show that better sketches bring accuracy gains on GEO, ATIS, and DJANGO. On WIKISQL, the sketches predicted by COARSE2FINE are marginally better compared with ONESTAGE. Performance improvements on this task are mainly due to the fine meaning decoder. We conjecture that by decomposing decoding into two stages, COARSE2FINE can better match table columns and extract condition values without interference from the prediction of condition operators. Moreover, the sketch provides a canonical order of condition operators, which is beneficial for the decoding process  (Vinyals et al., 2016; Xu et al., 2017) . 

 Conclusions In this paper we presented a coarse-to-fine decoding framework for neural semantic parsing. We first generate meaning sketches which abstract away from low-level information such as arguments and variable names and then predict missing details in order to obtain full meaning representations. The proposed framework can be easily adapted to different domains and meaning representations. Experimental results show that coarseto-fine decoding improves performance across tasks. In the future, we would like to apply the framework in a weakly supervised setting, i.e., to learn semantic parsers from question-answer pairs and to explore alternative ways of defining meaning sketches. Figure1: We first generate the meaning sketch a for natural language input x. Then, a fine meaning decoder fills in the missing details (shown in red) of meaning representation y. The coarse structure a is used to guide and constrain the output decoding. 

 Figure 3 : 3 Figure 3: Fine meaning decoder of the WHERE clause used for WIKISQL. 

 Table schema : schema Pianist Conductor Record Company Year of Recording Format [1] != 'as': a : if len ( NAME ) < NUMBER or NAME [ NUMBER ] != STRING : 17.9 WIKISQL 13.3 13.0 2.7 x : What record company did conductor Mikhail Snitko record for after 1996? y : SELECT Record Company WHERE (Year of Recording > 1996) AND (Conductor = Mikhail Snitko) a : WHERE > AND = 

 Table 1 : 1 Examples of natural language expressions x, their meaning representations y, and meaning sketches a. The average number of tokens is shown in the second column. with previous systems, despite employing relatively simple sequence decoders. 

 Algorithm 1 Sketch for GEO and ATISInput: t: Tree-structure ?-calculus expression t.pred: Predicate name, or operator name Output: a: Meaning sketch (count $0 (< (fare $0) 50:do))?(count#1 (< fare@1 ?)) function SKETCH(t) if t is leaf then No nonterminal in arguments return "%s@%d" % (t.pred, len(t.args)) if t.pred is ? operator, or quantifier then e.g., count Omit variable information defined by t.pred t.pred ? "%s#%d" % (t.pred, len(variable)) for c ? argument in t.args do if c is nonterminal then c ? SKETCH(c) else c ? "?" Placeholder for terminal return t 

 Table - - Aware Input Encoder Given a table schema with M columns, we employ the special token " " to concatenate its header names ? ? ? ? LSTM units Vectors Attention Question-to-Table Attention || college || number of presidents || Input Question Column 1 Column 2 Figure 2: Table-aware input encoder (left) and ta- ble column encoder (right) used for WIKISQL. 

 , we use bi-directional LSTMs to encode the whole sequence. Next, for column c k , the LSTM hidden states at positions c k,1 and c k,|c k | are concatenated. Finally, the concatenated vectors are used as the encoding vectors {c k } M k=1 for table columns. As mentioned earlier, the meaning representations of questions are dependent on the tables. As shown in Figure2, we encode the input question x into {e t } |x| t=1 using LSTM units. At each time step t, we use an attention mechanism towards table column vectors {c k } M k=1 to obtain the most relevant columns for e t . The attention score from e t to c k is computed via u t,k ? exp{?(e t ) ? ?(c k )}, where ?(?) is a one-layer neural network, and M k=1 u t,k = 1. Then we compute the con-text vector c e t = M k=1 u t,k c k to summarize the relevant columns for e t . We feed the concate-nated vectors {[e t , c e t ]} |x| t=1 into a bi-directional LSTM encoder, and use the new encoding vectors {? t } |x| t=1 to replace {e t } |x| t=1 in other model com- ponents. We define the vector representation of input x as: 

 Table 2 presents our results on GEO and ATIS. Overall, we observe that COARSE2FINE outperforms ONESTAGE, which suggests that disentangling high-level from low-level information dur- Method Accuracy Retrieval System 14.7 Phrasal SMT 31.5 Hierarchical SMT 9.5 SEQ2SEQ+UNK replacement 45.1 SEQ2TREE+UNK replacement 39.4 LPN+COPY (Ling et al., 2016) 62.3 SNM+COPY (Yin and Neubig, 2017) 71.6 ONESTAGE 69.5 COARSE2FINE 74.1 ? sketch encoder 72.1 + oracle sketch 83.0 Table 3: DJANGO results. Accuracies in the first and second block are taken from Ling et al. (2016) and Yin and Neubig (2017). ing decoding is beneficial. The results also show that removing the sketch encoder harms perfor- mance since the decoder loses access to additional contextual information. Compared with previous neural models that utilize syntax or grammatical information (SEQ2TREE, ASN; the second block in Table 2), our method performs competitively despite the use of relatively simple decoders. As an upper bound, we report model accuracy when gold meaning sketches are given to the fine mean- ing decoder (+oracle sketch). As can be seen, pre- dicting the sketch correctly boosts performance. The oracle results also indicate the accuracy of the fine meaning decoder. Table 3 reports results on DJANGO where we observe similar tendencies. COARSE2FINE out- performs ONESTAGE by a wide margin. It is also superior to the best reported result in the literature (SNM+COPY; see the second block in the table 

 Table 4 : 4 Evaluation results on WIKISQL. Accuracies in the first block are taken from Zhong et al. (2017)  and Xu et al. (2017) . Method Accuracy Execution Accuracy SEQ2SEQ 23.4 35.9 Aug Ptr Network 43.3 53.3 SEQ2SQL (Zhong et al., 2017) 48.3 59.4 SQLNET (Xu et al., 2017) 61.3 68.0 ONESTAGE 68.8 75.9 COARSE2FINE 71.7 78.5 ? sketch encoder 70.8 77.7 ? table-aware input encoder 68.6 75.6 + oracle sketch 73.0 79.6 Method GEO ATIS DJANGO WIKISQL ONESTAGE 85.4 85.9 73.2 95.4 COARSE2FINE 89.3 88.0 77.4 95.9 

 Table 5 : 5 Sketch accuracy. For ONESTAGE, sketches are extracted from the meaning representations it generates. COUNT(President) WHERE (College = A)", but the query over table " College Number of Presidents " would be "SELECT Number of Presidents WHERE (College = A)". 

			 https://docs.python.org/3/library/ tokenize 

			 agg op ? {empty, COUNT, MIN, MAX, SUM, AVG}. 3 cond op ? {=, <, >}.
