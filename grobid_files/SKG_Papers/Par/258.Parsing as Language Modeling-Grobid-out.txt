title
Parsing as Language Modeling

abstract
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing -93.8 F 1 on section 23, using 2-21 as training, 24 as development, plus tri-training. When trees are converted to Stanford dependencies, UAS and LAS are 95.9% and 94.1%.

Introduction Recent work on deep learning syntactic parsing models has achieved notably good results, e.g.,  Dyer et al. (2016)  with 92.4 F 1 on Penn Treebank constituency parsing and  Vinyals et al. (2015)  with 92.8 F 1 . In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F 1 , with a comparatively simple architecture. In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem. Section 2 looks more closely at three of the most relevant previous papers. We then describe our exact model (Section 3), followed by the experimental setup and results (Sections 4 and 5). There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.) 

 Language Modeling Formally, a language model (LM) is a probability distribution over strings of a language: P (x) = P (x 1 , ? ? ? , x n ) = n t=1 P (x t |x 1 , ? ? ? , x t?1 ), (1) where x is a sentence and t indicates a word position. The efforts in language modeling go into computing P (x t |x 1 , ? ? ? , x t?1 ), which as described next is useful for parsing as well. 

 Parsing as Language Modeling A generative parsing model parses a sentence (x) into its phrasal structure (y) according to argmax y ?Y(x) P (x, y ), where Y(x) lists all possible structures of x. If we think of a tree (x, y) as a sequence (z)  (Vinyals et al., 2015)  as illustrated in Figure  1 , we can define a probability distribution over (x, y) as follows: P (x, y) = P (z) = P (z 1 , ? ? ? , z m ) = m t=1 P (z t |z 1 , ? ? ? , z t?1 ), (2) which is equivalent to Equation (1). We have reduced parsing to language modeling and can use language modeling techniques of estimating P (z t |z 1 , ? ? ? , z t?1 ) for parsing. 

 Previous Work We look here at three neural net (NN) models closest to our research along various dimensions. The first  (Zaremba et al., 2014)  gives the basic language modeling architecture that we have adopted, while the other two  (Vinyals et al., 2015; Dyer et al., 2016)  are parsing models that have the current best results in NN parsing. 

 LSTM-LM The LSTM-LM of  Zaremba et al. (2014)  turns (x 1 , ? ? ? , x t?1 ) into h t , a hidden state of an LSTM  (Hochreiter and Schmidhuber, 1997; Gers et al., 2003; Graves, 2013) , and uses h t to guess x t : P (x t |x 1 , ? ? ? , x t?1 ) = P (x t |h t ) = softmax(W h t )[x t ], where W is a parameter matrix and [i] indexes ith element of a vector. The simplicity of the model makes it easily extendable and scalable, which has inspired a character-based LSTM-LM that works well for many languages  (Kim et al., 2016)  and an ensemble of large LSTM-LMs for English with astonishing perplexity of 23.7  (Jozefowicz et al., 2016) . In this paper, we build a parsing model based on the LSTM-LM of  Zaremba et al. (2014) .  et al. (2015)  observe that a phrasal structure (y) can be expressed as a sequence and build a machine translation parser (MTP), a sequence-tosequence model, which translates x into y using a conditional probability: 

 MTP 

 Vinyals P (y|x) = P (y 1 , ? ? ? , y l |x) = l t=1 P (y t |x, y 1 , ? ? ? , y t?1 ), where the conditioning event (x, y 1 , ? ? ? , y t?1 ) is modeled by an LSTM encoder and an LSTM decoder. The encoder maps x into h e , a set of vectors that represents x, and the decoder obtains a summary vector (h t ) which is concatenation of the decoder's hidden state (h d t ) and weighted sum of word representations ( n i=1 ? i h e i ) with an alignment vector (?). Finally the decoder predicts y t given h t . Inspired by MTP, our model processes sequential trees. 

 RNNG Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree  (Dyer et al., 2016) : P (x, y) = P (a) = m t=1 P (a t |a 1 , ? ? ? , a t?1 ), (3) where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2). RNNG and our model differ in how they compute the conditioning event (z 1 , ? ? ? , z t?1 ): RNNG combines hidden states of three LSTMs that keep track of actions the model has taken, an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM's hidden state as shown in the next section. 

 Model Our model, the model of  Zaremba et al. (2014)  applied to sequential trees and we call LSTM-LM from now on, is a joint distribution over trees: P (x, y) = P (z) = m t=1 P (z t |z 1 , ? ? ? , z t?1 ) = m t=1 P (z t |h t ) = m t=1 softmax(W h t )[z t ], where h t is a hidden state of an LSTM. Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y (x), whose size is polynomial, and use LSTM-LM to find y that satisfies argmax y ?Y (x) P (x, y ). (4) 

 Hyper-parameters The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50. We initialize starting states with previous minibatch's last hidden states  (Sutskever, 2013) . The forget gate bias is initialized to be one  (Jozefowicz et al., 2015)  and the rest of model parameters are sampled from U(?0.05, 0.05). Dropout is applied to non-recurrent connections  (Pham et al., 2014)  and gradients are clipped when their norm is bigger than 20  (Pascanu et al., 2013) . The learning rate is 0.25 ? 0.85 max  ( ?15, 0)  where is an epoch number. For simplicity, we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax  (Morin and Bengio, 2005)  or noise contrastive estimation (Gutmann and Hyv?rinen, 2012). 

 Experiments We describe datasets we use for evaluation, detail training and development processes. 1 

 Data We use the Wall Street Journal (WSJ) of the Penn Treebank  (Marcus et al., 1993)  for training (2-21), development (24) and testing (  23 ) and millions of auto-parsed "silver" trees  (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015)  for tritraining. To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword  (Parker et al., 2011)  with a product of eight Berkeley parsers (Petrov, 2010) 2 and ZPar  (Zhu et al., 2013)  and select 24 million trees on which both parsers agree  (Li et al., 2014) . We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ (Vinyals et 1 The code and trained models used for experiments are available at github.com/cdg720/emnlp2016.  2  We use the reimplementation by  Huang et al. (2010) .    (Charniak, 2000)  performed better when trained on all of 24 million trees than when trained on resampled two million trees. Given x, we produce Y (x), 50-best trees, with Charniak parser and find y with LSTM-LM as  Dyer et al. (2016)  do with their discriminative and generative models. 3 

 Training and Development 

 Supervision We unk words that appear fewer than 10 times in the WSJ training (6,922 types) and drop activations with probability 0.7. At the beginning of each epoch, we shuffle the order of trees in the training data. Both perplexity and F 1 of LSTM-LM (G) improve and then plateau (Figure  2 ). Perplexity, the Base Final  Vinyals et al. (2015)    

 Semi-supervision We unk words that appear at most once in the training (21,755 types). We drop activations with probability 0.45, smaller than 0.7, thanks to many silver trees, which help regularization. We train LSTM-LM (GS) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only. Training takes 26 epochs and 68 hours on a Titan X. LSTM-LM (GS) achieves 92.5 F 1 on the development. 

 Results 

 Supervision As shown in Table  2 , with 92.6 F 1 LSTM-LM (G) outperforms an ensemble of five MTPs  (Vinyals et al., 2015)  and RNNG  (Dyer et al., 2016) , both of which are trained on the WSJ only. 

 Semi-supervision We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 (HC)  (Vinyals et al., 2015) ; and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of English-German translation sentence pairs  (Luong et al., 2016) . We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature. Parsers' parsing performance along with their training data is reported in Table  3 . LSTM-LM (GS) outperforms all the other parsers with 93.1 F 1 . 

 Improved Semi-supervision Due to search errors -good trees are missing in 50-best trees -in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y (x). To mitigate the search problem, we tri-train Charniak (GS) on all of 24 million NYT trees in addition to the WSJ, to yield Y (x). As shown in Table  3 , both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y (x). A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F 1 . When trees are converted to Stanford dependencies, 5 UAS and LAS are 95.9% and 94.1%, 6 more than 1% higher than those of the state of the art dependency parser  (Andor et al., 2016) . Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered  (Kong and Smith, 2014) . 

 Conclusion The generative parsing model we presented in this paper is very powerful. In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models  (Dyer et al., 2016) . We suspect building large models with character embeddings would lead to further improvement as in language modeling  (Kim et al., 2016; Jozefowicz et al., 2016) . We also wish to develop a complete parsing model using the LSTM-LM framework.  Figure 1: A tree (a) and its sequential form (b).There is a one-to-one mapping between a tree and its sequential form. (Part-of-speech tags are not used.) 
