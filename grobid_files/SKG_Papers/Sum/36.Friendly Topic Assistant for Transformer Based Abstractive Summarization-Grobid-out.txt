title
Friendly Topic Assistant for Transformer Based Abstractive Summarization

abstract
Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformerbased models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.

Introduction Automatic summarization, requiring both document understanding and text generation, is a comprehensive task in natural language processing (NLP). Extractive approaches  (Wong et al., 2008; Liu, 2019; Zhang et al., 2019c)  identify and then concatenate the most representative sentences as a summary. By contrast, abstractive summarization  (See et al., 2017; Narayan et al., 2018)  is more challenging, aiming to generate a summary via rephrasing and introducing new concepts/words. Our work focuses on abstractive summarization, for which sequence-to-sequence (S2S) models are widely studied. * Equal contribution. ? Corresponding author. Recently, equipped with the attention mechanism  (Vaswani et al., 2017) , some Transformerbased language models  (Subramanian et al., 2019; Zhang et al., 2019b; Dong et al., 2019; Liu and Lapata, 2019; Lewis et al., 2019; Raffel et al., 2019)  are built with an encoder-decoder structure. These models benefit from pre-training on large-scale corpus, and then are fine-tuned to adapt to the summarization task. As a result, the encoder with bidirectional self-attention (SA) extracts document-token features, the decoder with left-to-right SA generates the summary, and the cross attention (CA) bridges the document and summary tokens. Though achieving appealing performances, these Transformer-based models are better at exploring the relationships among local tokens than the document global semantics. Further, due to the limited position index during pre-training, most Transformer-based models have a maximum capacity of input tokens. Thus, they often truncate the length of a document to satisfy the length limitation of the encoder, which may lose some important semantics, especially for long documents. Global semantics are important to summarization  (Narayan et al., 2018; Ailem et al., 2019; , since one need to comprehend the entire content before generate summaries. Compared with language models, topic models tell global semantics more explicitly. Basically, topic models, such as LDA  (Blei et al., 2003)  and PFA  (Zhou et al., 2012) , represent each document as a bagof-word (BOW) vector and then factor the count vector as a product of topics and topic proportions, as shown in Fig.  1 . Topics are global variables, describing the distributions over all tokens in the vocabulary. Topic proportions are local (documentspecific) features, describing the weights of corresponding topics in each document. Therefore, topic models explore the word co-occurrence patterns, i.e., semantics. However, no Transformer-based model considers these explicit semantics. In this paper, we rearrange and further explore the semantics of the topic model and develop a friendly topic assistant (TA) for Transformer-based abstractive summarization models. By introducing only a small number of parameters into the finetuning stage, TA is a flexible plug-and-play model, consisting of three modules: ? Semantic-informed attention (SIA): It is often observed that the learned attentive patterns of many heads are not as reasonable as we expect  (Clark et al., 2019; Michel et al., 2019) . This motivates us to employ the semantic "distribution over topics" as a token representation to construct an explicit semantic-similarity matrix among tokens, which is further used as the attention weights of a newly added head. ? Topic embedding with masked attention (TEMA): Since a topic is a distribution over tokens in the vocabulary, we use the mixture of token embeddings to represent the corresponding topic embedding. Thus, topics with large proportions for a document can be considered as extra input tokens of the decoder. Further, a topic describes a co-occurrence pattern of tokens with similar semantics, that is more likely to help the decoder to generate new tokens or concepts not included in the current document. To prevent the topic features affected by the summary-token features via attention, we perform masked attention in the decoder. ? Document-related modulation (DRM): Conditional biasing is an efficient way to integrate conditions into the network with a small number of extra parameters  (Dumoulin et al., 2018) . The topic-proportion vector is a lowdimensional document representation, conditioned on which we infer a document-related bias to modulate some hidden layers of the decoder. TA does not break any structure of the original Transformer network, and hence is able to be jointly learned with a pre-trained model during the fine-tuning stage. Besides, SIA, TEMA, and DRM are cooperated with some basic Transformer modules, such as embedding and multi-head attention. Therefore, we can plug an arbitrary combination of these three modules into various Transformerbased models. 2 Related work 

 Transformer-based models for document summarization Pre-training and fine-tuning have attracted much attention in Transformer-based models for various NLP tasks. Equipped with pre-trained Bert encoder  (Devlin et al., 2019) ,  Liu (2019) ;  Liu and Lapata (2019)  propose the BertSUM for both extractive and abstractive tasks;  Zhang et al. (2019c)  propose a hierarchical Bert model for extractive summarization, where the low-level and high-level Berts are built for sentence and document understanding, respectively. Although the above methods achieve better performance than LSTM-based models, their Bert encoder pre-trained for document understanding may not well match the decoder trained from scratch for the summary generation  (Rothe et al., 2019; Yang et al., 2019) . To consider document understanding and generation in a unified framework, some S2S pre-training models are proposed for general purpose, such as MASS  (Song et al., 2019) , UniLM  (Dong et al., 2019) , T5  (Raffel et al., 2019) , and BART  (Lewis et al., 2019) , which are further fine-tuned for downstream tasks, summarization included. Aiming at designing a pre-training objective tailored for abstractive text summarization,  Zhang et al. (2019b)  propose the PEGASUS that achieves the state-of-the-art performance. 

 S2S models combined with Topic models To complement global semantics for S2S models that often focus on sequential information, topic models  (Blei et al., 2003; Zhou et al., 2012)  are considered to be combined with S2S models.  Zhang et al. (2016)  represent each word as a distribution over topics, and construct a topic-informed RNN model for neural machine translation. Based on the RNN-based pointer-generator network  (See et al., 2017) ,  Ailem et al. (2019)  develop a topic augmented decoder that generates a summary conditioned on both the input document and the latent topics of the document. They find that the latent topics reveal more global semantic information that can be used to bias the decoder to generate words. With similar considerations,  Narayan et al. (2018)  propose another topic-conditioned S2S model under the CNN framework. Although these models have demonstrated the advantages of combining S2S learning with topic models, integrating topic information into Transformer-based summarization models is still an underexplored research area. 

 Background TA is a friendly plug-and-play model that is compatible with many transformer-based summarization models. To illustrate TA without loss of generality, we choose the BertSUM  (Liu and Lapata, 2019)  as an example Transformer-based model, and PFA  (Zhou et al., 2012)  as an example topic model. 

 BertSUM: a Transformer-based summarization model Given a data pair {x, y}, where the document x has N 1 tokens and the summary y has N 2 tokens (N 2 < N 1 ), BertSUM maximizes the following likelihood N 2 j=1 p(y j |{x i } N 1 i=1 , y i<j ), (1) where x i and y i denote the i-th token in document and summary, respectively. BertSUM adopts an encoder-decoder architecture, as shown in Fig.  1 . The encoder is a pretrained twelve-layer Bert  (Devlin et al., 2019) , each layer mainly including a bidirectional SA and a fully-connected network (FNN). The encoder outputs the document-token features H ? R N 1 ?d model at the top layer, where d model is the output dimension of each module (e.g., embedding, SA, CA, and FNN) in Transformer. The decoder is a randomlyinitialized six-layer Transformer decoder  (Vaswani et al., 2017) , each layer mainly including SA, CA, and FNN. Due to the auto-regressive nature of the summary generation in (1), the decoder performs left-to-right SA. The CA forces the summary-token features to attend over all features in H. 

 Poisson factor analysis (PFA) Topic models are good at capturing global semantics of texts  (Zhang et al., 2019a; Wang et al., 2020) . PFA  (Zhou et al., 2012)  is a typical topic model inferred by Gibbs sampling or variational autoencoder  (Zhang et al., 2018) . Specifically, representing document x as a BOW vector b ? Z V , where Z = {0, 1, ? ? ? } and V is the vocabulary size, PFA models b under the Poisson likelihood as b ? Poisson (?) , ? ? Gamma (r, 1) . (  2 ) In (2), the k-th column of ? ? R V ?K + , denoted as ? k ? R V + , represents the k-th topic, which is a 

 Multi-Head Attention 

 Semantic-Informed Attention Feed Forward Add & Norm Add & Norm 

 Masked Self-Attention Multi-Head Attention 

 Semantic-Informed Attention Token Embedding + Position Embedding Token Embedding + Position Embedding Topic Embedding 

 Document Summary Masked Cross-Attention 

 Multi-Head Attention 

 Semantic-Informed Attention Feed Forward Add & Norm 

 Generated Summary Linear + Softmax  distribution over all tokens in the vocabulary. For this purpose, PFA applies a Dirichlet prior on ? k as ? k ? Dirichlet (? k ). ? ? R K is the documentspecific topic proportion vector (document feature) that represents the strength of the document on each topic. Thus, using the law of total expectation on (2), we have E [b | ?, ?] = ?, which means that a document can be decomposed as a weighted summation of topics, as illustrated in Fig.  1 . V K Q V V K Q V V K Q V 

 Topic assistant for Transformer Given a corpus, we train a PFA based on documents. Then, we use the extracted topics and topic proportions to build three plug-and-play modules to help the Transformer fine-tuning, including semanticinformed attention, topic embedding with masked attention, and document-related modulation. 

 Semantic-informed attention (SIA) In Transformer-based models, the multi-head attention explores the relationships among tokens by calculating the token similarities in implicit feature spaces. Specifically, assume we have h heads, thus the attention function Att(?) in the i-th head is formulated as: head i = Att(Q i , K i , V i ) = A i V i , (3) A i = softmax Q i K i T ? d k , i = 1, ? ? ? , h, (4) where, A i is the attention matrix, Q i , K i , and V i are learnable features, denoting queries, keys, and  However, recent works have illustrated that most attention heads learn simple, and often redundant, positional patterns  (Clark et al., 2019; Michel et al., 2019) . To improve the representation, some works incorporate external information, such as syntax, into the Transformer-based neural machine translation  (Currey and Heafield, 2019; Deguchi et al., 2019) . Inspired by their achievements and to focus on our summarization task, we attempts to inject the semantics learned from a topic model into the attention mechanism. Besides, Raganato et al. (  2020 ) tried to fix the attention matrices of many heads according to token positions, finding that the performance do not drop and is even better in some cases. Motivated by this phenomenon, we introduce an extra head (the (h + 1)-th head) with a fixed attention matrix to express a semantic-informed attentive pattern. Recapping ? in (2), each column, ? k , is a distribution over all tokens, representing a topic. From another view, each row, ? v,: , is a token representation, as shown in Fig.  1 . With normalization {? v } V v=1 = ? v,: /||? v,: || 1 , (5) ? v can be interpreted as a distribution over topics. Thus, we can measure the similarity between tokens using the cosine distance, i.e., cos(? v 1 , ? v 2 ), which is an explicit and fixed semantic relation. In Fig.  2 , based on XSum  (Narayan et al., 2018) , we use UMAP 1 to project {? v } V v=1 in (5) into a 2dimensional space to visualize their relations. We choose six regions, and randomly select 10 example tokens from each region. Clearly, i) words with similar meanings are often grouped together, describing a field; ii) if two fields are semantically related, their corresponding tokens are closely distributed, such as "Economic-Government" and "Astronomy-Airplane". These phenomena indicate that ? v in SIA is able to describe the semantic relations among tokens. To sum up, we consider SIA as an extra head (the (h+1)-th head) in every attention layer, as shown in Fig.  3 (a), with its attention matrix formally stated as A h+1 = softmax cos(? v 1 , ? v 2 ) ? d k , (6) ? ? ? ? ? ? ? v 1 , v 2 : document-token indexes, in encoder SA v 1 , v 2 : summary-token indexes, in decoder SA v 1 : summary-token index v 2 : document-token index in decoder CA where, [?] denotes a matrix. Then, the output of multi-head attention is obtained by: Concat(head 1 , ? ? ? , head h , head h+1 )W a , = Concat(head 1 , ? ? ? , head h )W a ori + head h+1 W a add (7) where, W a ? R (h+1)dv?d model is rearranged as two parameter matrices W a ori ? R hdv?d model and W a add ? R dv?d model . Clearly, W a add encapsulates the parameters brought by the SIA. 

 Topic embedding with masked attention (TEMA) Given a corpus, the topic model is able to learn global topics ?. For a specific document x, the corresponding topic proportion vector ? illustrates the importance degree of every topic. Therefore, those important topics represent the major or pertinent semantics of the document, which is expected to help the decoder to generate a summary. For this purpose, we perform topic embedding so that the Transformer-based models can understand such topic representation. Recapping ? in (2), each column (topic), ? k , is a distribution over all tokens in the vocabulary. Thus, we consider each topic embedding as a mixture of all token embeddings, as shown in Fig.  1 , formally stated as: E topic = ? T E token (8) where, E topic ? R K?d model and E token ? R V ?d model are the topic and token embedding matrices, respectively. Clearly, topics and tokens lie in the same embedding space, making it possible to measure the relationships between document-topics and summary-tokens via attention. Specifically, we choose the top-n topics according to ?, and consider these n topic embeddings as extra decoder inputs to guide the generation. We expect that the attention mechanism could fuse the topic information into the generation. Meanwhile, we should prevent the topic features polluted by the summary-token features via attention. Therefore, we build two kinds of masks for the SA and CA in decoder, as shown in Fig.  3(b) . As discussed before, a topic describes a cooccurrence pattern of all tokens. Moreover, recalling (8), each topic embedding vector can be interpreted as a semantic clustering center of all token embedding vectors, surrounded by tokens with similar semantics, as shown in Fig.  5 (a) later. Using topics as inputs, the decoder is more likely to generate some recapitulative or new concepts that do not appear in the current document. 

 Document-related modulation (DRM) Feature biasing is an efficient way to integrate conditions  (Dumoulin et al., 2018) .  Subramani et al. (2019)  introduced a sentence-specific bias into a pre-trained language model, showing superior performance on out-of-sample reconstruction. As shown in (2), the topic proportion vector ? is a latent representation of document x, which can be considered as a conditioning information to fine-tune the Transformer-based models. To this end, we leverage ? to infer a bias to modulate one hidden layer in every decoder layer. Specifically, in the l-th decoder layer, we infer a global feature bias via: z (l) = ? T W (l) b ? R d model . ( 9 ) where, W b ? R K?d model is a parameter matrix in DRM. The bias vector z  (l)  is then added to every position of the output of the CA block (before add and norm), as shown in Fig.  1 . 

 Properties of TA TA has three attractive properties, making it friendly to practical applications. Small parameter footprint TA introduces three modules for the original Transformer encoderdecoder architecture: SIA, TEMA, and DRM. Among them, TEMA needs no extra parameters while SIA and DRM only introduce a small number of parameters compared with the original models, detailed illustrated in Table  8 . Therefore, TA can be applied in many Transformer encoder-decoder structures without adding too much memory footprint or sacrificing the learning and test speed. Plug-and-play The pipeline of pre-training and then fine-tuning has been widely accepted in NLP community, especially for transformer-based models. There are mainly two reasons. Firstly, many well pre-trained models provide checkpoints for users to fine-tune on their own tasks. Secondly, the Transformer models are getting bigger and bigger  (Sanh et al., 2019) , resulting in a fact that it is almost impossible to pre-train such a big model on a personal computer. Thus, models with plug-andplay property are attractive to Transformer-based models  (Dathathri et al., 2019) . Although we introduce TA based on a specific model, BertSUM, TA owns flexible plug-and-play property, since SIA, TEMA, and DRM do not break any structure of the original network. In experiments, shown in Table  5  later, we also demonstrate the effectiveness of TA on other Transformer-based summarization models, such as BART  (Lewis et al., 2019) , UNILM  (Dong et al., 2019) , and MASS  (Song et al., 2019) . 

 Efficient training The autoencoding variational inference (AVI)  (Zhang et al., 2018)  makes PFA scalable to big corpus and fast in out-of-sample prediction (calculating document-specific ?). In experiments, we find that the engineering-friendly pipeline training strategy 2 achieves attractive performance. 

 Experiments 

 Datasets We evaluate the effectiveness and efficiency of TA on three benchmark datasets, including the CNN/DailyMail (CNN/DM)  (Hermann et al., 2015) , the New York Times Annotated Corpus (NYT)  (Sandhaus, 2008)  and the XSum  (Narayan et al., 2018) . The summary styles of these datasets varies from highlights, composed of several sentences, to very brief one sentence. Table  1  provides the statistics of these datasets. See more detailed descriptions in Appendix A. We perform data preprocessing following  Liu and Lapata (2019) . 

 Implementation details Given a dataset, we first train the PFA based on the documents in the training set to obtain ?, composed of 256 topics. More analysis on the number of topics can be found in Apendix B. For each document, we infer the corresponding ? using the AVI in  Zhang et al. (2018) . According to the values in ?, we choose top-5 topics to perform topic embedding in TEMA. We adopts the settings in the original Transformer-based models. Following  Liu and Lapata (2019) , in the test stage, we use beam search with size 5, select the top-3 checkpoints based on their evaluation loss on the validation set, and report the averaged results on the test set. More detailed settings can be found in Appendix B. Our code is available at https://github.com/BoChenGroup/TA. 

 Quality evaluation on summarization We evaluate the quality of the generated summaries using ROUGE scores  (Lin, 2004) . We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) to assess informativeness, and the longest common subsequence (ROUGE-L) to assess fluency. 

 TA with BertSUM We first combine TA with BertSUM on the abstractive summarization task. Given BertSUM checkpoints 3 on CNN/DM and XSum provided by Liu 2 1) Pre-train a Transformer; 2) Train PFA to extract ? and ?; 3) Fine-tune the Transformer+TA. 3 https://github.com/nlpyang/PreSumm   2, 3 , and 4, respectively. Methods in the first group are LSTM-based or CNNbased models. Compared with them, the outperformance of BertSUM illustrates that the combination of a pre-trained Bert encoder and a Transformer decoder is a better S2S structure. Though having the same structure as the Transformer, the BertSUM employs a Bert encoder pre-trained on a very large corpus, showing higher scores. Equipped with TA, BertSUM+TA achieves superior performance than the BertSUM, with only a few extra parameters, which will be further illustrated in Table  8 .   (Lewis et al., 2019) , UNILM  (Dong et al., 2019)  and MASS  (Song et al., 2019) . Results of the UNILM on XSum are obtained by running the public code. Others are from  Zhang et al. (2019b) .  42.12 19.50 39.01 39.75 17.24 31.95 MASS+TA 43.06 19.98 39.88 41.12 18.05 32.75 UNILM 43.33 20.21 40.51 42.63 19.10 33.13 UNILM+TA 43.87 20.78 40.65 43.70 20.01 34.56 BART 44.16 21.28 40.90 45.14 22.27 37.25 BART+TA 44.47 21.39 41.32 45.76 22.68     Model CNN/DM XSum R1 R2 RL R1 R2 RL MASS 

 TA with some advanced Transformers As discussed above, TA is a plug-and-play model, that is friendly to many Transformer encoderdecoder architectures. To illustrate it, we plug TA into BART  (Lewis et al., 2019) , UNILM  (Dong et al., 2019) , and MASS  (Song et al., 2019) , which are some advanced language models for document understanding and generation. Based on their pre-trained models, we jointly fine-tune the model with TA for CNN/DM and XSum, respectively, following their settings in public codes. The ROUGE comparisons 4 are shown in Table  5 , while the numbers of new parameters brought by TA are listed in Table  8 . It is observed that TA is able to improve different types of Transformer encoder-decoder models for abstractive summarization with a few extra parameters.  4  All these methods did not provide results on NYT. 

 TA with different-length documents To analyze the effectiveness of TA as the documents have different amounts of tokens, we calculate the improvement of BertSUM+TA over Bert-SUM in terms of the ROUGE scores, with the results shown in Fig.  4 . Note that, as the number of document-tokens exceeds 512 (the length limitation of the Bert encoder), both BertSUM and BertSUM+TA use the initial 512 document-tokens as the inputs of the encoder. Compared with BertSUM that ignores the subsequent document-tokens, BertSUM+TA is able to reserve these information in some degree, since the topic model extracts global semantics from all tokens in the document. As a result, with the increase of the document length, the improvement produced by TA gets more evident. In other words, the global semantics introduced by TA is indeed helpful to Transformer-based models on the summarization task, especially for long documents. 

 Semantic similarity As mentioned before, TA aims to exploit the semantics provided by the topic model to boost the summariazation performance. To evaluate the semantic similarities between the generated summary and the document (or the gold summary), we propose a new criterion, Semantic Similarity (SS). Given a set of topics ? and two pieces of text, D 1 and D 2 , we firstly infer the topic proportions of these two pieces of text, i.e., ? 1 and ? 2 . Then, the simantic similarity (between D 1 and D 2 ) with respect to ? can be measured via the cosine similarity between ? 1 and ? 2 , as: SS(D 1 , D 2 ; ?) = ? T 1 ? 2 ? 1 ? 2 . ( 10 ) In our case, after learning the topics of the documents, we use them to further infer the topic proportions of the document ? d , topic proportions of the gold summary (ground truth) ? g , and the topic proportions of the generated summary ? s . Then, we measure the cosine similarities between ? s and ? d , ? s and ? g . The averaged SS scores on CNN/DM and XSum are summarized in Table  6 . It can be seen that, with the help of TA, the generated summaries are closer to (have higher similarities to) the document, and also closer to the ground truth in the semantic space.  

 Ablation study TA includes three modules: SIA, TEMA, and DRM. In order to understand the effectiveness of each part, we perform ablation studies by combining each module with the BertSUM. As shown in Table  7 , all these modules are able to promote the summarization performance in different degrees. Specifically, SIA introduces explicit semantic relations between tokens. Though effective, SIA mainly focuses on the local relations as the standard Transformer attention does. Compared with SIA, TEMA and DRM are better at introducing global semantics (topics and topic proportions) into the Transformer-based models, achieving more evident improvements. This illustrates that the global semantics, a special "summary", is useful to the summarization task. 

 Model size We plug SIA, TEMA, DRM, and TA into some base models 5 . The amount and ratio of the newly added parameters are listed in Table  8 . Clearly, TA introduces a few parameters, less than 10%. Therefore, TA has a friendly memory footprint to the Transformer models. 

 Effectiveness of TEMA As analyzed in Sections 5.6 and 5.7, TEMA is effective for the summarization and adds surprisingly no extra parameter for the Transformers, which excites our curiosity to further analyze TEMA. TEMA utilizes topic embeddings as part of the decoder inputs. Assisted by the masked attention,  TEMA helps the decoder to perform conditional generation, with topics acting as the conditions. Therefore, we plan to investigate the generative ability of TEMA in a pure decoder architecture. To this end, we plug TEMA into the GPT2 small 6  (Radford et al., 2019)  and perform fine-tuning, where we consider each training summary as a training sample. After fine-tuning, we perform summary generation based on the document-topics only 7 . Fig.  5(a)  shows some example topics learned from the documents in CNN/DM and XSum. We consider two kinds of conditional topics: i) two randomly selected topics and ii) top-three topics of a document. Some generated sentences are provided in Fig.  5(b) . Whether the topics are representative of a specific document or not, the decoder is able to generate fluent and meaningful samples, that well match the conditional topics. Moreover, compared with setting-ii, the style of the generated sentences under setting-i is not similar to that of the news in CNN/DM and XSum. This further illustrates that topics are able to provide new concepts for the generation. Table  9  provides the perplexities on the test set. Clearly, with TEMA, GPT2 achieves lower perplexities. 

 Generated summary examples In Fig.  6 , we show an example of the generated summaries of BertSUM and BertSUM+TA. It can be seen that BertSUM+TA generates some meaningful and recapitulative words that do not appear 1: animal puppy pets terri dog cat owner mary horse playing 2: road motor incident ride scrutiny roads vehicles wheel riding bike 3: doctor suffering legs stroke arms bodies issue severe attacks depression 4: armed robbery violence victim witness police murder crime arrested knife 5: tourist tourism video camera visit stay youtube videos hotel view Five example topics of CNN/DM 6: horse bull horses Jockey racing trainer horn ride chase hurdle 7: car driver driven vehicle ford drove bmw Nissan garage parked 8: media social comments interview newspaper journalist report headline editor critic 9: government florida texas chicago states carolina miami ohio indiana federal 10: economy growth forecast rate rates economic global markets slow price Five example topics of XSum (a) Some example topics learned by PFA 1+3: Lisa and Mike of St. Louis were playing with their six cats at home<q>Their pet was found suffering from respiratory depression and severe hear attacks<q>In less time, Mike and Lisa found that they were having issues with their bodies. 1+2: The two animals were spotted riding a motor on the road<q>They came under scrutiny after they were spotted on a road trip<q>But the couple were not prosecuted over the incident. 6+7: The trainer riding his horse chase a man who drive a nissan car escaping from the hall. 

 Generations based on two randomly selected topics 2+4+5: The man, who is not identified, is seen riding a bike away from the scene<q>The police said that the video was shot by a witness who is a tourist<q>The video was posted on youtube and has been view more than 1.5 million times. 8+9+10: The journalist interview the officer of government in miami about the economy and he said the markets have a high growth rate after some steps taken by the state. Gold: Stan Freberg was famed comedian, song parodist<q>he later became adman, did a number of outrageous commercials<q>Yankovic said : " he is a legend, an inspiration, and a friend." BertSUM: "he was and will always be my hero," his son wrote on facebook<q>Freberg won a grammy award in 1959 for "the best of the stan freberg shows"<q>Freberg died of natural causes at today morning. BertSUM+TA: Reporter said Stan Freberg was a famous actor died at Santa Monica hospital <q>Freberg did some important advertisement <q>he won a grammy award in 1959<q>He is remembered by his amazing life. Document: (the hollywood report) Stan Freberg, whose freewheeling comic career in advertising garnered him worldwide acclaim and whose satirical entertainments abounded on tv, the radio and on records, has died. T1: famous, famously, fame, known, famed, well, know, remember, world, best; T2: actor, actress, play, sing, disney, comic, hollywood, cartoon, show, humor ; T3: business, advertise, commercial, rise, bank, economy, financial, money, product, growth; T4: writer, journalist, report, wrote, write, magazine, story, reporter, media, newspaper; T5: star, amazing, actor, film, oscar, legend, award, entertain, starring, inspiration; Gold: For domestic economy, India's central bank has unexpectedly held interest rates at a six-year low . 

 Generations based on top 

 BertSUM: The bank of india has cut the cost of borrowing by 0.25 % to 2.5 % MASS: India's bank has cut the cost of borrowing by 0.25 % to 2.5 %, in an effort to boost growth. 

 BertSUM+TA: The central bank of india has cut the cost of borrowing to a six-year low. UNILM: India's bank has cut the cost from of borrowing 0.25 % to 2.5 % for improving the economy. MASS+TA: India 's central bank has cut the cost of borrowing to a six-year low to boost growth. UNILM+TA: India's central bank has cut the cost from of borrowing to a six-year low for improving the economy. BART: India's bank has held interest rates at a low level for domestic economy. BART+TA: India's central bank has held interest rates at a six-year record low for domestic economy. in the document but conveyed by the topics. For example, BertSUM+TA summarizes shows, played, singing as actor, and summarizes worldwide, famously as remembered and famous. This is due to the fact that a topic describes a co-occurrence pattern of words with similar semantics. Fig.  7  provides some generated examples as TA combined with different models. It can be seen that some neglected words, such as "central" and "six-year", are generated with the help of TA. More examples can be found in Appendix C. 

 Conclusion and future work In this paper, we explore and rearrange semantics of a topic model and then propose a friendly plug-and-play TA for Transformer-based abstractive summarization models. By introducing a small number of parameters, TA is able to further improve the performance of these models, especially under a long-document scenario. In the future, we will study the effectiveness of TA on other NLP tasks, such as the document-level translation, and investigate whether TA is useful for Transformer pre-training.   19.60 20.01 20.29 20.40 20.58 20.57 RL 39.18 39.46 39.57 39.62 39.67 39.69 (204, 045/11, 332/11, 334)  and follow the pre-processing in  Narayan et al. (2018) . To satisfy the maximum capacity of the encoder in the base model, such as 512 for BertSUM, we use truncated document as the encoder input. 
