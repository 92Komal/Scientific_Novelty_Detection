title
Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization

abstract
Current Chinese social media text summarization models are based on an encoderdecoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.

Introduction Text summarization is to produce a brief summary of the main ideas of the text. For long and normal documents, extractive summarization achieves satisfying performance by selecting a few sentences from source texts  (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016) . However, it does not apply to Chinese social media text summarization, where texts are comparatively short and often full of noise. Therefore, abstractive text summarization, which is based on encoder-decoder framework, is a better choice  (Rush et al., 2015; Hu et al., 2015) . For extractive summarization, the selected sentences often have high semantic relevance to the text. However, for abstractive text summarization, current models tend to produce grammatical Text: ? ? ? ? ? Last night, several people were caught to smoke on a flight of China United Airlines from Chendu to Beijing. Later the flight temporarily landed on Taiyuan Airport. Some passengers asked for a security check but were denied by the captain, which led to a collision between crew and passengers. RNN: ? China United Airlines exploded in the airport, leaving several people dead. Gold: ? Several people smoked on a flight which led to a collision between crew and passengers. and coherent summaries regardless of its semantic relevance with source texts. Figure  1  shows that the summary generated by a current model (RNN encoder-decoder) is similar to the source text literally, but it has low semantic relevance. In this work, our goal is to improve the semantic relevance between source texts and generated summaries for Chinese social media text summarization. To achieve this goal, we propose a Semantic Relevance Based neural model. In our model, a similarity evaluation component is introduced to measure the relevance of source texts and generated summaries. During training, it maximizes the similarity score to encourage high semantic relevance between source texts and sum-maries. The representation of source texts is produced by an encoder, while that of summaries is computed by a decoder. We introduce a gated attention encoder to better represent the source text. Besides, our decoder generates summaries and provide the summary representation. Experiments show that our proposed model has better performance than baseline systems on the social media corpus. 

 Background: Chinese Abstractive Text Summarization Current Chinese social media text summarization model is based on encoder-decoder framework. Encoder-decoder model is able to compress source texts x = {x 1 , x 2 , ..., x N } into continuous vector representation with an encoder, and then generate the summary y = {y 1 , y 2 , ..., y M } with a decoder. In the previous work  (Hu et al., 2015) , the encoder is a bi-directional gated recurrent neural network, which maps source texts into sentence vector {h 1 , h 2 , ..., h N }. The decoder is a unidirectional recurrent neural network, which produces the distribution of output words y t with previous hidden state s t?1 and word y t?1 : p(y t |x) = sof tmaxf (s t?1 , y t?1 ) (1) where f is recurrent neural network output function, and s 0 is the last hidden state of encoder h N . Attention mechanism is introduced to better capture context information of source texts  (Bahdanau et al., 2014) . Attention vector c t is represented by the weighted sum of encoder hidden states: c t = N ? i=1 ? ti h i (2) ? ti = e g(st,h i ) ? N j=1 e g(st,h j ) (3) where g(s t , h i ) is a relevant score between decoder hidden state s t and encoder hidden state h i . When predicting an output word, the decoder takes account of attention vector, which contains the alignment information between source texts and summaries. 

 Proposed Model Our assumption is that source texts and summaries have high semantic relevance, so our proposed model encourages high similarity between The encoder compresses source texts into semantic vectors, and the decoder generates summaries and produces semantic vectors of the generated summaries. Finally, the similarity function evaluates the relevance between the sematic vectors of source texts and generated summaries. Our training objective is to maximize the similarity score so that the generated summaries have high semantic relevance with source texts. 

 Text Representation There are several methods to represent a text or a sentence, such as mean pooling of RNN output or reserving the last state of RNN. In our model, source text is represented by a gated attention encoder  (Hahn and Keller, 2016) . Every upcoming word is fed into a gated attention network, which measures its importance. The gated attention network outputs the important score with a feedforward network. At each time step, it inputs a word vector e t and its previous context vector h t , then outputs the score ? t . Then the word vector e t is multiplied by the score ? t , and fed into RNN encoder. We select the last output h N of RNN encoder as the semantic vector of the source text V t . A natural idea to get the semantic vector of a summary is to feed it into the encoder as well. However, this method wastes much time because we encode the same sentence twice. Actually, the last output s M contains information of both source text and generated summaries. We simply compute the semantic vector of the summary by subtracting h N from s M : V s = s M ? h N (4) Previous work has proved that it is effective to represent a span of words without encoding them once more  (Wang and Chang, 2016) . 

 Semantic Relevance Our goal is to compute the semantic relevance of source text and generated summary given semantic vector V t and V s . Here, we use cosine similarity to measure the semantic relevance, which is represented with a dot product and magnitude: cos(V s , V t ) = V s ? V t ?V s ?V t ? (5) Source text and summary share the same language, so it is reasonable to assume that their semantic vectors are distributed in the same space. Cosine similarity is a good way to measure the distance between two vectors in the same space. 

 Training Given the model parameter ? and input text x, the model produces corresponding summary y and semantic vector V s and V t . The objective is to minimize the loss function: L = ?p(y|x; ?) ? ?cos(V s , V t ) (6) where p(y|x; ?) is the conditional probability of summaries given source texts, and is computed by the encoder-decoder model. cos(V s , V t ) is cosine similarity of semantic vectors V s and V t . This term tries to maximize the semantic relevance between source input and target output. 

 Experiments In this section, we present the evaluation of our model and show its performance on a popular social media corpus. Besides, we use a case to explain the semantic relevance between generated summary and source text. 

 Dataset Our dataset is Large Scale Chinese Short Text Summarization Dataset (LCSTS), which is constructed by  Hu et al. (2015) .  

 Experiment Setting To alleviate the risk of word segmentation mistakes  (Xu and Sun, 2016) , we use Chinese character sequences as both source inputs and target outputs. We limit the model vocabulary size to 4000, which covers most of the common characters. Each character is represented by a random initialized word embedding. We tune our parameter on the development set. In our model, the embedding size is 400, the hidden state size of encoderdecoder is 500, and the size of gated attention network is 1000. We use Adam optimizer to learn the model parameters, and the batch size is set as 32. The parameter ? is 0.0001. Both the encoder and decoder are based on LSTM unit. Following the previous work  (Hu et al., 2015) , our evaluation metric is F-score of ROUGE: ROUGE-1, ROUGE-2 and ROUGE-L  (Lin and Hovy, 2003) . 

 Baseline Systems RNN. We denote RNN as the basic sequence-tosequence model with bi-directional GRU encoder and uni-directional GRU decoder. It is a widely used language generated framework, so it is an important baseline. RNN context. RNN context is a sequence-tosequence framework with neural attention. Attention mechanism helps capture the context information of source texts. This model is a stronger baseline system. 

 Results and Discussions We compare our model with above baseline systems, including RNN and RNN context. We refer to our proposed Semantic Relevance Based neural model as SRB. Besides, SRB with a gated attention encoder is denoted as +Attention. Table  1  Model ROUGE-1 ROUGE-2 ROUGE-L RNN (W)  (Hu et al., 2015)  17.7 8.5 15.8 RNN (C)  (Hu et al., 2015)  21.5 8.9 18.6 RNN context (W)  (Hu et al., 2015)  26.8 16.1 24.1 RNN context (C)  (Hu et al., 2015)  29.9 mantic relevance evaluation by the similarity function. Therefore, SRB with gated attention encoder is able to generate summaries with high semantic relevance to source text. Figure  3  is an example to show the semantic relevance between the source text and the summary. It shows that the main idea of the source text is about the reason why Shanghai has few giant company. RNN context produces "Shanghai's giant companies" which is literally similar to the source text, while SRB generates "Shanghai has few giant companies", which is closer to the main idea in semantics. It concludes that SRB produces summaries with higher semantic similarity to texts. Table  2  summarizes the results of our model and state-of-the-art systems. COPYNET has the highest socres, because it incorporates copying mechanism to deals with out-of-vocabulary word problem. In this paper, we do not implement this mechanism in our model. In the future work, we will try to incorporates copying mechanism to our model to solve the out-of-vocabulary problem. 

 Related Work Abstractive text summarization has achieved successful performance thanks to the sequence-tosequence model  (Sutskever et al., 2014)  and attention mechanism  (Bahdanau et al., 2014) .  Rush et al. (2015)  first used an attention-based encoder to compress texts and a neural network language decoder to generate summaries. Following this work, recurrent encoder was introduced to text summarization, and gained better performance  (Lopyrev, 2015; Chopra et al., 2016) . Towards Chinese texts,  Hu et al. (2015)  built a large corpus of Chinese short text summarization. To deal with unknown word problem,  Nallapati et al. (2016)  proposed a generator-pointer model so that the decoder is able to generate words in source texts.  Gu et al. (2016)  also solved this issue by incorporating copying mechanism. Our work is also related to neural attention model. Neural attention model is first proposed by  Bahdanau et al. (2014) . There are many other methods to improve neural attention model  (Jean et al., 2015; Luong et al., 2015)  and accelerate the training process  (Sun, 2016) . 

 Conclusion Our work aims at improving semantic relevance of generated summaries and source texts for Chinese social media text summarization. Our model is able to transform the text and the summary into a dense vector, and encourage high similarity of their representation. Experiments show that our model outperforms baseline systems, and the generated summary has higher semantic relevance. Figure 1 : 1 Figure 1: An example of RNN generated summary. It has high similarity to the text literally, but low semantic relevance. 
