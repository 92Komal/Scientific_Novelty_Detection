title
Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization

abstract
Deep reinforcement learning (RL) has been a commonly-used strategy for the abstractive summarization task to address both the exposure bias and non-differentiable task issues. However, the conventional reward ROUGE-L simply looks for exact n-grams matches between candidates and annotated references, which inevitably makes the generated sentences repetitive and incoherent. In this paper, instead of ROUGE-L, we explore the practicability of utilizing the distributional semantics to measure the matching degrees. With distributional semantics, sentence-level evaluation can be obtained, and semantically-correct phrases can also be generated without being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language.

Introduction Abstractive summarization is a task of paraphrasing a long article with fewer words. Unlike extractive summarization, abstractive summaries can include tokens out of the article's vocabulary. There exists several encoder-decoder approaches such as attention-based architecture  (Bahdanau et al., 2015; Rush et al., 2015; Nallapati et al., 2016; , incorporating graph techniques  (Moawad and Aref, 2012; Ganesan et al., 2010) , and adding pointer generator  (Vinyals et al., 2015; Nallapati et al., 2016; See et al., 2017; Bello et al., 2017) . However, long sentence generation suffers from exposure bias  (Bahdanau et al., 2017)  as the error accumulates during the decoding process. * Equal contributions. Many innovative deep RL methods  (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016)  are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to optimal transport usage  (Napoles et al., 2012) . However, commonly used automatic evaluation metrics for generating sentence-level rewards count exact n-grams matches and are not robust to different words that share similar meanings since the semantic level reward is deficient. Currently, many studies on contextualized word representations  (Peters et al., 2018; Devlin et al., 2019)  prove that they have a powerful capacity of reflecting distributional semantic. In this paper, we propose to use the distributional semantic reward to boost the RL-based abstractive summarization system. Moreover, we design several novel objective functions. Experiment results show that they outperform the conventional objectives while increasing the sentence fluency. Our main contributions are three-fold: ? We are the first to introduce DSR to abstractive summarization and achieve better results than conventional rewards. ? Unlike ROUGE, our DSR does not rely on crossentropy loss (XENT) to produce readable phrases. Thus, no exposure bias is introduced. ? DSR improves generated tokens' diversity and fluency while avoiding unnecessary repetitions. 

 Methodology Background While sequence models are usually trained using XENT, they are typically evaluated at test time using discrete NLP metrics such as BLEU  (Papineni et al., 2002) , ROUGE  (Lin, 2004) , METEOR  (Banerjee and Lavie, 2005) . Therefore, they suffer from both the exposure bias and non-differentiable task metric issues. To solve these problems, many resorts to deep RL with sequence to sequence model  (Paulus et al., 2018; Ranzato et al., 2016; Ryang and Abekawa, 2012) , where the learning agent interacts with a given environment. However, RL models have poor sample efficiency and lead to very slow convergence rate. Therefore, RL methods usually start from a pretrained policy, which is established by optimizing XENT at each word generation step. L XENT = ? n t=1 log P (y t |y 1 , . . . , y t?1 , x). (1) Then, during RL stage, the conventional way is to adopt self-critical strategy to fine-tune based on the target evaluation metric, L RL = n t=1 log P ( ?t | ?1 , . . . , ?t?1 , x) (2) ? (r metric (y b ) ? r metric (?)) Distributional Semantic Reward During evaluating the quality of the generated sentences, ROUGE looks for exact matches between references and generations, which naturally overlooks the expression diversity of the natural language. In other words, it fails to capture the semantic relation between similar words. To solve this problem, distributional semantic representations are a practical way. Recent works on contextualized word representations, including ELMO  (Peters et al., 2018) , GPT  (Radford et al., 2018) , BERT  (Devlin et al., 2019) , prove that distributional semantics can be captured effectively. Based on that, a recent study, called BERTSCORE  (Zhang et al., 2019) , focuses on sentence-level generation evaluation by using pre-trained BERT contextualized embeddings to compute the similarity between two sentences as a weighted aggregation of cosine similarities between their tokens. It has a higher correlation with human evaluation on text generation tasks comparing to existing evaluation metrics. In this paper, we introduce it as a DSR for deep RL. The BERTSCORE is defined as: R BERT = y i ?y idf(y i ) max ?j ? y i ?j y i ?y idf(y i ) (3) P BERT = y j ?y idf(y j ) max ?i ? y j ?i y j ?y idf(y j ) (4) F BERT = 2 R BERT ? P BERT R BERT + P BERT (5) where y and ? represent BERT contextual embeddings of reference word y and candidate word ?, respectively. The function idf(?) calculates inverse document frequency (idf). In our DSR, we do not use the idf since  Zhang et al. (2019)  requires to use the entire dataset including test set for calculation. Besides, ROUGE do not use similar weight, so we do not include idf for consistency. 3 Experimental Setup 

 Datasets Gigaword corpus It is an English sentence summarization dataset based on annotated Gigaword  (Napoles et al., 2012) . A single sentence summarization is paired with a short article. We use the OpenNMT provided version It contains 3.8M training, 189k development instances. We randomly sample 15k instances as our test data. CNN/Daily Mail dataset It consists of online news articles and their corresponding multisentence abstracts  (Hermann et al., 2015; Nallapati et al., 2016) . We use the non-anonymized version provided by  See et al. (2017) , which contains 287k training, 13k validation, and 11k testing examples. We truncate the articles to 400 tokens and limit the summary lengths to 100 tokens. 

 Pretrain We first pretrain a sequence-to-sequence model with attention using XENT and then select the best parameters to initialize models for RL. Our models have 256-dimensional hidden states and 128dimensional word embeddings and also incorporates the pointer mechanism  (See et al., 2017)  for handling out of vocabulary words. 

 Baseline In abstractive summarization, ROUGE  (Lin, 2004 ) is a common evaluation metric to provide a sentence-level reward for RL. However, using ROUGE as a pure RL objective may cause too many repetitions and reduced fluency in outputs.  Paulus et al. (2018)  propose a hybrid learning objective that combines XENT and self-critical ROUGE reward  (Paulus et al., 2018) . L baseline = ?L rouge + (1 ? ?)L XENT (6) where ? is a scaling factor and the F score of ROUGE-L is used as the reward to calculate L Rouge . In our experiment, we select ? = 0.998 for Gigaword Corpus and ? = 0.9984 for CNN/Daily Mail dataset. 1 Note that we do not avoid repetition during the test time as  Paulus et al. (2018)  do, because we want to examine the repetition of sentence directly produced after training. 

 Proposed Objective Functions Inspired by the above objective function  (Paulus et al., 2018) , we optimize RL models with a similar loss function as equation 2. Instead of ROUGE-L, we incorporate BERTSCORE, a DSR to provide sentence-level feedback. In our experiment, L DSR is the self-critical RL loss (equation 2) with F BERT as the reward. We introduce the following objective functions: DSR+ROUGE: A combined reward function of ROUGE and F BERT L 1 = ?L DSR + (1 ? ?)L rouge (7) In our experiment, we select ? = 0.5 for both datasets to balance the influence of two reward functions. DSR+XENT: BERT reward with XENT to make the generated phrases more readable. L 2 = ? L DSR + (1 ? ? )L XENT (8) In our experiment, we select ? = 0.998 for Gigaword Corpus and ? = 0.9984 for CNN/Daily Mail dataset. DSR: Pure F BERT objective function without any teacher forcing. L 3 = L DSR (9) 4 Results For the abstractive summarization task, we test our models with different objectives on the Gigaword and CNN/Daily Mail datasets. We choose F BERT and ROUGE-L as our automatic evaluation metrics. For multi-sentence summaries in Note that DSR model's ROUGE-L is high in training time but does not have a good generalization on test set, and ROUGE-L is not our target evaluation metrics. In the next section, we will do human evaluation to analyze the summarization performance of different reward systems. 

 Human Evaluation We perform human evaluation on the Amazon Mechanical Turk to assure the benefits of DSR on output sentences' coherence and fluency. We randomly sample 500 items as an evaluation set us-   

 Analysis Diversity Other than extractive summarization, abstractive summarization allows more degrees of freedom in the choice of words. While simply selecting words from the article made the task easier to train, higher action space can provide more paths to potentially better results  (Nema et al., 2017) . Using the DSR as deep RL reward will support models to choose actions that are not n-grams of the articles. In Table  3 : Qualitative analysis on repetition(Rep) / diversity(Div). They are calculated by the percentage of repeat/out-of-article n-grams (unigrams for Gigaword and 5-grams for CNN/Daily Mail) in generated sentences. an interesting and efficient way to express the relation between China and Germany. F BERT is also improved by making this change. In addition, the second example in Table  4  shows that RL model with DSR corrects the sentence' grammar and significantly improves the F BERT score by switching "down" to an unseen word "drops". On the other hand, when optimizing DSR to improve the diversity of generation, some semantically similar words may also be generated and harm the summarization quality as shown in the third example in Table  4 . The new token "wins" reduces the scores of both metrics. We also evaluate the diversity of a model quantitively by averaging the percentage of out-of-article n-grams in generated sentences. Results can be found in  Repetition Repetition will lead to lower F BERT as shown in the last example in Table  4 . Using DSR reduces the probability of producing repetitions. The average percentage of repeated ngrams in generated sentences are presented in the Table  3 . As shown in this table, unlike ROUGE, the DSR model can achieve high fluency without XENT; moreover, it produces the fewest repetitions among all the rewards. Table  4  gives an example that DSR produces a repeated word (from example 4), but it does not reflect the overall distribution of repeated word generation for all evaluated models. 

 Conclusion This paper demonstrates the effectiveness of applying the distributional semantic reward to reinforcement learning in abstractive summarization, and specifically, we choose BERTSCORE. Our experimental results demonstrate that we achieve better performance on Gigaword and CNN/Daily Mail datasets. Besides, the generated sentences have fewer repetitions, and the fluency is also improved. Our finding is aligned to a contemporaneous study  (Wieting et al., 2019)  on leveraging semantic similarity for machine translation. Table 1 : 1 Results on Gigawords and CNN/Daily Mail for abstractive summarization. Upper rows show the results of baselines. Rouge stands for the F score of Rouge-L. Gigawords CNN/Daily Mail Model FBERT PBERT RBERT ROUGE FBERT PBERT RBERT ROUGE XENT 65.78 67.61 64.53 40.77 62.77 62.18 63.79 29.46 ROUGE 61.46 60.89 62.70 42.73 60.11 59.31 61.33 33.89 ROUGE+XENT 66.50 67.24 66.28 42.72 61.38 61.07 62.17 33.07 DSR+ROUGE 66.48 66.79 66.65 42.95 65.01 65.92 64.56 33.61 DSR+XENT 67.02 67.69 66.85 42.29 66.64 66.06 67.63 31.28 DSR 67.06 67.34 67.28 41.73 66.93 66.27 67.98 30.96 Task Model v. base Win Gigawords Lose Tie CNN/Daily Mail Win Lose Tie Relevance DSR + XENT 31.0% 25.2% 43.8% 51.1% 34.1% 14.8% DSR 45.4% 27.2% 27.4% 48.7% 38.5% 12.8% Fluency DSR + XENT 40.2% 19.6% 40.2% 55.8% 28.5% 15.6% DSR 45.4% 28.8% 25.8% 54.0% 31.7% 14.3% 
