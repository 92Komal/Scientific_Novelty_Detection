title
Hierarchical Transformers for Multi-Document Summarization

abstract
In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments a previously proposed Transformer architecture  with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines. 1

Introduction Automatic summarization has enjoyed renewed interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets  (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018)  containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary  (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018) . Multi-document summarization -the task of producing summaries from clusters of themati-cally related documents -has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are relatively small (in the range of a few hundred examples) for training neural models. In an attempt to drive research further,  tap into the potential of Wikipedia and propose a methodology for creating a large-scale dataset (WikiSum) for multidocument summarization with hundreds of thousands of instances. Wikipedia articles, specifically lead sections, are viewed as summaries of various topics indicated by their title, e.g.,"Florence" or "Natural Language Processing". Documents cited in the Wikipedia articles or web pages returned by Google (using the section titles as queries) are seen as the source cluster which the lead section purports to summarize. Aside from the difficulties in obtaining training data, a major obstacle to the application of end-to-end models to multi-document summarization is the sheer size and number of source documents which can be very large. As a result, it is practically infeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them.  propose a two-stage architecture, where an extractive model first selects a subset of salient passages, and subsequently an abstractive model generates the summary while conditioning on the extracted subset. The selected passages are concatenated into a flat sequence and the Transformer  (Vaswani et al., 2017) , an architecture well-suited to language modeling over long sequences, is used to decode the summary. Although the model of  takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light  (Radev, 2000) . The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization  (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014) . Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding them as a flat sequence to the model. In this way, the model automatically learns richer structural dependencies among textual units, thus incorporating well-established insights from earlier work. Advantageously, the proposed architecture can easily benefit from information external to the model, i.e., by replacing inter-document attention with a graph-matrix computed based on the basis of lexical similarity  (Erkan and Radev, 2004)  or discourse relations  (Christensen et al., 2013) . We evaluate our model on the WikiSum dataset and show experimentally that the proposed architecture brings substantial improvements over several strong baselines. We also find that the addition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi-document summarization system. 

 Related Work Most previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words  (Erkan and Radev, 2004)  or on discourse relations  (Christensen et al., 2013) , and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature  (Erkan and Radev, 2004)  in order to compute the importance or salience of a passage recursively based on the entire graph. More recently,  Yasunaga et al. (2017)  propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences  (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015) . Although neural abstractive models have achieved promising results on single-document summarization  (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018) , the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer  (Zhang et al., 2018; Lebanoff and Liu, 2018) , where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives  (Ma et al., 2016; Chu and Liu, 2018) .  propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised machine learning prob- lem and for this purpose assume access to large, labeled datasets (i.e., source documents-summary pairs). In contrast to their approach, we use a learning-based ranker and our abstractive model can hierarchically encode the input documents, with the ability to learn latent relations across documents and additionally incorporate information encoded in well-known graph representations. 

 Model Description We follow  in treating the generation of lead Wikipedia sections as a multidocument summarization task. The input to a hypothetical system is the title of a Wikipedia article and a collection of source documents, while the output is the Wikipedia article's first section. Source documents are webpages cited in the References section of the Wikipedia article and the top 10 search results returned by Google (with the title of the article as the query). Since source documents could be relatively long, they are split into multiple paragraphs by line-breaks. More formally, given title T , and L input paragraphs {P 1 , ? ? ? , P L } (retrieved from Wikipedia citations and a search engine), the task is to generate the lead section D of the Wikipedia article. Our summarization system is illustrated in Figure 1. Since the input paragraphs are numerous and possibly lengthy, instead of directly applying an abstractive system, we first rank them and summarize the L -best ones. Our summarizer follows the very successful encoder-decoder architecture  (Bahdanau et al., 2015) , where the encoder encodes the input text into hidden representations and the decoder generates target summaries based on these representations. In this paper, we focus exclusively on the encoder part of the model, our decoder follows the Transformer architecture in-troduced in  Vaswani et al. (2017) ; it generates a summary token by token while attending to the source input. We also use beam search and a length penalty  (Wu et al., 2016)  in the decoding process to generate more fluent and longer summaries. 

 Paragraph Ranking Unlike  who rank paragraphs based on their similarity with the title (using tf-idfbased cosine similarity), we adopt a learningbased approach. A logistic regression model is applied to each paragraph to calculate a score indicating whether it should be selected for summarization. We use two recurrent neural networks with Long-Short Term Memory units (LSTM; Hochreiter and Schmidhuber 1997) to represent title T and source paragraph P : {u t1 , ? ? ? , u tm } = lstm t ({w t1 , ? ? ? , w tm }) (1) {u p1 , ? ? ? , u pn } = lstm p ({w p1 , ? ? ? , w pn }) (2) where w ti , w pj are word embeddings for tokens in T and P , and u ti , u pj are the updated vectors for each token after applying the LSTMs. A max-pooling operation is then used over title vectors to obtain a fixed-length representation ?t : ?t = maxpool({u t1 , ? ? ? , u tm }) (3) We concatenate ?t with the vector u pi of each token in the paragraph and apply a non-linear transformation to extract features for matching the title and the paragraph. A second max-pooling operation yields the final paragraph vector p: p i = tanh(W 1 ([u pi ; ?t ])) (4) p = maxpool({p 1 , ? ? ? , p n }) (5) Finally, to estimate whether a paragraph should be selected, we use a linear transformation and a sigmoid function: s = sigmoid(W 2 ( p)) ( 6 ) where s is the score indicating whether paragraph P should be used for summarization. All input paragraphs {P 1 , ? ? ? , P L } receive scores {s 1 , ? ? ? , s L }. The model is trained by minimizing the cross entropy loss between s i and ground-truth scores y i denoting the relatedness of a paragraph to the gold standard summary. We adopt ROUGE-2 recall (of paragraph P i against gold target text D) as y i . In testing, input paragraphs are ranked based on the model predicted scores and an ordering {R 1 , ? ? ? , R L } is gener- ated. The first L paragraphs {R 1 , ? ? ? , R L } are selected as input to the second abstractive stage. 

 Paragraph Encoding Instead of treating the selected paragraphs as a very long sequence, we develop a hierarchical model based on the Transformer architecture  (Vaswani et al., 2017)  to capture inter-paragraph relations. The model is composed of several local and global transformer layers which can be stacked freely. Let t ij denote the j-th token in the i-th ranked paragraph R i ; the model takes vectors x 0 ij (for all tokens) as input. For the l-th transformer layer, the input will be x l?1 ij , and the output is written as x l ij . 

 Embeddings Input tokens are first represented by word embeddings. Let w ij ? R d denote the embedding assigned to t ij . Since the Transformer is a nonrecurrent model, we also assign a special positional embedding pe ij to t ij , to indicate the position of the token within the input. To calculate positional embeddings, we follow  Vaswani et al. (2017)  and use sine and cosine functions of different frequencies. The embedding e p for the p-th element in a sequence is: e p [i] = sin(p/10000 2i/d ) (7) e p [2i + 1] = cos(p/10000 2i/d ) (8) where e p [i] indicates the i-th dimension of the embedding vector. Because each dimension of the positional encoding corresponds to a sinusoid, for any fixed offset o, e p+o can be represented as a linear function of e p , which enables the model to distinguish relative positions of input elements. In multi-document summarization, token t ij has two positions that need to be considered, namely i (the rank of the paragraph) and j (the position of the token within the paragraph). Positional embedding pe ij ? R d represents both positions (via concatenation) and is added to word embedding w ij to obtain the final input vector x 0 ij : pe ij = [e i ; e j ] (9) x 0 ij = w ij + pe ij (10) 

 Local Transformer Layer A local transformer layer is used to encode contextual information for tokens within each paragraph. The local transformer layer is the same as the vanilla transformer layer  (Vaswani et al., 2017) , and composed of two sub-layers: h = LayerNorm(x l?1 + MHAtt(x l?1 )) (11) x l = LayerNorm(h + FFN(h)) ( 12 ) where LayerNorm is layer normalization proposed in  Ba et al. (2016) ; MHAtt is the multihead attention mechanism introduced in Vaswani et al. (  2017 ) which allows each token to attend to other tokens with different attention distributions; and FFN is a two-layer feed-forward network with ReLU as hidden activation function. 

 Global Transformer Layer A global transformer layer is used to exchange information across multiple paragraphs. As shown in Figure  2 , we first apply a multi-head pooling operation to each paragraph. Different heads will encode paragraphs with different attention weights. Then, for each head, an inter-paragraph attention mechanism is applied, where each paragraph can collect information from other paragraphs by selfattention, generating a context vector to capture contextual information from the whole input. Finally, context vectors are concatenated, linearly transformed, added to the vector of each token, and fed to a feed-forward layer, updating the representation of each token with global information.  

 Multi-head Pooling a z ij = W z a x l?1 ij (13) b z ij = W z b x l?1 ij (14) ?z ij = exp(a z ij )/ n j=1 exp(a z ij ) (15) where W z a ? R 1 * d and W z b ? R d head * d are weights. d head = d/n head is the dimension of each head. n is the number of tokens in R i . We next apply a weighted summation with another linear transformation and layer normalization to obtain vector head z i for the paragraph: head z i = LayerNorm(W z c n j=1 a z ij b z ij ) (16) where W z c ? R d head * d head is the weight. The model can flexibly incorporate multiple heads, with each paragraph having multiple attention distributions, thereby focusing on different views of the input. 

 Inter-paragraph Attention We model the dependencies across multiple paragraphs with an inter-paragraph attention mechanism. Similar to self-attention, inter-paragraph attention allows for each paragraph to attend to other paragraphs by calculating an attention distribution: q z i = W z q head z i ( 17 ) k z i = W z k head z i (18) v z i = W z v head z i ( 19 ) context z i = m i=1 exp(q z i T k z i ) m o=1 exp(q z i T k z o ) v z i ( 20 ) where q z i , k z i , v z i ? R d head * d head are query, key, and value vectors that are linearly transformed from head z i as in  Vaswani et al. (2017) ; context z i ? R d head represents the context vector generated by a self-attention operation over all paragraphs. m is the number of input paragraphs. Figure  2  provides a schematic view of inter-paragraph attention. 

 Feed-forward Networks We next update token representations with contextual information. We first fuse information from all heads by concatenating all context vectors and applying a linear transformation with weight W c ? R d * d : We then add c i to each input token vector x l?1 ij , and feed it to a two-layer feed-forward network with ReLU as the activation function and a highway layer normalization on top: c i = W c [context 1 i ; ? ? ? ; context n head i ] (21) g ij = W o2 ReLU(W o1 (x l?1 ij + c i )) (22) x l ij = LayerNorm(g ij + x l?1 ij ) (23) where W o1 ? R d f f * d and W o2 ? R d * d f f are the weights, d f f is the hidden size of the feed-forward later. This way, each token within paragraph R i can collect information from other paragraphs in a hierarchical and efficient manner. 

 Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks  (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019) , much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However, there is nothing inherent in our model that restricts us to these, any graph modeling relationships across paragraphs could have been used instead. Our first graph aims to capture lexical relations; graph nodes correspond to paragraphs and edge weights are cosine similarities based on tf-idf representations of the paragraphs. Our second graph aims to capture discourse relations  (Christensen et al., 2013) ; it builds an Approximate Discourse Graph (ADG)  (Yasunaga et al., 2017)  over paragraphs; edges between paragraphs are drawn by counting (a) co-occurring entities and (b) discourse markers (e.g., however, nevertheless) connecting two adjacent paragraphs (see the Appendix for details on how ADGs are constructed). We represent such graphs with a matrix G, where G ii is the weight of the edge connecting paragraphs i and i . We can then inject this graph into our hierarchical transformer by simply substituting one of its (learned) heads z with G. Equation (20) for calculating the context vector for this head is modified as: context z i = m i =1 G ii m o=1 G io v z i (24) 4 Experimental Setup WikiSum Dataset We used the scripts and urls provided in  to crawl Wikipedia articles and source reference documents. We successfully crawled 78.9% of the original documents (some urls have become invalid and corresponding documents could not be retrieved). We further removed clone paragraphs (which are exact copies of some parts of the Wikipedia articles); these were paragraphs in the source documents whose bigram recall against the target summary was higher than 0.8. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with Sentence-Piece  (Kudo and Richardson, 2018) . Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall  (Lin, 2004)  of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad  (Duchi et al., 2011)  with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in  who use the tf-idf cosine similarity between each paragraph and the article title to rank the input paragraphs. We take the first L paragraphs from the ordered paragraph set produced by our ranker and the similarity-based method, respectively. We concatenate these paragraphs and calculate their ROUGE-L recall against the gold target text. The results are shown in Table  1 . We can see that our ranker effectively extracts related paragraphs and produces more informative input for the downstream summarization task. Training Configuration In all abstractive models, we apply dropout (with probability of 0.1) before all linear layers; label smoothing  (Szegedy et al., 2016)  with smoothing factor 0.1 is also used. Training is in traditional sequence-to-sequence manner with maximum likelihood estimation. The optimizer was Adam (Kingma and  Ba, 2014)  with learning rate of 2, ? 1 = 0.9, and ? 2 = 0.998; we also applied learning rate warmup over the first 8, 000 steps, and decay as in  (Vaswani et al., 2017) . All transformer-based models had 256 hidden units; the feed-forward hidden size was 1, 024 for all layers. All models were trained on 4 GPUs (NVIDIA TITAN Xp) for 500, 000 steps. We used gradient accumulation to keep training time for all models approximately consistent. We selected the 5 best checkpoints based on performance on the validation set and report averaged results on the test set. During decoding we use beam search with beam size 5 and length penalty with ? = 0.4  (Wu et al., 2016) ; we decode until an end-of-sequence token is reached. Comparison Systems We compared the proposed hierarchical transformer against several strong baselines: Lead is a simple baseline that concatenates the title and ranked paragraphs, and extracts the first k tokens; we set k to the length of the ground-truth target. LexRank  (Erkan and Radev, 2004 ) is a widelyused graph-based extractive summarizer; we build a graph with paragraphs as nodes and edges weighted by tf-idf cosine similarity; we run a PageRank-like algorithm on this graph to rank and select paragraphs until the length of the ground-truth summary is reached. Flat Transformer (FT) is a baseline that applies a Transformer-based encoder-decoder model to a flat token sequence. We used a 6-layer transformer. The title and ranked paragraphs were concatenated and truncated to 600, 800, and 1, 200 tokens. T-DMCA is the best performing model of  and a shorthand for Transformer Decoder with Memory Compressed Attention; they only used a Transformer decoder and compressed the key and value in selfattention with a convolutional layer. The model has 5 layers as in . Its hidden size is 512 and its feed-forward hidden size is 2, 048. The title and ranked paragraphs were concatenated and truncated to 3,000 tokens. Hierarchical Transformer (HT) is the model proposed in this paper. The model architecture is a 7-layer network (with 5 localattention layers at the bottom and 2 global attention layers at the top). The model takes the title and L = 24 paragraphs as input to produce a target summary, which leads to approximately 1, 600 input tokens per instance. 

 Results Automatic Evaluation We evaluated summarization quality using ROUGE F 1  (Lin, 2004) . We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. forms FT, and even T-DMCA when the latter is presented with 3, 000 tokens. Adding an external graph also seems to help the summarization process. The similarity graph does not have an obvious influence on the results, while the discourse graph boosts ROUGE-L by 0.16. We also found that the performance of the Hierarchical Transformer further improves when the model is presented with longer input at test time.  2  As shown in the last row of Table  2 , when testing on 3, 000 input tokens, summarization quality improves across the board. This suggests that the model can potentially generate better summaries without increasing training time. Table  3  summarizes ablation studies aiming to assess the contribution of individual components. Our experiments confirmed that encoding paragraph position in addition to token position within each paragraph is beneficial (see row w/o PP), as well as multi-head pooling (w/o MP is a model where the number of heads is set to 1), and the global transformer layer (w/o GT is a model with only 5 local transformer layers in the encoder). Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm  (Clarke and Lapata, 2010; Narayan et al., 2018) . We created a set of questions based on the gold summary under the assumption that it contains the most important information from the input paragraphs. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary. The more questions a system can answer, the better it is at summarization. We created 57 questions in total varying from two to four questions per gold summary. Examples of questions and their answers are given in Table  5 . We adopted the same scoring mechanism used in  Clarke and Lapata (2010) , i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system's score is the average of all question scores. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria: Informativeness (does the summary convey important facts about the topic in question?), Fluency (is the summary fluent and grammatical?), and Succinctness (does the summary avoid repetition?). We used Best-Worst Scaling  (Louviere et al., 2015) , a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales  (Kiritchenko and Mohammad, 2017) . Participants were presented with the gold summary and summaries generated from 3 out of 4 systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard, taking into account the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from ?1 (worst) to 1 (best). Both evaluations were conducted on the Amazon Mechanical Turk platform with 5 responses per hit. Participants evaluated summaries produced by the Lead baseline, the Flat Transformer, T-DMCA, and our Hierarchical Transformer. All evaluated systems were variants that achieved the best performance in automatic evaluations. As shown in Table  4 , on both evaluations, participants overwhelmingly prefer our model (HT). All pairwise comparisons among systems are statistically significant (using a one-way ANOVA with posthoc Tukey HSD tests; p < 0.01). Examples of system output are provided in Table  5 . 

 Pentagoet Archeological District 

 GOLD The Pentagoet Archeological District is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Castine, Maine. It is the site of Fort Pentagoet, a 17th-century fortified trading post established by fur traders of French Acadia. From 1635 to 1654 this site was a center of trade with the local Abenaki, and marked the effective western border of Acadia with New England. From 1654 to 1670 the site was under English control, after which it was returned to France by the Treaty of Breda. The fort was destroyed in 1674 by Dutch raiders. The site was designated a National Historic Landmark in 1993. It is now a public park. 

 QA What is the Pentagoet Archeological District? [a National Historic Landmark District] Where is it located? [Castine , Maine] What did the Abenaki Indians use the site for? [trading center] 

 LEAD The Pentagoet Archeological District is a National Historic Landmark District located in Castine, Maine. This district forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In the colonial period, Abenakis frequented the fortified trading post at this site, bartering moosehides, sealskins, beaver and other furs in exchange for European commodities. "Pentagoet Archeological district" is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda. The Australian golden whistler (Pachycephala pectoralis) is a species of bird found in forest, woodland, mallee, mangrove and scrub in Australia (except the interior and most of the north) Most populations are resident, but some in south-eastern Australia migrate north during the winter. 

 FT The Melanesian whistler (P. Caledonica) is a species of bird in the family Muscicapidae. It is endemic to Melanesia. 

 T-DMCA The Australian golden whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is endemic to Fiji. 

 HT The Melanesian whistler (Pachycephala chlorura) is a species of bird in the family Pachycephalidae, which is endemic to Fiji. Table  5 : GOLD human authored summaries, questions based on them (answers shown in square brackets) and automatic summaries produced by the LEAD-3 baseline, the Flat Transformer (FT), T-DMCA  and our Hierachical Transformer (HT). 

 Conclusions In this paper we conceptualized abstractive multidocument summarization as a machine learning problem. We proposed a new model which is able to encode multiple input documents hierarchically, learn latent relations across them, and additionally incorporate structural information from well-known graph representations. We have also demonstrated the importance of a learning-based approach for selecting which documents to summarize. Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin. In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks. Figure 1 : 1 Figure1: Pipeline of our multi-document summarization system. L source paragraphs are first ranked and the L -best ones serve as input to an encoder-decoder model which generates the target summary. 

 FTthe Pentagoet Archeological district is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda. It was listed on the national register of historic places in 1983. T-DMCA The Pentagoet Archeological District is a national historic landmark district located in castine , maine . this district forms part of the traditional homeland of the abenaki indians , in particular the Penobscot tribe. The district was listed on the national register of historic places in 1982. HT The Pentagoet Archeological district is a National Historic Landmark District located in Castine, Maine. This district forms part of the traditional homeland of the Abenaki Indians, in particular the Penobscot tribe. In the colonial period, Abenaki frequented the fortified trading post at this site, bartering moosehides, sealskins, beaver and other furs in exchange for European commodities. Melanesian Whistler GOLD The Melanesian whistler or Vanuatu whistler (Pachycephala chlorura) is a species of passerine bird in the whistler family Pachycephalidae. It is found on the Loyalty Islands, Vanuatu, and Vanikoro in the far southeastern Solomons. QA What is the Melanesian Whistler? [a species of passerine bird in the whistler family Pachycephalidae] Where is it found? [Loyalty Islands , Vanuatu , and Vanikoro in the far south-eastern Solomons] LEAD 

 R d denote the output vector of the last transformer layer for token t ij , which is used as input for the current layer. For each paragraph R i , for head z ? {1, ? ? ? , n head }, we first transform the input vectors into attention scores a z To obtain fixed-length paragraph representations, we apply a weighted- pooling operation; instead of using only one rep- resentation for each paragraph, we introduce a multi-head pooling mechanism, where for each paragraph, weight distributions over tokens are calculated, allowing the model to flexibly encode paragraphs in different representation subspaces by attending to different words. Let x l?1 ij ? ij and value vectors b z ij . Then, for each head, we calculate a probability distribution ?z ij over tokens within the paragraph based on attention scores: 

 Table 1 : 1 ROUGE-L recall against target summary for L -best paragraphs obtained with tf-idf cosine similarity and our ranking model. Methods ROUGE-L Recall L = 5 L = 10 L = 20 L = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42 On average, each input has 525 paragraphs, and each paragraph has 70.1 tokens. The average length of the target summary is 139.4 tokens. We split the dataset with 1, 579, 360 instances for training, 38, 144 for validation and 38, 205 for test. 

 Table 2 : 2 Test set results on the WikiSum dataset using ROUGE F 1 . Model ROUGE-1 ROUGE-2 ROUGE-L Lead 38.22 16.85 26.89 LexRank 36.12 11.67 22.52 FT (600 tokens, no ranking) 35.46 20.26 30.65 FT (600 tokens) 40.46 25.26 34.65 FT (800 tokens) 40.56 25.35 34.73 FT (1,200 tokens) 39.55 24.63 33.99 T-DMCA (3000 tokens) 40.77 25.60 34.90 HT (1,600 tokens) 40.82 25.99 35.08 HT (1,600 tokens) + Similarity Graph 40.80 25.95 35.08 HT (1,600 tokens) + Discourse Graph 40.81 25.95 35.24 HT (train on 1,600 tokens/test on 3000 tokens) 41.53 26.52 35.76 

 Table 2 summarizes our results. The first block in the table includes extractive systems (Lead, LexRank), the second block includes several variants of Flat Transformer-based models (FT, T-DMCA), while the rest of the table presents the results of our Hierarchical Transformer (HT).As can be seen, abstractive models generally outperform extractive ones. The Flat Transformer, achieves best results when the input length is set to 800 tokens, while longer input (i.e., 1, 200 tokens) actually hurts performance. The Hierarchical Transformer with 1, 600 input tokens, outper- Model R1 R2 RL HT 40.82 25.99 35.08 HT w/o PP 40.21 24.54 34.71 HT w/o MP 39.90 24.34 34.61 HT w/o GT 39.01 22.97 33.76 Table 3: Hierarchical Transformer and versions thereof without (w/o) paragraph position (PP), multi-head pooling (MP), and global transformer layer (GT). 

 Table 4 : 4 System scores based on questions answered by AMT participants and summary quality rating. Model QA Rating Lead 31.59 -0.383 FT 35.69 0.000 T-DMCA 43.14 0.147 HT 54.11 0.237 

			 Our code and data is available at https://github. com/nlpyang/hiersumm. 

			 This was not the case with the other Transformer models.
