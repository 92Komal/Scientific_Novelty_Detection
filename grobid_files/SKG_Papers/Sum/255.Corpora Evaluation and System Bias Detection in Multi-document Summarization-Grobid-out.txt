title
Corpora Evaluation and System Bias Detection in Multi-document Summarization

abstract
Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents. There is also no standard regarding what constitutes summary information in MDS. Adding to the challenge is the fact that new systems report results on a set of chosen datasets, which might not correlate with their performance on the other datasets. In this paper, we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state-of-theart models. We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus. Next, we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora. We then observe the extent to which system metrics are influenced, and bias is propagated due to corpus properties. The scripts to reproduce the experiments in this work are available at https://github.com/ LCS2-IIITD/summarization_bias.git. * Equal contribution; listed alphabetically. 1 https://duc.nist.gov/

Introduction Multi-document summarization (MDS) deals with compressing more than one document into a textual summary. It has a wide range of applications -gaining insights from tweets related to similar hashtags, understanding product features amongst e-commerce reviews, summarizing live blogs related to an ongoing match, etc. Most studies on MDS were performed during the DUC 1 and TAC 2 challenges starting in the early 2000s. Each version of the challenges released a new dataset. Most of the MDS systems submitted to these challenges were unsupervised and extractive in nature. Gradually, the data released in these challenges became the de facto for MDS. These datasets were manually curated and had less than a hundred instances each. The recent development of deep neural architecture has led to a significant increase in the number of supervised document summarization systems. Large labeled corpora which are mostly crowd-sourced have been introduced to meet the training requirements of the supervised systems. However, the crowd-sourced corpora widely differ in quality based on factors like genre, size of the community, presence of moderation in the community, etc. This is further aggravated by the complexity of the task, the hardness of accumulating labeled data, or more so in the definition of what constitutes a multi-document summary. Recently, a few large datasets for MDS have been introduced  (Fabbri et al., 2019; Chowdhury and Chakraborty, 2019a) . However, there has been no study to measure the relative complexity of these datasets. We observe that existing MDS systems behave differently on different corpora. For example, a system achieving state-of-the-art performance on one corpus fails to achieve reasonable performance on another. Although the ROUGE points of MDS systems are increasing day-by-day, manual inspection reveals an increased presence of bias in generated summaries. New systems are being introduced and evaluated on a few selected corpora, leading to difficulty in understanding whether the bias is introduced by the system or it is present in the corpus used for training. Our research questions are as follows: Q1. How should one model the quality of a MDS corpus as a function of its intrinsic properties? Q2. Why do the ROUGE-based ranks of different MDS systems differ across different corpora? How should an MDS system which intends to achieve high ROUGE scores across all corpora, look like? Q3. Why do systems show bias on different metrics, and which other system and corpus attributes are the reason behind it? Q4. Is the task of MDS almost solved, or is there still scope for improvement? We study five MDS corpora -DUC  (DUC, 2002) , TAC  (TAC, 2008) , Opinosis  (Ganesan et al., 2010) ,  Multinews (Fabbri et al., 2019) , and CQA-Summ  (Chowdhury and Chakraborty, 2019b) . We consider eight popular summarization systems -LexRank  (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004) , MMR  (Carbinell and Goldstein, 2017) , ICSISumm  (Gillick et al., 2008) , PG  (See et al., 2017) , PG-MMR  (Lebanoff et al., 2018) ,  Hi-Map (Fabbri et al., 2019) , and Copy-Transformer  (Gehrmann et al., 2018) . Our major contributions are four-fold: ? We propose a suite of metrics to model the quality of an MDS corpus in terms of -Abstractness, Inter Document Similarity (IDS), Redundancy, Pyramid Score, Layout Bias and Inverse-Pyramid Score. ? We develop an interactive web portal for imminent corpora to be uploaded and evaluated based on our proposed metrics. ? We explore different biases that the MDS systems exhibit over different corpora and provide insight into properties that a universal MDS system should display to achieve reasonable performance on all types of corpora. ? We look into metrics to capture bias shown by MDS systems and explore the extent to which corpus properties influence them. To the best of our knowledge, the current study is the first of its kind. 

 Background and Proposed Metrics Throughout the paper, we use the term candidate documents for the documents participating in summarization, and the term reference to indicate the ground-truth summary. Oracle summary is the extractive set of sentences selected from the candidate documents, exhibiting maximum ROUGE-N score w.r.t. the reference summary. It is an NP-hard problem  (Hirao et al., 2017) , and approximate solutions can be found greedily or using ILP solvers. Here, we briefly introduce a suite of corpus and system metrics proposed by us to better understand the MDS task. These metrics are further explained in detail in Supplementary. 

 Corpus Metrics ? Abstractness: It is defined as the percentage of non-overlapping higher order n-grams between the reference summary and candidate documents. A high score highlights the presence of more distinctive phrases in reference summary. The intuition behind quantifying the number of new words is to sync with the basic human nature of paraphrasing while summarizing. ? Inter Document Similarity (IDS): It is an indicator of the degree of overlap between candidate documents. Inspired by the theoretical model of relevance  (Peyrard, 2019a) , we calculate IDS of a set of documents as follows: IDS(Di) = D j ?S Relevance(Dj, Di) |S| (1) where D i is the i th candidate document, and S is the set of all documents other than D i . Here, Relevance(.,.) is defined as: Relevance(A, B) = ? i PA(?i). log(PB(?i)) (2) where P A (? i ) represents the probability distribution of the i th semantic unit 3 in document A. The further this score is from 0, the lesser inter document overlap there is in terms of semantic unit distribution. As shown in Equation 1, the numerator calculates relevance which can be interpreted as the average surprise of observing one distribution while expecting another. This score is small if the distributions are similar i.e., P A ? P B from Equation 2. ? Pyramid Score: We propose the metric Corpus Pyramid score to measure how well important information across documents is represented in the ground truth. As introduced by  (Nenkova and Passonneau, 2004) , Pyramid score is a metric to evaluate system summaries w.r.t. the pool of ground-truth summaries. We instead use this metric to quantitatively analyze the ground-truth summary w.r.t. candidate documents. The entire information set is split into Summarization Content Units (SCUs 4 ), and each SCU is assigned a weight based on the number of times it occurs in the text. A pyramid of SCUs is constructed with an SCU's weight, denoting its level, and a score is assigned to a text, based on the number of SCUs it contains. Pyramid score is defined as the ratio of a reference summary score and an optimal summary score. Higher values indicate that the reference summary covers the SCUs at the top of the pyramid better. SCUs present at the top are the ones occurring in most articles and thus can be deemed as important. ? Redundancy: The amount of information in a text can be measured as the negative of Shannon's entropy (H)  (Peyrard, 2019a) . H(D) = ? ? i PD(?i). log(PD(?i)) (3) where P D represents the probability distribution of documents D, and ? i represents the i th semantic unit 3 in the distribution. H(D) would be maximized for a uniform probability distribution when each semantic unit is present only once. The farther this score is from 0, the better a document is distributed over its semantic units in the distribution, hence lesser the redundancy. As evident from Equation 5, redundancy is maximized if all semantics units have equal distribution i.e., P (? i ) = P (? j ). The idea behind using redundancy is to quantify how well individual documents cover sub-topics, which might not be the core content but important nonetheless. Thus Redundancy(D) = Hmax ? H(D) (4) Since H max is constant, we obtain Redundancy(D) = ? i PD(?i). log(PD(?i)) (5) ? Layout Bias: We define Layout Bias across a document as the degree of change in importance w.r.t. the ground-truth over the course of candidate documents. We divide the document into k segments, calculate the importance of each segment w.r.t. the ground-truth by a similarity score, and average over the sentences in the segment. Positional importance of D j , the j th sentence in the document is denoted by: P ositionalImportance(Dj) = max 1?i?n sim( ? ? Dj, ? ? Ri) (6) where, ? ? R i is the vector representation of the i th sentence in the reference, sim is a similarity metric between two sentences, and n is the total number of sentences in the reference summary. A lower shift indicates that while generating reference summaries, all segments have been given similar importance within any 3-fold segmented article. ? Inverse-Pyramid Score (Inv Pyr): We propose Inverse-Pyramid score to quantify the bias that a reference summary exhibits w.r.t. its set of candidate documents. It measures the importance given to each document in the candidate set by the reference summary as: InvP yr(D, S) = V arj Dj ? Su (7) Here, D and S are the set of candidate documents for MDS and their summary respectively, V ar is the variance, D j and S u are the sets of SCUs 4 in the j th document of the candidate set and the reference summary respectively. Higher Inv Pyr scores suggest the difference in importance given to each document while generating the summary is higher. As evident from Equation 7, Variance across the similarities is high if the similarity scores across the document-summary pairs are uneven. 

 System Metrics ? ROUGE  (Lin, 2004 ) is a metric which computes the n-gram overlap recall value for the generated summary w.r.t. the reference summary. ? F1 Score with Oracle Summaries: Oracle summaries reflect the extractive selection of sentences that achieve the highest ROUGE score over the candidate documents given a reference summary. Similar to ROUGE-1, this metric also combines both precision and recall between the oracle and system summaries to calculate F1 Score. It is a better indicator of the presence of non-essential ngrams than ROUGE as it also takes precision into account. ? System Abstractness: Analogous to corpus abstractness, we compute the percentage of novel higher order n-grams in the generated summary w.r.t. the candidate documents. System abstractness is calculated using Coverage(D, S) = i?1..n (D ? S i ) C n (S) where D represents the set of n-grams in the candidate document, and S represents the set of n-grams in the i th system summary. The denominator denotes the total count of n-grams in a system summary. Finally, the values of all articles is normalized to get the score for the system ? Layout Bias: We propose this metric to capture which sections of the candidate documents comprise a majority of the information in the generated  summary. For neural abstractive systems, we concatenate candidate documents to form one large document and feed it to the neural model. We study two variations of this metric -The first variation involves segmenting this large document into k parts and then computing the similarity of n-gram tokens of system summaries w.r.t. the candidate document segment. The second variation includes shuffling the candidate documents before concatenating and then computing the n-gram similarity with the generated summary. ? Inter Document Distribution (IDD): We propose this metric to quantify the extent of contribution of each candidate document to form the generated summary. The relevance for system summaries is calculated by, Relevance(A, B) = ? i P A (? i ). log(P B (? i )) where P A represents the probability distribution of system summary S, and ? i represents the i th semantic unit in the distribution. IDD(D i ) = D j ?S Relevance(D j , D i ) Cardinality(S) ? Redundancy: It measures the degree to which system summaries can cover the distribution across semantic units generated from the candidate documents. Redundancy for candidate documents is given by Eq., Redundancy(D) = ? i S D (? i ). log(S D (? i )) where S D represents the probability distribution of a system summary D. ? i represents the i th semantic unit in the distribution. 3 Experimental Setup  

 MDS Systems To identify bias in system-generated summaries, we study a few non-neural extractive and neural abstractive summarization systems, which are extensively used for multi-document summarization. ? LexRank  (Erkan and Radev, 2004 ) is a graph based algorithm that computes the importance of a sentence using the concept of eigen vector centrality in a graphical representation of text. ? TextRank  (Mihalcea and Tarau, 2004 ) runs a modified version of PageRank  (Brin and Page, 1998)  on a weighted graph, consisting of nodes as sentences and edges as similarities between sentences. ? Maximal Marginal Relevance (MMR) (Carbinell and Goldstein, 2017) is an extractive summarization system that ranks sentences based on higher relevance while considering the novelty of the sentence to reduce redundancy. ? ICSISumm  (Gillick et al., 2008)  optimizes the summary coverage by adopting a linear optimization framework. It finds a globally optimal summary using the most important concepts covered in the document. ? Pointer Generator (PG) network  (See et al., 2017)  is a sequence-to-sequence summarization model which allows both copying words from the source by pointing or generating words from a fixed vocabulary. ? Pointer Generator-MMR: PG-MMR  (Lebanoff et al., 2018)  uses MMR along with PG for better coverage and redundancy mitigation. ? Hi-Map: Hierarchical MMR-Attention PG model  (Fabbri et al., 2019)  extends the work of PG and MMR. MMR scores are calculated at word level and incorporated in the attention weights for a better summary generation. ? Bottom-up Abstractive Summarization (CopyTransformer)  (Gehrmann et al., 2018)  uses transformer parameters proposed by  (Vaswani et al., 2017) ; but one of the attention heads chosen randomly acts as a copy distribution.  

 Inferences from Corpus Metrics ? News derived corpora show a strong layout bias where significant reference information is contained in the introductory sentences of the candidate documents (Fig.  2 ). ? Different MDS corpora vary in compression factors with DUC at 56.55, TAC at 54.68, Multinews at 8.18 and CQASumm at 5.65. A high compression score indicates an attempt to pack candidate documents to a shorter reference summary. ? There has been a shift in the size and abstractness of reference summaries in MDS corpora over timewhile DUC and TAC were small in size and mostly extractive (11% novel unigrams); crowd-sourced corpora like CQASumm are large enough to train neural models and highly abstractive (41.4% novel unigrams). ? Candidate documents in Opinosis, TAC and DUC feature a high degree of redundant information as compared to Opinosis and CQASumm, with instances of the former revolving around a single key entity while that of the latter tending to show more topical versatility. ? MDS corpora present a variation in interdocument content overlap as well: while Multinews shows the highest degree of overlap, CQA-Summ shows the least and the rest of the corpora show moderate overlap (see Fig.  1 ). ? Pyramid Score, the metric which evaluates if the important and redundant SCUs 4 from the candidate documents have been elected to be part of the reference summary, shows considerably positive values for DUC, TAC and Multinews as compared to crowdsourced corpora like CQASumm (Fig.  3 .b). ? Inverse-Pyramid Score, the metric which evaluates how well SCUs 4 of the reference summary are distributed amongst candidate documents, also shows better performance on human-annotated corpora compared to crowd-sourced ones (Fig.  3(b) ). ? A comparison amongst corpus metrics presents a    

 Inferences from System Metrics ? MDS systems under consideration are ranked differently in terms of ROUGE on different corpora; leading to a dilemma whether to declare a system superior to others without testing on all types of datasets (Fig.  4 (c)) and Table  2 ). ? MDS systems under consideration outperform abstractive summarization systems by up to ? Hi-Map and CopyTransformer generate more abstract summaries (17.5% and 16% novel unigrams respectively) in comparison to PG and PG-MMR (Fig.  4(a) ). ? Averaging over systems and comparing corpora, we notice that Multinews and CQASumm achieve the highest abstractness (27% and 7% respectively), which might be a result of these two corpora having the most abstract reference summaries (Fig.  4 (a) and (Table  2 )). ? Abstractive systems exhibit a 55% shift in importance between the first and the second segments of generated summaries, whereas extractive systems show an average shift of only 40%, implying that    2 ). ? In terms of Topic Coverage, extractive systems show better coverage than abstractive systems (Table  2 ), which might be a result of extractive systems being based on sentence similarity algorithms which find important sentences, reduce redundancy and increase the spread of information from different segments of the candidate document. (Fig.  5 ). 

 Discussion on Research Questions Q1. How should one model the quality of an MDS corpus as a function of its intrinsic metrics? What guidelines should be followed to propose MDS corpora for enabling a fair comparison with existing datasets? The quality of an MDS corpus is a function of two independent variables: the quality of the candidate documents and the quality of the reference summary. Our findings suggest that a framework for future MDS datasets should provide scores measuring their standing w.r. CQASumm. Hence as of today, no summarization system strictly outperforms others on every corpus. We also see that CopyTransformer which achieves state-of-the-art performance on Multinews achieves 10 points less than the best system on DUC. Similarly, LexRank, the state-of-the-art performer on CQASumm, achieves almost 12 points less than the best system on TAC. Therefore, a system that performs reasonably well across all corpora, is also missing. This is because different corpora are high on various bias metrics, and summarization systems designed for a particular corpus take advantage and even aggravate these biases. For example, summarization systems proposed on news based corpora are known to feed only the first few hundred tokens to neural models, thus taking advantage of the layout bias. Feeding entire documents to these networks have shown relatively lower performance. Systems such as LexRank are known to perform well on candidate documents with high inter-document similarity (e.g., Opinosis). Solving the summarization problem for an unbiased corpus is a harder problem, and for a system to be able to perform reasonably well on any test set, it should be optimized to work on such corpora. Q3. Why do systems show bias on different metrics, and which other system and corpus attributes are the reason behind it? We begin by studying how abstractness of generated summaries is related to the abstractness of corpora the system is trained on. For this, we calculate the Pearson correlation coefficient between the abstractness of generated summaries and references across different datasets. From correlation which implies that they are likely to generate more abstract summaries if the datasets on which they are trained have more abstract references. Lastly, we infer how Layout Bias in system-generated summaries is dependent on the layout bias of reference summaries. The last three highlighted columns of Table  3  infer that the abstractive systems such as PG, PG-MMR, Hi-Map and CopyTransformer show a high negative correlation for the end segments while maintaining a strongly positive one with the starting segment. On the other hand, extractive systems such as LexRank, TextRank, MMR and ICSISumm maintain a strongly positive correlation throughout the segments. On shuffling the source segments internally, we observe that extractive systems tend to retain their correlation with corpora while abstractive systems show no correlation at all (Fig.  2 ), proving that in supervised systems, the layout bias in system summaries propagates from the layout bias present in corpora. Q4. Is the task of MDS almost solved, or there is still plenty of scope remaining for improvement? In the previous sections, we computed the oracle extractive upper bound summary using greedy approaches to find the summary that obtains the highest ROUGE score given the candidate documents and references. We observe that the best summarization system on each corpus today obtains a score which is 39.6% of the extractive oracle upper bound on DUC, 47.8% on TAC, 75.02% on Opinosis, 54.5% on Multinews and 49.9% on CQASumm. This shows that there is enough scope for MDS systems to achieve double the ROUGE scores obtained by the best system to date on each corpus except Opinosis. Therefore, we believe that the task of MDS is only partially solved and considerable efforts need to be devoted to improving the systems. 

 Related Work Previous attempts to evaluate the quality of the benchmark summarization corpora are few in number and mostly from the time when corpora were manually accumulated.  (Hirao et al., 2004)  primarily used the intrinsic metrics of precision and recall to evaluate corpus quality. In addition, the authors proposed an extrinsic metric, called 'Pseudo Question Answering'. This metric evaluates whether a summary has an answer to a question that is otherwise answerable by reading the documents or not. Although effective, the cost of such an evaluation is enormous and is not scalable to modern day corpora sizes. For such corpora where multiple references are available,  (Benikova et al., 2016 ) used an inter-annotator agreement to model the quality of the corpora. They also used non-redundancy, focus, structure, referential clarity, readability, coherence, length, grammaticality, spelling, layout, and overall quality as quantitative features for an MDS corpus. Recently,  (Chowdhury et al., 2020)  proposed an MDS system that used the baseline PG model along with Hierarchical structural attention to take into account long-term dependencies for superior results compared to baseline models. There have been a series of very recent studies that look into how to strengthen the definition and discover system biases in single-document summarization. Very recently,  (Jung et al., 2019)  studied how position, diversity and importance are significant metrics in analyzing the toughness of singledocument summarization corpora. Another recent work  (Kryscinski et al., 2019)  extensively studied the Layout Bias in news datasets that most single-document summarization systems seem to exploit. Two seminal works, namely  (Peyrard, 2019a)  and  (Peyrard, 2019b) , exploited the theoretical complexity of summarization on the ground of importance, analyzing in-depth what makes for a good summary.  (Peyrard, 2019a)  mathematically modeled the previously intuitive concepts of Redundancy, Relevance and Informativeness to define importance in single-document summarization.  (Grusky et al., 2018)  proposed a new singledocument summarization corpus and quantified how it compares to other datasets in terms of di-versity and difficulty of the data. They introduced metrics such as extractive fragment density and extractive fragment coverage to plot the quality of SDS corpus. To the best of our knowledge, no comparative work exists for either corpora or systems in MDS, and the current paper is the first in this direction. 

 Conclusion In this paper, we aimed to study the heterogeneous task of multi-document summarization. We analyzed interactions between widely used corpora and several state-of-the-art systems to arrive at a line of conclusions. We defined MDS as a mapping from a set of non-independent candidate documents to a synopsis that covers important and redundant content present in the source. We proposed intrinsic metrics to model the quality of an MDS corpus and introduced a framework for future researches to consider while proposing a new corpus. We analyzed how ROUGE ranks of different systems vary differently on different corpora and described what a system that achieves reasonable performance on all corpora would look like. We evaluated how different systems exhibit bias and how their behavior is influenced by corpus properties. We also commented on the future scope for the task of MDS. Future directions to take forward this work would include a causal analysis of how corpus bias is responsible for bias in model prediction across different corpora and systems. This might bring forward measures to de-bias NLP algorithms with/without de-biasing the corpora. Figure 1 : 1 Figure 1: Heatmap depicting the corpus metric: Inter document similarity. We explain with a single instance of (a) DUC-2004, (b) DUC-2003, (c) TAC-2008, and (d) CQASumm, and highlight inter-document overlap. 
