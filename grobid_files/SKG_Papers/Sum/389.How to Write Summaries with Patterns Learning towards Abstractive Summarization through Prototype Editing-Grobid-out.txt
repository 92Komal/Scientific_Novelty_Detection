title
How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing

abstract
Under special circumstances, summaries should conform to a particular style with patterns, such as court judgments and abstracts in academic papers. To this end, the prototype document-summary pairs can be utilized to generate better summaries. There are two main challenges in this task: (1) the model needs to incorporate learned patterns from the prototype, but (2) should avoid copying contents other than the patternized wordssuch as irrelevant facts-into the generated summaries. To tackle these challenges, we design a model named Prototype Editing based Summary Generator (PESG). PESG first learns summary patterns and prototype facts by analyzing the correlation between a prototype document and its summary. Prototype facts are then utilized to help extract facts from the input document. Next, an editing generator generates new summary based on the summary pattern or extracted facts. Finally, to address the second challenge, a fact checker is used to estimate mutual information between the input document and generated summary, providing an additional signal for the generator. Extensive experiments conducted on a large-scale real-world text summarization dataset 1 show that PESG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.

Introduction Abstractive summarization can be regarded as a sequence mapping task that maps the source text to the target summary  (Rush et al., 2015; Li et al., 2017; Cao et al., 2018; Gao et al., 2019a) . It has drawn significant attention since the introduction prototype summary The court held that the defendant Wang had stolen the property of others for the purpose of illegal possession. The amount was large, and his behavior constituted the crime of theft. The accusation of the public prosecution agency was established. The defendant Wang has a criminal record and will be considered when sentencing. Since the defendant Wang did not succeed because of reasons other than his will, he could be punished lightly. After the defendant confessed his crimes to the case, he was given a lighter punishment according to law. summary The court held that the accused Zhang and Fan stole property and the amount was large. Their actions constituted the crime of theft. The accusation of the public prosecution agency was established and supported. This crime was committed within two years after the release of the defendants Zhang and Fan. Thus they are recidivists and this situation will be considered when sentencing. The fact that defendants Zhang and Fan surrendered themselves and pleaded guilty in court gives a lighter punishment according to law. of deep neural networks to natural language processing. Under special circumstances, the generated summaries are required to conform to a specific pattern, such as court judgments, diagnosis certificates, abstracts in academic papers, etc. Take the court judgments for example, there is always a statement of the crime committed by the accused, followed by the motives and the results of the judgment. An example case is shown in Table  1 , where the summary shares the same writing style and has words in common with the prototype summary (retrieved from the training dataset). Existing prototype based generation models such as  are all applied on short text, thus, cannot handle long documents summarization task. Another series of works focus on template-based methods such as  (Oya et al., 2014) . However, template-based methods are too rigid for our patternized summary generation task. Hence, in this paper, we propose a summarization framework named Prototype Editing based Summary Generator (PESG) that incorporates prototype document-summary pairs to improve summa-rization performance when generating summaries with pattern. First, we calculate the cross dependency between the prototype document-summary pair to obtain a summary pattern and prototype facts (explained in ? 4.2). Then, we extract facts from the input document with the help of the prototype facts (explained in ? 4.3). Next, a recurrent neural network (RNN) based decoder is used to generate a new summary, incorporating both the summary pattern and extracted facts (explained in ? 4.4). Finally, a fact checker is designed to provide mutual information between the generated summary and the input document to prevent the generator from copying irrelevant facts from the prototype (explained in ? 4.5). To evaluate PESG, we collect a large-scale court judgment dataset, where each judgment is a summary of the case description with a patternized style. Extensive experiments conducted on this dataset show that PESG outperforms the state-of-the-art summarization baselines in terms of ROUGE metrics and human evaluations by a large margin. Our contributions can be summarized as follows: ? We propose to use prototype information to help generate better summaries with patterns. ? Specifically, we propose to generate the summary incorporating the prototype summary pattern and extracted facts from input document. ? We provide mutual information signal for the generator to prevent copying irrelevant facts from the prototype. ? We release a large-scale prototype based summarization dataset that is beneficial for the community. 

 Related Work We detail related work on text summarization and prototype editing. Text summarization can be classified into extractive and abstractive methods. Extractive methods  (Narayan et al., 2018b; Chen et al., 2018)  directly select salient sentences from an article to compose a summary. One shortcoming of these models is that they tend to suffer from redundancy. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization  (Ma et al., 2018; Gao et al., 2019a; Chen et al., 2019)  is dedicated to abstractive summarization, which aims to generate new content that concisely para-phrases a document from scratch. Another line of research focuses on prototype editing.  (Guu et al., 2018)  proposed the first prototype editing model, which samples a prototype sentence from training data and then edits it into a new sentence. Following this work,  proposed a new paradigm for response generation, which first retrieves a prototype response from a pre-defined index and then edits the prototype response.  (Cao et al., 2018)  applied this method on summarization, where they employed existing summaries as soft templates to generate new summary without modeling the dependency between the prototype document, summary and input document. Different from these soft attention methods,  (Cai et al., 2018)  proposed a hard-editing skeleton-based model to promote the coherence of generated stories. Template-based summarization is also a hard-editing method  (Oya et al., 2014) , where a multi-sentence fusion algorithm is extended in order to generate summary templates. Different from all above works, our model focuses on patternized summary generation, which is more challenging than traditional news summarization and short sentence prototype editing. 

 Problem Formulation For an input document X = {x 1 , x 2 , . . . , x Tm }, we assume there is a ground truth summary Y = {y 1 , y 2 , . . . , y Tn }. In our prototype summarization task, a retrieved prototype document X = {x 1 , x2 , . . . , xTm } with a corresponding prototype summary ? = {? 1 , ?2 , . . . , ?Tn } is also attached according to their similarities with X. For a given document X, our model extracts salient facts from X guided by a prototype document X, and then generates the summary Y by referring to the prototype summary ? . The goal is to generate a summary Y that not only follows a patternized style (as defined by prototype summary ? ) but also is consistent with the facts in document X. 

 Model 

 Overview In this section, we propose our prototype editing based summary generator, which can be split into two main parts, as shown in Figure  1:  ? Summary Generator. (1) Prototype Reader analyzes the dependency between X and ? to de- (3) Editing Generator; (4) Fact Checker. termine the summary pattern and prototype facts. (2) Fact Extraction module extracts facts from the input document under the guidance of the prototype facts. (3) Editing Generator module generates the summary Y of document X by incorporating summary pattern and facts. ? Fact Checker estimates the mutual information between the generated summary Y and input document X. This information provides an additional signal for the generation process, preventing irrelevant facts from being copied from the prototype document. 

 Prototype Reader To begin with, we use an embedding matrix e to map a one-hot representation of each word in X, X, ? into a high-dimensional vector space. We then employ a bi-directional recurrent neural network (Bi-RNN) to model the temporal interactions between words: h x t = Bi-RNN x (e(x t ), h x t?1 ), (1) ?x t = Bi-RNN x (e(x t ), ?x t?1 ), (2) ?y t = Bi-RNN y (e(? t ), ?y t?1 ), (3) where h x t , ?x t and ?y t denote the hidden state of tth step in Bi-RNN for X, X and ? , respectively. Following  Gao et al., 2019b; Hu et al., 2019) , we choose long short-term memory (LSTM) as the cell for Bi-RNN. On one hand, the sections in the prototype summary that are not highly related to the prototype document are the universal patternized words and should be emphasized when generating the new summary. On the other hand, the sections in the prototype document that are highly related to the prototype summary are useful facts that can guide the process of extracting facts from input document. Hence, we employ a bi-directional attention mechanism between a prototype document and summary to analyze the cross-dependency, that is, from document to summary and from summary to document. Both of these attentions are derived from a shared similarity matrix, S ? R Tm?Tn , calculated by the hidden states of prototype document X and prototype summary ? . S ij indicates the similarity between the i-th document word xi and j-th summary word ?j and is computed as: S ij = ?( ?x i , ?y j ), ?(x, y) = w [x ? y ? (x ? y)], ( 4 ) where ? is a trainable scalar function that calculates the similarity between two input vectors. ? denotes a concatenation operation and ? is an element-wise multiplication. We use a s t = mean(S :t ) ? R to represent the attention weight on the t-th prototype summary word by document words, which will learn to assign high weights to highly related universal patternized words when generating a summary. From a s t , we obtain the weighted sum of the hidden states of prototype summary as "summary pattern" l = {l 1 , . . . , l Tn }, where l i is: l i = a s i ?y i . (5) Similarly, a d t = mean(S t: ) ? R assigns high weights to the words in a prototype document that are relevant to the prototype summary. A convolutional layer is then applied to extract "prototype facts" rt from the prototype document: rt = CNN(a d t ?x t ). (6) We sum the prototype facts to obtain the overall representation of these facts: q = Tm t rt . ( 7 ) 

 Fact Extraction In this section, we discuss how to extract useful facts from an input document with the help of prototype facts. We first extract the facts from an input document by calculating their relevance to prototype facts. The similarity matrix E is then calculated between the weighted prototype document a d i ?x i and input document representation h x j : where ? is the similarity function introduced in Equation  4 . Then, we sum up E ij along the length of the prototype document to obtain the weight E j = Tm ti E tj for j-th word in the document. Next, similar to Equation  6 , a convolutional layer is applied on the weighted hidden states E t h x t to obtain the fact representation r t from the input document: E ij = ?(a d i ?x i , h x j ), (8) r t = CNN(E t h x t ). (9) Inspired by the polishing strategy in extractive summarization  (Chen et al., 2018) , we propose to use the prototype facts to polish the extracted facts r t and obtain the final fact representation m . , as shown in Figure  2 . Generally, the polishing process consists of two hierarchical recurrent layers. The first recurrent layer is made up of Selective Recurrent Units (SRUs), which take facts r ? and polished fact q k as input, outputting the hidden state h k Tm . The second recurrent layer consists of regular Gated Recurrent Units (GRUs), which are used to update the polished fact from q k to q k+1 using h k Tm . SRU is a modified version of the original GRU introduced in  (Chen et al., 2018) , details of which can be found in Appendix A. Its difference from GRU lies in that the update gate in SRU is decided by both the polished fact q k and original fact r t together. The t-th hidden state of SRU is calculated as: h k t = SRU(r t , q k ). ( 10 ) We take h k Tm as the overall representation of all input facts r ? . In this way, SRU can decide to which degree each unit should be updated based on its relationship with the polished fact q k . Next, h k Tm is used to update the polished fact q k using the second recurrent layer, consisting of GRUs: m k+1 , q k+1 = GRU(h k Tm , q k ), (11) where q k is the cell state, h k Tm is the input and m k+1 is the output hidden state. q 0 is initialized using q in Equation 7. This iterative process is conducted K times, and each output m k is stored as extracted facts M = {m 1 , m 2 , . . . , m K }. In this way, M stores facts with different polished levels. 

 Editing Generator The editing generator aims to generate a summary based on the input document, prototype summary and extracted facts. As with the settings of prototype reader, we use LSTM as the RNN cell. We first apply a linear transformation on the summation of the summary pattern l = Tn i l i and input document representations h x Tm , and then employ this vector as the initial state d 0 of the RNN generator as shown in Equation  12 . The procedure of t-th generation is shown in Equation  13 : d 0 = W e [h x Tm ? l ] + b e , (12) d t = LSTM(d t?1 , [g i t?1 ? e(y t?1 )]), (13) where W e , b e are trainable parameters, d t is the hidden state of the t-th generating step, and g i t?1 is the context vector produced by the standard attention mechanism  (Bahdanau et al., 2015) . To take advantage of the extracted facts M and prototype summary l, we incorporate them both into summary generation using a dynamic attention. More specifically, we utilize a matching function f to model the relationship between the current decoding state d t and each v i (v i can be a extracted fact m i or summary pattern l i ): ? it = exp(f (v i , d t )) K j exp(f (v j , d t )) , (14) g * t = K i ? it v i , (15) where g * t can be g m t or g s t for attending to extracted facts or a summary pattern, respectively. We use a simple but efficient bi-linear layer as the matching function f = m i W f d t . As for combining g m t and g s t , we propose to use an "editing gate" ?, which is determined by the decoder state d t , to decide the importance of the summary pattern and extracted facts at each decoding step. ? = ? (W g d t + b g ) , ( 16 ) where ? denotes the sigmoid function. Using the editing gate, we obtain g h t which dynamically combines information from the extracted facts and summary pattern with the editing gate ?, as: g h t = [?g m t ? (1 ? ?) g s t ] . (17) Finally, the context vector g h t is concatenated with the decoder state d t and fed into a linear layer to obtain the generated word distribution P v : d o t = W o [d t ? g h t ] + b o , ( 18 ) P v = softmax (W v d o t + b v ) . ( 19 ) The loss is the negative log likelihood of the target word y t : L s = ? Tn t=1 log P v (y t ). (20) In order to handle the out-of-vocabulary (OOV) problem, we equip our decoder with a pointer network  (Gu et al., 2016; Vinyals et al., 2015; See et al., 2017) . This process is the same as the model described in  (See et al., 2017) , thus, is omit here due to limited space. What's more, previous work  (Holtzman et al., 2018)  has found that using a cross entropy loss alone is not enough for generating coherent text. Similarly, in our task, using L s alone is not enough to distinguish a good summary with accurate facts from a bad summary with detailed facts from the prototype document (see ? 6.2). Thus, we propose a fact checker to determine whether the generated summary is highly related to the input document. 

 Fact Checker To generate accurate summaries that are consistent with the detailed facts from the input document rather than facts from the prototype document, we add a fact checker to provide additional training signals for the generator. Following  (Hjelm et al., 2018) , we employ the neural mutual information estimator to estimate the mutual information between the generated summary Y and its corresponding document X, as well as the prototype document X. Generally, mutual information is estimated from a local and global level, and we expect the matching degree to be higher between the generated summary and input document than the prototype document. An overview of the fact checker is shown in Figure  3 . To begin, we use a local matching network to calculate the matching degree, for local features, between the generated summary and the input, as well as prototype document. Remember that, in   13 ), yields the local features of input extracted facts and the prototype facts: C r = {d Tn ? r 1 , . . . , d Tn ? r Tm }, (21) C f = {d Tn ? r1 , . . . , d Tn ? rTm }. (22) A 1 ? 1 convolutional layer and a fully-connected layer are applied to score these two features: ? r l = CNN l (C r ), ? f l = CNN l (C f ), (23) where ? r l ? R, ? f l ? R represent the local matching degree between the generated summary and input document and prototype document, respectively. We want the generated summary to be more similar to the input document than the prototype document. Thus, the optimization objective of the local matching network is to minimize L l : L l = ? log(? r l ) + log(1 ? ? f l ) . ( 24 ) We also have a global matching network to measure the matching degree, for global features, between the generated summary and the input document, as well as prototype document. To do so, we concatenate the representation of the generated summary with the final hidden state of the input document h x Tm and final state of the prototype document ?x Tm , respectively, and apply a linear layer to these: ? r g = relu(W m [d Tn ? h x Tm ] + b m ), (25) ? f g = relu(W m [d Tn ? ?x Tm ] + b m ), (26) where W m , b m are trainable parameters and ? r g ? R and ? f g ? R represent the matching degree between the generated summary and the input document, and prototype document, respectively. The objective of this global matching network, similar to the local matching network, is to minimize:  Finally, we combine the local and global loss functions to obtain the final loss L, which we use L to calculate the gradients for all parameters: L g = ? log(? r g ) + log(1 ? ? f g ) . ( 27 ) L = L g + ?L l + L s , (28) where , ? are both hyper parameters. To optimize the trainable parameters, we employ the gradient descent method Adagrad  (Duchi et al., 2010)  to update all parameters. 5 Experimental Setup 

 Dataset We collect a large-scale prototype based summarization dataset 2 , which contains 2,003,390 court judgment documents. In this dataset, we use a case description as an input document and the court judgment as the summary. The average lengths of the input documents and summaries are 595.15 words and 273.57 words respectively. The percentage of words common to a prototype summary and the reference summary is 80.66%, which confirms the feasibility and necessity of prototype summarization. Following other summarization datasets  (Grusky et al., 2018; Kim et al., 2019; Narayan et al., 2018a) , we also count the novel ngrams in a summary compared with the n-grams in the original document, and the percentage of novel n-grams are 51.21%, 84.59%, 91.48%, 94.83% for novel 1-grams to 4-grams respectively. The coverage, compression and density  (Grusky et al., 2018)  are commonly used as metrics to evaluate the abstractness of a summary. For the summaries in our dataset, the coverage percentage is 48.78%, compression is 2.28 and density is 1.31. We anonymize entity tokens into special tags, such as using "PERS" to replace a person's name. 

 Comparisons In order to prove the effectiveness of each module of PESG, we conduct several ablation studies, shown in Table  2 . We also compare our model with the following baselines: (1) Lead-3 is a commonly used summarization baseline  (Nallapati et al., 2017; See et al., 2017) , which selects the first three sentences of document as the summary. (2) S2S is a sequence-to-sequence framework with a pointer network, proposed by  (See et al., 2017) . (  3 ) Proto is a context-aware prototype editing dialog response generation model proposed by . (  4 ) Re 3 Sum, proby  (Cao et al., 2018) , uses an IR platform to retrieve proper summaries and extends the seq2seq framework to jointly conduct template-aware summary generation. (5) Uni-model was proposed by  (Hsu et al., 2018) , and is the current stateof-the-art abstractive summarization approach on the CNN/DailyMail dataset. (  6 ) We also directly concatenate the prototype summary with the original document as input for S2S and Uni-model, named as Concat-S2S and Concat-Uni, respectively. 

 Evaluation Metrics For the court judgment dataset, we evaluate standard ROUGE-1, ROUGE-2 and ROUGE-L  (Lin, 2004 ) on full-length F1 following previous works  (Nallapati et al., 2017; See et al., 2017; Paulus et al., 2018) , where ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) refer to the matches of unigram, bigrams, and the longest common subsequence respectively.  (Schluter, 2017)  notes that only using the ROUGE metric to evaluate summarization quality can be misleading. Therefore, we also evaluate our model by human evaluation. Three highly educated participants are asked to score 100 randomly sampled summaries generated by three models: Uni-model, Re 3 Sum and PESG. The statistical significance of observed differences between the performance of two runs is tested using a twotailed paired t-test and is denoted using (or ) for strong (or weak) significance for ? = 0.01. 

 Implementation Details We implement our experiments in Tensor-Flow  (Abadi et al., 2016)  on an NVIDIA GTX 1080 Ti GPU. The word embedding dimension is 256 and the number of hidden units is 256. The batch size is set to 64. We padded or cut input document to contain exactly 250 words, and the decoding length is set to 100. and ? from the Equation 28 are both set to 1.0. We initialize all of the parameters randomly using a Gaussian distribution. We use Adagrad optimizer (  Duchi The court held that the defendant PERS was drunk driving a motor vehicle on the road, and his behavior constituted a dangerous driving offence and should be punished according to law.    et al., 2010) as our optimizing algorithm and employ beam search with size 5 to generate more fluency summary sentence. We also apply gradient clipping  (Pascanu et al., 2013)  with range [?5, 5] during training. We use dropout  (Srivastava et al., 2014)  as regularization with keep probability p = 0.7. 

 T h e c o u r t h e l d t h a t t h e d e f e n d a n t o n t h e r o a d w a s d r u n k d r i v i n g m o t o r v e h i c l e h i s b e h a v i o r c o n s t i t u t e d 6 Experimental Result 

 Overall Performance We compare our model with the baselines listed in Table  3 . Our model performs consistently better than other summarization models including the state-of-the-art model with improvements of 6%, 12% and 6% in terms of ROUGE-1, ROUGE-2 and ROUGE-L. This demonstrates that prototype document-summary pair provides strong guidance for summary generation that cannot be replaced by other complicated baselines without prototype information. Meanwhile, directly concatenating the prototype summary with the original input does not increase performance, instead leading to drops of 9%, 17%, 8% and 1%, 3%, 2% in terms of ROUGE 1,2,L on the S2S and Unified models, respectively. As for the baseline model Proto, we found that it directly copies from the prototype summary as generated summary, which leads to a totally useless and incorrect summary. For the human evaluation, we asked annotators to rate each summary according to its consistency and fluency. The rating score ranges from 1 to 3, with 3 being the best. Table  4  lists the average scores of each model, showing that PESG outperforms the other baseline models in both fluency and consistency. The kappa statistics are 0.33 and 0.29 for fluency and consistency respectively, and that indicates the moderate agreement between annotators. To prove the significance of these results, we also conduct the paired student t-test between our model and Re 3 Sum (row with shaded background). We obtain a p-value of 2 ? 10 ?7 and 9 ? 10 ?12 for fluency and consistency, respectively. We also analyze the effectiveness of performance by the two hyper-parameters: ? and . It turns out that our model has a consistently good performance, with ROUGE-1, ROUGE-2, ROUGE-L scores above 39.5, 27.5, 39.4, which demonstrates that our model is very robust. 

 Ablation Study The ROUGE scores of different ablation models are shown in Table  5 . All ablation models perform worse than PESG in terms of all metrics, which demonstrates the preeminence of PESG. More importantly, by this controlled experiment, we can verify the contributions of each modules in PESG. 

 Analysis of Editing Generator We visualize the editing gate (illustrated in Equa- shown in Figure  4 . A lower weight (lighter color) means that the word is more likely to be copied from the summary pattern; that is to say, this word is a universal patternized word. We can see that the phrase ? (the court held that) has a lower weight than the name of the defendant (PERS), which is consistent with the fact that (the court held that) is a patternized word and the name of the defendant is closely related to the input document. We also show a case study in Table  6 , which includes the input document and reference summary with the generated summaries. Underlined text denotes a grammar error and a strike-through line denotes a fact contrary to the input document. We only show part of the document and summary due to limited space; the full version is shown in Appendix. As can be seen, the summary generated by Uni-model faces an inconsistency problem and the summary generated by Re 3 Sum is contrary to the facts described in the input document. However, PESG overcomes both of these problems and generates an accurate summary with good grammar and logic. 

 Analysis of Fact Extraction Module We investigate the influence of the iteration number when facts are extracted. Figure  5  illustrates the relationship between iteration number and the f-value of the ROUGE score. The results show that the ROUGE scores first increases with the number of hops. After reaching an upper limit it then begins to drop. This phenomenon demonstrates that the fact extraction module is effective by polishing the facts representation. 

 Conclusion In this paper, we propose a framework named Prototype Editing based Summary Generator (PESG), which aims to generate summaries in formal   
