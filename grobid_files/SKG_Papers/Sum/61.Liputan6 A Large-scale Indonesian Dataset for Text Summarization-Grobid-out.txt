title
Liputan6: A Large-scale Indonesian Dataset for Text Summarization

abstract
In this paper, we introduce a large-scale Indonesian summarization dataset. We harvest articles from Liputan6.com, an online news portal, and obtain 215,827 documentsummary pairs. We leverage pre-trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT-based models. We include a thorough error analysis by examining machinegenerated summaries that have low ROUGE scores, and expose both issues with ROUGE itself, as well as with extractive and abstractive summarization models.

Introduction Despite having the fourth largest speaker population in the world, with 200 million native speakers,  1  Indonesian is under-represented in NLP. One reason is the scarcity of large datasets for different tasks, such as parsing, text classification, and summarization. In this paper, we attempt to bridge this gap by introducing a large-scale Indonesian corpus for text summarization. Neural models have driven remarkable progress in summarization in recent years, particularly for abstractive summarization. One of the first studies was  Rush et al. (2015) , where the authors proposed an encoder-decoder model with attention to generate headlines for English Gigaword documents  (Graff et al., 2003) . Subsequent studies introduced pointer networks  (Nallapati et al., 2016b; See et al., 2017) , summarization with content selection  (Hsu et al., 2018; Gehrmann et al., 2018) , graph-based attentional models  (Tan et al., 2017) , and deep reinforcement learning  (Paulus et al., 2018) . More recently, we have seen the widespread adoption of pre-trained neural language models for summarization, e.g. BERT  (Liu and Lapata, 2019) , BART , and PEGASUS  (Zhang et al., 2020a) . Progress in summarization research has been driven by the availability of large-scale English datasets, including 320K CNN/Daily Mail document-summary pairs  (Hermann et al., 2015)  and 100k NYT articles  (Sandhaus, 2008)  which have been widely used in abstractive summarization research  (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2018; Zhang et al., 2020a) . News articles are a natural candidate for summarization datasets, as they tend to be well-structured and are available in large volumes. More recently, English summarization datasets in other flavours/domains have been developed, e.g. XSum has 226K documents with highly abstractive summaries  (Narayan et al., 2018) , BIGPATENT is a summarization dataset for the legal domain  (Sharma et al., 2019) , Reddit TIFU is sourced from social media  (Kim et al., 2019) , and  Cohan et al. (2018)  proposed using scientific publications from arXiv and PubMed for abstract summarization. This paper introduces the first large-scale summarization dataset for Indonesian, sourced from the Liputan6.com online news portal over a 10year period. It covers various topics and events that happened primarily in Indonesia, from October 2000 to October 2010. Below, we present details of the dataset, propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre-trained BERT models. We further conduct error analysis to better understand the limitations of current models over the dataset, as part of which we reveal not just modelling issues but also problems with ROUGE. To summarize, our contributions are: (1) we release a large-scale Indonesian summarization corpus with over 200K documents, an order of mag- 

 Dokumen: Liputan6.com, Jakarta : Gara-gara berusaha kabur saat diminta menunjukkan barang hasil curian, Rosihan bin Usman, tersangka pencurian tas wisatawan asing, baru-baru ini, tersungkur ditembak aparat Kepolisian Resor Denpasar Barat, Bali. Sebelumnya, Rosihan ditangkap massa setelah mencuri tas Nicholas Dreyden, wisatawan asing asal Inggris. Tas yang berisi dokumen keimigrasian dan surat penting itu diambil Rosihan setelah mengelabui korban. [7 kalimat dengan 78 kata setelahnya tidak ditampilkan] Ringkasan: Seorang pencuri tas wisatawan asing ditembak polisi. Ia berusaha kabur saat diminta menunjukan hasil curian. Karena itu, polisi menembaknya. 

 Example-2 

 Document: Liputan6.com, Jakarta: Because of trying to escape when asked to show stolen goods, Rosihan bin Usman, a suspect of the theft of a foreign tourist bag, recently fell down, shot by the West Denpasar Resort Police, Bali. Previously, Rosihan was arrested by the mob after stealing the bag of Nicholas Dreyden, a foreign tourist from England. The bag containing immigration documents and important letters was taken by Rosihan after tricking the victim. [7 sentences with 78 words are abbreviated from here] Summary: A foreign tourist bag thief was shot by police. He tried to run away when asked to show the loot. Because of this, the police shot him. 

 Dokumen: Liputan6.com, Jakarta : Organisasi Negara-negara Pengekspor Minyak (OPEC) mengakui mengalami kesulitan untuk menjaga stabilitas harga minyak dunia. Itu lantaran harga minyak terus melonjak sepanjang tahun ini. Hingga kini harga minyak mentah dunia masih mencapai tingkat tertinggi sejak pecah perang teluk sepuluh tahun silam. [3 kalimat dengan 57 kata tidak ditampilkan] Padahal , sebelumnya OPEC telah merevisi produksi minyak sebanyak tiga kali dalam enam bulan terakhir. Pertama, April hingga Juni dengan kenaikan mencapai 500 ribu barel dan terakhir, September ini, OPEC kembali menaikkan produksi sebesar 800 ribu barel per hari. [5 kalimat dengan 96 kata setelahnya tidak ditampilkan] Ringkasan: OPEC kesulitan menjaga stabilitas harga minyak dunia lantaran harga minyak dipasaran terus melonjak. Padahal, OPEC telah tiga kali menaikkan produksi dalam enam bulan terakhir. 

 Example-1 

 Document: Liputan6.com, Jakarta: The Organization of Petroleum Exporting Countries (OPEC) has admitted that it is having difficulty maintaining the stability of world oil prices. That's because oil prices continue to soar this year. Until now world crude oil prices have still reached the highest level since the gulf war broke out ten years ago. [3 sentences with 57 words are abbreviated from here] In fact, OPEC had previously revised oil production three times in the last six months. First, April to June with an increase of 500 thousand barrels and last, this September, OPEC has again increased production by 800 thousand barrels per day. [5 sentences with 96 words are abbreviated from here] Summary: OPEC is struggling to maintain the stability of world oil prices because oil prices on the market continue to soar. In fact, OPEC has raised production three times in the past six months. Figure  1 : Example articles and summaries from Liputan6. To the left is the original document and summary, and to the right is an English translation (for illustrative purposes). We additionally highlight sentences that the summary is based on (noting that such highlighting is not available in the dataset). nitude larger than the current largest Indonesian summarization dataset and one of the largest non-English summarization datasets in existence; 2 (2) we present statistics to show that the summaries in the dataset are reasonably abstractive, and provide two test partitions, a standard test set and an extremely abstractive test set; (3) we develop benchmark extractive and abstractive summarization models based on pre-trained BERT models; and (4) we conduct error analysis, on the basis of which we share insights to drive future research on Indonesian text summarization. 

 Data Construction Liputan6.com is an online Indonesian news portal which has been running since August 2000, and provides news across a wide range of topics including politics, business, sport, technology, health, and entertainment. According to the Alexa ranking of websites at the time of writing, 3 Liputan6.com is ranked 9th in Indonesia and 112th globally. The website produces daily articles along with a short description for its RSS feed. The summary is encapsulated in the javascript variable window.kmklabs.article and the key shortDescription, while the article is in the main body of the associated HTML page. We harvest this data over a 10-year window -from October 2000 to October 2010 -to create a largescale summarization corpus, comprising 215,827 document-summary pairs. In terms of preprocessing, we remove formatting and HTML entities (e.g. &quot, and ), lowercase all words, and segment sentences based on simple punctuation heuristics. We provide example articles and summaries, with English translations for expository purposes (noting that translations are not part of the dataset), in Figure  1 . As a preliminary analysis of the documentsummary pairs over the 10-year period, we binned the pairs into 5 chronologically-ordered groups containing 20% of the data each, and computed the proportion of novel n-grams (order 1 to 4) in the summary (relative to the source document). Based on the results in Figure  2 , we can see that the proportion of novel n-grams drops over time, implying that the summaries of more recent articles are less  abstractive. For this reason, we decide to use the earlier articles (October 2000 to Jan 2002) as the development and test documents, to create a more challenging dataset. This setup also means there is less topic overlap between training and development/test documents, allowing us to assess whether the summarization models are able to summarize unseen topics. For the training, development and test partitions, we use a splitting ratio of 90:5:5. In addition to this canonical partitioning of the data, we provide an "Xtreme" variant (inspired by Xsum; Narayan et al. (  2018 )) whereby we discard development and test document-summary pairs where the summary has fewer than 90% novel 4-grams (leaving the training data unchanged), creating a smaller, more challenging data configuration. Summary statistics for the "canonical" and "Xtreme" variants are given in Table  1 . We next present a comparison of Liputan6 (canonical partitioning) and IndoSum (the current largest Indonesian summarization dataset, as detailed in Section 6;  Kurniawan and Louvan (2018) ) in Table  2 . In terms of number of documents, Liputan6 is approximately 11 times larger than IndoSum (the current largest Indonesian summarization dataset), although articles and summaries in Liputan6 are slightly shorter. To understand the abstractiveness of the summaries in the two datasets, in Table  3  we present ROUGE scores for the simple baseline of using the first N sentences as an extractive summary ("LEAD-N "), and the percentage of novel n-grams in the summary.  4  We use LEAD-3 and LEAD-2 for IndoSum and Liputan6 respectively, based on the average number of sentences in the summaries (Table  2 ). We see that Liputan6 has consistently lower ROUGE scores (R1, R2, and RL) for LEAD-N ; it also has a substantially higher proportion of novel n-grams. This suggests that the summaries in Liputan6 are more abstractive than IndoSum. To create a ground truth for extractive summarization, we follow  Cheng and Lapata (2016)  and  Nallapati et al. (2016a)  in greedily selecting the subset of sentences in the article that maximizes the ROUGE score based on the reference summary. As a result, each sentence in the article has a binary label to indicate whether they should be included as part of an extractive summary. Extractive summaries created this way will be referred to as "ORACLE", to denote the upper bound performance of an extractive summarization system. 

 Summarization Models We follow  Liu and Lapata (2019)  in building extractive and abstractive summarization models using BERT as an encoder to produce contextual representations for the word tokens. The architecture of both models is presented in Figure  3 . We tokenize words with WordPiece, and append [CLS] (prefix) and [SEP] (suffix) tokens to each sentence. To further distinguish the sentences, we add even/odd segment embeddings (T A /T B ) based on the order of the sentence to the word embeddings. For instance, for a document with sentences [s 1 , s 2 , s 3 , s 4 ], the segment embeddings are [T A , T B , T A , T B ]. Position embeddings (P ) are also used to denote the position of each token. The WordPiece, segment, and position embeddings are summed together and provided as input to BERT. BERT produces a series of contextual representations for the word tokens, which we feed into a (second) transformer encoder/decoder for the extractive/abstractive summarization model. We detail the architecture of these two models in Sections 3.1 and 3.2. Note that this second transformer is initialized with random parameters (i.e. it is not pre-trained). For the pre-trained BERT encoder, we use mul-  tilingual BERT (mBERT) and our own IndoBERT  (Koto et al., to appear) .  5  IndoBERT is a BERT-Base model we trained ourselves using Indonesian documents from three sources: (1) Indonesian Wikipedia (74M words); (2) news articles (55M words) from Kompas, 6 Tempo  (Tala et al., 2003) ,  7  and Liputan6; 8 and (3) the Indonesian Web Corpus (90M words;  Medved and Suchomel (2017) ). In total, the training data has 220M words. We implement IndoBERT using the Huggingface framework, 9 and follow the default configuration of BERT-Base (uncased): hidden size = 768d, hidden layers = 12, attention heads = 12, and feed-forward = 3,072d. We train IndoBERT with 31,923 Word-Pieces (vocabulary) for 2 million steps. 

 Extractive Model After the document is processed by BERT, we have a contextualized embedding for every word token in the document. To learn inter-sentential relationships, we use the [CLS] embeddings ([x S 1 , x S 2 , .., x Sm ]) to represent the sentences, to which we add a sentence-level positional embedding (P ), and feed them to a transformer encoder (Figure  3 ). An MLP layer with sigmoid activation is applied to the output of the transformer encoder to predict whether a sentence should be extracted (i.e. ?S ? {0, 1}). We train the model with binary 9 https://huggingface.co/ mBERT / IndoBERT [CLS] Sent 1 [SEP] [CLS] Sent 2 [SEP] [CLS] Sent m [SEP] [PAD] T A T A T A T B T B T B T A T A T A T P P 2 P 1 x S1 x S2 x Sm  The transformer encoder is configured as follows: layers = 2, hidden size = 768, feed-forward = 2,048, and heads = 8. In terms of training hyperparameters, we train using the Adam optimizer with learning rate lr = 2e ?3 ? min(step ?0.5 , step ? warmup ?1.5 ) where warmup = 10, 000. We train for 50,000 steps on 3?V100 16GB GPUs, and perform evaluation on the development set every 2,500 steps. At test time, we select sentences for the extractive summary according to two conditions: the summary must consist of: (a) at least two sentences, and (b) at least 15 words. These values were set based on the average number of sentences and the minimum number of words in a summary. We also apply trigram blocking to reduce redundancy  (Paulus et al., 2018) . Henceforth, we refer to this model as "BERTEXT". 

 Abstractive Model Similar to the extractive model, we have a second transformer to process the contextualized embeddings from BERT. In this case, we use a transformer decoder instead (i.e. an attention mask is used to prevent the decoder from attending to future time steps), as we are learning to generate an abstractive summary. But unlike the extractive model, we use the BERT embeddings for all tokens as input to the transformer decoder (as we do not need sentence representations). We add to these BERT embeddings a second positional encoding before feeding them to the transformer decoder (Figure  3 ). The transformer decoder is initialized with random parameters (i.e. no pre-training). The transformer decoder is configured as follows: layers = 6, hidden size = 768, feed-forward = 2,048, and heads = 8. Following Liu and Lapata (2019), we use a different learning rate for BERT and the decoder when training the model: lr = 2e ?3 ? min(step ?0.5 , step ? 20, 000 ?1.5 ) and 0.1 ? min(step ?0.5 , step ? 10, 000 ?1.5 ) for BERT and the transformer decoder, respectively. Both networks are trained with the Adam optimizer for 200,000 steps on 4?V100 16GB GPUs and evaluated every 10,000 steps. For summary generation, we use beam width = 5, trigram blocking, and a length penalty  (Wu et al., 2016)  to generate at least two sentences and at least 15 words (similar to the extractive model). Henceforth the abstractive model will be referred to as "BERTABS". We additionally experiment with a third variant, "BERTEXTABS", where we use the weights of the fine-tuned BERT in BERTEXT for the encoder (instead of off-the-shelf BERT weights). 

 Experiment and Results We use three ROUGE  (Lin, 2004 ) F-1 scores as evaluation metrics: R1 (unigram overlap), R2 (bigram overlap), and RL (longest common subsequence overlap). In addition, we also provide BERTSCORE (F-1), as has recently been used for machine translation evaluation  (Zhang et al., 2020b) .  10  We use the development set to select the best checkpoint during training, and report the evaluation scores for the canonical and Xtreme test sets in Table  4 . For both test sets, the summarization models are trained using the same training set, but they are tuned with a different development set (see Section 2 for details). In addition to the BERT models, we also include two pointergenerator models  (See et al., 2017) : (1) the base model (PTGEN); and (2) the model with coverage penalty (PTGEN+COV).  11  We first look at the baseline LEAD-N and ORA-CLE results. LEAD-2 is the best LEAD-N baseline for Liputan6. This is unsurprising, given that in Table  2 , the average summary length was 2 sentences. We also notice there is a substantial gap between ORACLE and LEAD-2: 12-15 points for R1 and 5-7 points for BERTSCORE, depending on the test set. This suggests that the baseline of using the first few sentences as an extractive summary is ineffective. Comparing the performance between the canonical and Xtreme test sets, we see a substantial drop in performance for both LEAD-N and ORACLE, highlighting the difficulty of the Xtreme test set due to its increased abstractiveness. For the pointer-generator models, we see little improvement when including the coverage mechanism (PTGEN+COV vs. PTGEN), implying that there is minimal repetition in the output of PTGEN. We suspect this is due to the Liputan6 summaries being relatively short (2 sentences with 30 words on average). A similar observation is reported by  Narayan et al. (2018)  for XSum, where the summaries are similarly short (a single sentence with 23 words, on average). Next we look at the BERT models. Overall they perform very well, with both the mBERT and In-doBERT models outperforming the LEAD-N baselines and PTGEN models by a comfortable margin. IndoBERT is better than mBERT (approximately 1 ROUGE point better on average over most metrics), showing that a monolingually-trained BERT is a more effective pre-trained model than the multilingual variant. The best performance is achieved by IndoBERT's BERTEXTABS. In the canonical test set, the improvement over LEAD-2 is +4.4 R1, +2.62 R2, +4.3 R3, and +3.4 BERTSCORE points. In the Xtreme test set, BERTEXTABS suffers a substantial drop compared to the canonical test set (6-7 ROUGE and 2 BERTSCORE points), although the performance gap between it and LEAD-2 is about the same. 

 Model Canonical  

 Error Analysis In this section, we analyze errors made by the extractive (BERTEXT) abstractive (BERTEXTABS) models to better understand their behaviour. We use the mBERT version of these models in our analysis. 12 

 Error Analysis of Extractive Summaries We hypothesized that the disparity between ORA-CLE and BERTEXT (14.03 point difference for R1 in the canonical test set) was due to the number of extracted sentences. To test this, when extracting sentences with BERTEXT, we set the total number of extracted sentences to be the same as the number of sentences in the ORACLE summary. However, we found minimal benefit using this approach, suggesting that the disparity is not a result of the number of extracted sentences. To investigate this further, we present the frequency of sentence positions that are used in the summary in ORACLE and BERTEXT for the canonical test set in Figure  4a . We can see that BERTEXT tends to over-select the first two sentences as the summary. In terms of proportion, 65.47% of BERTEXT summaries involve the first two sentences. In comparison, only 42.54% of ORACLE summaries use sentences in these positions. One may argue that this is because the training and test data have different distributions under our chronological partitioning strategy (recall that the test set is sampled from the earliest articles), but that does not appear to be the case: as Figure  4b  shows, the distribution of sentence positions in the training data is very similar to the test data -43.14% of ORACLE summaries involve the first two sentences. 

 Error Analysis of Abstractive Summaries To perform error analysis for BERTEXTABS, we randomly sample 100 documents with an R1 score <0.4 in the canonical test set (which accounts for nearly 50% of the test documents). Two native Indonesian speakers examined these 100 samples to manually assess the quality of the summaries, and score them on a 3-point ordinal scale: (1) bad; (2) average; and (3) good. Each annotator is presented with the source document, the reference summary, and the summary generated by BERTEXTABS. In addition to the overall quality evaluation, we also asked the annotators to analyze a number of (finegrained) attributes in the summaries: ? Abbreviations: the system summary uses abbreviations that are different to the reference summary.  ? Morphology: the system summary uses morphological variants of the same lemmas contained in the reference summary. ? Synonyms/paraphrasing: the system summary contains paraphrases of the reference summary. ? Lack of coverage: the system summary lacks coverage of certain details that are present in the reference summary. ? Wrong focus: the system summarizes a different aspect/focus of the document to the reference summary. ? Unnecessary details (from document): the system summary includes unimportant but factually correct information. ? Unnecessary details (not from document): the system summary includes unimportant and factually incorrect information (hallucinations). We present a breakdown of the different error types in Table  5 . Inter-annotator agreement for the overall quality assessment is high (Pearson's r = 0.69). Disagreements in the quality label (bad, average, good) are resolved as follows: (1) {bad, average} ? bad; and (2) {good, average} ? good. We only have four examples with {bad, good} disagreement, which we resolved through discussion. Interestingly, more than half (60) of our samples were found to have good summaries. The primary reasons why these summaries have low ROUGE scores are paraphrasing (86.7%), and the inclusion of additional (but valid) details (75.0%). Abbreviations and morphological differences also appear to be important factors. These results underline a problem with the ROUGE metric, in that it is unable to detect good summaries that use a different set of words to the reference summary. One way forward is to explore metrics that consider sentence semantics beyond word overlap such as METEOR  (Banerjee and Lavie, 2005)  and BERTSCORE, 13 and question-answering system based evaluation such as APES  (Eyal et al., 2019)  and QAGS  (Wang et al., 2020) . Another way is to create more reference summaries (which will help with the issue of the system summaries including [validly] different details to the single reference). Looking at the results for average summaries (middle column), BERTEXTABS occasionally fails to capture salient information: 100% of the summaries have coverage issues, and 75.0% contain unnecessary (but valid) details. They also tend to use paraphrases (87.5%), which further impacts on a lower ROUGE score. Finally, the bad system summaries have similar coverage issues, and also tend to have a very different focus compared to the Dokumen: Liputan6.com , Jakarta : Langkah reshuffle yang dilakukan Presiden Abdurrahman Wahid , agaknya tak mendapat restu . Buktinya , Wakil Presiden Megawati Sukarnoputri kembali tidak hadir dalam pelantikan tiga menteri bidang ekonomi , Rabu ( 13/6 ) . [8 kalimat dengan 113 kata setelahnya tidak ditampilkan] Ringkasan manusia: wapres megawati sukarnoputri , kembali tidak hadir dalam pelantikan tiga menteri baru . dalam reshufle 1 juni , megawati juga tak muncul dalam pelantikan , karena merasa tak dilibatkan dalam reshuffle kabinet . 

 Ringkasan sistem [Bad]: presiden abdurrahman wahid kembali tidak hadir dalam pelantikan tiga menteri bidang ekonomi . ketidaksepakatan soal perombakan kabinet itu juga terjadi 1 juni silam . presiden meminta mereka lebih menjaga koordinasi antarmenteri . Example-2 of error analysis (Lack of coverage, wrong focus, and details that are not from the document) Document: Liputan6.com, Jakarta: The reshuffle step was taken by President Abdurrahman Wahid, apparently did not get the blessing. The proof, Vice President Megawati Sukarnoputri was again not present at the inauguration of three ministers in the economic sector, Wednesday (6/13). [8 sentences with 113 words are abbreviated from here] Gold Summary: Vice President Megawati Sukarnoputri, is not present at the inauguration of three new ministers again. In the reshuffle on June 1, Megawati also did not appear in the inauguration, because she felt not involved in the cabinet reshuffle. System Summary [Bad]: President Abdurrahman Wahid was again absent from the inauguration of three ministers in the economic sector. disagreement about the cabinet reshuffle also occurred 1 June ago. the president asked them to maintain more coordination between ministries. 

 Dokumen: Liputan6.com , Jakarta : Protes masih bergema menyambut Keputusan Menteri Tenaga Kerja dan Transmigrasi Nomor 78 Tahun 2001 . Kebijakan yang sengaja dikeluarkan sebagai wujud perubahan keputusan sebelumnya ini , sampai sekarang , masih mengundang kecaman keras dari pekerja di Indonesia . Itulah sebabnya , mereka menuntut Kepmenakertrans baru ini dicabut karena dinilai merugikan pekerja . [19 kalimat dengan 406 kata tidak ditampilkan] Sementara itu , SPSI secara tegas menolak segala bentuk negosiasi . [3 kalimat dengan 45 kata setelahnya tidak ditampilkan] Ringkasan manusia: pemberlakuan kepmenakertrans 78/2001 masih mengundang rasa tidak puas di dada sejumlah pekerja indonesia . maka , lahirlah tuntutan agar peraturan yang dinilai merugikan ini dicabut . 

 Ringkasan sistem [Good]: keputusan menteri tenaga kerja dan transmigrasi nomor 78 tahun 2001 mengundang kecaman keras dari pekerja di indonesia . mereka menuntut kepmenakertrans dicabut karena dinilai merugikan pekerja . spsi menolak negosiasi . Example-1 of error analysis (Abbreviation, morphoplogy, synonyms/paraphrashing, and details from the document)  reference summary (90.6%). In Figure  5  we show two representative examples from BERTEXTABS. The first example is considered good by our annotators, but due to abbreviations, morphological differences, paraphrasing, and additional details compared to the reference summary, the ROUGE score is <0.4. In this example, the gold summary uses the abbreviation kepmenakertrans while BERTEXTABS generates the full phrase keputusan menteri tenaga kerja dan transmigrasi (which is correct). The example also uses paraphrases (invites strong criticism to explain dissatisfaction), and there are morphological differences in words such as tuntutan (noun) vs. menuntut (verb). The low ROUGE score here highlights the fact that the bigger issue is with ROUGE itself rather than the summary. The second example is considered to be bad, with the following issues: lack of coverage, wrong focus, and contains unnecessary details that are not from the article. The first sentence President Abdurrahman Wahid was absent has nothing to do with the original article, creating a different focus (and confusion) in the overall summary. To summarize, coverage, focus, and the inclusion of other details are the main causes of low quality summaries. Our analysis reveals that abbreviations and paraphrases are another cause of summaries with low ROUGE scores, but that is an issue with ROUGE rather than the summaries. Encouragingly, hallucination (generating details not in the original document) is not a major issue for these models (notwithstanding that almost 20% of bad samples contain hallucinations). 

 Related Datasets Previous studies on Indonesian text summarization have largely been extractive and used small-scale datasets.  Gunawan et al. (2017)  developed an unsupervised summarization model over 3K news articles using heuristics such as sentence length, keyword frequency, and title features. In a similar vein,  Najibullah (2015)  trained a naive Bayes model to extract summary sentences in a 100-article dataset.  Aristoteles et al. (2012)  and  Silvia et al. (2014)  apply genetic algorithms to a summarization dataset with less than 200 articles. These studies do not use ROUGE for evaluation, and the datasets are not publicly available. Koto (2016) released a dataset for chat summarization by manually annotating chat logs from WhatsApp. 14 However, this dataset contains only 300 documents. The largest summarization data to date is IndoSum  (Kurniawan and Louvan, 2018) , which has approximately 19K news articles with manually-written summaries. Based on our analysis, however, the summaries of IndoSum are highly extractive. Beyond Indonesian, there is only a handful of non-English summarization datasets that are of sufficient size to train modern deep learning summarization methods over, including: (1) LCSTS  (Hu et al., 2015) , which contains 2 million Chinese short texts constructed from the Sina Weibo microblogging website; and (2) ES-News  (Gonzalez et al., 2019) , which comprises 270k Spanish news articles with summaries. LCSTS documents are relatively short (less than 140 Chinese characters), while ES-News is not publicly available. Our goal is to create a benchmark corpus for Indonesian text summarization that is both large scale and publicly available. 

 Conclusion We release Liputan6, a large-scale summarization corpus for Indonesian. Our dataset comes with two test sets: a canonical test set and an "Xtreme" variant that is more abstractive. We present results for several benchmark summarization models, in part based on IndoBERT, a new pre-trained BERT model for Indonesian. We further conducted extensive error analysis, as part of which we identified a number of issues with ROUGE-based evaluation for Indonesian. Figure 2 : 2 Figure 2: Proportion of novel n-grams over time in the summaries. 
