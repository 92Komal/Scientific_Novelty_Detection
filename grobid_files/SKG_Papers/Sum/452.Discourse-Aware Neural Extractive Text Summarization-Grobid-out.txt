title
Discourse-Aware Neural Extractive Text Summarization

abstract
Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model -DISCOBERT 1 . DISCOBERT extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity. To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks. Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.

Introduction Neural networks have achieved great success in the task of text summarization  (Nenkova et al., 2011; Yao et al., 2017) . There are two main lines of research: abstractive and extractive. While the abstractive paradigm  (Rush et al., 2015; See et al., 2017; Celikyilmaz et al., 2018; Sharma et al., 2019)  focuses on generating a summary word-by-word after encoding the full document, the extractive approach  (Cheng and Lapata, 2016; Narayan et al., 2018)  directly selects sentences from the document to assemble into a summary. The abstractive approach is more flexible and generally produces less redundant summaries, while the extractive approach enjoys better factuality and efficiency  (Cao et al., 2018) . Recently, some hybrid methods have been proposed to take advantage of both, by designing a two-stage pipeline to first select and then rewrite (or compress) candidate sentences  (Chen and Bansal, 2018; Gehrmann et al., 2018; Zhang et al., 2018; Xu and Durrett, 2019) . Compression or rewriting aims to discard uninformative phrases in the selected sentences. However, most of these hybrid systems suffer from the inevitable disconnection between the two stages in the pipeline. Meanwhile, modeling long-range context for document summarization remains a challenge  (Xu et al., 2016) . Pre-trained language models  (Devlin et al., 2019)  are designed mostly for sentences or a short paragraph, thus poor at capturing longrange dependencies throughout a document. Empirical observations  (Liu and Lapata, 2019)  show that adding standard encoders such as LSTM or Transformer  (Vaswani et al., 2017)  on top of BERT to model inter-sentential relations does not bring in much performance gain. In this paper, we present DISCOBERT, a discourse-aware neural extractive summarization model built upon BERT. To perform compression with extraction simultaneously and reduce redundancy across sentences, we take Elementary Discourse Unit (EDU), a sub-sentence phrase unit originating from  RST (Mann and Thompson, 1988; Carlson et al., 2001)  2 as the minimal selection unit (instead of sentence) for extractive summarization. Figure  1  shows an example of discourse segmentation, with sentences broken down into EDUs (annotated with brackets). By operating on the discourse unit level, our model can discard redundant details in sub-sentences, therefore retaining additional capacity to include more concepts or events, leading to more concise and informative summaries. Furthermore, we finetune the representations of discourse units with the injection of prior knowledge to leverage intra-sentence discourse relations. More specifically, two discourse-oriented graphs are proposed: RST Graph G R and Coreference Graph G C . Over these discourse graphs, Graph Convolutional Network (GCN) (Kipf and Welling, 2017) is imposed to capture long-range interactions among EDUs. RST Graph is constructed from RST parse trees over EDUs of the document. On the other hand, Coreference Graph connects entities and their coreference clusters/mentions across the document. The path of coreference navigates the model from the core event to other occurrences of that event, and in parallel explores its interactions with other concepts or events. The main contribution is threefold: (i) We propose a discourse-aware extractive summarization model, DISCOBERT, which operates on a subsentential discourse unit level to generate concise and informative summary with low redundancy. (ii) We propose to structurally model 2 We adopt RST as the discourse framework due to the availability of existing tools, the nature of the RST tree structure for compression, and the observations from  Louis et al. (2010) . Other alternatives includes Graph Bank  (Wolf and Gibson, 2005)  and PDTB  (Miltsakaki et al., 2004) .    2  shows an example of discourse segmentation and the parse tree of a sentence. Among these EDUs, rhetorical relations represent the functions of different discourse units. As observed in  Louis et al. (2010) , the RST tree structure already serves as a strong indicator for content selection. On the other hand, the agreement between rhetorical relations tends to be lower and more ambiguous. Thus, we do not encode rhetorical relations explicitly in our model. In content selection for text summarization, we expect the model to select the most concise and pivotal concept in the document, with low redundancy.  3  However, in traditional extractive summarization methods, the model is required to select a whole sentence, even though some parts of the sentence are not necessary. Our proposed approach can select one or several fine-grained EDUs to render the generated summaries less redundant. This serves as the foundation of our DISCOBERT model. 

 RST Graph When selecting sentences as candidates for extractive summarization, we assume each sentence is grammatically self-contained. But for EDUs, some restrictions need to be considered to ensure grammaticality. For example, Figure  2  illustrates an RST discourse parse tree of a sentence, where "[2] This iconic ... series" is a grammatical sentence but "[3] and shows ... 8" is not. We need to understand the dependencies between EDUs to ensure the grammaticality of the selected combinations. The detail of the derivation of the dependencies could be found in Sec 4.3. The construction of the RST Graph aims to provide not only local paragraph-level but also longrange document-level connections among EDUs. We use the converted dependency version of the tree to build the RST Graph G R , by initializing an empty graph and treating every discourse dependency from the i-th EDU to the j-th EDU as a directed edge, i.e., G R [i][j] = 1. 

 Coreference Graph Text summarization, especially news summarization, usually suffers from the well-known 'position bias' issue  (Kedzie et al., 2018) , where most of the key information is described at the very beginning Algorithm 1 Construction of the Coreference Graph GC . Require: Coreference clusters C = {C1, C2, ? ? ? , Cn}; mentions for each cluster Ci = {Ei1, ? ? ? , Eim}. Initialize the Graph GC without any edge GC [ * ][ * ] = 0. for i = 0 to n do Collect the location of all occurences {Ei1, ? ? ? , Eim} to L = {l1, ? ? ? , lm}. for j = 1 to m, k = 1 to m do GC [j][k] = 1 end for end for return Constructed Graph GC . of the document. However, there is still a decent amount of information spread in the middle or at the end of the document, which is often ignored by summarization models. We observe that around 25% of oracle sentences appear after the first 10 sentences in the CNNDM dataset. Besides, in long news articles, there are often multiple core characters and events throughout the whole document. However, existing neural models are poor at modeling such long-range context, especially when there are multiple ambiguous coreferences to resolve. To encourage and guide the model to capture the long-range context in the document, we propose a Coreference Graph built upon discourse units. Algorithm 1 describes how to construct the Coreference Graph. We first use Stanford CoreNLP  (Manning et al., 2014)  to detect all the coreference clusters in an article. For each coreference cluster, all the discourse units containing the mention of the same cluster will be connected. This process is iterated over all the coreference mention clusters to create the final Coreference Graph. Figure  1  provides an example, where 'Pulitzer prizes' is an important entity and has occurred multiple times in multiple discourse units. The constructed Coreference Graph is shown on the right side of the document 4 . When graph G C is constructed, edges among 1-1, 2-1, 20-1 and 22-1 are all connected due to the mentions of 'Pulitzer prizes'.  whole document on the token level. Then, a selfattentive span extractor is used to obtain the EDU representations from the corresponding text spans. The Graph Encoder takes the output of the Document Encoder as input and updates the EDU representations with Graph Convolutional Network based on the constructed discourse graphs, which are then used to predict the oracle labels. 

 DISCOBERT Model w 18 w 21 w 28 w 31 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? BERT SpanExt SpanExt SpanExt SpanExt h B 10 h B 11 h B 12 h B 18 h B 21 h B 28 h B 2* h B 30 h B 31 h B 32 h B 33 w 32 w 33 h B 3|18 w 3|18 w 41 h B 41 h B 4|14 w 4|14 h B 4* h B 50 h S 2 h S 3 h S 4 Stacked Discourse Graph Encoders h G 1 h G 2 h G 3 h G 4 MLP MLP MLP MLP ?2 ?3 ?4 ?1 1 0 1 0 Prediction ? ? ? DGE (k) FFNN h (k) LN Dropout Dropout LN h (k+1) 

 Graph Assume that document D is segmented into n EDUs in total, i.e., D = {d 1 , d 2 , ? ? ? , d n }, where d i denotes the i-th EDU. Following  Liu and Lapata (2019) , we formulate extractive summarization as a sequential labeling task, where each EDU d i is scored by neural networks, and decisions are made based on the scores of all EDUs. The oracle labels are a sequence of binary labels, where 1 stands for being selected and 0 for not. We denote the labels as Y = {y * 1 , y * 2 , ? ? ? , y * n }. During training, we aim to predict the sequence of labels Y given the document D. During inference, we need to further consider discourse dependency to ensure the coherence and grammaticality of the output summary. 

 Document Encoder BERT is a pre-trained deep bidirectional Transformer encoder  (Vaswani et al., 2017; Devlin et al., 2019) . Following  Liu and Lapata (2019) , we encode the whole document with BERT and finetune the BERT model for summarization. BERT is originally trained to encode a single sentence or sentence pair. However, a news article typically contains more than 500 words, hence we need to make some adaptation to apply BERT for document encoding. Specifically, we insert CLS and SEP tokens at the beginning and the end of each sentence, respectively.  5  In order to encode long documents such as news articles, we also extend the maximum sequence length that BERT can take from 512 to 768 in all our experiments. The input document after tokenization is denoted as D = {d 1 , ? ? ? , d n }, and d i = {w i1 , ? ? ? , w i i }, where i is the number of BPE tokens in the i-th EDU. If d i is the first EDU in a sentence, there is also a CLS token prepended to d i ; if d j is the last EDU in a sentence, there is a SEP token appended to d j (see Figure  3 ). The schema of insertion of CLS and SEP is an approach used in  Liu and Lapata (2019) . For simplicity, these two tokens are not shown in the equations. BERT model is then used to encode the document: {h B 11 , ? ? ? , h B n n } = BERT({w 11 , ? ? ? , w n n }) , where {h B 11 , ? ? ? , h B n n } is the BERT output of the whole document in the same length as the input. After the BERT encoder, the representation of the CLS token can be used as sentence representation. However, this approach does not work in our setting, since we need to extract the representation for EDUs instead. Therefore, we adopt a Self-Attentive Span Extractor (SpanExt), proposed in  Lee et al. (2017) , to learn EDU representation. For the i-th EDU with i words, with the output from the BERT encoder {h B i1 , h B i2 , ? ? ? , h B i i } , we obtain EDU representation as follows: ? ij = W 2 ? ReLU(W 1 h B ij + b 1 ) + b 2 a ij = exp(? ij ) i k=1 exp(? ik ) , h S i = i j=1 a ij ? h B ij , where ? ij is the score of the j-th word in the EDU, a ij is the normalized attention of the j-th word w.r.t. all the words in the span. h S i is a weighted sum of the BERT output hidden states. Throughout the paper, all the W matrices and b vectors are parameters to learn. We abstract the above Self-Attentive Span Extractor as h S i = SpanExt(h B i1 , ? ? ? , h B i i ). After the span extraction step, the whole document is represented as a sequence of EDU representations: h S = {h S 1 , ? ? ? , h S n } ? R d h ?n , which will be sent to the graph encoder. 

 Graph Encoder Given the constructed graph G = (V, E), nodes V correspond to the EDUs in a document, and edges E correspond to either RST discourse relations or coreference mentions. We then use Graph Convolutional Network to update the representations of all the EDUs, to capture long-range dependencies missed by BERT for better summarization. To modularize architecture design, we present a single Discourse Graph Encoder (DGE) layer. Multiple DGE layers are stacked in our experiments. Assume that the input for the k-th DGE layer is denoted as h (k) = {h (k) 1 , . . . , h (k) n } ? R d h ?n , and the corresponding output is denoted as h (k+1) = {h (k+1) 1 , . . . , h (k+1) n } ? R d h ?n . The k-th DGE layer is designed as follows: u (k) i = W (k) 4 ReLU(W (k) 3 h (k) i + b (k) 3 ) + b (k) 4 v (k) i = LN(h (k) i + Dropout(u (k) i )) w (k) i = ReLU j?N i 1 |N i | W (k) 5 v (k) j + b (k) 5 h (k+1) i = LN(Dropout(w (k) i ) + v (k) i ) , where LN(?) represents Layer Normalization, N i denotes the neighorhood of the i-th EDU node. h (k+1) i is the output of the i-th EDU in the k-th DGE layer, and h (1) = h S , which is the output from the Document Encoder. After K layers of graph propagation, we obtain h G = h (K+1) ? R d h ?n , which is the final representation of all the EDUs after the stacked DGE layers. For different graphs, the parameter of DGEs are not shared. If we use both graphs, their output are concatenated: h G = ReLU(W 6 [h G C ; h G R ] + b 6 ) . 

 Training & Inference During training, h G is used for predicting the oracle labels. Specifically, ?i = ?(W 7 h G i + b 7 ) where ?(?) represents the logistic function, and ?i is the prediction probability ranging from 0 to 1. The training loss of the model is binary cross-entropy loss given the predictions and oracles: L = ? n i=1 (y * i log(? i ) + (1 ? y * i ) log(1 ? ?i )) . For DISCOBERT without graphs, the output from Document Encoder h S is used for prediction instead. The creation of oracle is operated on EDU level. We greedily pick up EDUs with their necessary dependencies until R-1 F 1 drops. During inference, given an input document, after obtaining the prediction probabilities of all the EDUs, i.e., ? = {? 1 , ? ? ? , ?n }, we sort ? in descending order, and select EDUs accordingly. Note that the dependencies between EDUs are also enforced in prediction to ensure grammacality of generated summaries. 

 Experiments In this section, we present experimental results on two popular news summarization datasets. We compare our proposed model with state-of-the-art baselines and conduct detailed analysis to validate the effectiveness of DISCOBERT. 

 Datasets We evaluate the models on two datasets: New York Times (NYT)  (Sandhaus, 2008) , CNN and Dailymail (CNNDM)  (Hermann et al., 2015) . We use the script from  See et al. (2017)  to extract summaries from raw data, and Stanford CoreNLP for sentence boundary detection, tokenization and parsing  (Manning et al., 2014) . Due to the limitation of BERT, we only encode up to 768 BERT BPEs. Table  1  provides statistics of the datasets. The edges in G C are undirected, while those in G R are directional. For CNNDM, there are  287,226, 13,368 and 11,490  samples for training, validation and test, respectively. We use the un-anonymized version as in previous summarization work. NYT is licensed by LDC 6 . Following previous work  Xu and Durrett, 2019) , we use  137,778, 17,222 and 17,223  samples for training, validation, and test, respectively. 

 State-of-the-art Baselines We compare our model with the following state-ofthe-art neural text summarization models. Extractive Models: BanditSum treats extractive summarization as a contextual bandit problem, trained with policy gradient methods  (Dong et al., 2018) . NeuSum is an extractive model with seq2seq architecture, where the attention mechanism scores the document and emits the index as the selection . Compressive Models: JECS is a neural textcompression-based summarization model using BLSTM as the encoder  (Xu and Durrett, 2019) . The first stage is selecting sentences, and the second stage is sentence compression by pruning constituency parsing tree. BERT-based Models: BERT-based models have achieved significant improvement on CNNDM and NYT, when compared with LSTM counterparts. BertSum is the first BERT-based extractive summarization model  (Liu and Lapata, 2019) . Our baseline model BERT is the re-implementation of BertSum. PNBert proposed a BERT-based model with various training strategies, including reinforcement learning and Pointer Networks  (Zhong et al., 2019) . HiBert is a hierarchical BERT-based model for document encoding, which is further pretrained with unlabeled data . 

 Implementation Details We use AllenNLP  (Gardner et al., 2018)   NeuSum  41.59 19.01 37.98 BanditSum  (Dong et al., 2018)  41.50 18.70 37.60 JECS  (Xu and Durrett, 2019)  41.70 18.50 37.90 PNBERT  (Zhong et al., 2019)  42 encoding is based on DGL . Experiments are conducted on a single NVIDIA P100 card, and the mini-batch size is set to 6 due to GPU memory capacity. The length of each document is truncated to 768 BPEs. We use the pre-trained 'bert-base-uncased' model and fine tune it for all experiments. We train all our models for up to 80,000 steps. ROUGE  (Lin, 2004 ) is used as the evaluation metrics, and 'R-2' is used as the validation criteria. The realization of discourse units and structure is a critical part of EDU pre-processing, which requires two steps: discourse segmentation and RST parsing. In the segmentation phase, we use a neural discourse segmenter based on the BiLSTM CRF framework  (Wang et al., 2018)  7 . The segmenter achieved 94.3 F 1 score on the RST-DT test set, in which the human performance is 98.3. In the parsing phase, we use a shift-reduce discourse parser to extract relations and identify nuclearity  (Ji and Eisenstein, 2014)    8  . The dependencies among EDUs are crucial to the grammaticality of selected EDUs. Here are the two steps to learn the derivation of dependencies: head inheritance and tree conversion. Head inheritance defines the head node for each valid non-terminal tree node. For each leaf node, the head is itself. We determine the head node(s) of non-terminal nodes based on their nuclearity. 9 For example, in Figure  2 , the heads of text spans [1-5], [2-5],  [3-5] and [4-5]  need to be grounded to a single EDU. We propose a simple yet effective schema to convert RST discourse tree to a dependencybased discourse tree.  10  We always consider the dependency restriction such as the reliance of Satellite on Nucleus, when we create oracle during preprocessing and when the model makes the prediction. For the example in Figure  2 , if the model selects "[5] being carried ... Liberia." as a candidate span, we will enforce the model to select "[3] and shows ... 8," and "[2] This ... series," as well. The number of chosen EDUs depends on the average length of the reference summaries, dependencies across EDUs as mentioned above, and the length of the existing content. The optimal average number of EDUs selected is tuned on the development set. 

 Experimental Results Results on CNNDM Table  2 shows   The proposed DISCOBERT beats the sentencebased counterpart and all the competitor models. With the help of Discourse Graph Encoder, the graph-based DISCOBERT beats the stateof-the-art BERT model by a significant margin (0.52/0.61/1.04 on R-1/-2/-L on F 1 ). Ablation study with individual graphs shows that the RST Graph is slightly more helpful than the Coreference 9 If both children are N(ucleus), then the head of the current node inherits the head of the left child. Otherwise, when one child is N and the other is S, the head of the current node inherits the head of the N child. 10 If one child node is N and the other is S, the head of the S node depends on the head of the N node. If both children are N and the right child does not contain a subject in the discourse, the head of the right N node depends on the head of the left N node.  Graph, while the combination of both achieves better performance overall. Results on NYT Results are summarized in Table 3. The proposed model surpasses previous state-of-the-art BERT-based model by a significant margin. HIBERT * S and HIBERT * M used extra data for pre-training the model. We notice that in the NYT dataset, most of the improvement comes from the use of EDUs as minimal selection units. DIS-COBERT provides 1.30/1.29/1.82 gain on R-1/-2/-L over the BERT baseline. However, the use of discourse graphs does not help much in this case. 

 Grammaticality Due to segmentation and partial selection of sentence, the output of our model might not be as grammatical as the original sentence. We manually examined and automatically evaluated model output, and observed that overall, the generated summaries are still grammatical, given the RST dependency tree constraining the rhetorical relations among EDUs. A set of simple yet effective post-processing rules helps to complete the EDUs in some cases. Automatic Grammar Checking We followed  Xu and Durrett (2019)  to perform automatic grammar checking using Grammarly. Table  4  shows the grammar checking results, where the average number of errors in every 10,000 characters on CN-NDM and NYT datasets is reported. We compare DISCOBERT with sentence-based BERT model. 'All' shows the summation of the number of errors in all categories. As shown in the  summaries generated by our model have retained the quality of the original text. Human Evaluation We sampled 200 documents from the test set of CNNDM and for each sample, we asked two Turkers to grade three summaries from 1 to 5. Results are shown in Table  5 . Sent-BERT model (the original BERTSum model) selects sentences from the document, hence providing the best overall readability, coherence, and grammaticality. In some cases, reference summaries are just long phrases, so the scores are slightly lower than those from the sentence model. DISCOBERT model is slightly worse than Sent-BERT model but is fully comparable to the other two variants. Examples & Analysis We show some examples of model output in Table  6 . We notice that a decent amount of irrelevant details are removed from the extracted summary. Despite the success, we further conducted error analysis and found that the errors mostly originated from the RST dependency resolution and the upstream parsing error of the discourse parser. The misclassification of RST dependencies and the hand-crafted rules for dependency resolution hurted the grammaticality and coherence of the 'generated' outputs. Common punctuation issues include extra or missing commas, as well as missing quotation marks. Some of the coherence issue Clare Hines , who lives in Brisbane, was diagnosed with a brain tumour after suffering epileptic seizures. After a number of tests doctors discovered she had a benign tumour that had wrapped itself around her acoustic, facial and balance nerve -and told her she had have it surgically removed or she risked the tumour turning malignant. One week before brain surgery she found out she was pregnant. Jordan Henderson, in action against Aston Villa at Wembley on Sunday, has agreed a new Liverpool deal. The club's vice captain puts pen to paper on a deal which will keep him at Liverpool until 2020. Rodgers will consider Henderson for the role of club captain after Steven Gerrard moves to LA Galaxy at the end of the campaign but, for now, the England international is delighted to have agreed terms on a contract that will take him through the peak years of his career. originates from missing or improper or missing anaphora resolution. In this example "['Johnny is believed to have drowned,] 1 [but actually he is fine,'] 2 [the police say.] 3 ", only selecting the second EDU yields a sentence "actually he is fine", which is not clear who is 'he' mentioned here. 

 Related Work Neural Extractive Summarization Neural networks have been widely used in extractive summarization. Various decoding approaches, including ranking  (Narayan et al., 2018) , index prediction  and sequential labelling  (Nallapati et al., 2017; Zhang et al., 2018; Dong et al., 2018) , have been applied to content selection. Our model uses a similar configuration to encode the document with BERT as  Liu and Lapata (2019)  did, but we use discourse graph structure and graph encoder to handle the long-range dependency issue. Neural Compressive Summarization Text summarization with compression and deletion has been explored in some recent work.  Xu and Durrett (2019)  presented a two-stage neural model for selection and compression based on constituency tree pruning.  Dong et al. (2019)  presented a neural sentence compression model with discrete operations including deletion and addition. Different from these studies, as we use EDUs as minimal selection basis, sentence compression is achieved automatically in our model. 

 Discourse & Summarization The use of discourse theory for text summarization has been explored before.  Louis et al. (2010)  examined the benefit of graph structure provided by discourse relations for text summarization.  Hirao et al. (2013) ;  Yoshida et al. (2014)  formulated the summarization problem as the trimming of the document discourse tree.  Durrett et al. (2016)  presented a system of sentence extraction and compression with ILP methods using discourse structure.  Li et al. (2016)  demonstrated that using EDUs as units of content selection leads to stronger summarization performance. Compared with them, our proposed method is the first neural end-to-end summarization model using EDUs as the selection basis. Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units  (Erkan and Radev, 2004) .  Yasunaga et al. (2017)  employed a GCN on the relation graphs with sentence embeddings obtained from RNN.  Tan et al. (2017)  also proposed graphbased attention in abstractive summarization model.  Fernandes et al. (2018)  developed a framework to reason long-distance relationships for text summarization. 

 Conclusion In this paper, we present DISCOBERT, which uses discourse unit as the minimal selection basis to reduce summarization redundancy and leverages two types of discourse graphs as inductive bias to capture long-range dependencies among discourse units. We validate the proposed approach on two popular summarization datasets, and observe consistent improvement over baseline models. For future work, we will explore better graph encoding methods, and apply discourse graphs to other tasks that require long document encoding. Figure 1 : 1 Figure 1: Illustration of DISCOBERT for text summarization. Sentence-based BERT model (baseline) selects whole sentences 1, 2 and 5. The proposed discourse-aware model DISCOBERT selects EDUs {1-1, 2-1, 5-2, 20-1, 20-3, 22-1}. The right side of the figure illustrates the two discourse graphs we use: (i) Coref(erence) Graph (with the mentions of 'Pulitzer prizes' highlighted as examples); and (ii) RST Graph (induced by RST discourse trees). 
