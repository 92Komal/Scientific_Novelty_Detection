title


abstract
Rapid growth of multi-modal documents on the Internet makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in text summarization. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images. In addition, our model can generate informative summaries of images.

Introduction Summarizing multi-modal documents to get multi-modal summaries is becoming an urgent need with rapid growth of multi-modal documents on the Internet. Text-Image summarization is to summarize a document with text and images to generate a summary with text and images. The summarization approach is different from pure text summarization. It is also different from image summarization which summarizes an image set to get a subset of images. An image worths thousands of words  (Rossiter, et al., 2012) . Image plays an important role in information transmission. Incorporating images into text to generate text-image Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN Jingqiang Chen 1 , Hai Zhuge 1,2,3,4 1 Nanjing University of Posts and Telecommunications 2 Aston University; 3 Guangzhou University 4 Key Laboratory of Intelligent Information Processing, ICT, University of Chinese Academy of Sciences, Chinese Academy of Sciences cjq@njupt.edu.cn, haizhuge@gmail.com  summaries can help people better understand, memorize, and express information. Most of recent research focuses on pure text summarization, or image summarization. Little has been done on text-image summarization. Figure  1  and Figure  2  show an example of textimage summarization. Figure  1  is the original multi-modal news with text and images. The news has 17 sentences (with 322 words) and 4 images each of which has a caption. Figure  2  is the manually generated multi-modal summary. In the summary, the news is distilled to 3 sentences (with 36 words) and 2 images, and each summary sentence is aligned with an image. To generate such a text-image summary, the following problems should be considered: How to generate the text part? How to measure the importance of images, and extract important images to form the image summary? How to align sentences with images? In this paper, we propose a neural text-image summarization model based on the attentional hierarchical Encoder-Decoder model to solve the above problems. The attentional Encoder-Decoder model has been successfully used in sequence-to-sequence applications such as machine translation  (Luong et al., 2015) , text summarization  (Cheng and Lapata, 2016; Tan et al., 2017) , image captioning  (Liu et al., 2017a) , and machine reading comprehension  (Cui et al., 2016) . At the encoding stage, we use the hierarchical bi-directional RNN to encode the sentences and the text document, use the RNN and the CNN to encode the image set. In the decoding stage, we combine text encoding and image encoding as the initial state, and use the attentional hierarchical decoder which attends original sentences, images and captions to generate the text summary. Each generated sentence is aligned with a sentence, an image, or a caption in the original document. Based on the alignment scores, images are selected and aligned with the generated sentences. In the inference stage, we adopt the multi-modal beam search algorithm which scores beams based on bigram overlaps of the generated sentences and the attended captions. The main contributions are as follows: 1) We propose the text-image summarization task, and extend the standard DailyMail corpora by collecting images and captions of each news from the Web for the task. 

 2) We propose an RNN model to encode the ordered image set of the multi-model document as one of the initial states (the other is the text encoding) of the decoder. 3) We propose three multi-modal attentional mechanisms which attend the text and the images simultaneously when decoding. 4) Experiments show that attending images when decoding can improve text summarization, and that our model can generate informative image summaries. 

 Related Work Recent research on text summarization focuses on neural methods. Attentional Encoder-Decoder model is first proposed in  (Bahdanau et al., 2014)  and  (Luond et al., 2015)  to align the original text and the translated text in machine translation. The attention model is applied to sentence summarization by considering the neural language model and the attention model when generating next words  (Rush et al., 2015) . A selective Encoder-Decoder model that uses a selective gate network to control information from the encoder to the decoder for sentence summarization is proposed  (Zhou et al., 2017) . A neural document summarization model by extracting sentences and words is proposed  (Cheng and Lapata, 2016) . They use a CNN model to encode sentences, and then use a RNN model to encode documents. The model extracts sentences by computing the probability of sentences belonging to the summary based on an RNN model. The model extracts words from the original document based on an attentional decoder. An RNN-based extractive summarization named SummaRuNNer, treating summarization as a sentence classification problem is proposed  (Nallapati et al., 2016) . A logistic classifier is then applied using features computed based on the RNN model. A hierarchical Encoder-Decoder model, conserving the hierarchical structure of documents is proposed  (Li et al., 2015) . A graph-based attentional Encoder-Decoder model using a PageRank algorithm to compute the attention is proposed  (Tan et al., 2017) . Image captioning generates a caption for an image. Text-image summarization is similar to image captioning in that both utilize image information to generate text. Images are encoded with CNN models such as VGGNet  (Simonyan and Zisserman, 2014) , AlexNet  (Krizhevsky et al., 2012)  and GoogleNet  (Szegedy et al., 2014)  by extracting the last full-connected layers. An attentional model is used in image captioning by splitting an image into multiple parts which is attended in the decoding process  (Xu et al., 2015) . Image tags was used as additional information, and semantic attention model which attends image tags when decoding was proposed  (You et al., 2016) . The attention-based alignment of image parts and text is studied  (Liu et al., 2017a) , and the results show that the alignments is in high accordance with manual alignments. An image to an ordered recognized object set is encoded, and the attentional decoder is applied to generate captions  (Liu et al., 2017b) . Multi-modal summarization summarizes text, images, videos, and etc. It is an important branch of automatic summarization. Traditional multimodal summarization inputs multi-modal documents or pure text documents, and outputs multi-modal documents  (Wu, 2011; Greenbacker, 2011; Yan, 2012; Agrawal, 2011; Zhu, 2007; UzZaman, 2011) . For example,  Yan et al., (2012)  generate multi-modal timeline summaries for news sets by constructing a bi-graph between text and images, and apply a heterogeneous reinforcement ranking algorithm. Strategies to summarizing texts with images and the notion of summarization of things are proposed in  (Zhuge, 2016) . The deep learning related work ) treats text summarization as a sentence recommendation task and applies matrix factorization algorithm. They first retrieve images from Yahoo!, use the CNN to extract image features as the additional information of sentences, use Rouge maximization as the training object function which are trained with SGD. In test time, sentences are extracted based on the model and images are retrieved from the Search Engine. 

 Method Figure  3  shows the framework, a multi-modal attentional hierarchical encoder-decoder model. The hierarchical encoder-decoder is proposed in  (Li et al., 2015)  and extended by  (Tan et al. , 2017)  for document summarization through bringing in the graph-based attentional model. Our model consists of three parts: a hierarchical RNN to encode the original sentences and the captions, a CNN+RNN encoder to encode the image set, and a multi-modal attentional hierarchical RNN decoder. The input of our model is a multi-modal document MD = {D, PicSet}, where D is the main text of the multi-modal document and PicSet is the image-caption set ordered by the occurring order of images in the document. 

 Main Text Encoder The main text D consists of sentences, each of which consists of words. Let D=[s 1 , s 2 , ?, s |d| ], and ,1 ,2 , [ , , , ] i i i i i s s x x x ? ? where x i,j is the word embedding of the j th word in the s i . We use word2vec  (Mikolov et al., 2013)  to create word embeddings. GRU is used as the RNN cell  (Cho et al., 2014) . We use a hierarchical RNN encoder to encode the main text D to vector representation. The sentence encoder is adopted to encode sentences to vector representations. An <eos> token is appended to the end of each sentence. A bidirectional RNN is used as the sentence encoder: , , 1 , ( , ) s i j i j i j h GRU h x ? ? ? ? ? (1) , , , 1 ( , ) s i j i j i j h GRU h x ? ? ? ? ? (2) ,1 , 1 [ , ] sent i i i enc h h ? ? ? ? (3) where enc sent i denotes the vector representation of s i . It is the concatenation of ,1 i h ? and , 1 i h ? ? . We use enc sent i as inputs to the document encoder to encode the main text to vector representations. A bi-directional RNN is adopted as the document encoder: 1 ( , ) d sent i i i h GRU h enc ? ? ? ? ? (4) 1 ( , ) d sent i i i h GRU h enc ? ? ? ? ? (5) [ , ] i i i h h h ? ? ? (6) 1 1 [ , ] doc d enc h h? ? ? ? (7) where enc doc denotes the vector representation of the D, and h i is the concatenated hidden state of s i . 

 CaptionSet and ImageSet Encoder The ordered image-caption set PicSet consists of an ordered image set and an ordered caption set which are ordered by the occurring order in the multi-modal document. The image occurring order makes sense because images are often put near the most related sentences, and the sentences have strict order in the document. We treat the ordered caption set as a document, and apply the sentence encoder and the document encoder to the caption document. Then, we get the hidden state h cap i and the vector representation enc cap of the caption document. We use the CNN model to extract the vector representation of each image, and then use the RNN model to encode the ordered image set to vector representation. The CNN model we adopted is 19-layer VGGNet  (Simonyan and Zisserman, 2014) . We drop the last dropout layer and keep the last full-connected layer as the image's vector representation, the dimension of which is 4096. We then use a bi-directional RNN model to encode the ordered image set and the image features are used as inputs of the RNN model. 

 ( , ) img img img fea i i i h GRU h img ? ? ? ? ? (8) 1 ( , ) img img img fea i i i h GRU h img ? ? ? ? ? (9) [ , ] img img img i i i h h h ? ? ? (10) 1 1 [ , ] img img img enc h h ? ? ? ? (11) where img fea i is the vector representation of img i , enc img is the vector representation of the image set, and h img i is the hidden state of img i when encoding the image set. To our best knowledge, we are the first to adopt the RNN model to encode the image set. 

 Decoder In the decoding state, we adopt the hierarchical RNN decoder to generate text summaries. _ 0 _ _ tanh( ) dec doc doc dec img img dec cap cap h W enc V enc V enc ? ? ? ? ? ? (12) ? _ 1 1 1, 1 ( , ) dec sent i i i h GRU h h ? ? ? ? (13) ? _ 2 ( , ) dec sent i i i h GRU h c ? (14) _ , , 1 , 1 ( , ) dec word i j i j i j h GRU h y ? ? ? (15) max , , 

 max( ) Equation (  12 ) to (  16 ) are the equations of the hierarchical decoder which consists of a sentence decoder and a word decoder. soft i j i j y soft W h b ? ? (16) Equation (  12 ) computes the initial state 0 h for the sentence decoder by combining the decoding of the main text information and the decoding of the image information of the multi-modal document. To represent image information, we can use both of the image set decoding and the caption set decoding, or only use one of them, depending on the multi-modal attention mechanism introduced in the next subsection. The sentence decoder uses a two-level hidden output model  (Luong et al., 2015)  to generate the representation of the next sentence through equation (  13 ) and equation (  14 ). The two-level hidden output model consistently improves the summarization performance on different datasets  (Chen et al., 2016) . In equation (  14 ), the two-level model computes i h by capturing a direct interaction between ? i h and i c . ? i h is computed by equation (  13 ) using the preceding sentence decoder hidden state 1 i h ? and the word decoder hidden state 1, 1 i h ? ? which is the last hidden state of the preceding word decoder. And i c is the context of the sentence decoder computed based on the multimodal attention model. The word decoder uses the sentence representation generated by the sentence decoder as the initial state, and use the <sos> (start of sentence) token as the initial input. Equation (15) and equation (  16 ) generate the next hidden state and the next word. The output of the word decoder in the first step is a switch sign which is either <neod> token or <eod> token. The token <neod> means "not end of document", and the token <eod> means "end of document". If the first output is <eod>, the whole decoding process is finished. If the first output is <neod>, the token is used as the next input of the word decoder. The word decoding process is finished when it generates the <eos> token. The last hidden state of the word decoder is treated as the vector representation of the generated sentence and is used as next input of the sentence decoder. 

 Multi-Modal Attention We propose three multi-modal attention mechanisms to compute the sentence decoding context i c . Traditional attention mechanisms for text summarization computes the importance score of the sentence s j in the original document based on the relationship between the decoding hidden state ? i h and the original sentence encoding hidden state j h . We call the traditional attention model as Text Attention (attT for short), which is computed by equation (  17 ), (  18 ) and (  19 ): ? ? ( , ) tanh( ) T T T T i i j j att h h v W h U h ? ? (17) ? ? ? | | 1 exp( ( , )) ( , ) exp( ( , )) T i j T i j D T i j j att h h h h att h h ? ? ? ? (18) ? ? | | 1 ( ) ( , ) D T T i i j j j c h h h h ? ? ? ? (19) where ? ( , ) T i j att h h is the attention (Banahama et al., 2014), ? ( , ) T i j h h ? is the normalized attention, and ? ( ) T i c h is the context. The problem is that the multi-modal document has images and captions besides the main text. Therefore, we propose three multi-modal attention mechanisms which take images and captions into consideration. Text-Caption Attention (attTC for short). This attention model uses captions to represent the image information. attTC computes the attention score of the caption cap j based on the relationship between the caption encoding hidden state h cap i and the decoding hidden state ? j h .  ) ) ? ? ? ? | | | | 1 1 exp( ( , )) ( , ) exp( ( , )) exp( ( , )) TC i j TC i j D PicSet TC TC cap i i j j j j att h h h h att h h att h h ? ? ? ? ? ? ? (20) ? ? ? ? | | | | 1 1 exp( ( , )) ( , ) exp( ( , )) exp( ( , TC cap i j TC cap i j D PicSet TC TC cap i i j j j j att h h h h att h h att h h ? ? ? ? ? ? ? (21) ? ? ? | | | | 1 1 ( ) ( , ) ( , ) D PicSet TC TC TC cap cap i i i j j j j j j c h h h h h h h ? ? ? ? ? ? ? ? (22) Text-Image Attention (attTI for short). This attention model only uses images to represent the image information neglecting the captions. attTI computes the importance score of the image img j based on the relationship between the image encoding hidden state h img i and the decoding hidden state ? j h . ? ? ? ? | | | | 1 1 exp( ( , )) ( , ) exp( ( , )) exp( ( , )) TI img i j TI img i j D PicSet TI TI img i i j j j j att h h h h att h h att h h ? ? ? ? ? ? ? (23) ? ? ? | | | | 1 1 ( ) ( , ) ( , ) D PicSet TI TI TI img img i i i j j j j j j c h h h h h h h ? ? ? ? ? ? ? ? (24) Text-Image-Caption Attention (attTIC for short). This attention model uses both captions and images to represent the image information. attTIC computes the importance score of the caption cap j and the importance score of the image img j simultaneously, and then compute the context of the decoding hidden state ? i h using equation (  25 ). ? ? ? ? | | 1 | | 1 ( ) ( , ) ( , ) ( , ) D TIC TIC i i j j j PicSet TIC cap cap TIC img img i i j j j j j c h h h h h h h h h h ? ? ? ? ? ? ? ? ? ? (25) In the attention mechanisms, ? ( , ) i j h h ? is the normalized attention score of j h , ? ( , ) cap i j h h ? is the normalized attention score of cap j h , ? ( , ) img i j h h ? is the normalized attention score of img j h , and ? ( ) i c h is the context. The initial state of the decoder is computed by Equation (12) which can be adjusted according to different attention models. 

 Model Training Since there are no existing manual text-image summaries, and most of the existing training and testing data have pure text summaries, we decide to use pure text summaries as training data to train our models. The sentence-image alignment relationships can be discovered through training the multi-modal attention models. The loss function L of our summarization model is the negative log likelihood of generating text summaries over the training multi-modal document set MDS. We use the Adam (Kingma and Ba, 2014) gradient-based optimization method to optimize the model parameters. 

 Multi-Modal Beam Search Algorithm There are two major problems of the generation of summaries: one is the out-of-vocabulary problem, and the other is the low quality of the generated texts including information incorrectness and repetitions. For the OOV problem, we use the words in the attended sentences or captions in the original document to replace OOV tokens in the generated summary. Previous research uses the attended words to replace OOV tokens in the flatten encoder-decoder model which attends the words of the original word sequence  (Jean, et al., 2015) . Our model is hierarchical and multi-modal, and attends sentences, images, and captions when decoding. We use the following algorithm to find the replacement for the j th OOV in a generated sentence: Step 1: Order the original sentences and captions by the attending scores in descending order. Step 2: Return the j th OOV word in the ordered sentences and captions as the replacement. For the attTI mechanism that attends images neglecting captions, we use captions instead of the attended images in the algorithm. For the low-quality generated text problem, we adopt the hierarchical beam search algorithm  (Tan el al., 2017) . We extend the algorithm by adding caption-level and image-level beam search. The multi-modal hierarchical beam search algorithm comprises K-best word-level beam search and Nbest sentence-caption-level beam search. In particular, we use the corresponding captions instead of images in beam search algorithm for the attTI mechanism which attends images. 1 * 1 * ( ) ( ) ( ( , ) ( , )) t t t t t score y p y ref Y y s ref Y s ? ? ? ? ? ? ? (28) At the word-level search algorithm, we compute the score of generating word y t using equation (  28 ) where ref is a function calculating the ratio of bigram overlap between two texts, s * is the attended sentence or caption, and ? is the weighting factor. The added term aims to increase the overlap of the generated summary and the original text. At the sentence level and the caption level, we set the sentence beam width as N, and keep N-best previously un-referred sentences or captions which have highest attending scores. For each sentence beam, we try M sentences or captions and keep the one achieving best word-level scores. 

 Image Selection and Alignment We rank the images, select several most important images as the image summary, and align each sentence with an image in the image summary. The score of images is computed by equation (  29 ). | | , 1 ( )  29)  where ? i,j is the attention score of the j th image when generating the i th sentence of the text summary, and |TextSum| is the number of summary sentences. TextSum j i j i score img ? ? ? ? ( The images are ranked by the scores in descending order, and the top K images are selected to form the image summary ImgSum. We align each sentence i in TextSum to the image j in ImgSum such that ? i,j is the biggest. 

 Experiments 

 Data preparation We extend the standard DailyMail corpora through extracting the images and the captions from the html-formatted documents. We call the corpora as E-DailyMail. The standard DailyMail and CNN datasets are two widely used datasets for neural document summarization, which are originally built in  (Hermann et al., 2015)  by collecting human generated highlights and news stories from the news websites. We only the DailyMail dataset because it has more images and is easier to collect than CNN dataset does. We find that the text documents provided by the original DailyMail corpora contain captions. This is due to that all related texts are extracted from the html-formatted news when the corpora are created. We keep the original text documents unchanged in E-DailyMail. The split and statistics of E-DailyMail are shown in Table  1 . 

 Implementation We preprocess the text of the E-DailyMail corpora by tokenizing the text and replacing the digits with the <NUM> token. The 40k most frequent words in the corpora are kept and other words are replaced with OOV. Our model is implemented by using Google's open-source seq2seq-master project written with Tensorflow. We use one layer of the GRU cell. The dimension of the hidden state of the RNN decoder is 512. The dimension of the word embedding vector is 128. The dimension of the hidden state of the bi-directional RNN encoder is 256. We initialize the word embeddings with Google's word2vec tools  (Mikolov et al., 2013)  trained in the whole text of DailyMail/CNN corpora. We extract the 4096-dimension full-connected layer of 19-layer VGGNet  (Simonyan and Zisserman, 2014)  as the vector representation of images. We set the parameters of Adam to those provided in  (Kingma and Ba, 2014) . The batch size is set to 5. Convergence is reached within 800k training steps. It takes about one day for training 40k ~ 50k steps depending on the models on a GTX-1080 TI GPU card. The sentence beam width and the word beam width are set as 2 and 5 respectively. M is set as 3. The parameter ? is set as 3 or 300 tuned on the validation set. To train the multi-modal attention mechanism such as attTIC, we concatenate the matrix of text representations, image representations, and caption representations to one matrix M = [h 1 , h 2 , ... h |D| , h cap 1 , h cap 2 , ?, h cap |PicSet| , h img 1 , h img 2 , ?, h img |PicSet| ]. The parameters of the attention mechanisms are trained simultaneously. This way the model training can converge faster. 

 Evaluation of Text Summarization The widely used ROUGE  (Lin, 2004 ) is adopted to evaluate text summaries. We compare four attention models. HNNattTC-3, HNNattTIC-3, HNNattTI-3, and HNNattT-3 are our hierarchical RNN summarization models with the attTC, attIC, attTI, and attT attention mechanisms respectively, and 3 is the ? value. HNNattT is similar to the model introduced in  (Tan et al., 2017)  without the graph-based attention. We compare our models with HNNattT to show the influence of multi-modal attentions. The first 4 lines in Table  2   reasons is that the text documents provided by the DailyMail corpora contain captions. Captions are already parts of the text documents. The other reason is that captions distract attentions and cannot attract sufficient attentions from the original sentences, which will be discussed in the next subsection. We compare our methods with state-of-the-art neural summarization methods reported in recent papers on the DailyMail corpora. Extractive models include Lead which is a strong baseline using the leading 3 sentences as the summary, NN-SE  (Cheng and Lapata, 2016) , and SummaRuNNer-abs  (Nallapati et al., 2017)  which is trained on the abstractive summaries. Abstractive models include NN-ABS, NN-WE, LREG, though they are tested on 500 samples of the test set. LREG is a feature-based method using linear regression. NN-ABS is a simple hierarchical extension of  (Rush et al., 2015) . NN-WE is the abstractive model restricting the generation of words from the original document. The results are shown in the last 6 rows in Table  2 . Our method HNNattTI outperforms the three extractive models and the three abstractive models. We compare our models under the full-length F1 metric by setting the ? value as 300. According to  (Tan et al., 2017) , a large ? makes the generated summary has more overlaps with the attended texts, and thus partly overcome the repeated sentences problem in the generated summary. We do not incorporate the attention distraction mechanism  (Chen et al., 2016)  into our model, because we want to focus on our own model to see whether considering images improves text summarization. Results in Table 3 also show that HNNattTI performs better than HNNattT, HNNattTC, and HNNattTIC. To show the influence of our OOV replacement mechanism, we eliminate the mechanism from our models, and show the evaluation results in Table  4  and Table  5 . We can see from the two tables that the scores are lower than the corresponding scores in Table  2 and Table 3 . Our OOV replacement mechanism improves the summarization models, though the mechanism is relatively simple. In short, combining and attending images in the neural summarization model improves document summarization. 

 Evaluation of Image Summarization To evaluation the image summarization, the gold standard image summary is generated based on a greedy algorithm on the captions as follows: at each time i, choose img k to maximize Rouge We use the 1-image and 2-image random selected image summaries as the baselines which we compare our models with. The top 1 or 2 images ranked by our model are selected out to form the summaries. Results in Table  4  show that HNNattTI outperforms the random baseline, while HNNattTC and HNNattTIC perform worse. This implies that attending images can generate better sentence-image alignment in the multimodal summaries than the model attending captions does. And this can also partly explain why our summarization model attending images when decoding can generate better text summaries than the one attending captions does. ({cap 1 ,?cap i-1 ,cap k }, Abs_Sum) ? Rouge({cap 1 ,?cap i-1 }, Abs_Sum 

 Instance Figure  4  shows the text-image summary of the example demonstrated in Figure  1  generated by the HNNattTI model. In the summary, there are 2 images and 3 generated sentences, and each sentence is aligned with an image. The image summary has one common image with Figure  2 . The sentences are named by S1, S2, and S3 respectively. Table  7  shows the sentence-image alignment scores. The four images in the original document are numbered from top to bottom and left to right by IMG1, IMG2, IMG3, and IMG4. The summation of alignment scores for a summary sentence is less than 1, because the sentence is also aligned with the sentences in the original document. 

 Conclusions This paper proposes the text-image summarization task to summarize and align texts and images simultaneously. Most previous research summarizes texts and images separately, and few has been done on text-image summarization. We propose the multi-modal attentional mechanism which attends original sentences, images, captions simultaneously in the hierarchical encoder-decoder model, use the RNN model to encode the ordered image set as the initial state of the decoder, and propose the multimodal beam search algorithm which scores beams using the bigram overlaps of the generated sentences and the captions. The model is trained by using abstractive text summaries as the targets, and the attention scores of images are used to score images. The original DailyMail dataset is extended by collecting images and captions from the Web. Experiments show that our model attending images outperforms the models not attending images, three existing neural abstractive models and three existing extractive models. Experiments also show our model can generate informative summaries of images.  Figure 1 : 1 Figure 1: An example of multi-modal news taken from the DailyMail corpora. 

 Figure 2 : 2 Figure 2: The manually generated text-image summary. 

 Figure 3 : 3 Figure 3: The framework of our neural text-image summarization model. 

 y 1 , y 2 , ?, y |Y| ] is the word sequences of the summary corresponding to the main text D and the ordered image set PicSet, including the tokens <eos>, <neod> and <eod>.the multi-modal encoder-decoder model. 

 Figure 4 : 4 Figure 4: The generated text-image summary of the example in Figure 1. 

 Table 1 : 1 are the results with summary length of 75 bytes. The results show that HNNattTI has considerable improvement over HNNattT. An interesting observation is that HNNattTC and HNNattTIC are not better than HNNattT. One of the The split and statistics of the E-DailyMail corpora. D.L and S.L indicate the average number of sentences in the document and summary. I.N indicates the average number of images in the story. Sent.L and Cap.L indicates the average number of word in the sentence and the caption respectively. Train Dev Test 196557 12147 10396 D.L. S.L. I.N Sent.L Cap.L 26.0 3.84 5.42 26.86 24.75 

 Table 6 : 6 Image summarization using the recall metric for the 1-image or 2-images summary. ? is set as 300. )) where 

 Table 3 : 3 Comparison results on the DailyMail test set using full-length F1metric. Method Rouge-1 Rouge-2 Rouge-L HNNattTI-3 24.84 8.7 16.99 HNNattTC-3 18.61 6.7 13.44 HNNTattTIC-3 21.17 8.1 15.24 HNNattT-3 22.09 7.9 15.97 Lead 21.9 7.2 11.6 NN-SE 22.7 8.5 12.5 SummaRuNNer-abs 23.8 9.6 13.3 LREG(500) 18.5 6.9 10.2 NN-ABS(500) 7.8 1.7 7.1 NN-WE(500) 15.7 6.4 9.8 

 Table 2 : 2 Comparison results on the DailyMail test set using Rouge recall at 75 bytes.cap k is the caption of img k . The average number of images in summaries is 2.15. The average Rouge-1, Rouge-2, and Rouge-L scores of the caption summaries with respect to the ground truth summaries are43.85, 19.70, and 36.30   respectively.
