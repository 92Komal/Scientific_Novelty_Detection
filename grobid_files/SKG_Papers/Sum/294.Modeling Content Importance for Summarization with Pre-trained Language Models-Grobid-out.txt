title
Modeling Content Importance for Summarization with Pre-trained Language Models

abstract
Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to generalize to semantic units of longer text spans. In this work, we apply information theory on top of pretrained language models and define the concept of importance from the perspective of information amount. It considers both the semantics and context when evaluating the importance of each semantic unit. With the help of pre-trained language models, it can easily generalize to different kinds of semantic units (n-grams or sentences). Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.

Introduction and Related Work Text summarization aims to compress long document(s) into a concise summary while maintaining the salient information. It often consists of two critical subtasks, important information identification and natural language generation (for abstractive summarization). With the advancements of large pre-trained language models (PreTLMs)  (Devlin et al., 2019; Yang et al., 2019) , state-of-the-art results are achieved on both natural language understanding and generation. However, it is still unclear how well these large models can estimate "content importance" for a given document. Previous studies for modeling importance are either empirical-based, which implicitly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments  (Peyrard, 2019) . Benefiting from the large-scale summarization datasets  (Nallapati et al., 2016; Narayan et al., 2018) , data-driven approaches  (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019)  have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features  (Zheng and Lapata, 2019) , such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based methods  (Louis, 2014; Peyrard, 2019; Lin et al., 2006)  aim to formalize the concept of importance, and develop general-purpose systems by modeling the background knowledge of readers. This is based on the intuition that humans are good at identifying important content by using their own interpretation of the world knowledge. Theoretical models usually rely on information theory (IT)  (Shannon, 1948) .  Louis (2014)  uses Dirichlet distribution to represent the background knowledge and employs Bayesian surprise to find novel information.  Peyrard (2019)  instead models the importance with entropy, assuming the important words should be frequent in the given document but rare in the background. However, statistical method is only a rough evaluation for informativity, which largely ignores the effect of semantic and context. In fact, the information amount of units is not only determined by frequency, but also by its semantic meaning, context, as well as reader's background knowledge. In addition, bag-of-words approaches are difficult to generalize beyond unigrams due to the sparsity of n-grams when n is large. In this paper, we propose a novel and generalpurpose approach to model content importance for summarization. We employ information theory on top of pre-trained language models, which are expected to better capture the information amount of semantic units by leveraging their meanings and context. We argue that important content contains information that cannot be directly inferred from context and background knowledge. Large pretrained language models are suitable for our study since they are trained from large-scaled datasets consisting of diverse documents and thus containing a wide range of knowledge. We conduct experiments on popular summarization benchmarks of CNN/Daily Mail and New York Times corpora, where we show that our proposed method can outperform prior importance estimation models. We further demonstrate that our method can be adapted to model semantic units of different scales (n-grams and sentences). 

 Methodology In this section, we first estimate the amount of information by using information theory with pretrained language models ( ?2.1 and ?2.2), where we consider both the context and semantic meaning of a given text unit. We then propose a formal definition of importance for text summarization from a perspective of information amount ( ?2.3). 

 Information Theory Information theory (IT), as invented by  Shannon (1948) , has been used on words to quantify their "informativity". Concretely, IT uses the frequency of semantic units x i to approximate the probability P (x i ) and uses negative logarithm of frequency as the measurement for information, which is called self-info 1 : I(xi) = ? log 2 P (xi) (1) It approximates the information amount of a unit (e.g. word) in a given corpus. However, traditional IT suffers from the sparsity problem of longer n-grams and also ignores semantics and context. Advanced compression algorithms in  IT (Hirschberg and Lelewer, 1992)  attempt to model the context to better estimate the information amount. But due to the sparsity, they can only count up to third-order statistics. Statistical methods are nearly impossible to reliably calculate the probability of x i conditioned on its context,  1  The unit of information is "bit", with base of 2. In the rest of this paper, we omit base 2 for brevity. Here we take a subsequence x 3 x 4 as example. [M] denotes mask and PLMs/MLMs/ALMs are three different options for language models. I( x 3 x 4 |?) = ? log[P (x 3 |?)P (x 4 |?)], where conditions for different models are omitted for brevity. e.g., P (x i | ? ? ? , x i?1 , x i+1 , ? ? ? ), as the number of combinations of the context can be explosive. 

 Using Language Models in Information Theory With the development of deep learning, neural language models can efficiently predict the probability of a specified unit, such as a word or a phrase, given its context, which makes it feasible to calculate high-order approximation for the information amount. We thus propose to use neural language models to replace the statistical models for estimating the information amount of a given semantic unit. Language models can be categorized as follows, and we present information estimation method for each as shown in Fig.  1 . Auto-regressive Language Model (ALM)  (Bengio et al., 2000)  is the most commonly used probabilistic model to depict the distribution of language, which is usually referred as unidirection LM (UniLM). Given a sequence of tokens x 0:T = [x 0 , x 1 , ? ? ? , x T ], UniLMs use leftward content to estimate the conditional probability for each token: P (x t |x <t ) = g UniLM (x <t ), where g UniLM denotes a neural network for language model and x <t represents the sequence from x 0 to x t?1 . Then the joint probability of a subsequence is factorized as: P (xm:n|x<m) = n t=m P (xt|x<t) (2) After applying Eq. (  1 ) to both sides of Eq. (  2 ), we can obtain the information amount of the subse-quence conditioned on its context as: I(xm:n|x<m) = n t=m I(xt|x<t) (3) Masked Language Model (MLM) is proposed by  Taylor (1953)  and combined with pre-training by  Devlin et al. (2019)  to encode bidirectional context. MLM masks a certain number of tokens from the input sequence, then predicts these tokens based on the unmasked ones. The conditional probability of a masked token x t can be estimated as: P (x t |x =t ) = g MLM (x =t ) , where = t indicates that the t-th token is masked. Information amount of a given subsequence of the input is calculated as: I(xm:n|x / ?[m:n] ) = n t=m I(xt|x / ?[m,n] ) (4) Since MLMs encode both leftward and rightward context, intuitively, it can better estimate the information of current tokens than UniLMs. Permutation Language Model (PLM) is proposed by  (Yang et al., 2019)  to combine ALMs and MLMs, by considering the dependency between the masked tokens as well as overcoming the problem caused by discrepancy of pre-training and fine-tuning in MLMs. It models the dependency of the tokens by maximizing the expected likelihood of all possible permutations of factorization orders. The probability prediction can be formalized as: P (z t |z <t ) = g PLM (z <t ) where z denotes a possible permutation sequence of input. Information of a subsequence is estimated as: I(zm:n|z<m) = n t=m I(zt|z<t) (5) 

 Modeling Importance with Pre-trained Language Model We argue that important content should be hard to be predicted based on background knowledge only; it should be also difficult to be inferred from the context. Moreover, detecting important content is to find the most informative part from the input. As described in  (Shann, 1989) , the information amount is a quantification of the uncertainty we have for the semantic units. But the degree of uncertainty is relative to reader's background knowledge. The less knowledge the reader has, the more uncertainty the source shows. We thus employ pre-trained language models, which contain a wide range of knowledge, to represent background knowledge. If a semantic unit is frequently mentioned in the training corpus, it will get high probability during inference and thus low information amount. We further propose a notion of importance as the information amount conditional on the background knowledge: Imp(xi|X ? xi, K) = ? log PLM K (xi|X ? xi) (6) where X ? x i means the context excluding 2 the unit x i from input X and K denotes the knowledge encoded in the pre-trained model. In practice, when calculating the importance of a semantic unit, we first exclude all its occurrences from the input document, and let the PreTLMs predict the probability of each occurrence, based on which the information amount is calculated. As the same unit may appear at multiple positions in the input, summation is used as the final value of information amount. Based on our notion of importance, a summarization model is to maximize the overall importance of a subset x of the input X, with a length constraint, such as x i ?x |x i | < l max : arg max x?X Imp(x) = x i ?x Imp(xi|X ? xi, K) (7) 3 Experimental Setups Semantic Units and Tasks. Our theory can be generalized for evaluating the importance for any scale of semantic units. To verify the effectiveness of our theory, we instantiate the semantic unit with three common forms: unigram, bigram and sentence. In this way, our method can also be regarded as a general unsupervised information extraction system, serving as a keyphrase extraction or sentence-level extractive summarization model. As our method exploits the existed PreTLMs and needs no additional training, it has the potential of benefiting the low-resource languages and domains. In unigram scenario, we simply instantiate semantic unit x i as a token w t and calculate its importance with Imp(w t ) = ? log P (w t |w =t , K). For evaluation, top-k important ones are selected and F 1 score is calculated by comparing against the reference, where the value of k is set by grid search. Importance of bigrams, e.g., x i = w t w t+1 , can be represented as a joint probability of two tokens: Imp(w t w t+1 ) = ? log P (w t w t+1 |w t / ?[t,t+1] , K). Same as unigrams, F 1 score is computed to evaluate the accuracy. By extending the formula of bigram importance to longer sequences, we get importance definition  Best results per metric are in bold. Among our models (bottom), IMP yields significantly higher scores on all metrics except when using unigrams as semantic unit and with sentences (based on R-1) on NYT (Welch's t-test, p<0.05). 

 Models F 1 F 1 R-1 R-2 R-L F 1 F 1 R-1 R-2 R-L for a sentence as: Imp(s i ) = I(s i |w / ?s i , K) = ? log P (s i |w / ?s i , K). For evaluation, we select a subset of sentences with Eq. (  7 ) and calculate the ROUGE scores  (Lin, 2004)  against reference summary. The length constraints for CNN/DM and NYT are set to 105 and 95 tokens respectively. Datasets. We evaluate our method on the test set of two popular summarization datasets: CNN/Daily Mail (abbreviated as CNN/DM)  (Nallapati et al., 2017)  and New York Times  (Sandhaus, 2008) . Following  See et al. (2017)  3 , we use the nonanonymized version that does not replace the name entities, which is most commonly used in recent work. We preprocess them as described in  (Paulus et al., 2018) . For unigram experiments, we remove all the stop words and punctuation in the reference summaries and treat the notional words as the predicting targets. For bigram, we first collect all the bigrams in source document and then discard the ones containing stop words or punctuation. The rest bigrams are employed as the predicting targets. Comparisons. We compare our method with two types of models: (1) the methods that estimate importance for n-grams. We consider TF?IDF, a numerical statistic to reflect how important a term is to a document, and STM  (Peyrard, 2019) , a simple theoretic model for content importance based on statistical information theory. (2) unsupervised models for extractive summarization. We adopt centrality-based models LEXRANK  (Erkan and Radev, 2004) , TEXTRANK  (Mihalcea and Tarau, 2004)  and TEXTRANK+BERT  (Zheng and Lapata, 2019) , a frequency-based model SUM-BASIC  (Ani Nenkova, 2005) , and BAYESIANSR  (Louis, 2014)  which scores words or sentences with Bayesian surprise. 

 Results We conduct extensive experiments with pre-trained models 4 in all three types of language models, including ALM: GPT-2  (Radford et al., 2019) ; MLMs: BERT  (Devlin et al., 2019) , and DISTILL-BERT  (Sanh et al., 2019) ; PLMs: XLNET  (Yang et al., 2019) . As shown in Table  1 , our method IMP consistently outperform prior models. Among comparisons, we can see that theory-based methods, STM and BAYESIANSR, achieve better results. This is because they have statistics estimated for background distribution, which helps filter out common words. The significant advantage of our method verifies our hypothesis that pre-trained language models better characterize the background knowledge, which in turn more precisely calculate the importance of each semantic unit. Moreover, our methods have a more significant improvement on bigram-level prediction than unigram-level. This is due to the fact that IMP-based models overcome the sparsity issue, where they can evaluate the importance of a phrase by considering its semantic meaning and context. Surprisingly, our method can also generalize to sentence-level semantic units and serve as an unsupervised extract-based model for summarization. Our models achieve significantly higher ROUGE scores than previous work by average 2.02. This observation inspires a potential future direction for sentence-level importance modeling based on background knowledge as well as context information. We also compare the performance of PreTLMs in different categories. MLMs, including BERT and DISTILLBERT, have the best overall performance, since they are able to encode bidirectional context. PLM, i.e. XLNet, is slightly inferior to MLMs because the probabilities of the words are related to the order of their permutation, which may hurt importance estimation by our method. 

 Future Work In the future work, we would like to fine-tune the current language models on the target of max P (x i |X ? x i ) to better align with the interpretation of information theory. Currently, the PreTLMs mostly mask the text randomly, which still differ from our current method's objective. Background knowledge also deserves further investigation. The background knowledge of our methods comes from the pre-training process of language models, suggesting that the information distribution largely depends on the training data. Meanwhile, most PreTLMs are trained with Wikipedia or books, which may affect the determination of content importance from text with different styles. So domain-specific knowledge, such as genres or topics, can be included in the future work. 

 Conclusion We propose to use large pre-trained language models to estimate the information amount of given text units, by filtering out the background knowledge as encoded in the large models. We show that the large pre-trained models can be used as unsupervised methods for content importance estimation, where significant improvement over nontrivial baselines is achieved on both keyphrase extraction and sentence-level extractive summarization tasks. Figure 1 : 1 Figure1: Information amount evaluation with language models. Here we take a subsequence x 3 x 4 as example.[M] denotes mask and PLMs/MLMs/ALMs are three different options for language models. I(x 3 x 4 |?) = ? log[P (x 3 |?)P (x 4 |?)],where conditions for different models are omitted for brevity. 
