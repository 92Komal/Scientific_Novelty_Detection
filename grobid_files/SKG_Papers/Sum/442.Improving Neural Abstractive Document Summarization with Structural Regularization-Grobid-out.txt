title
Improving Neural Abstractive Document Summarization with Structural Regularization *

abstract
Recent neural sequence-to-sequence models have shown significant progress on short text summarization. However, for document summarization, they fail to capture the longterm structure of both documents and multisentence summaries, resulting in information loss and repetitions. In this paper, we propose to leverage the structural information of both documents and multi-sentence summaries to improve the document summarization performance. Specifically, we import both structural-compression and structuralcoverage regularization into the summarization process in order to capture the information compression and information coverage properties, which are the two most important structural properties of document summarization. Experimental results demonstrate that the structural regularization improves the document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperforms state-of-the-art neural abstractive methods.

Introduction Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation  (Bahdanau et al., 2014)  and image caption  (Vinyals et al., 2015) , has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization  (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; , which builds onesentence summaries from one or two-sentence in- * This work was done while the first author was doing internship at Baidu Inc.     1  on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov  (See et al., 2017)  system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document. put. However, the extension of sentence abstractive methods to document summarization task is not straightforward. I 19 I 20 O 1 O 2 O 3 O 4 O 5 (b) Seq2Seq-baseline O 1 O 2 O 3 (d) Hierarchical-baseline O 1 O 2 O 3 O 4 (c) Point-gen-cov O 1 O 2 (e) Our Method As long-distance dependencies are difficult to be captured in the recurrent framework  (Bengio et al., 1994) , the seq2seq models are not yet able to achieve convincing performance in encoding and decoding for a long sequence of multiple sentences  (Chen et al., 2017; Koehn and Knowles, Original Text (truncated) : the family of conjoined twin sisters who died 19 days after they were born have been left mortified (2) after they arrived at their gravesite to find cemetery staff had cleared the baby section of all mementos and tossed them in the rubbish (3) . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus (1) . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west (2) . scroll down for video . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus (1) . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare (3) . ' we took renee's aunts to see the girls for the first time and we found everything had been stripped away , ' mr howie told daily mail australia . ' we were devastated and mortified . we 've had a little shrine set up and we 've been adding to it since the funeral . ' it 's heartbreaking to know we 've set this up and it has been treated like rubbish . ' faith and hope were buried in a pink coffin and their family and friends released doves and pink and white balloons at their funeral . their family and friends had built up a small memorial with pink and white statues , flowers , pebbles and toys over the past 11 months . when they arrived on thursday , everything had been removed apart from a bunch of flowers . the twins were buried at pinegrove memorial park in western sydney after they died after just 19 days (2) . their family and friends had built a small shrine at their gravesite , which they have added to since the funeral . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when parents simon howie and renee young arrived on thursday they found the site completely bare (3) . Gold Reference: faith and hope howie were born with one body and two faces on may 8. they tragically died in hospital just 19 days after they were born . parents simon howie and renee young visit their grave at pinegrove in western sydney fortnightly . they arrived on thursday to find the grave bare of all the girls ' mementos . staff had cleared entire baby section and thrown belongings in rubbish . Seq2seq-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . Point-cov  (See et al., 2017) : faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial park in sydney 's west. Hierarchical-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition . they died in hospital less than a month after they died in hospital less than a month after they were born and laid them to rest at pinegrove memorial park in sydney 's west . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they were born on thursday they found the site completely bare . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when they found the site completely bare . Our Method: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition (1) . they died in hospital less than a month after they were born and their parents laid them to rest at pinegrove memorial park in sydney 's west (2) . family members have visited the grave every week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare (3) . Table  1 : Comparison of the generated summaries of four abstractive summarization models and the gold reference summary on a news article. The summaries generated by the seq2seq models, both the Seq2seq-baseline model and the Point-cov model, lose some salient information. The Seq2seq-baseline model even contains serious information repetitions. The Hierarchicalbaseline model not only contains serious repetitions, but also makes non-grammatical or non-coherent sentences. On the contrary, the summary generated by our model contains more salient information and is more concise. Our model also shows the ability to generate a summary sentence by compressing several source sentences, such as shortening a long sentence. 2017). In document summarization, it is also difficult for the seq2seq models to discover important information from too much input content of a document  (Tan et al., 2017a,b) . The summary generated by the seq2seq models usually loses salient information of the original document or even contains repetitions (see Table  1 ). In fact, both document and summary naturally have document-sentence hierarchical structure, instead of being a flat sequence of words. It is widely aware that the hierarchical structure is necessary and useful for neural document modeling. Hierarchical neural models have already been successfully used in document-level language modeling  (Lin et al., 2015)  and document classification  (Yang et al., 2016) . However, few work makes use of the hierarchical structure of document and multi-sentence summary in document summarization. The basic hierarchical encoderdecoder model  is also not yet able to capture the structural properties of both document and summary (see Figure  1  1 ), resulting in 1 To simulate the sentence-level attention mechanism on the gold reference summary, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and the corresponding source document sentences and normalize them into attention distributions. The sentence-level attention distributions of the Seq2seq-baseline model and the Point-gen-cov model are computed by summing the attention weights of all words in each sentence and then normalized across sentences. more serious repetitions and even nonsensical sentences (see Table  1 ). In document summarization, information compression and information coverage are the two most important structural properties. Based on the hierarchical structure of document and summary, they can be realized at the sentencelevel as: (1) Structural-compression: each summary sentence is generated by compressing several specific source sentences; (2) Structuralcoverage: different summary sentences usually focus on different sets of source sentences to cover more salient information of the original document. Figure  1 (a) intuitively shows the two properties in human-written gold reference summaries. We import both structural-compression and structural-coverage regularizations into the document summarization process based on a hierarchical encoder-decoder with hybrid sentenceword attention model. Typically, we design an effective learning and inference algorithm to explicitly model the structural-compression and structural-coverage properties of document summarization process, so as to generate more informative and concise summaries (see Table  1 ). We conduct our experiments on benchmark datasets and the results demonstrate that properly modeling the structural-compression and structural-coverage properties based on the hier- archical structure of document and summary, improves document summarization performance significantly. Our model is able to generate more informative and concise summaries by enhancing sentences compression and coverage, and significantly outperforms state-of-the-art seq2seq-based abstractive methods, especially on summarizing long documents with long summaries. 

 Hierarchical Encoder-Decoder Model In this section, we introduce our baseline hierarchical encoder-decoder model which consists of two parts: a hierarchical encoder and a hierarchical decoder, as shown in Figure  2 . Similar to , both the encoder and decoder consists of two levels: a sentence level and a word level. The main distinction is that we design a hybrid sentence-word attention mechanism on the hierarchical decoder to help organize summary content and realize summary sentences. 

 Hierarchical Encoder The goal of the encoder is to map the input document to a hidden vector representation. We consider a source document X as a sequence of sentences: X = {s i }, and each sentence s i as a sequence of words: s i = {w ij }. The wordlevel encoder encodes the words of a sentence into a sentence representation, and the sentencelevel encoder encodes the sentences of a document into the document representation. In this work, both the word-level encoder and sentencelevel encoder use the bidirectional Gated Recurrent Unit (BiGRU)  (Chung et al., 2014) . The word-level encoder sequentially updates its hidden state upon each received word, as h i,j = BiGRU (h i,j?1 , e i,j ) where h i,j and e i,j denote the hidden state and the embedding of word w i,j , respectively. The concatenation of the forward and backward final hidden states in the word-level encoder is indicated as the vector representation r i of sentence s i , which is used as input to the sentencelevel encoder. The sentence-level encoder updates its hidden state after receiving each sentence representation, as h i = BiGRU (h i?1 , r i ) where h i denotes the hidden state of sentence s i . The concatenation of the forward and backward final states in the sentence-level encoder is used as the vector representation of document d. In the hierarchical encoder architecture, long dependency problem will be largely reduced at both the sentence level and the word level, so it can better capture the structural information of the input document. 

 Hierarchical Decoder with Hybrid Sentence-Word Attention The goal of the decoder is to generate output summary according to the representation of the input document. Let Y = {s ? i } indicates a candidate summary of document X, and each sentence s ? i consists of a sequence of words  s ? i = {w ? ij }. ? t,k?1 in sentence s ? t . In this work, we design a hybrid sentenceword attention mechanism based on the hierarchical encoder-decoder architecture, which contains both sentence-level attention and word-level attention, to better exploit both the sentence-level information and word-level information from the input document and the output summary. 

 Sentence-level Attention The sentence-level attention mechanism is designed on the sentence-level encoder and decoder, which is used to help our model to detect important and relevant source sentences in each sentence generation step. ? i t indicates how much the t-th summary sentence attends to the i-th source sentence, which is computed by ? i t = e f (h i ,h ? t ) / ? l e f (h l ,h ? t ) where f is the function modeling the relation between h i and h ? t . We use the function f (a, b) = v T tanh(W a a + W b b), where v, W a , W b are all learnable parameters. Then the sentence level context vector c s t when generating the tth sentence s ? t can be computed as: c s t = ? i ? i t h i , which is incorporated into the sentence-level decoding process. 

 Word-level Attention with Sentence-level Normalization The word-level attention is designed on the wordlevel encoder and decoder, which is used to help our model to realize the summary sentence by locating relevant words in the selected source sentences in each word generation step. Let ? i,j t,k denotes how much the j-th word in source sentence s i contributes to generating the k-th word in summary sentence s ? t , which is computed by ? i,j t,k = e f (h i,j ,h ? t,k ) / ? l e f (h i,l ,h ? t,k ) . Since the word-level attention above is within each source sentence, we normalize it by sentencelevel attentions to get word attention over all source words, as ? i t,k = ? i t,k ? i t . Then the wordlevel context vector when generating word w ? t,k can be computed as: c w t,k = ? i ? j ? i,j t,k h i,j , which is also incorporated into the word-level decoding process. At each word generation step, the vocabulary distribution is calculated from the context vector c w t,k and the decoder state h ? t,k by: P vocab (w ? t,k ) = sof tmax(Wv(Wc[h ? t,k , c w t,k ] + bc) + bv) (1) where W v , W c , b c and b v are learned parameters. We also incorporate the copy mechanism  (See et al., 2017)  based on the normalized wordlevel attention to help generate out-of-vocabulary (OOV) words during the sentence realization process. 

 Structural Regularization Although the above hierarchical encoder-decoder model is designed based on the documentsentence hierarchical structure, it can't capture the basic structural properties of document summarization (see Figure  1 (d) and Table  1 ). How-ever, the hierarchical architecture makes it possible for importing structural regularization to capture the sentence-level characteristics of document summarization process. In this work, we propose to model the structural-compression and structural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase. 

 Structural Compression Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts  (Li et al., 2013; Durrett et al., 2016)  and sentence fusion-based methods which merge information from several different source sentences  (Barzilay and McKeown, 2005; Cheung and Penn, 2014) . As shown in Figure  1 , each summary sentence in the human-written reference summary is also created by compressing several specific source sentences. In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by: strCom(? t ) = 1 ? 1 logN N ? i=1 ? i t log? i t (2) where ? t denotes the sentence-level attention distribution when generating the tth summary sentence and N denotes the length of distribution ? t . The right part in the above formula is actually the entropy of the distribution ? t . As the attention distribution becomes sparser, the entropy of the distribution becomes lower, so the value of strCom(? t ) defined above will become larger. Sparse sentence-level attentions help the model compress and generalize several specific source sentences which are salient and relevant in the sentence generation process. Note that, 0 ? strCom(? t ) ? 1. 

 Structural Coverage A good summary should have the ability to cover most of the important information of an input document. As shown in Figure  1 , the humanwritten reference summary covers the information of many source sentences. Coverage has been used as a measure in many traditional document summarization research, such as the submodularbased methods which optimize the information coverage of the summary with similarity-based coverage metrics  (Lin and Bilmes, 2011; Chali et al., 2017) . In this work, we simply model the structuralcoverage property of summary based on the hierarchical architecture by encouraging different summary sentences to focus on different sets of source sentences so that the summary can cover more salient sentences of the input document. We measure the structural-coverage of summary based on the sentence-level attention distributions: strCov(? t ) = 1 ? ? i min(? i t , t?1 ? t ? =0 ? i t ? ) (3) which is used to encourage different summary sentences to focus on different sets of source sentences during the summary generation process. As the sentence-level attention distributions of different summary sentences become more diversified, the summary will cover more source sentences, which is effective to improve the informativeness and conciseness of summaries. Note that, 0?strCov(? t ) ? 1. 

 Model Learning Experimental results reveal that the properties of structural-compression and structural-coverage are hard to be captured by both the seq2seq models and the hierarchical encoder-decoder baseline model, which largely restricts their performance (Section 4). In this work, we model them explicitly by regulating the sentence-level attention distributions based on the hierarchical encoderdecoder framework. The loss function L of the model is the mix of negative log-likelihood of generating summaries over training set T , the structural-compression loss and the structuralcoverage loss: L = ? (X,Y )?T {?logP (Y |X; ?) + ?1 ? t strCom(?t) structural?compression loss + ?2 ? t strCov(?t) structural?coverage loss } (4) where ? 1 and ? 2 are hyper-parameters tuned on the validation set. We use Adagrad  (Duchi et al., 2011)  with learning rate 0.1 and an initial accumulator value 0.1 to optimize the model parameters ?. 

 Hierarchical Decoding Algorithm The traditional beam search algorithm that widely used for text generation can only help generate fluent sentence, and is not easy to extend to the sentence level. The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other  Tan et al., 2017a) . We propose a hierarchical beam search algorithm with structural-compression and structural-coverage regularization. The hierarchical decoding algorithm has two levels: K-best word-level beam search and N -best sentence-level beam search. At the word-level, the vanilla beam search algorithm is used to maximize the accumulated score P (s ? t ) of generating current summary sentence s ? t . At the sentencelevel, N -best beam search is realized by maximizing the accumulated score score t of all the sentences generated, including the sentences generation score, structural-compression score and structural-coverage score, which are defined as: scoret = t ? t ? =0 { P (s ? t ? )+?1strCom(? t ? )+?2strCov(? t ? )} (5) where ? 1 and ? 2 are factors introduced to control the influence of structural regularization during the decoding process. 

 Experiments 

 Experimental Settings We conduct our experiments on the CNN/Daily Mail dataset  (Hermann et al., 2015) , which has been widely used for exploration on summarizing documents with multi-sentence summaries  See et al., 2017; Tan et al., 2017a; Paulus et al., 2017)  use 512-dimensional hidden states. The dimension of word embeddings is 128, which is learned from scratch during training. We use a vocabulary of 50k words for both the encoder and decoder. We trained our model on a single Tesla K40m GPU with a batch size of 16 and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 300 epochs. After tuning on the validation set, parameters ? 1 , ? 2 , ? 1 and ? 2 , are set as -0.5, -1.0, 1.2 and 1.4, respectively. At the test time, we use the hierarchical decoding algorithm with sentence-level beam size 4 and word-level beam size 8. 

 Evaluation ROUGE Evaluation. We evaluate our models with the widely used ROUGE  (Lin, 2004)  toolkit. We compare our system's results with the results of state-of-the-art neural summarization approaches reported in recent papers, including both abstractive models and extractive models. The extractive models include SummaRuNNer  (Nallapati et al., 2017)  and SummaRuNNer-abs which is similar to SummaRuNNer but is trained directly on the abstractive summaries. The abstractive models include: 1) Seq2seq-baseline, which uses the basic seq2seq encoder-decoder architecture with attention mechanism, and incorporates with copy mechanism  (See et al., 2017)  to alleviate the OOV problem. 2) ABS-temp-attn , which uses Temporal Attention on the seq2seq architecture to overcome the repetition problem. 3) Point-cov  (See et al., 2017) , which is an extension of the Seq2seq-baseline model by importing word-coverage mechanism to reduce repetitions in summary. 4) Graph-attention  (Tan et al., 2017a)  uses a graph-ranking based attention mechanism based on a hierarchical architecture to identify important sentences. 5) Hierachical-baseline, which just uses the basic hierarchical encoder-decoder with hybrid attention model proposed in this paper. Results in Table  2  show that our model significantly outperforms all the neural abstractive baselines and extractive baselines. An interesting observation is that the performance of the Hierarchical-baseline model are lower than the Seq2seq-baseline model, which demonstrates the difficulty for a traditional model to identify the structural properties of document summarization process. Our model outperforms the Hierarchical-baseline model by more than 4 ROUGE points, which demonstrates that the structural regularization improves the document summarization performance significantly. To verify the superiority of our model on generating long summaries, we also compare our method with the best seq2seq model Point-cov  (See et al., 2017)  by evaluating them on a test set w.r.t. different length of reference summaries. The results are shown in Table  3 , which demonstrate that our model is better at generating long summary than the seq2seq model. As the summary becomes longer, our system will obtain larger advantages over the baseline (from +0.22 Rouge-1, +0.08 Rouge-2 and +0.39 Rouge-L for summary less than 100 words, rising to +5.00 Rouge-1, +0.54 Rouge-2 and +4.88 Rouge-L for summaries more than 150 words). Human Evaluation. In addition to the ROUGE evaluation, we also conducted human evaluation on 50 random samples from CNN/DailyMail test set and compared the summaries generated by our method with the outputs of Seq2seq-baseline and Point-cov  (See et al., 2017) . Three data annotators were asked to compare the generated summaries    1 ) and strCov (Equation  2 ) scores. with the human summaries, and assess each summary from four independent perspectives: (1) Informative: How informative the summary is? (2) Concise: How concise the summary is? (3) Coherent: How coherent (between sentences) the summary is? (4) Fluent: How fluent, grammatical the sentences of a summary are? Each property is assessed with a score from 1(worst) to 5(best). The average results are presented in Table  4 . The results show that our model consistently outperforms the Seq2seq-baseline model and the previous state-of-the-art method Point-cov. As shown in Table  1 , the summary generated by Seq2Seq-Baseline usually contains repetition of sentences or phrases, which seriously affects its informativeness, conciseness as well as coherence. The Point-cov model effectively alleviates the information repetition problem, however, it usually loses some salient information and mainly copies original sentences directly from the input document. The summaries generated by our method obviously contains more salient information and are more concise through sentences compression, which shows the effectiveness of the structural regularization in our model. The results also show that the sentence-level modeling of document and summary in our model makes the generated summaries achieve better inter-sentence coherence. 

 Discussion 

 Model Validation To verify the effectiveness of each component in our model, we conduct several ablation experiments. Based on the Hierarchical-baseline model, several different structural regularizations are added one by one: +strCom indi-   5 . Our method much outperforms all the compared systems, which verifies the effectiveness of each component of our model. Note that, both the structural-compression and structural-coverage regularization significantly affect the summarization performance. The higher structuralcompression and structural-coverage scores will lead to higher ROUGE scores. Therefore, we can conclude that the structural-compression and structural-coverage regularization based on our hierarchical model have significant contributions to the increase of ROUGE scores. 

 Structural Properties Analysis We further compare the ability of different models in capturing the structural-compression and structural-coverage properties of document summarization. Figure  3  shows the comparison results of 4000 document-summary pairs with 14771 reference-summary sentences sampled from CNN/Daily Mail dataset. Figure  3(a)  shows that most samples (over 95%) fall into the righttop area in human-made summaries, which indicates high structural-compression and structural-  

 Effects of Structural Regularization The structural regularization based on our hierarchical encoder-decoder with hybrid attention model improves the quality of summaries from two aspects: (1) The summary covers more salient information and contains very few repetitions, which can be seen both qualitatively (Table  1  and Figure  1 ) and quantitatively (Table  5  and Figure  4 ). (  2 ) The model has the ability to shorten a long sentence to generate a more concise one or compress several different sentences to generate a more informative one by merging the information from them. Table  6  shows several examples of abstractive summaries produced by sentence compression in our model. 

 Related Work Recently some work explored the seq2seq models on document summarization, which exhibit some undesirable behaviors, such as inaccurately reproducing factual details, OOVs and repetitions. To alleviate these issues, copying mechanism  (Gu et al., 2016;  has been incorporated into the encoderdecoder architecture to help generate information correctly. Distraction-based attention model Original Text: luke lazarus , a 23-year-old former private school boy , was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in kings cross , inner sydney in may 2013 .(...) Summary: luke lazarus was jailed for at least three years on march 27 for raping an 18-year-old virgin in an alleyway outside his father 's soho nightclub in may 2013 . Original Text: (...) amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner , who was also working .wilkinson , who was a british airways cabin crew attendant , was ordered to pay back a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . (...) Summary: amy wilkinson , 28 , claimed housing benefit and council tax benefit even though she was living in a home owned by her mother and her partner . she was ordered to pay back a total of 17,604 that she claimed over two years when she appeared at south and east cheshire magistrates court last week . Original Text: (...) a grand jury charged durst with possession of a firearm by a felon , and possession of both a firearm and an illegal drug : 5 ounces of marijuana , said assistant district attorney chris bowman , spokesman for the district attorney . millionaire real estate heir robert durst was indicted wednesday on the two weapons charges that have kept him in new orleans even though his lawyers say he wants to go to los angeles as soon as possible to face a murder charge there . his arrest related to those charges has kept durst from being extradited to los angeles , where he 's charged in the december 2000 death of longtime friend susan berman .(...) Summary: durst entered his plea during an arraignment in a new orleans court on weapons charges that accused him of possessing both a firearm and an illegal drug , marijuana . the weapons arrest has kept durst in new orleans even though he is charged in the december 2000 death of a longtime friend . Table  6 : Examples of sentences compression or fusion by our model. The link-through denotes deleting the non-salient part of the original text. The italic denotes novel words or sentences generated by sentences fusion or compression.  (Chen et al., 2016)  and word-level coverage mechanism  (See et al., 2017)  have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied to improve the document summarization performance from global sequence level  (Paulus et al., 2017) . Hierarchical Encoder-Decoder architecture is first proposed by  to train an auto-encoder to reconstruct multi-sentence paragraphs. In summarization field, hierarchical encoder has first been used to alleviate the long dependency problem for long inputs  (Cheng and Lapata, 2016; .  Tan et al. (2017b)  also propose to use a hierarchical encoder to encode multiple summaries produced by several extractive summarization methods, and then decode them into a headline. However, these models don't model the decoding process hierarchically.  Tan et al. (2017a)  first use the hierarchical encoder-decoder architecture on generating multisentences summaries. They mainly focus on incorporating sentence ranking into abstractive document summarization to help detect important sentences. Different from that, our work mainly tends to verify the necessity of leveraging document structure in document summarization and studies how to properly capture the structural properties of document summarization based on the hierarchical architecture to improve the performance of document summarization. 

 Conclusions In this paper we analyze and verify the necessity of leveraging document structure in document summarization, and explore the effectiveness of capturing structural properties of document summarization by importing both structuralcompression and structural-coverage regularization based on the proposed hierarchical encoderdecoder with hybrid attention model. Experimental results demonstrate that the structural regularization enables our model to generate more informative and concise summaries by enhancing sentences compression and coverage. Our model achieves considerable improvement over state-ofthe-art seq2seq-based abstractive methods, especially on long document with long summary. I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 

 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 I 1 I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 I 10 I 11 I 12 I 13 I 14 I 15 I 16 I 17 I 18 I 19 I 20 

 Figure 1 : 1 Figure 1: Comparison of sentence-level attention distributions for the summaries in Table1on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017)  system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document. 

 Figure 2 : 2 Figure 2: Our hierarchical encoder-decoder model with structural regularization for abstractive document summarization. 

 Figure 3 : 3 Figure 3: Comparisons of structural-compression and structural-coverage analysis results on random samples from CNN/Daily Mail datasets, which demonstrate that both the Seq2seq-baseline model and the Hierarchical-baseline model are not yet able to capture them properly, but our model with structural regularizations achieves similar behavior with the gold reference summary. 

 Figure 4 : 4 Figure 4: The structural regularization reduces undesirable repetitions while summaries from the Seq2seq-baseline and the Hierarchical-baseline contains many n-gram repetitions. 

 The hierarchical decoder organizes summary Y sentence by sentence, and realizes each sentence word by word. In this work, both the sentencelevel decoder and word-level decoder use a single layer of unidirectional GRU. The sentence-level decoder receives the document representation d as initial state h ? 0 and predicts sentence represen-tations sequentially by h ? t = GRU (h ? t?1 , r ? t?1 ), where h ? t denotes the hidden state of the tth sum-mary sentence s ? t and r ? t?1 denotes the encoded representation of the previously generated sen- tence s ? t?1 . The word-level decoder receives a sen-tence representation h ? t as initial state h ? t,0 and pre-dicts word representations sequentially by h ? t,k = GRU (h ? t,k?1 , e t,k?1 ) where h ? t,k denotes the hid-den state of word w ? t,k in sentence s ? t and e t,k?1 de- notes the embedding of previously generated word w 

 Table 2 : 2 Rouge F1 scores on the test set. All our ROUGE scores have a 95% confidence interval of at most ?0.25 as reported by the official ROUGE script. . The CNN/DailyMail dataset contains input sequences of about 800 to- kens in average and multi-sentence summaries of up to 200 tokens. The average number of sen- tences in documents and summaries are 42.1 and 3.8, respectively. We use the same version of non-anonymized data (the original text without pre-processing) as See et al. (2017), which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. For all experiments, the word-level encoder and decoder both use 256-dimensional hidden states, and the sentence-level encoder and decoder both 

 Table 3 : 3 Comparison results w.r.t different length of reference summary. < 100 indicates the reference summary has less than 100 words (occupy 94.47% of test set). , which 

 Table 4 : 4 Human evaluation results. Method Informat. Concise Coherent Fluent Seq2seq-b. 2.79  * 2.52  * 2.68  * 3.57 Point-cov 3.17  * 2.92  * 3.00  * 3.54 Our Model 3.67 3.39 3.51 3.70 Method R-1 R-2 R-L strCom strCov Hierarchical-b. 34.95 14.79 32.68 0.22 0.31 +strCom 37.03 16.21 34.44 0.64 0.71 +strCov 39.52 17.12 36.44 0.65 0.87 +hierD 40.30 18.02 37.36 0.68 0.93 * indicates the difference between Our Model and other models are statistic significant (p < 0.05) by two-tailed t-test. 

 Table 5 : 5 Results of adding different components of our method in terms of ROUGE-1, ROUGE-2, ROUGE-L, str-Com (Equation 

			 h 1 h 2 h 3 h 1,1 h 1,2 h 1,3 h 2,1 h 2,2 h 2,3 h 3,1 h 3,2 h 3,3 h t?1 ' h t ,k?1 ' ? 1...t
