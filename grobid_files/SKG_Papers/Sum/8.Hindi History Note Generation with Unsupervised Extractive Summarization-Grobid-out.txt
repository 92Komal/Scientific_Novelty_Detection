title
Hindi History Note Generation with Unsupervised Extractive Summarization

abstract
In this work, the task of extractive single document summarization applied to an education setting to generate summaries of chapters from grade 10 Hindi history textbooks is undertaken. Unsupervised approaches to extract summaries are employed and evaluated. Tex-tRank, LexRank, Luhn and KLSum are used to extract summaries. When evaluated intrinsically, Luhn and TextRank summaries have the highest ROUGE scores. When evaluated extrinsically, the effective measure of a summary in answering exam questions, TextRank summaries performs the best.

Introduction Our task is to apply text summarization to generate notes for school students where the medium of instruction is Hindi. The motivation for this work is that students studying under the Indian Central Board of Secondary Education (CBSE) have a lack of additional resources given their medium of instruction. Online resources are limited, with most reference guide material being published in English. Given the vast quantities of information that students are made to memorize, we believe that our tool will help provide students with an outline, a text summary, that could serve as both a big picture introduction and a pre-exam study guide. We focus on this task as each year over 18 million students give the grade ten exam. As Hindi is a low resource language, we believe that such a tool could help students learn better. From prior research  (Verma et al., 2019)  on comparative text summarization in English and Hindi, we see that summarization results vary drastically for different languages and subject matters. While extensive research has been done for summarization techniques in English  (Luhn, 1958; Edmundson, 1969; Carbonell and Goldstein, 1998 ; Pal and * These authors contributed equally to this work.  Saha, 2014; Erkan and Radev, 2004) , directly applying said methods to Hindi text performs poorly  (Verma et al., 2019) . Frequency, graph and feature based approaches have been investigated previously to extract summaries from Hindi text and have shown to perform well on news documents  (Vijay et al., 2017) . Rule based methods  (Gupta and Garg, 2016) , and improvements to graph based methods incorporating semantic information from the text  (Kumar et al., 2015)  perform well for Hindi documents from various domains. We wish to address the task of extractive text summarization in Hindi as it applies to learning history in an education setting for school students using unsupervised algorithms. The main reason behind choosing unsupervised methods for this task is that these algorithms do not require a dedicated training set annotated by individuals with subject specific knowledge. Secondly, employing a supervised approach for a particular domain constrains the portability of the trained model to be applied on different domains. Furthermore, the efficiency or goodness of the generated summaries for a particular task rely on accurate and reliable human annotated summaries used for training. To the best of our knowledge, there exists no work that addresses Hindi text summarization in the academic domain as a note generating tool for students. This made it difficult to compare our approaches with existing work that deals with different domains of text data. In this work, we investigate unsupervised graph, term frequency and probability based single document summarization methods. Our work will build on previous linguistic analyses (for instance, no direct way to identify proper nouns) in Hindi  (Paul et al., 2013)  to deal with the nuances of summarizing history written in Hindi  (Garg et al., 2012) . Our code is publicly available on GitHub 1 . 

 Materials We used the grade 10 Hindi history textbook  (NCERT, 2018 (NCERT, -2019  prescribed by the CBSE and published by the National Council of Educational Research and Training (NCERT) as the dataset. The Textbook is available in PDF format and is about 200 pages in length. There are 8 chapters (articles) in the book. Each chapter contains around 400 sentences comprising about 18 words each. This amounts to approximately 7200 words per chapter. To evaluate generated summaries, reference summaries of length 75 sentences are manually created for each chapter using the exact sentences from the textbook (extractive summarization). The annotators drafting the reference summaries are proficient in Hindi, have studied history at a high school level and are familiar with the course content and exam structure. In order to perform an extrinsic evaluation, we considered questions from the three most recent exam papers, from 2017-2019, and their corresponding rubrics 2 . The exams contain 3 types of questions -very short answer questions (1 mark each), short answer questions (3 marks each) and long answer questions (5 marks each) requiring 1, 3 and 5 sentences from the text respectively. There are a total of 35 questions in these exam papers. A sample very short question from the 2017 examination is as follows: A student's response that would score one full point is as follows: 

 Methodology The basic idea behind the task of extractive summarization is that individual sentences in the source document are scored and ranked to extract the top n sentences as a summary. In this work, four unsupervised methods are investigated to score and rank the input document sentences. The methods used here are KLSum  (Aria and Vanderwende, 2009 ), Luhn Summarization (Luhn, 1958 ), TextRank (Mihalcea and Tarau, 2004  and LexRank  (Erkan and Radev, 2004) . The summaries generated by these methods are evaluated and compared against each other intrinsically and extrinsically. TextRank and LexRank are graph based approaches. KLSum utilizes a probabilistic approach. Luhn uses a naive ranking algorithm based on word significance. 

 KLSum Kullback-Leibler summarization (KLSum)  (Aria and Vanderwende, 2009 ) is a probabilistic take on the extractive summarization problem. The basic idea here is to extract a summary, a set of sentences from the source document, whose unigram distribution is as close to the unigram distribution of the source document as possible. The closeness between the source and summary document distributions is determined by the KL divergence  (Kullback and Leibler., 1951 ) KL(D||S), where D(w) and S(w) are the unigram distributions of the word w in the source D and summary S document respectively. KL(D||S) = w D(w)(log(D(w)) ? log(S(w)) (1) The empirical unigram distribution of a document is the term frequency of words in the given document which is computed as : Here, L is the maximum number of words in the summary S. Since optimizing the above objective is exponential in the number of sentences in the source document, a greedy approach is taken. Starting with an empty summary, the summary is extracted iteratively. At each iteration, the sentence which results in minimum KL(D||S) is added to the summary until the intended number of sentences is reached. 

 Luhn Summarization Luhn summarization  (Luhn, 1958 ) is a simple and naive summarization algorithm where the relative significance of each sentence in the source document is considered for selection in the summary. The basic idea exploited in this method is that an author of a document writing about a concept tends to repeat the same words to represent a specific notion. When such significantly repeating words are positioned relatively closer in a document, within a sentence for example, the sentence as a whole becomes significant enough to be considered in a summary. The relative significance of each sentence is captured with the number of significant words and their physical proximity within a sentence. Each sentence is grouped into clusters beginning and ending with significant words. These first and last significant words of clusters are significantly related if the physical distance between them, intervened by insignificant words, is under a threshold. If more than one such cluster is found in a sentence, the cluster with the highest significance factor is assigned to the sentence. The sentences are then ranked relative to the other to generate the summary. Numerically, a word is considered significant if its term frequency is more than a specified threshold. The significance factor of a cluster C in a sentence is computed as follows: Signif icance(C) = # of significant words in C # of words in C (4) 

 TextRank TextRank  (Mihalcea and Tarau, 2004 ) is a graph based approach which scores sentences in the given document based on the PageRank  (Page et al., 1999)  algorithm. The basic principle here is that sentences within the document recommend each other and the sentences with the highest recommendation scores are considered to be in the generated summary. This involves constructing a graphical representation of the document, G(V, E), where each sentence in the document is a vertex V linked to all other vertices through edges E in the undirected graph. The edge between two vertices i and j are weighted by a similarity metric w ij to capture the recommendation between sentences s i and s j which is calculated as follows: wij = # of w k |w k si, sj log(|si|) + log(|sj|) (5) Here, w k are shared tokens between sentences s i and s j . The PageRank algorithm is run on this constructed graph until convergence to find the importance of each vertex as per the update equation below. W S(vi) = (1 ? d) + d v j ?In(v i ) wji v k ?Out(v j ) w jk W S(vj) (6) In the above equation, the importance score W S of vertex v i is a function of damping factor d, incoming edge weights to a given vertex v i , w ji , and importance score W S(v j ) of neighbouring vertex v j . The vertices are ranked based on importance and the top n sentences from the document are taken as the summary. 

 LexRank Like TextRank, LexRank  (Erkan and Radev, 2004 ) is a graph based sentence scoring algorithm based on the PageRank algorithm. However, LexRank differs in the way recommendations between sentences are computed. wij = w s i ,s j tfw,s i * tfw,s j * idf 2 w w s i (tfw,s i idfw) 2 w s j (tfw,s j idfw) 2 (7) tfw,s i = # of times the word w occurs in sentence si # of words in sentence si (8) idfw = log # of sentences in the document (1 + # of sentences with term w) The similarity metric w ij , between sentences s i and s j , is the idf-modified-cosine similarity  (Erkan and Radev, 2004 ) computed between N dimensional vector representation of sentences.N is the number of unique terms in the document. For each word present in a sentence, the corresponding dimension in the N dimensional vector is set to the idf value of the word to construct the vector mapping of the sentence. 

 Results The machine generated summaries are evaluated using intrinsic and extrinsic measures. Intrinsic (quantitative) evaluation uses ROUGE score  (Lin, 2004)  which is a recall based metric that compares similar n-grams in generated summaries against the handmade summaries. It is found that ROUGE based evaluation correlates with human based evaluation in comparing machine generated summaries with ideal summaries  (Lin and Hovy, 2003) . Hence, we consider ROUGE-1 and ROUGE-2 scores for this evaluation, which is the percentage of overlapping unigrams and bigrams respectively between the generated and handmade summaries. The main idea of this work is to create a study/revision guide for students to help them understand the study material and do well on exams. Hence, the ability to answer exam questions is an indicator of a good summary. The Extrinsic (qualitative) evaluation measures how good the summary is in helping students perform well in the history exam. This is carried out by going through the summaries generated by the above mentioned algorithms and making a decision on how many points can be scored on very short and short answer questions given only the sentences in the summary. The scoring is done manually by human evaluators who refer to the examination grading rubric which is available online. Length of the summary is an important factor to be considered when generating summaries. The challenge is to balance recall and precision, i.e. to capture as much important information as possible from the whole document while avoiding the inclusion of superfluous information. Such a summary with the right length should make for a faster and better learning experience for students. Fig.  1  shows the relationship between the length of summary, in sentences, with respect to the shared unigrams with the reference summary. While longer summaries have more overlap, the decrease in slope indicates that a decreasing percentage of added sentences match the reference. Thus, 75 sentences was selected as the model summary length.   

 Intrinsic Evaluation Results Table  1  describes the ROUGE scores of different algorithm generated summaries when compared to the human generated reference summaries. We see Luhn based summaries have the highest ROUGE-1 and ROUGE-2 scores of 0.74 and 0.45 when compared to other algorithms. In this case, ROUGE-1 and ROUGE-2 follow a similar distribution. 

 Extrinsic Evaluation Results The generated summaries were evaluated based on their ability to answer questions on three years' (2017-2019) history exam papers in Hindi. We compare their exam scores with the baseline, the exam scores of the hand generated reference summaries. This comparison is done by evaluators who have studied Hindi and history at a high school level while referring to the grading rubric provided by the CBSE board. It is important to note that the full text is sufficient to answer all of the exam questions scoring 100%. The reference summaries scored 67.3% outperforming summaries of Tex-tRank scoring 53.1%, LexRank scoring 40.8%, Luhn scoring 38.8% and KLSum scoring 46.9% as shown in Table  2 . 

 Discussion When evaluated extrinsically on question answering ability, we see that human generated reference summaries are able to score better on exam questions when compared to machine generated summaries. Among the unsupervised approaches, Tex-tRank scores the most on exam questions, 53.1%. Since TextRank is able to answer approximately 80% of exam questions that the reference summaries answer, we believe that note generation by TextRank provides a good supplementary study tool for students. We observed the impact of Hindi on the ROUGE metric. The presence of stop words, ambiguous pronouns and other commonly used connecting terms in Hindi artificially raise the n-gram overlap without adding useful information. For example, consider the two sentences below: The two sentences have completely different meanings, sharing only subject, Gandhi in common. The English sentences have two unigrams in common, 'Gandhi' and 'the' out of a total of thirteen unique unigrams, approximately a 15% overlap. On the other hand, the Hindi sentences have a total of five unigrams in common out of a total of fourteen unique unigrams, approximately a 36% overlap. This aspect of the Hindi language, with an abundance of connecting terms, would also raise the ROUGE metric of sentences which need not convey useful information. We reevaluated the importance of the ROUGE score for the chosen task. We notice that a good ROUGE score is not a good indicator of a summary's ability to serve as a study aid. This is evident from the extrinsic evaluation. Luhn summarization, which has the highest ROUGE-1 score (0.74), performs poorly on the question answering task scoring only 38.8%. Conversely, KLSum having the lowest ROUGE-1 score (0.39) performs better than Luhn summarization extrinsically, obtaining approximately 47%. This relationship between the ROUGE and exam scores of the summaries can be confirmed by the Spearman's rho and Kendall's tau coefficients  (Yue et al., 2002)  , which are -0.4 and -0.33 respectively. The negative coefficients indicate a weak correlation between the summary's ROUGE score and its question answering ability. This shows that, in addition to ROUGE, it is important to formulate evaluation mechanisms that align with chosen application to evaluate machine generated summaries. We noticed that machine generated summaries have sentences with ambiguous subjects. While the algorithms may identify an important fact, it cannot attribute it to a subject. Consider the following sentence: When the machine generated summary contains only the second sentence it is able to answer the question "What caused the school rebellion?" (expelling the girl from school) but cannot identify the subject (school's principal) who carried out the action without the preceding sentence. This is a structure we see often in Hindi where one sentence in English corresponds to two in Hindi. In the English version of the text, the fact is stated as "The principal, also a colon, expelled her." As the input text documents to the models were not pre-processed, we observed models treating the same entity differently. For instance, the tokens Gandhi and Gandhi-ji. The addition of an honorific suffix 'ji' results in both terms being treated as different. Since the rule of removing the suffix 'ji' applies only to proper nouns, we cannot generalize this as a stemmer rule. We believe that a TextRank based summarization tool would prove effective for other subjects whose exam questions test factual knowledge like Geography or Biology. However, further testing is required before its portability can be validated. Also, we believe the project would benefit from an Entity Recognizer, as a pre-processing step, to solve both ambiguous subjects problems and the ambiguity caused by the honorific suffixes in the summaries. Nevertheless, we believe that this project represents a step in the right direction towards providing a note generation tool for students in Hindi medium schools. tf t,d = # of times the term t occurs in document d Total # of terms in document d (2) Here, tf t,d represents term t frequency in text document d . The term frequencies are smoothened to ensure non-zero values. Mathematically, the optimization problem is defined as below: S * = min S:words(S)?L KL(D||S)(3) 

 Figure 1 : 1 Figure 1: Overlapping unigrams vs summary length of generated summary. 

 Table 1 : 1 Intrinsic Evaluation: Comparison of ROUGE scores for LexRank, TextRank, Luhn and KLSum summaries compared against the reference summaries. Algorithms ROUGE-1 ROUGE-2 LexRank 0.56 0.25 TextRank 0.72 0.44 Luhn 0.74 0.45 KLSum 0.39 0.17 Algorithms Exam scores LexRank 40.8% TextRank 53.1% Luhn 38.8% KLSum 46.9% Reference Summaries 67.3% 

 Table 2 : 2 Extrinsic Evaluation: Comparison of exam scores for reference summaries, LexRank, TextRank, Luhn, and KLSum summaries. 

			 Code: https://github.com/dhineshkumar-r/Unsupervised-Extractive-Summarization-Hindi-Note-Generation 

			 NCERT Solutions: https://byjus.com/ncert-solutions/
