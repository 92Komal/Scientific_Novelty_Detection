title
Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization

abstract
Most traditional summarization methods treat their outputs as static and plain texts, which fail to capture user interests during summarization because the generated summaries are the same for different users. However, users have individual preferences on a particular source document collection and obviously a universal summary for all users might not always be satisfactory. Hence we investigate an important and challenging problem in summary generation, i.e., Interactive Personalized Summarization (IPS), which generates summaries in an interactive and personalized manner. Given the source documents, IPS captures user interests by enabling interactive clicks and incorporates personalization by modeling captured reader preference. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Evaluation results in ROUGE metrics indicate the comparable performance between IPS and the best competing system but IPS produces summaries with much more user satisfaction according to evaluator ratings. Besides, low ROUGE consistency among these user preferred summaries indicates the existence of personalization.

Introduction In the era of information explosion, people need new information to update their knowledge whilst information on Web is updating extremely fast. Multidocument summarization has been proposed to address such dilemma by producing a summary de-livering the majority of information content from a document set, and hence is a necessity. Traditional summarization methods play an important role with the exponential document growth on the Web. However, for the readers, the impact of human interests has seldom been considered. Traditional summarization utilizes the same methodology to generate the same summary no matter who is reading. However, users may have bias on what they prefer to read due to their potential interests: they need personalization. Therefore, traditional summarization methods are to some extent insufficient. Topic biased summarization tries for personalization by pre-defining human interests as several general categories, such as health or science. Readers are required to select their possible interests before summary generation so that the chosen topic has priority during summarization. Unfortunately, such topic biased summarization is not sufficient for two reasons: (1) interests cannot usually be accurately pre-defined by ambiguous topic categories and (2) user interests cannot always be foreknown. Often users do not really know what general ideas or detail information they are interested in until they read the summaries. Therefore, more flexible interactions are required to establish personalization. Due to all the insufficiencies of existed summarization approaches, we introduce a new multidocument summarization task of Interactive Personalized Summarization (IPS) and a novel solution for the task. Taking a document collection as input, the system outputs a summary aligned both with source corpus and with user personalization, which is captured by flexible human?system interactions. We build an experimental system on 4 real datasets to verify the effectiveness of our methods compared with 4 rivals. The contribution of IPS is manifold by addressing following challenges: ? The 1st challenge for IPS is to integrate user interests into traditional summary components. We measure the utilities of these components and combine them. We formulate the task into a balanced optimization framework via iterative substitution to generate summaries with maximum overall utilities. ? The 2nd challenge is to capture user interests through interaction. We develop an interactive mechanism of "click" and "examine" between readers and summaries and address sparse data by "click smoothing" under the scenario of few user clicks. We start by reviewing previous works. In Section 3 we provide IPS overview, describe user interaction and optimize component combination with personalization. We conduct empirical evaluation and demonstrate the experimental system in Section 4. Finally we draw conclusions in Section 5. 

 Related Work Multi-Document Summarization (MDS) has drawn much attention in recent years and gained emphasis in conferences such as ACL, EMNLP and SIGIR, etc. General MDS can either be extractive or abstractive. The former assigns salient scores to semantic units (e.g. sentences, paragraphs) of the documents indicating their importance and then extracts top ranked ones, while the latter demands information fusion(e.g. sentence compression and reformulation). Here we focus on extractive summarization. Centroid-based method is one of the most popular extractive summarization method. MEAD  and NeATS  (Lin and Hovy, 2002)  are such implementations, using position and term frequency, etc. MMR  (Goldstein et al., 1999)  algorithm is used to remove redundancy. Most recently, the graph-based ranking methods have been proposed to rank sentences or passages based on the "votes" or "recommendations" between each other. The graphbased methods first construct a graph representing the sentence relationships at different granularities and then evaluate the saliency score of the sentences based on the graph. TextRank  (Mihalcea and Tarau, 2005)  and LexPageRank  (Erkan and Radev, 2004)  use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. improve the graph-ranking algorithm by differentiating intradocument and inter-document links between sentences (2007b) and incorporate cluster information in the graph model to evaluate sentences  (2008) . To date, topics (or themes, clusters) in documents have been discovered and used for sentence selection for topic biased summarization  (Wan and Yang, 2008; Gong and Liu, 2001) .  Wan et al.  have proposed a manifold-ranking method to make uniform use of sentence-to-sentence and sentence-totopic relationships to generate topic biased summaries (2007a).  Leuski et al. in (2003)  pre-define several topic concepts, assuming users will foresee their interested topics and then generate the topic biased summary. However, such assumption is not quite reasonable because user interests may not be forecasted, or pre-defined accurately as we have explained in last section. The above algorithms are usually traditional extensions of generic summarizers. They do not involve interactive mechanisms to capture reader interests, nor do they utilize user preference for personalization in summarization.  have proposed a summarization biased to neighboring reading context through anchor texts. However, such scenario does not apply to contexts without human-edited anchor texts like Wikipedia they have used. Our approach can naturally and simultaneously take into account traditional summary elements and user interests and combine both in optimization under a wider practical scenario. 

 Interactive Personalized Summarization Personalization based on user preference can be captured via various alternative ways, such as eyetracking or mouse-tracking instruments used in  (Guo and Agichtein, 2010) . In this study, we utilize interactive user clicks/examinations for personalization. Unlike traditional summarization, IPS supports human?system interaction by clicking into the summary sentences and examining source contexts. The implicit feedback of user clicks indicates what they are interested in and the system collects preference information to update summaries if readers wish to. We obtain an associated tuple <q, c> between a clicked sentence q and the examined contexts c. As q has close semantic coherence with neighboring contexts due to consistency in human natural language, we consider a window of sentences centered at the clicked sentence q as c, which is a bag of sentences. The window size k is a parameter to set. However, click data is often sparse: users are not likely to click more than 1/10 of total summary sentences within a single generation. We amplify these tiny hints of user interest by click smoothing. We change the flat summary structure into a hierarchical organization by extracting important semantic units (denoted as u) and establishing linkage between them. If the clicked sentence q contains u, we diffuse the click impact to the correlated units, which makes a single click perform as multiple clicks and the sparse data is smoothed. Problem Formulation Input: Given the sentence collection D decomposed by documents, D = {s 1 , s 2 , . . . , s |D| } and the clicked sentence record Q = {q 1 , q 2 , . . . }, we generate summaries in sentences. A user click is associated with a tuple <q, (u), c> where the existence of u depends on whether q contains u. The collection of semantic units is denoted as M = {u 1 , u 2 , . . . , u |M | }. Output: A summary S as a set of sentences {s 1 , s 2 , . . . , s |S| } and S ? D according to the prespecified compression rate ? (0 < ? < 1). After the overview and formulation of IPS problem, we move on to the major components of User Interaction and Personalized Summarization. 

 User Interaction Hypertexify Summaries. We hypertexify the summary structure by establishing linkage between semantic units. There are several possible formats for semantic units, such as words or n-grams, etc. As single words are proved to be not illustrative of semantic meanings  (Zhao et al., 2011)  and n-grams are rigid in length, we choose to extract semantic units at a phrase granularity. Among all phrases from source texts, some are of higher importance to attract user interests, such as hot concepts or popular event names. We utilize the toolkit provided by  (Zhao et al., 2011)  based on graph proximity LDA  (Blei et al., 2003)  to extract key phrases and their corresponding topic. A topic T is represented by {(u 1 , ?(u 1 , T )), (u 2 , ?(u 2 , T )), . . . } where ?(u, T ) is the probability of u belonging to topic T . We invert the topic-unit representation in Table  1 , where each u is represented as a topic vector. The correlation corr(.) between u i , u j is measured by cosine similarity sim(.) on topic distribution vector u i , u j . corr(u i , u j ) = sim topic ( u i , u j ) (1) Table  1 : Inverted representation of topic-unit vector. u 1 ?(u 1 , T 1 ) ?(u 1 , T 2 ) . . . ?(u 1 , T n ) u 2 ?(u 2 , T 1 ) ?(u 2 , T 2 ) . . . ?(u 2 , T n ) . . . . . . . . . . . . . . . u |M | ?(u |M | , T 1 ) ?(u |M | , T 2 ) . . . ?(u |M | , T n ) When the summary is hypertexified by established linkage, users click into the generated summary to examine what they are interested in. A single click on one sentence become multiple clicks via click smoothing when the indicative function I(u|q) = 1. I(u|q) = 1 q contains u; 0 otherwise. (2) The click smoothing brings pseudo clicks q associated with u and contexts c . The entire user feedback texts A from q can be written as: A(q) = I(u|q) |M | j=1 corr(u , u)(u +?c )+?c (3) where ? is the weight tradeoff between u and associated contexts c. If I(u|q) = 0, only the examined context c is feedbacked for user preference; otherwise, correlative contexts with u are taken into consideration, which is a process of impact diffusion. 

 Personalized Summarization Traditional summarization involves two essential requirements: (1) coverage: the summary should keep alignment with the source collection, which is proved to be significant  (Li et al., 2009) . (2) diversity: according to MMR principle  (Goldstein et al., 1999)  and its applications  (Wan et al., 2007b; Wan and Yang, 2008) , a good summary should be concise and contain as few redundant sentences as possible, i.e., two sentences providing similar information should not both present. According to our investigation, we observe that a well generated summary should properly consider a key component of (3) user interests, which captures user preference to summarize what they are interested in. All above requirements involve a measurement of similarity between two word distributions ? 1 and ? 2 . Cosine, Kullback-Leibler divergence D KL and Jensen Shannon divergence D JS are all able to measure the similarity, but  (Louis and Nenkova, 2009)  indicate the superiority of D JS in summarization task. We also introduce a pair of decreasing/increasing logistic functions, L 1 (x) = 1/(1 + e x ) and L 2 (x) = e x /(1 + e x ), to map the divergence into interval [0,1]. V is the vocabulary set and tf denotes the term frequency for word w. D JS (? 1 ||? 2 ) = 1 2 [D KL (? 1 ||? 2 )+D KL (? 2 ||? 1 )] where D KL (? 1 ||? 2 ) = k?V p(w|? 1 )log p(w|? 1 ) p(w|? 2 ) where p(w|?) = tf (w, ?) w tf (w , ?) . Modeling Interest for User Utility. Given a generated summary S, users tend to scrutinize texts relevant to their interests. Texts related to user implicit feedback are collected as A = |Q| i=1 A(q i ). Intuitively, the smaller distance between the word distribution of final summary (? S ) and the word distribution of user preference (? A ), the higher utility of user interests U user (S) will be, i.e., U user (S) = L 1 (D JS (? S ||? A )). (4) We model the utility of traditional summarization U trad (S) using a linear interpolation controlled by parameter ? between utility from coverage U c (S) and utility U d (S) from diversity: U trad (S) = U c (S) + ? ? U d (S). (5) Coverage Utility. The summary should share a closer word distribution with the source collection  (Allan et al., 2001; Li et al., 2009) . A good summary focuses on minimizing the loss of main information from the whole collection D. Utility from coverage U c (S) is defined as follows and for coverage utility, smaller divergence is desired. U c (S) = L 1 (D JS (? S ||? D )). (6) Diversity Utility. Diversity measures the novelty degree of any sentence s compared with all other sentences within S, i.e., the distances between all other sentences and itself. Diversity utility U d (S) is an average novelty score for all sentences in S. For diversity utility, larger distance is desired, and hence we use the increasing function L 2 as follows: U d (S) = 1 |S| s?S L 2 (D JS (? s ||? (S?s) ) ). (7) 

 Balanced Optimization Framework A well generated summary S should be sufficiently aligned with the original source corpus, and also be optimized given the user interests. The utility of an individual summary U(S) is evaluated by the weighted combination of these components, controlled by parameter ? for balanced weights. U(S) = U trad (S) + ? ? U user (S) (8) Given the sentence set D and the compression rate ?, there are ?|D| out of |D| possibilities to generate S. The IPS task is to predict the optimized sentence subset of S * from the space of all combinations. The objective function is as follows: S * = argmax S U(S). (9) As U(S) is measured based on preferred interests from user interaction within a generation in our system, we extract S iteratively to approximate S * , i.e, maximize U(S) based on the user feedbacks from the interaction sessions. Each session is an iteration. We use a similar framework as we have proposed in  (Yan et al., 2011) . During every session, the top ranked sentences are strong candidates for the summary to generate and the rank methodology is based on the metrics U(.). The algorithm tends to highly rank sentences which are with both coverage utility and interest utility, and are diversified in balance: we rank each sentence s according to U(s) under such metrics. Consider S (n?1) generated in the (n-1)-th session which consists of top ?|D| ranked sentences, as well as the top ?|D| ranked sentences in the n-th iteration (denoted by O (n) ), they have an intersection set of Z (n) = S n?1 ?O n . There is a substitutable sentence set X (n) = S (n?1) ? Z  (n)  and a new candidate sentence set Y (n) = O (n) ? Z  (n)  . We substitute x  (n)  sentences with y  (n)  , where x (n) ? X  (n)  and y (n) ? Y  (n)  . During every iteration, our goal is to find a substitutive pair <x, y> for S: <x, y> : X ? Y ? R. To measure the performance of such a substitution, a discriminant utility gain function ?U x,y ?U (n) x (n) ,y (n) = U(S (n) ) ? U(S (n?1) ) = U((S (n?1) ? x (n) ) ? y (n) ) ? U(S (n?1) ) (10) is employed to quantify the penalty. Therefore, we predict the substitutive pair by maximizing the gain function ?U x,y over the state set R, with a size of Y k=0 A k X C k Y , where <x, y>? R. Finally the objective function of Equation (  9 ) changes into maximization of utility gain by substitute x with ? during each iteration: < x, ? >= argmax x?X ,y?Y ?U x,y . (11) Note that the objectives of interest utility optimization and traditional utility optimization are not always the same because the word distributions in these texts are usually different. The substitutive pair <x, y> may perform well based on the user preference component while not on the traditional summary part and vice versa. There is a tradeoff between both user optimization and traditional optimization and hence we need to balance them by ?. The objective Equation (  11 ) is actually to maximize ?U(S) from all possible substitutive pairs between two iteration sessions to generate S. The algorithm is shown in Algorithm 1. The threshold is set at 0.001 in this study. 

 Experiments and Evaluation 

 Datasets IPS can be tested on any document set but a tiny corpus to summarize may not cover abundant effective interests to attract user clicks indicating their end if 21: end while preference. Besides, the scenario of small corpus is not quite practical for the exponential growing web. Therefore, we test IPS on large real world datasets. We build 4 news story sets which consist of documents and reference summaries to evaluate our proposed framework empirically. We downloaded 5197 news articles from 10 selected sources. As shown in Table  2 , three of the sources are in UK, one of them is in China and the rest are in US. We choose them because many of these websites provide handcrafted summaries for their special reports, which serve as reference summaries. These events belong to different categories of Rule of Interpretation (ROI) (Kumaran and  Allan, 2004) . Statistics are in Table  3 . 

 Experimental System Setups ? Preprocessing. Given a collection of documents, we first decompose them into sentences. Stop-words are removed and words stemming is performed. Then the word distributions can be calculated. ? User Interface Design. Users are required to specify the overall compression rate ? and the system extracts ?|D| sentences according to user utility Figure  1 : A demonstration system for Interactive Personalized Summarization when compression rate ? is specified (e.g. 5%). For convenience of browsing, we number the selected sentences (see in part 3). Extracted semantic units, such as "drilling mud", are in bold and underlined format (see in part 1). When the user clicks a sentence (part 4), the clicked sentence ID is kept in the click record (part 2). Mis-clicked records revocation can be operated by clicking the deletion icon "X" (see in part 3). Once a sentence is clicked, user can track the sentence into the popup source document to examine the contexts. The selected sentences are highlighted in the source documents (see in part 5). and traditional utility. User utility is obtained from interaction. The system keeps the clicked sentence records and calculates the user feedback by Equation (3) during every session. Consider sometimes users click into the summary due to confusion or mis-operations, but not their real interests. The system supports click records revocation. More details of the user interface is demonstrated in Figure  1 . 

 Evaluation Metrics We include both subjective evaluation from 3 evaluators based on their personalized interests and preference, and the objective evaluation based on the widely used ROUGE metrics . Evaluator Judgments Evaluators are requested to express an opinion over all summaries based on the sentences which they deem to be important for the news. In general a summary can be rated in a 5-point scale, where "1" for "terrible", "2" for "bad", "3" for "normal", "4" for "good" and "5" for "excellent". Evaluators are allowed to judge at any scores between 1 and 5, e.g. a score of "3.3" is adopted when the evaluator feels difficult to decide whether "3" or "4" is more appropriate but with preference towards "3". 

 ROUGE Evaluation The DUC usually officially employs ROUGE measures for summarization evaluation, which measures summarization quality by counting overlapping units such as the N-gram, word sequences, and word pairs between the candidate summary and the reference summary. We use ROUGE-N as follows: ROUGE-N = S?{RefSum} N-gram?S Count match (N-gram) S?{RefSum} N-gram?S Count (N-gram) where N stands for the length of the N-gram and N-gram?RefSum denotes the N-grams in the reference summaries while N-gram?CandSum denotes the Ngrams in the candidate summaries. Count match (Ngram) is the maximum number of N-gram in the candidate summary and in the set of reference summaries. Count (N-gram) is the number of N-grams in the reference summaries or candidate summary. According to , among all sub-metrics in ROUGE, ROUGE-N (N=1, 2) is relatively simple and works well. In this paper, we evaluate our experiments using all methods provided by the ROUGE package (version 1.55) and only report ROUGE-1, since the conclusions drawn from different methods are quite similar. Intuitively, the higher the ROUGE scores, the similar two summaries are. 

 Algorithms for Comparison We implement the following widely used multidocument summarization algorithms as the baseline systems, which are all designed for traditional summarization without user interaction. For fairness we conduct the same preprocessing for all algorithms. Random: The method selects sentences randomly for each document collection. Centroid: The method applies MEAD algorithm  to extract sentences according to the following parameters: centroid value, positional value, and first-sentence overlap. GMDS: The Graph-based MDS proposed by  (Wan and Yang, 2008)  first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality. IPS ini : The initial generated summary from IPS merely models coverage and diversity utility, which is similar to the previous work described in  (Allan et al., 2001)  with different goals and frameworks. IPS: Our proposed algorithms with personalization component to capture interest by user feedbacks. IPS generates summaries via iterative sentence substitutions within user interactive sessions. RefSum: As we have used multiple reference summaries from websites, we not only provide ROUGE evaluations of the competing systems but also of the reference summaries against each other, which provides a good indicator of not only the upper bound ROUGE score that any system could achieve, but also human inconsistency among reference summaries, indicating personalization. 

 Overall Performance Comparison We take the average ROUGE-1 performance and human ratings on all sets. The overall results are shown in Figure  2  and details are listed in Tables 4?6. From the results, we have following observations: ? Random has the worst performance as expected, both in ROUGE-1 scores and human judgements. ? The ROUGE-1 and human ratings of Centroid and GMDS are better than those of Random. This is mainly because the Centroid based algorithm takes into account positional value and first-sentence overlap, which facilitates main aspects summarization and PageRank-based GMDS ranks the sentence using eigenvector centrality which implicitly accounts for information subsumption among all sentences. ? In general, the GMDS system slightly outperforms Centroid system in ROUGE-1, but the human judgements of GMDS and Centroid are of no significant difference. This is probably due to the difficulty ? The results of ROUGE-1 and ratings for IPS ini are better than Random but worse than Centroid and GMDS. The reason in this case may be that IPS ini does not capture sufficient attributes: coverage and diversity are merely fundamental requirements. ? Traditional summarization considers sentence selection based on corpus only, and hence neglects The ROUGE-1 performance for IPS is not as ideal as that of GMDS. This situation may result from the divergence between user interests and general information provided by mass media propaganda, which again motivates the need for personalization. Although the high disparities between different human evaluators have been observed in  (Gong and Liu, 2001) , we still examine the consistency among 3 evaluators and their preferred summaries to prove the motivation of personalization in our work. 

 Consistency Analysis for Personalization The low ROUGE-1 scores of RefSum indicate the inconsistency among reference summaries. We conduct personalization analysis from two perspectives: (1) human rating consistency and (2) content consistency among human supervised summaries. We calculate the mean and variance of rating variations among evaluator judgements, listed in Table The high rating consistency of IPS indicates people tend to favor summaries generated according to their interests. We next examine content consistency of these summaries with high rating consistency. As shown in Table  9 , although highly scored, these human supervised summaries still have low content consistency (especially Evaluator 2). The low content consistency between RefSum and supervised summaries shows reader have individual personalization. Note that the inconsistency among evaluators is larger than that between RefSum and supervised summaries, indicating interests take a high proportion in evaluator supervised summaries. 

 Parameter Settings ? controls coverage/diversity tradeoff. We tune ? on IPS ini and apply the optimal ? directly in IPS. According to the statistics in  (Yan et al., 2010) , the semantic coherent context is about 7 sentences. Therefore, we empirically choose k=3 for the examined context window. The number of topics is set at n=50. We assign an equal weight (? = 1) to semantic units and examined contexts according to analogical research of summarization from implicit feedbacks via clickthrough data  (Sun et al., 2005) . ? is the key parameter in IPS approach, controlling the weight of user utility during the process of interactive personalized summarization. Through Figure  3 , we see when ? is small In Figure  4  we examine how ? attracts user clicks and regeneration counts until satisfaction. As the result indicates, both counts increase as ? increases. When ? is small (from 0.01 to 0.1), readers find no more interesting aspects through clicks and regenerations and stop due to the bad user experience. As ? increases, the system mines more relevant sentences according to personalized interests and hence attracts user clicks and intention to regenerate.  

 Conclusion We present an important and novel summarization problem, Interactive Personalized Summarization (IPS), which generates summaries based on human?system interaction for "interests" and personalization. We formally formulate IPS as a combination of user utility and traditional summary utility, such as coverage and diversity. We implement a system under such framework for experiments on real web datasets to compare all approaches. Through our experiments we notice that user personalization of interests plays an important role in summary generation, which largely increase human ratings due to user satisfaction. Besides, our experiments indicate the inconsistency between user preferred summaries and reference summaries measured by ROUGE, and hence prove the effectiveness of personalization. Algorithm 1 1 Regenerative Optimization 1: Input: D, , ? 2: for all s ? D do 3: calculate U trad (s) 4: end for 5: S ? top ?|D| ranked sentences 6: while new generation=TRUE do 7: collect clicks and update utility from U to U ? Z, Y ? O ? Z 15: for all <x, y> pair where x ? X , y ? Y do 16: ?U x,y = U((S ? x) ? y) ? U(S) 
