title
Translation Quality Estimation by Jointly Learning to Score and Rank

abstract
The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits crosslingual sentence embeddings from pretrained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.

Introduction The translation quality estimation (QE) task  (Fonseca et al., 2019)  aims to evaluate the quality of a translation based on the translation and the source sentence without using reference translations. The QE task includes word-level QE, sentence-level QE, document-level QE and QE as a Metric tasks. The QE as a Metric task requires QE models to score a translation on the sentence level similar to the sentence-level QE task, but these two tasks are different as the goal of the sentence-level QE task  (Martins et al., 2017)  is to predict the percentage of edits needed to fix the translation for post-editing purposes while the goal of the QE as a Metric task is to estimate the general quality of the translation like machine translation (MT) evaluation metrics, such as BLEU  (Papineni et al., 2002)  and Meteor  (Denkowski and Lavie, 2014) , except without using reference translations. Supervised learning of the QE as a Metric task requires human evaluation of translation quality as training data. Human evaluation of translation quality is generally very costly and can be performed in different ways, such as Direct Assessment (DA: requiring human assessors to assign an absolute score to a translation)  (Barrault et al., 2019; Graham et al., 2013 Graham et al., , 2014 Graham et al., , 2017  or Relative Ranking (RR: requiring human assessors to rank different translations)  (Bojar et al., 2015) . Since the QE as a Metric task requires QE models to assign an absolute score to a translation, DA human evaluation data can be straightforwardly used as training data for the QE as a Metric task. In order to also make use of the RR human evaluation data, we propose a multi-task learning QE model that jointly learns two tasks, score a translation and rank two translations. Multi-task learning of these two closely related tasks enables us to use both DA and RR human evaluation data for training the QE model and improve performance compared to learning these two tasks separately. Our model performs translation quality estimation based on cross-lingual sentence embeddings from pretrained multilingual language models  (Devlin et al., 2019; Conneau et al., 2019)  and does not need reference translations. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task  (Ma et al., 2019) . A number of previous works also used sentence embeddings for evaluating translation quality  (Shimanaka et al., 2018; Guzm?n et al., 2015; Gupta et al., 2015) . However,  Shimanaka et al. (2018) ;  Gupta et al. (2015) 's models only learn to score a translation and  Guzm?n et al. (2015) 's model only learns to rank two translations while our model jointly learns to score a translation and rank two translations in order to make use of different types of human evaluation data for model training. In addition,  Shimanaka et al. (2018) ;  Guzm?n et al. (2015) ;  Gupta et al. (2015) 's models use the reference translation for evaluating translation quality while our QE model does not require reference translations. There are existing QE models  (Lo, 2019;  that do not need the reference translation and perform translation quality estimation based on cross-lingual word/sentence embeddings, but these QE models give relatively poor and unstable results for different language pairs  (Ma et al., 2019)    

 Our Approach We propose a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model is based on cross-lingual sentence embeddings from multilingual BERT (M-BERT)  (Devlin et al., 2019; Reimers and Gurevych, 2019) . To compute the sentence embedding for a given sentence, we feed this sentence into M-BERT and then perform MEAN pooling over the output of M-BERT to obtain fixedsize sentence embedding. We fine-tune M-BERT for the QE tasks. The scoring task To score a translation t given the source sentence s, we use the cosine similarity between the source sentence embedding s and the target sentence embedding t as the score of the translation. 1 Equation 1 gives the loss function for the scoring task, where Y human (0 ? Y human ? 1) is the normalized DA score of the translation assigned by human assessors. Lscore = cos sim s, t ? Y human 2 (1) The ranking task To rank two translations t 1 and t 2 given the source sentence s, we compute Euclidean distances between source and target sentence embeddings Euc dis s, t 1 and Euc dis s, t 2 . The translation that has a smaller Euclidean distance with the source is predicted to be the better translation. 2 Equation 2 gives the loss function for the ranking task, where t b is the better translation and t w is the worse translation according to the human ranking. By minimizing L rank , we want Euc dis s, t b to be at least ? less than Euc dis s, t w . We tuned ? on the development set and finally set ? = 1 in our experiments. 3 L rank = ReLU Euc dis s, t b ? Euc dis s, tw + ? (2) Multi-task learning We train our QE model to jointly learn the scoring task and the ranking task via multi-task learning. The main advantage of multi-task learning for these two closely related tasks is that we can use both DA and RR human evaluation data for training the QE model and improve performance compared to learning these two tasks separately. We test our method on the WMT 2019 QE as a Metric task which requires QE models to assign an absolute score to a translation. We show that, on the QE as a Metric task, our multi-task learning method can achieve significantly better results compared to only training the QE model to learn the scoring task with DA human evaluation data.  (DA)  (Barrault et al., 2019; Graham et al., 2013 Graham et al., , 2014 Graham et al., , 2017 . Direct Assessment requires human assessors to assign an absolute score (between 0 and 100) to a translation based on general translation quality. We normalize DA scores to [0, 1] for training our model for the scoring task. Before 2016, WMT performed human evaluation for NEWS translation tasks via Relative Ranking  (Bojar et al., 2015) . Relative Ranking requires human assessors to rank different translations based on general translation quality. The rank results of any two translations that are not tied can be used to train our model for the ranking task. As described in the previous section, our model is based on cross-lingual sentence embeddings from M-BERT  (Devlin et al., 2019) . Other than M-BERT, we also tested another pretrained multilingual language model XLM-RoBERTa  (Conneau et al., 2019)  which achieves better results than M-BERT on various cross-lingual tasks. Finally, we trained six QE models for comparison,  

 Experiments 

 Segment-Level Results Tables 1, 2 and 4 give results of our models and the winning systems of the WMT 2019 QE as a Metric task (segment-level). We also show results of sent-BLEU and the winning systems of the WMT 2019 Metrics task. Compared to the QE as a Metric task, the Metrics task allows the usage of the reference translation for translation quality estimation. In Tables  1, 2  and 4, M NO and X NO had bad results, which shows that pretrained multilingual language models without fine-tuning do not perform well on the QE task; X MU (M MU) generally outperformed X DA (M DA), which shows that training the QE model with both DA and RR data to jointly learn the scoring and ranking tasks via multi-task learning can achieve better quality estimation results than only training the QE model to learn the scoring task with the DA data. Results also show that XLM-RoBERTa outperformed M-BERT for the QE task. Our best model X MU 6 achieved new state-of-the-art results for all language pairs on WMT 2019 QE as a Metric task and outperformed sentBLEU for 14 out of 18 language pairs on WMT 2019 Metrics task. Particularly, among all the languages in the test sets,  6  The training process of X MU takes 3 days with 1 GPU. MAX CLS MEAN M MU 0.641 0.646 0.648 X MU 0.647 0.667 0.694 Gujarati (gu) and Lithuanian (lt) do not occur in the training data of the QE task. Nevertheless, our model still got good results (outperforming sent-BLEU) for gu-en, lt-en, en-gu and en-lt tasks. In contrast, UNI  (Ma et al., 2019) , UNI+  (Ma et al., 2019) , YiSi-2  (Lo, 2019)  and YiSi-2 srl  (Lo, 2019)  give significantly worse and unstable results for different language pairs. Pooling Strategy Other than performing MEAN pooling to obtain sentence embeddings, we also tested MAX pooling or using the CLS token representation as the sentence embedding  (Devlin et al., 2019) . Results in Table  5  show that MEAN pooling achieved the best results for the QE task. Cosine Similarity for the Ranking Task We also tried to use cosine similarity instead of Euclidean distance for ranking the two translations in the ranking task. That is we used Equation 3 instead of Equation 2 as the loss function for the ranking task. ? was tuned to be 0.1. 7 Results are shown in Table  6 . Our multi-task learning QE model X MU (M MU) achieved better results when using Euc dis for the ranking task compared to using cos sim for the ranking task; X MU (M MU) always outperformed X DA (M DA) no matter Euc dis or cos sim was used for the ranking task. L rank = ReLU cos sim s, tw ? cos sim s, t b + ? (3) 

 System-Level Results Tables 7, 8 and 9 give results of our best model (X MU) and the winning systems of the WMT   2019 QE as a Metric task (system-level). We also show results of BLEU and the winning systems of the WMT 2019 Metrics task. For system-level evaluation, metrics which can use the reference translations for quality estimation, such as BLEU, generally achieved consistently high correlation with human evaluation for all language pairs. In contrast, QE models (including our QE model and submitted systems for the QE as a Metric task) are not allowed to use the reference translations for quality estimation and tend to generate more unstable results: high correlation with human evaluation for some language pairs but very low or even negative Pearson correlation with human evaluation for some other language pairs. For example, our QE model beat BLEU for zh-en, en-de and en-kk directions but got negative Pearson correlation with human evaluation for gu-en, ru-en, en-ru and fr-de directions. Among all QE models which do not use the reference translations, our model achieved the highest Pearson correlation with hu-man evaluation for 13 out of 18 language pairs. Compared to Tables 1, 2 and 4, our model tends to produce more unstable results for system-level evaluation than segment-level evaluation, likely because the segment-level correlation is computed using about 2000 segments for a language pair while the system-level correlation is computed using only about 10 systems for a language pair, therefore the segment-level correlation is more stable. 

 Conclusion This paper presents a multi-task leaning QE model that jointly learns two tasks, score a translation and rank two translations. The scoring and ranking results performed by human assessors can be used as training data for learning the scoring and ranking tasks respectively. Multi-task learning of these two closely related tasks enables us to make use of both types of human evaluation data for model training and improve performance compared to learning these two tasks separately. Our model obtains new state-of-the-art results on the WMT 2019 QE as a Metric task and outperforms sentBLEU on the WMT 2019 Metrics task. XLM-RoBERTa MULTI-TASK (X MU)where models 1, 2 and 3 use M-BERT for sentence embedding; models 4, 5 and 6 use XLM-RoBERTa for sentence embedding; NO-TRAIN means we do not fine-tune M-BERT (XLM-RoBERTa) for the QE tasks and simply use the pretrained model for sentence embedding; DA-ONLY means we only train the QE model to learn the scoring task with DA data; MULTI-TASK means we train the QE model with both DA and RR data to jointly learn the scoring task and the ranking task via multi-task learning. 
