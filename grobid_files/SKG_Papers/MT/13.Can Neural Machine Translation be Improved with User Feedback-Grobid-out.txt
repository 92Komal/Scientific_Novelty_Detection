title
Can Neural Machine Translation be Improved with User Feedback?

abstract
We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay ecommerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments-five-star ratings of translation quality-and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit taskbased feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.

Introduction In commercial scenarios of neural machine translation (NMT), the one-best translation of a text is shown to multiple users who can reinforce highquality (or penalize low-quality) translations by explicit feedback (e.g., on a Likert scale) or implicit feedback (by clicking on a translated page). In such settings this type of feedback can be easily collected in large amounts. While bandit feedback 1 in form of user clicks on displayed ads is the standard learning signal for response prediction in online advertising  (Bottou et al., 2013) , bandit learning for machine translation has so far been restricted to simulation experiments  (Sokolov et al., 2016b; Lawrence et al., 2017b ; * The work for this paper was done while the first author was an intern at eBay.  1  The fact that only feedback for a single translation is collected constitutes the "bandit feedback" scenario where the name is inspired by "one-armed bandit" slot machines.  Nguyen et al., 2017; Bahdanau et al., 2017) . The goal of our work is to show that the gold mine of cheap and abundant real-world human bandit feedback can be exploited successfully for machine learning in NMT. We analyze and utilize human reinforcements that have been collected from users of the eBay e-commerce platform. We show that explicit user judgments in form of fivestar ratings are not reliable and do not lead to downstream BLEU improvements in bandit learning. In contrast, we find that implicit task-based feedback that has been gathered in a cross-lingual search task can be used successfully to improve task-specific metrics and BLEU. Another crucial difference of our work to previous research is the fact that we assume a counterfactual learning scenario where human feedback has been given to a historic system different from the target system. Learning is done offline from logged data, which is desirable in commercial settings where system updates need to be tested before deployment and the risk of showing inferior translations to users needs to be avoided. Our offline learning algorithms range from a simple bandit-to-supervised conversion (i.e., using translations with good feedback for supervised tuning) to transferring the counterfactual learning techniques presented by  Lawrence et al. (2017b)  from statistical machine translation (SMT) to NMT models. To our surprise, the bandit-to-supervised conversion proved to be very hard to beat, despite theoretical indications of poor generalization for exploration-free learning from logged data  (Langford et al., 2008; Strehl et al., 2010) . However, we show that we can further improve over this method by computing a task-specific reward scoring function, resulting in significant improvements in both BLEU and in task-specific metrics. 2 Related Work  Sokolov et al. (2016a,b)  introduced learning from bandit feedback for SMT models in an interactive online learning scenario: the MT model receives a source sentence from the user, provides a translation, receives feedback from the user for this translation, and performs a stochastic gradient update proportional to the feedback quality.  showed that the objectives proposed for log-linear models can be transferred to neural sequence learning and found that standard control variate techniques do not only reduce variance but also help to produce best BLEU results.  Nguyen et al. (2017)  proposed a very similar approach using a learned word-based critic in an advantage actor-critic reinforcement learning framework. A comparison of current approaches was recently performed in a shared task where participants had to build translation models that learn from the interaction with a service that provided e-commerce product descriptions and feedback for submitted translations .  Lawrence et al. (2017b,a)  were the first to address the more realistic problem of offline learning from logged bandit feedback, with special attention to the problem of exploration-free deterministic logging as is done in commercial MT systems. They show that variance reduction techniques used in counterfactual bandit learning  (Dud?k et al., 2011; Bottou et al., 2013)  and off-policy reinforcement learning  (Precup et al., 2000; Jiang and Li, 2016 ) can be used to avoid degenerate behavior of estimators under deterministic logging. 3 User Feedback 

 Explicit Feedback via Star Ratings One way to collect reinforcement signals from human users of the eBay platform is by explicit ratings of product title translations on a five-point Likert scale. More specifically, when users visit product pages with translated titles, they can inspect the source when hovering with the mouse over the title. Then five stars are shown with the instruction to 'rate this translation'. A screenshot of an implementation of this rating interface is shown in Figure  1 . The original title, the translation and the given star rating are stored. For the experiments in this paper, we focus on translations from English to Spanish. The user star rating data set contains 69,412 rated product titles with 148k individual ratings. Since 34% of the titles were rated more than once, the ratings for each title are averaged. We observe a tendency towards high ratings, in fact one half of the titles are rated with five stars (cf. Appendix C). To investigate the reliability and validity of these ratings, we employed three bilingual annotators ('experts') to independently re-evaluate and give five-star ratings for a balanced subset of 1,000 product title translations. The annotators were presented the source title and the machine translation, together with instructions on the task provided in Appendix B. The inter-annotator agreement between experts is relatively low with Fleiss' ? = 0.12 (Fleiss, 1971). Furthermore, there is no correlation of the averaged 'expert' ratings and the averaged user star ratings (Spearman's ? = ?0.05). However, when we ask another three annotators to indicate whether they agree or disagree with a balanced subset of 2,000 user ratings, they agree with 42.3% of the ratings (by majority voting). In this binary meta-judgment task, the interannotator agreement between experts is moderate with ? = 0.45. We observe a strong tendency of the expert annotators to agree with high user ratings and to disagree with low user ratings. Two examples of user ratings, expert ratings and expert judgment are given in Table 1. In the first example, all raters agree that the translation is good, but in the second example, there is a strong disagreement between users and experts. This analysis shows that it is generally not easy for non-professional users of the e-commerce platform, and even for expert annotators, to give star ratings of translations in the domain of usergenerated product titles with high reliability. This problem is related to low validity, i.e., we do not know whether the users' response actually expresses translation quality, since we cannot control the influence of other factors on their judgment, e.g., the displayed image (see Figure  1 ), the prod- uct itself, or the users' general satisfaction with the e-commerce transaction, nor can we exclude the possibility that the user judgment is given with an adversarial purpose. Furthermore, we do not have control over the quality of sources 2 , nor can we discern to which degree a user rating reflects fluency or adequacy of the translation. 

 Task-Based Implicit Feedback Another form of collecting human reinforcement signals via the eBay e-commerce platform is to embed the feedback collection into a cross-lingual information retrieval task. The product title translation system is part of the search interaction of a user with the e-commerce platform in the following way: When a user enters a query in Spanish, it is first translated to English (query translation), then a search engine retrieves a list of matching products, and their titles are translated to Spanish and displayed to the user. As soon as the user clicks on one of the translated titles, we store the original query, the translated query, the source product title and its translation. From this collection we filter the cases where (a) the original query and the translated query are the same, or (b) more than 90% of the words from the query translation are not contained in the retrieved source title. In this way, we attempt to reduce the propagation of errors in query translation and search. This leaves us with a dataset of 164,065 tuples of Spanish queries, English product titles and their Spanish translations (15% of the original collection). Note that this dataset is more than twice the size of the explicit feedback dataset. An example is given in Table  2 . The advantage of embedding feedback collection into a search task is that we can assume that users who formulate a search query have a genuine intent of finding products that fit their need, and are also likely to be satisfied with product title translations that match their query, i.e., contain 2 Most titles consist of a sequence of keywords rather than a fluent sentence. See  Calixto et al. (2017)  for a fluency analysis of product titles. terms from the query in their own language. We exploit this assumption in order to measure the quality of a product title translation by requiring a user to click on the translation when it is displayed as a result of the search, and then quantifying the quality of the clicked translation by the extent it matches the query that led the user to the product. For this purpose, we define a word-based matching function match(w, q) that evaluates whether a query q contains the word w: match(w, q) = 1, if w ? q 0, otherwise. (1) Based on this word-level matching, we compute a sequence-level reward for a sentence y of length T as follows: recall(y, q) = 1 T T t=1 match(y t , q). (2) 4 Learning from User Feedback Reward Functions. In reinforcement and bandit learning, rewards received from the environment are used as supervision signals for learning. In our experiments, we investigate several options to obtain a reward function ? : Y ? [0, 1] from logged human bandit feedback: 1. Direct User Reward: Explicit feedback, e.g., in the form of star ratings, can directly be used as reward by treating the reward function as a black box. Since human feedback is usually only available for one translation per input, learning from direct user rewards requires the use of bandit learning algorithms. In our setup, human bandit feedback has been collected for translations of a historic MT system different from the target system to be optimized. This restricts the learning setup to offline learning from logged bandit feedback. to score translations either against a logged reference or a logged query. The first option requires a bandit-to-supervised conversion of data where high-quality logged translations are used as references against which BLEU or other MT quality metrics can be measured. 

 Reward Scoring The second option uses logged queries to obtain a matching score as in Equation  2 . 3. Estimated Reward: Another option to extend bandit feedback to all translations is to learn a parametric model of rewards, e.g., by optimizing a regression objective. The reward function is known, but the model parameters need to be trained based on a history of direct user rewards or by evaluations of a reward scoring function. In the following, we present how rewards can be integrated in various objectives for NMT training. Maximum Likelihood Estimation by Banditto-Supervised Conversion. Most commonly, NMT models are trained with Maximum Likelihood Estimation (MLE, Equation  3 ) on a given parallel corpus of source and target sequences D = {(x (s) , y (s) )} S s=1 L MLE (?) = S s=1 log p ? (y (s) |x (s) ). (3) The MLE objective requires reference translations and is agnostic to rewards. However, in a banditto-supervised conversion, rewards can be used to filter translations to be used as pseudo-references for MLE training. We apply this scenario to explicit and implicit human feedback data in our experiments. 

 Reinforcement Learning by Minimum Risk Training. When rewards can be obtained for several translations per input instead of only for one as in the bandit setup, by using a reward estimate or scoring function, Minimum Risk Training (MRT, Equation  4 ) can be applied to optimize NMT from rewards. R MRT (?) = S s=1 ?S(x (s) ) q ? ? (?|x (s) ) ?(?), (4) where sample probabilities are renormalized over a subset of translation samples S(x) ? Y(x): q ? ? (?|x) = p ? (?|x) ? y ?S(x) p ? (y |x) ? . The hyperparameter ? controls the sharpness of q (see  Shen et al. (2016)) . With sequence-level rewards, all words of a translation of length T are reinforced to the same extent and are treated as if they contributed equally to the translation quality. A word-based reward function, such as the match with a given query (Equation  1 ), allows the words to have individual weights. The following modification of the sequence-level MRT objective (Equation  4 ) accounts for word-based rewards ?(y t ): R W-MRT (?) = S s=1 ?S(x (s) ) T t=1 q ? ? (? t |x (s) , ?<t ) ?(y t ) , (5) where ?(y t ) in our experiments is a matching score (1). In the following we use the bracketed prefix (W-) to subsume both sentence-level and word-level training objectives. When output spaces are large and reward functions sparse, (W-)MRT objectives typically benefit from a warm start, i.e., pre-training with MLE. Following , we furthermore adopt a linear combination of MLE and (W-)MRT to stabilize learning: R (W-)MIX (?) = ? ? R MLE (?) + R (W-)MRT (?). Counterfactual Learning by Deterministic Propensity Matching. Counterfactual learning attempts to improve a target MT system from a log of source sentences, translations produced by a historic MT system, and obtained feedback L = {(x (h) , y (h) , ?(y (h) ))} H h=1 . For the special case of deterministically logged rewards  Lawrence et al. (2017b)  introduced the Deterministic Propensity Matching (DPM) objective with self-normalization as a multiplicative control variate  (Swaminathan and Joachims, 2015 ): 3 R DPM (?) = 1 H H h=1 ?(y (h) ) p? (y (h) |x (h) ), (6) where translation probabilities are reweighted over the current mini-batch B ? H, B H: p? (y (h) |x (h) ) = p ? (y (h) |x (h) ) B b=1 p ? (y (b) |x (b) ) . We additionally normalize the log probability of a translation y by its length |y|: p norm ? (y|x) = exp ( log p ? (y|x) |y| ). Counterfactual Learning by Doubly Controlled Estimation.  Lawrence et al. (2017b)  furthermore propose the Doubly Controlled objective (DC, Equation  7 ) implementing the idea of doubly robust estimation  (Dud?k et al., 2011; Jiang and Li, 2016)  for deterministic logs. In addition to learning from the historic reward for the logging system, the reward for other translations is estimated by a parametrized regression model that is trained on the log ? : Y ? [0, 1]. This objective contains both a multiplicative (probability reweighting) and an additive (reward estimate) control variate, hence the name. 4 R DC (?) = 1 H H h=1 ?(y (h) ) ? ? (y (h) ) ? p? (y (h) |x (h) ) + y?S(x (h) ) ? (y) p ? (y|x (h) ) (7) As for MRT, the expectation over the full output space is approximated with a subset of k sample translations S(x) ? Y(x). Relative Rewards. With the objectives as defined above, gradient steps are dependent on the magnitude of the reward for the current training instance. In reinforcement learning, an average reward baseline is commonly subtracted from the current reward with the primary goal to reduce variance  (Williams, 1992) . As a side effect, the current reward is relativized, such that the gradient step is not only determined by the magnitude of the current rewards, but is put into relation with previous rewards. We found this effect to be particularly beneficial in experiments with suboptimal reward estimators or noisy rewards and therefore apply it to all instantiations of the DPM and DC objectives. For DPM, the running average of historic rewards ?h = 1 h h i=1 ?(y (i) ) is subtracted from the current reward. For DC we apply this to both types of rewards in Equation  7 : 1) the logged reward ?(y (h) ), from which we subtract its running average ?h instead of the estimated reward ? (y (h) ), and 2) the estimated reward ? (y), from which we hence subtract the average estimated reward ?h = 1 h h i=1 1 k y ?S(x (i) ) ? (y ). 5 Experiments 

 NMT Model In our experiments, learning from feedback starts from a pre-trained English to Spanish NMT model that has not seen in-domain data (i.e., no product title translations). The NMT baseline model (BL) is a standard subword-based encoder-decoder architecture with attention  (Bahdanau et al., 2015) , implemented with TensorFlow  (Abadi et al., 2015) . The model is trained with MLE on 2.7M parallel sentences of out-of-domain data until the early stopping point which is determined on a small in-domain dev set of 1,619 product title translations. A beam of size 12 and length normalization  are used for beam search decoding. For significance tests we used approximate randomization  (Clark et al., 2011) , for BLEU score evaluation (lowercased) the multi-bleu script of the Moses decoder  (Koehn et al., 2007) , for TER computation the tercom tool  (Snover et al., 2006) . For MRT, DC and (W-)MIX models we set k = 5, for (W-)MIX models ? = 0.5 and ? = 0.05. For all NMT models involving random sampling, we report average results and standard deviation (in subscript) over two runs. Further details about training data and hyperparameters settings are described in Appendix D. 

 Reward Estimator The model architecture for the reward estimator used in the DC objective is a bilingual extension of the convolutional neural network (CNN) for  sentence classification proposed by  Kim (2014) . Both source and target sequences are padded up to a pre-defined maximum sequence length T max , their embeddings are concatenated and further processed by a 1D-Convolution over the time dimension with several filters of sizes from 2 to 15, which is then followed by a max-over-time pooling and fed to a fully-connected output layer (Figure  2 ). The model is trained to minimize the mean squared error (MSE) on the training portion of the logged feedback data (60k for simulated sentence-BLEU feedback, 62,470 for star rating feedback). The word embeddings of the reward estimator are initialized by the word embeddings of the trained baseline NMT system and fine-tuned further together with the other CNN weights. The best parameters are identified by early-stopping on the validation portion of the feedback data (2,162 for the simulation, 6,942 for the star ratings). Please find a detailed description of the model's hyperparameters in Appendix D.4. Results for a stand-alone evaluation of the reward estimator on the validation portions of the feedback data are given in Table  3 . The estimator models sBLEU much more accurately than the user star ratings. This is due to large variance and skew of the user ratings. An MSE-trained estimator typically predicts values around the mean, which is not a suitable strategy for such a skewed distribution of labels, but is successful for the prediction of normal-distributed sBLEU. 

 Explicit Star Rating Feedback Counterfactual Bandit Learning. As shown in Table  4 , counterfactual learning with DPM and DC on the logged star ratings as direct reward does not yield improvements over the baseline model in terms of corpus BLEU or TER. A randomization of feedback signals for translations gives the same results (DPM-random), showing that counterfactual learning from logged star ratings is equivalent to learning from noise. Evaluating the models in terms of estimated user reward, however, we find an improvement of +1.49 for DC, +0.04 for DPM over the baseline (53.93) (not shown in Table  4 )but these improvements do not transfer to BLEU because the reward model largely over-estimates the translation quality of translations with major faults. Hence it is not desirable to optimize towards this signal directly. Bandit-to-Supervised Conversion. In the following setup, we utilize the user ratings to filter the log by using only five star rated translations, and perform supervised learning of MLE and MIX using sBLEU against pseudo-references as reward function. Table  4  shows that this filtering strategy leads to large improvements over the baseline, for MLE and even more for MIX, even though the data set size is reduced by 42%. However, around the same improvements can be achieved with a random selection of logged translations of the same size (MIX small, containing 55% fivestar ratings). Using all logged translations for training MIX achieves the best results. This suggests that the model does not profit from the feedback, but mostly from being exposed to in-domain translations of the logging system. This effect is similar to training on pseudo-references created by back-translation  (Sennrich et al., 2016b,a) . 

 Task-Based Implicit Feedback Bandit-to-Supervised Conversion. We apply the same filtering technique to the logged implicit feedback by treating translations with recall = 1 as references for training MIX with sBLEU (reduction of the data set by 62%). The results in Table 5 show that large improvements over the baseline can be obtained even without filtering, BLEU and TER scores being comparable to the ones observed for training on explicit user ratings. Task-based Feedback. The key difference between the implicit feedback collected in the query- Table  4 : Results for models trained on explicit user ratings evaluated on the product titles test set. 'small' indicates a random subset of logged translations of the same size as the filtered log that only contains translations with an average rating of five stars ('stars = 5'). The differences in BLEU are not significant at p ? 0.05 between MIX models, but over other models. Table  5 : Results for models trained on implicit task-based feedback data evaluated on the product titles test set. 'small' indicates a random subset of logged translations of the same size as the filtered log that only contains translations that contain all the query words ('recall = 1'). The BLEU score of MIX (small) significantly differs from MIX (all) at p ? 0.05, the score of MIX (recall = 1) does not. Other differences are significant. title data and the explicit user ratings, is that it can be used to define reward functions like recall or match (Equations 2, 1). For the experiments we train W-MIX, the word-based MRT objective (Equation  5 ) linearly combined with MLE, on the logged translations accompanying the queries (160k sentences). This combination is essential here, since the model would otherwise learn to produce translations that contain nothing but the query words. To account for usergenerated language in the queries and subwords in the MT model, we soften the conditions for a match, counting tokens as a match that are part of a word w that is either contained in the query, or has edit distance to a word in the query with dist(w, q i ) < max(3, 0.3 ? |w|). Table  6  repeats the best MIX results from Table  4  and 5, and evaluates the models with respect to query recall. We also report the query recall for the logged translations and the out-of-domain baseline. These results are compared to W-MIX training on implicit feedback data described in Sec-  

 Conclusion We presented methods to improve NMT from human reinforcement signals. The signals were logged from user activities of an e-commerce platform and consist of explicit ratings on a five-point Likert scale and implicit task-based feedback collected in a cross-lingual search task. We found that there are no improvements when learning from user star ratings, unless the noisy ratings themselves are stripped off in a bandit-to-supervised conversion. Implicit task-based feedback can be used successfully as a reward signal for NMT optimization, leading to improvements both in terms of enforcing individual word translations and in terms of automatic evaluation measures. In the future, we plan transfer these findings to production settings by performing regular NMT model updates with batches of collected user behavior data, especially focusing on improving translation of ambiguous and rare terms based on rewards from implicit partial feedback. 
