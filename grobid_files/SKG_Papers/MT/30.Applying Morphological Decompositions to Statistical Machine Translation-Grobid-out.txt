title
Applying morphological decomposition to statistical machine translation

abstract
This paper describes the Aalto submission for the German-to-English and the Czechto-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR. Statistical machine translation has focused on using words, and longer phrases constructed from words, as tokens in the system. In contrast, we apply different morphological decompositions of words using the unsupervised Morfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the Germanto-English translation. However, we did not see improvements for the Czech-to-English translations.

Introduction The effect of morphological variation in languages can be alleviated by using word analysis schemes, which may include morpheme discovery, part-ofspeech tagging, or other linguistic information. Words are very convenient and even efficient representation in statistical natural language processing, especially with English, but morphologically rich languages can benefit from more fine-grained information. For instance, statistical morphs discovered with unsupervised methods result in better performance in automatic speech recognition for highly-inflecting and agglutinative languages .  Virpioja et al. (2007)  applied morph-based models in statistical machine translation (SMT) between several language pairs without gaining improvement in BLEU score, but obtaining re-ductions in out-of-vocabulary rates. They utilized morphs both in the source and in the target language. Later, de  Gispert et al. (2009)  showed that Minimum Bayes Risk (MBR) combination of word-based and morph-based translation models improves translation with Arabicto-English and Finnish-to-English language pairs, where only the source language utilized morphbased models. Similar results have been shown for Finnish-to-English and Finnish-to-German in performance evaluation of various unsupervised morpheme analysis algorithms in Morpho Challenge 2009 competition . We continue the research described above and examine how the level of decomposition affects both the individual morph-based systems and MBR combinations with the baseline word-based model. Experiments are conducted with the WMT10 shared task data for German-to-English and Czech-to-English language pairs. 

 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit  from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. 

 Morphological models for words Morfessor  (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc. ) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, morphs, stored in the model lexicon. We use both the Morfessor Baseline  (Creutz and Lagus, 2005b)  and the Morfessor Categories-MAP  (Creutz and Lagus, 2005a)  algorithms. 1 Both are formulated in a maximum a posteriori (MAP) framework, i.e., the learning algorithm tries to optimize the product of the model prior and the data likelihood. The generative model applied by Morfessor Baseline assumes that the morphs are independent. The resulting segmentation can be influenced by using explicit priors for the morph lengths and frequencies, but their effect is usually minimal. The training data has a larger effect on the results: A larger data set allows a larger lexicon, and thus longer morphs and less morphs per word  (Creutz and Lagus, 2007) . Moreover, the model can be trained with or without taking into account the word frequencies. If the frequencies are included, the more frequent words are usually undersegmented compared to a linguistic analysis, whereas the rare words are oversegmented  (Creutz and Lagus, 2005b ). An easy way to control the amount of segmentation is to weight the training data likelihood by a positive factor ?. If ? > 1, the increased likelihood results in longer morphs. If ? < 1, the morphs will be shorter and the words more segmented. Words that are not present in the training data can be segmented using an algorithm similar to Viterbi. The algorithm can be modified to allow new morphs types to be used by using an approximative cost of adding them into the lexicon  (Virpioja and Kohonen, 2009) . The modification prevents oversegmentation of unseen word forms. In machine translation, this is important especially for proper nouns, for which there is usually no need for translation. The Morfessor Categories-MAP algorithm extends the model by imposing morph categories of stems, prefixes and suffixes, as well as transition probabilities between them. In addition, it applies a hierarchical segmentation model that allows it to construct new stems from smaller pieces of "nonmorphemes"  (Creutz and Lagus, 2007) . Due to these features, it can provide reasonable segmentations also for those words that contain new morphemes. The drawback of the more sophisticated model is the slower and more complex training algorithm. In addition, the amount of the segmenta-tion is harder to control. Morfessor Categories-MAP was applied to statistical machine translation by  Virpioja et al. (2007)  and de  Gispert et al. (2009) . However,  report that Morfessor Baseline outperformed Categories-MAP in Finnish-to-English and German-to-English tasks both with and without MBR combination, although the differences were not statistically significant. In all the previous cases, the models were trained on word types, i.e., without using their frequencies. Here, we also test models trained on word tokens. 

 Statistical machine translation We utilize the Moses toolkit  for statistical machine translation. The default parameter values are used except with the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 

 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination  (Sim et al., 2007) . N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score  (Kumar and Byrne, 2004) . In this work, the focus of the system combination is not to combine different translation systems (e.g., Moses and Systran), but to combine systems trained with the same translation algorithm using the same source language data with with different morphological decompositions. 

 Experiments The German-to-English and Czech-to-English parts of the ACL WMT10 shared task data were investigated. Vanilla SMT models were trained with Moses using word tokens for MBR combination and comparison purposes.  

 Data The data used in the experiments consisted of Czech-to-English (CZ-EN) and German-to-English (DE-EN) parallel language data from ACL WMT10. The data was divided into distinct training, development, and evaluation sets. Statistics and details are shown in Table  1 . Aligned data from Europarl v5 and News Commentary corpora were included in training German-to-English SMT models. The English part from the same data sets was used for training a 5-gram language model, which was used in all translation tasks. The Czech-to-English translation model was trained with CzEng v0.9 (training section 0) and News Commentary data. The monolingual German and Czech parts of the training data sets were used for training the morph segmentation models with Morfessor. The data sets news-test2009, news-syscomb2009 and news-syscombtune2010 from the ACL WMT 2009 and WMT 2010, were used for development. The news-test2008, news-test2010, and news-syscombtest2010 data sets were used for evaluation. 

 Preprocessing All data sets were preprocessed before use. XMLtags were removed, text was tokenized and characters were lowercased for every training, development and evaluation set. Morphological models for German and Czech were trained using a corpus that was a combination of the respective training sets. Then the models were used for segmenting all the data sets, including development and evaluation sets, with the Viterbi algorithm discussed in Section 2.1. The modification of allowing new morph types for outof-vocabulary words was not applied. The Moses cleaning script performed additional filtering on the parallel language training data. Specifically, sentences with over 80 words were removed from the vanilla Moses word-based models. For morph-based models the limit was set to 100 morphs, which is the maximum limit of the Giza++ alignment tool. After filtering with a threshold of 100 tokens, the different morph seg-  

 Results The details of the ACL WMT10 submissions are shown in Table  2 . The results of experiments with different morphological decompositions and MBR system combinations are shown in Table  3 . The significances of the differences in BLEU scores between the word-based model (Words) and models with different morphological decompositions was measured by dividing each evaluation data set into 49 subsets of 41-51 sentences, and using the one-sided Wilcoxon signed rank test (p < 0.05). 

 Segmentation We created several word segmentations with Morfessor baseline and Morfessor Categories-MAP (CatMAP). Statistics for the different segmentations are given in Table  3 . The amount of segmentation was measured as the average number of morphs per word (m/w) and as the percentage of segmented words (s-%) in the training data. Increasing the data likelihood weight ? in Morfessor Baseline increases the amount of segmentation for both languages. However, it had little effect on the proportion of segmented words in the three evaluation data sets: The proportion of segmented word tokens was 10-11 % for German and 8-9 % for Czech, whereas the out-of-vocabulary rate was 7.5-7.8 % for German and 4.8-5.6 % for Czech. Disregarding the word frequency information in Morfessor Baseline (nofreq) produced more morphs per word type and segmented nearly all words in the training data. The Morfessor CatMAP algorithm created segmentations with the largest number of morphs per word, but did not segment as many words as the Morfessor Baseline without the frequencies. 

 Morph-based translation systems The models with segmented source language performed worse individually than the word-based models. The change in the BLEU score was statistically significant in almost all segmentations and Morfessor Baseline (? = 0.5) 

 aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9 Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are trained from the segmented source language into unsegmented target language with Moses. all evaluation sets. Morfessor Baseline (? = 0.5) was the best individual segmented model for both German and Czech in the sense that it had the lowest number of significant decreases the BLEU score compared to the word-based model. Removing word frequency information with Morfessor Baseline and using Morfessor CatMAP gave the lowest BLEU scores with both source languages. 

 Translation system combination For the DE-EN language pair, all MBR system combinations between each segmented model and the word-based model had slightly higher BLUE scores than the individual word-based model. Nearly all improvements were statistically significant. The BLEU scores for the MBR combinations in the CZ-EN language pair were mostly not significantly different from the individual word-based model. Two scores were significantly lower. 

 Discussion We have applied concatenative morphological analysis, in which each original word token is segmented into one or more non-overlapping morph tokens. Our results with different levels of segmentation with Morfessor suggest that the optimal level of segmentation is language pair dependent in machine translation. Our approach for handling rich morphology has not been able to directly improve the translation quality. We assume that improvements might still be possible by carefully tuning the amount of segmentation. The experiments in this paper with different values of the ? parameter for Morfessor Baseline were conducted with the word frequencies. The parameter had little effect on the proportion of segmented words in the evaluation data sets, as frequent words were not segmented at all, and out-of-vocabulary words were likely to be oversegmented by the Viterbi algorithm. Future work includes testing a larger range of values for ?, also for models trained without the word frequencies, and using the modification of the Viterbi algorithm proposed in  Virpioja and Kohonen (2009) . It might also be helpful to only segment selected words, where the selection would be based on the potential benefit in the translation process. In general, the direct segmentation of words into morphs is problematic because it increases the number of tokens in the text and directly increases both model training and decoding complexity. However, an efficient segmentation decreases the number of types and the out-of-vocabulary rate  (Virpioja et al., 2007) . We have replicated here the result that an MBR combination of a morph-based MT system with  a word-based MT system can produce a BLEU score that is higher than from either of the individual systems  (de Gispert et al., 2009; . With the DE-EN language pair, the improvement was statistically significant with all tested segmentation models. However, the improvements were not as large as those obtained before and the results for the CZ-EN language pair were not significantly different in most cases. Whether this is due to the different languages, training data sets, the domain of the evaluation data sets, or some problems in the model training, is currently uncertain. One very different approach for applying different levels of linguistic analysis is factor models for SMT , where pre-determined factors (e.g., surface form, lemma and part-of-speech) are stored as vectors for each word. This provides better integration of morphosyntactic information and more control of the process, but the translation models are more complex and the number and factor types in each word must be fixed. Our submissions to the ACL WMT10 shared task utilize unsupervised morphological decomposition models in a straightforward manner. The individual morph-based models trained with the source language words segmented into morphs did not improve the vanilla word-based models trained with the unsegmented source language. We have replicated the result for the Germanto-English language pair that an MBR combination of a word-based and a segmented morphbased model gives significant improvements to the BLEU score. However, we did not see improvements for the Czech-to-English translations. Several different morphological segmentation models for German and Czech were trained with Morfessor. Each segmentation model corresponds to a morph-based SMT model trained with Moses. The word-based vanilla Moses model is compared to each morphbased model as well as to several MBR com-binations between word-based translation models and morph-based translation models. Quantitative evaluation is carried out using the BLEU score with re-cased and re-tokenized translations. 

 mentations for DE-EN training data from combined Europarl and News Commentary data sets ranged from 1 613 556 to 1 624 070 sentences. Similarly, segmented CZ-EN training data ranged from 896 163 to 897 744 sentences. The vanilla words-based model was trained with 1 609 998 sentences for DE-EN and 897 497 sentences for CZ-EN. 

 Table 1 : 1 Data sets for the Czech-to-English and German-to-English SMT experiments, including the number of aligned sentences and the average number of words per sentence in each language. The data sets used for model training, development and evaluation are marked. Training is divided into German (DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training. Data set Statistics Training Development Evaluation Sentences Words per sentence SM LM TM DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN Europarl v5 1 540 549 23.2 25.2 x x x News Commentary 100 269 21.9 18.9 21.5 x x x x x CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x news-test2009 2 525 21.7 18.8 23.2 x news-syscomb2009 502 19.7 17.2 21.1 x news-syscombtune2010 455 20.2 17.3 21.0 x news-test2008 2 051 20.3 17.8 21.7 x news-test2010 2 489 21.7 18.4 22.3 x news-syscombtest2010 2 034 22.0 18.6 22.6 x Submission Segmentation model for source language BLEU-cased (news-test2010) aalto DE-EN WMT10 Morfessor Baseline (? = 0.5) 17.0 aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5 aalto CZ-EN WMT10 

 Table 3 : 3 Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models. The source language is segmented with the shown algorithms. The amount of segmentation in the training data is measured with the average number of morphs per word (m/w) and as proportion of segmented words (s-%) against the word-based model (Words). The trained translation systems are evaluated independently (No MBR) and in Minimum Bayes Risk system combination of word-based translation systems (MBR). Unchanged ( ? ), significantly higher ( + ) and lower ( ? ) BLEU scores compared to the word-based translation model (Words) are marked. The best morph-based model for each column is emphasized. 

			 The respective software is available at http://www. cis.hut.fi/projects/morpho/
