title
Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation

abstract
We describe our submission to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE)  (Mayhew et al., 2020) . We view MT models at various training stages (i.e., checkpoints) as human learners at different levels. Hence, we employ an ensemble of multicheckpoints from the same model to generate translation sequences with various levels of fluency. From each checkpoint, for our best model, we sample n-Best sequences (n = 10) with a beam width = 100. We achieve 37.57 macro F 1 with a 6 checkpoint model ensemble on the official English to Portuguese shared task test data, outperforming a baseline Amazon translation system of 21.30 macro F 1 and ultimately demonstrating the utility of our intuitive method.

Introduction Machine Translation (MT) systems are usually trained to output a single translation. However, many possible translations of a given input text can be acceptable. This situation is common in online language learning applications such as Duolingo, 1 Babbel 2 , and Busuu.  3  In applications of this type, learning happens via translation-based activities while evaluation is performed by comparing learners' responses to a large set of human acceptable translations. Figure  1  shows an example of a typical situation extracted from the Duolingo application. The main set up of the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE 2020)  (Mayhew et al., 2020)  is such that one starts with a set of English sentences (prompts) and then generates highcoverage sets of plausible translations in the five target languages: Portuguese, Hungarian, Japanese, Korean, and Vietnamese. For instance, if we want to translate the English (En) sentence "is my explanation clear?" to Portuguese (Pt), all the translated Portuguese sentences illustrated in Table  1  would be acceptable.  4  Limited training data. One challenge for training a sufficiently effective model we faced is the limited size of the source training data released by organizers (4, 000 source English sentences coupled with 226, 466 Portuguese target sentences). We circumvent this limitation by training a model on a large dataset acquired from the OPUS corpus (as described in Section 3), which gives us a powerful MT system that we build on (see Section 4.2). We then exploit the STAPLE-provided training data in multiple ways (see  Sections 4.3 and 4.4)  to extend this primary model as a way to nuance the model to the shared task domain. Paraphrase via MT. In essence, the shared task is a mixture of MT and paraphrase. This poses a second challenge: there is no paraphrase dataset to train the system on. For this reason, we resort to using outputs from the MT system in place of paraphrases. This required generating multiple sentences for each source sentence. To meet this need, we generate multiple translation hypotheses (n-Best) using a wide beam search (Section 5.1), perform 'round-trip' translations exploiting these multiple outputs (Section 5.2), and employ ensembles of checkpoints (Section 5.3). Diverse outputs. A third challenge is that the target Portuguese sentences provided for training by organizers are produced by learners of English at various levels of fluency. This makes some of these Portuguese translations inarticulate (i.e., not quite fluent). MT systems are not usually trained to produce inarticulate translations (part of the time), and hence we needed to offer a solution that matches the different levels of language learners who produced the translations. Intuitively, we view MT systems trained at various stages (i.e., checkpoint) as learners with various levels of fluency. As such, we employ an ensemble of checkpoints to generate translations matching the different levels of learner fluency (Section 5.3). Ultimately, our contributions lie in alleviating the 3 challenges listed above. The remainder of the paper is organized as follows: Section 2 is a brief overview of related work. In Section 3, we describe the data we use for both training and fine-tuning our models. Section 4 presents the proposed MT system. Section 5 describes our different methods. We discuss our results in Section 6, and conclude in Section 7. 

 Related Work We focus our related work overview on the task of paraphrase generation and its intersection with machine translation. Paraphrasing is the task of expressing the same textual units (e.g. sentence) with alternative forms using different words while keeping the original meaning intact.  5  Over the last few years, MT has been the dominant approach for paraphrase generation. For instance,  Barzilay and McKeown (2001) ;  Pang et al. (2003)  use multiple translations of the same text to train a paraphrase system. Similarly,  Bannard and Callison-Burch (2005)   More recently, advances in neural machine translation (NMT) have spurred interest in paraphrase generation  (Sutskever et al., 2014; Aharoni et al., 2019) . For example,  Prakash et al. (2016)  employ a stacked residual LSTM network to learn a sequence-to-sequence model on paraphrase data. A parpahrase model with adversarial training is presented by  (Li et al., 2017) .  Wieting and Gimpel (2017) ;  Iyyer et al. (2018)  propose a translation-based paraphrasing system, which is based on NTM to translate one side of a parallel corpus. Paraphrase generation with pivot NMT is used by  (Mallinson et al., 2017; Yu et al., 2018) . 

 Data 

 Shared task data As part of the STAPLE 2020 shared task, only training data were released. The target training split is a total of 526, 466 of learner translations of 4, 000 input (source) English sentences. We note that the number of translations of each English sentence varies, with an average of ? 132 Portuguese target sentences for each English source sentence. As shared task organizers point out, this training dataset can be used as a reference/anchor points, and also serves as a strong baseline. For evaluation, a sets of 60, 294 translations (learner-crafted sentences) of 500 input English sentences were available on Colab. Test data were also made available only via Colab and comprised 500 English sentences learner-translated    (Rozis and Skadin?, 2017) , translation memories (DGT)  (Steinberger et al., 2013) , Open-Subtitles  (Creutz, 2018) , SciELO Parallel  (Soares et al., 2018) , JRC-Acquis Multilingual  (Steinberger et al., 2006) , Tanzil (Zarrabi-Zadeh, 2007), Eu-roparl Parallel  (Koehn, 2005) , TED 2013  (Cettolo et al., 2012) , Wikipedia  (Wo?k and Marasek, 2014) , Tatoeba 8 , QCRI Educational Domain  (Abdelali et al., 2014) , GNOME localization files, 9 Global Voices, 10 KDE4, 11 , Ubuntu,  12  and Multilingual Bible  (Christodouloupoulos and Steedman, 2015) . To train our models, we extract more than 77.7M parallel (i.e., English-Portuguese) sentences from the whole collection. The extracted dataset comprises more than 1.5B English tokens and 1.4B Portuguese tokens. More details about the training dataset are given in Table  2 . 

 Pre-Processing Pre-processing is an important step in building any MT model as it can significantly affect the end results. We remove punctuation and tokenize all data with the Moses tokenizer  (Koehn et al., 2007) . We also use joint Byte-Pair Encoding (BPE) with 60K split operations for subword segmentation  (Sennrich et al., 2016) . 

 Models In this section, we first describe the architecture of our models. We then explain the different ways we train the models on various subsets of the data. 

 Architecture Our models are mainly based on a Convolutional Neural Network (CNN) architecture  (Kim, 2014; Gehring et al., 2017) . This convolutional architecture exploits BPE  (Sennrich et al., 2016) . The architecture is as follows: 20 layers in the encoder and 20 layers in the decoder, a multiplicative attention  in every decoder layer, a kernel width of 3 for both the encoder and the decoder, a hidden size 512, and an embedding size of 512, and 256 for the encoder and decoder layers respectively. We use a Fairseq implementation  (Ott et al., 2019) . 

 Basic En?Pt Models We trained two MT models, English-to-Portuguese (En?Pt) and Portuguese-to-English (Pt?En), on 4 V100 GPUs, following the setup described in  Ott et al. (2018) . For both models, the learning rate was set to 0.25, a dropout of 0.2, and a maximum tokens of 4, 000 for each mini-batch. We train our models on the 77.7M parallel sentences of the OPUS dataset described in Section 3. Validation is performed on the development data from STAPLE 2020  (Mayhew et al., 2020) . 

 En?Pt Extended Model We use the training data of the STAPLE 2020 shared task 13 to create a new En-Pt parallel dataset. More specifically, at the target side, we use all the Portuguese gold translations while duplicating the same English source sentence at the source side. This results in a new training set of 251, 442 En-Pt parallel sentences. We refer to this training dataset as STAPLE-TRAIN, or simply S-TRAIN. We then merge OPUS and S-TRAIN to train an En?Pt model from scratch. We refer to this new model as the extended model. 

 En?Pt Fine-Tuned Model Fine-tuning with domain-specific data, from a domain of interest, can be an effective strategy when it is desirable to develop systems for such a domain  (Ott et al., 2019 (Ott et al., , 2018 . Motivated by this, we experiment with using the STAPLE-based S-TRAIN parallel dataset from the previous subsection to fine-tune our En?Pt basic model for 5 epochs.  14  We will refer to the model resulting from this fine-tuning process simply as the finetuned model. 

 Model Deployment Methods In order to enhance the 1-to-n En-Pt translation, we propose three methods based on the previously discussed MT models (see section 4). These methods are n-Best prediction, multi-checkpoint translation, and paraphrasing. 

 n-Best Prediction We first use our three MT models (basic, extended, and fine-tuned) with a beam search size of 100 to generate n-Best translation hypotheses. We then use the average log-likelihood to score each of these hypotheses. Finally, we select the hypothesis with the n highest score as our output. 

 Paraphrasing Paraphrasing is an effective data augmentation method which is commonly used in MT tasks  (Poliak et al., 2018; Iyyer et al., 2018) . In order to extend the list of accepted Portuguese translations, we use both of our En?Pt and Pt?En models, as follows: 1. Translate the English sentences using the En?Pt model. For instance, we generate n-Best (n = 10) Portuguese sentences for each English source sentence. 2. Then, we use the Pt?En model to get n -Best English translations (we experiment with n = 1, 3, and 5) for each of the 10 Portuguese sentence. At this point, we would have 10 * n new English sentences (oftentimes with duplicate generations that we remove). These new sentences represent paraphrases of the original English sentence. 3. After de-duplication, the new English sentences are fed to the En?Pt model to get the 1-Best Portuguese translation.  

 Multi-Checkpoint Translation Our third method is based on saving the models at given epochs (checkpoints) during training. We use the m last checkpoints (models) to generate the n-Best translation hypotheses (the same way as our n-Best prediction method). We then de-duplicate the outputs of all the m models and use them in evaluation. We now describe our evaluation. 

 Evaluation In order to evaluate our methods, we carry out a number of experiments. First, we consider performance of each proposed method on the official training and development datasets of STA-PLE  (Mayhew et al., 2020) . Our models were ultimately evaluated on the shared task test data. We now describe STAPLE evaluation metrics and baselines as provided by organizers, before report-ing on our results on training, development, and test. 

 Evaluation Metrics & Baselines Weights of Translation. We note that each Portuguese translated sentence has a weight as provided in the gold dataset. The weights of translations correspond to user (learner) response rates. These weights are used primarily for scoring. The STAPLE 2020 shared task data takes the format illustrated in Table  3 . Metrics. Performance of MT systems in the shared task is quantified and scored based on how well a model can return all human-curated acceptable translations, weighted by the likelihood that an English learner would respond with each translation  (Mayhew et al., 2020) . As such, the main scoring metric is the weighted macro F 1 , with respect to the accepted translations. To English Sentence : is my explanation clear? Weights Portuguese Translation 0.26739 -minha explicac ?o est? clara? 0.16168 -minha explicac ?o ? clara? 0.11109 -a minha explicac ?o ? clara? 0.08778 -est? clara minha explicac ?o? 0.05717 -minha explanac ?o est? clara? English Sentence : this is my fault. 

 Weights Portuguese translation 0.17991 -isto ? minha culpa. 0.10664 -isso ? minha culpa. 0.08944 -esta ? minha culpa. 0.07794 -isto ? culpa minha. 0.06803 -? minha culpa. Table  3 : English sentences with their Portuguese translation and Weights samples from shared task train data. compute weighted macro F 1 (see formula 6), the weighted F 1 for each English sentence (s) is calculated and the average over all the sentences in the corpus is computed. The weighted F 1 (see formula 5) is computed using the unweighted precision (see formula 1) and the weighted recall (see formulas 2, 3 and 4). 

 P recision (s) = T Ps T Ps + F Ns (1) W T Ps = s?T Ps weight(t) (2) W F Ns = s?F Ns weight(t) (3) W eighted Recall (s) = W T Ps W T Ps + W F Ns (4) W eighted F 1(s) = 2 ? P rec. (s) ? W. Recall (s) P rec. (s) + W. Recall (s) (5) W eighted M acro F1 = s?S W eighted F 1(s) |S| (6) Baselines. We adopt the two baselines offered by the task organizers. These are based on Amazon and Fairseq translation systems and are at 21.30% and 13.57%, respectively. More information about these baselines can be reviewed at the shared task site listed earlier. 

 Evaluation on TRAIN and DEV In this section, we report the results of our 3 proposed methods, (a) n-Best prediction, (b) paraphrasing, and (c) multi-checkpoint translation using the MT models presented in section 4. Evaluation on TRAIN. For (a) the n-Best prediction method, we explore the 4 different values of n in the set {5, 10, 15, 20}. For (b) the paraphrase method, we set the number of Portuguese sentences to n = {1, 3, 5}. Finally, (c) the multi-checkpoint method was tested with 4 different values for the number of checkpoints m = {2, 4, 6, 8}. For paraphrasing and multi-checkpoint translation, we fix the number of n-best translations n to 10, varying the values of n and m only when evaluating our extended model. This leads us to identifying the best evaluation values of n = 3 and m = 6, which we then use when evaluating our basic and fine-tuned models. Evaluation on DEV. For evaluation on the STAPLE development data, we adopt the same procedure followed for evaluation on the train split.   

 Evaluation on TEST In test phase, we submitted translations from 3 systems for the STAPLE English-Portuguese sub-task. The 3 systems are based on our multi-checkpoint translation with the extended model. The number of checkpoints used was m = {4, 6, 8}, and n is fixed to 10 (i.e., the best value of n identified on training data with our extended model). Table  5  shows the results of our 3 final submitted systems as returned by the shared task organizers. Discussion. Our results indicate that when the multi-checkpoint method with the extended model and only two last checkpoints (m = 4) is used, the macro F 1 score reaches 37.07% (with a best precision of 60.14%). This method with m = 6 represents our best macro F 1 score 37.57% for the English-Portuguese translation sub-task. We note that with this configuration we outperform the Amazon and Fairseq translation baseline systems (at +15.92% and +23.99%, respectively) provided by the task organizers. We also observe that when m is set to 8, the macro F 1 slightly decreases to 37.21%. Ultimately, our findings show the utility of using multiple checkpoint ensembles as a way to mimic the various levels of language learners. Simple as this approach is, we find it quite intuitive. 

 Conclusion In this work, we described our contribution to the 2020 Duolingo Shared Task on Simultaneous Translation And Paraphrase for Language Education (STAPLE)  (Mayhew et al., 2020) . Our system targeted the English-Portuguese sub-task. Our models effectively make use of an approach based on n-Best prediction and multi-checkpoint translation. Our use of the OPUS dataset for training proved quite successful. In addition, based on our results, our intuitive deployment of a multi-checkpoint ensemble coupled with n-Best decoded translations seem to mirror leaner proficiency. As future work, we plan to explore other methods on new language pairs. Figure 1 : 1 Figure 1: Translations proposed by English language learners at various levels of fluency, from diverse backgrounds. Our multi-checkpoint ensemble models mimic learner fluency.4 
