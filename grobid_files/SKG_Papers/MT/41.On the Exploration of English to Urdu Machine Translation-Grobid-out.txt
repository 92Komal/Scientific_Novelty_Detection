title
On the Exploration of English to Urdu Machine Translation

abstract
Machine Translation is the inevitable technology to reduce communication barriers in today's world. It has made substantial progress in recent years and is being widely used in commercial as well as non-profit sectors. Such is only the case for European and other high resource languages. For English-Urdu language pair, the technology is in its infancy stage due to scarcity of resources. Present research is an important milestone in English-Urdu machine translation, as we present results for four major domains including Biomedical, Religious, Technological and General using Statistical and Neural Machine Translation. We performed series of experiments in attempts to optimize the performance of each system and also to study the impact of data sources on the systems. Finally, we established a comparison of the data sources and the effect of language model size on statistical machine translation performance.

Introduction Machine translation (MT) for low resource languages has been a challenging task  (Irvine, 2013; Zoph et al., 2016) . The dimensionality of difficulty increases when it comes to translating between a morphologically rich and morphologically poor language  (Habash and Sadat, 2006) . In this study, we will be presenting one such pair, English to Urdu translation, with English being a morphologically simple language while Urdu is a language with rich inflectional and derivational morphology. In case of Urdu-English translation topological distance between both languages is the biggest hurdle to get best results  (Jawaid et al., 2016; . Findings of  WMT 2011 evaluation (Callison-Burch et al., 2011  reported Urdu-English translation to be a relatively difficult problem. With some works on rule based systems (RBMT)  (Tafseer and Alvi, 2002; Karamat, 2006; Naila Ata, 2007)  and a small cascade of works on phrase based SMT systems  (Jawaid and Zeman, 2011; Ali et al., 2013; Jawaid et al., 2014a) , hierarchical MT systems  (Khan et al., 2013; Jawaid et al., 2014a)  and NMT using transfer learning from a high resource language  (Zoph et al., 2016) , it is still an arena requiring much work. Present study is a consolidated study in this regard. In this study we present results of some of the unexplored areas with reference to this language pair. Previous works have built general domain translation systems, we present a domain analysis on Technological, Religious and General domain translations (Section 5). This study is also an attempt to initiate the field of MT for Bio-medical domain despite zero resources available for the language pair. Effect of smaller and larger language models on translations are also explored. We have explored and used all the freely available English-Urdu corpora and also developed various small corpora by using human translations, synthetic corpora by machine translation and Hindi to Urdu transliteration. Starting with a brief review of previous works we describe the resources used in Section 3 followed by detailed results in Section 4 The paper concludes with a brief discussion on results. 

 Related Works Perhaps,  Tafseer and Alvi (2002)  presents one of the earliest attempts on English to Urdu translation based on transforming the parse tree of the English sentence to Urdu using transformation rules. Issues relating to translation for verbs in context of English to Urdu RBMT using lexical functional grammar are discussed by  (Karamat, 2006) . A minimal English to Urdu RBMT system is presented in (Naila  Ata, 2007)    (Jawaid and Zeman, 2011)  used phrase based models to solve the long distance word reordering problem between the two languages. They used Emille  (Baker et al., 2002) , Treebank  (Marcus et al., 1993) , Quran and Bible corpora and report improvement in BLEU scores by the proposed reordering scheme. Our general domain systems are built using these above mentioned corpora.  (Jawaid and Zeman, 2011)  used phrase based models to solve the long distance word reordering problem between the two languages. They used Emille  (Baker et al., 2002) , Treebank  (Marcus et al., 1993) , Quran and bible corpora and report improvement in BLEU scores by the proposed reordering scheme. We also use these corpora in our general domain systems. Building up on previous work  (Jawaid et al., 2014a)  present a comparison of phrase based versus hierarchical systems. They have added AFRL corpus (not free) to the earlier system and reported the hierarchical systems to outperform phrase based systems.  (Ali et al., 2010; Ali et al., 2013)  built SMT using parallel ahadith corpus from Sahih bukhari and Sahih Muslim.  (Khan et al., 2013 ) also presented a hierarchical SMT system. Several other studies have also contributed, for instance  (Shahnawaz and Mishra, 2013)  and  (Khan Jadoon et al., 2017)  present neural systems trained on small corpora.  

 Data Collection Data collection and its cleaning is an important but a challenging part for NLP, including machine translation. Our Data collection scheme included 1) an extensive search of all the freely available parallel corpora. 2) Synthetic parallel corpus creation using a good translation system and 3) transliteration from a highly similar language, Hindi. We have categorised the corpora in four categories, General, Biomedical, Religious and Technology, each explained in subsections 3.1, 3.2, 3.3, and 3.4 respectively. Corpus details are summarized in table 1. 

 General This section lists the corpora and their details for general category. 1. The Emille 1 corpus  (Baker et al., 2002)  is a collection of annotated, parallel and monolingual data in written and spoken form. It consists of multi domain corpora  (social, legal, educational, health, etc.)     (Marcus et al., 1993) . The Urdu corpus was available online and we were able to get English sentences from LDC Treebank. 3. Indic 3 is a freely available multi-domain parallel corpus created by using crowd-sourcing  (Post et al., 2012) . 4. TDIL 4 is an Indian Language Technology Proliferation and Deployment Center. We were able to get a sample of this corpus in domains of tourism, art, culture and architecture etc. 5. Opus 5 project  (Tiedemann, 2012)  provides freely available annotated corpora to the research community. We used their English-Urdu corpus comprising of Tanzil, Tatoeba, OpenSubtitles {2016, 2018}, Ubuntu, GNOME and Global Voices. Tanzil was a religious corpus, whereas Ubuntu and Gnome were technology related corpora. We further sub categorized these according to the domains as shown in table 1. 6. Flickr corpora are the human and automatic translations of the flickr 8 6 Image to text Corpus. The human translations are done from English captions to Urdu by human translators and Google translate was used for automatic translations. 7. National Language Translations (NLT) are the translation documents obtained from a translation agency. We collected translations of various articles, books, survey reports etc. The data collected was in raw form, it was cleaned and sentence aligned. 8. UMC002 Hindi-Urdu transliterations. Hindi and Urdu are almost similar languages having different writing scripts. To overcome data scarceness we experimented with transliterations from Hindi to Urdu. A similar scheme has been used by  (Durrani et al., 2014)  but in the opposite direction, .i.e they transliterated from Urdu to Hindi. 

 Bio-Medical Since no prior work exists in the Biomedical domain for English-Urdu, consequently there were no separate parallel corpora available. However, Emille corpus had a small part comprising of 0.055M English and 0.075 Urdu words respectively in health domain. We used these as Biomedical corpus. Furthermore, we developed Biomedical parallel corpora by using ideas from unsupervised learning techniques successfully used for other language pairs, where translations are used as additional bi-texts to cover up for data scarcity  (Lambert et al., 2011)  and domain adaptation (Abdul  Rauf et al., 2016; Hira et al., 2019) . We collected Biomedical parallel corpora from various sources and translated them. We are working on using domain adapted translation and language models for the biomedical domain, however, the translations used in this work are done using google translate. We used the following corpora: 1. Scielo 7 corpus contains documents retrieved from the scielo database comprising of titles and abstracts of published articles in bio-medical domain. Our Scielo corpus comprises of 0.022M sentences. Overall it contains 0.60M English and 0.65M Urdu words. 2. Jang 8 group of news is a Pakistan based media corporation. Their newspapers are published in both Urdu and English independently,but they are not the translations of each other. We cleaned and extracted 6k English sentences from the health news section and translated to Urdu to be used as parallel corpus. We got a corpus of 0.11M words in English and 0.14M words in Urdu. 3. EMEA 9 is a parallel corpus extracted out of documents published by European Medical Agency. The corpus is freely available in a number of language pairs but is not available in Urdu. We downloaded English part of corpus available in plain text and selected data related to medicines, disease, treatment and instructions. We automatically translated the extracted dataset and produced Urdu parallel translations. At the end of translation process we got a parallel dataset comprising of 1.03M words in Urdu and 0.82M words in English. 

 Religious This section lists the corpora and their details for religious category. 1. UMC005  (Jawaid and Zeman, 2011)  provides 6414 sentence pairs from Bible and 7957 sentence pairs form Quran corpus. 2. QBJ corpus, which is another collection of Quran+Bible+Joshua was also available online with their own test and dev sets. The data consists of 1.02M English words and 1.13M Urdu words. 3. Tanzil is a collection of online Quranic Translations by different scholars and is a sub part of OPUS corpus. The corpus contains 878 bi-texts with total of 0.75M sentence fragments having 19.0M English tokens and 23.1M Urdu tokens. 

 Technology This consists of English-Urdu Parallel corpus from localization files of Ubuntu and Gnome. Ubuntu contains 3.03k sentences and 0.1M, 0.2M English and Urdu tokens respectively, Gnome has 0.05M English and 0.06M Urdu tokens. 

 Monolingual Urdu Corpus Monolingual corpus is an essential resource for building language models for SMT. We used the corpus developed by  (Jawaid et al., 2014b) . This corpus consists of 95.4 million Urdu words, representing 5.4 million sentences of various domains including science, news, religion and education. We also collected Urdu monolingual documents from Jang (0.03M sentences) and other sources comprising of (0.06M sentences) as shown at the end of table 1. Urdu side of all parallel corpora was also used to build the large language model used in the indicated experiments in results. 

 Data Preprocessing Data cleaning and preprocessing is highly important for the performance of MT systems. The corpora provided by Emillie, NLT and Penn Tree-bank were partially parallel 9 http://opus.nlpl.eu/EMEA.php so we sentence aligned them using LF sentence aligner.  10  Due to the topological distance between the two languages we were not able to get fully aligned parallel corpus using LF aligner, thus manual alignment was done to ensure correctness. 

 Experimental Framework To demonstrate the performance of MT systems on the corpora collected and generated in this work, we performed a number of experiments for SMT and a few experiments for NMT. This section provides the description of the experimental frameworks and settings used for building SMT and NMT systems. 

 Statistical Machine Translation: The goal of SMT is to produce a target sentence e from a source sentence f . Among all possible target language sentences the one with the highest probability is chosen: e * = arg max e Pr(e|f ) (1) = arg max e Pr(f |e) Pr(e) (2) where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This approach is usually referred to as the noisy source-channel approach in SMT  (Brown et al., 1993) . Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model. Common practice is to use phrases as translation units  (Koehn et al., 2003; Och and Ney, 2003a ) instead of the original word-based approach. A phrase is defined as a group of source words f that should be translated together into a group of target words ?. The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P (?| f ) and P ( f |?). The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e * = arg max P r(e|f ) = arg max e {exp( i ? i h i (e, f ))} (3) The feature functions h i are the system models and the ? i weights are typically optimized to maximize a scoring function on a development set. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. To built standard phrase-based SMT systems we used Moses toolkit  (Koehn et al., 2007) , with the default settings for all the parameters. A 5-gram KenLM  (Heafield, 2011)  language model was used. For individual systems the language models were trained on the target side of the corpus. For experiments on size of the language model, all the available monolingual and target side corpus was used (122.5M Urdu words). 10 https://sourceforge.net/projects/aligner/ Word-alignment was done using Giza++  (Och and Ney, 2003b)  with grow-diag-final-and symmetrization method. Maximum sentence length was chosen to be 100. A distortion limit of 6 with 100-best list was used. Msdbidirectional-fe feature was used for lexical reordering with the phrase limit of 5. Systems were tuned on the development data using the MERT  (Och, 2003) . BLEU  (Papineni et al., 2002)  scores were computed on dev and test sets of the corpora, as well as on standard test sets. BLEU scores were calculated using multi-bleu.perl. Scoring is case sensitive and includes punctuation. 

 Neural Machine Translation: We used OpenNMT 11  (Klein et al., 2017)  for building Neural MT systems. Two layered encoder-decoder architecture with global attention  (Luong et al., 2015)  was used. We used RNN size of 500 and LSTM for cell structure for both encoder and decoder, applying dropout of 0.3 for each input cell. Translations were evaluated on BLEU scores to enable comparison with the corresponding SMT systems. 

 Development and Test sets Most of the corpora available online had their own development (dev) and test sets, so we evaluated the systems according to these dev and test sets. To be able to compare the systems in each domain, we created Standard test set (STS) for each domain comprising of 1k sentences. We randomly selected sentences from test sets of each data source of the particular domain. This was done on the basis of data set size and combined these specific sized chunks so that each data-set is represented on the basis of its size in the standard test set. We also used the test set of CLE 9 which was used to evaluate the general domain systems and the standard Scielo test set for Bio-Medical domain. 

 Results and Discussion One of the endeavours of our study is to present domain specific translation results. As is common in machine learning approaches, the domain of the system being built depends on the data used to train the system. MT performance quickly degrades when the testing domain is different from the training domain.  

 Standalone SMT Systems To build the best domain specific SMT system, we first explored the performance of each corpora for standalone SMT systems. T anzil and Genome showed the best performance for Religious and technology domains respectively. While over-fitting is observed in these two domains. The performance of the systems, built for these two domains, have shown a uniform trend for both self and standard test sets. 

 Effect of size of Language Model Along with, the exploration of best SMT system for each category we also investigated the effect of the size of language model on each standalone SMT system. To explore this dimension, a large language model was also build by concatenating the Urdu text of all the bi-texts and the monolingual corpus mentioned in section 3.5. The scores for large LM are shown in the third column in table 2. It is observed that the BLEU scores of all the standalone systems approximately doubled with large LM. Figure  1  shows these results graphically for each domain. These results highlight the effect of bigger language model on SMT quality, obviously a bigger language model helps improve translation quality by improving the grammar of the output sentences. 

 Concatenated SMT Systems After building standalone systems for each corpus, we selected the corpora which resulted in best BLEU scores, for building systems by concatenating different combinations of corpora. We selected systems on the basis of best score among the standalone systems from each domain (baseline system) and concatenated them with system having second highest BLEU score. Table  3  reports these results.  Table  3 : Results of SMT on baselines and addition of bitexts. 

 Bio-Medical Domain Bio-medical domain is an interesting domain as the corpora are not of same type. Emille are the health domain sentences taken from the Emille corpus, Jang sentences are taken from a semi-parallel comparable corpus and then sentence aligned and human corrected. Whereas, EM EA and Scielo are synthetic forward translated corpora. EM EA was chosen as baseline, for bio-medical domain, having the highest score 44.45 amongst other three standalone systems. Then, we built a system on EM EA concatenated with the second best system Scielo, having score of 25.95 (table  2 ). The BLEU score of the resultant system EM EA+Scielo is 50.34 (table  3 ). We can see an improvement in the score after concatenation of these two data-sets. Note that this system is built with only forward translated synthetic corpus, and we get an appreciable BLEU score. This system EM EA + Scielo, is further concatenated with jang corpus (standalone score 17.78) and the resultant score of the EM EA + scielo + jang system is 49.76, which is a bit lower than the previous system's score. Contrary to the standard test set scores, addition of bitexts did not improve scores for dev and test, rather resulted in a de- Emille is again a standard biomedical corpus comprising of health documents from the EMILLE corpus (section 3.6), and its concatenation improved the overall BLEU score. An increase of 6.26 points upon the addition of just 0.86M words of Scielo+Jang +Emille corpora to 1.03M words of EMEA (baseline), has been observed which is a significant gain. These are encouraging results for the development of standard corpora for the Bio-medical domain. 

 Religious, General and Technology Domain For the religious domain we have two corpora namely T anzil and the other is concatenation of Quran, Bible and Joshua(QBJ). Firstly we built two standalone systems for both corpora as shown in  

 Impact of Various Corpora We performed series of experiments using transliterations, human and machine translated data to compare the performance of such systems. These results are reported in  

 NMT Systems We are presenting NMT system performance only for Bio-Medical domain. Table  5  shows the results of our experiments for NMT. We maintained the same baseline and corpus concatenation combination as used in SMT experiments. The results of Bio-Medical NMT are lower than the corresponding SMT systems (Table  3 ). This is expected as NMT systems don't perform well with small amounts of corpus. A unanimous observation is that addition of bitexts improves the systems across all dev and test sets, a slight deviation to this trend is observed when Emille is added to EM EA + Scielo + jang (last row in Table  5  ). 

 Conclusion We presented domain based results on SMT and NMT systems for translation from English to Urdu. This is the first work being reported on several domains for the English-Urdu language pair. We collected corpora for four main domains namely Bio-medical, Religious, Technology and General. We experimented with various methods to reduce data scarcity which include, the use of automatic translations and transliterations. We also collected and compiled human translations from translation agencies as well as produced human translations of Flickr 8k dataset. We performed series of experiments in attempts to optimize the performance of each system and also to study the impact of data sources on the systems. Finally, we established a comparison of the data sources and the effect of Language Model size on statistical machine translation performance. Figure 1 : 1 Figure 1: Comparison of systems built on small and large language model (x-axis represents words in millions) 
