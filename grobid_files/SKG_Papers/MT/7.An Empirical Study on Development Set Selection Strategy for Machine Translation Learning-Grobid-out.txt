title
An Empirical Study on Development Set Selection Strategy for Machine Translation Learning *

abstract
This paper describes a statistical machine translation system for our participation for the WMT10 shared task. Based on MOSES, our system is capable of translating German, French and Spanish into English. Our main contribution in this work is about effective parameter tuning. We discover that there is a significant performance gap as different development sets are adopted. Finally, ten groups of development sets are used to optimize the model weights, and this does help us obtain a stable evaluation result.

Introduction We present a machine translation system that represents our participation for the WMT10 shared task from Brain-like Computing and Machine Intelligence Lab of Shanghai Jiao Tong University (SJTU-BCMI Lab). The system is based on the state-of-the-art SMT toolkit MOSES . We use it to translate German, French and Spanish into English. Though different development sets used for training parameter tuning will certainly lead to quite different performance, we empirically find that the more sets we combine together, the more stable the performance is, and a development set similar with test set will help the performance improvement. 

 System Description The basic model of the our system is a log-linear model  (Och and Ney, 2002) . For given source lan-guage strings, the target language string t will be obtained by the following equation, tI 1 = arg max t I 1 {p ? m 1 (t I 1 | s J 1 )} = arg max t I 1 { exp[ M m=1 ? m h m (t I 1 , s J 1 )] tI 1 exp[ M m=1 ? m h m ( tI 1 , s J 1 )] }, where h m is the m-th feature function and ? m is the m-th model weight. There are four main parts of features in the model: translation model, language model, reordering model and word penalty. The whole model has been well implemented by the state-of-the-art statistical machine translation toolkit MOSES. For each language that is required to translated into English, two sets of bilingual corpora are provided by the shared task organizer. The first set is the new release (version 5) of Europarl corpus which is the smaller. The second is a combination of other available data sets which is the larger. In detail, two corpora, europarl-v5 and news-commentary10 are for German, europarl-v5 and news-commentary10 plus undoc for French and Spanish, respectively. Details of training data are in Table  1 . Only sentences with length 1 to 40 are acceptable for our task. We used the larger set for our primary submission. We adopt word alignment toolkit GIZA++  (Och and Ney, 2003)  to learn word-level alignment with its default setting and grow-diag-final-and parameters. Given a sentence pair and its corresponding word-level alignment, phrases will be extracted by using the approach in  (Och and Ney, 2004) . Phrase probability is estimated by its relative frequency in the training corpus. Lexical reordering is determined by using the default setting of MOSES with msd-bidirectional parameter. For training the only language model (English), the data sets are extracted from monolingual parts of both europarl-v5 and news-commentary10, which include 1968914 sentences and 47.48M words. And SRILM is adopted with 5-gram, interpolate and kndiscount settings  (Stolcke, 2002)  . The next step is to estimate feature weights by optimizing translation performance on a development set. We consider various combinations of 10 development sets with 18207 sentences to get a stable performance in our primary submission. We use the default toolkits which are provided by WMT10 organizers for preprocessing (i.e., tokenize) and postprocessing (i.e., detokenize, recaser). 

 Development Set Selection 

 Motivation Given the previous feature functions, the model weights will be obtained by optimizing the following maximum mutual information criterion, which can be derived from the maximum entropy principle: ?M 1 = arg max ? M 1 { S i=1 log p ? M 1 (t i | s i )} As usual, minimum error rate training (MERT) is adopted for log-linear model parameter estimation  (Och, 2003) . There are many improvements on MERT in existing work  (Bertoldi et al., 2009; Foster and Kuhn, 2009) , but there is no demonstration that the weights with better performance on the development set would lead to a better result on the unseen test set. In our experiments, we found that different development sets will cause significant BLEU score differences, even as high as one percent. Thus the remained problem will be how to effectively choose the development set to obtain a better and more stable performance. 

 Experimental Settings Our empirical study will be demonstrated through German to English translation on the smaller corpus. The development sets are all development sets and test sets from the previous WMT shared translation task as shown in Table  2 , and labeled as dev-0 to dev-9. Meanwhile, we denote 10 batch sets from batch-0 to batch-9 where the batch-i set is the combination of dev-sets from dev-0 to dev-i.  

 On the Scale of Development Set Having 20 different development sets (10 dev-sets and batch-sets), 20 models are correspondingly trained.The decode results on the test set are summarized in Table  3  and Figure  1 . The dotted lines are the performances of 10 different development sets on the two test sets, we will see that there is a huge gap between the highest and the lowest score, and there is not an obvious rule to follow. It will bring about unsatisfied results if a poor development set is chosen. The solid lines represents the performances of 10 incremental batch sets on the two test sets, the batch processing still gives a poor performance at the beginning, but the results become better and more stable when the development sets are continuously enlarged. This sort of results suggest that a combined development set may produce reliable results in the worst case. Our primary submission used the combined development set and the results as  

 On BLEU Score Difference To compare BLEU score differences between test set and development set, we consider two groups of BLEU score differences, For each development set, dev-i, the BLEU score difference will be computed between b 1 from which adopts itself as the development set and b 2 from which adopts test set as the development set. For the test set, the BLEU score difference will be computed between b 1 from which adopts each development set, dev-i, as the development set and b 2 from which adopts itself as the development set. These two groups of results are illustrated in Figure  2  (the best score of the test set under self tuning, newstest2009 is 17.91). The dotted lines have the inverse trend with the dotted in Figure  1 (because the addition of these two values is constant), and the solid lines have the same trend with the dotted, which means that the good performance is mutual between test set and development sets: if tuning using A set could make a good result over B set, then vice versa. 

 On the Similarity between Development Set and Test Set This experiment is motivated by  (Utiyama et al., 2009) , where they used BLEU score to measure the similarity of a sentences pair and then extracted sentences similar with those in test set to The trend of BLEU score differences construct a specific tuning set. In our experiment, we will try to measure data set similarity instead. Given two sets of sentences, one is called as candidate(cnd) set and the other reference(ref) set. For any cnd sentence, we let the whole ref set to be its reference and then multi-references BLEU score is computed for cnd set. There comes a problem that the sentence penalty will be constant for any cnd sentence, we turn to calculate the average length of whose sentences which have common n-gram with the given cnd sentence. Now we may define three measures. The measure which uses dev-and batch-sets as cnd sets and news-test2009 set as ref set is defined as precision-BLEU , and the measure which uses the above sets on the contrary way is defined as recall-BLEU. Then F1-BLEU is defined as the harmonic mean of precision-BLEU and recall-BLEU. These results are illustrated in Figure  3 . From the figure, we find that F1-BLEU plays an important role to predict the goodness of a development set, F1-BLEU scores of batch-sets have an ascending curve and batch data set sequence will cause a stable good test performance, the point on dev-sets which has high F1-BLEU(eg, dev-0,4,5) would also has a good test performance. 

 Related Work The special challenge of the WMT shared task is domain adaptation, which is a hot topic in recent years and more relative to our experiments. Many existing works are about this topic  (Koehn and Schroeder, 2007; Nakov, 2008; Nakov and Ng, 2009; Paul et al., 2009; Haque et al., 2009) . However, most of previous works focus on language model, translation phrase table, lexicons model and factored translation model, few of them pay attention to the domain adaptation on the development set. For future work we consider to use some machine learning approaches to select sentences in development sets more relevant with the test set in order to further improve translation performance. 

 Conclusion In this paper, we present our machine translation system for the WMT10 shared task and perform an empirical study on the development set selection. According to our experimental results, Choosing different development sets would play an important role for translation performance. We find that a development set with higher F1-BLEU yields better and more stable results.    Figure 2: The trend of BLEU score differences 
