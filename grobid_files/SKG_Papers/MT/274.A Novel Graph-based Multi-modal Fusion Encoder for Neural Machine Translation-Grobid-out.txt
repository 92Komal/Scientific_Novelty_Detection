title
A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation

abstract
Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.

Introduction Multi-modal neural machine translation (NMT)  (Huang et al., 2016;  has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information  (Zhou et al., 2018) . It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based translation, since the visual context helps to resolve ambiguous multi-sense words  (Ive et al., 2019) . Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens  (Huang et al., 2016; , or to learn the joint multi-modal representation  (Zhou et al., 2018; Calixto et al., 2019) ; (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context  (Huang et al., 2016; Ive et al., 2019) ; and (3) representing each image as spatial features, which can be exploited as extra context  Delbrouck and Dupont, 2017a; Ive et al., 2019) , or a supplement to source semantics  (Delbrouck and Dupont, 2017b ) via an attention mechanism. Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair. For example, as shown in Figure  1 , the noun phrase "a toy car" semantically corresponds to the blue dashed region. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation. However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions  Tan and Bansal, 2019) .  

 Image Text Two boys are playing with a toy car In this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. We first represent the input sentence and image with a unified multi-modal graph. In this graph, each node indicates a semantic unit: textual word or visual object, and two types of edges are introduced to model semantic relationships between semantic units within the same modality (intra-modal edges) and semantic correspondences between semantic units of different modalities (inter-modal edges) respectively. Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding. Particularly, during this process, we distinguish the parameters of two modalities, and sequentially conduct intraand inter-modal fusions to learn multi-modal node representations. Finally, these representations can be exploited by the decoder via an attention mechanism. Compared with previous models, ours is able to fully exploit semantic interactions among multimodal semantic units for NMT. Overall, the major contributions of our work are listed as follows: ? We propose a unified graph to represent the input sentence and image, where various semantic relationships between multi-modal semantic units can be captured for NMT. ? We propose a graph-based multi-modal fusion encoder to conduct graph encoding based on the above graph. To the best of our knowledge, our work is the first attempt to explore multimodal graph neural network (GNN) for NMT. ? We conduct extensive experiments on Multi30k datasets of two language pairs. Experimental results and in-depth analysis indicate that our encoder is effective to fuse multi-modal information for NMT. Particularly, our multi-modal NMT model significantly outperforms several competitive baselines. ? We release the code at https://github.com/ DeepLearnXMU/GMNMT. 

 NMT with Graph-based Multi-modal Fusion Encoder Our multi-modal NMT model is based on attentional encoder-decoder framework with maximizing the log likelihood of training data as the objective function. 

 Encoder Essentially, our encoder can be regarded as a multimodal extension of GNN. To construct our encoder, we first represent the input sentence-image pair as a unified multi-modal graph. Then, based on this graph, we stack multiple multi-modal fusion layers to learn node representations, which provides the attention-based context vector to the decoder. 

 Multi-modal Graph In this section, we take the sentence and the image shown in Figure  1  as an example, and describe how to use a multi-modal graph to represent them. Formally, our graph is undirected and can be formalized as G=(V ,E), which is constructed as follows: In the node set V , each node represents either a textual word or a visual object. Specifically, we adopt the following strategies to construct these two kinds of nodes: (1) We include all words as separate textual nodes in order to fully exploit textual information. For example, in Figure  1 , the multimodal graph contains totally eight textual nodes, each of which corresponds to a word in the input sentence; (2) We employ the Stanford parser to identify all noun phrases in the input sentence, and then apply a visual grounding toolkit  (Yang et al., 2019)  to detect bounding boxes (visual objects) for each noun phrase. Subsequently, all detected visual objects are included as independent visual nodes. In this way, we can effectively reduce the negative impact of abundant unrelated visual objects. Let us revisit the example in Figure  1 , where we can identify two noun phrases "Two boys" and "a toy car" from the input sentence, and then include three visual objects into the multi-modal graph. To capture various semantic relationships between multi-modal semantic units for NMT, we consider two kinds of edges in the edge set E: (1) Any two nodes in the same modality are connected by an intra-modal edge; and (2) Each textual node representing any noun phrase and the corresponding visual node are connected by an inter-modal edge. Back to Figure  1 , we can observe that all visual nodes are connected to each other, and all textual nodes are fully-connected. However, only nodes v o 1 and v x 1 , v o 1 and v x 2 , v o 2 and v x 1 , v o 2 and v x 2 , v o 3 and v x 6 , v o 3 and v x 7 , v o 3 and v x 8 are connected by inter-modal edges. 

 Embedding Layer Before inputting the multi-modal graph into the stacked fusion layers, we introduce an embedding layer to initialize the node states. Specifically, for each textual node v x i , we define its initial state H (0) x i as the sum of its word embedding and position encoding  (Vaswani et al., 2017) . To obtain the initial state H (0) o j of the visual node v o j , we first extract visual features from the fully-connected layer that follows the ROI pooling layer in Faster-RCNN  (Ren et al., 2015) , and then employ a multilayer perceptron with ReLU activation function to project these features onto the same space as textual representations. 

 Graph-based Multi-modal Fusion Layers As shown in the left part of Figure  2 , on the top of embedding layer, we stack L e graph-based multimodal fusion layers to encode the above-mentioned multi-modal graph. At each fusion layer, we sequentially conduct intra-and inter-modal fusions to update all node states. In this way, the final node states encode both the context within the same modality and the cross-modal semantic information simultaneously. Particularly, since visual nodes and textual nodes are two types of semantic units containing the information of different modalities, we apply similar operations but with different parameters to model their state update process, respectively. Specifically, in the l-th fusion layer, both updates of textual node states H (l) x ={H (l) x i } and visual node states H (l) o ={H (l) o j } mainly involve the following steps: Step1: Intra-modal fusion. At this step, we employ self-attention to generate the contextual representation of each node by collecting the message from its neighbors of the same modality. Formally, the contextual representations C (l) x of all textual nodes are calculated as follows: 1 C (l) x = MultiHead(H (l?1) x , H (l?1) x , H (l?1) x ), (1) where MultiHead(Q, K, V) is a multi-head selfattention function taking a query matrix Q, a key matrix K, and a value matrix V as inputs. Similarly, we generate the contextual representations C (l) o of all visual nodes as C (l) o = MultiHead(H (l?1) o , H (l?1) o , H (l?1) o ). (2) In particular, since the initial representations of visual objects are extracted from deep CNNs, we apply a simplified multi-head self-attention to preserve the initial representations of visual objects, where the learned linear projects of values and final outputs are removed. Step2: Inter-modal fusion. Inspired by studies in multi-modal feature fusion  (Teney et al., 2018; Kim et al., 2018) , we apply a cross-modal gating mechanism with an element-wise operation to gather the semantic information of the cross-modal neighbours of each node. Concretely, we generate the representation M (l) x i of a text node v x i in the following way: M (l) x i = j?A(vx i ) ? i,j C (l) o j , (3) ? i,j = Sigmoid(W (l) 1 C (l) x i + W (l) 2 C (l) o j ), (4) where A(v x i ) is the set of neighboring visual nodes of v x i , and W (l) 1 and W (l) 2 are parameter matrices. Likewise, we produce the representation M (l) o j of a visual node v o j as follows: M (l) o j = i?A(vo j ) ? j,i C (l) x i , (5) ? j,i = Sigmoid(W (l) 3 C (l) o j + W (l) 4 C (l) x i ), (6) where A(v o j ) is the set of adjacent textual nodes of v o j , and W (l) 3 and W (l) 4 are also parameter matrices. The advantage is that the above fusion approach can better determine the degree of inter-modal fusion according to the contextual representations of each modality. Finally, we adopt position-wise feed forward networks FFN( * ) to generate the textual node states H (l) x and visual node states H (l) o : H (l) x = FFN(M (l) x ), (7) H (l) o = FFN(M (l) o ), (8) where M (l) x = {M (l) x i }, M (l) o = {M (l) o j } denote the above updated representations of all textual nodes and visual nodes respectively. 

 Decoder Our decoder is similar to the conventional Transformer decoder. Since visual information has been incorporated into all textual nodes via multiple graph-based multi-modal fusion layers, we allow the decoder to dynamically exploit the multi-modal context by only attending to textual node states. As shown in the right part of Figure  2 , we follow  Vaswani et al. (2017)  to stack L d identical layers to generate target-side hidden states, where each layer l is composed of three sub-layers. Concretely, the first two sub-layers are a masked self-attention and an encoder-decoder attention to integrate targetand source-side contexts respectively:  9 ) E (l) = MultiHead(S (l?1) , S (l?1) , S (l?1) ), ( T (l) = MultiHead(E (l) , H (Le) x , H (Le) x ), (10) where S (l?1) denotes the target-side hidden states in the l-1-th layer. In particular, S (0) are the embeddings of input target words. Then, a position-wise fully-connected forward neural network is uesd to produce S (l) as follows: S (l) = FFN(T (l) ). (11) Finally, the probability distribution of generating the target sentence is defined by using a softmax layer, which takes the hidden states in the top layer as input: P (Y |X, I) = t Softmax(WS (L d ) t + b), ( 12 ) where X is the input sentence, I is the input image, Y is the target sentence, and W and b are the parameters of the softmax layer. 

 Experiment We carry out experiments on multi-modal English?German (En?De) and English?French (En?Fr) translation tasks. 

 Setup Datasets We use the Multi30K dataset  (Elliott et al., 2016) , where each image is paired with one English description and human translations into German and French. Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively. In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively. Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding  (Sennrich et al., 2016)  with 10,000 merge operations. Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by  Yang et al. (2019)  to detect associated visual objects of the identified noun phrases. For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects. In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively. 3 Finally, we compute 2,048-dimensional features for these objects with the pre-trained ResNet-100 Faster-RCNN  (Ren et al., 2015) . Settings We use Transformer  (Vaswani et al., 2017)  as our baseline. Since the size of training corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En?De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has L d =4 layers 4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as  (Vaswani et al., 2017) . Finally, we use the metrics BLEU  (Papineni et al., 2002)  and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. Baseline Models In addition to the text-based Transformer  (Vaswani et al., 2017) , we adapt several effective approaches to Transformer using our visual features, and compare our model with them 5 : ? ObjectAsToken(TF)  (Huang et al., 2016) . It is a variant of the Transformer, where all visual objects are regarded as extra source tokens and placed at the front of the input sentence. ? Enc-att(TF)  (Delbrouck and Dupont, 2017b ). An encoder-based image attention mechanism is incorporated into Transformer, which augments each source annotation with an attention-based visual feature vector. ? Doubly-att(TF)  (Helcl et al., 2018) . It is a doubly attentive Transformer. In each decoder layer, a cross-modal multi-head attention sublayer is inserted before the fully connected feed-forward layer to generate the visual context vector from visual features. We also display the performance of several dominant multi-modal NMT models such as Doubly-att(RNN) , Soft-att(RNN)  (Delbrouck and Dupont, 2017a) , Stochastic-att(RNN)  (Delbrouck and Dupont, 2017a) , Fusion-conv(RNN)  (Caglayan et al., 2017) , Trg-mul(RNN)  (Caglayan et al., 2017) , VMMT(RNN)  (Calixto et al., 2019)  and Deliberation Network(TF)  (Ive et al., 2019)  on the same datasets. 

 Effect of Graph-based Multi-modal Fusion Layer Number L e The number L e of multi-modal fusion layer is an important hyper-parameter that directly determines the degree of fine-grained semantic fusion in our encoder. Thus, we first inspect its impact on the EN?DE validation set. Figure  3  provides the experimental results using different L e and our model achieves the best performance when L e is 3. Hence, we use L e =3 in all subsequent experiments. 

 Results on the En?De Translation Task Table  1  shows the main results on the En?De translation task. Ours outperforms most of the existing models and all baselines, and is comparable to Fusion-conv(RNN) and Trg-mul(RNN) on ME-TEOR. The two results are from the state-of-the-art system on the WMT2017 test set, which is selected based on METEOR. Comparing the baseline models, we draw the following interesting conclusions: First, our model outperforms ObjectAsToken(TF), which concatenates regional visual features with text to form attendable sequences and employs self-attention mechanism to conduct intermodal fusion. The underlying reasons consist of two aspects: explicitly modeling semantic correspondences between semantic units of different modalities, and distinguishing model parameters for different modalities. Second, our model also significantly outperforms Enc-att(TF). Note that Enc-att(TF) can be considered as a single-layer semantic fusion encoder. In addition to the advantage of explicitly modeling semantic correspondences, we conjecture that multi-layer multi-modal semantic interactions are also beneficial to NMT. Third, compared with Doubly-att(TF) simply using an attention mechanism to exploit visual in- formation, our model achieves a significant improvement, because of sufficient multi-modal fusion in our encoder. Besides, we divide our test sets into different groups based on the lengths of source sentences and the numbers of noun phrases, and then compare the performance of different models in each group. Figures  4 and 5  report the BLEU scores on these groups. Overall, our model still consistently achieves the best performance in all groups. Thus, we confirm again the effectiveness and gen- Table  2 : Ablation study of our model on the EN?DE translation task. 

 Model En?Fr Test2016 Test2017 BLEU METEOR BLEU METEOR Existing Multi-modal NMT Systems Fusion-conv(RNN)  (Caglayan et al., 2017)  53.5 70.4 51.6 68.6 Trg-mul(RNN)  (Caglayan et al., 2017)  54.7 71.3 52.7 69.5 Deliberation Network(TF)  (Ive et al., 2019)  59.8 74.4 --Our Multi-modal NMT Systems Transformer  (Vaswani et al., 2017)  59.5 73.7 52.0 68.0 ObjectAsToken(TF)  (Huang et al., 2016)  60.0 74.3 52.9 68.6 Enc-att(TF)  (Delbrouck and Dupont, 2017b)  60.0 74.3 52.8 68.3 Doubly-att(TF)  (Helcl et al., 2018)  59.9 74.1 52.4 68.1 Our model 60.9 74.9 53.9 69.3  erality of our proposed model. Note that in the sentences with more phrases, which are usually long sentences, the improvements of our model over baselines are more significant. We speculate that long sentences often contain more ambiguous words. Thus compared with short sentences, long sentences may require visual information to be better exploited as supplementary information, which can be achieved by the multi-modal semantic interaction of our model. We also show the training and decoding speed of our model and the baselines in Table  4 . During training, our model can process approximately 1.1K tokens per second, which is comparable to other multi-modal baselines. When it comes to decoding procedure, our model translates about 16.7 sentences per second and the speed drops slightly compared to Transformer. Moreover, our model only introduces a small number of extra parameters and achieves better performance. 

 Ablation Study To investigate the effectiveness of different components, we further conduct experiments to compare our model with the following variants in Table  2:  (1) w/o inter-modal fusion. In this variant, we apply two separate Transformer encoders to learn the semantic representations of words and visual objects, respectively, and then use the doublyattentive decoder  (Helcl et al., 2018)  to incorporate textual and visual contexts into the decoder. The result in line 3 indicates that removing the intermodal fusion leads to a significant performance drop. It suggests that semantic interactions among multi-modal semantic units are indeed useful for multi-modal representation learning. (2) visual grounding ? fully-connected. We make the words and visual objects fully-connected to establish the inter-modal correspondences. The result in line 4 shows that this change causes a significant performance decline. The underlying reason is the fully-connected semantic correspondences introduce much noise to our model. (3) different parameters ? unified parameters. When constructing this variant, we assign unified parameters to update node states in different modalities. Apparently, the performance drop reported in line 5 also demonstrates the validity of our ap-proach using different parameters. (4) w/ attending to visual nodes. Different from our model attending to only textual nodes, we allow our decoder of this variant to consider both two types of nodes using doubly-attentive decoder. From line 6, we can observe that considering all nodes does not bring further improvement. The result confirms our previous assumption that visual information has been fully incorporated into textual nodes in our encoder. (5) attending to textual nodes ? attending to visual nodes. However, when only considering visual nodes, the model performance drops drastically (line 7). This is because the number of visual nodes is far fewer than that of textual nodes, which is unable to produce sufficient context for translation. 

 Case Study Figure  6  displays the 1-best translations of a sampled test sentence generated by different models. The phrase "a skateboarding ramp" is not translated correctly by all baselines, while our model correctly translates it. This reveals that our encoder is able to learn more accurate representations. 

 Results on the En?Fr Translation Task We also conduct experiments on the EN?Fr dataset. From Table  3 , our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT. 

 Related Work Multi-modal  NMT Huang et al. (2016)  first incorporate global or regional visual features into attention-based NMT.  also study the effects of incorporating global visual features into different NMT components.  Elliott and K?d?r (2017)  share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT  (Caglayan et al., 2016; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018) . Recently,  Ive et al. (2019)  propose a translate-and-refine approach and  Calixto et al. (2019)  employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design,  Elliott (2018)  reveal that visual information seems to be ignored by the multimodal NMT models.  Caglayan et al. (2019)  conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. Note that if we directly adapt the approach proposed by  Huang et al. (2016)  into Transformer, the model (ObjectAsToken(TF)) also involves multimodal fusion. However, ours is different from it in following aspects: (1) We first learn the contextual representation of each node within the same modality, so that it can better determine the degree of inter-modal fusion according to its own context. (2) We assign different encoding parameters to different modalities, which has been shown effective in our experiments. Additionally, the recent study LXMERT  (Tan and Bansal, 2019 ) also models relationships between vision and language, which differs from ours in following aspects: (1)  Tan and Bansal (2019)  first apply two transformer encoders for two modalities, and then stack two cross-modality encoders to conduct multi-modal fusion. In contrast, we sequentially conduct self-attention and cross-modal gating at each layer. (2)  Tan and Bansal (2019)  leverage an attention mechanism to implicitly establish cross-modal relationships via large-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in  (Tan and Bansal, 2019) . Graph Neural Networks Recently, GNNs  (Marco Gori and Scarselli, 2005)  including gated graph neural network  (Li et al., 2016) , graph convolutional network  (Duvenaud et al., 2015; Kipf and Welling, 2017)  and graph attention network  (Velickovic et al., 2018)  have been shown effective in many tasks such as VQA  (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019 ), text generation  (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b ) and text representation  Yin et al., 2019;   

 Source: A boy riding a skateboard on a skateboarding ramp . 

 Reference: Ein junge f?hrt skateboard auf einer skateboardrampe . 

 Tranformer: Ein junge f?hrt auf einem skateboard auf einer rampe . 

 Doubly-att(TF): Ein junge f?hrt mit einem skateboard auf einer rampe . 

 Enc-att(TF): Ein junge f?hrt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge f?hrt auf einem skateboard auf einer rampe . 

 Our model: Ein junge f?hrt auf einem skateboard auf einer skateboardrampe . 2018a;  Xue et al., 2019) . In this work, we mainly focus on how to extend GNN to fuse multi-modal information in NMT. Close to our work,  Teney et al. (2017)  introduce GNN for VQA. The main difference between their work and ours is that they build an individual graph for each modality, while we use a unified multimodal graph. 

 Conclusion In this paper, we have proposed a novel graphbased multi-modal fusion encoder, which exploits various semantic relationships between multimodal semantic units for NMT. Experiment results and analysis on the Multi30K dataset demonstrate the effectiveness of our model. In the future, we plan to incorporate attributes of visual objects and dependency trees to enrich the multi-modal graphs. Besides, how to introduce scene graphs into multi-modal NMT is a worthy problem to explore. Finally, we will apply our model into other multi-modal tasks such as multimodal sentiment analysis. Figure 1 : 1 Figure1: The multi-modal graph for an input sentence-image pair. The blue and green solid circles denote textual nodes and visual nodes respectively. An intra-modal edge (dotted line) connects two nodes in the same modality, and an inter-modal edge (solid line) links two nodes in different modalities. Note that we only display edges connecting the textual node "playing" and other textual ones for simplicity. 
