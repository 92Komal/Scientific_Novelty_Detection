title
Multimodal Neural Machine Translation for English to Hindi

abstract
Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-theart results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for low resource languages. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the system for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in English to Hindi. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multimodal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves Bilingual Evaluation Understudy (BLEU) score of 33.57, Rankbased Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.

Introduction Multi-modal NMT aims to draw information from the input data from different modalities like text, image, and audio. By combining information from more than one modality, it attempts to amend the quality of low resource language translation. The work undertaken by  (Shah et al., 2016)  merges the visual features of images from the corresponding input data with textual features of the input bitext to translate sentences, which outperforms text-only translation. For text-only based NMT, encoder-decoder architecture is a widely used technique in the MT community. Because it handles various issues like variable-length phrases using sequence to sequence learning, the problem of long term dependency using Long Short Term Memory (LSTM)  (Sutskever et al., 2014) . However, in the case of very long sentences, the basic encoderdecoder architecture is unable to encode all the information. To resolve this issue, the attention mechanism is proposed which pays attention to all source words locally as well as globally  (Bahdanau et al., 2015; Luong et al., 2015) . For Indian language translation, attention-based NMT yields remarkable performance  Laskar et al., 2019b,a) . Besides, without modifying the system architecture, NMT performance can be improved using monolingual data  (Sennrich et al., 2016; Zhang and Zong, 2016) , which is very effective in the case of low resource language translation. This paper investigates English to Hindi translation using the multimodal concept with monolingual data to improve the translation quality at the WAT2020 translation task. 

 Related Works The literature survey finds out very limited existing works on English-Hindi language pair translation using multi-modal NMT  (Dutta Chowdhury et al., 2018; Sanayai Meetei et al., 2019; Laskar et al., 2019c) . The work by  (Dutta Chowdhury et al., 2018)     (Nakazawa et al., 2020; Parida et al., 2019) . NMT settings of , achieves BLEU score 24.2 for Hindi to English translation. Moreover, in the WAT2019 multi-modal translation task of English to Hindi,  (Sanayai Meetei et al., 2019)  based on recurrent neural network (RNN)  achieves BLEU score of 12.58, 28.45 for the challenge and evaluation test respectively. And, on the same task of WAT2019, we have achieved the highest BLEU score of 20.37, 40.55 for the challenge and evaluation test respectively  (Laskar et al., 2019c) . We have used RNN encoder and doubly-attentive RNN decoder based model . The loophole in the existing works of English to Hindi translation using multi-modal NMT is that they have not used monolingual data to improve the performance of multi-modal NMT  (Sennrich et al., 2016) . In this paper, we have used monolingual corpus in the pre-training step to enhance the performance of the multi-modal NMT for English to Hindi translation respectively. 

 Dataset Description Hindi Visual Genome 1.1 consists of parallel text and image data, which is provided by the WAT2020 organizers  (Nakazawa et al., 2020; Parida et al., 2019) .  IITB 1  (Kunchukuttan et al., 2018)  and English monolingual data from WMT16 2 as shown in Table  2 . 

 System Description We have used OpenNMT-py  (Klein et al., 2017)  to setup our multi-modal NMT and text-only NMT systems. The key process of the operations include data preprocessing, system training to generate an optimum trained model, and then obtained trained model is used in the testing/translation process to predict translation on the given unseen data. 

 Data Preprocessing For multi-modal translation, pre-trained CNN with VGG19 is used for the extraction of global and local features from the provided image dataset. The pre-trained CNN with VGG19 is publicly available in OpenNMT-py. In the text-only and multi-modal task, we have used GloVe  (Pennington et al., 2014)  to pretrain on monolingual data of English-Hindi and generated global vectors of word embedding. The OpenNMT-py tool is used to create a vocabulary size of 5004 for both source and target sentences. We have not used any word-segmentation technique. Our    

 Training The training process for each track is carried out separately. For multi-modal translation, the obtained pretrained vectors, extracted visual features from data preprocessing are fine-tuned with the parallel text data during the training process. We have used bidirectional RNN (BRNN) at encoder type and doubly-attentive RNN at decoder type following default settings of . BRNN uses two distinct RNN, one for the forward direction and another for backward, and two different attention mechanisms are incorporated across the source words and visual features at a single RNN decoder. Two layer LSTM networks having 500 nodes in each layer are used in both encoder and decoder. Our multi-modal NMT is trained on a single GPU up to 40 epochs with 0.3 drop out, batch size 40 and the best model is obtained at epoch 10. For text-only translation, we have not used visual features and only used pretrained vectors of monolingual data to fine-tune with parallel corpus in the training process. The text-only NMT is trained up to 20,000 epoch since learning curve raises up to 18,000 and then drops. We have selected best trained model at epoch 18,000. The difference between  (Laskar et al., 2019c)  and this paper, is that in this work, our multi-modal NMT adopts BRNN at encoder type unlike RNN in  (Laskar et al., 2019c)  and utilizes pretrain word embeddings of monolingual corpus. 

 Testing In this process, the obtained trained models of both multi-modal and text-only NMT system, are used to translate the given test data in each track separately. 

 Result and Analysis The WAT2020 translation task organizer declared the evaluation result 3 of multi-modal translation task for English to Hindi and our system's results are presented in Table  3 . Our team name is CNLP-NITS and participated in text-only and multi-modal submission track of the same task. In text-only translation submission track, a total of four teams participated for both challenges and evaluation test data and for multi-modal translation submission track, only our team participated. The submitted predicted translations are evaluated via standard evaluation metrics namely, BLEU  (Papineni et al., 2002) , RIBES  (Isozaki et al., 2010)  and AMFM  (Banchs et al., 2015) . From the Table  2 , it is observed that our multi-modal NMT system obtained higher scores on the ground of BLEU, RIBES, AMFM than our text-only NMT system. This reasons about combination of visual and textual features in multi-modal NMT shows better performance than only textual features based NMT. Moreover, our systems used pretrained word embedding of monolingual data and adopted BRNN encoder that reasons about outperform previous work  (Laskar et al., 2019c ) at WAT2019. Figure  1  and 2 present best and worst performance our systems outputs, where included Google translation for comparative analysis. 

 Conclusion and Future Work This work participates in two different translation tracks at WAT2020 multi-modal translation task of English to Hindi namely: multi-modal and textonly. In this competition our multi-modal NMT achieves higher BLEU, RIBES and AMFM scores than text-only NMT. From the best of our knowledge, our multi-modal NMT achieves best score on English to Hindi multi-modal translation. In future work, more experiments, analysis will be carried out to enhance the performance of multi-modal NMT. Figure 1 : 1 Figure 1: Examples of our best predicted output on challenge test data. 
