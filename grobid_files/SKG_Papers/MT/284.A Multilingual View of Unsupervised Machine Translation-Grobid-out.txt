title
A Multilingual View of Unsupervised Machine Translation

abstract
We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one language in the (source, target) pair is not associated with any parallel data, but there may exist auxiliary parallel data that contains the other. This auxiliary data can naturally be utilized in our probabilistic framework via a novel cross-translation loss term. Empirically, we show that our approach results in higher BLEU scores over state-of-the-art unsupervised models on the WMT'14 English-French, WMT'16 English-German, and WMT'16 English-Romanian datasets in most directions.

Introduction The popularity of neural machine translation systems  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016)  has exploded in recent years. Those systems have obtained state-of-the-art results for a wide collection of language pairs, but they often require large amounts of parallel (source, target) sentence pairs to train  (Koehn and Knowles, 2017) , making them impractical for scenarios with resourcepoor languages. As a result, there has been interest in unsupervised machine translation  (Ravi and Knight, 2011) , and more recently unsupervised neural machine translation (UNMT)  (Lample et al., 2018; Artetxe et al., 2018) , which uses only monolingual source and target corpora for learning. Unsupervised NMT systems have achieved rapid progress recently  (Lample and Conneau, 2019; Artetxe et al., 2019; Ren et al., 2019; Li et al., 2020a) , largely thanks to two key ideas: one-the-fly back-translation (i.e., minimizing round-trip translation inconsistency)  (Bannard and Callison-Burch,  ?Work done as part of the Google AI Residency. 

 Fr En Ro 2005;  Sennrich et al., 2015; He et al., 2016; Artetxe et al., 2018)  and pretrained language models (Lample and Conneau, 2019;  Song et al., 2019) . Despite the difficulty of the problem, those systems have achieved surprisingly strong results. In this work, we investigate Multilingual UNMT (M-UNMT), a generalization of the UNMT setup that involves more than two languages. Multilinguality has been explored in the supervised NMT literature, where it has been shown to enable information sharing among related languages. This allows higher resource language pairs (e.g. English-French) to improve performance among lower resource pairs (e.g., English-Romanian)  (Johnson et al., 2017; Firat et al., 2016 ). Yet multilingual translation has only received little attention in the unsupervised literature, and the performance of preliminary works  (Sen et al., 2019;  is considerably below that of state-of-theart bilingual unsupervised systems  (Lample and Conneau, 2019; Song et al., 2019) . Another line of work has studied zero-shot translation in the presence of a "pivot" language, e.g., using French-English and English-Romanian corpora to model French-Romanian  (Johnson et al., 2017; Arivazhagan et al., 2019; Gu et al., 2019; Al-Shedivat and Parikh, 2019) . However, zero-shot translation is not unsupervised since one can perform two-step supervised translation through the pivot language. We introduce a novel probabilistic formulation of multilingual translation, which encompasses not only existing supervised and zero-shot setups, but also two variants of Multilingual UNMT: (1) a strict M-UNMT setup in which there is no parallel data for any pair of language, and (2) a novel, looser setup where there exists parallel data that contains one language in the (source, target) pair but not the other. We illustrate those two variants and contrast them to existing work in Figure  1 . As shown in Figures  1(c ) and 1(d), the defining feature of M-UNMT is that the (source, target) pair of interest is not connected in the graph, precluding the possibility of any direct or multi-step supervised solution. Leveraging auxiliary parallel data for UNMT as shown in Figure  1 (d) has not been well studied in the literature. However, this setup may be more realistic than the strictly unsupervised case since it enables the use of high resource languages (e.g. En) to aid translation into rare languages. For the strict M-UNMT setup pictured in Figure 1(c), our probabilistic formulation yields a multi-way back-translation objective that is an intuitive generalization of existing work  (Artetxe et al., 2018; Lample et al., 2018; He et al., 2020) . We provide a rigorous derivation of this objective as an application of the Expectation Maximization algorithm  (Dempster et al., 1977) . Effectively utilizing the auxiliary parallel corpus pictured in Figure  1(d ) is less straightforward since the common approaches for UNMT are explicitly designed for the bilingual case. For this setting, we propose two algorithmic contributions. First, we derive a novel cross-translation loss term from our probabilistic framework that enforces cross-language pair consistency. Second, we utilize the auxiliary parallel data for pre-training, which allows the model to build representations better suited to translation. Empirically, we evaluate both setups, demonstrating that our approach of leveraging auxiliary parallel data offers quantifiable gains over existing state-of-the-art unsupervised models on 3 language pairs: En?Ro, En?Fr, and En?De. Finally, we perform a series of ablation studies that highlight the impact of the additional data, our additional loss terms, as well as the choice of auxiliary language. 

 Background and Overview Notation: Before discussing our approach, we introduce some notation. We denote random variables by capital letters X, Y , Z, and their realizations by their corresponding lowercase version x, y, z. We abuse this convention to compactly write objects like the conditional density ppY " y|X " xq as ppy|xq or the marginalized distributions ppX " xq as ppxq, with the understanding that the lowercase variables are connected to their corresponding uppercase random variables. Given a random variable X, we write E x"X to mean the expectation with respect to x, where x follows the distribution of X. We use a similar convention for conditional distributions e.g. we write E y"pp?|xq to denote the expectation of Y conditioned on X " x. Similarly, we write HpXq or Hpppxqq to denote the entropy of the random variable X i.e. HpXq " E x"X r?log ppxqs. We reserve the use of typewriter font for languages e.g. X. Neural Machine Translation: In bilingual supervised machine translation we are given a training dataset D x,y . Each px, yq P D x,y is a (source, target) pair consisting of a sentence x in language X and a semantically equivalent sentence y in language Y. We train a translation model using maximum likelihood: L sup p?q " ? px,yqPDx,y log p ? py|xq In neural machine translation, p ? py|xq is modelled with the encoder-decoder paradigm where x is encoded into a set of vectors via a neural network enc ? and a decoder neural network defines p ? py|enc ? pxqq. In this work, we use a transformer  (Vaswani et al., 2017)  as the encoder and decoder network. At inference time, computing the most likely target sentence y is intractable since it requires enumerating over all possible sequences, and is thus approximated via beam search. 

 Unsupervised Machine Translation: The requirement of a training dataset D x,y with sourcetarget pairs can often be prohibitive for rare or low resource languages. Bilingual unsupervised translation attempts to learn p ? py|xq using monolingual corpora D x and D y . For each sentence x P D x , D y may not contain an equivalent sentence in Y, and vice versa. State of the art unsupervised methods typically work as follows. They first perform pre-training and learn an initial set of parameters ? based on a variety of language modeling or noisy reconstruction objectives  (Lample and Conneau, 2019; Lewis et al., 2019; Song et al., 2019)  over D x and D y . A fine-tuning stage then follows which typically uses back-translation  (Sennrich et al., 2016; Lample and Conneau, 2019; He et al., 2016)  that involves translating x to the target language Y, translating it back to a sentence x 1 in X, and penalizing the reconstruction error between x and x 1 . Overview of our Approach: The following sections describe a probabilistic MT framework that justifies and generalizes the aforementioned approaches. We first model the case where we have access to several monolingual corpora, pictured in Figure  1(c ). We introduce light independence assumptions to make the joint likelihood tractable and derive a lower bound, obtaining a generalization of the back-translation loss. We then extend our model to include the auxiliary parallel data pictured in Figure  1(d) . We demonstrate the emergence of a cross-translation loss term, which binds distinct pairs of languages together. Finally, we present our complete training procedure, based on the EM algorithm. Building upon existing work  (Song et al., 2019) , we introduce a pre-training step that we run before maximizing the likelihood to obtain good representations. 

 Multilingual Unsupervised Machine Translation In this section, we formulate our approach for M-UNMT. We restrict ourselves to three languages, but the arguments naturally extend to an arbitrary number of languages. Inspired by the recent style transfer literature  (He et al., 2020)  and some approaches from multilingual supervised machine translation  (Ren et al., 2018) , we introduce a generative model of which the available data can be seen as partially-observed samples. We first investigate the strict unsupervised case, where only monolingual data is available. Our framework naturally leads to an aggregate back-translation loss that generalizes previous work. We then incorporate the auxiliary corpus, introducing a novel crosstranslation term. To optimize our loss, we leverage the EM algorithm, giving a rigorous justification for the stop-gradient operation that is usually applied in the UNMT and style transfer literature  (Lample and Conneau, 2019; Artetxe et al., 2019; He et al., 2020) . 

 M-UNMT -Monolingual Data Only We begin with the assumption that we have three sets of monolingual data, D x , D y , D z for languages X, Y and Z respectively. We take the viewpoint that these datasets form the visible parts of a larger dataset D x,y,z of triplets px, y, zq which are translations of each other. We think of these translations as samples of a triplet pX, Y, Zq of random variables and write the observed data log-likelihood as: Lp?q " L Dx `LDy `LDz Our goal however is to learn a conditional translation model p ? . We thus rewrite the log likelihood as a marginalization over the unobserved variables for each dataset as shown below: Lp?q " ? xPDx log E py,zq "pY,Zq p ? px|y, zq (1) `? yPDy log E px,zq "pX,Zq p ? py|x, zq (2) `? zPDz log E px,yq "pX,Y q p ? pz|x, yq (3) Learning a model for p ? px|y, zq is not practical since the translation task is to translate z ? x without access to y, or y ? x without access to z. Thus, we make the following structural assumption: given any variable in the triplet pX, Y, Zq, the remaining two are independent. We implicitly think of the conditioned variable as detailing the content and the two remaining variables as independent manifestations of this content in the respective languages. Using the fact that p ? px|y, zq " p ? px|yq " p ? px|zq under this assumption, we rewrite the summand in p1q as follows: log E py,zq "pY,Zq p ? px|y, zq " log E py,zq "pY,Zq a p ? px|yqp ? px|zq. Next, note that all these expectations in Eq. 1, 2, and 3 are intractable to compute due to the number of possible sequences in each language. We address this problem through the Expectation Maximization (EM) algorithm  (Dempster et al., 1977) . We first use Jensen's inequality  enforce that reciprocal translation models are consistent. The joint terms e.g. E px,yq"p ? p?,?|zq log ppx, yq will vanish in our optimization procedure, as explained next. We use the EM algorithm to maximize Eq. 4. In our setup, the E-step at iteration t amounts to computing the expectations against the conditional distributions evaluated at the current set of parameters ? " ? ptq . We approximate this by removing the expectations and replacing the random variable with the mode of its distribution i.e. E y"p ? ptq p?|xq log p ? ptq px|yq ? p ? ptq px|?q where ? " arg max y p ? ptq py|xq. In practice, this amounts to running a greedy decoding procedure for the relevant translation models. The M-step then corresponds to choosing the ? which maximizes the resulting terms after we perform the E-step. Notice that for this step, the last three terms in Eq. 4 no longer possess a ? dependence, as the expectation was computed in the E-step with a dependence on ? ptq . These terms can therefore be safely ignored, leaving us with only the back-translation terms. By our approximation to the E-step, these expressions become exactly the loss terms that appear in the current UNMT literature  (Artetxe et al., 2019; Lample and Conneau, 2019; Song et al., 2019) , see Figure  2 (a) for a graphical depiction. Since computing the argmax is a difficult task, we perform a single gradient update for the M-step and define ? pt`1q inductively this way. 

 Auxiliary parallel data We now extend our framework with an auxiliary parallel corpus (Figure  1(d) ). We assume that we wish to translate from X to Z, and that we have access to a parallel corpus D x,y that maps sentences from X to Y. To leverage this source of data, we augment the log-likelihood L as follows: L aug p?q " Lp?q `? px,yq PDx,y log E z"Z p ? px, y|zq (6) Similar to how we handled the monolingual terms, we can utilize the EM algorithm to obtain an objective amenable to gradient optimization. By using the EM algorithm, we can substitute the distribution of Z in Eq. 6 with the one given by p ? pz|x, yq. The structural assumption we made in the case of monolingual data still holds: given any variable in the triplet pX, Y, Zq, the remaining two are independent. Using this assumption, we can rewrite the distribution p ? pz|x, yq as either p ? pz|xq or p ? pz|yq. Since we can decompose log p ? px, y|zq " log p ? px|zq `log p ? py|zq, we can leverage both formulations with an argument analogous to the one in ?3.1: log E z"Z p ? px, y|zq " log E z"Z p ? px|zqp ? py|zq ? E z"p ? p?|yq log p ? px|zq `E z"p ? p?|xq log p ? py|zq `E z"p ? p?|yq log ppzq `E z"p ? p?|xq log ppzq (7) A key feature of this lower bound is the emergence of the expressions: (8) Intuitively, those terms ensure that the models can accurately translate from Y to Z, then Z to X (resp. X to Z, then Z to Y). Because they enforce crosslanguage pair consistency, we will refer to them as cross-translation terms. In contrast, the backtranslation terms, e.g., Eq. 5, only enforced monolingual consistency. We provide a graphical depiction of these terms in Figure  2(b) . As in the case of monolingual data, we optimize the full likelihood with EM. During the E-step, we approximate the expectation with evaluation of the expectant at the mode of the distribution. As with ?3.1, the last two terms in Eq. 7 disappear in the M-step. 

 Connections with supervised and zero shot methods So far, we have only discussed multilingual unsupervised neural machine translation setups. We now derive the other configurations of Figure  1 , that is, supervised and zero-shot translation, through our framework. Supervised translation: Deriving supervised translation is straightforward. Given the parallel data dataset D x,y , we can rewrite the likelihood as: where the second term is a language model that does not depend on ?. Zero-shot translation: We can also connect the cross-translation term to the zero-shot MT approach from  Al-Shedivat and Parikh (2019) . Simplifying their setup, they consider three languages X, Y and Z with parallel data between X and Y as well as X and Z. In addition to the usual crossentropy objective, they also add agreement terms i.e. E z"p ? p?|xq log ppz|yq and E z"p ? p?|yq log ppz|xq. We show that these agreement terms are operationally equivalent to the cross-translation terms i.e. Eq. 8. We first obtain the following equality by a simple application of Bayes' theorem: log p ? py|zq " log p ? pz|yq `log ppyq ?log ppzq. We then apply the expectation operation E z"p ? p?|xq to both sides of this equation. From an optimization perspective, we are only interested in terms involving the learnable parameters so we can dispose of the term involving log ppyq on the right. Applying the same argument to log p ? px|zq, we obtain: log ppzq to both sides of this inequality, the left-hand side becomes the lower bound introduced in the previous subsection, consisting of the cross-translations terms. The right-hand side consists of the agreement terms from Al-Shedivat and Parikh (2019). We tried using this term instead of our cross-translation terms, but found it to be unstable. This could be attributed to the fact that we lack X ? Z parallel data, which is available in the setup of  Al-Shedivat and Parikh (2019) . E z" 

 Algorithm 1 PRE-TRAINING Input: Datasets D , number of steps N 1: Initialize ? ? ?0 2: for step in 1, 2, 3, ..., N do 3: Choose dataset D at random from D. 

 4: if D consists of monolingual data then 

 5: Sample batch x from D. 

 6: Masked version of x: x M ? MASKpxq 

 7: MASS Loss: ml ? log p ? px|x M q 8: Update: ? ? optimizer updatepml, ?q 

 9: else if D consists of parallel data then 10: Sample batch px, yq from D. 

 11: tl ? log p ? py|xq `log p ? px|yq 12: ? ? optimizer updateptl, ?q 

 13: end if 

 14: end for 4 Training algorithms We now discuss how to train the model end-to-end. We introduce a pre-training phase that we run before the EM procedure to initialize the model. Pretraining is known to be crucial for UNMT  (Lample and Conneau, 2019; Song et al., 2019) . We make use of an existing method, MASS, and enrich it with the auxiliary parallel corpus if available. We refer to the EM algorithm described in ?3 as finetuning for consistency with the literature. 

 Pre-training The aim of the pre-training phase is to produce an intermediate translation model p ? , to be refined during the fine-tuning step. We pre-train the model differently based on the data available to us. For monolingual data, we use the MASS objective  (Song et al., 2019) . The MASS objective consists of masking randomly-chosen contiguous segments 2 of the input then reconstructing the masked portion. We refer to this operation as MASK. If we have auxiliary parallel data, we use the traditional cross-entropy translation objective. We describe the full procedure in Algorithm 1. 

 Fine-tuning During the fine-tuning phase, we utilize the objectives derived in Section 3. At each training step we choose a dataset (either monolingual or bilingual), sample a batch, compute the loss, and update the weights. If the corpus is monolingual, we use the back-translation loss i.e. Eq. 5. If the corpus is bilingual, we compute the cross-translation terms i.e. Eq. 8 in both directions and perform one update  2  We choose the starting index to be 0 or the total length of the input divided by two with 20% chance for either scenario otherwise we sample uniformly at random then take the segment starting from this index and replace all tokens with a [MASK] token.  

 6: Sample batch x from D. 

 7: for l in L, l ? l D do 8: ?l ?Decode p ? p? l |xq. 

 9: bt l D ,l ? log p ? px|? l q. 

 10: ? ? optimizer updatepbt l D ,l , ?q. 

 11: end for 

 12: else if D consists of parallel data then 

 13: Sample batch px, yq from D. 

 14: lx ? Language of x. 

 15: ly ? Language of y. 

 16: for l in L, l ? lx, ly do  

 22: end for 23: end while for each term. We detail the steps in Algorithm 2. 

 Experiments We conduct experiments on the language triplets English-French-Romanian with English-French parallel data, English-Czech-German with English-Czech parallel data and English-Spanish-French with English-Spanish parallel data, with the unsupervised directions chosen solely for the purposes of comparing with previous recent work  (Lample and Conneau, 2019; Song et al., 2019; Ren et al., 2019; Artetxe et al., 2019) . 

 Datasets and preprocessing We use the News Crawl datasets from WMT as our sole source of monolingual data for all the languages considered. We used the data from years 2007-2018 for all languages except for Romanian, for which we use years 2015-2018. We ensure the monolingual data is properly labeled by using the fastText language classification tool  (Joulin et al., 2016)  and keep only the lines of data with the appropriate language classification. For parallel data, we used the UN Corpus  (Ziemski et al., 2016)  for English-Spanish, the 10 9 French-English Gigaword corpus 3 for the English-French and the CzEng 1.7 dataset  (Bojar et al., 2016)  for English-Czech. We preprocess all text by using the tools from Moses  (Koehn et al., 2007) , and apply the Moses tokenizer to separate the text inputs into tokens. We normalize punctuation, remove non-printing characters, and replace unicode symbols with their non-unicode equivalent. For Romanian, we also use the scripts from Sennrich 4 to normalize the scripts and remove diacretics. For a given language triplet, we select 10 million lines of monolingual data from each language and use Senten-cePiece  (Kudo and Richardson, 2018)  to create vocabularies containing 64,000 tokens of each. We then remove lines with more than 100 tokens from the training set. 

 Model architectures We use Transformers  (Vaswani et al., 2017)  for our translation models p ? with a 6-layer encoder and decoder, a hidden size of 1024 and a 4096 feedforward filter size. We share the same encoder for all languages. Following XLM  (Lample and Conneau, 2019) , we use language embeddings to differentiate between the languages by adding these embeddings to each token's embedding. Unlike XLM, we only use the language embeddings for the decoder side. We follow the same modification as done in  Song et al. (2019)  and modify the output transformation of each attention head in each transformer block in the decoder to be distinct for each language. Besides these modifications, we share the parameters of the decoder for every language. 

 Training configuration For pre-training, we group the data into batches of 1024 examples each, where each batch consists of either monolingual data of a single language or parallel data, but not both at once. We pad sequences up to a maximum length of 100 SentencePiece tokens. During pre-training, we used the Adam optimizer  (Kingma and Ba, 2015)  with initial learning rate of 0.0002 and weight decay parameter of 0.01, as well as 4,000 warmup steps and a linear decay schedule for 1.2 million steps. For fine-tuning, we used Adamax (Kingma and Ba, 2015) with the same learning rate and warmup steps, no weight decay, and trained the models until convergence. We used Google Cloud TPUs for pre-training and 8 NVIDIA V100 GPUs with a batch size of 3,000 tokens per GPU for fine-tuning. 

 Results Evaluation We use tokenized BLEU to measure the performance of our models, using the multibleu.pl script from Moses. Recent work  (Post, 4  https://github.com/rsennrich/wmt16-scripts 2018) has shown that the choice of tokenizer and preprocessing scheme can impact BLEU scores tremendously. Bearing this in mind, we chose to follow the same evaluation procedures used 6 by the majority of the baselines that we consider, which involves the use of tokenized BLEU as opposed to the scores given by sacreBLEU. Given the rise of popularity of SacreBLEU  (Post, 2018) , we also include BLEU scores computed from sacreBLEU 7 on the detokenized text for French and German. We exclude Romanian since most works in the literature traditionally use additional tools from Sennrich not used in sacreBLEU. Baselines We list our results in Table  1 . We also include the results of six strong unsupervised baselines: (1) XLM  (Lample and Conneau, 2019) , a cross-lingual language model fine-tuned with backtranslation; (2) MASS  (Song et al., 2019) , which uses the aforementioned pre-training task with back-translation during fine-tuning; (3) D2GPo  (Li et al., 2020a) , which builds on MASS and leverages an additional regularizer by use of a data-dependent Gaussian prior; (4) The recent work of  Artetxe et al. (2019)  which leverages tools from statistical MT as well subword information to enrichen their models; (5) the work of Ren et al. (  2019 ) that explicitly attempts to pre-train for UNMT by building cross-lingual n-gram tables and building a new pretraining task based on them; (6) mBART  (Liu et al., 2020) , which pre-trains on a variety of language configurations and fine-tunes with traditional onthe-fly back-transaltion. mBART also leverages Czech-English data for the Romanian-English language pair. Furthermore, we include concurrent work that also uses auxiliary parallel data: (8) The work of  Bai et al. (2020) , which performs pre-training and fine-tuning in one stage and replaces MASS with a denoising autoencoding objective; (9) the work of  Li et al. (2020b)  which also leverage a crosstranslation term and additionally include a knowledge distillation objective. We also include the results of our model after pre-training i.e. no backtranslation or cross-translation objective, under the title M-UNMT (Only Pre-Train). Our models with auxiliary data obtain better scores for almost all translation directions. Pretraining with the auxiliary data by itself gives com- petitive results in two of the three X ?En directions. Moreover, our approach outperforms all the baselines which also which also leverage auxiliary parallel data. This suggests that our improved performance comes from both our choice of objectives and the additional data. 

 Ablations We perform a series of ablation studies to determine which aspects of our formulation explain the improved performance. Impact of the auxiliary data We first examine the value provided by the inclusion of the auxiliary data, focusing on the triplet English-French-Romanian. To that end, we study four types of training configurations: (1) Our implementation of MASS  (Song et al., 2019)    Sennrich et al., 2016)  28.2 33.9 mBART  (Liu et al., 2020)  38.5 39.9  fine-tuning, with superior performance when the auxiliary data is available in both training phases. Impact of the additional objectives Given the strong performance of our model just after the pretraining phase, it would be plausible that the gains from multilinguality arise exclusively during the pre-training phase. To demonstrate that this is not the case, we investigate three types of finetuning configurations: (1) Disregard the auxiliary language and fine-tune using only back-translation with English and Romanian data as per  Song et al. (2019) . (2) Finetune with our multi-way backtranslation objective. (3) Finetune with our multi- way back-translation objective and leverage the auxiliary parallel data through the cross-translation terms. We name these configurations BT, M-BT, and Full respectively. We plot the results of training for 100k steps in Figure  3 , reporting the numbers on a modified version of the dev set from the WMT'16 Romanian-English competition where all samples with more than 100 tokens were removed. In the Ro ?En direction, the BLEU score of the Full setup dominates the score of the other approaches. Furthermore, the performance of BT decays after a few training steps. In the En ?Ro direction, the BLEU score for the BT and M-BT reach a plateau about 1 point under Full. Those charts illustrate the positive effect of the crosstranslation terms. We contrast the BLEU curves with the back-translation loss curves in Figure  3 (c) and 3(d). We see that even that though the BT configuration achieves the lowest back-translation loss, it does not attain the largest BLEU score. This demonstrates that using back-translation for the desired (source, target) pair alone is not the best task for the fine-tuning phase. We see that the multilinguality helps, as adding more back-translation terms with other languages involved improves the BLEU score at the cost of higher back-translation errors. From this viewpoint, the multilinguality acts as a regularizer, as it does for traditional supervised machine translation. Impact of the choice of auxiliary language In this study, we examine the impact of the choice of auxiliary language. We perform the same pretraining and fine-tuning procedure using either French, Spanish or Czech as the auxiliary language for the English-Romanian pair, with relevant parallel data of this auxiliary language into English. To isolate the effect of the language choice, we fixed the amount of monolingual data of the auxiliary language to roughly 40 million examples, as well as roughly 12.5 million lines of parallel data in the X-English direction. Table  3  shows the results, indicating that using French or Spanish yields similar BLEU scores. Using Czech induces inferior performance, demonstrating that choosing a suitable auxiliary language plays an important role for optimal performance. The configuration using Czech still outperforms the baselines, showing the value of having any auxiliary parallel data at all. 

 Conclusion and Future Work In this work, we explored a simple multilingual approach to UNMT and demonstrated that multilinguality and auxiliary parallel data offer quantifiable gains over strong baselines. We hope to explore massively multilingual unsupervised machine translation in the future. Figure 1 : 1 Figure 1: Different setups for English (En), French (Fr) and Romanian (Ro). The dashed edge indicates the target language pair. Full edges indicate the existence of parallel training data. 
