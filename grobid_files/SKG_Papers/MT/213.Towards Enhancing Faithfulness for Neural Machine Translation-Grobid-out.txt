title
Towards Enhancing Faithfulness for Neural Machine Translation

abstract
Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FENMT). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FENMT could improve translation quality by effectively reducing unfaithful translations.

Introduction Neural machine translation (NMT) based on the encoder-decoder framework  (Sutskever et al., 2014; Luong et al., 2015b)  has obtained state-of-the-art performance on many language pairs  (Wu et al., 2019; . Various neural architectures have been explored for modeling NMT under this framework, such as recurrent neural network (RNN)  Luong et al., 2015b, RNNSearch) , convolutional neural network (CNN)  (Gehring et al., 2016, Conv-S2S)  and self-attention network  (Vaswani et al., 2017, Transformer) . Compared with human translations or traditional statistical machine translation (SMT)  (Koehn et al., 2007b; Chiang, 2007) , NMT * Work done during the internship at Alibaba Group. can generate high-quality sentences that are very close to natural language. However, it usually appears some parts (e.g., phrase) from input sentences cannot be correctly translated, leading to that the translation is inadequate for direct using in some scenarios. This phenomenon appeals that enhancing the faithfulness of translations is an important aspect for further improving NMT. We summarize three possible causes for the unfaithfulness problem based on the encoder-decoder framework: 1). Some parts from input sentences are hard to encode, and thus cannot be translated correctly. 2). The decoder cannot fetch the correct contextual representation from the encoder. 3).The dominant language model of NMT prompts the decoder generates common words to make sure outputs are fluent. Several recent studies are proposed following one of the above perspectives and have achieved considerable effects.  Zheng et al. (2019)  proposed to divide the encoder output into past and future parts to fine-grained modeling contextual representation.  Feng et al. (2020)  proposed a faithfulness part to optimize the contextual representation before feeding into the decoder.  Kong et al. (2019)  proposed to use a coverage difference ratio metric as a reward to train NMT. In this paper, we propose a novel training strategy with a multi-task learning paradigm, taking into account the use of real translations for building a faithfulness enhanced NMT (named FENMT). Firstly, we align source and target sentences in the training set. Then, at each training epoch, we sample a subset from the training set and translate source sentences by the NMT in the this set. For convenience, we simply define a mistranslated fragment is a continues segment from a target sentence which does not appear in the translation. So, we can collect mistranslated fragments by comparing the translation and reference, and get the corresponding source words by the alignment relationship. After that, our multi-task learning paradigm (MTL) is incorporated into the training process to learn to correctly translate these mistranslated fragments. To make the most of the collected mistranslated fragments, the proposed MTL method considers all sides of the above hypotheses. Specifically, we employ a masked language model task  (Devlin et al., 2018)  on the encoder side to infer the input words didn't be correctly translated. This task can enhance the ability of modeling the whole input sentence and give the decoder accurate and complete representations. On the decoder side, we use a word alignment task to improve the alignment accuracy of the encoderdecoder attention (or cross-attention) to help the decoder to capture correct contextual representation. Furthermore, along with the NMT objective, an auxiliary max-margin objective based on contrastive learning is introduced in all decoding timesteps. The goal of this task is to avert the tendency of translating frequent but unrelated words. We implement the proposed approach based on Transformer  (Vaswani et al., 2017)  and evaluate it on WMT14 English to German (En?De), WMT17 Chinese to English (Zh?En) and WMT16 English to Romania (En?Ro) machine translation tasks. Both automatic and human evaluations show that the proposed FENMT could substantially improve the overall quality and faithfulness of translations. 

 The Proposed Approach We will introduce the whole procedure of the proposed FENMT model based on the advanced Transformer  (Vaswani et al., 2017) . We firstly show the details of how to collect mistranslated fragments and the multi-task learning paradigm at section 2.1 and 2.2, respectively. Then, the overall training strategy of our approach is represented at section 2.3. 

 Collecting Mistranslated Fragments Given a parallel training set B, we achieve the alignment matrix set A through a word alignment model trained by the parallel training set, and get the phrase table P according to the word alignment  (Koehn et al., 2003) . At the tth training epoch of NMT, we sample a subset B S t from the B. Given a sentence pair {x, y} from the B S t , where x is the source sequence (x 1 , ? ? ? , x i , ? ? ? , x I ) and y is the target sequence (y 1 , ? ? ? , y j , ? ? ? , y J ), I and J are the length of x and y, respectively. The alignment matrix A ? R J?I of {x, y} can be obtained from A, in which a j,i = 1 means y j aligns x i . We then translate the source sentence by ? = f ? t?1 (x), where f ? t?1 (?) is the NMT model, which parameters are ? and has been trained after t ? 1 epochs. ? is composed of (? 1 , ? ? ? , ?k , ? ? ? , ?K ), K is the sentence length. We define that a fragment in y is mistranslated when it does not appear in ? but is contained in P. Subsequently, we randomly sample consecutive parts from y included in P and compare them with ? to achieve mistranslated fragments. We denote a subsequence y T of y containing all words mistranslated, y t,j is the tth word of y T whose position in the y is j. Afterward, we can get the aligned source words of y T by using the alignment relationship. For a word y t,j , we collect source words when a j,? = 1. We denote the sequence having all aligned source words as x M , in which x m,i is the mth word of x M whose position in the x is i. A shortly case is shown in Figure  1 . 

 Multi-task Learning Paradigm Masked language model task for the encoder. The first hypothesis mentioned above is that the encoder cannot model mistranslated parts well, which leads to the subsequent module cannot translate them correctly. Here, we introduce a masked language model task (MLM) to further model these source words. Specifically, before feeding into the decoder, we ask for the source representation predicts mistranslated words which are masked at the input sentence (see Figure  2 ).  1  Formally, given the input sentence x from B S t and the mistranslated subsequence x M . We define a sequence x R , which likes x but the words in the x M will be replaced as a special <MASK> token with the probability of 80%, and as a random word or keep unchanged with the probability of 10% individually. This procedure is the same as  Devlin et al. (2018) . The encoder with the MLM task maximizes the conditional probability defined as: 

 Mask Language Model P (x m,i |x R ) = softmax(FFN(r E i )), (1) R E = Encoder(x R ), (2) where R E ? R l input ?d model is the output hidden states of the encoder, and r E i ? R d model is ith hidden state of R E . l input is the length of the input sentence and d model is the dimension of hidden state. Finally, the objective of the masked language model is L M = ?E x R ?B S t [E x m,i ?x M [log P (x m,i |x R )]]. (3) Word alignment task for the cross attention. After getting a better source contextual representation, i.e., the R E , whether the decoder can get the correct representation for each output word is another factor determining translation faithfulness. The cross-attention is the single connection between the encoder and decoder. A natural intuition is that improving the accuracy of cross-attention is helpful for getting faithful translations. Thus, we introduce a word alignment task for the crossattention here (see Figure  3 ). Specifically, given the target sentence y, we define the cross-attention weight matrix as C ? R J?I , the vector c j from C is the weight of jth decoder hidden state to the encoder representation. We then define the alignment label as B ? R J?I . Given the word y j in the y T , the corresponding alignment label vector b j is computed by: b j = softmax(a j ), ( 4 ) a 2,1 a 2,2 a 2,3 a 2,4 a 2,5 a 2,6 a 3,1 a 3,2 a 3,3 a 3,4 a 3,5 a 3,6 a 4,1 a 4,2 a 4,3 a 4,4 a 4,5 a 4,6 a 1,1 a 1,2 a 1,3 a 1,4 a 1,5 a 1,6  a 5,1 a 5,2 a 5,3 a 5,4 a 5,5 a 5,6   x1 x2 x3 x4 x5   where a j is from the alignment matrix A. Note that when using subword  (Sennrich et al., 2015; Devlin et al., 2018)  as input, alignment probability will be divided into the corresponding tokens equally (e.g., if a word is divided into two tokens, the probability for each token is 0.5). Generally, the decoder has N block and the cross attention from each block has H heads  (Vaswani et al., 2017) . We randomly choose two heads at each blocks to employ the word alignment objective. We define the selected attention weight matrix set as C = {C 1 , ? ? ? , C k , ? ? ? , C K }, where K = 2 * N . Then, the word alignment objective is L A = ?E C k ?C [E b j ?B,c j ?C k [b j log c j ]]. (5) This objective is used to guide the cross-attention to capture correct contextual information rather than only learn the word alignment information. So we only employing it on parts of attention head to avoid "overfitting" to the alignment task. Max-margin task for the decoder. Empirically, the language model in current NMT is more stronger than the translation model, so the NMT model tends to translate common words even unrelated to the source sentence  (Kong et al., 2019) . Only using cross-entropy objective isn't enough to keep translations faithful. Here, we introduce a max-margin objective based on contrastive learning to suppress the tendency of NMT to generate common but unfaithful words. Specifically, given the target sentence y and the translation ?, the max-margin loss is defined as L C = J j=1 L j , where L j is computed by L j = ? ? ? ? ? max(0, mg ? P (y j |y 1:j , x) where the mg is the margin, we empirically set to 0.2. The cross-entropy objective with this objective can prompt the decoder to translate fluent and faithful sentences. + P (? j )|? 1:j , x)) , y t,j ? y T 0 , y t,j / ? y T f ? t ( ? ) f ? t+1 ( ? ) 

 The Overall Training Strategy The standard NMT training objective is to minimize the negative log-likelihood by: L T = ?E {x,y}?B log P (y|x). (6) And the final training objective of our proposed approach is: L F = L T + ? ? L M + ? ? L A + ? ? L C , (7) where ?, ? and ? are used to balance the preference among the external losses, which are empirically set to 0.3 individually. Note that due to the different inputs, L M should be computed separately. The training strategy as follows: at the tth NMT training epoch, we are going to sample part of the sentences from the training set, the sampling ratio is computed by: ratio = max(d (t?1) * 20%, 5%), ( 8 ) where d is the decay rate, we set as 0.9 here. To avoid decreasing training efficiency, the sampled data will be translated by f ?t (?) at the tth epoch and used at the t + 1th epoch. And the first epoch will not use this method as a warm-up. The overview of the training strategy is shown in Figure  4  The NMT will begin to translate sampled sentences at the end of the tth epoch, which is synchronous with the training process. Then, when both of the training process and translation process are finished, the multi-task learning paradigm will be employed to continue train the NMT model. 

 Experiment 

 Implementation Detail We conduct experiments on the WMT data-sets 2 , including WMT17 Chinese to English CWMT part (Zh?En), WMT 14 English to German (En?De) and English to Romanian (En?Ro). On the Zh?En, our training set has about 7.5M sentence pairs. We use newsdev2017 as dev set which has 2002 sentence pairs, and newstest2017 as test set which has 2001 sentence pairs. On the En?De, our training set has about 4.5M sentence pairs. We use newstest2013 as dev set which has 3000 sentence pairs, and newstest2014 as test set, which has 3003 sentence pairs. On the En?Ro, our training set has about 0.6M sentence pairs. We use newstest2015 as dev set which has 2000 sentence pairs, and newstest2016 as test set which has 2000 sentence pairs. We apply the byte pair encoding (BPE)  (Sennrich et al., 2015)  to all language pairs and limit the vocabulary to 32K. All out-of-vocabulary words were mapped to the UNK token. The same training sets were used to train a word alignment model using fast align 3 . Then, the bilingual phrase table is extracted by  Koehn et al. (2003 Koehn et al. ( , 2007a . We limit the length of phrase is 2-4, and finally 6.7M, 3.4M and 0.2M phrases are extracted from Zh?En, En?De and En?Ro. Following Transformer-Base and Transformer-Big settings, we set the dimension of the input and output of all layers as 512/768, and that of the feed-forward layer to 2048/3072. We employ 8/12 parallel attention heads. The number of layers for the encoder and decoder are 6. Sentence pairs are batched together by approximate sentence length. Each batch has approximately 25000 source and 25000 target tokens. We use label smoothing with value 0.1 and dropout with a rate of 0.1. We use the Adam (Kingma and Ba, 2014) with the learning rate of 1e-3, ? 1 = 0.9, ? 2 = 0.98, and it was varied under the warm-up with 4000 steps. Other settings of Transformer follow  Vaswani et al. (2017) . We use beam search for heuristic decoding, and the size is set to 4. We use the sacreBLEU 4 to calcu- late case-sensitive BLEU  (Papineni et al., 2002)  as the automatic metric. We implement the proposed approach with the implementation of Transformer derived from the tensor2tensor 5 . 

 Automatic Evaluation Translation quality. The results are summarized in Table  1 . We implement the Transformer-Base and Transformer-Big as our baselines. Several Transformer systems with the same settings  (Vaswani et al., 2017; Hassan et al., 2018; Gu et al., 2017)  are reported as a comparison (line 1-6). Then, several related researches about improve faithfulness of NMT  (Kong et al., 2019; Zheng et al., 2019; Feng et al., 2020)  or exploiting translations for improving NMT  (Xia et al., 2017; ) also be reported (line 7-12). We implement three comparable approaches on our Transformer baseline, including: 1). self-supervised learning: we use the translations of training data as a self-supervision signal to fine tune the NMT model; 2). minimum risk training (MRT): we implement the MRT following  Shen et al. (2016) ; 3). Knowledge Distillation: we adopt the KL divergence to distill knowledge from Transformer-Big to Transformer-Base (line 13-15). The results on the ZH?EN task are shown in the third column of Table  1 . The improvement of our model (FENMT) could be up to 1.03 based on the Transformer-Base baseline (line 16 vs. line 1), and 1.44 base on the Transformer-Big baseline (line 17 vs. line 2). Then, the results on the En?De task are shown in the fourth column. On this task, the proposed model with base and big settings could attain 28.25 BLEU (+0.88) and 29.36 BLEU (+0.89), which outperforms all previous studies. We also experiment our method on low resource language pair of the En?Ro. Results are shown in the last column. The improvement is 1.20 BLEU on the base setting, which is a material improvement in low resource scenario. Experimental results on three machine translation tasks show that the proposed approach can improve translation quality which is not limited by the language or size of training data. Moreover, our method is more effect on Zh?En than De?En, which may appeal the unfaithful problem is more serious on the language pair which have a larger difference in morphology. Model size and efficiency. The number of parameters is shown in  setting is 0.86x compared with Transformer-Base, and based on the big setting is 0.94x compared with Transformer-Big.  6  Our approach only influence the training process of NMT, so the inference efficiency will not be affected. 

 Human Evaluation The automatic metric, i.e., BLEU, sometimes can't accurately evaluate translation quality. For example, the sentence missing content words has de-6 All comparisons here were on a single GPU (Tesla P100). crease more on faithfulness than missing function words, but the BLEU scores may be equal. So, we make detailed human evaluations to see the variations of translation quality in the real environment. 

 Number of mistranslations. We divide mistranslations into several types and each type has three degrees. We sample 100 sentences from the Zh?En test set, and invite a professional translator to label errors contained in these translations. The results are reported in Table  2 . Our method can reduce the number of mistranslations at the most of categories. Typically, our approach significantly reduce the number of the Omission, which means a continue part from the input doesn't be translated correctly. At the Addition category, our approach also achieves remarkable improvement even it's not a main error type in current NMT. Omission and Addition are two serious error types greatly hurting the faithfulness of translations. The reduction of these errors will improve the faithfulness of translations obviously. Translation quality ranking. Besides evaluating the error types in the sampled sentences, we also evaluate the overall quality for each sentence. Here, the translation quality is divided into 5 levels and give score 1 to 5 (larger is better) and a professional translator is invited to score them. The results are shown in Table  3 , the overall score of the proposed method is better than baseline (3.79 > 3.51). Specifically, the good (4) and excellent(5) translations from our approach are more than baseline (+75.0% and + 28.6%) by revising the errors from the bad (2) and understandable (3) translations  (-57.1% and -32.6%) . This results show that the reduction of mistranslations really improve the overall quality for human readers.    

 Analysis Ablation study. To further show the function of each task in our approach, we make ablation study in this section. Specifically, we investigate how the masked language model objective, word alignment objective, and max-margin objective affect the translation performance. The results are shown in Table  4 . Firstly, we analysis the effect of each task. The model achieves 0.63, 0.33 and 0.42 gains when only using masked language model (L M ), word alignment (L A ) and max-margin (L C ) individually. Then, the results of combining two of three tasks are shown in the second part. The masked language model combines word alignment or max-margin can get improvements of 0.77 and 0.73, which are close to the best performance. While the combination of word alignment and max-margin is not work well (+0.49). The above experimental results show that each task could get a decent improvement. But compared with improving the ability of the decoder, the high quality contextual representation learned from the masked language model is more important. Accuracy of phrase translation. We compute the accuracy of phrase translation on the En?De task to evaluate the proposed multi-task objective in a fine-grained aspect. The result are shown in Table  5 . The total number of phrases in the references is 8082. Our approach successfully translate the 6453 (79.8%) and the baseline correctly translate the 5676 (70.2%). The accuracy of our approach largely improves 9.6% compared with the baseline. Analysis of different sampling rate. The results of the FENMT with different sampling rate are shown in Table  6 . When the sampling rate is 5%, the performance decreases 0.46 compared with the rate computed by Eq. 8. When the sampling rate is larger than 20%, the performance does not change significantly. But the dynamical sampling rate will reduce the number of sentences needed to be translated, which can avoid dropping training efficiency. Analysis of sentence length. We group the En?De test set by the length of source sentences, and then re-evaluate the BLEU score of each group. The test set is divided into 7 subsets. Figure  5  shows the results. We find that our model outperforms the baseline in all categories in both base and big setting. The proposed model performs better on long sentences (e.g.,  [30, 60] ). Because long sentences are usually complex and difficult to translate which causes the number of mistranslations in them is more than short sentences. Our approach can avoid these mistranslations compared with baselines. Case study. We show two cases from the Zh?En task to see the difference between baseline and our approach, which are shown in Table  7 . Our approach could learn how to translate the difficult fragments in the input which are easier to be mistranslated. For example, the idiom "turn the table" in case 1 is translated to loss by the baseline, which only observe the word "?" in the input. In case 2, the baseline makes a serious mistake at the beginning of the sentence. The translation of "? Input ? Refer. whether you're approached from in front or behind, it will show you how to turn the tables on your mugger. Baseline whether he comes from the front, or from the front, it will teach you how to lose. FENMT whether you're approached from in front or back, it will show you how to turn the tables. 

 Input ?, ? Refer. asset custody mechanism is a major reason to explain the outbreak of risk events in private equity funds and other sectors . Baseline trust is an important reason for the outbreak of risk events in private equity fund . 

 FENMT asset custody mechanism is an important reason for the outbreak of risk events in private equity funds and other sectors .   2018 ) divided source representation into past and future parts to fine-grained control translation process. These studies focus on using source representation effectively. On the other hand, improving the ability of the decoder is another way.  Tu et al. (2017)  proposed to introduce a reconstruction loss to make translation can reconstruct the input sentence.  Weng et al. (2017)  proposed a bag-of-words loss to constrain decoding process. These methods are similar to multi-task learning, but the motivation of them are different. Recent studies found that Transformer also suffer this problem even its translation quality is far better than RNN model.  Kong et al. (2019)  proposed a coverage difference ratio metric as a reward to train the Transformer model.  proposed to model global representation in the source side to improve the source representation.  Zheng et al. (2019)  proposed a capsule based module to control the source representation dynamically in the decoding process.  ), Feng et al. (2020  and  Garg et al. (2019)  proposed to introduce word alignment information in Transformer to improve translation accuracy. However, they only focus on one side causing this problem while don't have an overall solution. Our study is the first work to pay attention to using mistranslations guides NMT model to avoid making these mistakes again. Multi-task learning in NMT. Multi-task learning has been widely used in NMT.  Dong et al. (2015)  proposed to share an encoder between different translation tasks to exploit multi lingual knowledge.  Luong et al. (2015a)  proposed to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder.  Zhang and Zong (2016)  and  Domhan and Hieber (2017)  proposed to use multi-task learning for incorporating source/target side monolingual data in NMT.  Zhou et al. (2019)  introduced noisy data with multi-task learning to improve the robustness of NMT. Different from these attempts, our approach wants to improve the faithfulness of current NMT model, while learning extra knowledge from other tasks. 

 Conclusion In this paper, we address the problem that current NMT can't generate faithful translations which will observably decrease translation quality. We propose a FENMT to learn the faithful translation from mistranslated parts. We implement the proposed method based on the Transformer model and evaluate it on three translation tasks. Both the automatic and human evaluations show that our approach can effectively improve the faithfulness of translations. Our work can employ on different text generation tasks, e.g., text summarization and dialogue, to enhance the key phrases (or terms) generation. In the future, we will continue investigate the learning method for effectively utilizing self-generated samples and expand to other text generation tasks. Figure 1 : 1 Figure1: The case of collecting mistranslated fragments. "litigation exhaustion" is the mistranslated fragment and "?" is the corresponding source word. 
