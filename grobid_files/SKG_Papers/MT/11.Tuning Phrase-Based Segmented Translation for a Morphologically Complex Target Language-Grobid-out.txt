title
Tuning Phrase-Based Segmented Translation for a Morphologically Complex Target Language

abstract
This article describes the Aalto University entry to the English-to-Finnish shared translation task in WMT 2015. The system participates in the constrained condition, but in addition we impose some further constraints, using no language-specific resources beyond those provided in the task. We use a morphological segmenter, Morfessor FlatCat, but train and tune it in an unsupervised manner. The system could thus be used for another language pair with a morphologically complex target language, without needing modification or additional resources.

Introduction In isolating languages, such as English, suitable smallest units of translation are easy to find using whitespace and punctuation characters as delimiters. This approach of using words as the smallest unit of translation is problematic for synthetic languages with rich inflection, derivation or compounding. Such languages have very large vocabularies, leading to sparse statistics and many out-of-vocabulary words. A synthetic language uses fewer words than an isolating language to express the same sentence, by combining several grammatical markers into each word and using compound words. This difference in granularity is problematic in alignment, when a word in the isolating language properly aligns with only a part of a word in the synthetic language. In order to balance the number of tokens between target and source, it is often possi-ble to segment the morphologically richer side. Oversegmentation is detrimental, however, as longer windows of history need to be used, and useful phrases become more difficult to extract. It is therefore important to find a balance in the amount of segmentation. A linguistically accurate segmentation may be oversegmented for the task of translation, if some of the distinctions are either unmarked or marked in a similar way in the other language. An increase in the number of tokens means that the distance spanned by dependencies becomes longer. Recurrent Neural Network (RNN) based language models have been shown to perform well for English  (Mikolov et al., 2011) . Their strength lies in being theoretically capable of modeling arbitrarily long dependencies. Moreover, a huge vocabulary is particularly detrimental for neural language models due to their computationally heavy training and need to marginalize over the whole vocabulary during prediction. As morphological segmentation can reduce the vocabulary size considerably, using RNN language models seems even more suitable for this approach. Our system is designed for translation in the direction from a morphologically less complex to a more complex language. The opposite direction -simplifying morphology -has received more attention, especially with English as the target language. Of the target languages in this year's task, Finnish is the most difficult to translate into, shown by  Koehn (2005)  and reconfirmed by the evaluations of this shared task. Even though the use of supervised linguistic tools (such as taggers, parsers, or morphological analyzers) was allowed in the constrained condition, our method does not use them. It is therefore applicable to other morphologically complex target languages. 

 Related work The idea of transforming morphology to improve statistical machine translation (SMT) is well established in the literature. An early example is  Nie?en and Ney (2004) , who apply rule-based morphological analysis to enhance German?English translation. In particular, many efforts have focused on increasing the symmetry between languages in order to improve alignment.  Lee (2004)  uses this idea for Arabic?English translation. In this translation direction, symmetry is increased through morphological simplification. It has been shown that a linguistically correct segmentation does not coincide with the optimal segmentation for purposes of alignment, both using rule-based simplification of linguistic analysis  (Habash and Sadat, 2006) , and through the use of statistical methods  (Chung and Gildea, 2009) . Using segmented translation with unsupervised statistical segmentation methods has yielded mixed results.  Virpioja et al. (2007)  used Morfessor Categories-MAP in translation between three Nordic languages, including Finnish, while  Fishel and Kirik (2010)  used Morfessor Categories-MAP in English?Estonian translation. In these studies, segmentation has in many cases worsened BLEU compared to word-based translation. The main benefit of segmentation has been a decrease in the ratio of untranslated words. Salameh et al. (2015) translate English?Arabic, and find that segmentation is most useful when the extracted phrases are morphologically productive, and that using a word-level language model reduces this productivity (albeit increasing the BLEU score). The desegmentation process, and the effect of different strategies for marking the word-internal token boundaries, have mostly been examined in recombining split compound words.  Stymne and Cancedda (2011)  explore different marking strategies, including use of part-of-speech tags, in order to allow the trans-lation system to produce compounds unseen in the training data. 

 System overview An overview of the system is shown in Figure 1. The four main contributions of this work are indicated by numbered circles: 1. Use of unsupervised Morfessor FlatCat  (Gr?nroos et al., 2014)  for morphological segmentation, 2. Tuning the morphological segmentation directly to balance the number of translation tokens between source and target, 3. A new marking strategy for morph boundaries, 4. Rescoring n-best lists with RNNLM  (Mikolov et al., 2010) . Our system extends an existing phrasebased SMT system to perform segmented translation, by adding pre-processing and post-processing steps, with no changes to the decoder. As translation system to be extended, we used the Moses release 3.0  (Koehn et al., 2007) . We used GIZA++ alignment, and a 5-gram LM with modified-KN smoothing. Many Moses settings were left at their default values: phrase length 10, grow-diagfinal-and alignment symmetrization, msdbidirectional-fe reordering, and distortion limit 6. The standard pre-processing steps not specified in Figure  1  consist of normalization of punctuation, tokenization, and statistical truecasing. All three of these were performed with the tools included in Moses. In addition, the parallel data was cleaned and duplicate sentences were removed. Cleaning was performed after morphological segmentation, as the segmentation can increase the length in tokens of a sentence. The post-processing steps are the reverse of the pre-processing steps: desegmentation, detruecasing, and detokenization. Rescoring of the n-best list was done before postprocessing. The feature weights were tuned using MERT  (Och, 2003) , with BLEU  (Papineni et al., 2002)  of the post-processed hypothesis against a tuning set as the metric. 20 random restarts per MERT iteration were used, with iterations repeated until convergence. A similar MERT procedure was also used for choosing the interpolation weights for rescoring, with 100 random restarts in a single iteration. A single-iteration approach was chosen, as there was no need to translate a new n-best list during the MERT for rescoring. 

 Morphological segmentation For morphological segmentation, we use the latest Morfessor variant, FlatCat  (Gr?nroos et al., 2014) . Morfessor FlatCat is a probabilistic method for learning morphological segmentations, using a prior over morph lexicons inspired by the Minimum Description Length principle  (Rissanen, 1989) . Morfessor FlatCat applies a Hidden Markov model for morphotactics. Compared to Morfessor Baseline, it provides morph category tags (stem, prefix, suffix) and has superior consistency especially in compound word splitting. In contrast to Categories-MAP  (Creutz and Lagus, 2005) , used for statistical machine translation e.g. by Clifton and  Sarkar (2011) , it supports semi-supervised learning and hyper-parameter tuning. No annotated data was used in the training of Morfessor FlatCat, neither in training nor parameter tuning. Instead of aiming for a linguistic morphological segmentation, our goal was to balance the number of translation tokens between source and target languages. In order to bring the number of tokens on the Finnish target side closer to the English source side, we segmented the Finnish text with an unsupervised Morfessor FlatCat model, tuned specifically to achieve this balance. The corpus weight hyper-parameter ? was chosen by minimizing the sentence-level difference in token counts between the English and the segmented Finnish sides of the parallel corpus ? = arg min ? ? (e,f )?(E,F ) #(e) ? # ( M (f ; ?) ) , (1) where # gives the number of tokens in the sentence, and M (f ; ?) is the segmentation with a particular ?. Numbers and URLs occurring in the parallel corpus were passed through Morfessor unseg-mented, but translated by Moses without any special handling. 

 Morph boundary marking strategy In the desegmentation step, consecutive tokens are concatenated either with or without an intermediary space. Morph boundaries must be distinguished from word boundaries, so that the desegmentation step can reconstruct the words correctly. There are various ways to mark the boundaries, some of them shown in Table  1 . A common way is to attach a symbol to all morphs on the right (or left) side of the morph boundary. We call this strategy right-only. Alternatively both-sides of the boundary can be marked. In this strategy, a decision must be made whether to be aggressive or conservative in joining morphs, if the translation system outputs an incorrect sequence where the markers do not match up on both sides. For these experiments we chose the conservative approach, removing the unmatched marker from a half-marked boundary, and treating it as a word boundary. A downside of the right-only and both-sides strategies is that a stem is marked differently depending on whether it has a prefix attached or not, even if the surface form of the stem does not change. The morph categories produced by FlatCat can be used for marking boundaries according to the structure of the word. We can mark affixes from the side that points towards the stem, leaving stems unmarked regardless of the presence of affixes. However, this would leave the boundaries between compound parts indistinguishable from word boundaries, making some additional marking necessary. Marking affixes by category and compound boundaries with a special linking token is called the compound-symbol strategy. Instead marking the last morpheme in the compound modifiers (non-final compound parts), results in the compound-left strategy. After initial unimpressive results with the compound marking strategies, we concluded that segmenting the compound modifiers does not lead to productive translation phrases, in contrast to boundaries between compound parts and boundaries separating inflective affixes. In response, we formulated the advanced The sequence of morph categories is used for grouping the morphs into compound parts. A word consists of one or more compound parts. Each compound part consists of exactly one stem, and any number of preceding prefixes and following suffixes. CompoundPart = Pre * Stm Suf * Word = CompoundPart + (2) For all compound parts except the last one, the affixes are rejoined to their stem. Morphs of length 5 or above were treated as stems, regardless of the category assigned to them by FlatCat. Prefixes and compound modifiers are marked with a trailing '+', suffixes are marked with a leading '+', and the stems of the wordfinal compound parts are left unmarked. 

 Rescoring n-best lists Segmentation of the word forms increases the distances spanned by dependencies that should be modeled by the language model. To compensate this, we apply a strong recurrent neural network language model (RNNLM)  (Mikolov et al., 2010) . The additional language model is used in a separate rescoring step, to speed up translation, and for ease of implementation. The RNNLM model was trained on morphologically segmented data. Morphs occurring only once were removed from the vocabulary, and replaced with <UNK>. The parameters were set to 300 nodes in the hidden layer, 500 vocabulary classes, 2M direct connections of Table  2 : The data sets used for different purposes. "en-fi" signifies that parallel data was used, "fi" signifies monolingual data, or using only the Finnish side of parallel data. order 4, backpropagation through 5 time steps, with blocksize 25. At translation time, 1000-best lists of morph segmented hypotheses produced by Moses were scored using the RNNLM. The Moses features were extended by including the RNNLM score as an additional feature. A new linear combination of the features was optimized with MERT, and used for the final hypothesis ranking. For the BLEU measurement in MERT the segmented hypothesis was post-processed (including desegmentation) and compared to an un-preprocessed reference. 

 Data The data sets used in training and tuning are shown in Table  2 . Both europarl v8 and wikititles were used as parallel training data, but only europarl was used for tuning the hyperparameter ?, as the titles do not follow a typical sentence structure. The Finnish side of the parallel sets was used to extend the monolingual training data. The monolingual data were concatenated for LM training, instead of interpolating different n-gram models. After cleaning, the combined parallel training data contained 2,004,450 sentences. The parallel set used for testing during development is test2006, a europarl subset of 2000 sentences sampled from three last months of 2000.  

 Results Table  3  shows cased BLEU scores on the indomain development set and out-of-domain test set, for various configurations. The entry marked word is a baseline system without segmentation. When evaluating on the in-domain development set, most configurations that use segmentation achieve worse BLEU compared to the word baseline. Only the best configurations, using the advanced strategy, are able to achieve slightly higher BLEU. Switching domains to the test corpus leads to a larger difference, in favor of the segmenting methods. The choice of morph boundary marking strategy and the sentence-based tuning of the segmentation had a moderate effect on BLEU. The addition of rescoring did not improve BLEU on the in-domain dev-test corpus, but resulted in a slight improvement on the out-of-domain test corpus. The proportion of word tokens that were segmented into at least two parts was 19.8%. The joining of compound modifiers did not have a large effect on the total number of tokens, causing a reduction from 49,524,520 to 49,475,291 (0.1%). Using the sentence-level balancing, the optimal value for the corpus weight hyperparameter ? was 0.7. The change in the number of tokens caused by the joining of compound modifiers did not affect the optimum. Balancing the token count of the whole corpus yielded a much lower ? of 0.4, leading to oversegmentation and lower BLEU. The weight of the RNNLM in the final linear combination was 0.092, compared to 0.119 of the n-gram LM. This indicates that it is able to complement the n-gram model, but does not dominate it. In the human evaluation of WMT15, the system with advanced morph boundary marking strategy and RNNLM rescoring was ranked in tied second place of five methods participating in the constrained condition. 

 Conclusions To improve English-to-Finnish translation in a phrase-based machine translation system, we tuned an unsupervised morphological segmentation preprocessor to balance the token count between source and target languages. Appropriate choice of morph boundary marking strategy and amount of segmentation brought the BLEU score slightly above a word-based baseline, in contrast to some previous work with unsupervised segmentation  (Virpioja et al., 2007; Fishel and Kirik, 2010) . To compensate for the need of longer contexts, we added a recurrent neural network language model as a rescoring step. It did not help for the in-domain development corpus, but improved results on the out-of-domain test corpus. Possible directions for future work include Minimum Bayes Risk combination of translation hypotheses from systems trained with different segmentations and marking strategies  (De Gispert et al., 2009) , using morphology generation instead of segmented translation (Clifton and  Sarkar, 2011) , and improving the alignment directly in addition to balancing of token counts  (Snyder and Barzilay, 2008) . Figure 1 : 1 Figure 1: A pipeline overview of training and testing of the system. Main contributions are hilighted with numbers 1-4. 

 Table 3 : 3 1 Results of evaluation. 1 http://matrix.statmt.org/test_sets/list
