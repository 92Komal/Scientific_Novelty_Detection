title
ESPnet-ST IWSLT 2021 Offline Speech Translation System

abstract
This paper describes the ESPnet-ST group's IWSLT 2021 submission in the offline speech translation track. This year we made various efforts on training data, architecture, and audio segmentation. On the data side, we investigated sequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech translation. Specifically, we used multi-referenced SeqKD from multiple teachers trained on different amounts of bitext. On the architecture side, we adopted the Conformer encoder and the Multi-Decoder architecture, which equips dedicated decoders for speech recognition and translation tasks in a unified encoder-decoder model and enables search in both source and target language spaces during inference. We also significantly improved audio segmentation by using the pyannote.audio toolkit and merging multiple short segments for long context modeling. Experimental evaluations showed that each of them contributed to large improvements in translation performance. Our best E2E system combined all the above techniques with model ensembling and achieved 31.4 BLEU on the 2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of tst2021. * *Equal contribution 1 https://sites.google.com/ view/iwslt-evaluation-2019/ speech-translation/off-limit-ted-talks 2 https://ict.fbk.eu/ must-c-release-v2-0/

Introduction This paper presents the ESPnet-ST group's English?German speech translation (ST) system submitted to the IWSLT 2021 offline speech translation track. ESPnet  (Watanabe et al., 2018)  has been widely used for many speech applications; automatic speech recognition (ASR), textto-speech , speech translation , machine translation (MT), and speech separation/enhancement  (Li et al., 2021) . The purpose of this submission is not only to show the recent progress on ST researches, but also to encourage future research by building strong systems along with the open-sourced project. This year we focused on (1) sequence-level knowledge distillation (SeqKD)  (Kim and Rush, 2016) , (2) Conformer encoder  (Gulati et al., 2020) , (3) Multi-Decoder architecture  (Dalmia et al., 2021) , (4) model ensembling, and (5) better segmentation with a neural network-based voice activity (VAD) system  (Bredin et al., 2020)  and a novel algorithm to merge multiple short segments for long context modeling. Our primary focus was E2E models, although we also compared them with cascade systems with our best effort. All experiments were conducted with the ESPnet-ST toolkit , and the recipe is publicly available at https://github.com/espnet/ espnet/tree/master/egs/iwslt21. 

 Data preparation In this section, we describe data preparation for each task. The corpus statistics are listed in Table  1 . We removed the off-limit talks following previous evaluation campaigns 1 . To fit the GPU memory, we excluded utterances having more than 3000 speech frames or more than 400 characters. All sentences were tokenized with the tokenizer.perl script in the Moses toolkit  (Koehn et al., 2007) . 

 ASR We used  Must-C (Di Gangi et al., 2019) , Must-C v2 2 , ST-TED  (Jan et al., 2018) , Librispeech  (Panayotov et al., 2015) , and TEDLIUM2  (Rousseau et al., 2012)  corpora. We used the cleaned version of ST-TED following   data was augmented by three-fold speed perturbation  (Ko et al., 2015)  with speed ratios of 0.9, 1.0, and 1.1 except for Librispeech. We removed case information and punctuation marks except for apostrophes from the transcripts. The 5k unit vocabulary was constructed based on the byte pair encoding (BPE) algorithm  (Sennrich et al., 2016)  with the sentencepiece toolkit 3 using the English transcripts only. 

 E2E-ST We used Must-C, Must-C v2, and ST-TED only. The shared source and target vocabulary of BPE16k units was constructed using cased and punctuated transcripts and translations. 

 MT We used available bitext for WMT20 4 in addition to the in-domain TED data used for E2E-ST systems. We first performed perplexity-based filtering with an in-domain n-gram language model (LM) (Moore and Lewis, 2010). We controlled the WMT data size by thresholding and obtained three data pools: 5M, 10M, and 20M sentences. Next, we removed non-printing characters and performed language identification with the langid.py toolkit  (Lui and Baldwin, 2012)  5 and kept sentences whose lan-  guage IDs were identified correctly on both English and German sides. We also removed sentences having more than 250 tokens in either language or a source-target length ratio of more than 1.5 with the clean-corpus-n.perl script in Moses. Finally, we removed sentences having CJK and other unrelated characters in either language with the built-in regex module in Python. The resulting data size is shown in Table  2 . We found that our filtering strategy removed 22-37% of data. Note that the above filtering process was performed over the WMT data only. For each data size, the joint source and target vocabulary of BPE32k units was constructed using cased and punctuated sentences after the filtering. We did not use additional monolingual data. 3 System 

 Conformer encoder Conformer encoder  (Gulati et al., 2020 ) is a stacked multi-block architecture and has shown consistent improvement over a wide range of E2E speech processing applications  (Guo et al., 2020) . The architecture of each block is depicted in Figure  1 . It includes a multi-head self-attention module, a convolution module, and a pair of position-wise feed-forward modules in the Macaron-Net style. While the self-attention module learns the long- Recent studies have shown improvements by introducing Conformer in the E2E-ST task  (Guo et al., 2020; Inaguma et al., 2021b) , which motivated us to adopt this architecture as our system. 

 SeqKD Sequence-level knowledge distillation (Se-qKD)  (Kim and Rush, 2016 ) is an effective method to transfer knowledge in a teacher model to a student model via discrete symbols. Our recent studies  (Inaguma et al., 2021a,b)  showed a large improvement in ST performance with this technique. Unlike the previous studies, however, we used more training data than bitext in ST training data to train teacher MT models. We translated source transcripts in the ST training data by the teacher MT models with a beam width of 5 and then replaced the original ground-truth translation with the generated translation. We used cased and punctuated transcripts as inputs to the MT teachers. We also combined both the original and pseudo translations as data augmentation (multi-referenced training) (Gordon and Duh, 2019). 

 Multi-Decoder architecture The Multi-Decoder is an E2E-ST model using Searchable Hidden Intermediates to decompose the overall ST task into ASR and MT subtasks  (Dalmia et al., 2021) . As shown in Figure  2 , the Multi-Decoder consists of two encoder-decoder models, an ASR sub-net and a subsequent MT subnet, where the hidden representations of the ASR decoder are passed as inputs to the encoder of the MT sub-net. During inference, the best ASR decoder hidden representations are retrieved using beam search decoding at this intermediate stage. Since this framework decomposes the overall ST task, it brings several advantages of cascaded approaches into the E2E setting. For instance, the Multi-Decoder allows for greater search capabilities and separation of speech and text encoding. However, one trade-off is a greater risk of error propagation from the ASR sub-net to the downstream MT sub-net. To alleviate this issue, we condition the decoder of the MT sub-net on the ASR encoder hidden representations in addition to the MT encoder hidden representations using multisource cross-attention. This improved variant of the architecture is called the Multi-Decoder with Speech Attention. 

 Model ensembling We use posterior probability combination to ensemble models trained with different data and architectures. During inference, we perform a posterior combination at each step of beam search decoding by first computing the softmax normalized posterior probabilities for each model in the ensemble and then taking the mean value. In this ensembling approach, a single unified beam search operates over the combined posteriors of the models to find the most likely decoded sequence. 

 Segmentation How to segment audio during inference significantly impacts ST performances  (Gaido et al., 2020; Pham et al., 2020; Potapczyk and Przybysz, 2020; Gaido et al., 2021) . This is because the ST systems are usually trained with utterances segmented based on punctuation marks  (Di Gangi et al., 2019)  while the audio segmentation by voice activity detection (VAD) at test time does not access such meta information. Since VAD splits a long speech recording into chunks by silence regions, it would prevent models from extracting semantically coherent contextual information. Therefore, it is very important to seek a better segmentation strategy in order to minimize this gap in training and test conditions and evaluate models correctly. In fact, the last year's winner obtained huge improvements by using their own segmentation strategy. Motivated by this fact, we investigated two VAD systems apart from the provided segmentation. Specifically, we used WebRTC 6 and pyannote.audio  (Bredin et al., 2020)   return Q 22: end function bRTC, we set the frame duration, padding duration, and aggressive mode to 10ms, 150ms, and 3, respectively. For pyannote.audio, we used a publicly available model pre-trained on the DIHARD corpus  (Ryant et al., 2019) . However, we observed that VAD systems are more likely to generate short segments because they do not take contextual information into account. Therefore, we propose a novel algorithm to merge multiple short segments into a single chunk to enable long context modeling by self-attention in both encoder and decoder modules. The proposed algorithm is shown in Algorithm 1. We first perform VAD and obtain multiple segments. Then, we check the segments in a greedy way from left to right and merge adjacent segments if (1) the total utterance duration is below a threshold M dur [10ms] and (2) the time interval of the two segments is below a threshold M int [10ms]. This process continues until no segment is merged in an iteration. Although recent studies proposed similar methods  (Potapczyk and Przybysz, 2020; Gaido et al., 2021) , our algorithm is a bottom-up approach while theirs are top-down. 

 Experimental setting In this section, we describe the experimental setting for each task. The detailed configurations for each task are summarized in Table  3 .  

 Feature extraction We extracted 80-channel log-mel filterbank coefficients computed with 25-ms window size and shifted every 10-ms with 3-dimensional pitch features using the Kaldi toolkit  (Povey et al., 2011) . The features were normalized by the mean and the standard deviation calculated on the entire training set. We applied SpecAugment  (Park et al., 2019)  with mask parameters (m T , m F , T, F ) = (2, 2, 40, 30) and time-warping for both ASR and E2E-ST tasks. 

 ASR We used both Transformer and Conformer architectures. The encoder had two CNN blocks followed by 12 Transformer/Conformer blocks following  (Karita et al., 2019; Guo et al., 2020) . Each CNN block consisted of a channel size of 256 and a kernel size of 3 with a stride of 2 ? 2, which resulted in time reduction by a factor of 4. Both architectures had six Transformer blocks in the decoder. In both encoder and decoder blocks, the dimensions of the self-attention layer d model and feed-forward network d ff were set to 512 and 2048, respectively. The number of attention heads H was set to 8. The kernel size of depthwise separable convolution in Conformer blocks was set to 31. We optimized the model with the joint CTC/attention objective  (Watanabe et al., 2017)  with a CTC weight of 0.3. We also used CTC scores during decoding but did not use any external LM for simplicity. We adopted the best model configuration from the Librispeech ASR recipe in ESPnet. 

 MT We used the Transformer-Base and -Big configurations in  (Vaswani et al., 2017) . 

 E2E-ST We used the same Conformer architecture as ASR except for the vocabulary. We initialized the en- On the decoder side, we initialized parameters like BERT  (Devlin et al., 2019) , where weight parameters were sampled from N (0, 0.02), biases were set to zero, and layer normalization parameters were set to ? = 0, ? = 1. This technique led to better translation performance and faster convergence. 

 Results 

 ASR 

 Architecture We compared Transformer and Conformer ASR architectures in Table  4 . We observed that Conformer significantly outperformed Transformer. Therefore, we use the Conformer encoder in the following experiments. 

 Segmentation Next, we investigated the VAD systems and the proposed segment merging algorithm for long context modeling in Table  5 . We used the same decoding hyperparameters tuned on Must-C. We firstly observed that merging short segments was very effective probably because it alleviated frame classification errors in the VAD systems. Among three audio segmentation methods, we confirmed that pyannote.audio significantly reduced the WER while WebRTC had negative impacts compared to the provided segmentation. Specifically, we found that the dihard option in pyannote.audio worked very well while the rest options did not. The optimal maximum duration M dur was around 2000 frames (i.e., 20 seconds). In the last experiments, we tuned the maximum interval M int among {50, 100, 150, 200} and found 50 and 100 (i.e., 0.5 and 1 second) was best on average. Compared to the provided segmentation, we obtained a 49.6% improvement on average. 

 MT In this section, we show the results of our MT systems used for cascade systems and pseudo labeling in SeqKD. We report case-sensitive detokenized BLEU scores  (Papineni et al., 2002)  with the multi-bleu-detok.perl script in Moses. We carefully investigated the effective amount of WMT training data to improve the performance of the TED domain. The results are shown in Table  6 . We confirmed that adding the WMT data improved the performance by more than 4 BLEU. Regarding the WMT data size, using up to 10M sentences was helpful, but 20M did not show clear improvements, probably because of the undersampling of the TED data. Oversampling as in multilingual NMT  (Arivazhagan et al., 2019)  could alleviate this problem, but this is beyond our scope. After training with a mix of the WMT and TED data, we also tried to finetune the model with the TED data only, but this did not lead to clear improvement, especially for the IWSLT test sets. Increasing the model capacity was not helpful, although the conclusion might change by adding more training data and evaluating the model in other domains such as news. Because our primary focus to use MT systems was pseudo labeling for SeqKD, we decided to use the Base configuration to speed up decoding. Finally, we checked the BLEU scores on the Must-C training data used for SeqKD. We observed that adding more WMT data decreased the BLEU score, from which we can conclude that using more WMT data gradually changed the MT output from the TED style. Therefore, we decided to use the models trained on WMT5M and WMT10M as teachers for SeqKD. 

 Speech translation 

 E2E-ST SeqKD The results are shown in (A1) achieved 35.63 BLEU on the Must-C tst-COMMON set, and it is the new state-of-theart record to the best of our knowledge. Surprisingly, it even outperformed text-based MT systems in Table  6 . On the other hand, unlike our observations in  (Inaguma et al., 2021a,b) , SeqKD (A2-4) degraded the performance on the Must-C tst-COMMON set. However, the results on the Must-C dev and tst-HE sets showed completely different trends, where we observed better BLEU scores by SeqKD in proportion to the WMT data used for training the teachers. Therefore, after tuning audio segmentation, we also evaluated the models on the unsegmented IWSLT test sets. Here, we used the pyannote.audio based segmentation with (M dur , M int ) = (2000, 100) as described in ?5.1.2. Then, we confirmed large improvements with SeqKD by 2-6 BLEU, and therefore we decided to determine the best model based on the IWSLT test sets. Multi-referenced training consistently improved the BLEU scores on the IWSLT sets. For example, A4 outperformed A1 by 6.02 BLEU on tst2019 although the tst2019 set was well-segmented (WER: 6.0%). Given these observations, we recommend evaluating ST models on  provements over the best single model, B1. We found that an ensemble of all of our models, A1-4 and B1, achieved the best result of 23.61 BLEU on tst2019 and outperformed B1 by 2.55 BLEU. Although A1 as a single system performs worse on tst2019 than the other single systems as shown in Table  7 , including it in an ensemble with the two best single systems, B1 and A4, still yielded a slight gain of 0.32 BLEU (E2). Therefore, we can conclude that weak models are still beneficial for ensembling. 

 Segmentation Similar to ?5.1.2, we also investigated the impact of audio segmentation for E2E-ST models. To this end, we used the A4 model. Note that we used the same decoding hyperparameters tuned on Must-C. The results are shown in Table 9. We confirmed a similar trend to ASR. Although (M dur , M int ) = (1500, 100) showed the best performance on average, we decided to use (M dur , M int ) = (2000, 100) for submission considering the best performance on the latest IWSLT test, tst2019. 

 Cascade system We also evaluated the cascade system with the Conformer ASR and the Transformer-Base MT trained on the WMT10M data (C1). The MT model was trained by feeding source sentences without case information and punctuation marks. The results in Table  9  showed that the BLEU scores correlated to the WER in Table ,5 and the performance was comparable with that of A4. Although there is some room for improving the performance of the cascade system further by using in-domain English LM, it is difficult to conclude which modeling (cascade or E2E) is effective because the cascade system had more model parameters in the ASR decoder and MT encoder. This means that the E2E model could also be enhanced by using a similar amount of parameters. 

 Final system Our final system was the best ensemble system E4, using the pyannote.audio based segmentation with (M dur , M int ) = (2000, 200) 8 . This system, which was our primary submission, scored 24.14 BLEU on tst2019 as shown in Table  10 . Compared to the result in Table  8 , it improved by 0.53 BLEU thanks to better audio segmentation. It was also slightly higher than the IWSLT20 winner's submission by SPROL  (Potapczyk and Przybysz, 2020) . We also present the results on tst2020 and tst2021 in Table  10 . Our primary submission E4 outperformed the result of last year's winner system on tst2020. 

 Conclusion In this paper, we have presented the ESPnet-ST group's offline systems on the IWSLT 2021 submission. We significantly improved the baseline Conformer performance with multi-referenced SeqKD, Multi-Decoder architecture, segment merging algorithm, and model ensembling. Our future work includes scaling training data and careful analysis of the performance gap in different test sets. 
