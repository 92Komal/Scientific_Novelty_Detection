title
Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation

abstract
Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.

Introduction Speech translation (ST) increasingly receives attention from the machine translation (MT) community recently. To learn the transformation between speech in the source language and the text in the target language, conventional models pipeline automatic speech recognition (ASR) and text-to-text MT model  (B?rard et al., 2016) . However, such pipeline systems suffer from error propagation. Previous works show that deep end-to-end models can outperform conventional pipeline systems with sufficient training data  (Weiss et al., 2017; Inaguma et al., 2019; . Nevertheless, well-annotated bilingual data is expensive and hard to collect  (Bansal et al., 2018a,b; Duong et al., 2016) . Multitask learning plays an essential role in leveraging a large amount of monolingual data to improve representation in ST. Multitask ST models have two jointly learned decoding parts, namely the recognition and translation part. The recognition part firstly decodes the speech of source language into the text of source language, and then based on the output of the recognition part, the translation part generates the text in the target language. Variant multitask models have been explored  (Anastasopoulos and Chiang, 2018) , which shows the improvement in low-resource scenario. Although applying the text of source language as the intermediate information in multitask end-toend ST empirically yielded improvement, we argue whether this is the optimal solution. Even though the recognition part does not correctly transcribe the input speech into text, the final translation result would be correct if the output of the recognition part preserves sufficient semantic information for translation. Therefore, we explore to leverage word embedding as the intermediate level instead of text. In this paper, we apply pre-trained word embedding as the intermediate level in the multitask ST model. We propose to constrain the hidden states of the decoder of the recognition part to be close to the pre-trained word embedding. Prior works on word embedding regression show improved results on  MT (Jauregi Unanue et al., 2019; Kumar and Tsvetkov, 2018) . Experimental results show that the proposed approach obtains improvement to the ST model. Further analysis also shows that constrained hidden states are approximately isospectral to word embedding space, indicating that the decoder achieves speech-to-semantic mappings. 

 Multitask End-to-End ST model Our method is based on the multitask learning for ST  (Anastasopoulos and Chiang, 2018) , including speech recognition in the source language and translation in the target language, as shown in Fig.  1(a) . The input audio feature sequence is first encoded into the encoder hidden state sequence h = h 1 , h 2 , . . . , h T with length T by the pyramid encoder  (Chan et al., 2015) . To present speech recognition in the source language, the attention mechanism and a decoder is employed to produce source decoder sequence ? = ?1 , ?2 , . . . , ?M , where M is the number of decoding steps in the source language. For each decoding step m, the probability P (? m ) of predicting the token ?m in the source language vocabulary can be computed based on the corresponding decoder state ?m . To perform speech translation in the target language, both the source language decoder state sequence ? and the encoder state sequence h will be attended and treated as the target language decoder's input. The hidden state of target language decoder can then be used to derived the probability P (y q ) of predicting token y q in the target language vocabulary for every decoding step q. Given the ground truth sequence in the source language ? = ?1 , ?2 , . . . , ?M and the target language y = y 1 , y 2 , . . . , y Q with length Q, multitask ST can be trained with maximizing log likelihood in both domains. Formally, the objective function of multitask ST can be written as: L ST = ? M L src + ? Q L tgt = ? M m ? log P (? m ) + ? Q q ? log P (y q ), (1) where ? and ? are the trade-off factors to balance between the two tasks. 

 Proposed Methods We propose two ways to help the multitask endto-end ST model capture the semantic relation between word tokens by leveraging the source language word embedding as intermediate level. ? = {? 1 , ?2 , ...? |V | } , where V is the vocabulary set and ?v ? R D is the embedding vector with dimension D for any word v ? V , in the recognition task. We choose the source language decoder state (embedding) ? to reinforce since it is later used in the translation task. To be more specific, we argue that the embedding generated by the source language decoder should be more semantically correct in order to benefit the translation task. Given the pre-trained source language word embedding ?, we proposed to constrain the source decoder state ?m at step m to be close to its corresponding word embedding ?m with the two approaches detailed in the following sections. 

 Directly Learn Word Embedding Since semantic-related words would be close in terms of cosine distance  (Mikolov et al., 2018) , a simple idea is to minimize the cosine distance (CD) between the source language decoder hidden state ?m and the corresponding word embedding ?m for every decode step m, L CD = m 1 ? cos(f ? (? m ), ?m ) = m 1 ? f ? (? m ) ? ?m f ? (? m ) ?m , (2) where f ? (?) is a learnable linear projection to match the dimensionality of word embedding and decoder state. With this design, the network architecture of the target language decoder would not be limited by the dimension of word embedding. Fig.  1 (b) illustrates this approach. By replacing L src in Eq. (  1 ) with L CD , semantic learning from word embedding for source language recognition can be achieved. 

 Learn Word Embedding via Probability Ideally, using word embedding as the learning target via minimizing CD can effectively train the decoder to model the semantic relation existing in the embedding space. However, such an approach suffers from the hubness problem  (Faruqui et al., 2016)  of word embedding in practice (as we later discuss in Sec. 4.5). To address this problem, we introduce cosine softmax (CS) function  (Liu et al., 2017a,b)  to learn speech-to-semantic embedding mappings. Given the decoder hidden state ?m and the word embedding ?, the probability of the target word ?m is defined as P CS (? m ) = exp(cos(f ? (? m ), ?m )/?) ?v? ? exp(cos(f ? (? m ), ?v )/?) , (3) where cos(?) and f ? (?) are from Eq. (  2 ), and ? is the temperature of softmax function. Note that since the temperature ? re-scales cosine similarity, the hubness problem can be mitigated by selecting a proper value for ?. Fig.  1 (c) illustrates the approach. With the probability derived from cosine softmax in Eq. (  3 ), the objective function for source language decoder can be written as L CS = m ? log P CS (? m ). (4) By replacing L src in Eq. (  1 ) with L CS , the decoder hidden state sequence ? is forced to contain semantic information provided by the word embedding. 

 Experiments 

 Experimental Setup We used Fisher Spanish corpus  (Graff et al., 2010)  to perform Spanish speech to English text translation. And we followed previous works  (Inaguma et al., 2019)  for pre-processing steps, and 40/160 hours of train set, standard dev-test are used for the experiments. Byte-pair-encoding (BPE)  (Kudo and Richardson, 2018)  was applied to the target transcriptions to form 10K subwords as the target of the translation part. Spanish word embeddings were obtained from FastText pre-trained on Wikipedia  (Bojanowski et al., 2016) , and 8000 Spanish words were used in the recognition part. The encoder is a 3-layer 512-dimensional bidirectional LSTM with additional convolution layers, yielding 8? down-sampling in time. The decoders are 1024-dimensional LSTM, and we used one layer in the recognition part and two layers in the translation part. The models were optimized using Adadelta with 10 ?6 as the weight decay rate. Scheduled sampling with probability 0.8 was applied to the decoder in the translation part. Experiments ran 1.5M steps, and models were selected by the highest BLEU on four transcriptions per speech in dev set. 

 Speech Translation Evaluation Baseline: We firstly built the single-task end-toend model (SE) to set a baseline for multitask learning, which resulted in 34.5/34.51 BLEU on dev and test set respectively, which showed comparable results to  Salesky et al. (2019)    34.50 34.51 17.41 15.44 ME 35.35 35.49 23.30 20.40 CD 33.06 33.65 23.53 20.87 CS 35.84 36.32 23.54 21.72  Table  1 : BLEU scores trained on different size of data. we could see that ME outperforms SE in all conditions. High-resource: Column (a) in Table  1  showed the results trained on 160 hours of data. CD and CS represent the proposed methods mentioned in Sec. 3.1 and 3.2 respectively. We got mixed results on further applying pre-trained word embedding on ME. CD degraded the performance, which is even worse than SE, but CS performed the best. Results showed that directly learn word embedding via cosine distance is not a good strategy in the high-resource setting, but integrating similarity with cosine softmax function can significantly improve performance. We leave the discussion in Sec. 4.5. Low-resource: We also experimented on 40 hours subset data for training, as shown in column (b) in Table  1 . We could see that ME, CD and CS overwhelmed SE in low-resource setting. Although CD resulted in degrading performance in high-resource setting, it showed improvements in low-resource scenario. CS consistently outperformed ME and CD on different data size, showing it is robust on improving ST task. 

 Analysis of Recognition Decoder Output In this section, we analyzed hidden states s by existing methods. For each word v in corpus, we denoted its word embedding ?v as pre-trained embedding, and e v as predicted embedding. Note that because a single word v could be mapped by multiple audio segments, we took the average of all its predicted embedding. We obtained the top 500 frequent words in the whole Fisher Spanish corpus, and tested on the sentences containing only these words in test set. Eigenvector Similarity: To verify our proposed methods can constrain hidden states in the word embedding space, we computed eigenvector similarity between predicted embedding and pre-trained embedding space. The metric derives from Laplacian eigenvalues and represents how similar be- 160 hours 40 hours P@1 P@5 P@1 P@5 ME 1.85 6.29 1.11 9.  62 CD 61.48 77.40 56.30 69.25 CS 17.78 35.19 10.37 25.19  Table  3 : Precision@k of semantic alignment on test set. tween two spaces, the lower value on the metric, the more approximately isospectral between the two spaces. Previous works showed that the metric is correlated to the performance of translation task  (S?gaard et al., 2018; Chung et al., 2019) . As shown in Table  2 , predicted embedding is more similar to pre-trained embedding when models trained on sufficient data (160 v.s 40 hours). CD is the most similar case among the three cases, and ME is the most different case. Results indicated that our proposals constrain hidden states in pre-trained embedding space. Semantic Alignment: To further verify if predicted embedding is semantically aligned to pretrained embedding, we applied Procrustes alignment  method to learn the mapping between predicted embedding and pre-trained embedding. Top 50 frequent words were selected to be the training dictionary, and we evaluated on the remaining 450 words with cross-domain similarity local scaling (CSLS) method. Precision@k (P@k, k=1,5) were reported as measurements. As shown in Table  3 , CD performed the best, and ME was the worst one. This experiment reinforced that our proposals can constrain hidden states to the similar structure of word embedding space. 

 Speech Recognition Evaluation We further analyzed the results of speech recognition for ME and CS. To obtain the recognition results from Eq (3), simply take arg max v P CS (v). The word error rate (WER) of the source language recognition was reported in has worse WER, but higher BLEU compared with ME. We concluded that although leveraging word embedding at the intermediate level instead of text results in worse performance in speech recognition (this indicates that the WER of the recognition part does not fully determine the translation performance), the semantic information could somewhat help multitask models generate better translation in terms of BLEU. We do not include the WER of CD in Table  1  because its WER is poor (>100%), but interestingly, the BLEU of CD is still reasonable, which is another evidence that WER of the intermediate level is not the key of translation performance. 

 Cosine Distance (CD) v.s. Softmax (CS) Based on experimental results, we found that proposals are possible to map speech to semantic space. With optimizing CS, BLEU consistently outperformed ME, which shows that utilizing semantic information truly helps on ST. Directly minimizing cosine distance made the predicted embedding space closest to pre-trained embedding space, but performed inconsistently on BLEU in different data sizes. We inferred that the imbalance word frequency training and hubness problem  (Faruqui et al., 2016)  in word embedding space made hidden states not discriminated enough for the target language decoder while optimizing CS can alleviate this issue. 

 Conclusions Our proposals showed that utilizing word embedding as intermediate helps with the ST task, and it is possible to map speech to the semantic space. We also observed that lower WER in source language recognition not imply higher BLEU in target language translation. This work is the first attempt to utilize word embedding in the ST task, and further techniques can be applied upon this idea. For example, crosslingual word embedding mapping methods can be considered within the ST model to shorten the distance between MT and ST tasks. Figure 1 : 1 Figure 1: (a) Multitask ST model. Dotted arrows indicate steps in the recognition part. Solid arrows indicate steps in the translation part. (b) Directly learn word embedding via cosine distance. (c) Learn word embedding via cosine softmax function. Both (b)(c) are the recognition part in (a). 

 Table 2 : 2 Eigenvector similarity. 160 hours 40 hours dev test dev test ME 16.50 18.58 13.80 15.09 CD 2.60 3.44 3.95 3.63 CS 11.55 13.76 8.62 9.80 

 Table 4 . 4 Combining the results shown inTable 1, we could see that CS 160 hours 40 hours dev test dev test ME 43.13 38.57 53.42 54.70 CS 50.15 44.43 57.63 57.21 

 Table 4 : 4 Word error rate (%) trained on different size of data.
