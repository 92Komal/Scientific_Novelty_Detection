title
It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information

abstract
The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments

Introduction Machine translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks  (Sutskever et al., 2014; Bahdanau et al., 2015) , which generally surpass phrase-based systems  (Koehn, 2009)  in a variety of domains and languages  (Bentivogli et al., 2016; Toral and S?nchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019) . Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted  (Birch et al., 2008; . However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at filling this gap: Ceteris paribus, Figure  1 : Left: Decomposing the uncertainty of a sentence as mutual information plus language-inherent uncertainty: mutual information (MI) corresponds to just how much easier it becomes to predict T when you are given S. MI is symmetric but the relation between H(S) and H(T ) can be arbitrary. Right: estimating cross-entropies using models q MT and q LM invalidates relations between bars, except that H q? (?) ? H(?). XMI, our proposed metric, is no longer purely a symmetric measure of language, but now an asymmetric measure that mostly highlights models' shortcomings. is it easier to translate from English into Finnish or into Hungarian? And how much easier is it? Conversely, is it equally hard to translate Finnish and Hungarian into another language? Based on BLEU  (Papineni et al., 2002)  scores, previous work  (Belinkov et al., 2017)  suggests that translating into morphologically rich languages, such as Hungarian or Finnish, is harder than translating into morphologically poor ones, such as English. However, a major obstacle in the crosslingual comparison of MT systems is that many automatic evaluation metrics, including BLEU and METEOR  (Banerjee and Lavie, 2005) , are not cross-lingually comparable. In fact, being a function of n-gram overlap between candidate and reference translations, they only allow for a fair comparison of the performance between models when translating into the same test set in the same target language. Indeed, one cannot and should not draw conclusions about the difficulty of translating a source language into different target languages purely based on BLEU (or METEOR) scores. In response, we propose cross-mutual information (XMI), a new metric towards cross-linguistic comparability in NMT. In contrast to BLEU, this information-theoretic quantity no longer explicitly depends on language, model, and tokenization choices. It does, however, require that the models under consideration are probabilistic. As an initial starting point, we perform a case study with a controlled experiment on 21 European languages. Our analysis showcases XMI's potential for shedding light on the difficulties of translation as an effect of the properties of the source or target language. We also perform a correlation analysis in an attempt to further explain our findings. Here, in contrast to the general wisdom, we find no significant evidence that translating into a morphologically rich language is harder than translating into a morphologically impoverished one. In fact, the only significant correlate of MT difficulty we find is source-side type-token ratio. 2 Cross-Linguistic Comparability through Likelihoods, not BLEU Human evaluation will always be the gold standard of MT evaluation. However, it is both timeconsuming and expensive to perform. To help researchers and practitioners quickly deploy and evaluate new systems, automatic metrics that correlate fairly well with human evaluations have been proposed over the years  (Banerjee and Lavie, 2005; Snover et al., 2006; Isozaki et al., 2010; Lo, 2019) . BLEU  (Papineni et al., 2002) , however, has remained the most common metric to report the performance of MT systems. BLEU is a precisionbased metric: a BLEU score is proportional to the geometric average of the number of n-grams in the candidate translation that also appear in the reference translation for 1 ? n ? 4. 1 In the context of our study, we take issue with two shortcomings of BLEU scores that prevent a cross-linguistically comparable study. First, it is not possible to directly compare BLEU scores across languages because different languages might express the same meaning with a very different number of words. For instance, agglutinative languages like Turkish often use a single word to express what other languages have periphrastic constructions for. To be concrete, the expression "I will have been programming" is five words in En-1 BLEU also corrects for reference coverage and includes a length penalty, but we focus on the high-level picture. glish, but could easily have been one word in a language with sufficient morphological markings; this unfairly boosts BLEU scores when translating into English. The problem is further exacerbated by tokenization techniques as finer granularities result in more partial credit and higher n for the n-gram matches  (Post, 2018) . In summary, BLEU only allows us to compare models for a fixed target language and tokenization scheme, i.e. it only allows us to draw conclusions about the difficulty of translating different source languages into a specific target one (with downstream performance as a proxy for difficulty). Thus, BLEU scores cannot provide an answer to which translation direction is easier between any two source-target pairs. In this work, we address this particular shortcoming by considering an information-theoretic evaluation. Formally, let V S and V T be source-and target-language vocabularies, respectively. Let S and T be source-and target-sentence-valued random variables for languages S and T, respectively; then S and T respectively range over V * S and V * T . These random variables S and T are distributed according to some true, unknown probability distribution p. The cross-entropy between the true distribution p and a probabilistic neural translation model q MT (t | s) is defined as: H q MT (T | S) = (1) ? t?V * T s?V * S p(t, s) log 2 q MT (t | s) Since we do not know p, we cannot compute eq. (1). However, given a held-out data set of sentence pairs {(s (i) , t (i) )} N i=1 assumed to be drawn from p, we can approximate the true cross-entropy as follows: H q MT (T | S) ? (2) ? 1 N N i=1 log 2 q MT (t (i) | s (i) ) In the limit as N ? ?, eq. (  2 ) converges to eq. (  1 ). We emphasize that this evaluation does not rely on language tokenization provided that the model q MT does not  (Mielke, 2019) . While common in the evaluation of language models, cross-entropy evaluation has been eschewed in machine translation research since (i) not all MT models are probabilistic and (ii) we are often interested in measuring the quality of the candidate translation our model actually produces, e.g. under approximate decoding. However, an information-theoretic evaluation is much more suitable for measuring the more abstract notion of which language pairs are hardest to translate to and from, which is our purpose here. 

 Disentangling Translation Difficulty and Monolingual Complexity We contend that simply reporting cross-entropies is not enough. A second issue in performing a controlled, cross-lingual MT comparison is that the language generation component (without translation) is not equally difficult across languages  (Cotterell et al., 2018) . We claim that the difficulty of translation corresponds more closely to the mutual information MI(S; T ) between the source and target language, which tells us how much easier it becomes to predict T when S is given (see Figure  1 ). But what is the appropriate analogue of mutual information for cross-entropy? One such natural generalization is a novel quantity that we term cross-mutual information, defined as: XMI(S ? T ) = H q LM (T ) ? H q MT (T | S) (3) where H q LM (T ) denotes the cross-entropy of the target sentence T under the model q LM . As in ?2, this can, analogously, be approximated by the crossentropy of a separate target-side language model q LM over our held-out data set: XMI(S ? T ) ? (4) ? 1 N N i=1 log 2 q LM (t (i) ) q MT (t (i) | s (i) ) which, again, becomes exact as N ? ?. In practice, we note that we mix different distributions q LM (t) and q MT (t | s) and, thus, q LM (t) is not necessarily representable as a marginal: there need not be any distribution q(s) such that q LM (t) = s?V * S q MT (t | s) ? q(s). While q MT and q LM can, in general, be any two models, we exploit the characteristics of NMT models to provide a more meaningful, model-specific estimate of XMI. NMT architectures typically consist of two components: an encoder that embeds the input text sequence, and a decoder that generates translated output text. The latter acts as a conditional language model, where the source-language sentence embedded by the encoder drives the target-language generation. Hence, we use the decoder of q MT as our q LM to accurately estimate the difficulty of translation for a given architecture in a controlled way. In summary, by looking at XMI, we can effectively decouple the language generation component, whose difficulties have been investigated by  Cotterell et al. 2018 and , from the translation component. This gives us a measure of how rich and useful the information extracted from the source language is for the target-language generation component. 

 Experiments In order to measure which pairs of languages are harder to translate to and from, we make use of the latest release v7 of Europarl  (Koehn, 2005) : a corpus of the proceedings of the European Parliament containing parallel sentences between English (en) and 20 other European languages: Bulgarian (bg), Czech (cs), Danish (da), German (de), Greek (el), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl) and Swedish (sv). Pre-processing steps In order to precisely effect a fully controlled experiment, we enforce a fair comparison by selecting the set of parallel sentences available across all 21 languages in Europarl. This fully controls for the semantic content of the sentences; however, we cannot adequately control for translationese  (Stymne, 2017; Zhang and Toral, 2019) . Our subset of Europarl contains 190,733 sentences for training, 1,000 unique, random sentences for validation and 2,000 unique, random sentences for testing. For each parallel corpus, we jointly learn byte-pair encodings (BPE;  Sennrich et al., 2016)  for the source and target languages, using 16,000 merge operations. We use the same vocabularies for the language models. 2 Setup In our experiments, we train Transformer models  (Vaswani et al., 2017) , which often achieve state-of-the-art performance on MT for various language pairs. In particular, we rely on the Py-Torch  (Paszke et al., 2019)  re-implementation of the Transformer model in the fairseq toolkit  (Ott et al., 2019) . For language modeling, we use the decoder from the same architecture, training it at the sentence level, as opposed to commonly used fixedlength chunks. We train our systems using label smoothing (LS;  Szegedy et al., 2016; Meister et    Figure  2 : Some correlations between metrics in Table  1 , into and from English. More correlations in Figure  4 . 2020) as it has been shown to prevent models from over-confident predictions, which helps to regularize the models. We report cross-entropies (H q MT , H q LM ), XMI, and BLEU scores obtained using SACREBLEU  (Post, 2018) .  3  Finally, in a similar vein to  Cotterell et al. (2018) , we multiply crossentropy values by the number of sub-word units generated by each model to make our quantities independent of sentence lengths (and divide them by the total number of sentences to match our approximations of the true distributions). See App. A for experimental details. 

 Results and Analysis We train 40 systems, translating each language into and from English.  4  The models' performance in terms of BLEU scores, and the cross-mutual information (XMI) and cross-entropy values over the test sets are reported in Table  1  with significant values marked in App. B. 3 Signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.2.12.  4  Due to resource limitations, we chose these tasks because most of the information available in the web is in English (https://w3techs.com/technologies/ overview/content_language) and effectively translating it into any other language would reduce the digital language divide (http://labs.theguardian.com/ digital-language-divide/). Besides, translating into English gives most people access to any local information. Translating into English When translating into the same target language (in this case, English), BLEU scores are, in fact, comparable, and can be used as a proxy for difficulty. We can then conclude, for instance, that Lithuanian (lt) is the hardest language to translate from, while Spanish (es) is the easiest. In this scenario, given the good correlation of BLEU scores with human evaluations, it is desirable that XMI correlates well with BLEU. This behavior is indeed apparent in the blue points in the left part of Figure  2 , confirming the efficacy of XMI in evaluating the difficulty of translation while still being independent of the target language generation component. Translating from English Despite the large gaps between BLEU scores in Table  1 , one should not be tempted to claim that it is easier to translate into English than from English for these languages as often hinted at in previous work (e.g.,  Belinkov et al., 2017) . As we described above, different target languages are not directly comparable, and we actually find that XMI is slightly higher, on average, when translating from English, indicating that it is actually easier, on average, to transfer information correctly in this direction. For instance, translation from English to Finnish is shown to be easier than from Finnish to English, despite the large gap Figure  3 : H q LM (T ), decomposed into XMI(S ? T ), the information that the system successfully transfers, and H q MT (T | S), the uncertainty that remains in the target language, all measured in bits. Note that in XMI(S ? T ) the translation is from the left to the right argument.  in BLEU scores. This suggests that the former model is heavily penalized by the target-side language model; this is likely because Finnish has a large number of inflections for nouns and verbs. Another interesting example is given by Greek (el) and Spanish (es) in Table  1 , where, again, the two tasks achieve very different BLEU scores but similar XMI. In light of the correlation with BLEU when translating into English, this shows us that Greek is just harder to language-model, corroborating the findings of . Moreover, Figure  2  clearly shows that, as expected, XMI is not as well correlated with BLEU when translating from English, given that BLEU scores are not cross-lingually comparable. 

 Correlations with linguistic and data features Last, we conduct a correlation study between the translation difficulties as measured by XMI and the linguistic and data-dependent properties of each translation task, following the approaches of  and . Table  2 lists  Pearson's and Spearman's correlation coefficients for data-dependent metrics, where bold values indicate statistically significant results (p < 0.05) after Bonferroni correction (p < 0.0029). Interestingly, the only features that significantly correlate with our metric are related to the type-to-token ratio (TTR) for the source language and the distance between source and target TTRs. This implies that a potential explanation for the differences in translation difficulty lies in lexical variation. For full correlation results, refer to App. D. 

 Conclusion In this work, we propose a novel informationtheoretic approach, XMI, to measure the translation difficulty of probabilistic MT models. Differently from BLEU and other metrics, ours is language-and tokenization-agnostic, enabling the first systematic and controlled study of crosslingual translation difficulties. Our results show that XMI correlates well with BLEU scores when translating into the same language (where they are comparable), and that higher BLEU scores in different languages do not necessarily imply easier translations. In future work, we plan to extend this analysis across more translation pairs, more diverse languages and multiple domains, as well as investigating the effect of translationese or source-side grammatical errors  (Anastasopoulos, 2019) . 
