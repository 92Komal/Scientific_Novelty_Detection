title
An Analysis of Encoder Representations in Transformer-Based Machine Translation

abstract
The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.

Introduction Machine translation (MT) is one of the prominent tasks in Natural Language Processing, tackled in several ways  (Bojar et al., 2017) . Neural MT (NMT) has become the de-facto standard with a performance that clearly outperforms the alternative approach of Statistical Machine Translation  (Luong et al., 2015b; Bojar et al., 2016; Bentivogli et al., 2016) . NMT also improves training procedures due to the end-to-end fashion without tedious feature engineering and complex setups. During recent years, a lot of research has been done on NMT, designing new architectures, starting from the plain sequence-to-sequence model  (Sutskever et al., 2014; , to an improved version featuring an attention mechanism  (Bahdanau et al., 2015; Luong et al., 2015a) , to models that only use attention instead of recurrent layers  (Vaswani et al., 2017)  and models that apply convolution networks  (Gehring et al., 2017a,b) . Among the different architectures, the Transformer  (Vaswani et al., 2017)  has emerged as the dominant NMT paradigm.  1  Relying only on attention mechanisms, the model is fast, highly accurate and has been proven to outperform the widely used recurrent networks with attention and ensembling  (Wu et al., 2016 ) by more than 2 BLEU points. Improved translation quality is typically related to better representation of structural information. While other approaches make use of external information to improve the internal representation of NMT models  (Arthur et al., 2016; Niehues and Cho, 2017; Alkhouli and Ney, 2017) , the Transformer seems to be able to encode a lot of structural information without explicitly incorporating any structural constraints. However, being a rather new architecture, little is known about what the model exactly learns internally. A better understanding of the internal representations of neural models has become a major challenge in NMT  (Koehn and Knowles, 2017) . In this work we investigate the kind of linguistic information that is learned by the encoder. We start by training the Transformer system from English to seven languages, with different training set sizes, resulting in models that are not only trained for different target languages but also with expected differences in translation quality. First, we visually inspect the attention weights of the encoders, in order to find linguistic patterns. As the next step, we exploit the attention weights of the network to build a graph and induce tree structures for each sentence, showing whether syntactic dependencies between words have been learned or not in the spirit of  Williams et al. (2018)  and  Liu and Lapata (2018) . Additionally, following previous studies on how to analyze the internal representation of neural systems  (Adi et al., 2016; Shi et al., 2016; Belinkov et al., 2017a) , we probe the encoder weights of the trained models to address different sequence labeling tasks: Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic tagging. We evaluate the quality of the decoder on a given task to assess how discriminative the encoder representation is for that task. Lastly, in order to check whether the learned information can be transferred across models, we use the encoder weights of a high-resource language pair to initialize a low-resource language pair, inspired by the work of  Zoph et al. (2016) . We show that, also for the Transformer, the knowledge of an encoder representation can be shared with other models, helping them to achieve better translation quality. Overall, our analysis leads to interesting insights about strengths and weaknesses of the attention weights of the Transformer, giving more empirical evidence about the kind of information the model is learning at each layer: ? We find that each layer has at least one attention head that encodes a significant amount of syntactic dependencies. ? Consistent with previous findings on the sequence-to-sequence paradigm, probing the encoder to four different sequence labeling tasks reveals that lower layers tend to encode more syntactic information, whereas upper layers move towards semantic tasks. ? The information about the length of the input sentence starts to vanish after the third layer. ? The study corroborates that attention can be used to transfer knowledge between high-and low-resource languages. 

 Architecture The architecture of the Transformer system follows the so called encoder-decoder paradigm, trained in an end-to-end fashion. Without using any recurrent layer, the model takes advantage of the positional embedding as a mechanism to encode order within a sentence. The encoder, typically stacks 6 identical layers, in which each of them makes use of the so called multi-head attention and of a 2 sublayers feed-forward network, coupled with layer normalization and residual connection (see Figure  1 ). The multi-head attention mechanism computes attention weights, i.e., a softmax distribution, for each word within a sentence, including the word itself. Specifically: Attention(Q, K, V ) = sof tmax( QK T ? d k )V (1) where the input consists of queries Q and keys K of dimension d k , and values V of dimension d v . The queries, keys and values are linearly projected h times, to allow the model to jointly attend to information from different representation, concatenating the result, M ultiHead(Q, K, V ) = Concat(head1, ..., head h )W O where headi = Attention(QW Q i , KW K i , V W V i ) with parameter matrices W Q i ? R d model ?d k , W K i ? R d model ?d k , W V i ? R d model ?dv and W O ? R hdv?d model . 2 On top of the multi-head attention there is a feed-forward network that consists of two layers with a ReLU activation in between. Each encoder layer takes as input the output of the previous layer, allowing it to attend to all positions of the previous layer. The decoder has the same architecture as the encoder, stacking 6 identical layers of multi-head attention with feed-forward networks. However, here there are two multi-head attention sub-layers: i) a decoder self-attention and ii) a encoder-decoder attention. The decoder self-attention attends on the previous predictions made step by step, masked by one position. The second multi-head attention performs an attention between the final encoder representation and the decoder representation. To summarize, the Transformer model consists of three different attentions: i) the encoder selfattention, in which each position attends to all positions in the previous layer, including the position itself, ii) the encoder-decoder attention, in which each position of the decoder attends to all positions in the last encoder layer, and iii) the decoder self-attention, in which each position attends to all previous positions including the current position. In this work, we focus on analyzing the structure that is learned by the first type of attention weights of the model, i.e., the encoder self-attention, across different models with different target language and translation quality. 

 Methodology We aim at analyzing the encoder representation of different models by assessing their quality through several experiments: i) by visualizing the attention weights (Section 5), ii) by inducing tree structure from the encoder weights (Section 6), iii) by probing the encoder as input representation for various prediction tasks (Section 7), and iv) by transferring the knowledge of one encoder to another (Section 8). We start by looking for linguistic patterns through the visualization of the heat-maps of the encoder weights. Next, we use the softmax weights extracted from the multi-head attention to build maximum spanning trees from the input sentences, assessing the quality of the induced tree through dependency parsing. Additionally, we evaluate the ability of the decoder, using a fixed encoder representation as input, on several sequence labeling tasks, measuring how important the input features are for various tasks. As test bed we use four dif- The assumption is that if a property is well encoded in the input representation then it is easy for the decoder to predict that property. In practice, after training the MT system, we freeze the encoder parameters, and train one decoder layer for each task. The decoder layer is simpler than the original one used for MT; it consists only of one attention head and one feedforward layer with ReLU activation. Moreover, in order to output the right amount of labels, the decoder also has to learn implicitly the length of the input sentence. Note that our goal is not to beat the state of the art in a given task but rather to analyze the representation of an encoder trained for MT on different tasks referring to different linguistic properties. Finally, to assess whether the knowledge captured within an encoder is general enough to also be used for other models, we test a transfer learning scenario in which we use the encoder representation of a high resource language pair to initialize the encoder of a low resource language pair. Here, we assume that a model is better at encoding abstract linguistic properties if it can share useful information to enhance another weaker model. 

 Model setup We trained Transformer models 3 from English to seven languages, Czech, German, Estonian, Finnish, Russian, Turkish and Chinese, using the parallel data provided by the WMT18 shared task on news translation.  4  The parallel data come from different sources, mainly from Europarl  (Koehn, 2005) , News Commentary  (Tiedemann, 2012)  and ParaCrawl.  5  The data sets are partially noisy, especially ParaCrawl being on its first release, and to filter out potentially incorrect parallel sentences we used a language identifier 6 to tag each source and target sentence, discarding the sentences that do not match across languages  (Stymne et al., 2013; Zarin ?a et al., 2015) . As development set we used the provided newsdev data from the shared task, while using the newstest from WMT 2017 and 2018 as test data. A widely used technique to allow an open vocabulary is byte pair encoding  (Sennrich et al., 2016) , in which the source and target words are split into subword units. However, in this work we prefer to use the full word forms, allowing us to evaluate and compare the internal representation on standard sequence labeling benchmarks tagged with gold labels on the full word forms. Therefore, we use a large vocabulary of 100K words per language. General statistics on the training data are given in Table  1 . As can be seen, we ended up having an heterogeneous amount of data, ranging from 200K for Turkish up to 51M for Czech. We trained each model for maximum 20 epochs, taking the best one according to the development set as model to evaluate. The BLEU score 7 of each model is shown in Table  2 . Even though the scores seem low for the Transformer architecture for the MT task, we have to note that each model is trained using full word forms in order to facilitate the analysis of the encoder representation (our results are in line with the comparison between subword units and full word forms done by  Sennrich et al. (2016) ). SACREBLEU script  (Post, 2018) , with signature BLEU+case.mixed+lang.en-{targetLanguage}+numrefs.1+smooth.exp+test.wmt{17,18}+ tok.13a+version.1.3.0 We do not aim at beating the best system on the test data, as our main point is to analyze different encoder representations across models with different translation quality and target language. 

 Encoder Evaluation: Visualization One of the most straightforward ways of understanding the weights of a neural network is by visualizing them. In its base setting, the Transformer employs 6 layers with 8 different attention heads for each of them, making complete visualization difficult. Therefore, we focus only on attention weights with high scores that are visually interpretable. We discovered four different patterns shared across models: paying attention to the word itself, to the previous and next word and to the end of the sentence (Figure  2 ). We found that, usually on the first layer, i.e., layer 0, more attention heads focus their weights on the word itself, while on the subsequent layers the network moves the attention more on other words, e.g., on the next and previous word, and to the end of the sentence. This suggests that the transformer tries to find long dependencies between words on higher layers whereas it tends to focus on local dependencies in lower layers. 

 Encoder Evaluation: Inducing Tree Structure The architecture of the Transformer, linking each word with each other with an attention weight, can be seen as a weighted graph in which the words are the nodes and from which tree structure can be extracted. Even though the models are not trained to produce any trees or to a specific syntax task, we used the attention weights in each layer to extract a tree of the input sentences and inspect whether they reflect a dependency tree. We evaluated the induced trees on the English PUD treebank from the CoNLL 2017 Shared Task  (Zeman et al., 2017) . The PUD treebank consists of 1000 sentences randomly taken from on-line newswire and Wikipedia. We measure the performance as Unlabeled Attachment Score (UAS) with the official evaluation script 8 from the shared task, using gold segmentation and tokenization. Plus, given that our weights have no knowledge about the root of the sentence, we decided to use the gold root as starting node for the maximum spanning  tree algorithm. Specifically, we run the Chu-Liu-Edmonds algorithm  (Chu, 1965; Edmonds, 1967)  for each attention head of each layer of the models to extract the maximum spanning trees. Table  3  shows the F1-score of the induced structures. For comparison purposes, in this dataset, a state of the art supervised parser  (Dozat et al., 2017)  reaches 88.22 UAS F1-score and our random baseline, i.e., induced trees with random weights and gold root, achieves 10.1 UAS F1-score on average. 9 Given our findings in Section 5, we also computed a leftand right-branching baseline (with golden root), obtaining 10.39 and 35.08 UAS F1-score respectively. Although our models are not trained to produce trees, the best dependency trees induced on each layer are far better than the random baseline, suggesting that the models are learning some syntactic relationships. However, the best scores do not achieve results much beyond the right branching baseline, showing that it is difficult to encode more complex and longer dependencies. Overall, for all language pairs we notice the same performance trend across layers. Comparing 9 Even though not comparable in this setting, unsupervised systems developed to build dependency trees achieve on an English dataset UAS F1-score ranging from 27.9 to 51.4 when using the output of a PoS tagger system  (Alonso et al., 2017) . our low resource language pair, English-Turkish, to the other high resource languages, we can see that the models trained with larger dataset are able to induce better syntactic relationships, while among high resource languages all models are in the same ballpark, without any specific correlation with BLEU score, suggesting that it becomes more difficult to induce better dependency relations at a certain point. Figure  3  shows some examples of induced dependency trees. Interestingly enough, we can see that the trees with higher scores follow the patterns found in Section 5, in which each word is linked to the next one, so encoding most compounds and multi-word expressions. From visualizing other trees, even if they do not belong to the best attention head, we can see that they try to capture longer dependencies, as for dress and stuffy in the example in Figure  3 . 

 Encoder Evaluation: Probing Sequence Labeling Tasks We evaluated the encoder representation through four different sequence labeling tasks: Part-of-Speech (PoS) tagging, Chunking, Named Entity Recognition (NER) and Semantic tagging (SEM). In this test bed we used the trained weights of the encoder, keeping them fixed, training only one de- coder layer using one attention head and one feedforward layer. We then assess the quality of the encoder representation across stacked layers. Evaluation Benchmarks. We used a standard benchmark for each task: the Universal Dependencies English Web Treebank v2.0  (Zeman et al., 2017)  for PoS tagging, the CoNLL2000 Chunking shared task (Tjong Kim  Sang and Buchholz, 2000) , the CoNLL2003 NER shared task (Tjong Kim  Sang and De Meulder, 2003) , and the annotated data from the Parallel Meaning Bank (PMB) for Semantic tagging  (Abzianidze et al., 2017) . Each benchmark provides its own training, development and test data, except chunking in which we use 10% of the training corpus as validation, and the PMB in which we used the silver portion for training and the gold portion for test and dev (following the 80-20 split).  10  Table  5  reports general statistics on each benchmark, regarding the granularity of each task, the number of training and testing instances, and the average length of the test sentences. Evaluation Results. Table  4  reports the performance for each task and stacked layers, together with the error rate for sentence length prediction. For each language pair, we can see that the syntax information, i.e., the PoS task, is encoded mostly in the first 3 layers, corroborating the results in Section 6, while moving towards more semantic tasks, as NER and SEM we can see that in general the decoder needs more encoder layers to achieve better results. Another interesting finding is provided by the length mismatch between the output of the models and the gold labels. Clearly the models encode the information about the sentence length in the first three layers, and then the information starts to vanish with an increase of the error rate. The only exception is given by the SEM task, but as can be seen from the statistics in average sentence length is very short and so it is easier to predict. Overall, comparing the performance reached on these probing tasks with the BLEU score of each model, we can see again that the high resource language pairs achieve better results compared to our low resource language pair. Moreover, we notice that in general higher BLEU score correspond to higher probing results, confirming the trend that encoding linguistic proprieties within the encoder representation go on par with better translation quality  (Niehues and Cho, 2017; Kiperwasser and Ballesteros, 2018) . 

 Encoder Evaluation: Transfer learning To assess whether the knowledge encoded in the attention units can help other models in a low resource scenario, we additionally carried out an evaluation of the encoder representation in a transfer learning task. Similar to  Zoph et al. (2016) , we used the encoder weights from one high resource language, i.e., English-German, to train a Transformer system for our low resource language pair, English-Turkish. We provide two experiments: i) initializing and fine tuning the encoder weights (TL1), ii) initializing and keeping the encoder weights fixed (TL2). Table  6  shows the BLEU scores of the systems evaluated with and without transferring the encoder parameters. Both transfer learning settings are helpful to the decoder to reach a better translation quality, with almost 2 BLEU point more on the best scenario. Starting with a better encoder representation, taken from a high resource language pair, and then fine tuning the parameters on the low resource language achieves the best result, matching and corroborating previous findings on recurrent networks  (Zoph et al., 2016) . 

 Related Work The problem of interpreting and understanding neural networks is attracting more and more interest and work, with so many models and new architectures being published continuously each year. One of the first techniques to examine a neural network involves the analysis of activation patterns of the hidden layers  (Elman, 1991; Giles et al., 1992) . Nowadays, given its popularity, recurrent neural networks are the most evaluated networks, mainly investigated on the structures and linguistic properties they are encoding  (Linzen et al., 2016; Enguehard et al., 2017; Kuncoro et al., 2017; Gulordava et al., 2018) . Traditionally, a common way to inspect neural networks is by visualizing the hidden representation trained for a specific task  (Ding et al., 2017; Strobelt et al., 2018a,b) , and to evaluate them by assessing the properties through downstream tasks  (Chung et al., 2014; Greff et al., 2017) . Other recent studies look for hidden linguistic units that provide information on how the network works  (Karpathy et al., 2015; Qian et al., 2016; K?d?r et al., 2017) , while another line of analysis probes the representation learned by a neural network as input to a classifier of another task  (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a; Tran et al., 2018) . The most closely related work is by  Belinkov et al. (2017b) , in which they investigate the representation learned by the encoder of a sequence-tosequence NMT system across different languages. Unlike them, we studied a neural network without any recurrent layers, which allows us to induce a tree representation from the input sentence, probing the encoder representation towards more downstream tasks, and showing that the attention weights can also be used to transfer knowledge to low-resource languages. 

 Conclusion In this paper we investigated the kind of information that is captured by the encoder representation of a Transformer model trained for the task of Machine Translation. We analyzed and compared experimentally different models across several languages, including the visualization of weights, building tree structure from each sentence, probing the representation to four different sequence-labeling tasks and by transferring the encoder knowledge to a low resource language. Unlike most previous studies, where the analysis is made only on RNNs, we examined an architecture based on attention only. Our experimental evaluation sheds lights on interesting findings about dependency relations and syntactic and semantic behavior across layers. In future work, we plan to extend the analysis with probing tasks to evaluate other linguistic properties  (Conneau et al., 2018)  as well as to a recent evaluation dataset  (Sennrich, 2017) , tackling also the attention weights between the encoder and the decoder. Figure 1 : 1 Figure 1: The Transformer architecture (illustration from Vaswani et al. (2017)). 
