title
Linguistic evaluation of German-English Machine Translation using a Test Suite

abstract
We present the results of the application of a grammatical test suite for German?English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented.

Introduction For decades, the development of Machine Translation (MT) has been based on either automatic metrics or human evaluation campaigns with the main focus on producing scores or comparisons (rankings) expressing a generic notion of quality. Through the years there have been few examples of more detailed analyses of the translation quality, both automatic (HTER  (Snover et al., 2009) ,  Hjerson (Popovi?, 2011) ) and human  (MQM Lommel et al., 2014) . Nevertheless, these efforts have not been systematic and they have only focused on few shallow error categories (e.g. morphology, lexical choice, reordering), whereas the human evaluation campaigns have been limited by the requirement for manual human effort. Additionally, previous work on MT evaluation focused mostly on the ability of the systems to translate test sets sampled from generic text sources, based on the assumption that this text is representative of a common translation task  (Callison-Burch et al., 2007) . In order to provide more systematic methods to evaluate MT in a more fine-grained level, recent research has relied to the idea of test suites  (Guillou and Hardmeier, 2016; Isabelle et al., 2017) . The test suites are assembled in a way that allows testing particular issues which are the focus of the evaluation. The evaluation of the systems is not based on generic text samples, but from the perspective of fulfilling a priori quality requirements. In this paper we use the DFKI test suite for German?English MT  (Burchardt et al., 2017)  in order to analyze the performance of the 16 MT Systems that took part at the translation task of the Fourth Conference of Machine Translation. The evaluation focuses on 107 mostly grammatical phenomena organized in 14 categories. In order to apply the test suite, we follow a semiautomatic methodology that benefits from regular expressions, followed by minimal human refinement (Section 3). The application of the suite allows us to form conclusions on the particular grammatical performance of the systems and perform several comparisons (Section 4). 

 Related Work Several test suites have been presented as part of the Test Suite track of the Third Conference of Machine Translation  (Bojar et al., 2018a) . Each test suite focused on a particular phenomenon, such as discourse  (Bojar et al., 2018b) , morphology  (Burlot et al., 2018) , grammatical contrasts (Cinkova and  Bojar, 2018) , pronouns  (Guillou et al., 2018)  and word sense disambiguation  (Rios et al., 2018) . In contrast to the above test suites, our test suite is the only one that does such a systematic evaluation of more than one hundred phenomena. A direct comparison can be done with the latter related paper, since it focuses at the same language direction. Its authors use automated methods to extract text items, whereas in our test suite the test items are created manually. 

 Method The test suite is a manually devised test set whose contents are chosen with the purpose to test the performance of the MT system on specific phenomena or requirements related to quality. For each phenomenon a subset of relevant test sentences is chosen manually. Then, each MT system is requested to translate the given subset and the performance of the system on the particular phenomenon is calculated based on the percentage of the phenomenon instances that have been properly translated. For this paper we use the latest version of the DFKI Test Suite for MT on German to English. The test suite has been presented in  (Burchardt et al., 2017)  and applied extensively in last year's shared task  (Macketanz et al., 2018b) . The current version contains 5560 test sentences in order to control 107 phenomena organised in 14 categories. It is similar to the method used last year, with few minor corrections. The number of the test instances per phenomenon varies, ranging between a 20 and 180 sentences. A full list of the phenomena and their categories can be seen as part of the results in the Appendix. An example list of test sentences with correct and incorrect translations is available on GitHub 1 . 

 Construction and application of the test suite The construction and the application of the test suite follows the steps below, also indicated in Figure 1: (a) Produce paradigms: A person with good knowledge of German and English grammar devises or selects a set of source language sentences that may trigger translation errors related to particular phenomena. These sentences may be written from scratch, inspired from previous observations on common MT errors or drawn from existing resources  (Lehmann et al., 1996) . (b) Fetch sample translations: The source sentences are given as an input to easily accessible MT systems and their outputs are fetched. (c) Write regular expressions: By inspecting the MT output for every given sentence, the annotator writes rules that control whether the output contains a correct translation regarding the respective phenomenon. The rules are written as positive or This is done in contact with their developers or through the submission process of a shared task, as is the case described in this paper. (e) Apply regular expressions: The control rules are applied on the MT outputs in order to check whether the relevant phenomena have been translated properly. When the MT output matches a positive regular expression, the translation is considered correct (pass) whereas when the MT output matches a negative regular expression, the translation is considered incorrect (fail). Examples can be seen in Table  1 . In case an MT output does not match either a positive or a negative regular expression, or in case these contradict to each other, the automatic evaluation results in a uncertain decision (warning). (f) Resolve warnings and refine regular expressions: The warnings are given to the annotator, so that they manually resolve them and if possible refine the rules to address similar cases in the future. Through the iterative execution of steps (e) and (f) (which are an extension of steps (c) and (d) respectively) the rules get more robust and attain a better coverage. If needed, the annotator can add full sentences as rules, instead of regular expressions. For every system we calculate the phenomenonspecific translation accuracy as the the number of the test sentences for the phenomenon which were translated properly, divided by the number of all test sentences for this phenomenon:  

 Experiment Setup In the evaluation presented in the paper, MT outputs are obtained from the 16 systems that are part of the news translation task of the Fourth Conference on Machine Translation (WMT19). According to the details that the developers have published by the time this paper is written, 10 of the systems are declared to be Neural Machine Translation (NMT) systems and 9 of them confirm that they follow the Transformer paradigm, whereas for the rest 6 systems no details were given. For the evaluation of the MT outputs the software TQ-AutoTest  (Macketanz et al., 2018a)  was used. After processing the MT output for the 5560 items of the test suite, the automatic application of the regular expressions resulted to about 10% warnings. Consequently, one human annotator (student of linguistics) committed about 70 hours of work in order to reduce the warnings to 3%. The final results were calculated using 5393 test items, which, after the manual inspection, did not have any warning for any of the respective MT-outputs. Since we applied the same test suite as last year, this year's automatic evaluation is profiting from the manual refinement of the regular expressions that took place then. The first application of the test suite in 2018 resulted in about 10-45% of warnings depending on the system, whereas after this year's application, we only had 8-28%. This year's results are therefore based on 16% more valid test items, as compared to last year. 

 Results The results of the test suite evaluation can be seen in Tables  3 and 4 , where the significantly best systems for every category or phenomenon are boldfaced. The average accuracy per system is calculated either based on all test items (with the assumption that all items have equal importance) or based on the categories (with the assumption that all categories have equal importance). In any case, since the averages are calculated on an artificial test suite and not on a sample test set, one must be careful with their interpretation. 

 Linguistic categories Despite the significant progress of NMT and the recent claims for human parity, the results in terms of the test suite are somewhat mediocre. The MT systems achieve 75.6% accuracy in average for all given test items, which indicates that one out of four test items is not translated properly. If one considers the categories separately, only five categories have an accuracy of more than 80%: negation, where there are hardly any mistakes, followed by composition, function word, subordination and non-verbal agreement. The lowestperforming categories are the multi-word expressions (MWE) and the verb valency with about 66% accuracy. 

 Linguistic phenomena Most MT systems seem to struggle with idioms, since they could only translate properly only 11.6% of the ones in our test set, whereas a similar situation can be observed with resultative predicates (17.8%). Negated modal pluperfect and modal pluperfect have an accuracy of only 23-28%. Some of the phenomena have an accuracy of about 50%, in particular the domain-specific terms, the pseudo-cleft sentences and the modal of pluperfect subjunctive II (negated or not). We may assume that these phenomena are not correctly translated because they do not occur often enough in the training and development corpora. On the other side, for quite a few phenomena an accuracy of more than 90% has been achieved. This includes several cases of verbs declination concerning the transitive, intransitive and ditransitive verbs mostly on perfect and future tenses, the passive voice, the polar question, the infinitive clause, the conditional, the focus particles, the location and the phrasal verbs. 

 Comparison between systems As seen in Table  3 , the system that significantly wins most categories is Facebook with 11 categories and an average of 87.5% (if all categories counted equally), followed by DFKI and RW-TH which are in the best cluster for 10 categories. When it comes to averaging all test items, the best systems are RWTH and Online-A. On specific categories, the most clear results come in punctuation where NEU has the best performance with 100% accuracy, whereas Online-X has the worst with 31.7%. Concerning ambiguity, Facebook has the highest performance with 92.6% accuracy. In verb tense/aspect/mood, RWTH Aachen and Online-A have the highest performance with 84% accuracy, whereas in this category, MSRA.MADL has the lowest performance with 60.4%. For the rest of the categories there are small differences between the systems, since more than five systems fall into the same significance cluster of the best performance. When looking into particular phenomena (Table 4), Facebook has the higher accuracy concerning lexical ambiguity with an accuracy of 93.7%. NEU and MSRA.MADL do best with more than 95% on quotation marks. The best system for translating modal pluferect is online-A with 75.6%, whereas at the same category, Online-Y and Online-G perform worse, with less than 2.2%. On modal negated -preterite, the best systems are RWTH and UCAM with more than 95%. On the contrary, MSRA.MADL achieves the worst ac-curacy, as compared to other systems, in phenomena related to modals (perfect, present, preterite, negated modal Future I), where it mistranslates half of the test items. One system, Online-X, was the worst on quotation marks, as it did not convey properly any of them, compared to other systems that did relatively well. Online-Y also performs significantly worse than the other systems on domain-specific terms. 

 Comparison with last year's systems One can attempt to do a vague comparison of the statistics between two consequent years (Table 2). Here, the last column indicates the percentage of improvement from the average accuracy of all systems from last year's shared task 2 to the average accuracy of all systems of this year. Although this is not entirely accurate, since different systems participate, we assume that the large amount of the test items allows some generalisations to this direction. When one compares the overall accuracy, there has been an improvement of about 6%. When focusing on particular categories, the biggest improvements are seen at function words (+12.5%), non-verbal agreement (+9.7%) and punctuation (+8%). The smallest improvement is seen at named entity and terminology (+0.3%). We also attempt to perform comparisons of the systems which were submitted with the same name both years. Again, the comparison should be done under the consideration that the MT systems are different in many aspects, which are not possible to consider at the time this paper is written. The highest improvement is shown by the system Online-G, which has an average accuracy improvement of 18.7%, with most remarkable the one concerning negation, function words and non-verbal agreement. Online-A has also improved at composition, verb issues and non-verbal agreement and RWTH and UEDIN at punctuation. On the contrary, we can notice that UCAM deteriorated its accuracy for several categories, mostly for coordination and ellipsis (-13.1%), verb issues (?7.6%) and composition (-4.7%). JHU and Online-G and RWTH show some deterioration for three categories each, whereas Online-A seems to have worsened considerably regarding punctuation (-21.6%) and UEDIN regarding negation (?10.5%). 7.5 5.7 0.0 1.9 1.9 3.8 -1.8 3.8 4.7 Named entity and terminology 34 5.9 3.0 5.9 0.0 -3.0 -5.9 8.9 0.0 5.9 0.3 Negation 19 0.0 0.0 0.0 0.0 42.1 0.0 0.0 0.0 -10.5 6.6 Non-verbal agreement 48 12.5 10.4 12.5 0.0 22.9 2.1 -2.1 0.0 12.5 9.7 Punctuation 51 5.9 2.0 -21.6 0.0 -7.9 1.9 27.5 0.0 23.5 8.0 Subordination 31 3.3 6.5 -6.5 3.2 19.4 3.2 6.5 0.0 0.0 5.0 Verb tense/aspect/mood 3995 -4.0 -5.9 12.9 0.  

 Conclusion and Further Work The application of the test suite results in a multitude of findings of minor or major importance. Despite the recent advances, state-of-the-art German?English MT still translates erroneously one out of four test items of our test suite, indicating that there is still room for improvement. For instance, one can note the low performance on MWE and verb valency, whereas there are issues with idioms, resultative predicates and modals. Function words, non verbal agreement and punctuation on the other side have significantly improved. One potential benefit of the test suite would be to investigate the implication of particular development settings and design decisions on particular phenomena. For some superficial issues, such as punctuation, this would be relatively easy, as pre-and post-processing steps may be responsible. But for more complex phenomena, further comparative analysis of settings is needed. Unfortunately, this was hard to achieve for this shared task due to the heterogeneity of the systems, but also due to the fact that at the time this paper was written, no exact details about the systems were known. We aim at looking further on such an analysis in future steps.  Sluicing 18 88.9 88.9 83.3 88.9 88.9 88.9 88.9 88.9 83.3 77.8 88.9 83.3 88.9 88.9 88.9 88.9 87.        Figure 1 : 1 Figure 1: Example of the preparation and application of the test suite for one test sentence 
