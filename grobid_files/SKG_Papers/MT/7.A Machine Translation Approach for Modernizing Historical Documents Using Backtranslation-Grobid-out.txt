title
A Machine Translation Approach for Modernizing Historical Documents Using Backtranslation

abstract
Human language evolves with the passage of time. This makes historical documents to be hard to comprehend by contemporary people and, thus, limits their accessibility to scholars specialized in the time period in which a certain document was written. Modernization aims at breaking this language barrier and increase the accessibility of historical documents to a broader audience. To do so, it generates a new version of a historical document, written in the modern version of the document's original language. In this work, we propose several machine translation approaches for modernizing historical documents. We tested these approaches in different scenarios, obtaining very encouraging results.

Introduction Historical documents are an important part of our cultural heritage. With the aim of their preservation, there is an increase need of digitalazing-creating a digital text version which can be searched and automatically processed-of historical documents  [1] . However, there is an additional difficulty created by their linguistic properties: On the one hand, human language evolves with the passage of time. On the other hand, due to a lack of a spelling convention, orthography changes depending on the author and time period in which a given document was written. These problems make historical documents harder to read, and makes it harder to digitalize them (since their digital text version needs to be searched and automatically processed). The orthography problem has been well researched in the literature  [2, 3, 4, 5, 6, 7] . The proposed solution that aims to solve this problem is known as spelling normalization, and its goal is to adapt the document's spelling to modern standards in order to achieve an orthography consistency and increase the document's read-ability. However, while is true that spelling normalization makes historical documents easier to read, they are still hard to comprehend by contemporary people. This problem limits the accessibility of historical documents to scholars specialized in the time period in which the document was written. Modernization aims at breaking the language barrier, generating a new version of a historical document in the modern version of the language in which the document was originally written (see Fig.  1  for an example). Therefore, not only the orthography is updated. The lexicon and grammar are also modified in order to match the modern use of the document's language. The main drawback of this solution is that part of the document's original intention could be lost in the process (e.g., part of the rhyme in Fig.  1  is lost for the sake of clarity). Nonetheless, the document's clarity is increased and, thus, its accessibility to a broader audience. To the best of our knowledge, modernization of historical documents has been less researched in the literature. A shared task was organized in order to translate historical text to contemporary language  [9] . The shared task's main goal was spelling normalization. However, they also tackle modernization using a set of rules. Finally, there was an approach to modernize historical documents using Statistical Machine Translation (SMT)  [10] . In this work, we tackle modernization by using an SMT and Neural Machine Translation (NMT) approach. Additionally, since a frequent problem when working with historical documents is the scarce availability of parallel training data  [5] , we created two small parallel corpora (see Section 3.1) and generated synthetic data using backtranslation  [11] . Our main contributions are the followings: ? First use, to the best of our knowledge, of NMT and backtranslation for historical documents mod-  ernization. ? Comparison of approaches based on SMT and NMT. ? Experimented with three historical corpora-two of which were created for this work-from three different time periods and two different languages. The rest of this document is structure as follows: Section 2 introduces the different Machine Translation (MT) approaches used in our work. Then, Section 3 describes the experiments conducted in order to assess our proposal. After that, Section 4 presents and discusses the results of those experiments. Finally, in Section 5, conclusions are drawn. 

 Machine Translation In this section, we present the machine translation approaches used in our work. 

 Statistical Machine Translation Given a source sentence x, SMT aims at finding its best translation ?  [12] : ? = arg max y P r(y | x) (1) For years, the prevailing approach to compute this expression have been phrase-based models  [13] . These models rely on a log-linear combination of different models  [14] : namely, phrase-based alignment models, reordering models and language models; among others  [15, 16] . However, more recently, this approach has shifted into neural models (see Section 2.2). 

 Neural Machine Translation NMT is the neural approach to compute Eq. (  1 ). Frequently, it relies on a Recurrent Neural Network (RNN) encoder-decoder framework. In this framework, the source sentence is projected into a distributed representation at the encoding step. At the decoding step, the decoder generates its translation word by word  [17] . The system's input is a sequence of words in the source language. Each source word is linearly projected to a fixed-sized real-valued vector through an embedding matrix. These word embeddings are feed into a bidirectional  [18]  Long Short-Term Memory (LSTM)  [19]  network, resulting in a sequence of annotations produced by concatenating the hidden states from the forward and backward layers. The model features an attention mechanism  [20] , which allows the decoder to focus on parts of the input sequence, computing a weighted mean of annotations sequence. A soft alignment model computes these weights by weighting each annotation with the previous decoding state. The decoder is another LSTM network, conditioned by the representation computed by the attention model and the last word generated. Finally, a deep output layer  [21]  computes a distribution over the target language vocabulary. The model is trained by means of stochastic gradient descend, applied jointly to maximize the log-likelihood over a bilingual parallel corpus. At decoding time, the model approximates the most likely target sentence with beam-search  [17] . 

 Backtranslation Backtranslation  [11]  has become the norm when building state-of-the-art NMT systems, especially in resourcepoor scenarios  [22] . It is a useful technique to increase the training data by creating synthetic text from monolingual data. Given a monolingual corpus in the target language, and an MT system trained to translate from the target language to the source language, the synthetic data is generated by translating the monolingual corpus with the MT system. After that, the synthetic data is used as the source part of the corpus, and the monolingual data as the target part. Finally, this new corpus is mixed with the available training data in order to train a new MT system. In this work, to generate the synthetic data, we translate the monolingual data using an ad-hoc SMT system trained with the corpus' training partition. Additionally, since the datasets are considerable small, prior to mixing the synthetic corpus with the training partition, we replicate several times the training data in order to match the size of the synthetic data and avoid overfitting  [23] . Finally, we trained an NMT system with this new corpus. 

 Experimental Framework In this section, we present the corpora and metrics, and describe the MT systems used during the experimental session. 

 Corpora The first corpus used to assess our proposal was the Dutch Bible  [9] . This corpus consists in a collection of different versions of the Dutch Bible. More precisely, a version from 1637, another from 1657, another from 1888 and another from 2010. All versions contain the same text except for the 2010 version, which is missing the last books. Moreover, the authors mentioned that the translation from this last version is not very reliable. Additionally, due to Dutch not evolving significantly during this period, 1637 and 1657 versions are fairly similar. For this reason, we decided to only use the 1637 version-considering this as the original document-and the 1888 version-considering 19 th century Dutch as modern Dutch. To create the synthetic corpus (see Section 2.3), we collected all 19 th century Dutch books available at the Digitale Bibliotheek voor de Nederlandse letteren  1  and used them as monolingual data. The second corpus we used was El Quijote. We built this corpus using a version  [24]  of the original 17 th century Spanish novel by Miguel de Cervantes, and a 21 st century version modernized by Andr?s Trapiello  [25] . The first step was to split each document into sentences. Since the 17 th century version was faithful to the original manuscript (in which each document line is formed by a very few words), we replaced line breaks by spaces to create a single sentence, and removed empty lines. For consistency, we did the same to the 21 st century version. After that, we split each document into sentences by adding line breaks to relevant punctuation (i.e., dots, quotation marks, admiration marks, etc). Then, to ensure consistency, we checked special symbols (e.g., quotation marks) and made sure that the same character was used in both versions. Finally, in order to create a parallel corpus, we aligned both documents using Hunalign  [26] . Since the content of this corpus was a novel, we decided that, to create the synthetic corpus, it would be best to use monolingual data coming from Spanish literature. For this reason and, considering that Spanish hasn't changed significantly over the last decades, we decided to collect free-of-right late 20 th century Spanish novels from Project Gutenberg 2 . Finally, as a third corpus, we selected El Conde Lucanor. We built this corpus using a version of the original 14 th century Spanish novel by Don Juan Manuel, and a 21 st century version modernized by Luis L?pez Nieves  [27] . To create the parallel version, we followed the same steps than with El Quijote. However, unlike with El Quijote, the resulting corpus was too small to be able to use for training an MT system. Therefore, we decided to use it only as a test. Unable to find a suitable training corpus, we decided to test El Conde Lucanor using the systems created for El Quijote-despite the fact that the original documents were written three centuries apart from one another. Table  1  presents the corpora statistics. 

 Metrics In order to asses our proposal, we made use of the following well-known metrics: ? BiLingual Evaluation Understudy (BLEU)  [28] : computes the geometric average of the modified n-gram precision, multiplied by a brevity factor that penalizes short sentences. ? Translation Error Rate (TER)  [29] : computes the number of word edit operations (insertion, substitution, deletion and swapping), normalized by the number of words in the final translation. Confidence intervals (p = 0.05) were computed for all metrics by means of bootstrap resampling  [30] . 

 MT Systems We trained the SMT systems with Moses  [31] , following the standard procedure: we estimated a 5-gram language model-smoothed with the improved KneserNey method-using SRILM  [32] , and optimized optimized the weights of the log-lineal model with MERT  [33] . Additionally, we lowercased and tokenized the corpora using the standard scripts and, later, truecased the translated text using Moses' truecaser. To train the NMT systems, we used OpenNMT  [34] . We used LSTM units, following the findings from  [35] . We set the size of the word embedding and LSTM units to 1024. We used Adam  [36]  with a learning rate of 0.0002  [37] . The beam size was set to 6. Finally, the corpora were lowercased and tokenized-and, later, truecased and detokenized-using OpenNMT's tools. In order to reduce the vocabulary, we applied Byte Pair Encoding (BPE)  [38]  to both SMT and NMT systems. We trained the models with a joint vocabulary of 32000 BPE units. 

 Results In this section, we present and discuss the results of the experiments conducted in order to assess our proposal. Table  2  presents the experimental results. Dutch Bible contained an additional baseline which consisted in generating a modernized version of the text by applying a set of rules to the original document  [9] . This second baseline improved significantly (close to 40 BLEU points and 30 TER points) the standard baseline of considering the original document as the modernized version. However, the SMT approach improved those results even more (near 30 BLEU points and 15 TER points of improvement with respect to the second baseline, and 70 BLEU points and 50 TER points with respect to the standard baseline). The NMT approach yielded better results than the standard baseline (and improvement of around 25 BLEU points and 5 TER points), but worse results than the second baseline and the SMT approach. Most likely, this is due to the training corpus being too small, which is a well-known problem in NMT. Finally, the backtranslation approach yielded the worst results. These results represent an improvement over the standard baseline in term of BLEU (around 4 points), and a deterioration in terms of TER (around 8 points). Most likely, this is due to the monolingual data used for backtranslation not being similar enough to the training data. The experiments using El Quijote behaved similarly-taking into account that the only available baseline is the standard one-to Dutch Bible: The SMT approached yielded the best results (an improvement of Table  2 : Experimental results. Baseline system corresponds to considering the original document as the modernized version. Baseline 2 came with the Dutch Bible and is a modernized version of the text generated by applying a set of rules to the original document  [9] . SMT and NMT are the SMT and NMT approaches respectively. NMT Synthetic is the NMT system trained with the synthetic data generated through backtranslation. Best results are denoted in bold. close to 22 BLEU points and 14 TER points). The results yielded by the NMT approach were not significantly different to the baseline in terms of BLEU, and represented close to a 10 points deterioration in terms of TER. In this case, however, the backtranslation approach yielded nearly a 10 points improvement in terms of BLEU, and the same TER results as the NMT approach. Not being able to obtain enough suitable training data for El Conde Lucanor, we used the same systems than for El Quijote. However, these documents were written three centuries apart from one another (El Conde Lucanor is written in 14 th century Spanish and El Quijote in 17 th century Spanish). Therefore, the obtained results contained a low translation quality. Nonetheless, it is worth noting that the SMT approach yielded improvements over the baseline (around 3 BLEU points and 6 TER points). However, the NMT and backtranslating approached yielded a deterioration of 3 BLEU points (in both cases) and 10 and 75 TER points respectively. In general, SMT yielded the best results in all cases. NMT was able to improve Dutch Bible's baseline, yielding similar results to El Quijote's baseline and worse results than El Conde Lucanor's baseline. Finally, despite being successfully used in resources-poor scenarios, backtranslation was only able to improve results for the experiment using El Quijote, and these results were worse than the ones yielded by the SMT approach. 

 Qualitative Analysis Table  2  shows some examples of sentences modernized using the different MT approaches. The first example is a sentence from El Quijote. The hypothesis generated by the SMT approach is very closed to the reference. The main differences are a change in the order of actions (the original sentence says Y dejando de comer, se levant?, which is changed by the hypothesis into Y levant?ndose, dej? de comer) and some changes in the conjugation of verbs (e.g., dejando is changed into dej?). However, the main goal of modernization is not to generate a perfect equivalent version, but to make the document easier to comprehend-making the overall meaning more important than the exact choice of words. While sentences like these are penalized by the automatic metrics, they accomplish modernization's goal. The hypothesis generated by the NMT approach follows the same structure than the SMT hypothesis (it makes the same reordering and conjugation changes) but contains non-existent words (e.g., ancen) and has some errors (e.g, a los pies in stead of puesto a caballo). Therefore, some parts are easier to comprehend than in the original version, but the meaning of the sentence is not clear. Finally, the hypothesis generated by the backpropagation approach is almost the same as the one generated by the SMT approach (the only change is fierded in stead of fiereza). While this hypothesis is less correct than the SMT one, they are both equally easy to comprehend. The second example is from El Conde Lucanor, whose experiments were conducted using the systems trained with El Quijote. As a result, all the hypothesis are hard to comprehend. While the automatic metrics heavily penalize the backtranslation approach, in this case, is the one which is closer to modern Spanish. Moreover, it is the only hypothesis which preserves the name of the main characters (Lucanor and Patronio). Looking through the whole texts, the SMT and NMT hypothesis frequently changed the characters named into non-existent words, while the backtranslation approach rarely modified them. Finally, having trained the systems with El Quijote has a visible effect in the SMT and El Quijote Original: Y, leuantandose, dex? de comer, y fue a quitar la cubierta de la primera imagen, que mostro ser la de San Iorge puesto a cauallo, con vna serpiente enroscada a los pies, y la lanc ?a atrauessada por la boca, con la fierec ?a que suele pintarse. Modernized: Y dejando de comer, se levant? y fue a quitar la cubierta de la primera imagen, que result? ser la de san Jorge a caballo, con una serpiente enroscada a los pies y la lanza atraves?ndole la boca, con la fiereza que suele pintarse. SMT: Y levant?ndose, dej? de comer, y fue a quitar la cubierta de la primera imagen, que mostr? ser la de San Jorge puesto a caballo, con una serpiente enroscada a los pies, y la lanza atravesada por la boca, con la fiereza que suele pintarse. NMT: Y levant?ndose, dej? de comer, y fue a quitar la cubierta de la primera ancen, que mostr? ser la de San Marorge a los pies y la lanza ahabesada por la boca; NMT Synthetic : Y levant?ndose, dej? de comer, y fue a quitar la cubierta de la primera imagen, que mostr? ser la de San Jorge puesto a caballo, con una serpiente enroscada a los pies, y la lanza atravesada por la boca, con la fierded que suele pintarse. El Conde Lucanor Original: -Se?or conde Lucanor -dixo Patronio-, vien entiendo que el m?o consejo non vos faze grant mengua, pero vuestra voluntad es que vos diga lo que en esto entiendo, et vos conseje sobre ello, fazerlo he luego. Modernized: -Se?or Conde Lucanor -dijo Patronio-, bien s? que mi consejo no os hace mucha falta, pero, como confi?is en m?, SMT: --Se?or conde Lucanor -dijo Patroniorosa, vien entiendo que el m?o consejo non vos face grant mengua, pero vuestra voluntad es que vos diga lo que en esto entiendo, et vos aconseje en ello, ferlo he luego . NMT: Se?or conde Olcanor dijo dijo Pacasos -dijo en entiendo que el m?o consejo non os fazo felimengua y vuestra merced es que vos diga lo que en esto entiendo. NMT Synthetic : -Se?or conde Lucanor -dijo Patronio, vien entiendo que el m?o consejo non es face grant mengua, pero vuestra voluntad es que vos diga lo que en esto entiendo, et vos conseje sobre ello, tambi?n yo he dicho. Table  3 : Examples of modernizing a sentence using the different MT approaches. SMT and NMT are the SMT and NMT approaches respectively. NMT Synthetic is the NMT system trained with the synthetic data generated through backtranslation. NMT approaches (with verbs conjugations such as ferlo, or expression such as vuestra merced). This effect, however, is not so visible in the backtranslation hypothesis. All in all, neither hypothesis accomplishes the goal of improving the comprehension of the original sentence. Nonetheless, it is worth noting that this was a tricky example since the original sentence is modernized into a much shorter sentence (which reflects that 14 th century Spanish used longer expressions than modern Spanish). 

 Conclusions and Future Work In this work we proposed several machine translation approaches to modernize historical documents in order to break the language barrier and increase their accessibility to a broader audience. We tested our approaches using three historical datasets (two of which were created for this work) from three different time periods and two different languages. Our first approach was based in SMT and yielded, for all cases, the best results. With the exception of the dataset for which there were not available any suitable training data, this approach yielded significant improvements of around 22 to 67 BLEU points and 14 to 48 TER points. Since the available training data was fairly small, the approach based on NMT produced less satisfactory results. While it was able to yield improvements for one dataset, the rest of the experiment resulted in either not significantly different than the baseline, or yielding a deterioration in terms of translation quality. Finally, despite being successfully used in resourcespoor scenarios, backtranslation was only able to improve results for one dataset, and only in terms of BLEU. Our best hypothesis is that historical documents are very language-specific and, therefore, choosing the monolingual corpus to use for creating the synthetic data is extremely important. While we tried to create the monolingual datasets using similar topics, the corpora's topics were too specific: religious texts, a cavalry novel and medieval tales. In a future work, we would like to research the relation between the domains of the monolingual and training corpora deeper. Additionally, we want to explore the use of data selection techniques for constructing the monolingual corpus to use for backtranslation, and to create a training partition for cases in which we do not have suitable training data available (as was the case with El Conde Lucanor). Figure 1 : 1 Figure 1: Example of modernizing a historical document. The original text is Shakespeare Sonnet 18. The modernized version of the Sonnet was obtained from [8]. 

 Table 1 : 1 Corpora statistics. |S| stands for number of sentences, |T | for number of tokens and |V | for size of the vocabulary. Monolingual refers to the monolingual data used to create the synthetic data. M denotes million and K thousand. Dutch Bible El Quijote El Conde Lucanor Train Development Test Monolingual 35.2K |T | 870.4/862.4K 283.3/283.2K 10K |S| 53.8/42.8K 31.7/31.3K |V | 2000 2000 |S| 56.4/54.8K 53.2/53.2K |T | 9.1/7.8K 10.7/10.6K |V | 5000 2000 |S| 41.8/42.0K |T | 145.8/140.8K 10.5/9.0K 8.9/9.0K |V | 4.1M 567.0K |S| 88.3M 9.5M |T | |V | 2.0M 470.4K ------2252 62.0/56.7K 7.4/8.6K --- 

 NMT Synthetic 17.4 ? 0.5 65.6 ? 1.7 45.2 ? 1.3 50.6 ? 3.5 3.1 ? 0.2 165.1 ? 8.2 System Dutch Bible BLEU TER El Quijote BLEU TER El Conde Lucanor BLEU TER Baseline Baseline 2 SMT NMT 13.5 ? 0.3 57.0 ? 0.3 36.5 ? 0.8 43.3 ? 1.1 5.8 ? 0.3 89.6 ? 1.0 ----50.8 ? 0.4 26.5 ? 0.3 80.1 ? 0.5 9.9 ? 0.3 58.9 ? 1.0 29.4 ? 1.2 8.4 ? 0.3 83.8 ? 1.0 38.0 ? 0.6 51.7 ? 2.2 37.4 ? 1.2 51.5 ? 2.0 2.7 ? 0.2 99.5 ? 2.0 

			 Proceedings of the 15 th International Workshop on Spoken Language Translation Bruges, Belgium, October 29-30, 2018 

			 http://dbnl.nl/ 2 https://www.gutenberg.org/ 

			 Proceedings of the 15 th International Workshop on Language Translation Bruges, Belgium, October 29-30, 2018
