title
Infosys Machine Translation System for WMT20 Similar Language Translation Task

abstract
This paper describes Infosys' submission to the WMT20 Similar Language Translation shared task. We participated in Indo-Aryan language pair in the language direction Hindi to Marathi. Our baseline system is bytepair encoding based transformer model trained with the fairseq sequence modeling toolkit. Our final system is an ensemble of two transformer models, which ranked first in the WMT20 evaluation. One model is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source and target languages are the same alphabet languages. The other model is the result of experimentation with the proportion of back-translated data to the parallel data to improve translation fluency.

Introduction Neural Machine Translation  (Bahdanau et al., 2015; Vaswani et al., 2017)  is the most popular approach for machine translation. Transformerbased NMT has outperformed many recurrent neural network based models. There is scope for improvement in NMT, particularly for low-resource language pairs. Our techniques are experimented on the fairseq sequence modeling toolkit  (Ott et al., 2019)  for NMT. Our system is an ensemble of two transformer-based models. One designed for lowresource language pairs by taking advantage that both are same alphabet languages. The other model is built after experimenting on renowned backtranslation technique  (Sennrich et al., 2016a ) by exploiting target monolingual data. 

 Data Hindi-Marathi bitext data contains ?49K sentence pairs. Target monolingual data comprises of 326K Newscrawl sentences and 10,839K raw sentences. 

 Data Preprocessing Typical training sentence pairs comprises of a source and a target sentence. There are ?1K training sentence pairs where source or target contains multiple sentences delimited by '/'. Matching pair for these sentences is derived based on the proximity of token lengths between source and target sentence. Non-printable characters are removed, punctuations are normalized, and the data is tokenized, with the Moses tokenizer. Byte-pair encoding (BPE) has been adopted  (Sennrich et al., 2016a)  to build source and target sub-word vocabularies of size 22.5K and 32.8K respectively, when configured to construct with 60K symbols. 

 Data filtering 

 Bitext data Sentences with more than 175 words, sentences with no words, and sentence pairs exceeding length ratio of 1.5 are removed from training data. This eliminated around 18% of the overall real bitext data. 

 Synthetic data CommonCrawl n-grams raw monolingual files are processed 1 to remove sentences with invalid characters, strip leading and trailing whitespaces, and remove duplicate sentences. 

 System Overview Our Hindi-Marathi primary system is an ensemble of two transformer models. One is backtranslated model and the other model is trained on anonymized data. 

 Base Model Architecture and Hyperparameters Our model is built using fairseq 2  (Ott et al., 2019)  toolkit. The Transformer, an encoder-decoder architecture  (Vaswani et al., 2017)    2019 ). Threshold frequency is set such that only tokens occurring at least 10 times in the training data will be part of the vocabulary. Maximum tokens per GPU is set to 4000 and the batch size multiplier is fixated to 4 to set an effective batch size of 16000 tokens with dropout probability of 0.1. This led to improved performance. Reported detokenized test BLEU is 14.13 and hence these settings are adapted. 

 Backtranslation Back-translation is a popularly adapted data augmentation technique which aids in building better NMT systems, especially for low resource language pairs by leveraging monolingual corpora  (Sennrich et al., 2016a ). An intermediate system is first trained on parallel data which is used to translate target monolingual data into source language. Sampling is used as a method for inference  (Edunov et al., 2018) . Synthetic parallel data is constructed from the intermediate system generated synthetic source while the target is the provided monolingual data. The Bitext data filters are also applied to synthetic data but only removed sentences with more than 250 words. New training data is constructed by appending this synthetic parallel data to real bitext data and a final system that will translate from the source to the target language will be trained.   2016 ), also chose 1-to-1 ratio of real to synthetic parallel data for English-Russian news translation task. It is also known from past experiments that increasing the ratio of synthetic training data erratically, degrades system performance, depending on quality and domain of synthetic data  (Sennrich et al., 2016a; Currey et al., 2017; Poncelas et al., 2018) . In contrast, experiments conducted by  Stahlberg et al. (2018) , shows that performance of system does not reduce as long as the ratio of real parallel to synthetic parallel data does not exceed 1-to-8 (1.6M out of 3M Turkish monolingual data is preferred for training along with 0.2M of parallel corpus for English-Turkish).  Fadaee and Monz (2018) , claims, 1-to-5 real to synthetic parallel data ratio achieved best performance in news translation task for German-English with 4.5M parallel corpus. This limits from taking advantage of all available monolingual corpus. Only a small portion of it can be used as synthetic parallel training data. Oversampling  (Chu et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2018)  real parallel data can overcome this problem. By oversampling primary parallel data equivalent to the synthetic parallel data from all monolingual data, effective 1-to-1 ratio of bitext and synthetic parallel data can be retained. Experiment 1-to-1 ratio of bitext to synthetic data is chosen after experimentation with ratios (see Table  1 , Figure  1 ). It is crucial to find the ideal proportion of synthetic data to use. Utilization of all available out-ofdomain and raw monolingual corpora to the maximum effect can be further explored.  

 Ratio BLEU 

 Out-of-domain data Handling OOV BPE is applied on monolingual target data using byte pairs learnt during bitext BPE operation. Out-of-vocabulary (OOV) tokens in BPE applied monolingual target data are the tokens not in bitext vocabulary. These out-of-vocabulary tokens are replaced by a special symbol UNK in the monolingual target data (see  Experiment Since the intermediate model spot UNK symbol in the inputs during inferencing, inferenced data also contains UNK symbol.  Gulcehre et al. (2015) , claims to eliminate monolingual sentences with more than 10% UNK symbols for better performance.  Sennrich et al. (2016b) , claims to handle rare/unseen words by representing it in a sequence of sub-word units using existing vocabulary that was learnt on the parallel data. Our systems are experimented by excluding sentences with UNK symbol. Systems are trained with different proportions of real to synthetic data by eliminating all sentence pairs containing UNK in training data. Table  3  shows the study of model performance before and after removing sentence pairs containing UNK. 1-to-1 proportion of real and synthetic data with out-of-vocabulary tokens masked by UNK symbol scored best (18.76) out of all outcomes. Ratio All Data Data without UNK 1:0.5 18.08 17.73 1:1.0 18.76 18.34 1:5.0 14.49 16.60 Table  3 : BLEU scores on models with and without removing sentences containing UNK 

 Anonymization Analysis of the results of the model achieved 18.76 BLEU score, reveals that the translation accuracy is negatively impacted when UNK is generated. This is handled by building another model with bitext data only, where the similarity between source and target languages are anonymized by masking. This approach enables the model to specifically focus on learning the nuances of translation only (i.e.., enables the model to focus on the specific section in the source sentence that gets altered during translation). Language pair comprising same alphabetic languages contains same words between them carrying similar meaning. Numbers, names, geographic names, etc., also holds same script. i.e. tokens that are not language specific. The approach here is to anonymize those words which are equally present in source and target sentences. One special character is used to mask all those tokens. The special character is chosen in place of a special word to eliminate the possibility of splitting the special word during sub-word tokenization. This approach reduces the vocabulary size and the learning parameters of the model, preserving the context. This results in transforming sentences which appeared to be different in its raw form into duplicate sentences in its anonymized form, which are then deduplicated. Hi-Mr track with ?49K training sentences without masking technique generated source and target vocabulary of size 22.5K and 32.8K respectively. Anonymization reduced source and target vocabulary size to 20.9K and 31.0K respectively. This approach resulted in improvement of BLEU score by 1.2 over baseline. The impact of this approach is proportional to the similarity of source and target languages. The key observation is that this model performed better at translation of sentences that are translated poorly (with UNK tokens) by back-translation model. 

 Stacking Benefits of both the masking systems (masking OOV tokens with UNK symbol and masking similar tokens) are attained through stacking. Model trained on anonymized parallel data and the model trained on real bitext plus synthetic parallel data are ensembled to achieve 19.76 BLEU with Dev data. 

 Post-processing The anonymized words are preserved before inferencing and the inference results are decoded by replacing the special symbols with the preserved anonymized tokens followed by BPE detokenization. 

 Results Our novel anonymization technique improved BLEU by 1.2. Optimal proportion of backtranslated data improved BLEU by 3.5. Ensembling best systems improved BLEU by 1.0. (See  

 Conclusion This paper describes the techniques involved in our system submitted for the WMT20 Similar Language Translation task by Infosys. This winning Hindi-Marathi translation system is built based on NMT and evaluated based on the metric, BLEU. The domain-based data preprocessing and filtering techniques eases model learning. Adopting novel approach of anonymizing language agnostic tokens aided our system to focus more on tokens that matters in the translation. It is highly observed that the ratio of monolingual data used against bitext data plays a vital role in back-translated models. Improving translation accuracy and language fluency by utilizing all available out-of-domain monolingual corpora to the maximum effect can be further explored. 2 https://github.com/pytorch/fairseq 3.2.1 Bitext and Synthetic corpora proportion Related Work Real to synthetic parallel data close to 1-to-1 proportion works best for Sennrich et al. (2016a). Junczys-Dowmunt et al. ( 
