title
Towards Multimodal Simultaneous Neural Machine Translation

abstract
Simultaneous translation involves translating a sentence before the speaker's utterance is completed in order to realize real-time understanding in multiple languages. This task is significantly more challenging than the general full sentence translation because of the shortage of input information during decoding. To alleviate this shortage, we propose multimodal simultaneous neural machine translation (MSNMT), which leverages visual information as an additional modality. Our experiments with the Multi30k dataset showed that MSNMT significantly outperforms its textonly counterpart in more timely translation situations with low latency. Furthermore, we verified the importance of visual information during decoding by performing an adversarial evaluation of MSNMT, where we studied how models behaved with incongruent input modality and analyzed the effect of different word order between source and target languages.

Introduction Simultaneous translation is a natural language processing (NLP) task in which translation begins before receiving the whole source sentence. It is widely used in international summits and conferences where real-time comprehension is one of the essential aspects. Simultaneous translation is already a difficult task for human interpreters because the message must be understood and translated while the input sentence is still incomplete, especially for language pairs with different word orders (e.g. SVO-SOV)  (Seeber, 2015) . Consequently, simultaneous translation is more challenging for machines. Previous works attempt to solve this task by predicting the sentence-final verb  (Grissom II et al., 2014) , or predicting unseen syntactic constituents  (Oda et al., 2015) . Given the difficulty * These authors contributed equally to this paper of predicting future inputs based on existing limited inputs,  Ma et al. (2019)  proposed a simple simultaneous neural machine translation (SNMT) approach wait-k which generates the target sentence concurrently with the source sentence, but always k tokens behind, satisfying low latency requirements. However, previous approaches solve the given task by solely using the text modality, which may be insufficient to produce a reliable translation. Simultaneous interpreters often consider various additional information sources such as visual clues or acoustic data while translating  (Seeber, 2015) . Therefore, we hypothesize that using supplementary information, such as visual clues, can also be beneficial for simultaneous machine translation. To this end, we propose Multimodal Simultaneous Neural Machine Translation (MSNMT) that supplements the incomplete textual modality with visual information, in the form of an image. It will predict still missing information to improve translation quality during the decoding process. Our approach can be applied in various situations where visual information is related to the content of speech such as presentations with slides (e.g. TED Talks 1 ) and news video broadcasts 2 . Our experiments show that the proposed MSNMT method achieves higher translation accuracy than the SNMT model that does not use images by leveraging image information. To the best of our knowledge, we are the first to propose the incorporation of visual information to solve the problem of incomplete text information in SNMT. The main contributions of our research are as follows. We propose to combine multimodal and simultaneous NMT, therefore, discovering cases where such multimodal signals are beneficial for the end-task. Our MSNMT approach brings significant improvement in simultaneous translation quality by enriching incomplete text input information using visual clues. As a result of a thorough analysis, we conclude that the proposed method is able to predict tokens that have not appeared yet for source-target language pairs with different word order (e.g. English?Japanese). By providing an adversarial evaluation, we showed that the models indeed utilize visual information. 

 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation  (Ma et al., 2019) . SNMT can be realized with two types of policy: fixed and adaptive policies  (Zheng et al., 2019b) . Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb  (Grissom II et al., 2014; Matsubara et al., 2000)  and unseen syntactic constituents  (Oda et al., 2015) . Most dynamic models with adaptive policies  (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a Zheng et al., ,c, 2020  have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile,  Ma et al. (2019)  proposed a simple wait-k method with fixed policy, which generates the target sentence only from the source sentence that is delayed by k tokens. However, their model for simultaneous translation relies only on the source sentence. In this research, we concentrate on the wait-k approach with fixed policy, so that the amount of input textual context can be controlled to analyze better whether multimodality is effective in SNMT. Multimodal NMT (MNMT) for full-sentence machine translation has been developed to enrich text modality by using visual information  (Hitschler et al., 2016; Elliott and K?d?r, 2017) . While the improvement brought by visual features is moderate, their usefulness is proven by  Caglayan et al. (2019) . They showed that MNMT models are able to capture visual clues under limited textual context, where source sentences are synthetically degraded by color deprivation, entity masking, and progressive masking. However, they use an artificial set-ting where they deliberately deprive the models of source-side textual context by masking. However, our research has discovered an actual end-task and has shown the effectiveness of using multimodal data for it. Compared with the entity masking experiments  (Caglayan et al., 2019) , where they use a model exposed to only k words, our model starts by waiting for the first k source words and then generates each target word after receiving every new source token, eventually seeing all input text. In MNMT, visual features are incorporated into standard machine translation in many ways. Doubly-attentive models are used to capture the textual and visual context vectors independently and then combine these context vectors in a concatenation manner  (Calixto et al., 2017)  or hierarchical manner  (Libovick? and Helcl, 2017) . Some studies use visual features in a multitask learning scenario  (Elliott and K?d?r, 2017; Zhou et al., 2018) . Also, recent work on MNMT has partly addressed lexical ambiguity by using visual information  Lala and Specia, 2018; Gella et al., 2019)  showing that using textual context with visual features outperform unimodal models. In our study, visual features are extracted using image processing techniques and then integrated into an SNMT model as additional information, which is supposed to be useful to predict missing words in a simultaneous translation scenario. To the best of our knowledge, this is the first work that incorporates external knowledge into an SNMT model. 

 Multimodal Simultaneous Neural Machine Translation Architecture Our main goal is to investigate if image information would bring improvement on SNMT. As a result, two tasks could benefit from each other by combining them. In this section, we describe our MSNMT model, which is composed by combining an SNMT framework wait-k  (Ma et al., 2019 ) and a multimodal model  (Libovick? and Helcl, 2017) . We base our model on the RNN architecture, which is widely used in MNMT research  (Libovick? and Helcl, 2017; Caglayan et al., 2017a; Elliott and K?d?r, 2017; Zhou et al., 2018; Hirasawa et al., 2019) . The model takes a sentence and its corresponding image as inputs. The decoder of the MSNMT model outputs the target language sentence in a simultaneous and multimodal manner by attaching attention not only to the source sentence but also to the image related to the source sentence. 3 

 Simultaneous Translation We first briefly review standard NMT to set up the notations. The encoder of standard NMT model always takes the whole input sequence X = (x 1 , ..., x n ) of length n where each x i is a word embedding and produces source hidden states H = (h 1 , ..., h n ). The decoder predicts the next output token y t using H and previously generated tokens, denoted Y <t = (y 1 , ..., y t?1 ). The final output is calculated using the following equation: p(Y|X) = |Y| t=1 p(y t |X, y <t ) (1) Different from standard neural translation, in which each y i is predicted using the entire source sentence X, the simultaneous translation requires the model to translate concurrently with the growing source sentence. We incorporate the wait-k approach  (Ma et al., 2019)  for our simultaneous translation model. Instead of waiting for the whole sentence before translating, this model waits for only the first k tokens and starts to generate each target tokens after taking every new source token one by one. It stops taking new input tokens once the whole input sentence is on board. For example, if k = 3, the first target token is predicted using the first 3 source tokens, and the second target token using the first 4 source tokens. The wait-k decoding probability p wait-k is: p wait-k (Y|X) = |Y| t=1 p(y t |X ?g(t) , y <t ) (2) where g(t) is the wait-k policy function which decides how much input text to read and translate, X ?g(t) = (x 1 , ..., x g(t) ) and g(t) is 0 ? t ? n. g(t) is defined as follows: g(t) = min{k + t ? 1, n} (3) When k + t ? 1 is over source length n, g(t) is fixed to n, which means the remaining target tokens (including current step) are generated using the full source sentence. For full sentence translation, g(t) is constant g(t) = n. 

 Multimodal Translation We use a hierarchical attention combination technique  (Libovick? and Helcl, 2017)  to incorporate visual and textual features into an MNMT model. This model calculates the independent context vectors from the textual features h txt = (h txt 1 , ..., h txt n ) and the visual features h img = (h img 1 , ..., h img m ), which are extracted by the textual encoder and the image processing model, respectively. It then combines the resulting two vectors using a second attention mechanism, which helps to perform simultaneous translation taking into account visual information. Specifically, we compute the context vectors c f i for each image (f = img) and text (f = txt) modality independently using the following equations: e f i,j = ? f (s i , h f j ) (4) ? f i,j = exp(e f i,j ) |h f | l=1 exp(e f i,l ) (5) c f i = |h f | j=1 ? f i,j h f j (6) where ? f is a feedforward network for each modality f; s i is i-th decoder hidden state. We project these image and text context vectors into a common space and compute another distribution over the projected context vectors and their corresponding weighted average using the second attention: ?f i = ?(s i , c f i ) (7) ? f i = exp(? f i ) r?{img,txt} exp(? r i ) (8) ci = r?{img,txt} ? r i W r c r i ( 9 ) where ? is a feedforward network. Equation 8 calculates the second attention to combine the image and text vectors. W r is a weight matrix used to compute the context vector ci calculated from image and text features. The final hypothesis Y has the probability: p mnmt (Y|X, Z) = |Y| t=1 p(y t |X, Z, y <t ) (10) where Z represents input image features. 

 Multimodal Simultaneous Neural Machine Translation In this subsection, we describe the structure of the MSNMT model, which is a combination of the models described in Sections 3.1 and 3.2. The method for calculating the image context vector is the same as for MNMT; however, the text context vector (Equation  6 ) for the t-th step is calculated as follows: ?txt i = g(t) j=1 ? txt i,j h txt j (11) Thus ?txt i is calculated from the input text prefix determined by wait-k policy function g(t). Then we apply the second attention to ?txt i and c img i in order to calculate ci (Equation  9 ). The decoding probability becomes as follows: p msnmt (Y|X, Z) = |Y| t=1 p(y t |X ?g(t) , Z, y <t ) (12) 4 Experimental Setup 

 Dataset We experiment with our model in four translation directions consisting of 5 languages: English (En), German (De), French (Fr), Czech (Cs), and Japanese (Ja). All language pairs include En on the source side. We used the train, development, and test sets from the Multi30k  dataset published in the WMT16 Shared Task, which is a benchmark dataset generally used in MNMT research  (Libovick? and Helcl, 2017; Caglayan et al., 2019; Elliott and K?d?r, 2017; Zhou et al., 2018; Hirasawa et al., 2019)  for En?De, En?Fr and En?Cs. Nakayama et al. (  2020 ) released F30kEnt-JP dataset 4 which contains Japanese translations of first two original English captions for each image of the Flickr30k Entities dataset  (Plummer et al., 2017) . They follow the same annotation rules as the Flickr30k Entities dataset using exactly the same tags with entity types and IDs. We preprocessed this data as follows: 1) The parallel En?Ja data was created by taking alignment using corresponding IDs assigned to each Japanese translation entity with the IDs of Flickr30k entities.  5  2) The created parallel data was aligned with its corresponding images using text files named (image id).txt corresponding to each image in Flickr30k. 3) Finally, the created multimodal data was split to train, dev, and test following data splits of Multi30k using the same Multi30k image IDs. Note that the English side of En?Ja parallel data extracted from F30kEnt-JP and English side of Multi30k data are thought to be somewhat comparable but not strictly the same while their corresponding images are the same. Data split for all language pairs were as follows: training set, 29,000 sentence pairs, development set, 1,014 sentence pairs, and 1,000 sentence pairs for the test set. This dataset's average sentence length is 12-13 tokens for En, De, Fr, Cs and 20 tokens for Ja. We limit the vocabulary size of the source and the target languages after concatenating them to 10,000 sub-words  (Sennrich et al., 2016) . All sentences are preprocessed with lower-casing, tokenizing, and normalizing the punctuation using the Moses script 6 . To tokenize Japanese sentences, we used MeCab 7 with the IPA dictionary. Visual features are extracted using pre-trained ResNet  (He et al., 2016) . Technically, we encode all images in Multi30k with ResNet-50 and pick out the hidden state in the pool5 layer as a 2,048dimension visual feature. 

 Systems We compare the following models: 1. SNMT: We use only text modality for training data as a baseline for each wait-k model. 2. MSNMT: We use image modality along with text modality for a training data for each wait-k model. To train the above models, we utilize attention NMT  (Bahdanau et al., 2015)  with a 2-layer unidirectional GRU encoder and a 2-layer conditional GRU decoder. We use the open-source implementation of the nmtpytorch toolkit v3.0.0  (Caglayan et al., 2017b) . We first pre-train the MSNMT model for each k until convergence using only text data and use zeros for visual features.  In order to keep our experiments as pure as possible, we will not use additional data or other types of models. It will allow us to control the amount of input textual context, so we can easily analyze the relationship between the amount of textual and visual information. 

 Hyperparameters We use the same hyperparameters for SNMT and MSNMT for a fair comparison as follows. All models have word embeddings of 200 and recurrent layers of dimensionality 400 units with 2way sharing of embeddings in the network. We used Adam  (Kingma and Ba, 2015)  with a learning rate of 0.0004. Decoders were initialized with zeros. We used a minibatch size of 64 for training and 32 for fine-tuning. Rates of dropout applied on source embeddings, source encoder states and pre-softmax activations were 0.4, 0.5, and 0.5, respectively. We set the max length of the input to 100. wait-k experiments were conducted for 1, 3, 5, 7, and Full settings. For MSNMT only hyperparameters, the sampler type was set to approximate, and channels were set to 2048. The fusion type was set to hierarchical mode. 

 Evaluation We report BLEU scores calculated using Moses' multi-bleu.perl, which is a widely used evalu-   Ma et al. (2019)  to evaluate the latency for SNMT and MSNMT systems. 9 It calculates the degree of out of sync time with the input, in terms of the number of source tokens as follows: AL g (X, Y) = 1 ? g (|X|) ?g(|X|) t=1 g(t) ? t ? 1 r ( 13 ) where r = |Y|/|X| is the target-to-source length ratio and ? g is the decoding step when source sentence finishes: ? g (|X|) = min{t|g(t) = |X|} (14) 

 Results Table  1  illustrates the BLEU scores of MSNMT and SNMT models on the test set. MSNMT systems show significant improvements over SNMT systems for all language pairs when input textual information is limited. Note that the difference of BLEU scores between MSNMT and SNMT becomes larger as the k gets smaller, especially when the target language is distant from English in terms of word order (e.g. Cs and Ja). On the other hand, the availability of more tokens during the decoding process (k ? 5) leads to the text information becoming sufficient in some cases. Figure  1  shows translation quality against AL for four language directions. In all these figures, we observe that, as k increases, the gap between BLEU scores for MSNMT and SNMT decreases. We also observe that AL scores are better for MSNMT as k decreases. From these results, it can be seen that in terms of latency, the smaller k is, the more beneficial the visual clues become. 

 Analysis In this section, we provide a thorough analysis to further investigate the effect of visual data to produce a simultaneous translation by (a) providing adversarial evaluation; and (b) analyzing the impact of different word order for En?Ja language pair. 

 Adversarial Evaluation In order to determine whether MSNMT systems are aware of the visual context  (Elliott, 2018) , we perform the adversarial evaluation on the test set. We present our system with correct visual data with its source sentence (Congruent) as opposed to random visual data as an input (Incongruent)  (Elliott, 2018) .Therefore, we reversed the order of 1,000 images of the test set, so there will be no overlapping congruent visual data. Then we reconstruct image features for those images to use as an input. Results of image awareness experiments are shown in Table  2 . We can see the large difference in BLEU scores between MSNMT congruent (C columns) and incongruent (I columns) settings when k are small. This implies that our proposed model utilizes images for translation by learning to extract needed information from visual clues. The interesting part is for a full translation, where scores for the incongruent setting are very close to those of the congruent setting. The reason is that when textual information is enough, visual information becomes not that relevant in some cases. 

 How Source-Target Word Order Affects Translation In wait-k translations, for the En?Ja language pair with different word orders (SVO vs. SOV), some source tokens should be translated before they are presented to the decoder for grammaticality and fluency purposes. Hence, the model also needs to handle such cases well apart from the "usual" order. We hypothesized that MSNMT models, given additional visual information, are able to translate such cases better than SNMT models. Therefore, we investigated how many tokens were correctly translated that are not given as input yet. First, we quantitatively analyze how well we can translate entities that are not presented from the source yet but should exist in target sentences. To align the source and target entities, we use the entities' annotation attached to both the source and target sentences. Given that annotated entities have the same IDs and tags for both English and Japanese, we can align, calculate, and extract those entities from source and target sentences. If the index of the first token of the aligned target entity is not given as input at timestep k yet, we count them for each k scenario as # total entities (Table 4). For example, in Table  3  a wait-3 model should start translating after a token "rappelling" is presented to the model. And if an ID of the entity of "? (a body of water)" is in the target sentences but not in the inputted part yet, we count it as an entity that should be translated before being inputted to the model. Similarly, an entity of "? (cliff)" is already presented to the model at timestep 5, so we do not count those entities. If the same entity ID appears more than once in one sentence, we exclude those entities due to the impossibility of alignments. Finally, for each model during decoding, if those entities are included in the model's translation results with a perfect match from pre-calculated # total entities, we consider them as correctly translated.  10  Table  4  demonstrates the results. k column is to determine how many tokens a model waits before starting translating. Note that k=Full is not included because all entities are given at the time of translation. The reason that the total number of entities that were not inputted yet decreases when k increases (# total entities column) is that more entities are already available for the model for trans-lation. wait-k columns show how many entities were correctly translated by wait-k SNMT and MSNMT models from # total entities for each k scenario. Columns Full show upper-bounds of how many entities can be correctly translated if the models were trained with full sentences for entities from each k. Comparing Full results to wait-k for both SNMT and MSNMT shows that it is hard to correctly translate entities when k is small. Furthermore, comparing wait-k results of SNMT to MSNMT, it can be seen that the smaller value of k, the better MSNMT can handle different source-target word order than SNMT.  As an example, we sampled sentences and their images from the En?Ja test set (Figure  2 ) to compare the outputs of our systems. Table  5  lists their translations generated by SNMT (S) and MSNMT (M) models. In the first example, an SNMT model with wait-3 could not predict "? (sea, a body of water)" which appears at the end of the source sentence and generated an erroneous "? (rock)" which is not present neither in source text nor in a corresponding image. Contrarily, the MSNMT model with wait-3 was able to correctly predict "? (body of water)" even before it was inputted by capturing visual information. When a full sentence is given as an input, MSNMT translated it correctly using more information, unlike SNMT, which translated only from the given text and generated incorrect "? (climbing)" instead of "? (rappelling)". Interestingly, in the second example, the MSNMT model with wait-3 predicted "? (bicycles)" instead of "? (motorcycles)" at the beginning of the sentence, while the SNMT model with wait-3 was not able to generate any vehicle entities. Also, both MSNMT models with wait-3 and Full correctly captured that there were eight men, whilst both SNMT models incorrectly predicted about one and two men. From these results, we can conclude that visual clues pos-    Table  5 : Examples of En?Ja translations from test set using SNMT (S) and MSNMT (M) models (also refer to Figure  2 ). In () are shown their English meanings. The same colors indicate the same entity types. ? ? ? ? ? ? ? ? ? ? ? ? ? ? S wait-3 ? ? ? ? ? ? ? ? ? ? (someone climbs a cliff on a rock.) M wait-3 ? ? ? ? ? ? ? ? ? ? ? (a person is rappelling a cliff above the sea.) S Full ? ? ? ? ? ? ? ? ? ? ? (a itively impact generated translations where there is still a lack of textual information, especially when we deal with language pairs with different word order. 

 Conclusion In this paper, we proposed a multimodal simultaneous neural machine translation approach, which takes advantage of visual information as an additional modality to compensate for the shortage of input text information in the simultaneous neural machine translation. We showed that in a wait-k setting, our model significantly outperformed its text-only counterpart in situations where only a few input tokens are available to begin translation. We showed the importance of the visual information for simultaneous translation, especially in the low latency setup and for a language pair with word-order differences. We hope that our proposed method can be explored even further for various tasks and datasets. In this paper, we created a separate model for each value of wait-k. However, in future work, we plan to experiment on having a single model for all k values  (Zheng et al., 2019b) . Furthermore, we acknowledge the importance of investigating MSNMT effects on more realistic data (e.g. TED), where the utterance does not necessarily match a shown image while speaking and/or where its context can not be guessed from the shown image. Figure 1 : 1 Figure 1: Average Lagging scores. Results are the average of four runs. 
