title
Parallel FDA5 for Fast Deployment of Accurate Statistical Machine Translation Systems

abstract
We use parallel FDA5, an efficiently parameterized and optimized parallel implementation of feature decay algorithms for fast deployment of accurate statistical machine translation systems, taking only about half a day for each translation direction. We build Parallel FDA5 Moses SMT systems for all language pairs in the WMT14 translation task and obtain SMT performance close to the top Moses systems with an average of 3.49 BLEU points difference using significantly less resources for training and development.

Introduction Parallel FDA5 is developed for fast deployment of accurate statistical machine translation systems using an efficiently parameterized and optimized parallel implementation of feature decay algorithms  (Bic ?ici and Yuret, 2014) . Parallel FDA5 takes about half a day for each translation direction. We achieve SMT performance that is on par with the top constrained Moses SMT systems. Statistical machine translation (SMT) is a data intensive problem. If you have the translations for the source sentences you are translating in your training set or even portions of it, then the translation task becomes easier. If some tokens are not found in the training data then you cannot translate them and if some translated word do not appear in your language model (LM) corpus, then it becomes harder for the SMT engine to find its correct position in the translation. The importance of parallel FDA5 increases with the proliferation of training material available for building SMT systems. Table  2  presents the statistics of the available training and LM corpora for the constrained (C) systems as well as the statistics of the Parallel FDA5 selected training and LM corpora. Parallel FDA5 runs separate FDA5 models on randomized subsets of the training data and combines the selections afterwards. We run parallel FDA5 SMT experiments using Moses  (Koehn et al., 2007)  in all language pairs in WMT14  (Bojar et al., 2014)  and obtain SMT performance close to the top constrained Moses systems training using all of the training material. Parallel FDA5 allows rapid prototyping of SMT systems for a given target domain or task and can be very useful for MT in target domains with limited resources or in disaster and crisis situations  (Lewis et al., 2011) . 2 Parallel FDA5 for Instance Selection 

 FDA5 FDA is developed mainly for building high performance SMT systems using fewer yet relevant data that is selected for increasing the coverage of the test set features while maximizing their diversity  (Bic ?ici and Yuret, 2011; Bic ?ici, 2011) . Parallel FDA parallelize instance selection and significantly reduces the time to deploy accurate MT systems in the presence of large training data from weeks to half a day and still achieve state-ofthe-art SMT performance  (Bic ?ici, 2013) . FDA5 is developed for efficient parameterization, optimization, and implementation of FDA  (Bic ?ici and Yuret, 2014) . FDA5 can be used in both transductive learning scenarios where test set is used to select the training data or in active learning scenarios where training set itself is used to obtain a sorting of the training data and select. We run transductive learning experiments in this work such that the instance selection is performed for the given test set. According to SMT experiments performed on the 2 million sentence English-German section of the Europarl corpus  (Bic ?ici and Yuret, 2014) , FDA5 can increase the performance by 0.41 BLEU points compared to using all of the available training data and by  1 U ? shuffle(U) 2 U U U, M ? split(U, N ) 3 L ? {} 4 foreach U i ? U U U do 5 L i , s i ? FDA5(U i , F, M ) 6 L ? L ? L i , s i 7 L ? merge(L) 3.22 BLEU points compared to random selection. FDA5 is also used for selecting the training set in the WMT14 medical translation task  (Calixto et al., 2014)  and the tuning set in the WMT14 German-English translation task  (Li et al., 2014) . FDA5 has 5 parameters that effect the instance scores based on the three formulas used: ? Initialization: init(f ) = log(|U|/C U (f )) i |f | l (1) ? Decay: decay(f ) = init(f )(1+C L (f )) ?c d C L (f ) (2) ? Sentence score: sentScore(S) = 1 |S| s f ?F (S) fvalue(f ) (3) C L (f ) returns the count of feature f in L. d is the feature score polynomial decay factor, c is the feature score exponential decay factor, s is the sentence score length exponent, i is the initial feature score idf exponent, and l is the initial feature score n-gram length exponent. FDA5 is available at http://github.com/bicici/FDA and the FDA5 optimizer is available at http://github.com/bicici/FDAOptimization. 

 Parallel FDA5 Parallel FDA5 (ParFDA5) is presented in Algorithm 1, which first shuffles the training sentences, U and runs individual FDA5 models on the multiple splits from which equal number of sentences, M , are selected. We use ParFDA5 for selecting parallel training data and LM data for building SMT systems. merge combines k sorted arrays, L i , into one sorted array in O(M k log k) using their scores, s i , where M k is the total number of elements in all of the input arrays. 1 ParFDA5 makes FDA5 more scalable to domains with large training corpora and allows rapid deployment of SMT systems. By selecting from random splits of the original corpus, we work with different n-gram feature distributions in each split and prevent feature values from becoming negligible, which can enhance the diversity. 

 Language Model Data Selection We select the LM training data with ParFDA5 based on the following observation  (Bic ?ici, 2013) : No word not appearing in the training set can appear in the translation. It is impossible for an SMT system to translate a word unseen in the training corpus nor can it translate it with a word not found in the target side of the training set 2 . Thus we are only interested in correctly ordering the words appearing in the training corpus and collecting the sentences that contain them for building the LM. At the same time, a compact and more relevant LM corpus is also useful for modeling longer range dependencies with higher order n-gram models. We use 1-gram features for LM corpus selection since we don't know which phrases will be generated by the translation model. After the LM corpus selection, the target side of the parallel training data is added to the LM corpus. 

 Results We run ParFDA5 SMT experiments for all language pairs in both directions in the WMT14 translation task  (Bojar et al., 2014) , which include English-Czech (en-cs), English-German (en-de), English-French (en-fr), English-Hindi (en-hi), and English-Russian (en-ru). We true-case all of the corpora, use 150-best lists during tuning, set the LM order to a value between 7 and 10 for all language pairs, and train the LM using SRILM  (Stolcke, 2002) . We set the maximum sentence length filter to 126 and for GIZA++  (Och and Ney, 2003) ,    S 

 Optimized ParFDA5 Parameters Table  1  presents the optimized ParFDA5 parameters obtained using the development set. Translation direction specific differences are visible. A negative value for l shows that FDA5 prefers shorter features, which we observe mainly when the target language is English. We also observe higher exponential decay rates when the target language is mainly English. For optimizing the parameters for selecting LM corpus instances, we still use a parallel corpus and instead of optimizing for TCOV, we optimize for SCOV such that we select instances that are relevant for the target training corpus but still maximize the coverage of source features and be able to represent the source sentences within a translation task. The selected LM corpus is prepared for a translation task. 

 Data Selection We select the same number of sentences with Parallel FDA  (Bic ?ici, 2013) , which is roughly 15% of the training corpus for en-de, 35% for ru-en, 6% for cs-en, and 2% for en-fr. After the training set selection, we select the LM data using the target side of the training set as the target domain to select LM instances for. For en and fr, we have access to the LDC Gigaword corpora  (Parker et al., 2011; , from which we extract only the story type news. We select 15 million sentences for each LM not including the se-   2 . The size of the LM corpora includes both the LDC and the monolingual LM corpora provided by WMT14. Table  2  shows the significant size differences between the constrained dataset (C) and the ParFDA5 selected data. Table  2  also present the source and target coverage (SCOV and TCOV) in terms of the 2grams of the test set observed in the training data or the LM data. The quality of the training corpus can be measured by TCOV, which is found to correlate well with the BLEU performance achievable  (Bic ?ici and Yuret, 2011; Bic ?ici, 2011) . 

 Computing Statistics We quantify the time and space requirements for running ParFDA5 SMT systems for each translation direction. The space and time required for building the ParFDA5 Moses SMT systems are given in Table  3  where the sizes are in MB and the time in minutes. PT stands for the phrase table. We used Moses version 2.1.1, from www.statmt.org/moses. Building a ParFDA5 Moses SMT system takes about half a day. 

 Translation Results The results of our two ParFDA5 SMT experiments for each language pair and their tokenized BLEU performance, BLEUc, together with the LM order used and the top constrained submissions to the WMT14 are given in Table  4  3 , which use phrasebased Moses for comparison 4 . We observed significant gains (+0.23 BLEU points) using higher order LMs last year  (Bic ?ici, 2013 ) and therefore we use LMs of order 7 to 10. The test set contains 10,000 sentences and only 3000 of which are used for evaluation, which can make the transductive learning application of ParFDA5 harder. In the transductive learning setting, ParFDA5 is selecting target test task specific SMT resources and therefore, having irrelevant instances in the test set may decrease the performance by causing FDA5 to select more domain specific data and less task specific. ParFDA5 significantly reduces the time required for training, development, and deployment of an SMT system for a given translation  task. The average difference to the top constrained submission in WMT14 is 3.49 BLEU points. For en-ru and en-cs, true-casing the LM using a truecaser trained on all of the available training data decreased the performance by 0.5 and 0.9 BLEU points respectively and for cs-en and fr-en, increased the performance by 0.2 and 0.5 BLEU points. We use the true-cased LM results using a true-caser trained on all of the available training data for all language pairs where for hi-en, the true-caser is trained on the ParFDA5 selected training data. 

 LM Data Quality A LM training data selected for a given translation task allows us to train higher order language models, model longer range dependencies better, and at the same time, achieve lower perplexity as given in Table  5 . We compare the perplexity of the ParFDA5 selected LM with a LM trained on the ParFDA5 selected training data and a LM trained using all of the available training corpora. To be able to compare the perplexities, we take the OOV tokens into consideration during calculations  (Bic ?ici, 2013) . We present results for the cases when we handle OOV words with a cost of ?19 or ?11 each in Table  5 . We are able to achieve significant reductions in the number of OOV tokens and the perplexity, reaching up to 66% reduction in the number of OOV tokens and up to 80% reduction in the perplexity. 

 BLEUc S ? en en ? T cs-en de-en fr-en ru-en en-cs en-de en-fr en- In the FDA5 results  (Bic ?ici and Yuret, 2014) , we found that selecting 15% of the best training set size maximizes the performance for the English-German out-of-domain translation task and achieves 0.41 BLEU points improvement over a baseline system using all of the available training data. We run additional experiments selecting 15% of the training data for fr-en and cs-en language pairs to see the effect of increased training sets selected with ParFDA5. The results are given in Table  6  where most of the results improve. The slight performance decrease for cs-en may be due to using a true-caser trained on only the selected training data. We observe larger gains in the en ? T translations. 

 ParFDA5 versus Parallel FDA We compare this year's results with the results we obtained last year  (Bic ?ici, 2013)  in Table  7 . The task setting is different in WMT14 since the test set contains 10,000 sentences but only 3000 of these are used as the actual test set, which can make the transductive learning application of ParFDA5 harder. We select the same number of instances for the training sets but 5 million more instances for the LM corpus this year. The average difference to the top constrained submission in WMT13 was 2.88 BLEU points  (Bic ?ici, 2013)  and this has increased to 3.49 BLEU points in WMT14. On average, the performance improved 3.7 BLEU points when compared with ParFDA results last year. For the fr-en, en-fr, and en-ru trans-lation directions, we observe increases in the performance. This may be due to better modeling of the target domain by better parameterization and optimization that FDA5 is providing. We observe some decrease in the performance in en-de and deen results. Since the training material remained the same for WMT13 and WMT14 and the modeling power of FDA5 increased, building a domain specific rather than a task specific ParFDA5 model may be the reason for the decrease. 

 Conclusion We use parallel FDA5 for solving computational scalability problems caused by the abundance of training data for SMT models and LMs and still achieve SMT performance that is on par with the top performing SMT systems. Parallel FDA5 raises the bar of expectations from SMT with highly accurate translations and lower the bar to entry for SMT into new domains and tasks by allowing fast deployment of SMT systems in about half a day. Parallel FDA5 enables a shift from general purpose SMT systems towards task adaptive SMT solutions. Algorithm 1 : 1 Parallel FDA5 Input: Parallel training sentences U, test set features F, and desired number of training instances N . Output: Subset of the parallel sentences to be used as the training data L ? U. 
