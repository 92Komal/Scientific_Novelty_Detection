title
Multilingual Neural Machine Translation: Case-study for Catalan, Spanish and Portuguese Romance Languages

abstract
In this paper, we describe the TALP-UPC participation in the WMT Similar Language Translation task between Catalan, Spanish, and Portuguese, all of them, Romance languages. We made use of different techniques to improve the translation between these languages. The multilingual shared encoder/decoder has been used for all of them. Additionally, we applied back-translation to take advantage of the monolingual data. Finally, we have applied fine-tuning to improve the in-domain data. Each of these techniques brings improvements over the previous one. In the official evaluation, our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair.

Introduction Research in the field of Machine Translation (MT) has been growing during these last years. From statistical approaches  (Koehn et al., 2003)  to neural ones  (Bahdanau et al., 2015) , the progress has been impressive. Even after having achieved exceptional results based only on attention mechanisms  (Vaswani et al., 2017) , there are still many challenges and improvements remaining, for instance, multilingual translation from languages other than English, which have lower resources, and domain adaptation. In order to tackle these challenges, the Similar Language Task organized in the context of the Conference on Machine Translation (WMT 2020) has provided an appropriate setting for them. Within this task, the focus is the translation between languages that are different from English, and more specifically, the focus consists of translating languages that are from the same family. The families included are the following: South-Slavic, Indo-Aryan, and Romance. In our case, we have devoted the research to Romance languages, which include Spanish, Portuguese, and Catalan. The evaluation comprised all translation directions, but only provided parallel training data for Spanish-Portuguese and Spanish-Catalan. We approached the Portuguese-Catalan pair both from a pivot-based and zero-shot perspective. In this paper, we make use of the well-known multilingual shared encoder/decoder and we show its effectiveness when applied to languages of the same linguistic family. Additionally, we benefited from back-translation and fine-tuning. 

 Background In this section, we show an overview of neuralbased multilingual machine translation and domain adaptation using fine-tuning. 

 Multilingual translation When having multiple languages, there is the opportunity to use several NMT architectures, based in the Transformer  (Vaswani et al., 2017) . Among the alternatives, we can share encoders and decoders  (Johnson et al., 2017)  or have specific encoders and decoders for each language  (Escolano et al., 2020) . In this paper, we are using the shared approach and we are leaving as further work to compare with other ones. Shared encoder-decoder One direct approach is using a single encoder/decoder shared for all languages  (Johnson et al., 2017) . In this case, parameters and vocabulary are shared among all language pairs and it helps the generalization across languages improving the translation for the low resource language pairs  (Aharoni et al., 2019) . Additionally, the shared encoder/decoder allows using zero-shot easily, only by adding a tag in the source sentence. The source sentence has to contain the language abbreviation of the target language. So, when translating from Catalan to Spanish, we have to include the <2es> tag at the beginning of the Catalan source sentence, which means that we are translating into Spanish. <2es> Bon dia -> Buenos d?as Therefore, it is necessary to add the tag to indicate the target language, followed by the sentence to be translated. This is necessary both in training and inference. 

 Monolingual corpus selection for back-translation There is a large amount of monolingual data available for this task. Monolingual data can improve the system by using back-translation  (Sennrich et al., 2016) . However, back-translation is a process that consumes a lot of resources, so we decided to select the monolingual data within the target domain. selection criterion has been the TF-IDF (Term Frequency -Inverse Document Frequency), which defines the relevance of the words in a document. Using this criterion, we compared all the available monolingual data against the development set and only kept the files that had a higher score among all. 

 Domain adaptation One approach to improve the translation of a specific language domain is to make use of fine-tuning techniques. Fine-tuning consists of retraining a model that has already been trained with out-ofdomain data, with in-domain data. The disadvantage of fine-tuning is that it tends to overfit, due to the small amount of in-domain data used, compared to the out-of-domain data. Sometimes the final model might fall into the problem of catastrophic forgetting  (French, 1999) . One approach to avoid over-fitting and catastrophic forgetting is to do mixed fine-tuning, which consists of shuffling the in-domain with the outof-domain data, and then train normally on this combined data  (Chu and Dabre, 2019) . 

 Experimental Framework In this section, we describe the datasets used for the task, the data preprocessing, the training, and the evaluation of the bilingual and multilingual systems. 

 Data and Preprocessing Data Selection All the data used in our experiments has been provided by the organizers, so we did not make use of any additional parallel nor monolingual data. For the Catalan-Spanish and Spanish-Portuguese translation, we used all the parallel data available, which is about 11.3 million sentences for the Catalan-Spanish translation and 4.1 million sentences for the Spanish-Portuguese. For the Catalan-Portuguese we did not have any parallel data. We have also used monolingual data for backtranslation purposes. Two million sentences have been used from the CaWaC file for Catalan, about 1.1 million sentences from News-commentary-v15 and News-crawl-2019 files for Portuguese, and 1.5 million sentences from News-commentary-v15 and News-crawl-2015 for Spanish. The multilingual model has been trained using all the parallel data, and with pseudo-parallel data that has been obtained by applying back-translation. To achieve the back-translation we used our best system at the moment to perform the translation of the monolingual data, obtaining the pseudo-parallel corpus. As said in Section 2.2, the monolingual data has been selected using TF-IDF as the measure for text similarity 1 . We used 2/3 of the development set for fine-tuning purposes and 1/3 of the development set as a test set. Preprocessing We followed the standard procedure for preparing the data, which consists of normalizing, tokenizing, truecasing, and cleaning (limiting sentences from 1 to 50 words). To perform these actions we made use of the Moses 2 scripts. We extracted the joint subwords with byte-pair encoding (BPE) 3 . 

 Parameter Details The bilingual and multilingual models are both based on the Transformer architecture, implemented with fairseq toolkit 4 . We assigned six attention layers for the encoder and the decoder, each having four attention heads per layer, with an embedding dimension of 512. Additionally, all the models shared the source and target embeddings. The multilingual model shared the embeddings among all language pairs. Each batch was assigned to have a maximum number of tokens of 2048. The optimizer used was Adam, setting the betas to ? 1 = 0.9 and ? 2 = 0.98, with a learning rate of 5e-4 varied with the inverse square root of the step number. The warm-up steps were set equal to 4000, a dropout of 0.1, and a weight decay and gradient clipping norm set to 0. 

 Results The results show the improvements obtained by applying multilinguality, back-translation, and finetuning techniques. For the pair Catalan-Portuguese (CA-PT), in which there was no training data available. We have used the cascade technique, which consists of concatenating the translation of Catalan-to-Spanish and Spanish-to-Portuguese systems, and the other way around for the opposite direction. Also, we have used the multilingual system to obtain zero-shot translation for this pair. Table  1  shows that the multilingual model outperforms the bilingual model in all cases. Zeroshot performs worse than the cascade method. Applying back-translation to the multilingual model improves for most language pairs and directions. Finally, when applying fine-tuning to the backtranslation model, we see an improvement in all pairs and directions, except for the PT?CA direction with zero-shot. 

 Official evaluation results Here we report the official evaluation. We participated with our best system which was the multilingual model with back-translation and fine-tuning. For the CA-PT directions, we translated using the cascade technique, Table  2  reports the results on the evaluation test set. Our system was ranked 1st in the Portuguese-to-Spanish direction, 2nd in the opposite direction, and 3rd in the Catalan-Spanish pair. For the Catalan-Portuguese directions, the results were not released.  

 Discussion We will now discuss the results obtained for each system we have trained, comparing one against the others. Bilingual model compared to the Multilingual model We have shown that the multilingual model outperforms the bilingual model in all translations directions, with an improvement that varies from +0.4 to +6.9 BLEU. The multilingual model allows for a better generalization by sharing the vocabulary among all the languages. Additionally, the multilingual model allows for zero-shot translation. 

 Back-translation This technique allows us to make use of monolingual data. The improvement with this technique varies from +0.5 to +3.4 BLEU, except when using the monolingual Catalan data (ES?CA and PT?CA directions). This deterioration is probably due to the lower resemblance (estimated using the TF-IDF score) of the CaWaC dataset compared to the target domain. Fine-tuning We have applied fine-tuning to perform the domain adaptation. To do so, we added 2/3 of the development data set to the already trained model, which is the multilingual model with back-translation, since it was the best model we had so far. After doing so, we had to retrain the model from the last checkpoint, preventing it from overfitting. By applying fine-tuning, we were able to achieve improvements between +0.6 and +2.5 BLEU points (except in zero-shot). This finetuning improvement is achieved by using very few resources (1500 sentences) and less time compared to back-translation, which requires more resources and time. 

 Conclusion We have observed how using a multilingual shared encoder/decoder in languages from the same family improves bilingual translation. This is due to a positive transfer among these languages while sharing vocabulary and embeddings. Additionally, this multilingual shared system has been improved with both back-translation and fine-tuning methods. Table 2 : 2 Official BLEU scores for the evaluation of the final test set. Directions BLEU ES-CA 60.50 CA-ES 68.84 ES-PT 32.33 PT-ES 33.82 CA-PT 32.80 PT-CA 34.40 
