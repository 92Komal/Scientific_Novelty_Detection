title
English-Myanmar Supervised and Unsupervised NMT: NICT's Machine Translation Systems at WAT-2019

abstract
This paper presents the NICT's participation (team ID: NICT) in the 6th Workshop on Asian Translation (WAT-2019) shared translation task, specifically Myanmar (Burmese) -English task in both translation directions. We built neural machine translation (NMT) systems for these tasks. Our NMT systems were trained with language model pretraining. Back-translation technology is adopted to NMT. Our NMT systems rank the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score.

Introduction This paper describes the neural machine translation (NMT) systems 1 built for National Institute of Information and Communications Technology (NICT)'s participation in the the 6th Workshop on Asian Translation (WAT-2019) translation task  (Nakazawa et al., 2019) , specifically Myanmar (My) -English (En) for both translation directions. The remainder of this paper is organized as follows. In Section 2, we present the data preprocessing. In Section 3, we introduce the details of our NMT systems. Empirical results obtained with our systems are analyzed in Section 4 and we conclude this paper in Section 5. 

 Data Preprocessing As parallel data to train our systems, we used all the provided parallel data for all our targeted * Rui and Haipeng have equal contribution to this paper. This work was conductd when Haipeng visited NICT as an internship student. 1 This system is based on our WMT-2019 system  (Marie et al., 2019) . translation directions, including the training corpus "ALT" and "UCSY", and the "ALT" dev/test data. The statistics of our preprocessed parallel data are illustrated in  We used Moses tokenizer and truecaser for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. 

 MT Systems To build competitive NMT systems, we chose to rely on the Transformer architecture  (Vaswani et al., 2017)  since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model  (Lample and Conneau, 2019)  to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE)  (Sennrich et al., 2016b) . We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 

 TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM 3 in order to pretrain the NMT model on 8 GPUs 4 . The parameters for training the language model were set as listed in Table  3 . --lgs 'en-my' --mlm steps 'en,my,en-my,my-en' --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001  

 NMT We trained a Transformer-based NMT model with the pre-trained TLM using XLM toolkit. Our NMT system was consistently trained on 8 GPUs, with the following parameters listed in Table  4 . We performed NMT decoding with a single model according to the best BLEU  (Papineni et al., 2002)  and the perplexity scores. --lgs 'en-my' --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --batch size 32 --bptt 256 --optimizer adam inverse sqrt,beta1=0.9, beta2=0.98,lr=0.0001 --eval bleu true  

 Back-translation We also tried back-translation method  (Sennrich et al., 2016a)  to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language to generate pseudo parallel corpora. Then, the pseudo parallel corpora were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. 

 UNMT To the best of our knowledge, unsupervised NMT (UNMT)  (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Lample and Conneau, 2019)  has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT model for 50000 iterations. The En-My UNMT system was trained on 8 GPUs, with the parameters listed in Table  6  Table  5 : Results (BLEU-cased) of our MT systems on the test set. ALT denotes that ALT training data was used in this system; UCSY denotes that UCSY training data was used in this system; MONO denotes monolingual training data was used in this system. +TLM denotes that language model pretraining was used in this system; +back-translation denotes that back-translation was used in this system. --lgs 'en-my' --ae steps 'en,my' --bt steps 'en-my-en,my-en-my' --word shuffle 3 --word dropout 0.1 --word blank 0.1 --lambda ae '0:1,100000:0.1,300000:0' --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --batch size 32 --bptt 256 --optimizer adam inverse sqrt,beta1=0.9, beta2=0.98,lr=0.0001 --eval bleu true  

 Results Our systems are evaluated on the ALT test set and the results 5 are shown in Table  5 . Our observations from are as follows: 1) The results of UNMT are very low, highlighting that UNMT is still very far from exploitable for low-resource distant language pairs. 2) Language model pretraining showed significant improvement in the NMT systems for both translation directions. This demonstrates that language model pretraining is effective for  5  The results of BLEU are based on our own evaluation. For the official results, please refer to http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/index.html. low-resource machine translation. 3) For My-En translation direction, backtranslation could further improve translation performance, achieving 8 BLEU scores improvement. However, back-translation for En-My translation direction was unable to improve or even harm the NMT performance since the My monolingual data was noisy. 

 Conclusion We presented in this paper the NICT's participation in the WAT-2019 shared translation task. Our primary NMT submissions to the task performed the third in English-to-Myanmar and the second in Myanmar-to-English according to BLEU score. Our results also confirmed the positive impact of language model pretraining in NMT. Moreover, our results for UNMT highlighted that unsupervised machine translation is still very far from exploitable for low-resource distant language pairs. Table 1 . 1 Corpus #lines #tokens (My/En) train(ALT) 17.9K 1.0M / 410.2K train(UCSY) 208.6K 5.8M / 2.6M dev(ALT) 0.9K 57.4K / 22.1K test(ALT) 1.0K 58.3K / 22.7K 
