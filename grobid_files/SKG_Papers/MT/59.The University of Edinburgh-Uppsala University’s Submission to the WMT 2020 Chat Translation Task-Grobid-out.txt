title
's Submission to the WMT 2020 Chat Translation Task

abstract
This paper describes the joint submission of the University of Edinburgh and Uppsala University to the WMT'20 chat translation task for both language directions (English?German). We use existing state-of-the-art machine translation models trained on news data and fine-tune them on in-domain and pseudo-indomain web crawled data. We also experiment with (i) adaptation using speaker and domain tags and (ii) using different types and amounts of preceding context. We observe that contrarily to expectations, exploiting context degrades the results (and on analysis the data is not highly contextual). However using domain tags does improve scores according to the automatic evaluation. Our final primary systems use domain tags and are ensembles of 4 models, with noisy channel reranking of outputs. Our en-de system was ranked second in the shared task while our de-en system outperformed all the other systems. 1

Introduction and challenges The task's aim is to create machine translation (MT) systems to enable task-oriented communication between a service agent and a customer speaking different languages (English and German respectively). Like most dialogues, the texts can show strong context sensitivities, as the customer and the agent engage in a common activity and continually react to each other's utterances  (Hardmeier, 2014; Bawden, 2018) . However, the dialogues, which relate to ordering or reserving products and services from a limited set of providers, also follow fairly strong scripts and are anchored in a small discourse universe defined by the products on offer. Their context sensitivity is therefore counterbalanced by domain-specific conventions and expectations. Our design choices are informed by an initial manual inspection of the training data and a baseline translation, which revealed that the main challenges relate to idiomaticity: incorrect or poor translation of English idioms, named entities and politeness markers (e.g. formal vs. informal forms of address, or poor translation of English sir) and an incorrect use of domain-specific terminology. Almost always, the problems were the result of an excessively literal translation of the source text, and this literalness also frequently affected the reference translations themselves too. Surprisingly, we found few instances of phenomena explicitly requiring context to be correctly translated (e.g. we did not find pronominal anaphora to be a major problem in the dialogues examined).  2  The contextdependent instances we did find were more taskspecific (e.g. English Enjoy! should be translated differently depending on whether it is about a pizza (Guten Appetit!) or a film (Viel Spa?!)). We therefore focus on domain adaptation and general context modelling strategies. Our submissions are based on existing state-of-the-art MT systems for news translation, which we fine-tune on in-domain and pseudo-in-domain data. We also experiment with (i) adapting the models to the different speaker roles and to the different tasks during fine-tuning and (ii) exploiting preceding context through a simple but effective method of concatenating previous sentences to the current one. Our code and models are publicly available. 3 

 Data The task data consists of parallel task-oriented dialogues between an agent (English) and a customer (German) across six domains: (i) ordering pizza, (ii) making auto repair appointments, (iii) ordering a taxi, (iv) ordering movie tickets, (v) ordering coffee and (vi) making restaurant reservations. The dialogues were initially in English, retrieved from a subset of the TaskMaster-1 dataset  (Byrne et al., 2019)  and then manually translated into German at Unbabel.  4  Although the speaker tags are provided for each utterance, the conversations are not explicitly marked with their task domain. The task being to translate the agent's utterances from English into German and the customer's utterances from German to English, we evaluate each translation direction separately, using only the agent's utterances for en-de translation and the customer's utterances for de-en. For training however, we use the full set of 13,845 utterances for both directions. 

 Approaches We explore four approaches, each of which is detailed below: (i) pretraining using additional data sources, (ii) speaker adaptation, (iii) domain adaptation and (iv) incorporating previous context. Pretraining To account for the limited indomain data, we use pre-existing MT models trained for the WMT'19 news task  (Barrault et al., 2019)  and then continue training on pseudo-indomain web crawled data from the Paracrawl project 5  (Ba?n et al., 2020) , before fine-tuning on the in-domain chat training data. We compare two different base systems for each language direction: UEDIN models 6 (  (Bawden et al., 2019a)  and FAIR models . The pseudo-in-domain data on which training is continued is created by filtering Paracrawl data using dual conditional noisy cross-entropy filtering (Junczys-Dowmunt, 2018). This consists in training a neural language model for each language on the task training data, and jointly scoring each parallel sentence in Paracrawl using the two models. We take the top scoring 2.5 million subset of the original 34 million en-de sentences (those that most resemble the task data). Speaker adaption Distinguishing between the two speaker roles is important as they have different contributions to the dialogue; the customer's utterances are short, interrogative and informal, while the agent's utterances are often long, informative and more formal. We adapt our models to each speaker by using the speaker identity (provided with the task data) as a pseudo-token  (Sennrich et al., 2016a) : we prepend a speaker tag to each utterance on both the source and the target side. 

 Domain adaptation Knowing which task the dialogue belongs to (e.g. pizza, film) can be important for disambiguation, as described in Section 1. Similarly to speaker adaptation, we adapt to the different tasks (i.e. domains) by prepending a domain tag to each utterance on both the source and target side. We also consider a setup where all the utterances are tagged with speaker and domain-tags (see the example in Table  1 ). The dataset consists of chats across six different domains (pizza, auto, taxi, movie, coffee, and restaurant). As the domains are not indicated in the task dataset, we obtain domain tags by automatically classifying each dialogue as belonging to one of the six tasks using the English side of the data and a baseline German translation. The dialogue classifier is trained by unsupervised k-means clustering of the training set dialogues with scikit-learn  (Pedregosa et al., 2011) . As features, we use the nouns in the texts (as recognised by the SpaCy PoS tagger 7 ), which works substantially better than using all words. The 6 clusters are initialised to the word sets {pizza}, {auto, car, repair}, {ride}, {movie}, {coffee}, {dinner, restaurant}. Dialogues in the test set are then assigned to the cluster with the nearest centroid. To evaluate the classifier, we manually annotated 49 dialogues from the training set. Training only on the remainder of the training data, we achieved perfect accuracy on the annotated set. To simulate an online translation scenario, we also experimented with classification using only the initial utterances of each dialogue. In this setting, it was beneficial to project the feature space to a very low dimension using Latent Semantic Analysis (LSA). The best results with a macroaveraged F-score of 0.862 (precision 0.896; recall 0.867) were obtained by using the first 4 sentences and an LSA dimensionality of 5. However, since there was no online constraint in the shared task, we ultimately decided to use the more accurate full-dialogue classifier for our submission. Context-level MT Finally, we explore using linguistic context (varying numbers of previous utterances) to improve translation, with the aim that previous context can provide vital information for disambiguation or adaptation. We use the approach of concatenating varying numbers of previous sentences to the current sentence, separated by a sentence boundary token <break>  (Tiedemann and Scherrer, 2017; . This simple strategy was shown to be one of the most effective in a recent comparison of document-level MT approaches  (Lopes et al., 2020) . To distinguish between different speakers, we also add the speaker tag to the beginning of every utterance. The models are trained to translate both the context and the utterance into the target language (i.e. n-to-n strategy). The candidate utterance is then extracted from the generated output in a preprocessing step. Since the dialogues are bilingual (the agent and customer are speaking in different languages), the original versions of the previous sentences can be either in English or in German. While we always translate both the context and the current sentence into the target language on the target side, we consider two approaches to incorporate context in the source sentence: (i) ORIG: each previous sentence is in the original language of its speaker (if the context and current sentences are not produced by the same speaker, our input will be a mix of English and German) and (ii) SAME: the source context is provided in the same language as the current sentence (language consistency in the source input). At test time, this requires translating utterances sentence by sentence (as opposed to batch decoding); when the previous utterances are not from the same speaker, they must first be translated by the MT model in the opposite language direction for them to be used as context for the current sentence. 

 Experimental setup We compare two neural MT base system types, both WMT'19 news translation task submissions: UEDIN (University of Edinburgh;  Bawden et al. 2019a and FAIR (Facebook; . All models are transformer-big models  (Vaswani et al., 2017) : 6 encoder and 6 decoder layers, model dimension of 1024, 16 heads except that UEDIN has a feedforward dimension of 4096 for both the encoder and decoder, and FAIR models increase this dimension to 8192 in the encoder. UEDIN models are implemented in Marian  and FAIR models in Fairseq . Both model types are trained on parallel and backtranslated monolingual data from the WMT'19 news translation shared task  (Barrault et al., 2019) . For our final submission (using the base FAIR model), we also use noisy channel reranking , which requires MT models in both directions and a (target) language model. We describe the data processing techniques in Appendix A and list the hyper-parameters in Appendix B. 

 Experimental Results and Analysis We report automatic evaluation results in Section 5.1 and provide a qualitative manual comparison in Section 5.2. 

 Automatic evaluation results We report BLEU scores  (Papineni et al., 2002) , calculated with SACREBLEU 8  (Post, 2018)  on the dev set (beam size of 4). Pretraining The results in Table  2  show that indomain fine-tuning of the pretrained models always gives large gains. The pre-trained FAIR models are better than the pre-trained UEDIN models  (Barrault et al., 2019) . Fine-tuning on filtered paracrawl and then on the in-domain data gives a slight gain for the UEDIN models (particularly for de-en) but slightly degrades the FAIR models. We choose to take as a base the models fine-tuned on filtered paracrawl to fine-tune all subsequent models (with tags and context). Though these models perform similar to the FT 1 models, as these were trained on more data, they are likely to be more robust on unseen data. Note that all pretrained models outperform the baseline models trained just on the chat training data (shown in the first row). Table  2 : BLEU scores on the dev set for both pretrained models, and of each model fine-tuned on (i) in-domain data and (ii) filtered paracrawl then in-domain data. Effect of adding tags As shown in Table  3 , we observe that in general the performance of both systems improves with the addition of tags. The use of speaker tags improves the BLEU scores for UEDIN models while dialogue tags improve the BLEU scores for FAIR models. We did not observe an improvement in BLEU scores in models using both the tags over models that used a single tag. Context-level MT As shown in Table  4 , the contextual models perform similarly to the baseline for FAIR models while the performance degrades slightly with the UEDIN models. Increasing the number of contextual sentences degrades BLEU scores, most likely due to the necessity to translate longer sentences. It is also likely that the MT systems do not benefit from the addition of previous sentences because the particular chat dataset used contains utterances that do not need context to be correctly translated, contrary to expectations but in line with findings by  Mosig et al. (2020) . Using context in the same language (SAME) was more beneficial than the original context (ORIG). It is evident that SAME would perform better than ORIG as the pre-trained models were never exposed to such mix-language utterances. Despite fine-tuning a monolingual encoder on mix-language utterances, ORIG systems perform well.  sets: a 4-model ensemble, each model trained by first fine-tuning the pre-existing FAIR model on filtered paracrawl data, then on in-domain training data tagged with dialogue tags and then reranked using noisy channel reranking (n=20) . We note that noisy channel reranking is more effective for en-de than for de-en. Ensembling provides limited gains. We report our contrastive submissions for comparison. Our models were chosen on their respective performances on the dev set. We observe that the trends for dev set and test set are similar except for FT 2 + domaintags model without the noisy channel re ranking. 

 Final submission 

 Qualitative Evaluation As the gains in BLEU scores with different configurations are limited, it is difficult to identify if the models exhibit qualitative improvement. We created an evaluation set by selecting around 40 peculiar utterances in each translation direction from the development set and conducted an informal human evaluation by assigning scores of ?1, 0 or 1 to poor, acceptable or particularly good translations. The average score was used to guide model selection. As per the qualitative evaluation, there were few and similar errors across different models to draw any significant conclusions. Notably, the number of errors was higher for the en-de direction due to the production of literal translations. Our primary submission achieved a score of 85.357 on human evaluation using direct assessment. 6 Discussion and Future Work We observe that fine-tuning the WMT'19 newsadapted models on in-domain chat data is a strong baseline. The addition of tags, though helpful, has limited gains on BLEU, and the addition of context (intuitively an important component for any dialogue related task) actually degrades results. We speculate that this is due to the nature of the original dataset, which has limited linguistic diversity and utterances that are mostly context-independent  (Mosig et al., 2020) . The overall translation of this dataset was of excellent quality, allowing easy understanding of the dialogues. However, the translated chats exhibit translationese and in some cases lacked naturalness, also the case of the references themselves. An interesting avenue for data collection would be a spontaneous generation of chats two different languages which can roughly follow the same discourse as in  (Bawden et al., 2019b) . Table 1 : 1 Okay, got it. <speaker=customer> Perfekt. In Ordnung, verstanden. Domain <taxi> Perfect. Okay, got it. <taxi> Perfekt. In Ordnung, verstanden. Speaker+domain <taxi> <speaker=customer> Perfect. Okay, got it. <taxi> <speaker=customer> Perfekt. In Ordnung, verstanden. Examples from the dataset annotated with variants of speaker and domain tags. Adaptation Source text Target text Speaker <speaker=customer> Perfect. 

 Table 3 : 3 Dev set BLEU scores for fine-tuning with tags. Model en-de UEDIN FAIR UEDIN FAIR de-en FT2 + no tag FT2 + speaker FT2 + domain FT2 + speaker + domain 58.8 59.4 59.6 59.6 60.8 61.3 61.5 61.1 60.9 60.1 60.8 61.4 62.2 62.1 62.7 61.6 

 Table 4 : 4 Table5shows the results of our primary submission on both the dev and test Dev set BLEU scores for contextual MT models. The numbers before "prev" are the number of previous utterances used as context. Model en-de UEDIN FAIR UEDIN FAIR de-en FT2 + in-domain 58.8 60.8 60.9 62.2 In-domain data uses previous context (ORIG language) FT2 + 1 prev FT2 + 2 prev FT2 + 3 prev 58.2 56.1 53.3 60.3 60.2 59.5 58.9 58.7 56.7 61.8 61.5 61.7 In-domain data uses previous context (SAME language) FT2 + 1 prev FT2 + 2 prev FT2 + 3 prev 58.1 57.5 55.4 61.0 60.1 60.5 59.2 59.1 57.3 62.2 61.5 62.1 Model(FAIR) en-de dev test de-en dev test FT2 + domain-tags + noisy-channel re-ranking 62.0 60.1 62.9 61.8 61.5 60.3 62.7 60.6 + ensemble [primary] 62.1 60.2 63.1 62.4 FT1 [contrastive] 61.4 60.2 62.3 61.8 FT2 + 1-same [contrastive] 61.0 59.8 62.2 61.5 

 Table 5 : 5 The method-wise ablation of our final submission: a 4-model ensemble of FAIR based FT 2 models fine-tuned with in-domain training data tagged with domain tags. The outputs are obtained through noisychannel reranking. 

			 http://www.statmt.org/wmt20/ chat-task_results_DA.html 

			 We tested AllenNLP's coreference resolution tool (Gardner et al., 2018)  on a few examples where pronoun resolution seemed relevant and found that it performed very poorly in these cases, confirming similar conclusions by Bawden (2016) . We therefore decided not to model coreference explicitly.3 http://github.com/chardmeier/ WMT2020-Chat 

			 https://github.com/Unbabel/BConTrasT 5 https://www.paracrawl.eu 6 Although the WMT'19 submission included only de-en, we also use the similarly trained model for en-de. 

			 https://spacy.io 

			 Default parameters and case-sensitive evaluation.
