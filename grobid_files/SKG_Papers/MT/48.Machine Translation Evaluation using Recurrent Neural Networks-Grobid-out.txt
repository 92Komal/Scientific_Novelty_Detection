title
Machine Translation Evaluation using Recurrent Neural Networks

abstract
This paper presents our metric (UoW-LSTM) submitted in the WMT-15 metrics task. Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve the best results. We use a metric based on dense vector spaces and Long Short Term Memory (LSTM) networks, which are types of Recurrent Neural Networks (RNNs). For WMT-15 our new metric is the best performing metric overall according to Spearman and Pearson (Pre-TrueSkill) and second best according to Pearson (TrueSkill) system level correlation.

Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing  (Mikolov et al., 2013b; Socher et al., 2011) , sentiment analysis  (Socher et al., 2013b) , parsing  (Socher et al., 2013a)  and machine translation  (Mikolov et al., 2013a) . While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or Recurrent Neural Networks (RNNs) are able to capture semantic similarity for words  (Mikolov et al., 2013b) , segments  (Socher et al., 2011)  and documents  (Le and Mikolov, 2014)  naturally, traditional measures can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric  (Gupta et al., 2015)  is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks  (Tai et al., 2015) . LSTM is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed representations of sentences using distributed representations of words. Tree-LSTM  (Tai et al., 2015)  is a recent approach, which is an extension of the simple LSTM framework  (Hochreiter and Schmidhuber, 1997; Zaremba and Sutskever, 2014) . 

 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU  (Papineni et al., 2002)  and NIST  (Doddington, 2002) ) or word error rate. Other popular metrics like METEOR  (Denkowski and Lavie, 2014)  and TERp  (Snover et al., 2008)  also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER-0.821). Recent best performing metrics in the WMT-14 metric shared task  (Mach?cek and Bojar, 2014)  used a combination of different metrics. The top performing system DiskoTK-Party-Tuned  (Joty et al., 2014)  in the WMT-14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit  (Gim?nez and M?rquez, 2010) . The metric computes the number of common sub-trees between a reference and a translation using a convolution tree kernel  (Collins and Duffy, 2001) . The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED  (Gautam and Bhattacharyya, 2014) , uses linear interpolation of different metrics. LAY-ERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance  (Birch and Osborne, 2011)  to identify syntactic similarity, and dependency parsing  (De Marneffe et al., 2006)  and the Universal Networking Language 1 for semantic similarity. For our participation in the WMT-15 task, we used our metric ReVal  (Gupta et al., 2015) . ReVal metric is based on dense vector spaces and Tree Long Short Term Memory networks. This metric achieved state of the art results for the WMT-14 dataset. The metric including training data is available at https://github.com/rohitguptacs/ReVal. 

 LSTMs and Tree-LSTMs Recurrent Neural Networks allow processing of arbitrary length sequences, but early RNNs had the problem of vanishing and exploding gradients  (Bengio et al., 1994) . RNNs with LSTM  (Hochreiter and Schmidhuber, 1997)  tackle this problem by introducing a memory cell composed of a unit called constant error carousel (CEC) with multiplicative input and output gate units. Input gates protect against irrelevant inputs and output gates against current irrelevant memory contents. This architecture is capable of capturing important pieces of information seen in a bigger context. Tree-LSTM is an extension of simple LSTM. A typical LSTM processes the information sequentially whereas Tree-LSTM architectures enable sentence representation through a syntactic structure. Equation (1) represents the composition of a hidden state vector for an LSTM architecture. For a simple LSTM, c t represents the memory cell and o t the output gate at time step t in a sequence. For Tree-LSTM, c t represents the memory cell and o t represents the output gate corresponding to node t in a tree. The structural processing of Tree-LSTM makes it more favourable for representing sentences. For example, dependency tree structure captures syntactic features and model parameters capture the importance of words (content vs. function words). h t = o t ? tanh c t (1) 1 http://www.undl.org/unlsys/unl/unl2005/UW.htm Figure  1  shows simple LSTM and Tree-LSTM architectures. . . x 2 . . y 2 . . y 1 . x 1 . . y 3 . x 3 . . y 4 . x 4 . . y 5 . x 5 . .  x 1 . y 1 . . x 2 . y 2 . . x 3 . y 3 . . 

 Evaluation Metric We used the ReVal  (Gupta et al., 2015)  metric for this task. This metric represents both the reference (h ref ) and the translation (h tra ) using a dependency Tree-LSTM  (Tai et al., 2015)  and predicts the similarity score ? based on a neural network which considers both distance and angle between h ref and h tra : h ? = h ref ? h tra h + = |h ref ? h tra | h s = ? ( W (?) h ? + W (+) h + + b (h) ) p? = softmax ( W (p) h s + b (p) ) ? = r T p? (2) where, ? is a sigmoid function, p? is the estimated probability distribution vector and r T = [1 2...K]. The cost function J(?) is defined over probability distributions p and p? using regularised Kullback-Leibler (KL) divergence. J(?) = 1 n n ? i=1 KL ( p (i) ||p (i) ? ) + ? 2 ||?|| 2 2 (3) In Equation  3 , i represents the index of each training pair, n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows:  for 1 ? j ? K. Where, y ? [1, K] is the similarity score of a training pair. For example, for y = 2.7, p T = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. p j = ? ? ? ? ? y ? ?y?, j = ?y? + 1 ?y? ? y + 1, j = ?y? 0 otherwise To compute our training data we automatically convert the human rankings of the WMT-13 evaluation data into similarity scores between the reference and the translation. These translationreference pairs labelled with similarity scores are used for training. We also augment the WMT-13 data with 4500 pairs from the SICK training set  (Marelli et al., 2014) , resulting in a training dataset of 14059 pairs in total. The metric uses Glove word vectors  (Pennington et al., 2014)  and the simple LSTM, the dependency Tree-LSTM and neural network implementations by  Tai et al. (2015) . Training is performed using a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The memory dimension is 300, hidden dimension is 100 and compositional parameters are 541,800. Training is performed for 10 epochs. System level scores are computed by aggregating and normalising the segment level scores. Full details can be found in  (Gupta et al., 2015) . 2 

 Results The results for WMT-15 are presented in Table  1  and Table  2 . Table  1  shows system-level Pearson correlation (TrueSkill) (see  (Bojar et al., 2013)  for difference between TrueSkill and Pre-TrueSkill systemranking approaches) obtained on different language pairs as well as average (PAvg) over all language pairs. The second last column shows average Pearson correlation (Pre-TrueSkill). The last column shows average Spearman correlation (SAvg). The 95% confidence level scores are obtained using bootstrap resampling as used in the WMT-2015 metric task evaluation. Table  2  shows results on segment-wise Kendall tau correlation. The first section of Table  1  and Table  2  shows the results of our ReVal metric as UoW-LSTM, the second section shows the other four top performing metrics and the third section shows baseline metrics (BLEU, TER and WER for system-level and SENTBLEU for segment level). Table  1  shows that our metric obtains the best results overall for both Pearson (Pre-TrueSkill) and Spearman system-level correlation and second best overall using Pearson (TrueSkill) correlation. Table  2  shows that while improving over SENT-BLEU our metric does not obtain high segment level scores. 

 Conclusion and Future Work Our dense-vector-space-based ReVal metric is simple, elegant and fully competitive with the best of the current complex alternative approaches that involve system combination, extensive external resources, feature engineering and tuning. In future work we will investigate the difference between system and segment level evaluation scores. Figure 1 : 1 Figure 1: Tree-LSTM (left) and simple LSTM (right) 
