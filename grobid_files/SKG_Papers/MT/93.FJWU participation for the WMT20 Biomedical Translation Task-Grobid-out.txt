title
FJWU participation for the WMT20 Biomedical Translation Task

abstract
This paper reports system descriptions for FJWU-NRPU team for participation in the WMT20 Biomedical shared translation task. We focused our submission on exploring the effects of adding in-domain corpora extracted from various out-of-domain sources. Systems were built for French to English using indomain corpora through fine tuning and selective data training. We further explored BERT based models specifically with focus on effect of domain adaptive subword units.

Introduction In this paper, we present Neural Machine Translation (NMT) systems developed by Fatima Jinnah Women University for participation in WMT20, Biomedical shared Translation task. The systems are developed for translating English/French (EN/FR) in both directions for biomedical domain using fairseq  (Ott et al., 2019)  and BERT  (Devlin et al., 2018) . To tackle in-domain corpus shortage challenge, selective data training and fine tuning are explored. We focused our submission on investigating the effects of adding in-domain corpora extracted from out-of-domain sources of various domains, objective was to study the effect of domain non-relatedness in schemes involving data selection through information retrieval or any sentence selection method. We further explored BERT based models specifically with focus on effect of domain adaptive subword units. Neural Machine Translation systems have shown substantial growth with the ongoing introduction of new tool kits and training techniques to support developers in training models  (Bahdanau et al., 2014; Wu et al., 2016) . But the availability and cleaning of domain related corpora to achieve terminology advantage and fluency is still a challenge for many researchers as the accessible corpora is relatively small in size and comparatively noisy. In order to improve in-domain NMT systems, out-of-domain data is used and the most common method is to fine tune pre trained NMT models on in-domain data and selective data training  (Hira et al., 2019) . Our last years submission presented promising results using selective data training incorporating data retrieved from News Commentary corpus by building two layered RNN systems. We extend our framework to study the quality of retrieved sentences from 3 more parallel corpora. We did not restrict to parallel data for mining biomedical sentences, rather this year we included monolingual data in our framework and studied the effect of using Back Translations (BT) in our framework. For building NMT models we explored subword units and report the results on using pre-trained BERT fused embedding. 

 Data Selection Architecture Improving translation quality is a challenging task especially for domains where enough in-domain parallel corpus is not available to train a good translation system. To overcome data scarcity problem, several data selection techniques have been proposed over the years including information retrieval (IR)  (Rauf and Schwenk, 2011) , edit distances  (Wang et al., 2013) , cross entropy measures  (Axelrod et al., 2011)  and several others. We used the approach of relative query sentences using information retrieval to retrieve matching sentences from general domain corpora. French-English is not a resource scarce language pair and has numerous parallel corpora available for various domains. There exist sizable corpus for the Biomedical domain to train the initial systems, but the great difference of terminologies and language jargon in various sub domains makes it challenging as the results of previous years bio med- ical tasks indicate. Parallel corpora extracted from "other" easily available corpora, like comparable corpora and monolingual corpora do help improve MT performance  (Abdul-Rauf and Schwenk, 2009; Abdul-Rauf et al., 2016) . But, what is the effect of the domain of the corpus used to find the related sentences, is the question we focus on in our data selection design. Our aim is to study the improvements achieved by using the sentences from different genre/domain of corpora. However, to be able to extract sizeable amount of biomedical sentences, the corpora should not be very unrelated, for example, the Europarl corpus  (Koehn, 2005)  which is composed of Parliament proceedings would not be a good choice 1 . Our intent was to do a comparative study of quality of extracted sentences from varied but yet not too far off domain corpora. Thus, for mining related sentences from general domain corpora we used Books 2 , News Commentary 3 and WikiPedia 4 corpus obtained from Open Parallel Corpus (OPUS)  (Tiedemann, 2012) . French WikiPedia 5 (FrWikipediaMono) was also used which was available as monolingual corpus and was translated to English. Our data selection strategy is graphically presented in figure  1 . We followed the data selection approach based on IR as proposed by  (Abdul-Rauf et al., 2016) . The choice of corpus to use as queries was a critical one: queries should have maximum biomedical terminologies to enable targeting and choosing domain specific sentences from the general domain corpora. We chose Medline titles as queries hypothesising on the fact that the title essentially contains the specific domain terminology. We used English side of Medline titles as queries when retrieving similar sentences from English side of the corpora and French side of Medline titles as queries for IR from French side. We retrieved 10-best sentences and experimented with top-1, top-2 and top-3 sentences as shown in section 4. Table  1  shows the number of retrieved sentences per each corpus and the unique sentences chosen from these to build our models. 

 Corpus Sentences In  

 Corpora In this section, we present details of corpora used to train our systems, pre-processing and training parameters. We used the in-domain corpora provided by the organizers along with our mined in-domain sentences from the general domain corpora. The in-domain corpora included were: ? Ufal medical corpus, where a subset of medical corpora were extracted including CESTA, ECDC, EMEA, Subtitles and patTR medical corpus.  (Yepes et al., 2017)  ? Scielo corpus that included scientific biodomain articles.  (Neves et al., 2016)  ? EDP dataset containing documents from EDP database for scientific publications.  ? Medline abstracts and titles from publications.  (Bawden et al., 2019)  Books, News Commentary, WikiPedia and Fr-WikipediaMono corpora were used as the outdomain corpora to perform in data selective training experiments by extracting relevant in-domain sentences as explained in section 2. Development set included EDP, Scielo and Khresmoi  (Du?ek et al., 2017) . Medline test corpora provided by WMT18  and WMT19  (Bawden et al., 2019)  were used as test sets. 

 Pre-processing Our pre-processing pipeline includes data cleaning, punctuation normalization, tokenization, truecasing and subword segmentation. Data cleaning was done to remove noisy data. Some of the provided corpora, including EDP, Scielo and Subtitles, were not completely aligned so we used Microsoft's bilingual sentence aligner 6 (Moore, 2002) for their complete alignment. Empty lines, hyperlinks, parenthesis, white spaces if present at the beginning of sentences were removed. Sentences having more than 120 tokens were dropped using Moses cleaning scripts  (Koehn et al., 2007) , punctuation and normalization was also applied. Table  2  shows our corpus sizes in terms of number of sentences (after cleaning). For our French to English systems, we tokenized the corpora using Moses tokenizer 7 . Byte Pair Encoding (BPE) sub word units with a vocabulary of 32K units were computed on true cased data using subword-nmt  (Sennrich et al., 2015)  . BertTokenizer 8 was only used for our submitted English to French system.  

 ID 

 Training and Parameters We used Fairseq  (Ott et al., 2019) , an open-source toolkit for training simple transformer  (Vaswani et al., 2017)  model and Bert-nmt  9  for training BERT-fused NMT systems. Our experiments can be grouped in three categories depending upon the corpora used during training and their training approach. I) Models trained using all the indomain corpora provided by WMT. II) Models trained on all the in-domain WMT corpora with addition of in-domain corpus retrieved from out-ofdomain corpora using IR. III) Models fine tuned on Medline abstracts and titles (since test corpus is from Medline), from few models built in second category. We used transformer base  (Vaswani et al., 2017)    

 Experiments and Results In this section we report the details of the experiments we performed for our participation in the WMT20 Biomedical task. We performed several different experiments to investigate the performance of NMT with different training approaches. Several different models were trained for French to English translation direction and one model was trained for English to French translation direction. The experiments were conducted as an extension of our last year's submission  (Hira et al., 2019)  with two different objectives. First, to investigate the performance of BERT-fused NMT over state-ofthe art transformer model and the other to explore the effect of out-of-domain corpus used for selective data training. We evaluated our models on Medline 18 and Medline 19 test sets, scores were calculated using sacrebleu  (Post, 2018) . 

 Corpus Selection for Selective Data training The significant gains in performance due to selective data training, as achieved in our WMT19 participation moved us to explore further to catego- rize which out-of-domain corpus is a better choice. ID We extended out-of-domain corpora to four different resources for selective data training. Alongwith News Commentary, which was also used in WMT19 participation, we extended the list with WikiPedia corpus, Books corpus and back translated FrWikipediaMono corpus. These were used to build four different sets of models from S1 to S4 as listed in Table  3 . Adding the IR retrieved data has unanimously helped improve the scores to almost 3 BLEU points on both test sets. These models were trained using WMT20 indomain corpora with addition of selective top-1, top-2 and top-3 retrieved IR sentences. S1 represents models built using additional News commentary IR corpus. Best scores were obtained on top-2 yielding 33.1 and 36.8 BLEU points on Medline 18 and Medline 19 test sets. S2 consists of models trained on additional Books IR corpus and best scores were again achieved on top-2 giving 32.9 and 37.0 points on Medline 18 and Medline 19 test sets. S3 comprises of models trained using additional WikiPedia IR corpus that reveal change in trend by giving best points 33.1 on top-1 for Medline 18 and 37.2 on top-2 for Medline 19 test sets. Similarly, systems represented by S4 show the effect of adding back translated FrWikipedia-Mono IR corpus in training set, that followed the trend of S1 and S2 giving best points 32.5 and 36.9 on top-2 for Medline 18 and Medline 19 respectively. We can safely conclude that top-2 IR retrieved sentences give us the best score. As for the effect of domain/type of the corpus used for IR, we don't see any significant advantage of any corpus over the other. For example, News Commentary and Books are very different corpora, but still sentences from both the corpora yield more or less the same improvement. Same is the case with WikiPedia, whether parallel or monolingual. This is an expected outcome as the IR process retrieves the sentences most relevant to the query sentence (Medline titles in our case). 

 BERT-fused NMT To target our second objective, investigation of BERT-fused NMT performance over transformer model, we trained three models using in-domain data provided by WMT20 Bio-medical translation task; M 1, M 2 and M 3. And four models, R1 to R4, using additional IR data, for French to English translation direction. Whereas 1 model (M 4) for English to French translation direction, as shown in table  4 . M 1 was trained with simple transformer architecture without BERT fusion and it scored 33.2 and 36.3 BLEU points on Medline 18 and Medline 19 test sets respectively. M 2 was trained under BERT-fused NMT setting with cased multilingual BERT base fused in transformer architecture. This model yielded 29.5 BLEU score on Medline 18 and 32.6 BLEU score on Medline 19 test set. Unexpectedly M 2, despite being trained in BERT-fused NMT setting, didn't show improvements in BLEU points over simple transformer model (M 1). One reason of this unexpected decrease in the BLEU scores of M 2 over M 1 could be the use of BERT trained on general domain. It seems that BERT trained on much huge general domain corpus has suppressed the learned parameters from in-domain training corpus. M 3 was trained as similar to M 2 but with uncased BERT, to explore the difference in the performance of cased and uncased BERT model, and it showed little improvement than M 2 on Medline 18 test data with a difference of only 0.1 BLEU points whereas an increase of 2.1 BLEU points on Medline 19 test set, as listed in Table  4 . Based on this result, we selected uncased BERT model for our further experiments. Further, we tried to evaluate the performance of selective data training in BERT-fused NMT setting, and trained four models for this investigation as shown in Table  4 . R1 was trained over in-domain WMT20 corpus concatenated with top-2 queried all IR data, since these proved to be most beneficial as shown by the results from section 4.  

 Related Work Numerous challenges arise when dealing with biomedical data used for translation due to limited size of corpus and unstructured alignments. Various approaches have been adopted by researchers in WMT biomedical translation.  (Khan et al., 2018 ) submitted a NMT system that combined in-domain data set and used transfer learning approach to train the model along with ensemble learning.  (Huck et al., 2018)  trained by using transformer architecture using biomedical and news domain and employed cascaded word segmentation along with BPE.  (Tubay and Costa-juss?, 2018)  emphasize on using multi-source approach like Romance languages with in-domain data by implementing transformer architecture using OpenNMT in PyTorch.  (Carrino et al., 2019)  created terminology list for biomedical words using BabelNet API, inserted the information at a token level and trained NMT system using transformer model  (Vaswani et al., 2017) .  (Hira et al., 2019)  used selective learning for building additional corpus from out-of-domain data and incorporated transfer learning approach by using recurrent encoder decoder NN model for training of in-domain biomedical data.  (Peng et al., 2019)  trained their Transformer model on in-domain and out-of-domain data for six translations using transfer learning methods. The model used attention mechanism along with RELU activation function yielding better results for in-domain biomedical data.  (Saunders et al., 2019)  used transfer learning using Bayesian Interpolation for multi-domain data for ensemble weighting.  participated in WMT19 with four translation directions by creating concatenating corpora from UMLS, out-of-domain and in-domain data and trained the systems using Transformer model. 

 Conclusion In this paper, we present our submission for WMT20 Biomedical tasks. Our model trained for English to French language direction ranked third in official scores provided by WMT20. We trained different models to investigate the performance of BERT-fused NMT over transformer model and to explore the effect of selective data training in BERT-fused NMT for French to English language direction. Results show decline in performance of BERT-fused NMT models over transformer architecture as general domain BERT suppressed the learned parameters from in-domain training corpus. BERT-fused models yielded better results when fine tuned on in-domain corpus and trained with IR data. Figure 1 : 1 Figure 1: Data selection Architecture. 
