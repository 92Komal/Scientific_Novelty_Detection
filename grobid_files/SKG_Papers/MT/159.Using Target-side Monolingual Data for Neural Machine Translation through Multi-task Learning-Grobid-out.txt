title
Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning

abstract
The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.

Introduction In recent years, neural encoder-decoder models  (Kalchbrenner and Blunsom, 2013;  have significantly advanced the state of the art in NMT, and now consistently outperform Statistical Machine Translation (SMT)  (Bojar et al., 2016) . However, their success hinges on the availability of sufficient amounts of parallel data, and contrary to the long line of research in SMT, there has only been a limited amount of work on how to effectively and efficiently make use of monolingual data which is typically amply available. We propose a modified neural sequence-to-sequence model with atten-tion  Luong et al., 2015b)  that uses multi-task learning on the decoder side to jointly learn two strongly related tasks: target-side language modeling and translation. Our approach does not require any pre-translation or pre-training to learn from monolingual data and thus provides a principled way to integrate monolingual data resources into NMT training. 

 Related Work G?lc ?ehre et al.  (2015)  investigate two ways of integrating a pre-trained neural Language Model (LM) into a pre-trained NMT system: shallow fusion, where the LM is used at test time to rescore beam search hypothesis, requiring no additional finetuning and deep fusion, where hidden states of NMT decoder and LM are concatenated before making a prediction for the next word. Both components are pre-trained separately and fine-tuned together. More recently,  Sennrich et al. (2016)  have shown significant improvements by back-translating target-side monolingual data and using such synthetic data as additional parallel training data. One downside of this approach is the significantly increased training time, due to training of a model in the reverse direction and translation of monolingual data. In contrast, we propose to train NMT models from scratch on both bilingual and target-side monolingual data in a multi-task setting. Our approach aims to exploit the signals from target-side monolingual data to learn a strong language model that supports the decoder in making translation decisions for the next word. Our approach further relates to  Zhang and Zong (2016) , who investigate multi-task learning for sequenceto-sequence models by strengthening the encoder using source-side monolingual data. A shared encoder architecture is used to predict both, transla-tions of parallel source sentences and permutations of monolingual source sentences. In this paper we focus on target-side monolingual data and only update encoder parameters based on existing parallel data. In a broader context, multi-task learning has shown to be effective in the context of sequenceto-sequence models  (Luong et al., 2015a) , where different parts of the network can be shared across multiple tasks. 

 Neural Machine Translation We briefly recap the baseline NMT model  Luong et al., 2015b)  and highlight architectural differences of our implementation where necessary. Given source sentence x = x 1 , ..., x n and target sentence y = y 1 , ..., y m , NMT models p(y|x) as a target language sequence model, conditioning the probability of the target word y t on the target history y 1:t?1 and source sentence x. Each x i and y t are integer ids given by source and target vocabulary mappings, V src , V trg , built from the training data tokens. The target sequence is factorized as: p(y|x; ?) = m t=1 p(y t |y 1:t?1 , x; ?). (1) The model, parameterized by ?, consists of an encoder and a decoder part . For training set P consisting of parallel sentence pairs (x, y), we minimize the cross-entropy loss w.r.t ?: L ? = (x,y)?P ? log p(y|x; ?). (2) Encoder Given source sentence x = x 1 , ..., x n , the encoder produces a sequence of hidden states h 1 . . . h n through an Recurrent Neural Network (RNN), such that: ? ? h i = f enc (E S x i , ? ? h i?1 ), (3) where h 0 = 0, x i ? {0, 1} |Vsrc| is the one-hot encoding of x i , E S ? R e?|Vsrc| is a source embedding matrix with embedding size e, and f enc some non-linear function, such as the Gated Rectified Unit (GRU)  or a Long Short-Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  network. Attentional Decoder The decoder also consists of an RNN to predict one target word at a time through a state vector s: s t = f dec ([E T y t?1 ; st?1 ], s t?1 ), (4) where y t?1 ? {0, 1} |Vtrg| is the one-hot encoding of the previous target word, E T ? R e?|Vtrg| the target word embedding matrix, f dec an RNN, s t?1 the previous state vector, and st?1 the sourcedependent attentional vector. The initial decoder hidden state is a non-linear transformation of the last encoder hidden state: s 0 = tanh(W init h n + b init ). The attentional vector st combines the decoder state with a context vector c t : st = tanh(W s[s t ; c t ]), (5) where c t is a weighted sum of encoder hidden states: c t = n i=1 ? ti h i and brackets denote vector concatenation. The attention vector ? t is computed by an attention network  Luong et al., 2015b) : ? ti = sof tmax(score(s t , h i )) score(s, h) = v a tanh(W u s + W v h). (6) The next target word y t is predicted through a softmax layer over the attentional vector st : p(y t |y 1:t?1 , x; ?) = sof tmax(W o st + b o ) (7) where W o maps st to the dimension of the target vocabulary. Figure  1a  depicts this decoder architecture. Note that source information from c indirectly influences the states s of the decoder RNN as it takes s as one of its inputs. 

 Incorporating Monolingual Data 

 Separate Decoder LM layer The decoder RNN (Figure  1a ) is essentially a targetside language model, additionally conditioned on source-side sequences. Such sequences are not available for monolingual corpora and previous work has tried to overcome this problem by either using synthetically generated source sequences or using a NULL token as the source sequence  (Sennrich et al., 2016) . As previously shown empirically, the model tends to "forget" source-side information if trained on much more monolingual than parallel data.  In our approach we explicitly define a sourceindependent network that only learns from targetside sequences (a language model), and a sourcedependent network on top, that takes information from the source sequence into account (a translation model) through the attentional vector s. Formally, we modify the decoder RNN of Equation  4 to operate on the outputs an LM layer, which is independent of any source-side information: s t = f dec ([r t ; st?1 ], s t?1 ) (8) r t = f lm (E T y t?1 , r t?1 ) (9) Figure  1b  illustrates this separation graphically. 

 Multi-task Learning The separation from above allows us to train the target embeddings E T and f lm parameters from monolingual data, concurrent to training the rest of the network on bilingual data. Let us denote the source-independent parameters by ?. We connect a second loss to f lm to predict the next target word also conditioned only on target history information (Figure  1c ). Parameters for softmax layers are shared such that predictions of the LM layer are given by: p(y t |y 1:t?1 , ?) = sof tmax(W o r t + b o ). (10) Formally, for a heterogeneous data set Z = {P, M}, consisting of parallel and monolingual sentences (x, y), (y), we optimize the following joint loss: L ?,? = 1 |P| (x,y)?P ? log p(y|x; ?) +? 1 |M| y?M ? log p(y; ?), (11) where the source-independent parameters ? ? ? are updated by gradients from both mono-and parallel data examples, and source-dependent parameters ? are updated only through gradients from parallel data examples. ? ? 0 is a scalar to influence the importance of the monolingual loss. In practice, we construct mini-batches of training examples, where 50% of the data is parallel, and 50% of the data is monolingual and set ? = 1. Since parts of the decoder are shared among both tasks and we optimize both loss terms concurrently, we view this approach as an instance of multi-task learning rather than transfer learning, where optimization is typically carried out sequentially. 

 Experiments We conduct experiments for three different language pairs in the news domain: FR?EN, EN?DE, and CS?EN. 

 Data For   1 : BLEU/METEOR/TER scores on test sets for different language pairs. For BLEU and METEOR higher is better. For TER lower is better. WMT2016  (Bojar et al., 2016) . For FR?EN we use newscommentary-v9 as bilingual data, NewsCrawl 2009-13 as monolingual data, and news development and test sets from WMT 2014  (Bojar et al., 2014) . The number of sentences for these corpora is shown below:  EN?DE 242, 770 51, 315, 088 FR?EN 183, 251 51, 995, 709 CS?EN 191, 432  27,236,445 Data Set bilingual monolingual 

 Experimental Setup We tokenize all data and apply Byte Pair Encoding (BPE)  (Sennrich et al., 2015)  with 30k merge operations learned on the joined bilingual data. Models are evaluated in terms of BLEU  (Papineni et al., 2002) , METEOR  (Lavie and Denkowski, 2009)  and TER  (Snover et al., 2006)  on tokenized, cased test data. Decoding is performed using beam search with a beam of size 5. We implement all models using MXNet  (Chen et al., 2015)  1 . Baselines Our baseline model consists of a 1layer bi-directional LSTM encoder with an embedding size of 512 and a hidden size of 1024. The 1-layer LSTM decoder with 1024 hidden units uses an attention network with 256 hidden units. The model is optimized using Adam (Kingma and Ba, 2014) with a learning rate of 0.0003, no weight decay and gradient clipping if the norm exceeds 1.0. The batch size is set to 64 and the maximum sequence length to 100. Dropout  (Srivastava et al., 2014)  of 0.3 is applied to source word embeddings and outputs of RNN cells. We initialize all 1 Baseline systems are equivalent to an earlier version of Sockeye: https://github.com/awslabs/sockeye RNN parameters with orthogonal matrices  (Saxe et al., 2013)  and the remaining parameters with the Xavier  (Glorot and Bengio, 2010)  method. We use early stopping with respect to perplexity on the development set. We train each model configuration three times with different seeds and report average metrics across the three runs. Further, we train models with synthetic parallel data generated through back-translation  (Sennrich et al., 2016) . For this, we first train a baseline model in the reverse direction and then translate a random sample of 200k sentences from the monolingual target data. On the combined parallel and synthetic training data we train a new model with the same training hyper-parameters as the baseline. Language Model Layer The architecture with an additional source-independent LM layer (+LML) is trained with the same hyper-parameters and data as the baseline model. The LM RNN uses a hidden size of 1024. The multi-task system (+LML + MTL) is trained on both parallel and monolingual data. In practice, all +LML +MTL models converge before seeing the entire monolingual corpus and at about the same number of updates as the baseline. 

 Results Table  1  shows results on the held-out test sets. We observe that a separate LM layer does not significantly impact performance across all metrics. Adding monolingual data in the described multitask setting improves translation performance by a small but consistent margin across all metrics. Interestingly, the improvements from monolingual data are additive to the gains from ensembling of 3 models with different random seeds. However, the use of synthetic parallel data still outperforms our approach both in single and ensemble systems. While separating out a language model allowed us to carry out multi-task training on mixed data types, it constrains gradients from monolingual data examples to a subset of source-independent network parameters (?). In contrast, synthetic data always affects all network parameters (?) and has a positive effect despite source sequences being noisy. We speculate that training from synthetic source data may also act as a model regularizer. 

 Conclusion We proposed a way to directly integrate target-side monolingual data into NMT through multi-task learning. Our approach avoids costly pre-training processes and jointly trains on bilingual and monolingual data from scratch. While initial results show only moderate improvements over the baseline and fall short against using synthetic parallel data, we believe there is value in pursuing this line of research further to simplify training procedures. Figure 1 : 1 Figure 1: Illustration of the proposed decoder architecture. (a) Baseline model with a single-layer decoder RNN and attention (b) Addition of a source-independent LM layer that feeds into the source-dependent decoder (c) Multi-task setting next-word prediction from both layers; green softmax layers are shared. 
