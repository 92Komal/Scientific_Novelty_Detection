title
Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation

abstract
In this paper, we introduce our participation in the WMT 2019 Metric Shared Task. We propose a method to filter pseudo-references by paraphrasing for automatic evaluation of machine translation (MT). We use the outputs of off-the-shelf MT systems as pseudoreferences filtered by paraphrasing in addition to a single human reference (gold reference). We use BERT fine-tuned with paraphrase corpus to filter pseudo-references by checking the paraphrasability with the gold reference. Our experimental results of the WMT 2016 and 2017 datasets show that our method achieved higher correlation with human evaluation than the sentence BLEU (Sent-BLEU) baselines with a single reference and with unfiltered pseudo-references.

Introduction In general, automatic evaluation of MT is based on n-gram agreement between the system output and a manually translated reference of the source sentence. Therefore, automatic evaluation fails to evaluate a semantically correct sentence if the surface of the system output differs from that in the reference. To solve this problem, many automatic evaluation methods allow the use of multiple references that potentially cover various surfaces; in particular,  Finch et al. (2004)  reported that correlation between automatic evaluation results and human evaluation increases when multiple references are used for evaluation. However, owing to the time and costs involved in manually creating references, many datasets only include one reference per source sentence, which leads to improper translation evaluation, especially in the case of diverse machine translation systems. In order to obtain cheap references without any human intervention,  Albrecht and Hwa (2008)  used the outputs of off-the-shelf MT systems as pseudo-references; They showed that using mul-tiple references consisting of gold and pseudoreferences may yield higher correlation with human evaluation than using a single gold reference. However, because they did not consider the quality of the pseudo-references, this may result in using poor references. Thus, in some cases the correlation becomes worse when using multiple references consisting of gold and pseudo-references relative to only using a gold reference. To address the quality of pseudo-references, we filtered pseudo-references by checking the paraphrasability to the gold reference. Our approach can be applied to various MT evaluation metrics which can be evaluated with multiple references. The experimental results show that our method achieves higher correlation with human evaluation than the previous work. 2 Related Work  Albrecht and Hwa (2008)  showed that using the outputs of off-the-shelf MT systems as pseudoreferences in n-gram based metrics such as BLEU  (Papineni et al., 2002)  and METEOR  (Denkowski and Lavie, 2011 ) may yield higher correlation with human evaluation than using a gold reference. They use the outputs of off-theshelf MT systems as they are, whereas we filter them by paraphrasing the gold reference.  Kauchak and Barzilay (2006)  proposed a method to obtain a paraphrase of a gold reference that is closer in wording to the system output than the gold reference for MT evaluation. They evaluated an MT system using only the generated references, whereas we evaluated MT systems using multiple references, including those obtained by adding generated references to the gold reference. They generate a paraphrase of a gold reference, whereas we translate source sentences and identify whether the outputs are paraphrases of gold references. That is, they used only gold references whereas we used both source and gold reference information. 

 MT Evaluation Metric Using Filtered Multiple Pseudo-References 

 Overview Figure  1  shows the overview of our proposed method. The procedure of our proposed method is as follows. 1. Prepare off-the-shelf MT systems for generating pseudo-references. 2. Translate the source sentence in the evaluation data using the abovementioned MT systems. 3. Filter the outputs of off-the-shelf MT systems by checking the paraphrasability of being a paraphrase to the single gold reference. 4. Calculate the sentence evaluation score with multiple references obtained by adding filtered pseudo-references to the single gold references. 

 Automatic pseudo-reference generation Any MT system can be used as a pseudo-reference generation system except for the translation system to be evaluated. 1 There are no restrictions on the type of MT systems, such as neural machine translation (NMT) or statistical machine translation (SMT) systems, or the number of MT systems.  

 Filtering by paraphrasing We use Bidirectional Encoder Representations from Transformers (BERT)  (Devlin et al., 2019)  to filter pseudo-references by checking the paraphrasability with a gold reference. BERT is a new approach to pre-train language representations, and it obtains state-of-the-art results on a wide variety of natural language processing (NLP) tasks, including question answering (QA), semantic textual similarity (STS), natural language inference (NLI). The key to pre-training BERT is the prediction of masked words and of the next sentence. Masking words allows bidirectional learning, which improves joint training of language context relative to Embeddings from Language Models (ELMo)  (Peters et al., 2018) , which combines forward and backward training. Prediction of the next sentence leads to capturing the relationship between two sentences. Figure  2  shows the BERT model architec-cs-en de-en fi-en ru-en single reference 0.557 0.484 0.448 0.502 single reference + pseudo-references 0.565 0.499 0.543 0.456 single reference + filtered references (MAS) 0.576 0.473 0.517 0.469 single reference + filtered references (BERT) 0.589 0.519 0.572 0.490     For that reason, we use BERT to estimate the paraphrasability between pseudo-references and the gold reference. We fine-tune BERT with MRPC. The output of the classifier is the probability of the paraphrase from 0 to 1. We use pseudoreferences whose paraphrase probability is greater than 0.5. 

 Experiments 

 Data We used the segment-level evaluation datasets of Czech-English (cs-en), German-English (deen), Finnish-English (fi-en), Russian-English (ruen) language pair from WMT 2016  (Bojar et al., 2016)  and 2017  (Bojar et al., 2017) . The datasets consist of 560 pairs of sources and references, along with the outputs of each system and human evaluation scores. 

 Off-the-shelf MT systems We used Google Translation 2 and Bing Microsoft Translator 3 as MT systems to generate pseudoreferences. We chose these two MT systems because they are widely used, easy to use, and well known to have good performance. We automatically translated source files using each translation API. 

 Fine-tuning BERT with MRPC We use the pre-trained BERT-Base Uncased model 4 , which has 12 layer, 768 hidden, 12 heads system output gymnastics and freestyle exercises -where bayles defends the title of world champion -lie in the veil . gold reference balance beam and floor exercise -where biles is the defending world champion -lay in wait . and 110M parameters. We fine-tuned BERT with MRPC. MRPC is a dataset extracted from web news articles along with human annotations indicating whether each pair is a paraphrase. If the pair is paraphrase, the label is 1, if not, the label is 0. The original dataset consists of 4,077 sentences for training and 1,726 sentences for testing. We divided the test set in half and used it as development data. The numbers of sentences in each corpus and the accuracy of the fine-tuned BERT model are listed in Table  3 . Figure  3  shows the histogram of paraphrase score of pseudo-references in the fi-en language pair of WMT 2016. Due to the use of high quality MT systems, more than 50% of the pseudoreferences have paraphrase scores between 0.9 and 1.0. The same trend was observed in all languages and years. 

 Evaluation We calculated the SentBLEU score with system output and multiple references which consisted of a single gold reference and pseudo-references. The SentBLEU is computed using the sentencebleu.cpp 5 , a part of the Moses toolkit. It is a smoothed version of BLEU  (Lin and Och, 2004) . We followed the tokenization method for each year's dataset. We measured Pearson correlation identically to WMT 2016 and WMT 2017 between the automatic and human evaluation scores. In order to compare with our method, we also performed filtering by Maximum Alignment Similarity (MAS)  (Song and Roth, 2015) , which is one of the unsupervised sentence similarity measures based on alignments between word embeddings and is known to achieve good performance on Semantic Textual Similarity (STS) task. We used GloVe 6  (Pennington et al., 2014)  as word embeddings. We used pseudo-references whose MAS score is higher than 0.8. 

 Results Tables  1 and 2  show the segment-level Pearson correlation coefficients between automatic and human evaluation scores. The result shows that our proposed method outperforms the baselines except in the case of the ru-en language pair in WMT 2016 and filtering by MAS does not produce any consistent result. 

 Discussion Table  4  shows an example of pseudo-references with BERT's paraphrase score for the ru-en language pair in WMT 2017. The pseudo-reference from Bing translation has a low paraphrase score because "biles" in the gold reference remains as "bayles" in the pseudo-reference, and "floor exercise" became "freestyle exercise" in Bing translation. In the unfiltered method, the BLEU score is unreasonably high because the surface of the pseudo-reference from Bing translation is similar to the output sentence. Filtering the pseudoreferences prevents the problem. The pseudoreference from Google translation has different surfaces but carry the same meaning as in the gold reference. Our filtering method correctly retains the sentence because BERT assigned high paraphrase score. 6 https://nlp.stanford.edu/projects/glove/ Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors) 

 Conclusions We proposed a method to filter pseudo-references in terms of paraphrasability with a gold reference that addresses the problem of using poor pseudo-references from previous work  (Albrecht and Hwa, 2008) . We use BERT finetuned with MRPC to filter pseudo-references. By filtering pseudo-references in terms of paraphrasability with a gold reference, we can keep the references having the same meaning with the gold reference but different surface and solve the problem of using poor pseudo-reference from previous work. The experimental results show that our method outperforms baselines. Figure 1 : 1 Figure 1: Overview of the proposed method. 

 Figure 2 : 2 Figure 2: BERT model architecture for sentence pair classification. 

 single reference + filtered references (MAS) 0.524 0.586 0.650 0.517 single reference + filtered references 0.555 0.580 0.671 0.545 

 ture for sentence pair classification. In classification tasks where labels are attached to sentence pairs, BERT encodes sentence pairs together with a [CLS] token for classification and a [SEP] token for sentence boundaries; The output of the [CLS] token is used for the input of classifier of a feedforward neural network with softmax. BERT achieves state-of-the-art performance in a paraphrase identification task on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) with this architecture. 

 Figure 3 : 3 Figure 3: Histograms of paraphrase score of pseudoreferences in the fi-en language pairs of WMT 2016. 

 floor exercises -where biles defends the world champion title -lie in wait . (0.994) pseudo-reference (Bing) gymnastic log and freestyle exerciseswhere the bayles defends the title of world championlie in ambush . (0.215) human score: -1.497; SentBLEU: single reference: -1.118, without filtering: -0.335, filtering: -1.662Table 4: Example of pseudo-references in ru-en language pair of WMT 2017; The value in parentheses at the end of each pseudo-reference indicates the paraphrase score by BERT. Each score is standardized according to the mean and standard deviation to compare human evaluation and each SentBLEU score. 

 Table 1 : 1 Segment-level  Pearson correlation between SentBLEU and human evaluation scores in WMT 2016. 

 Table 2 : 2 Segment-level  Pearson correlation between SentBLEU and human evaluation scores in WMT 2017. corpus train dev test Accuracy MRPC 3,669 408 1726 0.845 

 Table 3 : 3 Numbers of sentences in each split of MRPC and accuracy of BERT. 

			 If the system to be evaluated were used as a pseudreference generation system, the output would be used as a reference. 

			 https://translate.google.com/ 3 https://www.bing.com/translator 4 https://github.com/google-research/bert 

			 https://github.com/moses-smt/ mosesdecoder/blob/master/mert/sentence-bleu.cpp
