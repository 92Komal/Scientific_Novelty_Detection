title
NLPRL System for Very Low Resource Supervised Machine Translation

abstract
This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario.

Introduction We report the results for our system that was used for our participation in the WMT20 shared task  (Barrault et al., 2019)  on very low resource Machine Translation (MT). The MT systems were built for the language pair Upper Sorbian (HSB) and German (GER) in both translation directions. The Sorbian languages are the West Slavic branch of the Indo-European languages, which have further categorized into two closely related languages, Upper Sorbian and Lower Sorbian. The categories of this language are recognized as a different and distinct language in the European Charter for Regional or Minority languages  (Dolowy-Rybinska, 2011) . Upper Sorbian is a minority language of Germany that is spoken by 10, 000 to 15, 000 speakers  (Elle, 2010) , although this number is continually declining (Do?owy-Rybi?ska, 2018). To counter this, attempts are being made to increase the number of Sorbian speakers through bilingual educational scenarios and MT 1 . Low resource MT was being attempted even before Neural Machine Translation (NMT) became the state-of-the-art. Several methods are used to improve the accuracy and quality of the lowresource SMT systems by using comparable corpora  (Irvine and Callison-Burch, 2013; Babych et al., 2007) , pivot language (English or non-English) technique  (Ahmadnia et al., 2017; Paul et al., 2013) , and using related resource-rich language  (Nakov and Ng, 2012) . We use a byte-level version of Byte Pair Encoding based model with a Transformer for our experiments. The main motivation was to try out this model for the shared task and see how it works under a shared task setting. 

 Background NMT is an end-to-end learning system  (Bahdanau et al., 2015) , based on the data-driven approach of machine translation, that requires a massive amount of parallel data for training. To overcome the lack of such data, several techniques have been tried out which are based on semi-supervised learning  (Zheng et al. (2019) ), unsupervised learning  (Sun et al. (2020) ), data augmentation  (Siddhant et al. (2020) ), transfer learning (Aji (  2020 )), meta-learning  (Li et al. (   2020 )), pivot-based  (Kim et al. (2019) ), and multilingual machine translation  (Dabre et al. (2020) ). A model-agnostic meta-learning algorithm  (Finn et al., 2017)  for low-resource NMT exploits the multilingual high-resource language tasks  (Gu et al., 2018b) .  Gu et al. (2018a)  achieved significant improvement in performance by utilizing a transfer-learning approach for extremely low resource languages. Another proposed solution is to use word segmentation units, e.g. characters  (Chung et al., 2016) , mixed word/characters  (Luong and Man-ning, 2016) , or more intelligent sub-words  (Sennrich et al., 2016) . It is claimed that an NMT model using such an approach is capable of openvocabulary translation by encoding rare and unknown words as sequences of sub-word units. The purpose of our experiments was to try out a supervised NMT system for the low resource language like HSB to GER and vice-versa for the WMT20 shared task. 

 System Description The standard Transformer architecture proposed by  Vaswani et al. (2017)  is used for this experiment. This architecture is able to handle long-term dependencies among input tokens, output tokens and between input-output by multi-head attention mechanism. Our method based on the model architecture of , which had used the Byte-level BPE (BBPE). The BBPE encoding is deployed on the Byte Pair Encoding (BPE)  (Sennrich et al., 2016) , which is a subword algorithm to find a way to represent the given entire text dataset with a small number of tokens. BPE tries to find a balance between characterand word-level hybrid representations, enabling the encoding of any rare words in the vocabulary with appropriate subword tokens without introducing any "unknown" tokens. These segmented byte sequences are encoded into variable-length tokens, i.e., n-grams, which leads to the generation of the BPE vocabulary with byte n-grams. Before being fed to the Transformer model, the learned BBPE passes through bidirectional GRU, which enables to retain contextualization between byte representation of BPE. 

 Experimental Setup We use the Fairseq 2  (Ott et al., 2019)  library to train the Transformer with the same learning rate as in the original paper. 

 Dataset and Preprocessing Our models were trained on the data provided by the Workshop on Machine Translation (WMT) 2020. The statistics about the training, validation and test sets are 60000, 2000 and 2000, respectively for both directional pairs (HSB -GER). We obtained 1727916 and 1710293 tokens of the GER and HSB, respectively, from the train set for preprocessing. The BPE vocabulary, Byte vocabulary and Character vocabulary are 16384, 2048 and 4096, respectively, for generating binary dataset by using fairseq-preprocess. The BBPE used as a subword BPE tokenizer, where preprocessing was performed using lowercasing only. This is beneficial from the low resource point of view, but it loses the case information for German, which could have affected the results. 

 Training Details We trained the Transformer model with Bi-GRU embedding, in which contextualization using the number of encoder and decoder layers are 2 with the dropout value 0.3. We trained our model with a batch size of 100, with the aid of Adam optimizer at 0.0005 learning rate. The learning rate has warmup update by 4000 to label smoothed cross-entropy loss function with label-smoothing value 0.1. 

 Results and Analysis The BBPE based Transformer model was evaluated on the blind test set at five different metrics provided by the task organizer, namely BLEU  (Papineni et al., 2002) , BLEU-cased, TER  (Snover et al., 2006) , BEER2.0  (Stanojevi? and Sima'an, 2014) , and CharacTER  (Wang et al., 2016) . The obtained metrics score for each pair to each experiment is specified in Table  1 . The prediction of the test set was generated by performing the best validation checkpoint. However, while comparing the BLEU score of the valid set with the test set, we obtained a difference of +3.21 for HSB?GER and +0.15 for GER?HSB pairs. Before submitting the predictions of the test set, the BLEU scores of best and last checkpoints were almost equal, as shown in Table  2 . Moreover, the vocabulary size plays a crucial role in data-driven approaches of MTs as well. Hence, we have increased the vocabulary size from 2048 to 4096 for generating BBPE, which led to a small decrement in the BLEU score. One possible reason for such decrement is the small vocabulary size that generates generalized BBPE for low-resource language. 

 Conclusion and future work We have report the results for a Transformer-based MT system for the pair of HSB?GER in very low resource settings.  Table 1 : 1 The introduced MT system works on Byte-level Byte Pair Encoding (BBPE), which yields 48.4 and 46.5 on HSB?GER and Obtained scores of different metrics on the test set, provided by the task organizers Pair BLEU BLEU cased TER BEER2.0 CharacTER HSB-GER 48.4 47.9 0.383 0.706 0.335 GER-HSB 46.5 45.9 0.389 0.696 0.323 Valid Test Vocab Pair Checkpoint Checkpoint Checkpoint (last) (best) (best) 2048 HSB-GER GER-HSB 45.92 46.62 45.19 46.35 48.4 46.5 4096 HSB-GER GER-HSB 45.77 46.96 45.09 46.24 -- 
