title
Look Harder: A Neural Machine Translation Model with Hard Attention

abstract
Soft-attention based Neural Machine Translation (NMT) models have achieved promising results on several translation tasks. These models attend all the words in the source sequence for each target token, which makes them ineffective for long sequence translation. In this work, we propose a hard-attention based NMT model which selects a subset of source tokens for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a reinforcement learning algorithm coupled with reward shaping strategy to efficiently train it. Experimental results show that the proposed model performs better on long sequences and thereby achieves significant BLEU score improvement on English-German (EN-DE) and English-French (EN-FR) translation tasks compared to the soft-attention based NMT.

Introduction In recent years, soft-attention based neural machine translation models  (Bahdanau et al., 2015; Gehring et al., 2017; Hassan et al., 2018)  have achieved state-of-the-art results on different machine translation tasks. The soft-attention mechanism computes the context (encoder-decoder attention) vector for each target token by weighting and combining all the tokens of the source sequence, which makes them ineffective for long sequence translation  (Lawson et al., 2017) . Moreover, weighting and combining all the tokens of the source sequence may not be required -a few relevant tokens are sufficient for each target token. Different attention mechanisms have been proposed to improve the quality of the context vector. For example,  Luong et al. (2015) ;  Yang et al. (2018)  proposed a local-attention mechanism to selectively focus on a small window of source tokens to compute the context vector. Even though local-attention has improved the translation quality, it is not flexible enough to focus on relevant tokens when they fall outside the specified window size. To overcome the shortcomings of the above approaches, we propose a hard-attention mechanism for a deep NMT model  (Vaswani et al., 2017) . The proposed model solely selects a few relevant tokens across the entire source sequence for each target token to effectively handle long sequence translation. Due to the discrete nature of the hard-attention mechanism, we design a Reinforcement Learning (RL) algorithm with reward shaping strategy  (Ng et al., 1999)  to train it. The proposed hard-attention based NMT model consistently outperforms the soft-attention based NMT model  (Vaswani et al., 2017) , and the gap grows as the sequence length increases. 

 Background A typical NMT model based on encoder-decoder architecture generates a target sequence y = {y 1 , ? ? ? , y n } given a source sequence x = {x 1 , ? ? ? , x m } by modeling the conditional probability p(y|x, ?). The encoder (? e ) computes a set of representations Z = {z 1 , ? ? ? , z m } ? R m?d corresponding to x and the decoder (? d ) generates one target word at a time using the context vector computed using Z. It is trained on a set of D parallel sequences to maximize the log likelihood: J 1 (?) = 1 N D i=1 log p y i |x i ; ? , (1) where ? = {? e , ? d }. In recent years, among all the encoder-decoder architectures for NMT, Transformer  (Vaswani et al., 2017)  has achieved the best translation quality . The encoder and decoder blocks of the Transformer are composed of a stack of N (=6) identical layers as shown in Figure  1a . Each layer in the encoder contains two sublayers, a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Each decoder layer consists of three sublayers; the first and third sub-layers are similar to the encoder sub-layers, and the additional second sub-layer is used to compute the encoderdecoder attention (context) vector based on the soft-attention based approaches  (Bahdanau et al., 2015; Gehring et al., 2017) . Here we briefly describe the soft computation of encoder-decoder attention vector in the Transformer architecture. Please refer  Vaswani et al. (2017)  for the detailed architecture. For each target word ?t , the second sub-layer in the decoder computes encoder-decoder attention a t based on the encoder representations, Z. In practice we compute the attention vectors simultaneously for all the time steps by packing ?t 's and z i 's in to matrices. The soft attention of the encoder-decoder, A i , for all the decoding steps is computed as follows: A i ( ? i?1 , Z) = softmax ? i?1 Z ? d Z, ?i ? N, (2) where d is the dimension and ? i?1 ? R n?d is the decoder output from the previous layer. 

 Proposed Model Section 3.1 introduces our proposed hard-attention mechanism to compute the context vector for each target token. We train the proposed model by designing a RL algorithm with reward shaping strategy -described in Section 3.2. 

 Hard Attention Instead of computing the weighted average over all the encoder output as shown in Eq. 2, we specifically select a subset of encoder outputs (z i 's) for the last layer (N ) of the decoder using the hard-attention mechanism as shown in  Figure 1a . This allows us to efficiently compute the encoder-decoder attention vector for long sequences. To compute the hard-attention between the last layers of the Transformer encoder-decoder blocks, we replace the second sub-layer of the decoder block's last layer with the RL agent based attention mechanism. Overview of the proposed RL agent based attention mechanism is shown in Figure  1b  and computed as follows: First, we learn the projections ? N ?1 , Z for ? N ?1 and Z as, ? N ?1 = tanh(W d 2 (W d 1 ? N ?1 + b d 1 ) + b d 2 ), Z = tanh(W e 2 (W e 1 Z + b e 1 ) + b e 2 ). We then compute the attention scores S as, S( ? N ?1 , Z) = ? N ?1 Z. (3) We apply the hard-attention mechanism on attention scores (S) to dynamically choose multiple relevant encoder tokens for each decoding token. Given S, this mechanism generates an equal length of binary random-variables, ? = {? 1 , ? ? ? , ? m } for each target token, where ? i = 1 indicates that z i is relevant whereas ? i = 0 indicates that z i is irrelevant. The relevant tokens are sampled using bernoulli distribution over each ? i for all the target tokens. This hard selection of encoder outputs introduces discrete latent variables and estimating them requires RL algorithms. Hence, we design the following reinforcement learner policy for the hard-attention for each decoding step t. ? t (r|s t , ? h ) = ? t i (4) where ? t i ? ? represents the probability of a encoder output (agent's action) being selected at time t, and s t ? S is the state of the environment. Now, the hard encoder-decoder attention, ?, is calculated as, follows: ? = tanh(W ? 2 (W ? 1 Z + b ? 1 ) + b ? 2 ) (5) ? = ? ? (6) Unlike the soft encoder-decoder attention A in Eq. 2, which contains the weighted average of entire encoder outputs, the hard encoder-decoder attention ? in Eq. 6 contains information from only relevant encoder outputs for each decoding step. 

 Strategies for RL training The model parameters come from the encoder, decoder blocks and reinforcement learning agent, which are denoted as ? e , ? d and ? h respectively. Estimation of ? e and ? d is done by using the objective J 1 in Eq. 1 and gradient descent algorithm. However, estimating ? h is difficult given their discrete nature. Therefore, we formulate the estimation of ? h as a reinforcement learning problem and design a reward function over it. An overveiw of the proposed RL training is given in Algorithm 1. We use BLEU  (Papineni et al., 2002)  score between the predicted sequence and the target sequence as our reward function, denoted as R(y , y), where y is the predicted output sequence. The objective is to maximize the reward with respect to the agent's action in Eq. 4 and defined as, Compute the reward r t using Eq. 8 J 2 (? h ) = n t=1 logp(r|s t , ? h )R(y , y) (7) 9 end 10 Compute J(?) = ?(J 1 (? e , ? d ) + J 2 (? h )) using Eq. 1 and Eq. 9 11 Update the parameters ? with gradient descent: ? = ? ? ?J(?) 12 end 13 Return: ? Reward Shaping To generate the complete target sentence, the agent needs to take actions at each target word, but only one reward is available for all these tens of thousands of actions. This makes RL training inefficient since the same terminal reward is applied to all the intermediate actions. To overcome this issue we adopt reward shaping strategy of  Ng et al. (1999) . This strategy assigns distinct rewards to each intermediate action taken by the agent. The intermediate reward, denoted as r t (y t , y), for the agent action at decoding step t is computed as: r t (y t , y) = R(y 1..t , y) ? R(y 1..t?1 , y) (8) During training, we use cumulative reward n t=1 r t (y t , y) achieved from the decoding step t to update the agent's policy. Entropy Bonus We add entropy bonus to avoid policy to collapse too quickly. The entropy bonus encourages an agent to take actions more unpredictably, rather than less so. The RL objective function in Eq. 7 becomes, ?2 (? h ) = J 2 (? h ) + ?H(? t (r|s t , ? h )) (9) We approximate the gradient ? ? h ?2 (? h ) by using REINFORCE  (Williams, 1992)     4 Experimental Results 

 Datasets We conduct experiments on WMT 2014 English-German (EN-DE) and English-French (EN-FR) translation tasks. The approximate number of training pairs in EN-DE and EN-FR datasets are 4.5M and 36M respectively; newstest2013 and newstest2014 are used as the dev and test sets. We follow the similar preprocessing steps as described in  Vaswani et al. (2017)  for both the datasets. We encode the sentences using word-piece vocabulary  (Wu et al., 2016)  and the shared source-target vocabulary size is set to 32000 tokens. 

 Results 

 Implementation Details We adopt the implementation of the Transformer  with transformer big settings. All the models are trained using 8 NVIDIA Tesla P40 GPUs on a single machine. The BLEU score used in the reward shaping is calculated similarly to  Bahdanau et al. (2017) ; all the n-gram counts start from 1, and the resulting score is mul-tiplied by the length of the target reference sentence. The beam search width (=4) is set empirically based on the dev set performance and ? in Eq. 9 is set to 1e-3 in all the experiments. 

 Models We compare the proposed model with the soft-attention based Transformer model  (Vaswani et al., 2017) . To check whether the performance improvements are coming from the hard-attention mechanism (Eq. 4) or from the sequence reward incorporated in the objective function (Eq. 7), we compare our work with previously proposed sequence loss based NMT method . This NMT method is built on top of the Transformer model and trained by combing cross-entropy loss and sequence reward (BLEU score). We also compare our model with the recently proposed Localness Self-Attention network  (Yang et al., 2018)  which incorporates a localness bias into the Transformer attention distribution to capture useful local context. 

 Main Results Table  1  shows the performance of various models on EN-DE and EN-FR translation tasks. These test set case sensitive BLEU scores are obtained using SacreBLEU toolkit 1  (Post, 2018) . The BLEU score difference between our hard-attention based Transformer model and the original soft-attention based Transformer model indicates the effectiveness of selecting a few relevant source tokens for each target token. The performance gap between our method and sequence loss based Transformer  shows that the improvements are indeed coming from the hard-attention mechanism. Our approach of incorporating hard-attention into decoder's top selfattention layer to select relevant tokens yielded better results compared to the Localness Self-Attention  (Yang et al., 2018)  approach of incorporating localness bias only to lower self-attention layers. It can be noted that our model achieved 29.29 and 42.26 BLEU points on EN-DE and EN-FR tasks respectively -surpassing the previously published models. Analysis To see the effect of the hard-attention mechanism for longer sequences, we group the sequences in the test set based on their length and compute the BLEU score for each group. Table  2  shows the number of sequences present in each group. Figure  2  shows that Transformer with hard attention is more effective in handling the long sequences. Specifically, the performance gap between our model (THA) and the original Transformer model (TSA) grows bigger as sequences become longer. 

 Related Work Even though RL based models are difficult to train, in recent years, multiple works  (Mnih et al., 2014; Choi et al., 2017; Yu et al., 2017; Narayan et al., 2018; Sathish et al., 2018; Shen et al., 2018)  have shown to improve the performance of several natural language processing tasks. Also, it has been used in NMT  Wu et al., 2017; Bahdanau et al., 2017)  to overcome the inconsistency between the token level objective function and sequence-level evaluation metrics such as BLEU. Our approach is also related to the method proposed by  Lei et al. (2016)  to explain the decision of text classifier. However, here we focus on selecting a few relevant tokens from a source sequence in a translation task. Recently, several innovations are proposed on top of the Transformer model to improve performance and training speed. For example, Shaw   proposed efficient training strategies. These improvements are complementary to the proposed method. Incorporating these techniques will further improve the performance of the proposed method. 

 Conclusion In this work, we proposed a hard-attention based NMT model which focuses solely on a few relevant source sequence tokens for each target token to effectively handle long sequence translation. We train our model by designing an RL algorithm with the reward shaping strategy. Our model sets new state-of-the-art results on EN-DE and EN-FR translation tasks. Figure 1: (a) Overview of hard-attention based Transformer network. (b) Overview of RL agent based hardattention and objective function. 
