title
An Iterative Knowledge Transfer NMT System for WMT20 News Translation Task

abstract
This paper describes our submission to the WMT20 News translation shared task in English to Japanese direction. Our main approach is based on transferring knowledge of domain knowledge and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. We then fine-tune the model with parallel data and in-domain synthetic data which is generated by iterative back-translation. For additional gain, we generate final results with an ensemble model and re-rank them with averaged models and language models. Through these methods, we achieve +5.42 BLEU score compared to the baseline model.

Introduction This paper describes our submission to the WMT20 News translation task in English to Japanese direction. In this year, English-Japanese directions have newly established in News Translation Shared Task. The English-Japanese translation is not easy to deal with because of the difference in word order and the rich morphological characteristics of Japanese. Nevertheless, recent architectures for Neural Machine Translation (NMT), such as Transformer  (Vaswani et al., 2017) , show reasonable results when we have enough parallel data. Unfortunately, however, there is not much in-domain parallel data provided for English-Japanese task. To solve this issue, in this paper, we suggest the iterative knowledge transfer system which pre-trains the model with in-domain monolingual data. Our system is based on Transformer architecture. We pre-train the model to transfer linguistic characteristics and domain knowledge of monolingual data. Although there are various pre-training methods for NMT, MASS  (Song et al., 2019 ) is adopted in our system since MASS pre-trains the encoder and the decoder jointly and uses both labeled data and unlabeled data as the training data. To supplement insufficient in-domain parallel data, we generate synthetic data by back-translation from in-domain monolingual data. We also add some noise to the synthetic data. We then pre-train the model with the synthetic parallel data for supervised method and the monolingual data for unsupervised way. In fine-tuning step, we train the model with parallel corpus and perform the backtranslation with in-domain data for iterative finetuning. In addition, we adopt an ensemble and averaging methods which are simple but very effective to improve performance in deep learning. With ensemble and average models, we apply noisy channel re-ranking which shows higher performance compared to R2L re-ranking  (Yee et al., 2019) . Through these methods, we achieve +5.42 BLEU score  (Papineni et al., 2002; Post, 2018)  compared to the baseline model. 

 Approach Our system aims to encourage knowledge extraction of domain knowledge and linguistic characteristics by iteratively performing pre-training and fine-tuning. In this section, we explain techniques we use in each step. 

 Pre-training strategy MASS is a masked sequence to sequence pretraining method for the encoder-decoder based language generation tasks  (Song et al., 2019) . The advantage of MASS is that it uses the encoderdecoder framework to predict the masked part given the masked sentence. Several consecutive tokens in a sentence are randomly masked; the encoder takes them as input, and the decoder is trained to predict masked tokens. This method allows MASS to learn the capability of representation extraction. In this paper, we adopt both supervised and unsupervised prediction methods of MASS. There are plenty of in-domain monolingual corpus but insufficient in-domain parallel corpus. Thus, we generate synthetic data by back-translation and apply supervised prediction task. In addition, we use large amount of out-domain monolingual corpus for unsupervised prediction task to encourage the ability of language modeling. Let x ? X as an monolingual source sentence, and m is the number of tokens of sentence x. We denote x \u:v as an modified sentence of x where its position u to v are masked, 0 < u < v < m. x u:v denotes the original sentence fragment of x from u to v. Those sentences can have different fragment positions u and v for each. In the sentence fragment, we replace each masked token to a special symbol [M], so the number of words in the sentence is not changed. Then, we train model with the masked sentence x \u:v to predict the sentence fragment x u:v . Supervised setting is used also where bilingual sentence pair (x, y) ? (X , Y) can be leveraged for pre-training. It is trained to predict y from the input x \u:v . The log likelihood in the entire setting is as follows: L(?; (X , Y)) = 1 |Y| (x,y)?(X ,Y) log P (y|x \u:v ; ?) + 1 |X | (x,y)?(X ,Y) log P (x|y \u:v ; ?) + 1 |X | x?X log P (x u:v |x \u:v ; ?) + 1 |Y| y?Y log P (y u:v |y \u:v ; ?) (1) P (y|x \u:v ; ?) and P (x|y \u:v ; ?) denote the probability of translating a masked sequence to another language. This prediction task encourages the encoder to extract meaningful representations of masked input tokens in order to predict the unmasked output sequence. 

 Noised back-translation Inspired from the noised back-translation  (Edunov et al., 2018; Wu et al., 2019) , we add noise to the train corpus. Let X and Y denote two languages, and let X and Y denote two corresponding sentence corpora, a set of all sentences. Let B = {(x i , y i ) N i=1 } denote the bilingual training corpus, where x i ? X , y i ? Y, and N is the number of sentence pairs. Let M x = {x j } Nx j=1 and M y = {y j } Ny j=1 denote sets of monolingual sentences, where N x and N y are sizes of each set, x j ? X , y j ? Y. We then train models f b : X ? Y and g b : Y ? X on the given bilingual data B. Then, we build the following two synthetic datasets through the trained models: Bsx = {(x, f b (x))|x ? M x }, Bsy = {(y, g b (y))|y ? M y }, Btx = {(f b (x), x)|x ? M x }, Bty = {(g b (y), y)|y ? M y } (2) where Bsx , Bsy can be seen the forward translation of source-side monolingual data of X and Y and Btx , Bty can be seen the backward translation of target-side monolingual data of X and Y . We build following noise versions of the augmented datasets for training. Bn x = {(?(x), ?(y))|(x, y) ? ( Bsx ? Bty )}, Bn y = {(?(y), ?(x))|(y, x) ? ( Bsy ? Btx )} (3) where ?(x) denote the noised sentence of x, which consists of two types of noise: deleting tokens with probability 0.05 and swapping tokens in the sentence, implemented as a random permutation over the tokens with the uniform distribution but restricted to swapping words no further than three positions apart, where three is set empirically. 

 Noisy channel re-ranking Noisy channel re-ranking method  (Yee et al., 2019)  is derived from Bayes' rule. p(y|x) = p(x|y)p(y) p(x) (4) Let x as a source sequence and y as a target sequence. Since p(x) is constant for all y, only the channel model p(x|y) and the language model p(y) determine y when x is given. Score used for re-ranking can be calculated as follows: Preprocessing We use recaser in Moses  (Koehn et al., 2007)  to recase Japanese-English Subtitle Corpus where English side is lowercased. We also normalize punctuation marks and tokenize English corpus with Moses. We use Mecab  (Kudo, 2006)  to tokenize Japanese corpus. We adopt Sentencepiece  (Kudo and Richardson, 2018) ; separate vocabs with 32K tokens are generated for each language. Separate vocabs show higher score in BLEU than a joint vocab in English-Japanese. ? * logp(y|x) + ? * logp(x|y) + ? * logp(y) |y| p ( Filtering We first filter the parallel corpus based on length; sentences with more than 800 characters are removed from the training data. We then filter the training corpus with LangId  (Lui and Baldwin, 2012) . If LangIds of source or target side are mismatched, we filter out this data. Data selection Unlike English, there are not enough news data in Japanese, so we select data from Common Crawl and use them as in-domain data. To obtain data close to in-domain, we classify sentences into in-domain and out-domain based on the perplexity of in-domain and out-domain language model (Moore and Lewis, 2010). Let P P L in (s) as the perplexity for sequence s with the in-domain language model and P P L out (s) as same with the out-domain language model. To classify sentences as close to in-domain, We calculate a score as follows: S = P P L out (s) ? P P L in (s) (6) We train in-domain and out-domain language models respectively with KenLM  (Heafield, 2011) . The in-domain language model is trained with News Crawl corpus and the out-domain language model is trained with Common Crawl corpus. 

 Experimental setting Our system is based on Transformer-big model on Fairseq  (Ott et al., 2019)  1 , which consists of 6-layers encoder and decoder each with 1024 embedding & hidden size and 4096 feed-forward layer size. Our system is trained using MASS 2 on 16?V100 GPUs, both in pre-training and finetuning. 

 Pre-training Our entire training sequence is described in Figure  1 . For the phase 0, we randomly sample 10M sentences X 0 and Y 0 from each mono corpus for unsupervised prediction task and use all available parallel corpus D 0 for supervised task. We prepare two separated prediction tasks, supervised and unsupervised setups respectively. For the supervised setup, we randomly mask entire input tokens in each sentence by 30% probability. In the unsupervised setup, we mask the fragment by replacing consecutive tokens with symbol [M] from random start position u. It first chooses 30% from input tokens, and each i-th token will be replaced as (1) an unchanged i-th token by 80% of the time, (2) a random token by 10% of the time, and (3) a masked token [M] by 10% of the time. After pre-training of model P T 0 , two fine-tuned models N M T x?y and N M T y?x are trained with the parallel corpus, English-Japanese and Japanese-English direction respectively. 

 Lang Lines Remark en 20M ja 20M ja*-en 5M Randomly filtered en*-ja 5M LM-based filtered In the beginning of next phase, we create a new setup and train the model with training data mentioned in Table  2 . We add noised synthetic data X and Y to create following version of training data. It consists of Bsx , Bsy , Bn x and Bn y . X m and Y m consist of 20M mono corpora for unsupervised pretraining. 5M English mono corpus are randomly chosen from mono corpus, and 5M Japanese mono corpus are selected based on Equation  6 ; they are represented as X p and Y p in Figure  1 . Then, 5M mono corpora are translated with N M T x?y and N M T y?x respectively. P T 1 model is trained with above train corpus. Then, we train two fine-tuned models, N M T 1 x?y and N M T 1 y?x separately with parallel corpora in Table  3 . 

 Iterative fine-tuning After pre-training in phase 1, we create fine-tuned models with parallel corpus D 0 and synthetic corpus X and Y . Inspired from joint training  (Zhang et al., 2018) , we perform back-translation and fine-tune steps  iteratively in phase 2. Synthetic corpora for each steps are replaced to a newly generated ones from developed models, which are represented as X and Y in Figure  1 . 

 Advance decoding We improve our final result with noisy channel re-ranking method  (Yee et al., 2019) . The small difference is we use the different direct model for scoring instead of using the same model used for generation. To generate y, we first ensemble three models with final back-translated models, considering validation sets. We generate 44 n-bests results with 44 beam size with ensemble models. Then, we re-rank the results according to Equation  5 . The direct model for scoring is the averaged model of three models used for ensemble. This is faster and shows better results compared to the ensemble model. The channel model is an average model in the opposite direction. For language model, we use Transformer-big model, trained only with News domain monolingual corpus. Finally, we tune weights of each model and length penalty with validation sets. 

 Experimental Results Step  The results of English to Japanese direction are shown in Table  4 and 5 . Our final submission's BLEU score is 5.42 higher than the baseline model. For evaluation, multi-bleu.perl 3 is used after tokenizing with Mecab in Japanese. The baseline model is trained only with parallel data in Transformer-big architecture and is decoded with beam size 4. It shows great performance improvement when MASS is applied. When using synthetic data and adding noise to data in pre-training steps (Phase 1), it shows better results compared to it with only parallel data (Phase 0). Back-translation with the in-domain monolingual data increases the BLEU score most, and the score increases further in the next iteration. The ensemble model and large beam size also show better BLEU score. For the test set, we replace symbol ? to "pound" in source sentences as pre-processing. We re-rank and tune the parameters based on News Commentary parallel data set which shows better results than tuning with devset. Since we select best models based on devset in previous steps, using devset in re-ranking seems to result in overfitting. The final result of our submission is shown in Table 6. Characters based tokenizer and SacreBLEU 4 are used for evaluation in Ocelot. 

 Submission SacreBLEU chrF English-Japanese 41.0 0.351  

 Conclusions In this paper, we describe our submission to the WMT20 news translation task in English to Japanese direction. Our main approach is based on transferring knowledge from large amount of monolingual data by pre-training the model itera-tively using MASS. We then improve the system with several effective methods: noised and iterative back-translation, in-domain data selection, and re-ranking. Through these methods, we achieve competitive results compared to the baseline and prove that the iterative knowledge transfer system we proposed is effective. Figure 1 : 1 Figure 1: Illustration of training sequences of our system, where pre-trained models P T * on both side are identical but separated for clarity. 

 Table 1 : 1 5)where ?, ?, ? are tunable weight, and p is length penalty for target length |y|. The training data of the entire system is shown in Table1. We use News Commentary (NC) data as another validation set in addition to newsdev2020 (devset). Training corpora for our system 3 Experiments 3.1 Data Data statistics Dataset Lines Parallel Data Wiki Titles v2 0.7M WikiMatrix 3.89M Japanese-English Subtitle Corpus 2.8M The Kyoto Free Translation Task 0.44M TED Talks 0.24M Monolingual Data (En) Europarl v10 2.29M News Commentary v15 0.6M News Crawl 23.35M News Discussions 63.51M Monolingual Data (Ja) News Crawl 3.44M News Commentary v15 2983 Common Crawl 1773.97M 

 Table 2 : 2 An amount of training corpora for pretraining. * means back-translated data from correspond monolingual corpus. 

 Table 3 : 3 An amount of training corpora for fine-tuning 

 Table 4 : 4 En-Ja BLEU scores on WMT20 devset Model Test Baseline 20.51 Ensemble + Beam 44 25.05 Re-ranking(devset) 24.41 Re-ranking(NC) 25.93 

 Table 5 : 5 En-Ja BLEU scores on WMT20 test set. 

 Table 6 : 6 Automatic evaluation on WMT20 test set in Ocelot. 

			 https://github.com/pytorch/fairseq 2 https://github.com/microsoft/MASS 

			 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl 4 https://github.com/mjpost/sacrebleu
