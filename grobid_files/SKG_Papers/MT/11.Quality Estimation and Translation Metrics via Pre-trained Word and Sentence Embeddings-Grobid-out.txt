title
Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings

abstract
We propose the use of pre-trained embeddings as features of a regression model for sentencelevel quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).

Introduction Quality estimation  (Blatz et al., 2004; Specia et al., 2009)  aims to predict the quality of machine translation (MT) outputs without human references, which is what sets it apart from translation metrics like BLEU  (Papineni et al., 2002)  or TER  (Snover et al., 2006) . Most approaches to quality estimation are trained to predict the post-editing effort, i.e. the number of corrections the translators have to make in order to get an adequate translation. The effort is measured by the HTER metric  (Snover et al., 2006)  applied to human post-edits. In this paper, we introduce a light-weight neural method with pre-trained embeddings, that means it does not require any pre-training. The second proposed method is the extension of the first one: besides pre-trained embeddings, it takes log probability from any MT system as an input feature. In addition to the official datasets provided for this year's WMT sentence level shared task, we analyze the performance of our methods against the extended datasets made from previous years data. Using the extended datasets allows to get a more reliable score and avoid skewed distributions of the predicted metrics. Besides that we apply our method to predict direct human assessment (DA)  (Graham et al., 2017) . In direct human assessment humans compare the machine translation output with a reference translation not seeing a source translation. Usually MT metrics  (Ma et al., 2018)  are compared to DA, but we decided to compare our predictions as well, because there is a difference between a number of post-edits and a human assessment. For example, if everything in a translation is perfect except one thing: all indefinite articles are missed, the number of post-edits may be large enough and a score will be low whereas humans likely give it a high score. The main difference between MT metrics and quality estimation is that quality estimation is computing without reference sentences. 

 Architecture Our method performs sentence-level quality estimation of machine translation. As other stateof-the-art methods  (Kim et al., 2017; Fan et al., 2018) , we use a neural-based architecture. However, compared to the other neural-based methods, we do not train embeddings from scratch, that usually takes a lot of data and computational resources. Instead of that, we use already well trained and freely available embeddings. For our method we have picked BERT  (Devlin et al., 2018)  and LASER  (Artetxe and Schwenk, 2018)  multilingual embeddings toolkits. We extract both BERT and LASER embeddings and feed them into a feed-forward neural network. A sigmoid output layer produces the desirable score. In case of HTER prediction we can add log probability score obtained from a neural MT system as an additional feature to the described above feed-forward neural network. The whole architecture of our system is depicted in Fig.  1 . BERT embeddings are extracted from a deep bidirectional transformer encoder, which is pretrained on Wikipedia data, with the aim of generating a general-purpose "language understanding". LASER embeddings are extracted from bidirectional word-level recurrent encoder, where sentence embeddings are extracted from max-pooled word embeddings, trained on publicly available parallel corpora.  

 Experimental Settings In this section we analyze the performance of proposed methods on different prediction outputs (HTER and DA) and different datasets and compare them with another neural method DeepQuest  (Ive et al., 2018)  that does not require additional data. To predict HTER we take a dataset that contains source sentences, their translated outputs and HTER scores. It is domain-specific: IT or pharmaceutical depending on the language pair. As there is no large enough corpus with DA labels, we use a dataset that consists only of source sentences and their machine translation output. The domain of this corpus is more general and source sentences have taken from the open resources. 

 Experiments We have implemented our methods using the Keras toolkit. As a regression model we have used four-layered feed-forward neural network with sigmoid as a final activation function. To obtain a log probability score, we trained neural MT systems using sockeye toolkit. We used Transformer  (Vaswani et al., 2017)  as a network architecture with six layers in encoder and decoder, word vectors of size 512, batch size 50, and Adam (Kingma and Ba, 2015) as optimizer with an initial learning rate of 0.0002. We present two models with different set of features: ? LABE: embeddings extracted from LASER and BERT ? LABEL: embeddings extracted from LASER and BERT and log probability obtained from Transformer NMT model BERT embeddings are extracted for multilingual cased BERT model. Only the last layer of embeddings is extracted. BERT gives 728-dimension embeddings for each word, source and target embeddings are separated by a special token and then average pooling is used to get sentence embeddings for source and target sentences. 

 Data and Results of HTER Prediction 

 Data We gathered the data from WMT16 -WMT18 shared tasks on sentence-level quality estimation for English-German (En-De)  (Bojar et al., 2016a (Bojar et al., , 2017a , from WMT17 -WMT18 German-English (De-En) and from WMT 18 English-Czech (En-Cs). The En-De data contains translations from neural and statistical MT systems and De-En and En-Cs datasets contain outputs only from statistical MT. However, for our method there is no difference between neural and statistical MT output. En-De and En-Cs sentences on the IT domain and De-En -on the pharmaceutical domain. We removed duplicated sentences and randomly split data into training, dev and test sets in the 70/20/10 ratio. As a result, we got the following number of sentences: ? En-De: ? 55K/16K/8K ? De-En: ? 37K/10K/5K ? En-Cs: ? 29K/8K/4K We intentionally increased the size of the test sets to reduce the impact of skewed distributions towards high quality translations. These fluent translation have the HTER score equalled zero and make up 70% of all data. Such distribution where we have 70% of zeros and other 30% of data is uniform from 0 to 1 is hard to learn with a regression model. 

 Results Below we describe the results of our systems for two test datasets: the extended dataset is described above and the second one is the small dataset (around 1K sentences) provided by organizers of WMT19. Results for extended datasets The resulting Pearson and Spearman coefficients for the all given language pairs are presented in Table  1 . As one can see the highest values were obtained by applying the models LABEL, but the difference of the computing values is small. The obtained numbers for En-De and En-Cs are close to each other whereas the resulting coefficients for De-En are noticeably higher. Both our models showed the better performance than deepQuest.  

 Results for WMT 2019 The results for the small WMT dataset do not look so impressive (Table 2) compared to the results of extended datasets. Without knowledge of data, it is difficult to say what the reason for it. We can assume that it may be due to the skewed distribution of the given dataset. It is worth noting that the same En-De (nmt) dataset was given also in WMT18 shared task and looking at the results 1 , we can see a drop in performance for this dataset as well. 

 Data and Results for human assessment prediction 

 Data We took data from News Translation Tasks 2015-2018 years   2017a, 2018) for En-De, English-Finnish (En-Fi), English-Russian (En-Ru) (both directions for all three language pairs) and En-Cs. The data consists of source sentences and their translation. The number of unique source sentences (?10-11K for each language pair) are significantly less than the number of translation, because every source sentence has several translations obtained from different systems. We randomly split the data into training and dev sets in the ratio 80/20: ? En-De: ? 141K/35K ? De-En: ? 111K/28K ? En-Fi: ? 100K/25K ? Fi-En: ? 73K/18K ? En-Ru: ? 95K/24K ? Ru-En: ? 94K/24K ? En-Cs: ? 113K/28K As test sets we used DAseg-newstest2016  (Bojar et al., 2016b ) that consists of 560 sentences for each language pair. As fine-tuning sets we took DAseg-newstest2015  (Stanojevi? et al., 2015)  and DAseg-newstest2017  (Bojar et al., 2017b ) that gave us around 1K sentences per each language pair. 

 Results Below we describe the obtained results for new-stest2016  (Bojar et al., 2016b ) and compare them with results of metrics tasks. At the time of publication of the article, results of newstest2019 were not yet available. Results for DAseg-newstest2016 The both proposed methods are supervised, so to train models we need labels. As DA data is scarce resource we trained models using chrF++  (Popovi?, 2017)  (with default hyper-parameters) as labels. To investigate how the number of language pairs affects the performance of models, we trained several models: with one language pair in the training set, with four (De-En, En-De, En-Cs, En-Ru) and with seven language pairs. As can be seen in the Figure  2 , the best results were achieved with the mono language pair models, although the difference between mono-and multimodels is not large. We also fine-tuned our models by using human assessment data. Fine-tuned models showed a little bit better results compared to the non-tuned models (Figure  :  2). We compared the obtained results to the metrics results. For De-En the best resulting Pearson correlation coefficient for metrics is 0.601 and for En-Ru is 0.666  (Bojar et al., 2016b) , whereas the best scores of our models are 0.520 and 0.668 for De-En and En-Ru respectively. Our results are comparable to the metrics results, despite the fact that we did not use reference sentences in contrast to the metrics task. Results for DAseg-newstest2019 We prepared scores for all language pairs described in 3.3 by using non-tuned models trained on seven language pairs and for De-En, En-Ru, Ru-En, Fi-En by using fine-tuned models. Results of this submission will be available  (Fonseca et al., 2019) . 

 Conclusions We proposed neural-based models for quality estimation of machine translation. One of our models requires only freely available embeddings (LASER and BERT) and the second needs also log probability from any MT system (in our experiments, we use Transformer MT system). We analyzed performance of both models on different language pairs and different prediction outputs and compared them to another neural quality estimation system. Both our methods showed better results compared to another light-weight approach deepQuest and we got comparable results with the metrics tasks even without using references. Figure 1 : 1 Figure 1: The proposed methods: LABEL: it requires LASER and BERT to get embeddings and NMT system to compute log probability and LABE: it requires only LASER and BERT to get embeddings. 

 Figure 2 : 2 Figure 2: Pearson and Spearman correlation coefficients for LABE model and different number of language pairs in training dataset. We show average over three runs. Test dataset: newstest2016 

 Table 1 : 1 Pearson and Spearman correlation coefficients for the monolingual models LABE and LABEL, and deepQuest . For models LABE and LABEL we show PCC and SCC between ensemble of five runs and HTER. LABE LABEL deepQuest PCC SCC PCC SCC PCC SCC DEEN 0.599 0.586 0.64 0.615 0.368 0.347 ENDE 0.533 0.566 0.542 0.57 0.294 0.305 ENCS 0.542 0.532 0.557 0.549 0.446 0.433 

 Table 2 : 2 Pearson and Spearman correlation coefficients for the monolingual models LABE and LABEL. Test set: official test set of WMT19. We show PCC and SCC between ensemble of five runs and HTER. 2016a,
