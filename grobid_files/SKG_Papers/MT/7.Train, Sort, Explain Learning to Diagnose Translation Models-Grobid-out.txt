title
Train, Sort, Explain: Learning to Diagnose Translation Models

abstract
Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity.

Introduction A multi-dimensional diagnostic evaluation of performance or quality often turns out to be more helpful for system improvement than just considering a one-dimensional utilitarian metric, such as BLEU  (Papineni et al., 2002) . This is exemplified by, for instance, the pioneering work of . The authors introduced the attention mechanism responding to the findings of  who reported that neural translation quality degraded with sentence length. The attention mechanism was later picked up by  Vaswani et al. (2017)  for their attention-only Transformer model, which still is state of the art in machine translation (MT)  (Bojar et al., 2018) . Furthermore, while MT output approaches human translation quality and the claims for "human parity"  (Wu et al., 2016; Hassan et al., 2018)  increase, multi-dimensional diagnostic evaluations can be useful to spot the thin line between the machine and the human. Diagnostic (linguistic) evaluations require human-expert feedback, which, however, is very time-consuming to collect. For this reason, there is a need for tools that mitigate the effort, such as the ones developed by  Madnani (2011) ;  Popovi? (2011) ;  Berka et al. (2012) ;  Klejch et al. (2015) . In this paper we propose a novel approach for developing evaluation tools. Contrary to the above tools that employ string comparison methods such as BLEU, implementations of the new approach derive annotations based on a neural model of explainability. This allows both capturing of semantics as well as focusing on the particular tendencies of MT errors. Using neural methods for the evaluation and juxtaposition of translations has already been done by  Rikters et al. (2017) . Their method, however, can only be applied to attentionbased models and their translations. In contrast, our approach generalizes to arbitrary machine and even human translations. After first discussing the abstract approach in the next section, we present a concrete open-source implementation, "DiaMaT" (from Diagnose Machine Translations). 

 Approach The proposed approach consists of the three steps (1) train, (2) sort, and (3) explain. 

 Step 1: Train In a first step, inspired by generative adversarial networks  (Goodfellow et al., 2014; Wu et al., 2017; Yang et al., 2017)  we propose to train a model to distinguish machine from human translations. The premise is that if the classifier generalizes well after training, it has learned to recognize systematic or frequent differences between the two classes (herinafter also referred to as "class evidence"). Class evidence may be, for instance, style differences, overused n-grams but also errors. The text classifier can be implemented through various architectures, ranging from deep CNNs  (Conneau et al., 2017)  to recurrent classifiers built on top of pre-trained language models  (Howard and Ruder, 2018) . 

 Step 2: Sort In a second step, we suggest letting the trained classifier predict the labels of a test set which contains human and machine translations and then sort them by classification confidence. This is based on the assumption that if the classifier is very certain that a given translation was produced by a machine (translation moved to the top of the list in this step), then the translation should contain strong evidence for a class, i.e. errors typical for only the machine. Furthermore, even if we are dealing with a very human-like MT output, which means that our classifier may only slightly perform above chance, sorting by classification confidence should still move the few systematic differences that the classifier identified to the top.  

 2.3 Step 3: Explain  Arras et al. (2016 Arras et al. ( , 2017a  demonstrated the data exploratory power of explainability methods in Natural Language Processing (NLP). This is why in a third step, we propose to apply an explainability method to uncover and visualize the class evidence on which the classifier based its decisions. Our definition of an explanation follows , who define it as "the collection of features of the interpretable domain, that have contributed for a given example to produce a decision (e.g. classification or regression)." 1 In our case the interpretable domain is the plain text space. There exist several candidate explainability methods, one of which we present in the following as an example. Figure  2 : A heatmap of contribution scores in word vector space over a sequence of tokens. Red means positive contribution (score > 0), blue means negative contribution (score < 0). 

 Explainability and Interpretability Methods for Data Exploration In their tutorial paper,  discuss several groups of explainability methods. One group, for instance, identifies how sensitively a model reacts to a change in the input, others extract patterns typical for a certain class. Here, we discuss methods that propagate back contributions. The contribution flow is illustrated in Fig.  1 . At the top, the depicted binary classifier produced a positive output (input classified as class one). The classification decision is based on the fact that in the previous layer, the evidence for class one exceeded the evidence for class zero: The left and the right neuron contributed positively to the decision (reddish), whereas the middle neuron contributed negatively (blueish). Several explainability methods, such as Layerwise Relevance Propagation  (Bach et al., 2015)  or PatternAttribution , backtrack contributions layer-wise. The methods have to preserve coherence over highly non-linear activation functions. Eventually, contributions are projected into the input space where they reveal what the model considers emblematic for a class. This is what we exploit in step 3. Explainability methods in NLP  (Arras et al., 2016 (Arras et al., , 2017a Harbecke et al., 2018)  are typically used to first project scores into word-vector space resulting in heat maps as shown in Fig.  2 . To interpret them in plain text space, the scores are summed over the word vector dimensions to compute RGB values for each token, resulting in plain text heatmaps as shown in Fig.  3 . 

 Implementation For step 1 (training phase), DiaMaT 2 deploys a CNN text classifier, the architecture of which is depicted in Fig.  4 . The classifier consumes three embeddings: the embedding of a source and two translations of the source, one by a machine and one by a human. It then separately convolves over the embeddings and subsequently applies max pooling to the filter activations. The concatenated max features are then passed to the last layer, a fully connected layer with two output neurons. The left neuron fires if the machine translation was passed to the left input layer, the right neuron fires if the machine translation was passed to the right input layer. Note that this layer allows the model to combine features from all three inputs for its classification decision. For step 2 (sorting phase), DiaMaT offers to sort by unnormalized logit activations or by softmax activations. Furthermore, one can choose to use the machine neuron activation or the human neuron activation as the sorting key. For step 3 (explaining phase), DiaMaT employs the iNNvestigate toolbox  in the back-end that offers more than ten explainability methods: Replacing one method with another only requires to change one configuration value in DiaMaT, before repeating step 3 again. In step 3, DiaMaT produces explanations in the form of (token, score) tuple lists that are consumed by a front-end server which visualizes the scores as class evidence (see Fig.  3 ).  

 Datasets and Experiments We tested DiaMaT on a corpus translated by an NMT Transformer engine  (Vaswani et al., 2017)  conforming to the WMT14 data setup  (Bojar et al., 2014) . The NMT model was optimized on the testset of WMT13 and an ensemble of 5 best models was used. It was trained using OpenNMT  (Klein et al., 2017) , including Byte Pair Encoding  (Sennrich et al., 2015)  but no back-translation, achieving 32.68 BLEU on the test-set of WMT14. Next, we trained the CNN text classifier sketched in Fig.  4  for which we randomly drew 1M training samples (human references and machine translations alongside their sources) from the WMT18 training data  (Bojar et al., 2018) , excluding the WMT14 training data. The validation set consisted of 100k randomly drawn samples from the same set and we drew another 100k samples randomly for training the explainability method of choice, PatternAttribution, which learns explanations from data. All texts were embedded using pre-trained fastText word vectors  (Grave et al., 2018) . We evaluated the classifier on around 20k samples drawn from the official test sets, excluding WMT13. On this test set, the classifier achieved an accuracy of 75%, which is remarkable, considering the ongoing discussion about human parity  (Wu et al., 2016; Hassan et al., 2018) . We also used this test set for steps 2 and 3. Thus, neither the translation model, nor the text classifier, nor the explainability method encountered this split during training. For step 2, the machine translation was always passed to the right input layer and contributions to the right output neuron were retrieved with PatternAttribution.  4  We then sorted the inputs by the softmax activation of the machine neuron, which moved inputs for which the classifier is certain that it has identified the machine correctly to the top. 

 Demonstration and Observations We observed that the top inputs frequently contained sentences in which DiaMaT considered the token after a sentence-ending full stop strong evidence for the human (Fig.  3 , top segment). We take this as evidence that DiaMaT correctly recognized that the human generated multiple sentences instead of a single one more often than the machine did. At this point, we cannot, however, offer an explanation for why the token preceding the punctuation mark is frequently considered ev-idence for the machine. Furthermore, DiaMaT also regarded reduced negations ("n't") as evidence for the human (see Fig.  3 , middle segment) which again is reflected in the statistics. The machine tends to use the unreduced negation more frequently. The last segment in Fig.  3  shows how DiaMaT points to the fact that the machine more often produced sentence end markers than the human in cases where the source contained no end marker. The claims above are all statistically significant in the test set, according to a ? 2 test with ? = 0.001. 

 Future Work The inputs in Fig.  3  contain easily readable evidence. There is, however, also much evidence that is hard to read. In general, we can assume that with increasing architectural complexity, more complex class evidence can be uncovered, which may come at the cost of harder readability. In the future, it is worth exploring how different architectures and model choices affect the quality, complexity and readability of the uncovered evidence. For instance, one direction would be to to train the classifier on top of a pretrained language model  (Howard and Ruder, 2018; Devlin et al., 2019)  which could improve the classification performance. Furthermore, other explainability methods should also be tested. 

 Conclusion We presented a new approach to analyse and juxtapose translations. Furthermore, we also presented an implementation of the approach, DiaMaT. Dia-MaT exploits the generalization power of neural networks to learn systematic differences between human and machine translations and then takes advantage of neural explainability methods to uncover these. It learns from corpora containing millions of translations but offers explanations on sentence level. In a stress test, DiaMaT was capable of exposing systematic differences between a state-of-the-art translation model output and human translations. Figure 1 : 1 Figure 1: Contributions propagated from output to input space. Colors represent positive (red) and negative (blue) contributions. The Figure is adapted from Kindermans et al. (2018). 
