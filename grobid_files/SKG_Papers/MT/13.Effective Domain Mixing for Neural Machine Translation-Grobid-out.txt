title
Effective Domain Mixing for Neural Machine Translation

abstract
Neural Machine Translation (NMT) models are often trained on heterogeneous mixtures of domains, from news to parliamentary proceedings, each with unique distributions and language. In this work we show that training NMT systems on naively mixed data can degrade performance versus models fit to each constituent domain. We demonstrate that this problem can be circumvented, and propose three models that do so by jointly learning domain discrimination and translation. We demonstrate the efficacy of these techniques by merging pairs of domains in three languages: Chinese, French, and Japanese. After training on composite data, each approach outperforms its domain-specific counterparts, with a model based on a discriminator network doing so most reliably. We obtain consistent performance improvements and an average increase of 1.1 BLEU.

Introduction Neural Machine Translation (NMT)  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;  is an end-to-end approach for automated translation. NMT has shown impressive results  (Bahdanau et al., 2015; Luong et al., 2015a;  often surpassing those of phrase-based systems while addressing shortcomings such as the need for hand-engineered features. In many translation settings (e.g. web translation, assistant translators), input may come from more than one domain. Each domain has unique properties that could confound models not explicitly fitted to it. Thus, an important problem is to effectively mix a diversity of training data in a multi-domain setting. Our problem space is as follows: how can we train a translation model on multi-domain data to improve test-time performance in each constituent domain? This setting differs from the majority of work in domain adaptation, which explores how models trained on some source domain can be effectively applied to outside target domains. This setting is important, because previous research has shown that both standard NMT and adaptation methods degrade performance on the original source domain(s)  (Farajian et al., 2017; Haddow and Koehn, 2012) . We seek to prove that this problem can be overcome, and hypothesize that leveraging the heterogeneity of composite data rather than dampening it will allow us to do so. To this extent, we propose three new models for multi-domain machine translation. These models are based on discriminator networks, adversarial learning, and target-side domain tokens. We evaluate on pairs of linguistically disparate corpora in three translation tasks (EN-JA, EN-ZH, EN-FR), and observe that unlike naively training on mixed data (as per current best practices), the proposed techniques consistently improve translation quality in each individual setting. The most significant of these tasks is EN-JA, where we obtain state-of-the-art performance in the process of examining the ASPEC corpus  (Nakazawa et al., 2016)  of scientific papers and Sub-Crawl, a new corpus based on an anonymous manuscript  (Anonymous, 2017) . In summary, our contributions are as follows: ? We show that mixing data from heterogenous domains leads to suboptimal results compared to the single-domain setting, and that the more distant these domains are, the more their merger degrades downstream translation quality. ? We demonstrate that this problem can be circumvented and propose novel, generalpurpose techniques that do so. 

 Neural Machine Translation Neural machine translation  (Sutskever et al., 2014)  directly models the conditional log probability log p(y|x) of producing some translation y = y 1 , ..., y m of a source sentence x = x 1 , ..., x n . It models this probability through the encoder-decoder framework. In this approach, an encoder network encodes the source into a series of vector representations H = h 1 , ..., h n . The decoder network uses this encoding to generate a translation one target token at a time. At each step, the decoder casts an attentional distribution over source encodings  (Luong et al., 2015b; Bahdanau et al., 2014) . This allows the model to focus on parts of the input before producing each translated token. In this way the decoder is decomposing the conditional log probability into log p(y|x) = m ? t=1 log p(y t |y <t , H) (1) In practice, stacked networks with recurrent Long Short-Term Memory (LSTM) units are used for both the encoder and decoder. Such units can effectively distill structure from sequential data  (Elman, 1990) . The cross-entropy training objective in NMT is formulated as, L gen = ? (x,y)?D ? log p(y|x) (2) Where D is a set of (source, target) sequence pairs (x, y).  

 Models We now describe three models we are proposing that leverage the diversity of information in heterogeneous corpora. They are summarized in Figure  1 . We assume dataset D consists of source sequences X, target sequences Y and domain class labels D that are only known at training time. 

 Discriminative Mixing In the Discriminative Mixing approach, we add a discriminator network on top of the source encoder that takes a single vector encoding of the source c as input. This network maximizes P (d|H), the predicted probability of the correct domain class label d conditioned on the hidden states of the encoder H. It does so by minimizing the negative cross-entropy loss L disc = ? log p(d|H). In other words, the discriminator uses the encoded representation of the source sequence to predict the correct domain. Intuitively, this forces the encoder to encode domain-related information into the features it generates. We hypothesize that this information will be useful during the decoding process. The encoder can employ an arbitrary mechanism to distill the source into a single-vector representation c. In this work, we use an attention mechanism over the encoder states H, followed by a fully connected layer. We set c to be the attention context, and calculate it according to  Bahdanau et al. (2015) : c = ? j a j h j a = softmax(?) ?i = v T a tanh(W a h i ) The discriminator can be an arbitrary neural network. For this work, we fed c into a fully connected layer with a tanh nonlinearity, then passed the result through a softmax to obtain probabilities for each domain class label. The discriminator is optimized jointly with the rest of the Sequence-to-Sequence network. If L gen is the standard sequence generator loss described in Section 2, then the final loss we are optimizing is the sum of the generator and discriminator loss L = L gen + L disc . 

 Adversarial Discriminative Mixing We also experiment with an adversarial approach to domain mixing. This approach is similar to that of 3.1, except that when backpropagating from the discriminator network to the encoder, we reverse the gradients by multiplying them by ?1. Though the discriminator is still using ?L disc to update its parameters, with the inclusion of the reversal layer, we are implicitly directing the encoder to optimize with ?L disc . This has the opposite effect of what we described above. The discriminator still learns to distinguish between domains, but the encoder is forced to compute domaininvariant representations that are not useful to the discriminator. We hope that such representations lead to better generalization across domains. Note the connections between this technique and that of the Generative Adversarial Network (GAN) paradigm  (Goodfellow et al., 2014) . GANs optimize two networks with two objective functions (one being the negation of the other) and periodically freeze the parameters of each network during training. We are training a single network without freezing any of its components. Furthermore, we reverse gradients in lieu of explicitly defining a second, negated loss function. Last, the adversarial parts of this model are trained jointly with translation in a multitask setting. Note also that the representations computed by this model are likely to be applicable to unseen, outside domains. However, this setting is outside the scope of this paper and we leave its exploration to future work. For our setting, we hypothesize that the domain-agnostic encodings encouraged by the discriminator may yield improvements in mixed-domain settings as well. 

 Target Token Mixing A simpler alternative to adding a discriminator network is to prepend a domain token to the target sequence. Such a technique can be readily incorporated into any existing NMT pipeline and does not require changes to the model. In particular, we add a single special vocabulary word such as "domain=subtitles", per domain and prepend this token to each target sequence therein. The decoder must learn, similar to the more complex discriminator above, to predict the correct domain token based on the source representation at the first step of decoding. We hypothesize that this technique has a similar regularizing effect as adding a discriminator network. During inference, we remove the first predicted token corresponding to the domain. The advantage of this approach verses the similar techniques discussed in related work (Section 5) is that in our proposed method, the model must learn to predict the domain based on the source sequence alone. It does not need to know the domain a-priori. 

 Experiments 

 Datasets For the Japanese translation task we evaluate our domain mixing techniques on the standard ASPEC corpus  (Nakazawa et al., 2016)  consisting of 3M scientific document sentence pairs, and the SubCrawl corpus, consisting of 3.2M colloquial sentence pairs harvested from freely available subtitle repositories on the World Wide Web. We use standard train/dev/test splits (3M, 1.8k, and 1.8k examples, respectively) and preprocess the data using subword units 1  (Sennrich et al., 2015)  to learn a shared English-Japanese vocabulary of size 32,000. To allow for fair comparisons, we use the same vocabulary and sentence segmentation for all experiments, including singledomain models. To prove its generality, we also evaluate our techniques on a small set of about 200k/1k/1k training/dev/test examples of English Chinese (EN-ZH) and English-French (EN-FR) language pairs. For EN-ZH, we use a news commentary corpus from WMT'17 2 and a 2012 database dump of TED talk subtitles (Tiedemann, 2012). For EN-FR, we use professional translations of European Parliament Proceedings  (Koehn, 2005)  and a 2016 dump of the OpenSubtitles database  (Lison and Tiedemann, 2016) . The premise of evaluating on mixed-domain data is that the domains undergoing mixing are in fact disparate. We need to quantifiably measure the disparity therein to obtain fair, valid, and explainable results. Thus, we measured the distances between the domains of each language pair with A-distance, an important part of the upper generalization bounds for domain adaptation  (Ben-David et al., 2007) . Due to the intractability of computing A-distances, we instead compute a proxy for A-distance, dA , which is given theoretical justification in  Ben-David et al. (2007)  and used to measure domain distance in  Gani et al. (2015) ;  Glorot et al. (2011) . The proxy A-distance is obtained by measuring the generalization error ? of a linear bag-of-words SVM classifier trained to discriminate between the two domains, and setting dA = 2(1?2?). Note that by nature of its formulation, dA is only useful in comparative settings, and means little in isolation  (Ben-David et al., 2007) . However, it has a minimum value of 1, implying exact domain match, and a maximum of 2, implying that domains are polar opposites. 

 Experimental Protocol All models are implemented using the Tensorflow framework and based on the Sequenceto-Sequence implementation of  Britz et al. (2017)  3 . We use a 4-layer bidirectional LSTM encoder with 512 units, and a 4-layer LSTM decoder. Recall from Section 3 that we use Bahdanau-style attention  Bahdanau et al. (2015) . Dropout of 0.2 (0.8 keep probability) is applied to the input of each cell. We optimize using Adam and a learning rate of 0.0001  (Kingma and Ba, 2014; Abadi et al., 2016) . Each model is trained on 8 Nvidia K40m GPUs with a batch size of 128. The combined Japanese dataset took approximately a week to reach convergence. During training, we save model checkpoints every hour and choose the best one using the BLEU score on the validation set. To calculate BLEU scores for the EN-JA task, we follow the instruction from WAT 4 and use the KyTea tokenizer  (Neubig et al., 2011) . For the EN-FR and EN-ZH tasks, we follow the WMT '16 guidlines and tokenize with the Moses tokenizer.perl script  (Koehn et al., 2007) . 

 Results The results of our proxy-A distance experiment are given in Table  1 . dA is a purely comparative metric that has little meaning in isolation  (Ben-David et al., 2007) , so it is evident that the EN-JA and EN-ZH domains are more disparate, while the EN-FR domains are more similar. To understand the interactions between these models and mixed-domain data, we train and test on ASPEC, SubCrawl, and their concatenation. We do the same for the French and Chinese baselines. In general, our results support the hypothesis that the naive concatenation of data from disparate domains can degrade in-domain translation quality (Table  2 ). In both the EN-JA and EN-FR settings, the domains undergoing mixing are disparate enough to degrade  performance when mixed, and the proposed techniques recover some of this performance drop. In the EN-ZH setting, we observe that even when similar domains are mixed performance can drop. Notably, in this setting, the proposed techniques successfully improve performance over single-domain training. For a more detailed perspective on result, Figure  2a  depicts the mixeddomain/individual-domain performance differential as a function of domain distance. The two share a negative association, suggesting that the most distant two domains are, the more their merger degrades performance. This degradation is particularly strong in Japanese due the vast structural differences between formal and casual language. The vocabularies, conjugational patterns, and word attachments all follow different rules in this case  (Hori, 1986) . We then trained and tested our proposed methods on the same mixed data (Table  2 ). Our results generally agree with the hypothesis that the diversity of information in heterogeneous data can be leveraged to improve in-domain translation. Overall, we find that all of the proposed methods outperform their respective baselines in most settings, but that the discriminator appears the most reliable. It bested its counterparts in 4 of 6 trials, and was  the only approach that outperformed both individually fit and naively mixed baselines in every trial. Figure  2c  depicts the dynamics of the discriminator approach. More specifically, this figure shows the discriminator/naive-mixing performance differential as a function of domain distance. The two share a positive association, suggesting that the more distant two domains are, the more the discriminator helps performance. This may be because it is easier to classify distant domains, so the discriminator can fit the data better and its gradients encourage the upstream encoder to include more useful domain-related structure. The adversarial discriminator architecture yielded improvements on the small datasets, but underperformed on EN-JA. It is possible that the grammatical differences inherent to casual and polite domains are such that semantic information was lost in the process of forcing their encoded distributions to match. Additionally, adversarial objective functions are notoriously difficult to optimize on, and this model was prone to falling into poor local optimum during training. The simpler target token approach also yields improvement over the baselines, just barely surpassing that of the Discriminator for ASPEC. This approach has the practical benefit of requiring no architectural changes to an off-the-shelf NMT system. Our EN-FR results are particularly interesting. Though the data seem like they should come from sufficiently distant domains (parliament proceedings and subtitles), the domains are actually quite close according to dA (Table  1 ). Since these domains are so close, their merger is able to improve baseline performance. Thus, if the source and target domain are sufficiently close, then their merger does indeed help. Next, we investigated the optimization dynamics of these models by examining their learning curves. Curves for the baselines and discriminative models trained on EN-JA data are depicted in Figure  3a . Single-domain training clearly outperforms mixed training, and it appears that adding a discriminative strategy provides additional gains. From Figure  3b  we can see that the discriminator ap-proach (not reversing gradients), learns to fit the domain distribution quickly, implying that the Japanese domains were in fact quite distant and easily classifiable. 

 Related Work Our work builds on a recent literature on domain adaptation strategies in Neural Machine Translation. Prior work in this space has proposed two general categories of methods. The first proposed method is to take models trained on the source domain and finetune on target-domain data.  Luong and Manning (2015) ;  Zoph et al. (2016)  explores how to improve transfer learning for a low-resource language pair by finetuning only parts of the network.  Chu et al. (2017)  empirically evaluate domain adaptation methods and propose mixing source and target domain data during finetuning.  Freitag and Al-Onaizan (2016)  explored finetuning using only a small subset of target domain data. Note that we did not compare directly against these techniques because they are intended to transfer knowledge to a new domain and perform well on only the target domain. We are concerned with multi-domain settings, where performance on all constituent domains is important. A second strain of "multi-domain" thought in NMT involves appending a domain indicator token to each source sequence  (Kobus et al., 2016) . Similarly,  use a token for cross-lingual translation instead of domain identification. This idea was further refined by  Chu et al. (2017) , who integrated source-tokenization into the domain finetuning paradigm. While it requires no changes to the NMT architecture, these approaches are inherently limited because they stipulate that domain information for unseen test examples be known. For example, if using a trained model to translate user-generated sentences, we do not know the domain a-priori, and this approach cannot be used. Apart from the recent progress in domain adaptation for NMT, we draw on work that transfers knowledge between domains in semisupervised settings. Our strongest influence is adversarial domain adaptation  (Ganin et al., 2015) , where feature distributions in the source and target domains are matched with a Domain-Adversarial Neural Network (DANN). Another approach to this problem is that of  Long et al. (2015) , which measures and minimizes the distance between domain distribution means before training, thereby negating any unique properties. There is some overlap between past research in multi-domain statistical machine translation (SMT) and the ideas of this paper.  (Farajian et al., 2017)  compared the efficacy of phrase-based SMT and NMT on multipledomain data, observing similar performance degradations as us in mixed-domain settings. However, that study did not seek to understand the issue and offered no explanation, analysis, or solution to the problem. Another line of work merged data by only selecting examples with a propensity for relevance in a multi-domain setting  (Mandal et al., 2008; Axelrod et al., 2011) . In a strategy that echos NMT fine-tuning,  Pecina et al. (2012)  used a variety of in-domain development sets to tune hyperparameters to a generalized setting. Similar to our domain discriminator network, Clark et al. (  2012 ) crafted domain-specific features that are used by the decoder. However, some of these systems' features are downstream of binary indicators for domain identity. This approach, then, faces the same inherent limitations as source-tokenization: domain knowledge is required for inference. Furthermore, the domain features of this system are integral to the decoding process, while our discriminator network is an independent module that can be detached during inference. 

 Conclusion We presented three novel models for applying Neural Machine Translation to multidomain settings, and demonstrated their efficacy across six domains in three language pairs, and in the process achieved a new stateof-the-art in EN-JA translation. Unlike the naive combining of training data, these models improve their translational ability on each constituent domain. Furthermore, these models are the first of their kind to not require knowledge of each example's domain at inference time. All the proposed approaches outperform the naive combining of training data, so we advise practitioners to implement whichever most easily fits into their preexisting pipelines, but an approach based on a discriminator network offered the most reliable results. In future work we hope to explore the dynamics of adversarial discriminative training objectives, which force the model to learn domain-agnostic features, in the related problem of adaptation to unseen test-time domains. Figure 1 : 1 Figure 1: The novel mixing paradigms under consideration. Discriminative mixing (A), adversarial discriminative mixing (B), and target-side token mixing (C) are depicted. 
