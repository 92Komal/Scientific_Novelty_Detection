title
Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns

abstract
This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020)  to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.

Introduction Translation quality estimation (QE)  (Blatz et al., 2004; Specia et al., 2018 Specia et al., , 2020  aims to predict the quality of translation hypothesis without golden-standard human references, setting it apart from reference-based translation metrics. Existing reference-based evaluation metrics, e.g. BLEU  (Papineni et al., 2002) , METEOR  (Banerjee and Lavie, 2005) , NIST  (Doddington, 2002) , ROUGE  (Lin, 2004) , TER  (Snover et al., 2006) , are commonly used in language generation tasks including translation, summarization, and captioning but all heavily rely on the quality of given references. Recently,  (Edunov et al., 2020)  show that reference-based automatic evaluation metrics, e.g., BLEU, are not always reliable because the human translated references are translationese  (Koppel and   Example of mismatching error, Russian?English. On the left, token "?" is mismatched to "The" with the maximal probability (within the red rectangle) only. On the right, guided by our proposed cross-lingual patterns, "?" is correctly matched to the token "named" with the maximal probability (within the green rectangle.)  Ordan, 2011; Graham et al., 2019) . Thus, an automatic method with no access to any references, i.e., QE, is highly appreciated. ? ##? ? ? ##? ##? ? ? ##? ##? ##? ? ? ##? ##? ? ##? ##? In this paper, we mainly focus on sentence level QE metrics, where existing studies categorize it into two classes: 1) supervised QE with human assessment as supervision signal: a feature extractor stacked with an estimator  (Yankovskaya et al., 2019; Wang et al., 2016b; Fan et al., 2019) ; 2) unsupervised QE without human assessment, which normally based on the pre-trained word embeddings, for example, YISI  (Lo, 2019)  and BERTScore  (Zhang et al., 2020) . Our work follows the latter, where we adopt BERTScore  (Zhang et al., 2020)  without extra fine-tuning. In particular, we implement our approach upon the pre-trained multilingual BERT  (Devlin et al., 2019)  and XLM  (Conneau and Lample, 2019) . We first empirically reveal the mismatching issue when directly adopting BERTScore  (Zhang et al., 2020)  to QE task. Specifically, there exist lots of mismatching errors between source tokens and translated candidate tokens when performing greedy matching with pairwise similarity. Figure  1  shows an example of the mismatching error, where the Russian token "?" is mismatched to the English token "The" due to lacking of proper guidance. To alleviate this issue, we design two explicit cross-lingual patterns to augment the BERTScore as a QE metric: ? CROSS-LINGUAL ALIGNMENT MASKING: we design an alignment masking strategy to provide the pairwise similarity matrix with extra guidance. The alignment is derived from GIZA++  (Och and Ney, 2003)   

 Methods 

 BERTScore as Backbone A pre-trained multilingual model generates contextual embeddings of both source sentence and translated candidate sentence, such that this pair of sentences in different language can be mapped to the same continuous feature space. Given a source sentence x = x 1 , . . . , x k , the model generates a sequence of vectors x 1 , . . . , x k while the candidate ? = ?1 , . . . , ?l is mapped to ?1 , . . . , ?l . Different from the reference-based BERTScore, where they compute the pairwise similarity between reference sentence and translated candidate sentence, we calculate the pairwise similarity between the source and translated candidate with dotproduction, i.e., x i ?j . We adopt greedy matching to force each source token to be matched to the most similar target token in the translated candidate sentence. The QE function based on BERTScore backbone therefore can be formulated as: R BERT = 1 |x| x i ?x max ?j ? x i ?j , P BERT = 1 |?| ?j ? max x i ?x x i ?j , F BERT = 2 P BERT ? R BERT P BERT + R BERT . (1) where R BERT , P BERT and F BERT are inherited from Zhang et al. (  2020 ), representing Recall rate, Precision rate and F-score, respectively. 

 Alignment Masking Strategy With aforementioned QE function, we can follow  Zhang et al. (2020)  to obtain the distance between the source sentence and translated candidate sentence via directly adding up the maximum similarity score of each token pair. However, because there exist lots of mismatching errors (as shown in Figure  1 ), above sentence-level similarity calculation may be sub-optimal. Moreover, Zhang et al. (  2020 )'s calculation is suitable for monolingual scenario, which may be insensitive for cross-lingual computation. Thus, we propose to augment our QE metric with more cross-lingual signals. Inspired by  Ding et al. (2020) , where they show it's possible to augment cross-lingual modeling by leveraging cross-lingual explicit knowledge. we therefore employ word alignment knowledge from external models, e.g., GIZA++ 1 , as additional information. Alignment masking Both BERT  (Devlin et al., 2019)  and XLM  (Conneau and Lample, 2019)  utilize BPE tokenization . It should be noted that in this paper, by word alignment we mean alignment of BPE tokenized word and subword units. Given a tokenized source sentence x and candidate sentence ?, alignment  (Och and Ney, 2003)  is defined as a subset of the Cartesian product of position, A ? {(i, j) : i = 1, . . . , k; j = 1, . . . , l}. Alignment results represented by M is defined as: M = 1 (i, j) ? A 0 ? a ? 1 otherwise (2) M is a penalty function over the similarity of unaligned tokens. It's a mask like matrix to assign a penalty weight a 2 to the similarity of unaligned tokens while keeping that of aligned ones unchanged, as illustrated in Figure  2 . Thus, greedy matching is performed on a renewed similarity matrix, which is defined as the average of x i ?j and masked x i ?j by word alignment. For example, R BERT # Metrics en-de en-zh ro-en et-en ne-en si-en ru-en  (x i ?j +M?x i ?j ) (3) which can be characterized as balancing our proposed extra explicit cross-lingual patterns, i.e., word alignment. 

 Generation Score In additional to token similarity score, we introduce force-decoding perplexity of each target token as a cross-lingual generation score. For better coordination and considering our cross-lingual setting, we use the same pre-trained cross-lingual model, e.g. multilingual BERT, for both token embedding extraction and masked language model (MLM) perplexity generation. This cross-lingual generation score is added as: F BERT(ppl) = (1 ? ?) * F BERT + ? * ppl MLM (4) where the ? can be seen as a variable that regulates the interpolation ratio between F BERT and our proposed ppl MLM , making the generation score after combination more wisely. The effect of ? will be discussed in the experiments. 

 Experimental Results 

 Data Main experiments were conducted on the WMT20 QE Shared Task, Sentence-level Direct Assessment language pairs. The task contains 7 directions, including: ? English?German (en-de) ? English?Chinese (en-zh) ? Romanian?English (ro-en) ? Estonian?English (et-en) ? Nepalese?English (ne-en) ? Sinhala?English (si-en) ? Russian?English (ru-en) Each of them consists of 7K training data, 1K validation data and 1K test data.  

 Setup Based on our proposed QE metric in Section 2.1, we conduct the validataion and main experiments with two pre-trained cross-lingual model: bertbase-multilingual-cased 3 (12-layer, 768-hidden, 12-heads, trained on 104 languages) and xlmmlm-100-1280 4 (16-layer, 1280-hidden, 16-heads, on 100 languages) for both contextual embedding representation and generation score. The 9th layer of multilingual BERT and the 11th of XLM are used to generate contextual embedding representations. Furthermore, we obtain bidirectional word alignment of all the training, validation and test dataset with GIZA++. Notably, this work is a zero-shot approach that doesn't involve training on Direct Assessment (DA) scores, which makes our method suitable for real industry scenarios. 

 Ablation Study In order to maximize the advantages of our proposed method for zero-shot translation QE, we conducted extensive ablation studies. We report the results of ablation studies on the validation dataset. Effect of ? We conduct ablation studies to empirically decide the value of of ? in Equation  4 when introducing generation scores. We observe positive effect of proper weighted additional generation score on en-zh, ro-en, et-en, ne-en, si-en.  As illustrated in Figure  3 , considering the average performance, we pick ? = 0.01 from [0, 0.03]. 

 Effect of different pretrained models We also investigated the effect to deploy our proposed fixed cross-lingual patterns on different state-of-the-art large scale pre-trained models, e.g., XLM (Conneau and Lample, 2019) (xlm-mlm-100-1280), BERT (Zhang et al., 2020) (bert-base-multilingualcased). Table  3  lists a comparison of multilingual BERT and XLM in terms of the Pearson correlations with Direct Assessment (DA) scores. As seen, multilingual BERT outperforms XLM on almost all language pairs, excepting for si-en. One possible reason is that multilingual BERT is not pre-trained on Sinhala corpus while XLM does. In this end, we generate our final submission with XLM in sien direction, and with multilingual BERT in other directions. 

 Main Results In the main experiments, we evaluate the agreement of our approach with Direct Assessment (DA) scores on validation dataset, as DA scores of the test set are not available at this point. Baseline results, which are evaluated on test set though, are also listed for general comparison. As shown in Table  1 , our method could achieve improvements on 4 out of 6 directions, including en-zh, ro-en, et-en and ne-en. Particularly, combination of two strategies, i.e., CROSS-LINGUAL ALIGNMENT and CROSS-LINGUAL GENERATION SCORE, could achieve better performance on en-zh, ro-en and et-en directions. Besides Pearson correlations, we also calculated Kendall correlations for all language pairs. As seen in are same as Pearson correlations, validating the effectiveness of our proposed methods. 

 Official Evaluations The official automatic evaluation results of our submissions for WMT 2020 are presented in Table  4 . We participated QE (Sentence-Level Direct Assessment) in following language pairs: en-de, en-zh, ro-en, ne-en, si-en, ru-en, except for et-en. From the official evaluation results  (Specia et al., 2020)  in terms of absolute Pearson Correlation, our submission achieves higher performance than supervised baseline  (Kepler et al., 2019)  in ne-en and si-en (As shown in Table  4 ). Encouragingly, our proposed zero-shot QE metric could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. 

 Related Work MT evaluation Taking sentence-level evaluation as an example, reference-based metrics describe to which extend a candidate sentence is similar to a reference one  (Sellam et al., 2020) . BLEU  (Papineni et al., 2002) , METEOR  (Banerjee and Lavie, 2005) , NIST  (Doddington, 2002) , ROUGE  (Lin, 2004)  measure such similarity through n-gram matching, which is restricted to the exact form of sentences. TER  (Snover et al., 2006)  and CHAR-ACTER  (Wang et al., 2016b)  use edit distance at word or character level to indicate the distance between candidate and reference. Different from these metrics that are restricted to the exact form of sentences, recent dominated neural model metrics learn to evaluate with human assessment as supervision signal, such as BEER  (Stanojevi? and Sima'an, 2014)  and RUSE  (Shimanaka et al., 2018) , or oth-ers as YiSi  (Lo, 2019)  and BERTScore  (Zhang et al., 2020)  , evaluate with pre-trained word embedding, without using human assessment. Incorporating Explicit Knowledge Several approaches have incorporated pre-defined or learned features into neural networks.  Tai et al. (2015)  demonstrate that incorporating structured semantic information could enhance the representations.  feed the encoder cell combined embeddings of linguistic features including lemmas, subword tags, etc.  Ding et al. (2017)  leverage the domain knowledge to perform data selection to improve the machine translation models.  Ding and Tao (2019)  incorporate the structure patterns of sentences, i.e., syntax, into the Transformer network to enhance seq2seq modeling performance.  Raganato et al. (2020)  utilize the pre-defined fixed patterns to replace the attention weights and show promising results. Inspired by above works, we propose to augment zero-shot QE model with crosslingual patterns. 

 Conclusion and Future Work In this work, we revealed a mismatching issue in zero-shot QE modeling. To alleviate it, we introduced two explicit cross-lingual patterns based on BERTScore backbone. Extensive experiments indicated that our proposed patterns, without finetuning, the QE model can be improved marginally. Notably, our zero-shot QE method outperforms supervised QE model on 2 out of 6 directions, shedding light on zero-shot QE researches. In the future, we plan to explore more strategies for incorporating various auxiliary information and better in-domain fine-tuning  (Gururangan et al., 2020)  or introduce an non-autoregressive refiner  (Wu et al., 2020)  to address our revealed mismatching issue. Also, it will be interesting to apply QE metrics on document-level machine translations with considering the dropped pronoun  (Wang et al., 2016a (Wang et al., , 2018 . Weiyue  Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016b . Character: Translation edit rate on character level. In WMT. Di Wu, Liang Ding, Fan Lu, and J. Xie. 2020. Slotrefine: A fast non-autoregressive model for joint intent detection and slot filling. In EMNLP. E Yankovskaya, A T?ttar, M Fishel Volume 3 Shared Task Papers, Day, and 2019. 2019. Quality estimation and translation metrics via pre-trained word and sentence embeddings. In WMT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In ICLR. Figure 1:Example of mismatching error, Russian?English. On the left, token "?" is mismatched to "The" with the maximal probability (within the red rectangle) only. On the right, guided by our proposed cross-lingual patterns, "?" is correctly matched to the token "named" with the maximal probability (within the green rectangle.) 
