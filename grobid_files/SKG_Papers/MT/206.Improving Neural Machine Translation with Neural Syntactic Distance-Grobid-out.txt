title
Improving Neural Machine Translation with Neural Syntactic Distance

abstract
The explicit use of syntactic information has been proved useful for neural machine translation (NMT). However, previous methods resort to either tree-structured neural networks or long linearized sequences, both of which are inefficient. Neural syntactic distance (NSD) enables us to represent a constituent tree using a sequence whose length is identical to the number of words in the sentence. NSD has been used for constituent parsing, but not in machine translation. We propose five strategies to improve NMT with NSD. Experiments show that it is not trivial to improve NMT with NSD; however, the proposed strategies are shown to improve translation performance of the baseline model (+2.1 (En-Ja), +1.3 (Ja-En), +1.2 (En-Ch), and +1.0 (Ch-En) BLEU).

Introduction In recent years, neural machine translation (NMT) has been developing rapidly and has become the de facto approach for machine translation. To improve the performance of the conventional NMT models  Bahdanau et al., 2014) , one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model. Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures  (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017) , and (2) those that use linear-structured NNs to represent linearized syntax structures  Ma et al., 2017 Ma et al., , 2018 . For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in- * Corresponding author efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem. Although using a shorter sequence may improve the efficiency, some syntactic information is lost. We propose a method of using syntactic information in NMT that overcomes the disadvantages of both methods. The basis of our method is the neural syntactic distance (NSD), a recently proposed concept used for constituent parsing  (Shen et al., 2018; G?mez-Rodr?guez and Vilares, 2018) . NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. ? Extend NSD to dependency trees, which is inspired by the dependency language model  (Shen et al., 2010) . ? Use NSDs as input sequences 1 , where an NSD is regarded as a linguistic input feature  (Sennrich and Haddow, 2016 ). ? Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning  (Firat et al., 2016) . ? Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer  (Vaswani et al., 2017) . ? Add a loss function for NSD to achieve distance-aware training  (Shen et al., 2018) . 

 Neural Syntactic Distance (NSD) The NSD was firstly proposed by  Shen et al. (2018) . This is the first method of linearizing a constituent tree with a sequence of length n, without loss of information, where n is the number of words in the sentence. Formally, given the sentence w = (w 1 , . . . , w n ), for any pairs of contiguous words (w i , w i+1 ), we can define an NSD d(w i ), 2 where i ? [1, n ? 1]. In  Shen et al. (2018) , the NSD d S (w i ) is defined as the height of the lowest common ancestor (LCA) of the words.  3  Subsequently, in  G?mez-Rodr?guez and Vilares (2018) , the NSD d G (w i ) was defined as the number of the common ancestors of the words. To make the definition complete, we define d(w n ) as follows: 4 d S (w n ) = H, d G (w n ) = 0, (1) where H is the height of the constituent tree. It is easy to prove that d S (w i ) + d G (w i ) = H, i ? [1, n]. (2) We call d S and d G the absolute NSD. Furthermore, G?mez-Rodr?guez and Vilares (2018) define the relative NSD as follows: d R (w i ) = d G (w 1 ), i = 1, d G (w i ) ? d G (w i?1 ), i ? [2, n]. (3) Figure  1  illustrates these NSDs. It is easy to see the one-to-one correspondence relationship between the constituent tree and the (absolute or relative) NSDs. The effectiveness of all different NSDs has been proven on constituent parsing. However, there has been no attempt to use NSD in machine translation. 3 Strategies to improve NMT with NSD 

 Dependency NSD There are many previous studies on using dependency trees to improve NMT  (Nguyen Le et al., 2017; Wu et al., 2017) . Therefore, we extend NSD to dependency trees. Formally, the dependency NSD between two nodes is defined as follows: d D (w i ) = i ? h(i), (4) where h(i) is the index of the head of w i , and we let the index of root be 0. Note that d D (w i ) can be either positive or negative, representing the directional information. Figure  2  gives an example. 

 NSDs as Input Sequences It is easy to see that for w = (w 1 , .  e d i = E d [d d i + (max(d) ? min(d) + 1)]. (5) We call E d the distance embedding matrix and call e d the syntactic embedding sequence. Note that d can be the NSD on either the source side or the target side, so there are two possible E d , which are denoted as E s d and E t d , respectively. The embeddings are calculated as follows: x s i = f emb (E s w [w s i ], e ds i ), (6) x t i = f emb (E t w [w t i ], e dt i ), (7) where e ds i and e dt i are defined in Eq. 5 on the source side and target side, respectively, and E s w and E t w are the word embedding matrices on both sides, respectively. Inspired by  Sennrich and Haddow (2016) , function f emb is used to combine two vectors. This function has many different options, such as: f emb (x, e) = x e, (8) f + emb (x, e) = x + e, (9) f W b emb (x, e) = W f ? (x e) + b f , (10) where x, e, b f ? R d and W f ? R d?2d . The operator " " is the concatenation of two vectors. When NSD is used as the input sequence on the target side, there is one problem: e dt is unknown during testing. For this case, we use NSDs for both the input and output sequences, let the decoder predict NSD on-the-fly using the strategy introduced in Section 3.3, and use the predicted NSD to calculate e dt . 

 NSDs as Output Sequences An NSD can be used to form the output sequence to improve NMT using the idea of multi-task learning. Specifically, we train the model to predict the NSD sequence. When NSD is used as the output sequence of the encoder, we minimize the distance (e.g., cross entropy L ent dist , see Section 3.5 for details) between the predicted and the golden NSD sequences. When NSD is used as the output sequence of the decoder, besides minimizing the distance, we use the predicted NSD as the input of the next time step. Denote the hidden vector as h = (h 1 , . . . , h n ). For the encoder, h i = h s i and n = n s , while for the decoder, h i = h t i and n is the current time step of decoding. Then, we can obtain a sequence of predicted syntactic distance d = ( d1 , . . . , dn ), which is calculated as follows: p( di | h i ) = softmax(W d ? h i + b d ), (11) where W d and b d are parameters to be learned. By minimizing the distance between di and d i , NSD can be used to enhance NMT. 

 NSD as Positional Encoding (PE) PE is used by the Transformer  (Vaswani et al., 2017)  to encode the positions of words. Formally, it is defined as follows: x i = x i + P E(i), (12) P E(i) 2k = sin(i/10000 2k/d ), P E(i) 2k+1 = cos(i/10000 2k/d ), (13) where x i can be either x s i or x t i , and d is the dimension of the embedding vector. Similarly, we define syntactic PE as follows: P E(i) 2k = sin i + max(d) ? min(d) ? SP E 2k/d ? 2? , (15) P E(i) 2k+1 = cos i + max(d) ? min(d) ? SP E 2k/d ? 2? , (16) where ? SP E is a hyperparameter to be tuned. In this way, the periods of these two functions vary from 1 to ? SP E . We define syntactic PE in this way because (1) according to a quantitative analysis of the experimental datasets, we found that the ranges of possible values are quite different between NSD and word positions, so we tuned ? SP E instead of fixed it to 10000 as in Eqs. 13 and 14, and (2) d i may be negative, so we adjust it to be positive. 

 Distance-aware Training Instead of using conventional cross-entropy loss function during training, we use the following loss function to make the NMT model learn NSD better: L = L N M T + L dist + L ent dist . (17) The first item is the cross-entropy loss of the NMT model, which is L N M T = ? w s ,w t ?D log p(w t | w s ), ( 18 ) where D is the training dataset. The second item is the distance-aware loss, which is inspired by  Shen et al. (2018)  and is as follows: L dist = w s ,w t ?D (L s dist (w s ) + L t dist (w t )), L s dist (w s ) = ns i=1 (d i ? di ) 2 + i,j>i [1 ? sign(d i ? d j )( di ? dj )] + , (19) and L t dist can be defined similarly. The third item is the cross-entropy loss for NSD, which is as follows: L ent dist = w s ,w t ?D (L ent(s) dist (w s ) + L ent(t) dist (w t )), L ent(s) dist (w s ) = ? d i ?d s p(d i | h i ) log p( di | h i ), (20) and L ent(t) dist can be defined similarly. 

 Experiments 

 Configuration We experimented on two corpora: (1) ASPEC  (Nakazawa et al., 2016) , using the top 100K sentence pairs for training En-Ja models and top 1M sentence pairs for training Ja-En models, and (2) LDC, 5 which contains about 1.2M sentence pairs, for training En-Ch and Ch-En models. To tackling the problem of memory consumption, sentences longer than 150 were filtered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.  6  For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.  7  The test set is a concatenation of  NIST MT 2003 , 2004 , and 2005 . Constituent trees are generated by the parser of  Kitaev and Klein (2018)    8  , and dependency trees are generated by the parser of  Dyer et al. (2015)    9  . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT 10 (  Klein et al., 2017) , and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention has 5 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06. 6 https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 8 https://github.com/nikitakit/ self-attentive-parser 9 https://github.com/clab/lstm-parser 10 http://opennmt.net 8 heads, and the dropout probability is 0.1  (Srivastava et al., 2014) . The number of training epochs was fixed to 50, and we used the model which performs the best on the development set for testing. As for optimization, we used the Adam optimizer (Kingma and Ba, 2014), with ? 1 = 0.9, ? 2 = 0.998, and = 10 ?9 . Warmup and decay strategy for learning rate of  Vaswani et al. (2017)  are also used, with 8, 000 warmup steps. We also used the label smoothing strategy  (Szegedy et al., 2016)  with ls = 0.1. 

 Experimental Results Table  1  compares the effects of the strategies. We evaluate the proposed strategies using characterlevel BLEU  (Papineni et al., 2002)  for Chinese and Japanese, and case-insensitive BLEU for English. Comparison of different NSDs. The first five rows of Table  1  compare the results of using different NSDs. When NSD was used at the source side (En-Ja/En-Ch), all kinds of NSDs improved translation performance. This indicates that NSD can be regarded as a useful linguistic feature to improve NMT. In contrast, when NSD was used at the target side (Ja-En/Ch-En), d S and d G hurt the performance. This is because the values of d S and d G are volatile. A tiny change of syntactic structure often causes a big change of d S and d G . Since the model has to predict the NSD during decoding, once there is one error, the subsequent predictions will be heavily influenced. The use of d R and d D remedies this problem. Furthermore, the effects of d S and d G are similar, because they are equivalent in nature (refer to Eq. 2). NSD as PE. Rows 5 to 8 of Table  1  evaluate the use of dependency NSD (d D ) as syntactic PE. Note that for all the experiments, we used not only the syntactic PE but the conventional PE. Experiment results show that this strategy is indeed useful. When the dominators of Eqs. 15 and 16, ? SP E , were set to 10 4 , there was no improvement. When they were set to 40, the improvement was remarkable. This indicates that our design of syntactic PE is reasonable. NSD as input/output and source/target sequences. Rows 8 to 12 of Table  1  are the results of using dependency NSD (i.e., d D ) as the input and/or output sequences on both sides. First, for the choice of f emb , we can see that f emb and f + emb are similar, while f W b emb yields better performance. This is because the model has to learn W f and b f , which increases the model capacity. Second, performance improved for using NSDs both as input and output sequences, and combining both obtained further improvement. Third, NSDs improved the performance both on the source and the target sides. All these results indicate the robustness of NSDs. Effects of distance-aware training. The last three rows compare the different effects of the items in the loss function. When only L N M T are used, the performance is extremely poor. This is within expectations, because with only L N M T , weights related to NSDs are kept to the initial values and were not updated, and hence detrimental to learning. Adding L ent dist improves the results significantly, but the improvement is lower than that of L dist . This is because training with L ent dist treats different values of NSDs equally, while L dist penalizes larger differences between the predicted NSD and the golden NSD more severely. 

 Conclusion We proposed five strategies to improve NMT with NSD. We found relative NSDs and dependency NSDs are able to improve the performance consistently, while absolute NSDs hurt the performance for some cases. The improvement obtained by using NSDs is general in that NSDs can be used at both the source side and target side, both as input sequences and output sequences. Using NSDs as syntactic PE is also useful, and training with a distance-aware loss function is quite important. Figure 1 :Figure 2 : 12 Figure 1: Example of different NSDs. This example is from Shen et al. (2018). 
