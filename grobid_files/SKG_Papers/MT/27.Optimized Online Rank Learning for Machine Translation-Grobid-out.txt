title
Optimized Online Rank Learning for Machine Translation

abstract
We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results.

Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT)  (Och, 2003) , which directly minimize the loss of translation evaluation measures, i.e. BLEU  (Papineni et al., 2002) . MERT has been successfully used in practical applications, although, it is known to be unstable . To overcome this instability, it requires multiple runs from random starting points and directions  (Moore and Quirk, 2008) , or a computationally expensive procedure by linear programming and combinatorial optimization  (Galley and Quirk, 2011) . 

 Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron  (Liang et al., 2006) , maximum entropy  (Och and Ney, 2002; Blunsom et al., 2008) , Margin Infused Relaxed Algorithm (MIRA)  (Watanabe et al., 2007; Chiang et al., 2008b) , or pairwise rank optimization (PRO)  (Hopkins and May, 2011) . They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin  (Taskar et al., 2004) , conditional loglikelihood (or softmax loss)  (Berger et al., 1996) , risk  (Smith and Eisner, 2006; Li and Eisner, 2009) , or ranking  (Herbrich et al., 1999) . We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size  (Shalev-Shwartz et al., 2007) . Like  Hopkins and May (2011) , we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by  Haddow et al. (2011) , BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score  (Chiang et al., 2008a) , and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm  (Crammer et al., 2006) , in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the ranking loss. Learning is efficiently parallelized by splitting training data among shards and by merging parameters in each round  (McDonald et al., 2010) . Instead of simple averaging, we perform an additional line search step to find the optimal merging across parallel jobs. Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. We found significant gains over traditional MERT and other tuning algorithms, such as MIRA and PRO. 

 Statistical Machine Translation SMT can be formulated as a maximization problem of finding the most likely translation e given an input sentence f using a set of parameters ?  (Brown et al., 1993)  ? = arg max e p(e|f ; ?). (1) Under this maximization setting, we assume that p(?) is represented by a linear combination of feature functions h(f, e) which are scaled by a set of parameters w  (Och and Ney, 2002 ) ? = arg max e w ? h(f, e). ( 2 ) Each element of h(?) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of translated words or log of phrasal probability. In this paper, we concentrate on the problem of learning w, which is referred to as tuning. One of the standard methods for parameter tuning is minimum error rate training  (Och, 2003)  (MERT) which directly minimizes the task loss ?(?), i.e. negative BLEU  (Papineni et al., 2002) , given training data D = {(f 1 , e 1 ), ..., (f N , e N )}, sets of paired source sentence f i and its reference translations e i ? = arg min w ?( { arg max e w ? h(f i , e) } N i=1 , { e i } N i=1 ). (3) The objective in Equation 3 is discontinuous and non-convex, and it requires decoding of all the training data given w. Therefore, MERT relies on a derivative-free unconstrained optimization method, such as Powell's method, which repeatedly chooses one direction to optimize using a line search procedure as in Algorithm 1. Expensive decoding is approximated by an n-best merging technique in which decoding is carried out in each epoch of iterations t and the maximization in Eq. 3 is approxi-Algorithm 1 MERT 1: Initialize w 1 2: for t = 1, ..., T do ? Or, until convergence 3: Generate n-bests using w t 

 4: Learn new w t+1 by Powell's method 5: end for 6: return w T +1 mated by search over the n-bests merged across iterations. The merged n-bests are also used in the line search procedure to efficiently draw the error surface for efficient computation of the outer minimization of Eq. 3. 3 Online Rank Learning 

 Rank Learning Instead of the direct task loss minimization of Eq. 3, we would like to find w by solving the L 2regularized constrained minimization problem arg min w ? 2 ?w? 2 2 + ?(w; D) (4) where ? > 0 is a hyperparameter controlling the fitness to the data. The loss function ?(?) we consider here is inspired by a pairwise ranking method (Hopkins and May, 2011) in which pairs of correct translation and incorrect translation are sampled from nbests and suffer a hinge loss 1 M (w; D) ? (f,e)?D ? e * ,e ? max { 0, 1 ? w ? ?(f, e * , e ? ) 

 } (5) where e ? ? NBEST(w; f ) \ ORACLE(w; f, e) e * ? ORACLE(w; f, e) ?(f, e * , e ? ) = h(f, e * ) ? h(f, e ? ). NBEST(?) is the n-best translations of f generated with the parameter w, and ORACLE(?) is a set of oracle translations chosen among NBEST(?). Note that each e ? (and e * ) implicitly represents a derivation consisting of a tuple (e ? , ?), where ? is a latent structure, i.e. phrases in a phrase-based SMT, but we omit ? for brevity. M (?) is a normalization constant which is equal to the number of paired loss terms ?(f, e * , e ? ) in Equation  5 . Since it is impossible to enumerate all possible translations, we follow the convention of approximating the domain of translation by n-bests. Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the nbests other than those in ORACLE(?). Oracle translations are selected by minimizing the task loss, ?( { e ? ? NBEST(w; f i ) } N i=1 , { e i } N i=1 ) i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests  (Venugopal, 2005) . Equation 5 can be easily interpreted as a constant loss "1" for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning  (Taskar et al., 2004) . As an alternative, we would also consider a softmax loss  (Collins and Koo, 2005)  represented by 1 N ? (f,e)?D ? log Z O (w; f, e) Z N (w; f ) (6) where Z O (w; f, e) = ? e * ?ORACLE(w;f,e) exp(w ? f (f, e * )) Z N (w; f ) = ? e ? ?NBEST(w;f ) exp(w ? f (f, e ? )). Equation 6 is a log-linear model used in common NLP tasks such as tagging, chunking and named entity recognition, but differ slightly in that multiple correct translations are discriminated from the others  (Charniak and Johnson, 2005) . 

 Online Approximation Hopkins and May (2011) applied a MERT-like procedure in Alg. 1 in which Equation  4 was solved to obtain new parameters in each iteration. Here, we employ stochastic gradient descent (SGD) methods as presented in Algorithm 2 motivated by Pegasos  (Shalev-Shwartz et al., 2007) . In each iteration, we randomly permute D and choose a set of batches B t = {b t 1 , ..., b t K } with each b t j consisting of N/K training data. For each batch b in B t , we generate n-bests from the source sentences in b and compute oracle translations from the newly created n-bests Algorithm 2 Stochastic Gradient Descent 1: k = 1, w 1 ? 0 2: for t = 1, ..., T do 3: Choose B t = {b t 1 , ..., b t K } from D 4: for b ? B t do 5: Compute n-bests and oracles of b 6: Set learning rate ? k 7: w k+ 1 2 ? w k ? ? k ?(w k ; b) ? Our proposed algorithm solve Eq. 12 or 16 8: w k+1 ? min { 1, 1/ ? ? ?w k+ 1 2 ? 2 } w k+ 1 2 9: k ? k + 1 10: end for 11: end for 12: return w k (line 5) using a batch local corpus-BLEU  (Haddow et al., 2011) . Then, we optimize an approximated objective function arg min w ? 2 ?w? 2 2 + ?(w; b) (7) by replacing D with b in the objective of Eq. 4. The parameters w k are updated by the sub-gradient of Equation  7 , ?(w k ; b), scaled by the learning rate ? k (line 7). We use an exponential decayed learning rate ? k = ? 0 ? k/K , which converges very fast in practice  (Tsuruoka et al., 2009)  1 . The sub-gradient of Eq.7 with the hinge loss of Eq. 5 is ?w k ? 1 M (w k ; b) ? (f,e)?b ? e * ,e ? ?(f, e * , e ? ) (8) such that 1 ? w ? k ?(f, e * , e ? ) > 0. ( 9 ) We found that the normalization term by M (?) was very slow in convergence, thus, instead, we used M ? (w; b), which was the number of paired loss terms satisfied the constraints in Equation 9. In the case of the softmax loss objective of Eq. 6, the subgradient is ?w k ? 1 |b| ? (f,e)?b ? ?w L(w; f, e) w=w k (10) where L(w; f, e) = log (Z O (w; f, e)/Z N (w; f )). After the parameter update, w k+ 1 2 is projected within the L 2 -norm ball  (Shalev-Shwartz et al., 2007) . Setting smaller batch size implies frequent updates to the parameters and a faster convergence. However, as briefly mentioned in  Haddow et al. (2011) , setting batch size to a smaller value, such as |b| = 1, does not work well in practice, since BLEU is devised for a corpus based evaluation, not for an individual sentence-wise evaluation, and it is not linearly decomposed into a sentence-wise score  (Chiang et al., 2008a) . Thus, the smaller batch size may also imply less accurate batch-local corpus-BLEU and incorrect oracle translation selections, which may lead to incorrect sub-gradient estimations or slower convergence. In the next section we propose an optimized parameter update which works well when setting a smaller batch size is impractical due to its task loss setting. 

 Optimized Online Rank Learning 

 Optimized Parameter Update In line 7 of Algorithm 2, parameters are updated by the sub-gradient of each training instance in a batch b. When the sub-gradient in Equation 8 is employed, the update procedure can be rearranged as w k+ 1 2 ? (1? k )w k + ? (f,e)?b,e * ,e ? ? k M (w k ; b) ?(f, e * , e ? ) (11) in which each individual loss term ?(?) is scaled uni- formly by a constant ? k /M (?). Instead of the uniform scaling, we propose to update the parameters in two steps: First, we suffer the sub-gradient from the L 2 regularization w k+ 1 4 ? (1 ? ? k )w k . Second, we solve the following problem arg min w 1 2 ?w ? w k+ 1 4 ? 2 2 + ? k ? (f,e)?b,e * ,e ? ? f,e * ,e ? (12) such that w ? ?(f, e * , e ? ) ? 1 ? ? f,e * ,e ? ? f,e * ,e ? ? 0. The problem is inspired by the passive-aggressive algorithm  (Crammer et al., 2006)  in which new parameters are derived through the tradeoff between the amount of updates to the parameters and the margin-based loss. Note that the objective in MIRA is represented by We used a dual coordinate descent algorithm  2 to efficiently solve the quadratic program (QP) in Equation  14 , leading to an update arg min w ? 2 ?w ? w k ? 2 2 + ? (f, w k+ 1 2 ? w k+ 1 4 + ? (f,e)?b,e * ,e ? ? e * ,e ? ?(f, e * , e ? ). (15) When compared with Equation  11 , the update procedure in Equation 15 rescales the contribution from each sub-gradient through the Lagrange multipliers ? e * ,e ? . Note that if we set ? e * ,e ? = ? k /M (?), we satisfy the constraints in Eq. 14, and recover the update in Eq. 11. In the same manner as Eq. 12, we derive an optimized update procedure for the softmax loss, which replaces the update with Equation  10 , by solving the 2 Specifically, each parameter is bound constrained 0 ? ? ? ? k but is not summation constrained ? ? ? ? k . Thus, we renormalize ? after optimization. following problem arg min w 1 2 ?w ? w k+ 1 4 ? 2 2 + ? k ? (f,e)?b ? f (16) such that w ? ?(w k ; f, e) ? ?L(w k ; f, e) ? ? f ? f ? 0 in which ?(w ? ; f, e) = ? ?w L(w; f, e) w=w ? . Equation 16 can be interpreted as a cutting-plane approximation for the objective of Eq. 7, in which the original objective of Eq. 7 with the softmax loss in Eq. 6 is approximated by |b| linear constraints derived from the sub-gradients at point w k  (Teo et al., 2010) . Eq. 16 is efficiently solved by its Lagrange dual, leading to an update w k+ 1 2 ? w k+ 1 4 + ? (f,e)?b ? f ?(w k ; f, e) (17) subject to ? (f,e)?b ? f ? ? k . Similar to Eq. 15, the parameter update by ?(?) is rescaled by its Lagrange multipliers ? f in place of the uniform scale of 1/|b| in the sub-gradient of Eq. 10. We propose an optimized parallel training in Algorithm 3 which performs better mixing with respect to the task loss, i.e. negative BLEU. In line 5, w t+ 1 2 is computed by averaging w t+1,s from all the shards after local training using their own data D s . Then, the new parameters w t+1 are obtained by linearly interpolating with the parameters from the previous epoch w t . The linear interpolation weight ? is efficiently computed by a line search procedure which directly minimizes the negative corpus-BLEU. The procedure is exactly the same as the line search strategy employed in MERT using w t as our starting point with the direction w t+ 1 2 ? w t . The idea of using the line search procedure is to find the optimum parameters under corpus-BLEU without a Algorithm 3 Distributed training with line search 1: w 1 ? 0 2: for t = 1, ..., T do 3: 

 Line Search for Parameter Mixing w t,s ? w t ? Distribute parameters 4: Each shard learns w t+1,s using D s ? Line 3-10 in Alg. 2 5: w t+ 1 2 ? 1/S ? s w t+1,s ? Mixing 6: w t+1 ? (1 ? ?)w t + ?w t+ 1 2 ? Line search 7: end for 8: return w T +1 batch-local approximation. Unlike MERT, however, we do not memorize nor merge all the n-bests generated across iterations, but keep only n-bests in each iteration for faster training and for memory saving. Thus, the optimum ? obtained by the line search may be suboptimal in terms of the training objective, but potentially better than averaging for minimizing the final task loss. 

 Experiments Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. The training data consists of nearly 5.6 million bilingual sentences and additional monolingual data, English Gigaword, for 5-gram language model estimation. MT02 and MT06 were used as our tuning and development testing, and MT08 as our final testing with all data consisting of four reference translations. We use an in-house developed hypergraph-based toolkit for training and decoding with synchronous-CFGs (SCFG) for hierarchical phrase-bassed SMT  (Chiang, 2007) . The system employs 14 features, consisting of standard Hiero-style features  (Chiang, 2007) , and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model . For faster experiments, we precomputed translation forests inspired by  Xiao et al. (2011) . Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences and by keeping the target side of the inter-sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework  (Huang and Chiang, 2007)  with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss 3 . MERT learns parameters from forests  (Kumar et al., 2009)  with 4 restarts and 8 random directions in each iteration. We experimented on a variant of PRO 4 , in which the objective in Eq. 4 with the hinge loss of Eq. 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver 5 . Our MIRA solves the problem in Equation 13 in line 7 of Alg. 2. For a systematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO. For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration. The hyperparameter ? for PRO and ORO was set to 10 ?5 , selected from among {10 ?3 , 10 ?4 , 10 ?5 }, and 10 2 for MIRA, chosen from {10, 10 2 , 10 3 } by preliminary testing on MT06. Both decoding and learning are parallelized and run on 8 cores. Each online learning took roughly 12 hours, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU. Table  1  presents our main results. Among the parameters from multiple iterations, we report the outputs that performed the best on MT06. With Moses  (Koehn et al., 2007) , we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively. We denote the "O-" prefix for the optimized parameter updates discussed in Section 4.1, and the "-L" suffix csie.ntu.edu.tw/ ?cjlin/liblinear with the solver type of 3.   31.16 ? 23.20  Table  1 : Translation results by BLEU. Results without significant differences from the MERT baseline are marked ?. The numbers in boldface are significantly better than the MERT baseline (both measured by the bootstrap resampling  (Koehn, 2004)  with p > 0.05). for parameter mixing by line search as described in Section 4.2. The batch size was set to 16 for MIRA and ORO. In general, our PRO and MIRA settings achieved the results very comparable to MERT. The hinge-loss and softmax objective OROs were lower than those of the three baselines. The softmax objective with the optimized update (O-ORO-L softmax ) performed better than the non-optimized version, but it was still lower than our baselines. In the case of the hinge-loss objective with the optimized update (O-ORO-L hinge ), the gain in MT08 was significant, and achieved the best BLEU. Figure  1  presents the learning curves for three algorithms MIRA-L, ORO-L hinge and O-ORO-L hinge , in which the performance is measured by BLEU on the training data (MT02) and on the test data (MT08). MIRA-L quickly converges and is slightly unstable in the test set, while ORO-L hinge is very stable and slow to converge, but with low performance on the training and test data. The stable learning curve in ORO-L hinge is probably influenced by our learning rate parameter ? 0 = 0.2, which will be investigated in future work. O-ORO-L hinge is less stable in several iterations, but steadily improves its BLEU. The behavior is justified by our optimized update procedure, in which the learning rate ? k is used as a tradeoff parameter. Thus, it tries a very aggressive update at the early stage of training, but eventually becomes conservative in updating parameters. Next, we compare the effect of line search for parameter mixing in Table  2 . Line search was very effective for MIRA and O-ORO hinge , but less effective for the others. Since the line search procedure directly minimizes a task loss, not objectives, this may hurt the performance for the softmax objective, where the margins between the correct and incorrect translations are softly penalized. Finally, Table  3  shows the effect of batch size selected from {1, 4, 8, 16}. There seems to be no clear trends in MIRA, and we achieved BLEU score of 24.58 by setting the batch size to 8. Clearly, setting smaller batch size is better for ORO, but it is the reverse for the optimized variants of both the hinge and softmax objectives. ing of the learning rate parameter will be required for a larger batch size. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpus-BLEU. 

 Related Work Our work is largely inspired by pairwise rank optimization  (Hopkins and May, 2011) , but runs in an online fashion similar to  (Watanabe et al., 2007; Chiang et al., 2008b) . Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by  Hopkins and May (2011)  or the corpus-BLEU statistics accumulated from previous translations generated by different parameters  (Watanabe et al., 2007; Chiang et al., 2008b) , we used a simple batch local corpus-BLEU  (Haddow et al., 2011)  in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation  (Smith and Eisner, 2006; Rosti et al., 2011) , which was not investigated in this paper. Training is performed by SGD with a parameter projection method  (Shalev-Shwartz et al., 2007)  for more accurate corpus-BLEU is addressed by optimally scaling parameter updates in the spirit of a passive-aggressive algorithm  (Crammer et al., 2006) . The derived algorithm is very similar to MIRA, but differs in that the learning rate is employed as a hyperparameter for controlling the fitness to training data which decays when training proceeds. The non-uniform sub-gradient based update is also employed in an exponentiated gradient (EG) algorithm  (Kivinen and Warmuth, 1997; Kivinen and Warmuth, 2001)  in which parameter updates are maximum-likely estimated using an exponentially combined sub-gradients. In contrast, our approach relies on an ultraconservative update which tradeoff between the amount of updates performed to the parameters and the progress made for the objectives by solving a QP subproblem. Unlike a complex parallelization by  Chiang et al. (2008b) , in which support vectors are asynchronously exchanged among parallel jobs, training is efficiently and easily carried out by distributing training data among shards and by mixing parameters in each iteration  (McDonald et al., 2010) . Rather than simple averaging, new parameters are derived by linearly interpolating with the previously mixed parameters, and its weight is determined by the line search algorithm employed in  (Och, 2003) . 

 Conclusion We proposed a variant of an online learning algorithm inspired by a batch learning algorithm of  (Hopkins and May, 2011) . Training is performed by SGD with a parameter projection  (Shalev-Shwartz et al., 2007)  using a larger batch size for a more accurate batch local corpus-BLEU estimation. Parameter updates in each iteration is further optimized using an idea from a passive-aggressive algorithm  (Cram-mer et al., 2006) . Learning is efficiently parallelized  (McDonald et al., 2010)  and the locally learned parameters are mixed by an additional line search step. Experiments indicate that better performance was achieved by our optimized updates and by the more sophisticated parameter mixing. In future work, we would like to investigate other objectives with a more direct task loss, such as maxmargin  (Taskar et al., 2004) , risk  (Smith and Eisner, 2006)  or softmax-loss (Gimpel and Smith, 2010), and different regularizers, such as L 1 -norm for a sparse solution. Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation  (Li and Eisner, 2009) . We would also like to explore other mixing strategies for parallel training which can directly minimize the training objectives like those proposed for a cutting-plane algorithm  (Franc and Sonnenburg, 2008) . e)?b,e * ,e ? ? f,e * ,e ? (13) If we treat w k+ 1 4 as our previous parameters and set ? = 1/? k , they are very similar. Unlike MIRA, the learning rate ? k is directly used as a tradeoff parameter which decays as training proceeds, and the subgradient of the global L 2 regularization term is also combined in the problem through w k+ 1 e)?b,e * ,e ? ? e * ,e ? ?(f, e * , e ? )? e)?b,e * ,e ? ? e * ,e ? ? ? k . 
