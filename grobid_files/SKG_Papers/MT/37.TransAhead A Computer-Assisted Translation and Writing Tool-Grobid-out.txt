title
TransAhead: A Computer-Assisted Translation and Writing Tool *

abstract
We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users.

Introduction More and more language workers and learners use the MT systems on the Web for information gathering and language learning. However, web translation systems typically offer top-1 translations (which are usually far from perfect) and hardly interact with the user. Text translation could be achieved more interactively and effectively if a system considered translation as a collaborative between the machine generating suggestions and the user accepting or overriding on those suggestions, with the system adapting to the user's action. Consider the source sentence "? ?" (We play an important role in closing this deal). The best man-machine interaction is probably not the one used by typical existing MT systems. A good working environment might be a translation assistant that offers suggestions and gives the user direct control over the target text. We present a system, TransAhead 1 , that learns to predict and suggest lexical translations (and their grammatical patterns) likely to follow the ongoing translation of a source text, and adapts to the user's choices. Example responses of TransAhead to the source sentence "? ? ? ? ? ? ? ? ? " and two partial translations are shown in Figure  1 . The responses include text and grammatical patterns (in all-cap labels representing parts-of-speech). TransAhead determines and displays the probable subsequent grammatical constructions and partial translations in the form of parts-of-speech and words (e.g., "IN[in] VBG[close,?]" for keywords "play role" where lexical items in square brackets are lemmas of potential translations) in a pop-up. TransAhead learns these constructs and translations during training. At run-time, TransAhead starts with a source sentence, and iterates with the user, making predictions on the grammar patterns and lexical translations, while adapting to the user's translation choices to resolve ambiguities in the source sentence related to word segmentation and word sense. In our prototype, TransAhead mediates between users and suggestion modules to translation quality and productivity. 

 Related Work Computer Assisted Translation (CAT) has been an area of active research. We focus on offering suggestions during the translation process with an emphasis on language learning. Specifically, our goal is to build a translation assistant to help translator (or learner-translator) with inline grammar help and translation. Unlike recent research focusing on professional (e.g.,  Brown and Nirenburg, 1990) , we target on both professional and student translators. More recently, interactive MT (IMT) systems have begun to shift the user's role from postediting machine output to collaborating with the machine to produce the target text.  Foster et al (2000)  describe TransType, a pioneering system that supports next word predictions. Along the similar line,  Koehn (2009)  develops caitra which predicts and displays phrasal translation suggestions one phrase at a time. The main difference between their systems and TransAhead is that we also display grammar patterns to provide the general patterns of predicted translations so a student translator can learn and become more proficient. Recent work has been done on using fullyfledged statistical MT systems to produce target hypotheses completing user-validated translation prefix in IMT paradigm.  Barrachina et al. (2008)  investigate the applicability of different MT kernels within IMT framework.  Nepveu et al. (2004)  and  Ortiz-Martinez et al. (2011)  further exploit user feedbacks for better IMT systems and user experience. Instead of triggered by user correction, our method is triggered by word delimiter and assists both translation and learning the target language. In contrast to the previous CAT research, we present a writing assistant that suggests grammar constructs as well as lexical translations following users' partial translation, aiming to provide users with choice to ease mental burden and enhance performance. 

 The TransAhead System 

 Problem Statement We focus on predicting a set of grammar patterns with lexical translations likely to follow the current partial target translation of a source text. The predictions will be examined by a human user directly. Not to overwhelm the user, our goal is to return a reasonable-sized set of predictions that contain suitable word choices and grammatical patterns to choose and learn from. Formally, Problem Statement: We are given a targetlanguage reference corpus Ct, a parallel corpus Cst, a source-language text S, and its translation prefix T p . Our goal is to provide a set of predictions based on Ct and Cst likely to further translate S in terms of grammar and text. For this, we transform S and T p into sets of ngrams such that the predominant grammar constructs with suitable translation options following T p are likely to be acquired.  

 Learning to Find Pattern and Translation In the training stage, we find and store syntaxbased phraseological tendencies and translation pairs. These patterns and translations are intended to be used in a real-time system to respond to user input speedily. First, we part of speech tag sentences in Ct. Using common phrase patterns (e.g., the possessive noun one's in "make up one's mind") seen in grammar books, we resort to parts-ofspeech (POS) for syntactic generalization. Then, we build up inverted files of the words in Ct for the next stage (i.e., pattern grammar generation). Apart from sentence and position information, a word's lemma and POS are also recorded. Subsequently, we use the procedure in Figure  2  to generate grammar patterns following any given sequence of words, either contiguous or skipped. The algorithm first identifies the sentences containing the given sequence of words, query. Iteratively, Step (3) performs an AND operation on the inverted file, InvList, of the current word w i and interInvList, a previous intersected results. After that, we analyze query's syntax-based phraseology (Step (5)). For each element of the form ([wordPosi(w 1 ),?, wordPosi(w n )], sentence number) denoting the positions of query's words in the sentence, we generate grammar pattern involving replacing words in the sentence with POS tags and words in wordPosi(w i ) with lemmas, and extracting fixed-window 2 segments surrounding query from the transformed sentence. The result is a set of grammatical patterns (i.e., syntax-based phraseology) for the query. The procedure finally returns top N predominant 2 Inspired by  (Gamon and Leacock, 2010) . syntactic patterns of the query. Such patterns characterizing the query's word usages in the spirit of pattern grammar in  (Hunston and Francis, 2000)  and are collected across the target language. In the fourth and final stage, we exploit Cst for bilingual phrase acquisition, rather than a manual dictionary, to achieve better translation coverage and variety. We obtain phrase pairs through a number of steps, namely, leveraging IBM models for bidirectional word alignments, grow-diagonalfinal heuristics to extract phrasal equivalences  (Koehn et al., 2003) . 

 Run-Time Grammar and Text Prediction Once translation equivalents and phraseological tendencies are learned, they are stored for run-time reference. TransAhead then predicts/suggests the following grammar and text of a translation prefix given the source text using the procedure in Figure  3 . We first slice the source text S into characterlevel ngrams, represented by {s i }. We also find the word-level ngrams of the translation prefix T p . But this time we concentrate on the ngrams, may skipped, ending with the last word of T p (i.e., pivoted on the last word) since these ngrams are most related to the subsequent grammar patterns. Step (  3 ) and (  4 ) retrieve translations and patterns learned from Section 3.2. Step (3) acquires the target-language active vocabulary that may be used to translate the source. To alleviate the word boundary issue in MT  (Ma et al. (2007) ), the word boundary in our system is loosely decided. Initially, TransAhead non-deterministically segments the source text using character ngrams for translations and proceeds with collaborations with the user to obtain the segmentation for MT and to complete the translation. Note that T p may reflect some translated segments, reducing the size of the active vocabulary, and that a user vocabulary of preference (due to users' domain knowledge or procedure PatternFinding(query,N,C t ) (1) interInvList=findInvertedFile(w1 of query) for each word wi in query except for w1 (2) InvList=findInvertedFile(wi) (3a) newInterInvList= ? ; i=1; j=1 (3b) while i<=length(interInvList) and j<=lengh(InvList) (3c) if interInvList[i].SentNo==InvList[j].SentNo (3d) Insert(newInterInvList, interInvList[i],InvList[j]) else (3e) Move i,j accordingly (3f) interInvList=newInterInvList (4) Usage= ? for each element in interInvList (5) Usage+={PatternGrammarGeneration(element,C t )} (6) Sort patterns in Usage in descending order of frequency (7) return the N patterns in Usage with highest frequency procedure MakePrediction(S,Tp) (1) Assign sliceNgram(S) to {si} (2) Assign sliceNgramWithPivot(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions and incorporate them into GramOptions (6) Return GramOptions errors of the system) may be exploited for better system performance. In addition, Step (  4 ) extracts patterns preceding with the history ngrams of {t j }. In Step (5), we first evaluate and rank the translation candidates using linear combination: At last, the algorithm returns the representative grammar patterns with confident translations expected to follow the ongoing translation and further translate the source. This algorithm will be triggered by word delimiter to provide interactive CAT and CALL environment. Figure  1  shows example responses of our working prototype. ( ) ( ) ( ) ( ) 1 

 Preliminary Results In developing TransAhead, we used British National Corpus and Hong Kong Parallel Text as target-language reference corpus and parallel training corpus respectively, and deployed GENIA tagger for lemma and POS analyses. To evaluate TransAhead in CAT and CALL, we introduced it to a class of 34 (Chinese) college freshmen learning English as foreign language. We designed TransAhead to be accessible and intuitive, so the user training tutorial took only one minute. After the tutorial, the participants were asked to translate 15 Chinese texts from  (Huang et al., 2011)  (half with TransAhead assistance called experimental group, and the other without any system help whatsoever called control group). The evaluation results show that the experimental group achieved much better translation quality than the control group with an average BLEU score  (Papineni et al., 2002)   difference in average translation quality between the experimental TransAhead group and the Google Translate, it is not hard for us to notice the source sentences were better translated by language learners with the help of TransAhead. Take the sentence "? ?" for example. A total of 90% of the participants in the experimental group produced more grammatical and fluent translations (see Figure  4 ) than that ("We conclude this transaction plays an important role") by Google Translate. Post-experiment surveys indicate that (a) the participants found Google Translate lack humancomputer interaction while TransAhead is intuitive to collaborate with in translation/writing; (b) the participants found TransAhead grammar and translation predictions useful for their immediate task and for learning; (c) interactivity made the translation and language learning a fun process (like image tagging game of (von Ahn and Dabbish, 2004)) and the participants found TransAhead very recommendable and would like to use it again in future translation tasks. Figure 1 . 1 Figure 1. Example TransAhead responses to a source text under the translation (a) "we" and (b) "we play an important role". Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes the confident translations of the source's character-based ngrams. The frequency of grammar pattern is shown in round brackets while the history (i.e., keyword) based on the user input is shown in shades. 

 predictions/suggestions: we MD VB[play, act, ..] (41369), ? we VBP[play, act, ..] DT (13138), ? we VBD[play, act, ..] DT (8139), ? Pop-up predictions/suggestions: play role IN[in] VBG[close, end, ..] (397), ? important role IN[in] VBG[close, end, ..] (110), ? role IN[in] VBG[close, end, ..] (854), Patterns for "we": we MD VB (41369), ?, we VBP DT (13138), ?, we VBD DT (8139), ? Patterns for "we play an important role": play role IN[in] DT (599), play role IN[in] VBG (397), ?, important role IN[in] VBG (110), ?, role IN[in] VBG (854), ?Translations for the source text: "?": we, ?; "?": close, end, ?; ?; "?": play, ?; "?": critical, ?; ?; "?": act, ?; ?; "?": heavy, ?; "?": will, wish, ?; "?": cents, ?; "?": outstanding, ? Input your source text and start to interact with TransAhead! 

 Figure 2 . 2 Figure 2. Automatically generating pattern grammar. 

 Figure 3 . 3 Figure 3. Predicting pattern grammar and translations at run-time. 

 Figure 4 . 4 Figure 4. Example translations with TransAhead assistance. 

 No previous work in CAT uses Google Translate for comparison. Although there is a of 35.49 vs. 26.46. Admittedly, the MT system Google Translate produced translations with a higher BLEU score of 44.82. Google Translate obviously has much more parallel training data and bilingual translation knowledge. 

			 http://140.114.214.80/theSite/TransAhead/ (Chrome only)
