title
Accurate Word Alignment Induction from Neural Machine Translation

abstract
Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights DO capture accurate word alignments and propose two novel word alignment induction methods SHIFT-ATT and SHIFT-AET. The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. SHIFT-ATT is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change. SHIFT-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized SHIFT-ATT alignments. Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and SHIFT-AET significantly outperforms GIZA++ by 1.4-4.8 AER points. 1

Introduction The task of word alignment is to find lexicon translation equivalents from parallel corpus  (Brown et al., 1993) . It is one of the fundamental tasks in natural language processing (NLP) and is widely studied by the community  (Dyer et al., 2013; Brown et al., 1993; Liu and Sun, 2015) . Word alignments are useful in many scenarios, such as error analysis  (Ding et al., 2017; Li et al., 2019) , the introduction of coverage and fertility models  (Tu et al., 2016) , inserting external constraints in interactive machine translation  (Hasler et al.,    SHIFT-ATT induces alignments for target word y i at decoding step i + 1 when y i is the decoder input, while NAIVE-ATT at step i when y i is the decoder output.  Chen et al., 2020)  and providing guidance for human translators in computer-aided translation  (Dagan et al., 1993) . Word alignment is part of the pipeline in statistical machine translation  (Koehn et al., 2003, SMT) , but is not necessarily needed for neural machine translation  (Bahdanau et al., 2015, NMT) . The attention mechanism in NMT does not functionally play the role of word alignments between the source and the target, at least not in the same way as its analog in SMT. It is hard to interpret the attention activations and extract meaningful word alignments especially from Transformer  (Garg et al., 2019) . As a result, the most widely used word alignment tools are still external statistical models such as FAST-ALIGN  (Dyer et al., 2013)  and  GIZA++ (Brown et al., 1993; Och and Ney, 2003) . Recently, there is a resurgence of interest in the community to study word alignments for the Transformer  (Ding et al., 2019; Li et al., 2019) . One simple solution is NAIVE-ATT, which induces word alignments from the attention weights between the encoder and decoder. The next target word is aligned with the source word that has the maximum attention weight, as shown in Fig.  1 . However, such schedule only captures noisy word alignments  (Ding et al., 2019; Garg et al., 2019) . One of the major problems is that it induces alignment before observing the to-be-aligned target token  (Peter et al., 2017; Ding et al., 2019) . Suppose for the same source sentence, there are two alternative translations that diverge at decoding step i, generating y i and y i which respectively correspond to different source words. Presumably, the source word that is aligned to y i and y i should change correspondingly. However, this is not possible under the above method, because the alignment scores are computed before prediction of y i or y i . To alleviate this problem, some researchers modify the transformer architecture by adding alignment modules that predict the to-be-aligned target token  (Zenkel et al., 2019 (Zenkel et al., , 2020  or modify the training loss by designing an alignment loss computed with full target sentence  (Garg et al., 2019; Zenkel et al., 2020) . Others argue that using only attention weights is insufficient for generating clean word alignment and propose to induce alignments with feature importance measures, such as leaveone-out measures  (Li et al., 2019)  and gradientbased measures  (Ding et al., 2019) . However, all previous work induces alignment for target word y i at step i, when y i is the decoder output. In this work, we propose to induce alignment for target word y i at step i + 1 rather than at step i as in previous work. The motivation behind this is that the hidden states in step i + 1 are computed taking word y i as the input, thus they can incorporate the information of the to-be-aligned target token y i easily. Following this idea, we present SHIFT-ATT and SHIFT-AET, two simple yet effective methods for word alignment induction. Our contributions are threefold: ? We introduce SHIFT-ATT (see Fig.  1 ), a pure interpretation method to induce alignments from attention weights of vanilla Transformer. SHIFT-ATT is able to reduce the Alignment Error Rate (AER) by 7.0-10.2 points over NAIVE-ATT and 5.5-7.9 points over FAST-ALIGN on three publicly available datasets, demonstrating that if the correct decoding step and layer are chosen, attention weights in vanilla Transformer are sufficient for generating accurate word alignment interpretation. ? We further propose SHIFT-AET , which extracts alignments from an additional alignment module. The module is tightly integrated into vanilla Transformer and trained with supervision from symmetrized SHIFT-ATT alignments. SHIFT-AET does not affect the translation accuracy and significantly outperforms GIZA++ by 1.4-4.8 AER points in our experiments. ? We compare our methods with NAIVE-ATT on dictionary-guided decoding  (Alkhouli et al., 2018) , an alignment-related downstream task. Both methods consistently outperform NAIVE-ATT, demonstrating the effectiveness of our methods in such alignment-related NLP tasks. 

 Background 

 Neural Machine Translation Let x = {x 1 , ..., x |x| } and y = {y 1 , ..., y |y| } be source and target sentences. Neural machine translation models the target sentence given the source sentence as p(y|x; ?): p(y|x; ?) = |y|+1 t=1 p(y t |y 0:t?1 , x; ?), (1) where y 0 = bos and y |y|+1 = eos represent the beginning and end of the target sentence respectively, and ? is a set of model parameters. In this paper, we use Transformer  (Vaswani et al., 2017)  to implement the NMT model. Transformer is an encoder-decoder model that only relies on attention. Each decoder layer attends to the encoder output with multi-head attention. We refer to the original paper  (Vaswani et al., 2017)  for more model details. 

 Alignment by Attention The encoder output from the last encoder layer is denoted as h = {h 1 , ..., h |x| }, and the hidden states at decoder layer l as z = {z l 1 , ..., z l |y|+1 }. For decoder layer l, we define the head averaged encoderdecoder attention weights as W l ? R (|y|+1)?|x| , in which the element W l i,j measures the relevance between decoder hidden state z l i and encoder output h j . For simplicity, below we use the term "attention weights" to denote the head averaged encoderdecoder attention weights. Given a trained Transformer model, word alignments can be extracted from the attention weights. More specifically, we denote the alignment score matrix as S ? R |y|?|x| , in which the element S i,j is the alignment score of target word y i and source word x j . Then we compute S with: S i,j = W l i,j (1 ? i ? |y|,1 ? j ? |x|) (2) and extract word alignments A with maximum a posterior strategy following  Garg et al. (2019) : A ij = 1 if j = argmax j S i,j 0 otherwise , (3) where A ij = 1 indicates y i is aligned to x j . We call this approach NAIVE-ATT.  Garg et al. (2019)  show that attention weights from the penultimate layer, i.e., l = L ? 1, can induce the best alignments. Although simple to implement, this method fails to obtain satisfactory word alignments  (Ding et al., 2019; Garg et al., 2019) . First of all, instead of the relevance between y i and x j , W l i,j measures the relevance between decoder hidden state z l i and encoder output h j . Considering that the decoder input is y i?1 and the output is y i at step i, z l i may better represent y i?1 instead of y i , especially for bottom layers. Second, since W l i,j is computed before observing y i , it becomes difficult for it to induce the aligned source token for the target token y i , as discussed in Section 1. As a result, it is necessary to develop novel methods for alignment induction. This method should be able to (i) take into account the relationship of z l i , y i and y i?1 , and (ii) adapt the alignment induction with the to-be-aligned target token. 

 Method In this section, we propose two novel alignment induction methods SHIFT-ATT and SHIFT-AET. Both methods adapt the alignment induction with the to-be-aligned target token by computing alignment scores at the step when the target token is the decoder input. 

 SHIFT-ATT: Alignment from Vanilla Transformer Alignment Induction NAIVE-ATT  (Garg et al., 2019)  induces alignment for target token y i at step i when y i is the decoder output and defines the alignment score matrix with Eq. 2. They find the best layer l to extract alignments by evaluating the AER of all layers on the test set. We instead propose to induce alignment for target token y i at step i + 1 when y i is the decoder input. We define the alignment score matrix S as: S i,j = W l i+1,j (1 ? i ? |y|,1 ? j ? |x|). (4) This is because W l i+1,j measures the relevance between z l i+1 and h j , and we use z l i+1 and h j to represent y i and x j respectively. With the alignment score matrix S, we can extract word alignments A using Eq. 3. We call this method SHIFT-ATT. Fig.  1  shows an alignment induction example to compare NAIVE-ATT and SHIFT-ATT. SHIFT-ATT uses z l i+1 to represent the to-bealigned target token y i while NAIVE-ATT uses z l i . We argue using z l i+1 is better. First, at bottom layers, we hypothesize that z l i+1 could better represent the decoder input y i than output y i+1 . Therefore we can use z l i+1 with small l to represent y i . Second, z l i+1 is computed after observing y i , indicating that SHIFT-ATT is able to adapt the alignment induction with the to-be-aligned target token. Our proposed method involves inducing alignments from source-to-target and target-to-source vanilla Transformer models. Following  Zenkel et al. (2019) , we merge bidirectional alignments using the grow diagonal heuristic  (Koehn et al., 2005) . Layer Selection Criterion To select the best layer l b to induce alignments, we propose a surrogate layer selection criterion without manually labelled word alignments. Experiments show that this criterion correlates well with the AER metric. Given parallel sentence pairs x, y , we train a source-to-target model ? x?y and a target-to-source model ? y?x . We assume that the word alignments extracted from these two models should agree with each other  (Cheng et al., 2016) . Therefore, we evaluate the quality of the alignments by computing the AER score on the validation set with the source-to-target alignments as the hypothesis and the target-to-source alignments as the reference. For each model, we can obtain L word alignments from L different layers. In total, we obtain L ? L AER scores. We select the one with the lowest AER score, and its corresponding layers of the sourceto-target and target-to-source models are the layers we will use to extract alignments at test time:  l b,x?y , l b,y?x = argmin i,j AER(A i x?y , A j y?x ). 

 SHIFT-AET: Alignment from Alignment-Enhanced Transformer To further improve the alignment accuracy, we propose SHIFT-AET, a word alignment induction method that extracts alignments from Alignment-Enhanced Transformer (AET). AET extends the Transformer architecture with a separate alignment module, which observes the hidden states of the underlying Transformer at each step and predicts the alignment scores for the current decoder input. Note that this module is a plug and play component and it neither makes any change to the underlying NMT model nor influences the translation quality. Fig.  2  illustrates the alignment module of AET at decoding step i. We add the alignment module only at layer l b , the best layer to extract alignments with SHIFT-ATT. The alignment module performs multi-head attention similar to the encoder-decoder attention sublayer. It takes the encoder outputs h = {h 1 , ..., h |x| } and the current decoder hidden state zl b i inside layer l b as input and outputs S i?1 , the alignment score corresponding to target word y i?1 : S i?1 = 1 N n softmax( (hG K n )(z l b i G Q n ) ? d k ), (5) where G K n ,G Q n ? R d model ?d k are the key and query projection matrices for the n-th head, N is the number of attention heads and d k = d model /N . Since we only care about the attention weights, the value-related parameters and computation are omitted in this module. To train the alignment module, we use the symmetrized SHIFT-ATT alignments extracted from vanilla Transformer models as labels. Specifically, while the underlying Transformer is pretrained and fixed (Fig.  2 ), we train the alignment module with the loss function following  Garg et al. (2019) : L a = ? 1 |y| |y| i=1 |x| j=1 ?p i,j log S i,j ), (6) where S = {S 1 ;. Once the alignment module is trained, we extract alignment scores S from it given a parallel sentence pair and induce alignments A using Eq. 3. 

 Experiments 

 Settings Dataset We follow previous work  (Zenkel et al., 2019 (Zenkel et al., , 2020  in data setup and conduct experiments on publicly available datasets for German-English (de-en) 3 , Romanian-English (ro-en) and French-English (fr-en) 4 . Since no validation set is provided, we follow  Ding et al. (2019)  to set the last 1,000 sentences of the training data before preprocessing as validation set. We learn a joint source and target Byte-Pair-Encoding  (Sennrich et al., 2016)  with 10k merge operations. Table  1  shows the detailed data statistics. 

 NMT Systems We implement the Transformer with fairseq-py 5 and use the transformer iwslt de en model configuration following  Ding et al. (2019) . We train the models with a batch size of 36K tokens and set the maximum updates as 50K and 10K for 

 Method Inter. Fullc de-en fr-en ro-en de?en en?de bidir fr?en en?fr bidir ro?en en?ro bidir Statistical Methods FAST-ALIGN  (Dyer et al., 2013  Evaluation We evaluate the alignment quality of our methods with Alignment Error Rate (Och and Ney, 2000, AER). Since word alignments are useful for many downstream tasks as discussed in Section 1, we also evaluate our methods on dictionaryguided decoding, a downstream task of alignment induction, with the metric BLEU  (Papineni et al., 2002) . More details are in Section 4.3. Baselines We compare our methods with two statistical baselines FAST-ALIGN and GIZA++ and nine other baselines: ? NAIVE-ATT  (Garg et al., 2019) : the approach we discuss in Section 2.2, which induces alignments from the attention weights of the penultimate layer of the Transformer. ? NAIVE-ATT-LA  (Garg et al., 2019) : the NAIVE-ATT method without layer selection. It induces alignments from attention weights averaged across all layers. ? SHIFT-ATT-LA: SHIFT-ATT method without layer selection. It induces alignments from attention weights averaged across all layers. ? SMOOTHGRAD : the method that induces alignments from word saliency, which is computed by averaging the gradient-based saliency scores with multiple noisy sentence pairs as input. ? SD-SMOOTHGRAD  (Ding et al., 2019) : an improved version of SMOOTHGRAD, which defines saliency on one-hot input vector instead of word embedding. ? PD  (Li et al., 2019) : the method that computes the alignment scores from Transformer by iteratively masking each source token and measuring the prediction difference. ? ADDSGD  (Zenkel et al., 2019) : the method that explicitly adds an extra attention layer on top of Transformer and directly optimizes its activations towards predicting the to-be-aligned target token. ? MTL-FULLC  (Garg et al., 2019) : the method that trains a single model in a multi-task learning framework to both predict the target sentence and the alignment. When predicting the alignment, the model observes full target sentence and uses symmetrized NAIVE-ATT alignments as labels. ? MTL-FULLC-GZ  (Garg et al., 2019) : the same method as MTL-FULLC except using symmetrized GIZA++ alignments as labels. It is a statistical and neural method as it relies on GIZA++ alignments. Among these nine baselines and our proposed methods, SMOOTHGRAD, SD-SMOOTHGRAD and PD induce alignments using feature importance measures, while the others from some form of attention weights. Note that the computation cost of methods with feature importance measures is much higher than those with attention weights. 7 

 Alignment Results 

 Comparison with Baselines Table 2 compares our methods with all the baselines. First, SHIFT-ATT, a pure interpretation method for the vanilla Transformer, significantly outperforms FAST-ALIGN and all neural baselines, and performs comparable with GIZA++. For example, it outperforms SD-SMOOTHGRAD, the state-ofthe-art method with feature importance measures to extract alignments from vanilla Transformer, by 8.7-11.1 AER points across different language pairs. The success of SHIFT-ATT demonstrates that vanilla Transformer has captured alignment information in an implicit way, which could be revealed from the attention weights if the correct decoding step and layer are chosen to induce alignments. Second, the method SHIFT-AET achieves new state-of-the-art, significantly outperforming all baselines. It improves over GIZA++ by 1.4-4.8 AER across different language pairs, demonstrating that it is possible to build a neural aligner better than GIZA++ without using any alignments generated from statistical aligners to bootstrap training. We also find SHIFT-AET performs either marginally better (de-en and ro-en) or on-par (fr-en) when comparing with MTL-FULLC-GZ, a method that uses GIZA++ alignments to bootstrap training. We evaluate the model sizes: the number of parameters in vanilla Transformer and AET are 36.8M and 37.3M respectively, and find that AET only introduces 1.4% additional parameters to the vanilla Transformer. In summary, by supervising the alignment module with symmetrized SHIFT-ATT alignments, SHIFT-AET improves over SHIFT-ATT and GIZA++ with negligible parameter increase and without influencing the translation quality. Comparison with  Zenkel et al. (2020)  Concurrent with our work,  Zenkel et al. (2020)  propose a neural aligner that can outperform GIZA++. Table  3  compares the performance of SHIFT-AET and the best method BAO-GUIDED (Birdir. Att. Opt. + Guided) in  Zenkel et al. (2020) . We observe that SHIFT-AET performs better than BAO-GUIDED in terms of alignment accuracy. SHIFT-AET is also much simpler than BAO-GUIDED. The training of BAO-GUIDED includes three stages: (i) train vanilla Transformer in sourceto-target and target-to-source directions; (ii) train the alignment layer and extract alignments on the training set with bidirectional attention optimization. This alignment extraction process is computational costly since bidirectional attention optimization fine-tunes the model parameters separately for each sentence pair in the training set; (iii) re-train the alignment layer with the extracted alignments as the guidance. In contrast, SHIFT-AET can be trained much faster in two stages and does not involve bidirectional attention optimization. Similar with MTL-FULLC  (Garg et al., 2019) , BAO-GUIDED adapts the alignment induction with the to-be-aligned target token by requiring full target sentence as the input. Therefore, BAO-GUIDED is not applicable in cases where alignments are incrementally computed during the decoding process, e.g., dictionary-guided decoding  (Alkhouli et al., 2018) . In contrast, SHIFT-AET performs quite well on such cases (Section 4.3). Therefore, considering the alignment performance, computation cost and applicable scope, we believe SHIFT-AET is more appropriate than BAO-GUIDED for the task of alignment induction. 

 Performance on Distant Language Pair To further demonstrate the superiority of our methods on distant language pairs, we also evaluate our methods on Chinese-English (zh-en). We use NIST corpora 8 as the training set and v1-tstset released by TsinghuaAligner  (Liu and Sun, 2015)     (Koehn, 2004)  set. The test set includes 450 parallel sentence pairs with manually labelled word alignments.  9  We use jieba 10 for Chinese text segmentation and follow the settings in Section 4.1 for data pre-processing and model training. The results are shown in Table 4. It presents that both SHIFT-ATT and SHIFT-AET outperform NAIVE-ATT to a large margin. When comparing the symmetrized alignment performance with GIZA++, SHIFT-AET performs better, while SHIFT-ATT is worse. The experimental results are roughly consistent with the observations on other language pairs, demonstrating the effectiveness of our methods even for distant language pairs. 

 Downstream Task Results In addition to AER, we compare the performance of NAIVE-ATT, SHIFT-ATT and SHIFT-AET on dictionary-guided machine translation  (Song et al., 2020) , which is an alignment-based downstream task. Given source and target constraint pairs from dictionary, the NMT model is encouraged to translate with provided constraints via word alignments  (Alkhouli et al., 2018; Hasler et al., 2018; Hokamp and Liu, 2017; Song et al., 2020) . More specifically, at each decoding step, the last token of the candidate translation will be revised with target constraint if it is aligned to the corresponding source constraint according to the alignment induction method. To simulate the process of looking up dictionary, we follow  Hasler et al. (2018)  and extract the pre-specified constraints from the test set and its reference according to the golden word alignments. We exclude stop words, and sample up to 3 dictionary constraints per sentence tionary constraint includes up to 3 source tokens. Table  5  presents the performance with different alignment methods. Both SHIFT-ATT and SHIFT-AET outperform NAIVE-ATT. SHIFT-AET obtains the best translation quality, improving over NAIVE-ATT by 1.1 and 1.5 BLEU scores on de?en and en?de translations, respectively. The results suggest the effectiveness of our methods in application to alignment-related NLP tasks. 

 Analysis Layer Selection Criterion To test whether the layer selection criterion can select the right layer to extract alignments, we first determine the best layer l b,x?y and l b,y?x based on the layer selection criterion. Then we evaluate the AER scores of alignments induced from different layers on the test set, and check whether the layers with the lowest AER score are consistent with l b,x?y and l b,y?x . The experiment results shown in Table  6  verify that the layer selection criterion is able to select the best layer to induce alignments. We also find that the best layer is always layer 3 under our setting, consistent across different language pairs. Relevance Measure Verification To investigate the relationship between z l i and y i?1 /y i , we design an experiment to probe whether z l i contain the identity information of y i?1 and y i , following  Brunner et al. (2019) . Formally, for decoder hidden state z l i , the input token is identifiable if there exists a func- tion g such that y i?1 = g(z l i ). We cannot prove the existence of g analytically. Instead, for each layer l we learn a projection function ?l to project from the hidden state space to the input token embedding space ?l i = ?l (z l i ) and then search for the nearest neighbour y k within the same sentence. We say that z l i can identify y i?1 if k = i ? 1. Similarly, we follow the same process to identify the output token y i . We report the identifiability rate defined as the percentage of correctly identified tokens. Fig.  3  presents the results on the validation set of de?en translation. We try three projection functions: a naive baseline ?naive l (z l i ) = z l i , a linear perceptron ?lin l and a non-linear multi-layer perceptron ?mlp l . We observe the following points: (i) With trainable projection functions ?lin l and ?mlp l , all layers can identify the input tokens, although more hidden states cannot be mapped back to their input tokens anymore in higher layers. (ii) Overall it is easier to identify the input token than the output token. For example, when projecting with mlp, all layers can identify more than 98% of the input tokens. However, for the output tokens, we can only identify 83.5% even from the best layer. Since z l i even may not be able to identify y i , this observation partially verifies that it is better to represent y i using z l i+1 than z l i . (iii) At bottom layers, the input tokens remain identifiable and the output tokens are hard to identify, regardless of the projection function we use. This confirms our hypothesis that for small l, z l i is more relevant to y i?1 than y i . AER v.s. BLEU During training, vanilla Transformer gradually learns to align and translate. To analyze how the alignment behavior changes at different layers with checkpoints of different translation quality, we plot AER on the test set v.s. BLEU on the validation set for de?en translation. We compare NAIVE-ATT and SHIFT-ATT, which align the decoder output token (align output) and decoder input token (align input) to the source tokens based on current decoder hidden state, respectively. The experiment results are shown in Fig.  4 . We observe that at the beginning of training, layers 3 and 4 learn to align the input token, while layers 5 and 6 the output token. However, with the increasing of BLEU score, layer 4 tends to change from aligning input token to aligning output token, and layer 1 and 2 begin to align input token. This suggests that vanilla Transformer gradually learns to align the input token from middle layers to bottom layers. We also see that at the end of training, layer 6's ability to align output token decreases. We hypothesize that layer 5 already has the ability to attend to the source tokens which are aligned to the output token, therefore attention weights in layer 6 may capture other information needed for translation. Finally, for checkpoints with the highest BLEU score, layer 5 aligns the output token best and layer 3 aligns the input token best. Alignment Example In Fig.  5 , we present a symmetrized alignment example from de-en test set. Manual inspection of this example as well as others finds that our methods SHIFT-ATT and SHIFT-AET tend to extract more alignment pairs than GIZA++, and extract better alignments especially for sentence beginning compared to NAIVE-ATT. 

 Related Work Alignment induction from RNNSearch  (Bahdanau et al., 2015)  has been explored by a number of works.  Bahdanau et al. (2015)  are the first to show word alignment example using attention in RNNSearch.  Ghader and Monz (2017)  further demonstrate that the RNN-based NMT system achieves comparable alignment performance to that of GIZA++. Alignment has also been used to improve NMT performance, especially in low resource settings, by supervising the attention mechanisms of RNNSearch  Alkhouli and Ney, 2017) . There is also a number of other studies that induce word alignment from Transformer.  Li et al. (2019) ;  Ding et al. (2019)  claim that attention may not capture word alignment in Transformer, and propose to induce word alignment with prediction difference  (Li et al., 2019)  or gradient-based measures  (Ding et al., 2019) .  Zenkel et al. (2019)  modify the Transformer architecture for better align-  ment induction by adding an extra alignment module that is restricted to attend solely on the encoder information to predict the next word.  Garg et al. (2019)  propose a multi-task learning framework to improve word alignment induction without decreasing translation quality, by supervising one attention head at the penultimate layer with GIZA++ alignments. Although these methods are reported to improve over head average baseline, they ignore that better alignments can be induced by computing alignment scores at the decoding step when the to-be-aligned target token is the decoder input. 

 Conclusion In this paper, we have presented two novel methods SHIFT-ATT and SHIFT-AET for word alignment induction. Both methods induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work. Experiments on three public alignment datasets and a downstream task prove the effectiveness of these two methods. SHIFT-AET further extends Transformer with an addi-tional alignment module, which consistently outperforms prior neural aligners and GIZA++, without influencing the translation quality. To the best of our knowledge, it reaches the new state-of-theart performance among all neural alignment induction methods. We leave it for future work to extend our study to more downstream tasks and systems. Figure 1 : 1 Figure 1: An example to compare our method SHIFT-ATT and the baseline NAIVE-ATT. The left is an attention map from the third decoder layer of the vanilla Transformer and the right are the induced alignments.SHIFT-ATT induces alignments for target word y i at decoding step i + 1 when y i is the decoder input, while NAIVE-ATT at step i when y i is the decoder output. 
