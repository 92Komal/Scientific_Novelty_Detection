title
Evaluating the morphological competence of Machine Translation Systems

abstract
While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models. This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.

Introduction It is nowadays unanimously recognized that Machine Translation (MT) engines based on the neural encoder-decoder architecture with attention  constitute the new state-of-the-art in statistical MT, at least for open-domain tasks  (Sennrich et al., 2016a) . The previous phrase-based (PBMT) architectures were complex  (Koehn, 2010)  and hard to diagnose, and Neural MT (NMT) systems, which dispense with any sort of symbolic representation of the learned knowledge, are probably worse in this respect. Furthermore, the steady progress of MT engines makes automatic metrics such as BLEU  (Papineni et al., 2002)  or METEOR  (Banerjee and Lavie, 2005)  less appropriate to evaluate and compare modern NMT systems. To better understand the strength and weaknesses of these new architectures, it is thus necessary to investigate new, more focused, evaluation procedures. Error analysis protocols, as proposed eg. by  Vilar et al. (2006) ;  Popovi? and Ney (2011)  for PBMT, are obvious candidates for such studies and have been used eg. in  (Bentivogli et al., 2016) . Recently, various new proposals have been put forward to better diagnose neural models, notably by  Linzen et al. (2016) ;  Sennrich (2017) , who focus respectively on the syntactic competence of Neural Language Models (NLMs) or of NMT; and by  Isabelle et al. (2017) ;  Burchardt et al. (2017) , who resuscitate an old tradition of designing test suites. Inspired by these (and other) works (see ? 4), we propose in this paper a new evaluation scheme aimed at specifically assessing the morphological competence of MT engines translating from English into a Morphologically Rich Language (MRL). Morphology poses two main types of problems in MT: (a) morphological variation in the source increases the occurrence of Out-of-Vocabulary (OOV) source tokens, the translation of which is difficult to coin; (b) morphological variation in the target forces the MT to generate forms that have not been seen in training. Morphological complexity is alo often associated to more flexible word orderings, which is mostly a problem when translating from a MRL . Reducing these issues is a legitimate and important goal for many language pairs. Our method for measuring the morphological competence of MT systems (detailed in ? 2) is mainly based on the analysis of minimal pairs, each representing a contrast that is expressed syntactically in English and morphologically in the MRL. By comparing the automatic translations of these pairs, it is then possible to approximately assess whether a given MT system has succeeded in generating the correct word form, carrying the proper morphological marks. In ? 3, we illustrate the potential of our evaluation protocol in a large-scale comparison of multiple MT engines having participated to the WMT'17 News Transla-tion tasks for the pairs English-Czech and English-Latvian.  1  We finally relate our protocol to conventional metrics ( ? 4), and conclude in ? 5 by discussing possible extensions of this methodology, for instance to other (sets of) language pairs. 2 Evaluation Protocol 

 Morphological competence and its assessment In traditional linguistics, morphology is "the branch of the grammar that deals with the internal structure of words" (Matthews, 1974, p. 9); the "structure of words" being further subdivided into inflections, derivations (word formation) and compounds. Languages exhibit a large variety of formal processes to express morphological/lexical relatedness of a set of word forms: alternations in suffix/prefix are the most common processes in Indo-European languages, where other language families recourse to circumfixation, reduplication, transfixation, or tonal alternations. They also greatly differ in the phenomena that are expressed through morphological alternations versus grammatical constructions. Our evaluation protocol is designed to assess the robustness of MT in the presence of morphological variation in the source and target, looking how source alternations (possibly implying to translate source OOVs) are reproduced in the target (possibly implying to generate target OOVs). The general principle is as follows: for each source test sentence (the base), we generate one (or several) variant(s) containing exactly one difference with the base, focusing on a specific target lexeme of the base; the variant differs on a feature that is expressed morphologically in the target, such as the person, number or tense of a verb; or the number or case of a noun or an adjective. This configuration is illustrated in Table  1 , where the first pair is an example of the tense contrast and the second pair an instance of the polarity contrast. We consider that a system behaves correctly with respect to a given contrast if the translation of the base and the variant reproduce the targeted contrast: for the first example in Table  1 , we expect to see in the translation of (1.a) and (1.b) different word forms accounting for the difference of verb tense: the translation of the variant should have a past form and any other case is considered as an error. Other modifications between the two 1 http://statmt.org/wmt17/. translations, such as the selection of different lemmas for both forms or any modification of the context, are considered irrelevant with respect to the specific morphological feature at study, and are therefore ignored. In the following sections, we detail and justify our strategy for generating contrastive pairs. 

 Sentence selection and morphological contrasts We consider the set of contrasts listed in Table  2 . We distinguish three subsets (denoted A, B, and C), which slightly differ in their generation and scoring procedures. Our choice for selecting this particular set of tests was dictated by a mixture of linguistic and also more practical reasons. From a linguistic standpoint, we were looking to cover a large variety of morphological phenomena in the target language, in particular we wished to include test instances for all open domain word classes (noun, verbs, adjectives). Our first set of tests (set A) is akin to paradigm completion tasks, adopting here a rather loose sense of "paradigm" which also includes simple derivational phenomena such as the formation of comparative for adjectives and mostly checks whether the morphological feature inserted in the source sentence has been translated. Tests in the set B look at various agreement phenomena, while tests in set C are more focused on the consistency of morphological choices. These three categories of tests slightly differ in their generation and scoring procedures. For each contrast in the A and B sets, sentence generation takes the following steps: 2 For set A, the creation of the variant (step 2) consists in replacing a word according to the morphological phenomenon to evaluate (see examples Table  1 ). This word is selected in such a way that its modification does not require a modification of any other word in the sentence. For instance, a singular subject noun is not replaced by its plural form, since the verb agreeing with it would also need to be replaced accordingly. Indeed, more than one modification would go against our initial idea of generating minimal pairs reflecting exactly one single contrast. For B-1 (complex NPs), we spot a personal pronoun that we changed into an NP consisting in an adjective and a noun. Both words are generated randomly with the only constraint that the noun should refer to a human subject and the adjective to a psychological state, yielding NPs such as "the happy linguist" or "the gloomy philosopher". In order to ensure that the context corresponds to a human subject, we selected pronouns that unambiguously refer to humans, such as "him", "her", "we" (avoiding "them"). For B-2 (coordinated NPs) the pronoun in the base sentence is transformed into a complex NP consisting of two coordinated nouns. Note that adjectives associated to these nouns, as well as adverbs, have been randomly inserted in order to produce some variation in the constructions. The B-3 contrasts are produced in a similar fashion, targeting verbs instead of nouns, with an additional random generation of a discourse marker that should not interfere with the translation, yielding variants like "he said and, as a matter of fact, shouted". 3 Those inser-tions were performed in order to increase the distance between the two verbs, making agreement between them harder. Finally, the B-4 contrasts are produced in the same way as for the A-set and simply consist in modifying a preposition. The C-set variants select a noun, an adjective or a verb and replace it with a random hyponym, producing an arbitrary number of sentences. Sentence selection slightly differs from the description above: during step 2, we generate as many variants as possible. Each variant is then scored with a language model and only the top four variants are kept, leading to buckets of five sentences. Those buckets are finally filtered in the same way as for the A and B sets, removing the 33% worst buckets based on their LM score (step 3). All the sentences were selected from the English News-2008 corpus provided at WMT. The choice of the news domain was dictated by our intention to evaluate systems submitted at WMT'17 4 News Translation task. Sentences longer than 15 tokens were removed in order to ensure a better focus on a specific part of the sentence in the MT output. The modifications of English sentences were based on a morpho-syntactic analysis produced with the TreeTagger  (Schmid, 1994)  and using the Pymorphy morphological generator 5 to change the inflection of a word. Hyponyms (synonyms and/or antonyms) were generated with WordNet  (Miller, 1995) . The 5-gram language model used for sentence selection was learned with KenLM  (Heafield, 2011)  on all English monolingual data available at WMT'15. 

 Scoring Procedures Regarding the scoring procedure, we again distinguish three cases (examples are in Table  3 ). ? set A: we compare the translations of base and variant and search for the word(s) in variant that are not in base. If one of these words contains the morphological feature associated with the source sentence modification, we report a success. Accuracy of each morphological feature is averaged over all the samples. In this set, we thus evaluate morphological information that should be conveyed from the source sentence, which leads to an assessment on the grammatical adequacy of the output towards the source. ? set B: we compare the translations of base and variant and check that (a) a pronoun in the former is replaced by a NP in the latter (b) the adjective and the noun in the NP share the same gender, number and case. A distinct accuracy rate per feature can then be reported; note that the situation is different in the complex and coordinated tests, as in the latter case some agreement properties may differ in the base and variant (eg. the NP gender agreement depends on the noun gender that may be different from the pronoun gender in base). For the test triggered by prepositions (B-4), we check whether the first noun on the right of a preposition carries the required case mark. Moreover, since we have prepositions associated to nouns in both base and variant, we performed this test on both sentences. This evaluation set checks for agreement and provides an insight about the morphological fluency of the produced translations. ? set C : in this set of tests, we wish to assess the consistency of morphological features with respect to lexical variation in a fixed context; accordingly, we measure the success based on the average normalized entropy of morphological features in the set of target sentences. Such scores can be computed either globally or on a per feature basis. The entropy is null when all variants have the same morphological features, the highest possible consistency; conversely, the normalized entropy is 1 when the five sentences contain different morphological features. For each set C-1, C-2 and C-3, we report average scores over 500 samples. In this setup, we measure the degree of certainty to which a system predicts morphological features across small lexical variations. Our scoring procedure needs access to morphological information in the target. For A and B sets, the translated sentences are passed through a morphological analysis, where several PoS can be associated with a word. This makes the evaluation less dependent on the tagger's accuracy. Therefore, when checking whether a specific morphological feature appears in the output (eg. negation of a verb), we look for at least one PoS tag indicating negation, ignoring all the others. For Czech, we used the Morphodita analyzer  (Strakov? et al., 2014) . We had no such resource  for Latvian and therefore used the LU MII Tagger  (Paikens et al., 2013)  to parse all Latvian monolingual data available at WMT'17. We then extracted a dictionary consisting of words and associated PoS from the automatic parses. We finally performed a coarse cleaning of this dictionary by removing the PoS that were predicted less than 100 times for a specific word. To run the morphological analysis of Latvian, we parsed the translated sentences with the tagger, then augmented the tagger predictions with our dictionary, producing the desired ambiguous analysis of the Latvian outputs. For the C-set, the translated sentence analyses are disambiguated: each word is mapped to a single PoS. This was required to compute the entropy. Indeed, we need to select only one morphological value for each base and variant sentence, given that the entropy is normalized according the total number of sentences in the bucket. 

 Experiments We have run the presented morphological evaluation 6 on several systems among which some were submitted at WMT'17. The description of the latter can be found in the proceedings of the Second Conference on Machine Translation (2017a). We briefly summarize the types of systems included in the English-to-Czech study: ? Phrase-based systems: The Moses baseline was trained on WMT'17 data and was not submitted at WMT'17. UFAL Chimera 7 was submitted at WMT'16 and is described in . ? Word based NMT: NMT words is a system trained on WMT'17 parallel data with a target vocabulary of 80k tokens. It was not submitted at WMT'17 and is used for contrast. ? BPE-based NMT: LIMSI NMT  (Burlot et al., 2017)  is based on NMTPY , UEDIN NMT  (Sennrich et al., 2017a)  on Nematus  (Sennrich et al., 2017b)  and UFAL NMT  (Bojar et al., 2017b)  on Neural Monkey  (Helcl and Libovick?, 2017) . ? NMT modeling target morphology: LIMSI FNMT  (Burlot et al., 2017)  and LIUM FNMT (Garc?a-Mart?nez et al., 2017) use a factored output predicting words and PoS, and UFAL NMT Chim.  (Bojar et al., 2017b)  uses Chimera  (Bojar et al., 2013) . All these models also use BPE segmentation. These systems are representative of different models across statistical MT history. Phrase-based systems are a former state of the art that wordbased NMT struggled to improve. The new state of the art is an NMT setup with an open vocabulary provided by byte pair encoding (BPE) segmentation  (Sennrich et al., 2016b) . Finally, we have a set of systems that are optimized in order to improve target morphology. The automatic scores of the systems submitted at WMT'17 8 are in Table 4 where we report BLEU, BEER  (Stanojevi? and Sima'an, 2014)  and CharacTER  (Wang et al., 2016) .  9  We also computed a morphology accuracy for these systems. Using output-to-reference alignments produced by METEOR on lemmas, we checked whether aligned words shared the same form. Our assumption is that two different forms associated to the same lemma correspond to two different inflections of the same lexeme, which allows us to locate positions that likely correspond to morphological errors. Table  5  lists the results for the A-set tests, which evaluate the morphological adequacy of the output wrt. the source sentence. The last column provides the mean of all scores for one system. We can note that all BPE-based NMT systems have a much higher performance than the phrase-based systems.  10  We explain the poor performance of the word-based NMT system by the use of a too small closed vocabulary: over the 18,500 sentences of the test suite, 12,016 unknown words were produced by this system. However, when it comes to predicting the morphology of closed class words, this systems performs much better: the accuracy computed for pronoun gender and number is similar to the ones of best BPE-based systems. As opposed to nouns and verbs (open classes), the set of pronouns in Czech is quite small; having observed all their inflections, the word-based system is in a better position to convey the target form. Despite important differences in automatic metric scores between UEDIN NMT system and LIMSI FNMT, we see that the latter always outperforms the former, except for the feminine pronoun prediction. The overall morphological accuracies (Table  4 ) show that UEDIN NMT provides more similar word forms with the reference translation, but these global scores fail to show the higher adequacy performance of LIMSI FNMT highlighted in the A-set. The results of the B-set evaluation for Czech are in Table  6  and are an estimate of the morphological fluency of the output. We observe here again that morphological phenomena such as agreement are better modeled by sequence-to-sequence models using BPE segmentation than phrase-based or word-based NMT systems. The overall best performance of UEDIN and UFAL NMT has to be noted, since both outperform systems that explicitly model target morphology. The results for the C-set for English-to-Czech are shown in Table  7 . We now observe that factored systems are less sensitive to lexical variations and make more stable morphological predictions. The differences with the entropy values computed for the phrase-based systems are spectacular, especially for verbal morphology. We understand this poor performance for phrase-based systems as a consequence of the initial assumption those systems rely on: the concatenation of phrases to constitute an output sentence does not help to provide a single morphological prediction in slightly various contexts. As an attempt to evaluate the error margin of our evaluation results, we have run a manual check of our evaluation measures. For this, we have taken all 500 sentence pairs reflecting past tense (A-set), as well as case (pronouns to nouns in B-set), and took translations from different systems randomly. We report on cases where the modification of the source created a "bad" (meaningless or ungrammatical) variant, as well as sample translations erroneously considered successful or unsuccessful. For past tense, we observe a low quantity of false positive (1.6%) and false negative (0.4%). The ratio of bad sources is quite low as well (3%), and is mostly related to cases where a word was given the wrong analysis in the first place, such as a noun labeled by the PoS-tagger as a verb, which was then turned into a past form. For pronouns to nouns, there are nearly no bad source sentences (0.2%): the transformation of pronouns into noun phrases is quite easy and safe. While false positive labels are lower (0.2%), there is a higher amount of false positive (4.4%), which was mainly due to our word-based NMT system that generates many unknown words and presents important differences between base and variant: several adjectives and nouns, not corresponding to the ones we generated in the source sentence, could then be considered during the evaluation. For English-to-Latvian, we have represented the same types of systems as for Czech, with an additional hybrid system. The scores and mor-   phological accuracies of the systems submitted at WMT'17 are in Table  8 . ? Phrase-based systems: The Moses baseline was trained on WMT'17 data and TILDE PBMT was provided by TILDE 11 and is described in . These systems did not take part in the official WMT'17 evaluation campaign. ? Word-based NMT: NMT words is a system trained on WMT'17 parallel data with a 80K target vocabulary. It was not submitted at WMT'17 and is used here as a contrast. ? BPE-based NMT: LIMSI NMT  (Burlot et al., 2017)  is based on NMTPY and UEDIN NMT  (Sennrich et al., 2017a)  on Nematus. ? NMT modeling target morphology: LIMSI FNMT  (Burlot et al., 2017)  and LIUM FNMT (Garc?a-Mart?nez et al., 2017) use a factored output predicting words and PoS. ? Hybrid system: TILDE hybrid is an ensemble of NMT models using a PBMT to process rare and unknown words. It was submitted at WMT'17  (Pinnis et al., 2017) . The results for the A-set evaluation are in Table 9. Compared to the previous Czech evaluation, there is a less clear difference between phrase-based and NMT systems based on BPE. Indeed, TILDE hybrid has the best mean performance and is only 5 points above our Moses baseline. A possible reason for that situation is the lower amount of parallel data available for English-Latvian, compared to English-Czech. We notice that there is no significant difference between the two NMT systems and LIMSI FNMT. With this language pair, word-based NMT performs significantly worse than all other systems on all morphological features, which is confirmed by the fluency evaluation in Table  10 . Here, the factored systems tend to have a better verbal fluency, whereas NMT systems perform better on nominal agreement: LIMSI FNMT has the best mean score, but is only 0.2 points above UEDIN NMT. The best system, TILDE hybrid, is now 21.1 points above the Moses baseline, which again seems to be the main reason for such high overall morphological accuracy in Table  8 . Table 11 confirms the higher performance of NMT and factored NMT systems, with a clear advantage for TILDE hybrid, which has the best accuracy in terms of fluency, like in the previous Table 10, which tends to show some correlation between both types of tests.  When it comes to morphological correction of the output, our evaluation clearly shows the superiority of BPE-based NMT systems over phrasebased ones. On the other hand, while we observed that factored models obtain a higher performance in terms of adequacy, NMT models are still very close to them in terms of fluency. Finally, factored models, as well as TILDE hybrid, clearly showed more confidence in their predictions through slight lexical variations. 

 Related work: evaluating morphology Automatic metrics Despite their well-known flaws, "general purpose" automatic metrics such as BLEU  (Papineni et al., 2002) , TER  (Snover et al., 2006)  or METEOR  (Banerjee and Lavie, 2005)  remain the preferred way to measure progress in Machine Translation. Evaluation campaigns aimed at comparing systems have long abandoned these measures and resort to human judgments, such as ranking  (Callison-Burch et al., 2007)  or direct assessment . To compensate for the inability of eg. BLEU to detect improvements targeting specific difficulties of MT, several problem-specific measures have been introduced over the years such as the LR-Score  (Birch and Osborne, 2010)  to measure the correctness of reordering decisions, MEANT  (Lo and Wu, 2011)  to measure the transfer of entailment relationships, or CharacTER  (Wang et al., 2016)  to better assess the success of translation into a MRL.  Stanojevi? and Sima'an (2014) 's BEER is a nice example of a sophisticated metric, based on a trainable mixture of multiple metrics: for MRLs, the inclusion of character n-gram matches and of reordering scores proves critical to reach good correlation with human judgments. In comparison, the proposal of  Wang et al. (2016)  simply computes a TER-like score at the character level, thereby partially crediting a system for predicting the right lemma with the wrong morphology. Error typologies Error analysis protocols, as proposed by  Vilar et al. (2006) ;  Popovi? and Ney (2011) ;  Stymne (2011)  for PBMT systems are obvious candidates for running diagnosis studies and have been used eg. by  Bentivogli et al. (2016) ; Toral Ruiz and S?nchez-Cartagena (2017); Costajuss? (2017);  Klubi?ka et al. (2017) . These works differ in the language pairs and in the error typology considered.  Bentivogli et al. (2016)  only recognizes three main error types which are automatically recognized based on aligning the hypotheses and references -for instance a morphological error is detected when the word form is wrong, whereas the lemma is correct; this definition is also adopted in , and decomposed at the level of morphological features in ;  (Klubi?ka et al., 2017)    pology derived from the MQM proposal 12 and adapted to the English:Croatian pair -morphological errors mostly correspond to "word form" errors and are too subtle to be automatically detected. A common finding of these studies is that NMT generates better agreements than alternatives such as PBMT or Hierarchical MT. Test suites The work of  Isabelle et al. (2017) ;  Burchardt et al. (2017)  resuscitates an old tradition of using carefully designed test suites  King and Falkedal (1990) ;  Lehmann et al. (1996)  to explore the ability of NMT to handle specific classes of difficulties. Test suites typically include a small set of handcrafted sentences for each targeted type of difficulty. For instance, Isabelle et al. (  2017 ) focuses on translating from English into French and is based on a set of 108 short sentences illustrating situations of morphosyntactic, lexico-syntactic and syntactical divergences between these two languages. Assessing a system's ability to handle these difficulties requires a human judge to decide whether the automated translation has successfully "crossed" the bridge between languages. 13 A similar methodology is used in the work of  Burchardt et al. (2017) , who use a test suite of approximately 800 segments covering a wide array of translation diffi-culties for the pair English-German. Test suites enable to directly evaluate and compare specific abilities of MT Engines, including morphological competences: again, both studies found that NMT is markedly better than PBMT when it comes to phenomena such as word agreement. The downside is the requirement to have expert linguists prepare the data as well as evaluate the success of the MT system, which is a rather expensive price to pay to get a diagnostic evaluation. Automatic test suites The work by  Linzen et al. (2016)  specifically looks at the prediction of the correct agreement features in increasingly complex contexts generated by augmenting the distance between the head and its dependent and the number of intervening distractors. A language model is deemed correct if it scores the correct agreement higher than any wrong one. One intriguing finding of this study is the very good performance of RNNs, provided that they receive the right kind of feedback in training. A similar approach is adapted for MT by  Sennrich (2017) , who looks at a wider range of phenomena. Contrastive pairs as automatically produced as follows: given a correct (source, target) pair p = (f , e), introduce one error in e yielding an alternative couple p = (f , e ). A system is deemed to perform correctly wrt. this contrastive pair if it scores p higher than p . This approach is fully automatic, looks at a wide range of contexts and phenomena and also enables to focus on specific errors types; a downside is the fact that the evaluation never considers whether e is the system's best choice given source f . Regarding specifically morphology, this study mostly considers (subject-verb, as well as modifier-head noun) agreement errors, but only compares error rates of variants of NMT systems. A typology of evaluation protocols The variety of evaluation protocols found in the literature can be categorized along the following dimensions: ? holistic vs analytic: a holistic metric provides a global sentence-or document-level score, of which the morphological ability is only one part; an analytic metric focuses on specific difficulties; ? coarse vs fine-grain: a coarse (analytic) metric only provides global appreciation of morphological competence; while a fine-grain metric distinguishes various types of errors; ? natural vs hand-crafted vs artificial: for the sake of this study, this distinction relates to the design of the test sentences -were they invented for the purpose of the evaluation or found in a corpus, or even generated using automatic processing ? ? automatic vs human-judgment: is scoring fully automatic or is a human judge involved ? ? scores can be distance-based, such as a global comparison with a reference translation, or a Boolean value that denotes success or failure wrt. a local test, or based on a comparison of model scores; Based on this analysis, the work reported here is analytic/fine-grain, uses artificial data, and computes automatic scores based on a local comparison with an expected value (mostly). This is the only one of that kind we are aware of. 

 Conclusion and Outlook In this paper, we have presented a new protocol for evaluating the morphological competence of a Machine Translation system, with the aim to measure progresses in handling complex morphological phenomena in the source or the target language. We have presented preliminary experiments for two language pairs, which show that NMT systems with BPE outperform in many ways the phrase-based MT systems. Interestingly, they also reveal subtle differences among NMT systems and indicate specific areas where improvements are still needed. This work will be developed in three main directions: ? improve the generation and scoring algorithms: our procedure for generating sentences relies on automatic morphological analysis, which can be error prone, and on crude heuristics. While these two sources of noise likely have a small impact on the final results, which represent an average over a large number of sentences, we would like to better evaluate these effects, and, if needed, apply the necessary fixes; ? refine our analysis of automatic scores: the numbers reported in ? 3 are averages over multiple sentences, and could be subjected to more analyses such as looking more precisely at OOVs, or taking frequency effects in considerations. This would allow to assess a system's ability to generate the right form for frequent vs rare vs unseen lemmas or morphological features. Frequency is also often correlated with regularity, and we also would like to assess morphological competence along those lines. Likewise, analyzing performance in agreement tests with respect to the distance between two coordinated nouns or verbs might also be revealing. ? increase the set of tests: we have focused on translating English into two MRLs having similar properties. Future work includes the generation of additional inflectional contrasts (introducing for instance mood or aspect, which are morphologically marked in many languages) as well as derivational contrasts (such as diminutives for nouns, or antonyms for adjectives). Again, this implies to improve our scoring and generation algorithms, and to adapt them to new languages. Table 3 : 3 Examples of sentences that pass the tests. 
