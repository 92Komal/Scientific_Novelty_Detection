title
The LIG machine translation system for WMT 2010

abstract
This paper describes the system submitted by the Laboratory of Informatics of Grenoble (LIG) for the fifth Workshop on Statistical Machine Translation. We participated to the news shared translation task for the French-English language pair. We investigated differents techniques to simply deal with Out-Of-Vocabulary words in a statistical phrase-based machine translation system and analyze their impact on translation quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs.

Introduction We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation  (WMT 2010)  for the French-English language pair. The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system's unknown words. Indeed, as shown in  (Carpuat, 2009) ,  (Habash, 2008)  and  (Niessen, 2004) , handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is a way to improve translation quality. After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement. In the last part, we evaluate their impact on translation quality using automatic and human evaluations. 

 Baseline System Setup 

 Used Resources We used the provided Europarl and News parallel corpora (total 1,638,440 sentences) to train the translation model and the News monolingual corpora (48,653,884 sentences) to train the language model. The 2008 News test corpora (news-test2008; 2,028 sentences) was used to tune the produced system and last year's test corpora (news-test2009; 3,027 sentences) was used for evaluation purposes. These corpora will be refered to as Dev and Test later in the paper. As preprocessing steps, we applied the PERL scripts provided with the corpora to lowercase and tokenise the data. 

 Language modeling The target language model is a standard n-gram language model trained using the SRI language modeling toolkit  (Stocke, 2002)  on the news monolingual corpus. The smoothing technique we applied is the modified Kneser-Ney discounting with interpolation. 

 Translation modeling The translation model was trained using the parallel corpus described earlier (Europarl+News). First, the corpus was word aligned and then, the pairs of source and corresponding target phrases were extracted from the word-aligned bilingual training corpus using the scripts provided with the Moses decoder  (Koehn et al., 2007) . The result is a phrase-table containing all the aligned phrases. This phrase-table, produced by the translation modeling, is used to extract several translations models. In our experiment we used thirteen standard translation models: six distortion models, a lexicon word-based and a phrase-based translation model for both direction, and a phrase, word and distortion penalty. 

 Tuning and decoding For the decoding (i.e. translation of the test set), the system uses a log-linear combination of the previous target language model and the thirteen translation models extracted from the phrasetable. As the system can be beforehand tuned by adjusting log-linear combination weights on a developement corpus, we used the Minimum Error Rate Training (MERT) method, by  (Och, 2003) . 

 Ways of Improvements 

 Discussion about Out-Of-Vocabulary words in PBMT systems Phrase-based statistical machine translation (PBMT) use phrases as units in the translation process. A phrase is a sequence of n consecutive words known by the system. During the training, these phrases are automaticaly learned and each source phrase is mapped with its corresponding target phrase. Throughout test set decoding, a word not being part of this vocabulary list is labeled as "Out-Of-Vocabulary" (OOV) and, as it doesn't appear in the translation table, the system is unable to translate it. During the decoding, Out-Of-Vocabulary words lead to "broken" phrases and degrade translation quality. For these reasons, we present some techniques to handle Out-Of-Vocabulary words in a PBMT system and combine these techniques before evaluating them. In a preliminary study, we automatically extracted and manually analyzed OOVs of a 1000 sentences sample extracted from the test corpus (news-test2009). There were altogether 487 OOVs tokens wich include 64.34% proper nouns and words in foreign languages, 17.62% common nouns, 15.16% conjugated verbs, 1.84% errors in source corpus and 1.02% numbers. Note that, as our system is configured to copy systematically the OOVs in the produced translated sentence, the rewriting of proper nouns and words in foreign language is straightforward in that case. However, we still have to deal with common nouns and conjugated verbs. 

 Initial sentence: "Cela ne marchera pas" souligna-t-il par la suite. 

 Normalised sentence: "Cela ne marchera pas" il souligna par la suite Figure  1 : Normalisation of the euphonious "t" 

 Term expansion with dictionary The first idea is to expand the vocabulary size, more specifically minimizing Out-Of-Vocabulary common nouns adding a French-English dictionary during the training process. In our experiment, we used a free dictionnary made available by the Wiktionary 1 collaborative project (wich aims to produce free-content multilingual dictionaries). The provided dictionnary, containing 15,200 entries, is added to the bilingual training corpus before phrase-table extraction. 

 Lemmatization of the French source verbs To avoid Out-Of-Vocabulary conjugated verbs one idea is to lemmatize verbs in the source training and test corpus to train a so-called lemmatized system. We used the freely available French lemmatiser LIA TAGG  (B?chet, 2001) . But, applying lemmatization leads to a loss of information (tense, person, number) which may affect deeply the translation quality. Thus, we decided to use the lematized system only when OOV verbs are present in the source sentence to be translated. Consequently, we differentiate two kinds of sentences: -sentences containing at least one OOV conjugated verb, and -sentences which do not have any conjugated verb (these latter sentences obviously don't need any lemmatization!). Thereby, we decided to build a combined translation system which call the lemmatized system only when the source sentence contains at least one Out-Of-Vocabulary conjugated verb (otherwise, the sentence will be translated by the standard system). To detect sentences with Out-Of-Vocabulary conjugated verb we translate each sentence with both systems (lemmatized and standard), count OOV and use the lemmatized translation only if it contains less OOV than the standard translation. For example, a translation containing k Out-Of-Vocabulary conjugated verbs and n others Out-Of-Vocabulary words (in total k+n OOV) with the standard system, contains, most probably, only n Out-Of-Vocabulary words with the lemmatised system because the conjugated verbs will be lemmatized, recognized and translated by the system. 

 Normalization of a special French form We observed, in the French source corpra, a special French form which generates almost always Out-Of-Vocabulary words in the English translation. The special French form, named euphonious "t", consists of adding the letter "t" between a verb (ended by "a", "e" or "c") and a personal pronoun and, then, inverse them in order to facilitate the prononciation. The sequence is represented by: verb-t-pronoun like annonca-t-elle, arrive-t-il, at-on, etc. This form concerns 1.75% of the French sentences in the test corpus whereas these account for 0.66% and 0.78% respetively in the training and the developement corpora. The normalized proposed form, illustrated below in figure  1 , contains the subject pronoun (in first posistion) and the verb (in the second position). This change has no influence on the French source sentence and accordingly on the correctness and fluency of the English translation. 

 Adaptation of the language model Finally, for each system, we decided to apply different language models and to look at those who perfom well. In addition to the 5-gram language model, we trained and tested 3-gram and 4-gram language models with two different kinds of vocabularies : -the first one (conventional, refered to as n-gram in table  3 ) contains an open-vocabulary extracted from the monolingual English training data, and -the second one (refered to as n-gramvocab in table 3) contains a closed-vocabulary extracted from the English part of the bilingual training data. In both cases, language model probabilities are trained from the monolingual LM training data but, in the second case, the lexicon is restricted to the one of the phrase-table. 

 Experimental results In the automatic evaluation, the reported evaluation metric is the BLEU score  (Papineni et al., 2002)  computed by MTEval version 13a. The results are reported in table 1. Note that in our experiments, according to the resampling method of  (Koehn, 2004) , there are significative variations (improvement or deterioration), with 95% certainty, only if the difference between two BLEU scores represent, at least, 0.33 points. To complete this automatic evaluation, we performed a human analysis of the systems outputs. 

 Standard systems 

 Term expansion with dictionary Regarding the results of automatic evaluation (table 1, system (2)), adding the dictionary do not leads to a significant improvement. The OOV rate and system perplexity are reduced but, ignoring the tuned system which presents lower performance, the BLEU score decreases significatly on the test set. The BLEU score of the system augmented with the dictionary is 24.50 whereas the baseline one is 24.94. So we can conclude that there is not a meaningfull positive contribution, probably because the size of the dictionary is very small regarding the bilingual training corpus. We found out very few Out-Of-Vocabulary words of the standard system recognized by the system with the dictionary, see figure  2  for example (among them : coupon, cafard, blonde, retardataire, m?dicaments, pamplemousse, etc.). But, as the dictionnary is very small, most OOV common words like h?tesse and clignotant are still unknown. Regarding the output sentences, we note that there are very few differences and the quality is equivalent. The dictionary used is to small to extend the system's vocabulary and most of words still Out-Of-Vocabulary are conjugated verbs and unrecognized forms. 

 Baseline system: A cafard fled before the danger, but if he felt fear? 

 System with dictionary: A blues fled before the danger, but if he felt fear? Figure  2 : Example of sentence with an OOV common noun 

 Normalisation of special French form Considering the BLEU score, the normalization of French euphonious "t" have, apparently, very few repercussion on the translation result (table 1, system (3)) but the human analysis indicates that, in our context, the normalisation of euphonious "t" brings a clear improvement as seen in example 3. Consequently, this preprocessing is kept in the final system. 

 Tuning We can see in  

 System with normalisation: "It will not work" he stressed afterwards. Figure  3 : Example of sentence with a "verb-tpronoun" form gap between the developement and test corpora (ie the Dev set may be not representative of the Test set). So, even if it is recommanded in the standard process, we do not tune our system (we use the default weights proposed by the Moses decoder) and add the developement corpus to train it. In this case, the training set contains 1,640,468 sentences (the initial 1,638,440 sentences and the 2,028 sentences of the developement set). This slightly improves the system (from 24.98, the BLEU score raise to 25,05 after adding the developpement set to the training). 

 Lemmatised systems Results of lemmatised systems are reported on table 2. First, we can notice that, in this particular case, the tuning (with MERT method) is mandatory to adapt the weights of the log linear model. Our analysis of the tuned weight of the lemmatised system shows that, in particular, the word penalty model has a very low weight (this favours short sentences) and the lexical word-based translation models have a very low weight (no use of the lexical translation probability). We also notice that the lemmatization leads to a real drop-off of OOV rate (fall from 2.32% for the baseline, to 2.23% for the lemmatized system) and perplexity (fall from 207 for the baseline, to 178 for the lemmatized system). We can observe a clear decrease of the performance with the lemmatized system (BLEU score of 20.50) compared with a nonlemmatized one (BLEU score of 24.94). This can be significatively improved applying euphonious "t" normalization to the source data (BLEU score of 22.14). Almost all French OOV conjugated verbs with the standard system were recognized by the lemmatized one (trierait, joues, testaient, immerg?e, ?conomiseraient, baisserait, pr?pares, etc.) but the small decrease of the translation quality can be explained, among other things, by several tense errors. See illustration in figure  4 . So, we conclude that the systematic normalization of French verbs, as a pre-process, reduce the Out-Of-Vocabulary conjugated verbs but decrease slighly the final translation quality. The use of such a system is helpfull especially when the sentence contains conjugated verbs (see example 5). 

 Adaptation of the language model We applied five differents language models (3gram and 4-gram language models with selected vocabulary or not and a 5-gram language model) to the four standard systems and the two lemmatised one. The results, reported in table  3 , show that BLEU score can be significantly different depending on the language model used. For example, the fifth system (5) obtained a BLEU score of 21.48 with a 3-gram language model and a BLEU score of 22.84 with a 4-gram language model. We can also notice that five out of our six systems outperform using a language model with selected vocabulary (n-gram-vocab). One possible explanation is that with LM using selected vocabulary (ngram-vocab), there is no loss of probability mass for english words not present in the translation table. 

 Final combined system Considering the previous observations, we believe that the best choice is to apply the lemmatized system only if necessary i.e. only if the sentence contains OOV conjugated verbs, otherwise, a standard system should be used. We consider system (4), with 4-gram-vocab language model (selected vocabulary) without tuning, as the best standard system and system (6), with 3-gram-vocab language model (selected vocabulary) not tuned either, as the best lemmatized system.   18% 175 27.81 (9.20) 22.14 (10.82)  Table  2 : Lemmatised systems BLEU scores with tuning (without tuning)/ LM 5-gram Baseline system: You will be limited by the absence of exit for headphones. Lemmatised system: You are limited by the lack of exit for ordinary headphones. reference: You will be limited by the absence of output on ordinary headphones. Figure  4 : Example of sentences without OOV verbs system translations are those of the lemmatized system (6) when we translate sentences with one or more Out-Of-Vocabulary conjugated verbs and those of the un-lemmatized system (4) otherwise. Around 6% of test set sentences were translated by the lemmatized system. Considering the results reported in table 4, the combined system's BLEU score is comparable to the standard one (25.11 against 25.17). 

 System Test score sentences (4) Standard sys. 25.17 94 % (6) Lemmatised sys. 22.89 6% (7) Combined 25.11 100 % Table  4 : Combined system's results and % translated sentences by each system 

 Human evaluation We compared two data set. The first set (selected sent.) contains 301 sentences selected from test data by the combined system (7) to be translated by the lemmatized system (6) whereas the second set (random sent.) contains 301 sentences randomly picked up. The latter is our control data set. We compared for both groups the translation hypothesis given by the lemmatized system and the standard one. We performed a subjective evaluation with the NIST five points scales to measure fluency and adequacy of each sentences through SECtra w interface  (Huynh et al., 2009) .We involved a total of 6 volunteers judges (3 for each set). We evaluated the inter-annotator agreement using a generalized version of Kappa. The results show a slight to fair agreement according  (Landis, 1977) . The evaluation results, detailled in table 5 and 6, showed that both fluency and adequacy were im-proved using our combined system. Indeed, for a random input (random sent.), the lemmatized system lowers the translations quality (fluency and adequacy are degraded for, respectively, 35.8% and 37.5% of the sentences), while it improves the quality for sentences selected by the combined system (for "selected sent.", fluency and adequacy are improved or stable for 81% of the sentences). Adequacy selected sent. random sent. (6) ? (4) 81% 62.4% (6) < (4) 18.9% 37.5%  

 Conclusion and Discussion We have described the system used for our submission to the WMT'10 shared translation task for the French-English language pair. We propose dsome very simple techniques to improve rapidely a statistical machine translation. Those techniques particularly aim at handling Out-Of-Vocabulary words in statistical phrasebased machine translation and lead an improved fluency in translation results. The submited system (see section 4.4) is a combination between a standard system and a lemmatized system with appropriate setup. Table 1 : 1 table 1 that the usual tuning with Minimum Error Rate Training algorithm deteriorates systematically performance scores on the test set, for all systems. This can be explained by the Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram System OOVs ppl Dev score Test score (1) Baseline 2.32% 207 29.72 (19.93) 23.77 (24.94) (2) + dictionary 2.30% 204 30.01 (23.92) 24.32 (24.50) (3) + normalization 2.31% 204 30.07 (19.90) 23.99 (24.98) (4) + normalization + Dev data 2.30% 204 / (/) / (25,05) Baseline system: "It will not work" souligna-t-il afterwards. 
