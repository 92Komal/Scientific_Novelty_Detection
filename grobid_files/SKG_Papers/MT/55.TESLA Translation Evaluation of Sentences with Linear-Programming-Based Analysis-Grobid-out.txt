title
TESLA: Translation Evaluation of Sentences with Linear-programming-based Analysis

abstract
We present TESLA-M and TESLA, two novel automatic machine translation evaluation metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks.

Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE  (Pado et al., 2009)  and ULC  (Gim?nez and M?rquez, 2008)  performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as ME-TEOR  (Banerjee and Lavie, 2005) , MaxSim  (Chan and Ng, 2008) , wpF and wpBleu  (Popovi? and Ney, 2009)  exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU  (Papineni et al., 2002)  and its variants, TER  (Snover et al., 2006) , among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. We believe that the lightweight linguistic approaches are a good compromise given the current state of computational linguistics research and resources. In this paper, we devise TESLA-M and TESLA, two lightweight approaches to MT evaluation. Specifically: (1) the core features are Fmeasures derived by matching bags of N-grams; (2) both recall and precision are considered, with more emphasis on recall; and (3) WordNet synonyms feature prominently. The main novelty of TESLA-M compared to METEOR and MaxSim is that we match the Ngrams under a very expressive linear programming framework, which allows us to assign weights to the N-grams. This is in contrast to the greedy approach of METEOR, and the more restrictive maximum bipartite matching formulation of MaxSim. In addition, we present a heavier version TESLA, which combines the features using a linear model trained on development data, making it easy to exploit features not on the same scale, and leaving open the possibility of domain adaptation. It also exploits parallel texts of the target language with other languages as a shallow semantic representation, which allows us to model phrase synonyms and idioms. In contrast, METEOR and MaxSim are capable of processing only word synonyms from WordNet. The rest of this paper is organized as follows. Section 2 gives a high level overview of the evaluation task. Sections 3 and 4 describe TESLA-M and TESLA, respectively. Section 5 presents experimental results in the setting of the WMT 2009 shared evaluation task. Finally, Section 6 concludes the paper. 

 Overview We consider the task of evaluating machine translation systems in the direction of translating the source language to the target language. Given a reference translation and a system translation, the goal of an automatic machine translation evaluation algorithm such as TESLA(-M) is to output a score predicting the quality of the system translation. Neither TESLA-M nor TESLA requires the source text, but as additional linguistic resources, TESLA makes use of phrase tables generated from parallel texts of the target language and other languages, which we refer to as pivot languages. The source language may or may not be one of the pivot languages. 

 TESLA-M This section describes TESLA-M, the lighter one among the two metrics. At the highest level, TESLA-M is the arithmetic average of Fmeasures between bags of N-grams (BNGs). A BNG is a multiset of weighted N-grams. Mathematically, a BNG B consists of tuples (b i , b W i ), where each b i is an N-gram and b W i is a positive real number representing its weight. In the simplest case, a BNG contains every N-gram in a translated sentence, and the weights are just the counts of the respective N-grams. However, to emphasize the content words over the function words, we discount the weight of an N-gram by a factor of 0.1 for every function word in the Ngram. We decide whether a word is a function word based on its POS tag. In TESLA-M, the BNGs are extracted in the target language, so we call them bags of target language N-grams (BTNGs). 

 Similarity functions To match two BNGs, we first need a similarity measure between N-grams. In this section, we define the similarity measures used in our experiments. We adopt the similarity measure from MaxSim as s ms . For unigrams x and y, ? If lemma(x) = lemma(y), then s ms = 1. ? Otherwise, let a = I(synsets(x) overlap with synsets(y)) b = I(POS(x) = POS(y)) where I(?) is the indicator function, then s ms = (a + b)/2. The synsets are obtained by querying WordNet  (Fellbaum, 1998) . For languages other than English, a synonym dictionary is used instead. We define two other similarity functions between unigrams: s lem (x, y) = I(lemma(x) = lemma(y)) s pos (x, y) = I(POS(x) = POS(y)) All the three unigram similarity functions generalize to N-grams in the same way. For two N-grams x = x 1,2,...,n and y = y 1,2,...,n , s(x, y) = 0 if ?i, s(x i , y i ) = 0 1 n n i=1 s(x i , y i ) otherwise 3. 

 Matching two BNGs Now we describe the procedure of matching two BNGs. We take as input the following: 1. Two BNGs, X and Y . The ith entry in X is x i and has weight x W i (analogously for y j and y W j ). 2. A similarity measure, s, that gives a similarity score between any two entries in the range of 0 to 1. Intuitively, we wish to align the entries of the two BNGs in a way that maximizes the overall similarity. As translations often contain one-to-many or many-to-many alignments, we allow one entry to split its weight among multiple alignments. An example matching problem is shown in Figure  1a , where the weight of each node is shown, along with the similarity for each edge. Edges with a similarity of zero are not shown. The solution to the matching problem is shown in Figure  1b , and the overall similarity is 0.5 ? 1.0 + 0.5 ? 0.6 + 1.0 ? 0.2 + 1.0 ? 0.1 = 1.1. Mathematically, we formulate this as a (realvalued) linear programming problem 1 . The variables are the allocated weights for the edges The value of the objective function is the overall similarity S. Assuming X is the reference and Y is the system translation, we have w(x i , y j ) ?i, j We maximize i,j s(x i , y j )w(x i , y j ) subject to w(x i , y j ) ? 0 ?i, j j w(x i , y j ) ? x W i ?i i w(x i , y j ) ? y W Precision = S j y W j Recall = S i x W i The F-measure is derived from the precision and the recall: F = Precision ? Recall ? ? Precision + (1 ? ?) ? Recall In this work, we set ? = 0.8, following MaxSim. The value gives more importance to the recall than the precision. 

 Scoring The TESLA-M sentence-level score for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions s ms and s pos . We thus have 3 ? 2 = 6 features for TESLA-M. We can compute a system-level score for a machine translation system by averaging its sentencelevel scores over the complete test set. 

 Reduction When every x W i and y W j is 1, the linear programming problem proposed above reduces to weighted bipartite matching. This is a well known result; see for example,  Cormen et al. (2001)  for details. This is the formalism of MaxSim, which precludes the use of fractional weights. If the similarity function is binary-valued and transitive, such as s lem and s pos , then we can use a much simpler and faster greedy matching procedure: the best match is simply g min( x i =g x W i , y i =g y W i ). 

 TESLA Unlike the simple arithmetic average used in TESLA-M, TESLA uses a general linear combination of three types of features: BTNG Fmeasures as in TESLA-M, F-measures between bags of N-grams in each of the pivot languages, called bags of pivot language N-grams (BPNGs), and normalized language model scores of the system translation, defined as 1 n log P , where n is the length of the translation, and P the language model probability. The method of training the linear model depends on the development data. In the case of WMT, the development data is in the form of manual rankings, so we train SVM rank  (Joachims, 2006)  on these instances to build the linear model. In other scenarios, some form of regression can be more appropriate. The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 

 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++  (Och and Ney, 2003)  or the Berkeley aligner  (Liang et al., 2006; Haghighi et al., 2009) . We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers  (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009) . We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, the phrases good morning and hello are unrelated even with a synonym dictionary, but they both very often align to the same French phrase bonjour, and we conclude they are semantically related to a high degree. 

 Segmenting a sentence into phrases To extend the concept of this semantic representation of phrases to sentences, we segment a sentence in the target language into phrases. Given a phrase table, we can approximate the probability of a phrase p by: P r(p) = N (p) p N (p ) (1) where N (?) is the count of a phrase in the phrase table  .  We then define the likelihood of segmenting a sentence S into a sequence of phrases (p 1 , p 2 , . . . , p n ) by: P r(p 1 , p 2 , . . . , p n |S) = 1 Z(S) n i=1 P r(p i ) (2) where Z(S) is a normalizing constant. The segmentation of S that maximizes the probability can be determined efficiently using a dynamic programming algorithm. The formula has a strong preference for longer phrases, as every P r(p) is a small fraction. To deal with out-of-vocabulary (OOV) words, we allow any single word w to be considered a phrase, and if N (w) = 0, we set N (w) = 0.5 instead. 

 BPNGs as sentence level semantic representation Simply merging the phrase-level semantic representation is insufficient to produce a sensible sentence-level semantic representation. As an example, we consider two target language (English) sentences segmented as follows: We can collect the N-gram statistics of this ensemble of French sentences efficiently from the confusion network representation. For example, the trigram Bonjour , Querrien 2 would receive a weight of 0.9 ? 1.0 = 0.9 in Figure  2 . As with BTNGs, we discount the weight of an N-gram by a factor of 0.1 for every function word in the N-gram, so as to place more emphasis on the content words. The collection of all such N-grams and their corresponding weights forms the BPNG of a sentence. The reference and system BPNGs are then matched using the algorithm outlined in Section 3.2. 

 Scoring The TESLA sentence-level score is a linear combination of (1) BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions s ms and s pos , (2) BPNG F-measures for unigrams, bigrams, and trigrams based on similarity functions s lem and s pos for each pivot language, and (3) normalized language model scores. In this work, we use two language models. We thus have 3 ? 2 features from the BTNGs, 3 ? 2 ? #pivot languages features from the BPNGs, and 2 features from the language models. Again, we can compute system-level scores by averaging the sentence-level scores. 

 Experiments 

 Setup We test our metrics in the setting of the WMT 2009 evaluation task  (Callison-Burch et al., 2009) . The manual judgments from WMT 2008 are used as the development data and the metric is evaluated on WMT 2009 manual judgments with respect to two criteria: sentence level consistency and system level correlation. The sentence level consistency is defined as the percentage of correctly predicted pairs among all the manually judged pairs. Pairs judged as ties by humans are excluded from the evaluation. The system level correlation is defined as the average Spearman's rank correlation coefficient across all translation tracks. 

 Pre-processing We POS tag and lemmatize the texts using the following tools: for English, OpenNLP POS-tagger 3 and WordNet lemmatizer; for French and German, TreeTagger 4 ; for Spanish, the FreeLing toolkit  (Atserias et al., 2006) ; and for Czech, the Morce morphological tagger 5 . For German, we additionally perform noun compound splitting. For each noun, we choose the split that maximizes the geometric mean of the frequency counts of its parts, following the method in  (Koehn and Knight, 2003) : max n,p 1 ,p 2 ,...,pn n i=1 N (p i ) 1 n The resulting compound split sentence is then POS tagged and lemmatized. Finally, we remove all non-alphanumeric tokens from the text in all languages. To generate the language model features, we train SRILM  (Stolcke, 2002)  trigram models with modified Kneser-Ney discounting on the supplied monolingual Europarl and news commentary texts. We build phrase tables from the supplied news commentary bitexts. Word alignments are produced by the Berkeley aligner. The widely used phrase extraction heuristic in  is used to extract phrase pairs and phrases of up to 4 words are collected. 

 Into-English task For each of the BNG features, we generate three scores, for unigrams, bigrams, and trigrams respectively. For BPNGs, we generate one such triple for each of the four pivot languages supplied, namely Czech, French, German, and Spanish.   1  compares the scores of TESLA and TESLA-M against three participants in WMT 2009 under identical settings 6 : ULC (a heavyweight linguistic approach with the best performance in WMT 2009), MaxSim, and ME-TEOR. The results show that TESLA outperforms all these systems by a substantial margin, and TESLA-M is very competitive too. 

 Out-of-English task A synonym dictionary is required for target languages other than English. We use the freely available Wiktionary dictionary 7 for each language. For Spanish, we additionally use the Spanish WordNet, a component of FreeLing. Only one pivot language (English) is used for the BPNG. For the English-Czech task, we only have one language model instead of two, as the Europarl language model is not available. Tables  2 and 3  show the sentence-level consistency and system-level correlation respectively of TESLA and TESLA-M against the best reported results in WMT 2009 under identical setting. The results show that both TESLA and TESLA-M give very competitive performances. Interestingly, TESLA and TESLA-M obtain similar scores in the out-of-English task. This could be because we use only one pivot language (English), compared to four in the into-English task. We plan to investigate this phenomenon in our future work. 

 Conclusion This paper describes TESLA-M and TESLA. Our main contributions are: (1) we generalize the bipartite matching formalism of MaxSim into a more expressive linear programming framework;  6  The original WMT09 report contained erroneous results. The scores here are the corrected results released after publication. 7 www.wiktionary.org   Figure 1: A BNG matching problem 
