title
Metric for Automatic Machine Translation Evaluation based on Universal Sentence Representations

abstract
Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Although it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.

Introduction This paper describes a segment-level metric for automatic machine translation evaluation (MTE). MTE metrics having a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU  (Lin and Och, 2004) , which is a smoothed version of BLEU  (Papineni et al., 2002) , Blend  (Ma et al., 2017) , MEANT 2.0  (Lo, 2017) , and chrF++  (Popovi?, 2017) , which achieved excellent results in the WMT-2017 Metrics task  (Bojar et al., 2017) . Therefore, they can exploit only limited information for segment-level MTE. In other words, MTE metrics based on character N-grams or word N-grams cannot make full use of sentence representations; they only check for word matches. We propose a segment-level MTE metric by using universal sentence representations capable of capturing information that cannot be captured by local features based on character or word Ngrams. The results of an experiment in segmentlevel MTE conducted using the datasets for to-English language pairs on WMT-2016 indicated that the proposed regression model using sentence representations achieves the best performance. The main contributions of the study are summarized below: ? We propose a novel supervised regression model for segment-level MTE based on universal sentence representations. ? We achieved state-of-the-art performance on the WMT-2016 dataset for to-English language pairs without using any complex features and models. 

 Related Work DPMF comb  (Yu et al., 2015a)  achieved the best performance in the WMT-2016 Metrics task  (Bojar et al., 2016) . It incorporates 55 default metrics provided by the Asiya MT evaluation toolkit 1  (Gim?nez and M?rquez, 2010) , as well as three other metrics, namely, DPMF  (Yu et al., 2015b) , REDp  (Yu et al., 2015a) , and ENTFp  (Yu et al., 2015a) , using ranking SVM to train parameters of each metric score. DPMF evaluates the syntactic similarity between an MT hypothesis and a reference translation. REDp evaluates an MT hypothesis based on the dependency tree of the reference translation that comprises both lexical and syntactic information. ENTFp  (Yu et al., 2015a)  evaluates the fluency of an MT hypothesis.  Figure  3 : Outline of our metric. After the success of DPMF comb , Blend 2  (Ma et al., 2017)  achieved the best performance in the WMT-2017 Metrics task  (Bojar et al., 2017) . Similar to DPMF comb , Blend is essentially an SVR (RBF kernel) model that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely, BEER  (Stanojevi? and Sima'an, 2015) , CharacTER  (Wang et al., 2016) , DPMF and ENTFp. BEER  (Stanojevi? and Sima'an, 2015)  is a linear model based on character N-grams and replacement trees. Charac-TER  (Wang et al., 2016)  evaluates an MT hypothesis based on character-level edit distance. DPMF comb is trained through relative ranking of human evaluation data in terms of relative ranking (RR). The quality of five MT hypotheses of the same source segment are ranked from 1 to 5 via comparison with the reference translation. In contrast, Blend is trained through direct assessment (DA) of human evaluation data. DA provides the absolute quality scores of hypotheses, by measuring to what extent a hypothesis adequately expresses the meaning of the reference translation. The results of the experiments in segment-level MTE conducted using the datasets for to-English language pairs on WMT-2016 showed that Blend achieved a better performance than DPMF comb (Table  2 ). In this study, we use Blend and propose a supervised regression model trained using DA human evaluation data. 

 Instead of using local and lexical features, 2 http://github.com/qingsongma/blend ReVal 3  (Gupta et al., 2015a,b)  proposes using sentence-level features. It is a metric using Tree-LSTM  (Tai et al., 2015)  for training and capturing the holistic information of sentences. It is trained using datasets of pseudo similarity scores, which is generated by translating RR data, and out-domain datasets of similarity scores of SICK 4 . However, the training dataset used in this metric consists of approximately 21,000 sentences; thus, the learning of Tree-LSTM is unstable and accurate learning is difficult (Table  2 ). The proposed metric uses sentence representations trained using LSTM as sentence information. Further, we apply universal sentence representations to this task; these representations were trained using largescale data obtained in other tasks. Therefore, the proposed approach avoids the problem of using a small dataset for training sentence representations. 

 Regression Model for MTE Using Universal Sentence Representations The proposed metric evaluates MT results with universal sentence representations trained using large-scale data obtained in other tasks. First, we explain two types of sentence representations used in the proposed metric in Section 3.1. Then, we explain the proposed regression model and feature extraction for MTE in Section 3.2. 

 Universal Sentence Representations Several approaches have been proposed to learn sentence representations. These sentence representations are learned through large-scale data so cs-en de-en fi-en ro-en ru-en tr-en WMT-2015 500 500 500 -500 -WMT-2016 560 560 560 560 560 560 Table  1 : Number of DA human evaluation datasets for to-English language pairs 8 in WMT-2015  and  WMT-2016 (Bojar et al., 2016 . that they constitute potentially useful features for MTE. These have been proved effective in various NLP tasks such as document classification and measurement of semantic textual similarity, and we call them universal sentence representations. First, Skip-Thought 5  builds an unsupervised model of universal sentence representations trained using three consecutive sentences, such as s i?1 , s i , and s i+1 . It is an encoderdecoder model that encodes sentence s i and predicts previous and next sentences s i?1 and s i+1 from its sentence representation ? s i (Figure  1 ). As a result of training, this encoder can produce sentence representations. Skip-Thought demonstrates high performance, especially when applied to document classification tasks. Second, InferSent 6  (Conneau et al., 2017)  constructs a supervised model computing universal sentence representations trained using Stanford Natural Language Inference (SNLI) datasets  7 (Bowman et al., 2015) . The Natural Language Inference task is a classification task of sentence pairs with three labels, entailment, contradiction and neutral; thus, InferSent can train sentence representations that are sensitive to differences in meaning. This model encodes sentence pairs u and v and generates features by sentence representations ? u and ? v with a bi-directional LSTM architecture with max pooling (Figure  2 ). InferSent demonstrates high performance across various document classification and semantic textual similarity tasks. 

 Regression Model for MTE In this paper, we propose a segment-level MTE metric for to-English language pairs. This problem can be treated as a regression problem that estimates translation quality as a real number from an MT hypothesis t and a reference translation r. Once d-dimensional sentence vectors ? t and ? r are generated, the proposed model applies the follow-5 https://github.com/ryankiros/skip-thoughts 6 https://github.com/facebookresearch/InferSent 7 https://nlp.stanford.edu/projects/snli/ ing three matching methods to extract relations between t and r (Figure  3 ). ? Concatenation: ( ? t, ? r) ? Element-wise product: ? t * ? r ? Absolute element-wise difference: | ? t ? ? r| Thus, we perform regression using 4ddimensional features of ? t, ? r, ? t * ? r and | ? t ? ? r|. 

 Experiments of Segment-Level MTE for To-English Language Pairs We performed experiments using evaluation datasets of the WMT Metrics task to verify the performance of the proposed metric. 

 Setups Datasets. We used datasets for to-English language pairs from the WMT-2016 Metrics task  (Bojar et al., 2016)  as summarized in Table  1 . Following  Ma et al. (2017) , we employed all other to-English DA data as training data (4,800 sentences) for testing on each to-English language pair (560 sentences) in WMT-2016. Features. Publicly available pre-trained sentence representations such as Skip-Thought 5 and InferSent 6 were used as the features mentioned in Section 3. Skip-Thought is a collection of 4,800-dimensional sentence representations trained on 74 million sentences of the BookCorpus dataset . InferSent is a collection of 4,096-dimensional sentence representations trained on both 560,000 sentences of the SNLI dataset  (Bowman et al., 2015)  and 433,000 sentences of the MultiNLI dataset  (Williams et al., 2017) . Model. Our regression model used SVR with the RBF kernel from scikit-learn 9 . Hyperparameters were determined through 10-fold cross cs-en de-en fi-en ro-en ru-en tr-en Avg. SentBLEU  (Bojar et al., 2016)  0.557 0.448 0.484 0.499 0.502 0.532 0.504 Blend  (Ma et al., 2017)  0.709 0.601 0.584 0.636 0.633 0.675 0.640 DPMF comb  (Bojar et al., 2016)  0.713 0.584 0.598 0.627 0.615 0.663 0.633 ReVal  (Bojar et al., 2016)  0.577 0.528 0.471 0.547 0.528 0.531 0.530 SVR with Skip-Thought 0.665 0.571 0.609 0.677 0.608 0.599 0.622 SVR with InferSent 0.679 0.604 0.617 0.640 0.644 0.630 0.636 SVR with InferSent + Skip-Thought 0.686 0.611 0.633 0.660 0.649 0.646 0.648 Table  2 : Segment-level Pearson correlation of metric scores and DA human evaluations scores for to-English language pairs in  WMT-2016 . validation using the training data. We examined all combinations of hyper-parameters among C ? {0.01, 0.1, 1.0, 10}, ? ? {0.01, 0.1, 1.0, 10}, and ? ? {0.01, 0.1, 1.0, 10}. There are three comparison methods: Blend  (Ma et al., 2017) , DPMF comb  (Yu et al., 2015a) , and ReVal  (Gupta et al., 2015a,b) , as described in Section 2. Blend and DPMF comb are MTE metrics that exhibited the best performance in the WMT-2017 Metrics task  (Bojar et al., 2017)  and WMT-2016 Metrics task, respectively. We compared the Pearson correlation of each metric score and DA human evaluation scores. 

 Result As can be seen in Table  2 , the proposed metric, which combines InferSent and Skip-Thought representations, surpasses the best performance in three out of six to-English languages pairs and achieves state-of-the-art performance on average. 

 Discussion These results indicate that it is possible to adopt universal sentence representations in MTE by training a regression model using DA human evaluation data. Since Blend is an ensemble method using combinations of various MTE metrics as features, our results show that universal sentence representations can consider information more abundantly than a complex model. Since ReVal is also based on sentence representations, we conclude that universal sentence representations trained on a large-scale dataset are more effective for MTE tasks than sentence representations trained on a small or limited in-domain dataset. 

 Error Analysis We re-implemented Blend 10  (Ma et al., 2017)  and compared the evaluation results with the proposed metric.  11  We analyzed 20% of the pairs of MT hypotheses and reference translations (112 sentence pairs ? 6 languages = 672 sentence pairs) in descending order of DA human score in each language pair. In other words, the top 20% of MT hypotheses that were close to the meaning of the reference translations for each language pair were analyzed. Among these, only Blend estimates the translation quality as high for 70 sentence pairs, and only our metric estimates the translation quality as high for 88 sentence pairs. Surface. Among pairs estimated to have high translation quality by each method, there were 26 pairs in Blend and 42 pairs in the proposed method with a low word surface matching rate between MT hypotheses and reference translations. This result shows that the proposed metric can evaluate a wide range of sentence information that cannot be captured by Blend. Unknown words. There were 26 MT hypotheses consisting of words that were treated as unknown words in Skip-Thought or InferSent that were correctly evaluated in Blend. On the other hand, there were 26 MT hypotheses that were correctly evaluated in the proposed metric. This result shows that the proposed metric is affected by unknown words. However, it is also true that there are some MT hypotheses containing unknown words that can be correctly evaluated. Therefore, we analyzed further by focusing on sentence length. There were 17 MT hypotheses consisting of words that were treated as unknown words by either Skip-Thought or InferSent with a short length (15 words or less) that were correctly evaluated in Blend. However, in the proposed metric, there were only two MT hypotheses that were correctly evaluated. This result indicates that the shorter the sentence, the more likely is the proposed metric to be affected by unknown words. 

 Conclusions In this study, we tried to apply universal sentence representation to MTE based on the DA of human evaluation data. Our segment-level MTE metric achieved the best performance on the WMT-2016 dataset. We conclude that: ? Universal sentence representations can consider information more comprehensively than an ensemble metric using combinations of various MTE metrics based on features of character or word N-grams. ? Universal sentence representations trained on a large-scale dataset are more effective than sentence representations trained on a small or limited in-domain dataset. ? Although a metric based on SVR with universal sentence representations is not good at handling unknown words, it correctly estimates the translation quality of MT hypotheses with a low word matching rate with reference translations. Following the success of In-ferSent  (Conneau et al., 2017) , many works  (Wieting and Gimpel, 2017; Cer et al., 2018; Subramanian et al., 2018)  on universal sentence representations have been published. Based on the results of our work, we expect that the MTE metric will be further improved using these better universal sentence representations. Figure 1 : 1 Figure 1: Outline of Skip-Thought. 

 Figure 2 : 2 Figure 2: Outline of InferSent.Figure3: Outline of our metric. 

			 http://asiya.lsi.upc.edu/ 

			 https://github.com/rohitguptacs/ReVal 4 http://clic.cimec.unitn.it/composes/sick.html 

			 en: English, cs: Czech, de: German, fi: Finnish, ro: Romanian, ru: Russian, tr: Turkish 9 http://scikit-learn.org/stable/ 

			 http://github.com/qingsongma/blend11  The average Pearson correlation of all language pairs after re-implementing Blend was 0.636, which is a little lower than the value reported in their paper. However, we judged that the following discussion will not be affected by this difference.
