title
Mitigating Gender Bias in Machine Translation with Target Gender Annotations

abstract
When translating "The secretary asked for details." to a language with grammatical gender, it might be necessary to determine the gender of the subject "secretary". If the sentence does not contain the necessary information, it is not always possible to disambiguate. In such cases, machine translation systems select the most common translation option, which often corresponds to the stereotypical translations, thus potentially exacerbating prejudice and marginalisation of certain groups and people. We argue that the information necessary for an adequate translation can not always be deduced from the sentence being translated or even might depend on external knowledge. Therefore, in this work, we propose to decouple the task of acquiring the necessary information from the task of learning to translate correctly when such information is available. To that end, we present a method for training machine translation systems to use word-level annotations containing information about subject's gender. To prepare training data, we annotate regular source language words with grammatical gender information of the corresponding target language words. Using such data to train machine translation systems reduces their reliance on gender stereotypes when information about the subject's gender is available. Our experiments on five language pairs show that this allows improving accuracy on the WinoMT test set by up to 25.8 percentage points.

Introduction Most modern natural language processing (NLP) systems learn from natural language data. Findings of social sciences and corpus linguistics, however, indicate various forms of bias in the way humans *First authors with equal contribution. use language  (Coates, 1987; Butler, 1990; Fuertes-Olivera, 2007; Rickford, 2016) . Thus the resulting NLP resources and systems also suffer from the same socially constructed biases, as well as inaccuracies and incompleteness  (J?rgensen et al., 2015; Prates et al., 2019; Vanmassenhove et al., 2019; Bordia and Bowman, 2019; Davidson et al., 2019; Tan and Celis, 2019) . Due to the prevalent use of NLP systems, their susceptibility to social biases becomes an increasingly significant concern as NLP systems not only reflect the biases learned but also amplify and perpetuate them further  (Hovy and Spruit, 2016; Crawford, 2017; HLEG, 2019) . This work concerns mitigating the manifestations of gender bias in the outputs of neural machine translation (NMT) systems in scenarios where the source language does not encode the information about gender that is required in the target language. An example is the translation of the English sentence "The secretary asked for details." into Latvian. In English, the gender of "secretary" is ambiguous. In Latvian, however, there is a choice between the masculine noun "sekret?rs" and the feminine noun "sekret?re". In cases when sentences do not contain the necessary information, NMT systems opt for translations which they have seen in training data most frequently. Acquiring the necessary information, however, might require analysis of the text beyond the level of individual sentences or require incorporation of external knowledge. Falling back to biases, however, happens not only in the absence of the required information as NMT systems produce stereotyped translations even when clues about the subject's correct gender are present in the sentence  (Stanovsky et al., 2019) . This is in line with findings by  Vanmassenhove et al. (2019)  who suggest that NMT systems produce biased outputs not only because of the biases present in data but also due to their tendency to exacerbate them. To provide means for incorporation of external and explicit gender information, we propose a method for training NMT systems to use wordlevel gender annotations. To prepare training data, we project grammatical gender information of regular target language words onto the corresponding source language words. Albeit in some cases redundant, we expect that the grammatical gender information contains a useful learning signal that helps narrowing down the lexical choice of the correct target translation. As a result, the NMT system learns to rely on these annotations when and where they are available. In particular, in experiments on five language pairs, we show that the methods proposed here can be used in tandem with off-the-shelf co-reference resolution tools to improve accuracy on the WinoMT challenge set  (Stanovsky et al., 2019)  by up to 25.8 percentage points. 

 Related work Recent recommendations for ethics guidelines for trustworthy AI recommend removing socially constructed biases at the source, the training data, prior to model training  (HLEG, 2019) . An example of work on debiasing training data is  Zhao et al. (2018)  where authors identified sentences containing animate nouns and changed their grammatical gender to the opposite.  Zmigrod et al. (2019)  take it further by ensuring that not only the animate nouns but also the rest of the sentence is reinflected from masculine to feminine (or vice-versa), thus preserving the morpho-syntactic agreement of the whole sentence. The applicability of this line of work is still to be established as reinflecting sentences with co-references or pairs of parallel sentences in NMT pose an additional challenge. A different take on addressing gender biases in NMT outputs is the work on alternative generation: given a gender-ambiguous source sentence and its translation, provide an alternative translation using the opposite gender.  Habash et al. (2019)  approach this as a gender classification and reinflection task for target language sentences to address the first person singular cases when translating from English into Arabic.  Bau et al. (2018)  analyze trained NMT models to identify neurons that control various features, including gender information, that are used to generate the target sentence. In practice, however, such solutions are limited to simple source sentences where only one alternative in the target language is possible. A complementary approach is addressing gender bias in NMT as a problem of domain mismatch. When translating TED talks,  Michel and Neubig (2018)  propose to adapt the NMT model for each speaker's attributes, thus also implicitly addressing previously poorly translated first-person singular cases.  Saunders and Byrne (2020)  describe methods for NMT model adaptation using a handcrafted gender-balanced dataset and a translation re-scoring scheme based on the adapted models. The closest line of work to ours is the work on the incorporation of external gender information in the NMT input.  Elaraby et al. (2018)  and  Vanmassenhove et al. (2018)  prepend training data sentences with speaker gender information to improve spoken language translation when translating into languages with grammatical gender.  Moryossef et al. (2019)  undertakes a similar approach at the inference time using phrases (e.g. "she said:") that imply the speaker's gender. The methods proposed in this work differ from the previous work in terms of annotation granularity: we propose to use token level annotations, while the previous work used one annotation per sentence. As our training data annotations are solely based on grammatical gender, preparing them does not require any external gender information. Thus our approach is also simpler in terms of training data preparation compared to the previous work  (Elaraby et al., 2018; Vanmassenhove et al., 2018) . 

 Social Impact We propose methods to mitigate the manifestations of gender bias in the outputs of NMT. Specifically, these methods provide explicit means to incorporate information about subjects referential or social gender in NMT, thus reducing gender-based stereotyping when translating into languages which encode for grammatical gender in animate nouns. An example of a use case and a beneficiary group is the translation of occupational nouns into languages which mark gender and people for whom stereotypes of their profession do not align with their gender. While these methods can relieve gender-based representational harms by reducing stereotyped translations, they, unfortunately, provide no means for better representation of non-binary gender identities. 

 Methods When translating from languages without grammatical gender to languages with grammatical gender, s?kot ar Polijas santehni?i un beidzot ar Indijas datorprogramm?t?ju vai gr?matvedi ! out with the Polish plumber , in with the Indian computer programmer or accountant ! U U F M U U U F M U F U Target to source alignment 

 Target sentence Gender annotations 1-0 1-1 1-2 2-3 3-4 6-6 6-7 6-8 7-9 8-10 8-11 9-12 10-13 11-14 U U U F M U U U U F M M U F U Source sentence Gender annotations certain words in the source sentence may not contain all the necessary information to produce an adequate and accurate translation. Examples are pronouns (e.g. I, me, they, them, themselves), animate nouns such as job titles and proper nouns such as names and surnames, which depending on the sentence context can be ambiguous and consequently can be translated poorly. Previous work has also shown that NMT systems are better at translating sentences that align with socially constructed gender stereotypes because they are more frequently seen in training data  (Stanovsky et al., 2019; Prates et al., 2019) . To circumvent the degradation of NMT outputs due to 1) socially constructed biases and 2) absence of necessary information, we propose a method for training NMT systems to be aware of and use wordlevel target gender annotations (TGA). For training, we use data where regular source language words are annotated with the grammatical gender of their target language translations. We obtain such data by, first, morphologically tagging target language sentences to obtain information about their grammatical gender-F for feminine, M for masculine, N for neuter, and U for cases where grammatical gender is unavailable. Then, we use word-level statistical alignments to project this information from the target language to the source language words (see Figure  1  for an illustration). We use source-side factors  to integrate the projected annotations as an additional input stream of the NMT system. To ensure that the NMT systems are capable of producing adequate translations when gender annotations are not available-a frequently expected case at the test time-we apply TGA dropout. We do so by randomly replacing annotations for a random number of words with U. While useful for animate nouns, such annotations might seem otherwise redundant because the majority of nouns in training data can be expected to be inanimate. However, for some inanimate nouns, the target language grammatical gender annotations can help narrowing down the lexical choice during training. An example is the translation of "injury" into Latvian, where "injury|F" would result in "trauma" while "injury|M" would correspond to "ievainojums". Besides disambiguating animate nouns, annotations also disambiguate the grammatical gender of pronouns, proper nouns. Furthermore, grammatical gender annotations also concern adjectives and verbs, which in some languages have to agree in gender with the nouns they describe. Consequently, we expect that during training the NMT model will learn to use these annotations, as they contain valuable information about words in the target sentence. At inference time, we lean heavily on the observation that there the grammatical gender of animate nouns, pronouns, and proper nouns, and the intended referential gender coincide considerably. This is, however, a heuristic and not a rule (see  Hellinger and Motschenbacher (2015)  for counterexamples). Nevertheless, we assume that it is possible to use TGA in a referential sense of gender, thus injecting the NMT model with additional information about the subject's gender. Sources of such information can vary; in this paper, we showcase how to use TGA together with off-the-shelf co-reference resolution tools. 

 Evaluation: WinoMT Test Suite To measure the extent to which gender annotations reduce NMT systems' reliance on gender stereotypes, we use the WinoMT test suite  (Stanovsky et al., 2019) . WinoMT builds on the previous work on addressing gender bias in co-reference resolution by combining Winogender  (Rudinger et al., 2018)  and WinoBias  (Zhao et al., 2018)  datasets in a test suite for automatic evaluation of gender bias in MT. All sentences in the WinoMT test set follow the Winograd Schema where anaphora resolution is required to find an antecedent for an ambiguous pronoun  (Hirst, 1981) . In the case of datasets designed for evaluation of gender bias, the ambiguous pronoun refers to one of two entities which are referred to using titles of their professions. Professions and pronouns are chosen so that they either align with or diverge from the gender stereotypes of each profession as reported by the U.S. Bureau of Labor Statistics  (Zhao et al., 2018) . WinoMT tests if the grammatical gender of the translation of an antecedent matches the gender of the pronoun in the original sentence. Testing is done by morphologically analysing the target translation and aligning it with the source sentence. The WinoMT test suite scores MT outputs using multiple metrics: Accuracy -the percentage of correctly translated antecedents, ?G -difference in F 1 score between sentences with masculine and feminine antecedents, ?S -difference in accuracy between the set of sentences that either align with or diverge from the gender stereotypes of each profession. Saunders and Byrne (2020) also propose to report M:F -ratio of translations using masculine and feminine antecedents. 

 Experimental Setting Languages and Data In all our experiments, we choose one source language without grammatical gender and five Indo-European languages in which nouns have grammatical gender (see Table  1 ). For all language pairs, we use training data from WMT news translation tasks. We do the necessary cleaning and filtering with Moses  (Koehn et al., 2007)  pre-processing tools. etary data that we obtain from Tilde Data Libarary by combining all EN-LV parallel corpora. The proprietary data are pre-processed using the Tilde MT platform  (Pinnis et al., 2018) . Table  1  summarizes training data source and size statistics prior to adding TGA. For all systems and language pairs, we use byte pair encoding (BPE)  (Gage, 1994;  to prepare joint source and target language BPE sub-word vocabularies. We use 30K BPE merge operations and use a vocabulary threshold of 50. NMT Systems We use the default configuration of the Transformer  (Vaswani et al., 2017)  NMT model implementation of the Sockeye NMT toolkit  (Hieber et al., 2020) . The exception is the use of source-side factors  with the dimensionality of 8 for systems using TGA, which changes the model's combined source embedding dimensionality from 512 to 520. We train all models using early stopping with patience of 10 based on their development set perplexity  (Prechelt, 1998) . could also render WinoMT evaluation unreliable. Thus we first benchmark several morphological taggers on grammatical gender feature classification. We use Latvian as a development language because of the availability of lexicon-based and datadriven morphological analysis tools. Specifically, we use the Universal Dependencies 1 test set to compare two data-driven tools -the Stanza toolkit  (Qi et al., 2020)  and UDPipe  (Straka and Strakov?, 2017) . Additionally, we evaluate a dictionarybased morphological analyser and statistical tagger 2 by  Paikens et al. (2013) . Table  2  gives F-1 scores on masculine and feminine feature tagging. 

 Morphological Taggers Results indicate that none of the taggers exhibits salient bias in their tagging performance. As the only non-neural system yields better F-1 scores than the other two systems, we further compare Stanza and the tagger by  Paikens et al. (2013)  in their impact on BLEU and WinoMT metrics. Results indicated that the choice of the tagger does not have a notable effect on BLEU scores. In terms of WinoMT accuracy scores, the NMT system that was trained using TGA prepared with Stanza yields an accuracy that is about 3% better than the system using the tagger by  Paikens et al. (2013) . Thus, in all remaining experiments, we use the Stanza tagger as it provides pre-trained models for a wide range of languages. 

 TGA in Training Data Preparing training data with TGA requires statistical word alignments between words of source and target language sentences and a target language morphological tagger. To obtain word alignments, we use fast align  (Dyer et al., 2013) . To obtain grammatical gender information of target language words, we use the Stanza morphological tagger. When training NMT systems with TGA, we combine two copies of the original training data: one where all source-side 1 https://github.com/ UniversalDependencies/UD_Latvian-LVTB 2 https://github.com/PeterisP/LVTagger factors are set to U and the other containing TGA. 

 TGA During Inference In training data, TGA annotate regular source language words with the grammatical gender information of corresponding target language words. We do not have access to the target language sentence during inference. Thus, we use co-reference resolution tools and extract the referential gender information from the source sentence instead. To do so, we first use co-reference resolution tools to obtain the co-reference graph. We then identify sub-graphs which contain gendered pronouns. Finally, we propagate the gender information within the graph and annotate the antecedents (see Figure  2 ). We set the annotations for the remaining unannotated words to U. We use neural co-reference resolution tools by AllenNLP 3  (Lee et al., 2017)  and Hugging Face 4 (based on work by  Clark and Manning (2016) ). We refer to these systems as TGA AllenNLP and TGA HuggingFace respectively. We also report the performance of NMT with TGA, when TGA use oracle information directly taken from WinoMT datasets and refer to these as TGA Oracle. Evaluation We evaluate general translation quality using the BLEU  (Papineni et al., 2002)  metric evaluated over WMT test sets. To calculate BLEU, we use SacreBLEU 5  (Post, 2018)  on cased, detokenized data. Reference test sets are only preprocessed using Moses punctuation normalization script 6 . We use the WinoMT test suite  (Stanovsky et al., 2019)  to measure gender bias of our NMT systems. 

 Results and Discussion Results from experiments evaluating gender bias using the WinoMT test suite are provided in Table 3. First, we observe that all baseline systems show a strong bias towards generating translations using masculine forms. The EN-RU baseline system is the most biased as it produces only one translation hypothesis with a feminine antecedent for every 8.4 hypotheses containing masculine antecedents. Meanwhile the EN-DE baseline system is the least biased with the M:F ratio being much lower -2.6 (see the last column of Table  3 ). Our baseline systems for EN-DE, EN-FR and EN-RU language pairs, however, show comparable ?G and WinoMT accuracy results to those reported by  Stanovsky et al. (2019)  for several publicly available commercial systems. These results confirm that our baselines, although being strongly biased, are not unordinary. Results from experiments using TGA with oracle gender information show an improvement in WinoMT accuracy and ?G for all language pairs (see Table  3  TGA Oracle). These results demonstrate that when training MT systems to use TGA reduces their reliance on gender stereotypes when information about the subject's gender is available, proving the usefulness of methods proposed here. Despite the availability of oracle gender information, none of the systems is entirely bias-free or obtains 100% accuracy. Thus methods proposed here could be combined with others, such as those proposed by  Saunders and Byrne (2020) , to achieve further improvements. Effect on BLEU As expected, using TGA with reference sentence grammatical gender annotations has a positive effect on BLEU, thus confirming our hypothesis why and how the NMT system learns to rely on TGA as an additional source of information during training (see Table  4 ). It is equally important, however, that, when training NMT systems to use TGA, it does not degrade their performance when gender information is not necessary or is unavailable. Thus we test our systems for such cases by setting all TGA values to U and compare them to the baseline systems (see Table  4 ). To test for statistically significant differences between the results of NMT systems we use pairwise bootstrap resampling  (Koehn, 2004)  and significance threshold of 0.05. Results indicate no statistically significant differences between systems using uninformative TGA values and their baseline counterparts with an exception of results for EN-RU systems (?0.4 BLEU), which we find to be statistically significant. Effect of Data Size To analyze gender bias and TGA performance depending on the quality and size of the training data, we use much larger EN-LV proprietary data (see Table  1 ) to train productiongrade NMT systems and contrast them with EN-LV WMT data systems (see the two EN-LV sections in Table  3 and Table 5 ). First of all, we notice that although the large data baseline has higher WinoMT accuracy than the WMT data system, it has a similar ?G. Decomposing ?G as male and female grammatical gender F-1 scores (Table  5 ), however, clarifies that, although similarly skewed, the large data baseline has higher F-1 scores than the WMT data baseline. Next, we note, that larger training data size has a positive effect on the system's ability to use TGA more effectively as the large data system using TGA has a greater improvement on the two metrics measuring bias -?G and M:F 7 than its WMT data counterpart relative to its baseline. These findings suggest that TGA is a method that is applicable not only in small data settings but also in large data settings, such as commercial systems, for which it is even more effective. Plugging-in Co-reference Resolution Tools Finally, we experiment with TGA using gender information provided by two off-the-shelf co-reference resolution tools, AllenNLP and Hugging Face. Re- sults show that using TGA with either of the tools outperforms baseline systems for all languages pairs. Furthermore, TGA with gender information provided by AllenNLP shows only a 4.5 to 7.1% drop in WinoMT accuracy compared to results when using TGA with oracle information. To put this in perspective, Saunders and Byrne (2020) required a handcrafted gender-balanced profession set and additional rescoring models, for their EN-DE system to obtain comparable WinoMT accuracy and ?G without loss of translation quality. In contrast, the methods proposed here require tools that are readily available, making them easily applicable in practice. 

 Conclusions We proposed a method for training MT systems to use word-level annotations containing information about the subject's gender. To prepare training data, the method requires a morphological tagger to annotate regular source language words with grammatical gender information of the corresponding target language words. During inference, annotations can be used to provide information about subjects' referential or social gender obtained by analyzing text beyond sentence boundaries or externally. In experiments with five language pairs, we showed that using such gender annotations reduces NMT systems' reliance on gender stereotypes in principle. We then further showed one way for how these findings can be used in practice by using off-the-shelf co-reference resolution tools. The method proposed here decouples the task of acquiring the necessary gender information from the task of learning to translate correctly when such information is available. Thus system's ability to use such information can be achieved independently from its availability at training time. This allows for application-specific sources of gender information. Examples are the translation of chat or social media content, where users may choose to indicate their gender or translation of whole documents, where gender information may be obtained using annotations and anaphora resolution. Thus, we believe that the methods proposed here, will provide means to limit the propagation of gender stereotypes by NMT systems when translating into languages with grammatical gender. The source code to reproduce our results for the publicly available data sets is published on GitHub 8 . Figure 1 : 1 Figure 1: Illustration of target to source projections of grammatical gender annotations. Sample sentences taken from the English-Latvian development set of the WMT2017 News Translation Task. 

 Figure 2 : 2 Figure 2: WinoMT test suite translation process with TGA distilled from the output of automatic coreference resolution tool. 

 Table 1 : 1 To see how TGA is affected by data size, we also use much larger EN-LV propri-Training data set source and size in millions of sentences prior to adding TGA. Source # Sent. News Test EN-DE WMT19 64.1M 2018 EN-FR WMT15 39.1M 2015 EN-LV Tilde 22.7M 2017 EN-LV WMT17 4.5M 2017 EN-LT WMT19 3.6M 2019 EN-RU WMT17 25.0M 2015 

 Table 3 : 3 Results on WinoMT test suite. WMT Data Systems Acc. ? G ? S M:F EN-DE Baseline TGA Oracle TGA HuggingFace 77.6 66.7 89.0 TGA AllenNLP 81.5 10.2 -4.7 -0.1 -2.0 14.4 1.7 11.9 11.1 2.6 1 1.6 1.4 EN-FR Baseline TGA Oracle TGA HuggingFace 67.8 48.6 81.5 TGA AllenNLP 74.4 29.8 1.4 4.9 1.6 11.8 2.8 12.4 10.1 5.5 1.2 2 1.6 EN-LV Baseline TGA Oracle TGA HuggingFace 38.6 27.9 42.7 TGA AllenNLP 39.3 26.0 15.9 19.7 18.1 9.6 10.3 18.1 18.6 3.9 2.9 3.0 2.8 EN-LT Baseline TGA Oracle TGA HuggingFace 43.4 38.0 52.8 TGA AllenNLP 47.2 32.6 15.2 22.5 17.7 6.5 4.0 7.6 5.1 5.9 2.7 3.9 3.1 EN-RU Baseline TGA Oracle TGA HuggingFace 45.4 32.3 55.9 TGA AllenNLP 51.4 37.7 10.6 24.8 17.0 14.1 14.0 13.7 15.2 8.4 2.5 4.4 3.2 Proprietary Large Data System Acc. ? G ? S M:F EN-LV Baseline TGA Oracle TGA HuggingFace 46.2 42.0 55.1 TGA AllenNLP 49.9 27.9 4.8 13.5 10.8 16.6 18.2 24.1 23.1 4.9 1.7 2.6 2.3 

 Table 4 : 4 Comparison of test set performance measured in BLEU for Baseline systems and systems trained using TGA. TGA: performance when using reference sentence grammatical gender annotations. All TGA=U: performance when all annotations set to be unknown. Basline TGA All TGA= U EN-DE 45.4 49.5 45.3 EN-FR 36.6 40.9 36.4 EN-LV 16.6 18.9 17.0 EN-LT 14.8 16.6 14.7 EN-RU 27.1 31.6 26.7 Male Female F-1 P R F-1 P R WMT Data System Baseline 47.2 48.3 46.2 21.2 53.9 13.2 TGA Oracle 58.5 56.0 61.3 42.5 74.7 29.7 Proprietary Large Data System Baseline 58.8 50.8 69.7 30.9 70.3 19.8 TGA Oracle 66.9 65.8 68.0 62.1 83.3 49.5 

 Table 5 : 5 Results of antecedent translation. Reporting grammatical gender F-1 score, precision (P) and recall (R) for EN-LV systems trained on WMT and proprietary large data. 

			 https://github.com/allenai/allennlp 4 https://github.com/huggingface/ neuralcoref 5 SacreBLEU hash: BLEU+case.mixed+numrefs. 1+smooth.exp+tok.13a+version.1.3.6 6 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ tokenizer/normalize-punctuation.perl 

			 ?S results are not reliable or comparable when M:F ratios are large or differ by a large value. See result section of Saunders and Byrne (2020)  for more discussion. 

			 https://github.com/artursstaf/ mitigating-gender-bias-wmt-2020
