title
Towards Compact and Fast Neural Machine Translation Using a Combined Method

abstract
Neural Machine Translation (NMT) lays intensive burden on computation and memory cost. It is a challenge to deploy NMT models on the devices with limited computation and memory budgets. This paper presents a four stage pipeline to compress model and speed up the decoding for NMT. Our method first introduces a compact architecture based on convolutional encoder and weight shared embeddings. Then weight pruning is applied to obtain a sparse model. Next, we propose a fast sequence interpolation approach which enables the greedy decoding to achieve performance on par with the beam search. Hence, the time-consuming beam search can be replaced by simple greedy decoding. Finally, vocabulary selection is used to reduce the computation of softmax layer. Our final model achieves 10? speedup, 17? parameters reduction, <35MB storage size and comparable performance compared to the baseline model.

Introduction Neural Machine Translation (NMT)  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015)  has recently gained popularity in solving the machine translation problem. Although NMT has achieved state-of-the-art performance for several language pairs  Wu et al., 2016) , like many other deep learning domains, it is both computationally intensive and memory intensive. This leads to a challenge of deploying NMT models on the devices with limited computation and memory budgets. Numerous approaches have been proposed for compression and inference speedup of neural networks, including but not limited to low-rank approximation  (Denton et al., 2014) , hash function , knowledge distillation  (Hinton et al., 2015) , quantization  (Courbariaux et al., 2015; Han et al., 2016; Zhou et al., 2017)  and sparsification  (Han et al., 2015; Wen et al., 2016) . Weight pruning and knowledge distillation have been proved to be able to compress NMT models  (See et al., 2016; Kim and Rush, 2016; . The above methods reduce the parameters from a global perspective. However, embeddings dominate the parameters in a relatively compact NMT model even if subword  (Sennrich et al., 2016 ) (typical about 30K) is used. Character-aware methods  (Ling et al., 2015; Lee et al., 2016)  have fewer embeddings while suffer from slower decoding speed  (Wu et al., 2016) . Recent work by  has shown that weight sharing can be adopted to compress embeddings in language model. We are interested in applying embeddings weight sharing to NMT. As for decoding speedup,  Gehring et al. (2016) ;  Kalchbrenner et al. (2016)  tried to improve the parallelism in NMT by substituting CNNs for RNNs .  Kim and Rush (2016)  proposed sequencelevel knowledge distillation which allows us to replace beam search with greedy decoding.  Gu et al. (2017)  exploited trainable greedy decoding by the actor-critic algorithm  (Konda and Tsitsiklis, 2002) .  Wu et al. (2016)  evaluated the quantized inference of NMT. Vocabulary selection  Mi et al., 2016; L'Hostis et al., 2016)  was commonly used to speed up the softmax layer. Search pruning was also applied to speed up beam search  (Hu et al., 2015; Wu et al., 2016; . Compared to search pruning, the speedup of greedy decoding is more attractive. Knowledge distillation improves the per- formance of greedy decoding while the method needs to run beam search over the training set, and therefore results in inefficiency for tens of millions of corpus. The trainable greedy decoding using a relatively sophisticated training procedure. We prefer a simple and fast approach that allows us to replace beam search with the greedy search. In this work, a novel approach is proposed to improve the performance of greedy decoding directly and the embeddings weight sharing is introduced into NMT. We investigate the model compression and decoding speedup for NMT from the views of network architecture, sparsification, computation and search strategy, and test the performance of their combination. Specifically, we present a four stage pipeline for model compression and decoding speedup. Firstly, we train a compact NMT model based on convolutional encoder and weight sharing. The convolutional encoder works well with smaller model size and is robust for pruning. Weight sharing further reduces the number of embeddings by several folds. Then weight pruning is applied to get a sparse model. Next, we propose fast sequence interpolation to improve the performance of greedy decoding directly. This approach uses batched greedy decoding to obtain samples and therefore is more efficient than  Kim and Rush (2016) . Finally, we use vocabulary selection to reduce the computation of the softmax layer. Our final model achieves 10? speedup, 17? parameters reduction, <35MB storage size and comparable performance compared to the baseline model. 

 Method 2.1 Compact Network Architecture Our network architecture is illustrated in Figure  1 . This architecture works well with fewer parameters, which allows us to match the performance of the baseline model at lower capacity. The convolutional encoder is similar to  Gehring et al. (2016) , which consists of two convolutional neural networks: CNN-a for attention score computation and CNN-c for the conditional input to be fed to the decoder. The CNNs are constructed by blocks with residual connections  (He et al., 2015) . We use the relu6 1 non-linear activation function instead of tanh in  Gehring et al. (2016)  and achieve better training stability. To compress the embeddings, the cluster based 2-component word representation is introduced: we cluster the words into C classes by word2vector 2  (Mikolov et al., 2013) , and each class contains up to L words. Then the conventional embedding lookup table is replaced by C + L unique vectors. For each word, we first do a lookup from C class embeddings according to which cluster the word belongs, next we do another lookup from L location embeddings according to the location of the word. We concatenate the results of the two embedding lookup as the 2-component word representation. As a result, the number of embeddings is reduced from about C ?L to C +L. Referring to  Gehring et al. (2016) , position embeddings are concatenated to convey the absolute positional information of each source word within a sentence. 

 Reference (Y): Sample (S): Interpolated Sample: replace the official said the grenade explosion did not cause any casualties or damage . the official said the grenade blast did not cause any casualties or damage . the official said the grenade blast did not cause any death or injury nor any damage .  

 Boundary words 

 Weight Pruning Then the iterative pruning  (Han et al., 2015)  is applied to obtain a sparse network, which allows us to use sparse matrix storage. In order to further reduce the storage size, most sparse matrix index of our pruned model is stored using uint8 and uint16 depend on the matrix dimension. 

 Fast Sequence Interpolation Let (X, Y ) be a source-target sequence pair. Given X as input, S is the corresponding greedy decoding result using a well trained model. Then we make two assumptions: (1) Let S be a sequence close to S. If training with (X, S), S will replace S to become the result of greedy decoding with a probability P ( S, S). (2) The following relationship holds: P ( S, S) ? sim( S, S) sim( S, S) > sim(Y, S) where sim is a function measuring closeness such as edit-distance. If S has higher evaluation metric 3 (we write as E) than S, according to (2) we have: P ( S, S) > P (Y, S) E( S, Y ) > E(S, Y ) We note that using S as a label is more attractive than Y for improving the performance of greedy decoding. The reason is that S and Y are often quite different  (Kim and Rush, 2016) , resulting in a relatively low P (Y, S). We bridge the gap between S and Y by interpolating inner sequence between them. Specifically, we edit S toward Y , which can be seen as interpolation. Editing is a heuristic operation as illustrated in Figure  2 . Concretely, let S s be a subsequence of S and let Y s be Algorithm 1 Editing algorithm of fast sequence interpolation. if (s i == y j ) then 3: Input: (X, Y, S, k): (X, for 1 ? p ? k + 1, 1 ? q ? k + 1 do 4: if (s i+p == y j+q ) then Replace subsequence of S: S s ? Y s 12: Break 13: end for 14: S ? S a subsequence of Y . Given that: ? ? ? ? ? ? ? S = (s 0 , ..., s p , S s , s q , ..., s n ) Y = (y 0 , ..., s p , Y s , s q , ..., y m ) length(S s ) k length(Y s ) k where k is the length limit of S s and Y s . The interpolated sample S has the following form: S = (s 0 , ..., s p , Y s , s q , ..., s n ) To obtain the target sequence ? for training, we substitute S for Y according to the following rule: ? = S E( S, Y ) ? E(S, Y ) > ? Y otherwise where ? aims to ensure the quality of S. We define substitution rate as the ratio of S substituting Y over the training set. In summary, the following procedure is done iteratively: (1) get a new batch of (X, Y ), (2) run batched greedy decoding on X, (3) edit S to obtain S, (4) get ? according to the substitution rule, (5) train on the batched (X, ? ). 

 Vocabulary Selection We use word alignment 4  (Dyer et al., 2013)  to build a candidate dictionary. For each source word, we build a list of candidate target words. When decoding, top n candidates of each word are merged to form a short-list for softmax layer. We do not apply vocabulary selection in training. 3 Experiments For the two translation task, top 50K and 30k most frequent words are kept respectively. The rest words are replaced with UNK. We only use sentences of length up to 50 symbols. We do not use any UNK handling methods for fair comparison. The evaluation metric is case-insensitive BLEU  (Papineni et al., 2002)  as calculated by the multi-bleu.perl script. Hyper-parameters: For the baseline model, we use a 2-layer bidirectional GRU encoder (1 layer in each direction) and a 1-layer GRU decoder. In Baseline L , the embedding size is 512 and the hidden size is 1024. In Baseline S , the embedding size is 256 and the hidden size is 512. Our baseline models are similar to the architecture in DL4MT 6 . For the convolutional encoder model, 512 hidden units are used for the 6-layer CNN-a, and 256 hidden units are used for the 8-layer CNN-c. The embedding size is 256. The hidden size of the de- coder is 512. The kernel width in CNNs is 3. The number of clusters for both source and target vocabulary is 6. The editing rule for fast sequence interpolation is detailed in Algorithm 1. We use the top 50 candidates for each source word in vocabulary selection. The initial dropout rate is 0.3, and gradually decreases to 0 as the pruning rate increases. We use AdaDelta optimizer and clip the norm of the gradient to be no more than 1. Our methods are implemented with TensorFlow 7  (Abadi et al., 2015) . We run one sentence decoding for all models under the same computing environment 8 . 

 Results and Discussions Our experimental results are summarized in Table  1 .   layer and the performance with increasing pruning rate are detailed in Figure  4 . The compact architecture reduces the decoding time by only 20%. The reason is that the decoding time is dominated by the softmax layer. After applying fast sequence interpolation, we replace beam search with greedy decoding, which results in a speedup of over 5? with little loss in performance. We find that the details of the editing rules have little effect on FSI. Because we only accept S that BLEU improved by more than the threshold ?, otherwise we choose the gold target sequence. Figure  3  shows that appropriate substitution rate is important for fast sequence interpolation. We conjecture that edited samples are still worse than gold target sequences, and therefore relatively high substitution rate may   

 Conclusion and Future Work We investigate the model compression and decoding speedup for NMT from the views of network architecture, sparsification, computation and search strategy, and verify the performance on their combination. A novel approach is proposed to improve the performance of greedy decoding and the embeddings weight sharing is introduced into NMT. In the future, we plan to integrate weight quantization into our method. Figure 1 : 1 Figure 1: Our network architecture. The 2-component word representation contains class embeddings and location embeddings. Position embeddings are concatenated to convey the absolute positional information of each source word. 
