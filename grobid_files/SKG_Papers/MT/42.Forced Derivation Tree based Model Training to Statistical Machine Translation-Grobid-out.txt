title
Forced Derivation Tree based Model Training to Statistical Machine Translation

abstract
A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems. We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities. As the first step in this line of research, we verify the effectiveness of our approach in a BTGbased phrasal system, and propose four FDTbased component models. Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.

Introduction Most of today's SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models. However, such annotations do have their drawbacks in training. On one hand, word links predicted by automatic aligners such as GIZA++  (Och and Ney, 2004 ) often contain errors. This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German. The descent of the word alignment quality will lead to inaccurate component models straightforwardly. On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models  (Tillman, 2004; Xiong et al., 2006; Galley and Manning, 2008)  and sequence models  (Banchs et al., 2005; Quirk and Menezes, 2006; Vaswani et al., 2011) . Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures. But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation. This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs. An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e. The principle advantage of this work is two-fold. First, using alignments induced from the 1-best FDTs of all sentence pairs, the overall alignment quality of training corpus can be improved. Second, comparing to word alignments, FDTs can provide richer structured knowledge for various component models to extract training instances. Our FDT-based model training approach performs via three steps: (1) generation, where an FDT space composed of different FDTs is generated for each sentence pair in training corpus by the forced decoding technique; (2) inference, where the optimal FDTs are extracted from the FDT space of each sentence pair based on both derivation and alignment qualities measured by a memory-based re-ranking model; (3) training, where various component models are trained based on the optimal FDTs extracted in the inference step. Our FDT-based model training approach can be adapted to SMT systems with arbitrary paradigms. As the first step in this line of research, our approach is verified in a phrase-based SMT system on both English-to-Japanese and Chinese-to-English translation tasks . Significant improvements are reported on both translation quality (up to 1.31 BLEU) and word alignment quality (up to 3.15 F-score). 

 Forced Derivation Tree for SMT A forced derivation tree (FDT) of a sentence pair {f, e} can be defined as a pair G =< D, A >: ? D denotes a derivation that can translate f into e accurately, using a set of translation rules. ? A denotes a set of word links (i, j) indicating that e i ? e aligns to f j ? f . In this section, we first describe how to generate FDTs for each sentence pair in training corpus, which is denoted as the generation step, and then present how to select the optimal FDT for each sentence pair, which is denoted as the inference step. We leave a real application of FDTs to the model training in a phrase-based SMT system in Section 3. 

 Generation We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps: In step 3: (1) all partial hypotheses that do not match any sequence in e will be discarded; (2) derivations covering identical source and target words but with different alignments will be kept as different partial candidates, as they can produce different FDTs for the same sentence pair. For each {f, e}, the probability of each G ? H(f, e) is computed as: p(G|H(f, e)) = exp{?(G)} ? G ? ?H(f,e) exp{?(G ? )} (1) where ?(G) is the FD model score assigned to G. For each sentence pair, different alignment candidates can be induced from its different forced derivation trees generated in the generation step, because FD can use phrase pairs with different internal word links extracted from other sentence pairs to reconstruct the given sentence pair, which could lead to better word alignment candidates. 

 Inference Given an FDT space H(f, e), we propose a memorybased re-ranking model (MRM), which selects the best FDT ? as follows: ? = argmax G?H(f,e) exp{ ? i ? i h i (G)} ? G ? ?H(f,e) exp{ ? i ? i h i (G ? )} = argmax G?H(f,e) ? i ? i h i (G) (2) where h i (G) is feature function and ? i is its feature weight. Here, memory means the whole translation history that happened in the generation step will be used as the evidence to help us compute features. From the definition we can see that the quality of an FDT directly relates to two aspects: its derivation D and alignments A. So two kinds of features are used to measure the overall quality of each FDT. (I) The features in the first category measure the derivation quality of each FDT, including: ? h(?| f ), source-to-target translation probability of a translation rule r = { f , ?}. h(?| f ) = ? {f,e}?C frac H(f,e) ( f , ?) ? {f,e}?C ? ? frac H(f,e) ( f , ? ) (3) frac H(f,e) ( f , ?) denotes the fractional count of r used in generating H(f, e): frac H(f,e) ( f , ?) = ? G?H(f,e) 1 r (G)p(G|H(f, e)) 1 r (G) is an indicator function that equals 1 when r is used in G and 0 otherwise. In practice, we use p H(f,e) (r) of r to approximate frac H(f,e) ( f , ?) when the size of H(f, e) is too large to enumerate all FDTs: p H(f,e) (r) = ?(r)O(head(r)) ? v?tail(r) I(v) Z(f ) where ?(r) is the weight of translation rule r in the FDT space H(f, e), Z is a normalization factor that equals to the inside probability of the root node in H(f, e), I(v) and O(v) are the standard inside and outside probabilities of a node v in H(f, e), head(r) and tail(r) are the head node and a set of tail nodes of a translation rule r in H(f, e) respectively. ? h( f |?), target-to-source translation probability of a translation rule r = { f , ?}. h( f |?) = ? {f,e}?C frac H(f,e) ( f , ?) ? {f,e}?C ? f ? frac H(f,e) ( f ? , ?) (4) ? h # (r), smoothed usage count for translation rule r = { f , ?} in the whole generation step. h # (r) = 1 1 + e {? ? {f,e}?C frac H(f,e) ( f ,?)} (5) In this paper, the sigmoid function is used to make sure that the feature values of different translation rules are in a proper value range. ? h r (G), number of translation rules used in G. ? h d (G), structure-based score of G. For FDTs generated by phrase-based paradigms, it can be computed by distortion models; while for FDTs generated by syntax-based paradigms, it can be computed by either parsing models or syntactic LMs  (Charniak et al., 2003) . The overfitting issue in the generation step can be alleviated by leveraging memory-based features in the inference step. h # (r) is used to penalize those long translation rules which tend to occur in only a few training sentences and are used few times in FD, h r (G) adjust our MRM to prefer FDTs consisting of more translation rules, h d (G) is used to select FDTs with better parse tree-like structures, which can be induced from their derivations directly. (II) The features in the second category measure the alignment quality of each FDT, including: ? word pair translation probabilities trained from IBM models  (Brown et al., 1993) ; ? log-likelihood ratio (Moore, 2005); ? conditional link probability (Moore, 2005); ? count of unlinked words; ? counts of inversion and concatenation. Many alignment-inspired features can be used in MRM. This paper only uses those commonly-used ones that have already been proved useful in many previous work  (Moore, 2005; Moore et al., 2006; Fraser and Marcu, 2006; Liu et al., 2010) . Following the common practice in SMT research, the MERT algorithm  (Och, 2003)  is used to tune feature weights in MRM. Due to the fact that all FDTs of each sentence pair share identical translation, we cannot use BLEU as the error criterion any more. Instead, alignment F-score is used as the alternative. We will show in Section 5 that after the inference step, alignment quality can be improved by replacing original alignments of each sentence pair with alignments induced from its 1-best FDT. Future work could experiment with other error criterions, such as reordering-based loss functions  (Birch et al., 2010; Talbot et al., 2011; Birch and Osborne, 2011)  or span F1 (DeNero and Uszkoreit, 2011). 

 Training in Phrase-based SMT As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system  (Xiong et al., 2006) , which employs Bracketing Transduction Grammar (BTG)  (Wu, 1997)  to parse parallel sentences. The reason of choosing this system is due to the prominent advantages of BTG, such as the simplicity of the grammar and the good coverage of syntactic diversities between different language pairs. We first describe more details of FDTs under BTG. Then, four FDT-based component models are presented. 

 BTG-based FDT Given a sentence pair f = {f 0 , ..., f J } and e = {e 0 , ..., e I } in training corpus, its FDT G generated based on BTG is a binary tree, which is presented by a set of terminal translation states T and a set of non-terminal translation states N , where:  ? each terminal translation state 1: S = { f[i,j) , ?[i ? ,j ? ) , ?, m, m ? , R} S ? T is a 3- tuple { f[i,j) , ?[i ? ,j ? ) , ?}, in which f[i,j) denotes the word sequence that covers the source span [i, j) of f , ?[i ? ,j ? ) denotes the target translation of f[i,j) , which is the word sequence that covers the target span [i ? , j ? ) of e at the same time, ? is a set of word links that aligns f[i,j) and ?[i ? ,j ? ) . ? each non-terminal translation state S ? N is a 5-tuple { f[i,j) , ?[i ? ,j ? ) , ?, m, m ? , R} 1 . The first 3 elements have the same meanings as in T , while m and m ? denote two split points that divide S into two child translation states, S l and S r , R denotes a BTG rule, which is either a [?] operation or a ? operation 2 . The relationship between S l , S r and S is illustrated in Figure  1 . All terminal translation states of the sentence pair {f, e} are disjoint but cover f [0,J+1) and e [0,I+1) at the same time, where J = |f | and I = |e|, and all non-terminal translation states correspond to the partial decoding states generated during decoding. 

 FDT-based Translation Model First, an FDT-based translation model (FDT-TM) is presented for our BTG-based system. Given sentence pairs in training corpus with their corresponding FDT spaces, we train FDT-TM in two different ways: (1) The first only uses the 1-best FDT of each sentence pair. Based on each alignment A induced from each 1-best FDT G, all possible bilingual phrases are extracted. Then, the maximum likelihood estimation (MLE) is used to compute probabilities and generate an FDT-TM. (2) The second uses the n-best FDTs of each sentence pair, which is motivated by several studies  (Venugopal et al., 2008; Liu et al., 2009) . For each sentence pair {f, e}, we first induce n alignments {A 1 , ..., A n } from the top n FDTs ? = {G 1 , ..., G n } ? H(f, e). Each A k is annotated with the posterior probability of its corresponding FDT G k as follows: p(A k |G k ) = exp{ ? i ? i h i (G k )} ? G k ? ? exp{ ? i ? i h i (G k ? )} (6) where ? i ? i h i (G k ) is the model score assigned to G k by MRM. Then, all possible bilingual phrases are extracted from the expanded training corpus built using n-best alignments for each sentence pair. The count of each phrase pair is now computed as the sum of posterior probabilities, instead of the sum of absolute frequencies. Last, MLE is used to compute probabilities and generate an FDT-TM. 

 FDT-based Distortion Model In Xiong's BTG system, training instances of the distortion model (DM) are pruned based on heuristic rules, aiming to keep the training size acceptable. But this will cause the examples remained cannot cover all reordering cases that could be met in real decoding procedures. To overcome this drawback, we propose an FDT-based DM (FDT-DM). Given the 1-best FDT G of a sentence pair {f, e}, all non-terminal translation states {S 1 , ..., S K } are first extracted. For each S k , we split it into two child translation states S kl and S kr . A training instance can be then obtained, using the BTG operation R ? S k as its class label and boundary words of two translation blocks ( fS kl , ?S kl ) and ( fS kr , ?S kr ) contained in S kl and S kr as its features. Last, the FDT-DM is trained based on all training instances by a MaxEnt toolkit, which can cover both local and global reordering situations due to its training instance extraction mechanism. Figure  2  shows an example of extracting training instances from an FDT.  

 FDT-based Source Language Model We next propose an FDT-based source language model (FDT-SLM). Given the 1-best FDT G of a sentence pair {f, e}, we first extract a reordered source word sequence f ? = {f ? 0 , ..., f ? J } from G, based on the order of terminal translation states in G which covers the target translation e from left to right. This procedure can be illustrated by Algorithm 1. Then, all reordered source sentences of training corpus are used to train a source LM. During decoding, each time when a new hypothesis is generated, we obtain its reordered source word sequence as well, compute a LM score based on FDT-SLM and use it as a new feature: h SLM (f ? ) = J ? k=1 p(f ? k |f ? k?n+1 , ..., f ? k?1 ) (7) 

 FDT-based Rule Sequence Model The last contribution in this section is an FDT-based rule sequence model (FDT-RSM). Given the 1-best FDT G of a sentence pair {f, e}, we first extract a sequence of translation rule applications {r 1 , ..., r K } based on Algorithm 2, where  h RSM (f, e) = K ? k=1 p(r k |r k?n+1 , ..., r k?1 ) (8) Algorithm 2: Sequence Extraction in FDT-RSM 1 let r ? = ?; 2 let S = {S 1 ? , ..., S K ? } represents an ordered sequence of terminal translation states whose target phrases cover e from left to right orderly; 3 foreach S ? S in the left-to-right order do 4 extract a phrase pair ( f[i,j) , ?[i ? ,j ? ) ) from S; 5 add r k = ( f[i,j) , ?[i ? ,j ? ) ) to r ? ; 6 end 7 return r ? as a rule sequence. The main difference between FDT-SLM and FDT-RSM is that the former is trained based on monolingual n-grams; while the latter is trained based on bilingual phrases. Although these two models are trained and computed in an LM style, they are used as reordering features, because they help SMT decoder find better decoding sequences. Of course, the usage of FDTs need not be limited to the BTG-based system, and we consider using FDTs generated by SCFG-based systems or traditional left-to-right phrase-based systems in future.   2010 ) used forced alignment to re-estimate translation probabilities using a leaving-one-out strategy. We consider the usage of FD in Section 2.1 to be a direct extension of these approaches, but one that generates FDTs for parallel data rather than focusing on phrase segmentation or probability estimation. 

 Pre-reordering Pre-reordering (PRO) techniques  (Collins et al., 2005; Xu et al., 2009; Genzel et al., 2010; Lee et al., 2010)  used features from syntactic parse trees to reorder source sentences at training and translation time. A parser is often indispensable to provide syntactic information for such methods. Recently, DeNero and Uszkoreit (  2011 ) proposed an approach that induced parse trees automatically from wordaligned training corpus to perform PRO for a phrasebased SMT system, instead of relying on treebanks. First, binary parse trees are induced from wordaligned training corpus. Based on them, a monolingual parsing model and a tree reordering model are trained to pre-reorder source words into the targetlanguage-like order. Their work is distinct from ours because it focused on inducing sentence structures for the PRO task, but mirrors ours in demonstrating that there is a potential role for structure-based training corpus in SMT model training. 

 Distortion Models Lexicalized distortion models  (Tillman, 2004; Zens and Ney, 2006; Xiong et al., 2006; Galley and Manning, 2008; )   proposed a rule Markov model to capture dependencies between minimal rules for a top-down tree-tostring system. The key difference between FDT-RSM and previous work is that the rule sequences are extracted from FDTs, and no parser is needed. 

 Experiments 

 Data and Metric Experiments are carried out on English-to-Japanese (E-J) and Chinese-to-English (C-E) MT tasks. For E-J task, bilingual data used contains 13.3M sentence pairs after pre-processing. The Japanese side of bilingual data is used to train a 4-gram LM. The development set (dev) which contains 2,000 sentences is used to optimize the log-linear SMT model. Two test sets are used for evaluation, which contain 5,000 sentences (test-1) and 999 sentences (test-2) respectively. In all evaluation data sets, each source sentence has only one reference translation. For C-E task, bilingual data used contains 0.5M sentence pairs with high translation quality, including LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LD-C2006E34, LDC2006E85 and LDC2006E92. A 5gram LM is trained on the Xinhua portion of LDC English Gigaword Version 3.0. NIST 2004 (MT04) data set is used as dev set, and evaluation results are measured on NIST 2005 (MT05) and NIST 2008 (MT08) data sets. In all evaluation data sets, each source sentence has four reference translations. Default word alignments for both SMT tasks are performed by GIZA++ with the intersect-diag-grow refinement. Translation quality is measured in terms of case-insensitive BLEU  (Papineni et al., 2002)  and reported in percentage numbers. 

 Baseline System The phrase-based SMT system proposed by Xiong et al. (  2006 ) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. The maximum lengths for the source and target phrases are 5 and 7 on E-J task, and 3 and 5 on C-E task. The beam size is set to 20. 

 Translation Quality on E-J Task We first evaluate the effectiveness of our FDT-based model training approach on E-J translation task, and present evaluation results in Table  1 , in which BTG denotes the performance of the baseline system. FDT-TM denotes the improved system that uses FDT-TM proposed in Section 3.2 instead of original phrase table. As described in Section 3.2, we tried different sizes of n-best FDTs to induce alignments for phrase extraction and found the optimal choice is 5. Besides, in order to make full use of the training corpus, for those sentence pairs that are failed in FD, we just use their original word alignments to extract bilingual phrases. We can see from Table  1  that FDT-TM outperforms the BTG system significantly. FDT-DM denotes the improved system that uses FDT-DM proposed in Section 3. best FDTs without any restriction. This makes our new DM can cover both local and global reordering situations that might be met in decoding procedures. We can see from Table  1  that using FDT-DM, significant improvements can be achieved. FDT-SLM denotes the improved system that uses FDT-SLM proposed in Section 3.4 as an additional feature, in which the maximum n-gram order is 4. However, from Table  1  we notice that with FDT-SLM integrated, only 0.2 BLEU improvements can be obtained. We analyze decoding-logs and find that the reordered source sequences of n-best translations are very similar, which, we think, can explain why improvements of using this model are so limited. FDT-RSM denotes the improved system that uses FDT-RSM proposed in Section 3.5 as an additional feature. The maximum order of this model is 3. From Table  1  we can see that FDT-RSM outperforms BTG significantly, with up to 0.48 BLEU improvements. Comparing to FDT-SLM, FDT-RSM performs slightly better as well. We think it is due to the fact that bilingual phrases can provide more discriminative power than monolingual n-grams do. Last, all these four FDT-based models (FDT-TM, FDT-DM, FDT-SLM and FDT-RSM) are put together to form an improved system that is denoted as FDT-ALL. It can provide an averaged 1.2 BLEU improvements on these three evaluation data sets. Pre-reordering (PRO) is often used on language pairs, e.g. English and Japanese, with very different word orders. So we compare our method with PRO as well. We re-implement the PRO method proposed by Genzel (2010) and show its results in Table  1 . On dev and test-2, FDT-ALL performs comparable to PRO, with no syntactic information needed at all. 

 BLEU 

 Translation Quality on C-E Task We then evaluate the effectiveness of our FDT-based model training approach on C-E translation task, and present evaluation results in    1 , the gains coming from the first two FDT-based models become small on C-E task. This might be due to the fact that the word alignment quality in C-E task is more reliable than that in E-J task for TM and DM training. 

 Effect on Alignment Quality We compare the qualities of alignments predicted by GIZA++ and alignments induced from 1-best FDTs. For E-J task, 575 English-Japanese sentence pairs are manually annotated with word alignments. 382 sentence pairs are used as the dev set, and the other 193 sentence pairs are used as the test set. For C-E task, 491 Chinese-English sentence pairs are manually annotated with word alignments. 250 sentence pairs are used as the dev set, and the other 241 sentence pairs are used as the test set. Both Japanese and Chinese sentences are adapted to our own word segmentation standards respectively. Table  3  shows the comparison results. Comparing to C-E language pair (S-V-O), E-J language pair (S-O-V) has much lower F-scores, due to its very different word order.  From Table  3  we can see that the F-score improves on all language pairs when using alignments induced from 1-best FDTs, rather than GIZA++. 

 Effect on Classification Accuracy In the BTG system, the MaxEnt model is used as a binary classifier to predict reordering operations of neighbor translation blocks. As the baseline DM and our FDT-DM have different mechanisms on training instance extraction procedures, we compare the classification accuracies of these two DMs in Table  4  to show the effect of different training instances. The MaxEnt toolkit  (Zhang, 2004 ) is used to optimize feature weights using the l-BFGS method  (Byrd et al., 1995) . We set the iteration number to 200 and Gaussian prior to 1 for avoiding overfitting.   

 Conclusions In this paper, we have presented an FDT-based model training approach to SMT. As the first step in this research direction, we have verified our method on a phrase-based SMT system, and proposed four FDTbased component models. Experiments on both E-J and C-E tasks have demonstrated the effectiveness of our approach. Summing up, comparing to plain word alignments, FDTs provide richer structured knowledge for more accurate SMT model training. Several potential research topics can be explored in future. For example, FDTs can be used in a prereordering framework. This is feasible in the sense that FDTs can provide both tree-like structures and reordering information. We also plan to adapt our FDT-based model training approach to SCFG-based and traditional left-to-right phrase-based systems. Figure Figure 1: S = { f[i,j) , ?[i ? ,j ? ) , ?, m, m ? , R} is denoted by the dark-shaded rectangle pair. It can be split into two child translation states, S l , which is denoted by the lightshaded rectangle pair, and S r , which is denoted by the white rectangle pair. Dash lines within rectangle pairs denote their internal alignments and solid lines with rows denote BTG rules. (a) uses [?] to combine two translation states, while (b) uses ?. Both S l and S r belong to T ?N . 

 Figure 1: S = { f[i,j) , ?[i ? ,j ? ) , ?, m, m ? , R} is denoted by the dark-shaded rectangle pair. It can be split into two child translation states, S l , which is denoted by the lightshaded rectangle pair, and S r , which is denoted by the white rectangle pair. Dash lines within rectangle pairs denote their internal alignments and solid lines with rows denote BTG rules. (a) uses [?] to combine two translation states, while (b) uses ?. Both S l and S r belong to T ?N . 

 Figure 2 : 2 Figure 2: An example of extracting training instances from an FDT, where solid lines with rows denote BTG operations and dash lines denote alignments. Two instances can be extracted from this FDT, where 0 and 1 denote a [?] operation and a ? operation respectively. In DM training, the number (0 or 1) in each instance is used as a label, while boundary words are extracted from each instance's two phrase pairs and used as lexical features. 

 Algorithm 1 : 1 Sequence Extraction in FDT-SLM 1 let f ? = ?; 2 let S = {S 1 ? , ..., S K ? } represents an ordered sequence of terminal translation states whose target phrases cover e from left to right orderly; 3 foreach S ? S in the left-to-right order do 4 extract f[i,j) from S; 5 append f[i,j) to f ? ; 6 append a blank space to f ? ; 7 end 8 return f ? as a reordered source word sequence. r k = ( f[i,j) , ?[i ? ,j ? ) ) denotes the k th phrase pair. Figure 3 gives an example of extracting a rule sequence from an FDT. An FDT-RSM is trained based on all rule sequences extracted from training corpus. During decoding, each time when a new hypothesis is generated, we compute an FDT-RSM score based on its rule sequence and use it as a new feature: 

 Figure 3 : 3 Figure 3: An example of extracting a rule sequence from an FDT. In order to generate the correct target translation, the desired rule sequence should be r 2 ? r 3 ? r 1 . 

 are widely used in phrase-based SMT systems. Training instances of these models are extracted from word-aligned sentence pairs. Due to efficiency reasons, only parts of all instances are kept and used in DM training, which cannot cover all possible reordering situations that could be met in decoding. In FDT-DM, by contrast, training instances are extracted from FDTs. Such instances take both local and global reordering cases into consideration. 4.4 Sequence Models Feng et al. (2010) proposed an SLM in a phrasebased SMT system. They used it as a reordering feature in the sense that it helped the decoder to find correct decoding sequences. The difference between their model and our FDT-SLM is that, in their work, the reordered source sequences are extracted based on word alignments only; while in our FDT-SLM, such sequences are obtained based on FDTs. Quirk and Menezes (2006) proposed a Minimal Translation Unit (MTU) -based sequence model and used it in their treelet system; Vaswani et al. (2011) 

 3 instead of original distortion model. Comparing to baseline D-M which has length limitation on training instances, training examples of FDT-DM are extracted from 1- 

 Table 1 : 1 FDT-based model training on E-J task. dev test-1 test-2 BTG 20.60 20.27 13.15 FDT-TM 21.21 20.71(+0.44) 13.98(+0.83) FDT-DM 21.13 20.79(+0.52) 14.25(+1.10) FDT-SLM 20.84 20.50(+0.23) 13.36(+0.21) FDT-RSM 21.07 20.75(+0.48) 13.59(+0.44) FDT-ALL 21.83 21.34(+1.07) 14.46(+1.31) PRO 21.89 21.81 14.69 

 Table 2, from which we can see significant improvements as well. BLEU MT03 MT05 MT08 BTG 38.73 38.01 23.78 FDT-TM 39.14 38.31(+0.30) 24.30(+0.52) FDT-DM 39.27 38.56(+0.55) 24.50(+0.72) FDT-SLM 38.97 38.22(+0.21) 24.04(+0.26) FDT-RSM 39.06 38.33(+0.32) 24.13(+0.35) FDT-ALL 39.59 38.72(+0.71) 24.67(+0.89) 

 Table 2 : 2 FDT-based model training on C-E taskComparing to numbers in Table 

 Table 3 : 3 Comparison of alignment qualities predicted by GIZA++ and induced from 1-best FDTs. 

 Table 4 shows that when using training instances extracted from FDTs, classification accuracy of reorderings improves on both E-J and C-E tasks. This is because FDTs can provide more deterministic and structured knowledge for training instance extraction, which can cover both local and global reordering cases. baseline DM FDT-based DM E-J 93.67% 95.60%(+1.93%) C-E 95.85% 97.52%(+1.67%) 

 Table 4 : 4 Comparison of classification accuracies of DMs based on instances extracted by different mechanisms. 

			 We sometimes omit m, m ? and R for a simplicity reason. 2 A [?] operation combines the translations of two consecutive source spans [i, m) and [m, j) in a monotonic way; while a ? operation combines them in an inverted way.
