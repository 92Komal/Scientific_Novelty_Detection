title
Lite Training Strategies for Portuguese-English and English-Portuguese Translation

abstract
Despite the widespread adoption of deep learning for machine translation, it is still expensive to develop high-quality translation models. In this work, we investigate the use of pre-trained models, such as T5  (Raffel et al., 2019)  for Portuguese-English and English-Portuguese translation tasks using low-cost hardware. We explore the use of Portuguese and English pretrained language models and propose an adaptation of the English tokenizer to represent Portuguese characters, such as diaeresis, acute and grave accents. We compare our models to the Google Translate API and MarianMT on a subset of the ParaCrawl dataset, as well as to the winning submission to the WMT19 Biomedical Translation Shared Task. We also describe our submission to the WMT20 Biomedical Translation Shared Task. Our results show that our models have a competitive performance to state-of-the-art models while being trained on modest hardware (a single 8GB gaming GPU for nine days). Our data, models and code are available at https://github.com/ unicamp-dl/Lite-T5-Translation.

Introduction With the advent of deep neural networks, results in machine translation have recently improved over classical statistical strategies  (Wu et al., 2016; Artetxe et al., 2018) . For instance, in the Third and Fourth Conference on Machine Translation (WMT18  (Edunov et al., 2018)  and WMT19  (Ng et al., 2019) ), the top-performing systems for English-German and German-English competitions were based on transformers  (Vaswani et al., 2017) . Transformer models are state-of-the-art architectures for Machine Translation (MT) tasks and are capable of translating the same word to different words based on the context. For instance, the word 'bank' in Portuguese can be translated to 'bench' or 'bank' depending on the context. This work explores translation strategies using language models pre-trained on Portuguese and English corpora. More specifically, we investigate the use of Text-to-Text Transfer Transformer (T5) pre-trained model for these tasks. T5 is a text-to-text Transformer trained with a similar masked language modeling objective as BERT. In this model, all target tasks are cast as sequenceto-sequence tasks. An illustration of T5 for the English-Portuguese translation task is shown in Figure  1 . The main contributions of this work are: ? We show that it is possible to train translation models that are competitive with the state of the art using few computational resources. We trained our models on a gaming desktop with an Nvidia RTX2070 GPU, i5 CPU, and 32GB RAM. In comparison, the winning submission of the WMT19 Biomedical Translation Shared Task used four NVIDIA V100 GPUs, each being approximately ten times more expensive than an RTX2070. ? We created and made public ParaCrawl 99k, a dataset of 99k sentence pairs extracted from ParaCrawl's v6.0 English-Portuguese parallel corpus  (Ba?n et al., 2020) . This large test corpus allows researchers to evaluate their models on a general-domain translation task. ? We evaluated Google Translate on ParaCrawl 99k, allowing other researchers to compare their results to a high-quality commercial system. ? We developed an adaptation for the English pre-trained tokenizer and achieved better results on English-Portuguese translation tasks than using the tokenizer without any changes. This allows us to efficiently adapt language models to a vocabulary that was not seen during pre-training. 

 Related Work The winning system of WMT'19 Biomedical competition for en-pt and pt-en translation tasks  (Soares and Krallinger, 2019a ) is a Neural Machine Translation (NMT) system. They used OpenNMT-py to train a transformer model on seven parallel corpora. However, differently from our models, their model was trained from scratch. Recent works  (Peters et al., 2018; Devlin et al., 2018)  have shown the advantages of using pre-trained models for tasks such as questionanswering and text classification. The intuition is to allow the network to use information from pretraining language representations to increase the performance on specific tasks.  evaluated the use of a pre-trained encoder-decoder model for translation. Both encoder and decoder weights were tied, but they were pre-trained on different languages. This is an expensive strategy for techniques that use a trainable tokenizer, such as SentencePiece  (Kudo and Richardson, 2018) , because it is necessary to re-train the entire model if the vocabulary changes, as new token embeddings need to be learned. Many commercial systems, such as Google Translate (GT) and Amazon Translate (AT), have an excellent performance on MT, but they are expensive if one needs to translate vast amounts of text. For example, we estimate that it would cost 50,000 USD to translate the 20 million sentences of ParaCrawl using GT. Unfortunately, no commercial system that we are aware of provides metric scores on public datasets that would allow us to compare their systems to ours. 

 Methods We proposed two main strategies for translating: using a T5 model pre-trained on a Portuguese corpus and adapting the original T5 tokenizer to work with Portuguese texts. 

 Pre-trained Language Model We evaluated four different scenarios: English-Portuguese translation with T5 pre-trained on a Portuguese corpus; English-Portuguese translation with T5 pre-trained on an English corpus; Portuguese-English translation with T5 pre-trained on an English corpus; Portuguese-English translation with T5 pre-trained on a Portuguese corpus. These variations allow us to evaluate how the language used during pre-training affects the translation's performance. 

 Adaptation of the English tokenizer to Portuguese Here we investigate if we can adapt to the English-Portuguese translation task a model already pretrained on languages other than Portuguese. We observe that using a non-Portuguese tokenizer can cause translation problems, since some Portuguese characters cannot be represented, such as letters with the tilde accent (e.g. '?'). To fix this issue, we propose an adaptation of the original T5 tokenizer using a pre-processing and postprocessing strategy. The tokenizer's adaptation allows it to represent all possible characters in the Portuguese language. We can divide this adaptation into two stages: Token Completion and Word Regrouping. The first stage allows the use of Portuguese special characters, such as accented vowels, whereas the second stage merges these extra tokens back to form correct words. 

 Token Completion Stage In this step, we start adding to the tokenizer Portuguese accented vowels that were not present in it. We ended up adding fourteen of those characters, as well as the word 'n?o', which is the most common word in the ParaCrawl pt-en dataset. A list of all added tokens is available in Table  1 . The addition of these tokens allowed the model to learn and generate them in en-pt translation. This is also an inexpensive method for increasing the number of words that can be represented since only the embeddings of the new tokens have to be learned from scratch. The existing token embeddings, which represent the majority of the non-Portuguese tokens, were already learned during the pre-training phase and can be reused in the fine-tuning phase. We show in Table  2  some encoding and decoding examples after adding tokens to the tokenizer.  ? ? ? ? ? ? ? ? ? ? ? ? ? ? n?o 

 Word Regrouping Stage When adding tokens directly to the tokenizer, the HuggingFace's  (Wolf et al., 2019)  SentencePiece  implementation used in our work interprets the result as a new complete token, i.e., not part of a word. For example, the word 'p?o' is broken into three different tokens 'p' '?' 'o'. In this step, we regroup the added tokens of vowels with accents separated erroneously by the tokenizer. We find in the translated text all tokens added in the Token Completion step, and merge them with their neighboring words. In Figure  2 , we illustrate our algorithm. 

 Datasets We trained our models using six different datasets, and we evaluate our system on two datasets: the WMT19 Biomedical Translation Task dataset and a subset of 99,000 sentence pairs of the ParaCrawl dataset. We also present the results of our submission to the WMT20 Biomedical Translation Task competition.  

 Training Datasets We have two different strategies for training our models depending on the test datasets. For the evaluation on the ParaCrawl dataset, we only trained the models on ParaCrawl data. ParaCrawl is a public parallel corpus of many European languages available online. Its v6.0 version contains approximately 20M English-Portuguese sentence pairs. Due to our small computational budget, we randomly selected approximately 5M pairs for training. For WMT19 and WMT20 Biomedical Translation Tasks, we train our models on the ParaCrawl dataset as well as on the following datasets, which are mostly of the same domain as WMT's Biomedical data: ? EMEA Corpus  (Tiedemann, 2012) : A parallel corpus of European Medicines Agency documents. ? CAPES Parallel Dataset  (Soares et al., 2018b) : A parallel corpus of theses and dissertations abstracts collected from the CAPES website. ? Scielo Dataset  (Soares et al., 2018a) : A parallel corpus of scientific articles collected from SciELO. ? JRC-Acquis  (Steinberger et al., 2006) : A parallel corpus of European Union (EU) documents in all official EU languages. ? Biomedical Domain Parallel Corpora  (N?v?ol et al., 2018) : A repository of the challenge that contains links to different parallel corpora. We used the Medline, Scielo, and ReBEC training datasets. Besides being of the same domain as WMT's Biomedical task, an advantage of these datasets over ParaCrawl is that they are in Brazilian Portuguese, such as most of WMT's Biomedical data. The number of sentence pairs used for training from each dataset is shown in Table  3 . 

 Corpus Sent   

 Testing Datasets We created a general-domain test set from the ParaCrawl dataset. We begin by randomly selecting 128,000 sentence pairs from its 20M pairs. ParaCrawl is originally deduplicated, but similar sentences still might exist in our split of the training and test sets. Thus, we apply as stricter deduplication process to increase the quality of our test set. To increase the speed in verifying similarity of sentence pairs, we used MinHash and Locality-Sensitive Hashing (LSH)  (Rajaraman and Ullman, 2011)  to compare sentences of training and test datasets. We set a Jaccard similarity threshold to 0.7, i.e., all sentences with similarity greater than 0.7 were discarded from the test set. LSH found 28,913 sentences in the test set with a similarity score above 0.7 of sentences in the training set. The final test set ended up having 99,087 sentence pairs, which we called ParaCrawl 99k test set. This dataset and its corresponding translations using GT are available in our GitHub repository. We also evaluated our system on the WMT19 Biomedical Shared Task test set. This is a dataset composed of approximately 500 parallel sentences of Medline abstracts. Finally, we submitted our results to the WMT20 Biomedical Shared Task competition. The WMT20 test set has 544 parallel sentences for the English-Portuguese translation task and 498 sentences for the Portuguese to English task. 

 Experiments We conducted several experiments using different configurations of T5. We divided the experiments into two groups: model hyperparameter optimization and different pre-training studies. All experiments were performed on a desktop computer with an Nvidia 8GB RTX 2070 Super, 32 Gb RAM memory, and a 4-core Intel processor running on Ubuntu 18.04. We used PyTorch  (Paszke et al., 2017) , HuggingFace Transformer, and Pytorch-Lightning (Falcon, 2019) frameworks to train and evaluate our models. 

 Model Hyperparameter Optimization We tuned the hyperparameters using the original T5 checkpoint available in the HuggingFace library. This model was pre-trained on a corpus whose majority of documents were in English with a small proportion of German, French, and Romanian documents. We first conducted a small training using 100k sentence pairs and evaluated on another 50k sentence pairs to determine some hyperparameters of the T5 model, such as batch size and the maximum length of tokens in the source and target sentences. We also evaluated the optimizer and found the best convergence with the AdamW Optimizer  (Loshchilov and Hutter, 2017) . All hyperparameters used are in Table  4 . With this configuration, we evaluated the performance of adding Portuguese-only characters to the tokenizer in comparison to using the original T5 tokenizer. The results are available in Table  5 . Our proposed tokenizer adaption resulted in an improvement of almost 5 BLEU points over the original tokenizer in the en-pt translation task. All BLEU scores reported in this paper were generated using Sacre-BLEU  (Post, 2018)  with "intl" tokenization. After finding these hyperparameters, we analyzed the trade-off between model sizes in a subset of the ParaCrawl dataset of 1M sentence pairs and evaluated them in a 150k sentence subset. We did  The performance would possibly increase if we used large models such as T5-large, T5-3B, or T5-11B. However, we could fit only the T5-base model in our 8GB GPU. We used batch accumulation to achieve batches of size 256 as the T5-small can only handle batch size 4 in 8GB. Thus, one of the contributions of this work is to show that it is possible to train translation models that are close to the state of the art on a relatively inexpensive hardware. We also conducted experiments changing SSL and TSL lengths. We found that it is not necessary to set large TSL and SSL values, e.g., 256 for both, but it is essential to choose a value that can represent the distribution of target and source sequences. In later sections, we conduct experiments with TSL and SSL 140 and 160, respectively, since 99.8 % of our training dataset is shorter than these values. All experiments in the following sections using Tokenizer's Adaptation Steps (3.2) were performed using the best pre-processing and post-processing strategies presented in Table  6 . 

 Pre-training Studies We also evaluated the effects of pre-training the model in a corpus of the same language of the target language. The intuition here is that it would be eas- ier for the model to learn the target language than having previous knowledge of the source language. Since the tokenizer mainly has tokens of one of the two languages, it is better to have a smaller quantity of tokens to learn. This is because, if the Senten-cePiece tokenizer does not have the word in its vocabulary, it will use subtokens to form the original word. For example, the sentence 'They like to drink coconut water' is represented by six tokens in English SentencePiece and thirteen tokens in Portuguese SentencePiece. We are not evaluating here the possibility to train the pre-training model from scratch with both languages together, as it is not possible with our modest hardware setup. For the Portuguese pre-trained model, we used PTT5-base model  (Carmo et al., 2020)  with Portuguese tokenizer. PTT5 was pre-trained on BrWAC, a large corpus of Brazilian Portuguese webpages. PTT5 started training using T5's official published weights as initial weights, so it also uses English learning in its model. For the English pre-trained model, we used the Huggingface implementation of T5 with its default tokenizer, which is based on SentencePiece. In Table  7 , we compare both models with Google Translate in the ParaCrawl 99k test set. Both models perform similarly in the Portuguese-English translation task, but the Portuguese pre-trained model gives a better result than the English pretrained model in the English-Portuguese translation task. We are on par with Google Translate on en-pt, but a few BLEU points below on pt-en. 

 WMT19 and WMT20 Results We now evaluate our models on the WMT19 Biomedical Translation Task and the results of our best models and official submission to the WMT20 Biomedical Translation Task. In Table  8 , we show WMT19 results of our models as well as the winning submission of WMT19 Biomedical tasks  (Soares and Krallinger, 2019b)  and the MarianMT  (Junczys-Dowmunt et al., 2018)  implementation available on the HuggingFace's Transformer Library. 1 MarianMT's translation uses a multilingual Romance model, which is only possible to set the target language. This explains Mari-anMT's low performance on Portuguese to English translation since the model has to infer that the input sentence is in Portuguese. Models pre-trained on Portuguese obtained the best performance in both translation tasks. Notably, we achieved an improvement of +6.31 BLEU points in the English to Portuguese translation task by using the Portuguese pre-trained model and +9.75 with an increase of target and source sequence lengths. We also obtained an improvement of +0.62 in the Portuguese to English translation task using the Portuguese pre-trained model and +2.27 when increasing target and source sequence lengths. We believe that the improvement of Portuguese pre-training models is associated with PTT5's training strategy that uses English pre-trained weights as initial weights. The intuition is that PTT5 carries information from the English model too. The results for WMT20's challenge are in Table 9. Our submission is 2.17 BLEU points below the winning team in Portuguese-English, but it is 4.48 BLEU points higher than the baseline. For the English-Portuguese task, our results are below the baseline. That can be attributed to not using the Portuguese pre-trained model, which was not available at the time of our submission. As noted above, we achieved a large improvement on WMT19 when we switched from the English to the Portuguese pre-trained model. Therefore, we assume that a Portuguese pre-trained model would obtain superior results to the baseline on WMT20. We also evaluated our Portuguese pre-training models with the best participants submissions of pt-en en-pt MarianMT 27.91 47.44 BSC  (Soares and Krallinger, 2019b)    the WMT20 Biomedical Shared Task. The results of this comparison are in Table  10  and are slightly different from Table  9  since we are using Sacre-BLEU as the evaluation script. Our best translation models are 1.3-1.5 BLEU points bellow the winning submission (Sheffield) in both translation directions (pt-en and en-pt).  

 Conclusions and Future Work We show that it is possible to develop English-Portuguese translation models close to the state of the art using modest hardware. Despite not reaching the same level of performance of Google Translate on pt-en, the fact that our system was devel-oped mostly by the first author on its personal computer shows that implementing high-quality machine translation systems has become possible for anyone, including small companies and research labs. We also cannot guarantee that Google did not use our testing data for training. We presented our submission strategies for the WMT20 Biomedical Translation Shared Task using a T5 model. We show that a simple adaption of the original T5 tokenizer to the Portuguese language largely improves the translation quality and does not require any further pre-training, which is expensive. However, we achieve the best results with models pre-trained on Portuguese. As directions for future work, we plan to experiment with larger models and models pre-trained on both Portuguese and English languages simultaneously, as recent work showed that this a successful strategy  (Wu et al., 2016; Arivazhagan et al., 2019) . We believe that we could improve the translation results with larger and more complex models  (Lepikhin et al., 2020) . Figure 1 : 1 Figure 1: The text-to-text framework used by T5. The purple boxes and red connections represent the task used in this work. Figure adapted from (Raffel et al., 2019). 

 Figure 2 : 2 Figure 2: An example of separated tokens merged back into a single word. Our algorithm searches for an isolated special token (in this case, '?') and merges it with its neighbors. It can be merged at the beginning, middle, or end of a sentence. 

 Table 1 : 1 List of tokens added to the T5 tokenizer by our adaption method. 

 Table 2 : 2 Comparing tokenizer results before and after adding the Portuguese tokens. 

 Table 3 : 3 Number of sentence pairs of each domainspecific dataset used to train our models for the WMT19 and WMT20 Biomedical tasks. 

 Table 4 : 4 Hyperparameters used for training the models. Hyperparameters Values Batch Size 256 Source Sequence Length (SSL) 96 Target Sequence Length (TSL) 160 Learning Rate 5 ? 10 ?3 eps 1 ? 10 ?5 Translation Type BLEU Original T5 tokenizer 31.15 + Portuguese characters 35.95 (+4.8) 

 Table 5 : 5 Effects in performance of using our adaption of the original T5 tokenizer to the English-Portuguese translation task. Numbers are from ParaCrawl's 99k en-pt test set.not use any sentence from the test set. The results of this analysis are reported in Table6. We trained the T5-small and T5-base models with different epoch sizes. Training 3 epochs of T5-small takes almost the same time as training one epoch with a T5-base model. 

 Table 6 : 6 Effects in performance of different strategies for adapting the original T5 tokenizer to Portuguese. Numbers are from our dev set of ParaCrawl. Translation Type Sacre BLEU Adding Top 25 words in Port. + T5-small + 3 epochs 43.03 Adding tokens of Table 1 in Port. + T5-small + 3 epochs 43.48 Adding tokens of Table 1 in Port. + T5-base + 1 epoch 44.52 

 Table 7 : 7 BLEU comparison between GT and our approach in Paracrawl 99k test set. pt-en en-pt Google Translate API 51.20 45.17 Ours -English pre-training 46.49 44.56 Ours -Portuguese pre-training 46.35 45.44 

 Table 9 : 9 BLEU scores on WMT20's automatic evaluation. Since the Portuguese T5 model was not available at the time of our submission, we used the original (English) T5. Hence, results for en-pt and pt-en could be improved by switching to the Portuguese pre-trained model. Team Names pt-en en-pt Sheffield 48.16 44.57 Unicamp DL 45.99 38.08 baseline 41.51 39.77 

 Table 10 : 10 BLEU scores on the test set of WMT20 Biomedical Shared Task. Team Names pt-en en-pt Unicamp DL 48.67 40.18 Sheffield 52.25 46.69 Ours -Portuguese pre-training + TSL=256 and SSL=256 ? 45.33 + TSL=140 and SSL=160 50.75 ? 

			 https://huggingface.co/transformers/ model_doc/marian.html
