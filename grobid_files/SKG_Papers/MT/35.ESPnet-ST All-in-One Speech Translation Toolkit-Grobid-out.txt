title
ESPnet-ST: All-in-One Speech Translation Toolkit

abstract
We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-toend speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pretrained models are downloadable. The toolkit is publicly available at https://github. com/espnet/espnet.

Introduction Speech translation (ST), where converting speech signals in a language to text in another language, is a key technique to break the language barrier for human communication. Traditional ST systems involve cascading automatic speech recognition (ASR), text normalization (e.g., punctuation insertion, case restoration), and machine translation (MT) modules; we call this Cascade-ST  (Ney, 1999; Casacuberta et al., 2008; Kumar et al., 2014) . Recently, sequence-to-sequence (S2S) models have become the method of choice in implementing both the ASR and MT modules (c.f.  (Chan et al., 2016; Bahdanau et al., 2015) ). This convergence of models has opened up the possibility of designing end-to-end speech translation (E2E-ST) systems, where a single S2S directly maps speech in a source language to its translation in the target language  (B?rard et al., 2016; . E2E-ST has several advantages over the cascaded approach: (1) a single E2E-ST model can reduce latency at inference time, which is useful for time-critical use cases like simultaneous interpretation. (2) A single model enables back-propagation training in an end-to-end fashion, which mitigates the risk of error propagation by cascaded modules. (3) In certain use cases such as endangered language documentation  (Bird et al., 2014) , source speech and target text translation (without the intermediate source text transcript) might be easier to obtain, necessitating the adoption of E2E-ST models  (Anastasopoulos and Chiang, 2018) . Nevertheless, the verdict is still out on the comparison of translation quality between E2E-ST and Cascade-ST. Some empirical results favor E2E  while others favor Cascade ; the conclusion also depends on the nuances of the training data condition  (Sperber et al., 2019) . We believe the time is ripe to develop a unified toolkit that facilitates research in both E2E and cascaded approaches. We present ESPnet-ST, a toolkit that implements many of the recent models for E2E-ST, as well as the ASR and MT modules for Cascade-ST. Our goal is to provide a toolkit where researchers can easily incorporate and test new ideas under different approaches. Recent research suggests that pre-training, multi-task learning, and transfer learning are important techniques for achieving improved results for E2E-ST  (B?rard et al., 2018; Anastasopoulos and Chiang, 2018; Bansal et al., 2019; . Thus, a unified toolkit that enables researchers to seamlessly mix-and-match different ASR and MT models in training both E2E-ST and Cascade-ST systems would facilitate research in the field.  1  ESPnet-ST is especially designed to target the ST task. ESPnet was originally developed for the 1 There exist many excellent toolkits that support both ASR and MT tasks (see    ASR task  (Watanabe et al., 2018) , and recently extended to the text-to-speech (TTS) task  (Hayashi et al., 2020) . Here, we extend ESPnet to ST tasks, providing code for building translation systems and recipes (i.e., scripts that encapsulate the entire training/inference procedure for reproducibility purposes) for a wide range of ST benchmarks. This is a non-trivial extension: with a unified codebase for ASR/MT/ST and a wide range of recipes, we believe ESPnet-ST is an all-in-one toolkit that should make it easier for both ASR and MT researchers to get started in ST research. 1 ? ? ? - - - - OpenSeq2seq 2 - - - - - NeMo 3 - - - - - RETURNN 4 - - - - - - - - SLT.KIT 5 - - - - Fairseq 6 - - - - - - Tensor2Tensor 7 - - - - - - - - ? OpenNMT-{py, tf} 8 - - - - - - - - - Kaldi 9 - - - - - - - - Wav2letter++ 10 - - - - - - - - The contributions of ESPnet-ST are as follows: ? To the best of our knowledge, this is the first toolkit to include ASR, MT, TTS, and ST recipes and models in the same codebase. Since our codebase is based on the unified framework with a common stage-by-stage processing  (Povey et al., 2011) , it is very easy to customize training data and models. ? We provide recipes for ST corpora such as Fisher-CallHome  (Post et al., 2013) ,  Libri-trans (Kocabiyikoglu et al., 2018) , How2  (Sanabria et al., 2018) , and Must-C (Di  Gangi et al., 2019a)  2 . Each recipe contains a single script (run.sh), which covers all experimental processes, such as corpus preparation, data augmentations, and transfer learning. ? We provide the open-sourced toolkit and the pre-trained models whose hyper-parameters are intensively tuned. Moreover, we provide interactive demo of speech-to-speech translation hosted by Google Colab. 3 2 Design 

 Installation All required tools are automatically downloaded and built under tools (see Figure  1 ) by a make command. The tools include (1) neural network libraries such as PyTorch  (Paszke et al., 2019) , (2) ASR-related toolkits such as Kaldi  (Povey et al., 2011) , and (3) MT-related toolkits such as Moses  (Koehn et al., 2007)  and sentencepiece  (Kudo, 2018) . ESPnet-ST is implemented with Pytorch backend. 

 Recipes for reproducible experiments We provide various recipes for all tasks in order to quickly and easily reproduce the strong baseline systems with a single script. The directory structure is depicted as in Figure  1 . egs contains corpus directories, in which the corresponding task directories (e.g., st1) are included. To run experiments, we simply execute run.sh under the desired task directory. Configuration yaml files for feature extraction, data augmentation, model training, and decoding etc. are included in conf. Model directories including checkpoints are saved under exp. More details are described in Section 2.4.   !85G4# %&G&9%&9* G58$&##79+ !85G4# %&G&9%&9* G58$&##79+ !85G4# %&G&9%&9* G58$&##79+ .'# )6) .3 37) )6) 43 

 Tasks We support language modeling (LM), neural textto-speech (TTS) in addition to ASR, ST, and MT tasks. To the best of our knowledge, none of frameworks support all these tasks in a single toolkit. A comparison with other frameworks are summarized in Table  1 . Conceptually, it is possible to combine ASR and MT modules for Cascade-ST, but few frameworks provide such examples. Moreover, though some toolkits indeed support speechto-text tasks, it is not trivial to switch ASR and E2E-ST tasks since E2E-ST requires the auxiliary tasks (ASR/MT objectives) to achieve reasonable performance. 

 Stage-by-stage processing ESPnet-ST is based on a stage-by-stage processing including corpus-dependent pre-processing, feature extraction, training, and decoding stages. We follow Kaldi-style data preparation, which makes it easy to augment speech data by leveraging other data resources prepared in egs. Once run.sh is executed, the following processes are started. Stage 0: Corpus-dependent pre-processing is conducted using scripts under local and the resulting text data is automatically saved under data. Both transcriptions and the corresponding translations with three different treatments of casing and punctuation marks (hereafter, punct.) are generated after text normalization and tokenization with tokenizer.perl in Moses; (a) tc: truecased text with punct., (b) lc: lowercased text with punct., and (3) lc.rm: lowercased text without punct. except for apostrophe. lc.rm is designed for the ASR task since the conventional ASR system does not generate punctuation marks. However, it is possible to train ASR models so as to generate truecased text using tc.  Stage 6: (Cascade-ST recipe only) The system is evaluated by feeding ASR outputs to the MT model. 

 Multi-task learning and transfer learning In ST literature, it is acknowledged that the optimization of E2E-ST is more difficult than individually training ASR and MT models. Multitask training (MTL) and transfer learning from ASR and MT tasks are promising approaches for this problem  B?rard et al., 2018; Sperber et al., 2019; Bansal et al., 2019) . Thus, in Stage 4 of the E2E-ST recipe, we allow options to add auxiliary ASR and MT objectives. We also support options to initialize the parameters of the ST encoder with a pre-trained ASR encoder in asr1, and to initialize the parameters of the ST decoder with a pre-trained MT decoder in mt1. 

 Speech data augmentation We implement techniques that have shown to give improved robustness in the ASR component. 

 Speed perturbation We augmented speech data by changing the speed with factors of 0.9, 1.0, and 1.1, which results in 3-fold data augmentation. We found this is important to stabilize E2E-ST training. SpecAugment Time and frequency masking blocks are randomly applied to log mel-filterbank features. This has been originally proposed to improve the ASR performance and shown to be effective for E2E-ST as well  (Bahar et al., 2019b) . 

 Multilingual training Multilingual training, where datasets from different language pairs are combined to train a single model, is a potential way to improve performance of E2E-ST models  Di Gangi et al., 2019c) . Multilingual E2E-ST/MT models are supported in several recipes. 

 Additional features Experiment manager We customize the data loader, trainer, and evaluator by overriding Chainer  (Tokui et al., 2019)  modules. The common processes are shared among all tasks. Large-scale training/decoding We support job schedulers (e.g., SLURM, Grid Engine), multiple GPUs and half/mixed-precision training/decoding with apex  (Micikevicius et al., 2018) .  5  Our beam search implementation vectorizes hypotheses for faster decoding  (Seki et al., 2019) . Performance monitoring Attention weights and all kinds of training/validation scores and losses for ASR, MT, and ST tasks can be collectively monitored through TensorBoard. Ensemble decoding Averaging posterior probabilities from multiple models during beam search decoding is supported. 

 Example Models To give a flavor of the models that are supported with ESPnet-ST, we describe in detail the construction of an example E2E-ST model, which is used later in the Experiments section. Note that there are many customizable options not mentioned here. Automatic speech recognition (ASR) We build ASR components with the Transformer-based hybrid CTC/attention framework  (Watanabe et al., 2017) , which has been shown to be more effective than RNN-based models on various speech corpora  (Karita et al., 2019) . Decoding with the external LSTM-based LM trained in the Stage 3 is also conducted  (Kannan et al., 2017) . The transformer uses 12 self-attention blocks stacked on the two VGG blocks in the speech encoder and 6 self-attention blocks in the transcription decoder; see  (Karita et al., 2019)  for implementation details. 

 Machine translation (MT) The MT model consists of the source text encoder and translation decoder, implemented as a transformer with 6 selfattention blocks. For simplicity, we train the MT model by feeding lowercased source sentences without punctuation marks (lc.rm)  (Peitz et al., 2011) . There are options to explore characters and different subword units in the MT component. End-to-end speech translation (E2E-ST) Our E2E-ST model is composed of the speech encoder and translation decoder. Since the definition of parameter names is exactly same as in the ASR and MT components, it is quite easy to copy parameters from the pre-trained models for transfer learning. After ASR and MT models are trained as described above, their parameters are extracted and used to initialize the E2E-ST model. The model is then trained on ST data, with the option of incorporating multi-task objectives as well. Text-to-speech (TTS) We also support end-toend text-to-speech (E2E-TTS), which can be applied after ST outputs a translation. The E2E-TTS model consists of the feature generation network converting an input text to acoustic features (e.g., log-mel filterbank coefficients) and the vocoder network converting the features to a waveform. Tacotron 2  (Shen et al., 2018) , Transformer-TTS , FastSpeech  (Ren et al., 2019) , and their variants such as a multi-speaker model are supported as the feature generation network. WaveNet  (van den Oord et al., 2016)  and Parallel WaveGAN  are available as the vocoder network. See  Hayashi et al. (2020)  for more details. 

 Experiments In this section, we demonstrate how models from our ESPnet recipes perform on benchmark speech  translation corpora: Fisher-CallHome Spanish En?Es, Libri-trans En?Fr, How2 En?Pt, and Must-C En?8 languages. Moreover, we also performed experiments on IWSLT16 En-De to validate the performance of our MT modules. All sentences were tokenized with the tokenizer.perl script in the Moses toolkit  (Koehn et al., 2007) . We used the joint source and target vocabularies based on byte pair encoding (BPE)  (Sennrich et al., 2016)  units. ASR vocabularies were created with English sentences only with lc.rm. We report 4-gram BLEU  (Papineni et al., 2002)  scores with the multi-bleu.perl script in Moses. For speech features, we extracted 80-channel log-mel filterbank coefficients with 3-dimensional pitch features using Kaldi, resulting 83-dimensional features per frame. Detailed training and decoding configura-  

 Fisher-CallHome Spanish (Es?En) Fisher-CallHome Spanish corpus contains 170hours of Spanish conversational telephone speech, the corresponding transcription, as well as the English translations  (Post et al., 2013) . All punctuation marks except for apostrophe were removed  (Post et al., 2013; Kumar et al., 2014; . We report case-insensitive BLEU on Fisher-{dev, dev2, test} (with four references), and CallHome-{devtest, evltest} (with a single reference). We used 1k vocabulary for all tasks. Results are shown in Table  2 . It is worth noting that we did not use any additional data resource. Both MTL and transfer learning improved the performance of vanilla Transformer. Our best system with SpecAugment matches the current state-ofthe-art performance  Libri-trans corpus contains 236-hours of English read speech, the corresponding transcription, and the French translations . We used the clean 100-hours of speech data and augmented translation references with Google Translate for the training set  (B?rard et al., 2018; Bahar et al., 2019a,b) . We report case-insensitive BLEU on the test set. We used 1k vocabulary for all tasks. Results are shown in Table  3 . Note that all models used the same data resource and are competitive to previous work. 

 How2 (En? Pt) How2 corpus contains English speech extracted from YouTube videos, the corresponding transcription, as well as the Portuguese translation  (Sanabria et al., 2018) . We used the official 300-hour subset for training. Since speech features in the How2 corpus is pre-processed as 40-channel log-mel filterbank coefficients with 3-dimensional pitch features with Kaldi in advance, we used them without speed perturbation. We used 5k and 8k vocabularies for ASR and E2E-ST/MT models, respectively. We report case-sensitive BLEU on the dev5 set. Results are shown in Table  4 . Our systems significantly outperform the previous RNN-based model  (Sanabria et al., 2018) . We believe that our systems can be regarded as the reliable baselines for future research. weeks with 16 GPUs, while ESPnet-ST requires just 1-2 days with a single GPU. The fast inference of ESPnet-ST can be confirmed in our interactive demo page (RTF 0.7755). 

 Must-C (En? 8 langs) Must-C corpus contains English speech extracted from TED talks, the corresponding transcription, and the target translations in 8 language directions (De, Pt, Fr, Es, Ro, Ru, Nl, and It) (Di  Gangi et al., 2019a) . We conducted experiments in all 8 directions. We used 5k and 8k vocabularies for ASR and E2E-ST/MT models, respectively. We report case-sensitive BLEU on the tst-COMMON set. Results are shown in Table  5 . Our systems outperformed the previous work (Di  Gangi et al., 2019b)  implemented with the custermized Fairseq 7 with a large margin. 

 MT experiment: IWSLT16 En ? De IWSLT evaluation campaign dataset  (Cettolo et al., 2012)  is the origin of the dataset for our MT experiments. We used En-De language pair. Specifically, IWSLT 2016 training set for training data, test2012 as the development data, and test2013 and test2014 sets as our test data respectively. We compare the performance of Transformer model in ESPnet-ST with that of Fairseq in Table 6. ESPnet-ST achieves the performance almost comparable to the Fairseq. We assume that the performance gap is due to the minor difference in the implementation of two frameworks. Also, we carefully tuned the hyper-parameters for the MT task in the small ST corpora, which is confirmed from the reasonable performances of our Cascaded-ST systems. It is acknowledged that Transformer model is extremely sensitive to the hyper-parameters such as the learning rate and the number of warmup steps  (Popel and Bojar, 2018) . Thus, it is possible that the suitable sets of hyper-parameters are different across frameworks. 

 Conclusion We presented ESPnet-ST for the fast development of end-to-end and cascaded ST systems. We provide various all-in-one example scripts containing corpus-dependent pre-processing, feature extraction, training, and inference. In the future, we will support more corpora and implement novel techniques to bridge the gap between end-to-end and cascaded approaches. Figure 1 : 1 Figure 1: Directory structure of ESPnet-ST 
