title
Huawei's Submissions to the WMT20 Biomedical Translation Task

abstract
This paper describes Huawei's submissions to the WMT20 biomedical translation shared task. Apart from experimenting with finetuning on domain-specific bitexts, we explore effects of in-domain dictionaries on enhancing cross-domain neural machine translation performance. We utilize a transfer learning strategy through pre-trained machine translation models and extensive scope of engineering endeavors. Four of our ten submissions achieve state-of-the-art performance according to the official automatic evaluation results, namely translation directions on English?French, English?German and English?Italian.

Introduction Neural machine translation (NMT) models built upon the Transformer architecture  (Vaswani et al., 2017)  start to dominate the leader board of WMT biomedical shared tasks in recent years  (Bawden et al., 2019) . In-domain data (parallel and monolingual corpora) have been widely used in finetuning general domain NMT models. Despite ongoing improvements on the translation quality observed from recent biomedical shared tasks, domain adaptation remains an open problem. The in-domain data is hard to obtain and, as a consequence, greatly limits the cross-domain translation capability an NMT model can offer. Domain terminologies, on the other hand, are regarded as critical resources to improve the quality of machine translation by mitigating effects of scarce in-domain bitexts  (Bawden et al., 2019) . However, few research works leverage domain-specific terminologies (or dictionaries) in training cross-domain NMT systems. In this paper, we present the system architecture and research approaches underpinning Huawei's submissions to the WMT20 biomedical translation task. We implement two NMT systems to maximize the performances of the shared task. The system I is an in-house NMT system built upon the transformer-big architecture  (Vaswani et al., 2017)  and trained using general domain data. We explore means to enhance cross-domain coverage of an NMT model by finetuning the NMT model with in-domain bitexts. We also investigate the effects of domain dictionaries in this domain adaptation process. Reusing pre-trained models has been regarded as an efficient way of transfer learning. Pre-trained NMT models ) are adopted in the system II to this end. All NMT systems are evaluated against the test set released in the WMT19 biomedical shared task. We submitted translated results for a total of ten language directions between English (EN) and other five languages including French (FR), German (DE), Italian (IT), Russian (RU) and Chinese (ZH). Four of the submissions achieve the best BLEU scores according to the official automatic evaluation results. Substantial increases in BLEU scores are recorded in translation directions of DE?EN (+3.9 BLEU), ZH?EN (+3.5 BLEU), and EN?DE (+2.8 BLEU) compared to our submissions last year  (Peng et al., 2019) . The improvements on EN?DE can be ascribed to strong pre-trained NMT baseline models and a series of optimization techniques, for example, in-domain data augmentation and a reranking method with strong language models. High-quality in-domain data and large-scale back-translation contribute to the improvements of the ZH?EN model. is created from back-translating monolingual data. For the system II, "IND-Aug." is the pre-processed IND data in combination with the data selected from some OOD data based on the similarity to the Medline data. M is for "million," and K stands for "thousand". 

 The Data WMT20.  1  The in-domain data consist of bitexts from EMEA  (Tiedemann, 2012) , UFAL, 2 Pubmed, and Medline.  3  The data is processed by methods in the next section. The test data for the system I are from the WMT19 shared task. The data used for finetuning the system II are different from those for the system I. The system II only focuses on Medline as we discovered it is the most effective IND data for this shared task. The development (dev.) set for the system II is the OK-aligned test data from the WMT19 biomedical shared task. A batch of monolingual Medline data in English dated before July 2018 has been extracted to provide a basis for data augmentation and noisy channel model reranking . It produces the augmented IND data for the ZH?EN translation direction via back-translation ("IND-Aug." in Table  1 ). Due to time and resource constraints, we could not fully explore this monolingual Medline data in other translation directions. 

 The Approaches The proposed systems are finetuned and enhanced using the following methods. All models are trained on Tesla V100 GPUs. Systems I and II use batch sizes of 6,144 and 8,000 tokens respectively in the finetuning process. 

 In-domain Dictionary Bilingual dictionaries have been studied in the machine translation community for various purposes. The lexicons are used to enhance the translation quality for rare and unknown words in the parallel corpus  (Zhang and Zong, 2016) . Research works in domain adaptation for NMT showed that incorporating domain-specific dictionaries is a viable solution  (Hu et al., 2019; Thompson et al., 2019; Peng et al., 2020) . Inspired by these studies, we apply domain-specific dictionaries derived from SNOMED-CT, 4 which is a collection of multilingual clinical terminology, to finetune general domain NMT models to boost cross-domain coverage. The dictionaries are treated as bitexts attached to the end of training data. 

 Reranking Apart from adopting a data-driven approach mentioned above, we also apply a transfer learning approach by reusing the publicly available pre-trained NMT models provided at fairseq .  5  After finetuning the selected pre-trained NMT models on the in-domain data, we apply a noisy channel model reranking method  random search for the best performing candidate on the validation data. ? 1 logP (y|x) + ? 2 logP (y) + ? 3 logP (x|y) (1) Due to time constraints, we did not implement the reranking approach on the system I. 

 Data Processing A data processing pipeline is applied to enhance the quality of training data: ? Data cleaning is implemented to filter out noisy data. An important step is to handle misalignment in the parallel corpus. An alignment model trained by fast-align  (Dyer et al., 2013)  6 is applied to this end  (Lu et al., 2018) . In addition, we remove bitexts with a source and target sentence length ratio exceeding a certain threshold (i.e., 2.5). A language detection tool 7 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. Other noisy data, such as those with HTML tags and extra spaces, are removed. ? Scripts from Moses  (Koehn et al., 2007)  are used to perform punctuation normalization and tokenization. SentencePiece  (Kudo and Richardson, 2018)  segments words into subwords. ? We extract "in-domain" data which are close to Medline from general domain data by using TFIDF-based similarities. Similar data augmentation approaches can be identified in  Wang et al. (2017)  and  Peng et al. (2020) . ? Post-processing is performed after decoding to detokenize subwords and remove undesirable spaces between special characters and numbers, i.e., converting "23 -25" into "23-25". 

 Experimental Results The systems are trained with OOD data and finetuned using IND data to produce the submitted results. We benchmarked the submissions using WMT19 test data. The BLEU scores are calculated using the MTEVAL script from Moses  (Koehn et al., 2007) . Results are shown in Table  2  and Table  4 . The final two rows demonstrate the scores of our submissions on this year's test sets and the best official records released by the organizers. 

 English ? French The system I is our in-house system equipped with an extensive data processing pipeline to handle noisy data, i.e., the application of sentence alignment and language detection tools. Our EN?FR and FR?EN submissions achieve the best official results in the WMT20 shared task. IND bitexts and "IND-Dict." have contributed to up to 2.7 BLEU in enhancing the baseline performance. We presume the improvement is due to the enhanced domain coverage the IND data brought forth. Note that even with much larger OOD bitexts than last year, the system produces similar benchmark scores. It appears an over-representation of OOD data is not helpful in cross-domain NMT. An analysis of domain coverage is performed to investigate the effect of IND information on cross-domain translation. We count the number of unique terms (1-2 grams)  at the intersection of a data source (i.e., the OOD training data) and the test data. Table  3  indicates that the increase of BLEU may be associated with a level of domain coverage enhancement. An increasing number of distinctive IND terms is recorded. 

 English ? German We perform ablation tests on pre-trained NMT models (the system II) in English ? German under various conditions. As shown in  

 Other Translation Directions The submissions for other translation directions are illustrated in Table  2 and Table 3   

 Conclusion This paper depicts Huawei's submissions to the WMT20 biomedical shared task. For all ten translation directions, we have explored the effects of using IND bitexts and dictionaries on enhancing the performances of cross-domain NMT. We have demonstrated the benefits of the transfer learning strategy of reusing pre-trained NMT models. Four of our ten submissions achieve the best records according to the released WMT20 official results. Table 1 : 1 Data used for training and finetuning systems I and II. Note that "IND-Dict." refers to the in-domain dictionary. "IND-Aug." is the augmented data derived from processing IND data. For the system I, "IND-Aug." Table 1 captures the number of sentences pairs used in this shared task. The system I is trained using in-house general domain data (OOD) and finetuned on the in-domain data (IND) provided by 
