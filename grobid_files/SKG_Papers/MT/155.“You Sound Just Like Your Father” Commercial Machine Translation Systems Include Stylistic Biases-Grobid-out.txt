title
"You Sound Just Like Your Father" Commercial Machine Translation Systems Include Stylistic Biases

abstract
The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages "sound" older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. These results open up interesting new research avenues in machine translation to take stylistic considerations into account.

Introduction Translating what is being said is arguably the most important aspect of machine translation, and has been the main focus of all its efforts so far. However, how something is said also has an impact on how the final translation is perceived.  have pointed out that demographic aspects of language do play a role in translation, and could help in personalization. As  Vanmassenhove et al. (2018)  have shown, gendered inflections like "Sono stanco/a" (Italian I am tired) are an important aspect of correct translations. In many cases, capturing the style of a document is equally important as its content: translating a lover's greeting as "I am entirely pleased to see you" might be semantically correct, but seems out of place. Demographic factors  (age, gender, etc.)  all manifest in language, and therefore influence style: we do not expect a 6-year old to sound like an adult, and would not translate a person to seem differently gendered. However, in this paper, we show such a change is essentially what happens in machine translation: authors sound on average older and more male. Prior work  (Rabinovich et al., 2017)  has shown that translation weakens the signal for gender pre-diction. We substantially extend this analysis in terms of languages, demographic factors, and types of models, controlling for demographically representative samples. We show the direction in which the predicted demographic factors differ in the translations, and find that there are consistent biases towards older and more male profiles. Our findings suggest a severe case of overexposure to writings from these demographics  (Hovy and Spruit, 2016) , which creates a self-reinforcing loop. In this paper, we use demographicallyrepresentative author samples from five languages (Dutch, English, French, German, Italian), and translate them with three commercially available machine translation systems (Google, Bing, and DeepL). We compare the true demographics with the predicted demographics of each translation (as well as a control predictor trained on the same language). Without making any judgment on the translation of the content, we find a) that there are substantial discrepancies in the perceived demographics, and b) that translations tend to make the writers appear older and considerably more male than they are. 

 Contributions We empirically show how translations affect the demographic profile of a text. We release our data set at https://github.com/ MilaNLProc/translation_bias. Our findings contribute to a growing literature on biases in NLP (see  Shah et al. (2020)  for a recent overview). 

 Data We use the Trustpilot data set from  Hovy et al. (2015) , which provides reviews in different languages, and includes information about age and gender. We use only English, German, Italian, French, and Dutch reviews, based on two criteria: 1) availability of the language in translation mod-els, and 2) sufficient data for representative samples (see below) in the corpus. For the English data, we use US reviews, rather than UK reviews, based on a general prevalence of this variety in translation engines. 

 Translation Data For each language, we restrict ourselves to reviews written in the respective language (according to langid 1  (Lui and Baldwin, 2012) ) that have both age and gender information. We use the CIA factbook 2 data on age pyramids to sample 200 each male and female. We use the age groups given on the factbook, i.e.,  15-24, 25-54, 55-64, and 65+ . Based on data sparsity in the Trustpilot data, we do not include the under-15 age group. This sampling procedure results in five test sets of about 400 instances each (the exact numbers vary slightly according to rounding and the proportions in the CIA factbook data), balanced for binary gender. The exception is Italian, where the original data is so heavily skewed towards male reviews that even with downsampling, we only achieve a 48:52 gender ratio. We then translate all non-English test sets into English, and the English test set into all other languages, using three commercially available machine translation tools: Bing, DeepL, and Google Translate. 

 Profile Prediction Data We use all instances that are not part of any test set to create training data for the respective age and gender classifiers (see next section). Since we want to compare across languages fairly, the training data sets need to be of comparable size. We are therefore bounded by the size of the smallest available subset (Italian). We sample about 2500 instances per gender, according to the respective age distributions. This sampling results in about 5000 instances per language (again, the exact number varies slightly based on the availability of samples for each group and rounding). We again subsample to approximate the actual age and gender distribution, since, according to  Hovy et al. (2015) , the data skews strongly male, while otherwise closely matching the official age distributions. 

 Methods To assess the demographic profile of a text, we train separate age and gender classifiers for each language. These classifiers allow us to compare the predicted profiles in the original language with the predicted profiles of the translation, and compare both to the actual demographics of the test data. We use simple Logistic Regression models with L 2 regularization over 2-6 character-grams, and regularization optimized via 3-fold crossvalidation.  3  The numbers in For each non-English sample, we predict the age and gender of the author in both the original language and in each of the three English translations (Google, Bing, and DeepL). I.e., we use the respective language's classifier described above (e.g., a classifier trained on German to predict German test data), and the English classifier described above for the translations. E.g., we use the age and gender classifier trained on English data to predict the translations of the German test set. For the English data, we first translate the texts into each of the other languages, using each of the three translation systems. Then we again predict the author demographics in the original English test set (using the classifier trained on English), as well as in each of the translated versions (using the classifier trained on the respective language). E.g., we create a German, French, Italian, and Dutch translation with each Google, Bing, and DeepL, and classify both the original English and the translation. We can then compare the distribution of age groups and genders in the predictions with the actual distributions. If there is classifier bias, both the predictions based on the original language and the predictions based on the translations should be skewed in the same direction. We can measure this difference by computing the Kullback-Leibler (KL) divergence of the predicted distribution from the true sample distribution. In order to see whether the predictions differ statistically significantly from the original, we use a use a ? 2 contingency test and report significance at p <= 0.05 and p <= 0.01. If instead there is a translation bias, then the translated predictions should exhibit a stronger skew than the predictions based on the original language. By using both translations from and into English, we can further tease apart the direction of this effect. 

 Results 

 Gender Translating into English Table  2  shows the results when translating into English. It shows for each language the test gender ratio, the predicted ratio from classifiers trained in the same language, as well as their KL divergence from the ratio in the test set, and the ratio predictions and KL divergence on predictions of an English classifier on the translations from three MT systems. For most languages, there exists a male bias in predictions of the original language. The translated English versions create an even stronger skew. The notable exception is French, which most translation engines render in a demographically faithful manner. Dutch is slightly worse, followed by Italian (note, though, that the Italian data was so heavily imbalanced that we could not sample an even distribution for the test data). Somewhat surprisingly, the gender skew is strongest for German, swinging by as much as 15 percentage points. Translating from English Table  3  shows the results when translating from English into the various languages. The format is the same as for Table  2 . Again we see large swings, normally exacerbating the balance towards men. However, translating into German with all systems produces estimates that are a lot more female than the original data. This result could be the inverse effect of what we observed above. Again, there is little change for French, though we also see some female bias in two MT systems. Figure  1  shows the kernel density plots for the four age groups in each language (rows) in the same language prediction, and in the English translation. In all cases, the distributions are reasonably close, but in all cases, the predictions overestimate the most prevalent class. 

 Age To delve a bit deeper into this age mismatch, we also split up the sample by decade (i.e., seven classes: 10s, 20s, etc., up to 70s+). Figure  2  shows the results. The caveat here is that the overall performance is lower, due to the higher number of classes. We also can not guarantee that the distribution still follows the true demographics, since we are subsampling within the larger classes given by the CIA factbook. However, the results still strongly suggest that the observed mismatch is driven predominantly by overprediction of the 50s decade. Because this decade often contributed strongly to the most frequent age category (25-54), predictions did not differ as much from gold in the previous test. It Table  3 : Gender split (%) and KL divergence from gold for each language when translated from English. * = split differs significantly from gold split at p <= 0.05. * * = significant difference at p <= 0.01. also explains the situation of the Italian predictor. In essence, English translations of all these languages, irrespective of the MT system, sound much older than they are. 

 Discrepancies between MT Systems All three tested commercial MT systems are close together in terms of performance. However, they also seem to show the same systematic translation biases. The most likely reason is the use of biased training data. The fact that translations into English are perceived as older and more male than translations into other languages could indicate that there is a larger collection of unevenly selected data in English than for other languages. 

 Related Work The work by  Rabinovich et al. (2017)  is most similar to ours, in that they investigated the effect of translation on gender. However, it differs in a few key points: they show that translation weakens the predictive power, but do not investigate the direction of false predictions. We show that there is a definitive bias. In addition, we extend the analysis to include age. We also use various commercially available MT tools, rather than research systems. Recent research has suggested that machine translation systems reflect cultural and societal bi-ases  (Stanovsky et al., 2019; Escud? Font and Costa-juss?, 2019) , though mostly focusing on data selection and embeddings as sources. Work by ; Mirkin and Meunier (2015) has set the stage for considering the impact of demographic variation  (Hovy et al., 2015)  and its integration in MT more general. There is a growing literature on various types of bias in NLP. For a recent overview, see  Shah et al. (2020) . 

 Conclusion We test what demographic profiles author attribute tools predict for the translations from various commercially available machine translation tools. We find that independent of the MT system and the translation quality, the predicted demographics differ systematically when translating into English. On average, translations make the author seem substantially older and more male. Translating from English into any of the other languages shows more mixed results, but similar tendencies. for Data Science and Analytics (BIDSA) and the Data and Marketing Insights (DMI) unit. Figure 1 : 1 Figure 1: Density distribution and KL for age prediction in various languages and different systems in original and when translated into English. Solid yellow line = true distribution. * = predicted distribution differs significantly from gold distribution at p <= 0.05. * * = significant difference at p <= 0.01. 
