title
Towards a Better Evaluation of Metrics for Machine Translation

abstract
An important aspect of machine translation is its evaluation, which can be achieved through the use of a variety of metrics. To compare these metrics, the workshop on statistical machine translation annually evaluates metrics based on their correlation with human judgement. Over the years, methods for measuring correlation with humans have changed, but little research has been performed on what the optimal methods for acquiring human scores are and how human correlation can be measured. In this work, the methods for evaluating metrics at both system-and segment-level are analyzed in detail and their shortcomings are pointed out.

Introduction In the past, machine translation (MT) metrics have been extensively studied and evaluated, at both system-and segment-level  (Bojar et al., 2016 (Bojar et al., , 2017 Ma et al., 2018 Ma et al., , 2019 . When performing systemlevel evaluation, the average score of a MT system is taken into account. Segment-level evaluation uses each sentence (segment) separately to compute correlation. The results of these metric evaluations are critical to the way MT metrics are perceived. In particular the correlation with human judgment is of great importance. For this reason, an understanding for the workings of the evaluation method is required. Proposals to identify relevant system-level human scores have been discussed  (Koehn, 2012; Sakaguchi et al., 2014) , but no comprehensive analysis on this topic has been conducted. In particular, detailed studies on the segment-level evaluation are neglected, although it is an integral part of the metric evaluation. Since the goal of a metric is to evaluate a translation as close as possible to a human's rating, it is important to clearly define the methods of determining human score and the methods of correla-tion measurement. This work aims to present an overview of the methods used in the evaluation, analyze their strengths and weaknesses, and propose solutions to some of the pitfalls of the methods. 

 Human Scores To measure the correlation between the score of a metric and the score of a human, a method of determining human scores is required. Thus, a person has to judge the quality of a translated sentence. This is not a simple task, as different people may have different opinions about the exact quality of the translation. Another aspect to consider is that in order to calculate correlation, the score must be quantifiable in some way. Thus, the methods used to detect human judgment must use a sufficient number of human judges for them to be reproducible. In the Workshop on Statistical Machine Translation (WMT), three different methods are used to determine the human score: direct assessment (DA) , relative ranking (RR)  (Stanojevic et al., 2015)  and, in recent years, relative ranking out of direct assessment (DARR)  (Bojar et al., 2017) . 

 Direct Assessment The DA measures the quality of a translation on a scale from 0 to 100, based on the adequacy and fluency of the sentence. To obtain the score, the human judges are provided with a reference translation and the output of a single MT system, which makes the evaluation process monolingual. To ensure reproducibility, a large number of judges are needed -at least 15  (Ma et al., 2019) . Additionally, scores are standardized  to eliminate individual distortions, such as judges who only provide high or low scores. Furthermore, a form of quality control is applied to filter out judges who exhibit a high variance in comparison to their peers. Overall, DA is one of the best ways to obtain human judgement. It provides a numerical score that can be easily used in common statistical methods, such as Spearman's ?  (Spearman, 1987)  or Pearson's r  (Pearson and Galton, 1895) , at both the segment-and the system-level. However, to obtain statistically significant correlation measurements and ensure reproducibility, a high number of human scores are required. For the segment-level it is therefore infeasible to obtain DA scores. This leads to the need to use a completely different method for determining human judgements at the segmentlevel. Another possibility is to establish a relative ranking of the few obtained DA scores (DARR). 

 Relative Ranking The RR produces, as the name implies, a ranking between multiple translations. In WMT, the judges are presented with five system outputs with the corresponding source and reference sentence, making the evaluation process bilingual. Each judge ranks the five sentences from the best to worst, taking equality (tie) into account. To simplify the evaluation, identical sentences from different systems are collapsed into one. The resulting relative ranking of five tuples is not as straightforward to use for correlation calculation, since most correlation coefficients rely on absolute ranking information. One approach to obtaining a correlation is to use a variant of Kendall's ?  (Kendall, 1938; Mach?cek and Bojar, 2014) . This entails converting the scores produced by metrics into relative rankings. Naturally, this has the disadvantage that the fine granularity of the scores is lost. However, this method can be used for both segment-and system-level correlation calculations. Another option used in WMT16  (Bojar et al., 2016)  is to convert relative rankings to absolute rankings through TrueSkill  (Herbrich et al., 2006; Sakaguchi et al., 2014) . This method uses the relative rankings to estimate an absolute score for each system, which is then used to calculate the correlation (by Pearson's r or Spearman's ?). The score of each system is represented by a Gaussian distribution, with the mean of the predicted score of the system and the variance of the confidence in that prediction. Due to the nature of the method, it can only be used for the system-level correlation calculation. This, in turn, makes it difficult to interpret the results since normally two different correlation calculation methods must be used for the different evaluation levels. 

 DARR Due to the difficulty of obtaining enough DA scores for a statistically significant segment-level correlation calculation,  Bojar et al. (2017)  introduced the concept of obtaining a relative ranking from the DA used at the system-level and termed DARR. For this purpose, all possible sentence pairs, for which a DA score is available, are generated between all participating systems. These sentence pairs are then filtered to remove ties. The criterion used by  Ma et al. (2019)  is to remove sentence pairs, whose difference on the DA scale is less than 25. This should lead to the removal of all ties and produce an RR that scores the systems only as better or worse. However, this is not the case. Table  1  shows the RR of sentences with a sentence identifier (SID) on different language pairs (LP). The system that has achieved a better translation according to the DA score for these sentences is under the column better. In this case, the sentences generated by both systems are completely identical, as can be seen in Table  2 , although they have been classified as different according to the DARR method. Such identical sentences occur across multiple language pairs in the WMT19 data set. Another important aspect is that tie filtering is not applied to the metrics scores and therefore ties are possible for metrics. This makes the correlation calculation, especially for identical sentences, a difficult task. It is therefore of interest to determine how many identical sentences are present after filtering. For this reason, a brief analysis is carried out on the basis of the WMT19 data using six language pairs, which is shown in Table  3 . There are no identical sentences for the language pairs Gujarati?English (gu-en) and Kazakh?English (kk-en). However, for all other language pairs, especially Chinese?English (zh-en), there are identical sentences. Note that these identical sentences are present after the tie filtering. Table 3 also shows the amount of ties produced by two metrics: YiSi-1  (Lo, 2019)  and EED  (Stanchev et al., 2019) . It is clear that a significant amount of the ties for the two metrics come from identical sentences. In addition, a considerable amount of data is eliminated. Figure  1    difference in DA scores is below the specified value, are considered as ties. Note that the threshold influences the amount of data used immensely. By having virtually no threshold (a threshold of 1) the average number of sentences is five times higher than when using a threshold of 50. The threshold of 25 used by WMT19 almost halves the amount of data used to acquire the correlation. Figure  1 : The average number of sentences over different language pairs (to-English and from-English directions) when excluding ties based on various equivalence thresholds. Overall DARR provides a method for calculating the correlation at the segment-level in a scenario where there is not enough DA data. However, removing ties as part of the human component makes the evaluation unfair. This is aggravated by the fact that after DA-based tie filtering, not all ties are successfully removed. One possible solution, which remains to be tested, is to consider the ties of the human component carefully. This would at least equal the domain of metrics and human scores. 

 Measuring the Correlation Obtaining human scores is only part of the correlation calculation. The other one is to use both human and metric scores to compute their similarity or correlation. The case, where both human and metric scores are represented by absolute values, is straightforward to compute using methods such as Pearson's r or Spearman's ?. However, DA relies on a large amount of annotators that cannot always be guaranteed, especially at the segment-level. In the case where RR or DARR is used for human scores, this task is not that easy. For this reason, the focus here is on the case where a form of RR is used -typically for the segment-level correlation calculation. As previously mentioned, WMT uses a form of Kendall's ? to obtain a correlation given the relative ranking. The coefficient definition in its most general form is shown in Equation (  1 ) ? = |concordant ? discordant| |concordant + discordant| , ( 1 ) where the concordant pairs denote cases in which there is agreement between the metric and the hu-man score, and the discordant pairs cases in which there is disagreement. To formally define agreement and disagreement, a matrix can be used as described by  Mach?cek and Bojar (2014) . The various matrix formulations that have been used in WMT over the years are shown in Table  4 . For metric scores to be interpretable in these matrices, a relative ranking must be constructed from the absolute scores for each participating metric. This is achieved by performing a pairwise comparison of the participating systems at the segment-level, taking into account ties. All three matrices treat matches and mismatches identically: ? discordant pairs are always cases where there are disagreements between the human and metric scores: {<, >} or {>, <}, ? concordant pairs are always cases where the scores match: {<, <} or {>, >}. The only difference between the three methods is the treatment of ties. Table  4a  ignores the existence of ties. However, this is not desirable since ties are possible for metrics. Therefore, metrics are not evaluated on the same amount of sentences. This can be particularly detrimental to metrics that produce a large number of ties. For example, a metric with 99 ties and 1 concordant pair would achieve perfect correlation, while a metric without ties and 70 concordant and 30 discordant pairs would give a correlation of 0.4. The discrepancy in the results due to the data difference is evident. On the other hand, incorporating ties while not considering human score ties can also lead to undesirable results. In Table  4c , which is used in WMT19, metric ties are considered as a discordant pair {<, =} and {>, =}. Since ties are not defined (or included) in human scores, every tie produced by a metric results in a discordant pair. This in turn reduces its correlation. Thus, a "perfect" metric would never produce a tie between two sentences. This assumption does not reflect reality. In addition, the matrix is not symmetric since there are more possible discordant pairs than concordant ones. This means that a reasonable interpretation of the negative correlation is not possible. Therefore, metrics that have a negative correlation, such as TER  (Snover et al., 2006) , CHARACTER  (Wang et al., 2016)  and EED  (Stanchev et al., 2019) , must be mapped from an error (or edit) rate (E) to an accuracy score to ensure a relatively fair evaluation. This is not trivial, as there is no standard way to convert these metrics into the accuracy rate: neither 1 ? E nor ?E is optimal. A middle ground between the penalization and the ignoring of ties is the matrix in Table  4b . The ties are not penalized directly, but affect the overall correlation since they are part of the denominator: ? = |concordant ? discordant| |concordant + discordant + ties| (2) Since there is no hard penalization for metrics that produce more ties, such metrics are at a disadvantage. For example, a metric with 20 ties and 80 concordant pairs would achieve a correlation of 80/(80 + 20) = 0.8, although all non-tie pairs achieve perfect correlation. On the other hand, a metric that overproduces ties, for example, with 80 ties and 20 concordant pairs, would have a correlation of 20/(20 + 80) = 0.2. It can also be argued that measuring the correlation on metrics with a too high percentage of ties is not significant, since there are too few sentence pairs that are concordant or discordant. One possible solution to the problem is shown in Table  5 . The cases where there is clear agreement or disagreement between humans and metrics remain unchanged. In cases of tie disagreements, a soft penalization is added. This soft penalization is realized the same manner as in Table  4b  using Equation (2). In the case where both the metric and human scores tie the two systems, a concordant pair (1) for accuracy-based metrics and a discordant pair (-1) for error rate-based metrics are given. This allows the process to be symmetrical and avoids the problem of having to map error rate to accuracy or vice versa. In addition, ties can now positively affect the correlation and all metrics are evaluated on the same amount of data. Naturally, this alteration of the evaluation method requires that ties be included in the RR. When using DARR, this can be achieved by considering all pairs, where the DA score difference is less than 25, and where the system translations are identical, as ties. A disadvantage of this method is that a distinction has to be made between metrics that aim for a strong negative correlation and metrics that aim for a strong positive correlation. Moreover, the exact range, where a tie is considered, is not necessarily clear. 

 Metric < = > Human < 1 X -1 = X X X > -1 X 1 (a) No tie penalization Metric < = > Human < 1 0 -1 = X X X > -1 0 1 (b) Soft tie penalization Metric < = > Human < 1 -1 -1 = X X X > -1 -1 1 (c) Hard tie penalization Table 4: Kendall's ? evaluation matrices  (Mach?cek and Bojar, 2014; Ma et al., 2019) .  Metric < = > Human < 1 0 -1 = 0 {1,-1} 0 > -1 0 1 

 Discussion The MT metric evaluation is an area that needs further investigation. This work gives an overview of the methods used so far and highlights some of their shortcomings. The system-level assessment currently seems to be good, but the evaluation methods at the segment-level still need to be explored (in particular, if there is not enough DA data to directly calculate the correlation at the segment-level): ? It might not be a good idea to rule out tie cases: in theory, there are identical translations and translations of the same quality, and the metrics should be able to give them the same score; in practice, we have shown that excluding all tie cases eliminated a large proportion of the scores collected, which will have a significant impact on the final results. However, it is difficult to clearly define the tie cases for human evaluations, as in DA, on a scale from 0 to 100, different human annotators can give different scores for identical translations. ? The threshold for tie cases is not well defined. Further studies on the threshold value can be carried out. And also whether a threshold should be applied to the automatic metric scores. This study itself may not be a theoretically well-defined task, but some insight could be gained by examining the performance of various metrics under different thresholds. ? The used correlation coefficient is not sym-metrical. Then the metrics with negative correlations have to be preprocessed before the evaluation, which can lead to inconsistencies. The proposed solution may also have potential problems as described, but it is worth doing further studies to define a better correlation coefficient. In general, the task of creating a metric evaluation that is fair and reproducible for all metric types remains to be solved and deserves more attention and study. Table 1 : 1 depicts the effect of varying the equivalence threshold, i.e. cases, in which the RR human scores for segment-level with their corresponding sentences from WMT19 1 . LP data SID better worse de-en newstest2019 1200 uedin.6749 UCAM.6461 zh-en newstest2019 604 Baidu-system.6940.zh-en MSRA.MASS.6996.zh-en en-zh newstest2019 1351 NEU.6830 UEDIN.6158 LP SID system-generated hypotheses (identical for both systems) de-en 1200 Music cabaret: Gender understanding with heart -Wolbeck -Westphalian News zh-en 604 China Resources Beer closed at HK $28.85 on Friday, down nearly 4.5% in the past month. en-zh 1351 ? 

 Table 2 : 2 Corresponding sentences from Table1for the two systems. LP #sentences #identical YiSi-1 EED sentences #ties #ties gu-en 31k 0 2 27 kk-en 27k 0 74 115 zh-en 31k 152 336 361 en-gu 11k 5 8 13 en-kk 18k 23 53 64 en-zh 19k 84 205 455 

 Table 3 : 3 Number of identical sentences vs. ties in the WMT19 corpus used for human correlation. 

 Table 5 : 5 Integration of human ties in Kendall's ? . 

			 Scripts and data from: http://ufallab.ms.mff.cuni.cz/ ?bojar/ wmt19-metrics-task-package.tgz
