title
Korean-to-Japanese Neural Machine Translation System using Hanja Information

abstract
In this paper, we describe our TMU neural machine translation (NMT) system submitted for the Patent task (Korean?Japanese) of the 7th Workshop on Asian Translation (WAT 2020 , Nakazawa et al., 2020 . We propose a novel method to train a Korean-to-Japanese translation model. Specifically, we focus on the vocabulary overlap of Korean Hanja words and Japanese Kanji words, and propose strategies to leverage Hanja information. Our experiment shows that Hanja information is effective within a specific domain, leading to an improvement in the BLEU scores by +1.09 points compared to the baseline.

Introduction The Japanese and Korean languages have a strong connection with Chinese owing to cultural and historical reasons  (Lee and Ramsey, 2011) . Many words in Japanese are composed of Chinese characters called Kanji. By contrast, Korean uses the Korean alphabet called Hangul to write sentences in almost all cases. However, Sino-Korean 1 (SK) words, which can be converted into Hanja words, account for 65 percent of the Korean lexicon  (Sohn, 2006) . Table  1  presents an example of conversions of SK words into Hanja, which are compatible with Japanese Kanji words. In addition, several studies have suggested that overlapping tokens between the source and target languages can improve the translation accuracy  (Sennrich et al., 2016; Zhang and Komachi, 2019) .  Park and Zhao (2019)  trained a Korean-to-Chinese translation model by converting Korean SK words from Hangul into Hanja to increase the vocabulary overlap. In other words, the meaning of a vocabulary overlap on NMT is that each corresponding word's embeddings are the same.  Conneau et al. (2018)  and  Lample and Conneau (2019)  improved translation accuracy by making embeddings between source words and their corresponding target words closer. From this fact, we hypothesize that if the embeddings of each corresponding word are closer, the translation accuracy will improve. Based on this hypothesis, we propose two approaches to train a translation model. First, we follow  Park and Zhao (2019) 's method to increase the vocabulary overlap to improve the Korean-to-Japanese translation accuracy. Therefore, we perform Hangul to Hanja conversion pre-processing before training the translation model. Second, we propose another approach to obtain Korean and Japanese embeddings that are closer to Korean SK words and their corresponding Japanese Kanji words. SK words written in Hangul and their counterparts in Japanese Kanji are superficially different, but we make both embeddings close by using a loss function when training the translation model. In addition, in this study, we used the Japan Patent Office Patent Corpus 2.0, which consists of four domains, namely chemistry (Ch), electricity (El), mechanical engineering (Me), and physics (Ph), whose training, development, and test-n1 2 data have domain information. Our methods are more effective when the terms are derived from Chinese characters; therefore, we expect that the effect will be different per domain.  there are domains in which there are many terms derived from Chinese characters. Therefore, to examine which Hanja information is the most useful in each domain, we perform a domain adaptation by fine-tuning the model pre-trained by all training data using domain-specific data. In this study, we examine the effect of Hanja information and domain adaptation in a Koreanto-Japanese translation. The main contributions of this study are as follows: ? We demonstrate that Hanja information is effective for Korean to Japanese translations within a specific domain. ? In addition, our experiment shows that the translation model using Hanja information tends to translate literally. 

 Related Work Several studies have been conducted on Korean-Japanese neural machine translation (NMT).  trained a Korean-to-Japanese translation model using a transformer-based NMT system with relative positioning, back-translation, and multi-source methods. There have been other attempts that combine statistical machine translation (SMT) and NMT  (Ehara, 2018; Hyoung-Gyu Lee and Lee, 2015) . Previous studies on Korean-Japanese NMT did not use Hanja information, whereas we train a Korean-to-Japanese translation model using data in which SK words were converted into Hanja words.  Sennrich et al. (2016)  proposed byte-pair encoding (BPE), i.e., a sub-word segmentation method, and suggested that overlapping tokens by joint BPE is more effective for training the translation model between European language pairs. Zhang and Komachi (2019) increased the overlap of tokens between Japanese and Chinese by decomposing Japanese Kanji and Chinese characters into an ideograph or stroke level to improve the accuracy of Chinese-Japanese unsupervised NMT. Following previous studies, we convert Korean SK words from Hangul into Hanja to increase the vocabulary overlap.  Conneau et al. (2018)  proposed a method to build a bilingual dictionary by aligning the source and target word embedding spaces using a rotation matrix. They showed that word-by-word translation using the bilingual dictionary can improve the translation accuracy in low-resource language pairs. Lample and Conneau (2019) improved the translation accuracy by fine-tuning a pre-trained cross-lingual language model (XLM). The authors observed that the bilingual word embeddings in XLM are similar. Based on these facts, we hypothesize that if the embeddings of each corresponding word become closer, the translation accuracy will improve. In this study, we use Hanja information to make the embeddings of each corresponding word closer to each other. Some studies have focused on exploiting Hanja information.  Park and Zhao (2019)  focused on the linguistic connection between Korean and Chinese. They used the parallel data in which the Korean sentences of SK words were converted into Hanja to train a translation model and demonstrated that their method improves the translation quality. In addition, they showed that conversion into Hanja helped translate the homophone of SK words.  Yoo et al. (2019)  proposed an approach of training Korean word representations using the data in which SK words were converted into Hanja words. They demonstrated the effectiveness of the representation learning method on several downstream tasks, such as a news headline generation and sentiment analysis. To train the Korean-to-Japanese translation model, we combine these two approaches using Hanja information for training the translation model and word embeddings to improve the translation quality. In domain adaptation approaches for NMT,  Bapna and Firat (2019) ;  Gu et al. (2019)  trained an NMT model pre-trained with massive parallel data and retrained it with small parallel data within the target domain. In addition,  Hu et    proposed an unsupervised adaptation method that retrains a pre-trained NMT model using pseudoin-domain data. In this study, we perform domain adaptation by fine-tuning a pre-trained NMT model with domain-specific data to examine whether Hanja information is useful, and if so, in which domains. 

 NMT with Hanja Information Our model is based on the transformer architecture  (Vaswani et al., 2017) , and we share the embedding weights between the encoder input, decoder input, and output to make better use of the vocabulary overlap between Korean and Japanese. We do not use language embedding  (Lample and Conneau, 2019)  to distinguish the source and target languages. We propose two models using the Hanja information described below. 

 NMT with Hanja Conversion We expect that the translation accuracy will improve by converting Korean SK words into Hanja to increase the source and target vocabulary overlap. In the Hanja-conversion model, we converted SK words written in Hangul into Hanja via preprocessing. Table  2  presents an example of the Hanja conversion. This conversion can increase the number of superficially matching words with Japanese sentences. We trained the translation model after the conversion. 

 NMT with Hanja Loss In the Hanja-loss model, we make the embeddings of the SK word and its corresponding Japanese Kanji word closer to each other. We use a loss function to achieve this goal as follows: L = L T + N n=1 (1 ? Sim(E(S n ), E(K n )) (1) where L is the loss function per-batch and L T is the loss of the transformer architecture. In addition, S and K are the lists of SK words and its corresponding Japanese Kanji words in the batch, respectively, and N is the length of the S and K lists (e.g., when the sentence is the example of Table 2 and the batch size is one, S = (?, ? ?, ?), K = (?, ?, ?) and N = 3.). Here, E is a function that converts words into embedding, and Sim is a cosine similarity function. Therefore, the Hanja-loss function (Equation  1 ) decreases when the SK word and Japanese Kanji word vectors become more similar. We extract Kanji words in Japanese sentences to obtain K, and then normalize Kanji into traditional Chinese and convert it into Hangul using a Chinese character into Hangul conversion tool. If the conversion tool cannot convert normalized Kanji into Hangul, we remove the Kanji word from K. To obtain S, we search for the same Hangul words from the parallel Korean sentence. The reason for using the Kanji-to-Hangul conversion is the ambiguity of Hangul-to-Hanja conversion. Conversion of Kanji into Hangul is mostly unique 3 . For example, the SK word "?" can be converted into "? (mountain)" or "? (acid)" in Hanja and Kanji, respectively, and the SK word into Hanja word conversion has certain ambiguity. By contrast, the Kanji word "?" can be converted uniquely into the SK word "?." 

 Domain Adaptation We examine the effect of domain adaptation, which uses domain-specific data for retraining the pretrained model trained by all training data. We translate the test data for each domain using a domainspecific translation model. For training and validation, we split the training and development data into four domains: chemistry, electricity, mechanical engineering, and physics using domain information. We use these data to build domain-specific translation models. For testing, we use the domain information annotated with the test-n1 data. However, the test-n2 and test-n3 data do not have domain information. Therefore, we train a domain prediction model by fine-tuning Korean or Japanese BERT  (Devlin et al., 2019)  using the labeled training data of the Japan Patent Office Patent Corpus 2.0 to predict the domain information of test-n2 and test-n3 data. 

 Experimental Settings 

 Implementation We use the fairseq 4 implementation of the transformer architecture for the baseline model and the Hanja-conversion model and extend the implementation for the Hanja-loss model. Table  7  presents some specific hyperparameters that are used in all models. To train the domain prediction model for domain adaptation (Section 4), we used the Bidi-rectionalWordPiece tokenizer, character model of KR-BERT 5  (Lee et al., 2020)  as the Korean BERT and the bert-base-japanese-whole-word-masking 4 https://github.com/pytorch/fairseq 5 https://github.com/snunlp/KR-BERT model 6 as the Japanese BERT. 

 Data To train the Korean-to-Japanese translation model, we used the Korean?Japanese dataset of the Japan Patent Office Patent Corpus 2.0, which consists of training, development, test-n, test-n1, test-n2, and test-n3 data. We apply the following preprocess for each model. 

 Baseline model We tokenize Korean sentences using MeCab 7 with the mecab-ko dictionary 8 , and Japanese sentences with the IPA dictionary. After tokenization, we delete sentences with more than 200 words from the training data and apply shared byte-pair encoding (BPE,  Sennrich et al., 2016)  with a 30k merge operation size. Table  3  presents the statistics of the pre-processed data. 

 NMT with Hanja Conversion To train the Hanja-conversion model, we convert South Korean words into Hanja using a Hanja-tagger 9 and normalize Hanja and Kanji in parallel sentences into traditional Chinese using OpenCC 10 . After conversion, we apply the same pre-processing with the baseline model. Table  3  also presents the statistics of pre-processed data 11 for the Hanja-conversion model. In addition, Table  4  presents the statistics on the overlap of tokens between Korean and Japanese per-domain training data. 

 NMT with Hanja Loss We use the same preprocessed data as the baseline model. To extract the set of SK words and Kanji (Section 3.2) for the Hanja-loss model, we normalize Kanji into traditional Chinese using OpenCC and convert it into Hangul using Hanja    

 Domain Adaptation We split the training, development, and test-n1 data using domain information, where the distribution of each domain is equal. We use the domain prediction model to split the test-n2 and test-n3 data. After splitting the data, we apply the same pre-processing as the baseline model. In addition, we use the same BPE model as the baseline model. Table  5  presents the test-n2 and test-3 data sizes of each domain. 

 Results Table  6  presents the BLEU scores of a single model. We indicate the best scores in bold. In a single model, the Hanja-loss model achieves the highest scores for the test-n, test-n1, and test-n2 data. The test-n data reveals an improvement of +0.12 points from the baseline model. The test-n2 data indicate that the Hanjaconversion model cannot translate well on test-n2 data. The reason is that all words in the Korean sentences of test-n2 data are written without any segmentation, which causes many errors in Hanja conversion. Table  8  presents the BLEU and RIBES scores of the ensemble of four models and the domain adaptation ensemble models. When we use Japanese BERT to predict the domain, there is a slight improvement in the test-n and test-n2 data when compared with the baseline model  13  . In addition, Table  8  reveals no difference between the baseline and Hanja-loss models in the ensemble models. Table  9  presents the per-domain dev and test-n1 BLEU scores of each ensemble model. The Hanjaloss model is not better than the baseline model for all data, and there is no difference between the baseline and Hanja-loss models. 

 Discussion 

 Hanja-conversion Model versus Hanja-loss Model In this section, we compare the Hanja-conversion model and the Hanja-loss model. Table  6  indicates that the Hanja-loss model is better than the Hanjaconversion model in terms of the BLEU scores. The reason for this result is that Hangul into Hanja word conversion errors reduce the translation accuracy in the Hanja-conversion model. 

 Baseline Model versus Hanja-loss Model In this section, we compare the baseline model and the Hanja-loss model. Tables 6, 8 and 9 indicate no difference between the baseline and Hanja-loss models in terms of BLEU and RIBES scores. Table  10  presents the results of human evaluation. These figures are adequacy scores evaluated by WAT 2020 organizers. In human evaluation, the baseline model is better than the Hanja-loss model. However, the improvement in scores is less than +0.01 points. Our experiment using all the training data reveals that there is little difference between the baseline model and the Hanja-loss model. 

 Effect of Domain in the Hanja-loss Model In this section, we examine the effect of the Hanjaloss model in the domain-specific data. In the domain adaptation model of the Hanja-loss model, the test data of Ch indicates a deteriora-tion of -1.25 points for the baseline model. As the reason for this result, we consider that Hanja information is not necessary for the Ch domain because there is more vocabulary overlap than the other domains even without Hanja conversion (Table  4 ). 

 BLEU Scores Outputs Table 11 presents a successful output example of the Hanja-loss model. The baseline model cannot translate the word "? ?," which means "single yarn fiber," in the source sentence well, but the Hanja-loss model can translate it correctly. We also found that the Hanja-loss model tends to translate literally. In the output of Table  11 , the baseline model translates the word "?, "which means "use," into "?," whereas the Hanja-loss model translates it into "?." The word "?" can translate into both "?" and "?," but "?" is the Hanja form of the SK word "?." In Table  12 's output, the Hanja-loss model translates the word "?," which means "piece" into "?," which is the Hanja form of the SK word of "?," but the translated word in the reference is "?." Therefore, in the Hanja-loss model, if the reference sentence is not a literal translation, the BLEU scores are low. 

 Conclusions and Future Work In this paper, we described our NMT system submitted to the Patent task (Korean?Japanese) of the We also demonstrated that the effect of our proposed method is different for all domain data. However, some SK words are polysemous. Our proposed method treats embeddings of such SK words the same and cannot address this problem. Therefore, the problem of polysemous words is a major challenge for our proposed method. In this study, we focused on vocabulary overlap between Korean Hanja words and Japanese Kanji words. In addition, many Hanja and Kanji words are of Chinese origin. Therefore, in the future, we will attempt to develop a translation method that takes advantage of the vocabulary overlap among Korean, Japanese, and Chinese. Table 1 : 1 Example of conversion of SK words in Hangul into Hanja and Japanese translation into Kanji. 1 Sino-Korean (SK) refers to Korean words of Chinese origin. 

 Table 2 : 2 Korean sentence and sentence in which SK words are converted into Hanja, together with their Japanese and English translations. 

 al. (2019)    Korean Japanese Model Partition Sent. Tokens Types Tokens Types train 999,758 31,151,846 21,936 31,065,360 24,178 dev 2,000 106,433 6,475 104,307 6,098 Baseline test-n test-n1 5,230 2,000 272,975 108,327 9,530 6,425 269,876 106,947 9,018 6,137 test-n2 3,000 153,195 7,522 151,253 6,656 test-n3 230 11,453 1,666 11,676 1,644 train 999,755 32,066,032 26,046 30,474,136 27,541 dev 2,000 109,460 6,944 103,994 6,119 Hanja-conversion test-n test-n1 5,230 2,000 298,404 111,543 9,832 6,941 269,146 106,653 9,166 6,162 test-n2 3,000 175,131 6,410 150,844 6,717 test-n3 230 11,730 1,759 11,649 1,634 

 Table 3 : 3 Statistics of parallel data after each pre-processing. 

 Table 4 : 4 Statistics on vocabulary overlap between Korean and Japanese per-domain training data. The tokens and types are the numbers of overlap words and their types, and percentage is the percentage of their numbers to all data. Model Ch El Me Ph Baseline Tokens / Percentage Types / Percentage 270,406 / 3.9 5,681 /31.0 86,262 / 1.1 5,070 / 29.3 63,837 / 0.6 4,202 / 22.8 93,516 / 1.1 5,442/ 29.6 Hanja-conversion Tokens / Percentage 1,697,758 / 24.2 1,525,433 / 20.1 1,678,554 / 17.7 1,530,997 / 19.0 Types / Percentage 10,803 / 49.5 9,010 / 45.0 8,449 / 39.3 10,165 / 46.8 

 12  . Korean BERT Japanese BERT Partition Ch El Me Ph Ch El Me Ph test-n2 645 589 1,131 635 538 782 1,094 586 test-n3 17 43 64 106 19 60 49 102 

 Table 5 : 5 The test-n2 and test-n3 data size of each domain, which are predicted by the domain prediction models (Section 4). Korean BERT and Japanese BERT are the models used to train the domain prediction models. Model dev test-n test-n1 test-n2 test-n3 Baseline 68.41?0.11 71.84?0.18 72.46?0.09 73.42?0.25 45.12?0.50 Hanja-conversion 67.86?0.14 63.70?0.24 71.96?0.23 56.68?0.43 44.60?0.44 Hanja-loss 68.47?0.07 71.96?0.14 72.60?0.07 73.55?0.22 44.85?0.50 

 Table 6 : 6 BLEU scores of each single model. These BLEU scores are the average of the four models. The Hanjaloss model achieves the highest scores in the test-n, test-n1, and test-n2 data. Hyperparameter Value Embedding dimension 512 Attention heads 8 Layers 6 Optimizer Adam Adam betas 0.9, 0.98 Learning rate 0.0005 Dropout 0.1 Label smoothing 0.1 Max tokens 4,098 

 Table 7 : 7 Hyperparameters. 

 Table 9 presents the BLEU scores of each model trained by domain-specific data. The Hanja-loss model achieves the highest scores in the Me and Ph domains. Specifically, the test data in the field of physics reveals an improvement of +1.09 points for the baseline model. By contrast, in the Ch domain, there are no improvements in either the domain adaptation model or the model trained by per-domain training data. 

 Table 12 : 12 ReferencePGA ? ? ? ? ? ? .... ? ? ? ? ? EnglishAs PGA fiber, we can use the single yarn fiber ...Source PGA ? ? , ? ? ? ? ? ? ... ? ? ? ? ? . Source (Hanja) PGA ? ? , ? ? ? ? ? ? ... ? ? ? ? ? . Baseline PGA ? ? ? ? ? ? ? ... ? ? ? ? ? Hanja-loss PGA ? ? ? ? ? ? ... ? ? ? ? ?Table 11: A successful output example of the Hanja-loss model.Reference? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? EnglishIn one form, R1 and R2 include one or more deuterium atoms.Source ? ? ? ? ? , R 1 ? R 2 ? 1 ? ? ? ? ? ? ? ? . Source (Hanja) ? ? ? ? , R 1 ? R 2 ? 1 ? ? ? ? ? ? ? ? . Baseline ? ? ? ? ? ? 1 ? ? 2 ? ? ? ? ? ? ? ? ? Hanja-loss ? ? ? ? ? ? 1 ? ? 2 ? ? ? ? ? ? ? ? ? ? Anunsuccessful output example of the Hanja-loss model. 7th Workshop on Asian Translation. We proposed novel methods for training the Korean-to-Japanese translation model, which uses Hanja information. 

			  Here,  

			 Kanji-to-Hangul conversion has certain ambiguity owing to the initial sound rule in the Korean language. 

			 https://github.com/cl-tohoku/bert-japanese 7 http://taku910.github.io/mecab/ 8 https://bitbucket.org/eunjeon/mecab-ko-dic/src/master/ 9 https://github.com/kaniblu/hanja-tagger 10 https://github.com/BYVoid/OpenCC 11 The number of tokens differs from the baseline because we apply the tokenization after Hanja conversion.12 https://pypi.org/project/Hanja/ 

			 However, in Korean-to-Japanese translation, we cannot use Japanese BERT to predict the domain. 

			 Table 10: Human evaluation of each domain adaptation ensemble models in test-n data. We use Japanese BERT for domain prediction model.
