title
SAARSHEFF at SemEval-2016 Task 1: Semantic Textual Similarity with Machine Translation Evaluation Metrics and (eXtreme) Boosted Tree Ensembles

abstract
This paper describes the SAARSHEFF systems that participated in the English Semantic Textual Similarity (STS) task in SemEval-2016. We extend the work on using machine translation (MT) metrics in the STS task by automatically annotating the STS datasets with a variety of MT scores for each pair of text snippets in the STS datasets. We trained our systems using boosted tree ensembles and achieved competitive results that outperforms he median Pearson correlation scores from all participating systems.

Introduction Semantic Textual Similarity (STS) is the task of measuring the degree to which two texts have the same meaning  (Agirre et al., 2014) . For instance, given the two texts, "the man is slicing the tape from the box." and "a man is cutting open a box.", an STS system predicts a real number similarity score on a scale of 0 (no relation) to 5 (semantic equivalence). This paper presents a collaborative submission between Saarland University and University of Sheffield to the STS English shared task at SemEval-2016. We have submitted three supervised models that predict the similarity scores for the STS task using Machine Translation (MT) evaluation metrics as regression features. 

 Related Work Previous approaches have applied MT evaluation metrics for the STS task with progressively improv-ing results  (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) . At the pilot English STS-2012 task,  Rios et al. (2012)  trained a Support Vector Regressor using the lexical overlaps between the surface strings, named entities and semantic role labels and the BLEU  (Papineni et al., 2002)  and METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010) scores between the text snippets and their best system scored a Pearson correlation mean of 0.3825. The system underperformed compared to the organizers' baseline system 1 which scored 0.4356. For the English STS-2013 task,  Barr?n-Cede?o et al. (2013)  also used a Support Vector Regressor with an larger array of machine translation metrics (BLEU, METEOR, ROUGE  (Lin and Och, 2004) ,  NIST (Doddington, 2002) , TER  (Snover et al., 2006) ) with measures that compute similarities of dependency and constituency parses  (Liu and Gildea, 2005)  and semantic roles, discourse representation and explicit semantic analysis  (Gabrilovich and Markovitch, 2007)  annotations of the text snippets. These similarity measures are packaged in the Asiya toolkit  (Gim?nez and M?rquez, 2010) . They scored 0.4037 mean score and performed better than the Takelab baseline  ( ?ari? et al., 2012)  at 0.3639. At the SemEval-2014 Cross-level Semantic Similarity task  (Jurgens et al., 2014; Jurgens et al., 2015) , participating teams submitted similarity scores for text of different granularity. Huang and Chang (2014) used a linear regressor solely with MT evalu-ation metrics (BLEU, METEOR, ROUGE) to compute the similarity scores between paragraphs and sentences. They scored 0.792 beating the lowest common substring baseline which scored 0.613. In the SemEval-2015 English STS and Twitter similarity tasks, Bertero and Fung (2015) trained a neural network classifier using (i) lexical similarity features based on WordNet  (Miller, 1995) , (ii) neural auto-encoders  (Socher et al., 2011) , syntactic features based on parse tree edit distance  (Zhang and Shasha, 1989; Wan et al., 2006)  and (iii) MT evaluation metrics, viz. BLEU, TER, SEPIA  (Habash and Elkholy, 2008) , BADGER  (Parker, 2008)  and MEANT  (Lo et al., 2012) . For the classic English STS task in SemEval-2015,  Tan et al. (2015)  used a range of MT evaluation metrics based on lexical (surface ngram overlaps), syntactic (shallow parsing similarity) and semantic features (METEOR variants) to train a Bayesian ridge regressor. Their best system achieved 0.7275 mean Pearson correlation outperforming the token-cos baseline which scored 0.5871 while the top system  (Sultan et al., 2015)  achieved 0.8015. Another notable mention of MT technology in the STS tasks is the use of referential translation machines to predict and derive features instead of using MT evaluation metrics  (Bic ?ici and van Genabith, 2013; Bic ?ici and Way, 2014; Bicici, 2015) . 

 Approach Following the success of systems that use MT evaluation metrics, we train three regression models using an array of MT metrics based on lexical, syntactic and semantic features. 

 Feature Matrix Machine translation evaluation metrics utilize various degrees of lexical, syntactic and semantic information. Each metric considers several features that compute the translation quality by comparing a translation against one or several reference translations. We trained our system using the follow feature sets: (i) n-gram, shallow parsing and named entity overlaps (Asiya), (ii) BEER, (iii) METEOR and (iv) ReVal.  2014 ) introduced a range of language independent metrics relying on n-gram overlaps similar to the modified n?-gram precisions of the BLEU metric  (Papineni et al., 2002) . Different from BLEU, Gonz?lez et al. (  2014 ) computes n-gram overlaps using similarity coefficients instead of proportions. We use the Asiya toolkit  (Gim?nez and M?rquez, 2010)  to annotate the dataset with the similarity coefficients of n-gram overlap features described in this section. 

 Asiya Features 

 Gonz?lez et al. ( We use 16 features from both cosine similarity and Jaccard Index coefficients of the character-level and token-level n-grams from the order of bigrams to 5-grams. Additionally, we use the Jaccard similarity of the pseudo-cognates and the ratio of n-gram length as the 17th and 18th features. Adding a syntactic dimension to our feature set, we use 52 shallow parsing features described in  (Tan et al., 2015) ; they measure the similarity coefficients from the n-gram overlaps of the lexicalized shallow parsing (aka chunking) annotations. As for semantics, we use 44 similarity coefficients from Named Entity (NE) annotation overlaps between two texts. After some feature analysis, we found that 22 out of the 44 NE n-gram overlap features and 1 of the shallow parsing features have extremely low variance across all sentence pairs in the training data. We removed these features before training our models. 

 BEER Features Stanojevic and Simaan (2014) presents an MT evaluation metric that uses character n-gram overlaps, the Kendall tau distance of the monotonic word order  (Isozaki et al., 2010; Birch and Osborne, 2010)  and abstract ordering patterns from tree factorization of permutations  (Zhang and Gildea, 2007) . While Asiya features are agnostic to word classes, BEER differentiates between function words and non-function words when calculating its adequacy features. 

 METEOR Features METEOR first aligns the translation to its reference, then it uses the unigram mapping to see whether they match based on their surface forms, answer-answer headlines plagiarism postediting question-question  Similar to BEER features, METEOR makes a distinction between content words and function words and its recall mechanism weights them differently. We use all four variants of METEOR: exact, stem, synonym and paraphrase. 

 ReVal Features ReVal  (Gupta et al., 2015)  is a deep neural net based metric which uses the cosine similarity score between the Tree-based Long Short Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Tai et al., 2015)  dense vector space representations of two sentences. 

 Models We annotated the STS 2012 to 2015 datasets with the features as described in Section 3.1 and submitted three models to the SemEval-2016 English STS Task using (i) a linear regressor (Linear), (ii) boosted tree regressor (Boosted)  (Friedman, 2001)  and (iii) eXtreme Gradient Boosted tree re-gressor (XGBoost) (Chen and He, 2015; Chen and Guestrin, 2015). They were trained using all features described in Section 3. We have released the MT metrics annotations of the STS data and implementation of systems on https://github.com/alvations/stasis /blob/master/notebooks/ARMOR.ipynb 

 Results Table  1  presents the official results for our submissions to the English STS task. The bottom part of the table presents the median and the best correlation results across all participating teams for the respective domains. Our baseline linear model outperforms the median scores for all domains except the answeranswer domain. Our boosted tree model performs better than the linear model and the extreme gradient boosted tree model performs the best of the three. We note that our correlation scores for all three models is lower than the median for the answer-answer domain. Figure  1  shows the bubble chart of the L1 error analysis of our XGBoost model against the gold standard similarity scores for the answer-answer domain. The colored lines correspond to the integer annotations, e.g. the yellow line represents the data points where the gold-standard annotations are 1.0. The span of the line represents the span of predictions our model made for these texts. The size of the bubble represents the effect size of our predictions' contribution to the Pearson correlation score, i.e. how close our predictions are to the gold standards. 

 Discussion As we see from Figure  1 , the centroids of the bubbles represents our model's best predictions. Our predictions for texts that are annotated at 1 to 4 similarity scores are reasonably close to the gold standards but the model performs poorly for texts annotated with the 0 and 5 similarity scores. Looking at the texts that are rated 0, we see that there are cases where the n-grams within these texts are lexically / syntactically similar but the meaning of the texts are disparate. For example, this pair of text snippets, 'You don't have to know' and 'You don't have equipments/facilities' are rated 0 in the gold standards but from a machine translation perspective, a translator would have to do little work to change 'to know' to 'equipments/facilities'. Because of this, machine translation metrics would rate the texts as being similar and even suitable for post-editing. However, the STS task focuses only on the meaning of the text which corresponds more to the adequacy aspect of the machine translation metrics. Semantic adequacy is often overlooked in machine translation because our mass reliance on BLEU scores to measure the goodness of translation with little considerations for penalizing semantic divergence between the translation and its reference. On the other end of the spectrum, machine translation metrics remain skeptical when text snippets are annotated with a score of 5 for being semantically analogous but syntactically the texts are expressed in a different form. For example, given the text snippets, 'There's not a lot you can do about that' and 'I'm afraid there's not really a lot you can do', most machine translation metrics will not al-locate full similarity scores due to the difference in lexical and stylistic ways in which the sentences are expressed. Machine translation metrics' failure to capture similarity score extremes is evident in Figure  1  where there are no 0 and 5.0 predictions. 

 Conclusion In this paper, we have described our submission to the English STS task for SemEval-2016. We have annotated the STS2012-2016 datasets with machine translation (MT) evaluation metric scores and trained a baseline linear regression and two tree ensemble models with the annotated data and achieved competitive results compared to the median pearson correlation scores from all participating systems.  Figure 1 : 1 Figure 1: L1 Error Analysis on the answer-answer domain 
