title
Experiments in Medical Translation Shared Task at WMT 2014

abstract
This paper describes Dublin City University's (DCU) submission to the WMT 2014 Medical Summary task. We report our results on the test data set in the French to English translation direction. We also report statistics collected from the corpora used to train our translation system. We conducted our experiment on the Moses 1.0 phrase-based translation system framework. We performed a variety of experiments on translation models, reordering models, operation sequence model and language model. We also experimented with data selection and removal the length constraint for phrase-pair extraction.

1 System Description 

 Training Data Statistics and Preparation The training corpora provided to the medical translation shared task can be divided into 3 categories: Medical in-domain corpora: these corpora contain documents, patents, articles, terminology lists, and titles that are representative of the same medical domain as the development and test data sets (Table  1 , second column). Medical out-of-domain corpora: these corpora also contain medical documents, patents, articles, terminologies lists and titles, but describe a different domain from the development and test data sets (Table  1 , third column). General domain corpora: these corpora consist of general-domain text (WMT 2014 general translation subtask corpora), and encompass various domains. (We did not use these corpora in our system). 

 Corpus In Within all the provided training corpora from WMT 2014, 70.72% of the medical in domain bilingual sentences, and 100% of the medical out-of-domain bilingual sentences were obtained from patent document collections. Motivated by these percentages, we view the WMT 2014 medical translation shared task as similar to training a patent-specific translation system. The monolingual corpora are taken from 9 different corpora collections, and there is no clear demarcation of the in/out-of-domain boundaries (except the PatTR collection). Our method of differentiating between the in/out-of-domain monolingual corpora is that only English sentences from the third column of Table  1 , and the patent description documents from PatTR collection, are out-ofdomain monolingual corpora. All other English sentences are treated as an in-domain monolingual resource. A patent document usually comprises title, abstract, claims and description fields. The documents often use its unique formatting and contain linguistic idiosyncrasies, which distinguish patent-specific translation systems from general translation systems, in both training and translation phases  (Ceaus ?u et al., 2011) . We have also found that some common writing styles are constantly used, especially for long sentences. For example, a typical patent claim begins with Method of [X], which comprising: followed by a numbered list. The abstract field normally contains one paragraph only, but with multiple sentences. Those long sentences are necessarily filtered out to facilitate efficient word alignment, using a tool such as GIZA++  (Och, 2003)  word aligner with the default parameter settings. However, because statistical machine translation depends on the training data to estimate translation probability, more high quality training data often leads a better translation result. One possible method of including long sentences into the training cycle is to change the word aligner's parameter settings to handle longer sentences; however, aligning long sentences is time consuming. Our solution is to capture the styled long sentences and attempt to split them on both source and target side simultaneously according to the numbered list or sentence boundary indications. If the sentence number after splitting are matching in both source and target sides, and each sentence pair is within the token length ratio of 3, we assume the split attempt is successful, otherwise the sentences are kept unchanged and will be filtered out eventually. We applied our splitting attempt approach on the patent documents at the data preparation step which consequently results in 19.35% and 7.1% increase in the number of sentence pairs compared with the original medical in-domain (from 4053258 to 4837382) and overall medical (from 17862521 to 19124142) datasets respectively. Another finding from the training corpora is that the titles of the patent documents are often capitalized in the training corpora. Since we are training a true-cased translation system, and the translation inputs contain non-title sentences, capitalized training sentences will contribute biased weights to our true-case model. We addressed this issue by creating a lowercase version of the title corpora, then we trained our true-case model with the lowercased titles corpora and other non-title corpora. We also included the lowercased title corpora in the translation system training. We tokenized the training corpora using the tokenizer script distributed in the Moses 1.0 framework with additional patent document nonbreaking preferences observed during data preparation, such as Figs and FIGS etc., and a modified aggressive setting (split hyphen character in all cases). Other data preparation steps included character normalization, character/token based foreign language detection, HTML/XML tag removal, case insensitive duplication removal, longer sentence removal (2-80, length ratio 9), resulting in the preprocessed data shown in Table  2 . 

 Corpus In   

 Training Data Selection It is an open secret that high quality and large quantity of the parallel corpus are the two most important factors for a high-quality SMT system. These factors assist the word aligner in producing a precise alignment model, which in turn brings benefits to the other SMT training steps. The quantity factor also helps the SMT system to cover more translation input variations. In order to efficiently use the training corpora listed in Table 2, we explored some data selection methodologies. We used the feature decay algorithm  (Bicici et al., 2014)  to select the training instances transductively, using the source side of the test set. We built systems with the pre-defined selection proportions in token number, 1/64, 1/32, 1/16, 1/8, 1/2, 3/4 and 1 of all the in-domain medical training data, then searched for the best performing system using the test data set as our baseline (Table 3). For the purpose of making the potential baseline systems comparable, instance selection was employed after word alignment using word aligner MGIZA++  (Gao and Vogel, 2008 ) on all the available data. The transductive learning uses features extracted from the source data of the development set with the default feature decay algorithm weight settings. All of systems were trained using the default phrase-based training parameter settings of Moses 1.0 framework, with additional msd-bidirectional-fe reordering model  (Koehn et al., 2005) . We extract phrase pairs based on growdiag-final-and  (Koehn et al., 2003)  heuristics. The language model was created with open source IRSTLM toolkit  (Federico et al., 2008 ) using all the English in-domain data (monolingual and parallel). We used 5-gram with modied Kneser-Ney smoothing  (Kneser and Ney, 1995) . The tuning step used minimum error rate training (MERT)  (Och, 2003) . The performance was measured by the test data set in case insensitive BLEU score. Table  3 : Feature decay algorithm transductive learning selection on all in-domain data using extracted features from the source side of the test data set. We choose system uses 1/8 proportions of the in-domain data as our baseline system. Our results show that the system trained with 1/8 proportion of the in-domain medical training data (398,098 sentence pairs) selected by FDA outperformed the others. We chose this system as our baseline system. 

 Experiments 

 Maximum Phrase Length While extracting phrase pairs, collecting longer phrases is not guaranteed to produce a better quality phrase table than the shorter settings, even setting the maximum phrase length to three can achieve top performance  (Koehn et al., 2003) . We take this WMT 2014 opportunity to study the capability of long phrase lengths ( >=10 ). We trained translation models with phrase length setting from 10 to 15, employed them to our baseline system and compared the performance with the default setting (length = 7).  As stated in  (Koehn et al., 2003)  and expected, the size of the phrase table is linear with respect to the maximum phrase length restriction. Surprisingly, we also found the performance can still improve after the default length setting, until a peak point (Table  4 ). 

 Phrase Length Phrase It is also interesting to see the effect for each sentence in the test set when the default phrase length setting in Moses framework is changed. We first evaluated the sentence level BLEU scores for the systems listed in Table  4 , then compared them with our baseline system sentence level BLEU scores and categorised the compared results into increased, decreased or unaffected groups (Figure  1 ). We found that system with -max-phraselength set to 12 is influenced the least (158, 118 and 724 sentences have BLEU score increased, decreased and unaffected respectively) and with -max-phrase-length sets to 10 is influenced the most (261, 257 and 482 sentences have BLEU score increased, decreased and unaffected respectively). We then looked into the decoding phase and tried to discover the actual phrase length that was used to generate the translation outputs. We exposed the translation segmentations by triggering the -report-segmentation decoding parameter in the Moses framework and computed the percentage of different phrases used according to the phrase token number (Figure  2 ). The translation is mostly generated from short source phrases (length<4) in all the systems during decoding, which we think is the reason that setting phrase extraction to length 3 can achieve top performance. We did not carry out more experiments in this case, as we think there is no absolute maximum phrase length setting which can fit into all experiments and such experiments depend on many factors, such as the similarity between the training corpus and then testing data. The choice to set -max-phrase-length to 13 is purely directed by the BLEU score shown in Table  4 . 

 Reordering Models Ceaus ?u et al. (  2011 ) also found that long-range reordering is one of the characteristics of patent documents; however, long-range reordering increases the difficulty of SMT training and decoding. We experimented two approaches to address this challenge. Apart from the msd-bidirectional-fe lexical reordering model  (Koehn et al., 2005)  in our baseline system, the phrase-based orientation and hierarchical orientation reordering models (Galley and Manning, 2008) can capture long distance dependencies. The phrase-based orientation reordering model is similar to the lexical reordering approach, the only difference between these two models is the phrase-based reordering model performs reordering only on the phrase level, but the hierarchical reordering model does not have such constraint -it does not require phrases to be adjacent. OSM  (Durrani, 2011)    (Durrani, 2013b ) is a sequence model integrating the N-gram-based translation model and reordering model. It defines three operations for reordering and considers all reordering possibilities within a fixed window while searching. We experimented with both reordering models, and found that the system defined with three reordering models performs better (Table  5 ) than OSM. We then tried to use both OSM and the reordering models together, which produced the best system at this point. 

 Systems Test set case insensitive BLEU Baseline + 13 0.4448 + OSM 0.4472 + pho-ho 0.4551* + pho-ho + OSM 0.4561* Table  5 : Reordering Model or/and OSM results 

 Two Translation Models The back-off model aims to produce translations for the unknown words or unknown phrases in the primary translation table by yielding the phrase table translation probability from primary translation table to the back-off table, as in  (Koehn et al., 2012a)   p BO (e|f ) = p 1 (e|f ) if count 1 (f ) > 0 p 2 (e|f ) otherwise Moreover, we look at using the back off model as a domain adaptation approach, which is to constrain the translation options within the target domain unless no options can be found, in which case the translation will be selected from the backoff model. Phrase table fill-up  (Bisazza et al., 2011)  is a very similar approach with back-off models, it collects and uses the phrase pairs from the out-ofdomain phrase table only when the input is unavailable at the in-domain phrase table. It merges the in-domain and out-of-domain translation models into one, where the scores are taken from more reliable source. To distinguish the source of a phrase pair entry, fill-up assigns a binary value as an additional feature at the merged phrase table. We trained our out-of-domain translation model separately using all of the out-of-domain medical data listed at Table  2  with the same parameter settings as our baseline system, then employed Moses's back-off model feature to pass the primary and back-off translation models to the decoder at tuning and translation time. The fill-up tool was sourced from  (Bisazza et al., 2011 ) at Moses's distribution. Our experiment results (Table 6) show that the fill-up approach performed better than the back-off model approach. Test set case insensitive BLEU Baseline + 13 + pho-ho + OSM 0.4561 Back-off 0.4573 Fill-up 0.4599* Table  6 : Back-off and fill-up experiment results 

 Language Model Until now, we have reported our results using a language model trained with all in-domain medical data only. We also took the similar approach to  (Koehn et al., 2007)  and carried out language model experiments. We trained our out-of-domain language model with all the out-of-domain English sentences mentioned in section 1.1, then interpolated the in-domain and out-of-domain language model by optimizing the perplexity to the development data set. We received a similar picture to  (Koehn et al., 2007) , where the language model trained with only in-domain data performed the best (  

 Conclusion In this paper, we report our results on the WMT 2014 in the French to English translation direction. We shared our statistics for the bilingual corpora used to train our translation system. All systems were trained using the open source Moses 1.0 translation framework. Based on the feature set of Moses phrased-based translation system, we carried out our experiments on translation models, reordering models, operation sequence model and language model. We also experimented on data selection and releasing the length restriction while extracting phrase pairs. Figure 2 : 2 Figure 1: Sentence level BLEU score affects when enlarge -max-phrase-length 
