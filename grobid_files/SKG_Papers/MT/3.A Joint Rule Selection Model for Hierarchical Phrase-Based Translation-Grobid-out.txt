title
A Joint Rule Selection Model for Hierarchical Phrase-based Translation *

abstract
In hierarchical phrase-based SMT systems, statistical models are integrated to guide the hierarchical rule selection for better translation performance. Previous work mainly focused on the selection of either the source side of a hierarchical rule or the target side of a hierarchical rule rather than considering both of them simultaneously. This paper presents a joint model to predict the selection of hierarchical rules. The proposed model is estimated based on four sub-models where the rich context knowledge from both source and target sides is leveraged. Our method can be easily incorporated into the practical SMT systems with the log-linear model framework. The experimental results show that our method can yield significant improvements in performance.

Introduction Hierarchical phrase-based model has strong expression capabilities of translation knowledge. It can not only maintain the strength of phrase translation in traditional phrase-based models  (Koehn et al., 2003; Xiong et al., 2006) , but also characterize the complicated long distance reordering similar to syntactic based statistical machine translation (SMT) models  (Yamada and Knight, 2001; Quirk et al., 2005; Galley et al., 2006; Mi et al., 2008; Shen et al., 2008) . In hierarchical phrase-based SMT systems, due to the flexibility of rule matching, a huge number of hierarchical rules could be automatically learnt from bilingual training corpus  (Chiang, 2005) . SMT decoders are forced to face the challenge of proper rule selection for hypothesis generation, including both source-side rule selection and targetside rule selection where the source-side rule determines what part of source words to be translated and the target-side rule provides one of the candidate translations of the source-side rule. Improper rule selections may result in poor translations. There is some related work about the hierarchical rule selection. In the original work  (Chiang, 2005) , the target-side rule selection is analogous to the model in traditional phrase-based SMT system such as Pharaoh  (Koehn et al., 2003) . Extending this work,  integrate rich context information of non-terminals to predict the target-side rule selection. Different from the above work where the probability distribution of source-side rule selection is uniform,  (Setiawan et al., 2009)  proposes to select sourceside rules based on the captured function words which often play an important role in word reordering. There is also some work considering to involve more rich contexts to guide the source-side rule selection.  (Marton and Resnik, 2008; Xiong et al., 2009)  explore the source syntactic information to reward exact matching structure rules or punish crossing structure rules. All the previous work mainly focused on either source-side rule selection task or target-side rule selection task rather than both of them together. The separation of these two tasks, however, weakens the high interrelation between them. In this paper, we propose to integrate both source-side and target-side rule selection in a unified model. The intuition is that the joint selection of source-side and target-side rules is more reliable as it conducts the search in a larger space than the single selection task does. It is expected that these two kinds of selection can help and affect each other, which may potentially lead to better hierarchical rule selections with a relative global optimum instead of a local optimum that might be reached in the pre-vious methods. Our proposed joint probability model is factored into four sub-models that can be further classified into source-side and targetside rule selection models or context-based and context-free selection models. The context-based models explore rich context features from both source and target sides, including function words, part-of-speech (POS) tags, syntactic structure information and so on. Our model can be easily incorporated as an independent feature into the practical hierarchical phrase-based systems with the log-linear model framework. The experimental results indicate our method can improve the system performance significantly. 

 Hierarchical Rule Selection Model Following  (Chiang, 2005) , ?, ? is used to represent a synchronous context free grammar (SCFG) rule extracted from the training corpus, where ? and ? are the source-side and target-side rule respectively. Let C be the context of ?, ? . Formally, our joint probability model of hierarchical rule selection is described as follows: P (?, ?|C) = P (?|C)P (?|?, C) (1) We decompose the joint probability model into two sub-models based on the Bayes formulation, where the first sub-model is source-side rule selection model and the second one is the target-side rule selection model. For the source-side rule selection model, we further compute it by the interpolation of two submodels: ?P s (?) + (1 ? ?)P s (?|C) (2) where P s (?) is the context-free source model (CFSM) and P s (?|C) is the context-based source model (CBSM), ? is the interpolation weight that can be optimized over the development data. CFSM is the probability of source-side rule selection that can be estimated based on maximum likelihood estimation (MLE) method: P s (?) = ? Count( ?, ? ) Count(?) (3) where the numerator is the total count of bilingual rule pairs with the same source-side rule that are extracted based on the extraction algorithm in  (Chiang, 2005) , and the denominator is the total amount of source-side rule patterns contained in the monolingual source side of the training corpus. CFSM is used to capture how likely the sourceside rule is linguistically motivated or has the corresponding target-side counterpart. For CBSM, it can be naturally viewed as a classification problem where each distinct source-side rule is a single class. However, considering the huge number of classes may cause serious data sparseness problem and thereby degrade the classification accuracy, we approximate CBSM by a binary classification problem which can be solved by the maximum entropy (ME) approach  (Berger et al., 1996)  as follows: P s (?|C) ? P s (?|?, C) = exp[ i ? i h i (?, ?, C)] ? exp[ i ? i h i (? , ?, C)] (4) where ? ? {0, 1} is the indicator whether the source-side rule is applied during decoding, ? = 1 when the source-side rule is applied, otherwise ? = 0; h i is a feature function, ? i is the weight of h i . CBSM estimates the probability of the source-side rule being selected according to the rich context information coming from the surface strings and sub-phrases that will be reduced to non-terminals during decoding. Analogously, we decompose the target-side rule selection model by the interpolation approach as well: ?P t (?) + (1 ? ?)P t (?|?, C) (5) where P t (?) is the context-free target model (CFTM) and P t (?|?, C) is the context-based target model (CBTM), ? is the interpolation weight that can be optimized over the development data. In the similar way, we compute CFTM by the MLE approach and estimate CBTM by the ME approach. CFTM computes how likely the targetside rule is linguistically motivated, while CBTM predicts how likely the target-side rule is applied according to the clues from the rich context information.  Let s and t be the source sentence and target sentence, W be the word alignment between them, r s be a source-side rule that pattern-matches a sub-phrase of s, r t be the target-side rule patternmatching a sub-phrase of t and being aligned to r s based on W , and C(r) be the context features related to the rule r which will be explained in the following section. 

 Model Training of CBSM and CBTM For the training of CBSM, if the SCFG rule r s , r t can be extracted based on the rule extraction algorithm in  (Chiang, 2005) , ? = 1, C(r s ) is constructed as a positive instance, otherwise ? = 0, C(r s ) is constructed as a negative instance. For example in Figure  1 (a), the context of source-side rule "X 1 hezuo" that pattern-matches the phrase "youhao hezuo" produces a positive instance, while the context of "X 1 youhao" that pattern-matches the source phrase "de youhao" or "shuangfang de youhao" will produce a negative instance as there are no corresponding plausible target-side rules that can be extracted legally 1 . For the training of CBTM, given r s , suppose there is a SCFG rule set { r s , r k t |1 ? k ? n} extracted from multiple distinct sentence pairs in the bilingual training corpus, among which we assume r s , r i t is extracted from the sentence pair s, t . Then, we construct ? = 1, C(r s ), C(r i t ) 1 Because the aligned target words are not contiguous and "cooperation" is aligned to the word outside the source-side rule. as a positive instance, while the elements in { ? = 0, C(r s ), C(r j t ) |j = i ? 1 ? j ? n} are viewed as negative instances since they fail to be applied to the translation from s to t. For example in  

 Context-based features for ME training ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work  Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009) : 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures. 2. POS features, which are POS tags of the boundary source words covered by nonterminals. 3. Syntactic features, which are the constituent constraints of hierarchical source-side rules exactly matching or crossing syntactic subtrees. 4. Rule format features, which are nonterminal positions and orders in sourceside/target-side rules. This feature interacts between source and target components since it shows whether the translation ordering is affected. 

 5. Length features, which are the length of sub-phrases covered by source nonterminals. 

 Experiments 

 Experiment setting We implement a hierarchical phrase-based system similar to the Hiero  (Chiang, 2005)  and evaluate our method on the Chinese-to-English translation task. Our bilingual training data comes from FBIS corpus, which consists of around 160K sentence pairs where the source data is parsed by the Berkeley parser  (Petrov and Klein, 2007) . The ME training toolkit, developed by  (Zhang, 2006) , is used to train our CBSM and CBTM. The training size of constructed positive instances for both CBSM and CBTM is 4.68M, while the training size of constructed negative instances is 3.74M and 3.03M respectively. Following  (Setiawan et al., 2009) , we identify function words as the 128 most frequent words in the corpus. The interpolation weights are set to ? = 0.75 and ? = 0.70. The 5-gram language model is trained over the English portion of FBIS corpus plus Xinhua portion of the Gigaword corpus. The development data is from NIST 2005 evaluation data and the test data is from NIST 2006 and NIST 2008 evaluation data. The evaluation metric is the case-insensitive BLEU4  (Papineni et al., 2002) . Statistical significance in BLEU score differences is tested by paired bootstrap re-sampling  (Koehn, 2004) . 

 Comparison with related work Our baseline is the implemented Hiero-like SMT system where only the standard features are employed and the performance is state-of-the-art. We compare our method with the baseline and some typical approaches listed in Table  1  where XP+ denotes the approach in  (Marton and Resnik, 2008)  and TOFW (topological ordering of function words) stands for the method in  (Setiawan et al., 2009) . As  (Xiong et al., 2009) 's work is based on phrasal SMT system with bracketing transduction grammar rules  (Wu, 1997)  and  (Shen et al., 2009) 's work is based on the string-to-dependency SMT model, we do not implement these two related work due to their different models from ours. We also do not compare with    1 , all the methods outperform the baseline because they have extra models to guide the hierarchical rule selection in some ways which might lead to better translation. Apparently, our method also performs better than the other two approaches, indicating that our method is more effective in the hierarchical rule selection as both source-side and target-side rules are selected together. 

 Effect of sub-models Due to the space limitation, we analyze the effect of sub-models upon the system performance, rather than that of ME features, part of which have been investigated in previous related work. As shown in Table  2 , when sub-models are inte-grated as independent features, the performance is improved compared to the baseline, which shows that each of the sub-models can improve the hierarchical rule selection. It is noticeable that the performance of the source-side rule selection model is comparable with that of the target-side rule selection model. Although CFSM and CFTM perform only slightly better than the others among the individual sub-models, the best performance is achieved when all the sub-models are integrated. 

 Conclusion Hierarchical rule selection is an important and complicated task for hierarchical phrase-based SMT system. We propose a joint probability model for the hierarchical rule selection and the experimental results prove the effectiveness of our approach. In the future work, we will explore more useful features and test our method over the large scale training corpus. A challenge might exist when running the ME training toolkit over a big size of training instances from the large scale training data. 3. 1 1 The acquisition of training instances CBSM and CBTM are trained by ME approach for the binary classification, where a training instance consists of a label and the context related to SCFG rules. The context is divided into source context 
