title
Variational Neural Machine Translation

abstract
Models of neural machine translation are often from a discriminative family of encoderdecoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neural machine translation: a variational encoderdecoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.

Introduction Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than phrase-or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016) . Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations 1  Sutskever et al., 2014; . Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism  to identify semantic alignments between source and target words. Due to potential errors in these alignments, the attention-based context vector may be insufficient to capture the entire meaning of a source sentence, hence resulting in undesirable translation phenomena  (Tu et al., 2016) . Unlike the vanilla encoder-decoder framework, we model underlying semantics of bilingual sentence pairs explicitly. We assume that there exists a continuous latent variable z from this underlying semantic space. And this variable, together with x, We use solid lines to denote the generative model p ? (z|x)p ? (y|z, x), and dashed lines to denote the variational approximation q ? (z|x) to the intractable posterior p(z|x, y). Both variational parameters ? and generative model parameters ? are learned jointly. guides the translation process, i.e. p  (y|z, x) . With this assumption, the original conditional probability evolves into the following formulation: p(y|x) = z p(y, z|x)d z = z p(y|z, x)p(z|x)d z (1) This brings in the benefits that the latent variable z can serve as a global semantic signal that is complementary to the attention-based context vector for generating good translations when the model learns undesirable attentions. However, although this latent variable enables us to explicitly model underlying semantics of translation pairs, the incorporation of it into the above probabilistic model has two challenges: 1) the posterior inference in this model is intractable; 2) large-scale training, which lays the ground for the data-driven NMT, is accordingly problematic. In order to address these issues, we propose a variational encoder-decoder model to neural machine translation (VNMT), motivated by the recent success of variational neural models . Figure  1  illustrates the graphic representation of VNMT. As deep neural networks are capable of learning highly nonlinear functions, we employ them to fit the latentvariable-related distributions, i.e. the prior and posterior, to make the inference tractable. The former is modeled to be conditioned on the source side alone p ? (z|x), because the source and target part of a sentence pair usually share the same semantics so that the source sentence should contain the prior information for inducing the underlying semantics. The latter, instead, is approximated from all observed variables q ? (z|x, y), i.e. both the source and the tar-get sides. In order to efficiently train parameters, we apply a reparameterization technique  on the variational lower bound. This enables us to use standard stochastic gradient optimization for training the proposed model. Specifically, there are three essential components in VNMT (The detailed architecture is illustrated in Figure  2 ): ? A variational neural encoder transforms source/target sentence into distributed representations, which is the same as the encoder of NMT  (see section 3.1). ? A variational neural inferer infers the representation of z according to the learned source representations (i.e. p ? (z|x)) together with the target ones (i.e. q ? (z|x, y)), where the reparameterization technique is employed (see section 3.2). ? And a variational neural decoder integrates the latent representation of z to guide the generation of target sentence (i.e. p(y|z, x)) together with the attention mechanism (see section 3.3). Augmented with the posterior approximation and reparameterization, our VNMT can still be trained end-to-end. This makes our model not only efficient in translation, but also simple in implementation. To train our model, we employ the conventional maximum likelihood estimation. Experiments on both Chinese-English and English-German translation tasks show that VNMT achieves significant improvements over several strong baselines. 

 Background: Variational Autoencoder This section briefly reviews the variational autoencoder (VAE) . Given an observed variable x, VAE introduces a continuous latent variable z, and assumes that x is generated from z, i.e., p ? (x, z) = p ? (x|z)p ? (z) (2) where ? denotes the parameters of the model. p ? (z) is the prior, e.g, a simple Gaussian distribution. p ? (x|z) is the conditional distribution that models the generation procedure, typically estimated via a deep non-linear neural network. Similar to our model, the integration of z in Eq. (2) imposes challenges on the posterior inference as well as large-scale learning. To tackle these problems, VAE adopts two techniques: neural approximation and reparameterization. reparameterization h z h ? z h ? e log ? 2 ? h f s 3 s 2 s 1 s 0 y 0 y 1 y 2 y 3 ? ? 2,1 ? 2,2 ? 2,3 ? 2,4 (a) Variational Neural Encoder (c) Variational Neural Decoder (b) Variational Neural Inferer mean-pooling ? ? h 1 ? ? h 1 ? ? h 2 ? ? h 2 ? ? h 3 ? ? h 3 ? ? h 4 ? ? h 4 x 4 x 3 x 2 x 1 mean-pooling y 1 y 2 y 3 ? ? h 3 ? ? h 3 ? ? h 2 ? ? h 2 ? ? h 1 ? ? h 1 h e Neural Approximation employs deep neural networks to approximate the posterior inference model q ? (z|x), where ? denotes the variational parameters. For the posterior approximation, VAE regards q ? (z|x) as a diagonal Gaussian N (?, diag(? 2 )), and parameterizes its mean ? and variance ? 2 with deep neural networks. Reparameterization reparameterizes z as a function of ? and ?, rather than using the standard sampling method. In practice, VAE leverages the "location-scale" property of Gaussian distribution, and uses the following reparameterization: z = ? + ? (3) where is a standard Gaussian variable that plays a role of introducing noises, and denotes an element-wise product. With these two techniques, VAE tightly incorporates both the generative model p ? (x|z) and the posterior inference model q ? (z|x) into an end-toend neural network. This facilitates its optimization since we can apply the standard backpropagation to compute the gradient of the following variational lower bound: L VAE (?, ?; x) = ? KL(q ? (z|x)||p ? (z)) +E q ? (z|x) [log p ? (x|z)] ? log p ? (x) (4) KL(Q||P ) is the Kullback-Leibler divergence between Q and P . Intuitively, VAE can be considered as a regularized version of the standard autoencoder. It makes use of the latent variable z to capture the variations in the observed variable x. 

 Variational Neural Machine Translation Different from previous work, we introduce a latent variable z to model the underlying semantic space as a global signal for translation. Formally, given the definition in Eq. (  1 ) and Eq. (  4 ), the variational lower bound of VNMT can be formulated as follows: L VNMT (?, ?; x, y) = ?KL(q ? (z|x, y)||p ? (z|x)) +E q ? (z|x,y) [log p ? (y|z, x)] (5) where p ? (z|x) is our prior model, q ? (z|x, y) is our posterior approximator, and p ? (y|z, x) is the decoder with the guidance from z. Based on this formulation, VNMT can be decomposed into three components, each of which is modeled by a neural network: a variational neural inferer that models p ? (z|x) and q ? (z|x, y) (see part (b) in Figure  2 ), a variational neural decoder that models p ? (y|z, x) (see part (c) in Figure  2 ), and a variational neural encoder that provides distributed representations of a source/target sentence for the above two modules (see part (a) in Figure  2 ). Following the information flow illustrated in Figure  2 , we describe part (a), (b) and (c) successively. 

 Variational Neural Encoder As shown in Figure  2  (a), the variational neural encoder aims at encoding an input sequence (w 1 , w 2 , . . . , w T ) into continuous vectors. In this paper, we adopt the encoder architecture proposed by , which is a bidirectional RNN with a forward and backward RNN. The forward RNN reads the sequence from left to right while the backward RNN in the opposite direction (see the parallel arrows in Figure  2  (a)): ? ? h i = RNN( ? ? h i?1 , E w i ) ? ? h i = RNN( ? ? h i+1 , E w i ) (6) where E w i ? R dw is the embedding for word w i , and ? ? h i , ? ? h i are hidden states generated in two directions. Following , we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing long-distance dependencies. We further concatenate each pair of hidden states at each time step to build a set of annotation vectors (h 1 , h 2 , . . . , h T ), h T i = ? ? h T i ; ? ? h T i . In this way, each annotation vector h i encodes information about the i-th word with respect to all the other surrounding words in the sequence. Therefore, these annotation vectors are desirable for the following modeling. We use this encoder to represent both the source sentence {x i } T f i=1 and the target sentence {y i } Te i=1 (see the blue color in Figure  2 ). Accordingly, our encoder generates both the source annotation vectors {h i } T f i=1 ? R 2d f and the target annotation vectors {h i } Te i=1 ? R 2de . The source vectors flow into the inferer and decoder while the target vectors the posterior approximator. 

 Variational Neural Inferer A major challenge of variational models is how to model the latent-variable-related distributions. In VNMT, we employ neural networks to model both the prior p ? (z|x) and the posterior q ? (z|x, y), and let them subject to a multivariate Gaussian distribution with a diagonal covariance structure. 2 As shown in Figure  1 , these two distributions mainly differ in their conditions. 

 Neural Posterior Approximator Exactly modeling the true posterior p(z|x, y) exactly usually intractable. Therefore, we adopt an approximation method to simplify the posterior inference. Conventional models typically employ the mean-field approaches. However, a major limitation of this approach is its inability to capture the true posterior of z due to its oversimplification. Following the spirit of VAE, we use neural networks for better approximation in this paper, and assume the approximator has the following form: q ? (z|x, y) = N (z; ?(x, y), ?(x, y) 2 I) (7) The mean ? and s.d. ? of the approximate posterior are the outputs of neural networks based on the observed variables x and y as shown in Figure  2 (b) . Starting from the variational neural encoder, we first obtain the source-and target-side representation via a mean-pooling operation over the annotation vectors, i.e. h f = 1 T f T f i h i , h e = 1 Te Te i h i . With these representations, we perform a non-linear transformation that projects them onto our concerned latent semantic space: h z = g(W (1) z [h f ; h e ] + b (1) z ) (8) where W (1) z ? R dz?2(d f +de) , b (1) z ? R dz is the parameter matrix and bias term respectively, d z is the dimensionality of the latent space, and g(?) is an element-wise activation function, which we set to be tanh(?) throughout our experiments. In this latent space, we obtain the abovementioned Gaussian parameters ? and log ? 2 through linear regression: ? = W ? h z + b ? , log ? 2 = W ? h z + b ? (9) where ?, log ? 2 are both d z -dimension vectors. 

 Neural Prior Model Different from the posterior, we model (rather than approximate) the prior as follows: p ? (z|x) = N (z; ? (x), ? (x) 2 I) (10) We treat the mean ? and s.d. ? of the prior as neural functions of source sentence x alone. This is sound and reasonable because bilingual sentences are semantically equivalent, suggesting that either y or x is capable of inferring the underlying semantics of sentence pairs, i.e., the representation of latent variable z. The neural model for the prior p ? (z|x) is the same as that (i.e. Eq (8) and (  9 )) for the posterior q ? (z|x, y), except for the absence of h e . Besides, the parameters for the prior are independent of those for the posterior. To obtain a representation for latent variable z, we employ the same technique as the Eq. (  3 ) and reparameterized it as h z = ? + ? , ? N (0, I). During decoding, however, due to the absence of target sentence y, we set h z to be the mean of p ? (z|x), i.e., ? . Intuitively, the reparameterization bridges the gap between the generation model p ? (y|z, x) and the inference model q ? (z|x, y). In other words, it connects these two neural networks. This is important since it enables the stochastic gradient optimization via standard backpropagation. We further project the representation of latent variable h z onto the target space for translation: h e = g(W (2) z h z + b (2) z ) (11) where h e ? R d e . The transformed h e is then integrated into our decoder. Notice that because of the noise from , the representation h e is not fixed for the same source sentence and model parameters. This is crucial for VNMT to learn to avoid overfitting. 

 Variational Neural Decoder Given the source sentence x and the latent variable z, our decoder defines the probability over translation y as a joint probability of ordered conditionals: p(y|z, x) = Te j=1 p(y j |y <j , z, x) (12) where p(y j |y <j ,z, x) = g (y j?1 , s j?1 , c j ) The feed forward model g (?) (see the yellow arrows in Figure  2 ) and context vector c j = i ? ji h i (see the "?" in Figure  2 ) are the same as . The difference between our decoder and Bahdanau et al.  's decoder (2014)  lies in that in addition to the context vector, our decoder integrates the representation of the latent variable, i.e. h e , into the computation of s j , which is denoted by the bold dashed red arrow in Figure  2 (c) . Formally, the hidden state s j in our decoder is calculated by 3 s j = (1 ? u j ) s j?1 + u j sj , sj = tanh(W E y j + U [r j s j?1 ] + Cc j + V h e ) u j = ?(W u E y j + U u s j?1 + C u c j + V u h e ) r j = ?(W r E y j + U r s j?1 + C r c j + V r h e ) Here, r j , u j , sj denotes the reset gate, update gate and candidate activation in GRU respectively, and E y j ? R dw is the word embedding for target word. W, W u , W r ? R de?dw , U, U u , U r ? R de?de , C, C u , C r ? R de?2d f , and V, V u , V r ? R de?d e are parameter weights. The initial hidden state s 0 is initialized in the same way as  (see the arrow to s 0 in Figure  2 ). In our model, the latent variable can affect the representation of hidden state s j through the gate between r j and u j . This allows our model to access the semantic information of z indirectly since the prediction of y j+1 depends on s j . In addition, when the model learns wrong attentions that lead to bad context vector c j , the semantic representation h e can help to guide the translation process . 

 Model Training We use the Monte Carlo method to approximate the expectation over the posterior in Eq. (  5 ), i.e. E q ? (z|x,y)  [?]  1 L L l=1 log p ? (y|x, h (l) z ), where L is the number of samples. The joint training objective for a training instance (x, y) is defined as follows: L(?, ?) ?KL(q ? (z|x, y)||p ? (z|x)) + 1 L L l=1 Te j=1 log p ? (y j |y <j , x, h (l) z ) (13) where h (l) z = ? + ? (l) and (l) ? N (0, I) The first term is the KL divergence between two Gaussian distributions which can be computed and differentiated without estimation (see (Kingma and Welling, 2014) for details). And the second term is the approximate expectation, which is also differentiable. Suppose that L is 1 (which is used in our experiments), then our second term will be degenerated to the objective of conventional NMT, where the introduced noise increases its robustness, and reduces overfitting. We verify this point in our experiments. Since the objective function in Eq. (  13 ) is differentiable, we can optimize the model parameter ? and variational parameter ? jointly using standard gradient ascent techniques. 

 Experiments 4.1 Setup To evaluate the effectiveness of the proposed VNMT, we conducted experiments on both Chinese-English and English-German translation tasks. Our Chinese-English training data 4 consists of 2.9M sentence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data 5 consists of 4.5M sentence pairs with 116M English words and 110M German words 6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4  (Papineni et al., 2002)  metric to evaluate translation quality, and paired bootstrap sampling  (Koehn, 2004)  for significance test. We compared our model against two state-of-theart SMT and NMT systems: ? Moses  (Koehn et al., 2007) : a phrase-based SMT system. ? GroundHog : an attention-based NMT system. Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting h z to ? . For Moses, we adopted all the default settings except for the language model. We trained a 4-gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM 7 toolkit with modified Kneser-Ney smoothing. Importantly, we used all words in the vocabulary. For GroundHog, we set the maximum length of training sentences to be 50 words, and preserved the most frequent 30K (Chinese-English) and 50K (English-German) words as both the source and target vocabulary , covering approximately 98.9%/99.2% and 97.3%/93.3% on the source and target side of the two parallel corpora respectively . All other words were represented by a specific token "UNK". Following , we set d w = 620, d f = 1000, d e = 1000, and M = 80. All other settings are the same as the default configuration (for RNNSearch). During decoding, we used the beam-search algorithm, and set beam size to 10. For VNMT, we initialized its parameters with the trained RNNSearch model. The settings of our model are the same as that of GroundHog, except for some parameters specific to VNMT. Following VAE, we set the sampling number L = 1. Additionally, we set d e = d z = 2d f = 2000 according to preliminary experiments. We used the Adadelta algorithm for model training with ? = 0.95. With regard to the source and target encoders, we shared their recurrent parameters but not word embeddings. We implemented our VNMT based on Ground-Hog 8 . Both NMT systems are trained on a Telsa K40   GPU. In one hour, GroundHog processes about 1100 batches, while our VNMT processes 630 batches. 

 Results on Chinese-English Translation Table  1  summarizes the BLEU scores of different systems on the Chinese-English translation tasks. Clearly VNMT significantly improves translation quality in terms of BLEU on most cases, and obtains the best average results that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively. Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog. These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network. https://github.com/DeepLearnXMU/VNMT. 

 Results on Long Sentences We further testify VNMT on long sentence translation where the vanilla NMT usually suffers from attention failures  (Tu et al., 2016; Bentivogli et al., 2016) . We believe that the global latent variable can play an important role on long sentence translation. Our first experiment is carried out on 6 disjoint groups according to the length of source sentences in our test sets. Figure  3  shows the BLEU scores of two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Specifically, on the final group with the longest source sentences, our VNMT obtains the biggest improvement (3.55 BLEU points). Overall, these obvious improvements on all groups in terms of the length of source sentences indicate that the global guidance from the latent variable benefits our VNMT model. Our second experiment is carried out on a synthetic dataset where each new source sentence is a concatenation of neighboring source sentences in the original test sets. As a result, the average length of source sentences in the new dataset (> 50) is almost twice longer than the original one. Translation results is summarized in Table  2 , where our VNMT obtains significant improvements on all new test sets. This further demonstrates the advantage of introducing the latent variable. 

 Results on English-German Translation Table  3  shows the results on English-German translation. We also provide several existing NMT sys- 527 Source ? ? ? ? ? ? ? ? ? ? ? , ? ? ? ? ? ? ? , ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? , ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? , ? ? ? ? ? ? ? ? Reference the officials of the two countries have established the mechanism for continued dialogue down the road, including a confirmed schedule and model of the talks. this symbolizes the restart of the dialogue process between pakistan and india after an interruption of two years and has paved a foundation for the two countries to sort out gradually all the questions hanging in the air, including the kashmir dispute. it is also a realization of their precious sincerity for peace. Moses officials of the two countries set the agenda for future talks , and the pattern of a continuing dialogue mechanism . this marks a break in the process of dialogue between pakistan and india , two years after the restart of the two countries including kashmir dispute to gradually solve all the outstanding issues have laid the foundation of the two sides showed great sincerity in peace . GroundHog the two countries have decided to set up a mechanism for conducting continuous dialogue on the agenda and mode of the talks . this indicates that the ongoing dialogue between the two countries has laid the foundation for the gradual settlement of all outstanding issues including the dispute over kashmir . VNMT the officials of the two countries set up a mechanism for holding a continuous dialogue on the agenda and mode of the future talks, and this indicates that the ongoing dialogue between pakistan and india has laid a foundation for resolving all outstanding issues , including the kashmir disputes , and this serves as a valuable and sincere peace sincerity . tems that use the same training, development and testing data. The results show that VNMT significantly outperforms GroundHog and achieves a significant gain of 0.73 BLEU points (p < 0.01). With unknown word replacement  (Jean et al., 2015; Luong et al., 2015a) , VNMT reaches the performance level that is comparable to the previous state-of-theart NMT results. 

 Translation Analysis Table  4  shows a translation example that helps understand the advantage of VNMT over NMT . As the source sentence in this example is long (more than 40 words), the translation generated by Moses is relatively messy and incomprehensible. In contrast, translations generated by neural models (both GroundHog and VNMT) are much more fluent and comprehensible. However, there are essential differences between GroundHog and our VNMT. Specifically, GroundHog does not translate the phrase "? ?" at the beginning of the source sentence. The translation of the clause "? ? ? ? ? ? ? ? ?" at the end of the source sentence is completely lost. In contrast, our VNMT model does not miss or mistake these fragments and can convey the meaning of entire source sentence to the target side. From these examples, we can find that although attention networks can help NMT trace back to relevant parts of source sentences for predicting target translations, capturing the semantics of entire sentences still remains a big challenge for neural machine translation. Since NMT implicitly models variable-length source sentences with fixed-size hidden vectors, some details of source sentences (e.g., the red sequence of words in Table  4 ) may not be encoded in these vectors at all. VNMT seems to be able to capture these details through a latent variable that explicitly model underlying semantics of source sentences. The promising results suggest that VNMT provides a new mechanism to deal with sentence semantics. 5 Related Work 

 Neural Machine Translation Neural machine translation starts from the sequence to sequence learning, where  Sutskever et al. (2014)  employ two multilayered Long Short-Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a special end token is generated. In order to deal with issues caused by encoding all source-side information into a fixed-length vector,  introduce attention-based NMT that aims at automatically concentrating on relevant source parts for predicting target words during decoding. The incorporation of attention mechanism allows NMT to cope better with long sentences, and makes it really comparable to or even superior to conventional SMT. Following the success of attentional NMT, a number of approaches and models have been proposed for NMT recently, which can be grouped into different categories according to their motivations: dealing with rare words or large vocabulary  (Jean et al., 2015; Luong et al., 2015b; Sennrich et al., 2015) , learning better attentional structures  (Luong et al., 2015a) , integrating SMT techniques  (Cheng et al., 2015; Shen et al., 2015; Feng et al., 2016; Tu et al., 2016) , memory network  (Meng et al., 2015) , etc. All these models are designed within the discriminative encoder-decoder framework, leaving the explicit exploration of underlying semantics with a variational model an open problem. 

 Variational Neural Model In order to perform efficient inference and learning in directed probabilistic models on large-scale dataset, Kingma and Welling (2014) as well as  introduce variational neural networks. Typically, these models utilize an neural inference model to approximate the intractable posterior, and optimize model parameters jointly with a reparameterized variational lower bound using the standard stochastic gradient technique. This approach is of growing interest due to its success in various tasks.  revisit the approach to semisupervised learning with generative models and further develop new models that allow effective generalization from a small labeled dataset to a large unlabeled dataset.  Chung et al. (2015)  incorporate latent variables into the hidden state of a recurrent neural network, while  Gregor et al. (2015)  combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images. Very recently,  Miao et al. (2015)  propose a generic variational inference framework for generative and conditional models of text. The most related work is that of  Bowman et al. (2015) , where they develop a variational autoencoder for unsupervised generative language modeling. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although variational neural models have been widely used in NLP tasks and the variational decoding has been investigated for SMT  (Li et al., 2009) , the adaptation and utilization of variational neural model to neural machine translation, to the best of our knowledge, has never been investigated before. 

 Conclusion and Future Work In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables our model to be an end-to-end neural network that can be optimized through the stochastic gradient algorithms. Comparing with the conventional attention-based NMT, our model is better at translating long sentences. It also greatly benefits from a special regularization term brought with this latent variable. Experiments on Chinese-English and English-German translation tasks verified the effectiveness of our model. In the future, since the latent variable in our model is at the sentence level, we want to explore more fine-grained latent variables for neural machine translation, such as the Recurrent Latent Variable Model  (Chung et al., 2015) . We are also interested in applying our model to other similar tasks. Figure 1 : 1 Figure 1: Illustration of VNMT as a directed graph.We use solid lines to denote the generative model p ? (z|x)p ? (y|z, x), and dashed lines to denote the variational approximation q ? (z|x) to the intractable posterior p(z|x, y). Both variational parameters ? and generative model parameters ? are learned jointly. 
