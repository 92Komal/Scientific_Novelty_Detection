title
Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages

abstract
Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved. "Lowresourced"-ness is a complex problem going beyond data availability and reflects systemic problems in society. * ? to represent the whole Masakhane community. As MT researchers cannot solve the problem of low-resourcedness alone, we propose participatory research as a means to involve all necessary agents required in the MT development process. We demonstrate the feasibility and scalability of participatory research with a case study on MT for African languages. Its implementation leads to a collection of novel translation datasets, MT benchmarks for over 30 languages, with human evaluations for a third of them, and enables participants without formal training to make a unique scientific contribution. Benchmarks, models, data, code, and evaluation results are released at https://github. com/masakhane-io/masakhane-mt.

In this paper, we focus on the task of Machine Translation (MT), that plays a crucial role for information accessibility and communication worldwide. Despite immense improvements in MT over the past decade, MT is centered around a few highresourced languages. 

 Introduction Language prevalence in societies is directly bound to the people and places that speak this language. Consequently, resource-scarce languages in an NLP context reflect the resource scarcity in the society from which the speakers originate  (McCarthy, 2017) . Through the lens of a machine learning researcher, "lowresourced" identifies languages for which few digital or computational data resources exist, often classified in comparison to another language  (Gu et al., 2018; Zoph et al., 2016) . However, to the sociolinguist, "low-resourced" can be broken down into many categories: low density, less commonly taught, or endangered, each carrying slightly different meanings  (Cieri et al., 2016) . In this complex definition, the "low-resourced"-ness of a language is a symptom of a range of societal problems, e.g. authors oppressed by colonial governments have been imprisoned for writing novels in their languages impacting the publications in those languages  (Wa Thiong'o, 1992) , or that fewer PhD candidates come from oppressed societies due to low access to tertiary education  (Jowi et al., 2018) . This results in fewer linguistic resources and researchers from those regions to work on NLP for their language. Therefore, the problem of "lowresourced"-ness relates not only to the available resources for a language, but also to the lack of geographic and language diversity of NLP researchers themselves. The NLP community has awakened to the fact that it has a diversity crisis in terms of limited geographies and languages  (Caines, 2019; Joshi et al., 2020) : Research groups are extending NLP research to low-resourced languages  (Guzm?n et al., 2019; Hu et al., 2020; Wu and Dredze, 2020) , and workshops have been established  (Haffari et al., 2018; Axelrod et al., 2019; . We scope the rest of this study to machine translation (MT) using parallel corpora only, and refer the reader to  Joshi et al. (2019)  for an assessment of low-resourced NLP in general. Contributions. We diagnose the problems of MT systems for low-resourced languages by reflecting on what agents and interactions are necessary for a sustainable MT research process. We identify which agents and interactions are commonly omitted from existing low-resourced MT research, and assess the impact that their exclusion has on the research. To involve the necessary agents and facilitate required interactions, we propose participatory research to build sustainable MT research communities for low-resourced languages. The feasibility and scalability of this method is demonstrated with a case study on MT for African languages, where we present its implementation and outcomes, including novel translation datasets, benchmarks for over 30 target languages contributed and evaluated by language speakers, and publications authored by participants without formal training as scientists. 

 Background Cross-lingual Transfer. With the success of deep learning in NLP, language-specific feature design has become rare, and cross-lingual transfer methods have come into bloom  (Upadhyay et al., 2016; Ruder et al., 2019)  to transfer progress from high-resourced to low-resourced languages  (Adams et al., 2017; Wang et al., 2019; Kim et al., 2019) . The most diverse benchmark for multilingual transfer by  Hu et al. (2020)  allows measurement of the success of such transfer approaches across 40 languages from 12 language families. However, the inclusion of languages in the set of benchmarks is dependent on the availability of monolingual data for representation learning with previously annotated resources. The content of the benchmark tasks is English-sourced, and human performance estimates are taken from English. Most cross-lingual representation learning techniques are Anglo-centric in their design  (Anastasopoulos and Neubig, 2019) . 

 Multilingual Approaches. Multilingual MT  (Dong et al., 2015; Firat et al., 2016a,b; Wang et al., 2020)  addresses the transfer of MT from high-resourced to low-resourced languages by training multilingual models for all languages at once.  (Aharoni et al., 2019; Arivazhagan et al., 2019)  train models to translate between English and 102 languages, for the 10 most high-resourced African languages on private data, and otherwise on public TED talks  (Qi et al., 2018) . Multilingual training often outperforms bilingual training, especially for low-resourced languages. However, with multilingual parallel data being also Anglo-centric, the capabilities to translate from English versus into English vastly diverge  (Zhang et al., 2020) . Another recent approach, mBART  (Liu et al., 2020) , leverages both monolingual and parallel data and also yields improvements in translation quality for lower-resource languages such as Nepali, Sinhala and Gujarati.  3  While this provides a solution for small quantities of training data or monolingual resources, the extent to which standard BLEU evaluations reflect translation quality is not clear yet, since human evaluation studies are missing. Targeted Resource Creation.  Guzm?n et al. (2019)  develop evaluation datasets for lowresourced MT between English and Nepali, Sinhala, Khmer and Pashtolow. They highlight many problems with low-resourced translation: tokenization, content selection, and translation verification, illustrating increased difficulty translating from English into lowresourced languages, and highlight the ineffectiveness of accepted state-of-the-art techniques on morphologically-rich languages. Despite involving all agents of the MT process (Section 3), the study does not involve data curators or evaluators that understood the languages involved, and resorts to standard MT evaluation metrics. Additionally, how this effort-intensive approach would scale to more than a handful of languages remains an open question. 

 The Machine Translation Process We reflect on the process enabling a sustainable process for MT research on parallel corpora in terms of the required agents and interactions, visualized in Figure  1 . Content creators, translators, and curators form the dataset creation process, while the language technologists and evaluators are part of the model creation process. Stakeholders (not displayed) create demand for both processes. Stakeholders are people impacted by the artifacts generated by each agent in the MT process, and can typically speak and read the source or the target languages. To benefit from MT systems, the stakeholders need access to technology and electricity. Content Creators produce content in a language, where content is any digital or nondigital representation of language. For digi-tal content, content creators require keyboards, and access to technology. Translators translate the original content, including crowd-workers, researchers, or translation professionals. They must understand the language of the content creator and the target language. A translator needs content to translate, provided by content creators. For digital content, the translator requires keyboards and technology access. Curators are defined as individuals involved in the content selection for a dataset  (Bender and Friedman, 2018) , requiring access to content and translations. They should understand the languages in question for quality control and encoding information. Language Technologists are defined as individuals using datasets and computational linguistic techniques to produce MT models between language pairs. Language technologists require language preprocessors, MT toolkits, and access to compute resources. Evaluators are individuals who measure and analyse the performance of a MT model, and therefore need knowledge of both source and target languages. To report on the performance on models, evaluators require quality metrics, as well as evaluation datasets. Evaluators provide feedback to the Language Technologists for improvement. 

 Limitations of Existing Approaches If we place a high-resource MT pair such as English-to-French into the process defined above, we observe that each agent nowadays has the necessary resources and historical stakeholder demand to perform their role effectively. A "virtuous cycle" emerged where available content enabled the development of MT systems that in turn drove more translations, more tools, more evaluation and more content, which cycled back to improving MT systems. By contrast, parts of the process for existing low-resourced MT are constrained. Historically, many low-resourced languages had low demand from stakeholders for content creation and translation  (Wa Thiong'o, 1992) . Due to missing keyboards or limited access to technology, content creators were not empowered to write digital content  (Adam, 1997; van Esch et al., 2019) . This is a chicken-oregg problem, where existing digital content in a language would attract more stakeholders, which would incentivize content creators  (Kaffee et al., 2018) . As a result, primary data sources for NLP research, such as Wikipedia, often have a few hundred articles only for lowresourced languages despite large speaker populations, see Table  1 . Due to limited demand, existing translations are often domain-specific and small in size, such as the JW300 corpus  (Agi? and Vuli?, 2019)  whose content was created for missionary purposes. When data curators are not part of the societies from where these languages originate, they are are often unable to identify data sources or translators for languages, prohibiting them from checking the validity of the created resource. This creates problems in en-coding, orthography or alignment, resulting in noisy or incorrect translation pairs  (Taylor et al., 2015) . This is aggravated by the fact that many low-resourced languages do not have a long written history to draw from and therefore might be less standardized and using multiple scripts. In collaboration with content creators, data curators can contribute to standardization or at least recognize potential issues for data processing further down the line. As discussed in Section 1, language technologists are fewer in low-resourced societies. Furthermore, the techniques developed in highresourced societies might be inapplicable due to compute, infrastructure or time constraints. Aside from the problem of education and complexity, existing techniques may not apply due to linguistic and morphological differences in the languages, or the scale, domain, or quality of the data  (Hu et al., 2020; Pires et al., 2019) . Evaluators usually resort to potentially unsuitable automatic metrics due to time constraints or missing connections to stakeholders  (Guzm?n et al., 2019) . The main evaluators of low-resourced NLP that is developed today typically cannot use human metrics due to the inability to speak the languages, or the lack of reliable crowdsourcing infrastructure, identified as one of the core weaknesses of previous approaches (in Section 2). In summary, many agents in the MT process for low-resourced languages are either missing invaluable language and societal knowledge, or the necessary technical resources, knowledge, connections, and incentives to form interactions with other agents in the process. 

 Participatory Research Approach We propose one way to overcome the limitations in Section 3.1: ensuring that the agents in the MT process originate from the countries where the low-resourced languages are spoken or can speak the low-resourced lan-guages. Where this condition cannot be satisfied, at least a knowledge transfer between agents should be enabled. We hypothesize that using a participatory approach will allow researchers to improve the MT process by iterating faster and more effectively. Participatory research, unlike conventional research, emphasizes the value of research partners in the knowledge-production process where the research process itself is defined collaboratively and iteratively. The "participants" are individuals involved in conducting research without formal training as researchers. Participatory research describes a broad set of methodologies, organised in terms of the level of participation. At the lowest level is crowd-sourcing, where participants are involved solely in data collection. The highest level-extreme citizen science-involves participation in the problem definition, data collection, analysis and interpretation  (English et al., 2018) . Crowd-sourcing has been applied to lowresourced language data collection  Guevara-Rukoz et al., 2020; Millour and Fort, 2018) , but existing studies highlight how the disconnect between the data creation process and model creation process causes challenges. In seeking to create crossdisciplinary teams that emphasize the values in a societal context, a participatory approach which involves participants in every part of the scientific process appears pertinent to solving the problems for low-resourced languages highlighted in Section 3.1. To show how more involved participatory research can benefit low-resource language translation, we present a case study in MT for African languages. 

 Case Study: Masakhane Africa currently has 2144 living languages  (Eberhard et al., 2019) . Despite this, African languages account for a small fraction of available language resources, and NLP research rarely considers African languages. In the taxonomy of  Joshi et al. (2020) , African languages are assigned categories ranging from "The Left Behinds" to "The Rising Stars", with most languages not having any annotated data. Even monolingual resources are sparse, as shown in Table  1 . In addition to a lack of NLP datasets, the African continent lacks NLP researchers. In 2018, only five out of the 2695 affiliations of the five major NLP conferences were from African institutions  (Caines, 2019) . ? et al. (  2020 ) attribute this to a culmination of circumstances, in particular their societal embedding  (Alexander, 2009)  and socio-economic factors, hindering participation in research activities and events, leaving researchers disconnected and distributed across the continent. Consequently, existing data resources are harder to discover, especially since these are often published in closed journals or are not digitized  (Mesthrie, 1995) . For African languages, the implementation of a standard crowd-sourcing pipeline as for example used for collecting task annotations for English, is at the current stage infeasible, due to the challenges outlined in Section 3 and above. Additionally, no standard MT evaluation set for all of the languages in focus exists, nor are there prior published systems that we could compare all models against for a more insightful human evaluation. We therefore resort to intrinsic evaluation, and rely on this work becoming the first benchmark for future evaluations. We invite the reader to adopt a metaperspective of this case study as an empirical experiment: Where the hypothesis is that participatory research can facilitate low-resourced MT development; the experimental methodology is the strategies and tools employed to bring together distributed participants, enabling each language speaker to train, contribute, and evaluate their models. The experiment is evaluated in terms of the quantity and diversity of participants and languages, and the variety of research artifacts, in terms of benchmarks, human evaluations, publications, and the overall health of the community. While a set of novel human evaluation results are presented, they serve as demonstration of the value of a participatory approach, rather than the empirical focus of the paper. 

 Methodology To overcome the challenge of recruiting participants, a number of strategies were employed. Starting from local demand at a machine learning school (Deep Learning Indaba  (Engelbrecht, 2018) ), meetups and universities, distant connections were made through Twitter, conference workshops, 4 and eventually press coverage 5 and research publications.  6  To overcome the limited tertiary education enrollments in Sub-Saharan Africa  (Jowi et al., 2018) , no prerequisites were placed on researchers joining the project. For the agents outlined in Section 3, no fixed roles are imposed onto participants. Instead, they join with a specific interest, background, or skill aligning them best to one or more of agents. To obtain crossdisciplinarity, we focus on the communication and interaction between participants to enable knowledge transfer between missing connections (identified in Section 3.1), allowing a fluidity of agent roles. For example, someone who initially joined with the interest of using machine translation for their local language (as a stakeholder) to translate education material, might turn into a junior language technologist when equipped with tools and introductory material and mentoring, and guide content creation more specifically for resources needed for MT. To bridge large geographical divides, the community lives online. Communication occurs on GitHub and Slack with weekly video conference meetings and reading groups. Meeting notes are shared openly so that continuous participation is not required and time commitment can be organized individually. Subinterest groups have emerged in Slack channels to allow focused discussions. Agendas for meetings and reading groups are public and democratically voted upon. In this way, the research questions evolve based on stakeholder demands, rather than being imposed upon by external forces. The lack of compute resources and prior exposure to NLP is overcome by providing tutorials for training a custom-size Transformer model with JoeyNMT  (Kreutzer et al., 2019)  on Google Colab 7 . International researchers were not prohibited from joining. As a result, mutual mentorship relations emerged, whereby international researchers with more language technology experience guided research efforts and enabled data curators or translators to become language technologists. In return, African researchers introduced the international language technologists to African stakeholders, languages and context. 

 Research Outcomes Participants. A growth to over 400 participants of diverse disciplines, from at least 20 countries, has been achieved within the past year, suggesting the participant recruitment process was effective. Appendix A contains 7 https://colab.research.google.com detailed demographics of a subset of participants from a voluntary survey in February 2020. 86.5% of participants responded positively when asked if the community helped them find mentors or collaborators, indicating that the health of the community is positive. This is also reflected in joint research publications of new groups of collaborators. Research Artifacts. As a result of mentorship and knowledge exchange between agents of the translation process, our implementation of participatory research has produced artifacts for NLP research, namely datasets, benchmarks and models, which are publicly available online. 8 . Additionally, over 10 participants have gone on to publish works addressing language-specific challenges at conference workshops, such as  (Dossou and Emezue, 2020; Orife, 2020; ?ktem et al., 2020; Martinus et al., 2020; Marivate et al., 2020) . Dataset Creation. The dataset creation process is ongoing, with new initiatives still emerging. We showcase a few initiatives below to demonstrate how bridging connections between agents facilitates the MT process. 1. A team of Nigerian participants, driven by the internal demand to ensure that accessible and representative data of their culture is used to train models, are translating their own writings including personal religious stories and undergraduate theses into Yoruba and Igbo 9 . 2. A Namibian participant, driven by a passion to preserve the culture of the Damara, is hosting collaborative sessions with Damara speakers, to collect and translate phrases that reflect Damara culture around traditional clothing, songs, and prayers. 10 3. Creating a connection between a translator in South-Africa's parliament and a language technologist has enabled the process of data curation, allowing access to data from the parliament in South-Africa's languages (which are public but obfuscated behind internal tools). 11 . These stories demonstrate the value of including curators, content creators, and translators as participants. Benchmarks. We publish 45 benchmarks for neural translation models from English into 32 distinct African languages, and from French into two additional languages, as well as from English into three different languages. 12 Most were trained on the JW300 corpus  (Agi? and Vuli?, 2019) . From this corpus, we select the English sentences most commonly found (and longer than 4 tokens) in all languages, as a global set of test sources. For individual languages, test splits are composed by selecting the translations that are available from this subset. While this biases the test set towards frequent segments, it prevents crosslingual overlap between training and test data which has to be ensured for cross-lingual transfer learning. For training data, other sources like Autshumato  (McKellar, 2014) , TED  (Cettolo et al., 2012) , SAWA  (De Pauw et al., 2009) , Tatoeba 13 , Opus  (Tiedemann, 2012) , and data translated or curated by participants were added. Language pairs were selected based on the individual demands of each of the 32 participants, who voluntarily contributed the benchmarks they valued most. 16 of the selected target languages are categorized as "Left-behind" and 11 are categorized as "Scraping by" in the taxonomy of  (Joshi et al., 2020) . The benchmarks are hosted publicly, including model weights, configurations and preprocessing pipelines for full reproducibility. The benchmarks are submitted by individual or groups of participants in form of a GitHub Pull Request. By this, we ensure that the contact to the benchmark contributors can be made, and ownership is experienced. 

 Human MT Evaluation To our knowledge, there is no prior research on human evaluation specifically for machine translations of low-resourced languages. Until now, NLP practitioners were left with the hope that successful evaluation methodologies for high-resource languages would transfer well to low-resourced languages. This lack of study is due to the missing connections between the community of speakers (content creators and translators), and the language technologists. MT evaluations by humans are often done either within a group of researchers from the same lab or field (e.g. for WMT evaluations 14 ), or via crowdsourcing platforms  Post et al., 2012) . Speakers of low-resource languages are traditionally underrepresented in these groups, which makes such studies even harder  (Joshi et al., 2019; Guzm?n et al., 2019) . One might argue that human evaluation should not be attempted before reaching a viable state of quality, but we found that early evaluation results in an improved understanding of the individual challenges of the target languages, strengthens the network of the community, and most importantly, improves the connection and knowledge transfer between language technologists, content creators and curators. The "low-resourced"-ness of the addressed languages pose challenges for evaluation beyond interface design or recruitment of evaluators proficient in the target language. For the example of Igbo, evaluators had to find solutions for typing diacritics without a suitable keyboard. In addition, Igbo has many dialects and variations which the MT model is uninformed of. Medical or technical terminology (e.g., "data") is difficult to translate and whether to use loan words required discussion. Target language news websites were found to be useful for resolving standardization or terminology questions. Solutions for each language were shared and often also applicable for other languages. Data. The models are trained on JW300 data.  15  To gain real-world quality estimates beyond religious context, we assess the models' out-of-domain generalization by translating a English COVID-19 survey with 39 questions and statements regarding COVID-19,  16  where the human-corrected and approved translations can directly serve the purpose of gathering responses. The domain is challenging as it contains medical terms and new vocabulary. Furthermore, we evaluate a subset of the Multitarget TED test data  (Duh, 2018)    17  . The obtained translations enrich the TED datasets, adding new languages for which no prior translations exist. The size of the TED evaluations vary from 30 to 120 sentences. Details are given in Table  3 , Appendix B. Evaluators. 11 participants of the community volunteered to evaluate translations in their language(s), often involving family or friends to determine the most correct translations. The evaluator role is therefore taken by both stakeholders and language technologists. Within only 10 days, we gathered a total of 707 evaluated translations covering Igbo (ig), Nigerian Pidgin (pcm), Shona (sn), Luo (luo), Hausa (ha, twice by two different annotators), Kiswahili (sw), Yoruba (yo), Fon (fon) and Dendi (ddn). We did not impose prescriptions in terms of number of sentences to evaluate, or time to spend, since this was voluntary work, and guidelines or estimates for the evaluation of translations into these languages are non-existent. Evaluation Technique. Instead of a direct assessment  (Graham et al., 2013)  often used in benchmark MT evaluations  (Barrault et al., 2019; Guzm?n et al., 2019) , we opt for postediting. Post-edits are grounded in actions that can be analyzed in terms of e.g. error types for further investigations, while direct assessments require expensive calibration  (Bentivogli et al., 2018) . Embedded in the community, these post-edit evaluations create an asset for the interaction of various agents: for the language technologists for domain adaptation, or for the content creators, curators, or translators for guidance in standardization or domain choice. Results. Table  2  reports evaluation results in terms of BLEU evaluated on the benchmark test set from JW300, and human-targeted TER (HTER)  (Snover et al., 2006) , BLEU  (Papineni et al., 2002)  and ChrF  (Popovi?, 2015)  against human corrected model translations. For ha we find modest agreement between evaluators: Spearman's ? = 0.56 for sentence-BLEU measurements of the post-edits compared to the original hypotheses. Generally, we observe that the JW300 score is misleading, overestimating model quality (except yo). Training data size appears to be a more reliable predictor of generalization abilities, illustrating the danger of chasing a single benchmark. However, ig and yo both have comparable amounts   (Post, 2018) . of training data, JW300 scores, and carry diacritics, but exhibit very different evaluation performances, in particular on COVID. This can be explained by the large variations of ig as discussed above: Training data and model output are not consistent with respect to one dialect, while the evaluator had to decide on one. We also find difference in performance across domains, with the TED domain appearing easier for pcm and ig, while the yo model performs better on COVID. 

 Conclusion We proposed a participatory approach as a solution to sustainably scaling NLP research to low-resourced languages. Having identified key agents and interactions in the MT development process, we implement a participatory approach to build a community for African MT. In the process, we discovered successful strategies for distributed growth and communication, knowledge sharing and model building. In addition to publishing benchmarks and datasets for previously understudied languages, we show how the participatory design of the community enables us to conduct a human evaluation study of model outputs, which has been one of the limitations of previous approaches to low-resourced NLP. The sheer volume and diversity of participants, languages and outcomes, and that for many for languages featured, this paper constitutes the first time that human evaluation of an MT system has been performed, is evidence of the value of participatory approaches for low-resourced MT. For future work, we will (1) continue to iterate, analyze and widen our benchmarks and evaluations, (2) build richer and more meaningful datasets that reflect priorities of the stakeholders, (3) expand the focus of the existing community for African languages to other NLP tasks, and (4) help implement similar communities for other geographic regions with lowresourced languages. Figure 1 : 1 Figure 1: The MT Process, in terms of the necessary agents, interactions and external constraints and demand (excluding stakeholders). 
