title
Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation

abstract
This paper proposes a nonparametric Bayesian method for inducing Part-of-Speech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model  (Finkel et al., 2007)  to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.

Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing  (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010)  or constituency parsing  (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010;  on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure  1 . The Japanese noun "?" in  Example 1 corresponds to the English verb "use", while that in Example 2 corresponds to the English noun "usage". Thus, Japanese nouns act like verbs in English in one situation, and nouns in English in another. If we could discriminate POS tags for two cases, we might improve the performance of a Japanese-to-English SMT system. In the face of the above situations, this paper proposes an unsupervised method for inducing POS tags for SMT, and aims to improve the performance of syntax-based SMT by utilizing the induced POS tagset. The proposed method is based on the infinite tree model proposed by  Finkel et al. (2007) , which is a nonparametric Bayesian method for inducing POS tags from syntactic dependency structures. In this model, hidden states represent POS tags, the observations they generate represent the words themselves, and tree structures represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS tags based on bilingual observations, both models can induce POS tags by incorporating information from the other language. Consider, for example, inducing a POS tag for the Japanese word " ?" in Figure  1 . Under a monolingual induction method (e.g., the infinite tree model), the "?" in Example 1 and 2 would both be assigned the same POS tag since they share the same observation. However, our models would assign separate tags for the two different instances since the "? ?" in Example 1 and Example 2 could be disambiguated by encoding the target-side information, either "use" or "usage", in the observations. Inference is efficiently carried out by beam sampling  (Gael et al., 2008) , which combines slice sampling and dynamic programming. Experiments are carried out on the NTCIR-9 Japaneseto-English task using a binarized forest-to-string SMT system with dependency trees as its source side. Our bilingually-induced tagset significantly outperforms the original tagset and the monolingually-induced tagset. Further, our independent model achieves a more than 1 point gain in BLEU, which resolves the sparseness problem introduced by the bi-word observations. 

 Related Work A number of unsupervised methods have been proposed for inducing POS tags. Early methods have the problem that the number of possible POS tags must be provided preliminarily. This limitation has been overcome by automatically adjusting the number of possible POS tags using nonparametric Bayesian methods  (Finkel et al., 2007; Gael et al., 2009; Blunsom and Cohn, 2011; Sirts and Alum?e, 2012) .  Gael et al. (2009)  applied infinite HMM (iHMM)  (Beal et al., 2001; Teh et al., 2006) , a nonparametric version of HMM, to POS induction.  Blunsom and Cohn (2011)  used a hierarchical Pitman-Yor process prior to the transition and emission distribution for sophisticated smoothing.  Sirts and Alum?e (2012)  built a model that combines POS induction and morphological segmentation into a single learning problem.  Finkel et al. (2007)  proposed the infinite tree model, which represents recursive branching structures over infinite hidden states and induces POS tags from syntactic dependency structures. In the following, we overview the infinite tree model, which is the basis of our proposed model. In particular, we will describe the independent children   (Finkel et al., 2007) , where children are dependent only on their parents, used in our proposed model 1 . 

 Finite Tree Model We first review the finite tree model, which can be graphically represented in Figure  2 . Let T t denote the tree whose root node is t. A node t has a hidden state z t (the POS tag) and an observation x t (the word). The probability of a tree T t , p T (T t ), is recursively defined: p T (T t ) = p(x t |z t ) ? t ? ?c(t) p(z t ? |z t )p T (T t ? ), where c(t) is the set of the children of t. Let each hidden state variable have C possible values indexed by k. For each state k, there is a parameter ? k which parameterizes the observation distribution for that state: x t |z t ? F (? zt ). ? k is distributed according to a prior distribution H: ? k ? H. Transitions between states are governed by Markov dynamics parameterized by ?, where ? ij = p(z c(t) = j|z t = i) and ? k are the transition probabilities from the parent's state k. ? k is distributed according to a Dirichlet distribution with parameter ?: ? k |? ? Dirichlet(?, . . . , ?). The hidden state of each child z t ? is distributed according to a multinomial distribution ? zt specific to the parent's state z t : z t ? |z t ? Multinomial(? zt ). 

 Infinite Tree Model In the infinite tree model, the number of possible hidden states is potentially infinite. The infinite model is formed by extending the finite tree model using a hierarchical Dirichlet process (HDP)  (Teh et al., 2006) . The reason for using an HDP rather   (Ferguson, 1973 ) is that we have to introduce coupling across transitions from different parent's states. A similar measure was adopted in iHMM  (Beal et al., 2001) . H ?k ? ? ? ?k ?0 z1 z2 z3 x1 x2 x3 ? ? ? ? ? ? H k k ~) , ( DP , | ) ( GEM | 0 0 ? ? ? ? ? ? ? ? ? HDP is a set of DPs coupled through a shared random base measure which is itself drawn from a DP: each G k ? DP(? 0 , G 0 ) with a shared base measure G 0 , and G 0 ? DP(?, H) with a global base measure H. From the viewpoint of the stickbreaking construction 3  (Sethuraman, 1994) , the HDP is interpreted as follows: G 0 = ? ? k ? =1 ? k ? ? ? k ? and G k = ? ? k ? =1 ? kk ? ? ? k ? , where ? ? GEM(?), ? k ? DP(? 0 , ?), and ? k ? ? H. We regard each G k as two coindexed distributions: ? k , a distribution over the transition probabilities from the parent's state k, and ? k ? , an observation distribution for the state k ? . Then, the infinite tree model is formally defined as follows: ?|? ? GEM(?), ? k |? 0 , ? ? DP(? 0 , ?), ? k ? H, z t ? |z t ? Multinomial(? zt ), x t |z t ? F (? zt ). Figure  3  shows the graphical representation of the infinite tree model. The primary difference be-2 DP is a measure on measures. It has two parameters, a scaling parameter ? and a base measure H: DP (?, H). 3  Sethuraman (1994)  showed a definition of a measure G ? DP(?0, G0). First, infinite sequences of i.i.d variables (? ? k ) ? k=1 and (? k ) ? k=1 are generated: ? ? k |?0 ? Beta(1, ?0), ? k ? G0. Then, G is defined as: ? k = ? ? k ? k?1 l=1 (1 ? ? ? l ), G = ? ? k=1 ? k ? ? k . If ? is defined by this process, then we write ? ? GEM(?0).  

 Bilingual Infinite Tree Model We propose a bilingual variant of the infinite tree model, the bilingual infinite tree model, which utilizes information from the other language. Specifically, the proposed model introduces bilingual observations by embedding the aligned target words in the source-side dependency trees. This paper proposes two types of models that differ in their processes for generating observations: the joint model and the independent model. 

 Joint Model The joint model is a simple application of the infinite tree model under a bilingual scenario. The model is formally defined in the same way as in Section 2.2 and is graphically represented similarly to Figure  3 . The only difference from the infinite tree model is the instances of observations (x t ). Observations in the joint model are the combination of source words and their aligned target words 4 , while observations in the monolingual infinite tree model represent only source words. For each source word, all the aligned target words are copied and sorted in alphabetical order, and then concatenated into a single observation. Therefore, a single target word may be emitted multiple times if the target word is aligned with multiple source words. Likewise, there may be target words which may not be emitted by our model, if the target words are not aligned. Figure  4  shows the process of generating Example 2 in Figure  1  through the joint model, where aligned words are jointly emitted as observations. In Figure  4 , the POS tag of "?" (z 5 ) generates H ?k ? ? ? ?k ?0 ? ? ? ? ? z1 z2 z3 z4 z5 H' ?'k ? pay I ? ? ? ? ? NONE NONE usage fees z6 ? ' ' , ~) , ( DP , | ) ( GEM | 0 0 H H k k k ? ? ? ? ? ? ? ? ? ? Figure  5 : A Graphical Representation of the Independent Model the string "?+usage" as the observation (x 5 ). Similarly, the POS tag of "?" in Example 1 would generate the string "?+use". Hence, this model can assign different POS tags to the two different instances of the word "?", based on the different observation distributions in inference. 

 Independent Model The joint model is prone to a data sparseness problem, since each observation is a combination of a source word and its aligned target word. Thus, we propose an independent model, where each hidden state generates a source word and its aligned target word separately. For the aligned target side, we introduce an observation variable x ? t for each z t and a parameter ? ? k for each state k, which parameterizes a distinct distribution over the observations x ? t for that state. ? ? k is distributed according to a prior distribution H ? . Specifically, the independent model is formally defined as follows: ?|? ? GEM(?), ? k |? 0 , ? ? DP(? 0 , ?), ? k ? H, ? ? k ? H ? , z t ? |z t ? Multinomial(? zt ), x t |z t ? F (? zt ), x ? t |z t ? F ? (? ? zt ). When multiple target words are aligned to a single source word, each aligned word is generated separately from observation distribution parameterized by ? ? k . Figure  5  graphs the process of generating Example 2 in Figure  1  using the independent model. x ? t and ? ? k are introduced for aligned target words. The state of "?" (z 5 ) generates the Japanese word "?" as x 5 and the English word "usage" as x ? 5 . Due to this factorization, the independent model is less subject to the sparseness problem. 

 Introduction of Other Factors We assumed the surface form of aligned target words as additional observations in previous sections. Here, we introduce additional factors, i.e., the POS of aligned target words, in the observations. Note that POSs of target words are assigned by a POS tagger in the target language and are not inferred in the proposed model. First, we can simply replace surface forms of target words with their POSs to overcome the sparseness problem. Second, we can incorporate both information from the target language as observations. In the joint model, two pieces of information are concatenated into a single observation. In the independent model, we introduce observation variables (e.g., x ? t and x ? t ) and parameters (e.g., ? ? k and ? ? k ) for each information. Specifically, x ? t and ? ? k are introduced for the surface form of aligned words, and x ? t and ? ? k for the POS of aligned words. Consider, for example, Example 1 in Figure  1 . The POS tag of "?" generates the string "?+use+verb" as the observation in the joint model, while it generates "?", "use", and "verb" independently in the independent model. 

 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags  (Finkel et al., 2007; Liang et al., 2007)  so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities ? s k and observation distributions (? s k , ? ? s k ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag s t , and we use the pair (s t , z t ) as a state representation. 

 Inference In inference, we find the state set that maximizes the posterior probability of state transitions given observations (i.e., P (z 1:n |x 1:n )). However, we cannot evaluate the probability for all possible states because the number of states is infinite.  Finkel et al. (2007)  presented a sampling algorithm for the infinite tree model, which is based on the Gibbs sampling in the direct assignment representation for iHMM  (Teh et al., 2006) . In the Gibbs sampling, individual hidden state variables are resampled conditioned on all other variables. Unfortunately, its convergence is slow in HMM settings because sequential data is likely to have a strong correlation between hidden states  (Gael et al., 2008) . We present an inference procedure based on beam sampling  (Gael et al., 2008)  for the joint model and the independent model. Beam sampling limits the number of possible state transitions for each node to a finite number using slice sampling  (Neal, 2003) , and then efficiently samples whole hidden state transitions using dynamic programming. Beam sampling does not suffer from slow convergence as in Gibbs sampling by sampling the whole state variables at once. In addition,  Gael et al. (2008)  showed that beam sampling is more robust to initialization and hyperparameter choice than Gibbs sampling. Specifically, we introduce an auxiliary variable u t for each node in a dependency tree to limit the number of possible transitions. Our procedure alternates between sampling each of the following variables: the auxiliary variables u, the state assignments z, the transition probabilities ?, the shared DP parameters ?, and the hyperparameters ? 0 and ?. We can parallelize procedures in sampling u and z because the slice sampling for u and the dynamic programing for z are independent for each sentence. See  Gael el al. (2009)  for details. The only difference between inferences in the joint model and the independent model is in computing the posterior probability of state transitions given observations (e.g., p(z 1:n |x 1:n ) and p(z 1:n |x 1:n , x ? 1:n )) in sampling z. In the following, we describe each sampling stage. See  Teh et al., (2006)  for details of sampling ?, ?, ? 0 and ?. 

 Sampling u: Each u t is sampled from the uniform distribution on [0, ? z d(t) zt ], where d(t) is the parent of t: u t ? Uniform(0, ? z d(t) zt ). Note that u t is a positive number, since each transition probability ? z d(t) zt is larger than zero. 

 Sampling z: Possible values k of z t are divided into the two sets using u t : a finite set with ? z d(t) k > u t and an infinite set with ? z d(t) k ? u t . The beam sampling considers only the former set. Owing to the truncation of the latter set, we can compute the posterior probability of a state z t given ob-servations for all t (t = 1, . . . , T ) using dynamic programming as follows: In where x ?(t) (or u ?(t) ) denotes the set of x t (or u t ) on the path from the root node to the node t in a tree. In our experiments, we assume that F (? k ) is Multinomial(? k ) and H is Dirichlet(?, . . . , ?), which is the same in  Finkel et al. (2007) . Under this assumption, the posterior probability of an observation is as follows: p(x t |z t ) = ?xtk + ? ?k + N ? , where ?xk is the number of observations x with state k, ?k is the number of hidden states whose values are k, and N is the total number of observa- tions x. Similarly, p(x ? t |z t ) = ?x ? t k + ? ? ?k + N ? ? ? , where N ? is the total number of observations x ? . When the posterior probability of a state z t given observations for all t can be computed, we first sample the state of each leaf node and then perform backtrack sampling for every other z t where the z t is sampled given the sample for z c(t) as follows: p(z t |z c(t) , x 1:T , u 1:T ) ? p(z t |x ?(t) , u ?(t) ) ? t ? ?c(t) p(z t ? |z t , u t ? ). 

 Sampling ?: We introduce a count variable n ij ? n, which is the number of observations with state j whose parent's state is i. Then, we sample ? using the Dirichlet distribution: (? k1 , . . . , ? kK , ? ? k ? =K+1 ? kk ? ) ? Dirichlet(n k1 + ? 0 ? 1 , . . . , n kK + ? 0 ? K , ? 0 ? ? k ? =K+1 ? k ? ) , where K is the number of distinct states in z. 

 Sampling ?: We introduce a set of auxiliary variables m, where m ij ? m is the number of elements of ? j corresponding to ? i . The conditional distribution of each variable is p(m ij = m|z, ?, ? 0 ) ? S(n ij , m)(? 0 ? j ) m , where S(n, m) are unsigned Stirling numbers of the first kind 5 . The parameters ? are sampled using the Dirichlet distribution: (? 1 , . . . , ? K , ? ? k ? =K+1 ? k ? ) ? Dirichlet(m ?1 , . . . , m ?K , ?), where m ?k = ? K k ? =1 m k ? k . Sampling ? 0 : ? 0 is parameterized by a gamma hyperprior with hyperparameters ? a and ? b . We introduce two types of auxiliary variables for each state (k = 1, . . . , K), w k ? [0, 1] and v k ? {0, 1}. The conditional distribution of each w k is p(w k |? 0 ) ? w ? 0 k (1?w k ) n ?k ?1 and that of each v k is p(v k |? 0 ) ? ( n ?k ? 0 ) v k , where n ?k = ? K k ? =1 n k ? k . The conditional distribution of ? 0 given w k and v k (k = 1, . . . , K) is p(? 0 |w, v) ? ? ?a?1+m..? ? K k=1 v k 0 e ? 0 (? b ? ? K k=1 logw k ) , where m ? = ? K k ? =1 ? K k ? =1 m k ? k ? . 

 Sampling ?: ? is parameterized by a gamma hyperprior with hyperparameters ? a and ? b . We introduce an auxiliary variable ?, whose conditional distribu- tion is p(?|?) ? ? ? (1 ? ?) m?1 . The con- ditional distribution of ? given ? is p(?|?) ? ? ?a?1+K e ?(? b ?log?) . 

 Experiment We tested our proposed models under the NTCIR-9 Japanese-to-English patent translation task  (Goto et al., 2011) , consisting of approximately 3.2 million bilingual sentences. Both the development data and the test data consist of 2,000 sentences. We also used the NTCIR-7 development data consisting of 2,741 sentences for development testing purposes. 

 Experimental Setup We evaluated our bilingual infinite tree model for POS induction using an in-house developed syntax-based forest-to-string SMT system. In the training process, the following steps are performed sequentially: preprocessing, inducing a POS tagset for a source language, training a POS tagger and a dependency parser, and training a forest-to-string MT model. 

 Step 1. Preprocessing We used the first 10,000 Japanese-English sentence pairs in the NTCIR-9 training data for in-ducing a POS tagset for Japanese 6 . The Japanese sentences were segmented using MeCab 7 , and the English sentences were tokenized and POS tagged using TreeTagger  (Schmid, 1994) , where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset  (Asahara and Matsumoto, 2003)  and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++  (Och and Ney, 2003)  in both directions and then combining the alignments using the "grow-diag-finaland" heuristic  (Koehn et al., 2003) . Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha  (Kudo and Matsumoto, 2002) , which generates dependency structures using a phrasal unit called a bunsetsu 8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic 9 : first, the last function word inside each bunsetsu is identified as the head word 10 ; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency structure by preserving the head/modifier relationships of the determined head words. 

 Step 2. POS Induction A POS tag for each word in the Japanese sentences is inferred by our bilingual infinite tree model, ei-ther jointly (Joint) or independently (Ind). We also performed monolingual induction of  Finkel et al. (2007)  for comparison (M ono). In each model, a sequence of sampling u, z, ?, ?, ? 0 , and ? is repeated 10,000 times. In sampling ? 0 and ?, hyperparameters ? a , ? b , ? a , and ? b are set to 2, 1, 1, and 1, respectively, which is the same setting in  Gael et al. (2008) . In sampling z, parameters ?, ? ? , . . ., are set to 0.01. In the experiments, three types of factors for the aligned English words are compared: surface forms ('s'), POS tags ('P'), and the combination of both ('s+P'). Further, two types of inference frameworks are compared: induction (IN D) and ref inement  (REF ) . In both frameworks, each hidden state z t is first initialized to the POS tags assigned by MeCab (the IPA POS tagset), and then each state is updated through the inference procedure described in Section 3.5. Note that in REF , the sampling distribution over z t is constrained to include only states that are a refinement of the initially assigned POS tag. 

 Step 3. Training a POS Tagger and a Dependency Parser In this step, we train a Japanese dependency parser from the 10,000 Japanese dependency trees with the induced POS tags which are derived from Step 2. We employed a transition-based dependency parser which can jointly learn POS tagging and dependency parsing  (Hatori et al., 2011)  under an incremental framework 11 . Note that the learned parser can identify dependencies between words and attach an induced POS tag for each word. Step 4. Training a Forest-to-String MT In this step, we train a forest-to-string MT model based on the learned dependency parser in Step 3. We use an in-house developed hypergraph-based toolkit, cicada, for training and decoding with a tree-to-string model, which has been successfully employed in our previous work for system combination  (Watanabe and Sumita, 2011)  and online learning  (Watanabe, 2012) . All the Japanese and English sentences in the NTCIR-9 training data are segmented in the same way as in Step 1, and then each Japanese sentence is parsed by the dependency parser learned in Step 3, which simultaneously assigns induced POS tags and word dependencies. Finally, a forest-to-string MT model is learned with  the GHKM algorithm  (Mi and Huang, 2008)  after each parse tree is restructured into a binarized packed forest. Parameters are tuned on the development data using xBLEU  (Rosti et al., 2011)  as an objective and L-BFGS  (Liu and Nocedal, 1989)  as an optimization toolkit, since it is stable and less prone to randomness, unlike MERT  (Och, 2003)  or PRO  (Hopkins and May, 2011) . The development test data is used to set up hyperparameters, i.e., to terminate tuning iterations. When translating Japanese sentences, a parse tree for each sentence is constructed in the same way as described earlier in this step, and then the parse trees are translated into English sentences using the learned forest-to-string MT model. 

 Experimental Results Table  1  shows the performance for the test data measured by case sensitive BLEU  (Papineni et al., 2002) . We also present the performance of our baseline forest-to-string MT system (BS) using the original IPA POS tags. In Table  1 , numbers in bold indicate that the systems outperform the baselines, BS and M ono. Under the Moses phrase-based SMT system  (Koehn et al., 2007)  with the default settings, we achieved a 26.80% BLEU score. Table  1  shows that the proposed systems outperform the baseline M ono. The differences between the performance of Ind[s+P] and M ono are statistically significant in the bootstrap method  (Koehn, 2004)  POS induction when jointly encoding bilingual information into observations. Additionally, all the systems using the independent model outperform BS. The improvements are statistically significant in the bootstrap method  (Koehn, 2004) , with a 1% significance level. The results show that the proposed models can generate more favorable POS tagsets for SMT than an existing POS tagset. In Table  1 , REF s are at least comparable to, or better than, IN Ds except for M ono. This shows that REF achieves better performance by preserving the clues from the original POS tagset. However, REF may suffer sever overfitting problem for M ono since no bilingual information was incorporated. Further, when the full-level IPA POS tags 12 were used in BS, the system achieved a 27.49% BLEU score, which is worse than the result using the second-level IPA POS tags. This means that manual refinement without bilingual information may also cause an overfitting problem in MT. 

 Discussion 

 Comparison to the IPA POS Tagset Table  2  shows the number of the IPA POS tags used in the experiments and the POS tags induced by the proposed models. This table shows that each induced tagset contains more POS tags than the IPA POS tagset. In the experimental data, some of Japanese verbs correspond to genuine English verbs, some are nominalized, and others correspond to English past participle verbs or present participle verbs which modify other words. Respective examples are "I use a card.", "U sing the index is faster.", and "I explain using an example.", where all the underlined words correspond to the same Japanese word, "?", whose IPA POS tag is a verb. Ind[s+P] in REF generated the POS tagset where the three types are assigned to separate POS groups. The Japanese particle "?" is sometimes attached to nouns to give them adverb roles. For These examples show that the proposed models can disambiguate POS tags that have different functions in English, whereas the IPA POS tagset treats them jointly. Thus, such discrimination improves the performance of a forest-to-string SMT. 

 Impact of Tagging and Dependency Accuracy The performance of our methods depends not only on the quality of the induced tag sets but also on the performance of the dependency parser learned in Step 3 of Section 4.1. We cannot directly evaluate the tagging accuracy of the parser trained through Step 3 because we do not have any data with induced POS tags other than the 10,000sentence data gained through Step 2. Thus we split the 10,000 data into the first 9,000 data for training and the remaining 1,000 for testing, and then a dependency parser was learned in the same way as in Step 3. Table  3  shows the results. Original is the performance of the parser learned from the training data with the original POS tagset. Note that the dependency accuracies are measured on the automatically parsed dependency trees, not on the syntactically correct gold standard trees. Thus Original achieved the best dependency accuracy. In Table  3 , the performance for our bilinguallyinduced POSs, Joint and Ind, are lower than Original and M ono. It seems performing parsing and tagging with the bilingually-induced POS tagset is too difficult when only monolingual in-formation is available to the parser. However, our bilingually-induced POSs, except for Joint[P ], with the lower accuracies are more effective for SMT than the monolingually-induced POSs and the original POSs, as indicated in Table  1 . The tagging accuracies for Joint[P ] both in IN D and REF are significantly lower than the others, while the dependency accuracies do not differ significantly. The lower tagging accuracies may directly reflect the lower translation qualities for Joint[P ] in Table  1 . 

 Conclusion We proposed a novel method for inducing POS tags for SMT. The proposed method is a nonparametric Bayesian method, which infers hidden states (i.e., POS tags) based on observations representing not only source words themselves but also aligned target words. Our experiments showed that a more favorable POS tagset can be induced by integrating aligned information, and furthermore, the POS tagset generated by the proposed method is more effective for SMT than an existing POS tagset (the IPA POS tagset). Even though we employed word alignment from GIZA++ with potential errors, large gains were achieved using our proposed method. We would like to investigate the influence of alignment errors in the future. In addition, we are planning to prove the effectiveness of our proposed method for language pairs other than Japanese-to-English. We are also planning to introduce our proposed method to other syntax-based SMT, such as a string-to-tree SMT and a tree-to-tree SMT. Figure 1 : 1 Figure 1: Examples of Existing Japanese POS Tags and Dependency Structures 
