title
The DCU-ICTCAS MT system at WMT 2014 on German-English Translation Task

abstract
This paper describes the DCU submission to WMT 2014 on German-English translation task. Our system uses phrasebased translation model with several popular techniques, including Lexicalized Reordering Model, Operation Sequence Model and Language Model interpolation. Our final submission is the result of system combination on several systems which have different pre-processing and alignments.

Introduction On the German-English translation task of WMT 2014, we submitted a system which is built with Moses phrase-based model . For system training, we use all provided German-English parallel data, and conducted several pre-processing steps to clean the data. In addition, in order to improve the translation quality, we adopted some popular techniques, including three Lexicalized Reordering Models  (Axelrod et al., 2005; Galley and Manning, 2008) , a 9gram Operation Sequence Model  (Durrani et al., 2011)  and Language Model interpolation on several datasets. And then we use system combination on several systems with different settings to produce the final outputs. Our phrase-based systems are tuned with k-best MIRA  (Cherry and Foster, 2012)  on development set. We set the maximum iteration to be 25. The Language Models in our systems are trained with SRILM  (Stolcke, 2002) . We trained 

 Corpus Filtered Out (%) Bilingual 7.17 Monolingual (English) 1.05 Table  1 : Results of language detection: percentage of filtered out sentences a 5-gram model with Kneser-Ney discounting  (Chen and Goodman, 1996) . In the next sections, we will describe our system in detail. In section 2, we will explain our preprocessing steps on corpus. Then in section 3, we will describe some techniques we have tried for this task and the experiment results. In section 4, our final configuration for submitted system will be presented. And we conclude in the last section. 

 Pre-processing We use all the training data for German-English translation, including Europarl, News Commentary and Common Crawl. The first thing we noticed is that some Non-German and Non-English sentences are included in our training data. So we apply Language Detection  (Shuyo, 2010)  for both monolingual and bilingual corpora. For monolingual data (only including English sentences in our task), we filter out sentences which are detected as other language with probability more than 0.999995. And for bilingual data, A sentence pair is filtered out if the language detector detects a different language with probability more than 0.999995 on either the source or the target. The filtering results are given in Table  1 . In our experiment, German compound words are splitted based on frequency  (Koehn and Knight, 2003) . In addition, for both monolingual and bilingual data, we apply tokenization, normalizing punctuation and truecasing using Moses scripts. For parallel training data, we also filter out sentence pairs containing more than 80 tokens on either side and sentence pairs whose length ratio between source and target side is larger than 3. 

 Techniques In our preliminary experiments, we take newstest 2013 as our test data and newstest 2008-2012 as our development data. In total, we have more than 10,000 sentences for tuning. The tuning step would be very time-consuming if we use them all. So in this section, we use Feature Decay Algorithm (FDA) (Bic ?ici and Yuret, 2014) to select 2000 sentences as our development set. Table  2  shows that system performance does not increase with larger tuning set and the system using only 2K sentences selected by FDA is better than the baseline tuned with all the development data. In this section, alignment model is trained by MGIZA++  (Gao and Vogel, 2008)  with grow-diag-final-and heuristic function. And other settings are mostly default values in Moses. 

 Lexicalized Reordering Model German and English have different word order which brings a challenge in German-English machine translation. In our system, we adopt three Lexicalized Reordering Models (LRMs) for addressing this problem. They are word-based LRM (wLRM), phrase-based LRM (pLRM) and hierarchal LRM (hLRM). These three models have different effect on the translation. Word-based and phrase-based LRMs are focus on local reordering phenomenon, while hierarchical LRM could be applied into longer reordering problem. Figure  1  shows the differences  (Galley and Manning, 2008) . And Table  3  shows effectiveness of different LRMs. In our system based on Moses, we use wbe-msd-bidirectional-fe, phrase-msd-bidirectional-fe and hier-mslr-bidirectional-fe to specify these three LRMs. From Table  2 , we could see that LRMs significantly improves the translation.  

 Operation Sequence Model The Operation Sequence Model (OSM)  (Durrani et al., 2011)  explains the translation procedure as a linear sequence of operations which generates source and target sentences in parallel.  Durrani et al. (2011)  defined four translation operations: Generate(X,Y), Continue Source Concept, Generate Source Only (X) and Generate Identical, as well as three reordering operations: Insert Gap, Jump Back(W) and Jump Forward. These operations are described as follows. ? Generate(X,Y) make the words in Y and the first word in X added to target and source string respectively. ? Continue Source Concept adds the word in the queue from Generate(X,Y) to the source string. ? Generate Source Only (X) puts X in the source string at the current position. ? Generate Identical generates the same word for both sides. ? Insert Gap inserts a gap in the source side for future use. ? Jump Back (W) makes the position for translation be the Wth closest gap to the current position. ? Jump Forward moves the position to the index after the right-most source word. 

 Systems Tuning  = (o 1 o 2 ? ? ? o J ) is: p(O) = J j=1 p(o j |o j?n+1 ? ? ? o j?1 ) (1) where n indicates the number of previous operations used. In this paper we train a 9-gram OSM on training data and integrate this model directly into loglinear framework (OSM is now available to use in Moses). Our experiment shows OSM improves our system by about 0.8 BLEU (see Table  2 ). 

 Language Model Interpolation In our baseline, Language Model (LM) is trained on all the monolingual data provided. In this section, we try to build a large language model by including data from English Gigaword fifth edition (only taking partial data with size of 1.6G), English side of UN corpus and English side of 10 9 French-English corpus. Instead of training a single model on all data, we interpolate language models trained on each subset (monolingual data provided is splitted into three parts: News 2007-2013, Europarl and News Commentary) by tuning weights to minimize perplexity of language model measured on the target side of development set. In our experiment, after interpolation, the language model doesn't get a much lower perplexity, but it slightly improves the system, as shown in Table  2 . 

 Other Tries In addition to the techniques mentioned above, we also try some other approaches. Unfortunately all of these methods described in this section are non-effective in our experiments. The results are shown in Table  2 . ? Factored Model : We tried to integrate a target POS factored model into our system with a 9-gram POS language model to address the problem of word selection and word order. But experiment doesn't show improvement. The English POS is from Stanford POS Tagger  (Toutanova et al., 2003) . ? Translation Model Combination: In this experiment, we try to use the method of (Sennrich, 2012) to combine phrase tables or reordering tables from different subsets of data to minimize perplexity measured on development set. We try to split the training data in two ways. One is according to data source, resulting in three subsets: Europarl, News Commentary and Common Crawl. Another one is to use data selection. We use FDA to select 200K sentence pairs as in-domain data and the rest as out-domain data. Unfortunately both experiments failed. In Table  2 , we only report results of phrase table combination on FDA-based data sets. ? OSM Interpolation: Since OSM in our system could be taken as a special language model, we try to use the idea of interpolation similar with language model to make OSM adapted to some data. Training data are splitted into two subsets with FDA. We train 9-gram OSM on each subsets and interpolate them according to OSM trained on the development set. ? Sparse Features: For each source phrase, there is usually more than one corresponding translation option. Each different translation may be optimal in different contexts. Thus in our systems, similar to  (He et al., 2008)  which proposed a Maximum Entropy-based rule selection for the hierarchical phrasebased model, features which describe the context of phrases, are designed to select the right translation. But different with  (He et al., 2008) , we use sparse features to model the context. And instead of using syntactic POS, we adopt independent POS-like features: cluster ID of word. In our experiment mkcls was used to cluster words into 50 groups. And all features are generalized to cluster ID. 

 Submission Based on our preliminary experiments in the section above, we use LRMs, OSM and LM interpolation in our final system for newstest 2014. But as we find that Language Models trained on UN corpus and 10 9 French-English corpus have a very high perplexity and in order to speed up the translation by reducing the model size, in this section, we interpolate only three language models from monolingual data provided, English Gigaword fifth edition and target side of training data. In addition, we also try some different methods for final submission. And the results are shown in Table 4. ? Development Set Selection: Instead of using FDA which is dependent on test set, we use the method of  (Nadejde et al., 2013)  to select tuning set from newstest 2008-2013 for the final system. We only keep 2K sentences which have more than 30 words and higher BLEU score. The experiment result is shown in Table  4  ( The system is indicated as Baseline). ? Pre-processing: In our preliminary experiments, sentences are tokenized without changing hyphen. Thus we build another system where all the hyphens are tokenized aggressively. ? SyMGIZA++: Better alignment could lead to better translation. So we carry out some experiments on SyMGIZA++ aligner  (Junczys-Dowmunt and Sza, 2012) , which modifies the original IBM/GIZA++ word alignment models to allow to update the symmetrized models between chosen iterations of the original training algorithms. Experiment shows this new alignment improves translation quality. ? Multi-alignment Selection: We also try to use multi-alignment selection  (Tu et al., 2012)  to generate a "better" alignment from three alignmens: MGIZA++ with function growdiag-final-and, SyMGIZA++ with function grow-diag-final-and and fast alignment  (Dyer et al., 2013) . Although this method show comparable or better result on development set, it fails on test set. Since we build a few systems with different setting on Moses phrase-based model, a straightforward thinking is to obtain the better translation from several different translation systems. So we use system combination  (Heafield and Lavie, 2010)  on the 1-best outputs of three systems (indicated with * in table 4). And this results in our best system so far, as shown in Table  4 . In our final submission, this result is taken as primary. 

 Conclusion This paper describes our submitted system to WMT 2014 in detail.  Figure 1 : 1 Figure 1: Occurrence of a swap according to the three orientation models: word-based, phrasebased, and hierarchical. Black squares represent word alignments, and gray squares represent blocks identified by phrase-extract. In (a), block b i = (e i , f a i ) is recognized as a swap according to all three models. In (b), b i is not recognized as a swap by the word-based model. In (c), b i is recognized as a swap only by the hierarchical model. (Galley and Manning, 2008)   
