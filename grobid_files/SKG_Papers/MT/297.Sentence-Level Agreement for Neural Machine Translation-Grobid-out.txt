title
Sentence-Level Agreement for Neural Machine Translation

abstract
The training objective of neural machine translation (NMT) is to minimize the loss between the words in the translated sentences and those in the references. In NMT, there is a natural correspondence between the source sentence and the target sentence. However, this relationship has only been represented using the entire neural network and the training objective is computed in wordlevel. In this paper, we propose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. * Mingming Yang was an internship research fellow at NICT when conducting this work.

Introduction Neural network based methods have been applied to several natural language processing tasks  (Zhang et al., 2016; Li et al., 2019; . In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words  (Bahdanau et al., 2015) . It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. Based on this hypothesis,  Kuang et al. (2018)  proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation.  Tu et al. (2017)  incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation. However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships . Although Transformer model  (Vaswani et al., 2017)  has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization.  Liang et al. (2010)  have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem.  Su et al. (2016)  presented a sentence similarity method for spoken dialogue system to improve accuracy.  Rei and Cummins (2016)  proposed sentence similarity measures to improve the estimation of topical relevance.  2018)  used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator's primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation  (Nagao, 1984; Nio et al., 2013) , use the sentence similarity matching between the sentences to be translated and the sentences in the bilingual corpora to extract translation. Inspired by these studies, we establish a sentence-level agreement channel directly in the deep neural network to shorten the distance between the source and target sentence-level embeddings. Specifically, our model can be effectively applied to NMT in two aspects: ? Sentence-Level Agreement as Training Objective: we use the sentence-level agreement as a part of the training objective function. In this way, we not only consider the translation of the word level but also consider the sentence level. ? Enhance Source Representation: As our model can make the vector distribution of the sentence-level between source-side and target-side closer, we can combine their sentence-level embeddings to enhance the source representation. Experimental results on Chinese-to-English and English-to-German translation tasks demonstrate that our model is able to effectively improve the performance of NMT. 

 Neural Machine Translation In this section, we take the Transformer architecture proposed by  Vaswani et al. (2017) , which is the state-of-the-art translation architecture, as the baseline system. As an encoder-to-decoder architecture, X = {x 1 , x 2 , ..., x J } represents a source sentence and Y = {y 1 , y 2 , ..., y I } represents a target sentence. The encoder-to-decoder model learns to estimate the conditional probability from the source sentence to the target sentence word by word: P (y|x; ?) = I i=1 P (y i |y <i , x; ?), (1) where ? is a set of model parameters and y <i denotes a partial translation. Different from the other NMT, Transformer has the self-attention layers that can operate in parallel. A single self-attention layer has two sub-layers: a multi-head self-attention layer and a feed forward network. The feed forward network consists of two simple fully connected networks with a ReLU activation function in between: FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 , (2) where W 1 and W 2 are both linear transformation networks, b 1 and b 2 are both bias. We define H enc as the sentence representation of X via the self-attention layers in encoder, and H dec as the sentence representation of words Y via embedding layers in decoder. The parameters of Transformer are trained to minimize the following objective function on a set of training examples {(X n , Y n )} N n=1 : L mle = ? 1 N N n=1 Iy i=1 logP (y n i |y n <i , H enc , H dec ). (3) 3 Agreement on Source and Target Sentence Some studies  (Luong et al., 2015; Tu et al., 2016; Chen et al., 2017a,b; Kuang et al., 2018)  showed that improving word alignment is beneficial to machine translation. Their idea is based on word-level agreement and make the embeddings of source words and corresponding target words similar. In this paper, we investigate the sentence-level relationship between the source and target sentences. We propose a sentence-level agreement method which can make the sentencelevel semantics of the source and target closer. The entire architecture of the proposed method is illustrated in Figure  1 . 

 Sentence-Level Agreement First, we need to get the sentence-level representation of the source and target. Some studies showed that the Mean operation is an effective method to represent sentence of sequence words  (Mitchell and Lapata, 2010; Mikolov et al., 2013; Le and Mikolov, 2014) , especially for NMT . Motivated by this, we adopt Mean to represent the source and target sentences as shown in Figure  1(a) . Denote H enc is the mean of H enc and H dec is the mean of H dec . We design a Sentence Agreement Loss L mse to measure the distance between the source and target sentence-level vectors: L mse = || H enc ? H dec || 2 . (4) Finally, our goal is to improve translation with shortening the distance in sentence-level. Thus, the final objective of our model is composed of parts, the formula is as follows: L = L mle + L mse . (5) 

 Enhance Source Representation Sentence-level agreement helps make the targetside sentence representation closer to the source. Intuitively, we can also use this mechanism to strengthen the source representation to improve the translation. Further, we propose a simple and efficient architecture in Figure  1(b) . First, we map H enc to the target-side vector EH enc through a simple feed forward network TFFN by eq.(  2 ): EH enc = TFFN(H enc ). (6) In particular, we use a Tanh activation function instead of ReLU in the feed forward network. The value range of Tanh is -1 to 1, which indicates some information should be counterproductive. Our Enhanced Sentence Agreement Loss LE mse is to measure the distance between the source and target sentence-level vectors: LE mse = || EH enc ? H dec || 2 , ( 7 ) where EH enc is the mean of EH enc . Le and Mikolov (2014) use concatenation as the method to combine the sentence vectors to strengthen the capacity of representation. We also use the same method to combine H enc and EH enc : CH enc = Concat(H enc , EH enc ). (8) In this way, we can enhance the source representation with a sentence-level representation closer to the target-side. The updated translation training objective is: LE mle = ? 1 N N n=1 Iy i=1 logP (y n i |y n <i , CH enc , H dec ). (9) Thus, the final objective is as follows: NIST04, NIST05, NIST06 datasets are testsets. We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric  (Papineni et al., 2002) . The training data of English-German (EN-DE) translation is from WMT14, which consists of 4.5M sentence pairs. We use byte-pair encoding  (Sennrich et al., 2016b)  to segment words. The newstest2013 was used as a development set and the newstest2014 as test sets that are evaluated by SacreBLEU  (Post, 2018) . LE = LE mle + LE mse . ( 10 To efficiently train NMT models, we train each model with sentences of length up to 50 words. In this way, about 90% and 89% of ZH-EN and EN-DE parallel sentences are covered in the experiments. In addition, we use byte pair encoding  (Sennrich et al., 2016a)  with 32K merges to segment words into sub-word units for all languages to alleviate the out-of-vocabulary problem. We evaluate the proposed approaches on our re-implemented Transformer model  (Vaswani et al., 2017) . We test both the Base and Big models, which differ at the dimensionality of input and output (512 vs 1024), the number of attention head (8 vs 16) and the inner-layer size (2048 vs 4096). We set 6 layers for encoder and decoder. All the models were trained on a single NVIDIA P100 GPU, which is allocated a minibatch of 4096 tokens. About 200K minibathes are trained. 

 Performance Table  1  shows the performances measured in terms of BLEU score. On ZH-EN task, Transformer(Base) outperforms the existing systems EDR  (Tu et al., 2017)  and DB  (Kuang et al., 2018)  by 11.5 and 6.5 BLEU points. With respect to BLEU scores, all the proposed models (Row 4-5) consistently outperform Transformer(base) by 0.96 and 1.23 BLEU points. The big models (Row 7-8) also achieve similar improvement by 0.73 and 0.82 BLEU points on a larger parameters model. These findings suggest a sentence-level agreement between source-side and target-side is helpful for NMT. Further, we use it to enhance the source representation is an effective way to improve the translation. addition, the proposed methods gain similar improvements on EN-DE task. 

 Efficiency Analysis In In particular, by comparing Row 3 and 4, we find that our proposed methods achieve a similar performance with the Transformer(Big) and gain a faster speed with fewer parameters. It indicates that enhancing source representation with a sentence-level representation is an effective method for improving translation performance. 

 Sentence-Level Similarity Analysis We further study how the proposed models influenced sentence-level similarity in translation. For this, we follow the method of  Lapata and Barzilay (2005)  to measure sentence similarity. First, each sentence is represented by the mean of the distributed vectors of its words. Second, the similarity between source and target sentences is determined by the cosine of their means: sim = cos( H enc , H dec ). (11) As Table  3  shows, the sentence-level similarity of the proposed method is higher than the corresponding baselines. In addition, there is a correlation between NMT performance (BLEU) and the sentence-level similarity. This indicates that the proposed method can improve the sentence-level similarity between source and target sentences and the performance of NMT. 

 Conclusion In this work, we have presented a sentence-level agreement method for NMT. Our goal is to bring the sentence representation of the source-side and the target-side closer together. At the same time, we can utilize this information to enhance source representation. Our study suggests the source-totarget sentence-level relationship is very useful for translation. In future work, we intend to apply these methods to other natural language tasks. Table 1 : 1 ) Translation results for Chinese-English and English-German translation task. " ?": indicates statistically better than Transformer(Base/Big) (? < 0.01). 4 Experiments 4.1 Dataset For Chinese-English (ZH-EN) translation, our training data for the translation task consists of 1.25M Chinese-English sentence pairs extracted from LDC corpora 1 . The NIST02 testset is chosen as the development set, and the NIST03, 1 The corpora include LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 
