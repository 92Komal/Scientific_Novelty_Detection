title
Multi-Unit Transformers for Neural Machine Translation

abstract
Transformer models  (Vaswani et al., 2017)  achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage. 1

Introduction Transformer based models  (Vaswani et al., 2017)  have been proven to be very effective in building the state-of-the-art Neural Machine Translation (NMT) systems via neural networks and attention mechanism  (Sutskever et al., 2014; Bahdanau et al., 2014) . Following the standard Sequence-to-Sequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and positionwise feed-forward network. Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers  Wang et al., 2019a)  propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow  Meng et al., 2019; Li et al., 2018 Li et al., , 2019  in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness. Second, for the multiunit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way. In this paper, we propose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in parallel improves model capability and diversity by its varied feature compositions. Furthermore, inspired by the well-studied bagging  (Breiman, 1996)  and gradient boosting algorithms  (Friedman, 2001)  in the machine learning field, we design biased units with a sequential dependency to further boost model performance. Specifically, with the help of a module named bias module, we apply different kinds of noises to form biased inputs for corre-sponding units. By doing so, we explicitly establish information gaps among units and guide them to learn from each other. Moreover, to better leverage the power of complementariness, we introduce sequential ordering into the multi-unit setting, and force each unit to learn the residual of its preceding accumulation. We evaluate our methods on three widely used Neural Machine Translation datasets, NIST Chinese-English, WMT'14 English-German and WMT'18 Chinese-English. Experimental results show that our multi-unit model yields an improvement of +1.52, +1.90 and +1.10 BLEU points, over the baseline model (Transformer-Base) for three tasks with different sizes, respectively. Our model even outperforms the Transformer-Big on the WMT'14 English-German by 0.7 BLEU points with only 54% of parameters. Moreover, as an interesting side effect, our model only introduces mild inference speed decrease (about 3.1%) compared with the Transformer-Base model, and is faster than the Transformer-Big model. The contributions of this paper are threefold: ? We propose the Multi-Unit TransformErs (MUTE), to promote the expressiveness of Transformer models by introducing diverse and complementary parallel units. ? Aside from learning with identical units, we extend the MUTE by introducing bias module and sequential ordering to further model the diversity and complementariness among different units. ? Experimental results show that our models substantially surpass baseline models in three NMT datasets, NIST Chinese-English, WMT'14 English-German and WMT'18 Chinese-English. In addition, our models also show high efficiency in both the inference speed and parameter usage, compared with Transformer baselines. 

 Transformer Architecture The Transformer Architecture  (Vaswani et al., 2017)  for Neural Machine Translation (NMT) generally adopts the standard encoder-decoder paradigm. In contrast to RNN architectures, the Transformer stacks several identical self-attention based layers instead of recurrent units for better parallelization. Specifically, given an input sequence X = {x 1 , x 2 , ? ? ? , x n } in source language (e.g., English), the model is asked to predict its corre-sponding translation Y = {y 1 , y 2 , ? ? ? , y m } in target language (e.g., German). Encoder. Digging into the details of the model, a Transformer encoder consists of N e stacked layers, where each layer consists of two sub-layers, a multihead self-attention sub-layer and a position-wise feed-forward network (FFN) sub-layer. s k = SelfAttn(X k ) + X k , (1) F e (X k ) = s k + FFN(s k ), (2) where X k ? R n?d and F e (X k ) ? R n?d denote the inputs and outputs of the k-th encoder layer, respectively, and d is the hidden dimension. Decoder. The decoder follows a similar architecture, with an additional multihead cross-attention sub-layer for each of N d decoder layers. s k = SelfAttn(Y k ) + Y k , (3) c k = CrossAttn(s k , F e (X Ne )) + s k , (4) F d (Y k ) = c k + FFN(c k ), (5) where Y k ? R m?d and F d (Y k ) ? R m?d repre- sent the inputs and outputs of k-th decoder layer. Here, we omit layer norms among sub-layers for simplicity. We take the bundle of cascading subcomponents (i.e., attention modules and FFN) as a unit, and refer to the original Transformer and its variants with such cascade units as Single-Unit Transformer. For ease of reading, we refer to a single unit of encoder and decoder as F e and F d in the following sections. Then, the probability P (Y |X) is produced with another Softmax layer on top of decoder outputs, P (Y |X) = Softmax(W s ? F d (Y N d ) + b), (6) where W s ? R d?|V | and b ? R |V | are learnable parameters, and |V | denotes the size of target vocabulary. Then, a cross-entropy objective is computed by, L CE = t?(1,m) Y t log P (Y t |X), (7) where t represents the t-th step for decoding phase. 3 Model Layout  Then, we follow the standard usage by stacking several MUTE layers to constitute our encoder and decoder. In general, the encoder and decoder of the Transformer network share a similar architecture and can be improved with the same techniques. Without losing generality, we take the encoder as an example to further illustrate the MUTE. Given input X k of k-th layer, we feed it into I identical units {F 1 , ? ? ? , F i , ? ? ? , F I } with different learnable pa- rameters. s k i = SelfAttn i (X k ) + X k , (8) F e i (X k ) = s k i + FFN i (s k i ), (9) where i denotes the i-th unit. After collecting outputs for all I units, we combine them by a weighted sum, F e (X k ) = i?(1,I) ? i ? F e i (X k ), (10) where ? i ? R 1 represents the learnable weight for the i-th unit (Section 5.8) and F e (X k ) ? R n?d is the final output for the k-th layer. 

 Biased MUTE The multi-unit setting for Transformer resembles the well-known ensemble techniques in machine learning fields, in that it also combines several different modules into one and aims for better performance. In that perspective, borrowed from the idea of bagging  (Breiman, 1996) , we propose to use biased units instead of identical units, which results in creating information gaps among units and makes them learn from each other. More specifically, in training, we introduce a Bias-Module to create biased units, as shown in Figure  1(b) . For each layer, instead of giving the same inputs X k ? R n?d to all units, we transform each input with corresponding type of noises (e.g., swap, reorder, mask), in order to force the model to focus on different parts of inputs: X k i = Bias i (X k ), (11) F e (X k ) = i?(1,I) ? i ? F e i (X k i ), (12) where Bias i denotes the noise function for i-th unit. The noise operations 2 we investigated include, ? Swapping, randomly swap two input embeddings up to a certain range (i.e., 3). ? Disorder, randomly permutate a subsequence within a certain length (i.e., 3). ? Masking, randomly replace one input embedding with a learnable mask embedding. Note that, the identity mapping (i.e., no noise) can be seen as a special case of bias module and is included in our model design. Additionally, to get deterministic outputs, we disable the noises in the testing phase, which brings in the inconsistency between training and testing. Hence, we propose a switch mechanism with a sample rate p ? that determines whether to enable the bias module in training. This mechanism forces the model to adapt to golden inputs and mitigate the aforementioned inconsistency. 

 Sequentially Biased MUTE Although the bias module guides units of learning from each other by formulating such information gaps, it still lacks explicit complementarity modeling, i.e., mitigating these gaps. Here, based on the Biased MUTE, we propose a novel method to explicitly introducing a deep connection among units by utilizing the power of order (Figure  1(c) ). Sequential Dependency. Given the outputs from biased units F e i (X k i ), we permutate these outputs by a certain ordering function p(i) (e.g., i ? {1, 2, 3, 4} to p(i) ? {4, 2, 3, 1}), {G e i = F e p(i) (X k p(i) )|i ? (1, I)}, (13) where G e i is the i-th permutated output. The implementation of p(i) will be illustrated later. Then, we explicitly model the complementariness among units by introducing sequential dependency. Specifically, we compute an accumulated sequence { ?e i |i ? (1, I)} over the permutated out- puts G e i , ?e i = ?e i?1 + G e i , (14) where ?e i ? R n?d is the i-th accumulated output, and ?e 0 = 0. Through this sequential dependency, each permutated output G e i learns the residual of previous accumulated outputs ?e i?1  (He et al., 2016)  and serves as a complement to previous accumulated outputs. Finally, we normalize this accumulated sequence to keep the output norm stable and fuse all accumulated outputs. F e (X k ) = i?(1,I) ? i ? ?e i i . (15) Autoshuffle. Until now, we have modeled the sequential dependency between each of the units. The only problem left is how to gather a proper ordering of units. We propose to use the AutoShuffle Network . Mathematically, shuffling with specific order equals to a multiplication by a permutation matrix (i.e., every row and column contains precisely a single 1 with 0s elsewhere). Nevertheless, a permutation matrix only contains discrete values and can not be optimized by gradient descent. Therefore, we use a continuous matrix M ? R I?I with non-negative values M i,j ? (0, 1), i, j ? (1, I) to approximate the discrete permutation matrix. Particularly, M is regarded as a learnable matrix and is used to multiply the outputs of units, [? ? ? ; F e p(i) (X k p(i) ); ? ? ? ] = M ? [? ? ? ; F e i (X k i ); ? ? ? ], (16) where [?; ?] means the concatenation operation. To ensure M remains an approximation for the permutation matrix during training, we normalize M after each optimization step. M i,j = max(M i,j , 0), (17) M i,j = M i,j ? M ?,j , M i,j = M i,j ? M i, ? . (18) Then, we introduce a Lipschitz continuous nonconvex penalty, as proposed in  to guarantee M converge to a permutation matrix. L p = I i=1 [ I j=1 |M i,j | ? ( I j=1 M 2 i,j ) 1 2 ] + I j=1 [ I i=1 |M i,j | ? ( I i=1 M 2 i,j ) 1 2 ], (19) Please refer to Appendix D for proof and other details. Finally, the penalty is added to cross-entropy loss defined in equantion (7) as our final objective, L = L CE + ? k L k p , (20) where ? is a hyperparameter to balance two objectives and L k p is the penalty for the k-th layer. 

 Experimental Settings In this section, we elaborate our experimental setup on three widely-studied Neural Machine Translation tasks, NIST Chinese-English (Zh-En), WMT'14 English-German (En-De) and WMT'18 Chinese-English. Datasets. For NIST Zh-En task, we use 1.25M sentences extracted from LDC corpora 3 .   1 : Case-insensitive BLEU scores (%) of NIST Chinses-English (Zh-En) task. For all models with MUTE, we use four units. #Params. means the number of learnable parameters in the model. ? denotes the average BLEU improvement over dev set and test sets, compared with the "Transformer (Base)". Bold represents the best performance. " ?": significantly better than "Transformer + Relative (Base)" (p < 0.05); " ? ?": significantly better than "Transformer + Relative (Base)" (p < 0.01). 2016) with 32k merge operations and a shared vocabulary for English and German. We use new-stest2013 as our validation set and newstest2014 as our test set, which contain 3000 and 3003 sentences, respectively. For the WMT'18 Zh-En task, we use 18.4M preprocessed data, which is also tokenized and split using byte pair encoded (BPE)  (Sennrich et al., 2016) . We use newstest2017 as our validation set and newstest2018 as our test set, which contains 2001 and 3981 sentences, respectively. Evaluation. For evaluation, we train all the models with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respectively, and we select the model which performs the best on the validation set and report its performance on the test sets. We measure the caseinsensitive/case-sensitive BLEU scores using multibleu.perl 4 with the statistical significance test  (Koehn, 2004)    5  for NIST Zh-En and WMT'14 En-De, respectively. For WMT'18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script 6 . 4 https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/multibleu.perl 5 https://github.com/moses-smt/mosesde coder/blob/master/scripts/analysis/boots trap-hypothesis-difference-significance. pl 6 https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/mteval -v13a.pl Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in  (Vaswani et al., 2017) , namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. Besides, since noise types like swapping and reordering are of no effect on Transformer models with absolute position information, the MUTE models are implemented using relative position information  (Shaw et al., 2018) . In addition, we only apply multi-unit methods to encoders, provided that the encoder is more crucial to model performance  (Wang et al., 2019a) . All experiments on MUTE models are conducted with Transformer-Base setting. For the basic MUTE model, we use four identity units. As for the Biased MUTE and Sequentially Biased MUTE, we use four units including one identity unit, one swapping unit, one disorder unit and one masking unit. The sample rate p ? is set to 0.85. For more implementation details and experiments on sample rate, please refer to Appendix A and B. 

 Results Through experiments, we first evaluate our model performance (Section 5.1 and 5.2). Then, we analyze how each part of our model works (Section 5.4 and 5.6). Finally, we conduct experiments to further understand the behavior of our models (Sec- tion 5.7 and 5.8). 

 Results on NIST Chinses-English As shown in Table  1 , we list the performance of our re-implemented Transformer baselines and our approaches. We also list several existing strong NMT systems reported in previous work to validate the effectiveness of our models. By investigating results in Table  1 , we have the following observations. First, compared with existing NMT systems, our re-implemented Transformers are strong baselines. Second, all of our approaches substantially outperform our baselines, with improvement ranging from 1.12 to 1.52 BLEU points. Comparing our methods to "Transformer + Relative (Base)", our best approach (i.e., "MUTE + Bias + Seq. (Base, 4 Units)") still achieves a significant improvement of 1.0 BLEU points on multiple test sets. Third, among our approaches, we find that, even though our basic MUTE model has already surpassed existing strong NMT systems and our baselines, the bias module and sequential dependency can further boost the performance (i.e., from +1.12 to +1.52), which demonstrates that introducing complementariness does help the Multi-Unit Transformers. Fourth, we find it interesting that compared with the "Transformer + Relative (Big)", the basic "MUTE (Base, 4 Units)" can achieve better BLEU performance with only 54% of parameters, which indicates that our multi-unit approaches can leverage parameters more effectively and efficiently. 

 Results on WMT'14 English-German The results on WMT'14 En-De are shown in Table 2. We list several competitive NMT systems for comparison, which are divided into models based on Transformer-Base and models based on Transformer-Big. First of all, our models show significant BLEU improvements over two baselines in the Transformer-Base setting, ranging from +1.4 to +1.9 for "Transformer (Base)" and from +0.6 to +1.1 for "Transformer+Relative (Base)". That proves our methods perform consistently across languages and are still useful in large scale datasets. Next, among our own NMT methods, the sequentially biased model further improves the BLEU performance over our strong Multi-Unit model (from 28.8 to 29.3), which is consistent with our findings in the Zh-En 7 task and further proves the power of complementariness. Finally, compared with the existing NMT systems, we find that our models achieve comparable / better performance with much fewer parameters. The only exception is  (Wang et al., 2019a) , which learns a very deep (30 layers) Transformer. We regard these deep Transformer methods as orthogonal methods to ours, and it can be integrated with our MUTE models in future work. Additionally, we list several systems related to our multi-unit setting ("Existing Multi-Unit Style NMT Systems"), with diversity modeling or features space composition. As shown, MUTE models also outperform these methods, demonstrating the superiority of our methods in diverse and complementary modeling. 

 Results on WMT'18 Chinese-English In this section, we represent our results on WMT18 Chinese-English. The results are shown in  3. As we can see, our MUTE model still strongly outperforms the baseline "Transformer (Base)" and "Transformer+Relative (Base)" with +1.1 and +0.7 BLEU points. Noting that WMT'18 Zh-En has a much larger dataset (18.4M), and these findings proves that our model perform consistently well with different size of datasets. 

 Ablation Study In this section, we conduct the ablation study to verify each part of our proposed model. The results are shown in Table  4 . From our strongest Sequentially Biased model, we remove each of the four different units to validate which unit contributes the most to the performance. Then, we remove the bias module and sequential dependency independently to investigate each module's behavior. We come to the following conclusions: (1) All units make substantial contributions to "MUTE + Bias + Seq.", ranging from 0.44 to 0.79, proving the effectiveness of our design. (2) Among all units, the identity unit contributes most to our performance, which is consistent with our intuition that the identity unit should be responsible most for complementing other biased units. (3) The bias module and sequential dependency both contribute much to our Multi-Unit Transformers. We find it intriguing that, without the bias module, sequential dependency only provides marginal improvements. We conjecture that the complementary effect becomes minimal with no information gap among units.  

 Methods 

 Comparison with Averaging Checkpoints As we mentioned before, our MUTE models are inspired by ensembling methods. Therefore, it is necessary to compare our model with representative ensemble methods, e.g., averaging model checkpoints. Here, the comparison results of our MUTE model and averaging checkpoints in NIST Zh-En are shown in Table  5 . Since averaging checkpoints often leads to better generalization, we report the average BLEU scores over all test sets. We adopt two settings, namely averaging the last several (i.e., 5) checkpoints and averaging over models initialized with different seeds. The experiment with different initialization seeds fails. We conjecture the reason is that different seeds make models fall in different sub-optimals, and brutally combining them together makes the model perform badly. Then we average checkpoints over the last 5 saves, which gives us 44.97 BLEU points, which only outperforms the best checkpoint marginally (+0.14 in average), and MUTE performs much better  (45.43 and 45.82 BLEU points) . Specificially, our naive MUTE model suprasses the averaginig checkpoint method, and the sequential ordering and bias module enable a better interaction over different units. 

 Quantitative Analysis of Model Diversity Here, we empirically investigate which granularity should be used for better diversity among units. To verify the impact of multiple units compared with the single unit, we evaluate three different models: ? 4 Self. + 4 FFN, the model with four different self-attention modules and four different FFNs. ? 4 Self. + 1 FFN, the model with four selfattention modules and one shared FFN. ? 1 Self. + 4 FFN, the model with one shared self-attention module and four different FFNs. To control variables, these models include neither bias module nor sequential dependency. For each model, we evaluate the diversity among units for three outputs: (1) the outputs of selfattention modules, (2) the attention weights of selfattention modules, (3) the outputs of FFN modules. The diversity scores are computed by the exponential of the negative cosine distance among units, the same as proposed in  (Li et al., 2018) . DIV = exp(? o i o j |o i | ? |o j | ), (21) where DIV ? (0, 1) represents the diversity score for module outputs o i ? R d and o j ? R d . The results are shown in Table  6 . Above all, we find that multiple FFN layers can introduce diversity. As seen, "1 Self. + 4 FFN" produces a 0.381 diversity score on "FFN Sub.". Since we use a shared self-attention layer, which brings no diversity in the input-side of FFN layers, the difference is only brought by different FFN modules. We think the reason is that the RELU activation inside the FFN module serves as selective attention to filter out input information. Next, "4 Self. + 1 FFN" achieves 0.420 and 0.520 diversity scores for self-attention modules, which indicates that multiple self-attention modules focus on different parts of the input sequence and lead to diverse outputs. Then, "4 Self. + 4 FFN" has higher diversity scores than "4 Self. + 1 FFN" and "1 Self. + 4 FFN", which verifies our choice of using a combination of self-attention module and FFN as a basic unit. Finally, concerning the diversity scores among all models, we find that our full model with biased inputs and sequential dependency achieves the best diversity scores for all three outputs, which also achieves the best BLEU scores in previous experiments.  

 Effects on the Number of Units Another concern is how the MUTE models perform when increasing the number of units. Thus, in this section, we empirically investigate the effects on the number of units, in terms of model performance and inference speed. Here we use the basic Multi-Unit Transformer 8 . As shown in Figure  2 (a) and 2(b), increasing the number of units from 1 to 6 yields consistent BLEU improvement (from 46.5 to 47.5) with only mild inference speed decrease (from 890 tokens/sec to 830 tokens/sec). Besides, our model with four unit used in other experiments is faster than Transfomrer-Big (863 tokens/sec vs 838 tokens/sec). These results prove the computational efficiency of our MUTE model. We attribute this mild speed decrease (about 3.1% for four units and 6.7% for six units) for two reasons. First, the multi-unit model is naturally easy for parallelization. Each unit can be computed independently without waiting for other functions to finish. Second, we only widen the encoder, which is only computed once for each sentence translation. 

 Visualization We also present a visualization example of the learnable weights ? for units, shown in Figure  3 . As we can see, learnable weights ? show similar trends in "MUTE" and "MUTE + Bias". The weights for each unit within the same layer fall in a similar range (0.2 to 0.35), dispelling the worries that the biased units may be omitted or skipped. As for the "MUTE + Bias + Seq.", the weight distribution is very different. The weights nearly increase progressively when the unit index increases. Because the model weights are learned with backpropagation, the larger the weight, the more the model favors the corresponding unit. Thus, to some extent, this phenomenon demonstrates that the latter accumulated outputs are more potent than the preceding ones, and therefore, the latter single unit output complements the previous accumulation. 

 Related Work Recently Transformer-based models  (Vaswani et al., 2017; Ott et al., 2018; Wang et al., 2019a)  become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity. Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks  (Wu et al., 2019; Zhao et al., 2019; Lioutas and Guo, 2020) , improving the self-attention architecture  (Fonollosa et al., 2019; Wang et al., 2019b; Hao et al., 2019) , and deepening the Transformer architecture by dense connections  (Wang et al., 2019a) . Since our multi-unit framework makes no limitation about its unit, these models can be easily integrated into our multi-unit framework. There are also some works utilizing the power of multiple modules to capture complex feature representations in NMT.  use a vast network and a sparse gated function to select from multiple experts (i.e., MLPs).  Ahmed et al. (2017)  train a weighted Transformer by replacing the multi-head attention by self-attention branches. Nevertheless, these models ignore the modeling of relations among different modules. Then, some multihead attention variants  (Li et al., 2018 (Li et al., , 2019  introduce modeling of diversity or interaction among heads. However, complementariness is not taken into account in their approaches. Our MUTE models differ from their methods in two aspects. First, we use a powerful unit with a strong performance in diversity (Section 5.6). Second, we explicitly model the complementariness with bias module and sequential dependency. 

 Conclusion In this paper, we propose Multi-Unit Transformers for NMT to improve the expressiveness by introducing diverse and complementary units. In addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among  units. Experimental results show that our methods can significantly outperform the baseline methods and achieve comparable / better performance compared with existing strong NMT systems. In the meantime, our methods use much fewer parameters and only introduce mild inference speed degradation, which proves the efficiency of our models. 
