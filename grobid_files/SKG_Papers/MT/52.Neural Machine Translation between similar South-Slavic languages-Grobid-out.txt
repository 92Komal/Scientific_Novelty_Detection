title
Neural Machine Translation between similar South-Slavic languages

abstract
This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian-Slovenian and Serbian-Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data through back-translation. Automatic evaluation shows that multilingual systems with joint Serbian and Croatian data are better than bilingual, as well as that character-based cleaning leads to improved scores while using less data. The results also confirm once more that adding back-translated data further improves the performance, especially when the synthetic data is similar to the desired domain of the development and test set. This, however, might come at a price of prolonged training time, especially for multitarget systems.

Introduction Machine translation (MT) between closely related languages is, in principle, less challenging than translation between distantly related languages, but it is still far from being solved. While MT between closely related South-Western Slavic languages, Croatian, Slovenian and Serbian based on the rule-based (RBMT) and the phrase-based (PB-SMT) approaches has been investigated in the last years  (Etchegoyhen et al., 2014; Petkovski et al., 2014; Klubi?ka et al., 2016; Ar?an et al., 2016; Popovi? et al., 2016a) , to the best of our knowledge, the new state-of-the-art neural machine translation (NMT) has not been investigated yet for these languages. In this work, we first compare bilingual and multilingual systems in order to determine whether joining Serbian and Croatian data is useful. Afterwards, we investigate additional cleaning of remaining misaligned segments by using character n-gram matching scores  (Popovi?, 2015) . The beauty of the method for similar languages is that it can be applied directly to the given training corpus providing matching scores for each pair of the source-target segments. For distant languages, translation of one side of the training corpus would be required. Finally, we make use of monolingual data in each of the three languages by creating additional synthetic parallel training sets via back-translation  (Sennrich et al., 2016a; Poncelas et al., 2018; Burlot and Yvon, 2018) . 

 Language properties Common properties All three languages, Croatian, Serbian and Slovenian, belong to the South-Western Slavic branch. As Slavic languages, they have a very rich inflectional morphology for all word classes: six cases and three genders for all nouns, pronouns, adjectives and determiners. For verbs, person and many tenses are expressed by the suffix so that the subject pronoun is often omitted. There are two verb aspects, so that many verbs have perfective and imperfective form(s) depending on the duration of the described action. As for syntax, all three languages have quite a free word order, and neither language uses articles, either definite or indefinite. In addition to this, multiple negation is always used. 

 Croatian and Serbian Croatian and Serbian exhibit a large overlap in vocabulary and a strong morpho-syntactic similarity so that the speakers can understand each other without difficulties. Nev-ertheless, there is a number of small but notable and also frequently occurring differences between them. The largest differences between the two languages are in vocabulary: some words are completely different, some however differ only by one or two letters. Apart from lexical differences, there are also structural differences mainly concerning verbs: modal verb constructions, future tense, as well as conditional. Slovenian Even though Slovenian is very closely related to Croatian and Serbian, and the languages share a large degree of mutual intelligibility, a number of Croatian/Serbian speakers may have difficulties with Slovenian and the other way round. The nature of the lexical differences is similar to the one between Croatian and Serbian, namely a number of words is completely different and a number only differs by one or two letters. However, the amount of different words is much larger. In addition to that, the set of overlapping words includes a number of false friends (e.g. brati means to pluck in Croatian and Serbian but to read in Slovenian). The amount of grammatical differences is also larger and includes local word order, verb mood and/or tense formation, question structure, usage of cases, structural properties for certain conjunctions, as well as some other structural differences. Another important difference is the Slovenian dual grammatical number which refers to two entities (apart from singular for one and plural for more than two). It requires additional set of pronouns, as well as additional sets for noun, adjective and verb inflexion rules not existing either in Croatian or in Serbian. 

 Data For training, we used publicly available OPUS 1 parallel corpora  (Tiedemann, 2012)  indicated by the workshop organisers. OpenSubtitles is indicated for all translation directions. For Croatian-Slovenian, other corpora are indicated too, but they are either not sentence-aligned (JW300 ) or are extremely noisy (DGT, MultiParaCrawl ). Therefore, we decided to use only OpenSubtitles for all translation directions. It is worth noting that the organisers also indicated the SETIMES News parallel Croatian-Serbian corpus. Developing an additional Croatian-Serbian MT system for converting Serbian data into Croatian and vice versa was shown to be helpful for the PBSMT approach  (Popovi? and Ljube?i?, 2014; Popovi? et al., 2016b) . However, our preliminary experiments in this direction indicated that this technique is not helpful for the NMT approach. The original parallel data were filtered in order to eliminate noisy parts: too long segments (more than 100 words), segment pairs with disproportional sentence lengths, segments with more than 1/3 of non-alphanumeric characters, as well as duplicate segment pairs were removed. The statistics of the remaining subtitles together with the development and test sets is shown in Table  1 . The development and test sets were provided by the organisers and originate from Public Relations publications of a business intelligence company. 

 Additional cleaning of OpenSubtitles While a large number of noisy parts and misaligned segments was removed from OpenSubtitles by the basic filtering procedure, a number of misaligned segments still remained. In order to remove these, we applied additional cleaning based on the character n-gram F-score chrF usually used for MT evaluation  (Popovi?, 2015) . For the purpose of cleaning, the chrF score is calculated for each pair of segments in the training data. Due to similarity between the languages, the scores between the properly aligned segments are higher than the scores of misaligned segments. Nevertheless, the languages are sufficiently different so that some properly aligned short segments (or single words) can have low scores, too. Still, if those words also appear in longer sentences, they will not be removed. Preliminary experiments with different thresholds showed that keeping the segments with the chrF score equal or greater than 20 is the best option. 

 Using monolingual data In addition to the parallel OpenSubtitles corpora, we also used the monolingual data in each of the three languages which were indicated by the organisers, namely the mixed-domain data collected from Web, hrWac, slWac and hrWac  (Ljube?i? and Erjavec, 2011; Ljube?i? and Klubi?ka, 2014) . As a first step, we removed too long and too short sentences, keeping those between 5 and 60 words. Then, we removed sentences with more than 1/3 of non-alphanumeric characters, sentences with URLs, as well as duplicate sentences. Then, we wanted to rank these sentences according to the relevance for our experiments, namely according to their similarity to the development corpus. For this purpose, we used Feature Decay Algorithm (FDA)  (Bic ?ici and Yuret, 2011) . This method iteratively selects sentences from an initial set S based on the number of n-grams which overlap with an in-domain text Seed and adds these sentences to a selected set Sel. In addition, in order to promote a diversity, after a sentence is selected, its n-grams suffer a penalisation so that they are less likely to be selected in the following iterations. The default FDA system halves the score of an ngram each time it is selected. Therefore the score of a sentence s is computed as in Equation (1): score(s, Seed, Sel) = ngr?{s Seed} 0.5 C Sel (ngr) length(s) (1) where Sel is the set of sentences that have been selected and C S el(ngr) is the count of occurrences of the n-gram ngr. At the end, the set S is converted into the set Sel containing the same sentences, but ranked according to their relevance. For our experiments, the hrWac, slWac and srWac corpora represented the sets S, and the development sets in the corresponding target language were used as Seed. 

 Back-translated synthetic parallel corpora After ranking the monolingual corpora by FDA, back-translation was applied in order to create additional parallel training corpora. For each translation direction, the first two million best ranked sentences in the target language were translated into the source language by the corresponding NMT system. Translation from Slovenian: The first two million best ranked Serbian sentences and the first two mil-lion best ranked Croatian sentences were translated into Slovenian. Translation into Slovenian: Slovenian is the target language for two translation directions, and we wanted to have equally relevant Slovenian sentences for both directions. Therefore, we did not take the first two million sentences for one source language and the second two million for the other, because the Slovenian sentences for the first source language would be more relevant than those for the second source language. Instead, we took the first four million best ranked Slovenian sentences, and then translated every odd sentence into Serbian and every even sentence into Croatian. 

 MT systems All our systems are built using the Sockeye implementation  (Hieber et al., 2018)  of the Transformer architecture  (Vaswani et al., 2017) . The systems operate on sub-word units generated by byte-pair encoding (BPE)  (Sennrich et al., 2016b) . We set the number of BPE merging operations at 32000. We use shared vocabularies between the languages because they are similar. Multilingual systems are built using the same technique as  (Johnson et al., 2017)  and  (Aharoni et al., 2019) , namely adding a target language label "SR" or "HR" to each source sentence. We investigated the following set-ups: 

 Systems trained on OpenSubtitles The four bilingual systems, HR?SL, SR?SL, SL?HR and SL?SR, are trained separately for each language pair and each translation direction on about 11M parallel segments. The multisource system HR+SR?SL is trained for translation into Slovenian by joining Serbian and Croatian sources and removing duplicates, thus resulting in 20.2M parallel segments. The multitarget system SL?HR+SR is trained for translation from Slovenian on the reversed corpus of 20.2M segments with target language identificators "SR" and "HR" added to the source side.  

 Systems trained on cleaned OpenSubtitles 

 Results We evaluate our systems using the following three automatic overall evaluation scores: sacre-BLEU  (Post, 2018) , chrF  (Popovi?, 2015)  and char-acTER  (Wang et al., 2016) . The BLEU score is used because of the long tradition. The two character level scores are shown to correlate much better with human assessments  (Bojar et al., 2017; Ma et al., 2018) , especially for morphologically rich languages. In addition, the chrF score is recommended as a replacement for BLEU in a recent detailed study encompassing a number of automatic MT metrics  (Mathur et al., 2020) . In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus and the training time. Table  2  shows the results both on the development and on the test set for each of the four translation directions. First of all, it can be seen that the automatic scores are relatively low given the similarity of the languages. One reason is domain/genre discrepance between the training and the development/test sets. Another possible reason is the nature of the OpenSubtitles corpus. The majority of non-English texts in OpenSubtitles are namely human translations from English originals. Therefore, for translation from English, the source language is the original one and the target language is its human translation. 2 On the other hand, for translation not involving English, both sides are human translations, which can have a strong impact on performance  (Kurokawa et al., 2009; Vyas et al., 2018; Zhang and Toral, 2019) . These effects should be investigated in future work. 2 And other way round for translation into English. Results on the development set For the systems trained on OpenSubtitles, it can be seen that for each translation direction, multilingual systems yield better automatic scores than bilingual systems at the cost of slightly prolonged training time (from about 3 days to 3-4 days). Therefore we choose the two multilingual systems HR+SR?SL and SL?HR+SR as the baselines and we did not keep the bilingual systems for further experiments. The chrF cleaning of OpenSubtitles reduces the size of the corpus and the training time while slightly improving automatic scores. The reduction in time is slightly smaller for the multitarget translation from Slovenian (down to 2-3 days) than for the multisource translation into Slovenian (down to less than 2 days). Adding the back-translated data from Wac improves the automatic scores for more than 10 points for multisource translation (into Slovenian) and for 5 to 10 points for multitarget translation (from Slovenian). This could be expected, especially since the monolingual data was chosen to be similar to the development data. Nevertheless, this large improvement comes at a price. Although the increase of the corpus is not very large, from 10.8M to 14.8M, the training time increases to (more than) 3 days. It can be noted that for some set-ups, the multitarget system needs more training time. The probable reason is the diversity of the target part of the training corpus -the system has to deal with two target languages, and when synthetic data is added, also with two different domains/genres for each of them. Results on the test set Based on the results on the development set, we submitted the outputs of the systems with backtranslated data (HR+SR?SL CLEAN+BT, SL?HR+SR CLEAN+BT) as primary submissions. The outputs of the systems trained on cleaned data (HR+SR?SL CLEAN, SL?HR+SR CLEAN) were submitted as first contrastive, and the outputs of the baseline multilingual systems (HR+SR?SL, SL?HR+SR) as second contrastive submissions. The test sets were not at all translated by the initial bilingual systems, therefore the results are not available. It can be seen that the tendencies for the test set are almost the same as for the development set. The only difference is the larger improvement obtained by cleaning OpenSubtitles with the chrF scores. Further detailed analysis involving manual inspec- tion is needed to better understand this difference. 

 Summary and outlook This work investigates different set-ups for training NMT systems for translation between three closely related South-Slavic languages: Slovenian on one side, and Serbian and Croatian on the other side. We explore different sizes and types of training corpora, as well as bilingual and multilingual systems. Our results show that for all translation directions, multilingual systems with joint Croatian and Serbian data perform better than bilingual systems. The results also show that cleaning misaligned segments using character n-gram matching (chrF score) represents a fast and useful method for closely related languages, which improved the evaluation scores while reducing corpus size and training time. Finally, we confirm that adding backtranslated synthetic data, which is the usual practice in neural machine translation, can yield large improvements of evaluation scores also for these languages. Nevertheless, for multitarget translation, it might result in a prolonged training time due to increased variety of the target language side. Future work should include more genres and domains, as well as detailed analysis of errors and problems in order to further improve the performance of NMT between South Slavic languages. Two multilingual systems HR+SR?SL CLEAN and SL?HR+SL CLEAN are trained on joint OpenSubtitles corpora additionally cleaned by the chrF score. The cleaned corpus consists of 10.8M segments (instead of 20.2M). 3. Systems trained on cleaned OpenSubtitles and synthetic back-translated parallel Wac data Two multilingual systems HR+SR?SL CLEAN+BT and SL?HR+SR CLEAN+BT are trained on joint cleaned OpenSubtitles corpora together with the corresponding synthetic back-translated data selected from hrWac, slWac and srWac. The monolingual data was back-translated by the corresponding systems trained on cleaned OpenSubtitles. The training corpora consist of 14.8M segments. 

 Table 1 : 1 Corpus statistics. lang. set domain # sentences sl-hr train Subtitles 11 213 386 dev PR publications 2457 test PR publications 2582 sl-sr train Subtitles 11 780 062 dev PR publications 1259 test PR publications 1260 1 http://opus.nlpl.eu/ 

 Table 2 : 2 Results: Croatian?Slovenian (a), Serbian?Slovenian (b), Slovenian?Croatian (c) and Slovenian?Serbian: corpus size, training time, and the three automatic MT evaluation scores (BLEU, chrF and characTER). (a) Croatian?Slovenian training dev, hr?sl test, hr?sl system size time BLEU chrF chrTER BLEU chrF chrTER HR?SL 11.2M ?3 days 38.5 65.7 29.4 / / / HR+SR?SL 20.2M 3-4 days 38.8 65.9 29.5 34.7 62.2 34.5 HR+SR?SL CLEAN 10.8M <2 days 39.7 66.5 27.0 37.1 65.2 28.2 HR+SR?SL CLEAN+BT 14.8M ?3 days 53.9 77.7 18.9 51.9 76.4 20.0 (b) Serbian?Slovenian training dev, sr?sl test, sr?sl system size time BLEU chrF chrTER BLEU chrF chrTER SR?SL 11.8M ?3 days 40.6 67.2 30.3 / / / HR+SR?SL 20.2M 3-4 days 42.1 68.3 28.5 37.7 64.1 33.5 HR+SR?SL CLEAN 10.8M <2 days 42.2 68.6 26.9 41.2 68.1 26.5 HR+SR?SL CLEAN+BT 14.8M ?3 days 58.0 80.4 18.5 55.2 78.4 19.1 (c) Slovenian?Croatian training dev, sl?hr test, sl?hr system size time BLEU chrF chrTER BLEU chrF chrTER SL?HR 11.2M ?3 days 33.4 62.6 33.0 / / / SL?HR+SR 20.2M 3-4 days 36.0 63.8 32.6 30.3 58.9 40.0 CLEAN 10.8M 2-3 days 36.9 65.2 28.6 35.7 64.4 28.8 SL?HR+SR CLEAN+BT 14.8M >3 days 46.1 72.7 22.8 45.1 72.3 23.3 (d) Slovenian?Serbian training dev, sl?sr test, sl?sr system size time BLEU chrF chrTER BLEU chrF chrTER SL?SR 11.8M ?3 days 33.3 62.3 34.3 / / / SL?HR+SR 20.2M 3-4 days 34.8 63.4 33.4 32.0 60.0 36.4 SL?HR+SR CLEAN 10.8M 2-3 days 35.5 64.2 31.5 37.0 65.1 28.2 SL?HR+SR CLEAN+BT 14.8M >3 days 45.5 73.3 23.4 47.6 73.6 22.1
