title
What is Hidden among Translation Rules

abstract
Most of the machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.

Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example  (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008) , employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in  (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009)  to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view rules, which are observed from training data, as random variables generated by a hidden model. This generative process itself is also hidden. All possible generative processes can be represented with factorized structures such as weighted hypergraphs and finite state machines. This approach leads to a compact model that has better generalization capability and allows translation rules not explicitly observed in training date. This paper reports work-in-progress to exploit hidden relations among rules. Preliminary experiments show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 

 Hidden Models Let G = {(r, f )} be a grammar observed from parallel training data, where f is the frequency of a bilingual translation rule r. Let M be a hidden model that generates every translation rule r. For example, M could be modeled with a weighted hypergraph or finite state machine. For the sake of convenience, in this section we assume M is a meta-grammar M = {m}, where each m represents a meta-rule. For each translation r, there exists a hypergraph H r that represents all possible derivations D r = {d} that can generate rule r. Here, each derivation d is a hyperpath using metarules M d , where M d ? M. Thus, we can use hypergraph H r to characterize r. Translation rules in G can share nodes and meta-rules in their hypergraphs, so that M is more compact model than G. In the rest of this section, we will introduce three methods to quantify H r as features of rule r. It should be noted that there are more ways to exploit the compact model of M than these three. 

 Type 1 : A Generative Model Let ? be the parameters of a statistical model P r(m; ?) for meta-rules m in meta-grammar M estimated from the observed translation grammar G. The probability of a translation rule r can be calculated as follows. P r(r; ?) ? P r(H r ; ?) = d?Dr P r(d; ?) (1) By assuming separability, P r(d; ?) = m?M d P r(m; ?) (2) we can further decompose rule probability P r(r; ?) as below.  In practice, P r(r; ?) in (3) can be calculated through bottom-up dynamic programming on hypergraph H r . Hypergraphs of different rules can share nodes and meta-rules. This reveals the underlying relationship among translation rules. As a by-product of this generative model, we use the log-likelihood of a translation rule, log P r(r; ?), as a new dense feature. We call it Type 1 in experiments. 

 Type 2 : Meta-Rules as Sparse Features As given in (3), likelihood of a translation rule is a function over P r(m; ?), in which ? is estimated from the training data with a generative model. Previous work in  (Chiang et al., 2009)  showed the advantage of using a discriminative model to optimize individual weights for these factors towards a better automatic score. Following this practice, we treat each meta-rule m as a sparse feature. Feature value f (m) = 1 if and only if m is used in hypergraph H r . Otherwise, its default value is 0. We call these features Type 2 in experiments. The Type 2 system contains the log-likelihood feature in Type 1. 

 Type 3 : Posterior as Feature Values A natural question on the binary sparse features defined above is why all the active features have the same value of 1. We use these meta-rules to represent a translation rule in feature space. Intuitively, for meta-rules with closer connection to the translation rules, we hope to use relatively larger feature values to increase their effect. We formalize this intuition with the posterior probability that a meta-rule m is used to generate r, as below. f (m) ? P r(m|r; ?) (4) = P r(m, r; ?) P r(r; ?) = d?Dr,m?M d P r(d; ?) P r(r; ?) The posterior in (4) could be too sharp. Following the common practice, we smooth the posterior features with a scaling factor ?. f (m) ? P r(m|r) ? We use Type 3(?) to represent the posterior model with a scaling factor of ? in experiments. The Type 3 systems also contain the log-likelihood feature in Type 1. 

 Parameter Estimation Now we explain how to obtain parameter ?. With proper definition of the underlying model M, we can estimate ? with the traditional EM algorithm or Bayesian methods. In the next section, we will present an example of the hidden model. We will employ the EM algorithm to estimate the parameters in ?. Here, translation rules and their frequencies in G are observed data, and derivation d for each rule r is hidden. At the Expectation step, we search all derivations d in D r of each rule r and calculate their probabilities according to equation (2). At the Maximization step, we re-estimate ? on all derivations in proportion to their posterior probability. 

 Case Study In Section 2, we explored the use of meta-grammars as the underlying model M and developed three methods to define features. Similar techniques can be applied to finite state machines and other underlying models. Now, we introduce a POS-based underlying model to illustrate the generic model proposed in Section 2. We will show experimental results in Section 4. 

 Meta-rules on POS tags Let r ? G be a translation rule composed of a pair of source and target word strings (F w , E w ). Let F p and E p be the POS tags for the source and target sides respectively. For the sake of simplicity as the first attempt, we treat non-terminal as a special word X with POS tag X. Suppose we have a Chinese-to-English translation rule as below. 

 yuehan qu zhijiage ? john leaves for chicago We call NR VV NR ? NNP VBZ IN NNP (5) a translation rule in POS tags. We will propose an underlying model M to generate translation rules in POS tags instead of translation rules themselves. For the rest of this section, we take translation rules in POS tags as the target of our generative model. We define meta-rules on pairs of POS tag strings, e.g. NR VV ? NNP VBZ . We can decompose the probability of translation rule in (5) into a product on meta-rule probabilities via various derivations, such as  

 The Underlying Model and Features Now, we introduce a generative model M for translation rules in POS tags. We still use the example in (5) as shown in Figure  1 , where the top box represents the source side and the bottom box represents the target side. Dotted lines represent word alignments on three pairs of words. We first generate the number of source tokens of a translation rule with a uniform distribution for up to, for example, 7 tokens. Then we split the source side into chunks with a binomial distribution with a Bernoulli variable at the gap between each two continuous words, which splits the two words into two chunks with a probability of p. For example, the probability of obtaining two chunks NR VV and NR is (1 ? p)p, as shown in Figure  1 . To sum up, the probability of a derivation d for a translation rule r : F ? E is P r(d) ? P r ? 1 (|F |) ? P r ? 2 (F s ) ? m?M d P r ? 3 (|E m | | |F m |) ? m?M d P r ? 4 (m | |F m |, |E m |) (6) where F m and E m are source and target sides of a meta-rule m used in derivation d, and F s is a splitting of the source side. As for the distributions, we have ? 1 ? U nif orm ? 2 ? Binomial ? 3 ? Categorical ? 4 ? Categorical where ? 1 and ? 2 have pre-selected hyperparameters, and ? 3 and ? 4 are estimated with the EM algorithm. As for sparse features, we will obtain 7 meta-rule features as below. ? NR ? NNP ? VV ? VBZ ? VV ? VBZ IN ? NR VV ? NNP VBZ ? NR VV ? NNP VBZ IN ? VV NR ? VBZ IN NNP ? NR VV NR ? NNP VBZ IN NNP All of them respect the word alignment, which means that ? there is no alignment that aligns one word in a meta-rule with the other out of the same metarule, and ? there is at least one alignment within a metarule. 

 Implementation Details Even though the size of all possible meta-rules is much smaller than the space of translation rules, it is still too large to work with existing optimization methods for sparse features in MT, i.e. MIRA  (Chiang et al., 2009)  or L-BFGS  (Matsoukas et al., 2009) . In practice, we have to limit the feature space to around 20,000 dimensions. For this purpose, we first use a frequency based method to filter meta-rule features. Specifically, we first divide all the meta-rules into 100 bins, (|F |, |E|), where |F | is the number of words on the source side, and |E| the target side, 0 < |F |, |E| ? 10. For each bin, we keep the same top k-percentile of the meta-rules such that we obtain a total of 20,000 meta-rules as features. A shortcoming of this filtering method is that all these features are positive indicators, while lowfrequency negative indicators are discarded. In order to keep the features of various level of frequency, we define class features with a 3-tuple C(|F |, |E|, q), where |F | and |E| are numbers of source and target words as defined above, and q is the integer part of the log 2 value of the feature frequency in the training data. 

 System In this way, each meta-rule feature can be mapped to one of these classes. The value of a class feature equals the sum of the meta-rule features that mapped into this class. We have about 2,000 class features defined in this way. They are applied on both Type 2 and Type 3 features. 

 Experiments We carry out our experiments on web genre of Chinese-to-English translation. The training set contains about 10 million parallel sentences available to Phase 1 of the DARPA BOLT MT task. The tune set contains 1275 sentences. Each has four references. There are two test sets. Test-1 is from a similar source of the tune set, and it contains 1239 sentences. Test-2 is the web part of the MT08 evaluation data. Our baseline system is a home-made Hiero (Chiang, 2005) style system. The baseline rule set contains about 17 million rules. It contains about 40 dense features, including a 6-gram LM. The sparse feature optimization algorithm is similar to the MIRA recipe described in  (Chiang et al., 2009) . We optimize on TER-BLEU  (Snover et al., 2006; Papineni et al., 2001) . The BLEU, TER and T-B scores on the two tests are shown in Tables  1 and 2  system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation 1 , thanks to comprehensive features in the baseline system and more data in training. All the three types of systems provide consistent improvement on both test sets in terms of T-B, our optimization metric. Type 1 gives marginal improvement of 0.2. This shows the limitation of the generative feature. When we use meta-rules as binary sparse features in Type 2, we obtain about one point improvement on T-B on both sets. This shows the advantage of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 

 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to  (Huang et al., 2010) . The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research  (Xia, 2001; Prolo, 2002) . Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG)  (Joshi et al., 1975; Joshi and Schabes, 1997)  and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1 http://www.itl.nist.gov/iad/mig/tests/mt/2008/ hand,  (Dras, 1999)  showed how a meta-level grammar could help in modeling parallel operations in  (Shieber and Schabes, 1990) . Our work is another effort of statistical modeling of well-recognized linguistic insight in NLP and MT. 

 Conclusions and Future Work In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. As a case study to capitalize this model, we presented three methods to enrich rule modeling with features defined on a hidden model. Preliminary experiments verified gain of one point on TER-BLEU over a strong baseline in Chinese-to-English translation. As for future work, we plan to extend this work in the following aspects. ? To try other prior distributions to generate the number of source tokens. ? Unsupervised and semi-supervised learning of hidden models. ? To incorporate rich models into the generative process, e.g. reordering, non-terminals, structural information and lexical models. ? To improve the posterior model with better parameter estimation, e.g. Bayesian methods. ? To replace the exhaustive translation rule set with a compact meta grammar that can create and parameterize new translation rules dynamically, which is the ultimate goal of this line of work. ? P r(NR VV , NNP VBZ ) ? P r(NR, IN NNP ), and ? P r(NR, NNP) ? P r(VV , VBZ IN ) ? P r(NR, NNP). 

 Figure 1 : 1 Figure 1: An example 

 Suppose we split the target side into two parts, NNP VBZ and IN NNP, which respects the word alignments. It generates two meta-rules NR VV ? NNP VBZ and NR ? IN NNP, as shown in Figure 1. The probability for the first meta-rule is P r(|E| = 2 | |F | = 2) ? P r(NR VV , NNP VBZ | |F | = 2, |E| = 2), where |F | represents the number of source tokens, and |E| the number of target tokens. Similarly, the probability of the second one is as follows. P r(|E| = 2 | |F | = 1) ? P r(NR, IN NNP | |F | = 1, |E| = 2). 

 . It should be noted that, even though our metric of tuning is T-B, the baseline System BLEU% TER% T-B Baseline 25.80 56.96 31.16 Type 1 26.18 57.09 30.91 Type 2 26.63 56.64 30.01 Type 3 (1) 26.30 57.00 30.70 Type 3 (0.1) 26.34 56.73 30.39 Type 3 (0.01) 26.50 56.73 30.23 Table 2: scores on test-2 (MT08-WB)
