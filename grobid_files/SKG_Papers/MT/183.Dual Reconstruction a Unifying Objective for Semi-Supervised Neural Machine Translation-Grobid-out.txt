title
Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation

abstract
While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.

Introduction Taking advantage of monolingual training data via Back-Translation  (Sennrich et al., 2016a) , Iterative Back-Translation  Cotterell and Kreutzer, 2018)  or Dual Learning  (He et al., 2016)  has become a de facto requirement for building high quality Neural Machine Translation (NMT) systems  (Edunov et al., 2018; Hassan et al., 2018) . However, these methods rely on unrelated heuristic optimization objectives, and it is not clear what their respective strengths and weaknesses are, nor how they relate to the ideal but intractable objective of maximizing the marginal likelihood of the monolingual data (i.e., p ? (y) = x p ? (y | x)q(x) given target sentences y, an NMT model p ? (y | x), and the prior distribution q(x) on source x). Instead of proposing new methods, this paper sheds new light on how these established techniques work and how to use them. We introduce a dual reconstruction objective to theoretically ground the comparison of semi-supervised training strategies that leverage monolingual data from both source and target languages (Figure  1 ). In Section 3, we show that, under some assumptions, this * Work was done at the University of Maryland. 

 NMT Model ? " ? ?) NMT Model ? ( ? ?) ? ) ? ? + = ? . )~0 1 . 2) ? ( ? ? )) NMT Model ? ( ? ?) NMT Model ? " ? ?) ? ) ? source-target-source reconstruction target-source-target reconstruction inference reconstruction ? 3 = ? 2 )~4 5 2 .) ? " ? ? )) Figure  1 : Our dual reconstruction objective sums 1) a target-source-target objective J 1 on target sentences y using the NMT model q ? (x | y) for inference and p ? (y | x) for reconstruction, and 2) a sourcetarget-source objective J 2 on source sentences x using p ? (y | x) for inference and q ? (x | y) for reconstruction. Models connected by dotted arrows share parameters. objective remarkably shares the same global optimum as the intractable marginal likelihood objective where the model's marginal distribution p ? (y) coincides with the target sentence distribution p(y). We also show that Iterative Back-Translation (IBT) and Dual Learning can be viewed as different ways to approximate its optimization. Theory suggests that IBT approximates the dual reconstruction objective more closely than the more complex Dual Learning approach, and in particular that Dual Learning's additional language model loss is redundant. We investigate whether these differences matter in practice by conducting the first controlled empirical comparison of Back-Translation, IBT, and Dual Learning in high-resource (WMT de-en), low-resource (WMT tr-en), and cross-domain settings  (News?TED, de-en) . Results support our theory that the additional language model loss and policy gradient estimation in Dual Learning is redundant and show that IBT outperforms the more complex Dual Learning algorithm in terms of translation quality. Furthermore, we also compare different optimization strategies used in IBT to better balance translation quality against the computational cost. 

 Background Notation NMT models the probability of translating a source sequence x into a target y as p ? (y | x) = T t=1 p(y t | y <t , x; ?) where ? represents the model parameters, and T is the length of y  (Bahdanau et al., 2015) . The model computes the conditional probability of the next token at time t by p(? | y <t , x; ?) = softmax(a(h t )), where a(?) is a linear transformation, and h t is the hidden representation at step t usually modeled by an encoder-decoder network h t = f (y <t , x). In supervised settings, NMT models are trained to maximize the likelihood of parallel sentence pairs: J s (?) = (x,y)?D log p ? (y | x) given data D = {(x (n) , y (n) )} N n=1 . IBT and Dual Learning exploit large monolingual corpora which represent source and target language distributions better than the limited parallel corpora. Back-Translation trains the source-totarget translation model p ? (y | x) by maximizing the conditional log-likelihood of target language sentences y given pseudo source sentences x inferred by a pre-trained target-to-source translation model q ? (x | y) given y. IBT optimizes the dual translation models p ? (y | x) and q ? (x | y) via backtranslation in turn, both for semi-supervised  Hoang et al., 2018; Cotterell and Kreutzer, 2018; Niu et al., 2018)  and unsupervised MT  (Artetxe et al., 2018; Lample et al., 2018a,b) . Dual Learning takes the view of cooperative game theory where dual models collaborate with each other to learn to reconstruct the observed source and target monolingual sentences, and is widely used for semi-supervised  (He et al., 2016) , unsupervised  (Wang et al., 2019) , and zero-shot multilingual NMT  (Sestorain et al., 2018) . Concretely, Dual Learning optimizes p ? (y | x) and q ? (x | y) jointly by reconstructing the original target sentence y using p ? (y | x) given the source x inferred by q ? (x | y), and vice versa. The reconstruction loss is augmented with a language model loss and used to update both reconstruction and inference models via policy gradient  (Williams, 1992) . While Dual Learning and IBT each improve BLEU over Back-Translation  Cotterell and Kreutzer, 2018; He et al., 2016) , they have not been compared directly to each other.  Cotterell and Kreutzer (2018)  interpret Back-Translation as a variational approximation where the pseudo source x can be viewed as a latent vari-able and the target-to-source model q ? (x | y) is an inference network that approximates the posterior distribution p ? (x | y). Furthermore, they explain IBT as a way to better approximate the true posterior distribution with the target-to-source model. However, it is unclear how their heuristic objective relates to the ideal objective of maximizing the model's marginal likelihood of the target language monolingual data. More recently,  He et al. (2020)    p ? (x, y) = p ? (y | x)q(x) where the source x is randomly sampled from the prior distribution q(x) estimated by the empirical data distribution q data (x) based on the abundant source monolingual data M X = {x (m) } M m=1 : q data (x) = 1 |M X | , if x ? M X 0, otherwise and the target translation y is sampled from the translation model p ? (y | x) conditioned on x. Given the target sentence distribution p(y) estimated by the empirical data distribution p data (y) of target monolingual data M Y = {y (m) } M m=1 , we can view x as a latent variable and maximize the marginal log-likelihood J u (?) = E y?p(y) [log p ? (y)] where p ? (y) is the model's marginal likelihood p ? (y) = x p ? (x, y). The global optimum of the objective is achieved when the model's marginal distribution p ? (y) perfectly matches the target sentence distribution p(y).  1  However, directly optimizing the marginal likelihood p ? (y) is intractable due to the infinite space of x. We can instead apply variational autoencoding (VAE) models by introducing an inference network p ? (x | y) and maximize the variational lower-bound (ELBO) of log p ? (y): log p ? (y) ?E x?p ? (x | y) [log p ? (y | x)] ? D KL [ p ? (x | y)|| q(x)] (1) where D KL [ p ? || q] is the Kullback-Leibler (KL) divergence. However, estimating the prior distribution q(x) by the discrete data distribution q data (x) makes it difficult to directly compute the KL term. One can estimate q(x) using a language model (LM) trained to maximize the likelihood of the source monolingual data  (Miao and Blunsom, 2016; Baziotis et al., 2019) , at the cost of introducing additional model bias into the translation model. The non-differentiable KL term requires gradient estimators such as policy gradient  (Williams, 1992)  or Gumbel-softmax  (Jang et al., 2017) , which may introduce further training noise  (He et al., 2020) . To address these issues, we introduce the dual reconstruction objective, which includes two reconstruction terms that resemble the first term in the ELBO objective (Eq. (  1 )) while excluding the KL term that is challenging to optimize and show that this objective has desirable properties and can be better approximated in practice. Definition 3.1. Given prior distributions q(x) and p(y) over the sentences x in the source language space ? x and y in the target language space ? y , we define the dual reconstruction objective J dual (?, ?) for dual translation models p ? (y | x) and q ? (x | y) as the sum of the targetsource-target objective J 1 and source-target-source objective J 2 : J dual (?, ?) = J 1 (?, ?) + J 2 (?, ?) J 1 (?, ?) = E y?p(y) E x?q ? (x | y) [log p ? (y | x)] J 2 (?, ?) = E x?q(x) E y?p ? (y | x) [log q ? (x | y)] (2) For J 1 , the target-to-source model q ? (x | y) serves as the inference model to produce pseudo source sequences x given target sequences y and p ? (y | x) serves as the reconstruction model to reconstruct y given x, and vice versa for J 2 . We first define the mutual information constraint in Section 3.2 and show in Section 3.3 that J dual (?, ?) shares the same global optimum as the marginal likelihood objective which is intractable to optimize directly. 2 In Section 3.4, we compare and contrast how IBT and Dual Learning approximate J dual (?, ?). 

 Mutual Information Constraint The global optimum of the marginal likelihood objective is achieved when the model's marginal distribution p ? (y) = p(y). Given a translation model with enough capacity without any constraint on how the model output is dependent on the source context, this could lead to a degenerate solution p ? (y | x) = p(y) where the model ignores the source input and memorizes the monolingual training data. We constrain the translation model to avoid this situation, using the mutual information of a conditional distribution p ? (y | x) which measures how much y is dependent on x in p ?  (Hoffman and Johnson, 2016) . Here, this mutual information measures the degree to which model translations depend on the source. Definition 3.2. Given a prior distribution q(x) over x ? ? x , we define the mutual information I p ? of x and y in the conditional distribution p ? (y | x): I p ? = E x?q(x) [D KL [ p ? (y | x)|| p ? (y)]] (3) where p ? (y) is the marginal distribution: p ? (y) = x p ? (y | x)q(x) (4) To avoid the degenerate solution, we constrain the model's mutual information by: 0 ? I min ? I p ? ? I max ? max p?P XY I p (x; y) where I min and I max are pre-defined constant values between zero and the maximum mutual information between x and y given any joint distribution p(x, y) ? P XY whose marginals satisfy x p(x, y) = p(y) and y p(x, y) = q(x).  Hled?k et al. (2019)  prove that the maximum mutual information max p?P XY I p (x; y) = min(H [q(x)] , H [p(y)]), where H [q(x)] and H [p(y)] are the entropy of prior distributions q(x) and p(y). Thus, the maximum mutual information should be large enough to properly bound the model's mutual information if q(x) and p(y) are defined on large monolingual corpora M X and M Y . Intuitively, the constraint requires that the model's mutual information cannot be so small that the model ignores the source context nor so large such that is not robust to the noise in the source input. We will show in Section 4.4 that in practice, this constraint is met when jointly optimizing the supervised and unsupervised objectives without explicitly applying constrained optimization. 

 Understanding the Global Optimum of the Dual Reconstruction Objective We first characterize the upper bound of the dual reconstruction objective. Proposition 1. Given prior distributions q(x) and p(y) over x ? ? x and y ? ? y , if parameterized probability models p ? and q ? have enough capacity under the constraint that: 0 ? I min ? I p ? , I q ? ? I max ? max p?P XY I p (x; y) where I min and I max are pre-defined constant values between zero and the maximum mutual information between x and y given any joint distribution p(x, y) ? P XY whose marginals satisfy x p(x, y) = p(y) and y p(x, y) = q(x). Then, the dual reconstruction objective is upperbounded by J dual (?, ?) ? 2I max ? H [q(x)] ? H [p(y)], and the upper bound is achieved iff I q ? = I max I p ? = I max p ? (y | x) = q ? (x | y) q ? (x) p(y) q ? (x | y) = p ? (y | x) p ? (y) q(x) (5) Proof. First we prove that J 1 (?, ?) ? I max ? H [p(y)], and the upper bound is achieved iff I q ? = I max p ? (y | x) = q ? (x | y) q ? (x) p(y) To show this, we denote the posterior distribution Q(y | x) = q ? (x | y) q ? (x) p(y), and rewrite J 1 : J 1 =E y?p(y) E x?q ? (x | y) [log p ? (y | x)] =I q ? ? H [p(y)] ? D KL [ q ? (x | y)p(y)|| p ? (y | x)q ? (x)] Since the KL divergence between two distributions is always non-negative and is zero iff they are equal, we have J 1 (?, ?) ? I q ? ? H [p(y)] ? I max ? H [p(y)] and J 1 (?, ?) = I max ? H [p(y)] iff I q ? = I max D KL [ q ? (x | y)p(y)|| p ? (y | x)q ? (x)] = 0 The second equality holds iff p ? (y | x) = q ? (x | y) q ? (x) p(y) Similarly, we can prove that J 2 (?, ?) ? I max ? H [q(x)], and the upper bound is achieved iff I p ? = I max q ? (x | y) = p ? (y | x) p ? (y) q(x) thus J dual (?, ?) ? 2I max ? H [q(x)] ? H [p(y)] and the upper bound is achieved iff ? and ? satisfy Eq. (  5 ), concluding the proof. Proposition 1 shows that J dual (?, ?) has an upper bound that could be reached when the mutual information of p ? (y | x) and q ? (x | y) are maximized, and p ? (y | x) and q ? (x | y) are equal to the posterior distribution for each other. Next we show that the upper bound is indeed the global maximum of the objective J dual (?, ?), as there exists a solution for the above conditions (proof in Appendix A.2). Proposition 2. Given distributions q(x) and p(y) over x ? ? x and y ? ? y , if parameterized probability models p ? and q ? have enough capacity under the constraint that: 0 ? I min ? I p ? , I q ? ? I max ? max p?P XY I p (x; y) (6) where I min and I max are pre-defined constant values between zero and the maximum mutual information between x and y given any joint distribution p(x, y) ? P XY whose marginals satisfy x p(x, y) = p(y) and y p(x, y) = q(x). Then there exist ? * and ? * such that: I q ? * = I p ? * = I max p ? * (y | x) = q ? * (x | y) q ? * (x) p(y) q ? * (x | y) = p ? * (y | x) p ? * (y) q(x) (7) Finally, we connect the global optimum of the dual reconstruction objective to that of the marginal likelihood objective (proof in Appendix A.3). Theorem 1. Given prior distributions q(x) and p(y) over x ? ? x and y ? ? y , if parameterized probability models p ? and q ? have enough capacity under the constraint that: 0 ? I min ? I p ? , I q ? ? I max ? max p?P XY I p (x; y) where I min and I max are pre-defined constant values between zero and the maximum mutual information between x and y given any joint distribution p(x, y) ? P XY whose marginals satisfy x p(x, y) = p(y) and y p(x, y) = q(x). Let ? * , ? * be the global optimum of the dual reconstruction objective max ?,? J dual (?, ?), then q ? * (x) = q(x), p ? * (y) = p(y), and I q ? * = I p ? * = I max . Thus, while the marginal likelihood objective provides no guarantee for the model's mutual information, the global optimum of dual reconstruction objective guarantees that the mutual information of translation models p ? (y | x) and q ? (x | y) will be maximized to I max . 

 Practical Approximations Despite its desirable optimum, the dual reconstruction objective cannot be directly optimized since decoding is not differentiable. We compare how it is approximated by IBT vs. Dual Learning. 

 Gradient Approximation To estimate the dual reconstruction objective, one could use sampling or beam search from the model distribution. However, since neither approach is differentiable, the gradients ? ? J 2 and ? ? J 1 cannot be computed directly. IBT blocks the gradients ? ? J 2 and ? ? J 1 assuming that they are negligible, while Dual Learning approximates them by policy gradient  (Williams, 1992) , which can lead to slow and unstable training  (Henderson et al., 2018; . Proposition 1 shows that the objective is maximized when the mutual information is maximized to I max . Thus, maximizing the mutual information by other means can help side-step this issue. For example, combining the supervised and unsupervised training objectives  (Sennrich et al., 2016a; Cotterell and Kreutzer, 2018)  to train models jointly on the parallel and monolingual data can help. For unsupervised MT, the denoising auto-encoding objective introduced in Lample et al. (2018a) can be viewed as a way to maximize the mutual information. LM Loss Dual Learning combines the dual reconstruction objective with an LM loss to encourage the generated translations to be close to the target language domain. Theorem 1 suggests that the LM loss is redundant: optimizing the dual reconstruction objective implicitly pushes the output distributions of the source-to-target and target-tosource models toward the target and source language distributions respectively, which has the same effect intended by the LM loss. Optimization Strategy While Dual Learning uses batch-level updates, where back-translations are generated on-the-fly and the translation models p ? and q ? are updated alternately in data batches, IBT adopts different strategies based on the data settings. Batch-level IBT is used in unsupervised MT to quickly boost the model performance from a cold start  (Artetxe et al., 2018; Lample et al., 2018a) , while epoch-level IBT is used in semi-supervised MT, where a fixed model p ? is used to back-translate the entire monolingual corpus to train q ? until convergence and vice-versa for p ? . Summary This theoretical analysis suggests that the dual reconstruction objective is a good alternative to the intractable marginal likelihood objective, and that IBT approximates it more closely than the more complex Dual Learning objective. However, we do not know whether the Dual Reconstruction optimum is reached in practice. We therefore conduct an extensive empirical study to determine whether the differences in approximations made by IBT and Dual Learning matter. 

 Empirical Study We evaluate on six translation tasks (Table 1), including German?English (de-en), Turkish?English (tr-en) from WMT18  (Bojar et al., 2018) , and a cross-domain task which tests de-en models trained on WMT data on the TED test sets from IWSLT17  (Cettolo et al., 2017 ). 3 

 Model and Training Configuration We adopt the base Transformer model  (Vaswani et al., 2017) . We pre-train models with the supervised objective until convergence, and fine-tune on the mixed parallel and monolingual data as in prior work  (Sennrich et al., 2016a; Cotterell Task  Lang. Parallel Data Mono. Data Validation Test high-resource de-en News 4.5M News 5.0M newstest15 newstest16-18 low-resource tr-en News 0.2M News 0.8M newstest16 newstest17-18 cross-domain de-en News 4.5M TED 0.5M iwslt-test14 iwslt-test15-17 Table  1 : The empirical comparison spans three data conditions (and both translation directions). We report provenance and the number of sentences in parallel and monolingual training data, as well as validation and test sets for each setting. Monolingual data are randomly selected from "News Crawl: articles from 2015" for German?English and "News Crawl: articles from 2017" for Turkish?English, and TED talks data for TED. For preprocessing, we normalize punctuations and apply tokenization, true-casing, and joint source-target Byte Pair Encoding  (Sennrich et al., 2016b)  with 32, 000 operations. We set the maximum sentence length to 50. 

 Low 

 Baselines and Evaluation Our experiments are based on strong supervised baselines.  4  We compare semi-supervised models that are fine-tuned with Back-Translation, epochlevel and batch-level IBT, and Dual Learning with varying interpolation weights ? LM = {0, 0.1, 0.5} for the LM loss. 5 Following  He et al. (2016) , we use beam search with a beam size of 2 for inference in Dual Learning and IBT. We evaluate translation quality using sacre-BLEU 6 and total training time in hours. We also show learning curves for the approximated dual reconstruction loss (negative of the dual reconstruction objective in Eq. (  2 ), averaged over the training batches from both directions). 

 Findings Overview All semi-supervised training techniques improve translation quality over the supervised-only baseline (Table  2 ). The first iteration of IBT (i.e. Back-Translation) on monolingual data improves over the baseline significantly 7 by 0.7-3.4 BLEU. IBT is more effective in the direction where the model in the opposite direction is most improved by Back-Translation For example, in the high and low resource tasks where Back-Translation improves over the baseline more when translating out of English, the best performing IBT model significantly improves 4 de-en: 2-4 BLEU higher than the baseline of  Morishita et al. (2018) ; tr-en: on par or higher than the baseline of  Garc?a-Mart?nez et al. (2017) . 5 By contrast, prior work only reports results for ?LM = 0.005  (He et al., 2016) . Our preliminary result show that ?LM = 0.005 obtains similar results to ?LM = 0. 6 Version: BLEU+case.mixed+numrefs.1+smooth.exp+ tok.13a+version.1.2.11 7 All mentions of statistical significance are based on a paired Student's t-test with p < 0.05. BLEU over Back-Translation when translating into English, but not in the other direction. In the cross-domain scenario where Back-Translation improves more on de-en, IBT outperforms Back-Translation on en-de, but the improvement is not significant in the other direction. Impact of Policy Gradient Updating the inference model via policy gradient fails to lower the dual reconstruction loss and has little impact on BLEU. We compare Dual Learning (with ? LM = 0) to batch-level IBT, so that the only difference between the two approaches is whether the inference model is updated. Batch-level IBT achieves similar or higher BLEU than Dual Learning for all tasks, except for the low-resource en-tr task where the BLEU difference is small (< 0.2). In addition, batch-level IBT trains 30-50% faster than Dual Learning. Figure  2  shows that the policy gradient update has little impact on the dual reconstruction loss on all tasks. Impact of LM The best Dual Learning BLEU is obtained with ? LM = 0.5 on all tasks except for deen in the cross-domain setting (Table  2 ). However, it brings only small BLEU improvements (0.2-0.4) over Dual Learning without LM loss (? LM > 0), but causes the dual reconstruction loss to decrease slower (Figure  2 ), and slows down training by 20-40%. In all cases, IBT outperforms Dual Learning. Epoch vs. Batch IBT The best epoch-level IBT model outperforms batch-level IBT by 0.5-1.8 BLEU overall, at the cost of much slower training: 13 times longer in the high-resource setting, 1.5 times longer in the low-resource setting, and 3.5 times longer in the cross-domain setting. Running IBT for two iterations is a good choice to balance training efficiency and translation quality, as the third iteration does not help BLEU. 

 Mutual Information Analysis We test the hypothesis that the mutual information constraint is met when training models on the combined supervised and unsupervised objectives in the low-resource setting (the most adversarial condition with the fewest supervised training samples). The mutual information I p ? from Definition 3.2 can be computed by Hoffman and Johnson (2016): where prior distributions q(x) and p(y) are estimated by the empirical data distribution given the monolingual corpora  Table  3  shows the normalized mutual information ? ? log |D| where ? denotes the estimated mutual information. It shows that, when training with the combination of supervised and unsupervised objectives, the normalized mutual information is within a small range between (?2.6?10 ?4 , ?1.4? 10 ?4 ) and is lower than the maximum normalized mutual information log | D| ? log |D| ? 3.0 by a large margin. Thus, the mutual information can be bounded by appropriate values of I min and I max to satisfy the constraint. In addition, these results confirm that updating the inference model using policy gradient in Dual Learning does not effectively increase model's mutual information. I p ? =E x?q(x) [D KL [ p ? (y | x)|| p(y)]] ? D KL [ p ? (y)|| p(y)] (8) 

 Summary of Contributions We contribute theoretical and empirical results that improve our understanding of the connection between two seemingly distant semi-supervised training strategies for NMT: Iterative Back-Translation (IBT) and Dual Learning. On the theory side, we define a dual reconstruction objective which unifies semi-supervised NMT techniques that exploit source and target monolingual text. We prove that optimizing this objective leads to the same global optimum as the intractable marginal likelihood objective, where the model's marginal distribution coincides with the prior language distribution while also maximizing the model's mutual information between source and target. IBT approximates this objective more closely than Dual Learning, despite the more complex objective and update strategies used in the latter. We present a systematic empirical comparison of Back-Translation, IBT, and Dual Learning on six tasks spanning high-resource, low-resource, and cross-domain settings. Results support the theory that the LM loss and policy gradient estimation are unnecessary in Dual Learning, and show that IBT achieves better translation quality than Dual Learning. Analysis confirms that the mutual information constraint required to reach an interesting dual reconstruction optimum is satisfied in practice. These findings lead us to recommend batch-level IBT to quickly boost model performance at early training stages and epoch-level IBT to further improve quality. Our theory also suggests future directions for improving unsupervised MT via more effective methods to maximize the model's mutual information between source and target, and the potential of applying our dual reconstruction objective to other sequence-to-sequence tasks. learning rate to 0.001. We decay the learning rate by 50% and reload the best model after 5 checkpoints without validation perplexity improvement and apply early stopping after repeating the process for 5 times. We report the validation perplexity of the NMT and LM models in Table  4 , and the model sizes in Table  5 . All experiments are performed on a single NVIDIA GeForce GTX 1080 Ti GPU. Figure 2 : 2 Figure 2: Learning curves for the approximated dual reconstruction loss averaged over the training batches from both directions on the low-resource, highresource, and cross-domain tasks. 

 LM denotes the weight for the LM loss. We boldface the highest average scores and their ties based on the significance test. Overall, epoch-level IBT outperforms all other methods at the cost of much longer training time.and Kreutzer, 2018). We use the Adam optimizer (Kingma and Ba, 2015) with a batch size of 32 sentences and checkpoint the model every 2500 updates. At decoding time, we use beam search with a beam size of 5. The LMs in Dual Learning are RNNs (Mikolov et al., 2010)  with 512 hidden units. All model and training details are in Appendix B. -Resource ? LM hours tr-en BLEU 2017 2018 Avg en-tr BLEU 2017 2018 Avg baseline - 8.0 15.14 15.95 15.55 11.17 10.18 10.68 epoch-level IBT-1 - 86.1 16.36 16.44 16.40 15.08 12.98 14.03 epoch-level IBT-2 - 162.2 19.12 19.63 19.38 14.94 12.53 13.74 epoch-level IBT-3 - 237.5 18.76 19.01 18.89 15.04 12.93 13.99 batch-level IBT - 160.6 17.18 18.08 17.63 13.90 11.84 12.87 Dual Learning 0.0 313.2 17.07 18.00 17.54 14.17 11.91 13.04 Dual Learning 0.1 257.8 17.09 17.62 17.36 13.88 11.49 12.69 Dual Learning 0.5 421.2 17.33 18.36 17.85 14.54 12.30 13.42 High-Resource ? LM hours de-en BLEU 2016 2017 2018 Avg en-de BLEU 2016 2017 2018 Avg baseline - 26.7 31.95 27.74 34.59 31.43 29.18 23.46 34.53 29.06 epoch-level IBT-1 - 439.0 32.59 28.46 35.22 32.09 30.13 23.87 35.35 29.78 epoch-level IBT-2 - 850.9 33.64 29.13 36.37 33.05 29.99 24.42 35.60 30.00 epoch-level IBT-3 - 1261.6 33.43 29.07 36.17 32.89 29.93 24.24 35.46 29.88 batch-level IBT - 94.0 32.95 28.65 35.24 32.28 29.70 23.78 34.89 29.46 Dual Learning 0.0 128.2 32.79 28.47 35.10 32.12 29.37 23.50 34.67 29.18 Dual Learning 0.1 93.3 32.63 28.47 34.88 31.99 29.38 23.79 34.71 29.29 Dual Learning 0.5 152.1 32.89 28.69 35.32 32.30 29.58 23.65 34.88 29.37 Cross-Domain ? LM hours de-en BLEU 2015 2016 2017 Avg en-de BLEU 2015 2016 2017 Avg baseline - 26.2 27.11 27.37 23.65 26.04 26.35 23.10 21.69 23.71 epoch-level IBT-1 - 71.1 28.88 28.73 25.37 27.66 26.69 24.02 22.59 24.43 epoch-level IBT-2 - 115.0 28.70 28.72 25.37 27.60 27.57 24.50 22.78 24.95 epoch-level IBT-3 - 159.8 29.13 29.00 25.33 27.82 27.31 24.37 22.92 24.87 batch-level IBT - 45.0 28.03 27.78 24.53 26.78 26.84 23.64 22.35 24.28 Dual Learning 0.0 65.8 28.04 27.73 24.36 26.71 26.70 23.85 22.21 24.25 Dual Learning 0.1 59.3 27.77 27.84 24.51 26.71 26.99 23.86 22.59 24.48 Dual Learning 0.5 92.7 27.84 28.00 24.18 26.67 27.23 24.08 22.72 24.68 Table2: BLEU scores and total training time (hours) on the low-resource, high-resource, and cross-domain tasks. epoch-level IBT-1, IBT-2, and IBR-3 denotes models fine-tuned with IBT for 1-3 iterations, and ? 

 M X and M Y . Although computing I p ? directly is intractable, it can be approximated with a Monte Carlo estimate. Following Dieng et al. (2019), we approximate the two KL terms by Monte Carlo, where samples from p ? (y) can be obtained by ancestral sampling (we use beam search with beam size of five to sample from p ? (y | x)). tr-en en-tr baseline -2.47 -2.28 epoch-level IBT-1 -2.57 -2.23 epoch-level IBT-2 -2.18 -2.30 epoch-level IBT-3 -2.32 -2.42 batch-level IBT -1.51 -1.82 dual learning w/ ? LM = 0 -1.50 -1.80 dual learning w/ ? LM = 0.5 -1.44 -1.77 The marginal prob- ability p ? (y) = E x?q(x) [p ? (y | x)] can also be estimated by Monte Carlo. Due to data sparsity, the conditional likelihood p ? (y | x) will be near zero for most source sentences randomly sampled from q(x). To better estimate it, we smooth the data distribution of the original dataset D by gener- ating a randomly perturbed dataset D. 8 

 Table 3 : 3 Results on estimated mutual information ? in the low-resource setting. We report the normalized scores ? ? log |D| (on the scale of 10 ?4 ) averaged over the two test sets. The range of normalized scores should be [? log |D|, log | D| |D| ] = [?8.0, 3.0]. 

			 We will define constraints to guarantee avoiding the uninteresting solution where p ? (y | x) = p(y) in Section 3.2. 

			 We focus on key components of the proof and leave detailed derivations for supplemental material. 

			 We exclude Rapid and ParaCrawl corpora as they are noisy and thus require data filtering (Morishita et al., 2018) . 

			 We generate 20 perturbed sentences per source via random word dropping with probability of 0.1 and permutation with maximum distance of 3. 

			 http://www.statmt.org/wmt18/ translation-task.html 11 https://wit3.fbk.eu/mt.php?release= 2017-01-ted-test
