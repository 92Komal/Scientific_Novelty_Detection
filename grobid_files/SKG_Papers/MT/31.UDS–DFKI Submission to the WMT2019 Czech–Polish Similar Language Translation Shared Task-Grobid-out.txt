title
UDS-DFKI Submission to the WMT2019 Similar Language Translation Shared Task

abstract
In this paper we present the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. The first edition of this shared task featured data from three pairs of similar languages: Czech and Polish, Hindi and Nepali, and Portuguese and Spanish. Participants could choose to participate in any of these three tracks and submit system outputs in any translation direction. We report the results obtained by our system in translating from Czech to Polish and comment on the impact of out-of-domain test data in the performance of our system. UDS-DFKI achieved competitive performance ranking second among ten teams in Czech to Polish translation.

Introduction The shared tasks organized annually at WMT provide important benchmarks used in the MT community. Most of these shared tasks include English data, which contributes to make English the most resource-rich language in MT and NLP. In the most popular WMT shared task for example, the News task, MT systems have been trained to translate texts from and to English  (Bojar et al., 2016 (Bojar et al., , 2017 . This year, we have observed a shift on the dominant role that English on the WMT shared tasks. The News task featured for the first time two language pairs which did not include English: German-Czech and French-German. In addition to that, the Similar Language Translation was organized for the first time at WMT 2019 with the purpose of evaluating the performance of MT systems on three pairs of similar languages from three different language families: Ibero-Romance, Indo-Aryan, and Slavic. The Similar Language Translation  (Barrault et al., 2019)  task provided participants with train-ing, development, and testing data from the following language pairs: Spanish -Portuguese (Romance languages), Czech -Polish (Slavic languages), and Hindi -Nepali (Indo-Aryan languages). Participant could submit system outputs to any of the three language pairs in any direction. The shared task attracted a good number of participants and the performance of all entries was evaluated using popular MT automatic evaluation metrics, namely BLEU  (Papineni et al., 2002)  and TER  (Snover et al., 2006) . In this paper we describe the UDS-DFKI system to the WMT 2019 Similar Language Translation task. The system achieved competitive performance and ranked second among ten entries in Czech to Polish translation in terms of BLEU score. 

 Related Work With the widespread use of MT technology and the commercial and academic success of NMT, there has been more interest in training systems to translate between languages other than English  (Costa-juss?, 2017) . One reason for this is the growing need of direct translation between pairs of similar languages, and to a lesser extent language varieties, without the use of English as a pivot language. The main challenge is to overcome the limitation of available parallel data taking advantage of the similarity between languages. Studies have been published on translating between similar languages (e.g.  Catalan -Spanish (Costa-juss?, 2017) ) and language varieties such as European and Brazilian Portuguese  (Fancellu et al., 2014; Costa-juss? et al., 2018) . The study by  Lakew et al. (2018)  tackles both training MT systems to translate between European-Brazilian Portuguese and European-Canadian French, and two pairs of similar languages Croatian-Serbian and Indonesian-Malay. Processing similar languages and language varieties has attracted attention not only in the MT community but in NLP in general. This is evidenced by a number of research papers published in the last few years and the recent iterations of the VarDial evaluation campaign which featured multiple shared tasks on topics such as dialect detection, morphosyntactic tagging, crosslingual parsing, cross-lingual morphological analysis  (Zampieri et al., , 2019 . 

 Data We used the Czech-Polish dataset provided by the WMT 2019 Similar Language Translation task organizers for our experiments. The released parallel dataset consists of out-of-domain (or generaldomain) data only and it differs substantially from the released development set which is part of a TED corpus. The parallel data includes Europarl v9, Wiki-titles v1, and JRC-Acquis. We combine all the released data and prepare a large outdomain dataset. 

 Pre-processing The out-domain data is noisy for our purposes, so we apply methods for cleaning. We performed the following two steps: (i) we use the cleaning process described in  Pal et al. (2015) , and (ii) we execute the Moses  (Koehn et al., 2007)  corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then we use the Moses tokenizer to tokenize the out-domain corpus with 'no-escape' option. Finally, we apply true-casing. The cleaned version of the released data, i.e., the General corpus containing 1,394,319 sentences, is sorted based on the score in Equation  1 . Thereafter, We split the entire data (1,394,319) into two sets; we use the first 1,000 for validation and the remaining as training data. The released development set (Dev) is used as test data for our experiment. It should be noted noted that, we exclude 1,000 sentences from the General corpus which are scored as top (i.e., more in-domain like) during the data selection process. We prepare two parallel training sets from the aforementioned training data: (i) transfer-ence500K(presented next), collected 500,000 parallel data through data selection method  (Axelrod et al., 2011) , which are very similar to the indomain data (for our case the development set), and (ii) transferenceALL, utilizing all the released out-domain data sorted by Equation  1 . The transference500Ktraining set is prepared using in-domain (development set) bilingual cross-entropy difference for data selection as was described in  Axelrod et al. (2011) . The difference in cross-entropy is computed based on two language models (LM): a domain-specific LM is estimated from the in-domain (containing 2050 sentences) corpus (lm i ) and the out-domain LM (lm o ) is estimated from the eScape corpus. We rank the eScape corpus by assigning a score to each of the individual sentences which is the sum of the three cross-entropy (H) differences. For a j th sentence pair src j -trg j , the score is calculated based on Equation  1 . score = |H src (src j , lm i ) ? H src (src j , lm o )| + |H trg (trg j , lm i ) ? H trg (trg j , lm o )| (1) 4 System Architecture -The Transference Model Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture  (Vaswani et al., 2017)  is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self-and crossattention through so-called multi-head attentions, which are facilitated by parallelization. We use multi-head attention to jointly attend to information at different positions from different representation subspaces. The first encoder (enc 1 ) of our model encodes word form information of the source (f w ), and a second sub-encoder (enc 2 ) to encode sub-word (byte-pair-encoding) information of the source (f s ). Additionally, a second encoder (enc src?mt ) which takes the encoded representation from the enc 1 , combines this with the self-attention-based encoding of f s (enc 2 ), and prepares a representation for the decoder (dec e ) via cross-attention. Our second encoder (enc 1?2 ) can be viewed as a transformer based NMT's decoding block, however, without masking. The intuition behind our architecture is to generate better representations via both self-and cross-attention and to further facilitate the learning capacity of the feed-forward layer in the decoder block. In our transference model, one self-attended encoder for f w , f w = (w 1 , w 2 , . . . , w k ), returns a sequence of continuous representations, enc 2 , and a second selfattended sub-encoder for f s , f s = (s 1 , s 2 , . . . , s l ), returns another sequence of continuous representations, enc 2 . Self-attention at this point provides the advantage of aggregating information from all of the words, including f w and f s , and successively generates a new representation per word informed by the entire f w and f s context. The internal enc 2 representation performs crossattention over enc 1 and prepares a final representation (enc 1?2 ) for the decoder (dec e ). The decoder generates the e output in sequence, e = (e 1 , e 2 , . . . , e n ), one word at a time from left to right by attending to previously generated words as well as the final representations (enc 1?2 ) generated by the encoder. We use the scale-dot attention mechanism (like Vaswani et al. (  2017 )) for both self-and crossattention, as defined in Equation  2 , where Q, K and V are query, key and value, respectively, and d k is the dimension of K. attention(Q, K, V ) = sof tmax( QK T ? d k )V (2) The multi-head attention mechanism in the transformer network maps the Q, K, and V matrices by using different linear projections. Then h parallel heads are employed to focus on different parts in V. The i th multi-head attention is denoted by head i in Equation  3 . head i is linearly learned by three projection parameter matrices: W Q i , W K i ? R d model ?d k , W V i ? R d model ?dv ; where d k = d v = d model /h, and d model is the number of hidden units of our network. head i = attention(QW Q i , KW K i , V W V i ) (3) Finally, all the vectors produced by parallel heads are linearly projected using concatenation and form a single vector, called a multi-head attention (M att ) (cf. Equation  4 ). Here the dimension of the learned weight matrix W O is R d model ?d model . M att (Q, K, V ) = Concat n i:1 (head i )W O (4) 

 Experiments We explore our transference model -a twoencoder based transformer architecture, in CS-PL similar language translation task. 

 Experiment Setup For transferenceALL, we initially train on the complete out-of-domain dataset (General). The General data is sorted based on their in-domain similarities as described in Equation  1 . transferenceALLmodels are then fine-tuned towards the 500K (in-domain-like) data. Finally, we perform checkpoint averaging using the 8 best checkpoints. We report the results on the provided development set, which we use as a test set before the submission. Additionally we also report the official test set result. To handle out-of-vocabulary words and to reduce the vocabulary size, instead of considering words, we consider subword units  (Sennrich et al., 2016)  by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the Czech (CS) and Polish (PL), we define BPE tokens by jointly processing all parallel data. Thus, CS and PL derive a single BPE vocabulary. Since CS and PL belong to the similar language, they naturally share a good fraction of BPE tokens, which reduces the vocabulary size. We pass word level information on the first encoder and the BPE information to the second one. On the decoder side of the transference model we pass only BPE text. We evaluate our approach with development data which is used as test case before submission. We use BLEU  (Papineni et al., 2002)  and TER  (Snover et al., 2006) . 

 Hyper-parameter Setup We follow a similar hyper-parameter setup for all reported systems. All encoders, and the decoder, are composed of a stack of N f w = N f s = N es = 6 identical layers followed by layer normalization. Each layer again consists of two sub-layers and a residual connection  (He et al., 2016)  around each of the two sub-layers. We apply dropout  (Srivastava et al., 2014)  to the output of each sub-layer, before it is added to the sub-layer input and normalized. Furthermore, dropout is applied to the sums of the word embeddings and the corresponding positional encodings in both encoders as well as the decoder stacks. We set all dropout values in the network to 0.1. During training, we employ label smoothing with value ls = 0.1. The output dimension produced by all sub-layers and embedding layers is d model = 512. Each encoder and decoder layer contains a fully connected feed-forward network (F F N ) having dimensionality of d model = 512 for the input and output and dimensionality of d f f = 2048 for the inner layers. For the scaled dot-product attention, the input consists of queries and keys of dimension d k , and values of dimension d v . As multi-head attention parameters, we employ h = 8 for parallel attention layers, or heads. For each of these we use a dimensionality of d k = d v = d model /h = 64. For optimization, we use the Adam optimizer  (Kingma and Ba, 2015)  with ? 1 = 0.9, ? 2 = 0.98 and = 10 ?9 . The learning rate is varied throughout the training process, and increasing for the first training steps warmup steps = 8000 and afterwards decreasing as described in  (Vaswani et al., 2017) . All remaining hyper-parameters are set analogously to those of the transformer's base model. At training time, the batch size is set to 25K tokens, with a maximum sentence length of 256 subwords, and a vocabulary size of 28K. After each epoch, the training data is shuffled. After finishing training, we save the 5 best checkpoints which are written at each epoch. Finally, we use a single model obtained by averaging the last 5 checkpoints. During decoding, we perform beam search with a beam size of 4. We use shared embeddings between CS and PL in all our experiments. 

 Results We present the results obtained by our system in Our fine-tuned system on development set provides significant performance improvement over the generic model. We found +12.9 absolute BLEU points improvement over the generic model. Similar improvement is also observed in terms of TER (-16.9 absolute). It is to be noted that our generic model is trained solely on the clean version of training data. Before submission, we performed punctuation normalization, unicode normalization, and detokenization for the run. In Table  2  we present the ranking of the competition provided by the shared task organizers. Ten entries were submitted by five teams and are ordered by BLEU score. TER is reported for all submissions which achieved BLEU score greater than 5.0. The type column specifies the type of system, whether it is a Primary (P) or Constrastive (C) entry. 

 Team Type BLEU TER UPC-TALP P 7.9 85.9 Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-ofdomain data provided in the competition which made the task equally challenging to all participants. 

 Conclusion This paper presented the UDS-DFKI system submitted to the Similar Language Translation shared task at WMT 2019. We presented the results obtained by our system in translating from Czech to Polish. Our system achieved competitive performance ranking second among ten teams in the competition in terms of BLEU score. The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants. In future work, we would like to investigate how effective is the proposed hypothesis (i.e., word-BPE level information) in similar language trans-lation. Furthermore, we would like to explore the similarity between these two languages (and the other two language pairs in the competition) in more detail by training models that can best capture morphological differences between them. During such competitions, this is not always possible due to time constraints. Table 1 . 1 tested on model BLEU TER Dev set Generic 12.2 75.8 Dev set Fine-tuned* 25.1 58.9 Test set Generic 7.1 89.3 Test set Fine-Tuned* 7.6 87.0 Table 1: Results for CS-PL Translation; * averaging 8 best checkpoints. 
