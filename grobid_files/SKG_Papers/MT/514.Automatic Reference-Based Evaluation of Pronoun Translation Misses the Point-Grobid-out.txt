title
Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point

abstract
We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semiautomatic metrics and test suites in place of fully automatic metrics.

Introduction As the general quality of machine translation (MT) increases, there is a growing interest in improving the translation of specific linguistic phenomena. A case in point that has been studied in the context of both statistical  (Hardmeier, 2014; Guillou, 2016; Lo?iciga, 2017)  and neural MT  (Bawden et al., 2017; Voita et al., 2018)  is that of pronominal anaphora. In the simplest case, translating anaphoric pronouns requires the generation of corresponding word forms respecting the grammatical constraints on agreement in the target language, as in the following English-French example, where the correct form of the pronoun in the second sentence varies depending on which of the (equally correct) translations of the word bicycle was used in the first: (1) a. I have a bicycle. It is red. b. J'ai un v?lo. Il est rouge.  [ref]  c. J'ai une bicyclette. Elle est rouge.  [MT]  However, the problem is more complex in practice because there is often no 1 : 1 correspondence between pronouns in two languages. This is easily demonstrated at the corpus level by observing that the number of pronouns varies significantly across languages in parallel texts  (Mitkov *  Both authors contributed equally. and  Barbu, 2003) , but it tends to be difficult to predict in individual cases. In general MT research, significant progress was enabled by the invention of automatic evaluation metrics based on reference translations, such as BLEU  (Papineni et al., 2002) . Attempting to create a similar framework for efficient research, researchers have proposed automatic reference-based evaluation metrics specifically targeting pronoun translation: AutoPRF  (Hardmeier and Federico, 2010)  and  APT (Miculicich Werlen and Popescu-Belis, 2017) . We study the performance of these metrics on a dataset of English-French translations and investigate to what extent automatic evaluation based on reference translations provides insights into how well an MT system handles pronouns. Our analysis clarifies the conceptual differences between AutoPRF and APT, uncovering weaknesses in both metrics, and investigates the effects of the alignment correction heuristics used in APT. By using the fine-grained PROTEST categories of pronoun function, we find that the accuracy of the automatic metrics varies across pronouns of different functions, suggesting that certain linguistic patterns are captured better in the automatic evaluation than others. We argue that fully automatic wide-coverage evaluation of this phenomenon is unlikely to drive research forward, as it misses essential parts of the problem despite achieving some correlation with human judgements. Instead, semiautomatic evaluation involving automatic identification of correct translations with high precision and low recall appears to be a more achievable goal. Another more realistic option is a test suite evaluation with a very limited scope. 

 Pronoun Evaluation Metrics for MT Two reference-based automatic metrics of pronoun translation have been proposed in the literature. The first  (Hardmeier and Federico, 2010 ) is a variant of precision, recall and F-score that measures the overlap of pronouns in the MT output with a reference translation. It lacks an official name, so we refer to it as AutoPRF following the terminology of the DiscoMT 2015 shared task  (Hardmeier et al., 2015) . The scoring process relies on a word alignment between the source and the MT output, and between the source and the reference translation. For each input pronoun, it computes a clipped count  (Papineni et al., 2002)  of the overlap between the aligned tokens in the reference and the MT output. The clipped count of a given word is defined as the number of times it occurs in the MT output, limited by the number of times it occurs in the reference translation. The final metric is then calculated as the precision, recall and F-score based on these clipped counts. Miculicich Werlen and Popescu-Belis (2017) propose a metric called Accuracy of Pronoun Translation (APT) that introduces several innovations over the previous work. It is a variant of accuracy, so it counts, for each source pronoun, whether its translation can be considered correct, without considering multiple alignments. Since word alignment is problematic for pronouns, the authors propose an heuristic procedure to improve alignment quality. Finally, it introduces the notion of pronoun equivalence, assigning partial credit to pronoun translations that differ from the reference translation in specific ways deemed to be acceptable. In particular, it considers six possible cases when comparing the translation of a pronoun in MT output and the reference. The pronouns may be: (1) identical, (2) equivalent, (3) different/incompatible, or there may be no translation in: (4) the MT output, (5) the reference, (6) either the MT output or the reference. Each of these cases may be assigned a weight between 0 and 1 to determine the level of correctness. 

 The PROTEST Dataset We study the behaviour of the two automatic metrics using the PROTEST test suite . The test suite comprises 250 hand-selected personal pronoun tokens taken from the DiscoMT2015.test dataset of TED talk transcriptions and translations  and annotated according to the ParCor guidelines  (Guillou et al., 2014) . It is structured according to a linguistic typology motivated by work on func-tional grammar by  Dik (1978) and Halliday (2004) . Pronouns are first categorised according to their function: anaphoric: I have a bicycle. It is red. event: He lost his job. It was a shock. pleonastic: It is raining. addressee reference: You're welcome. They are then subcategorised according to morphosyntactic criteria, whether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic). Our dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite. The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation  (Hardmeier et al., 2015)  -four phrase-based SMT systems AUTO-POSTEDIT  (Guillou, 2015) , UU-HARDMEIER  (Hardmeier et al., 2015) , IDIAP  (Luong et al., 2015) , UU-TIEDEMANN  (Tiedemann, 2015) , a rule-based system ITS2  (Lo?iciga and Wehrli, 2015) , and the shared task baseline (also phrase-based SMT). Three NMT systems are included for comparison: LIMSI  (Bawden et al., 2017) , NYU  (Jean et al., 2014) , and YANDEX  (Voita et al., 2018) . Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines . The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases where it impacted the annotator's ability to make a judgement). The annotations were carried out by two bilingual English-French speakers, both of whom are native speakers of French. Our human judgements differ in important ways from the human evaluation conducted for the same set of systems at DiscoMT 2015  (Hardmeier et al., 2015) , which was carried out by non-native speakers over an unbalanced data sample using a gap-filling methodology. In the gap-filling task annotators are asked to select, from a predefined list (including an uninformative catch-all group "other"), those pronouns that could fill the pronoun translation slot. Unlike in the PROTEST evaluation, the pronoun translations were obscured in the MT output. This avoided priming the annotators with the output of the candidate translation, but it occasionally caused valid translations to be rejected because they were missed by the annotator. 

 Accuracy versus Precision/Recall There are three ways in which APT differs from Au-toPRF: the scoring statistic, the alignment heuristic in APT, and the definition of pronoun equivalence. APT is a measure of accuracy: It reflects the proportion of source pronouns for which an acceptable translation was produced in the target. AutoPRF, by contrast, is a precision/recall metric on the basis of clipped counts.  Hardmeier and Federico (2010)  motivate the use of precision and recall by pointing out that word alignments are not 1 : 1, so each pronoun can be linked to multiple elements in the target language, both in the reference translation and in the MT output. Their metric is designed to account for all linked words in such cases. To test the validity of this argument, we examined the subset of examples of 8 systems in our English-French dataset 1 giving rise to a clipped count greater than one 2 and found that these examples follow very specific patterns. All 143 cases included exactly one personal pronoun. In 99 cases, the additional matched word was the complementiser que 'that'. In 31 and 4 cases, respectively, it was a form of the auxiliary verbs avoir 'to have' and ?tre 'to be'. One example matched both que and a form of ?tre. Two had reflexive pronouns, and one an imperative verb form. With the possible exception of the two reflexive pronouns, none of this seems to be relevant to pronoun correctness. We conclude that it is more reasonable to restrict the counts to a single pronominal item per example. With this additional restriction, however, the recall score of AutoPRF becomes equivalent to a version of APT without equivalent pronouns and alignment correction. We therefore limit the remainder of our study to APT. 

 Effects of Word Alignment APT includes an heuristic alignment correction procedure to mitigate errors in the word alignment between a source-language text and its translation (reference or MT output). We ran experiments to 1 Excluding the YANDEX system, which was added later. 2 A clipped count greater than one for a given pronoun translation indicates that the MT output and the reference translation aligned to this pronoun overlap in more than one token. 

 Score APT assess the correlation of APT with human judgements, with and without the alignment correction heuristics. Table  1  displays the APT results in both conditions and the proportion of pronouns in the PROTEST test suite marked as correctly translated. For better comparison with the PROTEST test suite results, we restricted APT to the pronouns in the test suite. We used two different weight settings: 3 APT-A uses weight 1 for identical matches and 0 for all other cases. APT-B uses weight 1 for identical matches, 0.5 for equivalent matches and 0 otherwise. There is little difference in the APT scores when we consider the use of alignment heuristics. This is due to the small number of pronouns for which alignment improvements are applied for most systems (typically 0-12 per system). The exception is the ITS2 system output for which 18 alignment improvements are made. For the following systems we observe a very small increase in APT score for each of the two weight settings we consider, when alignment heuristics are applied: UU-HARDMEIER (+0.8), ITS2 (+0.8), BASELINE (+0.8), YANDEX (+0.8), and NYU (+0.4). However, these small improvements are not sufficient to affect the system rankings. It seems, therefore, that the alignment heuristic has only a small impact on the validity of the score. To assess differences in correlation with human judgment for pairs of APT settings, we run Williams's significance test  (Williams, 1959; Graham and Baldwin, 2014) . The test reveals that differences in correlation between the various configurations of APT and human judgements are not statistically significant (p > 0.2 in all cases).  

 Metric Accuracy per Category Like Miculicich Werlen and Popescu-Belis (2017), we use Pearson's and Spearman's correlation coefficients to assess the correlation between APT and our human judgements (Table  2 ). Although APT does correlate with the human judgements over the PROTEST test suite, the correlation is weaker than that with the DiscoMT gap-filling evaluations reported in Miculicich Werlen and Popescu-Belis (2017). A Williams significance test reveals that the difference in correlation (for those systems common to both studies) is not statistically significant (p > 0.3). Table  1  also shows that the rankings induced from the PROTEST and APT scores are rather different. The differences are due to the different ways in which the two metrics define pronoun correctness, and the different sources against which correctness is measured (reference translation vs. human judgement). We also study how the results of APT (with alignment correction) interact with the categories in PROTEST. We consider a pronoun to be measured as correct by APT if it is assigned case 1 (identical) or 2 (equivalent). Likewise, a pronoun is considered incorrect if it is assigned case 3 (incompatible). We compare the number of pronouns marked as correct/incorrect by APT and by the human judges, ignoring APT cases in which no judgement can be made: no translation of the pronoun in the MT output, reference or both, and pronouns for which the human judges were unable to make a judgement due to factors such as poor overall MT quality, incorrect word alignments, etc. The results of this comparison are displayed in Table  3 . At first glance, we can see that APT disagrees with the human judgements for almost a quarter (24.3%) of the assessed translations. The distribution of the disagreements over APT cases is very skewed and ranges from 8% for case 1 to 32% for case 2 and 49% for case 3. In other words, APT identifies correct pronoun translations with good precision, but relatively low recall. We can also see that APT rarely marks pronouns as equivalent (case 2). Performance for anaphoric pronouns is mixed. In general, there are three main problems affecting anaphoric pronouns (Table  4 ). 1) APT, which does not incorporate knowledge of anaphoric pronoun antecedents, does not consider pronoun-antecedent head agreement so many valid alternative translations involving personal pronouns are marked as incompatible (i.e. incorrect, case 3), but as correct by the human judges. Consider the following example, in which the pronoun they is deemed correctly translated by the YANDEX system (according to the human judges) as it agrees in number and grammatical gender with the translation of the antecedent extraits (clips). However, the pronoun translation ils is marked as incorrect by APT as it does not match the translation in the reference (elles). SOURCE: so what these two clips show is not just the devastating consequence of the disease, but they also tell us something about the shocking pace of the disease. . . YANDEX: donc ce que ces deux extraits  [masc.,pl.]  montrent n'est pas seulement la cons?quence d?vastatrice de la maladie, mais ils[masc. pl.] nous disent aussi quelque chose sur le rythme choquant de la maladie. . . REFERENCE: ce que ces deux vid?os[fem.,pl.] montrent, ce ne sont pas seulement les cons?quences dramatiques de cette maladie, elles[fem. pl.] nous montrent aussi la vitesse fulgurante de cette maladie. . . 2) Substitutions between pronouns are governed by much more complex rules than the simple pronoun equivalence mechanism in APT. For example, the dictionary of pronouns used in APT lists il and ce as equivalent. However, while il can often replace ce as a pleonastic pronoun in French, it has a much stronger tendency to be interpreted as anaphoric, rendering pleonastic use unacceptable if there is a salient masculine antecedent in the context. 3) APT does not consider the use of impersonal pronouns such as c' in place of the feminine personal pronoun elle or the plural forms ils and elles. As with anaphoric pronouns, APT incorrectly marks some pleonastic and event translations as equivalent, in disagreement with the human judges. Other common errors arise from 1) the use of alternative translations marked as incompatible (i.e. incorrect) by APT but correct by the human judges, for example il (personal) in the MT output when the reference contained the impersonal pronoun cela or c ?a (30 cases for pleonastic, 7 for event), or 2) the presence of il in both the MT output and reference marked by APT as identical but by the human judges as incorrect (3 cases for pleonastic, 15 event). 

 Category Some of these issues could be addressed by incorporating knowledge of pronoun function in the source language, of pronoun antecedents, and of the wider context of the translation surrounding the pronoun. However, whilst we might be able to derive language-specific rules for some scenarios, it would be difficult to come up with more general or language-independent rules. For example, il and ce can be anaphoric or pleonastic pronouns, but il has a more referential character. Therefore in certain constructions that are strongly pleonastic (e.g. clefts) only ce is acceptable. This rule would be specific to French, and would not cover other scenarios for the translation of pleonastic it. Other issues include the use of pronouns in impersonal constructions such as il faut [one must/it takes] in which evaluation of the pronoun requires consideration of the whole expression, or transformations between active and passive voice, where the perspective of the pronouns changes. 

 Conclusions Our analyses reveal that despite some correlation between APT and the human judgements, fully automatic wide-coverage evaluation of pronoun translation misses essential parts of the problem. Comparison with human judgements shows that APT identifies good translations with relatively high precision, but fails to reward important patterns that pronoun-specific systems must strive to generate. Instead of relying on fully automatic evaluation, our recommendation is to emphasise high precision in the automatic metrics and implement semiautomatic evaluation procedures that refer negative cases to a human evaluator, using available tools and methods . Fully automatic evaluation of a very restricted scope may still be feasible using test suites designed for specific problems  (Bawden et al., 2017) . Table 2 : 2 Correlation of APT and human judgements c1 c2 Pearson Spearman With alignment 1 0 0.848 0.820 heuristics 1 0.5 0.853 0.815 Without alignment 1 0 0.850 0.820 heuristics 1 0.5 0.855 0.811 APT Human Disagreement Category Cases Assess. 1 2 3 Dis. Ex. % Anaphoric intra sbj it 130 13 73 156 60 47 / 216 21.8 intra nsbj it 59 1 31 77 14 19 / 91 20.9 inter sbj it 104 21 111 142 94 63 / 236 26.7 inter nsbj it 21 0 7 8 20 13 / 28 46.4 intra they 131 0 95 154 72 37 / 226 16.4 inter they 126 0 108 129 105 47 / 234 20.1 sg they 57 0 66 83 40 58 / 123 47.2 group it/they 47 0 41 64 24 31 / 88 35.2 Event it 145 42 94 185 96 60 / 281 21.4 Pleonastic it 171 54 52 243 34 46 / 277 16.6 Generic you 117 0 70 186 1 69 / 187 36.9 Deictic sg you 95 0 47 140 2 45 / 142 31.7 Deictic pl you 91 0 7 97 1 6 / 98 6.1 Total 1,294 131 802 1,664 563 541 / 2,227 24.3 

 Table 3 : 3 Number of pronouns marked as correct/incorrect in the PROTEST human judgements, as identical (1), equivalent (2), and incompatible (3) by APT, and the percentage of disagreements, per category (Disagree [Dis.] / Examples [Ex.]) 

 Table 4 : 4 Common cases of disagreement for anaphoric, pleonastic, and event reference pronouns V E I O Anaphoric intra-sent. subj. it 22 9 8 8 intra-sent. non-subj. it 16 -1 2 inter-sent. subj. it 35 6 22 - inter-sent. non-subj. it ---13 intra-sent. they 25 -3 9 inter-sent. they 22 -3 22 singular they 40 --18 group it/they 21 --10 Event it -16 -44 Pleonastic it -11 -35 V: Valid alternative translation I: Impersonal translation E: Incorrect equivalence O: Other 

			 Personal recommendation by Lesly Miculicich Werlen.
