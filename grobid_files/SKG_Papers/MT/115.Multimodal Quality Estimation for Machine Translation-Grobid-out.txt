title
Multimodal Quality Estimation for Machine Translation

abstract
We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.

Introduction Quality Estimation (QE) for Machine Translation (MT)  (Blatz et al., 2004; Specia et al., 2009)  aims to predict the quality of a machine-translated text without using reference translations. It estimates a label (a category, such as 'good' or 'bad', or a numerical score) for a translation, given text in a source language and its machine translation in a target language  (Specia et al., 2018b) . QE can operate at different linguistic levels, including sentence and document levels. Sentence-level QE estimates the translation quality of a whole sentence, while document-level QE predicts the translation quality of an entire document, even though in practice in literature the documents have been limited to a small set of 3-5 sentences  (Specia et al., 2018b) . Existing work has only explored textual context. We posit that to judge (or estimate) the quality of a translated text, additional context is paramount. Sentences or short documents taken out of context may lack information on the correct translation of certain (esp. ambiguous) constructions. Inspired by recent work on multimodal machine learning  (Baltrusaitis et al., 2019; Barrault et al., 2018) , we propose to explore the visual modality in addition to the text modality for this task. Multimodality through vision offers interesting opportunities for real-life data since texts are in- * Two authors contributed equally. 

 Source (EN) Danskin Women's Bermuda Shorts 

 MT (FR) Bermuda Danskin f?minines court Table  1 : Example of incorrectly machine-translated text: the word shorts is used to indicate short trousers, but gets translated in French as court, the adjective short. Here multimodality could help to detect the error (extracted from the Amazon Reviews Dataset of  McAuley et al., 2015) . creasingly accompanied with visual elements such as images or videos, especially in social media but also in domains such as e-commerce. Multimodality has not yet been applied to QE. Table  1  shows an example from our e-commerce dataset in which multimodality could help to improve QE. Here, the English noun shorts is translated by the adjective court (for the adjective short) in French, which is a possible translation out of context. However, as the corresponding product image shows, this product is an item of clothing, and thus the machine translation is incorrect. External information can hence help identify mismatches between translations which are difficult to find within the text. Progress in QE is mostly benchmarked as part of the Conference on Machine Translation (WMT) Shared Task on QE. This paper is based on data from the WMT'18 edition's Task 4 -documentlevel QE. This Task 4 aims to predict a translation quality score for short documents based on the number and the severity of translation errors at the word level  (Specia et al., 2018a) . This data was chosen as it is the only one for which meta information (images in this case) is available. We extend this dataset by computing scores for each sentence for a sentence-level prediction task. We consider both feature-based and neural state-of-theart models for QE. Having these as our starting points, we propose different ways to integrate the visual modality. The main contributions of this paper are as follows: (i) we introduce the task of Multimodal QE (MQE) for MT as an attempt to improve QE by using external sources of information, namely images; (ii) we propose several ways of incorporating visual information in neural-based and featurebased QE architectures; and (iii) we achieve the state-of-the-art performance for such architectures in document and sentence-level QE. 

 Experimental Settings 

 QE Frameworks and Models We explore feature-based and neural-based models from two open-source frameworks: QuEst++: QuEst++  (Specia et al., 2015)  is a feature-based QE framework composed of two modules: a feature extractor module, to extract the relevant QE features from both the source sentences and their translations, and a machine learning module. We only use this framework for our experiments on document-level QE, since it does not perform well enough for sentence-level prediction. We use the same model (Support Vector Regression), hyperparameters and feature settings as the baseline model for the document-level QE task at WMT'18. deepQuest: deepQuest  (Ive et al., 2018 ) is a neural-based framework that provides state-of-theart models for multi-level QE. We use the BiRNN model, a light-weight architecture which can be trained at either sentence or document level. The BiRNN model uses an encoder-decoder architecture: it takes on its input both the source sentence and its translation which are encoded separately by two independent bi-directional Recurrent Neural Networks (RNNs). The two resulting sentence representations are then concatenated as a weighted sum of their word vectors, generated by an attention mechanism. For sentence-level predictions, the weighted representation of the two input sentences is passed through a dense layer with sigmoid activation to generate the quality estimates. For document-level predictions, the final representation of a document is generated by a second attention mechanism, as the weighted sum of the weighted sentence-level representations of all the sentences within the document. The resulting document-level representation is then passed through a dense layer with sigmoid activation to generate the quality estimates. Additionally, we propose and experiment with BERT-BiRNN, a variant of the BiRNN model. Rather than training the token embeddings with the task at hand, we use large-scale pre-trained token-level representations from the multilingual cased base BERT model  (Devlin et al., 2019) . During training, the BERT model is fine-tuned by unfreezing the weights of the last four hidden layers along with the token embedding layer. This performs comparably to the state-of-the-art predictorestimator neural model in  Kepler et al. (2019) . 

 Data WMT'18 QE Task 4 data: This dataset was created for the document-level track. It contains a sample of products from the Amazon Reviews Dataset  (McAuley et al., 2015)  taken from the Sports & Outdoors category. 'Documents' consist of the English product title and its description, its French machinetranslation and a numerical score to predict, namely the MQM score (Multidimensional Quality Metrics)  (Lommel et al., 2014) . This score is computed by annotating and weighting each word-level translation error according to its severity (minor, major and critical): MQM Score = 1 ? n min + 5n maj + 10n cri n where n is the total number of words, and n i is the number of errors annotated with the corresponding error severity. Additionally, the dataset provides one picture per product, as well as pre-extracted visual features, as we discuss below. For the sentence-level QE task, each document of the dataset was split into sentences (lines), where every sentence has its corresponding MQM score computed in the same way as for the document. We note that this variant is different from the official sentence-level track at WMT since for that task visual information is not available. Text features: For the feature-based approach, we extract the same 15 features as those for the baseline of WMT'18 at document level. For the neural-based approaches, text features are either the learned word embeddings (BiRNN) or pre-trained word embeddings (BERT-BiRNN). Visual features: The visual features are preextracted vectors with 4,096 dimensions, also provided in the Amazon Reviews  Dataset (McAuley et al., 2015) . The method to obtain the features uses a deep convolutional neural network which has been pre-trained on the ImageNet dataset for image classification  (Deng et al., 2009) . The visual features extracted represent a vectorial summary of the image taken from the last pooled layer of the network.  He and McAuley (2016)  have shown that this representation contains useful visual features for a number of tasks. 

 Multimodal QE We propose different ways to integrate visual features in our two monomodal QE approaches (Sections 3.1 and 3.2). We compare each proposed model with its monomodal QE counterpart as baseline, both using the same hyperparameters. 

 Multimodal feature-based QE The feature-based textual features contain 15 numerical scores, while the visual feature vector contains 4,096 dimensions. To avoid over-weighting the visual features, we reduce their dimensionality using Principal Component Analysis (PCA). We consider up to 15 principal components in order to keep a balance between the visual features and the 15 text features from QuEst++. We choose the final number of principal components to keep according to the explained variance with the PCA, so this number is treated as a hyperparameter. After analysing the explained variance for up to 15 kept principal components (see Figure  4  in Appendix), we selected six numbers of principal components to train QE models with (1, 2, 3, 5, 10, and 15). As fusion strategy, we concatenate the two feature vectors. 

 Multimodal neural-based QE Multimodality is achieved with two changes in our monomodal models: multimodality integration (where to integrate the visual features in the architecture), and fusion strategy (how to fuse the visual and textual features). We propose the following places to integrate the visual feature vector into the BiRNN architecture: ? embed -the visual feature vector is used after the word embedding layer; ? annot -the visual feature vector is used after the encoding of the two input sentences by the two bi-directional RNNs; ? last -the visual feature vector is used just before the last layer. To fuse the visual and text features, we reduce the size of the visual features using a dense layer with a ReLu activation and reshape it to match the shape of the text-feature vector. As fusion strategies between visual and textual feature vectors, we propose the following: ? conc -concatenation with both source and target word representations for the 'embed' strategy; concatenation with the text features for the 'last' strategy; ? mult -element-wise multiplication for the target word representations and concatenation for the source word representations for the 'embed' strategy; element-wise multiplication with the text features for the 'annot' and 'last' strategies; ? mult2 -element-wise multiplication for both source and target word representations (exclusive to the 'embed' model). Figure  1  presents the high-level architecture of the document-level BiRNN model, with the various multimodality integration and fusion approaches. For example, in the 'embed' setting, the visual features are fused with each word representation from the embedding layers. Since this strategy modifies the embedding for each word, it can be expected to have a bigger impact on the result. 

 Results We use the standard training, development and test datasets from the WMT'18 Task 4 track. For feature-based systems, we follow the built-in crossvalidation in QuEst++, and train a single model with the hyperparameters found by cross-validation. For neural-based models, we use early-stopping with a patience of 10 to avoid over-fitting, and all reported figures are averaged over 5 runs corresponding to different seeds. We follow the evaluation method of the WMT QE tasks: Pearson's r correlation as the main metric  (Graham, 2015) , Mean-Absolute Error (MAE) and Root-Mean-Squared Error (RMSE) as secondary metrics. For statistical significance on Pearson's r, we compute Williams test  (Williams, 1959)  as suggested by  Graham and Baldwin (2014) . For all neural-based models, we experiment with the all three integration strategies ('embed', 'annot' and 'last') and all three fusion strategies ('conc', 'mult' and 'mult2') presented in Section 3.2. This leads to 6 multimodal models for each BiRNN and BERT-BiRNN. In Tables  2 and 4 , as well as in Figures  2 and 3 , we report the top three performing models. We refer the reader to the Appendix for the full set of results. 

 Sentence-level MQE The first part of Table  2  presents the results for sentence-level multimodal QE with BiRNN. The best model is BiRNN+Vis-embed-mult2, achieving a Pearson's r of 0.535, significantly outperforming the baseline (p-value<0.01). Visual features can, therefore, help to improve the performance of sentence-level neural-based QE systems significantly. Figure  2  presents the result of Williams significance test for BiRNN model variants. It is a correlation matrix that can be read as follows: the value in cell (i, j) is the p-value of Williams test for the change in performance of the model at row i compared to the model at column j  (Graham, 2015) . With the pre-trained token-level representations from BERT (second half of   2 : Pearson correlation at sentence-level on the WMT'18 dataset. We report the monomodal models (BiRNN, BERT-BiRNN) and their respective top-3 best performing multimodal variants (+Vis). We refer the reader to the Appendix for the full set of results. Here, BERT, ann-mul and emb-mul2 correspond to the BERT-BiRNN, the BERT-BiRNN+Vis-annot-mult and the BiRNN+Vis-embed-mult2 models of Table  2 . son's r of 0.602. This shows that even when using better word presentations, the visual features help to get further (albeit modest) improvements. Table  3  shows an example of predicted scores at the sentence-level for the baseline model (BiRNN) and for the best multimodal BiRNN model (BiRNN+Vis-embed-mult2). The multimodal model has predicted a closer score (-0.002) to the gold MQM score (0.167) than the baseline model (-0.248). The French translation is poor (cumulative-split is, for instance, not translated) as the low gold MQM score shows. However, the (main) word stopwatch is correctly translated as chronom?tre in French. Since the associated picture indeed represents a stopwatch, one explanation for this improvement could be that the multimodal model may have rewarded this correct and important part of the translation. 

 Source (EN) The A601X stopwatch features cumulative-split timing. 

 MT (FR) Le chronom?tre A601X dispose calendrier cumulative-split. gold MQM score 0.167 BiRNN -0.248 BiRNN+Vis-embed-mult2 -0.002 Table  3 : Example of performance of sentence-level multimodal QE. Compared to the baseline prediction (BiRNN), the prediction from the best multimodal model (BiRNN+Vis-embed-mult2) is closer to the gold MQM score. This could be because the word stopwatch is correctly translated as chronom?tre in French, and the additional visual feature confirms it. This could lead to an increase in the predicted score to reward the correct part, despite the poor translation (extracted from the Amazon Reviews Dataset of  McAuley et al., 2015) . 

 Document-level MQE Table  4  presents the results for the documentlevel feature-based and BiRNN neural QE models.  1  The first section shows the official models from the WMT'18 QE Task 4 report  (Specia et al., 2018a) . The neural-based approach SHEF-PT is the winning submission, outperforming another neural-based approach (SHEF-mtl-bRNN). For our BiRNN models (second section), BiRNN+Visembed-conc performs only slightly better than the monomodal baseline. For the feature-based models (third section), on the other hand, the baseline monomodal QuEst++ is outperformed by various multimodal variants by a large margin, with the one with two principal components (QuEst+Vis-2) performing the best. The more PCA components kept, the worse the results (see Appendix for full set of results).    

 Conclusions We introduced Multimodal Quality Estimation for Machine Translation, where an external modality -visual information -is incorporated to featurebased and neural-based QE approaches, on sentence and document levels. The use of visual features extracted from images has led to significant improvements in the results of state-of-the-art QE approaches, especially at sentence level. The version of deepQuest for multimodal QE and scripts to convert document into sentencelevel data are available on https://github.com/ sheffieldnlp/deepQuest. A Appendix PCA analysis Figure  4  shows an almost linear relationship between the number of principal components and the explained variance of the PCA (see Section 3.1), i.e. the higher the number of principal components, the larger the explained variance. Therefore, we experimented with various numbers of components up to 15 (1, 2, 3, 5, 10, and 15) on the development set to find the best settings for quality prediction.  

 Complete results Tables  5 and 6  present the full set of results of our experiments on document and sentence-level multimodal QE on our main test set, the WMT'18 test set. These are a super-set of the results presented in the main paper but include all combinations of multimodality integration and fusion strategies for sentence-level prediction, as well as different numbers of principal components kept for document-level QuEst prediction models. Additional test set Tables  7 and 8  present the full set of results of our experiments on the WMT'19 Task 2 test set on document and sentencelevel multimodal QE, respectively. This was the follow-up edition of the WMT'18 Task 4, where the same training set is used, but a new test set is released. For document-level, we observe nuanced results with more modest benefits in using visual features, regardless of the integration method or fusion strategy. For sentence-level, we observe on the one hand quite significant improvements with a gain of almost 8 points in Pearson's r over BiRNN, our monomodal baseline without pre-trained word embedding.   Figure 1 : 1 Figure 1: High-level representation of the documentlevel BiRNN architecture which illustrates how the visual features are integrated into the model. The three different strategies are 'embed', 'annot' and 'last'. 

 Figure 2 : 2 Figure 2: Williams significance test of top models for sentence-level BiRNN on the WMT'18 dataset.Here, BERT, ann-mul and emb-mul2 correspond to the BERT-BiRNN, the BERT-BiRNN+Vis-annot-mult and the BiRNN+Vis-embed-mult2 models of Table2. 

 Figure 3 3 Figure3shows the Williams significance test for document-level QuEst++ on the WMT'18 dataset. 

 Figure 3 : 3 Figure 3: Williams significance test of top models for document-level QuEst++ on the WMT'18 dataset. 

 Figure 4 : 4 Figure 4: Explained variance of 15 components (cumulative sum) for the training set of the WMT'18 Task data at document level. 

 Table 2 ) 2 , the best model is BERT-BiRNN+Vis-annot-mult, achieving a Pear- Pearson MAE RMSE BiRNN 0.504 0.539 0.754 +Vis-last-conc 0.483 0.531 0.746 +Vis-embed-mult 0.473 0.534 0.753 +Vis-embed-mult2 0.535 0.569 0.792 BERT-BiRNN 0.590 0.455 0.659 +Vis-annot-mult 0.602 0.454 0.654 +Vis-embed-conc 0.576 0.474 0.694 +Vis-embed-mult 0.598 0.486 0.686 

 Table 

 It is interesting to note that almost all Pearson MAE RMSE BiRNN 0.495 0.531 0.788 +Vis-last conc 0.476 0.550 0.802 +Vis-last-mult 0.481 0.543 0.812 +Vis-annot-mult 0.494 0.531 0.793 +Vis-embed-conc 0.501 0.536 0.780 +Vis-embed-mult 0.481 0.567 0.819 +Vis-embed-mult2 0.491 0.575 0.831 QuEst 0.503 0.547 0.802 +Vis-1 0.497 0.545 0.801 +Vis-2 0.536 0.534 0.790 +Vis-3 0.528 0.538 0.793 +Vis-5 0.520 0.539 0.797 +Vis-10 0.520 0.536 0.796 +Vis-15 0.515 0.540 0.801 

 Table 5 : 5 Document-level results for BiRNN and QuEst++ on the WMT'18 dataset, with and without visual features. Pearson MAE RMSE BiRNN 0.504 0.539 0.754 +Vis-last-conc 0.483 0.531 0.746 +Vis-last-mult 0.462 0.511 0.733 +Vis-annot-mult 0.460 0.521 0.741 +Vis-embed-conc 0.467 0.541 0.765 +Vis-embed-mult 0.473 0.534 0.753 +Vis-embed-mult2 0.535 0.569 0.792 BERT-BiRNN 0.590 0.455 0.659 +Vis-last-conc 0.360 0.993 1.252 +Vis-last-mult 0.529 0.520 0.744 +Vis-annot-mult 0.602 0.454 0.654 +Vis-embed-conc 0.576 0.474 0.694 +Vis-embed-mult 0.598 0.486 0.686 +Vis-embed-mult2 0.570 0.573 0.770 

 Table 6 : 6 Sentence-level results for BiRNN and BERT-BiRNN on the WMT'18 Task 4 dataset, with and without visual features. multimodal variants achieve better performance compared to the monomodal BiRNN baseline, with a peak when the visual features are fused with the word embedding representations by elementwise multiplication. On the other hand, we do not observe any gain in using visual features on the WMT'19 test set compared to our monomodal baseline with pre-trained word-embedding (BERT-BiRNN). Here that the BERT-BiRNN baseline model already performs very well. According to the task organisers, the mean MQM value on the WMT'19 test set is higher than on the WMT'18 test set, but actually closer to the training data(Fonseca    

			 The BERT-BiRNN models performed very poorly at this level and more research on why is left for future work.
