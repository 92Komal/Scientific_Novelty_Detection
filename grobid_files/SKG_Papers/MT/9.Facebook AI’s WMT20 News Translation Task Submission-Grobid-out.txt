title
Facebook AI's WMT20 News Translation Task Submission

abstract
This paper describes Facebook AI's submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil ? English and Inuktitut ? English, where there are limited out-of-domain bitext and monolingual data. We approach the low resource problem using two main strategies, leveraging all available data and adapting the system to the target news domain. We explore techniques that leverage bitext and monolingual data from all languages, such as self-supervised model pretraining, multilingual models, data augmentation, and reranking. To better adapt the translation system to the test domain, we explore dataset tagging and fine-tuning on in-domain data. We observe that different techniques provide varied improvements based on the available data of the language pair. Based on the finding, we integrate these techniques into one training pipeline. For En ? Ta, we explore an unconstrained setup with additional Tamil bitext and monolingual data and show that further improvement can be obtained. On the test set, our best submitted systems achieve 21.5 and 13.7 BLEU for Ta ? En and En ? Ta respectively, and 27.9 and 13.0 for Iu ? En and En ? Iu respectively.

Introduction We participate in the WMT20 news translation task in two low resource language pairs (four directions), Tamil ? English (Ta ? En and En ? Ta) and Inuktitut ? English (Iu ? En and En ? Iu). These language pairs are challenging due to the lack of in-domain bitext training data and limited monolingual data. For Tamil, the available bitext corpora are from various sources; however, none of the sources is in the news domain, and each corpus is in limited size or noisy. Inuktitut encompasses the challenges present for Tamil, but is even more challenging because the quantity of available monolingual data is even less than the bitext data. We explore techniques that leverage available data from all languages. First, we investigate supervised learning together with pre-training using mBART  (Liu et al., 2020) . Second, inspired by the recent success of improving low resource languages through multilingual models  (Arivazhagan et al., 2019; Tang et al., 2020) , we explore the utility of multilingual models, in the form of multilingual pretraining and subsequent fine-tuning. Third, we leverage the monolingual data of the source and target languages using data augmentation techniques, such as back-translation  (Sennrich et al., 2015)  and self-training  (Ueffing, 2006; Zhang and Zong, 2016; . Following , we apply these techniques iteratively. Fourth, we use noisy-channel model reranking  to further boost performance. The reranking uses language modeling to select a more fluent hypothesis, which requires monolingual data in the target language. Additionally, we investigate how adding substantially more unconstrained data can further improve the performance of En ? Ta system. We incorporate data from bitext mining efforts such as CCMA-TRIX  (Schwenk et al., 2019)  and CCALIGNED , as well as additional monolingual data from CCNET ) curated from CommonCrawl. The additional data is used for iterative back-translation and to train stronger language models for noisy-channel reranking. In a complementary direction, we investigate ways to adapt the translation system to the target domain. We explore controlled generation by adding dataset tags to indicate domain. Furthermore, we fine-tune our system on the in-domain data. For all language directions, we obtain our final systems by fusing a combination of the tech-niques mentioned above. We observe that the bulk of the improvements in our systems are from iterative back-translation and self-training, except the En ? Iu system where we only have exceptionally limited quantities of Inuktitut monolingual data. Noisy-channel reranking provides further improvement on top of strong systems, especially for to-English directions where we have high-quality news-domain monolingual data to train a good language model. Each of the other techniques, including dataset tagging, fine-tuning on in-domain data, and ensembling also provides nice improvements. 

 Data For the constrained track, we use monolingual data from all languages provided in WMT20 for mBART pre-training  (Liu et al., 2020) , and we use bitext data between English and other languages for training the system from scratch or fine-tuning the pretrained mBART models. We also require English, Tamil, and Inuktitut monolingual data for techniques such as back-translation, self-training, and creating language models for noisy-channel reranking. For low resource languages, Tamil and Inuktitut, we use all the available monolingual data, e.g. NewsCrawl + CommonCrawl + Wikipedia dumps for Tamil, and CommonCrawl for Inuktitut. For English, we only use NewsCrawl as the monolingual data because it is sufficiently large, high-quality, and in the news domain. For the unconstrained track, we use Tamil monolingual data and Tamil-English mined bitext data from external sources based on CommonCrawl. The details are described in Section 2.2. 

 Data filtering 

 Bitext data For each data source for each language pair, we remove duplicate sentence pairs and use fastText  (Joulin et al., 2016a,b)  language identification to remove sentence pairs where either the source or the target sentence is not predicted as the expected language. The resulting size of the bitext data of each language pair is shown in Appendix Table  A .1. 

 Monolingual Data We use monolingual data after fastText language identification filtering from all languages provided in WMT20 to train our mBART model. CommonCrawl contains a large quantity of data, but is also quite noisy as it is crawled from the web. Furthermore, the sentences are not in the news domain. To clean the data and select the sentences closer to the news domain, we apply the indomain filtering method described in (Moore and Lewis, 2010) for languages that have NewsCrawl monolingual data. First, we train two n-gram language models  (Heafield, 2011)  on NewsCrawl and CommonCrawl respectively. Then, for each sentence from CommonCrawl, we obtain scores from these two language models, compute the difference between normalized log-probability, and we remove the lowest-scoring sentences. We heuristically examine the data and remove the bottom 30%-60% of sentences. Concretely, the scoring function is H N C (s) ? H CC (s), where s is the sentence, H N C (s) and H CC (s) are the word-normalized cross entropy scores for sentence s by n-gram language model trained on NewsCrawl and Common-Crawl data respectively. We concatenate sentences from different sources and remove duplicate sentences for each language. We show the detailed dataset statistics in Appendix Table  A .2. 

 Unconstrained setup for Tamil In the unconstrained track, additional data can be used. We incorporate two additional sources of data: noisy bitext from data mining and monolingual data. 

 Mined bitext data We use mined bitext data from CCMA-TRIX  (Schwenk et al., 2019)  and CCALIGNED , two complementary mining strategies. Both approaches use the web data from unconstrained CommonCrawl to identify noisy bilingual matched pairs. CCMATRIX embeds monolingual sentences using LASER  (Schwenk and Douze, 2017)  multilingual sentence embeddings. To identify matching bitext pairs, the distance from each sentence to each other sentence is calculated based on the distance in the embedding space. For CCALIGNED, documents that could correspond to bitext pairs are aligned first at the document level, then at the paragraph level, and finally at the sentence level. In total, we include 2M aligned English-Tamil mined sentences. 

 Monolingual data We used additional Tamil monolingual data from CommonCrawl snapshots between 2017-26 to 2020-10 extracted by CCNET . We break down the document-level structure from CCNET into sentences and apply further processing. We concatenate all the snapshots of the additional monolingual data, deduplicate the sentences, apply fastText language identification and remove sentences are not predicted as Tamil. The final data results in 125M sentences. Subsequently, we concatenate the unconstrained monolingual data with constrained monolingual data, and we use them for back-translation and training Tamil language model. 

 System overview We use the Transformer  (Vaswani et al., 2017)  as our model architecture for all of our systems. To better train models with datasets in different sizes, we use random search to select the hyperparameters that achieve the best BLEU score on the validation set. We use sentencepiece  (Kudo and Richardson, 2018)  to learn the subword units to tokenize the sentences. The details of selected hyper-parameters are listed in Appendix D. All our systems are trained with fairseq 1 . 

 Dataset tag Training and decoding the model with an indication of domain (such as a specified dataset tag)  (Kobus et al., 2016)  is a technique that allows us to control the output domain of the trained system. Similarly,  Caswell et al. (2019) ;  show that adding specific tag to back-translated and selftranslated data can improve model performance. We add dataset tags to all of our systems described in this paper, by pre-pending a domain specific tag to the source sentence during training. At test time, we sweep over all the possible tags that are used during training including "no tag", and we choose the tag that achieves the best BLEU score on validation set. We find that when training with dataset tag, the supervised systems are 0.9 and 0.5 BLEU score higher than the system trained without dataset tag for Ta ? En and En ? Ta respectively. See results in Table  1 . 1 https://github.com/pytorch/fairseq 

 Baseline systems We investigate a variety of baseline approaches as the starting point for our models. For both Tamil and Inuktitut languages, we explore four different baseline systems, (1) bilingual supervised, (2) multilingual supervised, mBART pretraining with (3) bilingual and (4) multilingual fine-tuning. These systems are trained with constrained bitext and monolingual data. We will then improve these baseline models, as described in subsequent sections. 

 Bilingual supervised To train the base bilingual systems, we pre-pend the dataset tag to the source sentence to differentiate data from different corpus and concatenate all data sources for that language.  et al. (2019)  shows that multilingual model can improve the model performance of medium and low resource languages, as multilingual models are often trained on greater quantities of data compared to bitext models. Thus, we investigate if multilingual supervised models can be stronger starting points. We use all the bitext data between English and other languages provided in WMT20 to train many-to-one (XX ? English) and one-to-many (English ? XX) models. One challenge of multilingual training is different language directions have different quantities of data, and the high resource language can starve for capacity while low resource language can benefit from the transfer. To balance the trade-off between learning and transfer, we follow  Arivazhagan et al. (2019)  with a temperature-based strategy to sample sentences from different languages. Furthermore, for each direction, we optimize the transfer by selecting the best temperature and model checkpoint based on the BLEU score of the target language pair validation set. 

 Multilingual supervised 

 Arivazhagan 

 mBART-pretraining with bilingual and multilingual fine-tuning For mid and low resource languages, the quantity of available bitext may be low, but large resources of monolingual data exist. This monolingual data can be used in the form of pre-training, followed by subsequent fine-tuning into translation models. We use mBART  (Liu et al., 2020 ) -a multilingual denoising pre-training approach -to pre-train our systems, which has shown substantial improvements com-pared to training the model from scratch. First, we pre-train mBART across 13 languages (Cs, De, En, Fr, Hi, Iu, Ja, Km, Pl, Ps, Ru, Ta, Zh) on all monolingual data provided by WMT 20. For pretraining, we used a batch size of 2048 sequences per batch and trained the model for 240K steps. We learn the SPM jointly on all languages. We sample the same amount of sentences from monolingual data of all languages to learn a vocabulary of 130, 000 subwords. In the fine-tuning stage, we use exactly the same data sources as the bilingual supervised model and multilingual supervised model. For multilingual fine-tuning, previously people have built bitext translation systems by fine-tuning pretrained mBART models. Recent work  Tang et al. (2020)  extended this to multilingual fine-tuning, which can create multilingual translation models from multilingual pre-trained models. Different from  Tang et al. (2020) , we tune the temperature rate separately for the four language directions we focus on. In the multilingual fine-tuning stage, we use random search to sweep over dropout, learning rate, and temperature sampling factor, and we select the model checkpoint based on the BLEU score evaluated on the target language pair validation set. 

 Iterative back-translation (BT) Back-translation  (Sennrich et al., 2015)  is an effective data augmentation technique to improve model performance with target side monolingual data. The method starts from training a target to source translation system, which is subsequently used to translate the monolingual data in the target language back to source language. Then the synthetic back-translated dataset is concatenated with the raw bitext data to train the source to target translation model. After the source to target model is improved, the same technique can be applied again to train the back-translation system in the reversed direction. We repeat the process for several iterations until no significant improvement is obtained. In all of our back-translation systems, we follow  to add dataset tags to both raw bitext data and back-translated data. We upsample the bitext data, and the upsampling ratio is selected based on parameter sweeping and validating the resulting improvement on the validation set. Beam search with beam size 5 is used when generating the synthetic sentences. 

 Noisy-channel reranking (NCD) Reranking is a technique that uses a separate model to score and better select hypotheses from the nbest list generated by the the source to target model. To rerank our system output, we use the noisychannel model  as the scoring model . Given a source and target sentence pair (x, y), the noisychannel model scores it with log P (y|x) + ? 1 log P (x|y) + ? 2 log P (y) (1) where log P (y|x), log P (x|y) and log P (y) are the forward model, backward model and language model scores. The weights, ? 1 and ? 2 , are tuned through random search on the validation set. All of our submitted test set hypotheses are ranked and selected by noisy-channel reranking. The language models used in noisy-channel reranking are Transformers. For constrained track, we use the monolingual data as described in Section 2 to train the language models for English, Tamil. For Inuktitut, we find that the monolingual data is very limited and even smaller than the size of bitext data, therefore we concatenate the CommonCrawl data with the Inuktitut side of the bitext data together to train the Inuktitut language model. For unconstrained Tamil language model, we train on the constrained data with the additional unconstrained data extracted by CCNET as described in Section 2.2. The SPM size, model hyper-parameters, and evaluation of the language models can be found in Appendix B. 

 Self-training (ST) Self-training  (Ueffing, 2006; Zhang and Zong, 2016;  is a method that leverage monolingual data in source language to improve the system performance. We use the trained source to target translation system to translate monolingual data in source language to target language. Similar to BT, the synthetic dataset can be concatenated with bitext data to train the source to target model again. We follow  and use the noisy-channel model to select the top synthetic sentence when decoding from monolingual data into the source language. We inject the same types of noise to the source side of synthetic data as   

 Fine-tuning (FT) on validation set Fine-tuning is a technique to adapt the model to the target domain when the initial model is not trained with training data in the target domain. In both Tamil and Inuktitut, none of the training data is in news domain as the test data, therefore we finetune our final systems on a portion of the validation data and evaluate on the rest of hold-out validation data. For Tamil systems, we split the validation data with a 75-25 split, where 75% of the data is used for fine-tuning and 25% of the data is used for evaluation. Ta ? En and En ? Ta systems are fine-tuned and evaluated on the same split of validation dataset. For Inuktitut systems, we split the validation set based on the domain -Nunavut Hansard or news. For each domain, we split the validation data with a 75-25 split for fine-tuning and evaluation. We fine-tune our best performing Iu ? En and En ? Iu systems in domain on the corresponding validation set split. 

 Results In this section, we describe the details of our systems, and we report SACREBLEU  (Post, 2018)  on the validation set for intermediate iterations and ablations. For our validation set fine-tuned systems, we report the BLEU score on our validation holdout set split. Our general strategy for all language directions was to identify the best performing baseline setting, then iteratively improve upon the baseline using back-translation and self-training. Finally, we apply noisy-channel reranking and fine-tuning on validation set to create our final submission. 

 Baseline We explore four different baseline approaches as described in Section 3.2 for each language direction in the constrained setup, Inuktitut ? English and Tamil ? English. The detailed results are shown in Table  2 . First, bilingual models are trained with bilingual bitext data. Next, we focus on multilingual training. The multilingual supervised models are trained with all the available bitext data provided by WMT20. We use the same SPM as described in Section 3.2.3. For both bilingual and multilingual models, we initialize the model weights either randomly or with pre-trained mBART model weights. Therefore, for each language direction, we have four combinations, bilingual supervised, multilingual supervised, mBART + bilingual fine-tuning and mBART + multilingual fine-tuning. We use dataset tags for all systems, and we sweep the tag that performs the best when decoding on the validation set. Additional details and hyper-parameters are provided in the Appendix D. For to-English directions, both multilingual models and mBART pretraining can get better model performance than bilingual supervised model as shown in Table  2 . For Ta ? En direction, mBART + multilingual fine-tuning performs the best with 20.4 BLEU, which outperforms bilingual supervised system by 3.2 BLEU score. For the Iu ? En direction, mBART + bilingual fine-tuning works the best and gets 32.9 BLEU score, which outperforms bilingual supervised baseline by 2.8 BLEU score. However, for from-English directions, we do not observe similar advantages with either multilingual model or mBART pretraining, and a properly tuned bilingual supervised model achieves the best results for both directions. We get 8.0 BLEU score for En ? Ta direction, and we get 16.1 BLEU score for En ? Iu direction. 

 Tamil systems 

 Constrained Ta ? En system For the Ta ? En system, we first use the En ? Ta bilingual baseline system (ensemble) to gener- ate back-translation data from English NewsCrawl data. We then train our first iteration backtranslation system ("iter1-BT") with upsampled bitext (upsampling ratio tuned on the validation set). Similarly, we train our second iteration backtranslation system ("iter2-BT") with upsampled bitext and back-translation data generated by En ? Ta iter1-BT system (ensemble). The iter2-BT system (ensemble) is then used to generate ST data from Tamil NewsCrawl, CommonCrawl and Wiki data. We combine it with iter2-BT system's data to train the iter2-BT+ST system. Finally, we fine-tune this system on the validation set and apply noisychannel reranking to select the hypotheses. We explore Transformer models of different capacities and choose Transformer big (with 8K feed-forward dimension) for a good balance of performance and training speed. the iter2-BT+ST system (and its ensemble/finetuned version), we further enlarge the encoder to 10 layers given higher data abundance. We can see from Table  3  that our training pipeline improves model performance steadily (? 1.3 validation BLEU) after iterations, and in-domain finetuning as well as noisy-channel reranking are very helpful to alleviate the effects of train-test domain mismatch. 

 Constrained En ? Ta system For the En ? Ta system, we first use the mBART+multi-FT baseline system for Ta ? En to generate back-translation data from the monolingual data. We add different back-translation dataset tags based on the source of monolingual data and train our first iteration back-translation system ("iter1-BT") by tuning upsampling ratios on the bitext and back-translation datasets. For the model architecture, we explore the options of training Transformers from scratch and fine-tuning a pretrained mBART model and find that the former performs better with ensembles. Doing one iteration of training with back-translation data gives 5.8 BLEU increase (Table  3 ). We further train the second iteration back-translation system ("iter2- BT") with back-translation data generated from the best iter1-BT Ta ? En system. As the gain from the second iteration is small (0.4 BLEU), we do not continue for the third iteration. Noisy-channel reranking is applied with the best systems from both language directions and the Tamil language model (Appendix B). We observe little gain (0.1 BLEU) and suspect it's due to the high perplexity of the language model. Further fine-tuning the iter2-BT model on the validation set gives 4.1 BLEU score improvement on the validation holdout set. back-translated data from unconstrained monolingual sources with back-translated data from WMT monolingual data from English and Tamil, with the WMT bitext and mined Ta ? En data. We used the same BPE and vocabulary as the constrained system. The data was deduplicated, and the validation and test data removed if an exact match was present in the training data. The mined data was additionally cleaned to remove sentences longer than 250 BPE tokens, as well as bitext pairs where the length between the source and target was greater than 2.5x difference. Subsequently, we trained a large Transformer sequence-to-sequence model on the total combined data using various data domain tags. After training was complete, we further fine-tuned on the validation set, as described in Section 4.2.2. We applied noisy-channel reranking when decoding test data. The forward model is ensembled with two of the best performing fine-tuned models. The backward model is the best performing model in Section 4.2.1, which is ensembled with two fine-tuned models. The language model is unconstrained Tamil language model described in Section 3.4. We rerank from best 20 hypothesises generated by ensembled forward model, and we achieve 20.2 BLEU score on validation set. 

 Inuktitut systems The Inuktitut validation and test set are composed of data from two different domains, the proceeding of the Legislative Assembly of Nunavut from Nunavut Hansard (NH) and news. We find that the model can be further improved if we optimize our translation training pipeline for these two domains separately, and therefore we train and report BLEU score separately for each domain. We also report the BLEU score on the whole validation set, where we use the domain-specific system to decode on the portion of the corresponding domain, concatenate the hypothesises and compute the BLEU score. 

 Constrained Iu ? En systems For the Iu ? En system, we use En ? Iu bilingual supervised system described in Section 4.1 for back-translation. The model used for decoding is an ensemble of 3 En ? Iu models, and we decode from the English NewsCrawl data. We concatenate the back-translated data with bitext data and sweep the upsampling ratio of the bitext data to find the best ratio. We experiment with both mBART pretraining + bilingual fine-tuning and training from scratch, and we find that mBART + bilingual finetuning works better on Nunavut Hansard domain of validation set, and training from scratch works better on news domain. The hypothesis is that the English NewsCrawl monolingual data for backtranslation is in-domain with the news domain validation set and there is huge amount of English NewsCrawl data, so the advantage of pretraining is not significant. We also experiment with selftraining on Iu ? En direction in Nunavut Hansard domain, where we use the source to target model (ensembled) to decode from the Inuktutit side of Nunavut Hansard 3.0 parallel corpus with noisychannel reranking; however, we do not observe any improvement. The best result at the first iteration is from the back-translation system, which outperforms baseline system by 2.2 BLEU score (Table  4 ), where most of the gain comes from improvement on news domain. We do not observe gains for doing the second iteration of back-translation for Iu ? En system, and we suspect that it is due to lack of improvement for our En ? Iu model from supervised approach to the first iteration. We then fine-tune the best iteration 1 Iu ? En models on validation data for each domain. The final domain-specific systems are ensembled from the fine-tuned models and followed by noisy-channel reranking. To use noisy-channel reranking for Nunavut Hansard domain, we finetune the English language model described in 3.4 on English side of the Nunavut Hansard 3.0 training data provided in WMT20. The best Iu ? En system we submit has 40.2 BLEU score on our validation holdout set. 

 Constrained En ? Iu systems We experiment with both self-training and backtranslation with the best baseline systems reported in 4.1 to improve En ? Iu system. For self-training, we use ensembled supervised En ? Iu model and beam decoding with beam size 5 to decode from English monolingual data. We decode from the English side of Nunavut Hansard 3.0 parallel corpus to train the model for Nunavut Hansard domain, and we decode from the English NewsCrawl data for news domain. However, we do not observe improvement for news domain, and there is only mild improvement (0.3 BLEU) for Nunavut Hansard domain as shown in Table  5 . For backtranslation, we use iteration 1 Iu ? En news domain model from 4.3.1 to decode constrained Inuktitut CommonCrawl data. We get no improvement on Nunavut Hansard domain and mild improvement (0.2 BLEU) on news domain. We use selftraining system for Nunavut Hansard domain and back-translation system for news domain, and it achieves 16.3 BLEU score on the validation set, which is merely 0.2 BLEU score improvement over baseline system. We then fine-tune the best systems we get on domain-specific validation set splits, followed by ensembling and noisy-channel reranking. The fine-tuning is very effective for the news domain, where we get 9.1 BLEU score improvement. This is expected because we do not have any training data from news domain. Our final submitted system achieves 22.0 on our validation holdout set. 

 Submitted system BLEU Ta ? En 21.5 En ? Ta 12.6 En ? Ta (unconst.) 13.7 Iu ? En 27.9 En ? Iu 13.0 Table  6 : Results of our best submitted systems of each direction. We report BLEU scores on newstest2020. 

 Conclusion This paper describes Facebook AI's Transformer based translation systems for the WMT20 news translation shared task. We focused on two lowresource languages pairs, Tamil ? English and Inuktitut ? English, and we explored the same set of techniques, including dataset tagging, mBART pretraining and fine-tuning, back-translation and self-training, fine-tuning on domain-specific data, ensembling, and noisy-channel reranking. We demonstrated strong improvements by stacking these techniques properly on three language directions, Ta ? En , En ? Ta , and Iu ? En . The En ? Iu direction is difficult to improve due to lack of target side monolingual data. Surprisingly, self-training does not work on En ? Iu either even we have huge amounts of in-domain English side monolingual data. We are interested in continued exploration on how to better leverage source side monolingual data to improve En ? Iu and other low resource languages where we do not have enough target side monolingual data. 
