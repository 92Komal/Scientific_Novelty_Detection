title
Computing Optimal Alignments for the IBM-3 Translation Model

abstract
Prior work on training the IBM-3 translation model is based on suboptimal methods for computing Viterbi alignments. In this paper, we present the first method guaranteed to produce globally optimal alignments. This not only results in improved alignments, it also gives us the opportunity to evaluate the quality of standard hillclimbing methods. Indeed, hillclimbing works reasonably well in practice but still fails to find the global optimum for between 2% and 12% of all sentence pairs and the probabilities can be several tens of orders of magnitude away from the Viterbi alignment. By reformulating the alignment problem as an Integer Linear Program, we can use standard machinery from global optimization theory to compute the solutions. We use the well-known branch-and-cut method, but also show how it can be customized to the specific problem discussed in this paper. In fact, a large number of alignments can be excluded from the start without losing global optimality.

1 Introduction  Brown et al. (1993)  proposed to approach the problem of automatic natural language translation from a statistical viewpoint and introduced five probability models, known as IBM 1-5. Their models were single word based, where each source word could produce at most one target word. State-of-the-art statistical translation systems follow the phrase based approach, e.g.  (Och and Ney, 2000; Marcu and Wong, 2002; Koehn, 2004; Chiang, 2007; Hoang et al., 2007) , and hence allow more general alignments. Yet, single word based models  (Brown et al., 1993; Brown et al., 1995; Vogel et al., 1996)  are still highly relevant: many phrase based systems extract phrases from the alignments found by training the single word based models, and those that train phrases directly usually underperform these systems  (DeNero et al., 2006) . Single word based models can be divided into two classes. On the one hand, models like IBM-1, IBM-2 and the HMM are computationally easy to handle: both marginals and Viterbi alignments can be computed by dynamic programming or even simpler techniques. On the other hand there are fertility based models, including IBM 3-5 and Model 6. These models have been shown to be of higher practical relevance than the members of the first class  (Och and Ney, 2003)  since they usually produce better alignments. At the same time, computing Viterbi alignments for these methods has been shown to be NP-hard  (Udupa and Maji, 2006) , and computing marginals is no easier. The standard way to handle these models -as implemented in GIZA++  (Al-Onaizan et al., 1999; Och and Ney, 2003)  -is to use a hillclimbing algorithm. Recently  Udupa and Maji (2005)  proposed an interesting approximation based on solving sequences of exponentially large subproblems by means of dynamic programming and also addressed the decoding problem. In both cases there is no way to tell how far away the result is from the Viterbi alignment. In this paper we solve the problem of finding IBM-3 Viterbi alignments by means of Integer Linear Programming  (Schrijver, 1986) . While there is no polynomial run-time guarantee, in practice the applied branch-and-cut framework is fast enough to find optimal solutions even for the large Canadian Hansards task (restricted to sentences with at most 75 words), with a training time of 6 hours on a 2.4 GHz Core 2 Duo (single threaded). Integer Linear Programming in the context of machine translation first appeared in the work of  Germann et al. (2004) , who addressed the translation problem (often called decoding) in terms of a travelings-salesman like formulation.  Recently, DeNero and Klein (2008)  addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. The main difference to our work is that they allow only consecutive words in the phrases. In their formulation, allowing arbitrary phrases would require an exponential number of variables. In contrast, our approach handles the classical single word based model where any kind of "phrases" in the source sentence are aligned to one-word phrases in the target sentence.  Lacoste-Julien et al. (2006)  propose an integer linear program for a symmetrized word-level alignment model. Their approach also allows to take the alignments of neighboring words into account. In contrast to our work, they only have a very crude fertility model and they are considering a substantially different model. It should be noted, however, that a subclass of their problems can be solved in polynomial time -the problem is closely related to bipartite graph matching. Less general approaches based on matching have been proposed in  (Matusov et al., 2004)  and  (Taskar et al., 2005) . Recently  Bodrumlu et al. (2009)  proposed a very innovative cost function for jointly optimizing dictionary entries and alignments, which they minimize using integer linear programming. They also include a mechanism to derive N-best lists. However, they mention rather long computation times for rather small corpora. It is not clear if the large Hansards tasks could be addressed by their method. An overview of integer linear programming approaches for natural language processing can be found on http://ilpnlp.wikidot.com/. To facilitate further research in this area, the source code will be made publicly available. 

 Contribution The key contribution of our work is a method to handle exact fertility models as arising in the IBM-3 model in a global optimization framework. This is done by a linear number of linear consistency constraints. Unlike all previous works on integer linear programming for machine translation, we do not solely use binary coefficients in the constraint matrix, hence showing that the full potential of the method has so far not been explored. At the same time, our method allows us to give a detailed analysis of the quality of hillclimbing approaches. Moreover, we give a more detailed description of how to obtain a fast problem-tailored integer solver than in previous publications, and include a mechanism to a priori exclude some variables without losing optimality. 

 The IBM-3 Translation Model Given a source sentence f J 1 , the statistical approach to machine translation is to assign each possible target sentence e I 1 a probability to be an accurate translation. For convenience in the translation process, this probability is usually rewritten as P (e I 1 |f J 1 ) = 1 p(f J 1 ) ? p(e I 1 ) ? p(f J 1 |e I 1 ) , and the training problem is to derive suitable parameters for the latter term from a bilingual corpus. Here, the probability is expressed by summing over hidden variables called alignments. The common assumption in single word based models is that each source position j produces a single target position a j ? {0, . . . , I}, where an artificial 0position has been introduced to mark words without a correspondence in the target sentence. The alignment of a source sentence is then a vector a J 1 , and the probability can now be written as p(f J 1 |e I 1 ) = a J 1 p(f J 1 , a J 1 |e I 1 ) . We will focus on training the IBM-3 model which is based on the concept of fertilities: given an alignment a J 1 , the fertility ? i (a J 1 ) = j:a j =i 1 of target word i expresses the number of source words aligned to it. Omitting the dependence on a J 1 (and defining p(j|0) = 1), the probability is expressed as p(f J 1 , a J 1 |e I 1 ) = p(? 0 |J) ? I i=1 ? i ! p(? i |e i ) ? j p(f j |e a j ) ? p(j|a j ) . (1) For the probability p(? 0 |J) of the fertility of the empty word, we use the modification introduced in  (Och and Ney, 2003) , see there for details. In summary, the model comprises a single word based translation model, an inverted zero-order alignment model and a fertility model. We now discuss how to find the optimal alignment for given probabilities, i.e. to solve the problem arg max a J 1 p(f J 1 , a J 1 |e I 1 ) (2) for each bilingual sentence pair in the training set. This is a desirable step in the approximate EM-algorithm that is commonly used to train the model. 

 Finding IBM-Viterbi Alignments via Integer Linear Programming Instead of solving (2) directly we consider the equivalent task of minimizing the negative logarithm of the probability function. A significant part of the arising cost function is already linear in terms of the alignment variables, a first step for the integer linear program (ILP) we will derive. To model the problem as an ILP, we introduce two sets of variables. Firstly, for any source position j ? {1, . . . , J} and any target position i ? {0, . . . , I} we introduce an integer variable x ij ? {0, 1} which we want to be 1 exactly if a j = i and 0 otherwise. Since each source position must be aligned to exactly one target position, we arrive at the set of linear constraints i x ij = 1 , j = 1, . . . , J . (3) The negative logarithm of the bottom row of (  1 ) is now easily written as a linear function in terms of the variables x ij : i,j c x ij ? x ij , c x ij = ? log p(f j |e i ) ? p(j|i) . For the part of the cost depending on the fertilities, we introduce another set of integer variables y if ? {0, 1}. Here i ? {0, . . . , I} and f ranges from 0 to some pre-specified limit on the maximal fertility, which we set to max(15, J/2) in our experiments (fertilities > J need not be considered). We want y if to be 1 if the fertility of i is f , 0 otherwise. Hence, again these variables must sum to 1: f y if = 1 , i = 0, . . . , I . (4) The associated part of the cost function is written as i,f c y if ? y if , c y if = ? log f ! p(f |e i ) , i = 1, . . . , I c y 0f = ? log p(? 0 = f |J) . It remains to ensure that the variables y if expressing the fertilities are consistent with the fertilities induced by the alignment variables x ij . This is done via the following set of linear constraints: j x ij = f f ? y if , i = 0, . . . , I . (5) Problem (  2 ) is now reduced to solving the integer linear program arg min  4 ), (  5 ) {x ij },{y if } i,j c x ij x ij + i,f c y if y if subject to (3), ( x ij ? {0, 1}, y if ? {0, 1} , (6) with roughly 2 I J variables and roughly J + 2I constraints. 

 Solving the Integer Linear Program To solve the arising integer linear programming problem, we first relax the integrality constraints on the variables to continuous ones: x ij ? [0, 1], y if ? [0, 1] , and obtain a lower bound on the problems by solving the arising linear programming relaxation via the dual simplex method. While in practice this can be done in a matter of milli-seconds even for sentences with I, J > 50, the result is frequently a fractional solution. Here the alignment variables are usually integral but the fertility variables are not. In case the LP-relaxation does not produce an integer solution, the found solution is used as the initialization of a branch-and-cut framework. Here one first tries to strengthen the LP-relaxation by deriving additional inequalities that must be valid for all integral solutions see e.g.  (Schrijver, 1986; Wolter, 2006)  and www.coin-or.org. These inequalities are commonly called cuts. Then one applies a branch-and-bound scheme on the integer variables. In each step of this scheme, additional inequalities are derived. The process is further sped-up by introducing a heuristic to derive an upper bound on the cost function. Such bounds are generally given by feasible integral solutions. We use our own heuristic as a plug-in to the solver. It generates solutions by thresholding the alignment variables (winner-take-all) and deriving the induced fertility variables. An initial upper bound is furthermore given by the alignment found by hillclimbing. We suspect that further speed-ups are possible by using so-called follow-up nodes: e.g. if in the branch-and-bound an alignment variable x ij is set to 1, one can conclude that the fertility variable y i0 must be 0. Also, sets of binary variables that must sum to 1 as in (  3 ) and (  4 ) are known as special ordered sets of type I and there are variants of branch-and-cut that can exploit these properties. However, in our context they did not result in speed-ups. Our code is currently based on the open source COIN-OR project 1 and involves the linear programming solver CLP, the integer programming solver CBC, and the cut generator library CGL. We have also tested two commercial solvers. For the problem described in this paper, CBC performed best. Tests on other integer programming tasks showed however that the Gurobi solver outperforms CBC on quite a number of problems. 

 Speed-ups by Deriving Bounds It turns out that, depending on the cost function, some variables may a priori be excluded from the optimization problem without losing global optimality. That is, they can be excluded even before the first LP-relaxation is solved. The affected variables have relatively high cost coefficients and they are identified by considering lower bounds and an upper bound on the cost function. Starting from the lower bounds, one can then identify variables that when included in a solution would raise the cost beyond the upper bound. An upper bound u on the problem is given by any alignment. We use the one found by hillclimbing. If during the branch-and-cut process tighter upper bounds become available, the process could be reapplied (as a so-called column cut generator). For the lower bounds we use different ones to exclude alignment variables and to exclude fertility variables. 1 www.coin-or.org 

 Excluding Alignment Variables To derive a lower bound for the alignment variables, we first observe that the cost c x ij for the alignment variables are all positive, whereas the cost c y if for the fertilities are frequently negative, due to the factorial of f . A rather tight lower bound on the fertility cost can be derived by solving the problem l F,1 = min {? i } I i=0 c y i? i s.t. i ? i = J , (7) which is easily solved by dynamic programming proceeding along i. A lower bound on the alignment cost is given by l A = j l A,j , where l A,j = min i=0,...,I c x ij . The lower bound is then given by l 1 = l F,1 + l A , and we can be certain that source word j will not be aligned to target word i if c x ij > l A,j + (u ? l 1 ) . 

 Excluding Fertility Variables Excluding fertility variables is more difficult as cost can be negative and we have used a constraint to derive l F,1 above. At present we are using a two ways to generate a lower bound and apply the exclusion process with each of them sequentially. Both bounds are looser than l 1 , but they immensely help to get the computation times to an acceptable level. The first bound builds upon l 1 as derived above, but using a looser bound l F,2 for the fertility cost: l F,2 = i min ? i c y i? i . This results in a bound l 2 = l F,2 + l A , and fertility variables can now be excluded in a similar manner as above. Our second bound is usually much tighter and purely based on the fertility variables: l 3 = i min ? i c y i? i + min J ?{1,...,J} : |J =? i | c x i (J ) , with c x i (J ) = j?J c x ij , and where the cost of the empty set is defined as 0. Although this expression looks rather involved, it is actually quite easy to compute by simply sorting the respective cost entries. A fertility variable y if can now be excluded if the difference between c y if and the contribution of i to l 3 exceeds u ? l 3 . We consider it likely that more variables can be excluded by deriving bounds in the spirit of (  7 ), but with the additional constraint that ? i = f for some i and f . We leave this for future work. 

 Experiments We have tested our method on three different tasks involving a total of three different languages and each in both directions. The first task is the wellknown Canadian Hansards 2 task (senate debates) for French and English. Because of the large dataset we are currently only considering sentence pairs where both sentences have at most 75 words. Longer sentences are usually not useful to derive model parameters. The other two datasets are released by the European Corpus Initiative 3 . We choose the Union Bank of Switzerland (UBS) corpus for English and German and the Avalanche Bulletins, originally released by SFISAR, for French and German. For the latter task we have annotated alignments for 150 of the training sentences, where one annotator specified sure and possible alignments. For details, also on the alignment error rate, see  (Och and Ney, 2003) . All corpora have been preprocessed with language-specific rules; their statistics are given in Table  1 . We have integrated our method into the standard toolkit GIZA++ 4 and are using the training scheme 1 5 H 5 3 5 4 5 for all tasks. While we focus on the IBM-3 stage, we also discuss the quality of the resulting IBM-4 parameters and alignments. Experiments were run on a 2.4 GHz Core 2 Duo with 4 GB memory. For most sentence pairs, the memory consumption of our method is only marginally more than in standard GIZA++ (600 MB). In the first iteration on the large Hansards task, however, there are a few very difficult sentence pairs where the solver needs up to 90 minutes and 1.5 GB . We observed this in both translation directions. 2 www.isi.edu/natural-language/ download/hansard/ 3 The entire CD with many more corpora is available for currently 50 Euros. 4 available at code.google.com/p/giza-pp/ .  

 Avalanche Bulletin 

 Evaluating Hillclimbing In our first set of experiments, we compute Viterbi alignments merely to evaluate the quality of the standard training process. That is, the model parameters are updated based on the alignments found by hillclimbing. Table  2  reveals that, as expected, hillclimbing does not always find the global optimum: depending on the task and iteration number, between 2 and 12 percent of all hillclimbing alignments are suboptimal. For short sentences (i.e. I, J ? 20) hillclimbing usually finds the global optimum. Somewhat more surprisingly, even when a good and hence quite focused initialization of the IBM-3 model parameters is given (by training HMMs first), the probability of the Viterbi alignment can be up to a factor of 10 37 away from the optimum. This factor occurred on the Hansards task for a sentence pair with 46 source and 46 target words and the fertility of the empty word changed from 9 (for hillclimbing) to 5. 

 Hillclimbing vs. Viterbi Alignments We now turn to a training scheme where the Viterbi alignments are used to actually update the model parameters, and compare it to the standard training scheme (based on hillclimbing). Table  2 : Analysis of Hillclimbing on all considered tasks. All numbers are for the IBM-3 translation model. Iteration 1 is the first iteration after the transfer from HMM, the final iteration is the transfer to IBM4. The factors are w.r.t. the original formulation, not the negative logarithm of it and are defined as the maximal ratio between the Viterbi probability and the hillclimbing probability. une baisse de la temp?rature a en g?n?ral stabilis? la couverture neigeuse . ein Temperaturr?ckgang hat die Schneedecke im allgemeinen stabilisiert . 

 Standard training (hillclimbing). une baisse de la temp?rature a en g?n?ral stabilis? la couverture neigeuse . ein Temperaturr?ckgang hat die Schneedecke im allgemeinen stabilisiert . Proposed training (Viterbi alignments). Indeed Table  3  demonstrates that with the new training scheme, the perplexities of the final IBM-3 iteration are consistently lower. Yet, this effect does not carry over to IBM-4 training, where the perplexities are consistently higher. Either this is due to overfitting or it is better to use the same method for alignment computation for both IBM-3 and IBM-4. After all, both start from the HMM Viterbi alignments. Interestingly, the maximal factor between the hillclimbing alignment and the Viterbi alignment is now consistently higher on all tasks and in all iterations. The extreme cases are a factor of 10 76 for the Canadian Hansards English ? French task and 10 30 for the Bulletin French ? German task. Table  4  demonstrates that the alignment error rates of both schemes are comparable. Indeed, a manual evaluation of the alignments showed that most of the changes affect words like articles or prepositions that are generally hard to translate. In many cases neither the heuristic nor the Viterbi alignment could be considered correct. An interesting case where the proposed scheme produced the better alignment is shown in Figure  1 . In summary, our results give a thorough justification for the commonly used heuristics. A test with the original non-deficient empty word model of the IBM-3 furthermore confirmed the impression of  (Och and Ney, 2003)  that overly many words are aligned to the empty word: the tendency is even stronger in the Viterbi alignments. 

 Optimizing Running Time The possibilities to influence the run-times of the branch-and-cut framework are vast: there are nu- merous ways to generate cuts and several of them can be used simultaneously. The CBC-package also allows to specify how many rounds of cuts to derive at each node. Then there is the question of whether to use the bounds derived in Section 5 to a priori exclude variables. Finally, branchand-cut need not be done on all variables: since solving LP-relaxations typically results in integral alignments, it suffices to do branch-and-cut on the fertility variables and only add the alignment variables in case non-integral values arise (this never happened in our experiments 5 ). We could not possibly test all combinations of the listed possibilities, and our primary focus was to achieve acceptable run-times for the large Hansards task. Still, in the end we have a quite uniform picture: the lowest run-times are achieved by using Gomory Cuts only. Moreover, including all variables for branching was between 1.5 and 2 times faster than only including fertility variables. Only by exploiting the bounds derived in Section 5 the run-times for the Hansards task in direction from English to French became acceptable. We believe that further speed-ups are possible by deriving tighter bounds, and are planning to investigate this in the future. We end up with roughly 6 hours for the Hansards task, roughly 3 minutes for the UBS task, and about 2.5 minutes for the Avalanche task. In all cases the run-times are much higher than in the standard GIZA++ training. However, we are now getting optimality guarantees where previously one could not even tell how far away one is from the optimum. And the Viterbi alignments of several sentence pairs can of course be computed in parallel. Lastly, we mention the possibility of setting a 

 Conclusion We present the first method to compute IBM-3 Viterbi alignments with a guarantee of optimality. In contrast to other works on integer linear programming for machine translation, our formulation is able to include a precise and very general fertility model. The resulting integer linear program can be solved sufficiently fast in practice, and we have given many comments on how problem-specific knowledge can be incorporated into standard solvers. The proposed method allows for the first time to analyze the quality of hillclimbing approaches for IBM-3 training. It was shown that they can be very far from the optimum. At the same time, this seems to happen mostly for difficult sentences that are not suitable to derive good model parameters. In future work we want to derive tighter bounds to a priori exclude variables, combine the method with the N-best list generation of  (Bodrumlu et al., 2009)  and evaluate on a larger set of corpora. Finally we are planning to test other integer programming solvers. Figure 1 : 1 Figure 1: Comparison of training schemes. Shown are the alignments of the final IBM-3 iteration. 
