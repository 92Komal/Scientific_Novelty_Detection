title
Tencent Neural Machine Translation Systems for the WMT20 News Translation Task

abstract
This paper describes Tencent Neural Machine Translation systems for the WMT 2020 news translation tasks. We participate in the shared news translation task on English ? Chinese and English ? German language pairs. Our systems are built on deep Transformer and several data augmentation methods. We propose a boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese ? English task.

Introduction Recently, Transformer  (Vaswani et al., 2017) , that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks  (Wang et al., 2018b; Sun et al., 2019) . In this year's translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese ? English, English ? Chinese and English ? German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models . In terms of data augmentation, we adopt R2L training  to all the tasks. Monolingual data is only used in English ? German task following the back-translation manner  (Sennrich et al., 2016b) . Different from the standard backtranslation, we add noise to the synthetic source sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning  (Sun et al., 2019)  is very effective in our three experiments and specially, we propose a boosted finetuning method for English ? Chinese tasks. We also take advantage of the combination methods to further improve the translation quality. The "greedy search ensemble algorithm"  is used to select the best combinations from single models. Then for English ? Chinese tasks we propose an iterative transductive ensemble (ITE) method based on the translation results of the ensemble models. For English ? German task, we apply the noise channel model for re-ranking  (Yee et al., 2019) . This paper was structured as follows: Section 2 describes the dataset. We present the detailed overview of our system in Section 3. The experiment settings and main results are shown in Section 4. Finally, we conclude our work in Section 5. 

 Dataset 

 Chinese ? English The bilingual data used in Chinese ? English task includes all the available corpus provided by WMT2020: News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT Corpus, Wiki-Matrix, Back-translated news. The Chinese sentences are segmented by jieba segmentor 1 while the English side is processed by Moses tokenizer. We collect 18M sentence pairs after filtering. 

 English ? German The bilingual data used in this task includes all the available corpus provided by WMT2020. For the Paracrawl part, We filter most of the data due to bad quality and collect 15M sentence pairs. Totally, 22M sentence pairs are used for training. Both the languages are tokenized by tokenize.perl script 2 . Then BPE is applied with 32K operations. The vocabulary is shared with 32K unique words. For monolingual data, we randomly select 80M sentences from NewsCrawl2017-2019 for backtranslation and 45M are used for training after filtering 

 Data Processing Pre-processing To pre-process the raw data, we apply a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generated subwords via BPE  (Sennrich et al., 2016c)  with pre-defined merge operations of 32,000. Filtering To improve the quality of data, we filtered noisy sentence pairs according to their characteristics in terms of language identification, duplication, length, invalid string and edit distance. More specifically, we filter out the sentences longer than 150 words. The word ratio between the source and the target must not exceed 1:1.3 or 1.3:1. According to our observations, the filtering method can significantly reduce noise issues including misalignment, translation error, illegal characters, overtranslation and under-translation. 3 System Overview 

 Model Architecture In our systems, we adopt four different model architectures with TRANSFORMER  (Vaswani et al., 2017) : ? DEEP TRANSFORMER  (Dou et al., 2018; Dou et al., 2019)  is the TRANSFORMER-BASE model with the 40layer encoder. ? HYBRID TRANSFORMER  is the TRANSFORMER-BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ON-LSTM  (Shen et al., 2019)  encoder. ? BIGDEEP TRANSFORMER is the TRANSFORMER-BIG model with 20 encoder layers. ? LARGER TRANSFORMER is similar to BIGDEEP model except that it uses 8192 as the FFN inner width. The main differences between these models are presented in Table  1 . To stabilize the training of deep model, we use the Pre-Norm strategy . The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize ? Transform ? dropout ? residual-add. All models are implemented on top of the open-source toolkit Fairseq 3  (Ott et al., 2019) . 

 Data Augmentation Data augmentation is a commonly used technique to improve the translation quality. There are various of methods to conduct data augmentation such as back-translation  (Sennrich et al., 2016a) , joint training  etc. In this section, we will introduce the methods we used in WMT2020. 

 Large-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT  (Sennrich et al., 2016a) . The method first trains an intermediate target-to-source system, which is used to translate target monolingual corpus into source. Then the synthetic parallel corpus is used to train models together with the bilingual data. In this work we apply the noise back-translations method as introduced in  (Lample et al., 2018) . When translating monolingual data we use an ensemble of two models to get better source translations. We follow  (Edunov et al., 2018)  to add noise to the synthetic source data. Furthermore, we use a tag at the head of each synthetic source sentence as  Caswell et al. (2019)  does. To filter the pseudo corpus, we translate the synthetic source into target and calculate a Round-Trip BLEU score, the synthetic pairs are dropped if the BLEU score is lower than 30. Notably, we only apply back translation to the English ? German task. We find that back translation decrease the translation quality to Chinese ? English tasks in our experiments.  

 R2L Training The approach is proposed by . The main idea is to integrate the information of Right-to-Left (R2L) models to Left-to-Right (L2R) ones. Following this work, we translate the source sentences of the parallel data with both a R2L model and a L2R model, and use the translated pseudo corpus to improve the L2R model. We drop the pseudo parallel data if the BLEU score lower than 15. This method is applied to all the three tasks. 

 Finetuning We use in-domain finetuning to further improve the model performance on news domain as previous study  (Sun et al., 2019)  shows that finetuning is very effective on the WMT2019 news translation tasks. For the three tasks, the finetuning is slight different and we will introduce them seprately in following of this section.  

 Re-ranking We use noisy channel model re-ranking method  (Yee et al., 2019) . This method is implemented in Fairseq 4 . Three features are used as following: Source-to-Target Model Instead of a single model, we use the ensemble model as source-totarget model. Four well-trained single models are used. The decoding beam size is set to 25. We collect the log probability of each translation candidates. Target-to-Source Model The target-to-source model is the channel mode which is used to translate the candidates back to source. We use a big transformer model for target-to-source. Language Models For language model, we train a small GPT-2 model with FFN=8192 for target monolingual data. Tuning We use random search to choose values in the range [0, 3) for ? 1 , ? 2 and length penalty. The parameters are tuned on development set. 

 Ensemble Model ensemble is a widely used technique in previous WMT workshops  Sun et al., 2019; Wang et al., 2018a)  which can boost the performance by combining the predictions of several models at each decoding step. In our work, we use two kinds of ensemble methods and finally the two are combined for further improvements. 

 Greedy Based Ensemble This method is proposed by . The method adopts an easy operable greedy-base strategy to search for a better single model combinations on the development set. For more detail, please refer to the original paper. We also train single models with different hyper parameters to ensure the diversity. We refer to this method as Ensemble in the following. 

 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by  Wang et al. (2020c) . The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble Input: Single models M m 1 , In-domain corpus D, E n 1 is n different ensemble combinations Output: Single models M m 1 1 Translate D with E n 1 and get D 1 n 2 Train each M m 1 on D ? D 1 n and get M 1 m , then M m 1 = M 1 m 3 t := 0 while not convergence do 4 Translate D with M m 1 and get D 1 n , then D 1 n = D 1 n ? D 1 n 5 Train each M m 1 on D ? D 1 n and get M 1 m , then M m 1 = M 1 m 6 t := t + 1 7 return , 4 Experiments and Results 

 Setups The implementation of our models is based on Fairseq 5 . All the single models are carried out on 8 NVIDIA V100 GPUs each of which have 32 GB memory. We use the Adam optimizer with ? 1 = 0.9 and ? 2 = 0.98. The gradient accumulation is used due to the high GPU memory consumption. The batch size is set to 8192 toknes per GPU and the "update-freq" parameter in Fairseq is set to 8. Specifically, for LARGE settings, the batch size is 4096 and "update-freq" is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 

 Chinese ? English Table  2  shows the Chinese ? English translation results on validation set. We train multiple single models in each settings and report the best scores in Table  2 . The R2L method can significantly improve the baseline by 2.45 BLEU scores. It is surprising to find a gain of almost 5 BLEU improvement on test2019 dataset. After we boost the in-domain corpus, we can achieve 1 more BLEU on the DEEP model. This illustrates that the finetuning is very effective on the WMT2019 test set. In our experiments, the ensemble models consists of 5 single models: 1 HYBRID, 1 BIGDEEP, 3 LARGER models. As shown in the Table2, the ensemble models outperform the best single model by 1.06 BLEU score. We then apply transductive ensemble to LARGER models and finally the performance achieves 38.99. We also find that the single models that applied TE cannot bring further improvement to ensemble results. We do not apply re-ranking to this task, as we find that the improvement is insignificant. Our WMT 2020 Chinese ? English submission achieves a SacreBLEU score of 36.8 and chrF score of 0.649. 

 English ? Chinese Table  3  shows the English ? Chinese translation results on validation set. We also train multiple single models and report the best scores in the Table. After applying R2L method, we achieve 0.4 to 1 BLEU. We can observe that the improvement from finetuning is not as high as Chinese ? English tasks, where only more 1 BLEU is gained. We also find that the boosted finetuning is harmful in this task, thus we omit the results.  In this task, the ensemble models consists of 4 single models: 1 DEEP, 2 BIGDEEP, 1 LARGER models. As shown in Table  4 , the ensemble models outperform the best single model by 0.4 BLEU score. We then apply noisy channel re-reranking to ensemble results and finally achieve 45.9 BLEU on the development set. We apply a post-processing procedure. After translating the source-side, we normalize the English quotations appearing in the German translations to German-style quotations. We find this can improve the BLEU score on development set by 1.4 points. 

 Conclusion This paper presents the Tencent Translation systems for WMT2020 Chinese ? English news translation tasks. We investigate various deep architectures to build strong baseline systems. Then popular data augmentation methods such as backtranslation and R2L training are used to improve the baselines. We also prove that in-domain finetuning is very effective for news translation tasks especially on Chinese ? English task. Finally, we adopt the greed-based ensemble algorithm and propose an iterative transductive ensemble method for further improvement. It is worth mentioning a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation  (Wang et al., 2020b)  and chat translation  (Wang et al., 2020a)  tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Table 1 : 1 Hyper-parameters of different Transformer models used in our system. DEEP HYBRID BIGDEEP LARGER Encoder Layer 40 40 20 20 Decoder Layer 6 6 6 6 Attention Heads 8 8 16 16 Embedding Size 512 512 1024 1024 FFN Size 2048 2048 4096 8192 
