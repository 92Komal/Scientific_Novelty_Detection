title
Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation

abstract
In this paper, we describe the Bering Lab's submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-theart cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a humanlabeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentencelevel QE among all submissions.

Introduction Machine translation quality estimation (QE) is the task of estimating the quality of machine-translated (MT) output given just the source text at various granularity levels (word, sentence, and document)  (Fonseca et al., 2019) . Word-level QE can be divided into target-side and source-side tasks. On the target-side, the goal is to predict whether each word in the MT sentence is OK or BAD and whether there are missing words between each word. The goal on the source-side is to predict whether each word in the source sentence is correctly translated or not. On the other hand, sentence-level QE aims to predict the Human Translation Error Rate (HTER)  (Snover et al., 2006)  of the MT sentence, which measures the required amount of human editing to fix the MT sentence. In this paper, we propose a cross-lingual language model fine-tuning approach with a few additional parameters for word-level and sentence-level QE. As a pre-trained cross-lingual language model, we use XLM-RoBERTa (XLM-R)  (Conneau et al., 2019) , which shows state-of-the-art performance for a wide range of cross-lingual transfer tasks. In addition, since labeling the QE dataset requires a large amount of human labor, we generate and utilize a huge artificial QE dataset to improve the performance of our model. Our contributions are summarized as follows. ? We propose an XLM-R-based neural network architecture for the QE. Our model can be jointly trained for both word-level and sentence-level QE. ? We generate a huge artificial QE dataset based on a parallel corpus with OpenNMT-py  (Klein et al., 2017)  and the TER tool  (Snover et al., 2006) . ? We train our model in two phases. First, we train our model with a huge artificially generated dataset. Then, we fine-tune the model with a human-labeled dataset. In the experiment using the WMT 2020 English-German word-level QE test set, we achieve an MCC of 0.597 and 0.454 for the target-side and source-side, respectively, showing the best and second best performance among all submitted systems, respectively. For the sentence-level QE test set, we achieve a Pearson correlation of 0.723, which ranks second among all submissions. 

 Methodology We fine-tune XLM-R  (Conneau et al., 2019)  with a few additional parameters for sentence-level and word-level QE as described in Figure  1 . We train our model in two phases: 1) pre-training with a huge artificial dataset and 2) fine-tuning with a human-labeled dataset. 

 Input Representation We follow the tokenization and input representation methods of XLM-R. A source sentence and the corresponding MT sentence are tokenized with the same BPE model  (Sennrich et al., 2016)  that is trained based on shared vocabulary through languages. The input of the XLM-R model is a concatenated sequence of source tokens and MT tokens with special tokens (<s>, </s>) as follows: <s> src 1 , ..., src |S| </s> </s> mt 1 , ..., mt |T | </s> 

 Sentence-level QE For sentence-level QE, we use the final hidden vector h (0) ? R H of XML-R corresponding to the first input token (<s>) as the pooled representation. We use two linear layers with tanh activation to predict sentence-level HTER as follows: r = W s h (0) + b 0 (1) y sent = w T s tanh(r) + b 1 (2) where W s ? R H?H , w s ? R H , b 0 ? R H and b 1 ? R 1 are trainable parameters and H is the dimension of hidden states. The loss function L sent is the mean squared error between y sent and the true HTER ?sent . L sent = M SE(y sent , ?sent ) (3) 

 Word-level QE Word-level QE consists of two parts: the sourceside and target-side. On the source-side, we predict whether each token in the source sentence is translated correctly or not. On the target-side, we predict whether each token in the MT sentence is OK or BAD, in addition to whether there are missing words between each word. Source-side QE For source-side QE, we use the final hidden vector h (i) ? R H of XLM-R corresponding to each token in the source sentence. We introduce a linear layer and sigmoid activation to predict the probability that each token is BAD as follows: P (i) src = sigmoid(w T o h (i) ), i ? (1, .., |S|) (4) where w o ? R H is a trainable parameter and |S| is the number of tokens in the source sentence. The loss function L src is the binary cross entropy with an additional weight c for BAD examples as follows: Lsrc = 1 |S| |S| i=1 c? (i) src log P (i) src + (1 ? ?(i) src ) log(1 ? P (i) src ) (5) Target-side QE For the target-side QE, we use the final hidden vector d (i) ? R H of XLM-R corresponding to each token in the MT sentence, including the last </s> token. We introduce two separate binary classification layers to predict the probability that each token in MT sentence is BAD as follows: P (i) tgt word = sigmoid(w T w d (i) ), i ? (1, .., |T |) (6) and the probability that missing words exist before each token as follows: P (i) tgt gap = sigmoid(w T g d (i) ), i ? (1, .., |T | + 1) (7 ) where w w , w g ? R H are trainable parameters and |T | is the number of tokens in the machine translated sentence. The loss function for target-side QE L tgt is the sum of the binary cross entropy for word L tgt word and gap L tgt gap that are defined in the same manner as Eq. (  5 ). We repeat this process with different data splits to build huge artificial triplets. Finally, we use the TER tool 1  (Snover et al., 2006)  to annotate sentence-level HTER scores and word-level tags for the MT sentences. We do not annotate sourceside word-level tags in this work as it additionally requires word alignment between source sentences and MT sentences. L tgt = L tgt word + L tgt gap ( 

 Pre-training QE Model We first pre-train our QE model with only the artificial dataset. In the pretraining step, we jointly train sentence-level QE and target-side word-level QE on a single model. The loss function for the pre-training step L pre train is the sum of the loss for sentence-level QE and target-side word-level QE. L pre train = L sent + L tgt (9) Since our artificial dataset does not include sourceside word-level tags, we do not include the training objective for source-side word-level QE in the pretraining step. 1 http://www.cs.umd.edu/ ?snover/tercom/ 2.5 Fine-Tuning on Human-Labeled Dataset After the pre-training, we fine-tune the model with only a human-labeled dataset. Unlike the pre-training step, each QE model (sentence-level, source-side and target-side of word-level) is trained separately in the fine-tuning step. For the sentence-level and target-side of wordlevel QE models, all the parameters are initialized with trained weights from the pre-training step. However, since our pre-trained model does not include source-side word-level QE, we randomly initialize the weight of a source-side specific parameter (w o in Eq. (  4 )) and the rest of the parameters are initialized with weights from the pre-trained model. 

 Ensemble For the sentence-level ensemble, we average the HTER prediction of multiple models. For the wordlevel, we use the majority voting ensemble. 

 Experiments 

 Experimental Setup We evaluate our model with WMT 2020 English-German QE dataset.  2  For the sentence-level QE evaluation, we use the Pearson correlation for sentence-level HTER prediction. For the wordlevel QE evaluation, we use the Matthews correlation coefficient (MCC) for both the target-side and source-side. To generate an artificial dataset for pre-training ( ?2.4), we use the English-German parallel corpus provided by the shared task that consists of 23,440,059 pairs. We use 90% of the pairs to train a Transformer-based  (Vaswani et al., 2017)  NMT model with OpenNMT-py  (Klein et al., 2017)  and the rest of the pairs are used to generate artificial triplets. As a result of running the process five times with different data splits, we achieve 11,720,029 artificial triplets. For the fine-tuning, we use only the official QE dataset that consists of 7,000 triplets as a humanlabeled dataset. 

 Model Configuration We use XLM-R-Large  (Conneau et al., 2019)    learning rate of 5e-6, and a batch size of 8 for 2 epochs. Additionally, we use dropout  (Hinton et al., 2012)  with a rate of 0.1 for the regularization. For word-level QE, we use a weight of 3.0 on the BAD class (c). For fine-tuning with the human-labeled dataset, we follow the same hyperparameters as the pre-training step but for 5 epochs with early stopping. For the ensembling, we train five models with different random seeds. 

 Experimental Result Table  1  shows the result of ablation analysis for sentence-level and word-level QE on the dev set. We conduct an ablation analysis of three aspects: 1) without an ensemble, 2) without pre-training with artificially generated dataset, 3) without fine-tuning with human-labeled dataset. When our model is trained with only the human-labeled dataset, Pearson correlation, target-side MCC and sourceside MCC drop by 0.12, 0.11, and 0.09, respectively. This result demonstrates that pre-training with the artificial dataset significantly improves performance for both sentence-level and word-level QE. When our model is trained with only the artificial dataset, Pearson correlation and target side MCC drop by 0.29 and 0.21, respectively. This result shows that fine-tuning with a human-labeled dataset is essential for our performance. Table  2  and 3 shows the official results for sentence-level and word-level QE for the WMT 2020 QE shared task. For both sentence-level and word-level QE, our systems significantly outperformed the official baseline systems  (Kepler et al., 2019) . Moreover, we achieve the best result on the target side of word-level QE among all submitted systems. We also achieve the second best results on the source side of word-level QE and sentence-level QE. 

 Conclusion This paper describes Bering Lab's submissions to the WMT 2020 QE shared task. We propose a twophase cross-lingual language model fine-tuning approach for word-level and sentence-level translation quality estimation. The experimental results show that pre-training with an artificially generated dataset significantly improves performance for both tasks. Overall, our submitted systems achieve the best result on the target side of word-level QE and the second best results on the source side of word-level QE and the sentence-level QE among all submissions. Figure 1 : 1 Figure 1: The XLM-R-based neural network architecture for word-level and sentence-level QE. 
