title
UoS Participation in the WMT20 Translation of Biomedical Abstracts

abstract
This paper describes the machine translation systems developed by the University of Sheffield (UoS) team for the biomedical translation shared task of WMT20. Our system is based on a Transformer model with TensorFlow Model Garden toolkit. We participated in ten translation directions for the English/Spanish, English/Portuguese, English/Russian, English/Italian, and English/French language pairs. To create our training data, we concatenated several parallel corpora, both from in-domain and out-ofdomain sources.

Introduction In this paper, we present the system developed by the University of Sheffield for the Biomedical Translation shared task in the Fifth Conference on Machine Translation (WMT20), which consists in translating scientific texts from the biological and health domain. Our participation in this task considered the English/Portuguese, English/Spanish, English/Russian, English/Italian, and English/French language pairs with translations in both directions. For that matter, we developed a machine translation (MT) system based on neural machine translation (NMT), using Google's TensorFlow Model Garden. 1 

 Related Works Previous participation in biomedical translation tasks include the works of  Costa-Juss? et al. (2016)  which employed Moses Statistic Machine Translation (SMT) to perform automatic translation integrated with a neural character-based recurrent neural network for model re-ranking and bilingual word embeddings for out of vocabulary 1 https://github.com/tensorflow/models (OOV) resolution. Given the 1000-best list of SMT translations, the RNN performs a re-scoring and selects the translation with the highest score. The OOV resolution module infers the word in the target language based on the bilingual word embedding trained on large monolingual corpora. Their reported results show that both approaches can improve BLEU scores, with the best results given by the combination of OOV resolution and RNN re-ranking. Similarly,  Ive et al. (   2016 ) also used the n-best output from Moses as input to a reranking model, which is based on a neural network that can handle vocabularies of arbitrary size. More recently, Tubay and Costa-Juss? (2018) employed multi-source language translation using romance languages to translate from Spanish, French, and Portuguese to English. They used data from SciELO and Medline abstracts to train a Transformer model with individual languages to English and also with all languages concatenated to English. In the last two WMT biomedical translation challenges (WMT18 and WMT19)  (Neves et al., 2018; Bawden et al., 2019) , the submissions that achieved the best BLEU scores for the ES/EN and PT/EN, in both directions  (Soares and Becker, 2018; Tubay and Costa-Juss?, 2018; Carrino et al., 2019; Saunders et al., 2019; , used the Transformer architecture with enhancements such as handling of terminology during tokenization  (Carrino et al., 2019) , multi-domain inference  (Saunders et al., 2019)  and exploitation of additional linguistic resources  (Soares and Becker, 2018; . 

 Resources In this section, we describe the language resources used to train both models. 

 Corpora We used both in-domain and general domain corpora to train our systems. For general domain data, we used the ParaPat patent corpus  (Soares et al., 2020) , which is available for several languages, included the ones we explored in our systems. As for in-domain data, we included several different corpora: ? The corpus of full-text scientific articles from SciELO  (Soares et al., 2018a) , which includes articles from several scientific domains in the desired language pairs, but predominantly from biomedical and health areas. ? A subset of the UFAL medical corpus 2 , containing the Medical Web Crawl data for the English/Spanish language pair. ? The EMEA corpus  (Tiedemann, 2012) , consisting of documents from the European Medicines Agency. ? A corpus of theses and dissertations abstracts (BDTD)  (Soares et al., 2018b)  from CAPES, a Brazilian governmental agency responsible for overseeing post-graduate courses. This corpus contains data only for the English/Portuguese language pair. ? A corpus from Virtual Health Library 3 (BVS), containing also parallel sentences for the language pairs explored in our systems. ? A corpus from SciELO  (Neves et al., 2016) , containing also parallel sentences from abstracts in English/Portuguese, English/Spanish, and English/French. A new crawl of MEDLINE using the Ebot provided by the National Library of Medicine.  4  Table  1  depicts the original number of parallel segments according to each corpora source. In Section 4.1, we detail the pre-processing steps performed on the data to comply with the task evaluation. 

 Experimental Settings In this section, we detail the pre-processing steps employed as well as the architecture of the Transformer. 

 Pre-processing As detailed in the description of the biomedical translation task, the evaluation is based on texts extracted from MEDLINE. Since two of our corpora, the one comprised of full-text articles from SciELO and the new crawl from PubMed, may contain a considerable overlap with MEDLINE data, we decided to employ a filtering step in order to avoid including such data. The first step in our filter was to download the parallel data from PubMed articles in Russian, French, and Italian. For that matter, we used the Ebot utility 5 provided by NLM using the queries ITA[la], FRE[la], and RUS[la], retrieving all results available. Once downloaded, we performed sentence alignment using LF-Aligner 6 . To perform the filtering, we decided to use simple case insensitive string matching with grep supplying the option -xvf and the test set in English. 

 NMT System As for the NMT system, we employed the official Google's implementation of the Transformer architecture  (Vaswani et al., 2017)  to train ten MT systems for the five language pairs. Tokenization was performed using the WordPiece unsupervised tokenizer with a vocabulary size of 32,000 on the initial training data, with a shared vocabulary between source and target. For systems where the target language was English, back-translation was used with a number of sentences equals to the initial training system where English was the source. For the Spanish/English language pair, the system used to produce the artificial parallel sentences was the one developed by , while for the other language pairs we used the same systems trained by our team. The parameters of our network for all language pairs excluding English/Portuguese are as follows. To train our systems, we used 5 Tensor Processing Units (TPUs) v3, with a number of 250,000 steps (for all systems with exception of Russian, which was trained with fewer steps). The models with the best perplexity value were chosen as final models. For the English/Russian language pair, incremental training was performed, since the size of the in-domain dataset was reduced. For such, we first trained our system in the out-of-domain data from patents for 100,000 steps. We then proceeded with additional training for 25,000 steps with in-domain data. 

 Results We now detail the results achieved by our Transformer systems on the official test data used in the shared task regarding automatic evaluation. Table 2 shows the BLEU scores  (Papineni et al., 2002)  for our systems for the 10 language pairs we participated. For the Spanish and Portuguese language pairs we achieve high competitive results. For ES/EN, the best system (NLE) achieved BLEU of 0.5075, while the second best achieved BLEU of 0.4662 (TRAMECAT), very close to our result of 0.4624. For the opposite direction, EN/ES, the best system (UCAM) achieved 0. the second best (Elhuyar NLP) 0.4498, while our system scored 0.4493. For the Portuguese language, in both directions we achieved the best scores, with an EN/PT BLEU of 0.4744 and PT/EN of 0.5334. The second team in both languages (UNICAMP DL) achieved scores of 0.4095 and 0.4988, respectively. As for the Russian, French, and Italian languages, our scores were not as competitive as the best systems, with the exception of FR/EN, which we stood as 3 out of 5 teams. After carefully checking our training data, we found encoding issues with the different gathered data for those languages, especially with the encoding and tokenization of words containing apostrophes in French and Italian, as well as the Cyrillic Kha. 

 Conclusions We presented the University of Sheffield (UoS) machine translation system for the biomedical translation shared task in WMT20. For our submission, we trained ten Transformers NMT systems, employing different corpora for each language pair. In addition, for systems with English as target language, back-translation was used, and for the Russian language, incremental training from Patent abstracts was used. For model building, we included several corpora from biomedical and health domain, and from out-of-domain data that we considered to have similar textual structure, such as books and patents. Prior training, we also pre-processed our corpora to ensure that we did not include any sentence from the released test set, which could produce biased models. Regarding future work, we are planning on optimizing our systems by performing pre-selection of out-of-domain data, aiming at selecting only the most similar sentences to the in-domain data. In addition, we plan to explore the potential use of domain-specific decoding, as proposed in  Saunders et al. (2019) . Table 2 : 2 4662, Official BLEU scores for the language pairs we submitted systems. These scores are evaluated on the "OK" aligned sentences. Language Pair BLEU EN/PT 0.4744 PT/EN 0.5334 EN/ES 0.4493 ES/EN 0.4624 EN/FR 0.3049 FR/EN 0.3514 EN/RU 0.2573 RU/EN 0.2936 EN/IT 0.2073 IT/EN 0.2276 

			 https://ufal.mff.cuni.cz/ufal_ medical_corpus 3 http://bvsalud.org/ 4 https://www.ncbi.nlm.nih.gov/Class/ PowerTools/eutils/ebot/ebot.cgi 

			 https://www.ncbi.nlm.nih.gov/Class/ PowerTools/eutils/ebot/ebot.cgi 6 https://sourceforge.net/projects/ aligner/
