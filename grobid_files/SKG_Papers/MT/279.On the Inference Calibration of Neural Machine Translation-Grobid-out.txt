title
On the Inference Calibration of Neural Machine Translation

abstract
Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the groundtruth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models. Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance. 1 * Work was done when Shuo Wang was interning at Tencent AI Lab under the Rhino-Bird Elite Training Program.

Introduction Calibration requires that the probability a model assigns to a prediction (i.e., confidence) equals to the correctness measure of the prediction (i.e., accuracy). Calibrated models are important in userfacing applications such as natural language processing  (Nguyen and O'Connor, 2015)  and speech recognition  (Yu et al., 2011) , in which one needs to assess the confidence of a prediction. For example, in computer-assisted translation, a calibrated machine translation model is able to tell a user when the model's predictions are likely to be incorrect, which is helpful for the user to correct errors. Figure  1 : Reliability diagrams in training and inference for the WMT14 En-De task. "Gap" denotes the difference between confidence and accuracy. Smaller gaps denotes better calibrated outputs. We find that the average gaps between confidence and accuracy are much larger in inference than in training  (i.e., 15.83 > 1.39) . 

 EnDe Dev The study of calibration on classification tasks has a long history, from statistical machine learning  (Platt et al., 1999; Niculescu-Mizil and Caruana, 2005)  to deep learning  (Guo et al., 2017) . However, calibration on structured generation tasks such as neural machine translation (NMT) has not been well studied. Recently,  M?ller et al. (2019)  and  Kumar and Sarawagi (2019)  studied the calibration of NMT in the training setting, and found that NMT trained with label smoothing  (Szegedy et al., 2016)  is well-calibrated. We believe that this setting would cover up a central problem of NMT, the exposure bias  (Ranzato et al., 2015)  -the training-inference discrepancy caused by teacher forcing in the training of auto-regressive models. In response to this problem, this work focuses on the calibration of NMT in inference, which can better reflect the generative capacity of NMT models. To this end, we use translation error rate (TER)  (Snover et al., 2006)  to automatically annotate the correctness of generated tokens, which makes it feasible to evaluate calibration in infer-ence. Experimental results on several datasets across language pairs show that even trained with label smoothing, NMT models still suffer from miscalibration errors in inference. Figure  1  shows an example. While modern neural networks on classification tasks have been found to be miscalibrated in the direction of over-estimation (i.e., confidence > accuracy)  (Guo et al., 2017) , NMT models are also under-estimated (i.e., confidence < accuracy) on low-confidence predictions. In addition, we found that miscalibrated predictions correlate well with the translation errors in inference. Specifically, the over-estimated predictions correlate more with over-translation and mis-translation errors, while the under-estimated predictions correlate more with under-translation errors. This demonstrates the necessity of studying inference calibration for NMT. By investigating the linguistic properties of miscalibrated tokens in NMT outputs, we have several interesting findings: ? Frequency: Low-frequency tokens generally suffer from under-estimation. Moreover, lowfrequency tokens contribute more to overestimation than high-frequency tokens, especially on large-scale data. ? Position: Over-estimation does not have a bias on the position of generated tokens, while under-estimation occurs more in the left part of a generated sentence than in the right part. ? Fertility: Predicted tokens that align to more than one source token ("fertility?2") suffer more from under-estimation, while tokens with fertility < 1 suffer from over-estimation. ? Syntactic Roles: Content tokens are more likely to suffer from miscalibration than content-free tokens. Specifically, verbs are more likely to suffer from over-estimation than under-estimation. ? Word Granularity: sub-words suffer more from both over-estimation and underestimation, while full words are less likely to be miscalibrated. Inspired by the finding that miscalibration on classification tasks is closely related to lack of regularization and increased model size  (Guo et al., 2017) , we revisit these techniques on the NMT (i.e., structured generation) task: ? Regularization Techniques: We investigate label smoothing and dropout  (Hinton et al., 2012) , which directly affect the confidence estimation. Both label smoothing and dropout improve the inference calibration by alleviating the over-estimation. Label smoothing is the key for well-calibration, which is essential for maintaining translation performance for inference in large search space. Inspired by this finding, we propose a novel graduated label smoothing approach, in which the smoothing penalty for high-confidence predictions is higher than that for low-confidence predictions. The graduated label smoothing can improve translation performance by alleviating inference miscalibration. ? Model Size: Increasing model size consistently improves translation performance at the cost of negatively affecting inference calibration. The problem can be alleviated by increasing the capacity of encoder only, which maintains the inference calibration and obtains a further improvement of translation performance in large search space. To summarize, the main contributions of our work are listed as follows: ? We demonstrate the necessity of studying inference calibration for NMT, which can serve as useful indicators of translation errors. ? We reveal certain linguistic properties of miscalibrated predictions in NMT, which provides potentially useful information for the design of training procedures. ? We revisit recent advances in architectures and regularization techniques, and provide variants that can boost translation performance by improving inference calibration. 

 Related Work Calibration on Classification Calibration on classification tasks has been studied for a long history in the statistics literature, including Platt scaling  (Platt et al., 1999) , isotonic regression  (Niculescu-Mizil and Caruana, 2005)  and many other methods for non-binary classification  (Zadrozny and Elkan, 2002; Menon et al., 2012; Zhong and Kwok, 2013)  Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures  (Kuleshov and Liang, 2015) .  Nguyen and O'Connor (2015)  verified the finding of  Niculescu-Mizil and Caruana (2005)  in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction  (Ott et al., 2018; ,  Kumar and Sarawagi (2019)  studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated.  M?ller et al. (2019)  investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models  (Vaswani et al., 2017) . 3 Definitions of Calibration 

 Neural Machine Translation Training In machine translation task, an NMT model F : x ? y maximizes the probability of a target sequence y = {y 1 , ..., y T } given a source sentence x = {x 1 , ..., x S }: P (y|x; ?) = T t=1 P (y t |y <t , x; ?), (1) where ? is a set of model parameters and y <t is a partial translation. At each time step, the model generates an output token of the highest probability based on the source sentence x and the partial translation y <t . The training objective is to minimize the negative log-likelihood loss on the training corpus. Inference NMT models are trained on the ground-truth data distribution (teaching forcing), while in inference the models generate target tokens based on previous model predictions, which can be erroneous. The training-inference discrepancy caused by teacher forcing in maximum likelihood estimation training (Equation  1 ) is often referred to as exposure bias  (Ranzato et al., 2015) . In this work, we aim to investigate the calibration of NMT in inference, which we believe can better reflect the generation capacity of NMT models. 

 Calibration of NMT Calibration requires that the probability a model assigns to a prediction (i.e., confidence) equals to the true correctness measure of the prediction (i.e., accuracy). Modern neural networks have been found to be miscalibrated in the direction of overestimation  (Guo et al., 2017) . In this study, we revisit the calibration problem in NMT. If an NMT model is well-calibrated, the gap between the confidence of the generated tokens and the accuracy of them will be small. 2 Expected Calibration Error (ECE) ECE is a commonly-used metric to evaluate the miscalibration, which measures the difference in expectation between confidence and accuracy  (Naeini et al., 2015) . Specifically, ECE partitions predictions into M bins {B 1 , . . . , B M } according to their confidence and takes a weighted average of the bin's accuracy/confidence difference: ECE = M m=1 |B m | N acc(B m ) ? conf (B m ) , ( 2 ) where N is the number of prediction samples and |B m | is the number of samples in the m-th bin. 

 EnJp & ZhEn Dev EnJp & ZhEn Dev  

 ECE in Training and Inference In the case of considering just the topmost token in structured prediction tasks (e.g., machine translation), the prediction is ? = arg max y?V P (y) with P (?) as confidence. The accuracy C(?) ? {1, 0} denotes whether the prediction ? is correct. In training, the correctness of the prediction ? is calculated as whether ? matches the ground-truth token y n : C(?) ? {1, 0}. However, in inference it is not straightforward to measure the accuracy of ?, since it requires to build an alignment between the generated tokens and the ground-truth tokens. To this end, we turn to the metric of Translation Error Rate (TER)  (Snover et al., 2006) , which measures the number of edits required to change a model output into the ground-truth sequence. Specifically, it assigns a label l ? {C, S, I} to each generated token. Figure  2  shows an example of TER labels of each generated token with respect to the reference. As a side product, TER annotations provide the information of translation errors. While TER only labels the mis-translation ("S") and over-translation ("I") errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label "D" from the ground-truth sequence to the generated sequence. 

 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 English-German (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE)  (Sennrich et al., 2016)  with 32K merge operations for all the three language pairs. We used BLEU  (Papineni et al., 2001)  to evaluate the NMT models. We used the TER toolkit  (Snover et al., 2006)  to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer  (Vaswani et al., 2017) . We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with ? 1 = 0.9, ? 2 = 0.98 and = 10 ?9 . We used the same warmup strategy for learning rate as  Vaswani et al. (2017)  with warmup steps = 4, 000. 

 Observing Miscalibration Reliability diagrams are a visual representation of model calibration, which plot accuracy as a function of confidence  (Niculescu-Mizil and Caruana, 2005) . Specifically, it partitions the output tokens into several bins according to their prediction confidence, and calculate the average confidence and accuracy of each bin. Figure  1  shows the reliability diagrams of both training and inference on En-De and Figure  3  shows those on En-Jp and Zh-En. Results are reported on the validation sets. NMT still suffers from miscalibration. The difference between training and inference ECEs is that when estimating training ECE, NMT models are fed with ground-truth prefixes  (Kumar and Sarawagi, 2019; M?ller et al., 2019) , while for inference ECE, NMT models are fed with previous model predictions. As seen, the training ECE is very small, indicating that NMT models are wellcalibrated in training. This is consistent with the findings of  Kumar and Sarawagi (2019) ;  M?ller et al. (2019) . However, the inference ECE is much higher, suggesting that NMT models still suffer from miscalibration in inference. NMT models are miscalibrated in directions of both over-and under-estimation. Modern neural networks have been found to be miscalibrated on classification tasks in the direction of overestimation  (Guo et al., 2017) . In contrast, NMT models also suffer from under-estimation problems. The under-estimation problem is more serious on En-Jp than on Zh-En, which we attribute to the smaller size of the training data of the En-Jp task. 

 Correlation with Translation Errors We investigated the calibration error of tokens with different TER labels. As the development set is small, to make the results more convincing, we sampled 100K sentences from the training set as a held-out set and retrained the NMT model on the remained training set excluding the held-out set. All results in this section is reported by the retrained model. We firstly compute the gap between the confidence and the accuracy of each token in each confidence bin on the held-out set. Tokens in bins whose gaps are less than a threshold are labeled as well-calibrated, otherwise they are labeled as miscalibrated. We use the inference ECE estimated on the development set as the threshold for each language pair respectively. Miscalibrated tokens can be divided into two categories: over-estimation and under-estimation. As shown in Table  1 , correct translations (i.e., "C") have higher correlations to well-calibrated predictions and erroneous translations (i.e., "S", "I", and "D") correlate more to miscalibrated predictions. This finding is more obvious when NMT models are trained on larger data (e.g., Zh-En). Table  2  lists the correlation between different translation errors and different kinds of miscalibration. We find that over-estimated predictions are closely correlated with over-translation and mis- translation errors, while the under-estimated predictions correlate well with under-translation errors. This finding demonstrates the necessity of studying inference calibration for NMT. 

 Linguistic Properties of Miscalibration In this section, we investigate the linguistic properties of miscalibrated tokens in NMT outputs. We explore the following five types of properties: frequency, position, fertility, syntactic roles, and word granularity. Frequency is generally related to miscalibration; position, fertility, and word granularity are three factors associated with structured prediction; syntactic roles or linguistic roles may vary across language pairs. The results in this section are reported on the held-out set by the retrained model. 

 Relative Change We use the relative change of the proportion of a certain category of tokens to quantify to what extent they suffer from the under/over-estimation. For instance, in the Zh-En task, high-frequency tokens account for 87.6% on the whole held-out set, and among over-estimated tokens, high-frequency tokens account for 77.3%, thus for over-estimation the relative change of highfrequency tokens is (77.3-87.6)/87.6=-11.76% in Zh-En. Accordingly, the value of the red rectangle of Zh-En is -11.76% in Figure  4a . Positive relative change denotes that a certain type of linguistic property accounts more in miscalibrated predictions than in all the predictions, suggesting this type of linguistic property suffers from the miscalibration problem. Similarly, negative relative change suggests that a certainty type of linguistic property is less likely to be impaired by the miscalibration problem. 

 Frequency We divide tokens into three categories based on their frequency, including High: the most 3,000 frequent tokens; Medium: the most 3,001-12,000 frequent tokens; Low: the other tokens. Low-frequency tokens are miscalibrated in the direction of under-estimation. Low-frequency tokens contribute more to overestimation. As shown in Figure  4a , the relative changes of low-and medium-frequency tokens are positive while those of high-frequency tokens are negative, regarding over-estimation. High-frequency tokens are less likely to be miscalibrated. We find the relative changes of high frequency tokens are negative across the three language pairs. The imbalance in token frequency plays an important role in the calibration of NMT. 

 Position In structured prediction, different positions may behave differently regarding miscalibration. Thus we divide all the tokens equally into three categories: Left: tokens on the left third; Middle: tokens on the middle third; Right: tokens on the right third. Figure  5  depicts the relative changes of these three positions. Since Japanese is a head-final language  (Wu et al., 2018) , we also include the results of Japanese-English ("Jp-En") for comparison. Under-estimation occurs more in the left part. This phenomenon is more obvious in left-branching languages (e.g., Japanese) than in right-branching languages (e.g., English and German), confirming that characteristics of a language play an important role in machine translation  (Wu et al., 2018) . Fertility indicates how many source tokens a target token is aligned to, which is highly related to inference in NMT. We use Fast Align  (Dyer et al., 2013)  to extract bilingual alignment. We distinguish between four categories regarding fertility: "? 2": target tokens that are aligned to more than one source tokens; "1": target tokens that are aligned to a single source token; "(0, 1)": target tokens that are aligned to a single source token along with other target tokens; "0": target tokens that are not aligned to any source token. Figure  6  plots the results. 

 Fertility Tokens aligning to less than one source token suffer from over-estimation. The extent grows with the data size. In addition, these tokens ("(0, 1)") are less likely to suffer from under-estimation. Tokens aligning to more than one source token suffer more from under-estimation. The relative change of fertility>=2 is much larger than that of the other types of fertility. Meanwhile, the null-aligned target tokens (fertility=0) also suffer from under-estimation problem instead of overestimation problem on the large-scale Zh-En data. 

 Syntactic Roles In this experiment, we investigate the syntactic roles of miscalibrated tokens.  3  Words in English and German sentences are labeled by Stanford POS tagger 4 , and Japanese sentences are labeled by Kytea 5 . We distinguish between the following POS tags: noun, verb, adjective, preposition, determiner, punctuation, and the others. Noun, verb, and adjective belong to content tokens. Preposition, determiner, punctuation and the others belong to content-free tokens. Content tokens are more likely to suffer from miscalibration. From Figure  7  we find that the most relative changes of content tokens (i.e., "Noun", "Verb" and "Adj") are positive, while most of the relative changes of the content-free tokens (i.e., "Prep.", "Dete.", "Punc.", "Others") are negative. Among content tokens, the verbs ("Verb") face the over-estimation problem instead of the underestimation problem. Surprisingly, the adjectives ("Adj") suffer from under-estimation problem on large data (e.g., En-De and Zh-En). 

 Word Granularity BPE segmentation is the preliminary step for current NMT systems, which may segment some words into sub-words. To explore the effect of word granularity on the miscalibration of NMT models, we divide the tokens after BPE segmentation into two categories: Sub-Words that are divided into word fragments by BPE (e.g., with "@@"), and Full Words that are not divided by BPE. Figure  8  depicts the results. Sub-words suffer more from miscalibration, while full words are less likely to be miscalibrated. The relative changes of sub-words are all positive for both over-and under-estimation, while those of full words are all negative.  Sennrich et al. (2016)  showed that BPE addresses the open-vocabulary translation by encoding rare and unknown words as sequences of sub-word units. Our results confirm their claim: the behaviors of sub-words and full words correlate well with those of low-and high-frequency tokens respectively. 6 Revisiting Advances in Architecture and Regularization  Guo et al. (2017)  have revealed that the miscalibration on classification tasks is closely related to lack of regularization and increased model size. In this section we check whether the conclusion holds on the inference of NMT models, which belong to a family of structured generation.  None-Constant-Graduate One criticism of NMT inference is that the translation performance inversely decreases with the increase of search space  (Tu et al., 2017) . Quite recently,  Kumar and Sarawagi (2019)  claimed that this problem can be attributed to miscalibration. Accordingly, we also report results on large beam size and find that reducing miscalibration can improve the NMT performance in large beam size. 

 Label 

 Regularization Techniques We revisit two important regularization techniques that directly affect confidence estimation: ? Label Smoothing  (Szegedy et al., 2016) : distributing a certain percentage of confidence from the ground truth label to other labels uniformly in training. ? Dropout  (Hinton et al., 2012) : randomly omitting a certain percentage of the neural networks on each training case, which has been shown effective to prevent the over-fitting problem for large neural networks. For comparison, we disable label smoothing or dropout to retrain the model on the whole training set. The results are shown in Table  3 . We find that label smoothing improves the performance by greatly reducing the over-estimation, at the cost of increasing the percentage of under-estimation error. Dropout alleviates the over-estimation problem, and does not aggravate under-estimation. Although label smoothing only marginally improves performance on top of dropout, it is essential for maintaining the translation performance in larger search space (i.e., Beam Size = 100). As seen from Table  3 , reducing ECE can only lead to marginal BLEU gains. We attribute this phenomenon to the fact that ECE is another metric to evaluate NMT models, which is potentially complementary to BLEU. Accordingly, ECE is not necessarily strictly negatively related to BLEU. Graduated Label Smoothing Inspired by this finding, we propose a novel graduated label smoothing approach, in which the smoothing penalty for high-confidence predictions is bigger than that for low-confidence predictions. We firstly use the model trained by vanilla label smoothing to estimate the confidence of each token in the training set, then we set the smoothing penalty to 0.3 for tokens with confidence above 0.7, 0.0 for tokens with confidence below 0.3, and 0.1 for the remaining tokens. As shown in  viating inference miscalibration, and the improvement is more significant in large beam size. Figure  9  shows the reliability diagrams of different label smoothing strategies. The graduated label smoothing can effectively calibrate the predictions with 0.4 ? confidence ? 0.8, while is less effective for low-(i.e., < 0.4) and high-confidence (i.e., > 0.8) predictions. We believe that the design of more advanced techniques to solve this problem is a worthwhile future direction of research. 

 Increased Model Size The model size of NMT models has increased significantly recently  (Bahdanau et al., 2015; Vaswani et al., 2017; . We evaluated the inference calibration of models with different sizes. We increase model size in the following two ways: ? Deeper model: both the encoder and the decoder are deepened to 24 layers; ? Wider model: the hidden size of the encoder and the decoder is widened to 1024. The BLEU score and inference ECE of different models are shown in Table  4 . Increasing model size negatively affects inference calibration. We find that increasing both the encoder and the decoder increases the inference calibration error despite increasing the BLEU, confirming the finding of  Guo et al. (2017)  that increased model size is closely related to model miscalibration. This leads to a performance drop in a larger search space (i.e., Beam Size = 100). Only enlarging the encoder improves translation quality while maintaining inference calibration. As the decoder is more directly related to the generation, it is more likely to result in miscalibration. In order to maintain the performance improvement and do not aggravate over-estimation, we propose to only increase the size of encoder and keep the decoder unchanged. Results in Table  4  indicate that only enlarging the encoder can achieve better performance with fewer parameters compared to enlarging both the encoder and the decoder. In a larger search space (i.e., Beam Size = 100), models with high inference ECE will generate worse translations while models with low inference ECE can achieve improved translation performance. 

 Conclusion Although NMT models are well-calibrated in training, we observe that they still suffer from miscalibration during inference because of the discrepancy between training and inference. Through a series of in-depth analyses, we report several interesting findings which may help to analyze, understand and improve NMT models. We revisit recent advances and find that label smoothing and dropout play key roles in calibrating modern NMT models. We further propose graduated label smoothing that can reduce the inference calibration error effectively. Finally, we find that increasing model size can negatively affect the calibration of NMT models and this can be alleviated by only enlarging the encoder. As well-calibrated confidence estimation is more likely to establish trustworthiness with users, we plan to apply our work to interactive machine translation scenarios in the future. Figure 3 : 3 Figure 3: Reliability diagrams on (a) En-Jp and (b) Zh-En datasets. Left: training, right: inference. 
