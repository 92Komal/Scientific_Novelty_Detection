title
Entity Projection via Machine-Translation for Cross-Lingual NER

abstract
Although over 100 languages are supported by strong off-the-shelf machine translation systems, only a subset of them possess large annotated corpora for named entity recognition. Motivated by this fact, we leverage machine translation to improve annotationprojection approaches to cross-lingual named entity recognition. We propose a system that improves over prior entity-projection methods by: (a) leveraging machine translation systems twice: first for translating sentences and subsequently for translating entities; (b) matching entities based on orthographic and phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Our approach improves upon current state-of-the-art methods for cross-lingual named entity recognition on 5 diverse languages by an average of 4.1 points. Further, our method achieves state-of-the-art F 1 scores for Armenian, outperforming even a monolingual model trained on Armenian source data. 1

Introduction While machine learning methods for various Natural Language Processing (NLP) tasks have progressed rapidly, the benefits accrue disproportionately among languages endowed with large annotated corpora. Owing to the dependence of stateof-the-art deep learning approaches on massive amounts of data, creating suitable datasets can be prohibitively expensive. This asymmetry between resource-rich and relatively under-resourced languages has inspired work on cross-lingual approaches that leverage annotated datasets from the former to build strong models for the latter. This paper focuses on cross-lingual approaches to Named Entity Recognition (NER), owing to NER's importance as a core component in information retrieval and question answering systems. Specifically, we focus on medium-resource languages. We define these to be languages for which although annotated NER corpora do not exist, offthe-shelf Machine Translation (MT) systems, such as Google Translate 2 , do. We are motivated by the fact that although there are fewer than 50 languages for which large NER datasets (greater than 200k tokens) with gold annotations are publicly available 3 , many more languages are supported by good-quality MT  (Wu et al., 2016) . Google Translate alone supports 103 languages 4 , many of which have either no, or only small, NER datasets. We address the setting where annotated corpora exist in the source (resource-rich) language-English in our experiments-but for the target (medium-resource) language, we can only afford to label a small validation set. We tackle this problem by first creating an unlabeled dataset in the target language by translating each sentence in the source dataset to the target language. For MT, we use Google Translate, motivated by its large coverage. Next, we annotate this dataset via entity projection-first aligning every entity in a source sentence with its counterpart in the corresponding target sentence (entity alignment) and then projecting the tags from source to target in the aligned entity pairs (tag projection). One consequence of relying on MT as opposed to word-by-word or phrase-byphrase translation is that the entity projection step can be difficult, owing to the frequency with which original sentences and their translated counterparts are not word-for-word aligned. Our proposed solution to this problem consists of (a) leveraging MT again for translating entities; (b) matching entities based on orthographic and  phonetic similarity; and (c) identifying matches based on distributional statistics derived from the dataset. Importantly, while our method depends on several matching heuristics, these techniques are remarkably portable across target languages, requiring the tuning of only two hyperparameters. Our method achieves state-of-the art F 1 scores for cross-lingual NER for Spanish (+1.1 points), German (+1.4 points), and Chinese (+5 points) and beats state-of-the-art baselines on Hindi (+2.1 points) and Tamil (+5 points). Further, it achieves state-of-the-art F 1 scores for Armenian, a mediumresource language, beating a monolingual model trained on Armenian source data by 0.4 points. 

 Related Work Cross-lingual approaches have been applied to many NLP tasks, including part-of-speech tagging  (Yarowsky et al., 2001; Xi and Hwa, 2005; Das and Petrov, 2011; T?ckstr?m et al., 2013) , parsing  Zeman and Resnik, 2008; Smith and Eisner, 2009; Ganchev et al., 2009) , and semantic role labeling  (Tonelli and Pianta, 2008; Pad? and Lapata, 2009; Titov, 2013, 2014) . Prior cross-lingual NLP papers cleave roughly into two distinct approaches: direct model transfer and annotation projection. 

 Direct model transfer These approaches apply models trained on the source language absent modification (to the model) to data from the target language by exploiting a shared representation for the two languages  (T?ckstr?m et al., 2012; Bharadwaj et al., 2016; Chaudhary et al., 2018; Kozhevnikov and Titov, 2014; Ni et al., 2017) . However, direct model transfer techniques face a problem when applied to markedly dissimilar languages: they lack of lexicalized (especially character-based) features, which are known to have predictive power for tasks such as NER.  Xie et al. (2018)  provide evidence for this in the cross-lingual setting, comparing otherwise similar annotation projection approaches that differ in their use of lexicalized features. 

 Annotation projection These approaches to cross-lingual NLP train a model in the target language. This requires first projecting annotations from the source data to the (unlabeled) target data. Many approaches in this category rely upon parallel corpora  (Yarowsky et al., 2001; Zeman and Resnik, 2008; Ehrmann et al., 2011; Fu et al., 2011; Ni et al., 2017) , first annotating the source data using a trained model and then projecting the annotations. Only a few works explore the use of MT to first translate a gold annotated corpus to obtain a synthetic parallel corpus and then project annotations  (Tiedemann et al., 2014) .  Shah et al. (2010)  go in the opposite direction, translating target to source using Google Translate, annotating the translated source sentences using a trained NER system and then projecting annotations back. When projecting annotations, one encounters the problem of word alignment. Most of the existing works  (Yarowsky et al., 2001; Shah et al., 2010; Ni et al., 2017)  rely upon unsupervised alignment models from statistical MT literature, such as IBM Models 1-6  (Brown et al., 1993; Och and Ney, 2003) . Other works focus on low-resource settings  (Mayhew et al., 2017; Xie et al., 2018)  perform translation word-by-word or phrase-by-phrase, and thus do not need to perform word alignment. Several papers explore heuristics such as using Wikipedia links across languages to align entities  (Richman and Schone, 2008; Nothman et al., 2013; Al-Rfou et al., 2015) , matching tokens based on their surface forms and transliterations either in an unsupervised manner  (Samy et al., 2005; Ehrmann et al., 2011)  or as features in a supervised model trained on a small seed dataset  (Feng et al., 2004) . Many of these papers often rely on language-specific features  (Feng et al., 2004)  and evaluate their alignment methods on only a few languages. To our knowledge, few works effectively use translation for annotation projection for NER, especially for medium-resource languages for which strong MT systems exist. Motivated by this research gap, we explore the use of MT systems for translating the dataset and for annotation projection and thus do not rely on parallel corpora. However, we demonstrate the efficacy of our projection method in all three settings: (a) translation from source to target, (b) using parallel corpora and (c) translation from target to source. 

 The Translate-Match-Project Method In our formulation, we are given an annotated NER corpus in the source language: D S A = {(x Si , y Si ) : i = 1, 2, ..., N }, where x Si = (x Si 1 , ..., x Si L Si ) is the ith source sentence with L Si tokens and y Si = (y Si 1 , ..., y Si L Si ) are the NER tags from a fixed tag set. We work with four tags: PER (person), ORG (organisation), LOC (location), and MISC (miscellaneous). We follow the commonly-used IOB (Inside Outside Beginning) tagging format  (Ramshaw and Marcus, 1999) . In our experiments, we work with English as the source language due to the availability of high-quality annotated corpora, e.g., CoNLL 2002  (Sang, 2002)  and OntoNotes 4.0  (Weischedel et al., 2011) . However, our method can easily be applied to any other resource-rich source language as well. See Figure  2  for an example of an annotated source sentence (blue), (x Si , y Si ). We are given a small labeled development set data (but no training data) in the target language T for tuning hyperparameters. Our method, denoted Translate-Match-Project (TMP), proceeds in three steps: First, we translate the annotated corpus in S to T using an off-the-shelf MT system (Google Translate). This results in an un-labeled dataset in the target language, D T = {x T i : i = 1, 2, ..., N } (Figure  2 ); Second, we identify and tag all named entities in the translated target sentences by entity projection, which involves entity alignment and tag projection. We perform entity alignment by first constructing a set of potential matches in the target sentence for every entity in the source sentence (candidate match generation, Section 3.1) and then by selecting the best matching pairs of source and target entities (best match selection, Section 3.2); Third, after alignment, we project the tag type (PER, LOC, etc.) from the source to the target entity in every pair of aligned entities by adhering to the IOB tagging scheme in target. In Figure  1 , we depict the complete pipeline. 

 Candidate match generation To generate candidate matches for an entity in a source sentence, we construct a set of its potential translations and then find matches for each in the corresponding target sentence. We find these matches by token-level matching and then concatenate matched tokens to obtain multi-token matches. We drop the index i below for ease of notation. Token-level matching Consider a source entity e S = (x S j , ..., x S k ). For every e S ? E S , where E S is the set of all entities in a source sentence, we obtain a set of potential translations in the target language, T (e S ), via MT. However, in some cases, translating a standalone entity produces a different translation from that which emerges when translating a full sentence. For example, Google Translate maps the source entity "UAE" to "Emiratos ?rabes Unidos" in most sentences but the word-by-word translation is "EAU". Similarly, the (person) name "Tang" (e.g., "Mr. Tang") remains "Tang" in translated sentences, but is translated to "Espiga" (Spanish for "spike", synonymous with the English word "tang"). We address these problems by augmenting T (?) with translations from publicly available bilingual lexicons ("UAE" translates to "Emiratos ?rabes Unidos" in one of the lexicons we use) and retain a copy of the source entity ("Tang" will now find a match in the target sentence). Finally, T ("UAE") looks roughly like: {"EAU" [Google Translate], "UAE" [copy], "Emiratos ?rabes Unidos" [lexicon]}. We note that lexicons exist for a large number of languages to-day 5 . However, we demonstrate that our method also works in absence of such lexicons in our case study for Armenian (Section 4.3). Next, we tokenize each candidate translation in T (e S ) to obtain a set of translation tokens for e S , T w (e S ). For example, T w ("UAE") = {"EAU", "UAE", "Emiratos", " ?rabes", "Unidos"}. We do this to allow for soft token-level matches because we observed empirically that matching exact entity phrases might result in few matches. Next, we obtain a match for each hypothesis token h ? T w (e S ) by matching it with each reference token x S l ? l ? {1, ..., L T } in the target sentence x T = (x T 1 , ..., x T L T ) of length L T . This match is carried out at (a) the orthographic (surface form) level; and (b) the phonetic level, by matching transliterations in the International Phonetic Alphabet (IPA) of the two tokens. In either case, we look for the longest sequence of characters in h that are an affix (prefix or suffix) of x T l . This soft affix-matching heuristic allows for inflection in morphologically-rich target languages. The (tokenlevel) score for the match is given as follows: s w (h, x T l ) = min n l L h , n l L x T l Here, L h and L x T l are the lengths (in characters) of the hypothesis and reference tokens and n l is the number of matching characters. We take minimum in order to enforce a stricter notion of fractional (soft) match. For example, the phrase "German first-time registrations..." [English] gets translated to "Los registros Alemanes por primera..."  [Spanish] . Using our matching heuristic, "Alem?n" ? T w ("German") matches to the reference token "Alemanes" with a score of 0.5, since n l = 4 ("Alem") and L x T l = 8 > L h = 6. Next, we define the matching (entity-level) score between a source entity e S and any target token x T l as follows: s e (e S , x T l ) = max h?T w (e S ) s w (h, x T l ) Note that the token-level scores, s w , include scores based on both orthographic and phonetic match, and thus, the entity-level scores, s e , correspond to the best token-level match (orthographic or phonetic) between any hypothesis token h ? T w (e S ) and a target token x T l . In Figure  3 , we depict the token-level matching procedure. The 5 Panlex  (Kamholz et al., 2014)  has lexicons for 10k different languages. score between the entity "American" and the target "estadounidense"(labeled 3 in the figure) is the maximum over matching scores between any token in {"americano", "american", "estadounidense"} and the target token ("estadounidense"), i.e., 1.0 (exact match) since the scores for the first two tokens in T w (?) are 0. Note some artifacts of token-level matching: (a) our matching heuristic currently handles prefixes or suffixes, but can potentially be extended with character edit distance for other types of affixes (e.g., circumfix) (b) every target token can match with multiple source entities (for e.g., "El" matches with both "American" and "US") and (c) some source entities might fail to find their true match ("US" fails to match with "EE.UU.", a possibly erroneous translation of "US" provided by Google Translate). Further, many matches are of very poor quality (especially those with stop words such as "El" and "en"). We address these issues and describe how to convert these token-level matches into spans to get multi-token target entities next. 

 Span match generation After token-level matching, we construct a list of potential entity spans in the target sentence that match with a given source entity e S by grouping adjacent target tokens for which a token-level matching score s e (e S , ?) is above a threshold ? (to remove spurious matches). In other words, we construct the following set: M(e S ) = {span(q, r) : s e (e S , x T u ) ? ? ? q ? u ? r} Here, span(q, r) = (x T q , ..., x T r ) is the phrase spanning tokens indexed from q to r (1 ? q, r ? L T ) in the target sentence. Further, we require span(q, r) to be maximal in the sense that ? q < q and ? r > r, span(q , r ) / ? M(?), i.e., any target token before or after the span have at best a weak match (s e (e S , ?) < ?) with e S .  In our running example, choosing ? = 0.25 results in the spans shown in Figure  4 , eliminating spurious matches (with "El" and "en") and concatenating "Barack" and "Obama" in the target sentence. However, the token "estadounidense" is still matched with two different source entities. We solve this problem in the next step. 

 Best match selection For selecting the best matching pair of entities, we first expand the set of potential translations T (?) to include all possible token-level permutations of the translations. We call this set T p (?). For example, T p ("UAE") = {"EAU", "UAE", "Emiratos ?rabes Unidos", "Emiratos Unidos ?rabes", " ?rabes Emiratos Unidos", ...}. Then, we greedily align e S with the target entity span from the set M(e S ), with the least character edit distance d E (?, ?) from any translation in T p (e S ), i.e., e T = argmin span(?,?)?M(e S ) d E (e S , span(?, ?)) In this manner, we form aligned entity pairs (e S , e T ), along which tags can then be projected. In our running example, since the edit distance between "estadounidense" (in T p (?)) and the target single-token span "estadounidense" is 0, and is lower than that with "estado", we match "estadounidense" with "American", tagging it B-MISC. 

 Distribution-based matching After selecting best matching pairs, there still remain some source entities that do not find any matching target entity ("US" in our example). These arise either due to significant differences between word-by-word and contextual sentencelevel translations either due to literal (e.g., "West Bank" gets translated to "Cisjordania" in sentences and "Banco Oeste" otherwise) or possibly incorrect translations (e.g., "U.S." gets translated to "EE.UU." in sentences and "NOSOTRAS" otherwise). We remedy this by exploiting corpus-level consistency in such discrepancies. For every unmatched source entity, we construct a set of topk potential matches ordered by their tf-idf (term frequency-inverse document frequency) scores, where tf is calculated over all sentences containing at least one unmatched entity and the idf score is calculated over the entire dataset to severely penalize commonly occurring tokens. Finally, we match each unmatched source entity with an unmatched span in its top-k list with the highest tf-idf score. Figure  5  shows the final matches and tags. 

 Experimental Evaluation Data In order to compare our method against benchmarks reported for prior approaches, we evaluate its performance on three European languages: Spanish (es), Dutch (nl) and German (de). Further, for a more extensive evaluation, we conduct additional experiments in an Indo-Aryan language (Hindi (hi)), a Dravidian language (Tamil (ta)), and Simplified Chinese (zh). For all languages except Chinese, we use English NER training data from the CoNLL 2003 shared task  (Sang and Meulder, 2003)  to translate into the target language. For Chinese, we sample the same number of sentences as in the CoNLL 2003 corpus (14,041) from the OntoNotes 4.0 (2012) dataset for English  (Weischedel et al., 2011)   ORG, LOC and MISC tags, while Hindi, Tamil and Chinese were preprocessed to contain only PER, LOC and ORG tags. We use MUSE groundtruth bilingual lexicons 7 (gold lexicon) for augmenting the set of potential entity translations and use Epitran  for obtaining IPA transliterations. Baselines We compare against four other annotation projection approaches that have achieved stateof-the-art results on some of our datasets.  Xie et al. (2018)  (BWET) use a bilingual lexicon induced using monolingual corpora  (Conneau et al., 2017)  to translate each source sentence word-by-word and then copy the corresponding NER tags using gold lexicons. As a ceiling for their method,  Mayhew et al. (2017)  used Google Translate with fast-align  (Dyer et al., 2013)  (fast-align), an unsupervised expectation maximization based algorithm, for entity alignment. Since this algorithm can produce multiple matches for a given source entity, we postprocess the alignments produced by this algorithm and select the longest match and then project tags in the same way as our method. Our third baseline is  Ni et al. (2017) (Co-decoding) , who use a co-decoding scheme on two different NER models. We also compare our method with Polyglot-NER  (Al-Rfou et al., 2015)  who use Wikipedia links to project entities. Finally, we also compare our performance with a model trained on annotated data in target language (Monolingual). 

 NER Model We use the state-of-the-art neural NER tagging model from  (Xie et al., 2018)  to train TMP and fast-align baseline for all languages. This model adds a self-attention layer to the characterand word-based BiLSTM + CRF model due to  Lample et al. (2016) . For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation (as recommended by  Reimers and Gurevych (2017) ) of F 1 measure. 7 https://github.com/facebookresearch/MUSE Hyperparameters For the fast-align baseline, we tune their ? parameter, which controls how much the model deviates from perfectly diagonal alignments, for each language separately. For TMP, we tune ?, the score threshold and k, the number of top candidates selected in distribution-based matching. We use the same hyperparameters for the NER model as  Xie et al. (2018)  for all our experiments. 

 Results Our technique outperforms previous state-of-theart cross-lingual methods on Spanish, German, Chinese, Hindi and Tamil and performs competitively on Dutch (Table  1 ). In particular, our method shows marked improvements over BWET, a wordby-word translation baseline, for languages such as German, Hindi, Tamil and Chinese that differ markedly in word ordering (with respect to English), demonstrating the impact of improved machine translation quality on final NER tagging accuracy. For more distant languages, word ordering can drastically affect the position of entities in a sentence, which can hurt performance on a test set in the target language. For instance, consider the Hindi word-by-word translation in Figure  6  (c), which is incoherent and violates the Subj-Obj-Verb ordering of Hindi. On languages that are closer to English, like Spanish and Dutch, the gains are comparatively modest, indicating that word order and quality MT is not critical for such languages. We also show improvements over the fast-align baseline, which performs unsupervised word-level alignment over the full sequence. This can lead to alignment errors for named entities, which tend to be low-frequency words. Moreover, since fastalign allows for multiple target words to be aligned to a given source word, several noisy tags are added to the target sentence (see Figures  6 (a ) and (b)). 

 Comparison of projection settings Having established the performance of TMP as a method for cross-lingual NER, in this section, we conduct deeper experiments to evaluate the effectiveness of the matching (M) and projection (P) steps of TMP over the other projection baseline, fast-align. As mentioned in Section 2, there are variants of the annotation projection paradigm for cross-lingual NER that require an entity projection step, namely (i) reversing the direction of machine translation and (ii) using parallel corpora. We compare MP with fast-align for Spanish and Hindi languages under both these settings. Reversing the direction of translation In this setting, we translate the target test set into the source language using Google Translate and then use the NER tagger with state-of-the-art results Flair 8 to tag entities in the translated English sentences. Finally, we employ MP/fast-align to project the tagged entities back to the target sentence. As shown in table 2, MP outperforms fast-align for both Spanish and Hindi and performs better than the forward direction translation for Hindi. This can be attributed to (a) the inherent difficulty of NER tagging in Hindi, which is morphologically richer than English and (b) the superior quality of the English NER model. 

 Parallel corpora In order to remove translation errors while evaluating TMP and fast-align, we 8 https://github.com/zalandoresearch/flair experiment with parallel corpora. For English-Spanish, we use the Europarl corpus  (Koehn, 2005)  and for English-Hindi, the IIT Bombay parallel corpus  (Kunchukuttan et al., 2017) . We again use Flair to obtain NER tags in English, which are then projected to their corresponding target sentences to generate a training dataset, which is used to train an NER model in the target language. To minimize confounding variables, we sample 14k (same as CoNLL) high quality tagged sentences (average confidence score > 0.9). Results in Table  2  show that MP once again outperforms fast-align. Further, it performs better than Forward for Hindi by a significant margin possibly because the chosen parallel corpus is closer in time period to the test set, thereby reducing distribution shift. 

 Case study: Armenian So far, we have only evaluated the performance of our method on languages for which large or moderately-sized gold annotated corpora already exist that provide an upper-bound for cross-lingual NER methods. Here, we evaluate our method on a true medium-resource language, Armenian. Recently,  Ghukasyan et al. (2018)  introduced a ground truth test corpus for Armenian along with a train corpus with silver annotations extracted from Wikipedia. This test dataset is comprised of 2566 sentences (53k tokens) from political, sports, local and world news between August 2012 and July 2018. Since the English CoNLL 2003 dataset contains sentences nearly two decades older, we expect to see significant distribution shift if we follow TMP (Forward approach). Further, we are not aware of any large English-Armenian parallel corpora. So, we choose the Reverse paradigm for this problem. We achieve an F 1 score of 62.6, which is significantly higher than that achieved by fast-align (44.8). Further, this is 0.4 points higher than the current state-of-the-art model trained on over 160k tokens of Armenian. Note that our model does not make use of any external resources for Armenian (gold lexicons, Epitran, etc.) other than an MT system. This provides evidence towards our proposed approach being an effective and generalizable cross-lingual NER method that can be used for rapid deployment to new languages. 

 Analysis Measuring alignment accuracy Since we do not possess ground truth word alignments for the "synthetic" parallel corpus generated through translation, we rely on heuristics to measure the accuracy of alignments. We measure the annotation miss rate among target sentences with equal or fewer tagged entities as compared to source. We also calculate the excess rate, representing the fraction of excess entities among sentences with more tagged entities. Both methods perform similarly in terms of miss rate, 0.79 % (MP) vs 0.83 % (fastalign) on Spanish and 3.96 % (MP) vs 3.48 % (fastalign) on Hindi. However, fast-align seems to add more noisy annotations as compared to MP, with higher excess rates for both Spanish (8.29 % vs 0.49 %) and Hindi (6.35 % vs 2.20 %). A representative illustration of these noisy tags is shown in Figure  6  (a). where fast-align tags frequent words like "El", "de", "en" as entities. To offer a more fine-grained evaluation of alignment performance, we manually annotate 100 examples from the translated Spanish and Hindi training data and calculate precision, recall and F 1 score. MP outperforms fast-align for both the languages (Table  3 ).  

 Ablation of features for alignment We also conduct an ablation study (Table  4 ) to understand the sources of our gains beyond a base model that uses translations only from Google Translate and orthographic affix matching. To this base model, we successively add various features of our method: phonetic matching, exact copy translations, gold lexicons and finally distribution-based alignment (dist) of remaining entities. For both languages, we observe that every additional feature improves the performance of tagging, with the most important features being phonetic matching for Spanish and use of gold lexicons for Hindi. Interestingly, addition of phonetic matching hurts Hindi because of the low value of the threshold (? = 0.25), which results in spurious matches due to phonetic matching. In Table  4 , we also see that the number of entities tagged (as a fraction of total entities) increase with the introduction of almost every feature (however, all matches might not be correct). This underscores the correlation between quality of entity alignment and performance on the downstream tagging task.  

 Sources of errors in TMP We also analyze mistakes made by TMP in aligning entities. Many false negative errors can be traced back to a high threshold ?, resulting in an empty set of candidate matches. Errors also arise due to noise and variation introduced in the contextual sentence level translation of a word (Figure  6  (c) where GIBBS is interpreted as an acronym, (d) where MEDVEDEV is mistranslated). This causes discrepancies between translations of standalone entities and those in context, thereby, causing TMP to not find a match. However, these errors can be reduced as off-the-shelf MT systems continue to improve. 

 Conclusion In this paper, we tackled the problem of entity projection for cross-lingual NER. Our proposed method leverages MT for translating entities, matches entities based on orthographic and phonetic similarity, and identifies matches based on distributional statistics derived from the dataset to achieve state-of-the-art results for cross-lingual NER on a diverse set of languages. Further, our method beats state-of-the-art monolingual baseline for Armenian, an actual medium-resource language (off-the-shelf translation systems exist, but largescale NER corpora do not). In the future, we would like to explore ways to extend our method to languages not supported by Google Translate through the use of pivot languages. While dependence on MT restricts our approach to languages covered by off-the-shelf MT systems, these systems continue to improve in coverage and quality, outpacing the availability of large-scale corpora for a variety of other tasks. Moreover as translation quality improves, approaches like ours are poised to benefit. Finally, although our method beats state-of-the-art baselines, not surprisingly, it falls short of NER models trained on large monolingual corpora. We suspect that a significant portion of this degradation is due to distribution shift (as evidenced by improvement in Hindi F 1 in Parallel regime). Thus one promising route to improving our models might be to incorporate domain adaptation techniques, which aim to build classifiers robust to various forms of distribution shift. Figure 1 : 1 Figure 1: A schematic diagram representing the chief steps in our method. 
