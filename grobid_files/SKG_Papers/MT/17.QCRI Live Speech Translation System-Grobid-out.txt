title
QCRI Live Speech Translation System

abstract
We present QCRI's Arabic-to-English speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient. 1

Introduction We present our Arabic-to-English SLT system consisting of three modules, the Web application, Kaldi-based Speech Recognition culminated with Phrase-based/Neural MT system. It is trained and optimized for the translation of live talks and lectures into English. We used a Time Delayed Neural Network (TDNN) for our speech recognition system, which has a word error rate of 23%. For our machine translation system, we deployed both the traditional phrase-based Moses and the emerging Neural MT system. The trade-off between efficiency and accuracy (BLEU) barred us from picking only one final system. While the phrasebased system was much faster (translating 24 tokens/second versus 9.5 tokens/second), it was also roughly 5 BLEU points worse (28.6 versus 33.6) compared to our Neural MT system. We therefore leave it up to the user to decide whether they care more about translation quality or speed. The real-time factor for the entire pipeline is 1.18 using Phrase-based MT and 1.26 using Neural MT. Our system is also robust to common English code-switching, frequent acronyms, as well as dialectal speech. Both the Arabic transcriptions and the English translations are presented as results to the viewers. The system is built upon modern web technologies, allowing it to run on any browser that has implemented these technologies. Figure  1  presents a screen shot of the interface. 

 System Architecture The QCRI live speech translation system is primarily composed of three fairly independent modules: the web application, speech recognition, and machine translation. Figure  2  shows the complete work-flow of the system. It mainly involves the following steps: 1) Send audio from a broadcast instance to the ASR server; 2) Receive transcription from the ASR server; 3) Send transcription to MT server; 4) Receive translation from the MT server; 5) Sync results with backend system and 6) Multiple watch instances sync results from the backend. Steps 1-5 are constantly repeated as new audio is received by the system through the broadcast page. Step 6 is also periodically repeated to get the latest results on the watch page. Both the speech recognition and machine translation mod-Figure  2 : Demo system overview ules have a standard API that can be used to send and receive information. The Web Application connects with the API and runs independently of the system used for transcription and translation. 

 Web application The web application has two major components; the frontend and the backend. The frontend is created using the React Javascript framework to handle the dynamic User Interface (UI) changes such as transcription and translation updates. The backend is built using NodeJS and MongoDB to handle sessions, data associated with these sessions and authentication. The frontend presents the user with three pages; the landing page, the watch page and the broadcast page. The landing page allows the user to either create a new session or work with an existing one. The watch page regularly syncs with the backend to get the latest partial or final transcriptions and translations. The broadcast page is meant for the primary speaker. This page is responsible for recording the audio data and collecting the transcriptions and translations from the ASR and MT systems respectively. Both partial transcriptions and translations are also presented to the speaker as they are being decoded. To avoid very frequent and abrupt changes, the rate of update of partial translations was configured based on a MIN NEW WORDS parameter, which defines the minimum number of new words required in the partial transcription to trigger the translation service. Both the partial and the final results are also synced to the backend as they are made available, so that the viewers on the watch page can experience the live translation. 

 Speech transcription We use the Speech-to-text transcription system that was built as part of QCRI's submission to the 2016 Arabic Multi-Dialect Broadcast Media Recognition (MGB) Challenge. Key features of the transcription system are given below: Data: The training data consisted of 1200 hours of transcribed broadcast speech data collected from Aljazeera news channel. In addition we had 10 hours of development data . We used data augmentation techniques such as Speed and Volume perturbation which increased the size of the training data to three times the original size  (Ko et al., 2015) . Speech Lexicon: We used a Grapheme based lexicon  (Killer and Schultz, 2003)  of size 900k. The lexicon is constructed using the words that occur more than twice in the training transcripts. Speech Features: Features used to train all the acoustic models are 40 dimensional hiresolution Mel Frequency Cepstral Coefficients (MFCC hires), extracted for each speech frame, concatenated with 100 dimensional i-Vectors per speaker to facilitate speaker adaptation  (Saon et al., 2013) . Acoustic Models: We experimented with three acoustic models; Time Delayed Neural Networks (TDNNs) , Long Short-Term Memory Recurrent Neural Networks (LSTM) and Bi-directional LSTM  (Sak et al., 2014) . Performance of the BLSTM acoustic model in terms of Word Error Rate is better than the TDNN, but TDNN has a much better real-time factor while decoding. Hence, for the purpose of the speech translation system, we use the TDNN acoustic model. The TDNN model consists of 5 hidden layers, each layer containing 1024 hidden units and is trained using Lattice Free Maximum Mutual Information (LF-MMI) modeling framework in Kaldi  (Povey et al., 2016) . Word Error Rate comparison of different acoustic models can be seen in Table  1 . For further details, see  Khurana and Ali (2016) . Language Model: We built a Kneser Ney smoothed trigram language model. The vocab size is restricted to the 100k most frequent words to improve the decoding speed and the real-time factor of the system. The choice of using a trigram model instead of an RNN as in our offline systems was essential in keeping the decoding speed at a reasonable value. Decoder Parameters: Beam size for the de-Model %WER TDNN 23.0 LSTM 20.9 BLSTM 19.3 Table  1 : Recognition results for the LF-MMI trained recognition systems. LM used for decoding is tri-gram. Data augmentation is used before training coder was tuned to give the best real-time factor with a reasonable drop in accuracy. The final value was selected to be 9.0. 

 Machine translation The MT component is served by an API that connects to several translation systems and allows the user to seamlessly switch between them. We had four systems to choose from for our demo, two of which were Phrase-based systems, and the two were Neural MT systems trained using Nematus  (Sennrich et al., 2016) . PB-Best: This is a competition-grade phrasebased system, also used for our participation at the IWSLT'16 campaign  (Durrani et al., 2016) . It was trained using all the freely available Arabic-English data with state-of-the-art features such as a large language model, lexical reordering, OSM  (Durrani et al., 2011)  and NNJM features  (Devlin et al., 2014) . 

 PB-Pruned: The PB-best system is not suitable for real time translation and has high memory requirements. To increase the efficiency, we dropped the OSM and NNJM features, heavily pruned the language model and used MML-filtering to select a subset of training data. The resulting system was trained on 1.2 M sentences, 10 times less the original data. NMT-GPU: This is our best system 2 that we submitted to the IWSLT'16 campaign  (Durrani et al., 2016) . The advantage of Neural models is that their size does not scale linearly with the data, and hence we were able to train using all available data without sacrificing translation speed. This model runs on the GPU. NMT-CPU: This is the same model as 3, but runs on the CPU. We use the AmuNMT (Junczys-2 without performing ensembling  Dowmunt et al., 2016)  decoder to use our neural models on the CPU. Because of computation constraints, we reduced the beam size from 12 to 5 with a minimal loss of 0.1 BLEU points. The primary factors in our final decision were 3-fold; overall quality, translation time and computational constraints. The translation time has to be small for a live translation system. The performance of the four systems on the official IWSLT test-sets is shown in Figure  3 . We also computed the translation speed of each of the systems.  3  The results shown in Figure  3  depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMT-CPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMT-GPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words by transliterating them in a post-decoding step  (Durrani et al., 2014) . 

 Combining Speech recognition and Machine translation To evaluate our complete pipeline, we prepared three in-house test sets. The first set was collected from an in-house promotional video, while the other two sets were collected in a quiet office environment. 3 PB-Pruned and NMT-CPU were run using a single CPU thread on our standard demo machine using a Intel(R) Xeon(R) E5-2660 @ 2.20GHz processor. PB-Best was run on another machine using a Intel(R) Xeon(R) E5-2650 @ 2.00GHz processor due to memory constraints. Finally, NMT-GPU was run using an NVidia GeForce GTX TITAN X GPU card. We analyzed the real time performance of the entire pipeline, include the lag induced by translation after the transcription is complete. With an average real-time factor of 1.1 for the speech recognition, our system keeps up with normal speech without any significant lag. The distribution of the real time factor speech recognition and translation for the in-house test sets is shown in Figure  4 .  

 Conclusion This paper presents QCRI live speech translation system for real world settings such as lectures and talks. Currently, the system works very well for Arabic including frequent dialectal words, and also supports code-switching for most common acronyms and English words. Our future aim is to improve the system in several ways; by having a tighter integration between the speech recognition and translation components, incorporating more dialectal speech recognition and translation, and by improving punctuation recovery of the speech recognition system which will help machine translation to produce better translation quality. Figure 1 : 1 Figure 1: Speech translation system in action. The Arabic transcriptions and English translations are shown in real-time as they are decoded. 
