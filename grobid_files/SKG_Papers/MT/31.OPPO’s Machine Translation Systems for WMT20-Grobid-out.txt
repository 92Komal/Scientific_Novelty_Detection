title
OPPO's Machine Translation Systems for WMT20

abstract
In this paper we demonstrate our (OPPO's) machine translation systems for the WMT20 Shared Task on News Translation for all the 22 language pairs. We will give an overview of the common aspects across all the systems firstly, including two parts: the data prepro cessing part will show how the data are pre processed and filtered, and the system part will show our models architecture and the tech niques we followed. Detailed information, such as training hyperparameters and the re sults generated by each technique will be de picted in the corresponding subsections. Our final submissions ranked top in 6 directions (English ? Czech, English ? Russian, French ? German and Tamil ? English), third in 2 directions (English ? German, English ? Japanese), and fourth in 2 directions (English ? Pashto and and English ? Tamil).

Introduction This paper describes the OPPO's submission to the Fifth Conference on Machine Translation (WMT20) news translation shared task. We built Transformer  (Vaswani et al., 2017) based systems for all the directions, and applied several well known, widelyused techniques, such as large scale backtranslation  (Sennrich et al., 2016a ) and forwardtranslation, model ensemble and rerank ing. Since all the systems share a roughly similar data preprocessing and training methods, to avoid duplication words, we will demonstrate the com mon knowledge in Section 2 firstly, which will be divided into two parts: the preprocessing part shows the data preprocessing pipeline and data fil tering pipeline, the latter is generally composed by rulebased filtering and alignmentbased filtering? the training part depicts the techniques we applied. Detailed information, including training hyperpa rameters, the results generated by each technique, and some other explorations will be listed in each corresponding direction in Section 3. Finally, we will summarize the report and indicate our final works. We used marian  to implement our systems for English ? {Khmer, Russian, Tamil} and French ? German task pairs, and fairseq  (Ott et al., 2019)  for the rest 1 . 

 System Overview We preprocess corpora in two stages. In the pre processing stage, data is converted but not filtered. The common pipeline of preprocessing including the following steps: ? Remove nonutf8 characters ? Unescape html characters, e.g. "&gt;" is con verted to ">" ? Normalize different kinds of spaces and punc tuations ? Tokenization 

 ? True case The last three steps are all processed by moses scripts. This pipeline is both applied for the paral lel corpora and monolingual corpora, and true case models are generally trained on the mixture of par allel and monolingual datasets. After preprocessing we filter the parallel cor pora according to statistical information and align ment information, set the thresholds according to our previous experiences. For the statistic perspec tive, we mainly focus on some heuristic rules, con tain but not limited in ? Pairs of which the source side and the target side are the same. ? Pairs contain blank lines. ? Pairs contain too long sentences (typically those have more than 200 words). ? Pairs that have abnormal sourcetarget length ratios. The sourcetarget length ratio is de fined as the words count ratio between the source and the target. Typically the upper bound is 2.5 and the lower bound is 0.4. ? Pairs that have irregular characterword length ratios. The characterword length ra tio is defined as the ratio between the count of characters and the count of words. Generally the upper bound is 12 and the lower bound is 1.5. ? Pairs that contain too long words. The length threshold for deciding whether the word is too long is 25 characters. For the alignment perspective, we use fast_align  (Dyer et al., 2013)  to acquire the alignment scores from source to target and vice versa, then we aver age the scores for each pair to calculate a data pair's sentencelevel alignment score. If a sentence pair's sentencelevel alignment score is lower than 15, it will be expelled from the final dataset. Having purified the corpus, we generally try to boost our systems using the following techniques, step by step: 1. Backtranslation and forwardtranslation. Us ing the trained models to translate big vol ume, monolingual corpus from the target side to source side (i.e. backtranslation  (Sennrich et al., 2016a) ) has been proved a very success ful method in the past practices. In our experi ments we can also see a general improvement brought by this technique, but it is not always the case. We also tried sampling based back translation proposed in  (Edunov et al., 2018) , and this is effective only in certain cases as well. Furthermore, we found translating from the monolingal corpus from source language can also bring gains for the models (consistent with the phenomenon depicted in  (Burlot and Yvon, 2018) ), but in this situation argmax based beam search should always be applied. We also followed  to it eratively backtranslate and forwardtranslate the corpus for several times. 2. Finetune. Adding too many synthetic par allel data generated by machine translation models could potentially modify the latent data distribution, and in some tasks the pro vided monolingual dataset has a small differ ence from the required domain (news), so af ter having trained models from the mixture of the original parallel corpus and the synthetic dataset, we continue finetune our models on the original parallel datasets only. Besides, for some lowresource tasks (such as tasks on Pashto and Khmer), even the official training datasets have relatively lower qualities, there fore only using training dataset to finetune is still not enough. For these tasks, we took one more step to finetune the models on the official released validation set, and we can al ways see a further improvement. 3. Ensemble. We generally train and finetune several different models and compose them into an ensemble model for a better result. 

 4. Reranking. With the ensemble model in the hand, we usually generate kbest candidates and use different scorers to score them. Scor ers can be divided into three groups: forward scorers are just another ensemble models composed by the forward translation models (models translate the source language to the target language). Suppose we have trained 6 base forward models, typically we compose all of them together to form a big ensemble model for generating final results (this model is also used as a scorer), and then additionally enumerate all the 5combinations of them to get another ( 6 5 ) = 5 scorers. Sometimes we furthermore enumerate all the 4combination to get ( 6 4 ) = 15 more scorers for better rerank ing. backward scorers are ensemble models that actually backtranslation models (models translate the target language to the source lan guage), and language models are ensemble language models of target language. For each group of the scorers, we may use the leftto right (l2r) models or righttoleft (r2l) mod els. For the latter form, we reverse the words orders for both source sentences and target sentences and train the models. The scores generated by those scorers are used as fea tures by the reranking model. For reranking, we mostly applied KBatched MIRA  (Cherry and Foster, 2012)  or noisy channel  (Yee et al., 2019) . 

 Experiments Details In this section we demonstrate our experiments de tails for each direction. For brevity we will ig nore the same preprocessing and techniques we in troduced in the previous section, mainly focus on how the techniques boosted the systems, and some other unique observations we found during the ex periments. In the text we will sometimes use ISO6391 twoletter codes for each language for short. Map ping between the abbreviations and full names can be found in Table  1 . For example, when talking about the English ? Chinese task, we may write EnZh for short, capitalizing the first letter of the ISO6391 codes for both source languages and target languages. For the direction pairs that in volve English, sometimes we use the nonEnglish language to indicate the whole pair, e.g. "Russian tasks" is used to indict the English ? Russian bi directional task. As this report is in the news task scope, we sometimes use "task" as a synonym of "direction", e.g. "EnZh task" means the direction that translates English to Chinese. By default, for every subtask we combine all the official provided parallel corpora into a big dataset then clean it, use the cleaned corpus to train our baseline models. We strictly followed the re quirement of the contest to use official released datasets only, so the systems we built are all con strained systems. If not mentioned, all of our base line models are trained on the parallel corpus only, and all the scores reported are calculated by sacre BLEU  (Post, 2018)  based on the results which has been removed BPE symbols, detruecased and deto kenized. We always apply BPE subwords  (Sen nrich et al., 2016b)  on the corpora, usually train TransformerBig models and tie the input and out put matrices of the decoder. For all the tasks, we used Adam optimizer  (Kingma and Ba, 2014)  Following the statistical information mined from the original parallel corpus, we con verted all traditional Chinese characters to their simplified counterparts. 2. Some websites use GB2312 to encode texts, therefore could convert Latin letters, digit characters and some other punctuation marks into full width form. Besides of some partic ular punctuation marks (full stops, commas, question marks and exclamation marks), we converted all the other symbols to half width form. 3. Chinese does not have explicit words bound aries, all the characters in the same clause are connected together. We used pkuseg  (Luo et al., 2019)  to segment words from the text. It should be noted that Japanese also has these three features, so the same process is also applied in the English ? Japanese systems. For data filtering stage, besides the heuristic rules we demonstrated in the previous section, we also compare the count of numbers and punctua tion marks between source side and target side. If the difference on number counts is greater than 3 or the difference on punctuation marks counts is greater than 5, the sentence pairs will also be re moved. 

 Training We combined the Chinese corpus and English cor pus together to train BPE. The total BPE opera tion merge counts is 36K. After learning BPE op erations, we built vocabularies for each language separately. The final vocabulary size for Chinese is 42K and for English is 23K. The model ar chitecture for both directions are all Transformer big. For ZhEn task, we tried different hyperpa rameters to train several models for getting en semble model: learning rates ranged from 0.0003 to 0.0008, warmup steps fixed at 16,000, dropout ranged from 0.2 to 0.3. For EnZh task, the hyper parameters are all fixed (but tried different random seeds): learning rate was 0.0003, warmup steps was 15,000, feed forward network dimension was 15,000. Entity substitution is experimented in the ZhEn system. We use StanfordNLP  (Qi et al., 2018)  to do the NER from parallel corpus and Chinese mono lingual datasets (Because in Chinese monolingual datasets an annotation usually follows a foreign name). After having extracted all the entities, we didn't use alignment information to build the map ping between Chinese entities and English entities, but constructed such relationship just according to cooccurrence frequency information: suppose an entity "?" occurs 50 times totally in the Chi nese corpus from 20 sentences, and in the corre sponding 20 English sentences "Beijing" occurs 51 times, "Shanghai" occurs 10 times, then we be lieve "?" can be translated to "Beijing" but not "Shanghai". With the entity mapping rules, we then replace the entities in the sentence pairs by different tags <tag1>, <tag2> ... and train mod els. In the inference time, model generates results with those tags, and we take another postedit stage to recover the entities, using the mapping rules as lookup tables. Table  2  shows our systems for ZhEn task, and 3 shows our systems for EnZh task. For ZhEn, we backtranslated 20M NewsCrawl and 17M NewsDiscussion monolingual datasets from En glish to Chinese, and forwardtranslated 13M Chi nese monolingual dataset to English (including XMU, LDC, etc.).   B ) is directly compared with the one trained by adding back translation data only (system A). The two phases fine tune, which is effective for the system A, has no obvious impact on system B 3.2 English ? Czech 

 System 

 Data Preprocessing The officially released English ? Czech dataset has a different format from the other subtasks. The dataset, which is called CzEng 2.0  (Kocmi et al., 2020) , contains not only parallel sentence pairs, but also the data source and three scores: alignment score calculated by dual conditional crossentropy filtering (JunczysDowmunt, 2018), and language scores to show of how confident the source is Czech and the target is English. This ex tra information can further help us to filter the cor pus. Having noticed that both CsEn and EnCs tasks would be evaluated on long, documentlevel news datasets, and the CzEng dataset contains some document information, we first analyzed the data sources given in the dataset, to determine which of them are near to the destination domain, and which are far away. The data sources were observed from four aspects: 1. Are the sentences more colloquial or more formal? 2. How well the data is aligned? 3. Can the sentences form a paragraph? 4. Is the corpus also in the news domain? With the features of the given data sources, we first set a hard condition to check whether a given sentence pair could be kept, then set different prob abilities to randomly drop some pairs from certain data sources. Constrained by the paper length we cannot list all of the rules for all the data sources here, but we can take some examples. For the data of which the source is news, we kept all of them? at the other extreme, for the commoncrawl data, we first removed all the data pairs of which the align ment scores are below than 0.25, or the probabil ities of the source sentences belonging to Czech are less than 0.9, then we removed 40% of the re mained data randomly. As the original dataset contains some paragraph information, we concatenated all the sentences that were originally in the same paragraph with a de limiter "|||" (for the sentences that come from the data sources of subtitles, subtitleE and subtitleM, we didn't concatenate them). After the initial filter ing, we kept 24.24 million data pairs (If we add in the czengtest data, the total volume is 24.44 mil lion pairs). The kept data were then processed and filtered by the pipeline presented in the previous section, and we finally got 14.4 million pairs. De tailed preprocessing information can be found in Table  4 . Step   

 Model Training As the evaluation for the En ? Cs tasks would be documentlevel, we first experimented to see if training a model on a dataset which contains many very long sentences can generate better trans lations for whole documents. We prepared the datasets in three different ways: 1. Concatenat ing all sentences that belong to the same document (as indicated in the original data sources), noted as "fulldoc"? 2. Concatenating three consecutive sentences together, and select the middle one as the final result from the generated translation, noted as "shortdoc"? 3. No special preprocessing, one line contains one sentence, noted as "nodoc". The ex periments results are shown in Table  5 . From the results we can find that no extra docu ment related preprocessing is the best preprocess ing, so we continued our improvement based on the dataset which does not contain documentlevel information. We first trained two models based on the full CzEng 2.0 dataset (including all the offi cial translated data). Models are all trained using TransformerBig architecture with norm clipping set to 0.1, dropout set to 0.3, gradient update fre quency set to 8, maximum tokens in a batch set to 6000. Warmup steps and learning rate varied from different experiments, the most common combi nation is warmup steps set to 16,000 and learn ing rate set to 0.001. During decoding the beam size is 5 and length penalty is 2.5 for CsEn, 2 for EnCs. The score of the CsEn model on offline test set (newstest2018) is 34.0 and the EnCs model on validation set (newstest2019) is 28.6. We use these two models backtranslated and forward translated several data, mixed our synthetic dataset with the original official whole datasets together, and trained several models. Models which have the best performances are selected for the final backtranslation and forwardtranslation, which are listed in Table  6 . We composed the models shown above as two ensemble models, one for each direction, and did another round of backtranslation and forward translation again. For the EnCs task, we pre pared two different final datasets as below. Two datasets are all generated by randomnessbased backtranslation, the difference is the full sampling one sample output words in the full vocabulary, whilst the topk one restricts the sampling pool in the words that are listed in the topk highest prob abilities for each step: As we observed the results generated by full sampling backtranslation sometimes contain very bad sentences, we check how many steps the decoder scores below 10 when decoding for a given input. If 20% of the step scores for a given sentence are below 10, then we discard the sentence pair. We found the models trained by topk sam pling based dataset are generally worse than those trained by full sampling based dataset, therefore se lected one topk sampling based model and three full sampling based model to form the final ensem ble model for decoding the test data. For the CsEn task, The final dataset is composed by 24 million original parallel data pairs, 24 million ensemble knowledge distillation data pairs, 50 million topk sampling backtranslated pairs, 10 million argmax beam search backtranslated pairs, and 17 million forwardtranslated pairs. We trained 4 models us ing different learning rate (varied from 0.0008 to 0.0015) on this dataset, and finetuned them using original parallel dataset (finetuning on EnCs mod els does not bring any gains). The finetuned mod els are used for the final ensemble model. We also applied FDA algorithm  (Bi?ici and Yuret, 2011)  on the parallel dataset, picked out 5 million sentence pairs that are similar to the test set and finetuned on this small dataset. The overview of our EnCs system is listed in Ta ble 7, and CsEn system is listed in Table 8 

 English ? German For En ? De tasks, we generally followed the process depicted in Section 2, cleaned 46.8 mil lion data pairs and kept 30.6 million. For data  preprocessing, we removed sentence pairs that contain too many punctuation marks, and too many [^A-Za-z] characters. In both directions we found neither backtranslation nor forward translation could yield any gains. In the EnDe we found ensemble knowledge distillation  (Freitag et al., 2017)  could improve the effect but in the DeEn task it did not help. The overview of our En ? De system is listed in Table  9 . 

 English ? Inuktitut We just adapted the official preprocessing script in the syllabic form to process the corpus. BPE was learned independently and the merge opera tions count is 16K. The overview of our En ? Iu system is listed in Table  10 .  Table  11 : Overview of our WMT20 English ? Japanese systems. Reranking follows noisychannel reranking  (Yee et al., 2019) . BLEU scores are reported on the offline official validation set, for EnJa, we report the characterlevel score. We trained BPE separately for both tasks, merge operations is 32K. Learning rate for training is 0.0003 and warmup steps is 15000. We tried two different feed forward network dimensions, 4096 and 15000, and found no big differences 

 English ? Japanese Our En ? Ja systems generally follow our En ? Zh systems depicted before, the difference was the upper bound of sentence length limit was set to 180 words, and we also set the lower bound to 3. For Japanese word segmentation we used mecab 2 . We cleaned 17.64 million parallel pairs and 13.7 mil lion left. For backtranslation, we used 16 million Japanese monolingual data and 13 million English monolingual data. The overview of our En ? Ja system is listed in Table  11  We tried to finetune the models using original parallel dataset, but didn't see any gain. After the test dataset was released, we applied FDA algo rithm and extracted 5000 sentences from the train ing dataset which are the most similar to the test data. These sentences are mixed with the origi nal validation dataset together, then 500 sentences are split out as a new validation set, the rest were used to finetune the models. This step improved our EnJa system by 1.3 BLEU and for JaEn it is 0.4 BLEU. However, as validation dataset changed and the scores on the new validation dataset were extremely high, this step is not listed in the Table  11 . 

 English ? Khmer For the Khmer tasks (and some other tasks in the following), The data preprocessing stages are slightly different from the way we depicted in the second section, stricter in the filtering part, which would remove the sentence pair if... 1. It is a duplicated example 2. The source or target side is empty 3. It contains urls 4. It has words that contain more than 4 consec utive repeated characters 5. It has unpaired quotation marks or parenthe ses (not applicable for Khmer tasks, but ap plied in the other tasks shown later) 6. The punctuation marks between the source and the target cannot be matched (not applica ble for Khmer tasks, but applied in the other tasks shown later) 7. The length ratio between the source and target is greater than 2.0 or less than 0.5 (for Khmer is between 0.33 and 3) 8. More than half of the tokens are not from the indicated language. We designed a regular ex pression (noted as regex for short) for each language according to its alphabet, if the word failed to pass the regex, we say it is not from the given language. For example, the regex for English is [a-zA-Z'-]+ The maximum sentence length we allowed is also set to 200 words. Similar to Chinese and Japanese, Khmer does not mark the words boundaries neither, so we used SEANLP 3 to do the Khmer word segmentation. After the cleaning, the 4.46 million pairs of sen tences had 351K lines left. It should be noted that the writing system of Khmer, Khmer script, is an abugida, means vow els do not have independent symbols, but are stuck after/above/below/in front of the consonants they follow. Roughly, the minimal meaningful unit of Khmer is called Khmer Character Cluster (KCC for short)  (Huor et al., 2004) , which should be re garded as a whole but actually contains several characters. Original BPE method would break KCC, but this is not what we expect, so we made some modification to keep it (the segmentation tool we used also considered this language fea ture). We combined Khmer corpus and English to train BPE together, the BPE merge operations count is 8K. To train the model, we tried different learning rate ranged from 0.0001 to 0.0004, and different warmup steps from 2,000 to 32,000. The overview of our Km ? En system is listed in    

 English ? Pashto Our Pashto systems used the similar process we de scribed in the Japanese tasks. We cleaned the 1 mil lion original parallel dataset and kept 700K pairs. BPE was jointly learned and the merge operations count is 10000, but the source language does not share vocabulary with the target. When training the models, the learning rate was set to 9 ? 10 ?4 and warmup steps was 6000. The overview of our En ? Ps system is listed in Table  13  As what we did in the Japanese tasks, we selected 10000 sentence pairs from the training dataset according to the test data using FDA, mixed them with official validation set and de vtest set to finetune our models for 5 epoch, then reranked the generated candidates. This improved our EnPs system by 1.6 BLEU and for PsEn the gain is 3.5. 

 English ? Polish For En ? Pl tasks, we generally followed the process depicted in En ? De tasks, cleaned 10.3 million data pairs and kept 5.265 million. The overview of our En ? Pl system is listed in  ? In the second round backtranslation, we added in the backtranslated results generated by our models, and continued training again. ? In the knowledge distillation step, we added in the knowledge distillation results on the base of the dataset produced in the previous step. After training had converged, models are continue trained using the mixture of orig inal parallel dataset and the knowledge distil lation results. Full results can be referred to Table  15 . 

 English ? Tamil Similar to Khmer, Tamil language also uses abugida. So with the same idea, we need to de termine the minimal unit to be separated during BPE training. Here we see syllables as the min   imal unit, use opentamil 4 to separate syllables, and use our modified subwordnmt to learn BPE separations. Cleaning process is the same as we described in the Khmer tasks, we cleaned all the parallel corpora which contains 660K pairs, and had 450K pairs left. For backtranslation, we used all available Tamil monolingual corpus (27 million lines totally) and 16 million English sentences sam pled from NewsCrawl 2019 and NewsCommen tary 2019. BPE is learned jointly, the merge op erations count is 10K. The overview of our En ? Ta system is listed in Table  16 . In the finetune stage, we randomly kept 200 sentences from the newsdev2020 as the validation set, and the rest 1,789 sentences are used to finetune the model. 

 French ? German Our Fr?De systems generally followed the steps we described in the Russian tasks, with two differ ences. The first is that we have only one round backtranslation, since for this task pair no official backtranslation dataset was released? the second is we didn't continue training using parallel corpus after the model had converged. Following the pro cess described in the Khmer tasks, we cleaned the 13.7 million data pairs and kept 11 million. For  In the step "Finetune" we finetuned our models using euelections_dev2019 backtranslation, we took 27 million French sen tences (combination of NewsCrawl 20172019 and News Commentary datasets) and 40 million Ger man sentences (from NewsCrawl 2019 only). We jointly learned BPE for the two langauges, the BPE merge operations count is 32K. We shared the vo cabulary among the two languages and tied all em bedding layers and output layer in the model. The overview of our Fr ? De system is listed in Table  17 . 

 Conclusion This report described OPPO's submissions to the WMT20 news translation task. We use the sim ilar data preprocess and filtering strategy for all the tasks, contains statistical information based rules and alignment information based rules. We trained TransformerBig models for all the direc tions and applied some mature techniques, like backtranslation, ensemble model, finetune and reranking, they generally all brought gains for the final results. Our final submissions ranked top in 6 directions (English ? Czech, English ? Rus sian, French ? German and Tamil ? English), third in 2 directions (English ? German, English ? Japanese), and fourth in 2 directions (English ? Pashto and and English ? Tamil). . All the main systems (i.e. submitted results) are gen erated by the model listed in the last row of the corresponding table in each task. Language Name ISO6391 Code Chinese zh Czech cs English en French fr German de Inuktitut iu Japanese ja Khmer km Pashto ps Polish pl Russian ru Tamil ta Table 1: ISO6391 codes for languages appear in news task of WMT20 3.1 English ? Chinese 3.1.1 Data Preprocessing Compared from the other languages in the shared task, especially the languages which use alpha betical writing systems, Chinese has three typical characteristics, leading to three extra preprocess ing steps we introduce below: 1. Chinese has two different writing systems: simplified Chinese and traditional Chinese. 
