title
Quality Estimation for Machine Translation output using linguistic analysis and decoding features

abstract
We describe a submission to the WMT12 Quality Estimation task, including an extensive Machine Learning experimentation. Data were augmented with features from linguistic analysis and statistical features from the SMT search graph. Several Feature Selection algorithms were employed. The Quality Estimation problem was addressed both as a regression task and as a discretised classification task, but the latter did not generalise well on the unseen testset. The most successful regression methods had an RMSE of 0.86 and were trained with a feature set given by Correlation-based Feature Selection. Indications that RMSE is not always sufficient for measuring performance were observed.

Introduction As Machine Translation (MT) gradually gains a position into production environments, the need for estimating the quality of its output is increasing. Various use cases refer to it as input assessment for Human Post-editing, as an extension for Hybrid MT or System Combination, or even a method for improving components of existing MT systems. With the current submission we are trying to address the problem of assigning a quality score to a single MT output per source sentence. Previous work includes regression methods for indicating a binary value of correctness  (Quirk, 2001; Blatz et al., 2004; Ueffing and Ney, 2007) , human-likeness  (Gamon et al., 2005)  or continuous scores  (Specia et al., 2009) . As we also work with continuous scores, we are making an effort to combine previous feature acquisition sources, such as language modelling  (Raybaud et al., 2009) , language fluency checking  (Parton et al., 2011) , parsing  (S?nchez-Martinez, 2011;  and decoding statistics  (Specia et al., 2009; Avramidis, 2011) . The current submission combines such previous observations in a combinatory experimentation on feature sets, feature selection methods and Machine Learning (ML) algorithms. The structure of the submission is as follows: The approach is defined and the methods are described in section 2, including features acquisition, feature selection and learning. Section 3 includes information about the experiment setup whereas the results are discussed in Section 4. 

 Methods 

 Data and basic approach This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012  (Callison-Burch et al., 2012) . The organizers provided an English-to-Spanish development set and a test set of 1832 and 422 sentences respectively, derived from WMT09 and WMT10 datasets. For each source sentence of the development set, participants were offered one translation generated by a state-ofthe-art phrase-based SMT system. The quality of each SMT translation was assessed by human evaluators, who provided a quality score in the range 1-5. Additionally, statistics and processing information from the execution of the SMT decoding algorithm were given. The approach presented here is making use of the source sentences, the SMT output and the quality scores in order to follow a typical ML paradigm: sentence suggestion . . . los l?deres de la Uni?n han descrito como deducciones pol?tico . . . number agreement La articular y ideol?gicamente convencido de asesino de masas . . . transform "y" to "e" Right after hearing about it, he described it as a "challenge. . . "  

 disambiguate -ing 

 Acquiring Features The features were obtained from two sources: the decoding process and the analysis of the text of the source and the target sentence. The two steps are explained below. 

 Features from text analysis The following features were generated with the use of tools for the statistical and/or linguistic analysis of the text. The baseline features included: ? Tokens count: Count of tokens in the source and the translated sentence and their ratio, unknown words and also occurrences of the target word within the translated sentence (averaged for all words in the hypothesis -type/token ratio) ? IBM1-model lookup: Average number of translations per source word in the sentence, unweighted or weighted by the inverse frequency of each word in the source corpus ? Language modeling: Language model probability of the source and translated sentence ? Corpus lookup: percentage of unigrams / bigrams / trigrams in quartiles 1 and 4 of frequency (lower and higher frequency words) in a corpus of the source language Additionally, the following linguistically motivated features were also included: ? Parsing: PCFG Parse  (Petrov et al., 2006)  loglikelihood, size of n-best tree list, confidence for the best parse, average confidence of all parse trees. Ratios of the mentioned target features to the corresponding source features. ? Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. Additionally, the ratio of the occurences of each tag in the target sentence by the corresponding occurences on the source sentence. ? Language quality check: Source and target sentences were subject to automatic rule-based language quality checking, providing a wide range of quality suggestions concerning style, grammar and terminology, summed up in an overall quality score. The process employed 786 rules for English and 70 rules for Spanish. We counted the occurences of every rule match in each sentence and the number of characters it affected. Sample rule suggestions can be seen in Table  1 . 

 Features from the decoding process The organisers provided a verbose output of the decoding process, including probabilistic scores from all steps of the execution of the translation search. We added the scores appearing once per sentence (i.e. referring to the best hypothesis), whereas for the ones being modified over the generation graph, their average (avg), variance (var) and standard deviation (std) was calculated. These features are: ? the log of the phrase translation probability (pC) and the phrase future cost estimate (c) ? the score component vector including the distortion scores (d 1...7 ), word penalty, translation scores (e.g. a 1 : inverse phrase translation probability, a 2 : inverse lexical weighting) 

 Feature Selection Experience has shown difficulties in including hundreds of features into training a statistical model. Several algorithms (such as Na?ve Bayes) require statistically-independent features. For others, a search space of hundreds of features may impose increased computational complexity, which is often unsustainable in the time and resources allocated. In these cases we therefore applied several common Feature Selection approaches, in order to reduce the available features to an affordable number. We used the Feature Selection algorithms of Relieff  (Kononenko, 1994) , Information Gain and Gain Ratio  (Kullback and Leibler, 1951) , and Correlation-based Feature Selection  (Hall, 2000) . The latter is known for producing feature sets highly correlated with the class, yet uncorrelated with each other; selection was done in two variations, greedy stepwise and best first. The data were discretised according to the algorithm requirements and features were scored in a 10fold cross-validation. 

 Machine Learning We tried to approach the issue with two distinct modelling approaches, classification and regression. 

 Classification algorithms In an effort to interpret Quality Estimation as a classification problem, we expect to build models that are able to assign a discrete value, as a measure of sentence quality. This bears some relation to the way the quality scores were generated; humans were asked to provide an (integer) quality score in the range 1-5. In our case, we try to build classifiers that do the same, but are also able to assign values with smaller intervals. For this purpose, we set up 4 sub-experiments, where the class value in our data was rounded up to intervals of 0.25, 0.5, 0.7 and 1.0 respectively. In this part of the experiment we used the Na?ve Bayes, k-nearest-neighbours (kNN), Support Vector Machines (SVM) and Tree classification algorithms. Na?ve Bayes' probabilities for our continuous features were estimated with locally weighted linear regression  (Cleveland, 1979) . 

 Regression algorithms Regression algorithms produce a model for directly predicting a quality score with continuous values. Experimentation here included Partial Least Squares Regression  (Stone and Brooks, 1990) , Multivariate Adaptive Regression Splines -MARS  (Friedman, 1991) , Lasso  (Tibshirani, 1994)  and Linear Regression. 

 Experiment and Results 

 Implementation PCFG parsing features were generated on the output of the Berkeley Parser  (Petrov and Klein, 2007) , trained over an English and a Spanish treebank  (Mariona Taul? and Recasens, 2008) . Ngram features have been generated with the SRILM toolkit  (Stolcke, 2002) . The Acrolinx IQ 1 was used to parse the source side, whereas the Language Tool 2 was applied on both sides. The feature selection and learning algorithms were implemented with the Orange  (Dem?ar et al., 2004)  and Weka  (Hall et al., 2009)  toolkits. 

 Experiment structure The methods explained in the previous section provide a wide range of experiment parameters. Consequently, we tried to extensively test all the possible parameter combinations. The development data were separated in two sets, one "training" set and one "keep-out" set, used to test the predictions. In order to give learners better coverage over the data, the development set was split in two ways (70% training -30% test and 90% training -10% test), so that all experiments get performed under both settings. The scores of these two were averaged 3 . 

 Results The small size of the dataset allowed for fast training and testing of the discrete classification problem, where we could execute 370 experiments. The regression problem was considerably slower, as only 36 experiments concluded in time. Feature generation resulted (described in Section 2.2) into 266 features, while 90 of them derived from language checking. Feature selection suggested several feature sets containing between 30 and 80 features. We ended up defining 22 feature sets, including the full feature set, the baseline feature set and a couple of manually selected feature sets. Unfortunately, due to size restrictions, not all features can be listed; though, indicative feature sets are listed in Table  5 . 

 5- The most important results of the classification approach can be seen in Table  2  and the results of the regression approach in Tables 3 (development set) and 4 (shared task test set). 

 Discussion 

 Machine Learning Conclusions Discrete classifiers (section 2.4.1) do not yield encouraging accuracy, as acceptable levels of accuracies appear only with a discretisation interval of 1.00, which though cannot be accepted due to its high Root Mean Square Error (RMSE). On the development keep-out set, the discretised Tree classi-fier seemingly outperforms all other methods (including the regression learners), since it yields a RMSE of 0.84, given several different feature vectors. Unfortunately, when applied to the final unknown test data, these classifiers performed obviously bad, providing the same single value for all sentences. We could attribute this to overfitting vs. sparse data and consider how we can handle this better in further work. Another remarkable observation was the incapability of the RMSE to objectively show the quality of the model, in situations where the predicted values are very close or equal to the average of all real values. A Support Vector Machine with RMSE = 0.86 ranked 3rd among the classifiers, although it "cheated" by producing only the average value: 3.25. This leads to the conclusion that the selection of the best algorithm is not just dictated by the lowest RMSE, but it should consider several other indications such as the standard deviation. We therefore resort to the regression learners (section 2.4.2), whose scores are not worse, having a RMSE of 0.855. We have to notice that the four The best-performing feature set (#19) which was chosen as the first submission (DFKI cfs-plsreg) trained with PLS regression, contains features indicated by Correlation-based Feature Selection, run with bestfirst on a 10-fold cross-validation. We used the features which were selected on the 100% or 90% of the folds. An equally best-performing feature set (#18) has resulted from exactly the same feature selection execution, but contains only features which were selected in all folds. The second submission (DFKI grcfs-mars) was chosen to differentiate both the feature set and the learning method, with respect to a decent interval. Feature Selection, run in a greedy-stepwise mode. The regression was trained with MARS. The baseline feature set (#2) performed worse. Noticeable was the RMSE of the feature set #4, with features selected based on their Gain Ratio, but we did not submit this due to its very narrow interval. 

 Feature conclusions The best performing feature set gives interesting hints on what worked as a best indication of translation quality. We would try to summarize them as follows: ? The language checking of the source sentence detected complex or embedded sentences, which are often not handled properly by SMT due to their complicated structure. ? The language checking of the target sentence detected several agreement issues. ? Parsing provided of source and target count of verbs, nouns, adjectives and secondary sentences; with the assumption that translations are relatively isomorphic, the loss of a verb or a noun or the inability to properly handle a secondary sentence, would mean a considerably bad translation outcome. The number of parse trees generated for each sentence can be an indication of ambiguity. ? Punctuation (dots, commas) often indicates a complex sentence structure. ? The most useful decoding features were the inverse phrase translation probability (a 1 ), the inverse lexical weighting (a 2 ), the phrase probability (pC) and future cost estimate (c) as well as statistics over their incremental values along the search graph.  Table 1 : 1 Sample suggestions generated by rule-based language checking tools, observed in development data each source and target sentence of the development set are being analyzed to generate a feature vector. One training sample is formed out of the feature vec- tor and the quality score (i.e. as a class value) of each sentence. A ML algorithm is consequently used to train a model given the training samples. The per- formance of each model is evaluated upon a part of the development set that was kept-out from training. 

 Table 2 : 2 Indicative discretised classification results, sorted by best performance and discretisation interval. Classifica- fold avg 70-30%, 90-10% folds algorithm feat. set discr. CA AUC RMSE MAE interval Tree #17, #20 0.25 15.40 54.10 0.84 0.67 1.5 5.0 Tree #23 0.25 14.60 53.50 0.85 0.68 2.0 5.0 Tree #12 0.25 13.90 52.00 0.86 0.69 1.8 5.0 Tree #4 0.25 14.50 53.70 0.86 0.69 2.0 5.0 SVM #16 0.25 16.00 60.40 0.86 0.69 3.2 3.2 kNN #22 0.25 12.30 55.50 1.00 0.78 2.0 5.0 Tree #21 0.50 22.70 54.60 0.87 0.69 2.0 5.0 SVM #19 0.50 22.40 60.20 0.91 0.73 2.8 5.0 kNN #12 0.50 20.00 54.70 0.98 0.78 2.2 5.0 Naive #6 0.50 21.20 59.40 0.99 0.76 1.2 5.0 Tree #9 0.70 32.70 53.30 0.89 0.71 3.5 4.9 kNN #12 0.70 28.20 56.10 0.93 0.73 2.5 4.9 SVM #18 0.70 30.90 55.60 0.97 0.77 3.5 4.2 Tree #22 1.00 40.30 55.70 0.90 0.71 2.0 5.0 kNN #22 1.00 40.90 59.10 0.96 0.76 2.5 5.0 Naive #23 1.00 41.00 65.50 1.02 0.78 1.2 5.0 SVM #6 1.00 36.60 51.10 1.02 0.84 3.0 4.0 tion Accuracy (AC), Area Under Curve (AUC), Root Mean Square Error (RMSE) and Mean Average Error (MAE), Largest Error Percentage (LEP) and Smallest Error Percentage (SEP) 

 Table 4 : 4 Feature set #16 is the result of the Correlation-based Results of the submitted methods on the official testset learner feat. name RMSE MAE MARS #16 grcfs-mars 0.98 0.82 PLS #19 cfs-plsreg 0.99 0.82 

 Table 5 : 5 CC, NP, NN, JJ, comma trees, S, CC, VB, VP, NN, JJ, dot Decoding avg(a 2 ), a 1 , a 2 #16 Baseline LM, seen, punct, %uni q 1 , %bi q 1 , %bi q 4 , %tri q 4LM, target occ Checker score: style, spelling, quality; verb: agr, form, obj inf, close to subj; avoid parenth, complex sent, these those noun, np num agr, noun adj conf, repeat subj, wrong seq, wrong word, disamb that, use rel pron, use article, avoid dangling, repeat modal, use complement double punct, to too confusion, word repeat, det nom sing, pp v plural, pp v sing, nom adj plural, comma parenth space, nom adj fem, nom adj masc, nom adj sing, det nom fem, del nom sing, del nom masc, det nom plur Parsing trees, S, CC, JJ, comma, VB, NP, NN, VP trees, S, CC, JJ, NP, VB, NN, VP, dot, PP Decoding avg(pC), avg(a 1 ), std(pC), var(c), std(lm), avg(a 2 ), d 2 , std(c), a 1 , a 2 Indicative feature sets for the most successful quality estimation models. Features explained at section 2.2 feature 

			 http://www.acrolinx.com (proprietary) 2 http://languagetool.org (open-source) 3 Given the disparity of the test sizes, it would have in principle been better to use a weighted average. Though, this would not have lead to significant differences in the results.
