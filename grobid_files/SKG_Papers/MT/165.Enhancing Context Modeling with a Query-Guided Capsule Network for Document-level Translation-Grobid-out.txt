title
Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level Translation

abstract
Context modeling is essential to generate coherent and consistent translation for Document-level Neural Machine Translations. The widely used method for document-level translation usually compresses the context information into a representation via hierarchical attention networks. However, this method neither considers the relationship between context words nor distinguishes the roles of context words. To address this problem, we propose a query-guided capsule networks to cluster context information into different perspectives from which the target translation may concern. Experiment results show that our method can significantly outperform strong baselines on multiple data sets of different domains.

Introduction The encoder-decoder based Neural machine translation (NMT) models  (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017;  have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation  (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019) . Despite the great success of the above models, they are designed for sentence-level transla-Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation  (Miculicich et al., 2018) . In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is, it utilizes a wordlevel attention to represent a sentence and then a sentence-level attention to represent all the involved context. In this way, the final attention representation has to encode all the information needed for coherent and consistent translation, including reference information, tenses, expressions and so on. To get the multi-perspective information, it is necessary to distinguish the role of each context word and model their relationship especially when one context word could take on multiple roles  (Zhang et al., 2018b) . However, this is difficult to realize for the HAN model as its final representation for the context is produced with an isolated relevance with the query word which ignores relations with other context words. To address the problem, we introduce Capsule Networks into document-level translation which have proven good at modelling the parts-wholes relations between low-level capsules and highlevel capsules  (Hinton et al., 2011; Xiao et al., 2018; Sabour et al., 2017; Hinton et al., 2018; Gu and Feng, 2019) . With capsule networks, the words in a context source sentence is taken as lowlevel capsules and the information of different perspectives is treated as high-level capsules. Then in the dynamic routing process of capsule networks, all the low-level capsules trade off against each other and consider over all the high-level capsules and drop themselves at a proper proportion to the high-level capsules. In this way, the relation among low-level capsules and that between low-level capsules and high-level capsules are both explored. In order to make sure high-level capsules indeed cluster information needed by the target translation, we apply capsule networks to both sides of the current sentence and add a regularization layer using Pearson Correlation Coefficients to force the high-level capsules on the two sides to approach to each other. In addition, we still need to ensure the final output of capsule networks is relevant to the current sentence. Therefore we propose a Query-guided Capsule Network (QCN) to have the current source sentence to take part in the routing process so that high-level capsules can retain information related to the current source sentence. To the best of our knowledge, this is the first work which applies capsule networks to document-level translation tasks and QCN is also the first attempt to customize attention for capsule networks in translation tasks. We conducted experiments on three English-German translation data sets in different domains and the results demonstrate that our method can significantly improve the performance of document-level translation compared with strong baselines. 

 Background 

 Sentence-level NMT and Transformer Model Sentence-level NMTs are generally based on an encoder-decoder framework  (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; . In this framework, the encoder encodes the source sentence x = {x 1 , x 2 , ? ? ? , x M } into a sequence of continuous representations z = {z 1 , z 2 , ? ? ? , z M }. Given z, the decoder predicts target words in order and returns the target translation y = {y 1 , y 2 , ? ? ? , y N }. The objective function of NMT is to maximize the log-likelihood of a set of source-target language sentence pairs as L(?; x, y) = 1 |S| (x,y)?S log P (y|x; ?) (1) where S is the training set. As we implement our approaches based on the Transformer architecture  (Vaswani et al., 2017) , which is a strong sentence-level NMT baseline, we give a brief description of the Transformer model. The encoder and decoder are composed of similar layers which consists of two general types of sub-layers: multi-head attention mechanism and point-wise fully connected feed-forward network (FFN). Encoder: The encoder of the Transformer is composed of N identical layers. Each of these layers includes a multi-head self-attention mechanism that allows each position of the output of previous encoder layer to attend to all other positions, as well as a position-wise FFN, which is stacked on top of the multi-head self-attention, composed of two linear transformations and a ReLU activation function. Decoder: The architecture of the decoder is similar to the encoder, however, it employs an additional multi-head attention sub-layer over the encoder output between the multi-head self-attention sub-layer and position-wise FFN sub-layer. The multi-head self-attention sub-layer needs to mask the input target tokens in the future. 

 Document-level NMT The document-level translation task is to translate each source sentence with consideration of the previous context in the document. Formally, the translation of a document D containing |D| = J sentence pairs can be defined as given the source document X in order and the translation system generates each translation y j ? Y in order. The document translation probability can be defined as: P (Y |X; ?) = J j=1 P (y j |x j , D <j ; ?) (2) where x j , y j denote the j th source and target sentence respectively, and D <j denotes all of the previous sentence pairs in document. For each sentence x j , each target word is generated according to the source representation and the generated target hypothesis, therefore the Eq. (  2 ) can be formulated as: P (Y |X; ?) = J j=1 I j i=1 P (y j i |, y j <i , x j , D <j ; ?) (3) where y j i denotes the i th word of the j th translation y j with the length as I j and y j <i denotes the generated target hypothesis. The training objective of document-level NMTs is to maximize the log-likelihood of translations in document context as following: L(?; X, Y ) = 1 |D| (X,Y )?D log P (Y |X; ?) (4 ) where D is the training set of the DocNMT. Compared with the sentence-level NMTs, the critical part of document-level NMTs is to effectively capture and utilize the related contextual information when translating the to-be-translated source sentence. 

 Our Approach In this section, we introduce the proposed Queryguided Capsule Network (QCN) for enhancing the document-level NMT. First, we present the overall architecture of the network, and then we describe the QCN in detail. 

 Overall Architecture We aim to enhance the document-level NMT performance through effectively capturing and employing the contextual features in each historical sentence that related to the current sentence. We integrating a novel Query-guided Capsule Network into the sentence-level Transformerbased NMT  (Vaswani et al., 2017)  to capture the document-level contextual information for translating the current source sentence. As shown in Figure  1 , the overall architecture of our translation model is composed of three modules: ? The Query-guided Capsule Network takes the to-be-translated source sentence as the query to guide the procedure of retrieving related and helpful contextual features from historical sentences with a novel dynamic routing algorithm. ? Sub-layer-expanded Transformer contains a new sub-layer that attending the contextual features extracted from the QCN to effectively utilize them for translation. ? Regularization Layer contains two conventional Capsule Networks to unify the source sentence and the target sentence into an identical semantic space through computing an extra PCCs loss item at the training stage. 

 Query-guided Capsule Network The Capsule Network (CapsNet)  (Sabour et al., 2017)  was proposed to build parts-wholes relationships in the iterative routing procedure, which can be used to capture features in historical sentences from low level to high level. Capsules in the lower layer vote for those in the higher layer by aggregating their transformations with iteratively updated In each iteration, the PPCs is computed according to the input vector u i and the query vector q j , then p ij ?j|i and c ij ?j|i are added to obtain the higher-level capsules which will be used to update corresponding query later. coupling coefficients. However, there exists an obvious drawback of directly applying the Capsule Network into the document-level NMT for capturing contextual features. The reason is that the CapsNet can only extract internal features without considering whether features are related to the tobe-translated source sentence. To address this issue, we proposed the Queryguided Capsule Network (QCN), which employ the representation of the to-be-translated source sentence as the query to guide the feature extraction procedure of the Capsule Network. In this way, contextual features that generated from the QCN are based on the internal semantic relations among each historical sentence and the external semantic relations between historical sentences and the to-be-translated source sentence. The QCN is based on an improved dynamic routing algorithm which will be detailed introduced in the next section. Improved Dynamic Routing of QCN Given a query vector q and a set of input capsules u = {u 1 , u 2 , ? ? ? , u n }, the dynamic routing algorithm iteratively calculates the correlation between the query vector and each input capsule and updates output capsules. Specifically, query vector q is the representation of source to-be-translated sentence and input capsules u are all word embeddings in previous sentences, which can be for- Algorithm 1 Improved Dynamic Routing Input: r , q and u = {u1, u2, ? ? ? , un} Output: v = {v1, v2, ? ? ? , vm} 1: // Initialization 2: for each output capsule vj in higher-layer do 3: for each input capsule ui in lower-layer do 4: ?j|i ? W ij ui 5: ?ij ? 0 6: q j ? q 7: pij ? tanh(PCCs(ui, q j )) 8: PCCs computes Eq 7 9: end for 10: end for 11: 12: // Iteration 13: itr ? 0 14: repeat 15: for each input capsule ui in lower-layer do 16: ci ? softmax(?i) 17: end for 18: 19: for each output capsule vj in higher-layer do 20: sj ? n i=1 (cij + pij)? j|i 21: vj ? squash(sj) 22: squash computes Eq 8 23: end for 24: 25: for each output capsule vj in higher-layer do 26: for each input capsule ui in lower-layer do 27: ?ij ? ?ij + pij ?j|i vj 28: end for 29: q j ? q j +v j 2 

 30: for each input capsule ui in lower-layer do 31: pij ? tanh(PCCs(ui, q j )) 32: end for 33: end for 34: itr ? itr + 1 35: until itr = r mulated as: q = g( x?x embedding(x)) (5) u ?k i = f([embedding(x ?k i ); onehot(k)]) (6) where f and g are both linear transformation functions and k indicates the distance of historical sentence from the to-be-translated sentence. Each word embedding in historical sentences is concatenated with a distance-determined one-hot vector to provide positional markers. Compared to the dynamic routing method proposed by  (Sabour et al., 2017) , our improved dynamic routing method of QCN can model information that related to the query among input capsules. Algorithm 1 shows details of the algorithm and Figure  2  illustrates the interaction of the various components in the QCN. Initially, the improved dynamic routing algorithm of QCN recieves a sequence of lower-level capsules u = {u 1 , u 2 , ? ? ? , u n } and a query vector q and then calculates a Pearson Correlation Coefficients (PCCs) between q and each input capsule u i (line 7). The PCCs is a measure of the linear correlation between two variables. When PCCs is close to +1, it means they have very strong positive linear correlation, and close to -1 means total negative correlation. Given a pair of variables (A, B), the formula for computing PPCs is PCCs(A, B) = Cov(A, B) ? A ? B = (A ? 1 n n i=1 a i ) T (B ? 1 n n i=1 b i ) A ? B ( 7 ) where A and B are both n-dimension vectors, Cov is the covariance and ? A , ? B is the standard deviation of A and B respectively. The routing iteration process then computes coupling coefficients, denoted as c i (line 16), with regard to a input capsule u i and all the higherlevel capsules v. In the original dynamic routing algorithm  (Sabour et al., 2017) , coupling coefficients are only determined by the cumulative "agreement", which are the prior probabilities that capsule u i should be coupled to capsule v . However, in DocNMT situation, it is far from enough to cluster the high-level information from the capsules in lower-level that related to query vector according to a naive "agreement". To address this issue, we reduce the "agreement", when PCCs show a negative linear correlation between the query vector q and the input vector u i . Instead, we increase the "agreement", when PCCs are positive (line 27). The query vector q is initially tiled to length |v| and updated with the corresponding higher-level capsules v j in each iteration (line 29). Our routing iteration updates higher-level capsules by adding n i=1 p ij ?j|i to s j . This step can add more information related to the query vector and cut off the unrelated features (line 20). It is necessary to get different length of output capsules shrunk into the 0 to 1 interval using "squash" function (line 21) proposed by  Sabour et al. (2017)  which is shown in Eq.(8). squash(t) = ||t|| 2 1 + ||t|| 2 t ||t|| ( 8 ) where t can be the initial input capsule u i or the vector s j for predicting the output capsule. 

 Sub-layer-expanded Transformer To effectively utilize contextual features extracted from each historical sentence by QCN, we intro- duce an additional context-aware multi-head attention sub-layer in each layer of the Transformer encoder. The multi-head attention can attend to all the positions of the contextual features with outputs of the previous sub-layer as a query. Then the output of the context-aware multi-head attention sub-layer is fed into the point-wise feed-forward sub-layer in each layer of the encoder. The right part in Figure  1  shows details of the Sub-layerexpanded Transformer. Specifically, the equation of multi-head attention computing procedure is as following: MultiHead(Q,K,V ) = Concat(H 1 ,? ? ?, H h )W O H i = Attention(QW i Q , KW i K , V W i V ) (9) where W i Q , W i K and W i V are the parameter matrices, Q, K and V indicates the query, key and value representations. The computation of the attention function is as following: Attention(Q, K, V ) = softmax( QK T ? d k )V (10) where d k indicates the dimension of queries Q and keys K. 

 Regularization Layer To better modeling the document-level translation task, we incorporate a regularization layer (Figure  3 ) into the whole architecture to restrict the source sentence and target sentence to an identical semantic space. This layer separately feeds the inputs of encoder and decoder into two capsule networks Caps enc and Caps dec , and computes the PCCs between the outputs of two networks. We regard the PCCs as an extra regularization term in the final objective at the training stage. The loss function of our model can be formulated as: L(?; X, Y ) = 1 |D| ? J j=1 I j i=1 {log P (y j i |, y j <i , x j , D <j ; ?) + PCCs(Caps enc (x j ), Caps dec (y j ))} (11) where ? are parameters of the model, D <j are historical sentences of the to-be-translated source sentence, x j is the to-be-translated sentence and y j <i denotes the generated target hypothesis. 

 Experiments 

 Settings 

 Datasets and Evaluation Metrics We carry out experiments on English-German translation tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table  1 . ? TED. This corpus is a Machine Translation part of the IWSLT 2017  (Cettolo et al., 2012)  evaluation compaigns 1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. ? News. We take the sentence-aligned document-delimited News Commentary v11 corpus 2 as our training set. The WMT'16 news-test2015 and news-test2016 are used for development and testing respectively. ? Europarl. The corpus are extracted from the Europarl v7  (Koehn, 2005)  according to the method mentioned in  Maruf and Haffari (2018) . The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora 3 from  Maruf et al. (2019) . The tokenization and truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit 4  (Koehn et al., 2007) . We also apply segmentation into BPE subword units 5  (Sennrich et al., 2016)  with 30K merge operations. We use two metrics: BLEU  (Papineni et al., 2002)  and Meteor  (Lavie and Agarwal, 2007)  to evaluate the translation quality. 

 Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines  (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018) . We performed the same configuration on our models according to the settings of the  Maruf and Haffari (2018) . Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 for context-agnostic model and 0.2 for context-aware model. In the training phase, we use the default Adam optimizer (Kingma and Ba, 2014) with a fixed learning rate of 0.0001. The batch size is 1500 on TED dataset and 900 on both News and Europarl datasets. 

 Results and Analysis 

 Main Results Table  2  shows that our model surpasses all the context-agnostic  (Vaswani et al., 2017)  and context-aware  (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018)  baselines on TED and Europarl datasets. For TED dataset, the performance of our model greatly exceeds that of all other baselines, and is better than  Miculicich et al. (2018)  with a gain of +0.59 BLEU and +0.61 Meteor. For Europarl dataset, our model got improvements with a gain of +0.07 on BLEU metric, but the Meteor score is +0.64 higher than  Maruf et al. (2019)  which utilize the whole document as the contextual information, whereas we only using 3 previous sentences. Results on the sequence-level Transformer and our DocNMTs show that the captured contextual features provide helpful semantic information for enhancing the translation quality.   further improve the model performance on TED and Europarl datasets. For the restriction of the GPU memory, we have to filter long sentences to keep our model running. Although, it hurts the model performance on the "News" dataset (contains many long sentences), the QCN module and regularization term still bring improvements. 

 Effect of Contextual Information Scope To investigate the effect of contextual information scope, we carry on the number of historical sentences hyper-parameter experiments on the TED talk dataset. We fix the hyper-parameters of the QCN by setting both the number of higher-level capsules and routing iteration to four, and investigating the impact of changes in the number of historical sentences on BLEU and Meteor scores. Figure  4  shows that using one historical sentence in QCN can obtain the best Meteor score while the highest BLEU score is presented when we utilize two historical sentences. We found that the growth of Meteor and BLEU scores are opposite. Therefore, the choice of utilizing how many historical sentences is a trade-off between both scores. We choose three historical sentences as our final setting. Through experimentation, we also found it is not that the more historical sentences we utilize, the better translation performance is.  

 Effect of Feature Capsule Number QCN is the crucial part of our overall architecture and the positive and negative impact depends on the configuration of the QCN. Therefore, We also investigate the effect of the hyper-parameter of QCN: the number of feature capsules. We set the number of historical sentence as 3 according to the previously experimental results, and the number of routing iteration is set to 4. Figure  5  shows that Meteor score become highest when the number of higher-level capsules is set as 2, but BLEU score can obtain best score at 4. We finally choose 4 as the final setting because both BLEU and Meteor can obtain relatively good results. 

 Visualization of Agreement and PCCs Coupling Coefficients (CCs) can indirectly reflect the variation of the "agreement", so we visualize the coefficients in each routing iteration at the stage of decoding as shown in Figure  6 . In the first iteration, all coupling coefficients are initialized in a uniform distribution, all higher-level capsules are voted by lower-level capsules equally. Then, lower-level capsules are iteratively trained to send more information to the proper higher-level capsule. Figure  6  shows that most of input capsules are tend to vote the 2 ed feature capsule finally. Different from the CCs, PCCs show the linear correlation between query and inputs. Initially, the query vector is tiled with the number of feature capsules and is used to calculate the PCCs with each input capsules, as the Figure  7  shows that the color of every column is identical. As the iterative routing begins, each query is updated according to the higher-level capsules. The function of the PPCs is to increase or decrease the coupling coefficients value according to the positive or negative value. See Figure  7 , we can find that PCCs varies as iteration changes. 

 Related Work 

 Document-level Machine Translation Document-level machine translation became a hot research direction in the later stage of statistical machine translation era.  Hardmeier and Federico (2010)  represented the links between word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns.  Hardmeier et al. ( , 2013  first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks.  Xiong et al. (2019)  trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models' input or output.  Jean et al. (2017)  used additional context encoder to capture larger-context information.  Kuang et al. (2017) ;  Tu et al. (2018)  used a cache to memorize most relevant words or features in previous sentences or translations. Recently, several studies integrated additional modules into the Transformer-based NMTs for modeling contextual information.  (Voita et al., 2018; Zhang et al., 2018a) ,  Maruf and Haffari (2018)  proposed a document-level NMT using a memory-networks, and  Wang et al. (2017)  and  Miculicich et al. (2018)  integrated hierarchical attention network in RNN-based NMT or Transformer to model the document-level information.  Maruf et al. (2019)  used the whole document as the contextual information and firslty divided documentlevel translation tasks into two types: offline and online.  et al. (2011)  proposed the capsule conception to use vector for describing the pose of an object. The dynamic routing algorithm was proposed by  Sabour et al. (2017)  to build the partwhole relationship through the iterative routing procedure.  Hinton et al. (2018)  designed a new routing style based on the EM algorithm. Some researchers investigated to apply the capsule network for various tasks.  investigated a novel capsule network with dynamic routing for linear time NMT.  explored capsule networks for text classification with strategies to stabilize the dynamic routing process.  Gu and Feng (2019)  introduces capsule networks into Transformer to model the relations between different heads in multi-head attention. We specifically investigated dynamic routing algorithms for the document-level NMT. 

 Capsule Networks 

 Hinton 

 Conclusion We have proposed a novel Query-guided Capsule Network with an improved dynamic routing algorithm for enhancing context modeling for the document-level Neural Machine Translation Model. Experiments on English-German in different domains showed our model significantly outperforms sentence-level NMTs and achieved stateof-the-art performance on two of three datasets, which proved the effectiveness of our approaches. Figure 1 : 1 Figure 1: The overall architecture consists of three modules: the Query-guided Capsule Network in the upper left of this figure, the Regularization Layer in the lower left of this figure and the Sub-layer-expanded Transformer in the right of this figure. 
