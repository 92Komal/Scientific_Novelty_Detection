title
Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation

abstract
Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed -non-learnable -attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.

Introduction Models based on the Transformer architecture  (Vaswani et al., 2017)  have led to tremendous performance increases in a wide range of downstream tasks  (Devlin et al., 2019; , including Machine Translation (MT)  (Vaswani et al., 2017; Ott et al., 2018) . One main component of the architecture is the multi-headed attention mechanism that allows the model to capture longrange contextual information. Despite these successes, the impact of the suggested parametrization choices, in particular the self-attention mechanism with its large number of attention heads distributed over several layers, has been the subject of many studies, following roughly four lines of research. The first line of research focuses on the interpretation of the network, in particular on the analysis of attention mechanisms and the interpretability of the weights and connections  (Raganato and Tiedemann, 2018; Tang et al., 2018; Mare?ek and Rosa, 2019; Voita et al., 2019a; Brunner et al., 2020) . A closely related research area attempts to guide the attention mechanism, e.g. by incorporating alignment objectives  (Garg et al., 2019) , or improving the representation through external information such as syntactic supervision  (Pham et al., 2019; Currey and Heafield, 2019; Deguchi et al., 2019) . The third line of research argues that Transformer networks are over-parametrized and learn redundant information that can be pruned in various ways  (Sanh et al., 2019) . For example,  Voita et al. (2019b)  show that a few attention heads do the "heavy lifting" whereas others contribute very little or nothing at all. Similarly,  raise the question whether 16 attention heads are really necessary to obtain competitive performance. Finally, several recent works address the computational challenge of modeling very long sequences and modify the Transformer architecture with attention operations that reduce time complexity  (Shen et al., 2018; Sukhbaatar et al., 2019; Dai et al., 2019; Indurthi et al., 2019; Kitaev et al., 2020) . This study falls into the third category and is motivated by the observation that most self-attention patterns learned by the Transformer architecture merely reflect positional encoding of contextual information  (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Voita et al., 2019a; Correia et al., 2019) . From this standpoint, we argue that most attentive connections in the encoder do not need to be learned at all, but can be replaced by simple predefined patterns. To this end, we design, analyze and experimentally compare intuitive and simple fixed attention patterns. The proposed patterns are solely based on positional information and do not require any learnable parameters nor external knowledge. The fixed patterns reflect the importance of locality and pose the question whether encoder self-attention needs to be learned at all to achieve state-of-the-art results in machine translation. This paper provides the following contributions: ? We propose fixed -non-learnable -attentive patterns that replace the learnable attention matrices in the encoder of the Transformer model. ? We evaluate the proposed fixed patterns on a series of experiments with different language pairs and varying amounts of training data. The results show that fixed self-attention patterns yield consistently competitive results, especially in low-resource scenarios. ? We provide an ablation study to analyze the relative impact of the different fixed attention heads and the effect of keeping one of the eight heads learnable. Moreover, we also study the effect of the number of encoder and decoder layers on translation quality. ? We assess the translation performance of the fixed attention models through various contrastive test suites, focusing on two linguistic phenomena: subject-verb agreement and word sense disambiguation. Our results show that the encoder self-attention in Transformer-based machine translation can be simplified substantially, reducing the parameter footprint without loss of translation quality, and even improving quality in low-resource scenarios. Along with our contributions, we highlight our key findings that give insights for the further development of more lightweight neural networks while retaining state-of-the-art performance for MT: ? The encoder can be substantially simplified with trivial attentive patterns at training time: only preserving adjacent and previous tokens is necessary. ? Encoder attention heads solely based on locality principles may hamper the extraction of global semantic features beneficial for the word sense disambiguation capability of the MT model. Keeping one learnable head in the encoder compensates for degradations, but this trade-off needs to be carefully assessed. ? Position-wise attentive patterns play a key role in low-resource scenarios, both for related (German ? English) and unrelated (Vietnamese ? English) languages. 

 Related work Attention mechanisms in Neural Machine Translation (NMT) were first introduced in combination with Recurrent Neural Networks (RNNs)  (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) , between the encoder and decoder. The Transformer architecture extended the mechanism by introducing the so-called self-attention to replace the RNNs in the encoder and decoder, and by using multiple attention heads  (Vaswani et al., 2017) . This architecture rapidly became the de facto state-of-the-art architecture for NMT, and more recently for language modeling  (Radford et al., 2018)  and other downstream tasks  (Strubell et al., 2018; Devlin et al., 2019; Bosselut et al., 2019) . The Transformer allows the attention for a token to be spread over the entire input sequence, multiple times, intuitively capturing different properties. This characteristic has led to a line of research focusing on the interpretation of Transformer-based networks and their attention mechanisms  (Raganato and Tiedemann, 2018; Tang et al., 2018; Mare?ek and Rosa, 2019; Voita et al., 2019a; Vig and Belinkov, 2019; Clark et al., 2019; Kovaleva et al., 2019; Tenney et al., 2019; Lin et al., 2019; Jawahar et al., 2019; van Schijndel et al., 2019; Hao et al., 2019b; Rogers et al., 2020) . As regards MT, recent work  (Voita et al., 2019b)  suggests that only a few attention heads are specialized towards a specific role, e.g., focusing on a syntactic dependency relation or on rare words, and significantly contribute to the translation performance, while all the others are dispensable. At the same time, recent research has attempted to bring the mathematical formulation of selfattention more in line with the linguistic expectation that attention would be most useful within a narrow local scope, e.g. for the translation of phrases  (Hao et al., 2019a) . For instance,  Shaw et al. (2018)  replace the sinusoidal position encoding in the Transformer with relative position encoding to improve the focus on local positional patterns. Several studies modify the attention formula to bias the attention weights towards local areas  (Yang et al., 2018; Xu et al., 2019; Fonollosa et al., 2019) .  and  use convolutional modules to replace parts of self-attention, making the overall networks computationally more efficient.  Cui et al. (2019)  mask out certain tokens when computing attention, which favors local attention patterns and prevents redun-dancy in the different attention heads. All these contributions have shown the importance of localness, and the possibility to use lightweight convolutional networks to reduce the number of parameters while yielding competitive results . In this respect, our work is orthogonal to previous research: we focus only on the original Transformer architecture and investigate the replacement of learnable encoder self-attention by fixed, non-learnable attentive patterns. Recent analysis papers have identified a certain number of functions to which different selfattention heads tend to specialize.  Voita et al. (2019b)  identifies three types of heads: positional heads point to an adjacent token, syntactic heads point to tokens in a specific syntactic relation, and rare word heads point to the least frequent tokens in a sentence. Correia et al. (  2019 ) identify two additional types of heads: BPE-merging heads spread weight over adjacent tokens that are part of the same BPE cluster or hyphenated words, and interrogation heads point to question marks at the end of the sentence. In line with these findings, we design our fixed attention patterns and train NMT models without the need of learning them. In concurrent work,  You et al. (2020)  propose to replace learnable attention weights in Transformerbased NMT with hard-coded Gaussian distributions. This paper is complementary and differs in several respects: while  You et al. (2020)  consider three fixed patterns across the encoder-decoder architecture, we focus only on the encoder selfattention but present seven fixed patterns that cover additional known properties of self-attention. We study the relative impact of each of them and analyze their performance with respect to different numbers of encoder-decoder layers, and as semantic feature extractor for lexical ambiguity phenomena. Furthermore, in contrast to  You et al. (2020) , we show that our fixed patterns have a clear beneficial effect in low-resource scenarios. 

 Methodology In this section, we briefly describe the Transformer architecture and its self-attention mechanism, and introduce the fixed attention patterns. 

 Self-attention in Transformers The Transformer architecture follows the so-called encoder-decoder paradigm where the source sentence is encoded in a number of stacked encoder blocks, and the target sentence is generated through a number of stacked decoder blocks. Each encoder block consists of a multi-head self-attention layer and a feed-forward layer. For a sequence of token representations H ? R n?d (with sequence length n and dimensionality d), the self-attention model first projects them into queries Q ? R n?d , keys K ? R n?d and values V ? R n?d , using three different linear projections. Then, the attention energy ? i for position i in the sequence is computed by taking the scaled dot product between the query vector Q i and the key matrix K: ? i = softmax Q i K ? d ? R n (1) The attention energy is then used to compute a weighted average of the values V: Att(? i , V) = ? i V ? R d (2) For multi-head attention with h heads, the query, key and value are linearly projected h times to allow the model to jointly attend to information from different representations. The attention vectors of the h heads are then concatenated. Finally, the resulting multi-head attention is fed to a feed-forward network that consists of two linear layers with a ReLU activation in between. This multi-head attention is often called encoder self-attention, as it builds a representation of the input sentence that is attentive to itself. The decoder follows the same architecture as the encoder with multi-head attention mechanisms and feed-forward networks, with two main differences: i) an additional multi-head attention mechanism, called encoder-decoder attention, connects the last encoder layer to the decoder layers, and ii) future positions are prevented from being attended to, by masking, in order to preserve the auto-regressive property of a left-to-right decoder. The base version of the Transformer, the standard setting for MT, uses 6 layers for both encoder and decoder and 8 attention heads in each layer. In this work, we focus on the encoder self-attention and replace the learned attention energy ? by fixed, predefined distributions for all but one head. 

 Fixed self-attention patterns The inspection of encoder self-attention in standard MT models yields the somewhat surprising result that positional patterns, such as "previous token", "next token", or "last token of the sentence", are key features across all layers and remain even after pruning most of the attention heads  (Voita et al., 2019a,b; Correia et al., 2019) . Instead of costly learning these trivial positional patterns using millions of sentences, we choose seven predefined patterns, each of which takes the place of an attention head (see Figure  1 , upper row). Given the i-th word within a sentence of length n, we define the following patterns: 1. the current token: a fixed attention weight of 1.0 at position i, 2. the previous token: a fixed attention weight of 1.0 at position i ? 1, 3. the next token: a fixed attention weight of 1.0 at position i + 1, 4. the larger left-hand context: a function f over the positions 0 to i ? 2, 5. the larger right-hand context: a function f over the positions i + 2 to n, 6. the end of the sentence: a function f over the positions 0 to n, 7. the start of the sentence: a function f over the positions n to 0. For illustration, the attention energies for patterns 2 and 4 are defined formally as follows: ? (2) i,j = 1 if j = i ? 1 0 otherwise ? (4) i,j = f (4) (j) if j ? i ? 2 0 otherwise where f (4) (j) = (j + 1) 3 i?2 j=0 (j + 1) 3 The same function is used for all patterns, changing only the respective start and end points. 1 These predefined attention heads are repeated over all layers of the encoder. The eighth attention head always remains learnable.  2  It is customary in NMT to split words into subword units, and there is evidence that self-attention treats split words differently than non-split ones  (Correia et al., 2019) . Therefore, we propose a second variant of the predefined patterns that assigns the same attention values to all parts of the same word (see lower row of Figure  1 ). 

 Experiments We perform a series of experiments to evaluate the fixed attentive encoder patterns, starting with a standard German ? English translation setup (Section 4.1) and then extending the scope to lowresource and high-resource scenarios (Section 4.2). We use the OpenNMT-py  (Klein et al., 2017)  library for training, the base version of Transformer as hyper-parameters  (Vaswani et al., 2017) , and compare against the reference using sacreBLEU  (Papineni et al., 2002; Post, 2018 ) . 3 

 Results: Standard scenario To assess the general viability of the proposed approach and to quantify the effects of different numbers of encoder and decoder layers, we train models on a mid-sized dataset of 2.9M training sentences from the German ? English WMT19 news translation task  (Barrault et al., 2019)  with paired bootstrap resampling with 1000 resamples  (Koehn, 2004) . coding (BPE) segmentation  (Sennrich et al., 2016)  on the training corpus, using 35 000 merge operations. We train four Transformer models: ? 8L: all 8 attention heads in each layer are learnable, ? 7F token +1L: 7 fixed token-based attention heads and 1 learnable head per encoder layer, ? 7F word +1L: 7 fixed word-based attention patterns and 1 learnable head per encoder layer, ? 1L: a single learnable attention head per encoder layer. Each model is trained in 7 configurations: 6 encoder layers with 6 decoder layers, and 1 to 6 encoder layers coupled to 1 decoder layer. BLEU scores are shown in Table  1 . Results for the most powerful model (6+6) show that the two fixed-attention models are almost indistinguishable from the standard model, whereas the single-head model yields consistently slightly lower results. It could be argued that the 6-layer decoder is powerful enough to compensate for deficiencies due to fixed attention on the encoder side. The 6+1 configuration, which uses a single layer decoder, shows indeed a slight performance drop for German ? English, but no significant difference in the opposite direction. Overall translation quality drops significantly with three and less encoder layers, but the difference between fixed and learnable attention models is statistically insignificant in most cases. The fixed attention models always outperform the model with a single learnable head, which shows that the predefined patterns are indeed helpful. The (simpler) token-based approach seems to outperform the word-based one, but with higher numbers of decoder layers the two variants are statistically equivalent. 

 Results: Low-resource and high-resource scenarios We hypothesize that fixed attentive patterns are especially useful in low-resource scenarios since intuitive properties of self-attention are directly encoded within the model, which may be hard to learn from small training datasets. We empirically test this assumption on four translation tasks: ? German ? English (DE?EN), using the data from the IWSLT 2014 shared task  (Cettolo et al., 2014) . As prior work  (Ranzato et al., 2016; Sennrich and Zhang, 2019) , we report BLEU score on the concatenated dev sets: tst2010, tst2011, tst2012, dev2010, dev2012 (159 000 training sentences, 7 282 for development, and 6 750 for testing). Low-resource scenarios can be sensible to the choice of hyperparameters  (Sennrich and Zhang, 2019) . Hence, we apply three of the most successful adaptations to all our configurations: reduced batch size (4k ? 1k tokens), increased dropout (0.1 ? 0.3), and tied embeddings. Sentences are BPEencoded with 30 000 merge operations, shared between source and target language, but independent for Korean ? English. Results of the 6+6 layer configurations are shown in Table  2 .  5  The models using fixed attention consistently outperform the models using learned attention, by up to 3 BLEU. No clear winner between token-based and word-based fixed attention can be distinguished though. Our English ? Vietnamese models outperform prior work based on an RNN architecture by a large margin, but the German ? English and Korean ? English models remain below the heavily optimized models of  Sennrich and Zhang (2019) . However, we note that our goal is not to beat the state-of-the-art in a given MT setting but rather to show the performance of simple non-learnable attentive patterns across different language pairs and data sizes. Moreover, it is worth to mention that the Korean ? English dataset, being automatically created, includes some noise in the test data that may impact the comparison.  6  Finally, we also evaluate a high-resource scenario for German ? English with 11.5M training sentences.  7  Table  3  shows that the results of the fixed attention models do not degrade even when abundant training data allow all attention heads to be learned accurately. 

 Ablation study We perform an ablation study to assess the contribution of each attention head separately. To this end, we mask out one attention pattern across all encoder layers at test time. Table  4  shows the differences compared to the full model, on the midsized German ? English and on the Vietnamese ? English models, both in the 6+1 and 6+6 layer configurations. We find that heads 2, 3 and 4 (previous word, next word, previous context) are particularly important, whereas the impact of the remaining context heads is small. Head 1 (current word) is not useful in the token-based model, but shows slightly larger numbers in the word-based setting. The most interesting results concern the eighth, learned head. Its impact is significant, but in most cases lower than the three main heads listed above. Interestingly, disabling it causes much lower degradation in the 6+6 configurations, which suggests that a more powerful decoder can compensate for the absence of learned encoder representations. 

 Eight fixed heads The ablation study suggests that it is not crucial to keep one learnable head in the encoder layers, especially if the decoder is deep enough. Here, we assess the extreme scenario where the eighth attention head is fixed as well. The eighth fixed attentive pattern focuses on the last token, with a fixed weight of 1.0 at position n. Table  5  shows the results for the standard English ? German scenario and the low-resource English ? Vietnamese scenario. Overall, the learnable attention head is completely dispensable across both language pairs. As shown in Section 4.3, the impact of having learnable attention heads on the encoder side is negligible. Moreover, we also note that as we replace attention heads with non-learnable ones, our configurations reduce the number of parameters without degrading translation quality. 

 Analysis To further analyze the fixed attentive encoder patterns, we perform three targeted evaluations: i) on the sentence length, ii) on the subject-verb agreement task, and iii) on the Word Sense Disambiguation (WSD) task. The length analysis inspects the translation quality by sentence length. The subjectverb agreement task is commonly used to evaluate long-range dependencies  (Linzen et al., 2016; Tran et al., 2018; Tang et al., 2018) , while the WSD task addresses lexical ambiguity phenomena, i.e., words of the source language that have multiple translations in the target language representing different meanings  (Marvin and Koehn, 2018; Liu <10 [10,20)[20,30)[30,40)[40,50)    Pu et al., 2018; Tang et al., 2019) . For both tasks, we use contrastive test suites  (Sennrich, 2017; Popovi? and Castilho, 2019 ) that rely on the ability of NMT systems to score given translations. Broadly speaking, a sentence containing the linguistic phenomenon of interest is paired with the correct reference translation and with a modified translation with a specific type of error. A contrast is considered successfully detected if the reference translation obtains a higher score than the artificially modified translation. The evaluation metric corresponds to the accuracy over all decisions. We conduct the analyses using the DE-EN models from Section 4.1, i.e., 8L, 7F token +1L, and 7F token (H8 disabled). 

 Sentence length analysis To assess whether our fixed attentive patterns may hamper modeling of global dependencies supporting long sentences, we compute BLEU score by reference sentence length. 8 Despite the small performance gap between models, as we can see from Figure  2 , long sentences benefit from having learnable attentive patterns. This is clearly shown by the 7F token (H8 disabled) model, which is consistently degraded in almost every length bin. 

 Subject-verb agreement The predefined attention patterns focus on relatively small local contexts. It could therefore be argued that the fixed attention models would perform worse on long-distance agreement, and that disabling the learned head in particular would be catastrophic. We test this hypothesis by evaluating the models from Section 4.1 on the subject-verb agreement task of the English-German Lingeval test suite  (Sennrich, 2017) . Figure  3  plots the accuracies by distance between subject and verb. In the 6+6 layer configuration, no difference can be detected between the three examined scenarios. In the 6+1 layer configuration, the fixed-attention model does not seem to suffer from degraded results, whereas disabling the learned head leads to clearly lower results. This drop is due to the expected degradation of general translation quality (cf. Table  4 , ablation study) and is not worse than the degradation observed by disabling one of the fixed local context heads. 

 Word sense disambiguation It has been shown that the encoder of Transformerbased MT models includes semantic information beneficial for WSD  (Tang et al., 2018 (Tang et al., , 2019 . In this respect, a model with predefined fixed patterns may struggle to encode global semantic features. To this end, we evaluate our models on two German-English WSD test suites, ContraWSD (Rios  Gonzales et al., 2017)  and MuCoW  (Raganato et al., 2019) .  9  Table  6  shows the performance of our models on the WSD benchmarks. Overall, the model with 6 decoder layers and fixed attentive patterns  9  As MuCoW is automatically built using various parallel corpora, we discarded those ones included in our training. We only report the average result from the TED  (Cettolo et al., 2013)  and Tatoeba  (Tiedemann, 2012)   (7F token +1L) achieves higher accuracy than the model with all learnable attention heads (8L), while the 1-layer decoder models show the opposite effect. It appears that having 6 decoder layers can effectively cope with WSD despite having only one learnable attention head. Interestingly enough, when we disable the learnable attention head (7F token H8 disabled), performance drops consistently in both test suites, showing that the learnable head plays a key role for WSD, specializing in semantic feature extraction. 

 Conclusion In this work, we propose to simplify encoder selfattention of Transformer-based NMT models by replacing all but one attention heads with fixed positional attentive patterns that require neither training nor external knowledge. We train NMT models on different data sizes and language directions with the proposed fixed patterns, showing that the encoder self-attention can be simplified drastically, reducing parameter footprint at training time without degradation in translation quality. In low-resource scenarios, translation quality is even improved. Our extensive analyses show that i) only adjacent and previous token attentive patterns contribute significantly to the translation performance, ii) the trainable encoder head can also be disabled without hampering translation quality if the number of decoder layers is sufficient, iii) encoder attention heads based on locality patterns are beneficial in low-resource scenarios, but may affect the semantic feature extraction necessary for addressing lexical ambiguity phenomena. Apart from the consistent results given by our simple fixed encoder patterns, this work opens up potential further research for simpler and more efficient neural networks for MT, such as synthetic self-attention patterns  (Tay et al., 2020) . Figure 1 : 1 Figure 1: Token-based (upper row) and word-based (lower row) fixed attention patterns for the example sentence "a master of science fic## tion .". The word-based patterns treat the subwords "fic##" and "tion" as a single token. 
