title
Max-Margin Synchronous Grammar Induction for Machine Translation

abstract
Traditional synchronous grammar induction estimates parameters by maximizing likelihood, which only has a loose relation to translation quality. Alternatively, we propose a max-margin estimation approach to discriminatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU. In the max-margin estimation of parameters, we only need to calculate Viterbi translations. This further facilitates the incorporation of various non-local features that are defined on the target side. We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system. Experiments show that our max-margin method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.

Introduction Synchronous grammar induction, which refers to the process of learning translation rules from bilingual corpus, still remains an open problem in statistical machine translation (SMT). Although stateof-the-art SMT systems model the translation process based on synchronous grammars (including bilingual phrases), most of them still learn translation rules via a pipeline with word-based heuristics  (Koehn et al., 2003) . This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline * Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models  (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012)  or discriminative models  (Xiao et al., 2012) . Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood, 1 which only has a loose relation to translation quality  (He and Deng, 2012) . Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by  Xiao et al. (2012)  only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation. Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translation quality measured by BLEU  (Papineni et al., 2002) , and is also able to incorporate non-local features from target parse trees. We thus propose a max-margin estimation method to discriminatively induce synchronous grammar directly from sentence pairs without word alignments. We try to maximize the margin between a reference translation and a candidate translation with translation errors that are measured by BLEU. The more serious the translation errors, the larger the margin. In this way, our max-margin method is able to learn synchronous grammars according to their translation performance. We further incorporate various nonlocal features defined on target parse trees. We efficiently calculate the non-local feature values of a translation over its exponential derivation space using the inside-outside algorithm. Because our maxmargin estimation optimizes feature weights only by the feature values of Viterbi and reference translations, we are able to efficiently perform optimization even with non-local features. We apply the proposed max-margin estimation method to learn synchronous grammars for a hierarchical phrase-based translation system  (Chiang, 2007)  which typically produces state-of-the-art performance. With non-local features defined on target parse trees, our max-margin method significantly outperforms the baseline that uses synchronous rules learned from the traditional pipeline by 1.3 BLEU points on large-scale Chinese-English bilingual training data. The remainder of this paper is organized as follows. Section 2 presents the discriminative synchronous grammar induction model with the nonlocal features. In Section 3, we elaborate our maxmargin estimation method which is able to directly optimize BLEU, and discuss how we induce grammar rules. Local and non-local features are described in Section 4. Finally, in Section 5, we verify the effectiveness of our method through experiments by comparing it against both the traditional pipeline and max-likelihood estimation method. 

 Discriminative Model with Non-local Features Let S denotes the set of all strings in a source language. Given a source sentence s ? S, T (s) denotes all candidate translations in the target language that can be generated by a synchronous grammar G. A translation t ? T (s) is generated by a sequence of translation steps (r 1 , ..., r n ), where we apply a syn- 1 0 2 3 4 5 [1,3] [1,5] [0,5] [1,6] [0,6] [4,6] 1 0 2 3 4 5 6 bushi yu shalong juxing huitan r 1 : ? yu shalong ? with Sharon ? r 2 : ? X juxing huitan ? held a talk X ? r 3 : ? bushi X ? Bush X ? Figure  1 : A derivation of a sentence pair represented by a synchronous tree. The above and below part are the parses in the source language side and the target language side respectively. Left subscript of a node X denotes the source span, while right subscript denotes the target span. A dashed line denotes an alignment from a source span to a target span. The annotation for a dashed line corresponds to the rewriting rule used in the corresponding step of the derivation. chronous rule r ? G in one step. We refer to such a sequence of translation steps as a derivation (See Figure  1 ) and denote it as d ? D(s), where D(s) represents the derivation space of a source sentence. Given an input source sentence s, we output a pair ?t, d? in SMT. Thus, we study the triple ?s, t, d? in SMT. In our discriminative model, we calculate the value of a triple ?s, t, d? according to the following scoring function: f (s, t, d) = ? T ?(s, t, d) (1) where ? ? ? is a feature weight vector, and ? is the feature function. There are exponential outputs in SMT. Therefore it is necessary to factorize the feature function in order to perform efficient calculation over the SMT output space using dynamic programming. We decompose the feature function of a triple ?s, t, d? into (2) Our feature functions include both local and nonlocal features. A feature is a local feature if and only if it can be factored among the translation steps in a derivation. In other words, the value of a local feature for ?s, t, d? can be calculated as a sum of local scores in each translation step, and the calculation of each local score only requires to look at the rule used in corresponding step and the input sentence. Otherwise, the feature is a non-local feature. Our discriminative model allows to incorporate nonlocal features that are defined on target translations. For example, a rule feature in Figure  2 (a), which indicates the application of a specific rule in a derivation, is a local feature. A source span boundary feature in Figure  2 (b) that is defined on the source parse tree is also a local feature. However, a target span boundary feature in Figure  2(c) , which assesses the target parse structure, is a non-local feature. According to Figure  1 , the span is parsed in step r 2 , but it also depends on the translation boundary word "held" generated in previous step r 1 . We will describe the details of both local and non-local features that we use in Section 4. Non-local features enable us to model the target parse structure in a derivation. However, it is computationally expensive to calculate the expected values of non-local features over D(s), as non-local features require to record states of target boundary s, S, S s is a sentence in a source language; S means source training sentences; S denotes all the possible sentences; t, T, T symbols for the target language that similar to s, S, S; d, D derivation and derivation space; D(s) space of derivations for a source sentence; D(s, t) space of derivations for a source sentence with its translation; H(s) hypergraph that represents D(s); H(s, t) hypergraph that represents D(s, t); words and result in an extremely large number of states during dynamic programming. Fortunately, when integrating out derivations over the derivation space D(s, t) of a source sentence and its translation, we can efficiently calculate the non-local features. Because all derivations in D(s, t) share the same translation, there is no need to maintain states for target boundary words. We will discuss this computational problem in details in Section 3.3. In the proposed max-margin estimation described in next section, we only need to integrate out derivation for a Viterbi translation and a reference translation when updating feature weights. Therefore, the defined non-local features allow us to not only explore useful knowledge on the target parse trees, but also compute them efficiently over D(s, t) during maxmargin estimation. 

 Max-Margin Estimation In this section, we describe how we use a parallel training corpus {S, T} = {(s  (i)  , t  (i)  )} N i=1 to estimate feature weights ?, which contain parameters of the induced synchronous grammars and the defined non-local features. We choose the parameters that maximize the translation quality measured by BLEU using the max-margin estimation  (Taskar et al., 2004) . Margin refers to the difference of the model score between a reference translation t  (i)  and a candidate translation t. We hope that the worse the translation quality of t, the larger the margin between t and t  (i)  . In this way, we penalize larger translation errors more severely than smaller ones. This intuition is expressed by the following equation. min 1 2 ? 2 (3) s.t. f (s (i) , t (i) ) ? f (s (i) , t) ? cost(t (i) , t) ?t ? T (s (i) ) Here, f (s, t) is the feature function of a translation, and cost function cost(t  (i)  , t) measures the translation errors of a candidate translation t comparing with a reference translation t  (i)  . We define the cost function via the widely-used translation evaluation metric BLEU. We use the smoothed sentence level BLEU-4  (Lin and Och, 2004)  here: cost(t (i) , t) = 1 ? BLEU-4(t (i) , t) (4) In Section 3.1, we will discuss how we use the scoring function f (s, t, d) to calculate f (s, t). Then in Section 3.2, we recast the equation (  3 ) as an unconstrained empirical loss minimization problem, and describe the learning algorithm for optimizing ? and inducing G. Finally, we give the details of inference for the learning algorithm in Section 3.3. 

 Integrate Out Derivation by Averaging Although we only model the triple ?s, t, d? in the equation (1), it's necessary to calculate the scoring function f (s, t) of a translation by integrating out the variable of derivation as derivation is not observed in the training data. We use an averaging computation over all possible derivations of a translation D(s, t). We call this an average derivation based estimation: f (s, t) = 1 |D(s, t)| ? d?D(s,t) f (s, t, d) (5) The "average derivation" can be considered as the geometric central point in the space D(s, t). Another possible way to deal with the latent derivation is max-derivation, which uses the maxoperator over D(s, t). The max derivation method sets f (s, t) as max d?D(s,t) f (s, t, d). It is often adopted in traditional SMT systems. Nevertheless, we instead use average-derivation for two reasons.  2  First, as a translation has an exponential number of derivations, finding the max derivation of a reference translation for learning is nontrivial  (Chiang et al., 2009) . Second, the max derivation estimation will result in a low rule coverage, as rules in a max derivation only covers a small fraction of rules in the D(s, t). Because rule coverage is important in synchronous grammar induction, we would like to explore the entire derivation space using the average operator. 

 Learning Algorithm We reformulate the equation (  3 ) as an unconstrained empirical loss minimization problem as follows: min ? 2 ? 2 + 1 N N ? n=1 L(s (i) , t (i) , ?) (6) Where ? denotes the regularization strength for L2-norm. The loss function of a sentence pair L(s  (i)  , t  (i)  , ?) is a convex hinge loss function denoted by: max{0, ?f (s (i) , t (i) ) (7) + max t?T (s (i) ) ( f (s (i) , t) + cost(t (i) , t) ) } According to the second max-operator in the hinge loss function, the optimization towards BLEU is expressed by cost-augmented inference. Costaugmented inference finds a translation that has a maximum model score augmented with cost. t = max t?T (s (i) ) ( f (s (i) , t) + cost(t (i) , t) ) (8) We applied the Pegasos algorithm for the optimization of equation (  6 )  (Shalev-Shwartz et al., 2007) . This is an online algorithm, which alternates between stochastic gradient descent steps and projection steps. When the loss function is non-zero, it updates weights according to the sub-gradient of the hinge loss function. Using the average scoring function in the equation (  5 ), the sub-gradient of hinge loss function for a sentence pair is the difference of average feature values between a Viterbi translation Algorithm 1 UPDATE(s, t, ?, G) ? One step in online algorithm. s, t are short for s  (i)  , t  (i)   and a reference translation. ?L ? = 1 |D(s (i) , t (i) )| ? d?D(s (i) ,t (i) ) ?(s (i) , t (i) , d) ? 1 |D(s (i) , t)| ? d?D(s (i) , t) ?(s (i) , t, d) (9) Algorithm 1 shows the procedure of one step in the online optimization algorithm. The procedure discovers rules and updates weights in an online fashion. In the procedure, we first biparse the sentence pair to construct a synchronous hypergraph of a reference translation (line 1). In the biparsing algorithm, synchronous rules for constructing hyperedges are not required to be in G, but can be any rules that follow the form defined in Chiang (2007). Thus, the biparsing algorithm can discover new rules that are not in G. Then we collect the translation rules discovered in the hypergraph of the reference translation (line 2), which are rules indicated by hyperedges in the hypergraph. We then calculate the Viterbi translation according to the scoring function and cost function (see Section 3.3) (line 3), and build the synchronous hypergraph for the Viterbi translation (line 4). Finally, we update weights according to the Pegasos algorithm (line 5). The sub-gradient is calculated based on the hypergraph of Viterbi translation and reference translation. In practice, in order to process the data in a parallel manner, we use a larger step size of 1000 for the learning algorithm. In each step of our online optimization algorithm, we first biparse 1000 reference sentence pairs in parallel. Then, we collect grammar rules from the generated reference hypergraphs. After that, we compute the gradients of 1000 sentence pairs in parallel, by calculating feature weights over reference hypergraphs and Viterbi hypergraphs. Fi-nally, we update the feature weights using the sum of these gradients. 

 Inference There are two parts that need to be calculated in the learning algorithm: finding a cost-augmented Viterbi translation according to the scoring function and cost function (Equation  8 ), and constructing synchronous hypergraphs for the Viterbi and reference translation so as to discover rules and calculate average feature values in Equation (  9 ). Following the traditional decoding procedure, we resort to the cube-pruning based algorithm for approximation. To find the Viterbi translation, we run the traditional translation decoding algorithm  (Chiang, 2007)  to get the best derivation. Then we use the translation yielded by the best derivation as the Viterbi translation. In order to obtain the BLEU score in the cost function, we need to calculate the ngram precision. It is calculated in a way similar to the calculation of the ngram language model. The computation of BLEU-4 requires to record 3 boundary words in both the left and right side during dynamic programming. Therefore, even when we use a language model whose order is less than 4, we still expands the states to record 3 boundary words so as to calculate the cost measured by BLEU. We build synchronous hypergraphs using the cube-pruning based biparsing algorithm  (Xiao et al., 2012) . Algorithm 2 shows the procedure. Using a chart, the biparsing algorithm constructs k-best alignments for every source word (lines 1-5) and kbest hyperedges for every source span (lines 6-13) from the bottom up. Thus, a synchronous hypergraph is generated during the construction of the chart. More specifically, for a source span, it first creates cubes L for all source parses ? that are in-Algorithm 2 BIPARSE(s, t, ?) ?  (Xiao et al., 2012)   for all i, j s.t. j ? i = h do 9: L ? ? 10: for ? inferable from chart do 11: L ? L + ?chart[? 1 ], ..., chart[? |?| ]? 12: chart[X, i, j] ? KBEST(L,?,?) 13: H ? H + chart[X, i, j] ? save hyperedges 14: return H ferable from the chart (lines 9-11). Here ? i is a partial source parse that covers either a single source word or a span of source words. Then it uses the cube pruning algorithm to keep the top k derivations among all partial derivations that share the same source span [i, j] (line 12). Notably, this biparsing algorithm does not require specific translation rules as input. Instead, it is able to discover new synchronous grammar rules when constructing a synchronous hypergraph: extracting each hyperedge in the hypergraph as a synchronous rule. Based on the biparsing algorithm, we are able to construct the reference hypergraph H(s  (i)  , t  (i)  ) and Viterbi hypergraph H(s (i) , t). By the reference hypergraph, we collect new synchronous translation rules and record them in the grammar G. We also calculate the average feature values of hypergraphs using the inside-outside algorithm  (Li et al., 2009) , so as to compute the gradients. 

 Features One advantage of the discriminative method is that it enables us to incorporate arbitrary features. As shown in Section 2, our model incorporates both local and non-local features. 

 Local Features Rule features We associate each rule with an indicator feature. Each indicator feature counts the number of times that a rule appears in a derivation. In this way, we are able to learn a weight for every rule according to the entire structure of sentence. Word association features Lexicalized features are widely used in traditional SMT systems. Here we adopt two lexical weights called noisy-or features  (Zens and Ney, 2004) . The noisy-or feature is estimated by word translation probabilities output by GIZA++. We set the initial weight of these two lexical scores with equivalent positive values. The lexical weights enable our system to score and rank the hyperedges at the beginning. Although word alignment features are used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. 

 Source span boundary features We use this kind of feature to assess the source parse tree in a derivation. Previous work  (Xiong et al., 2010)  has shown the importance of phrase boundary features for translation. Actually, this kind of feature is a good cue for deciding the boundary where a rule is to be learnt. Following  Taskar et al. (2004) , for a bispan  [i, j, k, l]  in a derivation, we define the feature templates that indicates the boundaries of a span by its beginning and end words: {B : s i+1 ; E : s j ; BE : s i+1 , s j }. Source span orientation features Orientation features are only used for those spans that are swapping. In Figure  1 , the translation of source span [1, 3] is swapping with that of span [4, 5] by r 2 , thus orientation feature for span [1, 3] is activated. We also define three feature templates for a swapping span similar to the boundary features: {B : s i+1 ; E : s j ; BE : s i+1 , s j }. In practice, we add a prefix to the orientation features so as to distinguish these features from the boundary features. 

 Non-local Features Target span boundary features We also want to assess the target tree structure in a derivation. We define these features in a way similar to source span boundary features. For a bispan  [i, j, k, l]   Baseline is an in-house implementation of hierarchical phrase based system. Moses denotes the implementation of hierarchical phrased-model in Moses  (Koehn et al., 2007) . +Sparsef eature means that those sparse features used in the grammar induction are also used during decoding. The improvement of maxmargin over Baseline is statistically significant (p < 0.01). target span boundary as: {B : t k+1 ; E : t l ; BE : t k+1 , t l }. Target span orientation features Similar target orientation features are used for a swapping span [i, j, k, l] with feature templates {B : t k+1 ; E : t l ; BE : t k+1 , t l }. Relative position features Following Blunsom and  Cohn (Blunsom and Cohn, 2006) , we integrate features indicating the closeness to the alignment matrix diagonal. For an aligned word pair with source position i and target position j, the value of this feature is | i |s| ? j |t| |. As this feature depends on the length of the target sentence, it is a non-local feature. Language model We also incorporate an ngram language model which is an important component in SMT. For efficiency, we use a 3-gram language model trained on the target side of our training data during the induction of synchronous grammars. 

 Experiment In this section, we present our experiments on the NIST Chinese-to-English translation tasks. We first compare our max-margin based method with the traditional pipeline on a large bitext which contains 1.1 million sentences. We then present a detailed comparison on a smaller dataset, in order to analyze the effectiveness of max-margin estimation comparing with the max likelihood estimation  (Xiao et al., 2012) , and also the effectiveness of the non-local features that are defined on the target side. 

 Setup The baseline system is the hierarchical phrase based system  (Chiang, 2007) . We used a bilingual corpus that contains 1.1M sentences (44.6 million words) of up to length 40 from the LDC data.  3  Our 5-gram language model was trained by SRILM toolkit  (Stolcke, 2002) . The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST BLEU-4  (Papineni et al., 2002)  is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling  (Koehn, 2004) . We used minimum error rate training (MERT)  (Och, 2003)  to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted rules, count of glue rules, length of translation, and language model. For the lexical weights we used the noisy-or in all configurations including the baseline system. For the discriminative grammar induction, rule translation probabilities were calculated using the expectations of rules in the synchronous hypergraphs of sentence pairs. As our max-margin synchronous grammar induction is trained on the entire bitext, it is necessary to load all the rules into the memory during training. To control the size of rule pruning  (Huang, 2008)  when collecting rules as shown in line 2 of optimization procedure in Section 3.2. Furthermore, we aggressively discarded those large rules (The number of source symbols or the number of target symbols are more than two) that occur only in one sentence. Whenever the learning algorithm processes 50K sentences, we performed this discarding operation for large rules. 

 Result on Large Dataset Table  2  shows the translation results. Our method induces 59.4 million synchronous rules, which are 76.3% of the grammar size of baseline. Note that Moses allows the boundary words of a phrase to be unaligned, while our baseline constraints the initial phrase to be tightly consistent with word alignment. Therefore, Moses extract a much larger rule table than that of our baseline. With fewer translation rules, our method obtains an average improvement of +0.96 BLEU points on the three test sets over the Baseline. As the difference between the baseline and our max-margin synchronous grammar induction model only lies in the grammar, this result clearly denotes that our learnt grammar does outperform the grammar extracted by the traditional two-step pipeline. We also incorporate the sparse features during decoding in a way similar to  Xiao et al. (2012)  and  Dyer et al. (2011) . In order to optimize these sparse features with the dense features by MERT, we group features of the same type into one coarse "summary feature", and get three such features including: rule, phrase-boundary and phrase orientation features. In this way, we rescale the weights of the three "summary features" with the 8 dense features by MERT. We achieve a further improvement of +0.37 BLEU points. Therefore, our training algorithm is able to learn the useful information encoded by the sparse features for translation. 

 Comparison of Estimation Objective and Non-Local Feature We want to investigate whether the max-margin estimation is able to outperform the max-likelihood estimation method  (Xiao et al., 2012) . Therefore we carried out experiments to compare them directly. As the max-margin method is able to use non-local features, we compare two settings of features for the max-margin method. One uses only local features, the other uses both local and non-local features. Because the training procedure need to run on the entire corpus, which is time consuming, we therefore use a smaller corpus containing 50K sentences from the entire bitext for comparison. Table  3  shows the results. When using only local features, the max-margin method consistently outperforms the max-likelihood method in all three test sets. This clearly shows the advantage of learning grammars by optimizing BLEU over likelihood. When incorporating the non-local features into the max-margin method, we achieve further improvement against the max-margin method without non-local features. With non-local features, our max-margin estimation method outperforms the baseline by 1.5 BLEU points, and is better than the max-likelihood estimation by 0.5 BLEU points. Based on these results, we believe that non-local features, which encode information from target parse structures, are helpful for grammar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 

 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures  (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011) . Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours  (Blunsom et al., 2008; K?ri?inen, 2009; He and Deng, 2012) , or incorporating various features using labeled word aligned bilingual data  (Huang and Xiang, 2010) . However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training  (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012)  that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 

 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by BLEU. Max-margin estimation also provides us a convenient way to incorporate non-local features. Experiment results validate the effectiveness of optimizing parameters by BLEU, and the importance of incorporating non-local features defined on the target language. These results confirm the advantage of our max-margin estimation framework as it can both optimize BLEU and incorporate non-local features. Feature engineering is very important for discriminative models. Researchers have proposed various types of features for machine translation, which are often estimated from word alignments. We would like to investigate whether further improvement can be achieved by incorporating such features, especially the context model  (Shen et al., 2009)  in the future. Because our proposed model is quite general, we are also interested in applying this method to induce linguistically motivated synchronous grammars for syntax-based SMT. Figure 2 : 2 Figure 2: Example features for the derivation in Figure 1. Shaded nodes denote information encoded in the feature. 
