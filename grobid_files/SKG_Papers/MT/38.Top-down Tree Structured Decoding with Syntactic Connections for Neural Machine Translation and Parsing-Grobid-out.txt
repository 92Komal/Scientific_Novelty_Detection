title
Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing

abstract
The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017)  to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.

Introduction Neural machine translation (NMT) models were initially proposed as extensions of sequential neural language models  (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015)  or convolutions over n-grams in the decoder  (Kalchbrenner and Blunsom, 2013) . Early methods for discriminative training of machine translation models showed that the loss functions for translation were not sensitive to the production of certain important words such as verbs, without which the output sentence might be uninterpretable by humans. A good solution was to penalise such bad outputs using tree structures which get very low scores if important words like verbs are missing  (Chiang, 2005; Zollmann and Venugopal, 2006; Galley et al., 2006) . To this end, there has been a push to incorporate some syntax into NMT models:  Sennrich and Haddow (2016)  incorporate POS tags and dependency information from the source side of a translation pair in NMT models.  Stahlberg et al. (2016)  use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules  (Chiang, 2005) .  Eriguchi et al. (2016)  and  Bastings et al. (2017)  use tree-structured encoders to exploit source language syntax.  Aharoni and Goldberg (2017)  take the approach of serialising the parse trees to use in a sequential decoder.  Eriguchi et al. (2017)  propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs  (Dyer et al., 2015 (Dyer et al., , 2016  to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in  Eriguchi et al. (2017)  which uses RNNG, are bottom-up tree structured decoders. In contrast, we use a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by  Alvarez-Melis and Jaakkola (2017)  to model structural syntactic information for NMT. We call our novel NMT model Seq2DRNN, using DRNNs as a treestructured decoder combined with a sequential encoder and a novel syntax-aware attention model. All the previous work in syntax-aware NMT mentioned above has focused on dependency parsing as the syntactic model. In contrast, we wish to pursue phrase structure (aka constituency) based syntax-based NMT. We provide some analysis that shows that constituency information can help recover information in NMT decoding. We perform extensive experiments comparing our model against other state-of-the-art sequence to sequence and syntax-aware NMT models and show that our model can improve translation quality and reordering quality. The model performs translation and constituency parsing simultaneously so we also compare our parsing accuracy to other neural parsing models. 

 Model Description In this paper, source sentence will be written as f = x 1 , x 2 , ..., x n , target sentence as e = y 1 , y 2 , ..., y m where y j is a word. Additionally, p k represents a non-terminal symbol (constituent, phrase) in the target sentence constituency tree. [v; u] stands for concatenation of vectors v and u. W(x) is the word embedding of word x. The design of our NMT system follows the encoder-decoder model (also known as a sequence to sequence model) proposed by  Cho et al. (2014)  and  Sutskever et al. (2014) . Our system uses a standard bidirectional gated RNN (BiLSTM or bidirectional Long Short-Term Memory)  (Huang et al., 2015)  as the encoder and our proposed treestructured RNN as the decoder. 

 Sequence to Sequence NMT (Seq2Seq) Neural machine translation models generally consist of an encoder, a decoder and an attention model  (Luong et al., 2015; . The encoder is used to produce hidden representations of the source sentence, which is fed into the decoder along with the attention information to produce the translation output sequence. A common approach is to use bidirectional LSTMs as encoder, produce forward hidden states ? ? h i and backward hidden states ? ? h i , and the final representation h enc i is the concatenation of both: ? ? h i = ? RNN enc ( ? ? h i?1 , W x (x i )) ? ? h i = ? RNN enc ( ? ? h i+1 , W x (x i )) h enc i = [ ? ? h i ; ? ? h i ] (1) The decoder takes the output of the encoder and generates a sequence in target language. The attention mechanism provides additional context vectors c j which is a weighted average contribution of each h i source side encoding. h dec j = RNN dec (h dec j?1 , [o j?1 ; c j ]) o j = softmax(Uh dec j + b) (2) Here, U is the readout matrix and b is the bias vector. o j is the output word embedding. 

 NMT with a Tree-Structured Decoder (Seq2DRNN) The output translation from a translation system should convey the same meaning as the input. This includes the correct word choices but also the right information structure. Sentence structure can be viewed as starting with an action or state (described via verbs or other predicates) and the entities or propositions involved in that activity or state (usually described via arguments to verbs).  

 Doubly-Recurrent Neural Network The Doubly-Recurrent Neural Network model  (Alvarez-Melis and Jaakkola, 2017)  takes a vector representation as input and generates a tree.  Alvarez-Melis and Jaakkola (2017)  show that the DRNN model can effectively reconstruct trees but they do not use DRNNs within a full-scale NMT system. We also use DRNNs for phrasestructure (aka constituency) tree structures rather than dependency trees as in previous work. DRNN decoding proceeds top-down; the generation of nodes at depth d depend solely on the state of nodes at depth < d. Unlike previous work in tree-structured decoding for NMT by  Dyer et al. (2016)  and  Eriguchi et al. (2017) , the output sentence generation is not done in sequence, where the target word y j is generated after all y <j are generated. DRNN first predicts the structure of the sentence and then expands each component to predict words. When generating y j , information regarding the structure of words from 1 to j ? 1 and j + 1 to m can be used to aid prediction of y j . A DRNN consists of two recurrent neural network units, which separately process ancestral and fraternal information about nodes in the tree. Assuming a node is v, its immediate parent node is P (v) and its closest sibling on the left side (appears in the target language sequence just before v) is S(v). The label of node v is z v . Then the ancestral hidden representation h a and fraternal representation h f of a node are calculated with Equation 3. h a v = RNN a dec (h a P (v) , z P (v) ) h f v = RNN f dec (h f S(v) , z S(v) ) (3) h a v and h f v are then combined to produce the hidden state of node v for prediction (predictive hidden state h v , Equation  4 ), which is used to predict the labels of node v. h v = tanh(U f h f v + U a h a v ) (4 ) During the label prediction, DRNN first makes topological decisions: whether (i) the current node is a leaf node (node with no children, ? v ); then (ii) whether the current node has siblings on its righthand side (? v ). Both predictions are done using sigmoid activations: o a v = ?(u a h v ) ? v = 1 if o a v is activated. (5) o f v = ?(u f h v ) ? v = 1 if o f v is activated. (6) Then, label representation o v is predicted using ? v and ? v , and the predictive hidden state h v : o v = softmax(U o h v + ? v u a + ? v u f ) (7) At inference time each node at the same depth is expanded independently, therefore the whole process can be parallelised. This parallelism advantage is not observed in any of the sequential decoders that generate output sequence strictly from left to right nor from right to left ( ?4 has more discussion). 

 Parsing and Translating with DRNN A DRNN is capable of producing a tree structure with labels given an input vector representation. If we train the DRNN to produce parse trees from the output of an encoder RNN, this system will be able to translate and parse at the same time.  (Alvarez-Melis and Jaakkola, 2017)  in their paper provided a proof-of-concept NMT experiment using dependency trees. Instead of an single RNN unit to process fraternal information they had multiple for modelling the fraternal information on syntactic tree structure because dependency parse The model itself also disregards the sequentiality of natural language, and lack attention mechanisms to make it work in exchange for a strict top-bottom decoding procedure. We use constituency parse trees to represent sentences in the target language (Figure  1 ) because constituency or phrase-structure trees are more amenable to top-down derivation compared to dependency trees. It is also easier to model for DRNN, and presumably more capable at handling unknown words which is common in NMT systems with limited vocabulary size. Each node on the tree represents either a terminal symbol (a word) or a non-terminal symbol (a clause or phrase type). The sub-tree dominated by a non-terminal node is the clause or phrase identified with this non-terminal node label. A conventional bidirectional RNN (BiLSTM) encoder  (Cho et al., 2014; Sutskever et al., 2014)  is used to produce hidden states for the decoder (see Figure  2 ). We use breadth-first search to implement the Seq2DRNN decoder. Two queues are used here: current queue which is the queue containing all of the nodes on the currently being processed depth, and next queue with nodes on the next depth (Algorithm 1 has all the details). The decoding process starts from top to bottom, from root to its children, then to its grandchildren, and so on until the leaf nodes which are the output words. In our implementation, sentence clauses (S nodes) are generated as the children of the root node to generalise over sentence types and in case there are multiple sentences in a single translation pair. Initially, the current queue will only have one entry: the root node, which is initialised with the hidden representation of the source sentence. Each node in the current queue is expanded in the following manner: first generate all of its siblings and add them to the current queue, and if any node happens to be non-terminal, generate its first child and add it to the next queue. After the current queue is empty, make next the new current queue and start working on nodes at the next depth. For training, we use back-propagation through trees using the approach in  Goller and K?chler (1996) . In the forward pass, the source sentence is encoded into a hidden representation and fed into the decoder. The decoder generates the tree, predicts the labels of every node from root to leaves. Then in the backward pass, gradients are calculated and used to update the parameters. The loss calculation includes losses in topological predictions: o a v and o f v (Equations 5 and 6) and label predictions: o v (Equation  7 ). Loss(e) = v Loss label (o v , ?v )+ ? v Loss topo (o a v , ?a v )+ ? v Loss topo (o f v , ?f v ) (8) Here ? is a hyper-parameter. 

 Attention Mechanism Attention mechanisms usually work by adding an additional context vector during label prediction. We use a variation of an existing attention mechanism proposed by  Luong et al. (2015) . In our attention model, we produce a context vector c v for every node v by looking at all hidden states produced by the encoder h enc i , then calculating the weights and adding up the weighted hidden states. weight v,i = V a tanh(W a h v + U a h enc i ) ? R (9) c v = n i=1 ?eight v,i h enc i (10) After the calculation in Equation  9 , the weights are normalised with a softmax function before be- goto loop. both queues should be empty now ing used to calculate the context vector. The attention module allows the generation of labels to pay more attention to specific token representations of words in the input sentence. 

 SynC: Syntactic Connections for Language Generation (Seq2DRNN+SynC) A conventional Seq2Seq model uses an RNN language model  (Cho et al., 2014; Sutskever et al., 2014)  conditioned on the input representation produced by the encoder to generate the output one word at a time (Equation  11 ). The prediction of a word y j is directly conditioned on previously generated words where c j is the context vector. P(e) = j P(y j |y <j , c j ) (11) The problem with this word-level language model is that it treats a sentence as a plain sequence of symbols regardless of its syntactic construction. Sentences may contain multiple subordinate clauses and their boundaries are not wellmodelled by sequential language models. We propose a new method to connect the hidden units in the Seq2DRNN decoder that pays attention to contextual tree relationships. The prediction of the representation of a word or a constituent z j (if a constituent then p j , if a word then y j ) is defined as follows: P(z j | y <j , c j ) = P(z j | y <j , y k (?k, z j ? p k ? precedes(y k , z j )) | c j ) (12) The generation of the representation of a word/constituent z j , which is part of the clause  that contain it (z j ? p k ), with clauses before which (precedes(p k , z j )), is conditioned on the following information: i) Word-level: previously generated words y <j ; ii) Ancestral Clause: the clauses that contain the current word p k , i.e. (?kz j ? p k ); iii) Fraternal Clause: the clauses that precede the current clause p k , i.e. (?k precedes(p k , z j )). In practice, the generation of a node looks at the following representations: 1. Word-level: an RNN unit that produces the representation of previous words as a sequence y <j ; 2. Ancestral: treating the ancestors of the current node as a sequence (from root to the immediate parent), the representation of that sequence: p k (?k, z j ? p k ); 3. Fraternal: treating the previous siblings of the current node as well as the previous siblings of its parent node and so on as a sequence, the representation of that sequence: p k (?k, precedes(p k , z j )). SynC creates connections in the tree-structured decoder that pays attention to the structural context of generation of each terminal or non-terminal symbol in the phrase structure tree. For example in English, it is common for verb phrases to follow a noun phrase. But that noun phrase could itself be a subordinate clause with its own verb phrases. In this case, our goal is to explicitly model the fact that the previous phrase is a noun phrase instead of just the entire sequence of words. SynC can be easily incorporated in the proposed Seq2DRNN model (Seq2DRNN+SynC). In addition to the fraternal RNN unit that focuses on preceding sibling nodes, and the ancestral DRNN unit that focus on parent nodes, a node would also look at its parent's previous sibling state (the hidden in Andrei when starving likes cheese, the prediction will be made knowing that the preceding clause is a noun phrase. vector representation of preceding clauses from the very beginning of the sentence). When a nonterminal symbol is expanded into a sub-tree, it's first child will not have a previous sibling to provide fraternal information (S(v) = Null, as in Equ 3). In this case, SynC establishes connection between its first child and its parent's fraternal information provider for such fraternal RNN state (S(v) = S(P (v))). h f v = ? ? ? ? ? ? ? S(v) = Null, RNN f dec (h f S(v) , z S(v) ) S(v) = Null, S(v) := S(P (v)), RNN f dec (h f S(v) , z S(v) ) (13) An example is shown in Figure  4 . In this case, a word-level language model will regard starving as the previous word, which is less helpful for the prediction of a verb phrase likes cheese. 

 Experiments 

 Model Training Experiments in this paper utilise constituency trees on the target side, these trees are obtained by using the Stanford Lexical Parser  (Klein and Manning, 2003a)  which we chose for its speed and accuracy prior to training. This procedure of pre-parsing data is not required at test time, our NMT system would take a sentence as input and produces the translation in target language along with its constituency tree as output. We use the German-English dataset from IWSLT2017 1 for our experiments, and tst2010-2015 as the test set (Table  1 ). To compare with other decoders that utilise target-side syntactic information, we also evaluate on three more datasets from News Commentary v8 using newstest2016 as testset (Table  2 ). We replace all rarely occurring words with   

 Modelling details The implementation of all models in this paper is done using DyNet  (Neubig et al., 2017a)  with Autobatching  (Neubig et al., 2017b) . We use Long Short-Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  as RNN units. Each LSTM unit has 2 layers, with input and hidden dimension of 256. We use a minibatches of 64 samples. We use early stopping mechanism for all experiments and Adam optimiser (Kingma and Ba, 2015) as trainer. Note that this configuration is significantly smaller in both dimension and batch size than those presented in IWSLT2017 due to hardware limitations. All experiments are carried out on a single GTX 1080 Ti GPU with 11GB of VRAM. 

 Results Table  3  and Table  4  has the BLEU  (Papineni et al., 2002)  and RIBES  (Isozaki et al., 2010)  scores. In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output. The Seq2DRNN+SynC model performs better than the Seq2DRNN model. Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities than the baseline Seq2Seq model on the test data. In our News Commentary v8 tests, the same relative performance from Seq2DRNN(SynC) can be observed. The Seq2DRNN+SynC model is also able to out-perform the Str2Tree model proposed by  Aharoni and Goldberg (2017)  and NMT+RNNG by  Eriguchi et al. (2017)  in most cases. Note that Eriguchi et al. (  2017 ) used dependency information instead of constituency information as presented in our work and Aharoni and Goldberg (2017)'s work. Table  5  shows an example translation from all of the models we use in our experiments. Seq2Seq is able to translate with the correct vocabulary, but the sentences are often syntactically awkward. As the sentence length increases the syntactic fluency of Seq2Seq gets worse. Seq2DRNN is able to produce more syntactically fluent sentences since each lowest sub-clause contains typically ? 5 words. Seq2DRNN+SynC produces the best results in this example: produces more syntactically fluent sentences, chooses the right words in the right place more frequently. We also took several examples from our IWSLT17 experiment and blank out certain nouns by replacing them with unknown tokens (Table  6 ). Note that in our training set, most sentences do not have unknown tokens, and those that do only have at most 1. Our assumptions of the observed patterns in this case are: i) the proposed models are more capable at handling unknown tokens; ii) while Seq2DRNN is more capable at retaining the structure of the sentence, it cannot rely on a wider context to predict certain common phrases with noises in the source sentence; iii) the proposed Seq2DRNN+SynC model is more capable at handling unknown words both in the sense of being better at retaining sentence structure and handling noisy input. 

 Attention Module We visualise the attention weights of our Seq2DRNN+SynC model. Attention ?2.2.3 computes a context vector for each node in the tree (a weighted sum of the source side vector representations). For the translation pair in Figure  2 , we show the attention weight of each pair of word and node (Equation  9 ) in Figure  5 . The addition of syntax nodes in the output enables the attention model to be used more effectively and is also valuable for visual inspection of syntactic nodes in the output mapping to the input.    means "I", "bin" is "am", "doktor"'s literal translation is "doctor". Darker colour means higher weight (relevance score) as calculated in Equation  9 . The values of each column sum up to 1. The attention weights in this example perfectly align with the appropriate clauses. Additional example is provided in the appendix. 

 Parsing Quality To evaluate the parsing quality, we follow the approach by  Vinyals et al. (2015)  and train a DRNN(SynC) model to produce English to English(Tree) translation. We use the same data and experiment settings that  Vinyals et al. (2015)  used: the Wall Street Journal Penn Treebank English corpus with golden constituency structure, 256 for input/hidden dimension and 3 layers of RNN. We evaluate on section 23 of the aforementioned WSJ data using EVALB 2 . The results are presented in Table  7 . Although falling short behind more specifically 2 https://nlp.cs.nyu.edu/evalb/ designed models like the RNNG by  Dyer et al. (2016) , our model is able to produce better results than the LSTM+AD model proposed by  Vinyals et al. (2015) , which is more comparable to ours since they are also using an NMT model to do constituency parsing. Since our work is more focusing on the translation aspect, optimising and designing a dedicated parser is slightly off-topic here. Nevertheless, it is worth noting that in 50.89% of the cases, Seq2DRNN+SynC was able to produce output that perfectly matches the reference. The same number for sentences with less than 40 words is 52.16%, while the F-measure increases to 90.5. This shows Seq2DRNN(SynC) when doing parsing can produce outputs of similar quality when handling longer sentences. We also do evaluation on our translation results from the IWSLT dataset. Since translation results do not come with reference parse trees, we parse the output of our decoder using the same parser we used in our other experiments: the Stanford Parser. Constituency parsing evaluation is done using Precision/Recall/F1-scores on the output constituent spans (unlabelled) and spans and labels (labelled). The results are presented in Table  8 and Table 9 . The parser we use gets F1 score of 87.04 on Penn Treebank English constituency 0 src es war zeit zum abendessen und wir hielten ausschau nach einem restaurant. die gute nachricht ist , dass die person , die das gesagt hat ann coulter war . 

 S2S and it was time for dinner , and we were looking for a restaurant. the good news is that the person who said that was ann coulter . S2D it was the time for dinner , and we were looking for a restaurant. the good news is that the person who said that was ann coulter . S2D+L it was time for dinner , and we were looking for a restaurant. the good news is that the person who said that was ann coulter . 1 src es war zeit zum abendessen und wir hielten ausschau nach einem UNK. die gute UNK ist , dass die person , die das gesagt hat ann coulter war . 

 S2S and it was time for dinner , and we thought we were looking for a window. the good news is that the person who said that the teacher was ann coulter . S2D it was the time for dinner , and we were looking for a UNK pilot. the good motivator is that the person who said that was ann coulter . S2D+L it was time for dinner , and we were looking for a UNK. the good news is that the person who said that was ann coulter . 2 src es war zeit zum UNK und wir hielten ausschau nach einem UNK. die gute UNK ist , dass die UNK , die das gesagt hat ann coulter war . 

 S2S and it was time for the time time , and we thought we looked at a window search. the good news is that the UNK that the UNK , which was ann coulter . S2D it was the second time , and we would look for a UNK pilot. the good motivator is that the UNK group who said that was ann coulter. S2D+L it was a time for UNK , and we were looking for a UNK. the good news is that the people who said that was ann coulter . 3 src es war UNK zum UNK und wir hielten ausschau nach einem UNK. die gute UNK ist , dass die UNK , die das gesagt hat UNK war . 

 S2S UNK was a time time and UNK , looking for a UNK look for a look at a war. UNK is not a cop 's needs to be ozzie good to good good , which is that the UNK UNK that said that it was said . S2D it was time of time and guerrilla . the good motivator is that the UNK planner that said this was a UNK video. S2D+L it was time for the tone and night to watch the connection . the good news is that the people that said that was mandated . Table  6 : Unknown noun experiment samples. Substituted and correct nouns are marked in Bold, while incorrect elements are marked in underline. Examples shown are: no UNK; 1 UNK; 2 UNKs; 3 UNKs. When there are no unknown tokens, all three compared models are able to produce reasonably good if not identical translations. When there is only one UNK token, Seq2DRNN often does not use the context to predict an appropriate word or phrase. In contrast, both the Seq2Seq and Seq2DRNN+SynC were able to correctly predict that die gute UNK ist could be translated to the good news is. When there are 2 UNK tokens in the source sentence, Seq2Seq produces more incorrect predictions, Seq2DRNN makes some mistakes, while Seq2DRNN+SynC is able to get the most parts correct. Finally, when we replace 3 nouns, all models fail to some degree while Seq2Seq's output is the worst. Model F-measure Baseline  (Vinyals et al., 2015)  < 70 LSTM+AD  (Vinyals et al., 2015)   parsing  (Klein and Manning, 2003b  parse tree construction: the Seq2DRNN+SynC F1 score is comparable but lower than Seq2DRNN. 

 Related Work Recent research shows that modelling syntax is useful for various neural NLP tasks.  Dyer et al. (2015 Dyer et al. ( , 2016 ;  Vinyals et al. (2015) ;  Luong et al. (2016)  have works on language modelling and parsing,  Tai et al. (2015)  on semantic analysis, and  Zhang et al. (2016)  on sentence completion, etc.  Eriguchi et al. (2017)  showed that NMT model can benefit from neural syntactical parsing mod-els.  Choe and Charniak (2016)  showed that a neural parsing problem shares similarity to neural language modelling problem, which forms a building block of an NMT system. We can then make the assumption that structural syntactic information utilised in neural parsing models should be able to aid NMT, which is shown to be true here.  Zhang et al. (2016)  proposed TreeLSTM which is another structured neural decoder. TreeL-STM is not only structurally more complicated but also uses external classifiers.  Dong and Lapata (2016)  also proposed a sequence-to-tree (Seq2Tree) model for question answering. Both of these models are not designed for NMT and lack a language model. While operate from top-tobottom like Seq2DRNN(+SynC), TreeLSTM and Seq2Tree produce components that lack sequential continuity which we have shown to be nonnegligible for language generation. Aharoni and Goldberg (2017),  Wu et al. (2017), and Eriguchi et al. (2017)  experimented with NMT models that utilise target side structural syntax.  Aharoni and Goldberg (2017)  treated constituency trees as sequential strings (linearised-tree) and trained a Seq2Seq model to produce such sequences.  Wu et al. (2017)  proposed SD-NMT, which models dependency syntax trees by adding a shift-reduce neural parser to a standard RNN decoder.  Eriguchi et al. (2017)  in addition to  Wu et al. (2017) 's work, proposed NMT+RNNG which uses a modified RNNG generator  (Dyer et al., 2016)  to process dependency instead of constituency information as originally proposed by  Dyer et al. (2016) , making it consequently a StackLSTM sequential decoder with additional RNN units so it is still a bottom-up tree-structured decoder rather than a top-down decoder like ours. Nevertheless, all of these research showed that target side syntax could improve NMT systems. We believe these models could also be augmented with SynC connections (with NMT+RNNG one has to instead use constituency information). 

 Conclusions We propose an NMT model that utilises target side constituency syntax with a strictly top-down tree-structured decoder using Doubly-Recurrent Neural Networks (DRNN) incorporated into an encoder-decoder NMT model. We propose a new way of modelling language generation by establishing additional clause-based syntactic connections called SynC. Our experiments show that our proposed models can outperform a strong sequence to sequence NMT baseline and several rival models and do parsing competitively. In the future we hope to incorporate source side syntax into the model. We plan to explore the applications of SynC in NMT with more structured attention mechanisms, and potentially a hybrid phrase-based NMT systems with SynC, in which the model can benefit from SynC to be more extensible when handling larger lexicons. A Additional Translation Samples (IWSLT2017 German-English) The samples here are from our IWSLT2017 German-English testset. We compared the performances of all our proposed models as well as the baseline Seq2Seq model in Table  10 . We provide an additional example of our attention module visualisation in Figure  6  and for parser in Figure  7 . Source 1 m ist das h?chste, was ich gesehen habe. Literal 1 m is the highest that i've seen. Reference thirty-nine inches is the tallest structure i've seen. Seq2Seq and the highest thing is i've seen. Seq2DRNN one is the highest thing i've seen at that. Seq2DRNN+SynC feet is the highest thing i've seen. Source ich wei? nicht . sie wollten in die zeit zurck , bevor es autos gab oder twitter oder amerika sucht den superstar. Literal i dont know. they want to go back in time, before there were automobiles or twitter or america looking for superstar. Reference i dont know. they want to go back before there were automobiles or twitter or american idol. Seq2Seq i don't know. they were in the days, when they were cars before cars or the earnings, or america, and the country. 

 Seq2DRNN i don't want to know before time, they wanted to go back before the cars before they were cars or americans. Seq2DRNN+SynC i don't know. they wanted to go back in time, they wanted to go back into the before, before there had cars or twitter visitors. Table  10 : Translation Samples. Gold is the reference, and Literal is produced by a bilingual German-English speaker. The reason we include the literal translation is that sometimes the reference translation from the corpus can have additional components or be non-literal translations.  The sentence is randomly selected from our PennTreebank experiment. Figure 2 : 2 Figure 1: Seq2DRNN on Constituency Tree Encoder Ich bin Doktor 

 Figure 3 : 3 Figure 3: SynC example: the three types of information explicitly modelled when generating the current node/word likes in Andrei when starving likes cheese. 

 Figure 4 : 4 Figure 4: SynC in action; when generating the word likes 

 Sourcewir wiederholten diese ?bung mit denselben studenten. was glauben sie passiert nun? nun verstanden sie den vorteil des prototyping. so wurde aus demselben, schlechten team eines unter den besten. sie produzierten die h?chste konstruktion in der geringsten zeit. Literal we repeated this exercise with the same students. now what do you believe happened? now they understand the value of prototyping. so the same terrible team became one of the very best. they produced the tallest construction in the shortest time. Gold we did the exercise again with the same students. what do you think happened then? so now they understand the value of prototyping. so the same team went from being the very worst to being among the very best. they produced the tallest structures in the least amount of time. Seq2Seqwe repeated this with the same students. what happened you think differently? now, you know, the advantage of the design of the cycle. so, the same one of the team of the team among the best. it produced songs in the slightest building.Seq2DRNNwell repeat these queries with the same students. what do you think of this? now it understood the advantage of the interests. that's been made of the same thing of one thing. they produced the highest construction of the best time at the best time. Seq2DRNN+SynC we repeated this practice with the same students. what do you think happened? now, they understood the value of prototyping. it was being made of the same thing of one of the best ones. they produced the highest construction in the best time. 

 Figure 5 : 5 Figure 5: Attention Module in Seq2DRNN+SynC. "ich" 

 Figure 6 : 6 Figure 6: Attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9. The sentence is randomly selected from our IWSLT experiment. 

 Figure 7 : 7 Figure 7: Parser attention visualisation (Seq2DRNN+SynC). Darker colour means higher attention weight as defined in Equ 9. 

 Table 3 3 : IWSLT17 Experiment results UNK (Unknown) tokens. Only the top 50,000 most frequent words are kept. 

 Table 4 : 4 News Commentary v8 Experiment results. Seq2Seq and NMT+RNNG results are taken from Eriguchi et al. (2017),   Dataset DE-EN CS-EN RU-EN BLEU RIBES BLEU RIBES BLEU RIBES Seq2Seq 16.61 73.8 11.22 69.6 12.03 69.6 Str2Tree 16.13 - 11.65 - 11.94 - NMT+RNNG 16.41 75.0 12.06 70.4 12.46 71.0 Seq2DRNN 16.90 75.1 11.84 67.3 12.04 69.7 Seq2DRNN+SynC 17.21 75.8 12.11 70.3 12.96 71.1 Str2Tree (string-to-linearised-tree) results (no RIBES scores) come from Aharoni and Goldberg (2017) All numbers reported here are of non-ensemble models. 

 Table 5 : 5 Translation Sample. Gold is the reference, and Literal is produced by a bilingual German-English speaker. 

 Table 7 : 7 Parser scores. Numbers from (Vinyals et al., 2015)   88.3 Petrov (2010) 91.8 Dyer et al. (2016) 92.4 Seq2DRNN 89.4 Seq2DRNN+SynC 89.9 are of non-ensemble models. Unlabelled Prec. Rec. F1 Seq2DRNN 96.87 96.93 96.90 Seq2DRNN+SynC 96.43 95.89 96.16 

 Table 8 : 8 IWSLT Translation result constituency unlabelled scores. Reference parse trees obtained using Stanford Parser. 

 Table 9 : 9 IWSLT Translation result constituency labelled scores. Reference parse trees obtained using Stanford Parser. Labelled Prec. Rec. F1 Seq2DRNN 91.63 91.69 91.66 Seq2DRNN+SynC 90.73 90.22 90.48 ). The presence of SynC in the decoder influences
