title
An Effective Optimization Method for Neural Machine Translation: The Case of English-Persian Bilingually Low-Resource Scenario

abstract
In this paper, we propose a useful optimization method for low-resource Neural Machine Translation (NMT) by investigating the effectiveness of multiple neural network optimization algorithms. Our results confirm that applying the proposed optimization method on English-Persian translation can exceed translation quality compared to the English-Persian Statistical Machine Translation (SMT) paradigm.

Introduction Employing neural networks in Machine Translation (MT) significantly reduces the time-consuming and laborious operation steps such as word alignment, phrase extraction, feature selection, etc. Although the quality of Neural MT (NMT) models heavily rely on the quantity as well as the quality of the training dataset, considering the low-resource condition, the impact of NMT is still not as much as the Statistical Machine Translation (SMT). NMT has recently achieved great success, which surpasses SMT in many high-resource language pairs, and it has become the MT approach. In this paper, we compare the impact of multiple neural network optimization algorithms under with respect to the low-resource condition, and then, we proposes an effective optimization method for our case-study language pair. The motivation for choosing English and Persian as the case-study is the linguistic differences between these languages, which are from different language families and have significant differences in their properties, may pose a challenge for MT. Following  Ahmadnia and Dorr (2019) , lowresource languages are those that have fewer technologies and datasets relative to some measure of their international importance. In simple words, the languages for which bilingual training data is extremely sparse, requiring recourse to techniques that are complementary to standard MT approaches. The biggest issue with low-resource languages is the extreme difficulty of obtaining sufficient resources. Natural Language Processing (NLP) methods that have been created for analysis of lowresource languages are likely to encounter similar issues to those faced by documentary and descriptive linguists whose primary endeavor is the study of minority languages  (Ahmadnia et al., 2017) . Lessons learned from such studies are highly informative to NLP researchers who seek to overcome analogous challenges in the computational processing of these types of languages. Our results show that the proposed optimization algorithm for English-Persian NMT works well and improves translation results compared to the English-Persian SMT paradigm. This paper is organized as follows; Section 2 describes the methodology. The experimental results and analysis are covered by Section 3. Section 4 investigates the previous related work. Conclusions and future work are provided in Section 5. 

 Methodology NMT originates from sequence-to-sequence learning. So, in this paper, we take the attention-based (Attentional) NMT model. 

 Attention-based NMT Attentional NMT  (Bahdanau et al., 2015)  models are divided into three parts; ? Encoder that encodes the source sentences into vector sequences as source language representations. ? Decoder that acquires the source context information through attention mechanism and generates target word sequences in turn. ? Attention mechanism that connects encoders and decoders to make the whole model interrelated. In NMT module, a source sentence x = x 1 , x 2 , ..., x J is encoded into an internal representation h = h 1 , h 2 , ..., h J , and then h is decoded into a target sentence y = y 1 , y 2 , ..., y I . For example, to translate an English sentence the dog likes to eat an apple into Persian, each word is transformed into a 1-hot encoding vector (with a single 1 associated with the index of that word, and all other indexed values 0). Each word in the dataset has a distinct 1-hot encoding vector that serves as a numerical representation that serves as input to the model. The first step toward creating these vectors is to assign an index to each unique word in English (as the input language). This process is then repeated for Persian (as the output language). The assignment of an index to each unique word creates a vocabulary for each language. The encoder portion of the NMT model takes a sentence in English and creates a representational vector from this sentence. This vector represents the meaning of the sentence and is subsequently passed to a decoder which outputs the translation of the sentence in Persian. NMT models the conditional probability of the target sentence as: P (y|x) = I i=1 P (y i |y <i , x) (1) where y i is the target word emitted by the decoder at step i and y < i = (y 1 , y 2 , ..., y i?1 ). The conditional output probability of a target word y i defined as follows: P (y i |y <i , x) = sof tmax (f (d i , y i?1 , c i )) (2) where f is a non-linear function and d i = g(d i?1 , y i?1 , c i ), g is a non-linear function. c i is a context vector computed as the weighted sum of the hidden vectors h j , c i = J j=1 ? t,j h j , (3) where h j is the annotation of source word x j , ? t,j is computed by what is known as the attentional model, which focuses on sub-parts of the sentence during translation: ? ij = exp (score (d i , h j )) J j ? =1 exp (score (d i , h j ? )) (4) The score function above can be defined in some different ways as discussed by  Luong et al. (2015) . The attention mechanism supports memorization of long source sentences in NMT. Rather than building a single context vector out of the encoder's last hidden state, an attention model creates shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element. The context vector has access to the entire input sequence-for retention of the full context of the sentence-and controls the alignment between the source and target. Stated simply: the attention mechanism converts two sentences into a matrix where the words of one sentence form the columns, and the words of another sentence form the rows. From this, matches are obtained, thus identifying the relevant and yielding a positive impact on MT. Apart from improving the performance on MT, attention-based networks allow models to learn alignments between different modalities (different data types) for e.g., between speech frames and text or between visual features of a picture and its text description. 

 Optimization Method Following  Ahmadnia and Dorr (2020) , given a training dataset with N bilingual sentences, an attentional NMT training loss function is defined as the conditional log-likelihood: Loss = N n=1 I i=1 ?logP (y n i |y n <i , x n ) (5) The performance of NMT systems is determined by the method of model optimization. Three typical model optimization methods are as follows: ? Adam that combines the best properties of the "AdaGrad" and "RMSProp" algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. Its main advantage is that after offset correction, the learning rate of each iteration has a certain range, which makes the parameters more stable. Also, different parameters have different adaptive learning rates, which are suitable for large-scale data sets and high-dimensional parameter space (Kingma and Ba, 2015). ? Adadelta which is an extension of "Adagrad" that reduces its aggressive, monotonically decreasing learning rate. Instead of accumulat-ing all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size. Its advantage is that the learning rate is adaptive, and the experimental results are reasonable. However, the disadvantage is that the convergence speed is slower (Zeiler, 2012). ? Stochastic Gradient Descent (SGD) as an iterative method for optimizing an objective function with suitable smoothness properties can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof. Its advantage is that it is simple to implement and the experimental results are more stable and reliable under the appropriate learning rate scheduling scheme. While its disadvantage is that it is difficult to select the appropriate learning rate  (Robbins and Monro, 1951) . The SGD method calculates the gradient in each iteration of training corpus, and then updates the model parameters which is the most basic optimization method of neural network model. In this paper, this method refers to the mini-batch gradient descent. In the following section, we compare the different model optimization methods and make a comparative analysis of their application impact on English-Persian attentional NMT. 

 Experimental Results For the experiments, we utilized TEP 1  (Pilevar et al., 2011)  English-Persian parallel corpus that contains about 594K sentences. We allocated ?550K sentences to training step, ?10K sentences to validation step, and ?30K sentences to testing step. We employed Byte Pair Encoding (BPE)  (Sennrich et al., 2016b)  as an effective way to overcome the unknown word problem in standard NMT. In the experiments, we limited the vocabulary size to the most frequent 10K tokens and replacing the rest with a special token <UNK>. We accelerate training by discarding all sentences with more than 30 elements (either BPE units or actual tokens). The vector dimension of bilingual words is 512, the size of hidden layer is 1024, the beam size is 10, the size of mini-batch is 80, and the dropout of output layer is set to 0.1. In order to reduce the problem of unlisted words, the size of Persian and English dictionaries is set to 20K to cover about 95% words. In order to reduce fitting, we set epoch, as the maximum number of training rounds, to 60. BLEU  (Papineni et al., 2001)  is our standard evaluation metric. We employed the following experimental systems: ? Moses: We adapted the baseline system on top of Moses  (Koehn et al., 2007)  as a standard phrase-based SMT. ? RNNSearch: Which is compared with the method using by the paper under the same experimental setting. ? Mantis: We employed Mantis  (Cohn et al., 2016)  on top of DyNet as the attentional NMT open source system. Its cycle unit is Long Short-Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  that in which, the default parameter configuration is used. ? Mantis+Adadelta/SGD/Adam: which is used as the optimization method of model parameters for English-Persian NMT system. Mantis+SGD+DC represents learning rate Decay when the iteration exceeds 40 rounds, and the decay rate is 0.1. As seen in Table  1 , the translation impacts of Model 3, Model 4 and Model 8 are lower than SMT (Model 1). It shows that English-Persian attention-based NMT is ineffective considering the low-resource conditions. Also, the results conform to the characteristics of the general low-resource NMT systems. The results of Model 4, Model 5, Model 6, Model 8, and Model 10 demonstrate that increasing learning rate can extremely improve the English-Persian translation quality where when the optimization method of SGD and Adam were employed. However, when the learning rate is too high, the performance of translation system will be reduced. Therefore, it can be seen in the case of low-resource conditions, the actual NMT system is sensitive in various model optimization methods and corresponding learning rates. So, selecting the appropriate model optimization method and learning rate has a great influence on the final translation results. Furthermore, Model 7 has achieved the highest translation impact, which surpasses the SMT system using Moses and the NMT system using RNNSearch. It can be found that English-Persian NMT still achieves better translation results by adopting higher learning rates and learning rate scheduling strategies with fewer corpus when choosing appropriate model optimization methods. Figure  1  shows the convergence curves of different system models. ect can be achieved. Also, from the learning curve, it can be found that the system using Adadelta optimization method converges slowly, and the corresponding translation effect is the worst. Adam optimization method converges quickly. The SGD optimization method uses a large learning rate and achieves the effect of Adam optimization method in about 26 rounds. When the execution learning rate decreases, the translation performance can further be improved, and ultimately the best translation impact can be achieved. 

 Related Work To construct pseudo bilingual corpus, various useful methods have already been proposed; 1) Back-translation  (Sennrich et al., 2016a) , 2) Dual learning  (He et al., 2016) , and 3) Round-tripping  (Ahmadnia et al., 2018; Ahmadnia and Dorr, 2019) . Also, integrating additional language models to use monolingual corpus  (Zoph et al., 2016) , using transfer learning to transfer the model of high-resource language pairs to low-resource ones, etc. The core idea of the mentioned approaches is to integrate more external resources so that the NMT model can sufficiently acquire translation knowledge and augment translation quality. Although the above methods have practically achieved remarkable results, the disadvantage is that the application effect is limited by the quality of external (generated) sentences. In contrast the existing work, this paper compares various optimization methods of NMT models, and proposes a translation model optimization method which is useful in low-resource condition to enhance the effectiveness of English-Persian NMT. Our method does not employ any additional (generated) resources and has certain generality. 

 Conclusions and Future Work In this paper, an effective optimization method for bilingually low-resource NMT models was applied to English-Persian translation. The investigated optimization method significantly enhances the impact of the English-Persian NMT system, and surpasses the SMT system and the previous similar work, which achieves the best translation results. Noting worth that our optimization method not only does not depend on external resources but also it has language independence. As a future work, we want to investigate other methods of NMT to enhance effects in low-resource conditions, and the application of this method to other languages. Figure 1 : 1 Figure 1: Contrast of the convergence rates of various translation systems. 

 Table 1 : 1 English-Persian translation results. Model Translation system Learning rate BLEU 1 Moses - 34.80 2 RNNSerach - 38.51 3 Mantis+Adadelta - 32.06 4 Mantis+SGD 1 18.66 5 Mantis+SGD 2 38.42 6 Mantis+SGD 3 35.35 7 Mantis+SGD+DC 2 39.46 8 Mantis+Adam 0.001 34.90 9 Mantis+Adam 0.002 38.73 10 Mantis+Adam 0.003 37.64 

			 Proceedings of the 7th Workshop on Asian Translation, pages 45-49 December 4, 2020. c 2020 Association for Computational Linguistics 

			 http://opus.nlpl.eu/TEP.php
