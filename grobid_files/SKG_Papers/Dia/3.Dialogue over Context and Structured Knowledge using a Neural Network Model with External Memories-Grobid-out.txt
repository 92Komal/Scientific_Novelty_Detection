title
Dialogue over Context and Structured Knowledge using a Neural Network Model with External Memories

abstract
The Differentiable Neural Computer (DNC), a neural network model with an addressable external memory, can solve algorithmic and question answering tasks. There are various improved versions of DNC, such as rsDNC and DNC-DMS. However, how to integrate structured knowledge into these DNC models remains a challenging research question. We incorporate an architecture for knowledge into such DNC models, i.e. DNC, rsDNC and DNC-DMS, to improve the ability to generate correct responses using both contextual information and structured knowledge. Our improved rsDNC model improves the mean accuracy by approximately 20% to the original rsDNC on tasks requiring knowledge in the dialog bAbI tasks. In addition, our improved rs-DNC and DNC-DMS models also yield better performance than their original models in the Movie Dialog dataset.

Introduction Recently, deep neural networks have made significant progress in complex pattern matching of various tasks such as computer vision and natural language processing. However, these models are limited in their ability to represent data structures such as graphs and trees, to use variables, and to handle representations over long sequences. Recurrent neural networks (RNNs) can capture the long-range dependencies of sequential data and are also known as Turing-Complete  (Siegelmann and Sontag, 1995)  and therefore are capable of simulating arbitrary procedures, if properly wired. However, RNNs struggled with the vanishing gradients problem  (Bengio et al., 1994) . The long short-term memory (LSTM) architecture addressed this problem by taking gating mechanisms into RNN architectures and calculating the gradients by element-wise multiplication with the gate value at every time-step  (Hochreiter and Schmidhuber, 1997) . LSTMs became quite successful and helped to outperform traditional models because of the sequence to sequence model  (Sutskever et al., 2014)  and attention mechanisms  (Bahdanau et al., 2014; Luong et al., 2015) . Yet LSTM based models have not reached a real solution to the problems mentioned as the limitations of deep neural networks. On the other hand, in the von Neumann architecture, programs can be run by three fundamental mechanisms: a processing unit that performs simple arithmetic operations and logic ones, a control unit that takes control flow instructions such as sequential execution, conditional branch and loop, and a memory unit that data and instructions are written to and read from during computation  (Neumann, 1945) . This architecture can represent complex data structures and learn to perform algorithmic tasks. It separates computation by a processor (i.e. a processing unit and a control one) and memory, whereas neural networks mix computation and memory into the network weights. To improve the performance of standard neural networks,  Graves et al. (2014)  proposed a neural network model with an addressable external memory called Neural Turing Machine (NTM). The whole architecture of NTM is differentiable, therefore can be trained end-to-end including how to access to the memory. Further,  improved the memory access mechanism and proposed Differentiable Neural Computer (DNC). It solved algorithmic tasks over structured data such as traversal and shortest-path tasks of a route map and an inference task of a family tree. In the experiment on question answering with premises, input sequences were written to the memory and necessary information to infer the answer was read from the memory, hence representing variables. DNC was also able to learn long sequences by the dynamic memory access mechanism. There are various improved versions of DNC, such as rsDNC  (Franke et al., 2018)  and DNC-DMS  (Csord?s and Schmidhuber, 2019) . However, how to integrate structured knowledge into these DNC models remains a challenging research question. This paper investigates how to incorporate structured knowledge into DNC models. We extend single-memory unit DNC models to a multiple-memory architecture that leverages both contextual and structured knowledge information. We add an extra memory unit that encodes knowledge from knowledge bases. In contrast with RNNs, the memory-augmentation of DNC allows an explicit storage and manipulation of complex data structures over a long time-scale  (Franke et al., 2018) . Our main contributions are as follows: ? We incorporate a knowledge memory architecture into DNC models, i.e. DNC, rsDNC and DNC-DMS, to improve the ability to generate correct responses using both contextual information and structured knowledge. ? Our improved rsDNC model improves the mean accuracy by approximately 20% to the original rsDNC on tasks requiring knowledge in the dialog bAbI tasks . ? In addition, our improved rsDNC and DNC-DMS models also yield better performance than their original models in the Movie Dialog dataset . The whole paper is organized as follows. Section 2 briefly introduces the DNC, rsDNC and DNC-DMS models. We describe our proposed model in Section 3 and our experiments and detailed analysis in Section 4. Section 5 introduces related works. Finally, we conclude this paper and explore the future work in Section 6. 

 Differentiable Neural Computer The Differentiable Neural Computer (DNC) is a neural network coupled to an external memory matrix M ? R N ?W , as shown in Figure  1 . It uses attention mechanisms to define weightings over N locations of the memory matrix M that represent which locations should be read or written mainly. For the read operation, the read vector r is computed as a weighted sum over the memory locations  r = N i=1 M [i, ?]w r [i] where the '?' denotes all j = 1, ..., W . For the write operation, the memory M is modified by using a write weighting w w to first erase with an erase vector e, then add a write vector v: M [i, j] ? M [i, j](1 ? w w [i]e[j]) + w w [i]v[j] The weightings are defined by the following three attention mechanisms: ? Content-based addressing: compares a key vector to the content of each location in memory and calculates similarity scores to define a read weighting for associative recall or a write weighting to modify a relevant vector in memory. ? Temporal memory linkage: records tracks of consecutively written memory locations to be able to read sequences in the order of which locations were written to. ? Dynamic memory allocation: frees and allocates memory as needed for writing by representing the degree of each location usage which can be increased with each write and decreased after each read and reallocating memory with a low degree of usage. The whole system is differentiable and can be learned with gradient descent. Recently, variations of the DNC model have been proposed, such as the robust and scalable DNC (rsDNC)  (Franke et al., 2018)  and the DNC-DMS  (Csord?s and Schmidhuber, 2019) . Robust and Scalable DNC : Focusing on QA tasks,  Franke et al. (2018)  extended the DNC to be more robust and scalable (rsDNC), with the following four improvements: (1) using only the content-based memory unit to reduce memory consumption and training time, (2) applying layer normalization to lower the variance in performance between different runs, (3) using bypass dropout to make the memory unit's effect stronger, and (4) introducing a bidirectional architecture to encode input sequences in a more informative way. DNC-DMS : Csord?s and Schmidhuber (2019) tackled three problems of vanilla DNC and proposed an improved model called DNC-DMS. First, the lack of key-value separation makes the contentbased address distribution flat and prevents the model from accessing specific parts of a memory. By masking improper parts of both lookup key and memory content, the key-value separation can be controlled dynamically. Second, memory de-allocation mechanisms do not affect memory content which is crucial to content-based addressing and result in memory aliasing. Thus, DNC-DMS proposed to erase memory content completely. Lastly, chaining multiple reads with the temporal linkage matrix exponentially blurs the address distribution. Exponentiation and renormalization of the distribution reduced the effect of exponential blurring and improved sharpness of the link distribution. Although these methods lead to good improvements over the original DNC model, none of them addressed how to incorporate structured knowledge into the DNC explicitly. 

 Proposed Method We expand three models, DNC, rsDNC, and DNC-DMS, by adding an extra memory architecture to store structured knowledge. Therefore, our proposed model consists of a control unit and two memory units. One memory unit stores contextual information in the dialogue and we call it "context memory". The other memory unit stores knowledge information and we call it "knowledge memory". The differences from the original DNC models are to introduce the knowledge memory and add the operation for it. Figure  2  shows the overview of our proposed model based on DNC. The procedures at every time-step t are described as follows: !"#$%&$ '%'"() !"#$("* +,--. /#01$2+$. 31$01$2+$. ! " # " $ " % " & " ' & "() * + " 4#"5*%67%2 '%'"() & " * & "() ' , " -. -/ -0 ' -1 -0 * Figure 2: Overview of our proposed model based on DNC 1. The controller (RNN) receives an input vec- tor x t , a set of R read vectors r c t?1 = [r c,1 t?1 ; ...; r c,R t?1 ] (r c t?1 is a concatenation of r c,1 t?1 , ..., r c,R t?1 ) from the context memory ma- trix M c t?1 ? R N ?W at the previous time- step and a set of R read vectors r k t?1 = [r k,1 t?1 ; ...; r k,R t?1 ] from the knowledge memory matrix M k t?1 ? R N ?W at the previous timestep. It then emits an hidden vector h t . 

 By linear transformation of h t , an output vec- tor ? t = W y h t , an interface vector ? t = W ? h t that parameterizes the context memory interactions at the current time-step and an interface vector ? t = W ? h t for the knowledge memory are obtained. The W terms denote learnable weight matrices. 3. The write operation to the context memory is performed using ? t and its state is updated. The write operation to the knowledge memory is not performed. 4. Finally, the output vector y t is calculated by adding ? t to a vector obtained by multiplying the concatenation of the current read vectors r c t from the context memory and W c r and a vector obtained by multiplying the concatenation of the current read vectors r k t from the knowledge memory and W k r . y t = ? t + W c r r c t + W k r r k t The read vectors r c t and r k t are appended to the controller input at the next time-step. The read and write operations to two memories are performed by repeating the above procedures. To build the knowledge memory unit used in our models, we first run the original DNC model on a knowledge base (KB). We then use the pretrained memory unit as the knowledge memory unit in our proposed models. The process to build this knowledge memory is described in the next section. Knowledge Memory Building We built a knowledge memory unit with a knowledge base (KB). Facts in the KB have a Resource Description Framework (RDF) 1 triple structure "(subject, relation, object)". For example, information such as "Washington, D.C. is the capital of the U.S." is expressed as (the U.S., capital, Washington, D.C.). We applied the original DNC model, which has a single memory, to learn KB facts by giving the model all three components of a triple and any two of a triple and then making the model learn to return the other of a triple. For instance, when inputs for the model are '' the U.S.'', '' capital'', '' Washington, D.C.'', '' the U.S.'', and ''capital'', the output is '' Washington, D.C.''. The model was trained using all triples of the KB and produced a memory unit which stores the whole KB. We used this pre-trained memory unit as the knowledge memory unit in our proposed models. We trained the DNC model using memory dimensions of 512 ? 128 because the results of our proposed models were better than when we also trained using memory dimensions of 256 ? 64 which were the same as context memory dimensions in our proposed method. Whereas the context memory just stores each dialogue content in the dataset, the knowledge memory stores the whole content of the KB and thus it is reasonable that the knowledge memory needs to be larger than the context memory. We evaluated the model with the accuracy and used TransE  (Bordes et al., 2013 ) for KB's word embeddings. 

 Experiments We evaluated our approach on two dialogue datasets, the (6) dialog bAbI tasks  and the Movie Dialog dataset . Both datasets require context compre-hension and knowledge background, and provide dialogue data on a specific domain and RDF triple data to answer questions in the dialogue. 

 Implementation Details The hyperparameters of all models are mainly based on the original DNC paper . We trained all models using one layer LSTM  (Hochreiter and Schmidhuber, 1997)  with a hidden layer of size 256 , a batch of size 32, a learning rate of 1 ? 10 ?4 , context memory dimensions of 256 ? 64, knowledge memory dimensions of 512 ? 128, four read heads, one write head. We used the RMSProp optimizer  (Tieleman and Hinton, 2012)  with a momentum of 0.9. rsDNC models have a dropout probability of 10%, following  (Franke et al., 2018) . We used TransE  (Bordes et al., 2013)  for KB's word embeddings and GloVe embeddings  (Pennington et al., 2014)  for words that do not appear in the KB but appear in the dialogue such as "the" and "what" referring to  (Saha et al., 2018) . The dimension of each word embedding vector is 200. We stopped training if the result of a validation set drops ten epochs in a row and the model repeats this five times during training. We run every model three times under different random initializations and report the averaged results. 

 Dialog bAbI tasks The (6) dialog bAbI tasks  are a set of six dialogue tasks within the goal-oriented context of restaurant reservation. Among them, we focus on Task 5, which combines Tasks 1-4 to generate full dialogs. We also removed sentences starting with the special token "api call" from it in our work. Training, validation and test sets hold 1,000 examples, respectively. It also includes an Out-Of-Vocabulary (OOV) test set of 1,000 examples that include entities unseen in training and validation sets. The KB contains 8,400 facts in the restaurant domain such as ''resto seoul cheap korean 1stars, R cuisine, korean''. The number of entities is 3,635, the number of relations is 7, and the vocabulary size of the dialog is 2,043. 

 Results In Table  1 , we present the mean perresponse accuracy over three different runs for all models. For clarity, we use the following notation: DNC is the original DNC model, rsDNC refers to the robust and scalable DNC model proposed by  Franke et al. (2018)   DNC+KM outperformed the original DNC on both Full dialogs and OOV tasks. Table  2  shows the results in detail separating tasks that can be answered without KB facts and tasks that require KB facts to answer questions. The w/o KB facts task has 12,351 sentences and the w/ KB facts task contains 3,912 sentences in the test set. Though DNC and DNC+KM were the same on the w/o KB facts task (99.9%), there was a significant improvement of 14.45% on the w/ KB facts task, where the DNC and DNC+KM models obtained accuracy scores of 29.53% and 43.98%, respectively. rsDNC+KM achieved the best performance overall and also improved the results on the w/ KB facts task by 19.52%, where the rsDNC and rsDNC+KM models obtained accuracy scores of 68.66% and 49.14%, respectively. Focusing on the rsDNC+KM model, we visualized the results of read/write attention weights from/to memories at every time-step to investigate what the model wrote to memories and what it read from memories. Figure  3 , Figure  4 , and Figure  5  shows the attention weights visualization on a successful example where the outputs of the model are all correct, as presented in Table  3 . In Figure  3 , the horizontal axis represents locations in the context memory and the vertical axis represents inputs of data, or user's utterances and outputs of the model at every time-step. While the model takes input sequences, it returns nothing, and while there is no input, it generates responses, in other words, when input sequences are "can, you, make, a, restaurant, reservation, in, paris, -, -, -", the output sequences are "-  , -, -, -, -, -, -, -, i'm , on, it" ('-' is a padding word). Figure  3  shows write attention weights to the context memory corresponding to turn 2 to turn 6 in the example shown in Table  3 . There are strong attentions on KB words such as "paris" and it is interesting that there are also attentions on words used to change the conditions of a restaurant such as "actually" and "instead". Figure  4  shows read attention weights from the context memory corresponding to turn 8 to 14 in the example shown in Table  3 . The model's memory uses four read heads and we show one of them. The slot where "indian" was strongly written to has an attention when the model outputs restaurant information. The correct answer is an indian restaurant and therefore read information of "indian" is thought to be useful. Figure  5  presents read attention weights from the knowledge memory. There are attentions before the model answers restaurant information and distinctive features are not found compared to the context memory. We also examined a poor performance example where the model made mistakes in all w/ KB facts tasks, as shown in Table  4 . Figure  6  shows a part of the results of write attention weights to the context memory between turn 2 and 8 in Table 4. KB words such as "moderate" and "british" have strong attentions. Figure  7  presents a part of read attention weights from the context memory between turn 10 and 14 in Table  4 . There are attentions on a slot where "british" was written to and on another slot where "with" was written to when   the model outputs restaurant information. Unnecessary information of "with" may be a negative influence on the outputs of the model. Figure  8  shows read attention weights from the knowledge memory. There are attentions before the model answers restaurant information and it shows a similar behavior to read attention weights from the knowledge memory in the good example. Considering the visualized results, the contributions of the knowledge memory appear blurry (it might give the model some information about when to answer KB entities), however, the performance of rsDNC improves by adding the knowledge mem- Regarding DNC-DMS models, the scores of DNC-DMS+KM were lower than the ones of DNC-DMS. We also conducted experiments on DNC-MD models since Csord?s and Schmidhuber (2019) reported that DNC-MD performed better than DNC-DMS on QA tasks, and we found that the performance of the DNC-MD+KM model was higher than DNC-MD. We hypothesize this is due to the nature of the knowledge memory, which does not use the temporal memory linkage. The order of the KB triples when building knowledge memory and the order of the words in the dialogue are irrelevant, therefore sharpness enhancement does not work well.   and 4,766 for test due to the limitation of computational resources. The Movie Dialog dataset's KB is built from the Open Movie Database (OMDb) 2 and the MovieLens dataset 3 . We extracted only triples sharing their entities with entities that appeared in our reduced dialogue data from the original KB, and got 126,999 triples. The number of entities is 37,055, the number of relations is 10, and the vocabulary size of the dialogues is 26,314. Results Table  5  shows the mean hits@1 and hits@10 over three different runs of multiple models and our proposed models. Though our DNC+KM and DNC-MD+KM were worse than their corresponding original models, our rs-  Table  6 : Detailed results in the Movie Dialog dataset Task 3 (QA+Recommendation Dialog). "k=1" and "k=10" mean hits@1 and hits@10, respectively DNC+KM and DNC-DMS+KM performed better than their original models on both hits@1 and hits@10. In Table  6 , we provide the detailed results for a more specific analysis. Each task's outputs are a list of KB entities. Response 1 (Recs) is the first turn in the dialogue and requires a recommendation. The number of entities involved in response 1's test set is 5,421. Response 2 (QA) denotes the second turn and the model needs to answer factoid questions considering context from the previous turn. Response 2 has 9,867 entities in the test set since it is often asked to answer more than one entities. Response 3 (Similar) is the third turn where the model provides another recommendation given the user's extra information about their tastes. 4,939 entities are contained in the test set. In response 3 tasks, the hits@1 score of our model with the knowledge memory was higher in every DNC model. In rsDNC models, both hits@1 and hits@10 of rs-DNC+KM improved on all three tasks. Despite the 5 Related Work  Rae et al. (2016)  introduced the sparse DNC (SDNC) with a sparse memory access scheme called Sparse Access Memory (SAM). SAM controls memory modifications within a sparse subset and uses efficient data structures for content-based addressing, and therefore the memory size does not influence memory consumption.  Ben-Ari and Bekker (2017)  proposed a differentiable allocation mechanism to replace the non-differentiable sorting function of DNC and reduced training time. As different approaches to neural networks with memories, the dynamic memory network (DMN)  (Kumar et al., 2015)  and the DMN+  (Xiong et al., 2016) , and End-to-end memory networks  (Sukhbaatar et al., 2015)  can be mainly listed. They store sentences in a memory and look up related sentences to answer queries using the attention mechanism. The relation memory network (RMN)  (Moon et al., 2018)  uses MLP and makes a multi-hop approach to an external memory to find out relevant information. In contrast to the above models, our model explicitly incorporates a memory architecture to store structured knowledge. Key-Value Memory Networks  are based on End-to-end memory networks  (Sukhbaatar et al., 2015)  and operate a memory with the key-value structure. This structure makes the model flexible to encode knowledge sources and solves the gap between reading documents and using the KB.  Saha et al. (2018)  created Complex Sequential Question Answering (CSQA) dataset that consists of coherently linked questions which can be answered from a large scale KB. They combined the hierarchical recurrent encoder decoder (HRED) model  (Serban et al., 2015)  and the key-value memory network model  to solve their CSQA dataset. Unlike these models, our proposed model does not need to extract KB facts related to queries beforehand and can learn which KB facts the model should extract in a differentiable way. 

 Conclusion We added knowledge memory architecture to three DNC models, vanilla DNC, rsDNC, and DNC-DMS, and experimentally analyzed the effect of our addition on dialogue tasks that require background knowledge. Our proposed models, DNC+KM, rs-DNC+KM, and DNC-MD+KM outperformed their original models on full dialog tasks in the (6) dialog bAbI tasks dataset. In particular, each model obtained an improvement of approximately 14%, 20%, and 7%, respectively on tasks which require KB facts. In the Movie Dialog dataset, our rs-DNC+KM and DNC-DMS+KM performed better than their original models. In future work we will investigate the behavior of the knowledge memory in detail and study how to build and use the knowledge memory more effectively in the whole architecture. We will also conduct experiments with models that are different from DNC models such as Key-Value Memory Networks  to compare with our models. Figure 3 : 3 Figure 3: Visualized result of write attention weights to the context memory in a rsDNC+KM's successful example on the dialog bAbI tasks. The horizontal axis represents locations in the context memory and the vertical axis represents inputs of data and outputs of the model at every time-step. 
