title
Improving Multi-turn Dialogue Modelling with Utterance ReWriter

abstract
Recent research has made impressive progress in single-turn dialogue modelling. In the multi-turn setting, however, current models are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed architecture achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains. 1

Introduction Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching  (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; . Nonethless, multi-turn dialogue modelling still remains extremely challenging  (Vinyals and Le, 2015; Serban et al., 2016 Serban et al., , 2017 Shen et al., 2018a,b) . The challenge is multi-sided. One most important difficulty is the frequently occurred coreference and information omission in our daily conversations, especially in pro-drop languages like Chinese or Japanese. From our preliminary study of 2,000 Chinese multi-turn con-Table  1 : An example of multi-turn dialogue. Each utterance 3 is rewritten into Utterance 3 . Green means coreference and blue means omission. versations, different degrees of coreference and omission exist in more than 70% of the utterances. Capturing the hidden intention beneath them requires deeper understanding of the dialogue context, which is difficult for current neural networkbased systems. Table  1  shows two typical examples in multi-turn dialogues. "?"(he) from Context 1 is a coreference to "?"(Messi) and "? ?"(Why) from Context 2 omits the further question of "?"(Why do you like Tatanic most)?. Without expanding the coreference or omission to recover the full information, the chatbot has no idea how to continue the talk. To address this concern, we propose simplifying the multi-turn dialogue modelling into a singleturn problem by rewriting the current utterance. The utterance rewriter is expected to perform (1) coreference resolution and (2) information completion to recover all coreferred and omitted mentions. In the two examples from Table  1 , each utterance 3 will be rewritten into utterance 3 . Afterwards, the system will generate a reply by only looking into the utterance 3 without considering the previous turns utterance 1 and 2. This simplification shortens the length of dialogue con-text while still maintaining necessary information needed to provide proper responses, which we believe will help ease the difficulty of multi-turn dialogue modelling. Compared with other methods like memory networks  (Sukhbaatar et al., 2015)  or explicit belief tracking  (Mrk?i? et al., 2017) , the trained utterance rewriter is model-agnostic and can be easily integrated into other black-box dialogue systems. It is also more memory-efficient because the dialogue history information is reflected in a single rewritten utterance. To get supervised training data for the utterance rewriting, we construct a Chinese dialogue dataset containing 20k multi-turn dialogues. Each utterance is paired with corresponding manually annotated rewritings. We model this problem as an extractive generation problem using the Pointer Network . The rewritten utterance is generated by copying words from either the dialogue history or the current utterance based on the attention mechanism  (Bahdanau et al., 2014) . Inspired by the recently proposed Transformer architecture  (Vaswani et al., 2017)  in machine translation which can capture better intra-sentence word dependencies, we modify the Transformer architecture to include the pointer network mechanism. The resulting model outperforms the recurrent neural network (RNN) and original Transformer models, achieving an F1 score of over 0.85 for both the coreference resolution and information completion. Furthermore, we integrate our trained utterance rewriter into two online chatbot platforms and find it leads to more accurate intention detection and improves the user engagement. In summary, our contributions are: 1. We collect a high-quality annotated dataset for coreference resolution and information completion in multi-turn dialogues, which might benefit future related research. 2. We propose a highly effective Transformerbased utterance rewriter outperforming several strong baselines. 3. The trained utterance rewriter, when integrated into two real-life online chatbots, is shown to bring significant improvement over the original system. In the next section, we will first go over some related work. Afterwards, in Section 3 and 4, our collected dataset and proposed model are introduced. The experiment results and analysis are presented in Section 5. Finally, some conclusions are drawn in Section 6. 2 Related Work 

 Sentence Rewriting Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models  (Niehues et al., 2016; Junczys-Dowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017) . In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries  (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018) . In dialogue modelling,  Weston et al. (2018)  applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work,  Rastogi et al. (2019)  adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval  (Riezler and Liu, 2010) , semantic parsing  (Chen et al., 2016)  or question answering  (Abujabal et al., 2018) , but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 

 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms  (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj?rkelund and Kuhn, 2014) . At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems  (Clark and Manning, 2016a,b)  utilize distributed representations to reduce human labors.  Lee et al. (2017)  reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self-attention mechanism which could implicitly capture inter-word dependencies in an unsupervised way  (Vaswani et al., 2017) . However, when multiple coreferences occur, it has problems properly distinguishing them. Our proposed architecture is built upon the Transformer architecture, but perform coreference resolution in a supervised setting to help deal with ambiguous mentions. 

 Dataset To get parallel training data for the sentence rewriting, we crawled 200k candidate multi-turn conversational data from several popular Chinese social media platforms for human annotators to work on. Sensitive information is filtered beforehand for later processing. Before starting the annotation, we randomly sample 2,000 conversational data and analyze how often coreference and omission occurs in multi-turn dialogues. In the annotation process, human annotators need to identify these two situations then rewrite the utterance to cover all hidden information. An example is shown in Table  1 . Annotators are required to provide the rewritten utterance 3 given the original conversation [utterance 1,2 and 3]. To ensure the annotation quality, 10% of the annotations from each annotator are daily examined by a project manager and feedbacks are provided. The annotation is considered valid only when the accuracy of examined results surpasses 95%. Apart from the accuracy examination, the project manage is also required to (1) select topics that are more likely to be talked about in daily conversations, (2) try to cover broader domains and (3) balance the proportion of different coreference and omission patterns. The whole annotation takes 4 months to finish. In the end, we get 40k high-quality parallel samples. Half of them are negative samples which do not need any rewriting. The other half are positive samples where rewriting is needed. Table  3    

 Model 

 Problem Formalization We denote each training sample as (H, U n ? R). H = {U 1 , U 2 , . . . , U n?1 } represents the dialogue history containing the first n ? 1 turn of utterances. U n is the nth turn of utterance, the one that needs to be rewritten. R is the rewritten utterance after recovering all corefernced and omitted information in U n . R could be identical to U n if no coreference or omission is detected (negative sample). Our goal is to learn a mapping function p(R|(H, U n )) that can automatically rewrite U n based on the history information H. The process is to first encode (H, U n ) into s sequence of vectors, then decode R using the pointer network. The next section will explain the steps in order. 

 Encoder We unfold all tokens in (H, U n ) into (w 1 , w 2 , . . . , w m ). m is the number of tokens in the whole dialogue. An end-of-turn delimiter is inserted between each two turns. The unfolded sequence of tokens are then encoded with Transformer. We concatenate all tokens in (H, U n ) as the input, in hope that the Transformer can learn rudimentary coreference information within them by means of the self-attention mechanism. For each token w i , the input embedding is the sum of its word embedding, position embedding and turn embedding: I(w i ) = W E(w i ) + P E(w i ) + T E(w i ) The word embedding W E(w i ) and position embedding P E(w i ) are the same as in normal Transformer architectures  (Vaswani et al., 2017) . We add an additional turn embedding T E(w i ) to indicate which turn each token belongs to. Tokens from the same turn will share the same turn embedding. The input embeddings are then forwarded into L stacked encoders to get the final encoding representations. Each encoder contains a self-attention layer followed by a feedforward neural network.: E (0) = I(w 1 ), I(w 2 ), . . . , I(w m ) E (l) = FNN(MultiHead(E (l?1) , E (l?1) , E (l?1) )) FNN is the feedforward neural network and MultiHead(Q, K, V) is a multi-head attention function taking a query matrix Q, a key matrix K, and a value matrix V as inputs. Each self-attention and feedforward component comes with a residual connection and layer-normalization step, which we refer to  Vaswani et al. (2017)  for more details. The final encodings are the output from the Lth encoder E  (L)  . 

 Decoder The decoder also contains L layers, each layer is composed of three sub-layers. The first sub-layer is a multi-head self-attention: M l = MultiHead(D (l?1) , D (l?1) , D (l?1) ) D (0) = R. The second sub-layer is encoderdecoder attention that integrates E (L) into the decoder. In our task, as H and U n serve different purposes, we use separate key-value matrix for tokens coming from the dialogue history H and those coming from U n . The encoded sequence E (L) obtained from the last section is split into E (L) H (en- codings of tokens from H) and E (L) Un (encodings of tokens from U n ) then processed separately. The encoder-decoder vectors are computed as follows: C(H) l = MultiHead(M (l) , E (L) H , E (L) H ) C(U n ) l = MultiHead(M (l) , E (L) Un , E (L) Un ) The third sub-layer is a position-wise fully connected feed-forward neural network: D (l) = FNN([C(H) l ? C(U n ) l ]) where ? denotes vector concatenation. 

 Output Distribution In the decoding process, we hope our model could learn whether to copy words from H or U n at different steps. Therefore, we impose a soft gating weight ? to make the decision. The decoding probability is computed by combining the atten-tion distribution from the last decoding layer: p(R t =w|H, U n , R <t )=? i:(w i =w)?(w i ?H) a t,i +(1?) j:(w j =w)?(w j ?Un) a t,j a = Attention(M (L) , E (L) Un ) a = Attention(M (L) , E (L) H ) ? = ? w d D L t + w H C(H) L t + w U C(U n ) L t a and a are the attention distribution over tokens in H and U n respectively. w d , w H , and w U are parameters to be learned, ? is the sigmoid function to output a value between 0 and 1. The gating weight ? works like a sentinel to inform the decoder whether to extract information from the dialogue history H or directly copy from U n . If U n contains neither coreference nor information omission. ? would be always 1 to copy the original U n as the output. Otherwise ? becomes 0 when a coreference or omission is detected. The attention mechanism is then responsible of finding the proper coreferred or omitted information from the dialogue history. The whole model is trained endto-end by maximizing p(R|H, U n ). 

 Experiments We train our model to perform the utterance rewriting task on our collected dataset. In this section, we focus on answering the following two questions: (1) How accurately our proposed model can perform coreference resolution and information completion respectively and (2) How good the trained utterance rewriter is at helping off-theshelf dialogue systems provide more appropriate responses. To answer the first question, we compare our models with several strong baselines and test them by both automatic evaluation and human judgement. For the second question, we integrate our rewriting model to two online dialogue systems and analyze how it affects the humancomputer interactions. The following section will first introduce the compared models and basic settings, then report our evaluation results. 

 Compared Models When choosing compared models, we are mainly curious to see (1) whether the self-attention based Transformer architecture is superior to other networks like LSTMs, (2) whether the pointer-based generator is better than pure generation-based models and (3) whether it is preferred to split the attention by a coefficient ? as in our model. With these intentions, we implement the following four types of models for comparison: 1. (L/T)-Gen: Pure generation-based model. Words are generated from a fixed vocabulary. 2. (L/T)-Ptr-Net: Pure pointer-based model as in . Words can only be copied from the input. 3. (L/T)-Ptr-Gen: Hybrid pointer+generation model as in  See et al. (2017) . Words can be either copied from the input or generated from a fixed vocabulary. 4. (L/T)-Ptr-?: Our proposed model which split the attention by a coefficient ?. (L/T) denotes the encoder-decoder structure is the LSTM or Transformer. For the first three types of models, we unfold all tokens from the dialogue as the input. No difference is made between the dialogue history and the utterance to be rewritten. 

 Experiment Settings Transformer-based models We set the hidden size as 512. characters and 816 other tokens), including the end-of-turn delimiter and a special UNK token for all unknown words. In the testing stage, all models decode words by beam search with beam size set to 4. 

 Quality of Sentence ReWriting Precision Recall F1   

 Accuracy of Generation We first evaluate the accuracy of generation leveraging three metrics: BLEU, ROUGE, and the exact match score(EM) (the percentage of decoded sequences that exactly match the human references). For the EM score, we report separately on the positive and negative samples to see the difference. We report BLEU-1, 2, 4 scores and the F1 scores of ROUGE-1, 2, L. The results are listed in Table  4 . We can have several observations in response to the three questions proposed in the beginning of Section 5.1: 1. Transformer-based models lead to signif-icant improvement compare with LSTMbased counterparts. This implies the selfattention mechanism is helpful in identifying coreferred and omitted information. More analysis on how it helps coreference resolution can be seen in the next section. 2. The generation mode does not work well in our setting since all words can be retrieved from either H or U n . Pointer-based models outperform the more complex generationbased and hybrid ones. 3. Separately processing H and U n then combine their attention with a learned ? performs better than treating the whole dialogue tokens as s single input, though the improvement is less significant compared with previous two mentions. Overall our proposed model achieves remarkably good performance, with 55.84% of its generations exactly matches the human reference on the positive samples. For negative samples, our model properly copied the the original utterances in 98.14% of the cases. It suggests our model is already able to identify the utterances that do not need rewriting. Future work should work on improving the rewriting ability on positive samples. Coreference Resolution Apart from the standard metrics for text generation, we specifically test the precision, recall and F1 score of coreference resolution on our task. A pronoun or a noun is considered as properly coreferred if the rewritten utterance contains the correct mention in the corresponding referent. The result is shown in Table  5 . To compare with current state-of-the-  art models. We train the model from Lee et al. (2017) on our task and report the results on the first row. The result is quite consistent with the findings from the last section. Our final model outperforms the others by a large margin, reaching a precision score of 93% and recall score of 90%. It implies our model is already quite good at finding the proper coreference. Future challenges would be more about information completion. Figure  2  further provides an examples of how the Transformer can help implicitly learn the coreference resolution through the self-attention mechanism. The same example is also shown in Table  1 . The pronoun "?"(he) in the utterance is properly aligned to the mention "?"(Messi) in the dialogue history, also partially to "?"(player) which is the occupation of him.  Information Completion Similar as coreference resolution, we evaluate the quality of information completeness separately. One omitted information is considered as properly completed if the rewritten utterance recovers the omitted words. Since it inserts new words to the original utterance, we further conduct a human evaluation to measure the fluency of rewritten utterances. We randomly sample 600 samples from our positive test set. Three participants were asked to judge whether the rewritten utterance is a fluent sentence with the score 1(not fluent)-5(fluent). The fluency score for each model is averaged over all human evaluated scores. The results are shown in Table  7 . Basically the condition is similar as in Table  5 . T-Ptr-? achieves the best performance, with the F1 score of 0.86. The performance is slightly worse than coreference resolution since information omission is more implicit. Retrieving all hidden information is sometimes difficult even for humans. Moreover, the fluency of our model's generations is very good, only slightly worse than the human reference (4.90 vs 4.97). Information completeness does not have much effects on the fluency. Exam-  ples of rewritten utterances are shown in Table  6 . 

 Integration Testing In this section, we study how the proposed utterance rewriter can be integrated into off-the-shelf online chatbots to improve the quality of generated responses. We use our best model T-Ptr-? to rewrite each utterance based on the dialogue context. The rewritten utterance is then forwarded to the system for response generation. We apply on both a task-oriented and chitchat setting. The results are compared with the original system having no utterance rewriter. Task-oriented Our task-oriented dialogue system contains an intention classifier built on Fast-Text  (Bojanowski et al., 2017)  and a set of templates that perform policy decision and slot-value filling sequentially. Intention detection is a most important component in task-oriented dialogues and its accuracy will affect all the following steps. We define 30 intention classes like weather, hotel booking and shopping. The training data contains 35,447 human annotations. With the combination of our rewriter, the intention classier is able to achieve a precision of 89.91%, outperforming the original system by over 9%. The improved intention classification further lead to better conversations. An example is shown in Table  8 , a multiturn conversation about the weather. The user first asks "How is the weather in Beijing", then follows with a further question about "Then what clothes are suitable to wear". The original system wrongly classified the user intention as shopping since this is a common conversational pattern in shopping. In contrast, our utterance rewriter is able to recover the omitted information "under the weather in Beijing". Based on the rewritten utterance, the classifier is able to correctly detect the intention and provide proper responses. Chitchat Our social chatbot contains two separate engines for multi-turn and single-turn dialogues. Each engine is a hybrid retrieval and generation model. In real-life applications, a user query would be simultaneously distributed to these two engines. The returned candidate responses are then reranked to provide the final response. Generally the model is already able to provide rather high-quality responses under the single-turn condition, but under multi-turn conversations, the complex context dependency makes the generation difficult. We integrate our utterance rewriter into the single-turn engine and compare with the original model by conducting the online A/B test. Specifically, we randomly split the users into two groups. One talks with the original system and the other talks with the system integrated with the utterance rewriter. All users are unconscious of the details about our system. The whole test lasted one month. Table  9  shows the Conversation-turns Per Session (CPS), which is the average number of conversation-turns between the chatbot and the user in a session. The utterance rewriter increases the average CPS from 6.3 to 7.7, indicating the user is more engaged with the integrated model. Table  8  shows an example of how the utterance rewriter helps with the generation. After the rewriting, the model can better understand the dialogue is about the NBA team Warriors, but the original model feels confused and only provides a generic response. 

 Conclusion In this paper, we propose improving multi-turn dialogue modelling by imposing a separate utterance rewriter. The rewriter is trained to recover the coreferred and omitted information of user utterances. We collect a high-quality manually annotated dataset and designed a Transformer-pointer based architecture to train the utterance rewriter. The trained utterance rewriter performs remarkably well and, when integrated into two online chatbot applications, significantly improves the intention detection and user engagement. We hope the collected dataset and proposed model can benefit future related research. Figure 1 : 1 Figure 1: Architecture of our proposed model. Green box is the Transformer encoder and pink box is the decoder.The decoder computes the probability ? at each step to decide whether to copy from the context or utterance. 
