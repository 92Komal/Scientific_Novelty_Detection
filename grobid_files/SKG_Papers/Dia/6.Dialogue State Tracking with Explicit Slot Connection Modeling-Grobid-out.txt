title
Dialogue State Tracking with Explicit Slot Connection Modeling

abstract
Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on Mul-tiWOZ 2.0 and MultiWOZ 2.1 datasets.

Introduction Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users' goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue states consist of constraints and requests conveyed by user utterances, typically are represented by a set of predefined slots and their corresponding values. For instance, the user utterance "I am looking for a Korean restaurant in the centre" mentions two slots, food and area, whose values are Korean and centre respectively. Numerous methods are proposed to tackle the challenge of DST recently, and these methods can be mainly categorized into two types: fixed vocabulary and open vocabulary  (Eric et al., 2019) . Fixed vocabulary models are designed in the paradigm of multi-class classification, relying on a predefined  (We omit some turns and slots for simplicity.) ontology  (Henderson et al., 2014a; Mrk?i? et al., 2017; Zhong et al., 2018) . Open vocabulary approaches  (Xu and Hu, 2018; Wu et al., 2019; Ren et al., 2019)  break the assumption of predefined ontologies, turning to generate values only given target slots.  Wu et al. (2019)  propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Despite significant improvements achieved by those open vocabulary models, they always suffer from understanding enormous ellipsis and reference expressions in multi-domain scenarios. As shown in Table  1 , there are several slot connections across multiple domains and turns. For example, at the second turn, the value of the target slot attraction-area is informed by a referring expression "in the same area as the restaurant". Thus, the system needs to retrieve the value of its source slot restaurant-area. The last turn shows an obscurer utterance with multiple slot connections, in which target slots taxi-departure and taxi-destination are implicitly connected to their source slots attractionname and restaurant-name respectively. For those slots that need connections, existing methods attempt to find their values out from the lengthy dialogue history, which usually fail because of high learning complexity. In this paper, we formally consider the above challenge as related-slot problem and propose a novel model DST-SC (Dialogue State Tracking with Slot Connections) to address it. We follow previous work to build a copy-augmented encoderdecoder model. Specially, DST-SC is designed with a slot connecting mechanism to establish the connection between the target slot and its source slot explicitly. Thus it can take advantage of the source slot value directly instead of reasoning from preceding turns. The contributions of this work are two-fold: ? To the best of our knowledge, this work is the first one to discuss the related-slot problem in multi-domain DST and address it by explicitly modeling slot connections across domains. ? We demonstrate that DST-SC is more effective for handling the related-slot problem and outperforms state-of-the-art baselines. 

 Model In this section, we will describe DST-SC model in detail. DST-SC is an open vocabulary model based on the encoder-decoder architecture. As shown in Figure  1 , there are three components that contribute to obtain the target slot value: (1) word generation from the vocabulary; (2) word copying from the dialogue history; (3) value copying from the source slot. To reduce the burden on the decoder, DST-SC also equips with a slot gate  (Wu et al., 2019)  to predict for slot values of none and dontcare. 

 Encoder Our model uses a bi-directional GRU  to encode the dialogue history x = {w 1 , w 2 , ? ? ? , w m }, where m is the number of tokens in the dialogue history. Each input token is first embedded using a word embedding function ? emb and then encoded into a fix-length vector h i . h i = GRU(? emb (w i )). (1) 

 Word Generation We employ another GRU to decode slot values. Each slot is comprised of a domain name and a slot name, e.g., hotel-area. While decoding the j-th slot s j , its summed embedding is fed as the first input. The last hidden state of the encoder initializes the decoder hidden state. At decoding step t, the hidden state is represented as h j t . (The superscript j will be omitted for simplicity.) Following the vanilla attention-based decoder architecture , h t is used to apply attention over encoder outputs and aggregate them to get the context vector c t . a t i = softmax(f mlp ([ h t , h i ])), (2) c t = m i=1 a t i h i . (3) The distribution of generating token y t is given by: P gen (y t ) = softmax(W gen [ h t , c t ]). (4) 

 Word Copying The copy mechanism is shown to be effective in DST  (Lei et al., 2018; Xu and Hu, 2018; Wu et al., 2019) . Here, we follow  Wu et al. (2019)  to augment the vanilla attention-based decoder with pointergenerator copying, enabling it to capture slot values that explicitly occur in the dialogue history. P wc (y t = w) = i:w i =w a t i . (5) A soft gate g 1 is used to combine word copying distribution and generative distribution. g 1 = sigmoid(W g 1 [ h t , c t , ? emb (y t?1 )]), (6) P orig (y t ) = g 1 P gen (y t ) + (1 ? g 1 ) P wc (y t ). (7) 

 Slot Connecting Mechanism As claimed in Section 1, connecting the target slot with its source slot helps to decrease the reasoning difficulty. Therefore, we enhance the copyaugmented encoder-decoder model with a slot connecting mechanism to model slot correlations directly. When decoding the target slot s j , DST-SC infers its source slot from last dialogue states, then copies its value for the final distribution. Last dialogue states are represented by (slot, value) tuples: {(s 1 , v 1 ), (s 2 , v 2 ), ? ? ? , (s n , v n )}. We use h 0 as the query to attend the potential source slot. a k = softmax(f mlp ([ h 0 , s k ])), (8) where s k is the summed slot embedding, k ? {1, 2,  how related s k is to the target slot s j . It is computed only once at the first decoding step and maintained consistency to subsequent tokens in the value v k . At the t-th decoding step, the t-th token v kt contributes to form value copying distribution P vc (y t ). w 1 w m ? ? s 1 v 1 s 2 v 2 s n v n ? P vc (y t = w) = k: v kt =w a k . (9) Similar to the copy-augmented decoder, we combine value copying distribution and original distributions using a soft gate g 2 to get final output distribution. g 2 = sigmoid(W g 2 c 0 ), (10) P (y t ) = g 2 P vc (y t ) + (1 ? g 2 ) P orig (y t ). (11) 3 Experimental Setup 

 Datasets To evaluate the effectiveness of DST-SC, we conducted experiments on MultiWOZ 2.0  (Budzianowski et al., 2018)  and MultiWOZ 2.1 datasets  (Eric et al., 2019) . MultiWOZ 2.0 is a multi-domain dialogues corpus, and some annotation errors are corrected in MultiWOZ 2.1. 

 Baselines We compare DST-SC with several baseline methods. FJST and HJST  (Eric et al., 2019)  apply a separate feed-forward network to classify for every single state slot. HyST ) is a hybrid approach, which combines the joint tracking fixed vocabulary approach and open vocabulary approach. COMER  (Ren et al., 2019)  adopts three hierarchically stacked decoders to generate dialogue states. TRADE  (Wu et al., 2019)  generates dialogue states from the dialogue history using a copy-augmented decoder. 

 Implementation Details In our experiments, we used Glove  (Pennington et al., 2014)  and character embeddings  (Hashimoto et al., 2017)  to initialize word embeddings, each word is represented by a 400-dimensional vector. The hidden sizes of all GRU layers are set to 400. In the training phase, we used ground truth prior-turn dialogue states in the slot connecting mechanism. Adam optimizer (Kingma and Ba, 2015) is applied with 0.001 learning rate initially. The learning rate then reduced by a factor of 0.2, and the training stopped early when the performance in validation set was not improved for 6 consecutive epochs. We used a batch size of 32 and dropout rate of 0.2. Greedy search strategy is used for decoding, with maximum 10 decoded tokens and 50% probability of teacher forcing. Also, we followed previous work to utilize our model with the word dropout  (Wu et al., 2019)  by masking input tokens with a 20% probability. All experiments are averaged across 3 seeds. 

 Results and Analysis 

 Experimental Results We follow previous work to compare the performance of joint goal accuracy. We get the joint goal correct if the predicted state exactly matches the ground truth state for every slot. As shown in  MultiWOZ 2.0 and MultiWOZ 2.1, with the joint goal accuracy of 52.24% and 49.58%. 

 Related-slot Tests We conducted further related-slot tests to verify the effectiveness of DST-SC in solving the relatedslot problem. The dataset for related-slot tests is constructed by manually extracting dialogues with the related-slot problem from MultiWOZ 2.1 test set. We made an observation that slot connections are common at target slots such as attraction-area, hotel-area, hotel-book day and so on. We only need to focus on target slot accuracy of turns with slot connections. However, some target slots occur infrequently in the extracted dataset. Considering that target slots from different domains with the same slot type always correspond to similar slot connection expressions, we can neglect their domains and calculate the accuracy of each slot type instead. For example, we can calculate the accuracy of slot type price instead of calculating the accuracy of hotel-price range and restaurant-price range separately. Table  3  lists slot types and their corresponding target slots. To make more convincing tests, we performed data augmentations to get more samples for each slot type. We used two heuristic rules to augment the extracted data and obtained 100 dialogues for each slot type. (1) Paraphrasing: we rewrote some utterances to get multiple phrases with the same intent. For example, the phrase "in the same area as the restaurant" can be rewritten as "close to the restaurant". (2) Replacing values: we replaced some slot values to exclude the influence of overfitting. For example, the phrase "stay in the east" can be replaced as "stay in the west". 

 Slot Type Target Slots  As shown in Table  4 , DST-SC outperforms TRADE by a large margin at most slot types. Case 1 in Table  5  illustrates the advantage of DST-SC explicitly. We find that both generation and word copying miss the correct token. However, the slot connecting mechanism in DST-SC helps to find out the correct source slot and merges its value into P under the control of gate g 2 . Note that there are no obvious improvements on slot types departure and destination. We suspect that this is caused by lots of missing annotations for attraction-name, hotel-name and restaurant-name, which usually act as source slots for departure and destination. The absence of these critical information makes DST-SC pay less attention to values from source slots. As shown in case 2 in Table  5 , even if the slot connection mechanism has inferred the correct source slot, the unconfidence of g 2 leads to the final incorrect output. 

 Related Work Traditional approaches for dialogue state tracking  (Henderson et al., 2014b; Sun et al., 2014; Zilka and Jurc?cek, 2015; Mrk?i? et al., 2015)  rely on manually constructed semantic dictionaries to extract features from input text, known as delexicalisation. These methods are vulnerable to linguistic variations and difficult to scale. To overcome these problems,  Mrk?i? et al. (2017)     tion learning ability. By sharing parameters among slots  Zhong et al., 2018; Nouri and Hosseini-Asl, 2018) , the model is further improved to track rare slot values. These approaches are all designed in the paradigm of multi-class classification over predefined slot value candidates and usually referred to as fixed vocabulary approaches. Fixed vocabulary approaches always require a predefined ontology, which is usually impractical. Their applications are usually limited in a single domain. Therefore, several open vocabulary approaches in generative fashion  (Xu and Hu, 2018; Wu et al., 2019; Ren et al., 2019)  are proposed to handle unlimited slot values in more complicated dialogues. Open vocabulary models show the promising performance in multidomain DST. However, ellipsis and reference phenomena among multi-domain slots are still less explored in existing literature. 

 Conclusion In this paper, we highlight a regularly appeared yet rarely discussed problem in multi-domain DST, namely the related-slot problem. We propose a novel dialogue state tracking model DST-SC, which equips with the slot connecting mechanism to build slot connections across domains. Our model achieves significant improvements on two public datasets and shows effectiveness on relatedslot problem tests. Annotations complement for MultiWOZ dataset in the future might enable DST-SC to handle the related-slot problem more effectively and further improve the joint accuracy. Figure 1 : 1 Figure1: DST-SC model architecture (best viewed in color). Three processing flows leading to P gen , P wc , P vc are respectively generation (brown), copying from dialogue history (green), copying from last dialogue states (purple). 
