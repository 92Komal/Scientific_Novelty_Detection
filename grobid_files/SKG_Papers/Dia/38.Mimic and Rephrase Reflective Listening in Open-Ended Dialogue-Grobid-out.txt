title
Mimic and Rephrase: Reflective listening in open-ended dialogue

abstract
Reflective listening-demonstrating that you have heard your conversational partner-is key to effective communication. Expert human communicators often mimic and rephrase their conversational partner, e.g., when responding to sentimental stories or to questions they don't know the answer to. We introduce a new task and an associated dataset wherein dialogue agents similarly mimic and rephrase a user's request to communicate sympathy (I'm sorry to hear that) or lack of knowledge (I do not know that). We study what makes a rephrasal response good against a set of qualitative metrics. We then evaluate three models for generating responses: a syntax-aware rulebased system, a seq2seq LSTM neural models with attention (S2SA), and the same neural model augmented with a copy mechanism (S2SA+C). In a human evaluation, we find that S2SA+C and the rule-based system are comparable and approach human-generated response quality. In addition, experiences with a live deployment of S2SA+C in a customer support setting suggest that this generation task is a practical contribution to real world conversational agents.

Introduction Humans in conversation naturally engage in reflective (or active) listening, where they indicate they have heard and understood their partner by repeating or rephrasing what they have said. This strategy has its roots in Rogerian psychology  (Rogers, 1951)  as a counseling technique meant to build trust and empathy. Dialog agents benefit from the same strategy to keep conversations pleasant, especially when the agent cannot help. Reflective listening can be formalized into two aspects: (1) mimicking and rephrasing the conversational partner's utterance, and (  2  The last two responses  (d,e)  are incorrect while the first three (a,b,c) are all acceptable with varying levels of specificity. The best response (b) is the one that is neither too vague (a) nor too verbose and repetitive (c). an expressive speech act  (Searle, 1976)  appropriate for the utterance. For example, in Table  1  (b) we incorporate the speech act I don't know on top of the mimicked utterance when the swimming pool is open. In this paper, we propose a new task of generating mimic rephrasals for a given speech act. Table  1  illustrates the task with an example prompt and possible range of responses. The task is nontrivial to handle as naively putting the two parts together will result in responses that are either ungrammatical (e) or do not select the appropriate clause to rephrase (d). In our example, the first three responses all correctly convey the "I don't know" message. However, the blanket response (a) is overly vague and does not convey any understanding. Response (c) is specific but overly verbose and robotic. The best response is (b) since it contains enough details to signal understanding while remaining concise. To get this best response, the agent must ig-nore user flourishes ("Hmm. . . I'm curious"), identify the relevant portions of the prompt, correctly rephrase keywords (replace "I" with "you"), and coordinate arguments (using "when"). By simulating reflective listening, we believe that mimic rephrasals will allow goal-oriented dialog systems to still respond naturally to open-ended input from users. In that vein, there has been renewed interest in open-ended dialog systems using neural models. However,  Li et al. (2016a)  have noted that na?ve neural models tend to generate repetitive and dull responses such as "I don't know". While several attempts have been made to control various aspects of generation and hence produce more diverse output  Hu et al., 2017; Logeswaran et al., 2018) , we instead focus on expressing a single speech act (e.g., "I don't know"), but grounding it in diverse open-ended settings to simulate reflective listening. In this paper, we examine what makes for a good response for a given speech act. We create two datasets IDONTKNOW and EMOTIVE focusing on two speech acts demonstrating reflective listening, respectively stating that we do not know an answer and expressing sympathy. We analyze the quality of responses along different dimensions such as fluency (is the response grammatical?), appropriateness (is the response on topic?), specificity, repetitiveness, and conciseness. We also compare responses from rule-based and neural models to gain insight into the strengths/weaknesses of different models at this task. We demonstrate that the rule-based model is repetitive but performs well for simple cases, while a sequence-to-sequence model with attention  (Bahdanau et al., 2015)  and a copying mechanism  (Gu et al., 2016)  has more varied responses and compares favorably. We release our dataset, code, and experiments to the community. 1 2 Task: Mimic Rephrasals In this section, we introduce the task of Mimic Rephrasal more formally. We use the term speech act to describe the information we want to convey to our conversational partner. For example, a lack of knowledge, sympathy, etc. We define a prompt as an utterance by a conversational partner that should trigger some form of speech act. For example, "where is my car?" could be a prompt for the speech act conveying a lack of knowledge. Note that detection of these prompts and classification into the appropriate speech act-while important for a real-world system-is outside the scope of this task. The task is as follows: given a prompt and the target speech act, generate a mimic rephrasal of that prompt which conveys the speech act. We explore the two use-cases of rephrasing lack of knowledge (IDONTKNOW) and sympathy (EMOTIVE). The important goal of the task to generate a response that makes the user feel that they have been heard and understood. Directly measuring this is difficult. Instead, we propose five metrics to characterize the quality of mimic rephrasals: ? appropriateness Did the response include the topic of interest in the rephrasal? ? fluency Is the response grammatical? ? specificity How much detail from the input prompt is captured? ? conciseness Is the response to the point? ? repetitiveness Is the response repetitive? Looking at Table 1: (d) is not appropriate and (e) has low fluency. (a) to (c) are both appropriate and fluent with varying specificity (from low to high) and conciseness (from high to low). Intuitively, there is a tradeoff between specificity and repetitiveness: it should neither be too vague nor too repetitive. 

 Dataset Section 3.1 describes our process for collecting data, to document our dataset creation and to describe how to collect data for other speech actse.g., expressing gratitude, soliciting confirmation of intents, etc. The subsequent section (Section 3.2) describes some statistics of the two datasets in this paper: EMOTIVE and IDONTKNOW. 

 Data collection Our data collection pipeline has two phases. In the first phase, workers were asked to come up with a prompt or scenario. For example, a question to ask the dialog agent, or a sentiment laden scenario. During this phase, workers were asked to be as creative as possible, to explore a variety of sentence structures and lengths in the way they phrase their prompts, and diversity in the topics covered. In the second phase, we asked another set of workers to generate multiple responses each to the prompt.    Each example has a prompt (the utterance from the conversational partner), and a mimic rephrasal: the utterance that should be returned by the dialog agent. We found that splitting the task up into these two steps-generating prompts and then responsesimproved the quality of our collected sentence pairs. The interface used by Mechanical Turk workers is shown in Figure  1 . Workers are first asked to generate a number of prompts (scenarios) in Figure  1  (a). Once these prompts are collected, a different set of workers were asked to generate responses to the prompts, completing our dataset (see Figure  1  (b)). Workers were paid $0.10 per sentence in the prompt generation task, and $0.07 per sentence in the response generation task. 

 Dataset statistics We use the method described in Section 3.1 to collect two datasets IDONTKNOW and EMOTIVE. Both datasets are split into train/dev/test splits with a ratio of 70/15/15%. IDONTKNOW is a dataset for indicating that we don't know the answer to a question, or cannot execute a request. EMOTIVE is a dataset for expressing sympathy for the topic of the prompt, with a balanced distribution of positive and negative sentiment. Examples for the two datasets can be found in Table  2 , and statistics are given in Table  3 . The modest size of the training set (10 189) means that a good fraction of the test set contains out of vocabulary words: the 1 377 test examples contain 512 words not seen during training, motivating our use of a copy mechanism. We report statistics both on the full mimic rephrasal (MR), as well as for just the Mimic por- Table  3 : Statistics about the IDONTKNOW and EMOTIVE datasets. Mimic is here taken to be the mimicked utterance, without the preceding "I am happy/sorry/etc." or "I don't know". MR is the complete mimic rephrased response. tion of the mimic rephrasal. For example, for the MR "I do not know where you can apply for an Irish visa", the Mimic portion would be "where you can apply for an Irish visa." While the average MR is slightly longer than the original prompt, the Mimic portion averages 2.9 tokens (25%) shorter than the prompt. In addition, the high Jaccard similarity between the prompt and mimic portion suggests the task involves selecting key portions of the original sentence. 2 

 Methods In this section we describe three models for the task described above. These include a rule based baseline constructed with deterministic syntactic transformations as well as trained neural models. 

 Rule based baseline As a na?ve baseline, we use a set of handwritten syntactic rephrasing rules using Stanford CoreNLP . For example, for the IDONTKNOW dataset we developed 8 rules that match a Semgrex  (Chambers et al., 2007)  pattern to an associated dependency graph manipulation algorithm. For example, the rule based system matches the phrase "What is the difference between the debt and the deficit?" to a general type of pattern where the verb (in this case "is") needs to be extracted from the dependency graph and reattached at the end to produce "I do not know what the difference between the debt and the deficit is." The EMOTIVE rule based system extracts the root clause from the constituency parse of the sentence and adds enclosing phrasing ("Sorry to hear that..."). Both systems also use simple string manipulation to replace pronouns and correct casing. We note that the rules were developed by iterating on the training data. In the appendix, we include a histogram of how often each rule was fired in the IDONTKNOW rule based system. Although this is a strong baseline, it has some weaknesses. Writing and maintaining the rule set is difficult and time consuming. The Semgrex patterns and accompanying transformations are nontrivial and requires expert time to develop and maintain. Additionally, it is difficult to deterministically decide which portions of the prompt to keep or drop in the rephrasal. For instance, "I found out someone has been stealing from me" should drop the found out and respond with: "Sorry to hear that someone has been stealing from you". 

 Neural Models To address these issues, we develop a series of neural models for the task. Formally, let x = x 1 , . . . , x n be the source sentence, and y = y 1 , . . . , y m be the generated sequence of output tokens. We define a seq2seq model similar to existing neural MT models  (Cho et al., 2014)  for generating y given an input x, as well as a model augmented with a copy mechanism. Input embedding. All models use concatenated ELMo  (Peters et al., 2018)  and GloVE  (Pennington et al., 2014)  embedding for the input embeddings. The model architectures used for the EMOTIVE and IDONTKNOW datasets are identical with one exception. For the EMOTIVE task, an extra bit is appended to the word embeddings to specify whether the input has a positive or negative sentiment.  3  Baseline neural model. Our baseline neural model is a bidirectional LSTM  (Hochreiter and Schmidhuber, 1997)  with attention  (Bahdanau et al., 2015) . Formally, the encoder outputs a set of hidden states for each token given by { h1 , . . . , hn } = LST M {m(x 1 , . . . , x n )} for word embeddings m(x). The decoder is also an LSTM with hidden state initialized to the sum of the final hidden states of the forward and backward LSTMs contained in the bidirectional LSTM encoder. For encoder hidden state hs and decoder hidden state h t , the attention score a t (s) is defined as a t (s) = exp(score(h t , hs )) s exp(score(h t , hs )) where score(h t , hs ) = v T a ? tanh(W a [h t ; hs ] ) and v a and W a are learnable parameters. Decoding uses a modified beam search (see Section 4.3), 4 and the model is trained on the following cross entropy loss function: J = m i=1 ? log p v (y i = y * i |y <i , x) where y * i is the expected output token given in the training data and log p v (y i = y * i |y <i , x) is a distribution over the models vocabulary computed from the logits outputted by the model. Neural model with copying. The most effective neural model we implemented augments the baseline neural model with a copying mechanism  (Gu et al., 2016) . This allows the model to generalize better to unseen vocabulary, and more strongly enforces the core tenant of the task: that we should be mimicking the prompt. An overview of the model is shown in Figure  2 . The key difference from the baseline neural model is that we now generate output tokens using a combined softmax over the model's vocabulary and the tokens in the input: log p(y i = y * i |y <i , x) = log p v (y i = y * i |y <i , x)+ {j|x j =y * i } log p c (y i = copy(x j )|y <i , x), 3 We note that the rule-based system simply used different rules for different settings of this bit.  4  Initial experiments show that this modified beam search worked better where log p v (?) is the same as before, and log p c (?) is a distribution over the words in the input. For further details we defer the reader to  (Gu et al., 2016) . Similar to the baseline model, we decode the model using a modified beam search and train it using a cross-entropy loss. 

 Modified Beam Search We use a modified version of beam search  (Huang et al., 2017)  when generating output tokens to favor longer responses. The modified beam search first calculates the average ratio of output tokens to input tokens from the dev set, k. We then compute the average logit value of an individual output token, r(y i ), over all outputs produced on the dev set input, r avg . A modified perplexity, sc(y, x), is used to determine which beams to prune, where x is a series of input tokens and y is a proposed series of output tokens (a beam): sc(y, x) = sc(y) + r avg ? min{len(y), k ? len(x)} where sc(y) = len(y) i=1 r(y i ) is the standard perplexity for the generated output. Additionally, we found that for a given output ?, the score sc(?, x) provides a good measure of generation quality and is useful when filtering out poor or unacceptable output. 

 Training All models were implemented and trained using PyTorch  (Paszke et al., 2017) . The Adam (Kingma and Ba, 2014) optimizer was used for all gradient based optimization. We used a randomized hyperparameter grid search to determine the learning rate, number of layers, dropout, and the dimensions of the hidden layers. We used a learning rate of 0.000718 for all optimization. A dropout value of 0.1 is used for all models. All LSTMs are bidirectional with a single layer. Both sequence to sequence models for the IDONTKNOW task use a hidden size of 524 within the LSTM, a hidden size of 100 for the attention layer, a hidden size of 638 for the copy layer, and a dropout value of 0.1. Both sequence to sequence models for the EMOTIVE task use a hidden size of 600 within the LSTM, a hidden size of 200 for the attention layer, a hidden size of 650 for the copy layer, and a dropout value of 0.1. 

 Experiments We study what makes a good response by correlating human judgments of goodness (based on a 5-point Likert scale) to the set of qualitative metrics we defined in Section 2. The average score is 4.2 for IDONTKNOW and 3.7 for EMOTIVE. We also evaluated the performance of different models on these datasets using: (1) an automated BLEU and METEOR evaluation, (2) an A/B study comparing the model output with the gold response, and (3) the qualitative metrics. We conclude with a qualitative error analysis and some observations from a live deployment of the neural rephrasal model. We compare the responses generated by three models: (1) the Rule-based baseline; (2) S2SA: neural model consisting of a seq2seq model with attention, and (3) S2SA+C: neural seq2seq model with attention and copying. 

 Results BLEU/METEOR Following prior work on text generation, we use BLEU  (Papineni et al., 2002)  and METEOR  (Banerjee and Lavie, 2005)  to compare the performance of our model.  5  From the results shown in Table  4 , the neural model with copying (S2SA+C) are the rule-based baseline have comparable performance, with both significantly outperforming the baseline neural model (S2SA). A/B Test We perform a human evaluation of our model outputs by creating an A/B test where evaluators specify a preference for either the model output, or the gold human response (see Figure  3 ). A perfect score on this evaluation would be 50%, indicating that the model and human response are indistinguishable. We ran this test on 305 examples  When the model's response was identical to the human's, we assume it is preferred 50% of the time: the percentage of these examples is reported in the (I) column. selected from the test set. For each example which was not identical to the gold output, five Turk Workers were asked to choose which response they preferred. Table  5  shows the result of the study, showing that the copy mechanism outperforms both the rule-based baseline and the S2SA model for IDON-TKNOW. For EMOTIVE, the rule-based baseline is preferred. Qualitative Metrics To gain insight into the types of errors the different models are making, we elicited human assessment of three of the metrics defined in Section 2: 1. Appropriateness Evaluators make a binary choice as to whether the response included the correct part of the prompt. 2. Fluency Evaluators assess the grammatical correctness of the response by selecting on a 3 point Likert scale ranging from "not fluent" to "somewhat fluent" to "fluent". Scores are normalized to 1. 

 Specificity We present evaluators with a response and ask them to pick the original prompt from 4 choices (the original prompt, two distractors, and "none/multiple applies"). The distractors are chosen from the nearest neighbors of the prompt using an averaged GloVE sentence embedding. We also used the following automatic metrics as proxies for the remaining qualitative metrics: 1. Conciseness The length of the response normalized to the length of the prompt (smaller is more concise). 2. Repetitiveness We use METEOR  (Banerjee and Lavie, 2005)  to measure the overlap between prompt and response. Table  6  shows the results of the human judgment on 400 generated responses for the test set. The rule-based model does well on the EMOTIVE dataset. The S2SA model is overall worst on most metrics, except conciseness. On the other hand, the S2SA+C model performs best on appropriateness and fluency for the IDONTKNOW dataset, and compares favorably with the rule-based model for other metrics. We note that the S2SA+C model most closely matches the amount of specificity, conciseness, and repetitiveness in the human responses. Examples of human responses with their corresponding metrics is provided in the appendix, along with additional responses from the models and error analysis. 

 Analysis Next, we look at the correlation of our qualitative metrics to overall human quality judgments. Results of this analysis are in Table  7  and we provide additional visualizations in the appendix. The human response goodness score correlated positively with appropriateness, fluency, and to some extent with repetitiveness. On the other hand it correlated negatively with conciseness (i.e., shorter responses are preferred), while correlation with specificity was less pronounced. This seems to indicate that good responses are characterized by being appropriate and fluent, while having an appropriate level of detail (indicated by some amount of repetitiveness balanced with conciseness). We see a similar trend for the model responses: appropriateness and fluency are the most important attributes for when a model's response is preferred over the human's. To get a better qualitative understanding of the model's performance, we studied the responses generated by our models (see Table  8  for examples). For simple sentences, the IDONTKNOW rule-based responses are reasonable. However, for more complex sentences, it becomes challenging to identify relevant subclauses or to handle nontrivial constructions like conditional clauses (see aquarium example). As a result, the EMOTIVE rule-based responses, while grammatical, tend to be overly verbose. Both S2SA and S2SA+C are good at producing relatively fluent sentences, and performing the correct pronoun replacements ("you" with "we" and "I"). S2SA responses are often off-topic and inappropriate, with the model generating words that related to the topic but prone to drift (e.g., the teapot example). Since we train on a very small dataset, many words in the prompt are not seen during training. While the input word embedding can help during encoding, the decoder is nonetheless unable to generate words it has not seen during training. The two biggest errors S2SA+C makes are incorrectly identifying the relevant parts of the question  (e.g., the response "I do not know what reporter's transcript deposit are" to the question "What are Reporter's Transcript Deposit Costs?") and grammatical errors when rephrasing (e.g., the response "I do not know when the free trial is end." to the question "When does the free trial end?"). 

 Observations from a Live Deployment We deployed the S2SA+C model as part of a live chatbot for customer service that helped answer customer queries and perform simple tasks like tracking their packages.  6  As context, customers would ask the chatbot questions (e.g., "how do I ship pets?" or "I want to change the delivery address for my package") which were matched against a knowledge base containing frequently asked questions. If a question similarity model was unable to find a match, we tried to communicate to the user that we could not answer their request. Prior to this work, the chatbot would respond with a generic backoff message: "I'm sorry I didn't understand something you said" which resulted in users repeatedly rewording their request even if their request was genuinely outside of the chatbot's knowledge base, and ultimately expressing frustration with the chatbot. We incorporated mimic rephrasals into our system by responding with the output generated by the S2SA+C model trained on the IDONTKNOW dataset if its score, sc(y, x), was higher than a fixed threshold, and using the previous backoff response if not. We observed that when the model replied with a mimic rephrasal, users usually responded with gratitude, e.g. "Thanks for letting me know!", and either continued by asking a different question or leaving the conversation. Presented with the mimic response, users rarely wasted time rewording a request that was out of the scope of what the chatbot could handle. 

 Related Work Verbal mimicry is used in conversation to build social rapport  (Rogers, 1951; Rautalinko and Lisper, 2004) . This observation has been leveraged even in the early development of conversational agents, for example in systems such as Eliza  (Weizenbaum, 1966) , which engaged users by picking up keywords and repeating open ended questions back to the patient, or PARRY  (Colby et al., 1972) , which follows a similar strategy to rephrase utterances to indicate anger or fear. Our work applies mimicry in the task-oriented setting and studies how and when generated mimic rephrasals are preferred over human responses. More recently, sequence to sequence models have been used for open domain chatbots  (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015; Serban et al., 2016) . However, these models suffered from generic responses and turns that are semantically inconsistent and incoherent. To address these issues,  Li et al. (2016a)  introduced a maximum mutual information objective to encourage diversity. Consistency in dialog agents has been well studied in  Kobsa and Wahlster (1989)  inter-alia, and for neural methods by  Li et al. (2016b) . Rashkin et al. (  2019 ) also recognized the need to acknowledge others' feelings in a conversation and introduced a dataset for benchmarking emphathetic dialog models. Sequence-to-sequence models with copying was introduced in  Gu et al. (2016) . Such models have also been shown to be effective at semantic parsing  (Jia and Liang, 2016) , summarization  (See et al., 2017; Cao et al., 2018) , and task oriented dialog  (Eric and Manning, 2017) . Our task can in many ways be considered a controlled generation task. Other work in this area includes generating text conditioned on a sentiment to express , or controlled generation  (Hu et al., 2017)  by editing attributes  (Shen et al., 2017; Logeswaran et al., 2018; Lample et al., 2019) . These works can successfully change the tone and intent of an utterance, but tend to frequently rewrite enough of the content that the method is less effective for practical dialog applications.  Ke et al. (2018)  examined how to generate dialog responses with different sentence function (e.g., imperative, interrogative, etc.), which similarly allows for more distant rewriting than is optimal for our task. Finally, our task exhibits many of the challenges observed by  Bilu et al. (2015)  in the context of negating claims. Other work in generation addresses tasks such as rephrasals for generating paraphrases  (Prakash et al., 2016; Gupta et al., 2018) , sentence simplification  (Narayan et al., 2017) , and query rewriting for question answering  (Dong et al., 2017) . 

 Conclusions We proposed a new task and associated datasets for mimicking and rephrasing a speaker's prompt to communicate a given intent. We showed that both rule-based based and neural seq2seq models both approach human level performance. Additionally, we share observations from a real world deployment of the model to highlight how solving these tasks can potentially improve the end-user experience. We hope this will inspire future work in dialog agents, making these agents more fluent and personable. (a) Interface when generating prompts (b) Interface when generating mimic rephrasals 

 Figure 1 : 1 Figure 1: Our data collection pipeline. First, crowdworkers are asked to generate a given prompt or scenario (a). Then, a different worker is asked to generate the mimic rephrasal of the prompt generated by the first worker (b). 

 IDONTKNOW Prompt: I am legally resident in Northern Ireland, where can I apply for an Irish visa Mimic Rephrasal: I do not know where you can apply for an Irish visa P: When I register a domain, do I receive a website and a web hosting space MR: I do not know if you receive a website and a web hosting space when you register a domain P: I'm having difficulty signing up. Whom can I contact MR: I do not know who you can contact about your difficulty signing up EMOTIVE P: The sisters were able to reunite after 20 years MR: I am happy to hear the sisters were able to reunite after all this time P: The future looks brighter than I ever imagined MR: I 'm happy that your future looks bright to you P: My phone fell into the toilet and it's ruined now. MR: I am sad that your phone is ruined because it fell into the toilet 

 Figure 3 : 3 Figure 3: Example question in A/B test. The prompt asks the crowdworker to choose between the human response and the model output. In this example, one would prefer the Person B's response because it is more specific and exhibits reflective listening. 

 Table 1 : 1 ) incorporating Possible responses to indicate the dialog agent doesn't know the answer to a question (the prompt). Prompt: Hmm. . . I'm curious as to whether the swimming pool is open after 7pm? a. I do not know that b. I don't know when the swimming pool is open. c. I don't know as to whether the swimming pool is open after 7pm. d. I don't know if you are curious as to whether the swimming pool is open after 7pm. e. I don't know Hmm. . . I'm curious as to whether the swimming pool is open after 7pm? 

 Table 2 : 2 Examples of mimic rephrasals in the IDON-TKNOW and EMOTIVE datasets collected in this paper. 

 Table 4 : 4 BLEU and METEOR scores evaluated on the Dev and Test sets. The S2SA+C performs the best on IDONTKNOW and the rule-based performs the best on EMOTIVE. IDONTKNOW EMOTIVE Model Dev Test Dev Test BLEU Rule-based 78.9 79.4 47.0 46.6 S2SA 63.3 63.1 32.9 34.2 S2SA+C 79.9 79.7 44.6 46.3 METEOR Rule-based 84.6 85.3 54.4 54.1 S2SA 74.1 73.6 37.1 38.1 S2SA+C 88.4 88.5 51.5 52.8 

 Table 5 : 5 The percent of model responses that were (P)referred over the human responses in the A/B test portion of the user study on 305 IDONTKNOW examples and 400 EMOTIVE examples. 

 Table 6 : 6 Assessment and measures of Appropriateness, Fluency, Specificity, Conciseness and Repetitiveness. We bold the highest scores for App. and Flu., and closest to human for Spec., Con., and Rep. Model App. Flu. Spec. Con. Rep. IDONTKNOW Human 93.1 91.75 74.7 1.18 65.1 Rule-based 86.3 85.2 79.3 1.25 74.4 S2SA 49.5 66.8 24.4 1.12 45.7 S2SA+C 92.4 86.0 77.5 1.22 70.4 EMOTIVE Human 93.2 88.5 61.3 0.97 36.3 Rule-based 94.4 91.7 90.0 1.29 84.4 S2SA 29.8 74.4 7.0 0.97 17.7 S2SA+C 76.4 76.1 63.5 1.04 49.7 

 Table 7 : 7 Correlations between diagnostic metrics and human quality judgments for responses in the two datasets, with bold indicating statistically significant correlations. For all model responses, we use the A/B test preferences as a measure of quality judgment. For the human responses, we use a 5-pt Likert scale of the "goodness" of response as a quality judgment. Model App. Flu. Spec. Con. Rep. IDONTKNOW Human* 0.34 0.14 0.17 -0.39 0.38 Rules 0.39 0.42 0.17 -0.03 0.08 S2SA 0.54 0.30 0.35 0.00 0.23 S2SA+C 0.45 0.48 0.09 -0.19 0.21 EMOTIVE Human* 0.52 0.59 0.05 -0.17 -0.08 Rules 0.12 0.04 -0.08 -0.08 0.01 S2SA 0.63 0.23 0.22 -0.03 0.33 S2SA+C 0.41 0.41 0.00 0.02 -0.00 

 Table 8 : 8 Example responses from the different models, with errors highlighted in red. Note that the S2SA model tend to introduce random terms and S2SA+C model will retain numbers such as $ 500. 

			 Jaccard similarity is computed as the intersection over union of lemmatized non-stopword tokens between the prompt and Mimic portion. 

			 Specifically, we used NLTK (Loper and Bird, 2002)  to compute both BLEU and METEOR scores with one human reference for each example. The BLEU score is weighted equally between 1-grams, 2-grams, 3-grams, and 4-grams 

			 The live deployment was run at Eloquent Labs prior to their acquisition by Square Inc.
