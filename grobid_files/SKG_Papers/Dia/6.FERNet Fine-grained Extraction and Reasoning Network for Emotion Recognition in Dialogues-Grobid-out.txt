title
FERNet: Fine-grained Extraction and Reasoning Network for Emotion Recognition in Dialogues

abstract
Unlike non-conversation scenes, emotion recognition in dialogues (ERD) poses more complicated challenges due to its interactive nature and intricate contextual information. All present methods model historical utterances without considering the content of the target utterance. However, different parts of a historical utterance may contribute differently to emotion inference of different target utterances. Therefore we propose Fine-grained Extraction and Reasoning Network (FERNet) to generate target-specific historical utterance representations. The reasoning module effectively handles both local and global sequential dependencies to reason over context, and updates target utterance representations to more informed vectors. Experiments on two benchmarks show that our method achieves competitive performance compared with previous methods.

Introduction With the development of human-machine interaction (HMI) applications, textual dialogue scenes appear more frequently. These scenes request effective and high-performance emotion recognition systems helping in building empathetic machines  (Young et al., 2018) . Therefore, emotion recognition in dialogues (ERD) is getting growing attention from both academic and business community. Different from non-conversation scenes, the ERD task poses a more complicated challenge of modeling context-sensitive dependencies. Most of existing approaches adopt Convolution Neural Network (CNN)  (Krizhevsky et al., 2012) , followed by a max-pooling layer to obtain utterance representations  (Kim, 2014; Torres, 2018; Hazarika et al., 2018a,b; Ghosal et al., 2019) . The process proceeds without the guidance of the target utterance, thus generated historical utterance representations are indistinguishable toward different target utterances. Emotion recognition may fail in cases where historical utterances express various emotions toward various targets, which may confuse the emotion recognition of target utterances. As Figure  1  shows, for different target utterances B 1 and B 2 , the model should attend the words "good service" and "bad food" in A 1 , separately. In a word, it is desired to pay different attention to different words of a certain historical utterance to generate the target-specific historical utterance representation.  In this paper, we propose Fine-grained Extraction and Reasoning Network (FERNet) to generate target-specific historical utterance representations conditioned on the content of target utterances by using the multi-head attention mechanism  (Vaswani et al., 2017) , extracting more fine-grained, relevant and contributing information for emotion recognition. Besides, we devise the reasoning module, which employs historical utterances as a sequence of triggers, and updates the representation of the target utterance to a more informed vector as it observes historical utterances through time. In the reasoning process, the module models both short-term and long-term sequential dependencies effectively. We demonstrate the effectiveness of our method on two benchmarks. Experimental results show that our method achieves competitive performance compared with previous methods. 

 Related Work Primitive approaches deal with the ERD task as simple solely-sentence emotion recognition task with no consideration of the historical information  (Joulin et al., 2016; Chen et al., 2016; Yang et al., 2016; Chatterjee et al., 2019) . To exploit contextual information,  Poria et al. (2017) ;  Huang et al. (2019) ;  Jiao et al. (2019) ;  Hazarika et al. (2018a,b) ;  Torres (2018)  use RNN architecture,  Hazarika et al. (2018b,a)  use conversational memory networks  (Sukhbaatar et al., 2015) ,  Torres (2018) ;  Jiao et al. (2019)   Some of these works consider the context following the target utterance such as  Luo et al. (2018) ;  Saxena et al. (2018) ;  Ghosal et al. (2019)  and some variants of . However, this condition is quite incompatible with some practical situations like real-time dialogue systems in which we possess no future utterances while handling the target utterance. So in our paper, we only focus on the setting that only historical utterances can be utilized. 

 Proposed Model Each dialogue D consists of two parts denoted as D = {(U, S)}, where U = [u 1 , u 2 , ..., u n ] is a sequence of utterances ordered based on their temporal occurrence. S = [s 1 , s 2 , ..., s n ] denotes corresponding speakers and n is the number of utterances in the dialogue. The ERD task aims to predict Y = [y 1 , y 2 , ..., y n ], where y i ? C (1 ? i ? n) denotes the underlying emotion of the utterance u i . C is the set of candidate emotion categories. The FERNet consists of four successive modules: feature extraction module, attention module, reasoning module and output module. Figure  2  presents the overall architecture of the proposed model. 

 Feature Extraction Module We use two multi-layer bidirectional Gated Recurrent Unit (bi-GRU) Networks  (Tang et al., 2015)  to accumulate contextual information from two directions for each word of target utterances and historical utterances, separately. The inputs consist of  l k = [ ? ? h l k ; ? ? h l k ] is generated by concatenating the hidden states of the k?th time steps of forward and backward GRU, where l is the number of layers. 

 Attention Module We utilize multi-head attention mechanism  (Vaswani et al., 2017)  to focus on more relevant parts of each historical utterance according to the target utterance. We also employ residual connection  followed by layer normalization  (Ba et al., 2016)  to make model training easier. The target-specific representations of historical utterances are obtained by: X = Concat(head 1 , ...head t )W O (1) head i = Attention(QW Q i , KW K i , V W V i ) (2) Attention(Q, K, V ) = sof tmax( QK T ? d k )V (3) where queries Q = R are representations of target utterances, keys K and values V are contextual word representations for words of historical utterances. W Q i ? R d?d k ,W K i ? R d?d k ,W V i ? R d?dv and W O ? R tdv?d are parameter matrices, where d k is the dimension of queries and keys, d v is the dimension of values, d is the dimension of the output of feature extraction module and t is the number of heads. X = [x 1 , x 2 , ..., x n?1 ] are target-specific representations of historical utterances. 

 Reasoning Module The reasoning module takes target-specific historical utterance representations [x 1 , x 2 , ...x n?1 ] and target utterance representations R as inputs. Target utterance representations are updated through time and layers. Each unit in this module takes two inputs: R and x i (1 ? i ? n ? 1). The t?th unit updates R according to x t by: z t = ?(x t , R) = ?(W z (x t ? R) + b z ) (4) r t = ?(x t , R) = ?(W r (x t ? R) + b r ) (5) Rt = ?(x t , R) = tanh(W h [x t ; R] + b h ) (6) R t = z t r t Rt + (1 ? z t )R t?1 (7) where z t is the update gate, r t is a reset function, Rt is the candidate of updated representation of target utterance and R t is the updated representation after observing the t?th historical utterance. ? is sigmoid activation, tanh is hyperbolic tangent activation, ? is element-wise vector multiplication, and [; ] is vector concatenation along the last dimension. W z ? R d?d , W r ? R d?d , W h ? R d?2d are weight matrices, b z ? R d , b r ? R d , b h ? R d are bias terms. Specifically, z t measures the relevance between the target utterance representation and the tth historical utterance representation for fine-controlled gating. Compared with global attention computed over all historical utterances, the gate can be considered as local attention which models short-term sequential dependency. r t is a reset function to determine how much previous information should be ignored by resetting the candidate of updated representation of target utterance. As shown in  Sukhbaatar et al. (2015) , multi-hop can perform reasoning over multiple facts more effectively. So we stack several layers with outputs of the current layer used as inputs to the next layer. Besides, to model more abundant information, we compute ? ? R l t and ? ? R l t in both forward and backward directions and add them together to get R l t as the updated representation of the t?th unit in l?th layer: R l t = ? ? R l t + ? ? R l t (8) Finally, we get the updated representation of target utterance R update = R L n?1 , where n ? 1 and L are the number of units and the number of layers in the reasoning module, respectively. 

 Output Module After the feature extraction and reasoning modules, we obtain the updated representation of target utterance. To preserve original semantic content, we concatenate the updated representation and the original representation together: R f inal = [R update ; R] (9) We use a fully connected layer with softmax as activation to calculate emotion-class probabilities: P = sof tmax(W f R f inal + b f ) (10) where  W f ? R d class ?2d is a weight matrix, b f ? R d class is 

 Experiment Datasets We perform experiments on two benchmarks: IEMOCAP  (Busso et al., 2008)  and AVEC  (Schuller et al., 2012) . They are multimodal datasets involved in two-way dynamic conversations. In this paper, we only focus on using textual modality to recognize the emotion. Baselines We compare the FERNet with following existing approaches: CNN  (Kim, 2014) , c-LSTM  (Poria et al., 2017) , c-LSTM+Attention  (Poria et al., 2017) , Memnet  (Ba et al., 2016) , CMN  (Hazarika et al., 2018b) , DialogueRNN . 

 Training Details The training details such as hyper-parameters and settings we used are shown in Appendices. 

 Results The overall results of experiments are shown in Table  1 . We can see that our model outperforms baselines significantly on all evaluation metrics of both datasets. Specifically, our model surpasses DialogueRNN by 1.69% on weighted average F1score. For AVEC dataset, our model lower mean absolute error by 0.03, 0.027, 0.009 and 0.31 for valence, arousal, expectancy and power, separately. We attribute the enhancement to the fundamental improvement of FERNet, which are generating target-specific representations of historical utterances and handling both short-term and long-term sequential dependencies. 

 Discussion and Analysis Parameters We conduct experiments with different values of the number of historical utterances (N ) and the number of layers of reasoning module (L) on the IEMOCAP dataset. Results are shown in Figure  4 . We observe that as N increases, the performance of the model tends to be improved. [1] No way, when? When, when, when did it happen?  [excited]  [1]Just a couple days ago.  [excited]  I can't believe it. [2] I never thought you would get married.  [excited]  [2]I know me neither.  [excited]  No way , when ? When , when , when did it happen ? <EOS> contributes to the performance of emotion recognition. However, a further increase of N degrades the performance of the model. It is mainly due to that there is too much-unrelated information confusing the model. As for L, the trend is similar to the parameter N . Models with hops in the range of 2-8 outperform the single layer variant. However, with L increasing, the reasoning module deepens and may cause the gradient vanishing problem which damages the performance of the model.    high pitched audio can provide vital information for recognizing the emotion. Besides, we find our model misclassifies several "excited" utterances as "happy" utterances, several "sad" utterances as "frustrated" utterances, and vice versa. The reason is that it is hard for the model to distinguish the subtle difference between these similar emotions. Besides, we perform qualitative visualization of the attention module. The dialogue in Figure  3  shows that for different target utterances, the model allocates different attention to words of a historical utterance. It demonstrates the effectiveness of the attention module. 

 Conclusion In this paper, we propose FERNet to solve the ERD task. The model generates target-specific historical utterances according to the content of the target utterance using attention mechanism. The reasoning module effectively handles both local and global sequential dependencies to update the original representation of the target utterance to a more informed vector. Our model achieves competitive performance on two benchmarks. Figure 1 : 1 Figure 1: A dialogue shows that modeling intricate contextual information is crucial for emotion recognition. 
