title
Counterfactual Off-Policy Training for Neural Dialogue Generation

abstract
Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken. The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch. Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space. An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.

Introduction Open-domain dialogue generation  (Shang et al., 2015a; Vinyals and Le, 2015; Sordoni et al., 2015a)  intends to produce coherent responses given dialogue history. Nevertheless, it suffers from data insufficiency problem as there may exist many potential responses for a given dialogue history  (Li et al., 2016) . An ideal way of exploring the potential responses is to train the model by chatting with real users, which is usually time-consuming and labor-intensive in practice. Although replacing a real user with a user simulator could address the issue, the simulator only roughly approximates real user statistics, and its development process is costly  (Su et al., 2016) . In contrast, humans could independently reason potential responses based on past experiences from the true environment. Having observed a response, one might naturally ask himself or herself: "What Figure  1 : An example of a counterfactual response, which is a potential response inferred in hindsight from given observed response. would happen if I respond differently, while everything else in the environment remains the same." Answering the question will result in a potential response (as an example in Figure  1 ), and it is beneficial for improving future decision making  (Roese, 1997) . The potential response inferred in hindsight is called a counterfactual response, where the concept "counterfactual" describes the posterior process of reasoning the outcome of alternative actions (i.e., a different responding policy) that could have been taken while keeping everything else unchanged  (Buesing et al., 2019) . Motivated by this, we propose a counterfactual off-policy training (COPT) approach to explore potential responses. Building upon the adversarial learning framework, COPT casts a dialogue generator as a structural causal model (SCM), which describes a generation process with two ingredients: scenarios and causal mechanisms  (Wright, 1920; Buesing et al., 2019) . The scenario is a random noise variable that captures all unobserved yet relevant aspects of the environment, i.e., user profiles. The causal mechanism is a deterministic function that takes a scenario and dialogue history as input and outputs a response. In this way, reasoning a counterfactual response in an observed response's environment can be achieved by feeding the scenario of the observed response into the causal mechanism. After generating the counterfactual response, the generator will receive a reward from a discriminator and optimize itself accordingly. Intuitively, a counterfactual response is synthesized by grounding the model in the scenario where an observed response occurs, rather than the scenario sampled from scratch as standard adversarial learning-based approaches. This improves the quality of the synthesized responses and subsequently benefits the model that learns from the synthesis. To verify the effectiveness of our approach, we conduct experiments on the public available DailyDialog dataset  (Li et al., 2017b) . Experimental results show that our approach significantly outperforms previous adversarial learning-based approaches in both automatic and human evaluations. The contributions of this paper are summarized as follows: ? We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. ? Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. ? Our approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. 

 Related Work Dialogue Generation Data-driven dialogue systems can be roughly divided into two categories: retrieval-based  (Leuski et al., 2006; Ji et al., 2014;  and generation-based  (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015) . Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses  (Li et al., 2016) . One way to address the generic response problem is by introducing external knowledge, such as keywords  (Mou et al., 2016; Zhu et al., 2019b) , topics  (Xing et al., 2017) , persona information  Song et al., 2019) , and retrieved candidate responses  (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a) . Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder  (Bowman et al., 2016; Zhao et al., 2017)  and the generative adversarial network  (Goodfellow et al., 2014; Li et al., 2017a; Tuan and Lee, 2019) . Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, making our approach an off-policy algorithm and benefits the exploration of potential responses. 

 Counterfactual Reasoning The counterfactual reasoning is a concept derived from psychology. It describes the human capacity to learn from experience by reasoning the outcome of an alternative action that could have been taken  (Pearl and Mackenzie, 2018) . Combined with the SCM, counterfactual reasoning improves the performance of policy evaluation in reinforcement learning  (Buesing et al., 2019; Oberst and Sontag, 2019) . In the area of NLP, counterfactual reasoning in previous work is mainly used for data augmentation  (Qin et al., 2019; Fu et al., 2020; Kaushik et al., 2020) , which rewrites the original data given a counterfactual label or condition. In this paper, we connect the concept of counterfactual reasoning with the dialogue generation and are the first to cast a generation model as an SCM under the adversarial learning framework. 

 Method We cast a dialogue generation model as an SCM to explore potential responses by counterfactual reasoning during the training process. We will first review the concept of the SCM (Sec. 3.2), and then introduce our COPT approach (Sec. 3.3). 

 Notation We use capital letters for random variables (e.g., V ), lowercase letters for instances of random variables (e.g., v), and bold letters for vectors (e.g., V = {V 1 , ..., V N }). During the training process, we denote the response generated by COPT as counterfactual response. In contrast, the response of standard adversarial learning-based dialogue  V 1 U 1 V 3 U 3 V 2 U 2 V 1 U 1 V 3 U 3 V 2 U 2 f 2 (V 1 , U 2 ) (denoted by the orange square) is replaced by f T 2 (V 1 , U 2 ) (denoted by the purple square). generation (i.e.,  REGS Li et al., 2017a)  is denoted as standard response. 

 Background: Structural Causal Model A structural causal model over random variables V = {V 1 , ..., V N } consists of independent noise random variables U = {U 1 , ..., U N } with distribution P U and deterministic functions F = {f 1 , ..., f N } such that V i = f i (P A i , U i ) , where P A i ? V are the parents of V i in a given DAG  (Buesing et al., 2019) . U is called scenarios, and F is called causal mechanisms. Figure 2 (Left) shows an example of an SCM. Each random variable V i is determined by its parents in V , U i , and f i , e.g., V 2 = f 2 (V 1 , U 2 ). During the training process, we cast a dialogue generation model as an SCM over two random variables: dialogue history X and response Y . This is achieved by converting the conditional distribution P (Y |X) into a deterministic function Y = f ? (X, U ) (for more details see Sec. 3.3). The scenario U is a random noise variable that captures all unobserved yet relevant properties, like user profiles. The causal mechanism is denoted as f ? to highlight the role of the policy (parameters) ? of the model. The dialogue generation SCM makes it possible to sample counterfactual responses in the scenario where observed responses occur. This improves the quality of synthesized responses and subsequently helps the model to explore the highreward area of the potential response space in the training process. Intervention in SCM Given an SCM, an intervention T is defined as the replacement of some causal mechanisms. Figure  2  shows an example of intervention. The original causal mechanism f 2 (V 1 , U 2 ) in the left SCM is replaced with f T 2 (V 1 , U 2 ) , resulting in a new SCM in the right. Accordingly, intervention in our dialogue gener-ation SCM denotes the update of the policy. For instance, the update from the behavior policy ? that generates observed responses to the target policy ? that we aim to learn is the intervention of replacing f ? (X, U ) with f ? (X, U ). Counterfactual Reasoning in SCM Given an SCM and observed a variable V i = v i , counterfactual reasoning answers the question: "What the variable V i would have been if I take an intervention T while remaining everything else unchanged". In this way, generating a counterfactual response can be seen as querying: "Having observed a response Y = y, what the response Y would have been if I take an intervention by following the target policy ?, rather than the behavior policy ? that generates the observed responses". Typically, counterfactual reasoning answers the question by the following steps (as Figure  3 ): ? Observed Y = y when X = x, infer the scenario u in hindsight from Y = f ? (X, U ). ? Take an intervention by replacing the causal mechanism f ? (X, U ) with f ? (X, U ). ? Reason a counterfactual response ? = f ? (x, u) by the resulting new SCM. In the following sections, we denote an observed response from the training set as Y and a modelgenerated response as ? . 

 Counterfactual Off-Policy Training Our COPT approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. Without loss of generality, we take the combination of COPT and the reward for every generation step (REGS) model  (Li et al., 2017a)  as an example in this section. It consists of two main components: a generator G and a discriminator D. Generator The generator G is a sequence to sequence (Seq2Seq) model  (Sutskever et al., 2014)  equipped with the attention mechanism  (Bahdanau et al., 2015; Luong et al., 2015) . During the encoding process, G reads the dialogue history into hidden states using an encoder LSTM  (Hochreiter and Schmidhuber, 1997) : H i = LSTM(X i , H i?1 ), (1) where X i is the i-th word of the dialogue history, and H i denotes the corresponding hidden state. The architecture of our COPT approach. ? is the target policy that we aim to learn. ? is the behavior policy that generates observed responses. First, we infer the scenario u where the observed response occurs. Then we update the policy from ? to ?, which can be seen as an intervention on the left SCM and results in the right SCM. Then, the counterfactual response is reasoned in the inferred scenario u by the causal mechanism Y = f ? (X, U ). 

 SCM At the j-th decoding time step, the hidden states are summarized into a context vector C j by the attention mechanism. Subsequently, G predicts the distribution of the next word over the vocabulary by a decoder LSTM: S j = LSTM([ e( ?j?1 ), C j ], S j?1 ), (2) P ? j ( ?j |X, ?1:j?1 ) = softmax(S j ? O), (3) where the bracket [?,?] denotes concatenation, and e(?) denotes the embedding of a word. S j is the j-th hidden state of the decoder LSTM. ?j?1 is the word generated in the previous time step. O is the output matrix. We use the superscript in P ? j to highlight the role of the policy (parameters of G). Adversarial learning-based dialogue generation model is optimized according to the reward of responses sampled from P ? j ( ?j |X, ?1:j?1 ) ? R |V | (abbreviated as P ? j in the following), where |V | is the vocabulary size. Using the Gumbel-Max Trick (Luce, 2012), the sampling process can be achieved by: ?j =arg max ?j (log P ? j + U j ), (4) where the element of U j follows the standard Gumbel distribution. In this way, the generator turns into a Gumbel-Max SCM  (Oberst and Sontag, 2019) , whose scenarios and causal mechanisms are represented by U j and Equation  4 , respectively. From the perspective of the SCM, each response is generated in a scenario. For instance, a standard response is produced in a scenario sampled from scratch. In contrast, the scenario for a counterfactual response is inferred from an observed response y = {y j | y j = arg max y j (log p * j + u j )}, where * is the user's policy that generates the observed response in the true environment. However, the user's policy is not available in practice, which hinders the posterior inference of the scenario. To this end, we introduce a behavior policy ? instead and learn it by minimizing the MLE loss on observed responses. In this way, an observed response can be seen as generating in a scenario u j while following the policy ?: y j = arg max y j (log p ? j + u j ). According to  Oberst and Sontag (2019) , there are two ways to infer the scenario u j in hindsight from y j = arg max y j (log p ? j + u j ) given y j and ?. One way is the rejection sampling, which samples u j from the standard Gumbel distribution and rejects those where y j = arg max y j (log p ? j + u j ). The other way of the posterior inference makes use of the properties of the shifted Gumbel g = log p ? j + u j : the maximum of g follows the standard Gumbel distribution and is independent with the argmax of g  (Maddison et al., 2014) . Therefore, g can be obtained by first sampling a maximum and then sampling the remaining elements truncated at the maximum. And u j is subsequently computed by subtracting log p ? j from g. We employ the second method to infer the scenario in COPT because it is more time-efficient than rejection sampling 1 . Given the scenario inferred from the observed response, COPT reasons the counterfactual response by feeding the dialogue history and the scenario into the SCM (Equation  4 ). Then the discriminator evaluates the counterfactual response and returns a reward to the generator. Note that the counterfactual response and the SCM are utilized for the training process. During the inference process, responses are generated in the same way as the standard adversarial learning-based dialogue generation (beam search or sampling from P ? j , we use the former in our approach) because the observed response is not available. 

 Discriminator The discriminator D provides a reward for each generation step. It takes as input the dialogue history X, the word ?j produced in the current generation step, and the prefix ?1:j?1 in previous steps, where ? ? {Y , ? } can be either an observed response or a model-generated response. The output reward D( ?j |X, ?1:j?1 ) is the probability that ?j is human-generated. Concretely, D first reads X and ?1:j with an encoder-decoder model. Then, it computes the reward by a Multi-Layer Perceptron (MLP), which takes as input the last hidden state of the decoder. 

 Adversarial Learning We train G and D under the adversarial learning framework, where G tries to fool D by generating human-like responses while D aims to distinguish between model-generated and human-generated (the observed) responses. Since a response is a sequence of discrete tokens, we pass by the gradient of D to G using the policy gradient algorithm. In this way, G converts into an agent whose partially generated response and parameters define a state and a policy, respectively. At each generation step, the agent takes an action by producing a word and observes a reward from D to update its policy. Note that there are two policies in COPT: the target policy that we aim to learn and the behavior policy used for the reasoning of scenarios. The behavior policy is pre-trained and then froze during adversarial learning because it aims to maximize the likelihood of a fixed set of observed responses. Introducing the behavior policy makes COPT an off-policy approach because the counterfactual response, from which the target policy learns, is not entirely based on the target policy itself. The goal of the generator is to minimize the negative expected reward: J G (?) = ?E ?1:j ?G D( ?j |X, ?1:j?1 ), where ? is the parameters of ?. The gradient of ? can be derived by the  end for 14: end for likelihood ratio trick (Williams, 1992): J G (?) = ? E ?1:j ?G D( ?j |X, ?1:j?1 ) ? log G ? ( ?j |X, ?1:j?1 ), (5) where G ? ( ?j |X, ?1:j?1 ) is the probability of generating ?j with the policy ? given X and ?1:j?1 . The discriminator distinguishes between observed responses and model-generated responses. This is achieved by minimizing the following loss: J D (?) = ? E Y 1:j ?S log D(Y j |X, Y 1:j?1 ) (6) ? E ?1:j ?G log(1 ? D( ?j |X, ?1:j?1 )), where ? is the parameters of D. As a positive instance, Y 1:j is a prefix randomly sampled from observed response set S. A negative instance ?1:j for training D is a prefix of a standard response, rather than a counterfactual response. This is because the latter is of higher quality than the former (as shown in Sec. 4.7).  

 Pre-training 

 Experiments 

 Data The experiments are conducted on the DailyDialog dataset  (Li et al., 2017b ). 2 It is a multi-turn dialogue dataset and covers various topics of daily life. The dataset has already been divided into training, validation, and test sets, as shown in Table  1 . Given a dialogue that consists of K utterances, we divide it into K-1 instances. Each instance has at most three continuous utterances. The last utterance is the response, and the previous utterances are concatenated as the dialogue history. 

 Baselines We compare COPT with the following dialogue generation models: ? HRED (Serban et al., 2016): The hierarchical recurrent encoder-decoder. An implementation by  Park et al. (2018)  is available 3 . ? REGS  (Li et al., 2017a) : Reward for every generation step. Its discriminator is trained on partially generated responses to provide a reward for each generation step. ? DPGAN : The diversitypromoting GAN introduces a language model based discriminator to encourage the generation of informative responses. 4   

 Training Details We implement REGS, StepGAN, and their variants with COPT using OpenNMT  (Klein et al., 2017) , an open-source framework for building sequence to sequence models. We manually tune the parameters according to the perplexity on the validation set. The vocabulary consists of the most frequent 10,000 words. Including more words (up to 17,438, the total number of DailyDialog vocabulary) observes no improvement but takes more time for training. We use 300 dimensional GloVe  (Pennington et al., 2014)  vectors to initialize word embeddings. Both the encoder and the decoder are a two-layer LSTM in G and a single layer LSTM in D. The number of hidden units is 500. During the adversarial learning process, we use the ADAM algorithm to alternately optimize G and D for one batch and five batches. The batch size is 64. We have tested the learning rate from 1e-6 to 1e-3. REGS+COPT and StepGAN+COPT achieve the best performance on 1e-5. The number of parameters for all the baselines is in a range of 21M to 26M. Equipping an adversarial learning baseline with COPT will introduce extra parameters with the same amount of the generator's parameters. Contributed by the behavior policy, the parameters are learned by pre-training, and COPT will not increase the number of trainable parameters in adversarial learning. Table  2  shows the average training time. COPT may increase the training time due to the posterior inference of scenarios. But it facilitates the exploration of the high-reward area of the potential response space and subsequently improves the quality of responses.    

 Evaluation Metrics Automatic Evaluation We evaluate the diversity and the relevance of generated responses using distinct  (Li et al., 2016)  and BLEU  (Papineni et al., 2002) , respectively. The distinct-k is the number of distinct k-grams normalized by the number of words of responses. Since BLEU might correlate weakly with human judgments of quality in the single-reference setting  (Liu et al., 2016) , we use the multi-reference DailyDialog test set  (Gupta et al., 2019) , where each instance is augmented with four human-written diverse responses.  5  Human Evaluation The human evaluation is conducted on 200 instances randomly sampled from the test set. We create a project on Amazon Mechanical Turk  (Buhrmester et al., 2016)  (AMT) and employ five AMT workers to give a preference between two responses generated by our approach and a baseline.  6  To maintain the quality of the evaluation, the task is visible to workers whose approve rate is greater than 95%, and the number of approved is greater than 500. 

 Results Table  3  shows the results of automatic evaluation. Both REGS and StepGAN outperform HRED in distinct-1 and distinct-2, indicating that adversarial learning is beneficial for improving the diversity of responses. There is no increase in DPGAN compared with HRED in our experiments. We believe this is because the scale of the DailyDialog dataset is not large enough for sufficiently training the language model based discriminator. For the same reason, COPT is not added to DPGAN. After introducing COPT, both distinct-1 and distinct-2 in REGS and StepGAN further increase, and the improvement is significant (t-test, p <0.01). This suggests that COPT is model-agnostic to adversarial learning-based approaches and helps to promote the diversity. In terms of BLEU in Table  3 , both REGS and StepGAN achieve higher BLEU scores with COPT, and the improvements of BLEU-1 and BLEU-2 are significant (p <0.05). This demonstrates the effectiveness of COPT in improving the relevance of responses. The less significant result of BLEU-3 and BLEU-4 is mainly due to the sparsity of tri-grams and four-grams, which are harder to be covered by references than uni-grams and bi-grams. The human evaluation results are shown in Table 5. Our approach is clearly preferred as it   has more winning instances than losing instances (p <0.01). The results indicate that COPT helps improve the quality of responses. Following  Zhou et al. (2018)  and  Ke et al. (2018) , we measure the agreement of annotators using inter-rater consistency. The percentage of instances that at least three annotators have the same preference (3/5 agreement) is 84.18%. The percentage for 4/5 agreement is 46.89%. 

 Case Study Table  4  shows an example of responses generated by baselines and our approach. The response of DPGAN sometimes is not fluent and can be very long. We believe this is also because the scale of the DailyDialog dataset is not enough for the language model discriminator. The response of HRED is not as informative as that of our approach. Its first part is generic, and what the pronoun "that" refers to is not clear. The response of StepGAN is not infor-mative enough as well. In contrast, the response of REGS is quite informative, but its content is not entirely relevant to the dialogue history. After introducing COPT, the responses of REGS+COPT and StepGAN+COPT propose offering a discount to address Person B's concern of the price, which is both informative and relevant. 

 Analysis To further analyze COPT's effectiveness in exploring the high-reward area of the potential response space during the training process, we compare the reward of a counterfactual response and a standard response on the same 10,000 randomly sampled training instances. However, the comparison between the two types of responses could be biased if their rewards are computed by different discriminators. Besides, the quality of responses is determined not only by the way they generated (with or without COPT) but also by the generator. To focus on the analysis of COPT and eliminate the bias between generators and discriminators, we generate and evaluate the two types of responses using an identical generator and its corresponding discriminator. Here, we use REGS+COPT and StepGAN+COPT as testbeds because they could generate both the two types of responses. Figure  4  shows the distribution of rewards and the average reward. The percentage of counterfactual responses in the high reward interval (0.66, 1.00] is higher than that of standard responses. Meanwhile, counterfactual responses generated with COPT achieve a higher average than standard responses. The results demonstrate the effectiveness of the counterfactual response in exploring the high-reward area of the potential response space during the training process. Note that the distribution and the average between different epochs are not comparable due to the update of the discriminator as the training processes. 

 Conclusion We propose a model-agnostic approach, COPT, that can be applied to any adversarial learning-based dialogue generation models. In contrast to existing approaches, it learns on counterfactual responses inferred from the structural causal model, taking advantage of observed responses. This helps the model to explore the high-reward area of the potential response space. Experiments show that the COPT significantly improves the quality of the generated responses, which demonstrates the effectiveness of this approach. Figure 2 : 2 Figure 2: An example of an SCM and an intervention. Left: An SCM with random variables V , scenarios U , and causal mechanisms F represented by colored squares. Right: A new SCM after taking an intervention on the left SCM. The original causal mechanism f2 (V 1 , U 2 ) (denoted by the orange square) is replaced by f T 2 (V 1 , U2) (denoted by the purple square). 
