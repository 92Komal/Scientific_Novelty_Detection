title
MIE: A Medical Information Extractor towards Medical Dialogues

abstract
Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues. 1

Introduction With the advancement of the informatization process of the medical system, Electronic Medical Records (EMRs) are required by an increasing number of hospitals all around the world. Compared with conventional medical records, EMRs are easy to save and retrieve, which bring considerable convenience for both patients and doctors. Furthermore, EMRs allow medical researchers to investigate the implicit contents included, such as epidemiologic study and patient cohorts finding. 

 ? Contribution during internship at Institute of Automation, Chinese Academy of Sciences. 1 Data and codes are available at https://github. com/nlpir2020/MIE-ACL-2020. Despite the advantages, most doctors complain that writing EMRs makes them exhausted  (Wachter and Goldsmith, 2018) . According to the study of  Sinsky et al. (2016) , physicians spend nearly two hours doing administrative work for every hour of facetime with patients, and the most time-consuming aspect is inputting EMRs. We believe that automatically converting doctorpatient dialogues into EMRs can effectively remove the heavy burdens of doctors, making them more deliberate to communicate with their patients. One straightforward approach is the end-to-end learning, where more supervised data, i.e., dialogue-EMR pairs are needed. Unfortunately, such data is hard to acquire in medical domain due to the privacy policy. In this paper, We focus on extracting medical information from dialogues, which we think is an essential step for EMR generation. Extracting information from medical dialogues is an emerging research field, and there are only few previous attempts.  Finley et al. (2018)  proposed an approach that consists of five stages to convert a clinical conversation to EMRs, but they do not describe the detail method.  Du et al. (   2019 ) also focused on extracting information from medical dialogues, and successfully defined a new task of extracting 186 symptoms and their corresponding status. The symptoms were relatively comprehensive, but they did not concern other key information like surgeries or tests.  Lin et al. (2019)  collected online medical dialogues to perform symptom recognition and symptom inference, i.e., inference the status of the recognized symptoms. They also used the sequential labeling method, incorporated global attention and introduced a static symptom graph. sions are much more diverse than general texts. There are many medical terms in the dialogue, but many of them are not uttered formally, which will lead to performance degradation of conventional Natural Language Processing (NLP) tools. b) Available information is scattered in various dialogue turns, thus the interaction between turns should be also considered. In order to meet these challenges, we first annotate the dialogues in a window-sliding style, as illustrated in Figure  1 . Then, we propose MIE, a Medical Information Extractor constructed on a deep matching model. We believe our annotation method could put up with informal expressions, and the proposed neural matching model is able to harness the turn-interactions. We collect doctor-patient dialogues from a popular Chinese online medical consultation website, Chunyu-Doctor 2 , where medical dialogues are in text format. We focus on the cardiology domain, because there are more inquiries and less tests than other departments. The annotation method considers both effectiveness and feasibility. We define four main categories, including symptoms, tests, surgeries and other information, and we further define frequent items in the categories and their corresponding status at the same time. There are two merits of our annotation method: a) the annotation is much easier than the sequential labeling manner and does not need the labelers to be medical experts; b) we can annotate the circumstances that a single label is expressed by multiple turns. We totally annotate 1,120 dialogues with 18,212 segmented windows and obtain more than 40k labels. We then develop MIE constructed on a novel neural matching model. MIE model consists of four main components, namely encoder module, matching module, aggregate module and scorer module. We conduct extensive experiments, and MIE achieves a overall F-score of 69.28, which indicates our proposed approach is a promising solution for the task. To sum up, the contributions of this paper are as follows: ? We propose a new dataset, annotating 1,120 doctor-patient dialogues from online consultation medical dialogues with more than 40k labels. The dataset will help the following researchers. ? We propose MIE, a medical information extractor based on a novel deep matching model that can make use of the interaction between dialogue turns. ? MIE achieves a promising overall F-score of 69.28, significantly surpassing several competitive baselines. 

 Related Work Extracting information from medical texts is a longterm objective for both biomedical and NLP community. For example, The 2010 i2b2 challenge provides a popular dataset still used in many recent researches  (Uzuner et al., 2011)  Extracting medical information from dialogues just gets started.  Finley et al. (2018)  proposed a pipeline method to generate EMRs. The approach contains five steps: dialogue role labeling, Automatic Speech Recognition (ASR), knowledge extraction, structured data processing and Natural Language Generation (NLG)  (Murty and Kabadi, 1987) . The most important part is knowledge extraction, which uses dictionary, regular expression and other supervised machine learning methods. However, the detailed explanations are left out, which make us hard to compare with them.  Du et al. (2019)  aimed at generating EMRs by extracting symptoms and their status. They defined 186 symptoms and three status, i.e., experienced, not experienced and other. They proposed two models to tackle the problem. Span-Attribute Tagging Model first predicted the span of a symptom, and then used the context features to further predict the symptom name and status. The seq2seq model took k dialogue turns as input, and then directly generated the symptom name and status. They collected incredible 90k dialogues and annotated 3k of them, but the dataset is not public. The most similar work to ours is  (Lin et al., 2019) , which also annotated Chinese online medical dialogues. Concretely, they annotated 2,067 dialogues with the BIO (begin-in-out) schema. There are two main components, namely symptom recognition and symptom inference in their approach. The former utilized both document-level and corpus-level attention enhanced Conditional Random Field (CRF) to acquire symptoms. The letter serves determining the symptom status. Our work differs from  (Du et al., 2019)  and  (Lin et al., 2019)  mainly in the following two points: a) we only extract 45 symptom items, but the status are more detailed, furthermore, we extract surgeries, tests and other information; b) we use different extracting method. Since the annotation system is different, our approach does not need the sequential labeling, which relieves the labeling work. 3 Corpus Description 

 Annotation Method We collect doctor-patient dialogues from a Chinese medical consultation website, Chunyu-Doctor. The dialogues are already in text format. We select cardiology topic consultations, since there are more inquiries, while dialogues of other topics often depend more on tests. A typical consultation dialogue is illustrated in Figure  1 . The principle of the annotation is to label useful information as comprehensive as possible. A commonly utilized annotation paradigm is sequential labeling, where the medical entities are labeled using BIO tags  (Du et al., 2019; Lin et al., 2019; Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016) . However, such annotation methods cannot label information that a) expressed by multiple turns and b) not explicitly or not consecutively expressed. Such situations are not rare in spoken dialogues, as can be seen in Figure  1 . To this end, we use a window-to-information annotation method instead of sequential labeling. As listed in Table  1 , we define four main categories, and for each category, we further define frequent items. The item quantity of symptom, surgery, test and other info is 45, 4, 16 and 6, respectively. In medical dialogues, status is quite crucial that cannot be ignored. For example, for a symptom, the status of appearance or absence is opposite for a particular diagnose. So it is necessary to carefully define status for each category. The status options vary with different categories, but we use unified labels for clarity. The exact meanings of the labels are also explained in Table  1 . The goal of annotation is to label all the predefined information mentioned in the current dialogue. As the dialogues turn to be too long, it is difficult for giving accurate labels when finishing reading them. Thus, we divide the dialogues into pieces using a sliding window. A window consists of multiple consecutive turns of the dialogue. It is worth noting that the window-sliding annotations can be converted into dialogue-based ones like dialogue state tracking task  (Mrk?i? et al., 2017) , the later annotation state will overwrite the old one. Here, the sliding window size is set to 5 as  Du et al. (2019)  did, because this size allows the included dialogue turns contain proper amount of information. For windows with less than 5 utterances, we pad them at the beginning with empty strings. The sliding step is set to 1. We invite three graduate students to label the dialogue windows. The annotators are guided by two physicians to ensure correctness. The segmented windows are randomly assigned to the annotators. In all, we annotate 1,120 dialogues, leading to 18,212 windows. We divide the data into train/develop/test sets of size 800/160/160 for dialogues and 12,931/2,587/2,694 for windows, respectively. In total, 46,151 labels are annotated, averaging 2.53 labels in each window, 41.21 labels in each dialogue. Note that about 12.83% of windows have no gold labels, i.e., there is no pre-defined information in those windows. The distribution of the labels is shown in Table  2 . The status distribution is shown in Table  3 . The annotation consistency, i.e., the cohen's kappa coefficient  (Fleiss and Cohen, 1973)  of the labeled data is 0.91, which means our annotation approach is feasible and easy to follow.  Table  3 : The distribution of status over all labels. 

 Evaluation Metrics We evaluate the extracted medical information results as ordinary information extraction task does, i.e., Precision, Recall and F-measure. To further discover the model behavior, we set up three evaluation metrics from easy to hard. Category performance is the most tolerant metric. It merely considers the correctness of the category. Item performance examines the correctness of both category and item, regardless of status. Full performance is the most strict metric, meaning that category, item and the corresponding status must be completely correct. We will report both window-level and dialoguelevel results. Window-level: We evaluate the results of each segmented window, and report the micro-average of all the test windows. Some windows have no gold labels, if the prediction on a window with no gold labels is also empty, it means the model performs well, so we set the Precision, Recall and F-measure to 1, otherwise 0. Dialogue-level: First we merge the results of the windows that belong to the same dialogue. For labels that are mutually exclusive, we update the old labels with the latest ones. Then we evaluate the results of each dialogue, and finally report the micro-average of all the test dialogues. 

 Our Approach In this section, we will elaborate the proposed MIE model, a novel deep matching neural network model. Deep matching models are widely used in multiple natural language processing tasks such as machine reading comprehension  (Seo et al., 2017; Yu et al.) , question answering  (Yang et al., 2016)  and dialogue generation  (Zhou et al., 2018; Wu et al., 2017) . Compared with classification models, matching models are able to introduce more information of the candidate side and promote interaction between both ends. The architecture of MIE is shown in Figure  2 . There are four main components, namely encoder module, matching module, aggregate module and scorer module. The input of MIE is a doctor-patient I can't breathe out. It seems that there is phlegm in my throat. 

 Has cardiac ultrasound been done? No, what medicine should I take for myocarditis? dialogue window, and the output is the predicted medical information. 

 Encoder Module The encoder is implemented by Bi-LSTM (Hochreiter and Schmidhuber, 1997) with self-attention  (Vaswani et al., 2017) . Let the input utterance be X = (x 1 , x 2 , ..., x l ), the encoder works as follows: H = BiLSTM(X) a[j] = W H[j] + b p = softmax(a) c = X j p[j]H[j] (1) We denote H, c = Encoder(X) for brevity. H consists contextual representations of every token in input sequence X, and c is a single vector that compresses the information of the entire sequence in a weighted way. We denote a window with n utterances as To introduce more oral information, we also add item-related colloquial expressions collected during the annotation to the end of V . Having defined the basic structure of the encoder, we now build representations for utterances U in the dialogue window, and the candidate category-item pair V and its status S: {U [1], ...U[n]}. H utt c [i], c utt c [i] = Encoder utt c (U [i]) H utt s [i], c utt s [i] = Encoder utt s (U [i]) H can c , c can c = Encoder can c (V ) H can s , c can s = Encoder can s (S) (2) Where the superscript utt and can represents utterance encoder and candidate encoder respectively, the subscript c and s represents category encoder and status encoder respectively, and i 2 [1, n] is the index of utterance in the dialogue window. All the candidates will be encoded in this step, but we only illustrate one in the figure and equations for brevity. Note that U , V , S is encoded with encoders differ from utterance to candidate and from category to status in order to make each encoder concentrate on one specific type (category-specific and status-specific) of information. 

 Matching Module In this step, the category-item representation is treated as a query in attention mechanism to calculate the attention values towards original utterances. Then we can obtain the category-specific representation of utterance U [i] as q c [i]. a c [i, j] = c can c ? H utt c [i, j] p c [i] = softmax(a c [i]) q c [i] = X j p c [i, j]H utt c [i, j] (3) Meanwhile, the status representation is treated as another query to calculate the attention values towards original utterances. Then we can obtain the status-specific representation of utterance U [i] as q s [i]. a s [i, j] = c can s ? H utt s [i, j] p s [i] = softmax(a s [i]) q s [i] = X j p s [i, j]H utt s [i, j] (4) Where [i, j] denotes the jth word in the ith utterance. The goal of this step is to capture the most relevant information from each utterance given a candidate. For example, if the category-item pair of the candidate is Symptom: Heart failure, the model will assign high attention values to the mentions of heart failure in utterances. If the status of the candidate is patient-positive, the attention values of expressions like "I have", "I've been diagnosed" will be high. So the matching module is important to determine the existence of a category-item pair and status related expressions. 

 Aggregate Module The matching module introduced above have captured the information of the existence of categoryitem pairs and status. To know whether a candidate is expressed in a dialogue window, we need to obtain the category-item pair information and its status information together. In particular, we need to match every category-item representation q c [i] with q s [i]. Sometimes the category-item pair information and its status information appear in the same utterance. But sometimes, they will appear in different utterances. For example, many question-answer pairs are adjacent utterances. So we need take the interactions between utterances into account. Based on this intuition, we define two kinds of strategies to get two different models. 

 MIE-single: The first strategy assumes that the category-item pair information and its status information appear in the same utterance. The representation of the candidate in the ith utterance is a simple concatenation of q c [i] and q s [i]: f [i] = concat(q c [i], q s [i]) (5) Where f [i] consists information of categoryitem pair and its status which can be used to predict the score of the related candidate. The model only considers the interaction within a single utterance. The acquired representations are independent from each other. This model is called MIE-single. 

 MIE-multi: The second strategy considers the interaction between the utterances. To obtain the related status information of other utterances, we treat q c [i] as a query to get the attention values towards the representations of status, i.e., q s . Then we can obtain the candidate representation of the utterance: a[i, k] = q c [i] T W q s [k] p[i] = softmax(a[i]) e q s [i] = X k p[i, k]q s [k] f [i] = concat(q c [i], e q s [i]) (6) Where W is a learned parameter, and e q s is the new representation of the status, containing the relative information of other utterances. The utterance order is an important clue in a dialogue window. For example, the category-item pair information can hardly related to status information whose utterance is too far. In order to capture this kind of information, we also take utterance position into account. Concretely, we add positional encoding  (Vaswani et al., 2017)  to each q c and q s at the beginning. We denote this model as MIE-multi. The output of the aggregate module contains the information of a entire candidate, including category-item and status information. 

 Scorer Module The output of the aggregate module is fed into a scorer module. We use each utterance's feature f [i] to score the candidate, as it is already the candidatespecific representation. The highest score of all the utterances in the window is the candidate's final score: s utt [i] = feedforward(f [i]) y = sigmoid(max(s utt [i])) (7) Where feedforward is a 4 layer full-connection neural network. 

 Learning The loss function is the cross entropy loss defined as follows: L = 1 KL X k X l y k l log(b y k l )+ (1 y k l ) log(1 b y k l ) (8) The superscript k denote the index of the training sample, and l is the index of the candidate. K and L are the number of samples and candidates respectively. b y k l is the true label of the training sample. 

 Inference There could be more than one answer in a dialogue window. In the inference phase, we reserve all the candidates whose matching score is higher than the threshold of 0.5. Since the training process is performed in the window size, the inference phase should be the same situation. We also obtain the dialogue-level results by updating the results of windows as aforementioned. 

 Experiments In this section, we will conduct experiments on the proposed dataset. It is worth to note that we are not going to compare MIE with  (Du et al., 2019)  and  (Lin et al., 2019) , because a) they all employed sequential labeling methods, leading to different evaluation dimensions from ours (theirs are more strict as they must give the exact symptom positions in the original utterance), and b) their approaches were customized for sequential labeling paradigm, thus cannot be re-implemented in our dataset. 

 Implementation We use pretrained 300-dimensional Skip-Gram  (Mikolov et al., 2013)  embeddings to represent chinese characters. We use Adam  (Kingma and Ba, 2015)  optimizer. The size of the hidden states of both feed-forward network and Bi-LSTM is 400. We apply dropout  (Srivastava et al., 2014)  with 0.2 drop rate to the output of each module and the hidden states of feed-forward network for regularization. We adopt early stopping using the F1 score on the development set. 

 Baselines We compare MIE with several baselines. 1) Plain-Classifier. We develop a basic classifier model that uses the simplest strategy to accomplish the task. The input of the model are the utterances in the window. We concatenate all the utterances to obtain a long sequence, and encode it using a Bi-LSTM encoder, then we use self-attention to represent it as a single vector. Next, the vector is fed into a feed-forward classifier network. The output labels of the classifier consist of all the possible candidates. The encoder adopts category-specific parameters. 2) MIE-Classifier. To develop a more competitive model, we reuse MIE model architecture to implement an advanced classifier model. The difference between the classifier model and MIE is the way of obtaining q c and q s . Instead of matching, the classifier model treats c utt c and c utt s directly as q c and q s respectively. Thanks to the attention mechanism in the encoder, the classifier model can also capture the category-item pair information and the status information to some extent. To further examine the effect of turn-interaction, we develop two classifiers as we do in MIE. MIE-Classifiersingle treats each utterance independently, and the probability score of each utterance is calculated. The model uses a max-pooling operation to get the final score. MIE-Classifier-multi considers the turn-interaction as MIE-multi does. 

 Main Results The experimental results are shown in Table  4 . From the results, we can obtain the following observations. 1) MIE-multi achieves the best F-score on both window-level and dialogue-level full evaluation metric, as we expected. The F-score reaches 66.40 and 69.28, which are considerable results in such sophisticated medical dialogues. 2) Both of the models using multi-turn interactions perform better than models solely using single utterance information, which further indicates the relations between turns play an important role in dialogues. The proposed approach can capture the interaction. As a proof, MIE-multi achieves a 2.01% F-score improvement in dialogue-level full evaluation. 3) Matching-based methods surpass classifier models in full evaluation. We think the results are rational because matching-based methods can introduce candidate representation. This also motivates us to leverage more background knowledge in the future. Note that in category and item metrics, MIE-classifiers are better at times, but they fail to correctly predict the status information. 4) Both MIE models and MIE-classifier models overwhelm Plain-Classifier model, which indicates the MIE architecture is far more effective than the basic LSTM representation concatenating method. 5) Dialogue-level performance is not always better than window-level performance in full evalua-     tion. In our experiment, the classifier-based models perform better in window-level than dialogue-level in full evaluation. The possible reason is error accumulation. When the model predicts results the current window does not support, the errors will be accumulated with the processing of the next window, which will decrease the performance. 

 Error Analysis To further analyze the behavior of MIE-multi, we print the confusion matrix of category-item predictions, as shown in Figure  3 . We denote the matrix as A, A[i][j] means the frequency of the circumstance that the true label is i while MIE-multi gives the answer j.  possible reason is that they rarely appear in the training set, with frequency of 0.63%, 2.63%, 2.38% and 1.25%, respectively. The results reveal that the data sparse and uneven problems are the bottlenecks of our approach. 

 Case Discussion 

 Attention Visualization In this part, we will analyze some cases to verify the effectiveness of the model with best performance, e.g. MIE-multi. Particularly, we investigate an example shown in Figure  4 . To determine whether the candidate Symptom:Coronary heart disease (patient-negative) is mentioned in the window, we should focus on the interaction between the adjacent pair located in the last of the window. This adjacent pair is a question-answer pair, the category-item pair information is in the question of the doctor while the status information is in the answer of the patient. In this case, MIE- single does not predict right result due to its independence between utterances, while MIE-multi manages to produce the correct result. For better understanding, we utilize visualization for matching module and aggregate module. Figure  4 (a) is the attention heat map when the categoryitem pair information vector c can c matches the utterances category representations H utt c . We can observe that the attention values of the mention of coronary heart disease are relatively high, which illustrates that the model can capture the correct category-item pair information in the window. Figure  4 (b) is the attention heat map when the status information c can s matches the utterances status representation H utt s . The attention values of the expressions related to status such as "Yes" and "No" are high, and the expression "No" is even higher. So MIE-multi can also capture the status information in the window. We also visualize the interaction between the fourth utterance and the other utterances. In Figure  4 (c), the score of the fifth utterance is the highest, which is in line with the fact that the fifth utterance is the most relevant utterance in the window. In this way the model successfully obtains the related status information for the category-item pair information in the window. In a nutshell, MIE-multi can properly capture the category-item pair and status information. 

 The Effectiveness of Turn Interaction We demostrate a case in Figure  5  that can explicitly show the need for turn interaction, where MIE-multi shows its advancement. In this case, the label Symptom:Sinus arrhythmia (patient-positive) requires turn interaction information. Specifically, in the third utterance, the patient omits the reason that makes him sick. However, under the complete context, we can infer the reason is the sinus arrhythmia, since the patient consulted the doctor at the beginning of the window. The model need to consider the interaction between different utterances to get the conclusion. Interaction-agnostic model like MIE-single makes prediction on single utterance, and then sums them up to get the final conclusion. Consequently, it fails to handle the case when the expressions of category-item and status are separated in different utterances. As a result, MIEsingle only obtains the category-item information Symptom:Sinus arrhythmia, but the status prediction is incorrect. In contrast, MIE-multi is able to capture the interaction between different utterances and predicts the label successfully. 

 Conclusion and Future Work In this paper, we first describe a new constructed corpus for the medical information extraction task, including the annotation methods and the evaluation metrics. Then we propose MIE, a deep neural matching model tailored for the task. MIE is able to capture the interaction information between the dialogue turns. To show the advantage of MIE, we develop several competitive baselines for comparison. The experimental results indicate that MIE is a promising solution for medical information extraction towards medical dialogues. In the future, we should further leverage the internal relations in the candidate end, and try to introduce rich medical background knowledge into our work. Figure 1 : 1 Figure1: A typical medical dialogue window and the corresponding annotated labels. "Pos" is short for "positive" and "neg" is short for "negative". Text color and label color are aligned for clarity. All the examples in the paper are translated from Chinese. 
