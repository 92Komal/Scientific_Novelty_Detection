title
A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking

abstract
Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance. In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations. We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training. Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new stateof-the-art performance with considerable improvements (+1.24% and +5.98%). 1

Introduction Recently, task-oriented dialogue systems have attracted increasing attention in both industry and academia due to their broad application for helping users accomplish tasks through spoken interactions  (Young, 2002; Young et al., 2013; Gao et al., 2019a) . Dialogue state tracking (DST) is an essential part of dialogue management in task-oriented dialogue systems. Given current utterances and dialogue history, DST aims to determine the set of Table  1 : An example dialogue. At the last turn, it is necessary to capture relevant information in dialogue history to correctly predict the value of slot "food", which is underlined. "User" and "Sys" represent user utterance and system response respectively, and the italic text means dialogue states. goals that a user tries to inform at each turn which are represented as slot-value pairs  Henderson et al., 2014a) . As Table  1  shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies  (Sharma et al., 2019; . However, traditional DST models usually determine dialogue states by considering only utterances at current turn  (Henderson et al., 2014b; Zhong et al., 2018; Chao and Lane, 2019)  which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances  Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b)  or only utilize partial history  (Ren et al., 2019; Sharma et al., 2019)  or lack direct interactions between slots and history . Briefly, these methods are deficient in exploiting relevant context from dialogue history. 

 R E T R A C T E D Furthermore, there are differences in the frequency of different slots and different slot-value pairs. For example, in MultiWOZ 2.0 train set, there are 15384 samples related to the slot "trainday" while 5843 for the slot "attraction-name"; the slot-value pair (attraction-area, center) occurs 5432 times and (taxi-departure, royal spice) occurs only 9 times; etc. We refer to this problem as "slot imbalance", which makes the learning difficulties of different slots varies (Refer to Appendix for details). However, existing approaches usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of those hard slots and eventually damages the overall performance. To address the two aforementioned problems, we propose an effective model equipped with a contextual hierarchical attention network (CHAN) to fully exploit relevant context from dialogue history, and an adaptive objective to alleviate the slot imbalance problem. In CHAN, the slot firstly retrieves word-level relevant information from utterances at each turn. Then, these word-level relevant information will be encoded into contextual representations by rich interactions. Finally, the slot aggregates all contextual representations into turnlevel relevant information and then we combine it with word-level relevant information to obtain the outputs. To further enhance the ability to exploit relevant context, we employ a state transition prediction task to assist DST learning. For the slot imbalance problem, our adaptive objective can dynamically evaluate the difficulties in an accuracysensitive manner and then adaptively adjust the learning weights for different slots. Thus, it can balance the learning of all slots as far as possible. We evaluate the effectiveness of our model on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. Experimental results show that our model reaches 52.68% and 58.55% joint accuracy, outperforming previous state-of-the-art by +1.24% and +5.98%, respectively. The ablation study also demonstrates each module's effectiveness in our model. Our contributions are as follows: ? We propose an effective contextual hierarchical attention network to fully exploit relevant context from dialogue history and employ a state transition prediction task to further enhance it. ? We design an adaptive objective to address the slot imbalance problem by dynamically adjusting the weight of each slot. To the best of our knowledge, our method is the first to address the slot imbalance problem in DST. ? Experimental results show that our model achieves state-of-the-art performance with significant improvements over all previous models. 

 Approach As shown in Figure  1 , the proposed model consists of three components: 1) the contextual hierarchical attention network (CHAN); 2) the state transition prediction module; 3) the adaptive objective. We share all the model parameters for each slot to keep our model universal for all slots. 

 Problem Statement Given a dialogue X = {(U 1 , R 1 ), ..., (U T , R T )} of T turns where U t represents user utterance and R t represents system response of turn t, we define the dialogue state at each turn t as B t = {(s, v t ), s ? S} where S is a set of slots and v t is the corresponding value of the slot s. Following , we use the term "slot" to refer to the concatenation of a domain name and a slot name in order to represent both domain and slot information. For example, "restaurant-food". Similar to , we decompose the dialogue state tracking to a multilabel classification problem where we score each value with slot-related features in a non-parametric way and then choose the best candidate. We also add a literally "none" into the value set of each slot to represent that no corresponding value is tracked. 

 Contextual Hierarchical Attention Network Recently the pre-trained BERT language model  (Devlin et al., 2019)  shows powerful ability in universal contextual semantics representation, thus we employ BERT to encode utterances, slots and values. To better retrieve relevant context from dialogue history, we devise Slot-Word Attention and Slot-Turn Attention to query both relevant keywords and turns. Specifically, we exploit a Context Encoder between word-level and turn-level attention to capture contextual representations of relevant information from dialogue history. Furthermore, we devise a Global-Local Fusion Gate to balance the information from global context and local utterances. 

 R E T R A C T E D BERT BERT BERT Rt 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " O o K y M v 0 w u U x y 9 K X L 1 m W v O + 5 i e 6 8 = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X V W w r 1 F K S 6 b Q O T Z M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s O b m F x a X i s u l l d W 1 9 Y 3 y 5 l Y z i V L J e I N F Q S R v f C / h g Q h 5 Q w k V 8 J t Y c m / k B 7 z l D 8 9 0 v H X P Z S K i 8 F q N Y 9 4 Z e Y N Q 9 A X z F F G t q 2 6 m D t x J t 1 x x q o 5 Z 9 i x w c 1 B B v u p R + Q W 3 6 C E C Q 4 o R O E I o w g E 8 J P S 0 4 c J B T F w H G X G S k D B x j g l K 5 E 1 J x U n h E T u k 7 4 B 2 7 Z w N a a 9 z J s b N 6 J S A X k l O G 3 v k i U g n C e v T b B N P T W b N / p Y 7 M z n 1 3 c b 0 9 / N c I 2 I V 7 o j 9 y z d V / t e n a 1 H o 4 8 T U I K i m 2 D C 6 O p Z n S U 1 X 9 M 3 t L 1 U p y h A T p 3 G P 4 p I w M 8 5 p n 2 3 j S U z t u r e e i b 8 Z p W b 1 n u X a F O / 6 l j R g 9 + c 4 Z 0 H z s O o 6 V f f y q F I 7 z U d d x A 5 2 s U / z P E Y N 5 6 i j Y a p 8 x B O e r Q t L W m M r + 5 R a h d y z j W / L e v g A H M 6 R v A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O o K y M v 0 w u U x y 9 K X L 1 m W v O + 5 i e 6 8 = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X V W w r 1 F K S 6 b Q O T Z M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s O b m F x a X i s u l l d W 1 9 Y 3 y 5 l Y z i V L J e I N F Q S R v f C / h g Q h 5 Q w k V 8 J t Y c m / k B 7 z l D 8 9 0 v H X P Z S K i 8 F q N Y 9 4 Z e Y N Q 9 A X z F F G t q 2 6 m D t x J t 1 x x q o 5 Z 9 i x w c 1 B B v u p R + Q W 3 6 C E C Q 4 o R O E I o w g E 8 J P S 0 4 c J B T F w H G X G S k D B x j g l K 5 E 1 J x U n h E T u k 7 4 B 2 7 Z w N a a 9 z J s b N 6 J S A X k l O G 3 v k i U g n C e v T b B N P T W b N / p Y 7 M z n 1 3 c b 0 9 / N c I 2 I V 7 o j 9 y z d V / t e n a 1 H o 4 8 T U I K i m 2 D C 6 O p Z n S U 1 X 9 M 3 t L 1 U p y h A T p 3 G P 4 p I w M 8 5 p n 2 3 j S U z t u r e e i b 8 Z p W b 1 n u X a F O / 6 l j R g 9 + c 4 Z 0 H z s O o 6 V f f y q F I 7 z U d d x A 5 2 s U / z P E Y N 5 6 i j Y a p 8 x B O e r Q t L W m M r + 5 R a h d y z j W / L e v g A H M 6 R v A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O o K y M v 0 w u U x y 9 K X L 1 m W v O + 5 i e 6 8 = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X V W w r 1 F K S 6 b Q O T Z M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s O b m F x a X i s u l l d W 1 9 Y 3 y 5 l Y z i V L J e I N F Q S R v f C / h g Q h 5 Q w k V 8 J t Y c m / k B 7 z l D 8 9 0 v H X P Z S K i 8 F q N Y 9 4 Z e Y N Q 9 A X z F F G t q 2 6 m D t x J t 1 x x q o 5 Z 9 i x w c 1 B B v u p R + Q W 3 6 C E C Q 4 o R O E I o w g E 8 J P S 0 4 c J B T F w H G X G S k D B x j g l K 5 E 1 J x U n h E T u k 7 4 B 2 7 Z w N a a 9 z J s b N 6 J S A X k l O G 3 v k i U g n C e v T b B N P T W b N / p Y 7 M z n 1 3 c b 0 9 / N c I 2 I V 7 o j 9 y z d V / t e n a 1 H o 4 8 T U I K i m 2 D C 6 O p Z n S U 1 X 9 M 3 t L 1 U p y h A T p 3 G P 4 p I w M 8 5 p n 2 3 j S U z t u r e e i b 8 Z p W b 1 n u X a F O / 6 l j R g 9 + c 4 Z 0 H z s O o 6 V f f y q F I 7 z U d d x A 5 2 s U / z P E Y N 5 6 i j Y a p 8 x B O e r Q t L W m M r + 5 R a h d y z j W / L e v g A H M 6 R v A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O o K y M v 0 w u U x y 9 K X L 1 m W v O + 5 i e 6 8 = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X V W w r 1 F K S 6 b Q O T Z M w m Q g l d O c P u N U P E / 9 A / 8 I 7 Y w p q E Z 2 Q 5 M y 5 5 9 y Z e 6 8 f B y J R j v N a s O b m F x a X i s u l l d W 1 9 Y 3 y 5 l Y z i V L J e I N F Q S R v f C / h g Q h 5 Q w k V 8 J t Y c m / k B 7 z l D 8 9 0 v H X P Z S K i 8 F q N Y 9 4 Z e Y N Q 9 A X z F F G t q 2 6 m D t x J t 1 x x q o 5 Z 9 i x w c 1 B B v u p R + Q W 3 6 C E C Q 4 o R O E I o w g E 8 J P S 0 4 c J B T F w H G X G S k D B x j g l K 5 E 1 J x U n h E T u k 7 4 B 2 7 Z w N a a 9 z J s b N 6 J S A X k l O G 3 v k i U g n C e v T b B N P T W b N / p Y 7 M z n 1 3 c b 0 9 / N c I 2 I V 7 o j 9 y z d V / t e n a 1 H o 4 8 T U I K i m 2 D C 6 O p Z n S U 1 X 9 M 3 t L 1 U p y h A T p 3 G P 4 p I w M 8 5 p n 2 3 j S U z t u r e e i b 8 Z p W b 1 n u X a F O / 6 l j R g 9 + c 4 Z 0 H z s O o 6 V f f y q F I 7 z U d d x A 5 2 s U / z P E Y N 5 6 i j Y a p 8 x B O e r Q t L W m M r + 5 R a h d y z j W / L e v g A H M 6 R v A = = < / l a t e x i t > U t 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 b r L d 0 Q C Z i 9 r f a K o P A U I r G u h r I = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X F U x b q K U k 0 2 k N z Y v J R C i h O 3 / A r X 6 Y + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8 4 9 5 8 7 c e 7 0 k 8 F N p W a 8 l Y 2 F x a X m l v F p Z W 9 / Y 3 K p u 7 7 T S O B O M O y w O Y t H x 3 J Q H f s Q d 6 c u A d x L B 3 d A L e N s b X 6 h 4 + 5 6 L 1 I + j G z l J e C 9 0 R 5 E / 9 J k r i W o 7 / V w e 2 d N + t W b V L b 3 M e W A X o I Z i N e P q C 2 4 x Q A y G D C E 4 I k j C A V y k 9 H R h w 0 J C X A 8 5 c Y K Q r + M c U 1 T I m 5 G K k 8 I l d k z f E e 2 6 B R v R X u V M t Z v R K Q G 9 g p w m D s g T k 0 4 Q V q e Z O p 7 p z I r 9 L X e u c 6 q 7 T e j v F b l C Y i X u i P 3 L N 1 P + 1 6 d q k R j i T N f g U 0 2 J Z l R 1 r M i S 6 a 6 o m 5 t f q p K U I S F O 4 Q H F B W G m n b M + m 9 q T 6 t p V b 1 0 d f 9 N K x a o 9 K 7 Q Z 3 t U t a c D 2 z 3 H O g 9 Z x 3 b b q 9 v V J r X F e j L q M P e z j k O Z 5 i g Y u 0 Y S j q 3 z E E 5 6 N K 0 M Y E y P / l B q l w r O L b 8 t 4 + A A k A J G / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 b r L d 0 Q C Z i 9 r f a K o P A U I r G u h r I = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X F U x b q K U k 0 2 k N z Y v J R C i h O 3 / A r X 6 Y + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8 4 9 5 8 7 c e 7 0 k 8 F N p W a 8 l Y 2 F x a X m l v F p Z W 9 / Y 3 K p u 7 7 T S O B O M O y w O Y t H x 3 J Q H f s Q d 6 c u A d x L B 3 d A L e N s b X 6 h 4 + 5 6 L 1 I + j G z l J e C 9 0 R 5 E / 9 J k r i W o 7 / V w e 2 d N + t W b V L b 3 M e W A X o I Z i N e P q C 2 4 x Q A y G D C E 4 I k j C A V y k 9 H R h w 0 J C X A 8 5 c Y K Q r + M c U 1 T I m 5 G K k 8 I l d k z f E e 2 6 B R v R X u V M t Z v R K Q G 9 g p w m D s g T k 0 4 Q V q e Z O p 7 p z I r 9 L X e u c 6 q 7 T e j v F b l C Y i X u i P 3 L N 1 P + 1 6 d q k R j i T N f g U 0 2 J Z l R 1 r M i S 6 a 6 o m 5 t f q p K U I S F O 4 Q H F B W G m n b M + m 9 q T 6 t p V b 1 0 d f 9 N K x a o 9 K 7 Q Z 3 t U t a c D 2 z 3 H O g 9 Z x 3 b b q 9 v V J r X F e j L q M P e z j k O Z 5 i g Y u 0 Y S j q 3 z E E 5 6 N K 0 M Y E y P / l B q l w r O L b 8 t 4 + A A k A J G / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 b r L d 0 Q C Z i 9 r f a K o P A U I r G u h r I = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X F U x b q K U k 0 2 k N z Y v J R C i h O 3 / A r X 6 Y + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8 4 9 5 8 7 c e 7 0 k 8 F N p W a 8 l Y 2 F x a X m l v F p Z W 9 / Y 3 K p u 7 7 T S O B O M O y w O Y t H x 3 J Q H f s Q d 6 c u A d x L B 3 d A L e N s b X 6 h 4 + 5 6 L 1 I + j G z l J e C 9 0 R 5 E / 9 J k r i W o 7 / V w e 2 d N + t W b V L b 3 M e W A X o I Z i N e P q C 2 4 x Q A y G D C E 4 I k j C A V y k 9 H R h w 0 J C X A 8 5 c Y K Q r + M c U 1 T I m 5 G K k 8 I l d k z f E e 2 6 B R v R X u V M t Z v R K Q G 9 g p w m D s g T k 0 4 Q V q e Z O p 7 p z I r 9 L X e u c 6 q 7 T e j v F b l C Y i X u i P 3 L N 1 P + 1 6 d q k R j i T N f g U 0 2 J Z l R 1 r M i S 6 a 6 o m 5 t f q p K U I S F O 4 Q H F B W G m n b M + m 9 q T 6 t p V b 1 0 d f 9 N K x a o 9 K 7 Q Z 3 t U t a c D 2 z 3 H O g 9 Z x 3 b b q 9 v V J r X F e j L q M P e z j k O Z 5 i g Y u 0 Y S j q 3 z E E 5 6 N K 0 M Y E y P / l B q l w r O L b 8 t 4 + A A k A J G / < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " I 0 b r L d 0 Q C Z i 9 r f a K o P A U I r G u h r I = " > A A A C y n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w Y 0 l E 0 G X R j Q s X F U x b q K U k 0 2 k N z Y v J R C i h O 3 / A r X 6 Y + A f 6 F 9 4 Z U 1 C L 6 I Q k Z 8 4 9 5 8 7 c e 7 0 k 8 F N p W a 8 l Y 2 F x a X m l v F p Z W 9 / Y 3 K p u 7 7 T S O B O M O y w O Y t H x 3 J Q H f s Q d 6 c u A d x L B 3 d A L e N s b X 6 h 4 + 5 6 L 1 I + j G z l J e C 9 0 R 5 E / 9 J k r i W o 7 / V w e 2 d N + t W b V L b 3 M e W A X o I Z i N e P q C 2 4 x Q A y G D C E 4 I k j C A V y k 9 H R h w 0 J C X A 8 5 c Y K Q r + M c U 1 T I m 5 G K k 8 I l d k z f E e 2 6 B R v R X u V M t Z v R K Q G 9 g p w m D s g T k 0 4 Q V q e Z O p 7 p z I r 9 L X e u c 6 q 7 T e j v F b l C Y i X u i P 3 L N 1 P + 1 6 d q k R j i T N f g U 0 2 J Z l R 1 r M i S 6 a 6 o m 5 t f q p K U I S F O 4 Q H F B W G m n b M + m 9 q T 6 t p V b 1 0 d f 9 N K x a o 9 K 7 Q Z 3 t U t a c D 2 z 3 H O g 9 Z x 3 b b q 9 v V J r X F e j L q M P e z j k O Z 5 i g Y u 0 Y S j q 3 z E E 5 6 N K 0 M Y E y P / l B q l w r O L b 8 t 4 + A A k A J G / < / l a t e x i t > Rt < l a t e x i t s h a 1 _ b a s e 6 4 = " U y E 0 k R p 2 U G 4 B n L H x 7 2 9 R u p G e T i g = " > A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V 9 9 A G 1 l G Q 6 r U P z I p k o U g R / w K 1 + m v g H + h f e G a e g F t E J S c 6 c e 8 + Z u f f 6 S S A y 6 T i v B W t u f m F x q b h c W l l d W 9 8 o b 2 6 1 s j h P G W + y O I j T j u 9 l P B A R b 0 o h A 9 5 J U u 6 F f s D b / v h U x d s 3 P M 1 E H F 3 K u 4 T 3 Q m 8 U i a F g n i T q 4 r w v + + W K U 3 X 0 s m e B a 0 A F Z j X i 8 g u u M E A M h h w h O C J I w g E 8 Z P R 0 4 c J B Q l w P E + J S Q k L H O e 5 R I m 1 O W Z w y P G L H 9 B 3 R r m v Y i P b K M 9 N q R q c E 9 K a k t L F H m p j y U s L q N F v H c + 2 s 2 N + 8 J 9 p T 3 e 2 O / r 7 x C o m V u C b 2 L 9 0 0 8 7 8 6 V Y v E E M e 6 B k E 1 J Z p R 1 T H j k u u u q J v b X 6 q S 5 J A Q p / C A 4 i l h p p X T P t t a k + n a V W 8 9 H X / T m Y p V e 2 Z y c 7 y r W 9 K A 3 Z / j n A W t g 6 r r V N 2 z w 0 r t x I y 6 i B 3 s Y p / m e Y Q a 6 m i g S d 4 j P O I J z 1 b d i q z c u v 1 M t Q p G s 4 1 v y 3 r 4 A F K f k D 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " U y E 0 k R p 2 U G 4 B n L H x 7 2 9 R u p G e T i g = " > A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V 9 9 A G 1 l G Q 6 r U P z I p k o U g R / w K 1 + m v g H + h f e G a e g F t E J S c 6 c e 8 + Z u f f 6 S S A y 6 T i v B W t u f m F x q b h c W l l d W 9 8 o b 2 6 1 s j h P G W + y O I j T j u 9 l P B A R b 0 o h A 9 5 J U u 6 F f s D b / v h U x d s 3 P M 1 E H F 3 K u 4 T 3 Q m 8 U i a F g n i T q 4 r w v + + W K U 3 X 0 s m e B a 0 A F Z j X i 8 g u u M E A M h h w h O C J I w g E 8 Z P R 0 4 c J B Q l w P E + J S Q k L H O e 5 R I m 1 O W Z w y P G L H 9 B 3 R r m v Y i P b K M 9 N q R q c E 9 K a k t L F H m p j y U s L q N F v H c + 2 s 2 N + 8 J 9 p T 3 e 2 O / r 7 x C o m V u C b 2 L 9 0 0 8 7 8 6 V Y v E E M e 6 B k E 1 J Z p R 1 T H j k u u u q J v b X 6 q S 5 J A Q p / C A 4 i l h p p X T P t t a k + n a V W 8 9 H X / T m Y p V e 2 Z y c 7 y r W 9 K A 3 Z / j n A W t g 6 r r V N 2 z w 0 r t x I y 6 i B 3 s Y p / m e Y Q a 6 m i g S d 4 j P O I J z 1 b d i q z c u v 1 M t Q p G s 4 1 v y 3 r 4 A F K f k D 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " U y         Sentence Encoder. BERT leverages a special token  [CLS]  to aggregate the whole representation of a sentence and a special token  [SEP]  to indicate the end of a sentence. For user utterance U t = {w u 1 , ..., w u l } and system response R t = {w r 1 , ..., w r l } at dialogue turn t, we concatenate them with special tokens and encode them into contextual word representations h t as follows: E 0 k R p 2 U G 4 B n L H x 7 2 9 R u p G e T i g = " > A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V 9 9 A G 1 l G Q 6 r U P z I p k o U g R / w K 1 + m v g H + h f e G a e g F t E J S c 6 c e 8 + Z u f f 6 S S A y 6 T i v B W t u f m F x q b h c W l l d W 9 8 o b 2 6 1 s j h P G W + y O I j T j u 9 l P B A R b 0 o h A 9 5 J U u 6 F f s D b / v h U x d s 3 P M 1 E H F 3 K u 4 T 3 Q m 8 U i a F g n i T q 4 r w v + + W K U 3 X 0 s m e B a 0 A F Z j X i 8 g u u M E A M h h w h O C J I w g E 8 Z P R 0 4 c J B Q l w P E + J S Q k L H O e 5 R I m 1 O W Z w y P G L H 9 B 3 R r m v Y i P b K M 9 N q R q c E 9 K a k t L F H m p j y U s L q N F v H c + 2 s 2 N + 8 J 9 p T 3 e 2 O / r 7 x C o m V u C b 2 L 9 0 0 8 7 8 6 V Y v E E M e 6 B k E 1 J Z p R 1 T H j k u u u q J v b X 6 q S 5 J A Q p / C A 4 i l h p p X T P t t a k + n a V W 8 9 H X / T m Y p V e 2 Z y c 7 y r W 9 K A 3 Z / j n A W t g 6 r r V N 2 z w 0 r t x I y 6 i B 3 s Y p / m e Y Q a 6 m i g S d 4 j P O I J z 1 b d i q z c u v 1 M t Q p G s 4 1 v y 3 r 4 A F K f k D 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " U y E 0 k R p 2 U G 4 B n L H x 7 2 9 R u p G e T i g = " > A A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V 9 9 A G 1 l G Q 6 r U P z I p k o U g R / w K 1 + m v g H + h f e G a e g F t E J S c 6 c e 8 + Z u f f 6 S S A y 6 T i v B W t u f m F x q b h c W l l d W 9 8 o b 2 6 1 s j h P G W + y O I j T j u 9 l P B A R b 0 o h A 9 5 J U u 6 F f s D b / v h U x d s 3 P M 1 E H F 3 K u 4 T 3 Q m 8 U i a F g n i T q 4 r w v + + W K U 3 X 0 s m e B a 0 A F Z j X i 8 g u u M E A M h h w h O C J I w g E 8 Z P R 0 4 c J B Q l w P E + J S Q k L H O e 5 R I m 1 O W Z w y P G L H 9 B 3 R r m v Y i P b K M 9 N q R q c E 9 K a k t L F H m p j y U s L q N F v H c + 2 s 2 N + 8 J 9 p T 3 e 2 O / r 7 x C o m V u C b 2 L 9 0 0 8 7 8 6 V Y v E E M e 6 B k E 1 J Z p R 1 T H j k u u u q J v b X 6 q S 5 J A Q p / C A 4 i l h p p X T P t t a k + n a V W 8 9 H X / T m Y p V e 2 Z y c 7 y r W 9 K A 3 Z / j n A W t g 6 r r V N 2 z w 0 r t x I y 6 i B 3 s Y p / m e Y Q a 6 m i g S d 4 j P O I J z 1 b d i q z c u v 1 M t Q p G s 4 1 v y 3 r 4 A F K f k D 4 = < / l a t e x i t > ? BERT h 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " O E z c p b e 0 R v P B f D F f p 6 0 9 4 M T 9 j F 8 = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n 9 r R b L F l l S y 9 z H t g Z K C F b 1 a j 4 g j Z 6 i O A h R Q C G E J K w D w c J P S 3 Y s B A T 1 8 G E O E G I 6 z j D F A X S p p T F K M M h d k T f A e 1 a G R v S X n k m W u 3 R K T 6 9 g p Q m D k g T U Z 4 g r E 4 z d T z V z o r 9 z X u i P d X d x v R 3 M 6 + A W I k h s X / p Z p n / 1 a l a J P o 4 1 T V w q i n W j K r O y 1 x S 3 R V 1 c / N L V Z I c Y u I U 7 l F c E P a 0 c t Z n U 2 s S X b v q r a P j b z p T s W r v Z b k p 3 t U t a c D 2 z 3 H O g / p R 2 b b K 9 u V x q X K W j T q P P e z j k O Z 5 g g o u U E W N v A U e 8 Y R n 4 8 o Y G 3 f G / W e q k c s 0 u / i 2 j I c P 7 6 m V P Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O E z c p b e 0 R v P B f D F f p 6 0 9 4 M T 9 j F 8 = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n 9 r R b L F l l S y 9 z H t g Z K C F b 1 a j 4 g j Z 6 i O A h R Q C G E J K w D w c J P S 3 Y s B A T 1 8 G E O E G I 6 z j D F A X S p p T F K M M h d k T f A e 1 a G R v S X n k m W u 3 R K T 6 9 g p Q m D k g T U Z 4 g r E 4 z d T z V z o r 9 z X u i P d X d x v R 3 M 6 + A W I k h s X / p Z p n / 1 a l a J P o 4 1 T V w q i n W j K r O y 1 x S 3 R V 1 c / N L V Z I c Y u I U 7 l F c E P a 0 c t Z n U 2 s S X b v q r a P j b z p T s W r v Z b k p 3 t U t a c D 2 z 3 H O g / p R 2 b b K 9 u V x q X K W j T q P P e z j k O Z 5 g g o u U E W N v A U e 8 Y R n 4 8 o Y G 3 f G / W e q k c s 0 u / i 2 j I c P 7 6 m V P Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O E z c p b e 0 R v P B f D F f p 6 0 9 4 M T 9 j F 8 = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n 9 r R b L F l l S y 9 z H t g Z K C F b 1 a j 4 g j Z 6 i O A h R Q C G E J K w D w c J P S 3 Y s B A T 1 8 G E O E G I 6 z j D F A X S p p T F K M M h d k T f A e 1 a G R v S X n k m W u 3 R K T 6 9 g p Q m D k g T U Z 4 g r E 4 z d T z V z o r 9 z X u i P d X d x v R 3 M 6 + A W I k h s X / p Z p n / 1 a l a J P o 4 1 T V w q i n W j K r O y 1 x S 3 R V 1 c / N L V Z I c Y u I U 7 l F c E P a 0 c t Z n U 2 s S X b v q r a P j b z p T s W r v Z b k p 3 t U t a c D 2 z 3 H O g / p R 2 b b K 9 u V x q X K W j T q P P e z j k O Z 5 g g o u U E W N v A U e 8 Y R n 4 8 o Y G 3 f G / W e q k c s 0 u / i 2 j I c P 7 6 m V P Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " O E z c p b e 0 R v P B f D F f p 6 0 9 4 M T 9 j F 8 = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n 9 r R b L F l l S y 9 z H t g Z K C F b 1 a j 4 g j Z 6 i O A h R Q C G E J K w D w c J P S 3 Y s B A T 1 8 G E O E G I 6 z j D F A X S p p T F K M M h d k T f A e 1 a G R v S X n k m W u 3 R K T 6 9 g p Q m D k g T U Z 4 g r E 4 z d T z V z o r 9 z X u i P d X d x v R 3 M 6 + A W I k h s X / p Z p n / 1 a l a J P o 4 1 T V w q i n W j K r O y 1 x S 3 R V 1 c / N L V Z I c Y u I U 7 l F c E P a 0 c t Z n U 2 s S X b v q r a P j b z p T s W r v Z b k p 3 t U t a c D 2 z 3 H O g / p R 2 b b K 9 u V x q X K W j T q P P e z j k O Z 5 g g o u U E W N v A U e 8 Y R n 4 8 o Y G 3 f G / W e q k c s 0 u / i 2 j I c P 7 6 m V P Q = = < / l a t e x i t > h t 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " e U B D l w X l i n U p x Y a e K D 1 b K X c w X 7 g = " > A A A C 1 X i c j V H L S s N A F D 2 N r 1 p f U Z d u g k V w Y 0 l E 0 G X R j c s K 9 g F t K U k 6 b U P z I p k U S u h O 3 P o D b v W X x D / Q v / D O O A W 1 i E 5 I c u b c e 8 7 M v d e J f S / l p v l a 0 J a W V 1 b X i u u l j c 2 t 7 R 1 9 d 6 + R R l n i s r o b + V H S c u y U + V 7 I 6 t z j P m v F C b M D x 2 d N Z 3 w l 4 s 0 J S 1 I v C m / 5 N G b d w B 6 G 3 s B z b U 5 U T 9 c 7 g c 1 H z i A f z X o 5 P 7 F m P b 1 s V k y 5 j E V g K V C G W r V I f 0 E H f U R w k S E A Q w h O 2 I e N l J 4 2 L J i I i e s i J y 4 h 5 M k 4 w w w l 0 m a U x S j D J n Z M 3 y H t 2 o o N a S 8 8 U 6 l 2 6 R S f 3 o S U B o 5 I E 1 F e Q l i c Z s h 4 J p 0 F + 5 t 3 L j 3 F 3 a b 0 d 5 R X Q C z H i N i / d P P M / + p E L R w D X M g a P K o p l o y o z l U u m e y K u L n x p S p O D j F x A v c p n h B 2 p X L e Z 0 N q U l m 7 6 K 0 t 4 2 8 y U 7 B i 7 6 r c D O / i l j R g 6 + c 4 F 0 H j t G K Z F e v m r F y 9 V K M u 4 g C H O K Z 5 n q O K a 9 R Q J + 8 J H v G E Z 6 2 p z b Q 7 7 f 4 z V S s o z T 6 + L e 3 h A 1 g q l i M = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " e U B D l w X l i n U p x Y a e K D 1 b K X c w X 7 g = " > A A A C 1 X i c j V H L S s N A F D 2 N r 1 p f U Z d u g k V w Y 0 l E 0 G X R j c s K 9 g F t K U k 6 b U P z I p k U S u h O 3 P o D b v W X x D / Q v / D O O A W 1 i E 5 I c u b c e 8 7 M v d e J f S / l p v l a 0 J a W V 1 b X i u u l j c 2 t 7 R 1 9 d 6 + R R l n i s r o b + V H S c u y U + V 7 I 6 t z j P m v F C b M D x 2 d N Z 3 w l 4 s 0 J S 1 I v C m / 5 N G b d w B 6 G 3 s B z b U 5 U T 9 c 7 g c 1 H z i A f z X o 5 P 7 F m P b 1 s V k y 5 j E V g K V C G W r V I f 0 E H f U R w k S E A Q w h O 2 I e N l J 4 2 L J i I i e s i J y 4 h 5 M k 4 w w w l 0 m a U x S j D J n Z M 3 y H t 2 o o N a S 8 8 U 6 l 2 6 R S f 3 o S U B o 5 I E 1 F e Q l i c Z s h 4 J p 0 F + 5 t 3 L j 3 F 3 a b 0 d 5 R X Q C z H i N i / d P P M / + p E L R w D X M g a P K o p l o y o z l U u m e y K u L n x p S p O D j F x A v c p n h B 2 p X L e Z 0 N q U l m 7 6 K 0 t 4 2 8 y U 7 B i 7 6 r c D O / i l j R g 6 + c 4 F 0 H j t G K Z F e v m r F y 9 V K M u 4 g C H O K Z 5 n q O K a 9 R Q J + 8 J H v G E Z 6 2 p z b Q 7 7 f 4 z V S s o z T 6 + L e 3 h A 1 g q l i M = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " e U B D l w X l i n U p x Y a e K D 1 b K X c w X 7 g = " > A A A C 1 X i c j V H L S s N A F D 2 N r 1 p f U Z d u g k V w Y 0 l E 0 G X R j c s K 9 g F t K U k 6 b U P z I p k U S u h O 3 P o D b v W X x D / Q v / D O O A W 1 i E 5 I c u b c e 8 7 M v d e J f S / l p v l a 0 J a W V 1 b X i u u l j c 2 t 7 R 1 9 d 6 + R R l n i s r o b + V H S c u y U + V 7 I 6 t z j P m v F C b M D x 2 d N Z 3 w l 4 s 0 J S 1 I v C m / 5 N G b d w B 6 G 3 s B z b U 5 U T 9 c 7 g c 1 H z i A f z X o 5 P 7 F m P b 1 s V k y 5 j E V g K V C G W r V I f 0 E H f U R w k S E A Q w h O 2 I e N l J 4 2 L J i I i e s i J y 4 h 5 M k 4 w w w l 0 m a U x S j D J n Z M 3 y H t 2 o o N a S 8 8 U 6 l 2 6 R S f 3 o S U B o 5 I E 1 F e Q l i c Z s h 4 J p 0 F + 5 t 3 L j 3 F 3 a b 0 d 5 R X Q C z H i N i / d P P M / + p E L R w D X M g a P K o p l o y o z l U u m e y K u L n x p S p O D j F x A v c p n h B 2 p X L e Z 0 N q U l m 7 6 K 0 t 4 2 8 y U 7 B i 7 6 r c D O / i l j R g 6 + c 4 F 0 H j t G K Z F e v m r F y 9 V K M u 4 g C H O K Z 5 n q O K a 9 R Q J + 8 J H v G E Z 6 2 p z b Q 7 7 f 4 z V S s o z T 6 + L e 3 h A 1 g q l i M = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " e U B D l w X l i n U p x Y a e K D 1 b K X c w X 7 g = " > A A A C 1 X i c j V H L S s N A F D 2 N r 1 p f U Z d u g k V w Y 0 l E 0 G X R j c s K 9 g F t K U k 6 b U P z I p k U S u h O 3 P o D b v W X x D / Q v / D O O A W 1 i E 5 I c u b c e 8 7 M v d e J f S / l p v l a 0 J a W V 1 b X i u u l j c 2 t 7 R 1 9 d 6 + R R l n i s r o b + V H S c u y U + V 7 I 6 t z j P m v F C b M D x 2 d N Z 3 w l 4 s 0 J S 1 I v C m / 5 N G b d w B 6 G 3 s B z b U 5 U T 9 c 7 g c 1 H z i A f z X o 5 P 7 F m P b 1 s V k y 5 j E V g K V C G W r V I f 0 E H f U R w k S E A Q w h O 2 I e N l J 4 2 L J i I i e s i J y 4 h 5 M k 4 w w w l 0 m a U x S j D J n Z M 3 y H t 2 o o N a S 8 8 U 6 l 2 6 R S f 3 o S U B o 5 I E 1 F e Q l i c Z s h 4 J p 0 F + 5 t 3 L j 3 F 3 a b 0 d 5 R X Q C z H i N i / d P P M / + p E L R w D X M g a P K o p l o y o z l U u m e y K u L n x p S p O D j F x A v c p n h B 2 p X L e Z 0 N q U l m 7 6 K 0 t 4 2 8 y U 7 B i 7 6 r c D O / i l j R g 6 + c 4 F 0 H j t G K Z F e v m r F y 9 V K M u 4 g C H O K Z 5 n q O K a 9 R Q J + 8 J H v G E Z 6 2 p z b Q 7 7 f 4 z V S s o z T 6 + L e 3 h A 1 g q l i M = < / l a t e x i t > h t < l a t e x i t s h a 1 _ b a s e 6 4 = " F E J t G 0 H x t k 0 X x h I N I S 3 K x X c a R i c = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n c t o t l q y y p Z c 5 D + w M l J C t a l R 8 Q R s 9 R P C Q I g B D C E n Y h 4 O E n h Z s W I i J 6 2 B C n C D E d Z x h i g J p U 8 p i l O E Q O 6 L v g H a t j A 1 p r z w T r f b o F J 9 e Q U o T B 6 S J K E 8 Q V q e Z O p 5 q Z 8 X + 5 j 3 R n u p u Y / q 7 m V d A r M S Q 2 L 9 0 s 8 z / 6 l Q t E n 2 c 6 h o 4 1 R R r R l X n Z S 6 p 7 o q 6 u f m l K k k O M X E K 9 y g u C H t a O e u z q T W J r l 3 1 1 t H x N 5 2 p W L X 3 s t w U 7 + q W N G D 7 5 z j n Q f 2 o b F t l + / K 4 V D n L R p 3 H H v Z x S P M 8 Q Q U X q K J G 3 g K P e M K z c W W M j T v j / j P V y G W a X X x b x s M H j x u V g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F E J t G 0 H x t k 0 X x h I N I S 3 K x X c a R i c = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n c t o t l q y y p Z c 5 D + w M l J C t a l R 8 Q R s 9 R P C Q I g B D C E n Y h 4 O E n h Z s W I i J 6 2 B C n C D E d Z x h i g J p U 8 p i l O E Q O 6 L v g H a t j A 1 p r z w T r f b o F J 9 e Q U o T B 6 S J K E 8 Q V q e Z O p 5 q Z 8 X + 5 j 3 R n u p u Y / q 7 m V d A r M S Q 2 L 9 0 s 8 z / 6 l Q t E n 2 c 6 h o 4 1 R R r R l X n Z S 6 p 7 o q 6 u f m l K k k O M X E K 9 y g u C H t a O e u z q T W J r l 3 1 1 t H x N 5 2 p W L X 3 s t w U 7 + q W N G D 7 5 z j n Q f 2 o b F t l + / K 4 V D n L R p 3 H H v Z x S P M 8 Q Q U X q K J G 3 g K P e M K z c W W M j T v j / j P V y G W a X X x b x s M H j x u V g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F E J t G 0 H x t k 0 X x h I N I S 3 K x X c a R i c = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n c t o t l q y y p Z c 5 D + w M l J C t a l R 8 Q R s 9 R P C Q I g B D C E n Y h 4 O E n h Z s W I i J 6 2 B C n C D E d Z x h i g J p U 8 p i l O E Q O 6 L v g H a t j A 1 p r z w T r f b o F J 9 e Q U o T B 6 S J K E 8 Q V q e Z O p 5 q Z 8 X + 5 j 3 R n u p u Y / q 7 m V d A r M S Q 2 L 9 0 s 8 z / 6 l Q t E n 2 c 6 h o 4 1 R R r R l X n Z S 6 p 7 o q 6 u f m l K k k O M X E K 9 y g u C H t a O e u z q T W J r l 3 1 1 t H x N 5 2 p W L X 3 s t w U 7 + q W N G D 7 5 z j n Q f 2 o b F t l + / K 4 V D n L R p 3 H H v Z x S P M 8 Q Q U X q K J G 3 g K P e M K z c W W M j T v j / j P V y G W a X X x b x s M H j x u V g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F E J t G 0 H x t k 0 X x h I N I S 3 K x X c a R i c = " > A A A C 0 X i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 4 7 K i f U B b S 5 J O 2 6 F 5 M Z k I p R T E r T / g V n 9 K / A P 9 C + + M K a h F d E K S M + f e c 2 b u v W 7 s 8 0 R a 1 m v O W F h c W l 7 J r x b W 1 j c 2 t 4 r b O / U k S o X H a l 7 k R 6 L p O g n z e c h q k k u f N W P B n M D 1 W c M d n a t 4 4 5 a J h E f h t R z H r B M 4 g 5 D 3 u e d I o m 7 a g S O H b n 8 y n H Y n c t o t l q y y p Z c 5 D + w M l J C t a l R 8 Q R s 9 R P C Q I g B D C E n Y h 4 O E n h Z s W I i J 6 2 B C n C D E d Z x h i g J p U 8 p i l O E Q O 6 L v g H a t j A 1 p r z w T r f b o F J 9 e Q U o T B 6 S J K E 8 Q V q e Z O p 5 q Z 8 X + 5 j 3 R n u p u Y / q 7 m V d A r M S Q 2 L 9 0 s 8 z / 6 l Q t E n 2 c 6 h o 4 1 R R r R l X n Z S 6 p 7 o q 6 u f m l K k k O M X E K 9 y g u C H t a O e u z q T W J r l 3 1 1 t H x N 5 2 p W L X 3 s t w U 7 + q W N G D 7 5 z j n Q f 2 o b F t l + / K 4 V D n L R p 3 H H v Z x S P M 8 Q Q U X q K J G 3 g K P e M K z c W W M j T v j / j P V y G W a X X x b x s M H j x u V g A = = < / l a t e x i t > Slot-Word Attention Context Encoder Slot-Turn Attention + PE BERT h v t < l a t e x i t s h a 1 _ b a s e 6 4 = " Q r Y 6 j 4 5 0 p J M N 1 8 B S 7 O Q 4 A j N m N z o = " > A A A C 1 3 i c j V H L S s N A F D 2 N r 1 p f s S 7 d B I v g q i Q i 6 L L o x m U F + 5 C 2 l i S d t q F 5 k U y K J R R 3 4 t Y f c K t / J P 6 B / o V 3 x h M i f 1 L N 8 v 8 r 0 7 U w t H H i a z B o Z p C y Y j q 7 M w l k V 0 R N 9 e + V M X J I S R O 4 B 7 F I 8 K 2 V M 7 6 r E l N L G s X v T V l / E 1 m C l b s 7 S w 3 w b u 4 J Q 3 Y + D n O e V A / L B t 6 2 b g 4 K l V O s 1 H n s Y s 9 H N A 8 j 1 H B O a q o k f c N H v G E Z + V K u V X u l P v P V C W X a M i f 1 L N 8 v 8 r 0 7 U w t H H i a z B o Z p C y Y j q 7 M w l k V 0 R N 9 e + V M X J I S R O 4 B 7 F I 8 K 2 V M 7 6 r E l N L G s X v T V l / E 1 m C l b s 7 S w 3 w b u 4 J Q 3 Y + D n O e V A / L B t 6 2 b g 4 K l V O s 1 H n s Y s 9 H N A 8 j 1 H B O a q o k f c N H v G E Z + V K u V X u l P v P V C W X a M i f 1 L N 8 v 8 r 0 7 U w t H H i a z B o Z p C y Y j q 7 M w l k V 0 R N 9 e + V M X J I S R O 4 B 7 F I 8 K 2 V M 7 6 r E l N L G s X v T V l / E 1 m C l b s 7 S w 3 w b u 4 J Q 3 Y + D n O e V A / L B t 6 2 b g 4 K l V O s 1 H n s Y s 9 H N A 8 j 1 H B O a q o k f c N H v G E Z + V K u V X u l P v P V C W X a M i f 1 L N 8 v 8 r 0 7 U w t H H i a z B o Z p C y Y j q 7 M w l k V 0 R N 9 e + V M X J I S R O 4 B 7 F I 8 K 2 V M 7 6 r E l N L G s X v T V l / E 1 m C l b s 7 S w 3 w b u 4 J Q 3 Y + D n O e V A / L B t 6 2 b g 4 K l V O s 1 H n s Y s 9 H N A 8 j 1 H B O a q o k f c N H v G E Z + V K u V X u l P v P V C W X a E V U + 1 L x i E n 7 7 K p c c a q O W f Y 0 c H N Q Q b 7 q S f k F l + g h Q Y A M E R h i K M I h P E h 6 O n D h I C W u i z F x g h A 3 c Y Z 7 l M i b k Y q R w i N 2 S N E V U + 1 L x i E n 7 7 K p c c a q O W f Y 0 c H N Q Q b 7 q S f k F l + g h Q Y A M E R h i K M I h P E h 6 O n D h I C W u i z F x g h A 3 c Y Z 7 l M i b k Y q R w i N 2 S N E V U + 1 L x i E n 7 7 K p c c a q O W f Y 0 c H N Q Q b 7 q S f k F l + g h Q Y A M E R h i K M I h P E h 6 O n D h I C W u i z F x g h A 3 c Y Z 7 l M i b k Y q R w i N 2 S N E V U + 1 L x i E n 7 7 K p c c a q O W f Y 0 c H N Q Q b 7 q S f k F l + g h Q Y A M E R h i K M I h P E h 6 O n D h I C W u i z F x g h A 3 c Y Z 7 l M i b k Y q R w i N 2 S N h t = BERT f inetune ([R t ; U t ]) (1) where BERT f inetune means that it will be finetuned during training. Therefore, BERT f inetune will learn a corresponding generalization of sentence representations and adapt to dialogue state tracking task. For slot s and value v t , we adopt another pretrained BERT f ixed to encode them into contextual semantics vectors h s and h v t respectively. Different from utterances, we use the output vector of the special token  [CLS]  to obtain the whole sentence representation: h s = BERT f ixed (s) (2) h v t = BERT f ixed (v t ) where the weights of BERT f ixed are fixed during training thus our model can be scalable to any unseen slots and values with sharing the original BERT representation. Slot-Word Attention. The slot-word attention is a multi-head attention (MultiHead(Q, K, V)), which takes a query matrix Q, a key matrix K and a value matrix V as inputs. Refer to  (Vaswani et al., 2017)  for more details. For each slot s, the slotword attention summarizes word-level slot-related information from each turn t into a d-dimensional vector c word s,t , which can be determined as follows: c word s,t = MultiHead(h s , h t , h t ) (3) Context Encoder. The context encoder is a unidirectional transformer encoder, which is devised to model the contextual relevance of the extracted word-level slot-related information among {1, ..., t} turns. The context encoder contains a stack of N identical layers. Each layer has two sub-layers. The first sub-layer is a masked multi-head self-attention (MultiHead), in which Q = K = V. The second sub-layer is a positionwise fully connected feed-forward network (FFN), which consists of two linear transformations with a ReLU activation  (Vaswani et al., 2017) . FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (4) Formally, the output of the context encoder c ctx s,?t can be denoted as follows: m n = FFN(MultiHead(m n?1 , m n?1 , m n?1 )) m 0 = [c word s,1 + PE(1), ..., c word s,t + PE(t)] c ctx s,?t = m N (5) where m n is the output of the n-th layer of context encoder and PE(?) denotes positional encoding function. Note that residual connection and layer normalization are omitted in the formula. Slot-Turn Attention. To retrieve turn-level relevant information from contextual representation, we devise a slot-turn attention which is the multihead attention as follows: c turn s,t = MultiHead(h s , c ctx s,?t , c ctx s,?t ) (6) Therefore, the model can access word-level and turn-level relevant information from the historical dialogues. Global-Local Fusion Gate. To balance the information of global context and local utterances, we propose to dynamically control each proportion of contextual information and current turn information so that the model can not only benefit from relevant context but also keep a balance between global and local representations. Similar to Hochreiter and Schmidhuber (1997), we leverage a fusion gate mechanism, which computes a weight to decide how much global and local information should be combined according to c word s,t and c turn s,t . It can be defined as follows: g s,t = ?(W g [c word s,t ; c turn s,t ]) (7) c gate s,t = g s,t ? c word s,t + (1 ? g s,t ) ? c turn s,t where W g ? R 2d?d are parameters, ? means sigmoid activation function, and ? mean the pointwise and element-wise multiplication respectively. Finally, we use a linear projection to obtain query results with layer normalization and dropout: o s,t = LayerNorm(Linear(Dropout(c gate s,t ))) (8) We follow  to adopt L2 norm to compute the distance. Therefore, the probability distribution of value v t and the training objective can be defined as: p(v t |U ?t , R ?t , s) = exp(? os,t?h v t 2 ) v ?Vs exp(? os,t?h v t 2 ) L dst = s?S T t=1 ? log(p(v t |U ?t , R ?t , s)) (9) where V s is the candidate value set of slot s and vt ? V s is the ground-truth value of slot s. 

 State Transition Prediction To better capture relevant context, we further introduce an auxiliary binary classification task to jointly train with DST: State Transition Prediction (STP), which is to predict if the value for a slot is updated compared to previous turn. This module reads c gate s,t?1 and c gate s,t as inputs and the transition probability p stp s,t can be calculated as follows: c stp s,t = tanh(W c c gate s,t ) (10) p stp s,t = ?(W p [c stp s,t ; c stp s,t?1 ]) where W c ? R d?d , W p ? R 2d are parameters. Note that when t = 1, we simply concatenate c stp s,t with zero vectors. For this task, we calculate the binary cross entropy loss between ground-truth transition labels y stp s,t and the transition probability p stp s,t , which is defined as follows: L stp = s?S T t=1 ?y stp s,t ? log(p stp s,t ) (11) 

 Adaptive Objective Essentially, the slot imbalance problem can be considered as a kind of class imbalance because there is an imbalance among both different slots and different samples. Instead of treating all slots indiscriminately, it is important to balance the learning of different slots. Recently,  Lin et al. (2017)  propose a soft-sampling method, Focal Loss, to re-weight the losses of different classes. Inspired by their work, we design a novel adaptive objective for DST which evaluates the difficulty from each slot's accuracy on the validation set and adaptively adjusts the weight of each slot during optimization. We define the accuracy of slot s on validation set as acc val s . Our adaptive objective is based on the following intuitions: (1) If acc val s ? acc val s ; then slot s is more difficult than slot s . Suppose this slot-level difficulty is defined as ?; then ? s = 1 ? acc val s s ?S 1 ? acc val s ? |S| (12) (2) Suppose there are two samples {(U t , R t ), (s, v t )} and {(U t , R t ), (s , v t )}. If the former confidence is lower than the latter, then sample {(U t , R t ), (s, v t )} is more difficult than {(U t , R t ), (s , v t )}. Suppose this samplelevel difficulty is defined as ?; then ?(s, v t ) = (1 ? p(s, v t )) ? (13) where p(s, v t ) is the confidence of sample {(U t , R t ), (s, v t )} and ? is a hyper-parameter. 

 R E T R A C T E D This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. Thus, the adaptive objective is defined as follows: L adapt (s, v t ) = ? s ?(s, v t ) log(p(s, v t )) (14) Focal Loss assigns static learning weights on slots and doesn't change them anymore during the whole training. Compared to Focal Loss, our adaptive objective can fit data better by dynamically evaluate the difficulties in an accuracy-sensitive manner and then adaptively control the learning weights for different slots, which is proved in our experiments. If the difficulty of slot s is greater than the average difficulty of all slots, ? s would increase and enlarge the loss of s. Similarly, the optimization of sample {(U t , R t ), (s, v t )} with a low confidence p(s, v t ) would be encouraged by a larger loss. When an epoch ends, the adaptive objective re-evaluates the difficulty of each slot and updates ? s . Therefore, it can not only encourage the optimization of those hard slots and samples but also balance the learning of all slots. 

 Optimization In our model, we firstly jointly train the DST and STP tasks to convergence and then fine-tune DST task with the adaptive objective. During joint training, we optimize the sum of these two loss functions as following: L joint = L dst + L stp (15) At the fine-tuning phase, we adopt the adaptive objective to fine-tune DST task as following: L f inetune = s?S T t=1 L adapt (s, vt ) (16) 3 Experiments Setup  We evaluate our model on MultiWOZ 2.0  (Budzianowski et al., 2018)  and MultiWOZ 2.1  (Eric et al., 2019) , which are two of the largest public task-oriented dialogue datasets, including about 10,000 dialogues with 7 domains and 35 domain-slot pairs. MultiWOZ 2.1 shares the same dialogues with MultiWOZ 2.0 but it fixed previous annotation errors. The statistics are shown in Table 2. Following , we use only 5 domains {restaurant, hotel, train, attraction, taxi} excluding hospital and police since these two domains never occur in the test set. We preprocess the datasets following  2 . We use joint accuracy and slot accuracy as our evaluation metrics. Joint accuracy is the accuracy of the dialogue state of each turn and a dialogue state is evaluated correctly only if all the values of slots are correctly predicted. Slot accuracy only considers individual slot-level accuracy. 

 Baseline Models We compare our results with the following competitive baselines: DSTreader proposes to model DST as a machine reading comprehension task and extract spans from dialogue history  (Gao et al., 2019b) . GLAD-RCFS uses a heuristic rule to extract relevant turns and lets slot-value pairs to query relevant context from them  (Sharma et al., 2019) . HyST employs a hierarchical encoder and takes a hybrid way combining both predefined-ontology and open-vocabulary settings . TRADE encodes the whole dialogue context and decodes the value for every slot using a copyaugmented decoder . DST-QA proposes to model DST as a question answering problem and uses a dynamically-evolving knowledge graph to learn relationships between slot pairs  (Zhou and Small, 2019) . SOM-DST considers the dialogue state as an explicit fixed-size memory and proposes a selectively overwriting mechanism . SUMBT exploits BERT as the encoder of the utterances, slots and values. It scores every candidate slot-value pair in a non-parametric manner using a distance measurement . DST-picklist performs matchings between candidate values and slot-context encoding considering all slots as picklist-based slots  (Zhang et al., 2019) . GLAD-RCFS, HyST, SUMBT, DST-picklist are predefined-ontology models as well as our model and DSTreader, TRADE, DST-QA, SOM-DST are open-vocabulary models. 

 Model Ontology MultiWOZ 2.0 MultiWOZ 2.1 Joint (%) Slot (%) Joint (%) Slot (%) DSTreader  (Gao et al., 2019b)  ? 39.41 -36.40 -GLAD-RCFS  (Sharma et al., 2019)  46.31 ---HyST  42.33 -38.10 -TRADE  (Wu et al.,      (Eric et al., 2019) . 

 Settings We employ the pre-trained BERT model that has 12 layers of 784 hidden units and 12 self-attention heads 3 . For the multi-head attention, we set heads count and hidden size to 4 and 784, respectively. For the context encoder, we set the transformer layers to 6. We set the max sequence length of all inputs to 64 and the batch size to 32. In all training, we use Adam optimizer  (Kingma and Ba, 2015)  and set the warmup proportion to 0.1. Specifically, in the joint training phase, we set the peak learning rate to 1e-4. At the fine-tuning phase, we set ? to 2, peak learning rate to 1e-5. The training stopped early when the validation loss was not improved for 15 consecutive epochs. For all experiments, we report the mean joint accuracy over multiple different random seeds to reduce statistical errors. 4 Experiment Results   

 Main Results 

 Ablation Study As shown in Table  4 , we estimate the effectiveness of the proposed state transition prediction and adaptive objective on the MultiWOZ 2.1 test set. The results show that both state transition prediction task and adaptive objective can boost the performance. Removing the state transition prediction task reduces joint accuracy by 0.69%, and the joint accuracy decreases by 1.10% without the adaptive objective fine-tuning. Moreover, when we remove the state transition prediction task and don't fine-tune our model with adaptive objective (only CHAN remains), the joint accuracy decreases by 1.55%. Also, to explore the importance of adjusting the ? s adaptively, we replace the adaptive objective with original focal loss (? = 1, ? = 2), which leads to 0.45% drop. To prove the effectiveness of each module of the proposed CHAN, we conduct ablation experiments 

 R E T R A C T E D Figure  2 : The turn-level and word-level attention visualization of our model on an example from MultiWOZ 2.1 test set, which is predicting the value of slot "restaurant-name" at the 5th turn. The columns "0,1,2,3" are the index of each head of multi-head attention. Although there is no slot-related information at 5th turn, our model still makes the correct prediction by attending to historiacal relevant words "dojo noodle bar" and relevant turns {3,4}, which is highlighted in red. Best viewed in color. on the MultiWOZ 2.1 test set as shown in Table  5 . We observe that a slight joint accuracy drop of 0.24% after removing the global-local fusion gate, which proves the effectiveness of fusing global context and local utterances. Moreover, removing the slot-turn attention and context encoder leads to a decrease by 0.15% and 1.72% respectively, which demonstrates that the turn-level relevant information and the contextual representations of wordlevel relevant information are effective to improve the performance. Moreover, after we remove the aforementioned three modules and sum the wordlevel relevant information of {1, ? ? ? , t} turns as output, the joint accuracy reduces by 6.72%, which is much higher than the sum of above three reductions. It demonstrates that effectively modeling interactions with word-level relevant information of dialogue history is crucial for DST. 

 Attention Visualization Figure  2  shows the visualization of turn-level and word-level attention of the "restaurant-name" slot on a prediction example of our model at turn 5. The turn-level attention visualization indicates that our model attends to the turns {3, 4} that are semantically related to the given slots "restaurant-name"  while almost pays no attention to turns {1,2}. And from the word-level attention visualization, we can easily find that the "restaurant-name" slot attends to the "dojo noodle bar" with the highest weight in both turn 3 and turn 4. Although there is no slot-related information at turn 5, our model still makes the correct decision by exploiting relevant context from the historical dialogue. 

 Effects of Adaptive Obj. on Acc. per Slot As Figure  3  shows, we draw the accuracy changes of each slot on MultiWOZ 2.1 test set after finetuning our model with adaptive objective. We sort all slots in ascending order according to their fre- quency (The detailed accuracy results are in the Appendix). Thus, slots on the left side are relatively more difficult than slots on the right side. 

 R E T R A C T D After fine-tuning with the adaptive objective, most slots on the left side achieve significant improvements, which proves the adaptive objective can encourage the learning of the hard slots. Although adaptive objective tends to decrease the weight of slots on the right side, they also benefit from the fine-tuning. We think that this is because encouraging the optimizing of hard slots enhances our model by tracking more complicated dialogue states. It proves that our adaptive objective can not only improve the performance of relatively hard slots but also boost the performance of relatively easy slots. 

 Qualitative Analysis To explore the advantages of our model compared to baseline models, we conduct a human evaluation on a subset of the MultiWOZ 2.1 test set where our model makes correct predictions while SUMBT (a previous strong baseline) fails. We predefine three types of improvements: historical information inference improvement which means inferring historical information is necessary for correct decisions, current information inference improvement which means inferring current information is enough for correct decisions, and other improvements. As shown in Table  6 , 64.49% improvements come from historical information inference, which demonstrates that our model can better exploit relevant context from the dialogue history.  guage understanding modules to predict the current dialogue state  (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014)  or to jointly learn speech understanding  (Henderson et al., 2014b; Zilka and Jurcicek, 2015; . One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. 

 Related Work Recent neural network models are proposed for further improvements  (Mrk?i? et al., 2015; Hori et al., 2016; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Ren et al., 2019; Balaraman and Magnini, 2019) .  and  use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly.  Sharma et al. (2019)  employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner.  encode the dialogue history into a hidden state and then simply combine it with the slot to make decisions. These models are deficient in fully exploiting the relevant context in dialogue history.  Gao et al. (2019b)  introduce a slot carryover model to decide whether the values from the previous turn should be used or not and  introduce a state operation predictor to decide the operation with the previous state. Different from them, we consider the state transition prediction as an additional enhancement while they integrate it into their DST pipelines. Besides,  Zhong et al. (2018)  only employ local modules to model the slot-specific representations, which neglects the slot imbalance problem. The general backbone of our model is a hierarchical attention network that can effectively aggregate query-related information at multiple levels (Yang  Ying et al., 2018; Wang et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019) . 

 R E T R A C T E D 

 Conclusion We introduce an effective model that consists of a contextual hierarchical attention network to fully exploit relevant context from dialogue history and an adaptive objective to alleviate the slot imbalance problem in dialogue state tracking. Experimental results show that our model achieves state-of-theart performance of 52.68% and 58.55% joint accuracy with considerable improvements (+1.24% and +5.98%) over previous best results on MultiWOZ 2.0 and MultiWOZ2.1 datasets, respectively. Although our model is based on predefined ontology, it is universal and scalable to unseen domains, slots and values. The main contributions our model, CHAN and adaptive objective, can also be applied to open-vocabulary models. We will explore it in the future. 
