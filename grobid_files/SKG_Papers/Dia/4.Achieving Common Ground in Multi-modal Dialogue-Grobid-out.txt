title
Achieving Common Ground in Multi-modal Dialogue

abstract


All communication aims at achieving common ground (grounding): interlocutors can work together effectively only with mutual beliefs about what the state of the world is, about what their goals are, and about how they plan to make their goals a reality  (Clark et al., 1991) . Computational dialogue research, in particular, has a long history of influential work on how implemented systems can achieve common ground with human users, from formal results on grounding actions in conversation  (Traum, 1994)  to machine learning results on how best to fold confirmation actions into dialogue flow  (Levin et al., 1998; Walker, 2000) . Such classic results, however, offer scant guidance to the design of grounding modules and behaviors in cutting-edge systems, which increasingly combine multiple communication modalities, address complex tasks, and include the possibility for lightweight practical action interleaved with communication. This tutorial is premised on the idea that it's time to revisit work on grounding in human-human conversation, particularly Brennan's general and important characterization of grounding as seeking and providing evidence of mutual understanding  (Brennan, 1990) , in light of the opportunities and challenges of multi-modal settings such as human-robot interaction. In this tutorial, we focus on three main topic areas: 1) grounding in human-human communication; 2) grounding in dialogue systems; and 3) grounding in multi-modal interactive systems, including image-oriented conversations and humanrobot interactions. We highlight a number of achievements of recent computational research in coordinating complex content, show how these results lead to rich and challenging opportunities for doing grounding in more flexible and powerful ways, and canvass relevant insights from the A: A green bike with tan handlebars. B: Got it  (Manuvinakurike et al., 2017)  A: The green cup is called Bill. B: Ok, the green cup is Bill. [point to the inferred object] (Liu and Chai, 2015) literature on human-human conversation. We expect that the tutorial will be of interest to researchers in dialogue systems, computational semantics and cognitive modeling, and hope that it will catalyze research and system building that more directly explores the creative, strategic ways conversational agents might be able to seek and offer evidence about their understanding of their interlocutors. Grounding in human-human communication.  Clark et al. (1991)  argued that communication is accomplished in two phases. In the presentation phase, the speaker presents signals intended to specify the content of the contributions. In the second phase, the participants work together to establish mutual beliefs that serve the purposes of the conversation. The two phases together constitute a unit of communication-contributions.  Clark and Krych (2004)  show how this model applies to coordinated action, while  Stone and Stojni? (2015)  applies the model to text-and-video presentations. Coherence is key. Grounding in dialogue systems. Computer systems achieve grounding mechanistically by ensuring they get attention and feedback from their users, tracking user state, and planning actions with reinforcement learning to resolve problematic situations. We will review techniques for maintaining engagement  (Sidner et al., 2005; Bohus and Horvitz, 2014; Foster et al., 2017)  and problems that arises in joint attention  (Kontogiorgos et al., 2018)  and turn taking such as incremental interpretation (DeVault and  Stone, 2004; DeVault et al., 2011) , ambiguity resolution (De-Vault and  Stone, 2009)  and learning flexible dialogue management policies  (Henderson et al., 2005) . Similar questions have been studied in the context of instruction games  (Perera et al., 2018; Thomason et al., 2019; Suhr and Artzi, 2018) , and interactive tutoring systems  (Yu et al., 2016; Wiggins et al., 2019) . Grounding in multi-modal systems. Multimodal systems offer the ability to use signals such as nodding, certain hand gestures and gazing at a speaker to communicate meaning and contribute to establishing common ground  (Mavridis, 2015) . However, multi-modal grounding is more than just using pointing to clarify. Multi-modal systems have diverse opportunities to demonstrate understanding. For example, recent work has aimed to bridge vision, interactive learning, and natural language understanding through language learning tasks based on natural images  (Zhang et al., 2018; Kazemzadeh et al., 2014; De Vries et al., 2017a; Kim et al., 2020) . The work on visual dialogue games  (Geman et al., 2015)  brings new resources and models for generating referring expression for referents in images  (Suhr et al., 2019; Shekhar et al., 2018) , visually grounded spoken language communication  (Roy, 2002; Gkatzia et al., 2015) , and captioning  (Levinboim et al., 2019; Alikhani and Stone, 2019) , which can be used creatively to demonstrate how a system understand a user. Figure  1  shows two examples of models that understand and generate referring expressions in multimodal settings. Similarly, robots can demostrate how they understand a task by carring it out-in research on interactive task learning in human-robot interaction  (Zarrie? and Schlangen, 2018; Carlmeyer et al., 2018)  as well as embodied agents perform-Show me a restaurant by the river, serving pasta/Italian food, highly rated and expensive, not child-friendly, located near Cafe Adriatic. ( ing interactive tasks  (Gordon et al., 2018; Das et al., 2018)  in physically simulated environments  (Anderson et al., 2018; Tan and Bansal, 2018)  often drawing on the successes of deep learning and reinforcement learning  (Branavan et al., 2009; Liu and Chai, 2015) . A lesson that can be learned from this line of research is that one main factor that affects grounding is the choice of medium of communication. Thus, researchers have developed different techniques and methods for data collection and modeling of multimodal communication  (Alikhani et al., 2019; Novikova et al., 2016) . Figure  2  shows two example resources that were put together using crowdsourcing and virtual reality systems. We will discuss the strengths and shortcomings of these methods. We pay special attention to non-verbal grounding in languages beyond English, including German  (Han and Schlangen, 2018) , Swedish (Kontogiorgos, 2017), Japanese  (Endrass et al., 2013; Nakano et al., 2003) , French  (Lemaignan and Alami, 2013; Steels, 2001) , Italian  (Borghi and Cangelosi, 2014; Taylor et al., 1986) , Spanish  (Kery et al., 2019) , Russian  (Janda, 1988) , and American sign language  (Emmorey and Casey, 1995) . These investigations often describe important language-dependent characteristics and cultural differences in studying non-verbal grounding. Grounding in end-to-end language & vision systems. With current advances in neural mod-elling and the availability of large pretrained models in language and vision, multi-modal interaction often is enabled by neural end-to-end architectures with multimodal encodings, e.g. by answering questions abut visual scenes  (Antol et al., 2015; Das et al., 2017) . It is argued that these shared representations help to ground word meanings. In this tutorial, we will discuss how this type of lexical grounding relates to grounding in dialogue from a theoretical perspective  (Larsson, 2018) , as well as within different interactive application scenarios -ranging from interactively identifying an object  (De Vries et al., 2017b)  to dialogue-based learning of word meanings  (Yu et al., 2016) . We then critically review existing datasets and shared tasks and showcase some of the shortcomings of current vision and language models, e.g.  (Agarwal et al., 2018) . In contrast to previous ACL tutorials on Multimodal Learning and Reasoning, we will concentrate on identifying different grounding phenomena as identified in the first part of this tutorial. 

 Outline We begin by discussing grounding in humanhuman communication (?20 min). After that, we discuss the role of grounding in spoken dialogue systems (?30 min) and visually grounded interactions including grounding visual explanations in images and multimodal language grounding for human-robot collaboration (?90 min). We then survey methods for developing and testing multimodal systems to study non-verbal grounding (?20 min). We follow this by describing common solution concepts and barrier problems that cross application domains and interaction types (?20 min). 

 Prerequisites and reading list The tutorial will be self-contained. For further readings, we recommend the following publications that are central to the non-verbal grounding framework as of late 2019:  Figure 1 : 1 Figure 1: Examples of the generation and interpretation of grounded referring expressions in multimodal interactive settings. Grounding is making sure that the listener understands what the speaker said. 

 Figure 2 : 2 Figure 2: Content and medium affect grounding. This figure shows two examples of interactive multimodal dialogue systems. 

 Mathematics and Statistics for a year at San Diego State University and San Diego Mesa College. She has served as the program committee of ACL, NAACL, EMNLP, AAAI, ICRL, ICMI, and INLG and is currently the associate editor of the Mental Note Journal. email: ma1195@cs.rutgers.edu, webpage: www.malihealikhani.com Matthew Stone is professor and chair in the Department of Computer Science at Rutgers University; he holds a joint appointment in the Rutgers Center for Cognitive Science. His research focuses on discourse, dialogue and natural languagegeneration; he is particularly interested in leveraging semantics to make interactive systems easier to build and more human-like in their behavior. He was program co-chair for NAACL 2007, general co-chair for SIGDIAL 2014. He has also served as program co-chair for INLG and IWCS, as an information officer for SIGSEM, and on the editorial board for Computational Linguistics. email: mdstone@cs.rutgers.edu, website: www.cs.rutgers.edu/ ?mdstone/ Kallirroi Georgila. (Manuvinakurike et al., 2017) 4. Language to Action: Towards Interactive Task Learning with Physical Agents, Joyce Y. Chai by Joyce Y. Chai et al.(Chai et al., 2018) 5. It's Not What You Do, It's How You Do It: Grounding Uncertainty for a Simple Robot, Julian Hough and David Schlangen. (Hough and Schlangen, 2017) 6. Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz Data: Boot- strapping and Evaluation rieser-lemon by Verena Rieser and Oliver Lemon. (Rieser and Lemon, 2008) 7. A survey of nonverbal signaling methods for non-humanoid robots by Elizabeth Cha et al. (Cha et al., 2018) 8. The Devil is in the Details: A Magnifying Glass for the GuessWhich Visual Dialogue Game by Alberto Testoni et al. (Testoni et al., 2019) 4 Authors Malihe Alikhani is a 5th year Ph.D. student in the department of Computer sSience at Rut- gers University ma1195@cs.rutgers.edu, advised by Prof. Matthew Stone. She is pur- suing a certificate in cognitive science through the Rutgers Center for Cognitive Science and holds a BA and MA in Mathematics. Her re- search aims at teaching machines to understand and generate multimodal communication. She is the recipient of the fellowship award for ex- cellence in computation and data sciences from Rutgers Discovery Informatics Institute in 2018 and the Anita Berg student fellowship in 2019. Before joining Rutgers, she was a lecturer and an adjunct professor of 1. Grounding in communication, Herb Clark and Susan Brennan. (Clark et al., 1991) 2. Meaning and demonstration by Una Stojnic and Matthew Stone (Stone and Stojni?, 2015) 3. Using Reinforcement Learning to Model In- crementality in a Fast-Paced Dialogue Game, Ramesh Manuvinakurike, David DeVault and
