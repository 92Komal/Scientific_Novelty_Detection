title
BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues

abstract
Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos. To address this drawback, we propose Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues. Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-tospatial reasoning. The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting. The retrieved visual cues are used as contextual information to construct relevant responses to the users. Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark. We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.

Introduction A video-grounded dialogue agent aims to converse with humans not only based on signals from natural language but also from other modalities such as sound and vision of the input video. Recent efforts  (Alamri et al., 2018; Sanabria et al., 2019;  consider video-grounded dialogues as an extension of video Question-Answering (QA) Figure  1 : Examples of video-grounded dialogues from the benchmark datasets of Audio-Visual Scene Aware Dialogues (AVSD) challenge  (Alamri et al., 2018 . H: human, A: the dialogue agent.  (Tapaswi et al., 2016; Jang et al., 2017; Lei et al., 2018)  whereby the agent answers questions from humans over multiple turns rather than a single turn (See Figure  1 ). This is a very complex task as the dialogue agent needs to possess not only strong language understanding to generate natural responses but also sophisticated reasoning over video information, including the related objects, their positions and motions, etc. Compared to image-based NLP tasks such as image QA and captioning  (Antol et al., 2015; Xu et al., 2015; Goyal et al., 2017) , video-grounded dialogues are more challenging as the feature representation of a video involves both spatial and temporal dimensions. Ideally, a dialogue agent has to process information of both dimensions to address the two major questions: "where to look" (spatial reasoning) and "when to look" (temporal reasoning) in the video. However, current approaches in video-grounded dialogues  Le et al., 2019b; Sanabria et al., 2019)  often overlook spatial features and assume each spatial region is equally important to the current task (each spatial region is assigned with a uniform weight). Such approach is appropriate for cases where the video involves just few objects and spatial positions can be treated similarly. However, in many scenarios (e.g. examples in Figure  1 ), each video frame often contains multiple distinct objects and not all of them are relevant to the given question. Related tasks to video-grounded dialogues are video QA and video captioning. Previous efforts in these research areas such as  (Jang et al., 2017; Aafaq et al., 2019)  explicitly consider both spatial and temporal features of input video. These models learn to summarize spatial features based on their importance to question rather than considering each region equally. We are motivated by these approaches and propose to extend spatiotemporal reasoning to dialogues. However, rather than fixing on processing spatial inputs then learning temporal inputs, we note that in some cases, e.g. extended videos over a long period, it is more practical to first identify the relevant video segments before pinpointing the specific subjects of interest. Considering questions in a dialogue setting, it is appropriate to assume the questions are relevant to varying temporal locations of the video rather than just a small fixed segment. We, thus, propose to explore a bidirectional vision-language reasoning approach to fully exploit both spatial and temporallevel features through two reasoning directions. Our approach includes two parallel networks to learn relevant visual signals from the input video based on the language signals from user utterances. Each network projects the language-based features to a three-dimensional tensor which is then used to independently learn video signals following a reasoning direction either as spatial?temporal or temporal?spatial. The output from each network is dynamically combined by importance scores computed based on language and visual features. The weighted output is recurrently used as input to the reasoning modules to allow the models to progressively derive relevant video signals over multiple steps. Intuitively, spatial?temporal reasoning is more appropriate for human queries related to specific entities or for input video involving many objects. temporal?spatial reasoning is more suitable for human queries about a particular video segment or for videos of extensive lengths. We name our proposed approach Bidirectional Spatio-Temporal Learning (BiST), with the following contributions: (1) Rather than exploit-ing temporal-level information only, our approach equally emphasizes both spatial and temporal features of videos for higher-resolution queries of visual cues. (2) To tackle the diverse queried information from conversational queries, we propose a bidirectional strategy, denoted spatial?temporal, to enable comprehensive information diffusion between the two visual feature spaces. (3) Our models achieve competitive performance on the "AVSD" (Audio-Visual Scene Aware Dialogues) benchmark from the 7 th Dialogue System Technology Challenge (DSTC7)  (Alamri et al., 2018 . We adapt our models to a video QA task "TGIF-QA"  (Jang et al., 2017)  and achieve significant performance gains. (4) We conduct a comprehensive ablation and qualitative analysis and demonstrate the efficacy of our bidirectional reasoning approach. 

 Related Work Our work is related to two research topics: videogrounded dialogues and spatio-temporal learning. Video-grounded Dialogues. Following recent efforts that combine NLP and Computer Vision research  (Antol et al., 2015; Xu et al., 2015; Goyal et al., 2017) , video-grounded dialogues are extended from the two major research fields: video action recognition and detection  (Simonyan and Zisserman, 2014; Carreira and Zisserman, 2017)  and dialogues/QA  (Rajpurkar et al., 2016; Budzianowski et al., 2018; Gao et al., 2019a) . Approaches to video-grounded dialogues  (Sanabria et al., 2019; Le et al., 2019b)  typically use pretrained video models, such as 2D CNN models on video frames  (Donahue et al., 2015; Feichtenhofer et al., 2016) , and 3D CNN models on video clips  (Tran et al., 2015; Carreira and Zisserman, 2017) , to extract visual features. However, these approaches mostly exploit the superficial information from the temporal dimension and neglect spatial-level signals. These approaches integrate spatial-level features simply through sum pooling with equal weights to obtain a global representation at the temporal level. They are, thus, not ideal for complex questions that investigate entity-level or spatial-level information  (Jang et al., 2017; . The dialogue setting exacerbates this limitation as it allows users to explore various aspects of the video contents, including both low-level (spatial) and high-level (temporal) information, over multiple dialogue turns. Our approach aims to address this challenge in video-grounded dialogues by retrieving fine-grained information from video through a bidirectional reasoning framework. Spatio-temporal Learning. Most efforts in spatiotemporal learning focus on action recognition or detection tasks.  (Yang et al., 2019)  proposes to progressively refine coarse-scale information through temporal extension and spatial displacement for action detection.  uses a shared network of 2D CNNs over three orthogonal views of video to obtain spatial and temporal signals for action recognition.  (Qiu et al., 2019)  adopts a twopath network architecture that integrates global and local information of both temporal and spatial dimensions for video classification. Other research areas that investigate spatio-temporal learning include video captioning  (Aafaq et al., 2019) , video super-resolution  (Li et al., 2019b) , and video object segmentation  (Xu et al., 2019) . In general, spatio-temporal learning approaches aim to process higher-resolution information from complex videos that involve multiple objects in each video frame or motions over video segments  (Yang et al., 2019) . We are motivated by a similar reason observed in video-grounded dialogues and explore a vision-language bidirectional reasoning approach to obtain more fine-grained visual features. 

 BiST Model The input includes a video V , dialogue history of (t ? 1) turns (where t is the current turn), each including a pair of (human utterance H, dialogue agent response A) (H 1 , A 1 , ..., H t?1 , A t?1 ), and current human utterance H t . The output is a system response A t that can address current human utterance. The input video can contain features in different modalities, including vision, audio, and text (such as video caption or subtitle). Without loss of generalization, we can denote each text input as a sequence of tokens, each represented by a unique token index from a vocabulary set V : dialogue history X his , user utterance X que , text input of video X cap , and output response Y . We also denote L S as the length of a sequence S. For instance, L que is the length of X que . Our model is composed of four parts: (1) The encoders encode text sequences and video inputs, including visual, audio, and text features, into continuous representations. For non-text features such as vision and sound, we follow previous work  (Lei et al., 2018;  and assume access to pre-trained models. (2) Several neural reasoning components learn dependencies between user utterances/queries and video features of multiple modalities. For video visual features, we propose to learn dependencies at both spatial and temporal levels in two directions (see Figure  2 ). Specifically, we allow interaction between each token in user query and each spatial position or temporal step of the video. The outputs from spatial-based or temporal-based reasoning are sequentially incorporated in two directions, temporal?spatial and spatial?temporal. The bidirectional strategy enables information being fused dynamically and captures complex dependencies between textual signals from dialogues and visual signals from videos. (3) The decoder passes encoded system responses over multiple attention steps, each of which integrates information from textual or video representations. The decoder output is passed to a generator to generate tokens by an auto-regressive way. (4) The generator computes three distributions over the vocabulary set, one distribution as output from a linear transformation and the others based on pointer attention scores over positions of input sequences. 

 Encoders Text Encoder. We use an encoder to embed text-based input X into continuous representations Z ? R L X ?d . L X is the length of sequence X and d is the embedding dimension. A text encoder includes a token-level embedding layer and a layer normalization  (Ba et al., 2016) . The embedding layer includes a trainable matrix E ? R |V |?d , with each row representing a token in the vocabulary set V as a vector of dimension d. We denote E(X) as the embedding function that looks up the vector of each token in input sequence X: Z emb = E(X) ? R L X ?d . To incorporate the positional encoding layer, we adopt the approach from  (Vaswani et al., 2017)  with each token position represented as a sine or cosine function. The output from positional encoding and token-level embedding is combined through element-wise summation and layer normalization. The encoder outputs include representations for dialogue history Z his , user query Z que , video caption Z cap , and target response Z res . For target response, during training, the sequence is shifted left by one position to allow prediction in the decoding step i is auto-regressive on the previous positions 1, ..., (i?1). We share the embedding matrix E to encode all text sequences. "What is he doing?" "After that, what did he do?" Figure 2: Our bidirectional approach models the dependencies between text and vision in two reasoning directions: spatial?temporal and temporal?spatial. ? and ? denote dot-product operation and element-wise summation. Video Encoder. We make use of a 3D-CNN video model to extract spatio-temporal visual features. The dimensions of the resulting output depend on the configuration of sampling stride and clip length. We denote the output from a pretrained visual model as Z pre vis ? R F ?P ?d pre vis where F is the number of sampled video clips, P is the spatial dimension from a 3D CNN layer, and d pre vis is the feature dimension. We apply a linear layer with ReLU and layer normalization to reduce feature dimension to d d pre vis . For audio features, we follow similar procedure to obtain audio representation Z aud ? R F ?d . We keep the pretrained visual and audio models fixed and directly use extracted features to our dialogue models. 

 Bi-directional Reasoning We propose a bidirectional architecture whereby the text features are used to select relevant information in both spatial and temporal dimensions in two reasoning directions (See Figure  2 ). Temporal?spatial. In one direction, the user query is used to select relevant information along temporal steps of each spatial region independently. We first stack the encoded query features to P spatial positions and denote the stacked features as Z stack que ? R P ?Lque?d . For each spatial position, the model learns the dependencies between question and each of F temporal steps through an attention mechanism as follows: Z (1) t2s = Z T vis W (1) t2s ? R P ?F ?datt (1) Z (2) t2s = Z stack que W (2) t2s ? R P ?Lque?datt (2) 

 S (1) t2s = Softmax(Z (2) t2s Z (1) t2s T ) ? R P ?Lque?F (3) where d att is the dimension of the attention hidden layer, W t2s ? R d?datt and W (2) t2s ? R d?datt . The attention scores S (1) t2s are used to obtain weighted sum along the temporal dimension of each spatial position of Z vis . The resulting tensor is passed through a linear transformation and ReLU layer. The output contains temporally attended visual features and are combined with language features through skip connection. We denote the output by vector Z t t2s . From the temporally attended features, user query is used again to obtain dependencies along the spatial dimension. We use a similar attention network to model the interaction between each token in query and each temporally attended spatial region. Z (3) t2s = Z t t2s W (3) t2s ? R Lque?P ?datt (4) Z (4) t2s = Z que W (4) t2s ? R Lque?datt (5) S (2) t2s = Softmax(Z (3) t2s Z (4) t2s T ) ? R Lque?P (6) where W (3) t2s ? R d?datt and W (4) t2s ? R d?datt . The attention scores S (2) t2s is used to obtain the weighted sum of all spatial positions from Z t t2s . The output is temporal-to-spatially attended visual features and is incorporated into language features through skip connection. We denote the resulting output as Z t2s . Spatial?temporal. In this reasoning direction, similar neural operations are used to compute spatially attended features followed by temporally attended features. The main difference from the other reasoning direction is that we stacked the query features to F temporal steps to obtain Z stack que ? R F ?Lque?d . Other network components, including two attention layers, are as described in Equation 1 to 6. The final output is denoted as Z s2t . Previous approaches in video-based NLP tasks  (Yu et al., 2016; Jang et al., 2017;  focus on the interaction between global representations of questions and temporal-level representations of videos. This strategy potentially loses critical information on spatial variations in video frames. Our approach does not only emphasize both spatial and temporal feature spaces but also allows neural models to diffuse information from these feature spaces in two different ways. As we can consider spatial information as local signals and temporal information as global signals, our approach enables global-to-local and local-to-global diffusion of visual cues in video. This approach is similar to  (Qiu et al., 2019)  in which local and global visual signals are learned and diffused iteratively. However, different from this approach, our approach focuses on language-vision reasoning for more accurate visual information queries. Multimodal Reasoning. In addition to languagevision reasoning, our models also consider learning of other information dependencies between queries and audio inputs or textual video inputs. ? Language?Audio Reasoning. We adopt similar neural operations from language-vision reasoning. The difference is that we directly use the query features without stacking the features into Equation 1 to 3. The resulting output of text-audio reasoning is denoted as Z q2a which contains query-guided temporally attended features of Z aud . ? Language?Language Reasoning. This reasoning module focuses on the unimodal dependencies between user query and video caption (if the caption is available). As the caption can contain useful information about the video content, we apply the dot-product attention mechanism similarly as with audio features to obtain Z q2c . Multimodal Fusioning. Given the attended features, we combine them to obtained query-guided video representation, incorporating information from all modalities. We denote the concatenated representation in the following: Z q2vid = [Z que ; Z t2s ; Z s2t , Z q2a , Z q2c ] ? R Lque?5d where ; is the concatenation operation. The features are combined through an importance score matrix: S vid = Softmax(Z q2vid W q2vid ) ? R Lque?4 where W q2vid ? R 5d?4 . The scores from S vid are used to obtain the weighted sum of component video modalities, resulting in a fusion vector from multiple modalities. We denote the resulting output Z vid . Compared to previous work such as  Le et al., 2019b)  which generally treat all modalities equally, our multimodal features are fused in a question-dependent manner. Potentially, our approach can avoid noisy or unnecessary signals, e.g. audio features not needed for questions only concerning visual contents. 

 Response Decoder The decoder aims to decode system responses in an auto-regressive manner. During inference, a special token sos is fed to the decoder. The output token is then concatenated to this special token as input to the decoder again to decode the second token. This repeats until reaching a limit of decoding rounds or when the special token eos is predicted. We apply a similar decoding architecture as  (Le et al., 2019b) . The decoder includes three attention layers to incorporate contextual cues from textual components to the output token representations. The first layer is a self-attention to learn dependencies among the current tokens. Intuitively, this helps to shape a more semantically structured sequence. The second and third attention steps are used to capture contextual information from dialogue history and current user query to make the responses coherently connected to the whole dialogue context. To incorporate contextual cues from video components, our decoder is slightly different from  (Le et al., 2019b) . Instead of sequentially going through multiple attention layers, we only need one layer on the fused features Z vid . This is more memory efficient since it only requires a single attention operation. It also does not depend on the design decision of the ordering of attention layers. At decoding step j, we denote the decoder output as Z dec ? R j?d . 

 Pointer Generator Given the output from the decoder, the generator network is used to materialize responses in natural language. A linear transformation is used to obtain distribution over the vocabulary set V . P vocab = Softmax(Z dec W vocab ) ? R j?|V | where W vocab ? R d?|V | . We share the weights between W vocab and E as the semantics between source sequences and target responses are similar. To strengthen the model generation capability, we adopt pointer networks  (Vinyals et al., 2015)  to emphasize tokens from source sequences, i.e. user queries and video captions. We denote Ptr(Z 1 , Z 2 ) as the pointer network operation i.e. each token in Z 2 is "pointed" to all tokens in Z 1 through a learnable probability distribution. The resulting probability distribution is aggregated by all tokens in Z 1 to obtain  Ptr(Z 1 , Z 2 ) ? L Z 2 ? |V |. The final output distribution, de- noted P out ? R j?|V | , L = L Y j=0 ? log(P out (y j )). 

 Experiments 

 Experimental Setups Datasets. We use the AVSD benchmark from DSTC7  (Alamri et al., 2018  which contains dialogues grounded on the Charades videos  (Sigurdsson et al., 2016) . In addition, we adapt our models to the video QA benchmark TGIF-QA  (Jang et al., 2017) . (See Table  1  for a summary of the two datasets). To extract visual and audio features, we used 3D-CNN ResNext-101  (Xie et al., 2017)  pretrained on Kinetics  (Hara et al., 2018)  to obtain spatio-temporal visual features and VG-Gish pretrained on YouTube videos  (Hershey et al., 2017)  to extract (temporal) audio features. We sample video clips to extract visual features with a window size of 16 frames, and stride of 16 and 4 in AVSD and TGIF-QA respectively. In TGIF-QA experiments, we also extract visual features from pretrained ResNet-152  (He et al., 2016)  for a fair comparison with existing work. In AVSD experiments, we make use of the video summary as the video-dependent text input X cap . Training Procedure. We adopt the Adam optimizer (Kingma and     4 ) Frame: open-ended QA which can be answered from one video frame. strategy from  (Vaswani et al., 2017) . We set the learning rate warm-up steps equivalent to 5 epochs and train models up to 50 epochs. We select the best models based on the average loss per epoch in the validation set. We initialize all model parameters with uniform distribution  (Glorot and Bengio, 2010) . During training, we adopt the auxiliary auto-encoder loss function from  (Le et al., 2019b) . We adopt Transformer attention  (Vaswani et al., 2017)  in our models and select the following hyperparameters: d = d att = 128, N att = N dec = 3, and h att = 8 where N att and N dec are the number of Transformer blocks in multimodal reasoning and decoder networks and h att is the number of attention heads. We tuned other hyper-parameters following grid-search over the validation set. In AVSD experiments, we train our models by applying label smoothing  (Szegedy et al., 2016 ) on the target system responses Y . We adopt a beam search technique with a beam size 5. 

 Modifications for Video QA In many Video QA benchmarks such as TGIF-QA  (Jang et al., 2017) , the tasks are retrieval-based (e.g. output a single score for each output candidate) rather than generation-based as in many dialogue tasks. Following  (Fan et al., 2019) , we first concatenate the question with each candidate answer individually and treat this as Z que to our models. As there is no target response to be decoded, we adapt our models to this setting by using a trainable vector z j ? R d to represent a candidate response R j , replacing Z res ? R j?d in a dialogue, as input to the decoder. The output, denoted  where W out ? R d?1 . In this setting, we remove the language?language and language?audio reasoning modules. The loss function is the summed pairwise hinge loss  (Jang et al., 2017)  between scores of positive answer s p out and each negative answer Z j,dec ? R d , s n j,out . L = K j=1 max(0, m ? (s p out ? s n j,out )) where K is the total number of candidate answers and m is a hyper-parameter used as a margin between positive and negative answers. Training. Multiple-choice tasks, including Action and Transition, are trained following the pairwise loss with K = 5 and m = 1. Count task is trained with similar approach but as a regression problem with a single output score s out . The loss function is measured as mean square error between output s out and label y. The open-ended Frame task is trained as a generation task, similarly to the dialogue response generation task, with a single-token output. We use the the vector z ? R d as input to the decoder. The generator includes a single linear layer with W out ? R d?|v| . We do not apply pointer network in this case as the output is only a single-token response. 

 Results AVSD Results. We report the objective scores, including BLEU  (Papineni et al., 2002) , METEOR  (Banerjee and Lavie, 2005) , ROUGE-L  (Lin, 2004) , and CIDEr  (Vedantam et al., 2015) . These metrics, which formulate lexical overlaps between generated and ground-truth dialogue responses, are borrowed from language generation tasks such as machine translation and captioning. We compare our generated responses with 6 reference responses. Major baseline models are: (1) Baseline  (Alamri et al., 2018;  consists of LSTM-based encoder-encoder with attention layers between user queries and temporal-level visual and audio features. (2) Baseline+GRU+HierAttn.  (Le et al., 2019a)  extends (  1 ) through GRU and question-guided self-attention and caption attention. (3) FA+HRED (Nguyen et al., 2018) adopts FiLM neural blocks for language-vision dependency learning. (4) Video Summarization  (Sanabria et al., 2019)  reformulates the task as a video summarization task and enhances the models with transfer learning from a large-scale summarization benchmark. (5) Student-Teacher  adopts dual network architecture in which a student network is trained to mimic a teacher network trained with additional video-dependent text input. (  6 ) MTN  (Le et al., 2019b)  fuses temporal features of different modalities sequentially through a Transformer decoder architecture. (  7 ) FGA  (Schwartz et al., 2019)  consists of attention networks between all pairs of modalities and the models aggregate attention scores along edges of an attention graph. In Table  2 , we present the scores by different combinations of features, including vision Z vis , audio Z aud , and text Z cap . In all settings, our models outperform the existing approaches. The performance of our models in the visual-only setting shows the performance gain coming from our bidirectional language-vision reasoning approach. We  also observe a performance boost whenever the text feature from video is considered. When we add the audio features, however, the performance gain is not significant. This reveals a potential future extension in our work to better combine visual and audio feature representations. FGA  (Schwartz et al., 2019)  reports the CIDEr score of 0.806 in the visual-only setting. Compared to FGA, our performance gain indicates the efficacy of learning fine-grained dependencies between query and visual features at both spatial and temporal levels to select relevant information from video. TGIF-QA Results. We give the L2 loss for Count task and accuracy for the other three QA tasks (See Appendix A for description of baseline models). From Table  3 , our model outperforms existing approaches across all QA tasks, using either framelevel (appearance) feature, ResNet, or sequencelevel feature, ResNext. Our models perform better with ResNext as we expect sequence-level feature is more consistent than frame-level feature. Experiments on this benchmark show clearer performance gain of our bidirectional language-vision reasoning approach as the performance is not affected by errors of generation components as in the AVSD experiments. By focusing on learning high-resolution dependencies from spatio-temporal features, our models can fully exploit contextual cues and select better answers for video QA tasks. Impacts of Spatio-temporal Learning. We con-sider model variants based on the spatio-temporal dynamics and report the results in Table  4 . We noted that when using a single reasoning direction, the model with temporal?spatial performs better than one with the reverse reasoning direction. This observation is different from prior approaches of spatio-temporal learning such as  (Jang et al., 2017)  which are limited to the reasoning order spatial?temporal. This can be explained as the videos in the AVSD benchmark are typically longer than other QA benchmarks. It is practical to focus on temporal locations in frame sequences first before selecting spatial regions in individual frames. In addition, dialogue queries are positioned in a multi-turn setting whereby each turn is relevant to different video segments as the dialogue evolves. Potentially, this observation indicates an important difference of video-grounded dialogues compared to video QA. Secondly, we also observe that our model performance improves when we use both reasoning directions rather than only one of them. Our motivation for this approach is similar to  (Schuster and Paliwal, 1997)  who proposes a bidirectional strategy to process sequences in both forward and backward directions. Similarly, our approach exploits visual information through a bidirectional information diffusion strategy that can interpret information from both spatial or temporal aspects based on language input. Finally, we observe that using spatio-temporal features is better than only using one of them, demonstrating the importance of information in both dimensions. To obtain Z vis for spatial-only or temporal-only features, the spatio-temporal features are passed through an average pooling operation along the temporal or spatial dimensions respectively.  Ablation Analysis. We conduct experiments with model variants of different hyper-parameter settings. Specifically, we vary the the number of attention rounds N att and attention heads h att . From Table  5 , we noted the contribution of the multiround architecture to language-vision reasoning as the performance improves with larger reasoning steps, i.e. up to three attention rounds. However, we observe that as we increase to more than 3 reasoning steps, the model performance only improves slightly. We also note that using a multi-head attention mechanism is suitable for tasks dealing with information-intensive media such as video and dialogues. The multi-head structure enables feature projection to multiple subspaces and capture complex language-vision dependencies. Qualitative Analysis. In Figure  3 , we present some example outputs. We note that the predicted dialogue responses of BiST models are closer to the ground-truth responses. Particularly for complex questions that query specific segments (example B, C, D), and/or specific spatial locations (Example D), our approach can generally produce better   

 Conclusion We proposed BiST, a novel deep neural network approach for video-grounded dialogues and video QA, which exploits the complex visual nuances of videos through a bidirectional reasoning framework in both spatial and temporal dimensions. Our experimental results show that BiST can extract relevant, high-resolution visual cues from videos and generate quality dialogue responses/answers. A TGIF-QA Baselines In TGIF-QA experiments, we compare our models with the following baselines: (1) VIS  (Ren et al., 2015)  and (  2 ) MCB  (Fukui et al., 2016)  are two image-based VQA baselines which were adapted to TGIF-QA by  (Jang et al., 2017)   
