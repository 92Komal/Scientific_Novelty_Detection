title
Approximation of Response Knowledge Retrieval in Knowledge-grounded Dialogue Generation

abstract
This paper is concerned with improving dialogue generation models through injection of knowledge, e.g., content relevant to the post that can increase the quality of responses. Past research extends the training of the generative models by incorporating statistical properties of posts, responses and related knowledge, without explicitly assessing the knowledge quality. In our work, we demonstrate the importance of knowledge relevance and adopt a two-phase approach. We first apply a novel method, Transformer & Post based Posterior Approximation (TPPA) to select knowledge, and then use the Transformer with Expanded Decoder (TED) model to generate responses from both the post and the knowledge. TPPA method processes posts, post related knowledge, and response related knowledge at both word and sentence level. Our experiments with the TED generative model demonstrate the effectiveness of TPPA as it outperforms a set of strong baseline models. Our TPPA method is extendable and supports further optimization of knowledge retrieval and injection.

Introduction In recent years, there have been concerted efforts to model dialogue interactions and generate an appropriate response to an initial user statement, referred to as a post. Research has led to generative models, e.g., Sequence-to-Sequence  (Sutskever et al. (2014) ) and Transformer  (Vaswani et al., 2017) , that produce reasonable responses using the original post solely during the generation process. Recent studies  (Weston et al., 2018; Ghazvininejad et al., 2018; Zheng and Zhou, 2019)  explored more realistic dialogue models that include knowledge related to the posts, typically a collection of sentences that refer to the topics in the posts and responses. Consequently, the response generation Wiz Post: Yep. you've got to select for safety standards, of course, but when you're designing at a Mercedes level the folks buying those cars are going to expect a certain standard of comfort, too! Wiz Response: Especially, I think consumers expect great in Formula One, highest class auto racing. TPPA (top 1): Formula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single seat auto racing that is sanctioned by the Federation Internationale de l'Automobile (FIA). TPPA (top 2): Stock car racing is a form of automobile racing found mainly and most prominently in the United States and Canada, with Australia, New Zealand and Brazil also having forms of stock car auto racing. PRK (top 1): Mercedes is part of the McQueen family and is the longest serving McQueen on the series. PRK (top 2): He also won races in midget cars, and sprint cars. RRK (top 1): Formula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single seat auto racing that is sanctioned by the Federation Internationale de l'Automobile (FIA). RRK (top 2): The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950. Table  1 : Example of a post and a response from the Wizard of Wikipedia (Wiz) data set ( ?5.1) with top 2 ranked outputs from TPPA, the post-retrieved knowledge PRK and the response-retrieved knowledge RRK. Blue indicate words present in the Wiz response and RRK but not in PRK. process involves an information retrieval component that needs to be optimized for the selection and injection of relevant knowledge into the generative model. Evaluation of such approaches has shown that the knowledge based on posts alone may lack focus, i.e., may exhibit topic drifts and thus introduce noise. Table  1  illustrates Post-Retrieved Knowledge (PRK) that has a good overlap with the post but introduces content that is not present in the response and thus deemed non-relevant. By contrast, the Response-Retrieved Knowledge (RRK) shares content with the response, thus illustrating that dialogue training needs to incorporate relevant knowledge related to the response. In practice, however, the key challenge is to implement an effective selection of response related knowledge, considering that the responses to posts are not observed during dialogue generation. In this paper, we present the Transformer & Post based Posterior Approximation (TPPA) method that achieves that by applying multi-stage processing of posts, post related knowledge and response related knowledge to capture word and sentence level characteristics (through word embeddings, Transformer and max-pooling), that can be useful for ranking and selecting knowledge of new posts during the test phase. Table  1  illustrates the high overlap that TPPA outputs achieve with true responses (for postresponse pair from the Wizard of Wikipedia (Wiz) data collection  (Dinan et al., 2019) . Furthermore, we empirically demonstrate the effectiveness of TPPA, by injecting TPPA selected knowledge into generative models, in particular the Transformer Extended Decoder (TED) that allows integrating knowledge from multiple sources  (Zheng and Zhou, 2019) . The combination of TED and TPPA outperforms a set of strong baseline systems, including systems that do not separate knowledge selection from modelling response generation: Post-KS  (Lian et al., 2019)  and SKT (Sequential Latentknowledge Selection)  (Kim et al., 2020) . Most important contributions of our work are: 1. Empirical evidence that generative models with injecting response-retrieved knowledge outperform those that use only post-retrieved knowledge ( ?3). 2. New method for knowledge selection (TPPA) that includes Transformer-based representations of posts and post related knowledge to select relevant knowledge processed with word embedding and MaxPooling ( ?4). 3. Experimental results that demonstrate the benefit of TPPA knowledge injection into the TED generative model  (Zheng and Zhou, 2019) , outperforming state-of-the-art models on two publicly available data sets ( ?5, ?6). In addition, the separation of the knowledge selection from the generative models offers maximum flexibility for integrating and exploring alternative retrieval models and knowledge representations. We make our codes publicly available at https://github.com/tonywenuon/emnlp2020 tppa. 

 Related Work In this section we first discuss retrieval models and then knowledge injection into generative models. Retrieval Models. Most traditional retrieval models, such as BM25  (Robertson et al., 2004) , are unsupervised methods, relying on lexical matching between query terms and document text using different weighting and normalization schemes. In contrast, recent studies use neural ranking models, such as deep structured semantic models (DSSM)  (Huang et al., 2013; Shen et al., 2014) , weakly supervised neural ranking models  (Dehghani et al., 2017)  and jointly trained neural models  (Yan et al., 2016; Mitra et al., 2017) . They are built to respond to information needs represented by a query. We illustrate our approach by adopting BM25 for initial retrieval of relevant knowledge. We also use the post related results to create an extended representation of the post, similar to the pseudo-relevancefeedback in query-based search  (Cao et al., 2008) . Generative Models & Knowledge Injection. Injection of knowledge into generative models has been pursued to improve the quality of responses, considering that during dialogue generation only a post and related knowledge are observed.  Ghazvininejad et al. (2018)  encode and merge knowledge with a post representation, creating a final vector representation that is input into the decoder.  Tam (2020)  extends this method with a copy-mechanism that enables the model to generate response words either from the post or from the generative model. Zheng and Zhou (2019) use the Transformer Extended Decoder (TED) to incorporate words from multiple sources by assigning weights to knowledge sources based on relevance between the knowledge and the decoding words, and taking the weighted-sum vector to generate responses. Closest to our work is the PostKS model proposed by  Lian et al. (2019)  that includes a knowledge manager which fits the prior word distribution (from posts) to the posterior word distribution (with both post and response observed). By applying the Gumbel-Softmax method, they select the best knowledge for the dialogue generation. Similarly, the sequential latent-knowledge selection (SKT) proposed by  Kim et al. (2020)    Dialogue generation models that incorporate knowledge aim to expand the input beyond the observable post and incorporate a responder's knowledge. It is assumed that the available knowledge K p for a given post p includes content that is related to the response, although the quality of that knowledge is not certain. The key issue is, thus, to determine which of the knowledge statements k ? K p are relevant to the unobserved response r. During the training phase, where the post p, response r and K p are all available, we use p and r as queries to rank all the statements in K p and create the corresponding ranked lists: Response-retrieved Knowledge RRK and Post-retrieved Knowledge PRK, respectively. We use lower-case rrk 1 and prk 1 to indicate top 1 ranked item in RRK and PRK, respectively. 

 RRK Assessment on Wiz Training Data In this section, we analyze RRK for the Wiz training data ( ?5.1) where both posts p and responses r are known as well as the corresponding knowledge set K p . Assuming that we deploy a reasonable search algorithm, we expect that rrk 1 will have a high overlap with the response r that is used as a query. We also assume that generative models will be able to use rrk 1 to generate a good quality response considering its overlap with the true response. The objective of this section is to gain insights on what difference RRK can make compared to the use of PRK alone. Word count. We compare the number of common words (after removing stop words) between the original response r and the four sequences: (1) the post p, (2) prk 1 , i.e., the top 1 ranked item in PRK, (3) rrk 1 , i.e., the top 1 ranked item in RRK, and (4) a random post chosen from the data set. The distributions of word overlaps are shown in Figure  1 . The x-axis indicates the count of common words and y-axis shows the percentage of the posts p and responses r sample with the given word overlap. As expected, the word overlaps of p and prk 1 with r are similar, with the overlap of p and r being lower. For the randomly selected post p, the average term overlap with r is slightly lower but close to post p, suggesting that posts alone are not very informative for the response generation. The difference for prk 1 and rrk 1 is quite marked showing that rrk 1 has on average almost twice the overlap of the prk 1 (98% increase). Based on the Kolmogorov-Smirnov test, all the differences among the four groups in Figure  1  are statistically significant. For the Holl-E data set (that is another data set we used in ?5.1), a similar trend is observed. Response generation. We assess the effectiveness of RRK when injected into the generative model by conducting experiments with the standard Transformer  (Vaswani et al., 2017)  and the Transformer with Expanded Decoder (TED)  (Zheng and Zhou, 2019) . Transformer takes only a post while TED uses a post and multiple sources of knowledge to get the responses. Table  2a  shows the results for Transformer with (1) original post, (2) a randomly selected sentence, (3) prk 1 , (4) rrk 1 and (5) a human selected knowledge, i.e., a sentence provided in Wiz. Table  2  with results metrics (BLEU, METEOR and Div-2, ?5) show that replacing the original post by a randomly selected sentence reduces the performance significantly. Using prk 1 leads to lower performance, indicating a possible topic drift and noise. Using rrk 1 shows promising performance improvement; with higher retrieval performance, it may achieve the effectiveness of the human selected knowledge. Similarly, for the TED generative model, we incorporate the post content and evaluate the cumulative effect of adding knowledge from different sources. As expected, the best performance is achieved by the human selection of knowledge followed by the RRK (Table  2b ). In conclusion, it is worthwhile putting an ef- fort to create resources that represent a responder's knowledge and effective retrieval methods to retrieve knowledge relevant to the response content. Since the response is not available, we devise TPPA to leverage post p and post-retrieved knowledge PRK and train models to approximate RRK. 

 TPPA Method In this section, we describe the architecture and the process of selecting knowledge using the TPPA method. Figure  2  depicts three TPPA components: (1) Post Processing Unit comprising a word embedding and a Transformer that incorporates the post p and a set of n of retrieved prk i , where n is determined empirically (typically n = 10 out of 50 knowledge items in K p , on average). The results are a Transformer representation v p for the post and v PRK for all of the prks. In the end, a single v prk (representing the potentially most useful prk for identifying the rrk 1 ) is selected based on Auto-Pointer and Gumble Softmax algorithms. (2) Response Processing Unit that, during training, considers each response r and corresponding K p to get rrk 1 and a set of negs (i.e., m negative samples which are non-relevant knowledge to the rrk 1 ) in order to train a word embedding that forms knowledge representation (we call it as v k ). The number of negative examples m is selected empirically, to avoid overfitting. (3) Knowledge Selection Unit, a search component that uses v p and v prk as queries to score the knowledge representation v k . The score is a weighted sum of similarity metrics using a hyperparameter ? that can be chosen to emphasize the similarity with p or prk. TPPA operation consists of Phase 1: Training phase that utilizes training data (p, r, K p ) to train all the three components of the system based on known responses r; and Phase 2: Test phase during which individual post-knowledge samples (p, K p ) are processed in order to arrive at a selection of knowledge (k ? K p ) to be injected into the generative models. The post p and a set of prk i , i = 1, ..., n (i is the i-th ranked post-related knowledge) are processed with the same Transformer encoder to obtain word representations and then passed through the maxpooling to obtain the sequence semantic vector. e(p) = Transformer ? (e(wi)) 1 ? i ? L (1) vp = maxpool (e(p)) (2) where ? is the trainable parameter set inside the Transformer. p is the input post, w i is the i-th word of the p post sequence. L is the maximum post length. e(w i ) ? R d is the post word embedding for w i , and d is the embedding dimension. e(p) represents the semantic representation of all the words in the post while v p is the post representation (sentence-level). For the prk i , they follow exactly the same process following Equation 1 and 2. We consider multiple knowledge items prk i in order to construct an effective query for knowledge selection that complements the post and increases the chances of selecting knowledge that is relevant to the response. We train an auto-pointer to assign scores to each prk i . The auto-pointer module takes v PRK as input and outputs a PRK scores vector (v ap ) that indicates the importance degree of the prks. This is followed by a Gumbel-Softmax  (Jang et al., 2016)  module to select the best prk for knowledge retrieval: vap = (v PRK W T + b)W T auto pointer (3) v prk = Gumbel-Softmax(vap, v PRK ) (4) where v PRK ? R n?d represents all prk i representations obtained by Eq. 1 and 2 and v prk is the representation of the finally chosen post-related knowledge. W ? R d?d and b ? R d are trainable parameters; W auto pointer ? R 1?d is the trainable auto-pointer for selecting useful prk. 

 Response Processing Unit The knowledge representation v k is obtained by going through raw knowledge word embedding 1 and a max-pooling operation (seeing Figure  2  Response Processing Unit). The conduction of obtaining v k is similar to Eq. 1 and 2 but replacing the Transformer to a raw knowledge word embedding lookup operation. Since the objective is to augment vocabulary and avoid noise, during training, we constrain the positive knowledge to the highly relevant knowledge item, i.e. rrk 1 by using BM25. We also randomly select knowledge to be as negative samples (from the union of all K p after the rrk 1 s of the posts are removed). Both of the positive sample and negative samples will pass through the Response Processing Unit to gain their representations. 

 Knowledge Scoring and Selection Following the post v p and v prk representation and knowledge representation v k , we compute similarities S(p, k) and S(prk, k): S(p, k) = cosine(vp, v k ) vp ? v k ; S(prk, k) = cosine(v prk , v k ) v prk ? v k (5) where S(?) designates the similarity function; v p , v k and v prk refer to the representations of the post, knowledge and the selected prk, respectively. Depending on a type of dialogue, the response may incorporate the content of the post to a different degree. Thus, to support flexible scoring with regards to p and prk, we introduce a hyperparameter, ? to the final scoring function: Score(p, prk, k) = ? ? S(p, k) + (1 ? ?) ? S(prk, k) (6) We tune ? parameter on the training set and in the final Score(p, prk, k), setting it to 0.7 to give more importance to the post. After we get the scores of the positive and negative samples, for all the positive-negative sample pairs, we apply softmax to the similarity scores: P (ki|p, prk) = exp(?Score(p, prk, ki)) exp(?Score(p, prk, ki) (7) calculating the probability of each k i given the post p and prks. k i ? {rrk 1 ; neg 1 , neg 2 , . . . , neg m } are shown in the response processing unit in Figure  2 , where neg 1 , . . . , neg m are m negative samples. ? is a smoothing factor of the softmax function and is a trainable parameter  (Huang et al., 2013) . We maximise the difference between the positive sample and the negative samples scores. Loss = ?log(P (rrk1|p) + j log(negj|p)) (8) where P (rrk 1 |p) is the positive score, P (neg j |p) stands for the j-th negative score, where 1 ? j ? m. m is the number of negative samples. During training, all of the trainable parameters, including the post word embedding, Transformer architecture, auto-pointer and the knowledge word embedding, are updated by mini-batch gradient descent (the setup is in ?5.2). 

 TPPA Test Phase During the test phase, each new post p and corresponding K p is processed using the Post Processing Unit and Response Processing Units, with parameter obtained during the training phase. Each knowledge k i and its corresponding post are scored using the Score(p, prk, k i ) (Eq. 6) and TPPA returns the final rank of the knowledge candidates. 

 Experiments Our approach for knowledge injection separates the knowledge selection from the response generation models. We, thus, evaluate TPPA in terms of (1) precision in selecting relevant knowledge for a given post, judged by whether the rrk 1 can be ranked within top n position, and (2) effectiveness of the retrieved knowledge when injected into a response generation model. 

 Data We experiment with two publicly available data sets: Wizard of Wikipedia (Wiz)  (Dinan et al., 2019)  comprises controlled human-to-human dialogue interactions where the participant can assume the role of a teacher or a student and take turns to discuss a topic. A teacher answers a student's post based on pre-retrieved knowledge that is related to the current topic and the dialogue context. The Wiz data set consists of 22,311 dialogues with 201,999 turns. Each post-response pair is assigned the related-knowledge, i.e., manually selected relevant sentences or paragraphs from Wikipedia. Holl-E  (Moghe et al., 2018)  comprises dialogues between two Amazon Mturk workers 2 about a selected movie, supported by selected sources of background knowledge: movie plots, reviews, comments, and the fact tables related to the movie. A response to a post is either copied or suitably modified from the provided grounded knowledge, mixed from the four knowledge sources. Holl-E data contains 9,071 conversations, covering 921 movies. 

 Baselines, Setup and Metrics Baselines In our experiments we compare TPPA knowledge selection on the retrieval performance with three baseline models: BM25  (Robertson and Walker, 1994 ) is an unsupervised probabilistic retrieval algorithm, which is robust for short document (sentence) retrieval. DrQA  (Chen et al., 2017)  uses bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model. CNN-DSSM  (Shen et al., 2014)  uses CNN for semantic matching of queries and documents. In order to evaluate the effectiveness of the selected knowledge for response generation, we compare TPPA output with three models: WSeq  (Tian et al., 2017)  uses weighted sum and concatenation of the post and its contextual utter-ances, and obtain representations through an RNN. MemNet  (Ghazvininejad et al., 2018 ) leverages a multi-task learning framework to jointly train 'post-to-response', 'knowledge-to-response' and 'knowledge-to-knowledge' tasks for response generation. TED  (Zheng and Zhou, 2019)  adopts Transformer as the backbone framework to inject knowledge by assigning weights to the knowledge from multiple sources. Finally, we consider two methods that jointly train knowledge selection model and dialogue generation model, and use them in both sets of experiments: Post-KS  (Lian et al., 2019)  approximates posterior-distribution of knowledge, i.e., p(k|p, r) using prior-distribution p(k|p) and jointly train a knowledge selection model and a dialogue generation model. SKT  (Kim et al., 2020)  takes into a account context from multi-turn dialogues (current action and 2 prior turns) and considers knowledge selection as a sequential decision process. Experimental Setup In our experiments, the dimension of word embedding is 300, and the multihead number of Transformer is 4. The vocabulary is obtained by ranking the training data by word frequency, with the size of 50,000 top frequent terms selected. The minimum post length is set to 8 tokens. Each knowledge item is represented by a sentence. During model training, we use mini-batch size 64. Adam optimiser is used for optimisation. The initial learning rate is set to 0.001 and halved when reaching the plateau (decreasing patience is set to 2 epochs). All the experiments are run on a single TITAN V GPU. The TPPA model requires 2 hours to train on the Wiz data set. Metrics Quality of the generated responses is evaluated using five standard metrics: BLEU  (Papineni et al., 2002) , Meteor  (Banerjee and Lavie, 2005) , and Bert-Score (BS)  (Zhang et al., 2019)  that are based on co-occurrence of n-grams between the system response and the ground-truth, calculating the token similarity using contextual embeddings. In this work, the BS version we used is robertalarge L17 idf version=0.3.3(hug trans=2.8.0) 3 ; Diversity scores (Div-2)  (Li et al., 2015)  calculates the proportion of distinct bi-grams out of all the distinct words. For knowledge selection, we use P@n that calculates the precision at a given rank n, measuring whether the ground truth (rrk 1 ) exists within the top n retrieved knowledge. 

 Exp Model Wizard of Wikipedia (%) P@1 P@5 P@10 BM25 4.9  

 Experimental Results Knowledge Selection Evaluation. For the TPPA method, the quality of the selected knowledge is determined by the embedding parameters obtained during the training phase. They are, in turn, related to the knowledge resources used for training (Response Processing Unit) and the quality of the transformer representation of p and prk (Post Processing Unit), shown in Figure  2 . The resources are constructed from individual knowledge sets K p , where p is the post in the training set. For each training sample, it consists of a post p, a rrk 1 (i.e. the top 1 ranked response-retrieved knowledge), n prks (i.e. the top n ranked post-retrieved knowledge) and m negs (i.e. randomly chosen m sentences). Thus, 1rrk-1neg-10prk indicates that we selected the rrk 1 , 1 random knowledge item and top 10 prks for each p. In the test experiments, we monitor whether, for a new post p in the test set, different retrieval models rank its corresponding ground truth, i.e., rrk 1 for p within the top 1, 5, or 10 ranked items. at least one model that outperforms all other models on the Wiz and Holl-E data sets, on all three metrics P@1, P@5, and P@10. 

 Results in (2) The composition of the knowledge base affects the TPPA knowledge selection: for the Wiz data set and fixed number of 10prk, increasing the number of neg items improves the performance until reaching its plateau at 1rrk-30neg-10prk; for the Holl-E data set, the best combination is 1rrk-10neg-10prk. (3) For a fixed number of neg we vary the number of prks items and find that: (i) for Wiz and n=30, the optimal prk number is 1; and (ii) for Holl-E and neg=10 the optimal prk number is 10. Based on these findings we use 1rrk-30neg-1prk for Wiz and 1rrk-10neg-10prk for Holl-E as sets for TPPA to select knowledge for use with MemNet, WSeq and TED models on response generation. Response Generation Evaluation. We conduct the initial set of experiments to assess the robustness of the generative models (Table  4 ) and find that: (i) SKT and TED models outperform others, (ii) MemNet has unstable performance and constantly under-performs on Div-2. Furthermore, since SKT and Post-KS cannot inject multiple knowledge items, for further discussion, we choose experiments with WSeq and TED. We combine them with knowledge selection from (i) BM25, (ii) SKT (single knowledge item), (iii) CNN-DSSM (supervised search algorithm on post only), (iv) TPPA using both post and post-retrieved knowledge items, and (v) rrk i (i means top i ranked response-retrieved knowledge, it is set to 1, 5 and 10 in our setting), to determine the upper bound when responses are known). The comparisons for the two data sets are shown in Table  5 and Table 6 . We observe that: (1) Injecting knowledge from SKT, CNN-DSSM and TPPA generally outperforms the post only selection using BM25 (Table  5  and 6) on both the Wiz and Holl-E data sets in terms of the BLEU-4, METEOR and Bert-Score. TED performance suffers from increased knowledge injection. Indeed, for TED + rrk i , i.e., using 'perfect knowledge' the performance decreases with the increasing number of knowledge items.  Zheng and Zhou (2019)  claim that TED lacks a noise-filtering mechanism and thus underperforms with too much data. (2) Not surprisingly, knowledge selection methods with better retrieval performance achieve better response generation metrics. We consider Table  5  and 6 and the corresponding retrieval performance in Table  3 . For the Wiz data set, the TPPA with 1rrk-30neg-1prk achieves the best retrieval performance and better results (Table  5 ) on both generative models (TED and WSeq) across different settings. This is confirmed on the Holl-E data set (Table  6 ) where TPPA outperforms other models, including Post-KS and SKT. This confirms our conjecture that improving retrieval for knowledge injection should improve the response generation. Upper-bound Analysis. The upper bound for knowledge selection is the rrk i group. We observe how all of the retrieval models perform in combination with TED and WSeq (Table  5 and  6 ). For the sake of concreteness we focus on the BLEU-4 metric.  calculating UWOR. We further test whether the retrieved knowledge brings additional useful words. We calculate UWOR(k ? p, r), where k ? p is a set of words in the knowledge (k ? K p ) but not in the associated post p, i.e., {w|w ? k ? w / ? p}, w is the word of a sequence. The results are shown in Table  7 . For each experiment group in Table  7 , we select the top 1 ranked sentence for calculation. UWOR(p, r) values for the Wiz and Holl-E data sets are just 14.6% and 7.52%, respectively. Considering the TPPA, for Wiz the number of additionally added useful words are comparable to what the post brings (10.25% vs. 14.6%); for the Holl-E, the retrieved knowledge brings more than double the useful words than the post (15.98% vs. 7.52%). This demonstrates the effectiveness of TPPA that can expand additional useful words from knowledge. 

 Conclusions and Discussions Our investigations of the knowledge associated with post-response pairs lead us valuable insights into how well selected response-retrieved knowledge RRK can improve the performance of the generative models. Considering that response not observable in the test phase, we developed a TPPA method that selects knowledge items by the careful embedding of the knowledge and optimized representation of the post and post-related knowledge PRK. We empirically demonstrate the superiority of TPPA, and being separated from the generative models. This provides flexibility to explore alternative components and models. Despite its effectiveness, we now discuss one potential limitation of our TPPA model.We find that the quality of the knowledge base has a huge impact on the effectiveness of TPPA. The Wiz and Holl-E we experiment with are two data sets from which candidate knowledge items are of high quality and manually selected. As shown in Figure  1  for the Wiz dataset, rrk 1 group contains on average more than two common words than prk 1 group that would help to constitute the ground truth response. The same trend also holds for the Holl-E data set. However, when looking at the Reddit data set 4 , as shown in Figure  3 , we find that rrk 1 group and prk 1 group almost contain the same number of common words, compared to the ground-truth response. This is not surprising given the nature of this dataset: Reddit is an online forum where each post is typically initiated with a URL to a web page (grounding) that defines the topic of the post, provided by the author. However, the repliers of the post might not read that information at all and respond according to their own knowledge. Empirically, we find TPPA can not benefit from the knowledge under this circumstance and perform worse than the baselines. This implies that when knowledge is potential of low quality, using PRK as the source of evidence for pseudo relevance feedback can result in potential topic drift. In future work, we would like to (1) make TPPA more robust irrespective of the quality of provided knowledge; (2) develop an end-to-end model that directly model response generation with the help of response-related knowledge. jointly trains the knowledge selection and the dialogue generation model. Both methods consider knowledge relevance to posts and responses during training but do not leverage post-retrieved knowledge during testing. Our proposed Transformer & Post based Posterior Approximation (TPPA) model distinguishes itself by explicitly incorporating response related knowledge into training and applying pseudo relevance feedback approach by training an autopointer vector to identify potentially the most relevant knowledge. Combined with the TED generative model, TPPA leads to responses that outperform state-of-the-art methods ( ?6). 

 3 Problem Statement and Motivation 3.1 Key Notations and Research Objectives 

 Figure 1 : 1 Figure 1: Common words count distribution between each source and the target response on the Wiz training set. The dashed lines are the average count of common words of each group (after removing stop words). 

 4.1 TPPA Training phase 4.1.1 Post and PRK Processing 

 Figure 2 : 2 Figure 2: TPPA Architecture comprises (1) Post Processing Unit, (2) Response Processing Unit (right) and (3) Knowledge Selection Unit (middle). 

 Figure 3 : 3 Figure 3: Common words count distribution between each source and the target response on the Reddit training set. The dashed lines are the average count of common words of each group (after removing stop words). 

 Table 2 : 2 Injection of various sources into the Transformer and TED using Wiz data set. All the values are percentages reported by the performance metrics (%). (a) Transformer BLEU-4 METEOR Div-2 Original Post 1.76 6.6 7.3 Random Post 0.39 4.47 0.19 prk1 1.23 6.36 5.62 rrk1 2.85 7.99 12.88 Human selection 4.6 9.97 18.86 (b) TED BLEU-4 METEOR Div-2 Post+1 Random sentence 2.8 7.13 18.73 Post+prk1 3.35 8.45 16.2 Post+rrk1 8.14 11.36 24.63 Post+Human selection 10.06 13.13 25.7 

 Table 3 : 3 Retrieval precision on the Wiz and Holl-E data sets. '*' means t-test p < 0.05 compared with the baseline BM 25; ' ?' is the p < 0.05 compared with the best performing group. Bold indicates the best performance group when changing the number of negative samples. Underline indicates the best group among all methods. ? 18.6 ? 31.1 ? DrQA 4.1 ? 13.6* ? 21.7* ? CNN-DSSM 8.2* ? 31.3* ? 48.8* ? Post-KS 6.2* ? - - SKT 9.01* - - 1rrk-1neg-10prk 8.9* ? 33.0* ? 49.2* ? 1rrk-4neg-10prk 10.0* 36.5* ? 54.5* TPPA 1rrk-10neg-10prk 1rrk-20neg-10prk 9.8* 10.1* 36.4* ? 37.8* 54.2* ? 55.0* 1rrk-30neg-10prk 10.1* 38.0* 55.1* 1rrk-40neg-10prk 8.2* ? 31.3* ? 48.2* ? 1rrk-30neg-1prk 10.2* 38.4* 55.1* TPPA 1rrk-30neg-10prk 1rrk-30neg-20prk 10.1* 10.0* 38.0* 37.3* ? 55.1* 55.1* 1rrk-30neg-30prk 9.7* 35.2* ? 52.4* ? Exp Model P@1 Holl-E (%) P@5 P@10 BM25 10.5 ? 33.4 ? 48.5 ? DrQA 13.3* ? 29.4* ? 35.4* ? CNN-DSSM 15.2* ? 34.9* ? 50.0 ? Post-KS 5.5* ? - - SKT 11.6* ? - - 1rrk-1neg-10prk 13.6* ? 37.0* ? 51.3* ? 1rrk-4neg-10prk 15.5* ? 38.3* ? 52.7* ? TPPA 1rrk-10neg-10prk 1rrk-20neg-10prk 16.6* 14.8* ? 40.4* 36.9* ? 54.5* 51.1 ? 1rrk-30neg-10prk 15.7* ? 39.1* ? 53.2 ? 1rrk-40neg-10prk 16.2* 39.5* 53.2 1rrk-10neg-1prk 16.3* 39.0* ? 52.7* ? TPPA 1rrk-10neg-10prk 1rrk-10neg-20prk 16.6* 16.6* 40.4* 39.0* 54.5* 52.9* ? 1rrk-10neg-30prk 15.4* ? 38.6* 52.7* ? 

 Table 4 : 4 Performance of generative models MemNet, WSeq and TED with the best TPPA knowledge selection. Post-KS and SKT rely on their jointly trained models. BS refers to Bert-Score. Exp Model BLEU-4 Wizard of Wikipedia (%) METEOR Div-2 BS MemNet 1.24 6.39 2.24 81.5 WSeq 2.13 7.17 13.29 82.86 Post-KS 1.35 5.96 22.32 81.3 SKT 3.14 7.29 27.8 83.4 TED 3.91 8.82 18.16 82.9 Exp Model BLEU-4 Holl-E (%) METEOR Div-2 BS MemNet 5.59 7.63 0.18 84.6 WSeq 5.9 7.94 3.63 83.71 Post-KS 3.79 5.98 2.41 81.3 SKT 9.16 8.48 22.9 82.9 TED 12.66 10.37 17.95 84.1 Table 3 show that: (1) TPPA provides 

 Table 5 : 5 Knowledge-injection results on the Wizard of Wikipedia data set. The values are percentages (%). '*' means the t-test p < 0.05 compared with the BM25 algorithm. 'Top 1', 'Top 5' 'Top 10' denotes injecting top 1 or 5 or 10 ranking knowledge. BS is Bert-Score. Bold indicates the best score apart from the rrk i group. TED+Top 1 BLEU-4 METEOR Div-2 BS BM25 3.35 8.45 16.2 82.7 SKT 4.05* 8.82* 18.8* 82.8* CNN-DSSM 3.5 8.62 20.08* 82.8 TPPA 3.91* 8.82* 18.16 82.9 rrk1 8.14* 11.36* 24.63* 84.3* TED+Top 5 BLEU-4 METEOR Div-2 BS BM25 3.17 7.81 18.33 82.99 CNN-DSSM 3.81 8.82 16.98 83.16 TPPA 3.88* 8.97* 17.22* 83.23 rrk5 4.99* 10.49* 19.04* 83.7* TED + Top 10 BLEU-4 METEOR Div-2 BS BM25 3.01 7.98 15.7 83.2 CNN-DSSM 3.59* 8.98* 14.8* 83.38 TPPA 3.53* 9.09* 14.66* 83.4* rrk10 4.05* 9.56* 15.87* 83.6* WSeq+Top 1 BLEU-4 METEOR Div-2 BS BM25 1.94 6.98 12.96 82.76 SKT 2.0 7.02 13.73 82.8 CNN-DSSM 2.04 7.07 13.25 82.81 TPPA 2.13 7.17* 13.29 82.86 rrk1 2.23* 7.35* 13.23 83.0* WSeq+Top 5 BLEU-4 METEOR Div-2 BS BM25 2.05 7.18 17.59 82.85 CNN-DSSM 2.07 7.37 18.32 83.03* TPPA 2.15* 7.57* 18.55* 83.1* rrk5 2.61* 8.0* 18.75* 83.3* WSeq + Top 10 BLEU-4 METEOR Div-2 BS BM25 2.31 7.44 19.48 83.0 CNN-DSSM 2.44 7.88* 20.19 83.3* TPPA 2.59 7.97 19.72 83.35 rrk10 3.01* 8.67* 21.07 83.66* 

 Table 6 : 6 Knowledge-injection results on the Holl-E data set. The values are percentages (%). '*' means the t-test p < 0.05 compared with the BM25 algorithm. TED+Top 1 BLEU-4 METEOR Div-2 BS BM25 9.87 9.09 26.21 83.6 SKT 9.01 8.56 19.86* 83.4* CNN-DSSM 11.56* 9.84* 23.51* 83.9 TPPA 12.66* 10.37* 17.95* 84.1* rrk1 45.94* 30.61* 29.03* 89.6* TED+Top 5 BLEU-4 METEOR Div-2 BS BM25 11.4 10.22 24.16 83.9 CNN-DSSM 12.02 10.4 23.71 84.0 TPPA 12.92* 11.12* 17.87* 84.2 rrk5 21.81* 17.15* 24.96* 85.9* TED + Top 10 BLEU-4 METEOR Div-2 BS BM25 5.5 8.36 2.45 83.5 CNN-DSSM 5.39 8.24 2.6* 83.6 TPPA 5.6 8.24 2.53* 83.6 rrk10 6.53* 9.88* 2.75* 84.0* WSeq+Top 1 BLEU-4 METEOR Div-2 BS BM25 4.58 7.25 4.33 83.68 SKT 5.81* 7.77* 3.09 83.6* CNN-DSSM 5.6* 7.62* 4.48* 83.5* TPPA 5.9* 7.94* 3.63* 83.71 rrk1 6.5* 8.95* 4.6* 83.97* WSeq+Top 5 BLEU-4 METEOR Div-2 BS BM25 5.15 7.51 8.65 83.43 CNN-DSSM 5.53* 7.69 9.78* 83.17* TPPA 5.96* 7.74* 7.82* 83.59* rrk5 7.22* 9.55* 9.39* 83.85* WSeq + Top 10 BLEU-4 METEOR Div-2 BS BM25 5.28 7.15 13.85 83.43 CNN-DSSM 5.88* 7.35* 16.26* 83.3* TPPA 5.89* 7.43* 12.43* 83.7* rrk10 8.19* 10.41* 15.73* 84.3* 'Top 1', 'Top 5', 'Top 10' denotes injecting top 1 or 5, or 10 ranking knowledge. BS is Bert-Score. Bold indicates the best score apart from the rrk i group. 

 Table 5 5 and 6 show that low levels = overlap(p, r) / distinct(r) for post p and response r. The overlap(?) is the number of distinct overlapping useful words between two sequences. distinct(?) is a distinct number of words. We remove the stop words of the two sequences before 

 Table 7 : 7 The useful word overlapping rate results of Wiz and Holl-E data sets. All values are shown as percentages (%). 

			 Alternative approaches, e.g., using Transformer based representations, were considered but led to sub-optimal results within the current TPPA set up. 

			 Amazon Mturk is a crowd-sourcing marketplace that can employ workers to annotate corpus, https://www.mturk.com/. 

			 https://github.com/Tiiiger/bert score 

			 https://github.com/mgalley/DSTC7-End-to-End-Conversation-Modeling
