title
Learning an Unreferenced Metric for Online Dialogue Evaluation

abstract
Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.

Introduction Recent approaches in deep neural language generation have opened new possibilities in dialogue generation . Most of the current language generation efforts are centered around language modelling or machine translation , which are evaluated by comparing directly against the reference sentences. In dialogue, however, comparing with a single reference response is difficult, as there can be many reasonable responses given a context that have nothing to do with each other  (Liu et al., 2016) . Still, dialogue research papers tend to report scores based on word-overlap metrics from the machine translation literature (e.g. BLEU  (Papineni et al., 2002) , METEOR  (Denkowski and Lavie, 2014) ). However word-overlap metrics aggressively penalize the generated response based on lexical differences with the ground truth and correlate poorly to human judgements  (Liu et al., 2016) . One can build dialogue evaluation metrics in two ways: referenced metrics, which compare the generated response with a provided ground-truth response (such as the above word-overlap metrics), or an unreferenced metrics, which evaluate the generated response without any such comparison.  propose a learned referenced metric named ADEM, which learns an alignment score between context and response to predict human score annotations. However, since the score is trained to mimic human judgements, it requires collecting large-scale human annotations on the dataset in question and cannot be easily applicable to new datasets  (Lowe, 2019) . Recently,  Tao et al. (2017)  proposed a hybrid referenced-unreferenced metric named RUBER, where the metric is trained without requiring human responses by bootstrapping negative samples directly from the dataset. However, referenced metrics (including RUBER, as it is part referenced) are not feasible for evaluation of dialogue models in an online setting-when the model is pitched against a human agent (model-human) or a model agent (model-model)-due to lack of a reference response. In this setting, models are usually eval-uated directly by humans, which is costly and requires careful annotator training  (Li et al., 2019) . The contributions of this paper are (1) a completely unsupervised unreferenced metric MAUDE (Metric for automatic Unreferenced dialogue evaluation), which leverages state-of-the-art pretrained language models  (Devlin et al., 2018; Sanh et al., 2019) , combined with a novel discoursestructure aware text encoder and contrastive training approach; and (2) results showing that MAUDE has good correlation with human judgements. 

 Background We consider the problem of evaluating the response of a dialogue system, where an agent is provided with a sequence of sentences (or utterances) c = {u 1 , u 2 , ..., u n } (termed as context) to generate a response r = u n+1 . Each utterance, u i , can be represented as a set of words u i = {w 1 , w 2 , ..., w n }. An utterance u i can be represented as a vector as h i = f e (u i ), where f e is an encoder that encodes the words into a fixed vector representation. This work focuses on the evaluation of generative neural dialogue models, which typically consist of an encoder-decoder style architecture that is trained to generate u n+1 word-by-word . The response of a generative model is typically evaluated by comparing with the ground-truth response using various automatic word-overlap metrics, such as BLEU or METEOR. These metrics, along with ADEM and RUBER, are essentially single-step evaluation metrics, where a score is calculated for each contextresponse pair. If a dialogue D i contains n utterances, we can extract n ? 1 context-response pairs : (c 1 : {u 1 }, r 1 : {u 2 }), (c 2 : {u 1 , u 2 }, r 2 : {u 3 }), . . . , (c n?1 : {u 1 . . . u n?1 }, r n?1 : u n ). In this paper, we are interested in devising a scalar metric that can evaluate the quality of a contextresponse pair: score(c i , r i ) = R ? (0, 1). A key benefit of this approach is that this metric can be used to evaluate online and also for better training and optimization, as it provides partial credit during response generation. 

 Proposed model We propose a new model, MAUDE, for online unreferenced dialogue evaluation. We first describe the general framework behind MAUDE, which is inspired by the task of measuring alignment in natural language inference (NLI)  (Williams et al., 2017) . It involves training text encoders via noise contrastive estimation (NCE) to distinguish between valid dialogue responses and carefully generated negative examples. Following this, we introduce our novel text encoder that is designed to leverage the unique structural properties of dialogue. MAUDE is designed to output a scalar score(c i , r i ) = R ? (0, 1), which measures how appropriate a response r i is given a dialogue context c i . This task is analogous to measuring alignment in NLI, but instead of measuring entailment or contradiction, our notion of alignment aims to quantify the quality of a dialogue response. As in NLI, we approach this task by defining encoders f ? e (c) and f ? e (r) to encode the context and response, a combination function f comb (.) to combine the representations, and a final classifier f t (.), which outputs the alignment score: score(c, r) = ?(f t (f comb (f ? 1 e (c), f ? 2 e (r))). (1) The key idea behind an unreferenced dialogue metric is the use of Noise Contrastive Estimation (NCE)  (Gutmann and Hyv?rinen, 2010)  for training. Specifically, we train the model to differentiate between a correct response (score(c, r) ? 1), and a negative response (score(c, r) ? 0), where r represents a candidate false response for the given context c. The loss to minimize contains one positive example and a range of negative examples chosen from a sampling policy P (r): L = ? log(score(c, r))?E r?P (r) log(?score(c, r)). (2) The sampling policy P (r) consists of syntactic and semantic negative samples. Syntactic negative samples. We consider three variants of syntax level adversarial samples: wordorder (shuffling the ordering of the words of r), word-drop (dropping x% of words in r) and wordrepeat (randomly repeating words in r). Semantic negative samples. We also consider three variants of negative samples that are syntactically well formed, but represent corruption in the semantic space. First, we choose a response r j which is chosen at random from a different dialogue such that r j = r i (random utterance). Second, we use a pre-trained seq2seq model on the dataset, and pair random seq2seq generated response with r i (random seq2seq). Third, to provide a bigger variation of semantically negative samples, for each r i we generate high-quality paraphrases r b i using Back-Translation . We pair random Back-Translations r b j with r i as in the above setup (random back-translation). We also provide the paired r b i as positive example for the models to learn variation in semantic similarity. We further discuss the effect of different sampling policies in Appendix C. Dialogue-structure aware encoder. Traditional NLI approaches (e.g.,  Conneau et al. (2017) ) use the general setup of Equation 1 to score contextresponse pairs. The encoder f e is typically a Bidirectional LSTM-or, more recently, a BERT-based model  (Devlin et al., 2018) , which uses a large pre-trained language model. f comb is defined as in  Conneau et al. (2017) : f comb (u, v) = concat([u, v, u * v, u ? v]). (3) However, the standard text encoders used in these traditional NLI approaches ignore the temporal structure of dialogues, which is critical in our setting where the context is composed of a sequence of distinct utterances, with natural and stereotypical transitions between them. (See Appendix A for a qualitative analysis of these transitions). Thus we propose a specialized text encoder for MAUDE, which uses a BERT-based encoder f BERT e but additionally models dialogue transitions using a recurrent neural network: h u i = D g f BERT e (u i ), h u i+1 = f R (h u i , h u i ), c i = W.pool ?t?{u 1 ,...,u n?1 } (h t ) score(c i , r i ) = ?(f t ([h r i , c i , h r i * c i , h r i ? c i ])), (4) where h u i ? R d is a downsampled BERT repre- sentation of the utterance u i (using a global learned mapping D g ? R B?d ). h u i is the hidden repre- sentation of f R for u i , where f R is a Bidirectional LSTM. The final representation of the dialogue context is learned by pooling the individual hidden states of the RNN using max-pool (Equation  4 ). This context representation is mapped into the response vector space using weight W, to obtain c i . We then learn the alignment score between the context c i and response r i 's representation h r i following Equation  1 , by using the combination function f comb being the same as in Equation  3 . 

 Experiments To empirically evaluate our proposed unreferenced dialogue evaluation metric, we are interested in answering the following key research questions: ? Q1: How robust is our proposed metric on different types of responses? ? Q2: How well does the self-supervised metric correlate with human judgements? Datasets. For training MAUDE, we use Per-sonaChat  (Zhang et al., 2018) , a large-scale opendomain chit-chat style dataset which is collected by human-human conversations over provided user persona. We extract and process the dataset using ParlAI  (Miller et al.)  platform. We use the public train split for our training and validation, and the public validation split for testing. We use the human-human and human-model data collected by  See et al. (2019)  for correlation analysis, where the models themselves are trained on PersonaChat. Baselines. We use InferSent  (Conneau et al., 2017)  and unreferenced RUBER as LSTM-based baselines. We also compare against BERT-NLI, which is the same as the InferSent model but with the LSTM encoder replaced with a pre-trained BERT encoder. Note that these baselines can be viewed as ablations of the MAUDE framework using simplified text encoders, since we use the same NCE training loss to provide a fair comparison. Also, note that in practice, we use DistilBERT  (Sanh et al., 2019)  instead of BERT in both MAUDE and the BERT-NLI baseline (and thus we refer to the BERT-NLI baseline as DistilBERT-NLI). 1 . 

 Evaluating MAUDE on different types of responses We first analyze the robustness of MAUDE by comparing with the baselines, by using the same NCE training for all the models for fairness. We evaluate the models on the difference score, ? = score(c, r ground-truth ) ? score(c, r) (Table  6 ). ? provides an insight on the range of score function. An optimal metric would cover the full range of good and bad responses. We evaluate response r in three settings: Semantic Positive: responses that are semantically equivalent to the ground truth response; Semantic Negative: responses that are semantically opposite to the ground truth response; and Syntactic Negative: responses that have been adversarially modified in the lexical units. Ideally, we would want ? ? 1 for semantic and syntactic negative responses, ? ? 0 for semantic positive responses. We observe that the MAUDE scores perform robustly across all the setups. RUBER and InferSent baselines are weak, quite understandably so because they cannot leverage the large pre-trained language model data and thus is poor at generalization. DistilBERT-NLI baseline performs significantly better than InferSent and RUBER, while MAUDE scores even better and more consistently overall. We provide a detailed ablation of various training scenarios as well as the absolute raw ? scores in Appendix C. We also observe both MAUDE and DistilBERT-NLI to be more robust on zero-shot generalization to different datasets, the results of which are available in Appendix B. 

 Correlation with human judgements Metrics are evaluated on correlation with human judgements  Tao et al., 2017) , or by evaluating the responses of a generative model trained on the metric  (Wieting et al., 2019) , by human evaluation. However, this introduces a bias either during the questionnaire setup or during data post-processing in favor of the proposed metric. In this work, we refrain from collecting human annotations ourselves, but refer to the recent work by  See et al. (2019)  on PersonaChat dataset. Thus, the evaluation of our metric is less subject to bias. See et al. (  2019 ) conducted a large-scale human evaluation of 28 model configurations to study the effect of controllable attributes in dialogue generation. We use the publicly released model-human and human-human chat logs from  See et al. (2019)  to generate the scores on our models, and correlate them with the associated human judgement on a Likert scale.  See et al. (2019)  propose to use a multi-step evaluation methodology, where the hu- man annotators rate the entire dialogue and not a context-response pair. On the other hand, our setup is essentially a single-step evaluation method. To align our scores with the multi-turn evaluation, we average the individual turns to get an aggregate score for a given dialogue. We investigate the correlation between the scores and uncalibrated individual human scores from 100 crowdworkers (Fig.  2 ), as well as aggregated scores released by  See et al. (2019)  which are adjusted for annotator variance by using Bayesian calibration  (Kulikov et al., 2018)  (Table  2 ). In all cases, we report Spearman's correlation coefficients. For uncalibrated human judgements, we observe MAUDE having higher relative correlation in 6 out of 8 quality measures. Interestingly, in case of calibrated human judgements, DistilBERT proves to be better in half of the quality measures. MAUDE achieves marginally better overall correlation for calibrated human judgements, due to significantly strong correlation on specifically two measures: Interestingness and Engagingness. These measures answers the questions "How interesting or boring did you find this conversation?" and "How much did you enjoy talking to this user?". (Refer to Appendix B of  See et al. (2019)  for the full list of questions). Overall, using large pre-trained language models provides significant boost in the human correlation scores. 

 Conclusion In this work, we explore the feasibility of learning an automatic dialogue evaluation metric by leveraging pre-trained language models and the temporal structure of dialogue. We propose MAUDE, which is an unreferenced dialogue evaluation metric that leverages sentence representations from large pretrained language models, and is trained via Noise Contrastive Estimation. MAUDE also learns a recurrent neural network to model the transition between the utterances in a dialogue, allowing it to correlate better with human annotations. This is a good indication that MAUDE can be used to evaluate online dialogue conversations. Since it provides immediate continuous rewards and at the singlestep level, MAUDE can be also be used to optimize and train better dialogue generation models, which we want to pursue as future work. 
