title
UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues

abstract
Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose "UniConv" -a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, contextto-text, and end-to-end settings on the Multi-WOZ2.1 benchmark, achieving superior performance over competitive baselines. Human: hello Dialogue agent: hi ! how can i help you ? Dialogue state: {} Dialogue acts: [greeting] Human: i would like a place to eat in the expensive price range Dialogue agent: sure , what type of food are you interested in ? Dialogue state: {restaurant: {pricerange: expensive}} Dialogue acts: [request-food] Human: could you make a suggestion ? one in the centre ? Dialogue agent: fitzbillies restaurant is an expensive british restaurant in the centre . can i book that for you ? Dialogue state: {restaurant: {pricerange: expensive, area: centre}} Dialogue acts: [inform-restaurant, request-booking] ... ...

Introduction A conventional approach to task-oriented dialogues is to solve four distinct tasks: (1) natural language understanding (NLU) which parses user utterance into a semantic frame, (2) dialogue state tracking (DST) which updates the slots and values from semantic frames to the latest values for knowledge base retrieval, (3) dialogue policy which determines an appropriate dialogue act for the next system response, and (4) response generation which generates a natural language sequence conditioned on the dialogue act. This traditional pipeline modular framework has achieved remarkable successes in task-oriented dialogues  (Wen et al., 2017; Liu and Lane, 2017; Williams et al., 2017; Zhao et al., 2017) . However, such kind of dialogue system is not fully optimized as the modules are loosely integrated and often not trained jointly in an end-to-end manner, and thus may suffer from increasing error propagation between the modules as the complexity of the dialogues evolves. A typical case of a complex dialogue setting is when the dialogue extends over multiple domains. A dialogue state in a multi-domain dialogue should include slots of all applicable domains up to the current turn (See Table  1 ). Each domain can have shared slots that are common among domains or unique slots that are not shared with any. Directly applying single-domain DST to multi-domain dialogues is not straightforward because the dialogue states extend to multiple domains. A possible approach is to process a dialogue of N D domains multiple times, each time obtaining a dialogue state of one domain. However, this approach does not allow learning co-reference in dialogues in which users can switch from one domain to another. As the number of dialogue domains increases, traditional pipeline approaches propagate errors from dialogue states to dialogue policy and subsequently, to natural language generator. Recent efforts  Madotto et al., 2018; Wu et al., 2019b)  address this problem with an integrated sequence-to-sequence structure. These approaches often consider knowledge bases as memory tuples rather than relational entity tables. While achieving impressive performance, these approaches are not scalable to large-scale knowledgebases, e.g. thousands of entities, as the memory cost to query entity attributes increases substantially. Another limitation of these approaches is the absence of dialogue act modelling. Dialogue act is particularly important in task-oriented dialogues as it determines the general decision towards task completion before a dialogue agent can materialize it into natural language response (See Table  1 ). To tackle the challenges in multi-domain taskoriented dialogues while reducing error propagation among dialogue system modules and keeping the models scalable, we propose UniConv, a unified neural network architecture for end-to-end dialogue systems. UniConv consists of a Bi-level State Tracking (BDST) module which embeds natural language understanding as it can directly parse dialogue context into a structured dialogue state rather than relying on the semantic frame output from an NLU module in each dialogue turn. BDST implicitly models and integrates slot representations from dialogue contextual cues to directly generate slot values in each turn and thus, remove the need for explicit slot tagging features from an NLU. This approach is more practical than the traditional pipeline models as we do not need slot tagging annotation. Furthermore, BDST tracks dialogue states in dialogue context in both slot and domain levels. The output representations from two levels are combined in a late fusion approach to learn multi-domain dialogue states. Our dialogue state tracker disentangles slot and domain representation learning while enabling deep learning of shared representations of slots common among domains. UniConv integrates BDST with a Joint Dialogue Act and Response Generator (DARG) that simultaneously models dialogue acts and generates system responses by learning a latent variable representing dialogue acts and semantically conditioning output response tokens on this latent variable. The multitask setting of DARG allows our models to model dialogue acts and utilize the distributed representations of dialogue acts, rather than hard discrete output values from a dialogue policy module, on output response tokens. Our response generator incorporates information from dialogue input components and intermediate representations progressively over multiple attention steps. The output representations are refined after each step to obtain high-resolution signals needed to generate appropriate dialogue acts and responses. We combine both BDST and DARG for end-to-end neural dialogue systems, from input dialogues to output system responses. We evaluate our models on the large-scale Mul-tiWOZ benchmark , and compare with the existing methods in DST, context-to-text generation, and end-to-end settings. The promising performance in all tasks validates the efficacy of our method. 

 Related Work Dialogue State Tracking. Traditionally, DST models are designed to track states of singledomain dialogues such as  WOZ (Wen et al., 2017)  and DSTC2  (Henderson et al., 2014a)  benchmarks. There have been recent efforts that aim to tackle multi-domain DST such as  (Ramadan et al., 2018; Lee et al., 2019; Wu et al., 2019a; . These models can be categorized into two main categories: Fixed vocabulary models  (Zhong et al., 2018; Ramadan et al., 2018; Lee et al., 2019) , which assume known slot ontology with a fixed candidate set for each slot. On the other hand, open-vocabulary models  (Lei et al., 2018; Wu et al., 2019a; Le et al., 2020)  derive the candidate set based on the source sequence i.e. dialogue history, itself. Our approach is more related to the open-vocabulary approach as we aim to generate unique dialogue states depending on the input dialogue. Different from previous  generation-based approaches, our state tracker can incorporate contextual information into domain and slot representations independently. 

 Z dst Context-to-Text Generation. This task was traditionally solved by two separate dialogue modules: Dialogue Policy  (Peng et al., 2017 (Peng et al., , 2018  and NLG  (Wen et al., 2016; Su et al., 2018) . Recent work attempts to combine these two modules to directly generate system responses with or without modeling dialogue acts.  Zhao et al. (2019)  models action space of dialogue agent as latent variables.  predicts dialogue acts using a hierarchical graph structure with each path representing a unique act.  Pei et al. (2019) ;  Peng et al. (2019)  use multiple dialogue agents, each trained for a specific dialogue domain, and combine them through a common dialogue agent.  Mehri et al. (2019)  models dialogue policy and NLG separately and fuses feature representations at different levels to generate responses. Our models simultaneously learn dialogue acts as a latent variable while allowing semantic conditioning on distributed representations of dialogue acts rather than hard discrete features. End-to-End Dialogue Systems. In this task, conventional approaches combine Natural Language Understanding (NLU), DST, Dialogue Policy, and NLG, into a pipeline architecture  (Wen et al., 2017; Bordes et al., 2016; Liu and Lane, 2017; Liu and Perez, 2017; Williams et al., 2017; Zhao et al., 2017; Jhunjhunwala et al., 2020) . Another framework does not explicitly modularize these components but incorporate them through a sequence-to-sequence framework  Lei et al., 2018; Yavuz et al., 2019)  and a memory-based entity dataset of triplets  Madotto et al., 2018; Gangi Reddy et al., 2019; Wu et al., 2019b) . These approaches bypass dialogue state and/or act modeling and aim to generate output responses directly. They achieve impressive success in generating dialogue responses in open-domain dialogues with unstructured knowledge bases. However, in a task-oriented setting with an entity dataset, they might suffer from an explosion of memory size when the number of entities from multiple dialogue domains increases. Our work is more related to the traditional pipeline strategy but we integrate our dialogue models by unifying two major components rather than using the traditional four-module architecture, to alleviate error propagation from upstream to downstream components. Different from prior work such as  (Shu et al., 2019) , our model facilitates multi-domain state tracking and allows learning dialogue acts during response generation. 

 Method The input consists of dialogue context of t?1 turns, each including a pair of user utterance U and system response R, (U 1 , R 1 ), ..., (U t?1 , R t?1 ), and the user utterance at current turn U t . A taskoriented dialogue system aims to generate the next response R t . The information for responses is typically queried from a database based on the user's provided information i.e. inform slots tracked by a DST. We assume access to a database of all domains with each column corresponding to a specific slot being tracked. We denote the intermediate output, including the dialogue state of current turn B t and dialogue act as A t . We denote the list of all domains D = (d 1 , d 2 , ...), all slots S = (s 1 , s 2 , ...), and all acts A = (a 1 , a 2 , ...). We also denote the list of all (domain, slot) pairs as DS = (ds 1 , ds 2 , ...). Note that DS ? D ? S as some slots might not be applicable in all domains. Given the current dialogue turn t, we represent each text input as a sequence of tokens, each of which is a unique token index from a vocabulary set V : dialogue context X ctx , current user utterance X utt , and target system response X res . Similarly, we also represent the list of domains as X D and the list of slots as X S . In DST, we consider the raw text form of dialogue state of the previous turn B t?1 , similarly as  (Lei et al., 2018; Budzianowski and Vuli?, 2019) . In the context-to-text setting, we assume access to the ground-truth dialogue states of current turn B t . The dialogue state of the previous and current turn can then be represented as a sequence of tokens X prev st and X curr st respectively. For a fair comparison with current approaches, during inference, we use the model predicted dialogue states Xprev st and do not use X curr st in DST and end-to-end tasks. Following  (Wen et al., 2015; , we consider the delexicalized target response X dl res by replacing tokens of slot values by their corresponding generic tokens to allow learning valueindependent parameters. Our model consists of 3 major components (See Figure  1 ). First, Encoders encode all text input into continuous representations. To make it consistent, we encode all input with the same embedding dimension. Secondly, our Bi-level State Tracker (BDST) is used to detect contextual dependencies to generate dialogue states. The DST includes 2 modules for slot-level and domain-level representation learning. Each module comprises attention layers to project domain or slot representations and incorporate important information from dialogue context, dialogue state of the previous turn, and current user utterance. The outputs are combined as a context-aware vector to decode the corresponding inform or request slots in each domain. Lastly, our Joint Dialogue Act and Response Generator (DARG) projects the target system response representations and enhances them with information from various dialogue components. Our response generator can also learn a latent representation to generate dialogue acts, which condition all target tokens during each generation step. 

 Encoders An encoder encodes a text sequence X to a sequence of continuous representation Z ? R L X ?d . L X is the length of sequence X and d is the embedding dimension. Each encoder includes a token-level embedding layer. The embedding layer is a trainable embedding matrix E ? R V ?d . Each row represents a token in the vocabulary set V as a d-dimensional vector. We denote E(X) as the embedding function that transform the sequence X by looking up the respective token index: Z emb = E(X) ? R L X ?d . We inject the positional attribute of each token as similarly adopted in  (Vaswani et al., 2017) . The positional encoding is denoted as P E. The final embedding is the element-wise summation between token-embedded representations and positional encoded representations with layer normalization  (Ba et al., 2016) : Z = LayerNorm(Z emb + P E(X)) ? R L X ?d . The encoder outputs include representations of dialogue context Z ctx , current user utterance Z utt , and target response Z dl res . We also encode the dialogue states of the previous turn and current turn and obtain Z prev st and Z curr st respectively. We encode X S and X D using only token-level embedding layer: Z S = LayerNorm(E(X S )) and Z D = LayerNorm(E(X D )). During training, we shift the target response by one position to the left side to allow auto-regressive prediction in each generation step. We share the embedding matrix E to encode all text tokens except for tokens of target responses as the delexicalized outputs contain different semantics from natural language inputs. 

 Bi-level Dialogue State Tracker (BDST) Slot-level DST. We adopt the Transformer attention  (Vaswani et al., 2017) , which consists of a dot-product attention with skip connection, to integrate dialogue contextual information into each slot representation. We denote Att(Z 1 , Z 2 ) as the attention operation from Z 2 on Z 1 . We first enable models to process all slot representations together rather than separately as in previous DST models  (Ramadan et al., 2018; Wu et al., 2019a) . This strategy allows our models to explicitly learn dependencies between all pairs of slots. Many pairs of slots could exhibit correlation such as time-wise relation ("departure_time" and "arrival_time"). We obtain Z dst SS = Att(Z S , Z S ) ? R S ?d . We incorporate the dialogue information by learning dependencies between each slot representation and each token in the dialogue history. Previous approaches such as  (Budzianowski and Vuli?, 2019 ) consider all dialogue history as a single sequence but we separate them into two inputs because the information in X utt is usually more important to generate responses while X ctx includes more background information. We then obtain Z dst S,ctx = Att(Z ctx , Z dst SS ) ? R S ?d and Z dst S,utt = Att(Z utt , Z dst S,ctx ) ? R S ?d . Following  (Lei et al., 2018) , we incorporate dialogue state of the previous turn B t?1 which is a more compact representation of dialogue context. Hence, we can replace the full dialogue context to only R t?1 as the remaining part is represented in B t?1 . This approach avoids taking in all dialogue history and is scalable as the conversation grows longer. We add the attention layer to obtain Z dst S,st = Att(Z prev st , Z dst S,ctx ) ? R S ?d (See Figure  1 ). We further improve the feature representations by repeating the attention sequence over N dst S times. We denote the final output Z dst S . Domain-level DST. We adopt a similar architecture to learn domain-level representations. The representations learned in this module exhibit global information while slot-level representations contain local dependencies to decode multi-domain dialogue states. First, we enable the domain-level DST to capture dependencies between all pairs of domains. For example, some domains such as "taxi" are typically paired with other domains such as "attraction", but usually not with the "train" domain. We then obtain Z dst DD = Att(Z D , Z D ) ? R D ?d . We then allow models to capture dependencies between each domain representation and each token in dialogue context and current user utterance. By segregating dialogue context and current utterance, our models can potentially detect changes of dialogue domains from past turns to the current turn. Especially in multi-domain dialogues, users can switch from one domain to another and the next system response should address the latest domain. We then obtain Z dst D,ctx = Att(Z ctx , Z dst DD ) ? R D ?d and Z dst D,utt = Att(Z utt , Z dst D,ctx ) ? R D ?d se- quentially. Similar to the slot-level module, we refine feature representations over N dst D times and denote the final output as Z dst D . Domain-Slot DST. We combined domain and slot representations by expanding the tensors to identical dimensions i.e. D ? S ? d. We then apply Hadamard product, resulting in domain-slot joint features Z dst DS ? R D ? S ?d . We then apply a self-attention layer to allow learning of dependencies between joint domain-slot features: Z dst = Att(Z dst DS , Z dst DS ) ? R D ? S ?d . In this attention, we mask the intermediate representations in positions of invalid domain-slot pairs. Compared to previous work such as  (Wu et al., 2019a) , we adopt a late fusion method whereby domain and slot representations are integrated in deeper layers. 

 State Generator The representations Z dst are used as context-aware representations to decode individual dialogue states. Given a domain index i and slot index j, the feature vector Z dst [i, j, :] ? R d is used to generate value of the corresponding (domain, slot) pair. The vector is used as an initial hidden state for an RNN decoder to decode an inform slot value. Given the k-th (domain, slot) pair and decoding step l, the output hidden state in each recurrent step h kl is passed through a linear transformation with softmax to obtain output distribution over vocabulary set V : P inf kl = Softmax(h kl W inf ) ? R V where W inf dst ? R drnn? V . For request slot of k-th (domain,slot) pair, we pass the corresponding vector Z dst vector through a linear layer with sigmoid activation to predict a value of 0 or 1. P req k = Sigmoid(Z dst k W req ). Optimization. The DST is optimized by the crossentropy loss functions of inform and request slots: L dst = L inf + L req = DS k=1 Y k l=1 ? log(P inf kl (y kl )) + DS k=1 ?y k log(P req k ) ? (1 ? y k )(1 ? log(P req k )) 3.3 Joint Dialogue Act and Response Generator (DARG) Database Representations. Following , we create a one-hot vector for each domain d: x d db ? {0, 1} 6 and 6 i x d db,i = 1. Each position of the vector indicates a number or a range of entities. The vectors of all domains are concatenated to create a multi-domain vector X db ? R 6? D . We embed this vector as described in Section 3.1. Response Generation. We adopt a stackedattention architecture that sequentially learns dependencies between each token in target responses with each dialogue component representation. First, we obtain Z gen res = Att(Z res , Z res ) ? R Lres?d . This attention layer can learn semantics within the target response to construct a more semantically structured sequence. We then use attention to capture dependencies in background information contained in dialogue context and user utterance. The outputs are Z gen ctx = Att(Z ctx , Z gen res ) ? R Lres?d and Z gen utt = Att(Z utt , Z gen ctx ) ? R Lres?d sequentially. To incorporate information of dialogue states and DB results, we apply attention steps to capture dependencies between each response token representation and state or DB representation. Specifically, we first obtain Z gen dst = Att(Z dst , Z gen utt ) ? R Lres?d . In the context-to-text setting, as we directly use the ground-truth dialogue states, we simply replace Z dst with Z curr st . Then we obtain Z gen db = Att(Z db , Z gen dst ) ? R Lres?d . These attention layers capture the information needed to generate tokens that are towards task completion and supplement the contextual cues obtained in previous attention layers. We let the models to progressively capture these dependencies for N gen times and denote the final output as Z gen . The final output is passed to a linear layer with softmax activation to decode system responses auto-regressively: P res = Softmax(Z gen W gen ) ? R Lres? Vres Dialogue Act Modeling. We couple response generation with dialogue act modeling by learning a latent variable Z act ? R d . We place the vector in the first position of Z res , resulting in Z res+act ? R (Lres+1)?d . We then pass this tensor to the same stacked attention layers as above. By adding the latent variable in the first position, we allow our model to semantically condition all downstream tokens from second position, i.e. all tokens in the target response, on this latent variable. The output representation of the latent vector i.e.  ) by domain first row in Z gen , incorporates contextual signals accumulated from all attention layers and is used to predict dialogue acts. We denote this representation as Z gen act and pass it through a linear layer to obtain a multi-hot encoded tensor. We apply Sigmoid on this tensor to classify each dialogue act as 0 or 1: P act = Sigmoid(Z gen act W act ) ? R A . Optimization. The response generator is jointly trained by the cross-entropy loss functions of generated responses and dialogue acts: L gen = L res + L act = Yres l=1 ? log(P res l (y l )) + A a=1 ?y a log(P act a ) ? (1 ? y a )(1 ? log(P act a )) 4 Experiments 

 Dataset We evaluate our models with the multi-domain dialogue corpus MultiWOZ 2.0  and 2.1  (Eric et al., 2019)  (The latter includes corrected state labels for the DST task). From the dialogue state annotation of the training data, we identified all possible domains and slots. We identified D = 7 domains and S = 30 slots, including 19 inform slots and 11 request slots. We also identified A = 32 acts. The corpus includes 8,438 dialogues in the training set and 1,000 in each validation and test set. We present a summary of the dataset in Table  2 . For additional information of data pre-processing procedures, domains, slots, and entity DBs, please refer to Appendix A. 

 Experiment Setup We select d = 256, h att = 8, N dst S = N dst D = N gen = 3. We employed dropout  (Srivastava et al., 2014)  of 0.3 and label smoothing  (Szegedy et al., 2016)  on target system responses during training. 

 Model Joint Acc. HJST  (Eric et al., 2019)  35.55% DST Reader  36.40% TSCP  (Lei et al., 2018)  37.12% FJST  (Eric et al., 2019)  38.00% HyST  38.10% TRADE  (Wu et al., 2019a)  45.60% NADST  (Le et al., 2020)  49.04% DSTQA  (Zhou and Small, 2019)  51.17% SOM-DST  (Kim et al., 2020)  53.01% BDST (Ours) 49.55%   (Pei et al., 2019)  75.30% 59.70% 16.81 HDSA  82.90% 68.90% 23.60 Structured Fusion  (Mehri et al., 2019)  82.70% 72.10% 16.34 LaRL  (Zhao et al., 2019)  82.78% 79.20% 12.80 GPT2  (Budzianowski and Vuli?, 2019)   We adopt a teacher-forcing training strategy by simply using the ground-truth inputs of dialogue state of the previous turn and the gold DB representations. During inference in DST and end-to-end tasks, we decode system responses sequentially turn by turn, using the previously decoded state as input in the current turn, and at each turn, using the new predicted state to query DBs. We train all networks with Adam optimizer (Kingma and  Ba, 2015)  and a decaying learning rate schedule. All models are trained up to 30 epochs and the best models are selected based on validation loss. We used a greedy approach to decode all slots and a beam search with beam size 5. To evaluate the models, we use the following metrics: Joint Accuracy and Slot Accuracy  (Henderson et al., 2014b) , Inform and Success  (Wen et al., 2017) , and BLEU score  (Papineni et al., 2002) . As suggested by  Liu et al. (2016) , human evaluation, even though popular in dialogue research, might not be necessary in tasks with domain constraints such as MultiWOZ. We implemented all models using Pytorch and will release our code on github 1 . 

 Results DST. We test our state tracker (i.e. using only L dst ) and compare the performance with the baseline models in Table  3  (Refer to Appendix B for description of DST baselines). Our model can outperform fixed-vocabulary approaches such as HJST and FJST, showing the advantage of generating unique slot values rather than relying on a slot ontology with a fixed set of candidates. DST Reader model  does not perform well and we note that many slot values are not easily expressed as a text span in source text inputs. DST approaches that separate domain and slot representations such as TRADE  (Wu et al., 2019a ) reveal 1 https://github.com/henryhungle/ UniConv competitive performance. However, our approach has better performance as we adopt a late fusion strategy to explicitly obtain more fine-grained contextual dependencies in each domain and slot representation. In this aspect, our model is related to TSCP  (Lei et al., 2018)  which decodes output state sequence auto-regressively. However, TSCP attempts to learn domain and slot dependencies implicitly and the model is limited by selecting the maximum output state length (which can vary significantly in multi-domain dialogues). Context-to-Text Generation. We compare with existing baselines in Table  4  (Refer to Appendix B for description of the baseline models). Our model achieves very competitive Inform, Success, and BLEU scores. Compared to TokenMOE  (Pei et al., 2019) , our single model can outperform multiple domain-specific dialogue agents as each attention module can sufficiently learn contextual features of multiple domains. Compared to HDSA  which uses a graph structure to represent acts, our approach is simpler yet able to outperform HDSA in Inform score. Our work is related to Structured Fusion  (Mehri et al., 2019)  as we incorporate intermediate representations during decoding. However, our approach does not rely on pretraining individual sub-modules but simultaneously learning both act representations and predicting output tokens. Similarly, our stacked attention architecture can achieve good performance in BLEU score, competitively with a GPT-2 based model  (Budzianowski and Vuli?, 2019) , while consistently improve other metrics. For completion, we tested our models on MultiWOZ2.1 and achieved similar results: 87.90% Inform, 72.70% Success, and 18.52 BLEU score. Future work may further improve Success by optimizing the models towards a higher success rate using strategies such as LaRL  (Zhao et al., 2019) . Another direction is a data augmentation approach such as DAMD  (Zhang et al., Model  Joint Acc Slot Acc Inform Success BLEU TSCP (L=8)  (Lei et al., 2018)  31.64% 95.53% 45.31% 38.12% 11.63 TSCP (L=20)  (Lei et al., 2018)  37 2019) which achieves significant performance gain in this task. End-to-End. From Table  5 , our model outperforms existing baselines in all metrics except the Inform score (See Appendix B for a description of baseline models). In TSCP  (Lei et al., 2018) , increasing the maximum dialogue state span L from 8 to 20 tokens helps to improve the DST performance, but also increases the training time significantly. Compared with HRED-TS  (Peng et al., 2019) , our single model generates better responses in all domains without relying on multiple domainspecific teacher models. We also noted that the performance of DST improves in contrast to the previous DST task. This can be explained as additional supervision from system responses not only contributes to learn a natural response but also positively impact the DST component. Other baseline models such as  Wu et al., 2019b)  present challenges in the MultiWOZ benchmark as the models could not fully optimize due to the large scale entity memory. For example, following GLMP  (Wu et al., 2019b) , the restaurant domain alone has over 1,000 memory tuples of (Subject, Relation, Object). Ablation. We conduct a comprehensive ablation analysis with several model variants in Table  6  and have the following observations: ? The model variant with a single-level DST (by considering S = DS and N dst D = 0) (Row A2) performs worse than the Bi-level DST (Row A1). In addition, using the dual architecture also improves the latency in each attention layers as typically D + S DS . The performance gap also indicates the potential of separating global and local dialogue state dependencies by domain and slot level. ? Using B t?1 and only the last user utterance as the dialogue context (Row A1 and B1) performs as well as using B t?1 and a full-length dialogue history  (Row A5 and B3) . This demonstrates that the information from the last dialogue state is sufficient to represent the dialogue history up to the last user utterance. One benefit from not using the full dialogue history is that it reduces the memory cost as the number of tokens in a full-length dialogue history is much larger than that of a dialogue state (particularly as the conversation evolves over many turns). ? We note that removing the loss function to learn the dialogue act latent variable (Row B2) can hurt the generation performance, especially by the task completion metrics Inform and Success. This is interesting as we expect dialogue acts affect the general semantics of output sentences, indicated by BLEU score, rather than the model ability to retrieve correct entities. This reveals the benefit of our approach. By enforcing a semantic condition on each token of the target response, the model can facility the dialogue flow towards successful task completion. ? In both state tracker and response generator modules, we note that learning feature representations through deeper attention networks can improve the quality of predicted states and system responses. This is consistent with our DST performance as compared to baseline models of shallow networks. ? Lastly, in the end-to-end task, our model achieves better performance as the number of attention heads increases, by learning more high-resolution dependencies. 

 Domain-dependent Results DST. For state tracking, the metrics are calculated for domain-specific slots of the corresponding domain at each dialogue turn. We also report the DST separately for multi-domain and single-domain dialogues to evaluate the challenges in multi-domain dialogues and our DST performance gap as compared to single-domain dialogues. From our DST performs consistently well in the 3 domains attraction, restaurant, and train domains. However, the performance drops in the taxi and hotel domain, significantly in the taxi domain. We note that dialogues with the taxi domain is usually not single-domain but typically entangled with other domains. Secondly, we observe that there is a significant performance gap of about 10 points absolute score between DST performances in singledomain and multi-domain dialogues. State tracking in multi-domain dialogues is, hence, could be further improved to boost the overall performance.  Additionally, we report qualitative analysis and the insights can be seen in Appendix C. 

 Conclusion We proposed UniConv, a novel unified neural architecture of conversational agents for Multi-domain Task-oriented Dialogues, which jointly trains (1) a Bi-level State Tracker to capture dependencies in both domain and slot levels simultaneously, and (2) a Joint Dialogue Act and Response Generator to model dialogue act latent variable and semantically conditions output responses with contextual cues. The promising performance of UniConv on the MultiWOZ benchmark (including three tasks: DST, context-to-text generation, and end-to-end dialogues) validates the efficacy of our method. A Data Pre-processing First, we delexicalize each target system response sequence by replacing the matched entity attribute that appears in the sequence to the canonical tag domain_slot . For example, the original target response 'the train id is tr8259 departing from cambridge' is delexicalized into 'the train id is train_id departing from train_departure'. We use the provided entity databases (DBs) to match potential attributes in all target system responses. To construct dialogue history, we keep the original version of all text, including system responses of previous turns, rather than the delexicalized form. We split all sequences of dialogue history, user utterances of the current turn, dialogue states, and delexicalized target responses, into case-insensitive tokens. We share the embedding weights of all source sequences, including dialogue history, user utterance, and dialogue states, but use a separate embedding matrix to encode the target system responses. We summarize the number of dialogues in each domain in Table  2 . For each domain, a dialogue is selected as long as the whole dialogue (i.e. singledomain dialogue) or parts of the dialogue (i.e. in multi-domain dialogue) is involved with the domain. For each domain, we also build a set of possible inform and request slots using the dialogue state annotation in the training data. The details of slots and database in each domain can be seen in Table  9 . The DBs of 3 domains taxi, police, and hospital are not available as part of the benchmark. On average, each dialogue has 1.8 domains and extends over 13 turns. 
