title
Structured Attention for Unsupervised Dialogue Structure Induction

abstract
Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation. 1

Introduction Grammatical induction for capturing a structural representation of knowledge has been studied for some time  (De la Higuera, 2010) . Given the achievement in related areas like learning Hidden Markov acoustic models in speech recognition  (Bahl et al., 1986)  and sentence dependency parsing in language understanding  (Covington, 2001) , our work aims to explore a more sophisticated topic: learning structures in dialogues. Figure  1  shows the underlying semantic structure of conversations about bus information request from Sim-Dial dataset  (Zhao and Eskenazi, 2018) , with one example dialogue as shown in Table  1 . Another interesting type of dialogue structure is the interactive structure in multi-party dialogues. Figure  2  illustrates the interactive structure we learned from a dialogue sample in Ubuntu Chat Corpus  (Lowe et al., 2015) . Each node represents an utterance from different speakers in the dialogue with darker linkages represent stronger dependency relations between utterances. When speaker/addressee information is unavailable in the corpus, learning such a structure allows disentangling the conversation  and estimating the speaker labels. Discovering dialogue structures is crucial for various areas in computational linguistics, such as dialogue system building  (Young, 2006) , discourse analysis  (Grosz and Sidner, 1986) , and dialogue summarization  (Murray et al., 2005; Liu et al., 2010) . Through looking into this topic, we can further improve the capability of machines to learn more generalized, interpretable knowledge representation from data.   (Zhao and Eskenazi, 2018) . User intents are marked in bold. However, capturing structure from the conversation is still much under-explored. The complexity of dialogues could range from several-round task-oriented dialogues to tens-round multi-party chitchat. It is unclear that for these different categories of dialogues, what types of inductive biases or constraints we could add to reduce the search space. It also remains an unsolved question for formally evaluating the performance of dialogue structure induction algorithms. In this paper, we propose to use a combination of structured attention and unsupervised generative model to infer the latent structure in a dialogue. Specifically, instead of simply applying a softmax function on potentials between a decoder query and encoder hidden states, dynamic programming algorithms like Forward-Backward  (Devijver, 1985)  and Inside-Outside  (Lari and Young, 1990 ) could be used to efficiently calculate marginal probabilities from pairwise potentials with a structural constraint. Through embedding such structured attention layers in a Variational Recurrent Neural Network (VRNN) model, we can learn latent structures in dialogues by jointly re-generating training dialogues. Such a process requires no human annotation and is useful for dialogue analysis. In addition, by selecting appropriate structural biases or constraints, we can learn not only semantic structures but also interactive structures. A linear Conditional Random Field (CRF) attention layer is used in two-party dialogues to discover semantic structures. A non-projective dependency tree attention layer is embedded to learn an interactive structure that could help identify speaker/addressee information in multi-party dialogues that have tangled conversation threads, such as forum discussions. This paper makes the following contributions. We propose to incorporate a structured attention layer in VRNN to learn latent structures in dialogues. To our knowledge, no work connecting structured attention with unsupervised dialogue structure learning has been done. We prove our proposed VRNN-LinearCRF learns better structures than the baseline VRNN on the SimDial dataset for semantic structure learning in two-party dialogues. For interactive structure learning in multi-party dialogues, we combine VRNN with a non-projective dependency tree attention layer. It achieves similar generation performance as the baseline GSN model  (Hu et al., 2019)  on Ubuntu Chat Corpus  (Uthus and Aha, 2013; Lowe et al., 2015) , while our model can identify the speaker/addressee information without trained on explicit labels. We release our code as well as the processed datasets to help stimulate related researches. 

 Related Work Attention mechanism  (Vaswani et al., 2017)  has been widely adopted as a way for embedding categorical inference in neural networks for performance gain and interpretability  (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) . However, for many tasks, we want to model richer structural dependencies without abandoning end-to-end training. Structured Attention Networks  (Kim et al., 2017)  can extend attention beyond the standard soft-selection approach by attending to partial segments or subtrees. People have proven its effectiveness on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference  (Rush, 2020) . In this paper, we propose to utilize structured attention to explore dialogue structures. Specifically, we work on two types of dialogue structures, semantic structures (dialogue intent transitions), and interactive structures (addressee/speaker changes). Semantic structures have been studied extensively. Some previous works, such as (Jurafsky, 1997), learned semantic structures relying on human annotations, while such annotations are costly and can vary in quality. Other unsupervised studies used Hidden Markov Model (HMM)  (Chotimongkol, 2008; Ritter et al., 2010; Zhai and Williams, 2014) . Recently, Variational Autoencoders (VAEs)  (Kingma and Welling, 2013)  and their recurrent version, Variational Recurrent Neural Networks (VRNNs)  (Chung et al., 2015) , connects neural networks and traditional Bayes methods. Because VRNNs apply a point-wise nonlinearity to the output at every timestamp, they are also more suitable to model highly non-linear dynamics over the simpler dynamic Bayesian network models.  Serban et al. (2017)  proposed the VHRED model by combining the idea of VRNNs and Hierarchical Recurrent Encoder-Decoder (HRED)  (Sordoni et al., 2015)  for dialogue generation. Similarly,  Zhao et al. (2018)  proposed to use VAEs to learn discrete sentence representations.  Shi et al. (2019)  used two variants of VRNNs to learn the dialogue semantic structures and discussed how to use learned structure to improve reinforcement learning-based dialogue systems. But none of the previous work has tried to incorporate structured attention in VRNNs to learn dialogue structure. Compared to semantic structures, the interactive structure of dialogues is not clearly defined.  Elsner and Charniak (2008)  initiated some work about dialogue disentanglement, which is defined as dividing a transcript into a set of distinct conversations. Serban and Pineau (2015) tested standard RNN and its conditional variant for turn taking and speaker identification. Both of the tasks are highly related to understanding the interactive structure but not identical. Our task, different from both of them, aims to construct an utterance dependency tree to represent a multi-party dialogue's turn taking. The tree can not only be used to disentangle the conversations but also label each utterance's speakers and addressees. We compare our model with Graph Structured Network (GSN), recently proposed by  Hu et al. (2019) . GSN builds a conversation graph utilizing explicit speaker/addressee information in Ubuntu Chat Corpus (Uthus and Aha, 2013) to improve the dialogue generation performance. Our model shows similar generation performance as them while demonstrating its capability of learning the utterance dependency tree. 

 Problem Formulations We discuss the semantic and interactive dialogue structure learning separately. In task-oriented twoparty dialogues (between system and user), we want to discover a semantic probabilistic grammar shared by dialogues in the same domain. While for multi-party dialogues, e.g., conversations in a chatroom, which may have multiple conversations occur simultaneously, we are more interested in finding an interactive structure that could help disentangle the conversation and identify the speakers/addressees. Our method of structure learning is flexible to handle both problems with the formula-tions as shown below. For semantic dialogue structure learning, we formulate the problem as labeling the dialogue with a sequence of latent states. Each conversational exchange x i (a pair of system and user utterances at time step i) belongs to a latent state z i , which has an effect on the future latent states and the words the interlocutors produce. The latent dialogue state is defined to be discrete, i.e., z i ? {1, 2, ..., N }, where N is the number of states predefined from experience. Our goal is to generate the current sentence pair x i that maximizes the conditional likelihood of x i given the dialogue history while jointly learning a latent state sequence z = [z 1 , z 2 , ..., z n ]: x = argmax x |x| i=1 log(P (z <i |x <i )P (x i |z <i )). (1) Then, we can induce a probabilistic dialogue grammar by estimating the state transition probabilities through maximizing the likelihood of the parsed latent state sequences. A multi-party dialogue session can be formulated as an utterance-level dependency tree T(V, E), where V is the set of nodes encoding the utterances, E = {e i,j } m i<j ? {0, 1} indicates whether utterance i is the parent of utterance j, and m is the maximum number of possible edges. x = argmax x |x| i=1 log(P (T|x <i )P (x i |T)) = argmax x |x| i=1 log( i?1 j<k P (e j,k = 1|x <i )? P (x i |T)) = argmax x |x| i=1 i?1 j<k log(P (e j,k = 1|x <i ) + |x| i=1 log(P (x i |T) (2) Each path of the dependency tree represents a thread in the multi-party conversation in chronological order. Our goal is to generate the response x that maximizes the conditional likelihood of the response given the dialogue history while jointly learning a latent utterance dependency tree as shown in Equation  2 . The conditional likelihood is factorized into two parts, representing the encod- ing and decoding processes respectively. We can further reason about the speaker/addressee labels or disentangle the conversation by clustering the utterances from the learned tree. 

 Variational Recurrent Neural Network with Structured Attention The overall architecture of Structured-Attention Variational Recurrent Neural Network (SVRNN) is illustrated in Figure  3 . The LSTM  (Hochreiter and Schmidhuber, 2001)  word-level encoder marked in pink encodes each utterance into a sentence embedding. Then an utterance-level encoder VRNN with different structured attention layers encodes the dialogue history into a latent state z. A decoder marked in blue will decode the next utterances from the latent state. We describe more details about the key components of our model in the following subsections. 

 Variational Recurrent Neural Network The pursuit of using an autoencoder like Variational Recurrent Neural Network (VRNN) is to compress the essential information of the dialogue history into a lower-dimensional latent code. The latent code z is a random vector sampled from a prior p(z) and the data generation model is described by p(x|z). The VRNN contains a Variational Autoencoder (VAE) at each time step. The VAE consists of an encoder q ? (z|x) for approximating the posterior p(z|x), and a decoder p ? (x|z) for representing the distribution p(x|z). The variational inference attains its maximum likelihood by maximizing evidence lower bound (ELBO): E [log p ? (x|z)] ? KL (q ? (z|x) p(z)) ? log p(x). (3) For sequential data, the parameterization of the generative model is factorized by the posterior p (z t |x <t , z <t ) and the generative model p (x t |z ?t , x <t ), i.e., p(x ? T, z ? T ) = T t=1 p (x t |z ?t , x <t ) ? p (z t |x <t , z <t ) . (4) The learning objective function becomes maximizing the ELBO for all time steps E T t=1 ( ? KL (q(z t |x ?t , z <t ) p(z t |x <t , z <t )) + log p(x t |z ?t , x <t )) . (5) In addition, to mitigate the vanishing latent variable problem in VAE, we incorporate Bag-of-Words (BOW) loss and Batch Prior Regularization (BPR)  (Zhao et al., 2017)  with a tunable weight ?. By adjusting the ?, the VRNN based models can achieve a balance between clustering the utterance surface formats and attention on the context. 

 Linear CRF Attention As we formulate the semantic structure learning in two-party dialogues as a state tagging problem, we find it suitable to use a linear-chain Conditional Random Field (CRF) attention layer with VRNN. Define ? to be a random vector ? = [? 1 , ..., ? n ] with ? i ? {0, 1}. n is the number of utterances in a dialogue. The context vector c j given the current sentence hidden state h j and hidden state history h can thus be written as: c j = j?1 i=1 p(? i = 1|h, h j )h i . (6) We model the distribution over the latent variable ? with a linear-chain CRF with pairwise edges, p(? 1 , ..., ? n |h, h j ) = sof tmax( j?2 i=1 ? i,i+1 (? i , ? i+1 )), (7) where ? i,i+1 (k, l) is the pairwise potential for ? i = k and ? i+1 = l. The attention layer is a two-state CRF where the unary potentials at the j-th dialogue turn are: ? i (k) = h i W 1 h j , k = 0 h i W 2 h j , k = 1 , (8) where [h 1 , ..., h n ] are utterance level hidden states and W 1 , W 2 are parameters. The pairwise potentials can be parameterized as ? i,i+1 (? i , ? i+1 ) = ? i (? i ) + ? i+1 (? i+1 ) + h i h i+1 . (9) The marginal distribution p(? i = 1|x) can be calculated efficiently in linear-time for all i using message-passing, i.e., the forward-backward shown in Algorithm 1. C denotes the state space and t is the special start/stop state. Typically the forward-backward with marginals is performed in the log-space semifield R ? {?} with binary operations ? = logadd and ? = + for numerical precision. These marginals allow us to calculate the context vector. Crucially, the process from vector softmax to forward-backward algorithm is a series of differentiable steps, and we can compute the gradient of the marginals with respect to the potentials  (Kim et al., 2017) . This allows the linear CRF attention layer to be trained end-to-end as a part of the VRNN. Algorithm 1: Forward-Backward for Lin-earCRF Attention Input: potential ? ?[0, t ] ? 0 ?[n + 1, t ] ? 0 for i = 1, ..., n; c ? C do ?[i, c] ? y ?[i ? 1, y] ? ? i?1,i [y, c] end for for i = n, ..., 1; c ? C do ?[i, c] ? y ?[i + 1, y] ? ? i,i+1 [c, y] end for A ? ?[n + 1, t ] for i = 1, ..., n; c ? C do p(? i = c|x) ? exp(?[i, c] ? ?[i, c] ? ?A) end for return p 

 Non-projective Dependency Tree Attention For interactive structure learning in multi-party dialogues, we want to learn an utterance dependency tree from each dialogue. Therefore, we propose to use a non-projective dependency tree attention layer with VRNN for this purpose. The potentials ? i,j , which reflect the score of selecting the i-th sentence being the parent of the j-th sentence (i.e., x i ? x j ), can be calculated by ? i,j = tanh(s tanh (W 1 h i + W 2 h j + b)), (10) where s, b, W 1 , W 2 are parameters, h i , h j are sentence hidden states. The probability of a parse tree ? given the dialogue  (11)  where the latent variable ? i,j ? {0, 1} for all i = j indicates that the i-th sentence is the parent of the j-th sentence; and 1{? is valid} is a special global constraint that rules out configurations of ? i,j 's that violate parsing constraints. In our case, we specify each sentence has one parent and that must precede the child sentence, i.e, n i=1 ? i,j = 1 ? i,j = 0(i ? j). x = [x 1 , ..., x n ] is, p(?|x) = softmax(1{? is valid}? i =j 1{? i,j = 1}? i,j ), ( ) 12 It is possible to calculate the marginal probability of each edge p(? i,j = 1|x) for all i, j in O(n 3 ) time using the inside-outside algorithm with details explained in Appendix, which is a generalization of the forward-backward algorithm. Then the soft-parent or the context vector of the j-th sentence is calculated using parsing marginals, i.e., c j = n i=1 p(? i,j = 1|h, h j )h i . ( 13 ) The original embedding is concatenated with its context vector to form the new representation ?j = [h j ; c j ]. ( 14 ) 

 Decoder In order to generate a response to an utterance i, the decoder calculates a distribution over the vocabulary then sequentially predicts word w k using a softmax function: p(w| ?) = |w| k=1 P (w k | ?, w <k ) = |w| k=1 sof tmax(M LP (h dec k , c dec k )) h dec 0 = ?i h dec k = LST M (h dec k?1 , M LP (e w k?1 ; c dec k?1 )) c dec k = i j=1 sof tmax(h dec k W a ?j ) ?j , ( 15 ) where ?i is the hidden state for utterance i with structured attention, h dec k is the hidden state of the decoder LSTM, e w k?1 is the embedding of the predicted word at decoding time stamp (k ? 1), and c dec k is the attention-based context vector at decoding time stamp k. Note that the context vector here is calculated with the simple attention different from the structured attention we described before. W a is a matrix to learn the match degree of h dec k and ?j . 

 Experiments We incorporate structured attention in VRNNs to explore two types of dialogue structure, semantic structure, and interactive structure. 

 Semantic Structure Learning in Two-party Dialogues 

 Datasets We test the VRNN with Linear CRF Attention on the SimDial dataset  (Zhao and Eskenazi, 2018)     (Zhao and Eskenazi, 2018) . of simulated conversations. Dialogues are generated for information requests in four domains: bus, restaurant, weather, and movie. Table  1  shows an example dialogue in bus schedule request domain. Despite significant variations exist between dialogues of the same domain, we aim to explore a shared semantic structure among each dialogue domain. We validate our algorithm on this simulated dataset because these dialogues are generated using pre-defined templates that make recovering ground truth structures much easier. One recovered ground truth structure with transition probabilities is shown in Figure  1 . We have 800 dialogue samples for training, 100 for validation, and 100 for testing in each dialog domain. The length of the dialogues ranges from 6 to 13 utterances. The maximum length of an utterance is 33 words. 

 Evaluation Metrics Since the number of states is unknown during unsupervised training, we set the state number empirically to 10. Then the learned structure is essentially a state transition matrix of size 10 ? 10. However, the original structure could be another state transition matrix of any size depending on the domain complexity. This makes the model evaluation on the ground truth a problem because it requires us to measure the difference between two state transition matrices of different sizes. To alleviate this problem, we define two metrics: Structure Euclidean Distance (SED) and Structure Cross-Entropy (SCE). We first estimate a probabilistic mapping P s i ,s i between the learned states {s i , i = 1, 2, ..., M } and the true states {s i , i = 1, 2, ..., N }, through dividing the number of utterances that have the ground truth state s i and learned state s i by number of utterances with the ground truth state s i . And we let the reversed mapping probability P s i ,s i be the normalized transpose of P s i ,s i . Then SED and SCE are defined as: T sa,s b = i,j?{1,2,...,M } P sa,s i ? T s i ,s j ? P s j ,s b SED = 1 N a,b?{1,2,...,N } (T sa,s b ? T sas b ) 2 SCE = 1 N a,b?{1,2,...,N } ? log(T sa,s b )T sas b , (16) where T sa,s b is the learned transition probability from state s a to state s b and T sa,s b is the true transition probability. 

 Results and Analysis We compare the proposed VRNN-LinearCRF against other unsupervised methods: K-means clustering, Hidden Markov Model, D-VRNN  (Shi et al., 2019)  and VRNN with vanilla attention. D-VRNN is similar to our work but without structured attention. We use a bidirectional LSTM with 300 hidden units as the sentence encoder and a forward LSTM for decoding. 300-dimensional word embeddings are initialized with GloVe word embedding  (Pennington et al., 2014) . A dropout rate of 0.5 is adopted during training. We set the BOWloss weight ? to be 0.5. The whole network is trained with the Adam optimizer with a learning rate of 0.001 on GTX Titan X GPUs for 60 epochs. The training takes on average 11.2 hours to finish.   (Zhao and Eskenazi, 2018) . User intents are marked in bold. Transitions with P < 0.1 are omitted. To evaluate the learned structure, we compare VRNN-LinearCRF's output in Figure  4  with the ground truth dialogue structure in Figure  1 . A dialogue structure learned by VRNN without structured attention is also shown in the Appendix. We find our method generates similar structure compared to ground truth in the bus domain. Figure  5  shows all models' quantitative results. Having a lower value in SED and SCE indicates the learned structure is closer to the ground truth and better. Our method with BERT, VRNN-LinearCRF-BERT performs the best. K-means clustering performs worse than VRNN-based models because it only considers utterances' surface format and ignores the context information. Hidden Markov Model is similar to VRNN but lacks a continuous propagating hidden state layer. VRNN-LinearCRF observes the entire history of latent states but ignores the redundant transitions due to the structure attention. The model's performance further improves when replacing the vanilla LSTM encoder with a large scale pre-trained encoder like BERT  (Devlin et al., 2019) , as BERT provides better representations.  

 Interactive Structure Learning in Multi-party Dialogues We extend our method to learn interactive structure in multi-party dialogues. Specifically, we detect each utterance's speaker and addressee by constructing an utterance dependency tree. 

 Datasets We it provides the ground-truth of speaker/addressee information for evaluation. Though every record of Ubuntu Chat Corpus contains clear speaker ID, only part of the data has implicit addressee ID, coming as the first word in the utterance. We select addressee ID that appeared in a limited context and extract dialogue sessions with all utterances having verified speaker ID and addressee ID. We extract 20k dialogues with length ranging from 7 to 8 turns. Table  3  shows an example dialogue. 

 Results and Analysis Considering Ubuntu Chat Corpus have a large number of technical terminologies, we use a relatively larger vocabulary size of 30k. We use LSTMs and BERT as the sentence embedding encoder and two GRU  (Chung et al., 2014)  layers with 300 hidden units each as the decoder. The model converges after 100 epochs on GTX Titan X GPUs. The training procedure takes about 54 hours. To evaluate the learned utterance dependency tree, we compare it with the annotated speakeraddressee relation and find 68.5% utterances are assigned the correct parents. This is a reasonable number because the dependency relationship does not fully rely on the speaker/addressee informa-tion in a chatroom. A different interlocutor could answer others' questions even when the questions were not addressed to him/her. Figure  2  visualizes the learned interactive structure from the example in Table  3 . Specifically, utterance 4 largely depends on utterance 3, while utterance 6 and 7 are answering the question from utterance 5. We also compare the model's generation performance with Hierarchical Recurrent Encoder-Decoder (HRED) and Graph-Structured Network (GSN)  (Hu et al., 2019) . The GSN model uses the annotated speaker/addressee information to construct a dialogue graph for utterance encoding iteration. However, this is not required by our VRNN-Dependency-Tree since we generate the original dialogues while learning a dependency structure. For consistent comparison with previous work, we evaluate all models with BLEU 1 to 4, METEOR, and ROUGE L with the package in  (Chen et al., 2015) . All results are shown in Table  2 . We observe that the proposed VRNN-Dependency-Tree model without using any speaker annotation achieves similar generation performance compared to the state-ofthe-art method, GSN with speaker annotation. 

 Conclusion This paper proposed to inject structured attention into variational recurrent neural network models for unsupervised dialogue structure learning. We explored two different structure inductive biases: linear CRF for utterance-level semantic structure induction in two-party dialogues; and non-projective dependency tree for interactive structure learning in multi-party dialogues. Both models are proved to have a better structure learning performance over the state of the art algorithms. In the future, we will further explore how to explicitly incorporate linguistics information, such as named entities into the latent states.  A.2 Inside-Outside Algorithm Figure 2 : 2 Figure 2: Learned interactive structure from a multiparty dialogue sample in Ubuntu Chat Corpus (Uthus and Aha, 2013). 
