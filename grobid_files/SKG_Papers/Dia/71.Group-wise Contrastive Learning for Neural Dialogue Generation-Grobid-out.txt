title
Group-wise Contrastive Learning for Neural Dialogue Generation

abstract
Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the lowdiversity issue when it comes to the opendomain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multimapping relations prevalent in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches.

Introduction Open-domain human-machine dialogue systems, especially the generation-based genre, have attracted extensive attention recently. Typically, following the neural encoder-decoder paradigm, contemporary dialogue generation models  (Shang et al., 2015; Yan, 2018; Huang et al., 2020; Liu et al., 2020) , * Work done at JD.com. more often than not, are trained with Maximum Likelihood Estimation (MLE) principle to mimic human context-response pairs in the training corpus. While notable gains have been achieved under this learning framework, prior art  (Li et al., 2016a Zhang et al., 2018a)  suggests that naive MLE objective used for training neural dialogue generation models is not that effective enough and tends to result in issues like dull response generation. By optimizing the likelihood of training dialogues, neural models are inclined to assign high probabilities to "safe" responses, due to the fact that vacuous responses like "I don't know" are of relatively high frequencies in conversational datasets  (Li et al., 2016a) . One promising training framework for neural dialogue generation is adversarial learning  (Goodfellow et al., 2014; , where a discriminator provides rewards for the generator by contrastively distinguishing dialogues as humangenerated or machine-generated. However, the learning ability of GANs in text is drastically limited due to training instability and model collapse  (Nie et al., 2019; Caccia et al., 2020) . First, the discriminator is usually unlikely to be fooled very easily, and the generator can hardly learn from those ineffective rewards. Second, the generator is sometimes encouraged to mimic the highfrequency generic responses in the training corpus, because in some cases, the discriminator fails to distinguish a good response from a bad one: it can easily recognize contentful but less-grammatical responses as machine-generated, yet treat those human-generated dull responses as the oracle. In this paper, we introduce contrastive learning  (Hadsell et al., 2006; Gutmann and Hyv?rinen, 2012)  into dialogue generation, where the model explicitly perceives the difference between the wellchosen positive and negative utterances. From the perspective of contrastive learning, the discrim-  q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W < / l a t e x i t > Reading is my favorite hobby. A g iv e n t r a in in g p a ir r + < l a t e x i t s h a 1 _ b a s e 6 4 = " 7 q h K 8 y s r E Y p 5 6 i 4 P 3 a inator in adversarial learning considers humangenerated responses as positive utterances and synthetic ones as negative samples. Instead, this work deems highly-matched context-response pairs as positive samples and mismatched training pairs as negative samples. In particular, we utilize a pretrained baseline model as a reference. During contrastive learning, for context c and its response r, the target dialogue model is trained to give higher conditional probabilities p(r|c) for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. This training paradigm encourages the model to pull the positive data points together and push apart the negative samples, as exemplified in Figure  1 . As a result, our proposed training scheme explicitly takes the semantic associations and differences among training examples into account for dialogue modeling. Besides, by contrastively characterizing the distinctions relative to a strong reference, our method implicitly enhances the distinctiveness of the generated responses as well, and ensures that the overall performance of the target model is not inferior to the reference. V i Z b H M 5 O k = " > A A A B / 3 i c b V C 7 S g N B F L 0 b X z G + o p Y 2 g 0 E Q h L A b B S 2 D N p Y R z A O S N c x O Z p M h M 7 P L z K w Q l h T + g q 3 2 d m L r p 9 j 6 J U 4 2 W 2 j i g Q u H c + 7 l X E 4 Q c 6 a N 6 3 4 5 h Z X V t f W N 4 m Z p a 3 t n d 6 + 8 f 9 D S U a I I b Z K I R 6 o T Y E 0 5 k 7 R p m O G 0 E y u K R c B p O x j f z P z 2 I 1 W a R f L e T G L q C z y U L G Q E G y t 1 e o F I 1 c P Z t F + u u F U 3 A 1 o m X k 4 q k K P R L 3 / 3 B h F J B J W G c K x 1 1 3 N j 4 6 d Y G U Y 4 n Z Z 6 i a Y x J m M 8 p F 1 L J R Z U + 2 n 2 7 x S d W G W A w k j Z k Q Z l 6 u + L F A u t J y K w m w K b k V 7 0 Z u K / X i A W k k 1 4 5 a d M x o m h k s y D w 4 Q j E 6 F Z G W j A F C W G T y z B R D H 7 O y I j r D A x t r K S L c V b r G C Z t G p V 7 7 x a u 7 u o 1 K / z e o p w B M d w C h 5 c Q h 1 u o Q F N I M D h G V 7 g 1 X l y 3 p x 3 5 2 O + W n D y m 0 P 4 A + f z B 1 v x l p I = < / l a t e x i t > r - < l a t e x i t s h a 1 _ b a s e 6 4 = " Y G h G 3 k A 1 s z + 9 3 B P K C x v s P / i h B J o = " > A A A C C H i c b V D L S s N A F J 3 U V 6 2 P R l 2 6 G S y C G 0 t S B V 0 W 3 b i s Y B / Q x j K Z T t q h M 0 m Y u R F L y A / 4 C 2 5 1 7 0 7 c + h d u / R K n b R b a e u D C 4 Z x 7 O Z f j x 4 J r c J w v q 7 C y u r a + U d w s b W 3 v 7 J b t v f 2 W j h J F W Z N G I l I d n 2 g m e M i a w E G w T q w Y k b 5 g b X 9 8 P f X b D 0 x p H o V 3 M I m Z J 8 k w 5 A G n B I z U t 8 s 9 X 6 Y q u + 8 B e 4 T 0 N O v b F a f q z I C X i Z u T C s r R 6 N v f v U F E E 8 l C o I J o 3 X W d G L y U K O B U s K z U S z S L C R 2 T I e s a G h L J t J f O H s / w s V E G O I i U m R D w T P 1 9 k R K p 9 U T 6 Z l M S G O l F b y r + 6 / l y I R m C S y / l Y Z w A C + k 8 O E g E h g h P W 8 E D r h g F M T G E U M X N 7 5 i O i C I U T H c l U 4 q 7 W M E y a d W q 7 l m 1 d n t e q V / l 9 R T R I T p C J 8 h F F 6 i O b l A D N R F F C X p G L + j V e r L e r H f r Y 7 5 a s P K b A / Q H 1 u c P 4 G 6 a J A = = < / l a t e x i t > c + < l a t e x i t s h a 1 _ b a s e 6 4 = " O r O g V i w M 3 q S R R q N v y p Z D L t n D 6 f c = " > A A A C C H i c b V D L S s N A F J 3 U V 6 2 P R l 2 6 G S y C I J S k C r o s u n F Z w T 6 g j W U y n b R D Z 5 I w c y O W k B / w F 9 z q 3 p 2 4 9 S / c + i V O 2 y y 0 9 c C F w z n 3 c i 7 H j w X X 4 D h f V m F l d W 1 9 o 7 h Z 2 t r e 2 S 3 b e / s t H S W K s i a N R K Q 6 P t F M 8 J A 1 g Y N g n V g x I n 3 B 2 v 7 4 e u q 3 H 5 j S P A r v Y B I z T 5 J h y A N O C R i p b 5 d 7 v k x p d t 8 D 9 g j p a d a 3 K 0 7 V m Q E v E z c n F Z S j 0 b e / e 4 O I J p K F Q A X R u u s 6 M X g p U c C p Y F m p l 2 g W E z o m Q 9 Y 1 N C S S a S + d P Z 7 h Y 6 M M c B A p M y H g m f r 7 I i V S 6 4 n 0 z a Y k M N K L 3 l T 8 1 / P l Q j I E l 1 7 K w z g B F t J 5 c J A I D B G e t o I H X D E K Y m I I o Y q b 3 z E d E U U o m O 5 K p h R 3 s Y J l 0 q p V 3 b N q 7 f a 8 U r / K 6 y m i Q 3 S E T p C L L l A d 3 a A G a i K K E v S M X t C r 9 W S 9 W e / W x 3 y 1 Y O U 3 B + g P r M 8 f x Q K a E w = = < / l a t e x i t > c - < l a t e x i t s h a 1 _ b a s e 6 4 = " M g S C y 0 o P v F 9 9 0 Y h k T e 3 L I k 0 r 8 B w = " > A A A C C H i c b V D L S s N A F J 3 U V 6 2 P R l 2 6 G S y C G 0 t S B V 0 W 3 b i s Y B / Q x j K Z T t q h M 0 m Y u R F L y A / 4 C 2 5 1 7 0 7 c + h d u / R K n b R b a e u D C 4 Z x 7 O Z f j x 4 J r c J w v q 7 C y u r a + U d w s b W 3 v 7 J b t v f 2 W j h J F W Z N G I l I d n 2 g m e M i a w E G w T q w Y k b 5 g b X 9 8 P f X b D 0 x p H o V 3 M I m Z J 8 k w 5 A G n B I z U t 8 s 9 X 6 Y 0 u + 8 B e 4 T 0 N O v b F a f q z I C X i Z u T C s r R 6 N v f v U F E E 8 l C o I J o 3 X W d G L y U K O B U s K z U S z S L C R 2 T I e s a G h L J t J f O H s / w s V E G O I i U m R D w T P 1 9 k R K p 9 U T 6 Z l M S G O l F b y r + 6 / l y I R m C S y / l Y Z w A C + k 8 O E g E h g h P W 8 E D r h g F M T G E U M X N 7 5 i O i C I U T H c l U 4 q 7 W M E y a d W q 7 l m 1 d n t e q V / l 9 R T R I T p C J 8 h F F 6 i O b l A D N R F F C X p G L + j V e r L e Contrastively learning from one pair of positive and negative samples is quite straightforward, however, multi-mapping relations prevail in humanhuman conversations, where there exist multiple appropriate responses for a given context, and a response sometimes fits well to several contexts, known as one-to-many and many-to-one relations. Such complex multi-mapping relations are over-looked in previous learning framework, which hampers effective dialogue response learning. Furthermore, if a potential highly-matched utterance pair is treated as the negative sample or an outlier is used as the positive sample, the model may be confused. Therefore, in order to consider the multi-mapping phenomenon in human conversations and remedy the potential problematic false learning samples, and enhance the training stability, we augment contrastive learning with group-wise dual sampling, where groups of positive and negative instances are sampled regarding both the context and the response, respectively. To further depict subtle differences between instances in the group, we adapt the instance importance with the matching scores, and optimize the weighted loss. We show an illustration case to understand our learning framework in Figure  1 . Given a training context-response pair (c, r), for context "What are your hobbies? I love to cook", multiple highlymatched responses are organized as the positive samples r + , and the mismatched utterances are deemed as the negatives r -. On the dual direction, regarding the response "Reading is my favorite hobby", multiple sampled context utterances are similarly divided into c + and c -. Compared with the reference baseline, the target dialogue model is trained to give higher generation probabilities for positive instances, (c, r + ) and (c + , r), and lower probabilities for negatives (c, r -) and (c -, r). By this mean, the target model is actually induced to pull the positive sample pairs together and push the mismatched pairs apart, and thus learns from the distinctions between the positives and negatives. The proposed group-wise contrastive learning framework is suited for training various neural dialogue generation models. We conduct extensive studies on three large-scale conversation datasets using four popular dialogue models to assess the proposed approach. The experimental results confirm the effectiveness of our learning framework with very favorable performance over the baseline training approaches 1 . 

 Contrastive Dialogue Learning 

 Dialogue Learning by Comparison Given training data D containing context-response pairs {(c, r) i } N i=1 , a dialogue model parameterized by ? aims to map from the input context c to the output response r. To achieve this, conventional dialogue learning approaches search the parameter ? by maximizing the conditional probability p ? (r|c) over the training samples. MLE maximizes the loglikelihood of training pairs while adversarial based approaches rely on the discriminator to distinguish between good responses and bad ones. To combat the aforementioned drawbacks of traditional training approaches in dialogue learning, we advocate the use of contrastive learning to explicitly perceive the difference between the positive and negative samples. Inspired by  Gutmann and Hyv?rinen (2012) ;  Dai and Lin (2017) , we utilize a pretrained baseline model p n (?; ?), to provide the target dialogue model p m (?; ?) a strong reference when contrasting the positive samples and the negatives. Humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions. Intuitively, the target dialogue model is expected to give higher conditional probabilities p(r|c) for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. Towards this end, we define the difference between p m (r|c, ?) and p n (r|c, ?) as: D((c, r); ?, ?) = log pm(r|c, ?) pn(r|c, ?) . ( 1 ) We wish that D((c, r); ?, ?) > 0 for any positive pair and vice versa for any negative pair. Concretely speaking, we minimize the following loss  1  Code is available at https://github.com/ hengyicai/ContrastiveLearning4Dialogue function: L (?; D, ?) = ? 1 N (c,r)?D log ?(D((c, r) + ; ?, ?)) ? 1 N (c,r)?D log [1 ? ?(D((c, r) -; ?, ?))] , (2) where ?(?) is the sigmoid activation function, the given training pair (c, r) can be used as the positive sample (c, r) + and the negative sample (c, r)can be obtained through negative sampling using the given instance (c, r). Optimizing the dialogue model with the above objective function is reminiscent of nonlinear logistic regression in Noise-Contrastive Estimation (NCE)  (Gutmann and Hyv?rinen, 2012) . The underlying motivation of our formulation and NCE are essentially different. The reference model in our work is utilized to constrain the behaviors of the target model, rather than serve as a noise distribution to provide noise data. Another difference is that, instead of using the log-ratio between p m (?; ?) and p n (?; ?) to compute posterior classification probabilities as in NCE, we introduce the function D((c, r); ?, ?) to characterize the distinctions of intrinsic dialogue properties relative to the reference, and encourage the generation of positive samples as well as penalize the negative ones through minimizing the loss in Eq.(2). Besides, by contrastively characterizing the distinctions relative to a strong reference, our method implicitly enhances the distinctiveness of the generated response as well, and ensures that the overall performance of the target model is not inferior to the reference. 

 Contrastive Dual Sampling Nevertheless, in the presence of multi-mapping relations in human dialogues, effectively sampling the positive and negative pairs in conversation is not that straightforward and even runs the risk of introducing false learning samples. To manage the complex multi-mapping phenomenon in human conversations and enhance the training stability, we augment the contrastive learning with groupwise dual sampling, where groups of positive and negative instances are sampled regarding both the context and the response, respectively. To put it concretely, for each training instance (c, r), we find a group of positive examples {(c, r + ) i } k i=1 with highest matching degree and a group of negative examples {(c, r -) i } k i=1 with lowest matching degree, using an off-the-shelf pretrained matching < l a t e x i t s h a 1 _ b a s e 6 4 = " u 4 s K Z J g R U y 0 R I 4 h w i n 8 w 2 3 H x O d M = " > A A A C D X i c b V D L S s N A F J 3 U V 6 2 v + N i 5 G S x C R S l J F X R Z d O O y g n 1 A G 8 t k O q l D Z y Z h Z i L U k G / w F 9 z q 3 p 2 4 9 R v c + i V O 2 i y 0 9 c D l H s 6 5 l 3 s 5 f s S o 0 o 7 z Z R U W F p e W V 4 q r p b X 1 j c 0 t e 3 u n p c J Y Y t L E I Q t l x 0 e K M C p I U 1 P N S C e S B H G f k b Y / u s r 8 9 g O R i o b i V o 8 j 4 n E 0 F D S g G G k j 9 e 2 9 S s / n C U 5 P Y N b l X X K c p k d 9 u + x U n Q n g P H F z U g Y 5 G n 3 7 u z c I c c y J 0 J g h p b q u E 2 k v Q V J T z E h a 6 s W K R A i P 0 J B 0 D R W I E + U l k + 9 T e G i U A Q x C a U p o O F F / b y S I K z X m v p n k S N + r W S 8 T / / V 8 P n N Z B x d e Q k U U a y L w 9 H A Q M 6 h D m E U D B 1 Q S r N n Y E I Q l N b 9 D f I 8 k w t o E W D K h u L M R z J N W r e q e V m s 3 Z + X 6 Z R 5 P E e y D A 1 A B L j g H d X A N G q A J M H g E z + A F v F p P 1 p v 1 b n 1 M R w t W v r M L / s D 6 / A F G N Z t W < / l a t e x i t > (c, r ) < l a t e x i t s h a 1 _ b a s e 6 4 = " b m 9 B i w I L Y 0 R u s r P L 2 Y / 4 5 T C I U F g = " > A A A C D X i c b V D L S s N A F J 3 U V 6 2 v + N i 5 G S x C B S 1 J F X R Z d O O y g n 1 A G 8 t k O q l D Z y Z h Z i L U k G / w F 9 z q 3 p 2 4 9 R v c + i V O 2 i y 0 9 c D l H s 6 5 l 3 s 5 f s S o 0 o 7 z Z R U W F p e W V 4 q r p b X 1 j c 0 t e 3 u n p c J Y Y t L E I Q t l x 0 e K M C p I U 1 P N S C e S B H G f k b Y / u s r 8 9 g O R i o b i V o 8 j 4 n E 0 F D S g G G k j 9 e 2 9 S s / n C U 6 P Y d b l X X K S p k d 9 u + x U n Q n g P H F z U g Y 5 G n 3 7 u z c I c c y J 0 J g h p b q u E 2 k v Q V J T z E h a 6 s W K R A i P 0 J B 0 D R W I E + U l k + 9 T e G i U A Q x C a U p o O F F / b y S I K z X m v p n k S N + r W S 8 T / / V 8 P n N Z B x d e Q k U U a y L w 9 H A Q M 6 h D m E U D B 1 Q S r N n Y E I Q l N b 9 D f I 8 k w t o E W D K h u L M R z J N W r e q e V m s 3 Z + X 6 Z R 5 P E e y D A 1 A B L j g H d X A N G q A J M H g E z + A F v F p P 1 p v 1 b n 1 M R w t W v r M L / s D 6 / A F J Y 5 t Y < / l a t e x i t > (c + , r) < l a t e x i t s h a 1 _ b a s e 6 4 = " u m J 0 model to compute the matching scores between the given context and candidate responses. Similarly, 9 P h f o 2 C n S Y 2 W w 9 R F 9 + l l N Q k = " > A A A C D X i c b Z D L S s N A F I Y n 9 V b r L V 5 2 b g a L U F F K U g V d F t 2 4 r G A v 0 M Y y m U 7 q 0 J l J m J k I N e Q Z f A W 3 u n c n b n 0 G t z 6 J k z Y L b f 3 h w M d / z u E c f j 9 i V G n H + b I K C 4 t L y y v F 1 d L a + s b m l r 2 9 0 1 J h L D F p 4 p C F s u M j R R g V p K m p Z q Q T S Y K 4 z 0 j b H 1 1 l / f Y D k Y q G 4 l a P I + J x N B Q 0 o B h p Y / X t v U r P 5 w l O 7 5 L j 9 A R m L N O j v l 1 2 q s 5 E c B 7 c H M o g V 6 N v f / c G I Y 4 5 E R o z p F T X d S L t J U h q i h l J S 7 1 Y k Q j h E R q S r k G B O F F e M v k + h Y f G G c A g l K a E h h P 3 9 0 a C u F J j 7 p t J j v S 9 m u 1 l 5 r 8 9 n 8 9 c 1 s G F l 1 A R x Z o I P D 0 c x A z q E G b R w A G V B G s 2 N o C w p O Z 3 i O + R R F i b A E s m F H c 2 g n l o 1 a r u a b V 2 c 1 a u X + b x F M E + O A A V 4 I J z U A f X o A G a A I N H 8 A x e w K v 1 Z L 1 Z 7 9 b H d L R g 5 T u 7 4 I + s z x 9 H L J t W < / l a t e x i t > (c , r) < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 k Y X D I M Y q K 7 4 d H T D L 8 Y / C X X 2 X z c = " > A A A C D X i c b Z D L S s N A F I Y n 9 V b r L V 5 2 b g a L U E F L U g V d F t 2 4 r G A v 0 M Y y m U 7 q 0 J l J m J k I N e Q Z f A W 3 u n c n b n 0 G t z 6 J k z Y L b f 3 h w M d / z u E c f j 9 i V G n H + b I K C 4 t L y y v F 1 d L a + s b m l r 2 9 0 1 J h L D F p 4 p C F s u M j R R g V p K m p Z q Q T S Y K 4 z 0 j b H 1 1 l / f Y D k Y q G 4 l a P I + J x N B Q 0 o B h p Y / X t v U r P 5 w l O 7 5 K T 9 B h m L N O j v l 1 2 q s 5 E c B 7 c H M o g V 6 N v f / c G I Y 4 5 E R o z p F T X d S L t J U h q i h l J S 7 1 Y k Q j h E R q S r k G B O F F e M v k + h Y f G G c A g l K a E h h P 3 9 0 a C u F J j 7 p t J j v S 9 m u 1 l 5 r 8 9 n 8 9 c 1 s G F l 1 A R x Z o I P D 0 c x A z q E G b R w A G V B G s 2 N o C w p O Z 3 i O + R R F i b A E s m F H c 2 g n l o 1 a r u a b V 2 c 1 a u X + b x F M E + O A A V 4 I J z U A f X o A G a A I N H 8 A x e w K v 1 Z L 1 Z 7 9 b H d L R g 5 T u 7 4 I + s z x 9 K a J t Y < / l a t e x i t > (a) (b) c < l a t e x i t s h a 1 _ b a s e 6 4 = " t m p + 6 t T i 8 7 F h x Z C 7 U B j n Y G j 7 5 D w = " > A A A B / X i c b V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T c K W g Z t L C O Y B y R L m J 3 M J m P m s c z M C m E J / o K t 9 n Z i 6 7 f Y + i V O k i 0 0 8 c C F w z n 3 c i 4 n S j g z 1 v e / v J X V t f W N z c J W c X t n d 2 + / d H D Y N C r V h D a I 4 k q 3 I 2 w o Z 5 I 2 L L O c t h N N s Y g 4 b U W j m 6 n f e q T a M C X v 7 T i h o c A D y W J G s H V S s x u J j E x 6 p b J f 8 W d A y y T I S R l y 1 H u l 7 2 5 f k V R Q a Q n H x n Q C P 7 F h h r V l h N N J s Z s a m m A y w g P a c V R i Q U 2 Y z b 6 d o F O n 9 F G s t B t p 0 U z 9 f Z F h Y c x Y R G 5 T Y D s 0 i 9 5 U / N e L x E K y j a / C j M k k t V S S e X C c c m Q V m l a B + k x T Y v n Y E U w 0 c 7 8 j M s Q a E + s K K 7 p S g s U K l k m z W g n O K 9 W 7 i 3 L t O q + n A M d w A m c Q w C X U 4 B b q 0 A A C D / A M L / D q P X l v 3 r v 3 M V 9 d 8 f K b I / g D 7 / M H H N 6 V 5 g = = < / l a t e x i t > r < l a t e x i t s h a 1 _ b a s e 6 4 = " H 8 9 h Y t M N / T R Y q F 4 0 O m f g 0 w c g q z o = " > A A A B / X i c b V C 7 S g N B F L 3 r M 8 Z X 1 N J m M A h W Y T c K W g Z t L C O Y B y R L m J 3 M J m P m s c z M C m E J / o K t 9 n Z i 6 7 f Y + i V O k i 0 0 8 c C F w z n 3 c i 4 n S j g z 1 v e / v J X V t f W N z c J W c X t n d 2 + / d H D Y N C r V h D a I 4 k q 3 I 2 w o Z 5 I 2 L L O c t h N N s Y g 4 b U W j m 6 n f e q T a M C X v 7 T i h o c A D y W J G s H V S s x u J T E 9 6 p b J f 8 W d A y y T I S R l y 1 H u l 7 2 5 f k V R Q a Q n H x n Q C P 7 F h h r V l h N N J s Z s a m m A y w g P a c V R i Q U 2 Y z b 6 d o F O n 9 F G s t B t p 0 U z 9 f Z F h Y c x Y R G 5 T Y D s 0 i 9 5 U / N e L x E K y j a / C j M k k t V S S e X C c c m Q V m l a B + k x T Y v n Y E U w 0 c 7 8 j M s Q a E + s K K 7 p S g s U K l k m z W g n O K 9 W 7 i 3 L t O q + n A M d w A m c Q w C X U 4 B b q 0 A A C D / A M L / D q P X l v 3 r v 3 M V 9 d 8 f K b I / g D 7 / M H N J m V 9 Q = = < / l a t e x i t > {(c + , r) i } k i=1 and {(c -, r) i } k i=1 are also retrieved from the training set to serve as the context-side contrastive examples, as shown in Figure  2(a) . In this work, we adopt MSN  (Yuan et al., 2019) , a context-response matching network based on multihop selection, as the off-the-shelf matching model. Note that other sophisticated matching models can also be applied, e.g., deep attention matching network . 

 Group-wise Contrastive Learning For each training instance (c, r), as describe in ?2.2, we sample k different positive and negative pairs regarding both the dialogue context and its response to manage multi-mapping relations in conversation and stabilize the model training. The resultant well-chosen samples are composed of positive samples, {(c, r + ) i } k i=1 and {(c + , r) i } k i=1 , and the negatives, {(c, r -) i } k i=1 and {(c -, r) i } k i=1 . Then, the loss function is updated as: L (?; D, ?) = ? 1 N (c,r)?D 1 2k + 1 2k+1 i=1 log ?(D((c, r) + i ; ?, ?)) ? 1 N (c,r)?D 1 2k 2k i=1 log [1 ? ?(D((c, r) - i ; ?, ?))] . Given varied matching degrees of the collected context-response pairs in open-domain dialogue, indiscriminately training on such data impedes the model to perceive intra-group differences of these samples. We thus utilize the matching score s attached with each sample to adapt its instance effect on the group-wise contrastive dialogue learning. Specifically, for a given training example (c, r), the matching score s + of its positive pair lies in (0, 1] and the negative score slies in [?1, 0]. To induce the model learning from sample pairs with varied matching degrees discriminately, the loss function is finally defined to be: L(?; D, ?) = ? 1 N (c,r)?D 1 2k + 1 2k+1 i=1 log [s + i ? ?(D((c, r) + i ; ?, ?))] ? 1 N (c,r)?D 1 2k 2k i=1 log [1 + s - i ? ?(D((c, r) - i ; ?, ?))] . (4) The loss function L(?) reaches its lower bound when the positive and negative pairs can be perfectly distinguished, i.e., p m (r|c, ?) p n (r|c, ?) for the positive samples and p m (r|c, ?) p n (r|c, ?) for the negatives, which indicates that the target dialogue model is able to clearly contrast a group of positive candidates from the negative ones and generate highly-distinctive responses for the given contexts. 

 Discussion Neural sequence-to-sequence models trained with the MLE objective function are plagued by the lowdiversity issue when it comes to the open-domain conversational setting, in which bland and generic utterances usually dominate the data distribution. Since the objective of MLE is to maximize only the probabilities of ground-truth context-response pairs, it fails to capture the multi-mapping nature of human dialogues, not to mention the semantic differences among various candidates for a given example. While the proposed group-wise contrastive learning framework explicitly explores multiple variants of a given dialogue example by leveraging an off-the-shelf matching model, and implicitly guarantees the ground-truth generation probabilities through the contrastive constraints in Eq.(  4 ). Adversarial learning approaches and our proposed framework both involve an auxiliary model during the training process. However, GANs are learned via a competition between the target generator and the counteracting discriminator, which needs careful tuning to prevent model collapse in text modeling  (Caccia et al., 2020) , whereas in our framework, the auxiliary reference system models conversation data in the same direction with the target dialogue model, and is stable during the learning procedure. 

 Experiments 

 Experiment Settings Datasets We perform experiments on three conversation datasets: PersonaChat  (Zhang et al., 2018b) , Douban Corpus  and OpenSubtitles  (Lison and Tiedemann, 2016) . Per-sonaChat, an English-language dataset, contains multi-turn dialogues between pairs of speakers, collected via Amazon Mechanical Turk. Douban consists of daily conversations from a popular social networking service-Douban group 2 in China. OpenSubtitles contains human-human conversations converted from movie transcripts in English. Data statistics are listed in Table  1 . Experimental Models We apply the proposed group-wise contrastive learning framework to several state-of-the-art models, including (i) SEQ2SEQ: a LSTM-based sequence-to-sequence model with attention mechanisms  (Bahdanau et al., 2015) , (ii) HRED: a hierarchical recurrent neural dialogue generation model , (iii) TRANSFORMER: an encoder-decoder architecture relying solely on attention mechanisms  (Vaswani et al., 2017) , (iv) HRAN: a hierarchical recurrent attention network for multi-turn response generation  (Xing et al., 2018) . Each model is trained using two protocols: the vanilla MLE training procedure and the proposed groupwise contrastive learning procedure, keeping other configurations the same. Baselines We compare our group-wise contrastive learning framework against the following dialogue learning approaches: (i) ADVERSARIAL: an adversarial training approach for response generation , (ii) MMI: a training objective which maximums the mutual information between the dialogue context and its response  (Li et al., 2016a; Zhang et al., 2018c) , (iii) DEEPRL: a reinforcement learning framework for neural response generation with heuristic reward functions to boost response qualities  (Li et al., 2016b) , (iv) CVAE: a conditional variational auto-encoder learning framework to maximize the data likelihood, augmented with the KL-annealing technique  (Bowman et al., 2016)  and a BOW loss , and (v) DIALOGWAE: a conditional Wasserstein auto-encoder framework, modeling the distribution of dialogues by training a GAN within the latent variable space  (Gu et al., 2019) . 

 Automatic Evaluation Metrics We adopt several standard metrics widely used in existing works to measure the performance of dialogue generation models, including BLEU  (Papineni et al., 2002) , embedding-based metrics (Average, Extrema, Greedy and Coherence) 3  (Liu et al., 2016; Xu et al., 2018; Sedoc et al., 2019) , entropy-based metrics 4 (Ent-{1,2})  (Serban et al., 2017)  and distinct metrics (Dist-{1,2,3})  (Li et al., 2016a) . 

 Implementation and Reproducibility We implement our model in ParlAI  (Miller et al., 2017)  and train them on Nvidia P40 GPUs. All the models use pretrained word embeddings produced by fastText  (Bojanowski et al., 2017) , and the dimensionality of word vectors is 300. For experimental models, 2-layer LSTM-based encoder and decoder with hidden size 256 are used in SEQ2SEQ. We use the base Transformer configuration described in  Vaswani et al. (2017)  response decoder for both the HRED and HRAN. The GRU hidden size is set to 256. For both models using different training procedures, we pretrain them by MLE and the result checkpoint is adopted as the reference model used in our framework. We employ BM25 (Robertson and Zaragoza, 2009) to construct the index used during the contrastive dual sampling procedure. The group size k is set to 3. Regarding comparison models, we adopt the default configurations used in the original papers. We optimize models by Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001 and the batch size of 128. All systems are trained until the validation loss fails to decrease for 5 checkpoints. We compute the loss on the validation set at every 0.5 epochs and save the parameters for the top model. Evaluation scores on the test set from the saved model are finally reported. 

 Evaluation Results Performance on Experimental Models We instantiate the proposed framework on several stateof-the-art models for dialogue generation. Table  2  reports the automatic evaluation results of our learning framework and the conventional MLE training procedure. By training dialogue models using the proposed learning framework, we witness solid performance boosts on three conversation datasets in terms of almost all the evaluation metrics, compared to the vanilla training. Such improvements are also consistent across various experimental architectures, affirming the general applicability and superiority of the proposed framework. 

 Comparison with Baseline Approaches We compare our proposed framework with existing learning approaches designed for dialogue generation task. Table  3  summarizes the evaluation results. We observe that our learning framework outperforms previous approaches regarding the majority of evaluation metrics. It is worth noting that the proposed framework brings a relatively large improvement regarding both the response diversity and conversation coherence, indicating that our approach helps the dialogue model to generate not only informative but also context-relevant responses, which confirms our hypothesis that the group-wise contrastive learning encourages distinctiveness. 

 Human Evaluation We further conduct human evaluations to assess the proposed learning framework. We choose the PersonaChat as our evaluation corpus since its expressions are more close to the style of daily chat and are easier for the annotators to make judgments. Three crowd-sourced graduate students are employed to evaluate the quality of generated responses for 100 randomly sampled input contexts. During the evaluation, the annotators are requested to select a preferred response, or vote a tie, considering the following aspects of response quality: fluency, informativeness, coherence and engagingness. Table  4  summarizes the evaluation results and the Cohen's kappa scores  (Cohen, 1960)  to measure the intra-rater reliability. We observe that our learning framework brings more preferable replies compared with the competitors. This indicates that training the dialogue model with the proposed group-wise contrastive learning framework does improve the response quality. 

 Model Analysis 

 Effect of the Group-wise Learning Strategy To manage the multi-mapping relations in humanhuman conversation and stabilize the model training with noisy data, the dialogue model is induced to contrast a group of positive samples from the negative ones, pulling the matched sample pairs together and pushing the mismatched pairs apart. We ablate the group-wise learning from the framework by using only one pair of positive and negative samples to verify its effectiveness. As presented in Table  5  (a), we can see that disabling groupwise learning hurts performance significantly on all evaluation metrics. Note that ablating either the group-wise positive sampling (Table  5 (b) ) or group-wise negative sampling (Table  5  (c)) also leads to a performance drop with respect to the evaluation metrics. It demonstrates that the groupwise learning strategy plays a key role in achieving strong performance. Effect of the Dual Sampling In our framework, the contrastive samples can be organized regarding either the dialogue context or response, allowing the dialogue model to explore both the many-toone and one-to-many relations in human conversation. We investigate different sampling strategies in Table  5  (d) and (e). We notice that when both the response-side and context-side samplings together incorporated into the learning framework, the model achieves its best performance, verifying the effectiveness of the contrastive dual sampling. 

 Impact of Matching Scores To discriminatively exploit the sampled context-response pairs with varied matching degrees, we utilize the matching score attached with each sample to adapt its instance effect on the model training. We conduct the ablation test of this learning strategy by simply discarding the impact of matching scores as in Eq.(  3 ). As shown in Table  5  (f), training without considering matching degrees of samples leads to a consistent performance drop, which suggests that the system can benefit from perceiving finegrained differences within the group during contrastive learning. 

 Impact of Group Size We explore the impact of using different group size k in our group-wise contrastive learning framework in Figure  3 . We observe that increasing the group size k leads to continuous improvement on the Distinct metric while other reference-based metrics achieve the best results at a moderate group size. We conjecture that a larger group size allows the dialogue model to learn from more diverse expressions, meanwhile it also risks introducing more utterances that are Coherence k < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a       inconsistent with the references. T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > k < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > k < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > k < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > k < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A 0 o W M 7 w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F N 4 g 9 G F e + 3 z i a T g M 0 D M w d U e U K p 0 = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R m U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 

 Related Work Learning Methods for Dialogue Generation Typically, state-of-the-art neural dialogue generation models adopt Maximum Likelihood Estimation (MLE) as their learning approach, maximizing loglikelihood of model parameters on the training data. Though effective, well-known issues, including the notorious general dull response problem, are reported by prior art  (Li et al., 2016a;  on dialogue models trained with MLE. Alternative dialogue learning approaches are proposed to tackle the issues.  Li et al. (2016a) ;  Zhang et al. (2018c)  introduce the Maximum Mutual Information as the objective function to promote response diversity. Techniques of reinforcement learning (RL) and adversarial learning have been introduced into dialogue generation by  Li et al. (2016b  to better approximate the optimization goal of dialogue models. Conditional variational framework  Shen et al., 2017; Park et al., 2018)  has also shown a promise in dialogue generation.  Gu et al. (2019)  further introduce a conditional Wasserstein autoencoder that employs GAN  (Goodfellow et al., 2014)  to model the multimodal latent structures.  Cai et al. (2020)  design a multi-curriculum learning frame-work to facilitate the dialogue model training. Contrasted with existing learning methods for dialogue generation, the proposed framework in this work encourages the model to learn from the difference between well-chosen contrastive pairs, which explicitly models the multi-mapping relations in conversation and promotes the distinctiveness of the generated responses in the meantime. 

 Contrastive Learning The concept of learning by contrasting positive pairs against negative pairs  (Hadsell et al., 2006; Gutmann and Hyv?rinen, 2012)  has been successfully adopted in many tasks. For example, contrastive learning in language modeling task  (Mnih and Teh, 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2015)  aims to approximate the negative log-likelihood by training the model to correctly classify between generated noise samples and words observed in the training data. Contrastive visual representation learning  (van den Oord et al., 2018;  trains a generative model to score real data points higher than negative samples.  Dai and Lin (2017)  propose to use contrastive learning for image caption.  Clark et al. (2020)  use contrastive learning to train a discriminative model for language representation learning. Compared with existing work, samples used in this paper, instead of being sampled randomly, are carefully chosen to ex-hibit particular properties of human dialogues. Another difference is that, we manage multi-mapping relations prevalent in human conversation using many positives and many negatives, which captures both the intra-group and inter-group variability. 

 Conclusion In this work, we propose a group-wise contrastive dialogue learning approach, that explicitly perceives the difference between the well-chosen positive and negative utterances, and manages the multi-mapping relations in human conversations simultaneously. Given a training instance, the proposed learning framework first organizes a group of positive samples and negative samples regarding context-response matching degrees, and then trains the target dialogue model to give higher conditional probabilities for positive pairs and lower probabilities for the negatives. Extensive experimental results show that the proposed learning framework brings a solid favorable performance boost amongst various strong baseline approaches. What are your hobbies? I love to cook. r < l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t bV 1 A o = " > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 R w 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5 t b V 1 A o = " > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / B s n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w q Q x L G 2 j 6 F Z K 7 + 3 s h o Z M w 0 C u x k n t A s e 7 n 4 n 9 d L M b z 2 M 6 G S F L l i i 4 / C V B K M S X 4 + G Q r N G c q p J Z R p Y b M S N q a a M r Q l V W w J 3 v L J q 6 R 9 U f f c u n d / W W v c F H W U 4 Q R O 4 Rw 8 u I I G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j J a c Y u c Y / s D 5 / A H 0 G 5 E W < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " j u w 6 e Q U U F M 3 i 3 7 B w 8 Y s q 5t b V 1 A o = " > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 6 L L o x m U F + 8 A 2 l M l 0 0 g 6 d T M L M j V B C / 8 K N C 0 X c + j f u / Bs n b R b a e m D g c M 6 9 z L k n S K Q w 6 L r f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 6 1 T Z x q x l s s l r H u B t R w K R R v o U D J u 4 n m N A o k 7 w S T 2 9 z v P H F t R K w e c J p w P 6 I j J U L B K F r p s R 9 R H A d h p m e D a s 2 t u 3 O Q V e I V p A Y F m o P q V 3 8 Y s z T i C p m k x v Q 8 N 0 E / o x o F k 3 x W 6 a e G J 5 R N 6 I j 3 L F U 0 4 s b P 5 o l n 5 M w 
