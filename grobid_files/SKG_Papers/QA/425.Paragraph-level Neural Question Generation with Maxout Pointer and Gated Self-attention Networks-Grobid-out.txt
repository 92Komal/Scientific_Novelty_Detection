title
Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks

abstract
Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural models have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation -worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our model outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU 4).

Introduction Question generation (QG), aiming at creating questions from natural language text, e.g. a sentence or paragraph, is an important area in natural language processing (NLP). It is receiving increasing interests in recent years from both industrial and academic communities, due to the booming of Question-and-Answer (QnA) and conversation systems, including Alexa, Cortana, Google Assistant and Siri, the advancement of QnA or machine comprehension technologies together with the releases of datasets like SQuAD  (Rajpurkar et al., 2016)  and MS MARCO  (Nguyen et al., 2016) , and the success of language generation technologies for tasks like machine translation  (Wu et al., 2016 ) and text summarization  (See et al., 2017)  in NLP. A conversational system can be proactive by asking the user questions  (Shum et al., 2018) , while a QnA system can benefit from a large scale question-answering corpus which can be created by an automated QG system . Education is another key application where QG can help with reading comprehension  (Heilman and Smith, 2010) . In NLP, QG has been mainly tackled by two approaches: 1) rule-based approach, e.g.  (Heilman and Smith, 2010; Mazidi and Nielsen, 2014; Labutov et al., 2015)  2) neural QG approach: end-toend training a neural network using the sequence to sequence (also called encoder-decoder) framework, e.g.  (Du et al., 2017; Song et al., 2017; . In this paper, we adopt the second approach. More specifically, we focus on an answer-aware QG problem, which takes a passage and an answer as inputs, and generates a question that targets the given answer. It is also assumed the answer is comprised of certain spans of the text from the given passage. This is the exact setting of SQuAD, and similar problems have been addressed in, e.g.  Wang et al., 2017a) . A paragraph often contains much richer context than a sentence, as shown in Figure  1 .  (Du et al., 2017)  pointed out that about 20% questions in SQuAD require paragraph-level information to be asked and using the whole paragraph can improve QG performance on those questions. However, a paragraph can contain irrelevant information w.r.t. the answer for generating the question. The challenge is thus how to effectively utilize relevant information at paragraph-level for QG. Existing neural QG works conducted on SQuAD use 1 Paragraph: carolina suffered a major setback when thomas davis , an 11year veteran who had already overcome three acl tears in his career , went down with a broken arm in the nfc championship game . despite this , he insisted he would still find a way to play in the super bowl . his prediction turned out to be accurate 4 Paragraph: the victoria and albert museum ( often abbreviated as the v & a ) , london , is the world 's largest museum of decorative arts and design , housing a permanent collection of over 4.5 million objects . it was founded in 1852 and named after queen victoria and prince albert. the v & a is located in the brompton district of the royal borough of kensington and chelsea , in an area that has become known as " albertopolis " because of its association with prince albert , the albert memorial and the major cultural institutions with which he was associated . these include the natural history museum , the science museum and the royal albert hall . the museum is a non-departmental public body sponsored by the department for culture , media and sport . like other national british museums , entrance to the museum has been free since 2001 Human generated: when was the victoria and albert museum founded ? Sentence-level QG: when was prince albert and prince albert founded ? Paragraph-level QG: when was the victoria and albert museum founded ? Figure  1 : Examples where paragraph-level information is required to ask right questions. Sentences contain answers are in italic font, while answers are underscored. QG results are generated by model s2s-a-ct-mp-gsa. only a sentence as context, e.g.  (Du et al., 2017; Song et al., 2017; ; when applied to paragraph-level context  we observed large gaps compared to state-of-the-art results achieved by using sentence-level context. In this paper, we extend previous sequence to sequence attention model with a maxout pointer mechanism and a gated self-attention encoder which outperforms existing neural QG approaches with either sentence or paragraph as inputs. Fur-thermore, with paragraph-level inputs, it outperforms the results of previous approaches with sentence-level inputs, improving state-of-the-art result from 13.9 to  16.3 (BLEU 4) . This is the first model that demonstrates large improvement with paragraph as input over sentence as input. In addition, our model is more concise compared to most of existing ones, e.g.  Song et al., 2017) . Techniques like incorporating rich features  and policy gradient  (Song et al., 2017;  are orthogonal to ours and can be leveraged for further performance improvement in the future. 

 Our Model 

 Problem Definition We use P and A to represent input passage and answer respectively, and use Q to represent the generated question. "Passage" in this section can represent either a sentence or a paragraph. The task is to find Q that: Q = argmax Q P rob(Q|P, A) where passage is comprised of sequence of words:P = {x t } M t=1 , answer A must be sub spans of the passage. Words generated in Q = {y t } K t=1 are either from the input passage, {x t } M t=1 , or from a vocabulary V . Figure  2  illustrates the end-to-end structure of our model proposed in this paper. 

 Passage and Answer Encoding Different types of encoders are designed for various domains  (Chung et al., 2014; Hochreiter and Schmidhuber, 1997) . We are agnostic to the form of the encoder and simply use recurrent neural network (RNN) to present the encoding process: u t = RN N E (u t?1 , [e t , m t ]) (1) Answer Tagging. In Eq. 1, u t represents the RNN hidden state at time step t, e t is the word embedding representation of word x t in passage P . m t is the meta-word representation of whether word x t is in or outside the answer. [a, b] represents the concatenation of vector a and b. We call this approach answer tagging which is similar to the techniques in . For applications, it is essential to be able to generate question that is coherent to an answer.  If RN N E is bi-directional, u is the concatenated representation of the forward and backward passes: U = {[ ? ? u t , ? ? u t ]} M t=1 . Gated Self-attention. Our gated self-attention mechanism is designed to aggregate information from the whole passage and embed intra-passage dependency to refine the encoded passage-answer representation at every time step. It has two steps: 1) taking encoded passage-answer representation u as input and conducting matching against itself to compute self matching representation;  (Wang et al., 2017b)  2) combining the input with self matching representation using a feature fusion gate  (Gong and Bowman, 2017) . a s t = sof tmax(U T W s u t ) (2) s t = U ? a s t (3) Step 1. In Eq. 2, W s is a trainable weight matrix. In Eq. 3, s t is the weighted sum of all words' encoded representation in passage based on their corresponding matching strength to current word at t. s = {s t } M t=1 is the final self matching representation. f t = tanh(W f [u t , s t ]) (4) g t = sigmoid(W g [u t , s t ]) (5) ?t = g t f t + (1 ? g t ) u t (6) Step 2. The self matching representation s t is combined with original passage-answer representation u t as the new self matching enhanced representation f t , Eq. 4. A learnable gate vector g t , Eq. 5, chooses the information between the original passage-answer representation and the new self matching enhanced representation to form the final encoded passage-answer representation ?t , Eq. 6, where is the element-wise multiplication. 

 Decoding with Attention and Maxout Pointer In the decoding stage, the decoder is another RNN that generates words sequentially conditioned on the encoded input representation and the previously decoded words. d t = RN N D (d t?1 , y t?1 ) (7) p(y t |{y <t }) = sof tmax(W V d t ) (8) In Eq. 7, d t represents the hidden state of the RNN at time t where d 0 is passed from the final hidden state of the encoder. y t stands for the word generated at time t. The bold font y t is used to represent y t 's corresponding word embedding representation. In Eq. 8, first an affine layer projects d t to a space with vocabulary-size dimensions, then a sof tmax layer computes a probability distribution over all words in a fixed vocabulary V . W V is a trainable weight matrix. Attention. Attention mechanism  (Bahdanau et al., 2014; Luong et al., 2015)  has been used to improve sequence to sequence models' performance and has became a default setting for many applications. We use Luong attention mechanism to compute raw attention scores r t , Eq. 9. An attention layer, Eq. 12 is applied above the concatenation of decoder state d t and the attention context vector c t and its output is used as the new decoder state. r t = ?T W a d t (9) a d t = sof tmax(r t ) (10) c t = ? ? a d t (11) dt = tanh(W b [d t , c t ]) (12) Copy/Pointer. Copy mechanism  (Gu et al., 2016)  or pointer network  (See et al., 2017; Vinyals et al., 2015)  was introduced to allow both copying words from input via pointing, and generating words from a predefined vocabulary during decoding. Similar to  (Gu et al., 2016) , our pointer mechanism directly leverages raw attention scores r t = {r t,k } M k=1 over the input sequence which has a vocabulary of ?. Words at every time step (a pointer) are treated as unique copy targets and the final score on one word is calculated as the sum of all scores pointing to the same word, Eq. 13, where x k and y t stand for word vocabulary indices of the kth word in input and the tth word in decoded sequence respectively. The scores of the nonoccurence words are set to negative infinity which will be masked out by the downstream sof tmax function. sc copy (y t ) = ? ? ? ? ? k,where x k =yt r t,k , y t ? ? ? inf, otherwise (13) We then concatenate sc copy t with the generative scores (from Eq. 8 before sof tmax), [sc gen t , sc copy t ], which has dimension: |V | + |?|. Then we perform sof tmax on the concatenated vectors and sum up the probabilities pointing to same words. Taking sof tmax on the concatenated score vector enforces copy and generative modes to compete with each other due to the shared normalization denominator. Another popular solution is to do sof tmax independently to the scores from each mode, and then combine their output probabilities with a dynamic weight which is generated by a trainable network  (See et al., 2017) . We have tested both and didn't find significant difference in terms of accuracy on our QG task. We choose the former copy approach mainly because it doesn't add extra trainable parameters. Maxout Pointer. Despite the outstanding performance of existing copy/pointer mechanisms, we observed that repeated occurrence of words in the input sequence tends to cause repetitions in output sequence, especially when the input sequence is long, e.g. a paragraph. This issue exacerbates the repetition problem which has already been commonly observed in sequence to sequence models  (Tu et al., 2016; See et al., 2017) . In this paper, we propose a new maxout pointer mechanism to address this issue and improve the metrics for QG task. Related works  (Goodfellow et al., 2013)  have explored MLP maxout with dropout. Instead of combining all the scores to calculate the probability, We limit the magnitude of scores of repeated words to their maximum value, as shown in Eq. 14. The rest remains the same as in the previous copy mechanism. sc copy (y t ) = max k,where x k =yt r t,k , y t ? ? ? inf, otherwise (14) 3 Experiments In our experiments, we study the proposed model on the QG task on SQuAD  (Rajpurkar et al., 2016)  and MS MARCO  (Nguyen et al., 2016)  dataset, demonstrate the performance of proposed components on both sentence and paragraph inputs, and compare the model with existing approaches. 

 Dataset 

 SQuAD The SQuAD dataset contains 536 Wikipedia articles and more than 100K questions posed about the articles by crowd-workers. Answers are also provided to the questions, which are spans of tokens in the articles. Following  (Du et al., 2017; Song et al., 2017) , our experiments are conducted using the accessible part of SQuAD: train and development (dev*) sets. To be able to directly compare with their works, we adopt two types of data split: 1) Split1: similar to  (Du et al., 2017) , we use dev* set as test set, and split train set into train and dev sets randomly with ratio 90%-10%. The split is done at article level. However, we keep all samples instead of only keeping the sentence-question pairs that have at least one non-stop-word in common (with 6.7% pairs dropped) as in  (Du et al., 2017) . This makes our dataset harder for training and evaluation. 2) Split2: similar to , we split dev* set into dev and test sets randomly with ratio 50%-50%. The split is done at sentence level. 

 MS MARCO MS MARCO datasets contains 100,000 queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents. We picked a subset of MS MARCO data where answers are sub-spans within the passages, and use dev set as test set (7k), and split train set with ratio 90%-10% into train (51k) and dev (6k) sets. 

 Implementation Details We used 2 layers LSTM as the RNN cell for both encoding and decoding. For encoding, bidirectional LSTM was used. The cell hidden size was 600. Dropout with probability 0.3 was applied between vertical LSTM stacks. For word embedding, we used pre-trained GloVe word vectors with 300 dimensions  (Pennington et al., 2014) , and froze them during training. Dimension of answer tagging meta-word embedding was 3. Both encoder and decoder shared the same vocabulary of the most frequent 45k GloVe words. For optimization, we used SGD with momentum  (Qian, 1999; Nesterov, 1983) . Learning rate was initially set to 0.1 and halved since epoch 8 at every 2 epochs afterwards. Models were totally trained with 20 epochs. The mini-batch size for parameter update was 64. After training, we looked at the 4 models with lowest perplexities and selected the one which used the most number of epochs as final model. During decoding for prediction, we used beam search with the beam size of 10, and stopped decoding when every beam in the stack generates the EOS token. 

 Evaluation We conduct automatic evaluation with metrics: BLEU 1, BLEU 2, BLEU 3, BLEU 4  (Papineni et al., 2002) , METEOR  (Denkowski and Lavie, 2014)  and ROUGE-L  (Lin, 2004) , and use evaluation package released by  (Sharma et al., 2017)  to compute them. 

 Results and Analysis 

 Comparison of Techniques Table  1  shows evaluation results for different models on SQuAD Split1. Results with both sentence-level and paragraph-level inputs are included. Similar results also have been observed on SQuAD Split2. The definitions of the models under comparison are: s2s: basic sequence to sequence model s2s-a: s2s + attention mechanism s2s-a-at: s2s-a + answer tagging s2s-a-at-cp: s2s-a-at + copy mechanism s2s-a-at-mp: s2s-a-at + maxout pointer mechanism s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention Attention Mechanism s2s-a vs. s2s: we can see attention brings in large improvement on both sentence and paragraph inputs. The lower performance on paragraph indicates the challenge of encoding paragraph-level information. Answer Tagging s2s-a-at vs. s2s-a: answer tagging dramatically boosts the performance, which confirms the importance of answer-aware QG: to generate good question, we need to control/learn which part of the context the generated question is asking about. More importantly, answer tagging clearly reduces the gap between sentence and paragraph inputs, which could be explained with: by providing guidance on answer words, we can make the model learn to neglect noise when processing a long context. Copy Mechanism s2s-a-at-cp vs. s2s-a-at: as expected, copy mechanism further improves the performance on the QG task.  (Du et al., 2017)  pointed out most of the sentence-question pairs in SQuAD have over 50% overlaps in non-stop-words. Our results prove that sequence to sequence models with copy mechanism can very well learn when to generate a word and when to copy one from input on such QG task. More interestingly, the performance is lower when paragraph is given as input than sentence as input. The gap, again, reveals the challenge of leveraging longer context. We found that, when paragraph is given, the model tends to generate more repetitive words, and those words (often entities/concepts) usually appear multiple times in the context, Figure  3 . The repetition issue can also be seen for sentence input, but it is more severe for paragraph. 

 Maxout Pointer s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is designed to resolve the repetition issue brought by the basic copy mechanism, for example Figure  3 . The maxout pointer mechanism outperforms the basic copy mechanism in all metrics. Moreover, the effectiveness of maxout pointer is more significant when paragraph is given as the model input, as it reverses the performance gap between models Paragraph: a problem is regarded as inherently difficult if its solution requires significant resources , whatever the algorithm used . the theory formalizes this intuition , by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them , such as time and storage . other complexity measures are also used , such as the amount of communication ( used in communication complexity ) , the number of gates in a circuit ( used in circuit complexity ) and the number of processors ( used in parallel computing ) . one of the roles of computational complexity theory is to determine the practical limits on what computers can and can not do Human generated: what unit is measured to determine circuit complexity ? Basic copy QG: what is an example of a circuit complexity in complexity complexity ? Maxout Pointer QG: what is another name for circuit complexity ?  trained with sentence and paragraph inputs, Table  1 . To demonstrate that maxout pointer reduces repetitions in generated questions, we present the word duplication rates in generated questions for various models in Figure  4 . Word duplication rate was computed by counting the number of words which appear more than once, and then taking a ratio of them over the total word counts. As shown in Figure  4 , both the attention mechanism and the basic copy mechanism introduce more repetitions, although they improve overall accuracy according to Table  1 . For models trained with paragraph inputs, where the duplication rates are much higher, maxout pointer reduces the duplication rates to half of their values in the basic copy and to the same level as model trained with sentence inputs. Such repetition issue was also observed in other sequence to sequence applications, e.g.  (See et al., 2017)  who proposed a coverage model and coverage loss to resolve this issue. We implemented and tested their approach on our QG task. Even though the duplication ratio dropped as expected, we observed a slight decline in the accuracy when coverage loss was added. QG: what competition did thomas davis think he would play in ? Figure  5 : Self-attention alignments map: each row represents an alignment vector of self-attention. Gated Self-attention s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results demonstrate the effectiveness of gated selfattention, in particular, when working with paragraph inputs. This is the first time, as we know, taking paragraph as input is better than sentence for neural QG tasks. The observation is consistent across all metrics. Gated self-attention helps refine encoded context by fusing important information with the context's self representation properly, especially when the context is long. To better understand how gated self-attention works, we visualize the self alignment vectors at each time step of the encoded sequence for one example, in Figure  5 . This example corresponds to the example 1 in Figure  1 . We can see the alignments distribution concentrates near the answer sequence and the most relevant context: "thomas davis" in this example. Such alignments in turn would help to promote the most valuable information for decoding. 

 Beam Search Beam search is commonly used for decoding for predictions. Existing neural QG works, e.g.  (Du et al., 2017; , evaluated their models with beam search decoding. However, so far, none of them have reported the comparison between beam search decoding with greedy decoding. In this paper, we give such comparison for our best model: s2s-a-at-mp-gsa with both sentence and paragraph inputs in Table  2 . We can clearly see beam search decoding boosts all metrics for both sentence and paragraph inputs. The effectiveness of beam search has also been demonstrated for other tasks, like neural machine translation in  (Wu et al., 2016) . 

 Comparison with Existing Neural Question Generation Works On SQuAD dataset, we compare the BLEU, ME-TEOR, ROUGE-L scores of our best model, s2sa-at-mp-gsa, with the numbers in the existing works in Table  3 . Comparison with  (Du et al., 2017)  and  (Song et al., 2017)  is conducted on SQuAD data Split1, while comparison with  and  (Song et al., 2017)  is conducted on data SQuAD split2. Because  (Song et al., 2017)  had results on both splits, we compare with both of them. On MS MARCO dataset, we compare the BLEU 4 scores reported by  in Table  4 . Our model with maxout pointer and gated selfattention achieves the state-of-the-art results in QG. Note that all those existing works in SQuAD encoded only sentence-level information, the results from our model surpass them on the same sentence input while achieving much higher numbers when working with paragraph. 

 Case Study In Figure  1 , we present some examples for which paragraph-level information is needed to ask good/correct questions. Generated questions from model s2s-a-ct-mp-gsa are also presented for both sentence and paragraph inputs. Those examples demonstrate that generated questions contain richer information when paragraphs are provided   instead of sentences. In example 1 and 3, name "thomas davis" and "percy shelley" appear in the paragraphs not the sentences contain the answers. In example 2, paragraph-level QG can generate richer description "in 1953" from the paragraph, although human generated is "disneyland". Both generated questions lack one relative important piece of information "want abc to invest". In example 4, paragraph-level QG correctly identifies "it" is referring to the museum which is out of the sentence. 

 Related Work QG has been mainly tackled with two types of approaches. One is built on top of heuristic rules that creates questions with manually constructed template and ranks the generated results, e.g.  (Heilman and Smith, 2010; Mazidi and Nielsen, 2014; Labutov et al., 2015) . Those approaches heavily depend on human effort, which makes them hard to scale up to many domains. The other one, which is becoming increasingly popular, is to train an end-to-end neural network from scratch by using sequence to sequence or encoder-decoder framework, e.g.  (Du et al., 2017; Song et al., 2017; . The second one is more related to us, so we will focus on describing those approaches.  (Du et al., 2017)  pioneered the work of automatic QG using an end-to-end trainable sequence to sequence neural model. Automatic and human evaluation results showed that their system outperformed the previous rule-based systems  (Heilman and Smith, 2010; Rus et al., 2010) . However, in their study, there was no control about which part of the passage the generated question was asking about. Answer-aware sequence to sequence neural QG systems  encoded answer location information using an annotation vector corresponding to the answer word positions.  utilized rich features of the passage including answer positions.  deployed a two-stage neural model that detects key phrases and subsequently generates questions conditioned on them.  combined supervised and reinforcement learning in the training of their model using policy gradient techniques to maximize several rewards that measure question quality. Instead of using an annotation vector to tag the answer locations,  (Song et al., 2017)  proposed a unified framework for QG and question answering by encoding both the answer and the passage with a multi-perspective matching mechanism.  Wang et al., 2017a)  proposed joint models to address QG and question answering together.  conducted QG for improving question answering. Due to the mixed objectives including question answering, their approaches' performance on QG were lower than the state-of-the-art results. 

 Conclusion and Future Work In this paper, we proposed a new sequence to sequence network which contains a gated self-attention encoder and a maxout pointer decoder to address the answer-aware QG problem for long text input. We demonstrated the model can effectively utilize paragraph-level context, and outperformed the results with sentence-level context. The new model exceeded state-of-the-art approaches with either paragraph or sentence inputs. We would like to discuss some potential challenges when applying the QG model in practice. 1) Answer spans aren't provided as input. One straight-forward method is to extract entities or noun phrases and use them as potential answer spans. A neural entity selection model can also be leveraged to extract good answer candidates to improve the precision as proposed in . 2) An input passage does not contain any eligible answers. In such case, we do not expect the model to output valid questions. We could remove questions with low generation probability, while a better approach could be running entity selection or quality detection model before question generation step to eliminate ineligible passages. 3) An answer could be shared by different questions. We could output multiple questions using beam search. However, beam search does not guarantee to output diversified candidates. We would need to explicitly model diversity among candidates during generation, for example, leveraging the approach described in  (Li and Jurafsky, 2016) . Our future work lands in the following directions: incorporate rich features, such as POS and entity, in input passages; directly optimize sequence-level metrics with policy gradient; relax the constraint on answer to accept abstractive answers; jointly model question generation and question answering; ask multiple questions simultaneously with diverse perspectives. Figure 2 : 2 Figure 2: End-to-end diagram for the model with answer tagging, gated self-attention and maxout pointer mechanism. 
