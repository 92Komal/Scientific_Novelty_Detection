title
Crowdsourcing Question-Answer Meaning Representations

abstract
We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A qualitative analysis demonstrates that the crowd-generated questionanswer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, Nom-Bank, and QA-SRL) along with many previously under-resourced ones, including implicit arguments and relations. We also report baseline models for question generation and answering, and summarize a recent approach for using QAMR labels to improve an Open IE system. These results suggest the freely available 1 QAMR data and annotation scheme should support significant future work.

Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. Prop-Bank  (Palmer et al., 2005) ) and there have been efforts to study other parts of speech (e.g. Nom-Bank  (Meyers et al., 2004)  and  FrameNet (Baker et al., 1998) ) and introduce whole-sentence structures (e.g. AMR  (Banarescu et al., 2013) ). However, highly skilled and trained annotators are re-Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Who will join as nonexecutive director? -Pierre Vinken What is Pierre's last name? -Vinken Who is 61 years old? -Pierre Vinken How old is Pierre Vinken? -61 years old What will he join? -the board What will he join the board as? -nonexecutive director What type of director will Vinken be? -nonexecutive What day will Vinken join the board? -Nov. 29 quired to label data within these formulations for each new domain, and it takes significant effort to model each new type of relationship (e.g., noun arguments in NomBank). We propose a new method to annotate relatively complete representations of the predicate-argument structure of a sentence, which can be done easily by non-experts. We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs (see Figure  1 ). Following the QA-SRL formalism  (He et al., 2015) , each question-answer pair corresponds to a predicateargument relationship. There is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning-from a vector of real numbers to a logical form-should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an-swers as the representation. As with any other representation, this can then be reprocessed for downstream tasks. Indeed, the question-answer format facilitates reprocessing for tasks that are similar in form, for example Open IE (see Section 4). A key advantage of QAMRs is that they can be annotated with crowdsourcing. The main challenge is coverage, as it can be difficult for a single annotator to write all possible QA pairs for a sentence. Instead, we distribute the work between multiple annotators in a novel crowdsourcing scheme, which we use to gather a dataset of over 100,000 QA pairs for 5,000 sentences in Newswire and Wikipedia domains. Although QAMR questions' free-form nature is crucial for our approach, it means that predicates are not explicitly marked. However, with a simple predicate-finding heuristic, we can align QAMR to PropBank, NomBank, and QA-SRL and show high coverage of predicate-argument structure, including more than 90% of non-discourse relationships. Further analysis reveals that QAMRs also capture many phenomena that are not modeled in traditional representations of predicate-argument structure, including coreference, implicit and inferred arguments, and implicit relations (for example, with noun adjuncts). Finally, we report simple neural baselines for QAMR question generation and answering. We also highlight a recent result  (Stanovsky et al., 2018)  showing that QAMR data can be used to improve performance on a challenging task: Open Information Extraction. Together, these results show that there is significant potential for follow up work on developing innovative uses of QAMR and modeling their relatively comprehensive and complex predicate-argument relationships. 

 Crowdsourcing We gather QAMRs with a two-stage crowdsourcing pipeline 2 using monetary incentives and crowd-driven quality control to ensure high coverage of predicate-argument structure. Generation workers write QA pairs and validation workers answer or reject the generated questions. Full details of our setup are given in Appendix A. Generation Workers receive an English sentence with up to four target words. They are asked to write as many QA pairs as possible containing each target word in the question or answer, subject to light constraints (for example, the question must contain a word from the sentence and be answered in the sentence, and they must highlight the answer in the sentence). Workers must write at least one QA pair for each target word to receive the base pay of 20c. An increasing bonus of 3(k + 1) cents is paid for each k-th additional QA pair they write that passes the validation stage. Validation Workers receive a sentence and a batch of questions written by an annotator in the first stage (with no marked target words or answers). The worker must mark each question as invalid or redundant with another question, or highlight its answer in the sentence. Two workers validate and answer each set of questions. They are paid a base rate of 10c for each batch, with an extra 2c for each question past four. Quality control Question writers are disqualified if the percentage of valid judgments on their questions falls below 75%. Validators need to pass a qualification test and maintain above 70% agreement with others, where overlapping answer spans are considered to agree. 

 Data Preparation and Annotation We drew our data from 1,000 Wikinews articles from 2012-2015 and 1,000 articles from Wikipedia's 1,000 core topics, 3 partitioned by document into train, dev, and test, and preprocessed using the Stanford CoreNLP tools  (Manning et al., 2014) . We also annotated 253 sentences from the Penn Treebank  (Marcus et al., 1993)  chosen to overlap with existing resources for comparison (see Section 3). For each sentence, we group its non-stopwords sequentially into groups of 3 or 4 target words, removing sentences with no content words. By presenting workers with nearly-contiguous lists of target words, enforcing non-redundancy, and providing bonuses, we encourage exhaustiveness over all possible QA pairs. By allowing the target word to appear in the question or the answer, we make the requirements flexible enough that there is almost always some QA pair that can be written. Figure  2  shows agreement statistics for question validation. We removed questions either validator counted invalid or redundant, as well as questions not beginning with a wh-word, 4 which we found to be of low quality. We also annotated the partitions at different levels of redundancy to allow for more exhaustive dev, test, and comparison sets. See Table 1 for statistics. 

 Data Analysis In this section, we show that QAMR has high coverage of predicate-argument structure and uses a rich vocabulary to label fine-grained and implicit semantic relations. Coverage To show that QAMR captures the same kinds of predicate-argument relations as existing formalisms, we compare our data to Prop-Bank, NomBank, and QA-SRL. Since predicates in the questions are not explicitly marked, we use a simple predicate-finding heuristic to help align to other formalisms: for each minimal span that appears in the QAMR questions and answers (i.e., none of its subspans appear independently of it elsewhere in the QAMR), we compute its predicate score as the proportion of its appearances that are in a question rather than in an answer.  5  We then choose the span with the highest predicate score in each question as its predicate. We measure recall on the shared Penn Treebank sentences for each resource by randomly sampling n annotators out of 5 for each group of target  4  who, what, when, where, why, how, which, and whose 5 This follows the intuition that predicates are more likely to appear in the question; for example, see join in Figure  1 . words, which simulates the situation for the training set (1 annotator) and the dev/test sets (3 annotators). For each n we took the mean of 10 runs. Full details of our comparison are in Appendix B. Results are shown in Figure  3 . Single annotators cover over 60% of relationships, and coverage quickly increases with the number of annotators, reaching over 90% with all five. This shows that QAMR's representational capacity covers the vast majority of relevant predicate-argument relations in existing resources. However, coverage in our training set is low due to low annotation density. For a qualitative analysis, we sample 150 QA pairs (see Table  2  for examples).  6  Of our sample, over 90% of question-answer pairs correspond to a predicate-argument relation expressed in the sentence, 7 including arguments and modifiers of nouns and verbs as well as relationships like those within proper names (Table  2 , ex. 2c, 3a) and coreference  (ex. 3c, 4c) . Questions that do not align to predicate-argument structure often target shallow inferences (ex. 3b, 7c). 

 Rich vocabulary Annotators use the open question format to introduce a large vocabulary of external phrases which do not appear in the sentence. Overall, 5,687 different external phrases are introduced (excluding stopwords), appearing 25,952 times in 38.7% of the questions (see  Figure 4) . These include typing words like state and country (Table  2    We also find verbal paraphrases of noun compounds, as proposed by  Nakov (2008) . For example, where Gallup poll appears in the text, one annotator has written Who conducted the poll?, which explicates the relationship between Gallup and poll. Similarly, Who received the bailouts? is written for the phrase bank bailouts. Semantics, not just syntax Only 63% of QA pairs characterize their predicate-argument relation using the same syntactic relationship as in the sentence. 5% have answers coreferent with the syntactic argument (Table  2 , ex. 3c, 4c); 17% exhibit syntactic variation, using different prepositions  (ex. 4c, 6a) , alternating between active and passive (ex. 1b), or changing between the noun and verb form of the predicate (ex. 8a); 6% ask about implicit arguments (ex. 4b, 5c, 8b); and 6% ask about inferred relations (ex. 3b). 

 Models To establish initial baselines, we apply existing neural models for QAMR question generation and answering. We also briefly summarize a recent end task result, where QAMR annotations were used to improve an Open IE system. Question generation In question generation (QG), we learn a mapping from a sentence w to a set of questions q 1 , . . . , q m . We enumerate pairs of words (w q , w a ) from the sentence to seed the generator. During training, outputs are questions q and inputs are tuples (w, w q , w a ), where w q ? q and w a is in q's answer. We also add negative samples where the output is a special token and the input has w q , w a that never appear together. We use an encoder-decoder model with a copying mechanism  (Zhou et al., 2017)  to generate a question from an input sentence with tagging features for part of speech, w q , and w a . At test time, we run all pairs of content words (w i , w j ) where |i ? j| ? 5 through the model to yield a set of questions. On the QAMR test set, this achieves 28% precision and 24% recall with fuzzy matching (multi-BLEU 8 > 0.8). 

 Question answering The format of QAMRs allows us to apply an existing question-answering model  (Seo et al., 2016)  designed for the SQuAD  (Rajpurkar et al., 2016)  reading comprehension task to answer QAMR questions. Training and testing with the SQuAD metrics on QAMR, the model achieves 70.8% exact match and 79.7% F1 score. We further improve performance to 75.7% exact match and 83.9% F1 by pooling our training set with the SQuAD training data. The relative ease of QA in comparison to QG suggests that in QAMR, most of the information is contained in the questions. Open IE Finally, we also expect that the predicate-argument relationships represented in QAMRs will be useful for many end tasks. Such a result was recently shown for Open IE  (Stanovsky et al., 2018) , using our QAMR corpus. Open IE involves extracting tuples of natural language phrases that express the propositions asserted by a sentence. They show that, using a syntactic dependency parser, a QAMR can be converted to a list of Open IE extractions. Augmenting their training data with a conversion of our QAMR dataset yields state-of-the-art performance on several Open IE benchmarks  (Stanovsky and Dagan, 2016b; Xu et al., 2013; de S? Mesquita et al., 2013; Schneider et al., 2017) . The gains come largely from the extra extractions (e.g., with nominal predicates) that QAMRs support over traditional resources focusing on verbal predications. 

 Related Work In addition to the semantic formalisms  (Palmer et al., 2005; Meyers et al., 2004; Banarescu et al., 2013; He et al., 2015)  we have already discussed, FrameNet  (Baker et al., 1998)     (Abend and Rappoport, 2013) , HSPG treebanks  (Flickinger et al., 2017) , and the Groningen meaning bank  (Basile et al., 2012) . Crowdsourcing has also been applied to gather annotations of structure in the setup of multiple choice questions, for example, for Dowty's semantic proto-roles  (Reisinger et al., 2015; White et al., 2016)  and human-in-the-loop parsing and classification  (He et al., 2016; Duan et al., 2016; Werling et al., 2015) , while  Wang et al. (2017)  use crowdsourcing with question-answer pairs to annotate some PropBank roles directly. Our approach recovers paraphrases of noun compounds similar to those crowdsourced by  Nakov (2008) . More broadly, non-expert annotation has been used extensively to gather question-answer pairs over natural language texts, for example in reading comprehension  (Rajpurkar et al., 2016; Richardson et al., 2013; Nguyen et al., 2016)  and visual question answering  (Antol et al., 2015) . However, while these treat question answering as an end task, we regard it as a representation of predicateargument structure, and focus annotators on a smaller selection of text (a few target words in a single sentence, rather than a paragraph) aiming to achieve high coverage. 

 Conclusion QAMR provides a new way of thinking about meaning representation: using open-ended natural language annotation to represent rich semantic structure. This paradigm allows for representing a broad range of semantic phenomena with data easily gathered from native speakers. Our dataset has already been used to improve the performance of an Open IE system, and how best to leverage the data and model its complex phenomena is an open challenge which our annotation scheme could support studying at a relatively large scale.  
