title
MRC Examples Answerable by BERT without a Question are Less Effective in MRC Model Training

abstract
Models developed for Machine Reading Comprehension (MRC) are asked to predict an answer from a question and its related context. However, there exist cases that can be correctly answered by an MRC model using BERT, where only the context is provided without including the question. In this paper, these types of examples are referred to as "easy to answer", while others are as "hard to answer", i.e., unanswerable by an MRC model using BERT without being provided the question. Based on classifying examples as answerable or unanswerable by BERT without the given question, we propose a method based on BERT that splits the training examples from the MRC dataset SQuAD1.1 into those that are "easy to answer" or "hard to answer". Experimental evaluation from a comparison of two models, one trained only with "easy to answer" examples and the other with "hard to answer" examples demonstrates that the latter outperforms the former.

Introduction The Machine Reading Comprehension (MRC) task locates the best corresponding natural language answer when provided a question and its related context. In recent years, MRC models using neural networks have been proposed for SQuAD  (Pranav et al., 2016 (Pranav et al., , 2018 , which is a large-scale, high-quality English MRC dataset. Most recent neural network based MRC models have outperformed human performance  (Devlin et al., 2019) . Among those existing work, to analyze the difficulty of several popular MRC benchmarks such as bAbI , SQuAD  (Pranav et al., 2016) , CBT  (Hill et al., 2016) , CNN  (Hermann et al., 2015)  and Who-did-What  (Onishi et al., 2016) ,  Kaushik and Lipton (2018)  established sensible baselines for these datasets, and found that question-only and context-only (which is called passage-only in  Kaushik and Lipton (2018) ) models often performs surprisingly well. In particular, contextonly models achieve over 50% accuracy on 14 out of 20 bAbI tasks, and as for CBT, only the last one of the 20 sentences provided as a context is necessary to achieve a comparable accuracy. They also indicated that SQuAD is designed more carefully than other datasets and achieved F1 scores of only 4% and 14.8% respectively on questiononly and context-only models, which are relatively lower.  Kaushik and Lipton (2018)  demonstrated that published MRC datasets should characterize the level of difficulty, and specifically, the extent to which questions and contexts are essential. Moreover,  Kaushik and Lipton (2018)  also claimed that follow-up papers reporting improvements ought to report performance both on the full task and variations omitting questions and contexts. In view of the point demonstrated in  Kaushik and Lipton (2018) , we concentrate more on the difficulty of every single MRC example, and aim to split the examples into easy ones and hard ones. Given the MRC dataset SQuAD1.1 (where each MRC example denoted as the tuple ?Q, C, A? of the question Q, the context C, and the answer A) and the fine-tuned MRC model using BERT  (Devlin et al., 2019) , there exist contextonly examples that can be correctly answered, where only the context is provided without including the question. By focusing on this fact, this paper proposes a method that splits the MRC examples into binary classes of "easy to answer" or "hard to answer". A 10-fold cross-validation was applied on approximately 87,600 SQuAD1.1 training examples comprised of 12,500 "easy to answer" and 75,000 "hard to answer" classes. From the comparison of the two classes, the followings are two significant findings. (1) Based   2 ) An analysis of the position distribution of answers A within the context C, answers from the "easy to answer" MRC example class tend to be located around the beginning of the context compared with those from the "hard to answer" MRC example class. 

 Machine Reading Comprehension using Neural Networks Figure  1  shows the framework of MRC models that use neural networks. In the MRC model, when a question and context are input, the starting and ending positions of the answer with respect to the question within the context are predicted. Let ts be a set of test examples with each example denoted as s(? ts). Here, s is represented as s = ?Q, C, A?. Also, if a set of examples for training MRC models is denoted as tr, then the corresponding model is represented as m(tr). Then, the answer A predicted from an input test example s with the trained MRC model m(tr) is denoted as A = answer m(tr), s A Boolean predicate answerable classifies if the given test example s is "answerable" or "unanswerable" by the trained MRC model m(tr), and is defined according to if the predicted answer A is the same as the reference answer A as answerable m(tr), s  "easy to answer" and "hard to answer" classes. As illustrated in Figure  3 , the process designates 10% of the examples as "easy to answer" and "hard to answer" classes for testing through one fold of 10fold cross-validation, which is repeated ten times, resulting in 12,500 "easy to answer" and 75,100 = 1 ( A = A) 0 ( A ? = A) "hard to answer" classes. From this, we obtain the following three types of evaluation results. (   ts i ts i ? ts j = ? (i ? = j) As shown in   = 1, 2, 3, 4 ) a 1 (ts i ) = s ? ts i answerable m(tr i ), s = 1 a 2 (ts i ) = s ? ts i answerable m(tr i Q=? ), s = 1 a 3 (ts i ) = s ? ts i answerable m(tr i ), s Q=? = 1 a 4 (ts i ) = s ? ts i answerable m(tr i Q=? ), s Q=? = 1 ua ? (ts i ) = ts i ? a ? (ts i ) (? = 1, 2, 3, 4) The sets a ? (ts i ) (? = 1, 2, 3, 4) of "answerable" test MRC examples are obtained by evaluating the MRC model trained with the training sets tr i (with questions) or tr i Q=? (without questions) against s (with a question) or s Q=? (without a question). We define the set E of "easy to answer" MRC examples as the union of the three sets a ? (ts i ) (? = 2, 3, 4) of "answerable" test MRC examples. For these, we collect the "answerable" test MRC examples over the cases with questions removed either from the training or test MRC examples (a 1 (ts i ) is excluded because the questions are used in both the training and test MRC examples). The set H of "hard to answer" MRC examples is subsequently defined as the complement set of E.  4  Consequently, as shown in Table  2 , the set U of the complete SQuAD1.1 training examples is split into the set E of 12,487 "easy to answer" examples and the set H of 75,112 "hard to an- a ? = ? i=1,...,N a ? (tsi), ua ? = U ? a ? (? = 1, 2, 3, 4), where the number of examples in each set is provided in Table 1. swer" examples. Figure  4  compares the distributions of positions of answers within the contexts as a ratio of the "start position of an answer" to the "length of context". These results indicate that the answers of the "easy to answer" MRC examples tend to be located near the beginning of the context as compared with those of the "hard to answer" MRC examples.  5  We repeat this splitting procedure ten times and compare the numbers of "easy to answer" and "hard to answer" examples, where we have almost the same results as we report in this section. For examples of the "easy to answer" MRC examples, Figure  5  provides two cases, one of which is a typical "easy to answer" with its answer located exactly at the beginning of the context, and a second as the opposite class with its answer located exactly at the end of the context. 

 Effectiveness of "Hard to Answer" Examples in MRC Model Training We next evaluate the effectiveness of "hard to answer" and "easy to answer" MRC examples based  is used as the test set for each evaluation. For the evaluation measures, we utilize the exact match (EM), which is defined as the rate of examples with a predicted answer that exactly matches the reference answer. The macro average of the F1 score is calculated from the precision and recall between the token sequences of the predicted and reference answers. Also, although we omit the detailed evaluation results, in addition to BERT, we also applied Span-BERT  (Joshi et al., 2020)  7 (base & cased) and XLNet  (Yang et al., 2019)  8 (XLNet-Large, Cased) and obtained the similar results regarding both of 151 (1) the model trained with "hard to answer" examples outperformed that trained with "easy to answer" ones, and (2) answers from the "easy to answer" MRC example class tend to be located around the beginning of the context compared with those from the "hard to answer" MRC example class. 7 Related Work  Swayamdipta et al. (2020)  proposed a general framework of identifying three regions, namely, ambiguous, easy to learn, and hard to learn within a dataset, and applied the framework to several tasks such as natural language inference and sentence-level machine reading comprehension. It is concluded that ambiguous instances are useful for high performance, easy to learn instances are aid optimization, and hard to learn instances correspond to data errors. Following the conclusions of  Swayamdipta et al. (2020) , our future work include applying the framework of  Swayamdipta et al. (2020)  to the tasks of machine reading comprehension studied in this paper and investigating the difference of our notion of "easy to answer" / "hard to answer" and their notion of "easy to learn" / "hard to learn." Among other related work,  Sugawara et al. (2018)  studied splitting 12 MRC datasets into easy and hard subsets according to two types of simple lexical based heuristics and showed that the performance against easy subsets were lower than the whole datasets.  Min et al. (2018)  also studied to select minimal set of sentences within the context of existing MRC datasets to answer the MRC question. In the task of recognizing textual entailment that classifies the relation between a pair of two sentence as a premise and hypothesis, Tsuchiya (2018) compared two of the "Recognizing Tex-tual Entailment" datasets, SICK  (Bowman et al., 2015)  and SNLI  (Marelli et al., 2014) . Tsuchiya reported that the cases of SNLI had the correct textual entailment labels predicted when only the hypothesis sentence was provided and without the premise sentence. However, Tsuchiya (2018) also pointed out that, a hidden bias in the SNLI corpus caused much of the high accuracy achieved by the neural network based models that were trained with SNLI. Developing machine reading comprehension datasets requires an expensive and timeconsuming effort to manually create questions from paragraphs and extract spans of text from each paragraph to represent the answer to each question. The approach of active learning, in which the key idea is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns  (Settles, 1995 (Settles, , 2010 , could be applied to reduce the cost of developing MRC datasets. While there exists no previous study that applies the active learning technique for machine reading comprehension task, other work applied the technique to reduce the cost of developing datasets for other NLP tasks  (Sener and Savarese, 2018; Chen et al., 2019) , image classification  (Beluch et al., 2018; Fang et al., 2017) , as well as other machine learning tasks, such as predicting molecular energetics in the field of chemistry  (Smith et al., 2018) . 

 Conclusion We proposed a method based on BERT  (Devlin et al., 2019)  that splits the training examples from the MRC dataset SQuAD1.1 into classes of "easy to answer" and "hard to answer." Experimental evaluations of comparing the two models, one of which is trained only with the "easy to answer" examples and the other with the "hard to answer" examples, demonstrate that the latter outperformed the former. Future work includes applying the analysis procedure of this paper to several popular MRC benchmark datasets other than SQuAD  (Pranav et al., 2016)  and investigating whether the similar results are obtained. We also work on deeper analysis of the characteristics of "easy to answer" / "hard to answer" examples to find out features that are related to the disparity of training effectiveness. Figure 1 : 1 Figure 1: An MRC Model using Neural Networks 
