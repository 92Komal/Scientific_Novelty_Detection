title
Unsupervised Question Decomposition for Question Answering

abstract
We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, singlehop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HOTPOTQA over a strong baseline on the original, out-ofdomain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using subquestions is promising for shedding light on why a QA system makes a prediction. 1 * KC was a part-time research scientist at Facebook AI Research while working on this paper. 1 Our code, data, and pretrained models are available at https://github.com/facebookresearch/ UnsupervisedDecomposition.

Introduction It has been a long-standing challenge in AI to answer questions of any level of difficulty  (Winograd, 1991) . Question answering (QA) systems struggle to answer complex questions such as "What profession do H. L. Mencken and Albert Camus have in common?" since the required information is scattered in different places  (Yang et al., 2018) . However, QA systems accurately answer  simpler, related questions such as "What profession does H. L. Mencken have?" and "Who was Albert Camus?"  (Petrochuk and Zettlemoyer, 2018) . Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure  1 . This approach leverages strong performance on simple questions to help answer harder questions  (Christiano et al., 2018) . Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions  (Talmor and Berant, 2018; Min et al., 2019b) , which each require significant human effort. For example, DECOMPRC  (Min et al., 2019b)  decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part- Hard Question Simple Question 

 ? Step 1 Step 2 

 Seq2Seq or Step 2 Figure  2 : One-to-N Unsupervised Sequence transduction (ONUS): Step 1: We create a corpus of pseudodecompositions D by finding candidate sub-questions from a simple question corpus S which are similar to a multi-hop question in Q. Step 2: We learn to map multi-hop questions to decompositions using Q and D as training data, via either standard sequence-to-sequence learning (Seq2Seq) or unsupervised sequence-to-sequence learning (for ONUS). of-speech tags and over 50 keywords. Prior work also assumes that sub-questions only consist of words from the question, which is not always true. Decomposing arbitrary questions requires sophisticated natural language generation, which often relies on many, high-quality supervised examples. Instead of using supervision, we find it possible to decompose questions in a fully unsupervised way. We propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map from the distribution of hard questions to that of many simple questions. First, we automatically create a noisy "pseudo-decomposition" for each hard question by using embedding similarity to retrieve sub-question candidates. We mine over 10M possible sub-questions from Common Crawl with a classifier, showcasing the effectiveness of parallel corpus mining, a common approach in machine translation  (Xu and Koehn, 2017; Artetxe and Schwenk, 2019) , for QA. Second, we train a decomposition model on the mined data with unsupervised sequence-to-sequence learning, allowing ONUS to improve over pseudo-decompositions. As a result, we are able to train a large transformer model to generate decompositions, surpassing the fluency of heuristic/extractive decompositions. Figure 2 overviews our approach to decomposition. We validate ONUS on multi-hop QA, where questions require reasoning over multiple pieces of evidence. We use an off-the-shelf single-hop QA model to answer decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for HOTPOTQA, a standard benchmark for multi-hop QA  (Yang et al., 2018) , including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline  Min et al., 2019a ) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from  Min et al. (2019b) , and 11 points on the multi-hop dev set from  Jiang and Bansal (2019a) . Our method is competitive with state-of-the-art methods SAE  (Tu et al., 2020)  and HGN  (Fang et al., 2019)  that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in HOTPOTQA, highlighting the general nature of ONUS over prior work, such as IBM Watson  (Ferrucci et al., 2010)  and DECOMPRC  (Min et al., 2019b) , which decompose different question types separately. Without finetuning, our trained ONUS model can even decompose some questions in visual QA  (Johnson et al., 2017b)  and knowledge-base QA  (Talmor and Berant, 2018) , as well as claims in fact verification  (Thorne et al., 2018) , suggesting promising future avenues in other domains. 

 Method We now formulate the problem and describe our high-level approach, with further details in ?3. The goal of this work is to leverage a QA model that is accurate on simple questions for answering hard questions, without using annotated question decompositions. Here, we consider simple questions to be "single-hop" questions that require reasoning over one paragraph or piece of evidence, and we consider hard questions to be "multi-hop." Our aim is to train a multi-hop QA model M to provide the correct answer a to a multihop question q about a given context c (e.g., several paragraphs). Normally, we would train M to maximize log p M (a|c, q). To facilitate learn-ing, we leverage a single-hop QA model that may be queried with sub-questions s 1 , . . . , s N , whose "sub-answers" a 1 , . . . , a N may be given to M . M may then maximize the potentially easier objective log p M (a|c, q, [s 1 , a 1 ], . . . , [a N , s N ]). Supervised decomposition models learn to map each question q ? Q to a decomposition d = [s 1 ; . . . ; s N ] of N sub-questions s n ? S using annotated (q, d) examples. In this work, we do not assume access to strong (q, d) supervision. To leverage the single-hop QA model without supervision, we follow a three-stage approach: 1) map a question q into sub-questions s 1 , . . . , s N via unsupervised techniques, 2) find sub-answers a 1 , . . . , a N with the single-hop QA model, and 3) use s 1 , . . . , s N and a 1 , . . . , a N to predict a. 

 Unsupervised Question Decomposition To train an unsupervised decomposition model, we need suitable data. We assume access to a hard question corpus Q and simple question corpus S. Instead of using supervised (q, d) examples, we design an algorithm that creates pseudodecompositions d to form (q, d ) pairs from Q and S using an unsupervised method ( ?2.1.1). We then train a model to map q to a decomposition. We explore learning to decompose with standard and unsupervised sequence-to-sequence learning ( ?2.1.2). 

 Creating Pseudo-Decompositions Inspired by  Zhou et al. (2015)  in question retrieval, we create a pseudo-decomposition set d = {s 1 ; . . . ; s N } for each q ? Q by retrieving simple question s i from S. We concatenate s 1 ; . . . ; s N to form d used downstream. N may potentially vary based on q. To retrieve useful simple questions for answering q, we face a joint optimization problem. We want sub-questions that are both (i) similar to q according to a metric f (first term) and (ii) maximally diverse (second term), so our objective is: argmax d ?S s i ?d f (q, s i ) ? s i ,s j ?d ,i =j f (s i , s j ) (1) 

 Learning to Decompose With the above pseudo-decompositions, we explore various decomposition methods (details in ?3.2.3): PseudoD We use sub-questions from pseudodecompositions directly in downstream QA. Sequence-to-Sequence (Seq2Seq) We train a Seq2Seq model p ? to maximize log p ? (d |q). One-to-N Unsupervised Sequence transduction (ONUS) We use unsupervised learning to map one question to N sub-questions. We start with paired (q, d ) but do not learn from the pairing because it is noisy. Instead, we use unsupervised Seq2Seq methods to learn a q ? d mapping. 

 Answering Sub-Questions To answer the generated sub-questions, we use an off-the-shelf QA model. The QA model may answer sub-questions using any free-form text (i.e., a word, phrase, sentence, etc.). Any QA model is suitable, so long as it can accurately answer simple questions in S. We thus leverage good accuracy on questions in S to help answer questions in Q. 

 Learning to Recompose Downstream QA systems may use sub-questions and sub-answers in various ways. We train a recomposition model to combine the decomposed sub-questions/answers into a final answer, when also given the original input (context+question). 

 Experimental Setup We now detail the implementation of our approach. 

 Question Answering Task We test ONUS on HOTPOTQA, a standard multihop QA benchmark. Questions require information from two distinct Wikipedia paragraphs to answer ("Who is older, Annie Morton or Terry Richardson?"). For each question, HOTPOTQA provides 10 context paragraphs from Wikipedia. Two paragraphs contain question-relevant sentences called "supporting facts," and the remaining paragraphs are irrelevant, "distractor paragraphs." Answers in HOTPOTQA are either yes, no, or a text span in an input paragraph. Accuracy is measured with F1 word overlap and Exact Match (EM) between predicted and gold spans. 

 Unsupervised Decomposition 

 Training Data and Question Mining Supervised decomposition methods are limited by the amount of available human annotation, but our unsupervised method faces no such limitation, similar to unsupervised QA . Since we need to train data-hungry Seq2Seq models, we would benefit from large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard question. Thus, we take inspiration from parallel corpus mining in machine translation  (Xu and Koehn, 2017; Artetxe and Schwenk, 2019) . We use questions from SQUAD 2 and HOTPOTQA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with "wh"-words or end in "?" Next, we train an efficient, FastText classifier  (Joulin et al., 2017)  to classify between questions sampled from Common Crawl, SQUAD 2, and HOTPOTQA (60K in total). Then, we classify our Common Crawl questions, adding those classified as SQUAD 2 questions to S and those classified as HOTPOTQA questions to Q. Mining greatly increases the number of single-hop questions (130K ? 10.1M) and multi-hop questions (90K ? 2.4M), showing the power of parallel corpus mining in QA. 2 

 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N per question (Appendix ?A.1), but we found similar QA results with a fixed N = 2, which we use in the remainder for simplicity. 

 Similarity-based Retrieval To retrieve relevant sub-questions, we embed any text t into a vector v t by summing the FastText vectors  (Bojanowski et al., 2017)  3 for words in t and use cosine as our similarity metric f . 4 Let q be a multi-hop question with a pseudo-decomposition (s * 1 , s * 2 ) and v be the unit vector of v. Since N = 2, Eq. 1 simplifies to: (s * 1 , s * 2 ) = argmax {s 1 ,s 2 }?S v q vs 1 + v q vs 2 ? v s 1 vs 2 The last term requires O(|S| 2 ) comparisons, which is expensive as |S| > 10M. Instead of solving the above equation exactly, we find an approximate pseudo-decomposition (s 1 , s 2 ) by computing over S = topK {s?S} v q vs with K = 1000. We efficiently build S with FAISS  (Johnson et al., 2017a) . Random Retrieval For comparison, we test a random pseudo-decomposition baseline, where we retrieve s 1 , . . . , s N by sampling uniformly from S. Editing Pseudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g.,  Guu et al., 2018) , we replace each sub-question entity not in q with an entity from q of the same type (e.g., "Date" or "Location") if possible. 5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 

 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods  (Artetxe et al., 2018; Lample et al., 2018) , so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (  2019 ), a 12-block transformer  (Vaswani et al., 2017) . We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix ?B.2 for details. 

 Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU between generated decompositions and pseudo-decompositions. ONUS We finetune the pretrained encoderdecoder with back-translation  (Sennrich et al., 2016)  and denoising objectives simultaneously, similar to  Lample and Conneau (2019)  in unsupervised one-to-one translation.  6  For denoising, we produce a noisy input d by randomly masking, dropping, and locally shuffling tokens in d ? D, and we train a model with parameters ? to maximize log p ? (d|d ). We likewise maximize log p ? (q|q ) for a noised version q of q ? Q. For back-translation, we generate a multihop question q for a decomposition d ? D, and we maximize log p ? (d|q). Similarly, we maximize log p ? (q| d) for a model-generated decomposition d of q ? Q. We train on HOTPOTQA questions Q and their pseudo-decompositions D. 7 

 Single-hop Question Answering Model We finetune a pretrained model for single-hop QA following prior work from  Min et al. (2019b)  on HOTPOTQA, as described below. 8 Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict "no answer" otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in  Nie et al. (2019)  on HOT-POTQA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (  2018 ), we subtract a paragraph's "no answer" logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(s p ) of each span s p in a paragraph p ? {1, . . . , P } using the predicted span logit l(s p ) and "no answer" paragraph logit n(p) with p(s p ) ? e l(sp)?n(p) . ROBERTA LARGE  is used as our pretrained model. Training Data and Ensembling Similar to  Min et al. (2019b) , we train an ensemble of 2 single-hop QA models on SQUAD 2 and the "easy" (singlehop) subset of HOTPOTQA (see Appendix ?C for training details). We average model logits before predicting the answer. We use the single-hop QA ensemble as a black-box model once trained, never training the model on multi-hop questions. Returned Text Instead of returning only the predicted sub-answer span to the recomposition model, we return the sentence that contains the predicted sub-answer, which is more informative. 

 Recomposition Model Our recomposition model architecture is identical to the single-hop QA model, but the recomposition model also uses sub-questions and sub-answers as input. We append each (sub-question, sub-answer) pair to the question with separator tokens. We train one recomposition model on all of HOTPOTQA, also including SQUAD 2 examples used to train the single-hop QA model. All reported error margins show the mean and std. dev. across 5 recomposition training runs using the same decompositions.  

 Results on Question Answering We compare variants of our approach that use different learning methods and different pseudodecomposition training sets. As a baseline, we compare ROBERTA with decompositions to ROBERTA without decompositions. We use the best hyperparameters for the baseline to train our ROBERTA models with decompositions (see Appendix ?D.3 for hyperparameters). We report results on 3 dev set versions: (1) the original version,  9  (2) the multi-hop version from  Jiang and Bansal (2019a)  who created some distractor paragraphs adversarially to test multi-hop reasoning, and (3) the out-of-domain (OOD) version from  Min et al. (2019b)  who retrieved distractor paragraphs with the same procedure as the original version but excluded the original paragraphs. 

 Main Results Table  1  shows how unsupervised decompositions affect QA. Our ROBERTA baseline does quite well on HOTPOTQA (77.0 F1), in line with  Min et al. (2019a)  who achieved strong results using a BERT-based version of the model  (Devlin et al., 2019) . We achieve large gains over the ROBERTA baseline by simply adding sub-questions and sub-answers to the input. Using decompositions from ONUS trained on FastText pseudo-decompositions, we find a gain of 3.1 F1 on the original dev set, 11 F1 on multi-hop dev, and 10 F1 on OOD dev. ONUS decompositions even match the performance of using supervised and heuristic decompositions from DECOMPRC (i.e., 80.1 vs. 79.8 F1 on the original dev set). Pseudo-decomposition and ONUS training both contribute to decomposition quality. FastText pseudo-decompositions themselves provide an improvement in QA over the baseline (e.g., 72.0 vs. 67.1 F1 on OOD dev) and over random pseudo-decompositions (70.7 F1), validating our retrieval-based algorithm for creating pseudodecompositions. Seq2Seq trained on FastText pseudo-decompositions achieves comparable gains to FastText pseudo-decompositions (73.0 F1 on OOD dev), validating the quality of pseudodecompositions as training data. As hypothesized, ONUS improves over PseudoD and Seq2Seq by learning to align hard questions and pseudodecompositions while ignoring the noisy pairing (77.1 F1 on OOD dev). ONUS is relatively robust to the training data used but still improves further by using FastText vs. Random pseudodecompositions (77.1 vs. 76.5 F1 on OOD dev). We submitted the best QA approach based on dev evaluation (using ONUS trained on FastText pseudo-decompositions) for hidden test evaluation. We achieved a test F1 of 79.34 and Exact Match (EM) of 66.33. Our approach is competitive with state-of-the-art systems SAE  (Tu et al., 2020)  and HGN  (Fang et al., 2019) , which both (unlike us) learn from strong, supporting-fact supervision about which sentences are relevant to the question. 

 Question Type Breakdown To understand where decompositions help, we break down QA accuracy across 4 question types Figure  3 : Multi-hop QA is better when the single-hop QA model answers with the ground truth "supporting fact" sentences. We plot mean and std. over 5 QA runs. from  Min et al. (2019b) . "Bridge" questions ask about an entity not explicitly mentioned ("When was Erik Watts' father born?"). "Intersection" questions ask to find an entity that satisfies multiple separate conditions ("Who was on CNBC and Fox News?"). "Comparison" questions ask to compare a property of two entities ("Which is taller, Momhil Sar or K2?"). "Single-hop" questions are answerable using single-hop shortcuts or single-paragraph reasoning ("Where is Electric Six from?"). We split the original dev set into the 4 types using the supervised type classifier from  Min et al. (2019b) . Table  2  (left) shows F1 scores for ROBERTA with and without decompositions across the 4 types. ONUS decompositions improve QA across all types. Our single decomposition model does not need to be tailored to the question type, unlike  Min et al. (2019b)  who use a different model per question type. For single-hop questions, our QA approach does not require falling back to a single-hop QA model and instead learns to leverage decompositions in that case also (76.9 vs. 73.9 F1). 

 Answers to Sub-Questions are Crucial To measure the usefulness of sub-questions and sub-answers, we train the recomposition model with various, ablated inputs, as shown in Table  2  (right). Sub-answers are crucial to improving QA, as sub-questions with no answers or random answers do not help (76.9 vs. 77.0 F1 for the baseline). Only when sub-answers are provided do we see improved QA, with or without sub-questions (80.1 and 80.2 F1, respectively). It is important to provide the sentence containing the predicted answer span instead of the answer span alone (80.1 vs. 77.8 F1, respectively), though the answer span alone still improves over the baseline (77.0 F1). 

 How Do Decompositions Help? Decompositions help by retrieving important supporting evidence to answer questions. Fig.  3  shows that QA improves when the sub-answer sentences are gold "supporting facts." We retrieve these without relying on strong, supporting fact supervision, unlike many state-of-the-art models  (Tu et al., 2020; Fang et al., 2019; Nie et al., 2019) . 10 

 Example Decompositions To illustrate how decompositions help, Table  3  shows example sub-questions from ONUS with predicted sub-answers. Sub-questions are singlehop questions relevant to the multi-hop question. The single-hop QA model returns relevant subanswers, sometimes despite under-specified (Q2, SQ 1 ) or otherwise imperfect sub-questions (Q3, SQ 1 ). The recomposition model returns an answer consistent with the sub-answers. Furthermore, the sub-answers used for QA are in natural language, adding a level of interpretability to otherwise black-box, neural QA models. Decompositions are largely extractive, copying from the multi-  

 Analysis To better understand our system, we now analyze our pipeline by examining the model for each stage: decomposition, single-hop QA, and recomposition. 

 Unsupervised Decomposition Model Intrinsic Evaluation of Decompositions We evaluate the quality of decompositions on other metrics aside from downstream QA. To measure the fluency of decompositions, we compute the likelihood of decompositions using the pretrained GPT-2 language model  (Radford et al., 2019) . We train a BERT BASE classifier on the questionwellformedness dataset of  Faruqui and Das (2018) , and we use the classifier to estimate the proportion of sub-questions that are well-formed. We measure how abstractive decompositions are by computing (i) the token Levenstein distance between the multihop question and its generated decomposition and (ii) the ratio between the length of the decomposition and the length of the multi-hop question. We compare ONUS to DECOMPRC  (Min et al., 2019b) , a supervised+heuristic decomposition method. As shown in Table  4 , ONUS decompositions are more natural and well-formed than DECOMPRC decompositions. As an example, for Table  3  Q3, DECOMPRC produces the sub-questions "Is Coldplay from which country?" and "Is Pierre Bouvier from which country?" ONUS decompositions are also closer in edit distance and length to the multihop question, consistent with our observation that our decomposition model is largely extractive. 

 Quality of Decomposition Model A welltrained decomposition model should place higher probability on decompositions that are more helpful for QA. We generate N = 5 hypotheses from our best decomposition model using beam search, and we train a recomposition model to use the n th -ranked hypothesis as a question decomposition (Figure  4 , left). QA accuracy decreases as we use lower probability decompositions, but accuracy remains relatively robust, at most decreasing from 80.1 to 79.3 F1. The limited drop suggests that decompositions are still useful if they are among the model's top hypotheses, another indication that ONUS is trained well for decomposition. 

 Single-hop Question Answering Model Sub-Answer Confidence Figure  4  (right) shows that the single-hop model's sub-answer confidence correlates with downstream multi-hop QA accuracy on all dev sets. A low confidence sub-answer may be indicative of (i) an unanswerable or ill-formed sub-question or (ii) a sub-answer that is more likely to be incorrect. In both cases, the single-hop QA model is less likely to retrieve useful supporting evidence for answering the multi-hop question. Changing the Single-hop QA Model We find that our approach is robust to the single-hop QA model used. We test the BERT BASE ensemble from  Min et al. (2019b)   HOTPOTQA itself  (56.3 vs. 66.7 F1) . However, the model results in similar QA when used to answer single-hop sub-questions within our larger system (79.9 vs. 80.1 F1 for our ensemble). 

 Recomposition Model Varying the Base Model To understand how decompositions impact performance as the recomposition model gets stronger, we vary the base pretrained model.  

 Related Work Answering complex questions has been a longstanding challenge in natural language processing. Prior work explored decomposing questions with supervision and heuristic algorithms. IBM Watson  (Ferrucci et al., 2010)  decomposes questions into sub-questions in multiple ways or not at all. DECOMPRC  (Min et al., 2019b)  largely frames subquestions as extractive spans of a question, learning to predict span-based sub-questions via supervised learning on human annotations. In other cases, DE-COMPRC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and DE-COMPRC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language  (Talmor and Berant, 2018; Wolfson et al., 2020) . Examples include classical QA systems like SHRDLU  (Winograd, 1972)  and LUNAR  (Woods et al., 1974) , as well as neural Seq2Seq semantic parsers  (Dong and Lapata, 2016)  and neural module networks  (Andreas et al., 2015 (Andreas et al., , 2016 . Such methods usually require strong, program-level supervision to generate programs, as in visual QA  (Johnson et al., 2017c)  and on HOTPOTQA  (Jiang and Bansal, 2019b) . Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by HOTPOTQA. Such an approach is taken by SAE  (Tu et al., 2020)  and HGN  (Fang et al., 2019) , whose methods may be combined with ours. Unsupervised decomposition complements strongly and weakly supervised decomposition approaches. Our unsupervised approach enables methods to leverage millions of otherwise unusable questions, similar to work on unsupervised QA . When decomposition examples exist, supervised and unsupervised learning can be used in tandem to learn from both labeled and unlabeled examples. Such semi-supervised methods outperform supervised learning for tasks like machine translation  (Sennrich et al., 2016) . Other work on weakly supervised question generation uses a downstream QA model's accuracy as a signal for learning to generate useful questions. Weakly supervised question generation often uses reinforcement learning  (Nogueira and Cho, 2017; Wang and Lake, 2019; Strub et al., 2017; Das et al., 2017; , where an unsupervised initialization can greatly mitigate the issues of exploring from scratch  (Jaderberg et al., 2017) . 

 Conclusion We proposed a QA system that answers a question via decomposition, without supervised question decompositions, using three stages: (1) decompose a question into many sub-questions using One-to-N Unsupervised Sequence transduction (ONUS), (2) answer sub-questions with an off-the-shelf QA system, and (3) recompose sub-answers into a final answer. When evaluated on three HOTPOTQA dev sets, our approach significantly improved QA over an equivalent model that did not use decompositions. Our approach relies only on the final answer as supervision but works as effectively as state-ofthe-art methods that rely on much stronger supervision, such as supporting fact labels or example decompositions. We found that ONUS generates fluent sub-questions whose answers often match the gold-annotated, question-relevant text. Overall, this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems. 
