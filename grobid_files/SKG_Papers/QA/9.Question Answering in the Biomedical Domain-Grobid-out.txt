title
Question Answering in the Biomedical Domain

abstract
Question answering techniques have mainly been investigated in open domains. However, there are particular challenges in extending these open-domain techniques to extend into the biomedical domain. Question answering focusing on patients is less studied. We find that there are some challenges in patient question answering such as limited annotated data, lexical gap and quality of answer spans. We aim to address some of these gaps by extending and developing upon the literature to design a question answering system that can decide on the most appropriate answers for patients attempting to self-diagnose while including the ability to abstain from answering when confidence is low.

Introduction Question Answering (QA) is the downstream task of information seeking wherein a user presents a question in natural language, Q, and a system finds an answer or a set of answers from a collection of natural language documents or knowledge bases  (Lende and Raghuwanshi, 2016) , A, that satisfies the user's question  (Molla and Gonzlez, 2007) . Questions fall into one of two categories: factoid and non-factoid. Factoid QA provides brief facts to the users' questions; for example, Question: What day is it? Answer: Monday. Nonfactoid question answering is a more complex task. It involves answering questions that require specific knowledge, common sense or a procedure due to ambiguity or the scope of the question. An example from the Yahoo non-factoid question answer dataset 1 illustrates this: Question: Why is it considered unlucky to open an umbrella indoors?. The answer is not apparent and requires specific knowledge about cultural superstitions. 1 https://ciir.cs.umass.edu/downloads/nfL6/ Question answering is fundamental in highlevel tools such as chatbots  (Qiu et al., 2017; Yan et al., 2016; Amato et al., 2017; Ram et al., 2018) , search engines  (Kadam et al., 2015) , and virtual assistants  (Yaghoubzadeh and Kopp, 2012; Austerjost et al., 2018; Bradley et al., 2018) . However, being a downstream task, question answering suffers from pipeline error, as it often relies on the quality of several upstream tasks such as coreference resolution  (Vicedo and Ferr?ndez, 2000) , anaphora resolution  (Ram et al., 2018) , named entity recognition  (Aliod et al., 2006) , information retrieval  (Mao et al., 2014) , and tokenisation  (Devlin et al., 2019) . Thus, there has been a growing demand for these QA systems to deliver precise questionspecific answers  (Pudaruth et al., 2016)  and consequently has sparked much research into improving upon relevant natural language processing approaches  (Malik et al., 2013) , datasets  (Rajpurkar et al., 2016; Kocisk? et al., 2017)  and information retrieval techniques  (Weienborn et al., 2013; Mao et al., 2014) . These improvements have allowed the domain to evolve from shallow keyword matching to contextual and semantic retrieval systems  (Kadam et al., 2015) . However, most of these techniques have been focused on the opendomain  (Soares and Parreiras, 2018)  and the challenges harbouring the biomedical domain have not been well addressed and remain unsolved. Here, we define biomedical QA as either factoid or nonfactoid QA on biomedical literature. One such challenge is due to the creation of complex medical queries which require expert knowledge and up to four hours per query  (Russell-Rose and Chamberlain, 2017 ) to adequately answer. This requirement of expert knowledge leads to a lack of high-quality, publicly available biomedical QA datasets. Furthermore, medical datasets tend to be locked behind ethical, obligatory agreements and are usually small due to cost constraints and lack of domain experts for annotation  (Pampari et al., 2018; Shen et al., 2018) . Therefore, open-domain techniques which assume data-rich conditions are not suitable for direct application to the biomedical domain. Another challenge is clinical term ambiguity, which is due to the temporally and spatially varying nature of clinical terminology, and the frequent use of abbreviation and esoteric medical terminology  (see Table  1  for examples). It is difficult for systems to adequately disambiguate clinical words to be used in downstream QA systems due to the complexity of the ambiguity of medical terminology, such as abbreviations, due to their varying contexts. Though there are existing tools such as MetaMap  (Aronson and Lang, 2010)  to disambiguate these terms by mapping them to the UMLS (Unified Medical Language System) metathesaurus, coverage of these systems is low and mappings are often inaccurate  (Wu et al., 2012) . Furthermore, systems in the open-domain typically retrieve a long answer before extracting a short continuous span of text to present to the user  (Soares and Parreiras, 2018; Rajpurkar et al., 2016) . However, for biomedical responses, it is not always sufficient to retrieve short answer continuous spans, and Answer Evidence spans that are discontinuous that cross the sentence boundary are often required  (Pampari et al., 2018; Hunter and Cohen, 2006; Nentidis et al., 2018) . These problems are not yet solved in the biomedical domain and are reflected in the BioASQ challenge  (Nentidis et al., 2018) , an annual challenge with a biomedical question answering track. Currently, the state-of-the-art systems do not perform much better than random guess with an accuracy of 66.67% for binary question answering  (Chandu et al., 2017) , 24.24% for factoid (ranked list of named entities as answers) and an F1-score of 0.3312 for list-type (unranked list of named entities)  (Peng et al., 2015)  suggesting that there is much room for improvement in terms of algorithms and research. Furthermore, we found that there is a lack of a biomedical question answering system directed for patients. Biomedical question answering for patients is important as studies from the Pew Research Centre have shown that 35% of U.S. adults have diagnosed themselves using the information they found online 2 . Of these adults, 35% said that they did not get a professional opinion on their self-diagnosis, illustrating that patients may blindly trust the results of search engines without consulting a medical professional. This is cause for concern, as search engines tend to display the most severe ailments first which could lead to a potential waste of hospital resources or deterioration in patient health  (Korfage et al., 2006) . Furthermore, although there are negatives to searching symptoms via search engine, for the participants who visited doctors after self-diagnosis, research has revealed that doctor-patient relationships and patient compliance with treatment improve as the patients have a clearer understanding of their symptoms and potential disease after selfdiagnosis  (Cocco et al., 2018) . These studies motivate the need for a strong biomedical question answering question for patients as it will benefit patients who self-diagnose and patients who seek medical advice after looking up their symptoms online. Finally, we highlight that there is a lexical and semantic gap between clinical and patient language. For example, the expression "hole in lung" taken literally is about a punctured lung. However, this colloquialism refers to the condition known as Pleurisy (Ben Abacha and Demner-Fushman, 2019; Abacha and Demner-Fushman, 2016), illustrating that patients do not have the level of literacy to formulate complex medical queries nor understand them  (Graham and Brookey, 2008) . We aim to address the challenges in applying question answering to biomedical question answering for patients. We highlight that the current gaps of biomedical QA research stem from lack of clinical disambiguation tools, lack of highquality data, the quality of answer spans, weak algorithms and clinical-patient lexical gaps. Our goal is to present a patient biomedical QA system that can address the gaps in biomedical research and allows a patient to query their symptoms, diseases or available treatment options accurately, but will also abstain from providing answers in cases where there is low confidence in the best answer, question malformation or insufficiency of data to answer the question.  

 Literature Review Here, we detail a review of question answering in the open and biomedical domains. 

 Information Retrieval Approaches Biomedical QA systems up until 2015 relied heavily on Information Retrieval (IR) techniques such as tf-idf ranking  (Lee et al., 2006)  and entity extraction tools such as MetaMap  (Aronson and Lang, 2010)  in order to obtain candidate answers (by querying biomedical databases) and feature extraction before using machine learning models such as logistic regression  (Weienborn et al., 2013) . While other techniques included using cosine similarity between one-hot encoded vectors of answer and question for candidate reranking  (Mao et al., 2014) . However, these techniques were inherently bag-of-word approaches that ignored the context of words. Furthermore, these techniques relied on complete matches of question terms and answer paragraphs, which is not realistic in practice. Patients use different terminology to that of medical experts and biomedical literature  (Graham and Brookey, 2008) . In more recent years, more neural approaches to IR have been used in the biomedical space  (Nentidis et al., 2017 (Nentidis et al., , 2018 ) such as Position-Aware Convolutional Recurrent Relevance Matching  (Hui et al., 2017) , Deep Relevance Matching Model  (Guo et al., 2017)  and Attention Based Convolutional Neural Network  (Yin et al., 2015) . However, though these approaches do not rely on complete matching of words and capture semantics, they either ignore local or global contexts which are useful for disambiguation of clinical terminology and comprehension  (McDonald et al., 2018) . 

 Semantic-level Approach QA requires the retrieval of long answers before summarisation or retrieval of answer spans.  Punyakanok et al. (2004)  introduced the use of a question's dependency trees and candidate answers' dependency trees and aligning with the Tree Edit Distance metric to augment statistical classifiers such as Logistic Regression and Conditional Random Fields. However, these methods failed to capture complex semantic information due to a reliance on effective part-of-speech tagging and were not attractive end-to-end solutions. Otherwise, WordNet was utilised to extract semantic relationships and estimate semantic distances between answers and questions  (Terol et al., 2007) . However, WordNet suffered from being open-domain focused and also was not able to capture complex semantic information such as polysemy  (Molla and Gonzlez, 2007) . 

 Neural Approaches In recent years, approaches that use neural networks have become popular. Word embedding techniques such as Word2vec and GloVe can model the latent semantic distribution of language through unsupervised learning  (Chiu et al., 2016) . Furthermore, they are quickly adopted into neural networks as these models take fixed-sized vector inputs, where embeddings could be used as encoded inputs into neural networks such as LSTM  (Hochreiter and Schmidhuber, 1997)  and  CNN (LeCun et al., 1999)  in the biomedical domain  (Nentidis et al., 2017 (Nentidis et al., , 2018 . Though these embedding techniques were useful in capturing latent semantics, they did not distinguish between multiple meanings of clinical text  (Molla and Gonzlez, 2007; Vine et al., 2015) . There have been several solutions to this prob-lem  (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019)  proposed but they are not relevant specifically to the biomedical domain. Instead, we highlight BioBERT , a biomedical version of BERT  (Devlin et al., 2019)  which is a deeply bidirectional transformer  (Vaswani et al., 2017)  that is able to incorporate rich context into the encoding or embedding process that has pre-trained on the Wikipedia and PubMed corpora. However, this model fails to account for the spatial and temporal aspects of diseases in biomedical literature as temporality is not encoded into its input. Furthermore, Biobert uses a WordPiece tokeniser  (Wu et al., 2016)  which keeps a fixed-size vocabulary dictionary for learning new words. However, the vocabulary within the model is derived from Wikipedia, a general domain corpus, and thus Biobert is unable to learn distinct morphological semantics of medical terms like -phobia, where '-' denotes suffixation, meaning fear as it only has the internal representation for -bia. 

 Research Plan We list the research questions to address some of the research gaps in biomedical QA and the system we aim to design, alongside baseline approaches and methodology as starting points. We will also mention future directions to address these research questions. RQ1: What are the limitations of current biomedical QA? The limitations in current biomedical QA include the lack of: sufficient ambiguity resolution tools  (Wu et al., 2012) , robust techniques to using semantic neural approaches  Nentidis et al., 2018) . The lack of strong comprehension from systems to produce sufficient answer spans that cross the sentence boundary as reflected by poor results in ideal answer production in BioASQ  (Nentidis et al., 2018 (Nentidis et al., , 2017  and addressing issues using real-world patient queries rather than artificially curated queries  (Pampari et al., 2018; Guo et al., 2006)  which contain colloquial ambiguous nonmedical terminology such as hole in lung. In our research, we aim to address each of these gaps by researching into: higher coverage clinical ambiguity tools that use contexts in the spatial and temporal domains, summarisation techniques that can translate from biomedical terminology to patient language  (Mishra et al., 2014; Shi et al., 2018)  and tuning biomedical models to solve complex answer span tasks that cross sentence boundaries  (Kocisk? et al., 2017)  or require common sense  (Talmor et al., 2018) . RQ2: Data-driven approaches require highquality datasets. How can we construct or leverage existing datasets to mimic real-world biomedical question answering? By leveraging existing techniques such as variational autoencoder  (Shen et al., 2018)  and Snorkel  (Bach et al., 2018) , we will be able to generate, label and process additional data that can meet stringent data requirements of neural approaches. However, synthetic datasets generally perform weaker than handcrafted datasets  (Bach et al., 2018) . In order to bridge this gap in the research, we propose augmenting these data generation methods via crowd-sourcing methods with textual entailment  (Abacha and Demner-Fushman, 2016)  and natural language inference  (Johnson et al., 2016)  to improve the quality of the generated labels and data. For instance, we can use forums like Quora or medical specific forums such as Health24 3 and utilise techniques such as question entailment to find questions that are related to ones seen in the dataset in order to generate higher-quality annotated labels. We will then develop techniques that can combine synthetic and higher-quality labelled datasets that can be utilized downstream in a QA system. We will compare this against baselines such as majority voting and Snorkel to evaluate our approaches. Allowing the model to abstain from a decision, through comprehension, has been the focus of many datasets as of late  (Rajpurkar et al., 2016; Kocisk? et al., 2017) . We can use these datasets as a starting problem to solve before applying these techniques to the biomedical domain. However, we will also develop and research further techniques in order to allow for improved confidence and low uncertainty from the model. RQ3: How do we indicate the confidence of the answer that the model has provided? Often researchers interpret softmax or confidence scores from the classifier models as direct correlations to probability but often forget about uncertainties in this measurement  (Kendall and Gal, 2017) . Due to the real-world application and sensitivity of pre-dictions in a health-based QA system, there needs to be guarantees that predictions are of both high accuracy and low uncertainty. In order to account for uncertainty, techniques such as Inductive Conformal Prediction (Papadopoulos, 2008) and Deep Bayesian Learning  (Siddhant and Lipton, 2018)  can be used to model epistemic uncertainty, which is not inherently captured by the model during training, in order to make the loss function more robust to noise and uncertainty and thereby strengthen the predictions of the model. This would then allow softmax scores to be used as confidence scores within a reasonable level of uncertainty. RQ4: How do we include temporality or locality of diseases into answers? Diseases are non-static, they evolve such as the flu or are seasonal such as the summer cold. Current models utilise only static vector inputs, such as word embeddings, that do not account for this temporal aspect of the input. Furthermore, though diseases are non-static, they may be more likely in different countries as there is a spatiotemporal relationship where countries will experience different seasons and thus different diseases. In order to accommodate for these relationships, we can draw on prior research as starting points such as space-time local embeddings  (Sun et al., 2015) , dynamic word embeddings  (Bamler and Mandt, 2017)  or timeembeddings  (Barbieri et al., 2018)  as baselines and extend them into the biomedical setting. RQ5: How do we bridge the semantic gap between clinical text and terminology that a patient can understand? Most patients lack the expertise in utilising resources such as biomedical literature in order to self-diagnose. Therefore, knowledge or answers should be presented in a form that they can understand  (Graham and Brookey, 2008) . Biomedical language and patient language can be construed as two separate languages as biomedical language changes and evolves over time  (Yan and Zhu, 2018)  and also pose the same problems  (Hunter and Cohen, 2006) . Therefore, we can model this problem as a language translation problem and thus can use techniques in neural machine translation  (Qi et al., 2018; Chousa et al., 2018)  based on word embeddings. However, as biomedical language and patient English are primarily borne of the same language, this poses unique problems. For instance, a token in plain English may translate to several tokens in the biomedical space or vice versa. This is known as the alignment problem  (Qi et al., 2018) . We can potentially remedy this by borrowing ideas from n-gram embedding  (Zhao et al., 2017)  as a starting point or using Biobert  projected to a dual-language embedding space and use attention to produce the alignment. Furthermore, there are biomedical abbreviations that need to be disambiguated before translation  (Festag and Spreckelsen, 2017) , for which we would use direct, rulebased approaches using thesauri or tools such as Metamap  (Aronson and Lang, 2010)  as our baseline approaches and extend upon using data-driven approaches  (Wu et al., 2017) . 

 Experimental Framework 

 Datasets High-quality data is required to address the challenges we outlined. We therefore consider the following datasets: (1) MEDNLI  (Johnson et al., 2016; Goldberger et al., 2000)  for medical language inference; (2) i2b2 in the form of em-rQA  (Pampari et al., 2018)  for synthetic questionanswer pairs; (3) SQuAD  (Rajpurkar et al., 2016)  for open-domain transfer learning; (4) the question-answering datasets provided on MediQA 2019 4 ; (5) the question entailment dataset and MedQuAD (Ben Abacha and Demner-Fushman, 2019); (5) CLEF eHealth  (Suominen et al., 2018)  to utilize and evaluate IR methods; and (6) we will supplement our datasets by generating labels for unlabelled data by leveraging the signals from the labelled datasets through the use of tools such as Snorkel  (Bach et al., 2018)  and CVAE  (Shen et al., 2018) . 

 Evaluation Metrics In our experiments, we will evaluate our summarisation strategies with metrics such as ROGUE  (Lin, 2004) , in particular, rogue-2  (Owczarzak and Dang, 2009)  and BLEU  (Papineni et al., 2002) . For question-answering, we use standard ranking metrics such as Mean Average Precision and Mean Reciprocal Rank for evaluating candidate ranking and standard metrics such as f1-score, Precision, Accuracy and more medical targeted metrics such as sensitivity and specificity  (Parikh et al., 2008) . 

 Proposed Framework From the research questions mentioned, we propose a framework to unify their solutions. Embeddings To begin, we need to construct our date/seasonal embeddings  (Barbieri et al., 2018) , to do this, we will need datasets that have mentions of the seasonality and locality of disease entities. Also, we will require embeddings that are representative of the text, we will consider state-of-theart word-level context sensitive embeddings  Peters et al., 2018)  and word-level context insensitive embeddings  (Chiu et al., 2016)  and ensure they properly represent the biomedical datasets. For instance, BERT will need to pretrained with a biomedical vocabulary rather than a general purpose open-domain one, and, in doing so, we will be able to resolve ambiguity in polysemy or abbreviations. Furthermore, we will also be researching methodologies to handle out-of-vocabulary words as the current WordPiece tokenization  (Devlin et al., 2019)  or character-level embeddings  (Barbieri et al., 2018)  would not be sufficient to address esoteric terminology . The time embeddings and the word-level embeddings will be concatenated and used as input to the model. Model Architecture Given the success of multitask learning  (Zhao et al., 2018; Liu et al., 2019) , and having been proposed as the blocking task in NLP  (McCann et al., 2018 ) that needs to be solved. We therefore apply multi-task learning to this problem. From the state of the art multi-task learning models, we borrow the fundamental building blocks such as multi-headed self-attention  (Liu et al., 2019)  and multi-pointer generation  (Mc-Cann et al., 2018)  to be used as decisions in a Neural Architecture Search (NAS) (Zoph and Le, 2016). NAS will use reinforcement learning techniques to find a suitable architecture for multi-task learning. We elect to find the architecture to represent our problem this way due to one main reason. The reason is that the field of deep learning in NLP is quickly changing, and thus the stateof-the-art techniques will always change. Therefore, by having a tool that builds architectures from the building blocks of state-of-the-art models is vital. However, crucially, we must add Heteroscedastic Aleatoric Uncertainty and Epistemic Uncertainty minimisation to the model by adjusting the loss function and weight distribution which will allow the model to be more certain about decisions  (Kendall and Gal, 2017) . One such decision must be the ability to abstain from answering. Concretely, we use NAS to discover models for NMT from clinical text to the patient language by conditioning to an encoder-decoder structure. From here, using this model a starting point, NAS will add task-specific layers that will minimise the joint loss over the biomedical tasks such as question answering  (Nentidis et al., 2018) , question entailment  (Abacha and Demner-Fushman, 2016)  and natural language inference  (Johnson et al., 2016) . In doing so, multi-task learning will allow for stronger generalisability and end-to-end training  (McCann et al., 2018; Liu et al., 2019) . 

 Summary We highlight gaps within the literature in question answering in the biomedical domain. We outline challenges associated with implementing these systems due to the limitations of current work: lack of annotated data, ambiguity in clinical text and lack of comprehension of question/answer text by models. We motivate this research in the area of patient QA due to the high volume of medical queries in search engines that are trusted by patients. Our research aims to build upon the strengths of the current state-of-the-art and research new strategies in solving technical challenges to support a patient in retrieving the answers they require with low uncertainty and high confidence. Table 1 : 1 may mean hazard ratio depending on the context Esoteric terminology c.248T>C A gene mutation that does not appear in any open-domain corpus such as Wikipedia and has no layman definition Examples of ambiguity in biomedical text. Type Example Explanation Temporally varying Flu The Flu evolves every year and the cause is predicated on the year it is contracted Spatially varying Cancer Cancer is a disease that varies with severity based on location (Late stage brain cancer is much worse than early stage skin cancer) Abbreviation HR A common clinical abbreviation that typically means heart rate, but 

			 https://www.pewinternet.org/2013/01/15/health-online-2013/ 

			 https://www.health24.com/Experts 

			 https://sites.google.com/view/mediqa2019
