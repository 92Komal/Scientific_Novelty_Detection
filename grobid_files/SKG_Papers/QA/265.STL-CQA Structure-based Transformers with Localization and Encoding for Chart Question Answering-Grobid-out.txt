title
STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering

abstract
Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach. We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types. The proposed methodology shows a significant accuracy improvement compared to the stateof-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset. We also demonstrate interpretability while examining different components in the inference pipeline.

Introduction Charts Question Answering (CQA)  (Kafle et al., 2018; Kahou et al., 2017; Chaudhry et al., 2020; Methani et al., 2020a)  is the task designed on the lines of Visual Question Answering (VQA)  (Antol et al., 2015; Malinowski and Fritz, 2014)  which requires answering natural language questions about the data visualisations such as bar charts, pie charts, etc. The problem provides us with ability to understand charts using natural language queries, as well as grounding to the natural language statements for the reasoning operations being carried out to retrieve the final answer to the query. CQA is a challenging task because of the following reasons -(a) large question/answer vocabulary due to chart-specific words, (b) Requirements of multi-modal fine-grained reasoning through understanding of natural language question as well as the visualizations. This is different from VQA, where the answer dictionary is typically limited, and the reasoning is coarse-grained as compared to that required for data visualisations, where finer details like bar length and color can heavily influence both the reasoning and the answer. Despite data visualisations being ubiquitous in documents, the problem has received sparse attention in the literature. The earlier datasets like DVQA  (Kafle et al., 2018)  and FigureQA  (Kahou et al., 2017)  consist of charts generated from synthetic data, though there has been a push for data charts generated from real sources  (Chaudhry et al., 2020; Methani et al., 2020a)  as well. Due to the problems discussed above, the prior work noted that VQA algorithms cannot be applied directly to CQA. Hence, different CQA methods introduce modifications for the problem, while building on the backbone of VQA approaches. While FigureQA  (Kahou et al., 2017)  uses relational networks for question/answering, DVQA  (Kafle et al., 2018)  combines text detection and VQAbased attention modules to answer chart questions. LEAF-QA  (Chaudhry et al., 2020)  encodes question/answers in terms of chart elements, to handle infinite vocabulary problem, while resorting to a VQA-based model as the backbone. Though the approaches improve performance for various chart datasets, the challenges of robust reasoning over varied chart varieties, are far from being solved. We posit that this is mainly due to the non-exploitation of the significant characteristics of charts that distinguish them from plain natural images -the structure and set of chart elements. The structure of the charts along with the position of different chart elements must be exploited by the learning models to enable reasoning over them from natural language questions. In this paper, we propose a transformer-based model to exploit such structural properties of data visualisations, while also showing that our model can provide a much deeper and better interpretations to the generated answers. Our key contributions can be summarized as follows: ? We propose a transformers-based framework to fully utilize the structural properties of charts and achieves state-of-the-art performance on the task of charts question answering. ? We define a set of pre-training tasks for inducing structural knowledge of charts or data visualisations into the proposed model and demonstrate its effectiveness. ? We conduct a range of interpretability experiments to dissect the reasoning process of our model. ? We extend the recently proposed LEAF-QA dataset  (Chaudhry et al., 2020)  to generate a harder and more balanced dataset. 

 Related Works Visual Question Answering: The problem of Visual Question/Answering (VQA) has been explored extensively with a variety of datasets  (Malinowski and Fritz, 2014; Antol et al., 2015; Ren et al., 2015; Krishna et al., 2017; Kafle and Kanan, 2017)  with various approaches for joint understanding of images and text. A more closely related work to our problem is, however, TextVQA  (Singh et al., 2019)  which focuses on the problem of question/answering with scene texts, having infinite vocabulary. Correspondingly, a variety of solutions have also been proposed -the most successful have been based on attention  (Xu et al., 2015; Yang et al., 2016; Anderson et al., 2018)  and joint multimodal learning  (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2019; Li et al., 2020) . 

 Pre-training: The success of pre-training with  ELMo (Peters et al., 2018) , GPT  (Radford et al., 2018) ,  GPT-2 (Radford et al., 2019) , BERT  (Devlin et al., 2019)  has led to significant advancements in natural language understanding. These pre-training frameworks also motivated some of the recent works on multi-modal understanding  (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2019; Sun et al., 2019) . Our pre-training framework borrows ideas from these works with additional tasks designed specifically for understanding chart structure. To the best of our knowledge, ours is one of the first work to demonstrate the effectiveness of pre-training in inducing structural knowledge of charts. Charts Questions Answering: There has been several works lately addressing the problem of CQA. One line of work relies on using the chart figures and questions directly  (Kahou et al., 2017; Kafle et al., 2018 Kafle et al., , 2020 Chaudhry et al., 2020)  while others  (Methani et al., 2020a; Qian et al., 2020)  are focused on parsing out the chart data first to perform the task. Our approach falls in the first category. Apart from chart question answering, there have been works focused on chart data parsing  (Cliche et al., 2017; Kallimani et al., 2013; Savva et al., 2011)  or visual structure extraction  (Tsutsui and Crandall, 2017; Poco and Heer, 2017) . While they do not focus on natural language based understanding of charts, their components form the basis for our structural understanding of charts. 

 STL-CQA In this section, we describe our overall framework which is the first method to fully utilize the structural knowledge of charts for both question encoding and reasoning to perform the task of Charts Question Answering (CQA). We refer to our framework as STL-CQA -Structure-based Transformers with Localization and encoding for CQA. Even though prior works have attempted to utilize the chart structure for encoding questions, their reasoning frameworks still do not exploit this knowledge resulting in sub-optimal performances and offering much less insight into the reasoning process of these models. We divide our overall framework into three stages -Localization, Encoding, and Transformers-based structural attention. While the first two stages have been adopted from the existing state of the art frameworks, the novel reasoning stage makes our algorithm much more powerful and interpretable as discussed in the later sections. 

 Localization The first step in our multi-stage framework is the detection or localization of the chart elements used in different types of data visualisations. For this purpose, we leverage the advances in the object detection frameworks and use the Mask-R CNN  (He et al., 2017)  with a Resnet-101 backbone. We enlist the different categories of our elements and train the network from scratch on the training sub- set (described in Section 4.1) of around 198K images. Since the metadata provided in the public datasets such as DVQA  (Kafle et al., 2018 )/LEAF-QA  (Chaudhry et al., 2020)  1 consist of only bounding boxes, we convert them into masks using several approximations specially for pie/donut charts where we utilize the geometry of different figures to prepare masks (refer supplementary for details). The implementation is carried out through Detec-tron2  framework with a learning rate initialization of 0.00025 for 150, 000 iterations. 

 Encoding Unlike VQA, the text vocabulary in the case of CQA is much larger if not infinite. For each chart, a question about it consist of words whioch are very specific to that chart. For example -A chart showing GDP of different countries can have words like 'USA' or 'Canada' which might not be present in other charts at all. We, therefore, follow dynamic encoding scheme  (Chaudhry et al., 2020; Kafle et al., 2020)  to encode the questions. In this paper, we only report performance with a text oracle, which is same as the previous work  (Kafle et al., 2020; Chaudhry et al., 2020) . The oracle is a perfect OCR which provides access to the bounding boxes and content of different text areas on charts, while the role of the text area (x-title, y-title, etc.) is taken from our localization system. We use the bounding box information to assign the relative position to each of the text  The extracted strings and their positions are then used to replace the string of the question with standardized tokens. For example -if the token in the question string is 'USA' which is present as an x-axis label as its third element from the origin, we replace the token 'USA' with xlabel 3. The vocabulary of questions, thus, consists of both standardized tokens as well as natural language tokens such as greater, which etc. which are common to all questions. The vocabulary (or classes) of answers is determined in the exact similar manner. 

 Structure-based Transformers This is the novel and most important module of our framework which performs (a) chart structure understanding, (b) question understanding, and (c) reasoning over the chart to find the answer. We adapt the transformer-based frameworks from  (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2019; Li et al., 2020)  to perform reasoning over charts. We demonstrate, empirically, that the architecture is strongly suited for the task of CQA through extensive experiments. The architecture can be broken down into 4 stages: Input: The inputs to the model are two sequences of features. The question Q is broken down into a sequence of words {w 0 , w 1 , ...., w n } and encoded as a sequence of word embeddings {e 0 , e 1 , ....., e n } of dimension d e also taking the position into account: e i = word-emb(w i ) + pos-emb(i) (1) A normalisation layer is applied before providing the word embedding sequence as input to our model. For the chart image C, the model input is prepared by utilizing the output of Mask-RCNN  (Anderson et al., 2018) . We extract the features using the Resnet-101 backbone of our detection network and use the bounding boxes of different m chart elements {c 0 , c 1 , ...., c m } as well to encode the chart. Although natural images have larger number of possible class elements, in the case of data visualizations, more class elements are present simultaneously in an image. Further, reasoning in charts depends heavily on the correct detection of the geometry and type of each box. Hence, unlike  (Tan and Bansal, 2019) , where a fixed number of objects are extracted for every image even if there are several overlaps, we apply non-maximal suppression  (Neubeck and Van Gool, 2006)  to choose the most confident and distinct bounding boxes. Finally, the Resnet-101 network is used to extract the features of the final bounding boxes. A plot class which provides a bounding box of the plot region to provide a global picture, is also taken. This is necessary for answering the global information questions about the images (such as Is there a grid in the chart?). We found the performance to improve significantly after adding the global plot representation. Since, different images can have different number of chart elements, we pad the sequences to have a fixed length M for all charts. The chart input sequence is computed as below: f i = LayerNorm(W F r i + b f ) (2) p i = LayerNorm(W P x i + b p ) (3) c i = f i + p i 2 (4) where, r i corresponds to the Resnet-101 features of i th chart element, x i refers to corresponding bounding box coordinates, (W F , b f ) and (W P , b p ) are learnable parameters. 

 Chart Relation Transformer: The chart features computed in Eq. 4 are fed to a transformer with N CE layers each having a self-attention block and a feed-forward block both with residual connections as proposed originally by  (Vaswani et al., 2017)  . The chart-only transformer learns relationships between chart elements, agnostic of the question. We discuss more details about these relationships and their interpretation in Section 4.4. Question Transformer: This is again a transformer with N L layers each having a self-attention block and a feed-forward block with residual connections to encode the meaning of the question. The input to the encoder is as computed in Eq. 1. We also tried using token IDs along with positional embedding to distinguish between words from common vocabulary (eg. how, many) and standardized words for chart vocabulary (e.g. xtitle, legend title). but it did not yield any further improvement. Reasoning Module: The reasoning module is the cross-attention transformer block with N R layers which takes as input the contextual features generated by chart transformer and question transformer. Each layer consists of three blocks -cross-attention, self-attention, and feed-forward. In the cross-attention block of chart stream, chart features act as query in the attention formulation  (Bahdanau et al., 2014)  and the features from question stream act as keys as well as values while the vice-versa happens in the cross-attention block of question stream. This block is followed by a self-attention block and a feed-forward block acting independently in their own streams. All of the three blocks have residual connections. If the i th question token's features and j th chart element's features being used as input for k th layer are represented by Figure  3 : Overview of our pre-training task. MLM is used to recover the language tokens thus inducing both language structure and cross-modality understanding. NSP-like task is also used for the same properties. The triple head on each vision element predicts the position, the chart element class or category, and its attributes. 

 Q i k?1 and C j k?1 and attention with q query, k keys, and v values is represented by attn(q, k, v) then cross attention block for question stream can be represented as in Eq. 5 and self-attention block as in Eq. 6 Q i kcross = attn(Q i k?1 , C k?1 , C k?1 ) (5) Q i k self = attn(Q i kcross , Q kcross , Q kcross ) (6) where Q k : {Q 0 k , ..., Q n k } and C k : {C 0 k , ..., C m k }. We show the reasoning module as a single large block in Fig.  1 . Since the cross attention module is followed by self-attention module for each layer, the information (what is asked) from the cross-attention is used by the self-attention layers to perform various operations with each other. We discuss more about the reasoning operations carried out by the model in Section 4.4. The [CLS] token prepended to the question tokens captures the entire cross-modal information and is used to retrieve the final answer by applying a two layer perceptron over the contextual embedding for this token. The whole system is trained using a cross-entropy loss. 

 Pre-training In this section, we propose a set of pre-training tasks for our STL-CQA model. To the best of our knowledge, this is the first use of pre-training for charts question answering. Our proposed tasks are on the lines of pre-training literature in language modelling  (Devlin et al., 2019)  and VQA  (Tan and Bansal, 2019) . Our tasks can be primarily grouped into three categories: Chart Structure tasks consists of the tasks designed to induce the sense of different parameters which make up the properly defined structure of the chart. We focus on three major things -(a) Types of chart elements (b) Position of chart elements (c) Color and pattern of non-textual elements in charts. Unlike  (Tan and Bansal, 2019; Lu et al., 2019) , we do not pre-train our model on the features regression task. For the type of chart elements, we consider 23 chart categories and use a cross-entropy classification loss for each element over them. For the position of chart elements, we use positioning scheme similar to the one discussed in Section 3.2. Since, even along x-axis (or y-axis in case of horizontal graphs), we can have multiple groups, we use a positioning scheme for chart elements as well. For example, a stacked bar chart having a bar at third position on x-axis (left to right) and second position in legend box (top to bottom) is assigned a position 2 1 (zero-indexing). These positions are then treated as targets for a classification task using a linear position head like that for types of charts elements. For colors and patterns, we use the chart metadata. We treat a particular color and pattern combination as a category and train the model on identifying the color and patterns as a classification problem. Language/Question: For language tasks, as is prevalent in recent works, we train the model on standard MLM i.e. Masked Language Modelling task  (Devlin et al., 2019)  task. However, in our case, we do not just randomly mask any word. We specifically focus on chart vocabulary words or words which modify the meaning of the sentence such as higher, lower etc. During caption generation, we keep track of such words and pass their indices to random masking function so that only those indices are masked during the training stage. Cross Modal: For the reasoning module, we use only one pre-training task which is similar to the next sentence prediction task of BERT. We replace the original sentence with a mismatched sentence with a probability of 0.5 and then train a classifier to identify the mismatched sentence. 

 Experiments We conduct a range of experiments to demonstrate the efficacy of our proposed STL-CQA network. In this section, we describe the different datasets which were used along with the models and the obtained results. We evaluate the proposed STL-CQA method on recent chart question/answering datasets. DVQA  (Kafle et al., 2018)  has a large corpus of bar charts and associated question/answers. We use the splits as provided in  (Kafle et al., 2018)  for our experiments. We demonstrate that the proposed STL-CQA method outperforms the prior baselines. 

 Dataset LEAF-QA  (Chaudhry et al., 2020) , is a comprehensive chart question/answering dataset, covering 10 different types of charts and over 35 question templates. Using the publicly available chart annotations 2 , we further develop a more comprehensive question/answering corpus, LEAF-QA++. The original LEAF-QA utilizes automatic paraphrasing of questions to generate variations. We manually curate 3-8 paraphrase variations of question templates to greatly increase the diversity and naturalness of the questions. We further, add new data question types, increasing the number of template questions from 35 to 75. We add data questions, not present in the original corpus, which ask about chart component positional or values. The question set is balanced to avoid pre-dominant values in answers, especially for questions with common chart answers (like yes/no). We refer the reader to the supplementary for further details on the proposed LEAF-QA++ corpus. To prepare the data for pre-training, we generate 35 sentence templates for LEAF-QA++ using the metadata. Such templates are also augmented with a small list for each sentence which provides information about which are the relevant tokens for MLM masking. For each template, we use paraphrases which are written manually and also combine it with the templates of one-another with a probability of 0.5 thus producing a very high number of combinations. 

 Model Settings For all the experiments, we use N CE = 5, N L = 4, and N R = 5. We use 4 layers in language model, as the template-based questions even with paraphrasing, are less complex than the natural language. In fact, increasing the number of layers resulted in a deteriorated performance as the model overfitted to the vocabulary. For element relationship and reasoning blocks, we set N CE and N R to be 5 layers each. We use d e = 2048 for consistency, the maximum length for questions is 30 and the maximum number of chart elements is taken to be 45. Pre-training Details: We use 23 object categories, 5301 color and patterns combinations for attributes, and 63 different position combinations. We pretrain the model for 4 epochs on 4 V100 GPUs using an Adam Optimizer (Kingma and Ba, 2014) with an initial learning rate of 5 * 10 ?5 and batch size of 512. Fine Tuning Details: We fine-tune the model for 6 Epochs if it has been pre-trained or for 10 epochs if the model is being trained from the scratch. The batch size used is 512 and an Adam optimizer is used with an initial learning rate of 10 ?4 . 

 Results We show results on two datasets -DVQA and LEAF-QA++. We do not show our results on the LEAF-QA corpus as LEAF-QA++ is a superset of it. As discussed in Section 3.2, we assume access to an oracle in our experiments. We show comparisons with the current state-of-the-art models on these datasets. For DVQA comparison, we enlist the results from prior models, viz. QUES, IMG+QUES and SANDY  (Kafle et al., 2018) , PRe-FIL  (Kafle et al., 2020) , Plot-QA  (Methani et al., 2020b) . As shown in Table  3 , both STL-CQA and PreFIL outperform human baselines. STL-CQA further improves over PReFIL, specially in the complex reasoning questions. For LEAF-QA++, we use the LEAF-Net model, the state-of-the-art on LEAF-QA and train it with the hyper-parameters mentioned in  (Chaudhry et al., 2020) . As discussed in  (Chaudhry et al., 2020) , previous models trained on DVQA are not directly applicable to LEAF-QA, due to the higher complexity of charts in the latter. Our model shows a significant improvement in ac- 

 Structure Data Reasoning What type of graph is this ? What does the i bar from left in each group represent ? Between legend label i and legend label i , which has higher ytitle for xlabel i ? Is there a grid in this graph ? Does the value of legend label i monotonically increase over xtitle ? Does there exist any xtitle where legend label i has higher ytitle than legend label i? Is there a legend in this graph ? How many groups or stacks of bars have ratio less than 2 between highest and lowest value bars ? In what xtitle is the sum of legend label i and legend label i lower than legend label i ? How many labels are there in the legend ? In or at which xtitle does legend label i have the highest ytitle ? In or at which xtitle does legend label i and legend label i have the highest difference?    

 Interpretability One of the advantages of using structure-based interpretable elements in key, query and value of attention is the ease in grounding the attention weights to chart structure. In our case, each element in the language stream is a discrete token and elements in the visual stream correspond to chart elements. Thus, we are able to dissect the attention heads and interpret the semantic grounding of the attention weights. We isolate a single chart image with two questions in Fig.  4  to demonstrate the functions of three separate blocks as discussed in Section 3.3. Chart structure understanding is carried out with the visual understanding block. In this case, attention visualisa- We find these heads to be consistent even for other bars. We also find some attention heads establishing relationship between those bars which are from the same legend group. The question understanding visualisations (for two specific heads of last layer) for first question show a heavy focus on the two important parts of the question, contributing to determination of the answer i.e. less and ylabel 2 with some focus on 'how many' which determines that this is a counting question. The language understanding visualisation for these two heads for the second question also shows similar functions for them. The last layers of the reasoning block for first question shows [CLS] token (which is used in the answer head) putting almost all its attention in two bars. We find these two bars to be the one satisfying the criteria of being 'less than ylabel 2'. The answer of this question ('two') is predicted correctly by the model. For the second question [CLS] token puts all almost all its attention on xlabel 0 which is the correct answer while putting some attention on the bars which are contributing to the sum. Infact, the second highest attention is on the bar having highest value. 

 Discussion and Limitations While our proposed model is able to reason very effectively achieving state-of-the-art on the recent datasets, it is able to do so with an assumption of perfect OCR. Thus, it will be pertinent to have better OCR models for chart images. While reasoning in a fine grained manner has been an important part of CQA, the proposed STL-CQA method shows that reasoning could be performed with a high accuracy, given the elements of the charts have been detected accurately. Even though our model achieves near perfect accuracy on the public datasets, the current datasets are synthetic and may not represent the plethora of chart visualisation styles used in real life. Despite the significant progress in simulating real world chart understanding scenarios, especially in LEAF-QA++, there are underlying biases in the generation process (for e.g. due to using a single software like Matplotlib for generation). However, we believe that these are important steps towards the eventual goal of understanding charts in the wild. A further limitation is that the questions used in the existing datasets are template-based. Even though we make an attempt in LEAF-QA++ to increase the number of templates as well as manual generation of paraphrases to bring more diversity, the current templates do not capture the full range of variations in the questions which can be asked from the visualizations. The manually generated questions will also bring different ways to address the same text on chart images. For example -Gross Domestic Product could be addressed with its more common short form, 'GDP'. The current approach relies on text-string matches to encode questions, and works because the questions have been generated using the original chart text strings. This approach, will however fail in scenarios where the entity could be addressed through its variations. However, bringing human-generated questions into the proposed datasets is a challenge since human subjects would be required to possess a deep understanding of the different charts, before being able to ask reasonable and difficult questions. 

 Conclusion and Future Works In this work, we proposed an extension to the LEAF-QA data using the public metadata provided for the charts. We also proposed a transformersbased framework while emphasizing on the need to exploit the structural properties of chart, and showed its strong effectiveness by achieving stateof-the-art with a significant margin on the recent Chart Q/A datasets. We also defined and experimented with a set of pre-training tasks and showed the improvement due to pre-training on the problem of CQA. We used attention to dissect our model to show how each of its module functions to retrieve the final answer. We discussed the current line of CQA work and proposed future directions by outlining the limitations of the current datasets and models. Figure 1 : 1 Figure 1: Overview of our pipeline showing the three different stages of our overall pipeline. We first encode both the question string and chart image by locating the different chart elements. The reasoning module, then, processes both the encoded question as well as the encoded chart elements structurally in order to provide the final answer. 
