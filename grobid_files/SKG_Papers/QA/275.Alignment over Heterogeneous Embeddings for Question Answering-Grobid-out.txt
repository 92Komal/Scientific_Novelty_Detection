title
Alignment over Heterogeneous Embeddings for Question Answering

abstract
We propose a simple, fast, and mostlyunsupervised approach for non-factoid question answering (QA) called Alignment over Heterogeneous Embeddings (AHE). AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency of the corresponding question/answer term. AHE's similarity function operates over embeddings that model the underlying text at different levels of abstraction: character (FLAIR), word (BERT and GloVe), and sentence (InferSent), where the latter is the only supervised component. Despite its simplicity and lack of supervision, AHE obtains a new state-of-the-art performance on the "Easy" partition of the AI2 Reasoning Challenge (ARC) dataset (64.6% accuracy), toptwo performance on the "Challenge" partition of ARC (34.1%), and top-three performance on the WikiQA dataset (74.08% MRR), outperforming many other complex, supervised approaches. Our error analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information. We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE 1 .

Introduction The "deep learning tsunami"  (Manning, 2015)  has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks  (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b;  1 Code: https://github.com/vikas95/AHE Question -Which sequence of energy transformations occurs after a battery-operated flashlight is turned on? 1. electrical ? light ? chemical 2. electrical ?chemical ? light 3. chemical ? light ? electrical 4. chemical ? electrical ? light Supporting paragraph(s): "a chemical cell converts chemical energy into electrical energy; a flashlight chemical energy to light energy" Figure  1 : A multiple-choice question from the ARC dataset with the correct answer in bold font. This question is answered correctly by our alignment method that relies on contextualized word embeddings that capture the correct sequence, and cannot be answered correctly when relying on uncontextualized embeddings.  Tymoshenko et al., 2017; Xiong et al., 2016a; Radford et al., 2018; Li et al., 2018, inter alia) . However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes  (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia) . While a few recent efforts have adapted these alignment methods to operate over word representations  (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018) , they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe  (Pennington et al., 2014) . In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure  1  shows an example of a question from AI2's Reasoning Challenge (ARC) dataset , which is not answered correctly by a state-of-theart BoW alignment method  (Yadav et al., 2018) , but is correctly answered by our alignment approach when operating over Bidirectional Encoder Representations from Transformers (BERT) embeddings  (Devlin et al., 2018) . We propose a simple, fast, and mostlyunsupervised approach for non-factoid QA called Alignment over Heterogeneous Embeddings  (AHE) . AHE uses an off-the-shelf information retrieval (IR) component to retrieve likely supporting paragraphs from a knowledge base (KB) given a question and candidate answer. Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term. AHE's overall alignment score is the sum of the IDF weighted scores of each of the question/answer term. Importantly, AHE's alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR)  (Akbik et al., 2018) , word (BERT)  (Devlin et al., 2018) , and sentence (InferSent)  (Conneau et al., 2017) , where the latter is the only supervised component in the proposed approach. The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier. The contributions of our work are the following: 1. To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, including character, word, and sentence. 2. We obtain (near) state-of-the-art results (top three or higher) on three QA datasets: Wik-iQA  (Yang et al., 2015)  (74.08 mean reciprocal rank), ARC the Challenge partition (34.1% precision at 1 (P@1)) and ARC Easy (64.6 P@1). Our approach outperforms information retrieval methods, other unsupervised alignment approaches, and many supervised, neural approaches, despite the fact that it is mostly unsupervised and much simpler. Importantly, unlike many neural approaches, our results are robust across several datasets. Minimally, these results indicate that the work proposed here should be considered as a new, strong baseline for the task. 3. Our analysis indicates that alignments over character, word, and sentence embeddings capture substantially different semantic information. We highlight this complementarity with an oracle system that chooses the correct answer when it is proposed by any of the AHE's representations, which achieves 68% P@1 on ARC Challenge, 86% on ARC Easy, and 93.7% mean average precision (MAP) on WikiQA. We exploit this complementarity with a simple meta-classifier that learns when and how much to trust the predictions over each representation, which further improves the performance of unsupervised AHE. 

 Related Work We highlight major trends in the field, and how our work compares with them. We focus mostly on non-factoid QA, which is usually implemented in two forms: multiple-choice QA such as AI2's Reasoning Challenge  (ARC) , where the answer must be selected from multiple candidates and (optionally) supported by explanatory texts extracted from external knowledge bases ); or answer sentence selection, where candidate answer sentences are provided and the task is to select the sentences containing the correct answers  (Yang et al., 2015) . Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA  (Chakravarti et al., 2017) . We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms  (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016) . Some of these works also rely on structured knowledge bases  (Zhong et al., 2018a; Ni et al., 2018)  such as ConceptNet  (Speer et al., 2017) . Some approaches use query expansion methods in addition to the above methods  (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018) . For example,  Musa et al. (2018)  used a sequence to sequence model  (Sutskever et al., 2014)  to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely on annotated training data, and, some, on structured KBs, which are expensive to create  (Jauhar et al., 2016) . Further, as we demonstrate in Section 5, these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset. In contrast, our method is mostly unsupervised and does not require training. Even then, our approach performs well on three distinct QA datasets, with top three performance in all. Our work is inspired by previous efforts on using alignment methods for NLP  (Echihabi and Marcu, 2003) . Unsupervised alignment models have been proposed for several NLP tasks such as short text similarity  (Kenter and De Rijke, 2015) , answer phrase/sentence selection in reading comprehension (RC)  (Chakravarti et al., 2017) , document retrieval  (Kim et al., 2017) , etc. Other works have utilized word alignments as features in supervised models  (Surdeanu et al., 2011; Wang and Ittycheriah, 2015) . For example,  Wang and Ittycheriah (2015)  utilized the alignment of words between two questions as a feature in a feedforward neural network that matches similar FAQ questions. Recently,  Yadav et al. (2018)  showed that alignment methods remain competitive for non-factoid QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1  (Brown et al., 1993)  for alignment  (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018) . To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure  1 ). While contextualized representations have been shown to be extremely useful for multiple NLP tasks  (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018) , our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture complementary information, and combining them improves performance further. 

 Approach The core component of our approach computes the score of a candidate answer by aligning two texts. For multiple-choice questions, the first text consists of the question concatenated with the candidate answer, and the second is a supporting paragraph such as the one shown in Figure  1 , which consists of one or more sentences retrieved from a larger textual KB using an off-the-shelf IR system (Section 3.1). For answer selection tasks, the first text is the question and the second is the sentence that contains the candidate answer. Answer candidates are then sorted in descending order of their alignment scores. In both cases, the alignment approach operates over multiple contextualized embeddings that model the two texts at different levels of abstraction: character, word, and sentence. The overall architecture is illustrated in Figure  2 . We detail the alignment method in ?3.2, the multiple representations of text considered in ?3.3, and the ensemble strategies over these representations in ?3.4. 

 Retrieving Supporting Paragraphs For multiple-choice question datasets such as ARC, we retrieve supporting information from external KBs using Lucene, an off-the-shelf IR system 2 . We use as query the question concatenated with the corresponding answer candidate, and BM25  (Robertson et al., 2009)  as the ranking function 3 . For each query, we keep the top C Lucene documents, where each document consists of a sentence retrieved from the ARC corpus. Similar to our previous work  (Yadav et al., 2018) , we boost candidate answer terms by a factor of 3 while keeping question terms as it is in the BM25 ranking function. All texts were preprocessed by discarding the case of the tokens, removing the stop words from Lucene's list, and lemmatizing the remaining tokens using NLTK  (Bird, 2006) . For all experiments reported on the ARC dataset we used C = 20. Here we also calculate the IDF of each query term q i (required later during alignment): idf (q i ) = log N ? docfreq(q i ) + 0.5 docfreq(q i ) + 0.5 (1) where N is the number of documents (e.g., 14.3M for the ARC KB) and docf req(q i ) is the number of documents that contain q i . 

 Alignment Algorithm For representations that produce word embeddings (e.g., FLAIR, BERT, GloVe), we use the alignment algorithm in Figure  3 . Our method computes the alignment score of each query token with every token in the given KB paragraph, using the cosine Embedding representation 4 (InferSent) Embedding representation 3 (BERT) Embedding representation 2 (GloVe) Embedding representation 1 (FLAIR) 

 Ensemble Alignment Q F 1 KB F 1 Q F 2 KB F 2 Q F 3 KB F 3 ? ? ? ? ? ? Q F n KB F m Question + candidate answer text Supporting paragraph text  similarity of the two embedding vectors. Then, a max-pooling layer over this cosine similarity matrix is used to retrieve the most similar token in the supporting passage for each query token. Lastly, this max-pooled vector of similarity scores is multiplied with the vector containing the IDF values of the query tokens and the resultant vector is summed to produce the overall alignment score s for the given query Q a (formed from question Q and candidate answer a) and the supporting paragraph P j : KB 1 KB 2 ? ? ? KB m Q n ? ? ? . . . . . . . . . . . . . . . . . . . . . Q 3 ? ? ? Q 2 ? ? ? Q 1 ? ? ? Cosine similarity matrix Max-pool Query IDF s(Q a , P j ) = |Qa| i=1 idf (q i ) ? align(q i , P j ) (2) align(q i , P j ) = |P j | max k=1 cosSim(q i , p k ) (3) cosSim(q i , p k ) = q i ? p k || q i || ? || p k || (4) where q i and p k are the embedding vectors of the terms q i and p k . In addition to alignments over word-level embeddings, we include InferSent  (Conneau et al., 2017) , which generates sentence-level embeddings (see ?3.3 for details). For InferSent, the alignment score between a query Q a and a supporting paragraph P j is computed as the dot product of the two corresponding sentence vectors, Q a and P j , normalized using softmax over all candidate answers: s(Q a , P j ) = sof tmax( Q a ? P j ) (5) For ARC, the above alignment scores are computed for each supporting paragraph in the set of C paragraphs retrieved in ?3.1. For WikiQA, this score is computed just for the sentence containing the candidate answer. To aggregate the retrieved ARC paragraph scores (for ARC) into an overall score for the corresponding candidate answer, we consider: Max: selects the maximum alignment score between all available paragraphs as the final score for candidate answer a: S(cand a ) = C max j=1 (s(Q a , P j )) (6) Weighted average: averages all available paragraph scores, using as weights the inverse IR ranks of the corresponding paragraphs: S(cand a ) = C j=1 1 j (s(Q a , P j )) (7) During tuning, we observed that the max strategy is better for ARC Challenge, while the weighted average is better for ARC Easy. We conjecture that this happens because Challenge questions require information that is sparser in the collection, and, thus, including more than the top paragraph tends to introduce noise. 

 Text Representations AHE computes alignments over four different embedding representations that model the text at different levels of abstraction: character, word, and sentence (as detailed below). Although all these embeddings can be tuned for specific domains to improve performance, here we highlight the potential of publicly-available, pre-trained embeddings. Hence, we did not train embeddings on any domain specific corpus, and directly used off-the-shelf embeddings in all but one situation. The details of all four component embeddings of AHE are discussed below. Character-based embeddings: We used the FLAIR contextual character language model of  Akbik et al. (2018) . They used long short-term memory (LSTM) networks that operate at character level over the entire text to generate character embeddings (in both forward and backward directions). Similar to them, to generate the embedding for token i, we concatenate the embedding from the forward LSTM for the character following the token, with the embedding from the backward LSTM for the character preceding the token: w F LAIR i := h f t i +1?1 h b t i ?1 (8) where t i is the character offset of the i th token in the input text, and h is the corresponding LSTM's hidden state. We used the "mix-forward" and "mixbackward" pretrained models provided by the authors to produce two character embeddings, each of size 2048, resulting in word embeddings of size 4096. Word-based embeddings: We incorporated two different word-based embeddings: BERT -we used the Bidirectional Encoder Representations from Transformers (BERT) embedding model of  Devlin et al. (2018) . We concatenated the last four layers (as suggested by the authors 4 ) of the BERT Large language model, where each layer has size 1024, summing up to size 4096 embeddings for each token: w BERT i := [Layer ?1 , ...., Layer ?4 ] (9) 4 https://github.com/google-research/ bert GloVe -we also include GloVe embeddings  (Pennington et al., 2014) , under the hypothesis that these uncontextualized word embeddings will provide complementary information to the contextualized BERT embeddings. We used GloVe embeddings of size 300, trained over 840B tokens from Wikipedia, resulting in 2.2M words vocabulary. Sentence-based embeddings: Lastly, we used InferSent, the sentence-based embeddings of  Conneau et al. (2017) . InferSent was originally trained on several natural language inference (NLI) datasets to generate the sentence representations that maximize the probability of correct inference. This model achieved poor performance on our QA tasks (see rows 8a in Table  1  and row 7a in Table  2 ). Therefore, rather than using this NLI model, we trained InferSent on our data by maximizing the inference probability from the input query 5 to the supporting paragraph. We used the same number of supporting passages (C = 20) and the same scoring functions as explained in Section 3.2. We trained InferSent using batches of size 32, the Adam optimizer, learning rate = 0.001, and 50 epochs. We used max pooling over the token's LSTM hidden states to generate an overall sentence embedding. We tuned the sentence representation size on the development sets, 6 which resulted in 128 for WikiQA and 384 for ARC. 

 Aggregating Multiple Representations We aggregate the scores of candidate answers over the four different embedding representations using an unsupervised variant of the NoisyOr formula: N oisyOr M (i) = 1 ? ( M m=0 (1 ? ? m * S m i )) (10) which computes the overall score for answer candidate i. M is the total number of representations (e.g., 4 in our case), and S m i is the score of answer candidate i under representation m. Lastly, ? m is a hyperparameter used to dampen peaky distributions of answer probabilities. We included this hyperparameter because we observed that InferSent produces a probability distribution over candidate answers where one answer tends to take most of the probability mass, and these scores dominate in the NoisyOr. Thus, the ? m weights are set to 1 for all representations with the exception of InferSent, for which we tuned its value to 0.2. Of course, other types of aggregation are possible. To explore this space, we also implemented a supervised meta-classifier, which aims to learn the aggregation function directly from data. We implemented this multi-classifier as a feed forward network with two fully connected dense layers of hidden size 16 and K respectively, where K is the maximum number of candidate answers for the given dataset. The activation function of the first dense layer was tanh; we used a softmax in the second output layer. The input to this network was a vector of size M ? K. For example, for ARC this vector has a size 4 ? 5 = 20. For WikiQA this vector has size 4 ? 22 = 88. Each element in the input vector is the score of one candidate answer under a given representation. Additionally, for ARC we used an extra position in the input vector to indicate the grade of the corresponding exam question (provided in the dataset) with the intuition that the meta-classifier will learn to trust different representations for different grade levels. 

 Empirical Results We evaluate AHE on two QA tasks: AI2's Reasoning Challenge (ARC): this is a multiple-choice question dataset, containing science exam questions . The dataset is split in two partitions: Easy and Challenge, where the latter partition contains the more difficult questions that require reasoning Results and discussion: Tables  1 and 2  summarize the performance of multiple AHE variants, compared against several baselines and previous works, on two datasets. We draw several observations from these: (1) The mostly unsupervised AHE, i.e., with the only supervised component being the InferSent embeddings, has solid and stable performance across the three datasets: best on ARC Easy, second best on ARC Challenge (see lines 18 -21 in Table  1 ), and top three on WikiQA for MRR (see lines 21 -24 in Table  2 ). We find these results encouraging: AHE outperforms many complex supervised neural approaches, including methods having multiple RNNs and stacked attention layers  (Wang et al., 2017b; He and Lin, 2016; Miller et al., 2016; Yin et al., 2015; Miao et al., 2016; Musa et al., 2018; Mihaylov et al., 2018) , despite the fact that it relies mostly on simple, unsupervised components. (2) AHE ports well between different partitions (Easy and Challenge) of same dataset (ARC), unlike many of the previous approaches. For example, neural architectures that perform well on ARC Challenge perform worse than a simple IR baseline on ARC Easy (see, e.g., rows 14 and 15 in Table  1 ) or vice versa (see lines 9 -12). This lack of portability occurs despite these models being trained/tested within the same partition in Table  1 . To emphasize this issue, we explore more aggressive domain transfer settings in Section 5.2. (3) Ablation analysis -The alignment performance from individual components of AHE are shown in the baseline blocks of Tables  1 and 2  when selecting the correct answer when at least one of the representations proposes it, the oracle system achieves 85.1 P@1 on ARC Easy, 68.1 P@1 on ARC Challenge, and 93.71 MAP on WikiQA. The supervised AHE, which uses a feed-forward neural network to learn when to trust each representation demonstrates that (some of) this complementarity can be learned: the supervised AHE consistently outperforms its unsupervised counterpart, albeit by small amounts. Further, line 23 in Table  1  indicates that additional information about the questions (i.e., grade information) is beneficial, as it provides the meta-classifier more grounding on when to trust which representation. We analyze this complementarity further in Section 5.1. 

 Analysis To explore the potential of AHE and further understand its individual components, we conducted the following analyses: and [73 -86]% in the Easy partition. Our current meta-classifier only begins to mine this complementarity, but it is limited because it has no information about the question and candidate answers (other than their scores). We conjecture that considerable performance improvements are possible when such a meta-classifier includes additional information such as question type, question encoding, etc. Our initial results that include grade information (line 23 in Table  1 ) support this hypothesis. We leave a further exploration of this direction as future work. 

 Domain Transfer As shown in Table  1  and discussed in the previous section, many supervised neural methods do not perform robustly across different partitions (Easy and Challenge) of the same ARC dataset, even though they were trained within each partition. This raises the question of how stable is their performance when trained/tested in different domains, which is closer to a real-world deployment scenario? To answer this question, we trained and tested two state-of-the-art neural models, BiL-STM Max-out  (Mihaylov et al., 2018; Conneau et al., 2017)  and BiMPM  (Wang et al., 2017b) , across three domains: ARC Easy, ARC Challenge, and WikiQA. We selected these two approaches because of they are end-to-end neural methods, and they achieve good performance on all datasets. Further, BiMPM is reminiscent of a supervised alignment method, since it computes the overall similarity of question and answers by aligning the  tokens' LSTM hidden states. The results are summarized in Table  3 . The table highlights that the performance of these systems varies considerably based on the training domain, even underperforming a random baseline in some configurations. In contrast, the unsupervised AHE does not require training, and obtains state-of-theart, stable performance across the three datasets. This analysis suggests that future QA evaluations should consider domain transfer as another evaluation measure, to quantify the performance of QA systems under realistic scenarios. 

 Brief Qualitative Analysis We manually analyzed the questions answered incorrectly by AHE and observed that many of the candidate answers were partially answering the questions. As shown in Figure  5 , candidate answers 2 and 5 are partially answering the question, while candidate answers 1 and 3 provide topically relevant information. To select the correct answer in such complex questions, especially for short questions, a successful method would have to incorporate inference, e.g., recognizing process questions such as the one in the figure and coupling with it with a dedicated problem solving method  (Clark et al., 2013) . We leave the integration of inference methods with AHE as future work. 

 Conclusion We proposed a simple, mostly-unsupervised alignment model for non-factoid QA, which operates over multiple contextualized embedding representations that model the text at different levels of abstraction. Despite its simplicity, our approach obtains good performance (top three or higher) that is stable across three QA datasets. Our analysis indicates that the different levels of abstraction (character, word, sentence) capture distinct semantics. We showed that this can be modeled with a metaclassifier that learns when and how much to trust Question -how a water pump works? 1. A large, electrically driven pump (electropump) for waterworks near the Hengsteysee , Germany. 2. A pump is a device that moves fluids (liquids or gases), or sometimes slurries , by mechanical action. 3. Pumps can be classified into three major groups according to the method they use to move the fluid: direct lift, displacement, and gravity pumps. 4. Pumps operate by some mechanism (typically reciprocating or rotary), and consume energy to perform mechanical work by moving the fluid. 5. Pumps operate via many energy sources, including manual operation, electricity, engines , or wind power . the predictions over each representation, and that this has a beneficial impact on performance. All in all, our work indicates that the first, and possibly best, investment in the design of a QA system should be on contextualized embeddings rather than custom, complex neural architectures. When such embeddings are available, state-ofthe-art performance that is competitive with modern neural approaches for QA can be obtained with simple alignment-based aggregation strategies. Minimally, our work should be regarded as a new, strong baseline for non-factoid question answering or answer sentence selection. Figure 2 : 2 Figure 2: AHE architecture illustrated for the multiple-choice question setting. The left text consists with of the question concatenated with the answer candidate; the right text is a supporting paragraph retrieved from an external KB. The same alignment score is computed over multiple representations of text, and then aggregated through an ensemble model. 
