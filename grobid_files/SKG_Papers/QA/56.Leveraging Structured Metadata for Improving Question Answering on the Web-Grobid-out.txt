title
Leveraging Structured Metadata for Improving Question Answering on the Web

abstract
We show that leveraging metadata information from web pages can improve the performance of models for answer passage selection/reranking. We propose a neural passage selection model that leverages metadata information with a fine-grained encoding strategy, which learns the representation for metadata predicates in a hierarchical way. The models are evaluated on the MS MARCO (Nguyen  et al., 2016)  and Recipe-MARCO datasets. Results show that our models significantly outperform baseline models, which do not incorporate metadata. We also show that the finegrained encoding's advantage over other strategies for encoding the metadata.

Introduction Question answering (QA) is a long-standing task in NLP and IR. Having QA systems that perform well on real-world questions is of significant value for search engines and intelligent assistants. While some of the earliest work tackled the task of answering questions based on a large corpus  (Voorhees and Tice, 2000; Voorhees, 2003; Wang et al., 2007)  (albeit mostly focusing on simple fact-oriented questions), much of the recent work on QA has focused on answering questions in a less realistic setting -drawing the answer from a paragraph of text  (Rajpurkar et al., 2016; Joshi et al., 2017) , which is commonly referred to as machine reading comprehension  (MRC) . In this work, we tackle the more realistic problem -candidate answers passages selection/reranking for real-world questions on the web. In contrast to both MRC and early work on QA from a large corpus, web pages often provide an additional source of knowledge. In particular, and thanks in part to the Semantic Web initiative (Berners-Lee  et  al., 2001) , it is estimated that a non-trivial portion of web pages contain metadata annotations that provide a deeper understanding of the website content. The Web Data Commons project (M?hleisen and Bizer, 2012) estimates that 0.9 billion HTML pages out of the 2.5 billion pages (37.1%) in the Common Crawl web corpus 1 contain structured metadata. Figure  1  shows an example of this metadata which comes in the form of objectpredicate pairs annotated with schema.org tags -a set of tags/predicates defined in the schema.org 2 hierarchy. In the example, the hierarchical metadata is used to add more structure to the web page of a recipe, providing meaning to the otherwise unstructured content. This makes several aspects of the recipe explicit -the preparation time (PREPTIME), cooking time (COOKTIME), ingredients (INGREDIENTS), etc. Figure  2  shows the "recipe" object in schema.org; it contains several properties such as We hypothesize that leveraging this metadata, in addition to the textual content, will improve the performance of QA systems on the Web. Table  1  presents an example of a query and several candidate passages. The candidate answer passages are decorated by colored spans that denote a corresponding schema.org predicate property. The correct answer ("1 hour and 10 min") could be inferred from the metadata tag COOKTIME. While it seems clear from the example that the hierarchical schema.org metadata can be exploited in web QA, it will only be of true benefit if the use of metadata is prevalent in web pages. Luckily, this is the case as shown by Guha et al. who studied a sample of 10 billion web pages and showed that one third (31.3%) of the pages have schema.org markup. To date, the end-to-end web QA systems have not made use of this metadata information. We first explore how to incorporate (and the effect of incorporating) semantic web hierarchical metadata into statistical NLP models for web-based QA. More specifically, we introduce a fine-grained encoding method for metadata predicates, to better leverage the semantic information in it. We evaluate the models on the answer passage selection/re-ranking task of MS MARCO  (Nguyen et al., 2016) , that contains real user queries sampled from the Bing search engine, with the answer passages extracted from real-world web pages. Results show that our approaches outperform the baseline systems substantially, with more significant gains on the subset of queries whose candidate passages contain richer metadata tags. Our work demonstrates the importance of encoding metadata information for QA, and verifies our hypothesis that the metadata knowledge can significantly benefit the performance of the neural models. We also provide qualitative analysis that includes performance comparisons across domains. Our findings further provide motivation for webmasters to annotate their web pages with semantic schema.org markup and for question answering systems developer to leverage them. 

 Related Work Our work is related to several directions of work in semantic web, NLP and ML. 

 Metadata for NLP and ML Metadata like time stamp  (Blei and Lafferty, 2006)  and rating  (Mcauliffe and Blei, 2008)  have been successfully incorporated in document modeling. In community question answering, metadata is often used as hard features to improve the model performance -category metadata  (Cao et al., 2010; Zhou et al., 2015)  and user-level information and question-and answer-specific data  (Joty et al., 2018; Xu et al., 2018) . For answer quality prediction, author information  (Burel et al., 2012; Suggu et al., 2016)  has been often incorporated. In our work, we investigate how to leverage the general metadata knowledge from schema.org in web answer passage selection. Our metadata schema used, as compared to prior work mentioned, is structural and hierarchical, and applies to general web pages. The metadata could provide rich information to better understand the textual content on the web. 

 Semantic Web Berners-Lee et al. (  2001 ) described the vision of the Semantic Web. The authors envisioned an extension of the World Wide Web, in which information is given well-defined meaning by bringing structure to the content of web pages. Ten years later, several major search engines have come together to launch the schema.org initiative, that to focus on creating, maintaining and promoting a common set of schemas for structured data markup on web pages. Webmasters use this schema to add metadata tags to their websites in order to help search engines understand the content. The use of such metadata has gained more popularity over the years. 

 Leveraging Metadata for Answer Passage Selection In our setting of answer passage selection, the input to the system is a set of candidate passages p 1 , ..., p n , and a query q, the goal is to identify the passage that best answers the question. For each candidate passage p i , we have the URL i of the web page from where it is extracted. The web document from URL i , may contain a list of metadata object-predicate pairs (obj 1 , pred 1 ), ..., (obj m , pred m ). The detailed approach of obtaining the pairs is presented in Section (3.1). Each predicate pred j consists of a root r j and a property pro j (e.g., RECIPE and COOK-TIME for /RECIPE/COOKTIME, respectively). We denote the path between r j and pro j as pt j . 

 Generate Metadata-Decorated Passages Algorithm 1 generates the decorated answer passages with metadata. The example for a decorated passage is shown immediately after the algorithm. The spans are marked up with the metadata predicate features. The decorated results are later used as input for our models. To be more specific, given the queryPsgExample (including query, candidate answer passage, URL, label of whether is selected) and metadata object-predicate pairs as input, we aim to obtain the queryPsgExamples whose candidate answer passages are decorated. We first obtain all the metadata pairs (matchingMetaPairs) for the URL where the passage text appears (line 1). Then, for each metadata pair in matchingMetaPairs, we employ a similarity function (MetaSim in line 6) to first compute the similarity between all possible text spans of the passage and the object text in the metadata object-predicate pair; afterwards the function records the start and end offset of the text spans which have a similarity score higher than the threshold. In our case, we use  BLEU-4 (Papineni et al., 2002)    We propose a simple but effective neural network structure for building our base neural passage selector (NPS). Similar to the neural reader  (Hermann et al., 2015; Chen et al., 2017)  for MRC, we first obtain a feature-rich (including the fine-grained encoding of the metadata) contextualized representation for each token in the passage and query. The output layer takes the passage and query representations as input and makes the prediction. Fine-grained metadata embedding each predicate feature pred (e.g., /RECIPE/COOKTIME) includes the root r (RECIPE) and the property pro (COOKTIME). To leverage this information, we propose to leverage the hierarchy present on the predicate by learning the root embedding E r , the property embedding E pro , as well as the path embedding E pt (RECIPE?COOKTIME), instead of only learning an embedding of the entire predicate (/RECIPE/COOKTIME). Thus, the final predicate feature encoding for token t i is the concatenation of the three components: E pred (pred i ) = concat(E r (r i ), E pro (pro i ), E pt (pt i )). Passage & Query encoding We first represent each token t i in the passage with a vector representation and pass it through a multi-layer BiL-STM  (Hochreiter and Schmidhuber, 1997)  network to get the contextualized representation for each token (t 1 , t 2 ,...), where t i is the concatenation of: ? (Contextualized) word embedding: GloVe 840B.300d  (Pennington et al., 2014)  embeddings is used to initialize the embedding layer and is fine-tuned during training, we denote it as ti for token t i . Besides, we also use the pretrained contextualized representations produced by BERT , q1 , ..., qm , ..., t1 , ..., tn = BERT([CLS], q 1 , ..., q m , [SEP], t 1 , ..., t n ). For the i th token, the word embedding E(t i ) is the concatenation of the two. ? Metadata predicate embedding: We use the fine-grained predicate encoding of metadata pair (E pred (pred i )), as described above. Embedding for beginning (B_) and intermediate (I_) tokens of a decorated span are different and learned during training; For the other passage tokens that are not metadata-decorated, their predicate (O) embedding are filled with zero vectors. ? Aligned query embedding: Similar to  (Chen et al., 2017) , we also incorporate the aligned query embedding. This feature is intended to capture the similarity between t i and each query word q j . For the i th token t i . It is calculated as: j E(q j ) * sim(E(t i ), E(q j )). The encoding p k for candidate passage k is the sum of the token representations after the BiLSTM. Similarly, query token embedding q j is the concatenation of its contextualized word embedding (q j ) and the GloVe embedding. We pass it through another BiLSTM, and use the sum operation to obtain the query encoding q. Prediction Finally, the "Is_selected" score for passage k is calculated as a function of the passage encoding p k and the query encoding q: score(k) = softmax(p k W q). At test time, we calculate score(1), ..., score(n) for all the candidate answer passages, and select the passage with highest score: argmax k (score(k)). 

 Experiments and Analysis This section first presents the QA dataset that is used for evaluation, and then describe results comparing different methods (with or without leveraging the metadata information). 

 Datasets and Models We evaluate our models on the passage selection task of MS MARCO  (Nguyen et al., 2016) , to our knowledge, this is currently the only large-scale real-world QA/MRC dataset on general web pages, that is paired with URLs from which the candidate passages are extracted. To measure how the models perform when trained and tested on a subset of queries from a focused domain, where the usage of schema.org metadata is more prevalent, we extract the QA pairs of the recipes domain from MS MARCO dataset and extend it with extra QA pairs in this domain (Recipe-MARCO). Table  3  shows the number of queries for the datasets. Although WikiQA  (Yang et al., 2015)  and Natural Questions  (Kwiatkowski et al., 2019)   queries from real users, their answer candidates are restricted to be from Wikipedia. However, the adoption of schema.org tags in Wikipedia pages is very low (< 2.2% 3 ). This is significantly less than general web pages where the adoption rate of schema.org metadata is around 31.3%. Thus we do not use these datasets for evaluation. We follow previous work  (Yang et al., 2015; Tan et al., 2018)  on reporting precision@1 (P@1) and Mean Reciprocal Rank (MRR). P@1 measures whether the highest scoring answer passage returned matches the correct passage. MRR  (Voorhees and Tice, 2000)  evaluates the relative rank of the correct passage in the candidate passages. We compare our models to several baselines, S-Net  (Tan et al., 2018 ) is a prior state-of-theart model on MS MARCO, it also produces synthetic answers and use text generation metrics (e.g., BLEU and ROUGE-L). In this work, we only compare to its capability of passage re-ranking. NPS is the baseline "neural passage selector" which does not encode metadata information. It's similar to the implementation in  Dai and Callan (2019) . B-NPS is a version of our model which builds upon NPS and directly encodes the entire predicate. F-NPS is our main model -fine-grained metadata encoding enriched neural passage selector. We also report the results of selecting the first and a random passage. 

 Results and Analysis Table  4  shows the comparison of different methods on the candidate passage selection task. We see that: (1) By leveraging the metadata, both versions of our model (B-NPS and F-NPS) outperform the baseline NPS model; (2) With fine-grained encoding, F-NPS significantly outperforms all models in both P@1 and MRR. Particularly, F-NPS achieves higher P@1 than NPS by around 2%; (3) From the ablation study, we see the BERT pretrained representations consistently improve the performance, and leveraging the metadata information further improves it. We also present the results of different methods when trained and tested on Recipe-  MARCO. We see that the relative increase of performances for F-NPS is more substantial. Finally, we provide analysis on both the models and the effect of encoding metadata. Since not all web pages come with metadata, we turn our attention to the results describing the model performance on the portion of queries of MS MARCO that come with at least one metadata item ("M-Rich-MARCO"). We first perform analysis to understand how often the web pages in the dataset contain markup and how it affects the models performance. We see that for each query in MS MARCO, there are around 7.9 metadata pairs for its candidate passages; and 31.6 for queries in M-Rich-MARCO. On M-Rich-MARCO, the results we get on P@1 (F-NPS: 33.13, NPS 28.79) demonstrate that the performance gap between the model that leverages the metadata is larger than the general case. This, once again, demonstrates the effect of encoding metadata knowledge. To better understand how the models perform and the effect of metadata on specific web domains, we report in Table  5  P@1 of models (trained on entire MS MARCO) on domains that are richer with metadata (i.e., book, medical, person, organization and review). We observe that queries in "medical", "person" and "organization" domains have a larger presence in the dataset (> 10%). The table also shows the performance of NPS and F-NPS on each domain. We see that F-NPS outperform NPS across all these domains. And the improvement is more substantial as compared to evaluating on the entire test set (the second column of Table  4 ). 

 Conclusion We demonstrate benefits of incorporating metadata information from web pages for improving answer passage selection model. We describe methods for obtaining metadata and decorating passages with metadata object-predicate pairs, and a finegrained encoding strategy for leveraging metadata information in neural models. For future work, we'll investigate metadata for other tasks such as web entity linking and extraction. Figure 1: Metadata Example from SimplyRecipes. 
