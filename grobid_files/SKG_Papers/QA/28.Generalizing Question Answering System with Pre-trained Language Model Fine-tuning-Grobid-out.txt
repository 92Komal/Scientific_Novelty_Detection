title
Generalizing Question Answering System with Pre-trained Language Model Fine-tuning

abstract
With a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC) tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these models and techniques can generalize to out-ofdomain and unseen RC tasks. To enhance the generalization ability, we propose a multi-task learning framework that learns the shared representation across different tasks. Our model is built on top of a large pre-trained language model, such as XLNet, and then fine-tuned on multiple RC datasets. Experimental results show the effectiveness of our methods, with an average Exact Match score of 56.59 and an average F1 score of 68.98, which significantly improves the BERT-Large baseline by 8.39 and 7.22, respectively.

Introduction Reading comprehension (RC) is a fundamental human skills needed to answer questions that require knowledge of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) , the attention over attention mechanism (AoA)  (Cui et al., 2017) , and a multi-hop architecture using gated-attention readers  (Dhingra et al., 2017) . A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications  (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., 2017; Joshi et al., 2017) . However, each QA dataset is built for a particular domain and focus  (Talmor and Berant, 2019) . Dataset passages cover different topics, such as movies  (Saha et al., 2018) , news  (Trischler et al., 2017) , and biomedicine  (Tsatsaronis et al., 2012) . Also, the styles of questions (e.g., entity-centric, relational, other tasks reformulated as QA, etc.), the sources (e.g., crowd-workers, domain experts, exam writers, etc.), and the relationship of the question to the passage are different among datasets (e.g., collected as independent vs. dependent on evidence, multi-hop, etc). The availability of such datasets promotes the development of models that work well for only a specific domain. However, little attention  (Chung et al., 2017; Sun et al., 2018)  has been paid towards generalization, i.e., building QA systems that can generalize well on different datasets and transfer to new domains quickly. One major factor that could contribute to generalization, is effective contextual representation  (Talmor and Berant, 2019) . Recently, models pretrained on a large unlabeled corpus, by adding an extra final layer and fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task  (Radford et al., 2018; . This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet , the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how to adapt to new QA tasks using few or even no prior training examples.  McCann et al. (2018) ;  Liu et al. (2019) ;  Talmor and Berant (2019)  show that promising results can be obtained in transferring to new domains by training models on multiple tasks simultaneously using multi-task learn-ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models to learn useful representations more generally by unifying tasks under a single perspective. Thus, a model, which is trained on multiple source QA datasets, can achieve robust generalization and transferring ability. To summarize, we present our work for the MRQA 2019 shared task on generalization. We propose to use multi-task learning on different source QA datasets and fine-tune XLNet , to build a QA system which has general linguistic intelligence.  2 Related Work 

 MLP 

 Pre-trained Language Models Fine-tuning pre-trained language models via supervised learning has become the key to achieving state-of-the-art performance in various natural language processing (NLP) tasks. Among them, BERT  extracts contextual meaning through bidirectional encoding with a masked language model and a next-sentence prediction objective. Recently, XLNet , a permutation language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 

 Question Answering Unlike traditional knowledge-based QA  (Kalyanpur et al., 2012) , nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD  (Rajpurkar et al., 2016) , NewsQA  (Trischler et al., 2017) , Trivi-aQA  (Joshi et al., 2017) , SearchQA  (Dunn et al., 2017) , HotpotQA  (Yang et al., 2018) , NaturalQuestions  (Kwiatkowski et al., 2019) , DROP  (Dua et al., 2019) , RACE  (Lai et al., 2017) , DueRC  (Saha et al., 2018 ), BioASQ (Tsatsaronis et al., 2012 , TextbookQA , and RelationExtraction  (Levy et al., 2017) , have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poorly in unseen domains due to the data scarcity. 3 Methodology 

 Multi-task Learning 

 Baseline MRQA organizers have released the BERT-base and BERT-large models as baselines implemented using the AllenNLP  (Gardner et al., 2018)  platform.  1  The BERT transformer receives a passage and a question that is separated by an [SEP] token. On top of this, the baseline models deploys a linear layer to find the corresponding span which answers the question from the passage. 

 XLNet Model XLNet ) is a recently proposed generalized autoregressive pre-training model for language understanding which naively follows the Transformer(-XL)  architecture. Instead of the bidirectional encoding structure used in BERT , XLNet leverages a permutation language modeling objective and target-aware representations with a two-stream attention mechanism to enable the model to capture the context on both sides. Besides the datasets which are also used in the pre-training procedure of BERT , XLNet involves Giga5  (Parker et al., 2011)   

 Attention-over-Attention Attention-based neural networks have become a stereotype in most extractive QA systems and is well-known for its capability of learning the importance of distribution over the inputs. attentionover-attention (AoA) mechanism  (Cui et al., 2017)  is successful because it can generate an "attended attention" which considers the interactive information from both the query-to-document and document-to-query perspectives. Its effectiveness has been proved on public datasets such as the CNN, Children's Book Test, and SQuAD datasets. 

 Experiments 

 Preprocessing The original setting of the sequence length is 512 in the XLNet-large model, but because of the constraint on the computational ability of a single GPU, a trade-off is made between the size of the context and the performance of the model. The sequence length is set as 340 when fine-tuning on the GPU but kept at 512 on the tensor processing unit (TPU). All the datasets are tokenized with SentencePiece  (Kudo and Richardson, 2018)  and uniformed in lower cases. 

 Data Analysis Datasets Under the scenario of this task, the model should be trained on six training datasets.   Six in-domain datasets and six out-of-domain datasets are offered as development sets for evaluation. The characterization of the corresponding datasets is shown in Table  1 . The twelve known datasets differ from each other in terms of the source of the data, the type of questions, and whether inference (multi-hop) is required during QA. Moreover, the sources of the data on the development datasets are more diverse and not fully covered by the training datasets, which indicates that the generalization ability of the representations produced by the model can significantly improve the performance on the development datasets. Similarity Evaluation Following the similarity evaluation method utilized in  Talmor and Berant (2019) , we fine-tune XLNet with an additional MLP on a single GPU using the six training datasets separately, and then evaluate the model on all the in-domain and out-of-domain development sets. More details about fine-tuning the XL-Net model on the GPU are mentioned in ?4.4. The evaluation results can be found in Table  3 . When evaluating the in-domain datasets, the similarity can be computed as Similarity = P ij P j + P ji P i , (1) where P ij refers to the F1 score when fine-tuning XLNet on dataset D i and evaluating it on D j , while P i refers to the F1 score when fine-tuning and evaluating on D i . When evaluating the similarity between the in-domain datasets and out-ofdomain datasets, Similarity = 2?P ij P j , (2) where dataset D j is one of the in-domain datasets, while D i is among the out-of-domain datasets. We visualize the datasets using the forcedirected placement algorithm (Fruchterman and Reingold, 1991) for a more intuitive view, which is shown in Figure  2 . Each node represents a dataset, and the in-domain datasets and out-ofdomain datasets are distinguished by the size of the node. The nodes are linked by a set of edges acting as the springs, pulling nodes towards one another, while non-linked nodes are pushed apart. The weights of the edges act as the pulling force, influencing the distance and the relative position among nodes. In our case, we consider the similarities between nodes (datasets) as the pulling force. The nodes with higher similarity tend to be pulled closer and vise versa. From Figure  2 , the out-of-domain datasets tend to be pushed to the boundary of the figure, which indicates that they have lower similarity with the in-domain datasets. Except for the RelationExtraction dataset, all the out-of-domain datasets only have a strong relationship with one or two in-domain datasets but are positioned far from the others. This implies that to achieve consistently good performance on out-of-domain datasets, data samples from all the in-domain datasets are needed. 

 Data Feeding Methods Empirically, the data feeding order when training and fine-tuning has a great impact on the performance of the model. In terms of the fine-tuning procedure with the six training sets, we propose two methods for data feeding. The first method follows the idea of multi-task learning. In this task, because the six training sets differ in several aspects as explained in ?4.2, we consider them different tasks and leverage the model to fully explore the general semantic representations of the samples in the training datasets. During multi-task learning, we combine all the training datasets and shuffle them to reduce the reliance on the model on the order of the data. The second method is similar to curriculum learning  (Bengio et al., 2009) , but because of the sparse relation among the datasets, it's not practical to evaluate the difficulty and the degree of learning. So we simply propose to fine-tune the model using the training sets that are shuffled separately one after another with the same training steps. 

 Fine-tuning Methods Various fine-tuning methods based on XLNet are tested to identify the most effective method to achieve better generalization performance. During the fine-tuning procedure, all the methods share a learning rate of 1 ? 10 ?5 . Fine-tuning on TPU The trend of the pretrained models for language understanding  is to achieve better performance with larger models, but this leads to their reliance on better computational resources. Even the fine-tuning procedure of XL-Net  is hard to handle in a normal GPU such as GTX 1080Ti, because of the memory size and the processing speed. To make it possible to fine-tune the XLNet model and adapt it to QA tasks on a single GPU, we make modifications to the MLP structure and the hyper-parameters, which are listed in Table  2 . For the model on the GPU, only the last 13 layers are further tuned. Except for the reduction of the three hyper-parameters mentioned above, the MLP structure is also changed from a single large linear layer to a deeper but smaller structure. To fulfill the fine-tuning procedure on the original structure of XLNet with a larger additional linear layer and achieve better performance on development sets and test sets, we take advantage of the TPU  (Jouppi et al., 2017)  from the Google cloud service. The TPU is a machine learningoriented application-specific integrated circuit. It has a larger memory and faster computational speed than a GPU, since it consists of a large high bandwidth memory (HBM) and 32-bit floatingpoint multiply-accumulate systolic array matrix unit. In contrast to the computational power of a GTX 1080Ti (11.34 Tflops of 32-bit floatingpoint computation and 11 GB of memory), the TPU has 420 Tflops of a 32-bit floating-point computational speed and a 128 GB HBM, which allow us to train a deeper and larger model at a faster speed. Fine-tuning with MLP Leveraging an MLP as the additional structure for fine-tuning a pre- trained model is a common strategy of task adaptation. In this task, we test the performance of XLNet with an MLP when fine-tuning on both the GPU and TPU. Because of the limitation of the memory size on the GPU, the MLP structure differs from that on the TPU. More details are shown in Table  2 . Fine-tuning with AoA Layer We also test the performance of the model when fine-tuning XL-Net with an AoA layer on a single GPU. In this case, we add an additional AoA layer between the output layer of XLNet and MLP mentioned above. In the practical implementation of this method, the representations of the context and the query need to be split from the output of XLNet, while we can get the corresponding representation directly and separately when using BERT. 

 Results 

 Comparison between Data Feeding Methods Table  4  shows the performance of the XLNet models fine-tuned with the two data feeding methods mentioned in ?4.3 on the development sets. Both models are fine-tuned with an additional MLP on a single GPU based on XLNet-large. For the single-task XLNet model, we feed the data in the following order: SQuAD, NewsQA, TriviaQA, SearchQA, HotpotQA, and NaturalQuestions. In general, the multi-task data feeding method outperforms the method in which the datasets are fed one after another. On further observation, multitask learning tends to enable the model to achieve uniform generalization performance on unseen datasets, while the single-task feeding method better benefits the tasks that are similar to the last task that is involved during fine-tuning. The fact that the single-task model achieves better performance on RACE than that using the multi-task learning method is related to the higher similarity between RACE and NaturalQuestions, which we can figure out from Figure  2 . 

 Comparison between Fine-tuning Methods The results of the experiments on different finetuning methods are shown in Table  5 . All the experiments are evaluated on the development sets. Although the AoA layer improves the performance of BERT on the SQuAD dataset, which can be seen on the SQuAD leaderboard, it fails to improve generalization performance on XLNet. Moreover, while it takes 300k training steps to finish fine-tuning, we only need 100k training steps to fine-tune the XLNet model with an MLP (refer to ?4.4) on this QA task. The XLNet model finetuned with an MLP on the TPU achieves the best performance, both on average and on each development dataset. It outperforms the baseline by a large margin, but only requires 15k training steps for fine-tuning. The TPU shows its effectiveness on training with its ability to afford a larger model, batch size, and sequence length. 

 Comparison with Baseline The results on the test sets shown in Table  6  indicate that the multi-task XLNet-large model finetuned with a larger linear layer on the TPU con-sistently outperforms the BERT-large baseline by a huge margin. On the test set, our XLNet based model fine-tuned under the multi-task learning setting shows its robust generalization and transferring ability over the baseline. 

 Conclusion In this paper, we propose a multi-task framework to improve the generalization ability of question answering systems by leveraging large pre-trained language models. Experimental results indicate the effectiveness of our methods on broader QA tasks, with an average Exact Match score of 56.59 and an average F1 score of 68.98, which are significantly higher than the BERT-large baseline results by 8.39 and 7.22, respectively. Figure 1 : 1 Figure 1: The model architecture. GPU-version: The blue boxes (first half) of XLNet layers remain unchanged during fine-tuning and only green boxes are updated due to the GPU's memory limitation. TPUversion: All layers of XLNet are fine-tuned. 
