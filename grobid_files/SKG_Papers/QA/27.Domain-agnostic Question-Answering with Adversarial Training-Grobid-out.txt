title
Domain-agnostic Question-Answering with Adversarial Training

abstract
Adapting models to new domain without finetuning is a challenging problem in deep learning. In this paper, we utilize an adversarial training framework for domain generalization in Question Answering (QA) task. Our model consists of a conventional QA model and a discriminator. The training is performed in the adversarial manner, where the two models constantly compete, so that QA model can learn domain-invariant features. We apply this approach in MRQA Shared Task 2019 and show better performance compared to the baseline model.

Introduction Followed by the success of deep learning in various tasks, it becomes important to build a single model covering various domains without further fine-tuning to out-of-domain distribution. Because for real world application, a model is required to generalize to unseen sources of data. In case of Question Answering (QA) task which is one of the promising areas in NLP, however, models outperforming human on SQuAD  (Rajpurkar et al., 2016)  cannot generalize well to other datasets. Models rather overfit to a specific dataset and require additional training on other dataset to adapt to new domain  (Yogatama et al., 2019) . Thus, in order to build a domain-agnostic QA model which is capable of handling out-of-domain data, it is necessary for model to learn domaininvariant features rather than specific ones. In this paper, we apply adversarial training framework to train a QA model with domain-agnostic representation. As shown in Figure  1 , the model is divided into two components, which are the QA model and the domain discriminator. The discrim- * Equal contribution inator predicts domain label of hidden representation from QA model. During the training, the QA model tries to fool the discriminator so that the hidden representation becomes indistinguishable to the discriminator. Meanwhile the discriminator is trained to identify the domain label correctly. As a result, QA model can learn domaininvariant features. Our framework can be applied to any existing QA model because the architecture of QA model stays unchanged. We train and validate our method on 12 datasets (6 datasets for training and 6 datasets for validation) which are provided by MRQA Shared Task. Each training dataset is considered different domain for adversarial learning in which QA model learns domain-invariant feature representation by competing with discriminator. Our experimental result shows that the proposed method improves performance compared to baseline. 

 Related Works Pre-trained Language Model Recently, there have been several applications for using pretrained language models, such as ELMo , GPT  (Radford et al., 2018) , or BERT  (Devlin et al., 2018)  to transfer the knowledge from pre-training to various downstream NLP tasks. BERT is pretrained with bidirectional encoder  (Vaswani et al., 2017)  on large corpora. Unlike other auto-regressive language models (unidirectional or concatenation of forward and backward language model), BERT randomly masks some input tokens and predicts the masked tokens based on its context. The masked language model enables bidirectional representation, which leads to significant improvements on a number of NLP tasks, such as sentence classification, POS tagging or question answering. Domain Generalization Even though many deep learning models surpass human-level performance on various task, they perform poorly on out-of-domain dataset. To address this problem, domain adaptation and domain generalization are proposed, making models more robust to out-of-domain data. The difference between domain adaptation and domain generalization is that for domain generalization, data from the target domain is not available during training. Several methods for domain generalization exist. One of them is to train a model for each indomain dataset. When testing on out-of-domain, select the most correlated in-domain dataset and use that model for inference . Other works such as  (Ghifary et al., 2015; Muandet et al., 2013) , model is trained to learn a domain-invariant feature by using multi-view autoencoders and mean map embedding-based techniques. Other approaches  (Khosla et al., 2012; Li et al., 2017)  break down parameters of a model into domain-specific and domain-agnostic components during training with in-domain dataset, and use the domain invariant parameters for predicting data from unseen target domain. Recently, meta-learning has been proposed for domain generalization. Some methods  (Li et al., 2018a; Balaji et al., 2018; Li et al., 2019)  leverage meta-learning framework for domain generalization. 

 Adversarial Training The idea of adversarial training is originally proposed in the field of image generation  (Goodfellow et al., 2014) , known as Generative Adversarial Network (GAN). GAN is also adopted in text generation  (Yu et al., 2017)  with policy gradient for bypassing non-differentiable operation. The concept of adversarial training is not limited to the task of generation. It can be extended to text classification  (Chen et al., 2016; Chen and Cardie, 2018) , and relation extraction  (Wu et al., 2017) . Likewise, attempts are made to get language-invariant features with adversarial training  (Chen et al., 2016; . Adversarial training has been used for domain adaptation or domain generalization as well. In Domain-Adversarial Neural Network (DANN)  (Ganin et al., 2016) , it has two classifiers: one classifies task-specific class labels, and the other classifies whether the data belong to source or target domain. Recently, One approach  (Li et al., 2018b)  extends adversarial autoencoder by minimizing maximum mean discrepancy among different domains for domain-invariant feature representation. 

 Proposed Methodology We assume that there exists domain invariant feature representation such that QA model generalize well to predict answer on unseen out-ofdomain. In order to adapt to out-of-domain, adversarial learning procedure is leveraged for learning domain-invariant representation. We present our proposed method in detail in the following sections. 

 Problem Definition We formulate the task as follows: given the K indomain datasets D i , consisting of triplets of passage c, question q, and answer y, where D i = {c (k) i , q (k) i , y (k) i } N k i=1 . The model learned from {D i } K i=1 predicts answer y l j from c l j , q l j for each L out-of-domain datasets {D j } L j=1 . 

 Prediction Model Our method can be applied to any QA models which learn representation in the joint embedding space of passage and question. In this paper, we use BERT for QA because it is pre-trained on a large corpus and known to be generalized on several different tasks. As for standard QA task, the model is trained to minimize negative loglikelihood of answer y for all the given in-domain datasets, where N, y i,s , and y i,e are respectively the total number of in-domain data, the start position and the end position of answer in the passage. LQA = ? 1 N K k=1 N k i=1 log P ? (y (k) i,s |x (k) i , q (k) i )+ log P ? (y (k) i,e |x (k) i , q (k) i ) (1) 

 Adversarial Training Minimizing the cross-entropy as in equation (  1 ) does not ensure that the model will generalize on unseen domain. Rather it tends to overfit to certain datasets. Inspired by GAN  (Goodfellow et al., 2014) , we propose a simple yet effective method to regularize the model such that it learns domaininvariant features. In the adversarial training procedure, QA model learns to make the discriminator to be uncertain about its prediction. On the other hand, the discriminator is trained to classify the joint embedding of question and passage from QA model into the given K domains. If the QA model can project question and passage into an embedding space where the discriminator cannot tell the difference between embeddings from different K domains, we assume the QA model learns domain-invariant feature representation. We formulate the adversarial training as follows. A discriminator D is trained to minimize the cross-entropy loss as of equation (  2 ), where l is domain category and h ? R d is the hidden representation of both question and passage. In our experiment, we use [CLS] token representation from BERT for h. L D = ? 1 N K k=1 N k i=1 log P ? (l (k) i |h (k) i ) (2) For the QA model, it tries to maximize the entropy of P ? (l (k) i |h (k) i ). In other words, it minimizes Kullback-Leibler (KL) divergence between uniform distribution over K classes denoted as U(l) and the discriminator's prediction as in equation (3). Then the final loss for QA model is L QA + ?L adv where ? is a hyper-parameter for controlling the importance of the adversarial loss. In our experiments, we alternate between optimiz- ing QA model and discriminator. L adv = 1 N K k=1 N k i=1 KL(U(l) P ? (l (k) i |h (k) i )) (3) 4 Experiments and Result 

 Dataset We validate our adversarial model for MRQA Shared Task with 6 different out-of-domain datasets, which are BioASQ (BA)  (Tsatsaronis et al., 2012) , DROP (DP)  (Dua et al., 2019) , DuoRC (DR)  (Saha et al., 2018) , RACE (RA)  (Lai et al., 2017) , RelationExtraction (RE)  (Levy et al., 2017) , and TextbookQA (TQ)  (Kembhavi et al., 2017) . Table  1  shows the statistics and description of these datasets. Each dataset has about 1k samples. However, the number of samples from each dataset varies. Thus, we use stratified sampling in order to make class-balanced stochastic minibatch having certain amount of samples from all domains. We use maximum sequence length of 64 and 384 for question and passage respectively. But some examples are longer than 384. Therefore each passage is split into several chunks with a window size of 128. We discard samples without answers because all questions are considered to be answerable from given context in MRQA shared task. Note that the final evaluation shown in the Table 2 is conducted by MRQA organizers with additional 6 out-of-domain undisclosed private test datasets, which are BioProcess (BP)  (Scaria et al., 2013) , ComplexWebQuestion (CQ)  (Talmor and Berant, 2018) , MCTest (MC)  (Richardson et al., 2013) , QAMR (MR)  (Michael et al., 2017) , QAST (ST)  (Jitkrittum et al., 2009)  and TREC (TR)  (Voorhees, 2001) . 

 Implementation Details We implement our model based on the Hugging-Face's open-source BERT implementation 1 in Pytorch  (Paszke et al., 2017) . The performance of the baseline in our experiment differs from the official baseline of MRQA, which is based on Al-lenNLP . We follow the hyperparameters as BERT for our model. In detail, we use "bert-base-uncased" with a learning rate 3e-5 and a batch size of 64. Additionally, our model requires one more hyperparameter ?, which indicates the importance of adversarial loss as described in the equation (3). We find out that the value of 1e-2 for ? gives the best result in our experiments. The baseline and adversarial model are trained on V100 GPU for about 5 GPU hours. For training, we use 6 in-domain datasets, which are SQuAD, TriviaQA  (Joshi et al., 2017) , Natural Questions  (Kwiatkowski et al., 2019) , HotpotQA , SearchQA  (Dunn et al., 2017) , and NewsQA  (Trischler et al., 2016)  provided by MRQA. We select the best performing model on validation set, where models are trained for 1 or 2 epochs. The codes for our model are available at https://github.com/seanie12/mrqa. 

 Performance Comparison Table  2  shows the performance evaluation results of models on out-of-domain datasets. In the table, the model trained with our adversarial learning is named with '-adv'. The top of the table is the result of validation datasets while the bottom is the result of test datasets. As shown in the table, overall, the model with adversarial learning has better performance compared to the baseline in terms of both EM and F1 measures. For validation datasets, the average F1 score of our model is about 1.5 point higher than the baseline. In detail, our model outperforms the baseline in DP, DR, RC, and RA dataset by large margin. But the adversarial learning degrades performance in BA and RE. We can see the same aspect in terms of EM score. Similar to the result of validation datasets, our model shows better performance in terms of EM (Exact Match) and F1 on the most of test datasets except for ST. Overall, our model has superior performance with considerable margin of over 2 point in F1. 

 Discussion In this section, we discuss some trials that have failed to improve the performance but might be helpful for future works. 

 Span Refinement QA sample consists of a question, a passage, and an answer span. There could exist multiple answer spans because more than one phrase in the passage can be matched with the answer text. For simplicity, only the first occurrence of answer text is used for training in most of the baseline codes. However, considering context and semantic of the given question and answer, a certain phrase in the passage is more likely to be plausible answer span relevant to the question. In order to find the most plausible answer span, a question and sentences in the passage are encoded into fixed-size vectors with universal sentence encoder  (Cer et al., 2018) . We choose the span in a sentence, which is the most similar to the question in terms of cosine similarity, as golden span. In our experiment, this approach boosts up the performance of some datasets but degrades the performance a lot in the other datasets. 

 Meta Learning We apply meta learning to domain generalization  (Li et al., 2018a (Li et al., , 2019 Balaji et al., 2018)  to simulate train/test domain shift. For every epoch, one dataset is randomly selected as virtual test domain. As described in  (Finn et al., 2017) , QA model is trained to maximize meta objective, which leads to improve the performance in train domain, but also in test domain. But this requires to compute Hessian-vector products, which slows down the training. This is even worse for BERT because there are 110M parameters to fine-tune. Moreover, contrary to the previous works, the meta learning for domain generalization does not help improve the performance. 

 Conclusion We leverage adversarial learning to learn domaininvariant features. In our experiments, the proposed method consistently improves the performance of baseline and it is applicable to any QA model. In future work, we will try adversarial learning for pre-training model with diverse set of domains. Figure 1 : 1 Figure 1: Overall training procedure for learning domain-invariant feature representation. Model learns to predict start and end position in the passage and fool discriminator for domain-invariant representation. 
