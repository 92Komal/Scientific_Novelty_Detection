title
Complex Question Decomposition for Semantic Parsing

abstract
In this work, we focus on complex question semantic parsing and propose a novel Hierarchical Semantic Parsing (HSP) method, which utilizes the decompositionality of complex questions for semantic parsing. Our model is designed within a three-stage parsing architecture based on the idea of decompositionintegration. In the first stage, we propose a question decomposer which decomposes a complex question into a sequence of subquestions. In the second stage, we design an information extractor to derive the type and predicate information of these questions. In the last stage, we integrate the generated information from previous stages and generate a logical form for the complex question. We conduct experiments on COMPLEXWE-BQUESTIONS which is a large scale complex question semantic parsing dataset, results show that our model achieves significant improvement compared to state-of-the-art methods.

Introduction Semantic parsing is a task which maps natural language utterances into logical forms such as SQL queries that can be executed based on relational databases or knowledge bases directly. Semantic parsing is a long-standing and difficult problem in natural language processing. In recent studies, researchers usually treat natural language descriptions/questions as input and use different sequence-to-sequence frameworks to generate logical forms  (Xu et al., 2017; Dong and Lapata, 2016) . However, these methods ignore the decompositionality of a complex question which is usually composed of a set of sub-questions, the understanding of each sub-question could contribute to the semantic parsing of the original complex question. Figure  1  gives an example of a complex question and its logical form. The related subquestions in stage-1 and the corresponding predicate (relation) information of each sub-question in stage-2 could help to obtain the logical form of the complex question in stage-3. Question decomposition is important and many previous work utilize the decompositionality of complex questions to help question understanding.  Kalyanpur et al. (2012)  propose to use a suite of decomposition rules for question decomposition. The drawback of rule-based methods is that it needs experts to design rules and the rules are usually with low coverage and hard to be extended to other domains and tasks.  Talmor and Berant (2018)  propose a neural question decomposition approach to answer complex questions. They use the pointer network  (Vinyals et al., 2015)  to generate splitting points in the complex question and separate the complex question into a sequence of simple questions. This neural-based method alleviates the cost of manually designed rules or features. However, sometimes decomposing a complex question by splitting points may not find best sub-questions, and thus lose some information. For example, the sub-question "Who is Obama's daughter?' can not be generated by the splitting points of the complex question in Figure  1 . To address the above problem, we propose to use a more flexible neural generative question decom-poser to directly generate complete and natural sub-questions based on an input complex question, without word order and content restrictions. To parse a complex question into its corresponding logical form, we propose a hierarchical semantic parsing (HSP) model which is designed as a hierarchical neural sequence-to-sequence architecture. The underline idea of our HSP model is decomposition and integration. Specifically, as shown in Figure  1 , our HSP model first decomposes a complex question into sub-question sequence with a question decomposer (QD), and then extracts key semantic information based on the generated sub-questions and the original complex question with an information extractor (IE). Finally, HSP model integrates the previously generated auxiliary information and generates the logical form of the complex question. Our HSP model can be seen as a multi-stage reasoning process, with each stage focusing on different level of information and reducing the search space of logical forms step-by-step by integrating previously generated information. The main contributions of this paper are threefold: 1. We propose an effective and flexible question decomposition method; 2. We propose a hierarchical semantic parsing model based on a sequence-to-sequence paradigm which incorporates a question decomposer and an information extractor; 3. Experimental results demonstrate that the proposed model achieves a significant improvement in semantic parsing performance. 2 Related Work 

 Semantic Parsing Typically, traditional semantic parsing models  (Zettlemoyer and Collins, 2005; Mooney, 2007; Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2014; Yih et al., 2015; Yao, 2015)  are learned based on carefully designed features. For instance,  Kwiatkowski et al. (2011)  propose a combinatory categorical grammar induction technique for semantic parsing with different levels of features.  Liang et al. (2011) ;  Reddy et al. (2014)  build semantic parsers without relying on logical form annotations but through distant supervision.  Xiao et al. (2016) ;  Yin and Neubig (2017)  use syntax information to improve semantic parsing models.  Fan et al. (2017)  apply a transfer learning method in semantic parsing. To alleviate the cost of feature engineering, neural semantic parsing approaches have attracted significant attention  (Jia and Liang, 2016; Dong and Lapata, 2016; Herzig and Berant, 2017; Gardner et al., 2018; Goldman et al., 2018; Chen et al., 2018; . For example,  Jia and Liang (2016)  propose a framework to introduce data recombination and train a sequence-to-sequence model for semantic parsing.  propose to firstly parse a question to a coarse logical form then a fine-grained one based on a neural architecture. However, these approaches miss the opportunity to utilize question decomposition information for complex question semantic parsing. In this work, we leverage a sequence-tosequence architecture and design a neural hierarchical sequence-to-sequence model to capture the syntactic structure, e.g., question decomposition information of complex questions. 

 Question Decomposition Question decomposition has been successfully used in complex question answering  (Kalyanpur et al., 2012; Iyyer et al., 2016; Talmor and Berant, 2018; Song et al., 2018) .  Kalyanpur et al. (2012)  propose a framework using decomposition rules to identify facts in complex questions based on lexicon-syntactic features. The model then leverages the identified facts alone with a question rewriting component and a candidate reranker to generate final ranked answer list. Their work rely on feature engineering and manually designed rules which is difficult to be adapted to applications in other domains.  Iyyer et al. (2016)  propose a method for complex question answering based on tables. To answer complex questions, they split each complex question into several inter-related simple questions by crowd-sourcing, and design an end-to-end neural model to predict the answer based on the simple questions.  Talmor and Berant (2018)  propose a splitting-based question decomposition model to find splitting points in the original complex question and decompose it into a sequence of sub-questions. They then use a machine reading comprehension method to get the answers of each sub-question and compose the answers to obtain answer of the complex question. Sub-questions obtained by this method are usually incomplete. In this work, we propose a neural generative question decomposition approach to directly generate complete and natural sub-questions, which also improves the performance of complex question semantic parsing. 

 Model In this section, we introduce the architecture of our hierarchical semantic parsing model. The model receives complex question inputs and generates logical forms. It combines the sequenceto-sequence paradigm with a hierarchical parsing mechanism in a differentiable way and can be trained end-to-end. 

 Model Overview Our model treats complex questions and logical forms as sequences, learns to generate logical forms for questions. We denote a complex question as x = {x 1 , ? ? ? , x |x| }, and logical form as y = {y 1 , ? ? ? , y |y| }. To better model and generate logical forms, our model utilizes two types of intermediate representations: the decomposed representation(DR) and the semantic representation(SR). DR consists of decomposed simple questions and SR contains key information of the original complex question including question type and all predicates in the question. An example of the two intermediate representations format is shown in Table 1. Decomposed representation is denoted as z = {z 1 , ? ? ? , z |z| } and semantic representation is denoted as w = {w 1 , ? ? ? , w |w| }. Each training sample is a < x, y, z, w > quad. 

 Basic Architecture First we illustrate the basic structure of our model: a parsing unit. A parsing unit consists of an encoder network and a decoder network, based on the multi-head attention encoder/decoder of Transformer  (Vaswani et al., 2017) . Its input has two parts: the input sequence and additional information, and the output is the parsed target se-quence. The input sequence and target sequence are text utterances, and additional information is a sequence of vectors representing encoding for some kind of auxiliary information. In this subsection, we represent the input sequence of the paring unit as a = {a 1 , ? ? ? , a |a| }, input additional information as e = {e 1 , ? ? ? , e |e| }, e i ? R n , and output sequence as o = {o 1 , ? ? ? , o |o| }. 

 Encoder On the encoder side, the parsing unit encodes the input sequence a to context aware representation h = {h 1 , ? ? ? , h |a| }, h i ? R m . We introduce the Transformer encoder  (Vaswani et al., 2017)  here. The encoder first maps the sequence to word representations and then generates the output using a L layer Transformer encoder. The total process is denoted by: h = f enc (a) = f proc enc (f emb enc (a)) (1) 

 Decoder The decoder receives encoder output h and input additional information e, first fuses the two encoded representations by concatenating them to get fused representation [h, e]. At decoder time step t, with fused representation [h, e] and previous decoded output o <t = {o 1 , ? ? ? , o t?1 }, decoder calculates conditional probability P (o t |o <t , [h, e]). First decoder embedding function f emb dec maps previous decoder outputs o <t to word embeddings and add positional encoding to get decoder word representations. Like the encoder, decoder also stacks L identical layer and the word representations are then fed to these layers along with fused representation [h, e]. If we represent the l-th layer output vector of position j as k l j and represent l-th layer previous output as k l ?j = {k l 1 , ? ? ? , k l j }, the decoder layer output is k l j = Layer(k l?1 ?j , [h, e] ). Given the last layer output k L j , the probability of current word P j vocab (w) on target vocabulary  P j vocab (w) = Sof tmax(W o ? a L j + b o ) (2) The decode process is triggered with the start of sequence token "[BOS]" and terminated on the end of sequence token "[EOS]". 

 Copy Mechanism To tackle out-of-vocabulary words, we incorporate copy mechanism  (Gu et al., 2016)  in the decoder. At decode time step t, first we calculate the attention distribution over source sequence a using the bilinear dot product of last layer decoder output k L t and encoder output h, as Eq. 3 4 shows. u i t = k L t W q h i (3) ? t = Sof tmax(u t ) (4) Then we calculate copy probability P t copy ? [0, 1] as following equation. W q , W g , b g are learnable parameters: P t copy = ?(W g ? [k L t , h, e] + b g ) (5) Using P t copy we calculate the weighted sum of copy probability and generation probability to get the final predicted probability of extended vocabulary V + X , where X is set of out of vocabulary words in source sequence a: P t (w) = (1 ? P t copy )P vocab (w) + P t copy i:w i =w ? i t (6) The decoding process is formulated by Eq. 7. Note here that we use f t dec to represent one time step of the decoder with the copy mechanism process. For brevity we roll all time steps of the decoder, using Eq. 8 to denote P (b|[h, e]). o t = f t dec (f emb dec (o <t ), [h, e]) (7) o = f dec ([h, e]) (8) Following is the loss function of the basic architecture with parameters ?, b * t is the target word in time step t: L(?) = 1 T T t=1 ? log P (o t = o * t |a, e, o <t ) (9) 

 Hierarchical Semantic Parsing We now introduce HSP based on the above basic architecture, HSP is a bottom up multi-stage parsing process. Figure  2  illustrates the typical three stage HSP structure of our model, each stage process is similar to the basic architecture we elaborate above. As illustrated in the model overview subsection, we denote the input question as x and the output logical form as y. The train objective of the basic architecture in Eq. 9 directly minimizes the cross entropy between conditional probability P (y|x) and true probability of target sequence P (y * ). HSP mechanism turns the process into a multi-stage process by splitting the objective to several conditional probabilities' products. For our three stage HSP model shown in Figure  2 , the objective is P (y|x, z, w)P (w|x, z)P (z|x), in which z and w represent decomposed representation and semantic representation respectively. 

 Question Decomposer On the first stage of HSP, we design a question decomposer to decompose the complex question to simple question sequences. The input of the question decomposer is the complex question x, and the output is the decomposed representation z. The model first maps the input x to context aware representations h using the question encoder h = f enc 1 (x), at this stage no additional information is given, so fused representation is identical to h. Then with a decomposed decoder, the decomposed representation is predicted: z = f dec 1 (h). In Figure  2  the decoding process is unrolled to time steps and surrounded by a dotted frame, at each time step previous outputs are shifted right and fed into the decoder. The beginning of the blue line pointing to the decoder is fused representation used by the decoder, for question decomposer it is the question embedding. 

 Information Extractor The second stage of HSP extracts key information of complex questions, from the complex question itself and the decomposed simple questions. The input sequence of the information extractor is decomposed representation, additional information is question embedding, and the target output sequence is semantic representation. The encoder process encodes decomposed representation z using sub-question encoder: h z = f enc 2 (z). The fused representation [h, h z ] is then fed into the semantic decoder to decode semantic representation: w = f dec 2 ([h, h z ]). In Figure  2 , the ? notation on the top denotes the representation fusing process. 

 Semantic Parser The final stage of the HSP model is a semantic parser. It receives the context aware embedding of complex question and decomposed representation, and semantic representation sequence. It encodes the semantic representation h w = f enc 3 (w), concatenates the three part of representation [h, h w , h z ], and logical form are predicted conditioned upon the fusing representation: y = f dec 3 ([h, h w , h z ]). While the loss function of the basic architecture is as shown in Eq. 9, the training objective of HSP model is to minimize following loss functions as Eq. 10, where L 1 = ? log P (z|x), L 2 = ? log P (w|x, z) and L 3 = ? log P (y|x, z, w) denotes losses of three stages. ? 1 , ? 2 in the equation are two hyperparameters. L HSP (?) = ? 1 ? L 1 + ? 2 ? L 2 + L 3 (10) During inference, the model uses a three stage inference process, first getting the prediction of decomposed representation ? = argmax z P (z|x), and then predicting semantic representation ? = argmax w P (w|x, z), finally predicting logical form ? = argmax y P (y|x, z, w). Each sequence is obtained using a greedy search method like beam search. From a cognitive view, HSP can be seen as another form of attention mechanism, it helps the model concentrate on the most important semantic part first, and fills other skeletons step by step. From the point of modeling, HSP simplifies the generation by splitting the semantic part with logical form grammars, which simplifies the modeling task of each process. The HSP mechanism can also be regarded as a kind of information flow, the information parsed on the previous stages can provide a soft constraint for the generation process at a later stage. Note that we just introduce one particular form of HSP for semantic parsing in this section. HSP is actually a mechanism that is highly flexible; its structure can be applied to any sequence-tosequence framework and used in many structured sequence generation tasks. 

 Experiment 

 Settings During our experiments, we build a vocabulary for complex questions, all intermediate representations and logical forms. The vocabulary contains up to 30K words, constructed from all words with more than 4 occurrences in the corpus. All out-of-vocabulary words are represented by UNK. Our model uses pre-trained 6B tokens 300 dimensional Glove word embeddings  (Pennington et al., 2014) , for vocabulary words which do not have pre-trained embeddings(including three special words: UNK, BOS and EOS), we assign them uniform randomized values. During training, we update all word embeddings. Our model also uses a pre-trained Stanford-CoreNLP POS model  in the encoder embedding process. We use categorical POS annotations and map them to POS embedding vectors of dimension 30, the POS embedding vectors are initialized from uniform distribution U (?0.1, 0.1) and updated during training. The POS embeddings are concatenated with word embeddings to generate word representations. We fix hidden size of all encoder and decoder units to 300. The encoder and decoder of all HSP models are stacked by 6 identical layers. We train the model using Adam optimizer with ? 1 = 0.9, ? 2 = 0.98 and = 10 ?9 and use dynamic learning rate during training process. For regularization, we use dropout  (Srivastava et al., 2014)  and label smoothing  (Szegedy et al., 2016)  in our models and set the dropout rate to 0.2, set the label smoothing value to 0.1. During training, we train our models using minibatches of 128 samples, all models are trained for at most 20,000 steps, selecting the best model based on development set performance. After one model is trained, we use beam search of beam size 16 to generate logical form sequences. The implementations of our model would be released for further study 1 . 

 Dataset To evaluate the performance of our model on semantic parsing, we conduct experiments on Com-plexWebQuestions(v1.0) dataset  (Talmor and Berant, 2018)  released here 2 , which is built on the WebQuestions dataset  (Berant et al., 2014)  and consists of samples of complex question, decomposed question sequence and sparql format logical form. ComplexWebQuestions is a large scale semantic parsing dataset and contains 27734 training samples, 3480 development and 3475 test samples. The dataset has four types of complex questions: composition (46.7%), conjunctions (42.4%), superlatives (5.3%) and comparatives (5.6%). Each question is either the combination of two simple questions, or an extension of a simple question. We identify entities in logical forms and replace them with placeholders during training and inference. 

 Results We measure model performance by calculating the accuracy of generated logical forms, and compare performance of our approach(HSP) with various competitive baselines. In table 2, SP Unit denote for the semantic parsing unit, it uses the basic structure of HSP model with no intermediate representations, cooperates POS embedding, copy mechanism and Glove word embedding together with the Transformer. Table  2  presents all models' accuracy on development and test set. Note that we treat SP Unit as the performance baseline and calculate other models' accuracy gain or decline compared to it, recorded in parentheses in the table. SP Unit gets 59.91% accuracy on test set, 8.91% higher than Pointer-Generator which matches 51% golden sparql queries. We also observe that the performances of SEQ2SEQ and SEQ2TREE are lower than Pointer-Generator, the two models get 47.3% and 49.68% accuracy on test set. We think the reason is that Pointer-Generator's copy mechanism helps logical form generation. Transformer achieves 53.41% on the test set which is also 6.5% lower than SP Unit but higher than Pointer-Generator. This group of experiment proves that semantic parsing on ComplexWebQuestions is difficult for traditional sequence-to-sequence models, and SP Unit is more effective than some previous systems. The reason is that by combining self-attention with copy mechanism, POS embedding and other modules, SP Unit has good modeling ability for logical forms of complex questions. Coarse2Fine obtains 53.52% accuracy on the test set which is 1.84% lower than SP Unit. Our HSP model outperforms SP Unit by 6.27% accuracy which is a wide margin (with SP Unit as a baseline, the relative improvement of HSP is 10.5%). It proves the effectiveness of HSP mechanism. Compared to other neural semantic parsing models, HSP achieves significant improvement, proving that incorporate sub-questions and key information together boost logical form generation effectively. We think the key reason is that ques- 

 Model Dev (%) Relative perf Test (%) Relative perf SEQ2SEQ  Lapata, 2016) 50.22 -11.47 47.30 -12.61 SEQ2TREE (Dong and Lapata, 2016)  51.87 -9.82 49.68 -10.23 PointerGenerator  (See et al., 2017)  53.10 -8.59 51.00 -8.91 Transformer  (Vaswani et al., 2017)  56.78 -4.91 53.41 -6.50 Coarse2Fine  58 tion decomposition turns the complex question into simple questions and then solves simple questions in a divide-and-conquer manner, which simplify representation learning process of the model in each stage. 

 Ablation Analysis As the above part of  As Figure  3  shows, HSP has highest accuracy on the four type of question samples among the three models. Moreover, the accuracy of Transformer on composition and conjunction questions is comparable to that of Coarse2Fine and lower than HSP, showing that the HSP mechanism helps improve modeling capability. Finally, compared to Transformer, the accuracy of Coarse2Fine and HSP in comparative and superlative questions has been significantly improved, because these two models utilize additional information to enhance the robustness of the model, thus obtaining better results on types with much fewer training samples.  

 Performance on Different Training Data Volumes In Figure  4 , we depict the trends of test set accuracy with different portions of training data. The results of this experiment demonstrate that the performance of the HSP exceeds the other two baselines, regardless of the amount of training data. Moreover, as training data volume increases, the performance improvement that HSP can achieve is higher than the other two models. We think the reason is that as the training resources increase, HSP learns better question decomposer and information extractor and generates more accurate subquestions and key information, which help HSP semantic parser to obtain better logical form results. 

 Question Decomposition Results To further evaluate the effectiveness and generalization ability of our HSP model, we conduct question decomposition experiment with an HSP model variant and compare its performance to several neural models. We use case-insensitive Bleu-4  (Papineni et al., 2002)  and Rouge-L  (Lin, 2004)  as evaluation metrics for question decomposition. For all models, the input is the complex question, and the output is decomposed sub-question sequence with the same format as decomposed representation. Table  3  shows the question decomposition results of different models. PointerNetwork refers to the model  (Talmor and Berant, 2018 ) on splitting the complex question into sub-questions using splitting points predicted by a pointer network model  (Vinyals et al., 2015) . HSP(SR) refers to a two-stage HSP model for which we use semantic representation as intermediate representation. We observe that compared to PointerNetwork, the other two models obtain much better results, prov- We also perform ablation experiments on question decomposition to measure the impact of different modules, the results are also shown in Table 3. We examine four main modules in the HSP model: semantic representation(SR), POS embedding(POS), pre-trained Glove word embedding(Glove) and copy mechanism(Copy), and incrementally remove these modules from HSP(SR). Results show that without semantic representation in HSP, the model's Bleu-4 score decreases 2.6 points and the Rouge-L score decreases 1.9 points. The decrease of Bleu-4 score by removing HSP is only lower than removing the copy mechanism(4.1 points), and Rouge degradation is highest among the four ablation models. It indicates that HSP mechanism is vital for the model. 

 Conclusion In this work, we propose a novel hierarchical semantic parsing (HSP) model based on sequenceto-sequence paradigm. Experiments show that compared to several previous systems, HSP effectively improves performance. We also design a neural generative question decomposer which achieves much higher performance than splittingbased question decomposition approach. Further experiments also prove that the proposed neural generative question decomposer also benefits from the HSP mechanism. Figure 1 : 1 Figure 1: Example of question decomposition(QD), information extraction(IE) and semantic parsing(SP). 
