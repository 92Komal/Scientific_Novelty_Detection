title
Open-Domain Question Answering

abstract


Description Open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics, has been a longstanding problem in NLP, information retrieval (IR) and related fields  (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010) . Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension  (Chen, 2018) , modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models  (Chen et al., 2017; Min et al., 2019a)  or even implemented in a fully end-to-end fashion  Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020) . In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.  1  We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss modern datasets proposed for open-domain QA  (Voorhees et al., 1999; Berant et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019) , as well as common evaluation metrics and benchmarks. We plan to provide 1 All the tutorial materials will be released at https://github.com/danqi/acl2020-openqa-tutorial. a detailed discussion on available datasets -their collection methodology and properties -as well as insights on how these datasets should be viewed in the context of open-domain QA. Next, the focus will shift to cutting-edge models proposed for open-domain QA, which is also the central part of this tutorial. We divide existing models into three main categories: Two-stage retriever-reader approaches, Dense retriever and end-to-end training, and Retriever-free approaches. We will present the logical elements behind different sorts of models and discuss their pros and cons. Two-stage retriever-reader approaches. We will start by discussing two-stage retriever-reader frameworks for open-domain QA, pioneered by  Chen et al. (2017) : a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemented by traditional sparse vector space methods, such as TF-IDF or BM25 and the reader is implemented by neural reading comprehension models. We will further discuss several challenges and techniques arising in this area, including multi-passage training  (Clark and Gardner, 2018; , passage reranking  (Wang et al., 2018; Nogueira and Cho, 2019) , and denoising distantly-supervised data  (Lin et al., 2018) . Dense retriever and end-to-end training. The first category mainly employs a non-machine learning model for the retrieval stage. The second category will focus on how to learn the retriever component by replacing traditional IR methods with dense representations, as well as joint training of both components. Learning and searching in dense vector space is challenging, as it usually involves an enormous search space (easily ranging from millions to billions of documents). We will discuss in depth how this was achieved by existing models, including novel pre-training methods  Guu et al., 2020) , carefully-designed learning algorithms  (Karpukhin et al., 2020)  or a hybrid approach using both dense and sparse representations  (Seo et al., 2019) . Retriever-free approaches. The third category, which is a recent emerging trend, only relies on large-scale pre-trained models  (Radford et al., 2018; Devlin et al., 2018;  as implicit knowledge bases and doesn't require access to text data during inference time. These pretrained models will be used directly to answer questions, in a zero-shot manner  (Radford et al., 2019; Raffel et al., 2019)  or fine-tuned using questionanswer pairs. As these methods don't need a retriever component, we call them Retriever-free approaches. Up to this point, our tutorial has mainly focused on textual question answering. At the end, we also plan to discuss some hybrid approaches for answering open-domain questions using both text and large knowledge bases, such as Freebase  (Bollacker et al., 2008)  and Wikidata  (Vrande?i? and Kr?tzsch, 2014) , and give a critical review on how structured data complements the information from unstructured text. The approaches include (1) how to leverage structured data to guide the retriever or reader stage of existing textual QA systems  (Asai et al., 2020; Min et al., 2019b) , or (2) how to synthesize information from these two heterogeneous sources and build effective QA models on the combined information  (Sun et al., , 2019 . Finally, we will discuss some important questions, including (1) How much progress have we made compared to the QA systems developed in the last decade? (2) What are the main challenges and limitations of current approaches? (3) How to trade off the efficiency (computational time and memory requirements) and accuracy in the deep learning era? We hope our tutorial will not only serve as a useful resource for the audience to efficiently acquire up-to-date knowledge, but also provide new perspectives to stimulate the advances of open-domain QA research in the next phase. Prerequisites The tutorial will be accessible to anyone who has the basic knowledge of machine learning and natural language processing. The tutorial will target both NLP researchers/students in academia and NLP practitioners in industry. 

 Tutorial Outline The intended duration of this tutorial is 3.5 hours, including a half an hour break.   history of open-domain (textual) QA (a) Early QA systems (b) TREC QA competitions (c) IBM's DeepQA project (d) More recent developments: 2017-2020 4. Datasets & evaluation (a) Reading comprehension vs QA datasets (b) Categorization of QA datasets (c) Evaluation metrics 5. Two-stage retriever-reader approaches (a) General framework (b) Multi-passage training (c) Passage reranking (d) Denoising distantly supervised data 6. Dense retriever and end-to-end training (a) Dense passage retrieval (b) Joint training of retriever and reader (c) Dense-sparse phrase indexing 7. Retriever-free approaches 8. Open-domain QA using KBs and text (a) Improving retriever and reader using structured KBs (b) Answering questions over combined KBs and text 9. Open problems and future directions 

 Scott Wen-tau Yih Scott Wen-tau Yih is a Research Scientist at Facebook AI Research (FAIR), and his recent research focuses on continuous representations and neural network models, with applications in knowledge base embedding, semantic parsing and question answering. Yih received the best paper award from CoNLL'11, an outstanding paper award from ACL'15 and has served as an area co-chair and a program co-chair for several top conferences. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling, Deep Learning for NLP, Question Answering with Knowledge Base, Web and Beyond and NLP for Precision Medicine. Website: http://scottyih.org/. such as question answering and information extrac- tion. Before joining Princeton University, Danqi worked as a visiting scientist at Facebook AI Re- search (FAIR). She received her PhD from Stanford University (advised by Christopher Manning) in 2018 and B.Eng from Tsinghua University in 2012. Website: https://www.cs.princeton.edu/?danqic/. 3 Presenters Danqi Chen Danqi Chen is an Assistant Profes- sor of Computer Science at Princeton University and co-directs the Princeton NLP Group. Danqi's research interests lie within deep learning for natu- ral language processing, with an emphasis on the intersection between text understanding and knowl- edge representation/reasoning and applications
