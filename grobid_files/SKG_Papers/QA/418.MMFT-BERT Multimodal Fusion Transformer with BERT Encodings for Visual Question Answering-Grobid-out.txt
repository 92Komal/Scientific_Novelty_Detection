title
MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering

abstract
We present MMFT-BERT (MultiModal Fusion Transformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually and using a novel transformerbased fusion method to fuse them together. Our method decomposes the different sources of modalities, into different BERT instances with similar architectures, but variable weights. This achieves SOTA results on the TVQA dataset. Additionally, we provide TVQA-Visual, an isolated diagnostic subset of TVQA, which strictly requires the knowledge of visual (V) modality based on a human annotator's judgment. This set of questions helps us to study the model's behavior and the challenges TVQA poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method 1 .

Introduction In the real world, acquiring knowledge requires processing multiple information sources such as visual, sound, and natural language individually and collectively. As humans, we can capture experience from each of these sources (like an isolated sound); however, we acquire the maximum knowledge when exposed to all sources concurrently. Thus, it is crucial for an ideal Artificial Intelligence (AI) system to process modalities individually and jointly. One of the ways to understand and communicate with the world around us is by observing the environment and using language (dialogue) to interact with it . A smart We treat input modalities as a sequence. [FUSE] is a trainable vector; h q 0 j , h v 0 j , and h s 0 j are fixed-length features aggregated over question-answer (QA) pairs, visual concepts, and subtitles. Using a transformer encoder block,  [FUSE]  attends all source vectors and assigns weights based on the importance of each input source. Training end to end for VQA enables the MMFT module to learn to aggregate input sources w.r.t. the nature of the question. For illustration purposes, we show that for a single head, MMFT collects more knowledge from the visual source h v 0 j (green colored) than from the QA and subtitles. Best viewed in color. system, therefore, should be able to process visual information to extract meaningful knowledge as well as be able to use that knowledge to tell us what is happening. The story is incomplete if we isolate the visual domain from language. Now that advancements in both computer vision and natural language processing are substantial, solving problems demanding multimodal understanding (their fusion) is the next step. Answering questions about what can be seen and heard lies somewhere along this direction of investigation. In research towards the pursuit of combining language and vision, visual features are extracted using pre-trained neural networks for visual perception  (He et al., 2016; Ren et al., 2015) , and word embeddings are obtained from pre-trained language models  (Mikolov et al., 2013b,a; Pennington et al., 2014; Devlin et al., 2018)  and these are merged to process multiple modalities for various tasks: visual question answering (VQA), visual reasoning, visual grounding. TVQA , a video-based ques-tion answering dataset, is challenging as it provides more realistic multimodal question answers (QA) compared to other existing datasets . To answer TVQA questions, the system needs an understanding of both visual cues and language. In contrast, some datasets are focused either visually: MovieFIB  (Maharaj et al., 2017) , Video Context QA  (Zhu et al., 2017) , TGIF-QA  (Jang et al., 2017) ; or by language: MovieQA  (Tapaswi et al., 2016) ; or based on synthetic environments: Mari-oQA  (Mun et al., 2017)  and PororoQA . We choose TVQA because of its challenges. The introduction of transformers  (Vaswani et al., 2017)  has advanced research in visual question answering and shows promise in the field of language and vision in general. Here, we adopt the pretrained language-based transformer model, BERT  (Devlin et al., 2018)  to solve the VQA task. The human brain has vast capabilities and probably conducts processing concurrently. Like humans, an intelligent agent should also be able to process each input modality individually and collectively as needed. Our method starts with independent processing of modalities and the joint understanding happens at a later stage. Therefore, our method is one step forward toward better joint understanding of multiple modalities. We use separate BERT encoders to process each of the input modalities namely Q-BERT, V-BERT and S-BERT to process question (Q), video (V), and subtitles (S) respectively. Each BERT encoder takes an input source with question and candidate answer paired together. This is important because we want each encoder to answer the questions targeted at its individual source input. Thus, pairing up the question and candidate answers enables each stream to attend to the relevant knowledge pertinent to the question by using a multi-head attention mechanism between question words and a source modality. We then use a novel transformer based fusion mechanism to jointly attend to aggregated knowledge from each input source, learning to obtain a joint encoding. In a sense, our approach is using two levels of question-to-input attention: first, inside each BERT encoder to select only relevant input; and second, at the fusion level, in order to fuse all sources to answer the common question. We show in our experiments that using Q-BERT, a separate BERT encoder for question and answer is helpful. 

 Our contribution is three-fold: First, we propose a novel multi-stream end-to-end trainable architecture which processes each input source separately followed by feature fusion over aggregated source features. Instead of combining input sources before input to BERT, we propose to process them individually and define an objective function to optimize multiple BERTs jointly. Our approach achieves state-of-the-art results on the video-based question answering task. Second, we propose a novel MultiModal Fusion Transformer (MMFT) module, repurposing transformers for fusion among multiple modalities. To the best of our knowledge, we are the first to use transformers for fusion. Third, we isolate a subset of visual questions, called TVQA-Visual (questions which require only visual information to answer them). Studying our method's behavior on this small subset illustrates the role each input stream is playing in improving the overall performance. We also present detailed analysis on this subset. 

 Related Work Image-based Question Answering. Image-based VQA  (Yu et al., 2015; Antol et al., 2015; Zhu et al., 2016; Jabri et al., 2016; Chao et al., 2018)  has shown great progress recently. A key ingredient is attention  (Ilievski et al., 2016; Chen et al., 2015; Yu et al., 2017a,b; Xu and Saenko, 2016; Anderson et al., 2018) . Image based VQA can be divided based on the objectives such as generic VQA on real world images  (Antol et al., 2015; Goyal et al., 2017) , asking binary visual questions  and reasoning based VQA collecting visual information recurrently  (Kumar et al., 2016; Xiong et al., 2016; Weston et al., 2014; Sukhbaatar et al., 2015; Hudson and Manning, 2018)  to answer the question both in synthetic  (Johnson et al., 2016; Yang et al., 2018; Suhr et al., 2017)  as well as real image datasets  (Hudson and Manning, 2019) . Video-based Question Answering. Video-based QA is more challenging as it requires spatiotemporal reasoning to answer the question.  introduced a video-based QA dataset along with a two-stream model processing both video and subtitles to pick the correct answer among candidate answers. Some studies are: grounding of spatiotemporal features to answer questions  (Lei et al., 2019) ; a video fill in the blank version of VQA  (Mazaheri et al., 2017) ; other examples include  (Kim et al., 2019b,a; Zadeh et al., 2019; Yi et al., 2019; Mazaheri and Shah, 2018) . Figure  2 : Overview of the proposed approach. Q-BERT, V-BERT and S-BERT represent text encoder, visual encoder and subtitles encoder respectively. If h j =Q+A j is j th hypothesis, then Q-BERT takes h j , V-BERT takes visual concepts V+h j , and S-BERT takes subtitles S+h j as inputs respectively. The aggregated features from each BERT are concatenated with [FUSE], a special trainable vector, to form a sequence and input into the MMFT module (see section 3.2.4 for details). Outputs from the MMFT module for each answer choice are concatenated together and are input into a linear classifier to obtain answer probabilities. We optimize individual BERTs along with optimizing the full model together. Loss total denotes our objective function used to train the proposed architecture. At inference time, we take features only from the MMFT module. Representation Learning. BERT has demonstrated effective representation learning using selfsupervised tasks such as masked language modeling and next sentence prediction tasks. The pretrained model can then be finetuned for a variety of supervised tasks. QA is one such task. A singlestream approach takes visual input and text into a BERT-like transformer-based encoder; examples are: VisualBERT  (Li et al., 2019b) , VL-BERT  (Su et al., 2019) , Unicoder-VL  (Li et al., 2019a)  and B2T2  (Alberti et al., 2019) . Two-stream approaches need an additional fusion step; ViLBERT  and LXMERT  (Tan and Bansal, 2019)  employ two modality-specific streams for images. We take this a step further by employing three streams. We use a separate BERT encoder for the question-answer pair. We are specifically targeting video QA and do not need any additional pre-training except using pre-trained BERT. 

 Approach Our approach permits each stream to take care of the questions requiring only that input modality. As an embodiment of this idea, we introduce the Mul-tiModal Fusion Transformer with BERT encodings (MMFT-BERT) to solve VQA in videos. See fig.  1  for the proposed MMFT module and fig.  2  for illustration of our full architecture. 

 Problem Formulation In this work, we assume that each data sample is a tuple (V, T, S, Q, A, l) comprised of the following:V : input video; T : T = [t start ,t end ], i.e., start and end timestamps for answer localization in the video; S: subtitles for the input video; Q: question about the video and/or subtitles; A: set of C answer choices; l: label for the correct answer choice. Given a question with both subtitles and video input, our goal is to pick the correct answer from C candidate answers. TVQA has 5 candidate answers for each question. Thus, it becomes a 5-way classification problem. 

 MultiModal Fusion Transformer with BERT encodings (MMFT-BERT) 

 Q-BERT: Our text encoder named Q-BERT takes only QA pairs. The question is paired with each candidate answer A j , where, j = 0, 1, 2, 3, 4; |A| = C. BERT uses a special token [CLS] to obtain an aggregated feature for the input sequence, and uses [SEP] to deal with separate sentences. We, therefore, use the output corresponding to the [CLS] token as the aggregated feature from Q-BERT and [SEP] is used to treat the question and the answer choice as sep- arate sentences. The input I to the text encoder is formulated as: I q j = [CLS] + Q + [SEP] + A j , (1) where, + is the concatenation operator, [CLS] and [SEP] are special tokens, Q denotes the question, and A j denotes the answer choice j, I q j is the input sequence which goes into Q-BERT and represents the combination of question and the j th answer. We initiate an instance of the pre-trained BERT to encode each of the I q j sequences: h q 0 j = Q-BERT (I q j )[0], (2) where [0] denotes the index position of the aggregated sequence representation for only textual input. Note that, the [0] position of the input sequence is [CLS]. 

 V-BERT: We concatenate each QA pair with the video to input to our visual encoder V-BERT. V-BERT is responsible for taking care of the visual questions. Pairing question and candidate answer with visual concepts allows V-BERT to extract visual knowledge relevant to the question and paired answer choice. Input to our visual encoder is thus formulated as follows: I v j = [CLS] +V + "." + Q + [SEP] + A j , (3) where, V is the sequence of visual concepts 2 , "." is used as a special input character, I v j is the input sequence which goes into our visual encoder. h v 0 j = V -BERT (I v j )[0], (4) where, [0] denotes the index position of the aggregated sequence representation for visual input. 2 Visual concepts is a list of detected object labels using FasterRCNN  (Ren et al., 2015)  pre-trained on Visual Genome dataset. We use visual concepts provided by . 

 S-BERT: The S-BERT encoder applies attention between each QA pair and subtitles and results in an aggregated representation of subtitles and question for each answer choice. Similar to the visual encoder, we concatenate the QA pair with subtitles as well; and the input is: I s j = [CLS] + S + "." + Q + [SEP] + A j , (5) where, S is the subtitles input, I s j is the resulting input sequence which goes into the S-BERT encoder. h s 0 j = S-BERT (I s j )[0]. (6) where, [0] denotes the index position of the aggregated sequence representation for subtitles input. 

 Fusion Methods Let I i ? R d denote the feature vector for i th input modality with total n input modalities I 1 , I 2 , ..., I n , d represents the input dimensionality. We discuss two possible fusion methods: Simple Fusion: A simple fusion method is a Hadamard product between all input modalities and given as follows: h FUSE = I 1 I 2 ... I n , (7) where, h FUSE is the resulting multimodal representation which goes into the classifier. Despite being extremely simple, this method is very effective in fusing multiple input modalities. 

 MultiModal Fusion Tranformer (MMFT): The MMFT module is illustrated in fig.  1 . We treat I i as a fixed d-dimensional feature aggregated over input for modality i. Inspired by BERT  (Devlin et al., 2018) , we treat aggregated input features from multiple modalities as a sequence of features by concatenating them together. We concatenate a special trainable vector [FUSE] 3 as the first feature vector of this sequence. The final hidden state output corresponding to this feature vector is used as the aggregated sequence representation over input from multiple modalities denoted as h FUSE . h FUSE = MMFT (I 1 + I 2 + ... + I n )[0], (8) where, + is the concatenation operator, [0] indicates the index position of the aggregated sequence representation over all input modalities. In our case, we have three input types: QA pair, visual concepts and subtitles. For inputs i = {1, 2, 3} and answer index j = {0, 1, 2, 3, 4}, the input to our MMFT module is I 1 = h q 0 j , I 2 = h v 0 j , and I 3 = h s 0 j and the output is h FUSE denoting hidden output corresponding to the [FUSE] vector. Here, h q 0 j , h v 0 j , and h s 0 j are the aggregated outputs we obtain from Q-BERT, V-BERT and S-BERT respectively. 

 Joint Classifier Assuming a hypothesis for each tuple (V, T, S, Q, A j ), where A j ? A; j = 0, .., 4 denotes five answer choices, our proposed Transformer Fusion module outputs h FUSE j ? R d . We concatenate the aggregated feature representation for all the answers together and send this to a joint classifier to produce 5 answer scores, as follows: h f inal = h FUSE 0 + h FUSE 1 + ... + h FUSE 4 , ( 9 ) scores joint = classi f ier joint (h f inal ), (10 ) where, h f inal ? R C?d and scores joint ? R C , C denotes number of classes. 

 Objective Function Along with joint optimization, each of the Q-BERT, V-BERT and S-BERT are optimized with a single layer classifier using a dedicated loss function for each of them. Our objective function is thus composed of four loss terms: one each to optimize each of the input encoders Q-BERT, V-BERT and S-BERT, and a joint loss term over classification using the combined feature vector. The formulation of the final objective function is as follows: L total = L q + L vid + L sub + L joint , (11) where, L q , L vid , L sub , and L joint denote loss functions for question-only, video, subtitles, and joint loss respectively; all loss terms are computed using softmax cross-entropy loss function using label l. The model is trained end-to-end using L total . 3 [FUSE] is initialized as a d-dimensional zero vector. Input Model Acc (%) Q+V MTL  (Kim et al., 2019a)  44.42 Two-stream     68.48 STAGE  (Lei et al., 2019)  70.23 WACV20  (Yang et al., 2020)    All models are trained with localized input (w/ ts). 

 Dataset In TVQA, each question (Q) has 5 answer choices. It consists of 152K QA pairs with 21.8K video clips. Each question-answer pair has been provided with the localized video V to answer the question Q, i.e., start and end timestamps are annotated. Subtitles S have also been provided for each video clip. See supplementary work for a few examples. 

 TVQA-Visual To study the behavior of state-of-the-art models on questions where only visual information is required to answer the question correctly, we selected 236 such visual questions. Due to imperfections in the object detection labels, only approximately 41% of these questions have the adequate visual input available. We, therefore, refer to TVQA-Visual in two settings: TVQA-Visual (full):full set of 236 questions. A human annotator looked into the video carefully to ensure that the raw video is sufficient to answer the question without using subtitles. TVQA-Visual (clean): This is the subset of 96 questions where the relevant input was available, yet the models perform poorly. For this subset, we rely on a human annotator's judgement who verified that either the directly related visual concept or the concepts hinting toward the correct answer are present in the list of detected visual concepts. For instance, if the correct answer is "kitchen", ei-  Solid lines: validation accuracy, dotted lines: visual set accuracy. Although S-BERT is significantly above V-BERT for full validation set, for visual set, we can see that V-BERT is well above Q-BERT and S-BERT. This shows that each BERT contributes to the questions it is responsible for. Numbers are log-scaled. ther "kitchen" or related concepts (e.g. "stove", "plate", "glass", etcetera) should be present in the list of visual concepts. Thus, this easier subset is termed as TVQA-Visual (clean). TVQA-visual, although small, is a diagnostic video dataset for systematic evaluation of computational models on spatio-temporal question answering tasks and will help in looking for ways to make the V-stream contribution more effective. See supplementary material for the distribution of visual questions based on reasons for failure. If a model is correctly answering TVQA-visual questions which are not "clean" (the relevant concepts are missing from the visual input), that is because of statistical bias in the data. LSTM for question and each answer choice is concatenated and is input to a 5-way classifier to output 5 answer probability scores.  

 Experiments and Results 

 5 

 MMFT-BERT For video representation, we use detected attribute object pairs as visual features provided by . We follow  and only unique attribute-object pairs are kept. Q-BERT, V-BERT and S-BERT are initialized with BERT base pre-trained on lower-cased English text with masked language modeling task. The MMFT module uses single transformer encoder layer (L=1) with multi-head attention. We use 12 heads (H=12) for multi-head attention in the MMFT module for our best model. We initialize the MMFT module with random weights. A d-dimensional hidden feature output corresponding to [CLS] token is used as an aggregated source feature from each BERT. We concatenate these aggregated features for each candidate answer together to acquire a feature of size 5 ? d. A 5-way classifier is then used to optimize each of Q-BERT, V-BERT and S-BERT independently. For joint optimization of the full model, we treat the encoders' output as a sequence of features with the order [[FUSE], h q 0 j , h v 0 j , h s 0 j ] and input this into the MMFT module ([FUSE] is a trainable d-dimensional vector parameter). Output corresponding to  [FUSE]  token is treated as an accumulated representation h FUSE j over all input modalities for answer j. We concatenate h FUSE j for each answer choice to obtain h f inal for the joint classification. We learn four linear layers, one on top of each of the three input encoders and the MMFT encoder respectively. Thus, each linear layer takes a (5 ? d)-dimensional input and produces 5 prediction scores. Training Details. The entire architecture was implemented using Pytorch  (Paszke et al., 2019)  framework. All the reported results were obtained using the Adam optimizer (Kingma and Ba, 2014) with a minibatch size of 8 and a learning rate of 2e-5. Weight decay is set to 1e-5. All the experiments were performed under CUDA acceleration with two NVIDIA Turing (24GB of memory) GPUs. In all experiments, the recommended train / validation / test split was strictly observed. We use the 4th last layer from each BERT encoder for aggregated source feature extraction. The training time varies based on the input configuration. It takes ?4 hrs to train our model with Q+V and ?8-9 hrs to train on the full model for a single epoch. All models were trained for 10 epochs. Our method achieves its best accuracy often within 5 epochs. 

 Results All results here use the following hyperparameters: input sequence length max seq len=256, # heads H=12, # encoder layers L=1 for the MMFT module, and pre-trained BERT base weights for Q-BERT, V-BERT and S-BERT unless specified explicitly. With timestamp annotations (w/ ts). Columns with "w/ ts" in table  1  show results for input with timestamp localization. We get consistently better results when using localized visual concepts and subtitles. We get 1.7% and 0.65% improvement over WACV20  (Yang et al., 2020)  with simple fusion for Q+V and Q+V+S inputs respectively. When using the MMFT for fusion, our method achieves SOTA performance with all three input settings: Q+V (? 2.41%), Q+S (? 0.14) and Q+V+S (? 1.1%) (see table  1 ). Our fusion approach contributes to improved performance and gives best results for localized input. See also train our model on full length visual features and subtitles. Our method with simple fusion and MMFT on Q+V input outperforms Two-stream ) by absolute 6.49% and 5.59% with simple fusion and MMFT respectively. We truncate the input sequence if it exceeds max seq len. Subtitles without timestamps are very long sequences ( 49% of subtitles are longer than length 256), hence QA pair might be truncated. Thus, we rearrange our input without timestamps as follows: "Q [SEP] A j . V " and "Q [SEP] A j . S" for V-BERT and S-BERT respectively. Models with Q+S input are trained with max seq len=512 and Q+V+S models are trained with max seq len=256 due to GPU memory constraints. For Q+S and Q+V+S, we observe 69.92% and 65.55% with simple fusion, using MMFT produces 69.98% and 66.10% val. accuracy respectively. Results on test set. TVQA test-public set does not provide answer labels and requires submission of the model's predictions to the evaluation server. Only limited attempts are permitted. The server's evaluation results are shown in table  2 . MMFT improves results by (? 6.39%) on Q+V. For Q+V+S, WACV20 reported 73.57% accuracy with a different input arrangement than MMFT. When compared with the model with the same input, MMFT performs slightly better (? 0.17%). Due to limited chances for submission to the test server for evaluation, the reported accuracy for Q+V+S is from one of our earlier models, not from our best model. 

 Model Analysis Performance analysis on TVQA-Visual. To study the models, we evaluate Two-stream , WACV20  (Yang et al., 2020)  and our method on both TVQA-Visual (full) and TVQA-Visual (clean). See table 4 for full results. TVQA-Visual (full): Our method outperforms Two-stream  by 10.08% but drops by 0.43% compared to WACV20  (Yang et al., 2020) . TVQA-Visual (full) has approximately 59% of the questions with missing visual concept or require extra visual knowledge. All three models including ours were trained on visual concepts. Inadequate input, therefore, makes it difficult for the models to attend the missing information. TVQA-Visual (clean): We observe (? 11.46%) and (? 4.17%) improvement for clean set compared to Two-stream and WACV20. TVQA-Visual (clean) has relevant visual concepts or related concepts to the answer present in the input. Yet, it is challenging for existing methods (including ours) to perform well. Although our model observes significant improvement (?4-11%) over baselines for this experiment, the take away message is that it is not enough. This subset of TVQA, therefore, serves as a good diagnostic benchmark to study the progress of exploiting visual features for multimodal QA tasks. Performance analysis w.r.t multimodal attention. We study the behavior of the MMFT module for aggregating multimodal source inputs (Q, V, and S), we take our best model trained on all three sources, and evaluate it on questions which need knowledge about either the visual world, dialogue or both. We then visualize the average attention score map over all heads inside MMFT module (H=12) for each candidate answer, see fig.  5 . Top 2 rows show attention scores computed among all 3 input sources and the  [FUSE]  vector for visual questions. Since,  [FUSE]  is the aggregated output over all input modalities. For instance, visual part should contribute more if the question is about the visual world. We can see the attention map for the correct answer has high attention scores between V and [FUSE] vector. The incorrect answers attend to the wrong sources (either Q or S). Similar is the behavior for rows 3-5, where the question is about subtitles, and the correct answer gives most weight to the subtitles compared to the incorrect answers. Heatmaps for incorrect answers are either focused more on a wrong single input source or the combination of them. Positional Encodings for V-BERT. Positional encoding is done internally in BERT. When finetuned, for V-BERT, the positional encoding has no effect. This has been verified by training our Q+V model with simple fusion (Ours-SF), where the input to V-BERT is a shuffled sequence of objects; no drastic difference was observed (shuffled: 50.32% vs. not shuffled: 50.65%). 

 Ablations All ablations were done with Q+V+S input. See features pooled over time along with visual concepts. We used question-words-to-region attention for aggregating visual features; adding this aggregated visual feature to Ours-SF hurts the performance (71.82%); using object labels was consistently more useful than visual features in various other experimental settings. 

 Conclusion Our method for VQA uses multiple BERT encodings to process each input type separately with a novel fusion mechanism to merge them together. We repurpose transformers for using attention between different input sources and aggregating the information relevant to the question being asked. A Supplementary Material 
