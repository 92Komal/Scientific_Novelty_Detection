title
CNN for Text-Based Multiple Choice Question Answering

abstract
The task of Question Answering is at the very core of machine comprehension. In this paper, we propose a Convolutional Neural Network (CNN) model for textbased multiple choice question answering where questions are based on a particular article. Given an article and a multiple choice question, our model assigns a score to each question-option tuple and chooses the final option accordingly. We test our model on Textbook Question Answering (TQA) and SciQ dataset. Our model outperforms several LSTM-based baseline models on the two datasets.

Introduction Answering questions based on a particular text requires a diverse skill set. It requires look-up ability, ability to deduce, ability to perform simple mathematical operations (e.g. to answer questions like how many times did the following word occur?), ability to merge information contained in multiple sentences. This diverse skill set makes question answering a challenging task. Question Answering (QA) has seen a great surge of more challenging datasets and novel architectures in recent times. Question Answering task may require the system to reason over few sentences  (Weston et al., 2015) , table  (Pasupat and Liang, 2015) , Wikipedia passage  (Rajpurkar et al., 2016; Yang et al., 2015) , lesson  (Kembhavi et al., 2017) . Increase in the size of the datasets has allowed researchers to explore different neural network architectures  Cui et al., 2016; Xiong et al., 2016; Trischler et al., 2016)  for this task. Given a question based on a text, the model needs to attend to a specific portion of the text in order to answer the question. Hence, the use of attention mechanism  is common in these architectures. Convolutional Neural Networks (CNN) have been shown to be effective for various natural language processing tasks such as sentiment analysis, question classification etc.  (Kim, 2014) . However for the task of question answering, Long Short Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  based methods are the most common. In this paper we build a CNN based model for multiple choice question answering 1 . We show the effectiveness of the proposed model by comparing it with several LSTM-based baselines. The main contributions of this paper are (i) The proposed CNN model performs comparatively or better than LSTM-based baselines on two different datasets.  (Kembhavi et al., 2017; Welbl et al., 2017)  (ii) Our model takes question-option tuple to generate a score for the concerned option. We argue that this is a better strategy than considering questions and options separately for multiple choice question answering. For example, consider the question "The color of the ball is" with three options: red, green and yellow. If the model generates a vector which is to be compared with the three option embeddings, then this might lead to error since the three option embeddings are close to each other. (iii) We have devised a simple but effective strategy to deal with questions having options like none of the above, two of the above, all of the above, both (a) and (b) etc. which was not done before. (iv) Instead of attending on words present in the text, our model attends at sentence level. This helps the model for answering look-up questions since the necessary information required to answer such questions will often be contained in a single sentence. 

 Method Given a question based on an article, usually a small portion of article is needed to answer the concerned question. Hence it is not fruitful to give the entire article as input to the neural network. To select the most relevant paragraph in the article, we take both the question and the options into consideration instead of taking just the question into account for the same. The rationale behind this approach is to get the most relevant paragraphs in cases where the question is very general in nature. For example, consider that the article is about the topic carbon and the question is "Which of the following statements is true about carbon?". In such a scenario, it is not possible to choose the most relevant paragraph by just looking at the question. We select the most relevant paragraph by word2vec based query expansion  (Kuzi et al., 2016)  followed by tf-idf score (Foundation, 2011). 

 Neural Network Architecture We use word embeddings  (Mikolov et al., 2013)  to encode the words present in question, option and the most relevant paragraph. As a result, each word is assigned a fixed d-dimensional representation. The proposed model architecture is shown in Figure  1 . Let q, o i denote the word embeddings of words present in the question and the i th option respectively. Thus, q ? R d?lq and o i ? R d?lo where l q and l o represent the number of words in the question and option respectively. The question-option tuple (q, o i ) is embedded using Convolutional Neural Network (CNN) with a convolution layer followed average pooling. The convolution layer has three types of filters of sizes f j ?d ?j = 1, 2, 3 with size of output channel of k. Each filter type j produces a feature map of shape (l q + l o ? f j + 1) ? k which is average pooled to generate a k-dimensional vector. The three kdimensional vectors are concatenated to form 3kdimensional vector. Note that  Kim (2014)  used max pooling but we use average pooling to ensure different embedding for different question-option tuples. Hence, h i = CN N ([q; o i ]) ?i = 1, 2, .., n q (1) where n q is the number of options, h i is the output of CNN and [q; o i ] denotes the concatenation of q and o i i.e. [q; o i ] ? R d?(lq+l 0 ) . The sentences in the most relevant paragraph are embedded using the same CNN. Let s j denote the word embeddings of words present in the j th sentence i.e. s j ? R d?ls where l s is the number of words in the sentence. Then, d j = CN N (s j ) ?j = 1, 2, .., n sents (2) where n sents is the number of sentences in the most relevant paragraph and d j is the output of CNN. The rationale behind using the same CNN for embedding question-option tuple and sentences in the most relevant paragraph is to ensure similar embeddings for similar questionoption tuple and sentences. Next, we use h i to attend on the sentence embeddings. Formally, a ij = h i ? d j ||h i ||.||d j || (3) r ij = exp(a ij ) nsents j=1 exp(a ij ) (4) m i = nsents j=1 r ij d j (5) where ||.|| signifies the l 2 norm, exp(x) = e x and h i ? d j is the dot product between the two vectors. Since a ij is the cosine similarity between h i and d j , the attention weights r ij give more weighting to those sentences which are more relevant to the question. The attended vector m i can be thought of as the evidence in favor of the i th option. Hence, to give a score to the i th option, we take the cosine similarity between h i and m i i.e. score i = h i ? m i ||h i ||.||m i || (6) Finally, the scores are normalized using softmax to get the final probability distribution. p i = exp(score i ) nq i=1 exp(score i ) (7) where p i denotes the probability for the i th option. 

 Dealing with forbidden options We refer to options like none of the above, two of the above, all of the above, both (a) and (b) as forbidden options. During training, the questions CNN h 1 (q, o 1 ) CNN h 2 (q, o 2 ) CNN h n q (q, o nq ) CNN d 1 (s 1 ) CNN d n sents (s n sents ) Attention Layer Score Calculation Final ProbabilityDistribution m 1 m 2 m nq Figure  1 : Architecture of our proposed model. Attention layer attends on sentence embeddings d j 's using question-option tuple embeddings h i 's. Score Calculation layer calculates the cosine similarity between m i and h i which is passed through softmax to get the final probability distribution. having a forbidden option as the correct option were not considered. Furthermore, if a question had a forbidden option, that particular questionoption tuple was not taken into consideration. Let S = [score i ?i | i th option not in forbidden options] and |S| = k. During prediction, the questions having one of the forbidden options as an option are dealt with as follows: 1. Questions with none of the above/ all of the above option: If the max(S) ? min(S) < threshold then the final option is the concerned forbidden option. Else, the final option is argmax(p i ). 2. Questions with two of the above option: If the S (k) ?S (k?1) < threshold where S (n) denotes the n th order statistic, then the final option is the concerned forbidden option. Else, the final option is argmax(p i ). 3. Questions with both (a) and (b) type option: For these type of questions, let the corresponding scores for the two options be score i 1 and score i 2 . If the |score i 1 ? score i 2 | < threshold then the final option is the concerned forbidden option. Else, the final option is argmax(p i ). 

 Questions with any of the above option: Very few questions had this option. In this case, we always choose the concerned forbidden option. We tried different threshold values ranging from 0.0 to 1.0. Finally, the threshold was set to a value gave the highest accuracy on the training set for these kind of questions. 

 Training Details We tried two different CNN models, one having f j 's equal to 3,4,5 and other having f j 's equal to 2,3,4. We refer to two models as CN N 3,4,5 and CN N 2,3,4 respectively. The values of hyperparameters used are: d = 300, k = 100. The other hyperparamters vary from dataset to dataset. Since the number of options vary from question to question, our model generates the probability distribution over the set of available options. Similarly, the number of sentences in the most relevant paragraph can vary from question to question, so we set a ij = ? whenever d j was a zero vector. Cross entropy loss function was minimized during training. 

 Results and Discussion The accuracy of our proposed model on validation set of TQA and SciQ dataset  (Kembhavi et al., 2017; Welbl et al., 2017)  is given in Table  1 and  Table 2 . GRU bl refers to the model where CNN is replaced by Gated Recurrent Unit (GRU)  to embed question-option tuples and the sentences. The size of GRU cell was 100. For SciQ dataset, we used the associated passage provided with the question. AS Reader  (Kadlec et al., 2016)  which models the question and the paragraph using GRU followed by attention mechanism got 74.1% accuracy on the SciQ test set. However, for a question, they used a different corpus to extract the text passage. Hence it is not judicious to compare the two models.   1 : Accuracy for true-false and multiple choice questions on validation set of TQA dataset. can be seen from the Tables  1 and 2, CN N 2,3,4  gives the best performance on the validation set of both the datasets so we evaluate it on the test sets. Note that GRU bl highly overfits on the SciQ dataset which shows that CNN-based models work better for those datasets where long-term dependency is not a major concern. This rationale is also supported by the fact that CN N 2,3,4 performed better than CN N 3,4,5 on the two datasets. 

 Model Accuracy GRU bl 68.2% CN N 3,4,5 87.1% CN N 2,3,4 87.8% CN N 2,3,4 84.7% (test-set) Table  2 : Accuracy of the models on SciQ dataset. The first three accuarcies are on validation set. The last accuracy is of CN N 2,3,4 model on the test set. Baselines for TQA dataset: Three baselines models are mentioned in  Kembhavi (2017)  . These baseline models rely on word-level attention and encoding question and options separately. The baseline models are random model, Text-Only model and BIDAF Model  (Seo et al., 2016) . Text-Only model is a variant of Memory network  (Weston et al., 2014)  where the paragraph, question and options are embedded separately using LSTM followed by attention mechanism. In BIDAF Model, character and word level embedding is used to encode the question and the text followed by bidirectional attention mechanism. This model predicts the subtext within the text containing the answer. Hence, the predicted subtext is compared with each of the options to select the final option. Note that the result of the baseline models given in  Kembhavi (2017)  were on test set but the authors had used a different data split than the publicly released split. As per the suggestion of the authors, we evaluate CN N 2,3,4 model by combin-ing validation and test set. The comparison with the baseline models is given in Table  3 . As can be seen from Table  3 , CN N 2,3,4 model shows significant improvement over the baseline models. We argue that our proposed model outperforms the Text-Only model because of three reasons (i) sentence level attention, (ii) questionoption tuple as input, and (iii) ability to tackle forbidden options. Sentence level attention leads to better attention weights, especially in cases where a single sentence suffices to answer the question. If question is given as input to the model, then the model has to extract the embedding of the answer whereas giving question-option tuple as input simplifies the task to comparison between the two embeddings. SciQ dataset didn't have any questions with forbidden options. However, in the validation set of TQA, 433 out of 1530 multiple choice questions had forbidden options. Using the proposed threshold strategy for tackling forbidden options, CN N 2,3,4 gets 188 out of 433 questions correct. Without using this strategy and giving every question-option tuple as input, CN N 2,3,4 gets 109 out of 433 questions correct. 

 Conclusions and Future Work In this paper, we proposed a CNN based model for multiple choice question answering and showed its effectiveness in comparison with several LSTMbased baselines. We also proposed a strategy for dealing with forbidden options. Using questionoption tuple as input gave significant advantage to our model. However, there is a lot of scope for future work. Our proposed model doesn't work well in cases where complex deductive reasoning is needed to answer the question. For example, suppose the question is "How much percent of parent isotope remains after two half-lives?" and the lesson is on carbon dating which contains the definition of half-life. Answering this question using the definition requires understanding the definition and transforming the question into a numerical problem. Our proposed model lacks such skills and will have near random performance for such questions. Table 3 : 3 Accuracy of different models for true-false and multiple choice questions. Results marked with ( * ) are taken from Kembhavi (2017)  and are on test set obtained using a different data split. Result of our proposed model is on publicly released validation and test set combined. Model True-False Multiple Choice Random  * 50.0 22.7 Text-Only  * 50.2 32.9 BIDAF  * 50.4 32.2 CN N2,3,4 53.7 35.8 

			 The code is available at https://github.com/ akshay107/CNN-QA
