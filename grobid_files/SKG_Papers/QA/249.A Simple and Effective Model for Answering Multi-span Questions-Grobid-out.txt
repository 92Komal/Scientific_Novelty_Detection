title
A Simple and Effective Model for Answering Multi-span Questions

abstract
Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text. Naturally, models that return single spans cannot answer these questions. In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not. Our model substantially improves performance on span extraction questions from DROP and QUOREF by 9.9 and 5.5 EM points respectively.

Introduction The task of reading comprehension (RC), where given a question and context, one provides an answer, has gained immense attention recently. In most datasets and models  (Rajpurkar et al., 2016; Trischler et al., 2016; Seo et al., 2017; Yu et al., 2018; Kwiatkowski et al., 2019) , RC is set up as an extractive task, where the answer is constrained to be a single span from the input. This makes learning easier, since the model does not need to generate text abstractively, while still being expressive enough to capture a large set of questions. However, for some questions, while the answer is indeed extractive, i.e., contained in the input, it is not a single span. For example, in Figure  1  the answer includes two people who appear as noncontiguous spans in the context. Existing models  (Seo et al., 2017; Dua et al., 2019)  are by design unable to provide the correct answer to such multispan questions. While most work has largely ignored this issue, recent work has taken initial steps towards handling multi-span questions.  Hu et al. (2019)  proposed to predict the number of output spans for each question, and used a non-differentiable inference procedure to find them in the text, leading to a complex training procedure.  Andor et al. (2019)  proposed a Merge operation that merges spans, but is constrained to at most 2 spans.  Chen et al. (2020)  proposed a non-differentiable symbolic approach which outputs programs that compose single-span extractions. In this work, we propose a simple and fully differentiable architecture for handling multi-span questions that evades the aforementioned shortcomings, and outperforms prior work. Similar to  Yao et al. (2013) , who used a linear model over treebased features, we cast question answering as a sequence tagging task, predicting for each token whether it is part of the answer. At test time, we decode the answer with standard decoding methods, such as Viterbi. We show the efficacy of our approach on spanextraction questions from both the DROP  (Dua et al., 2019)  and QUOREF  datasets. Replacing the single-span architecture with our multi-span approach improves performance by 7.8 and 5.5 EM points respectively. Com-bining the single-span and multi-span architectures further improves performance by 2.1 EM on DROP, surpassing results by other span-extraction methods on both datasets. 2 Background: Single-span Model Setup Given a training set of question-contextanswer triplets (q i , c i , a i ) N i=1 , our goal is to learn a function that maps a question-context pair (q, c) to an answer a. We briefly review the standard singlespan architecture for RC , which we build upon. First, we encode the question and context with a pre-trained language model, such as BERT : h = Encoder([q, c]), where h = (h 1 , . . . , h m ) is a sequence of contextualized representations for all input tokens. Then, two parameterized functions (feed-forward networks), f start (h i ) and f end (h i ), are used to compute a score for each token, corresponding to whether that token is the start or the end of the answer. Last, the start and end probability for each token i is computed as follows: 3 Multi-span Model p start i = softmax (f start (h 1 ), . . . , f start (h m )) i , p end i = softmax (f end (h 1 ), . . . , f end (h m )) i , where both p start , p end ? R m?1 

 Span Extraction as Sequence Tagging Extracting a variable number of spans from an input text is standard in many natural language processing tasks, such as Named Entity Recognition (NER) and is commonly cast as a sequence tagging problem  (Ramshaw and Marcus, 1995) . Here we apply this approach to multi-span questions. Our model uses the same contextualized representations h, but rather than predicting start and end probabilities, it outputs a probability distribution over a set of tags for each token. We experiment with two tagging schemes. First, the wellknown BIO tagging  (Sang, 2000; Huang et al., 2015) , in which B denotes the first token of an output span, I denotes subsequent tokens in a span, and O denotes tokens that are not part of an output span. In addition, we experiment with a simpler IO tagging scheme, where words are tagged as either part of the answer (I) or not (O). Formally, given a tagging scheme with |S| tags (|S| = 3 for BIO and |S| = 2 for IO), for each of the m tokens, the probability for the tag of the i-th token is p i = softmax(f (h i )) (1) where p ? R m?|S| , and f is a parameterized function with |S| outputs. 

 Training Assume each answer a is a set of strings, where each string corresponds to a span in the input. We would like to train our model to predict the correct output for this set of spans. When the answer spans appear only once in the input, this is simple, since the ground-truth tagging is immediately available. However, there are many cases where a given answer span appears multiple times in the input. We next explain how to address this. To illustrate, consider the following simple example (assume a BIO scheme). Given the input "X Y Z Y Z" and the correct multi-span answer {"X", "Z"}, there are three possible gold taggings: B O B O B, B O B O O, and B O O O B. Thus, the groundtruth BIO cannot be determined unambiguously in this case. Figure  1  illustrates this issue with a real example from DROP.  1  To tackle the above issue, we enumerate over the set of all possibly-correct taggings, T , where given a multi-span answer a, a possibly-correct tagging is one in which all gold answer spans are tagged as such at least once.  2  We train our models by maximizing the marginal probability of all possibly-correct taggings: log p(T | h) = log T ?T m i=1 p i [T i ] , where p i [T i ] (see Eq. (  1 )) is the probability the model assigns to token i having the tag T i . The loss is minimized when p gives probability 1.0 to one of the possibly-correct taggings in T . 

 Decoding Spans from a Tagging At test time, given predicted tag probabilities p, we would like to find the most likely tagging T . Let V be the set of all valid taggings. We wish to find: T = arg max T ?V m i=1 p i [T i ]. For BIO tags, the set V comprises all taggings that don't include an I after an O, and the maximization problem can be solved in linear time using Viterbi decoding  (Viterbi, 1967)  as in  Yao et al. (2013) ;  Mehta et al. (2018) . For IO tags, all taggings are valid, and maximization is done by predicting the tag with highest probability in each token independently. Because answer spans are (practically) never adjacent in RC, an IO-tagging produces a set of spans by choosing all maximal spans that are contiguously tagged with I. 

 "Multi-Head" Models Some RC datasets contain questions where the output is not necessarily a span. For example, in DROP, the answer to some questions is a number that is not in the text, but can be computed by performing arithmetic operations. To handle such cases, many models  (Dua et al., 2019; Hu et al., 2019)  employ a multi-head architecture. In these models, each head z is a small module that takes the contextualized representations h as input and computes a probability distribution over answers p z (a | q, c) = p z (a | h). For example, in  Hu et al. (2019) , there are two heads that output spans, and three heads that output numbers. To determine which head to use for each question, an additional module is trained: p head (z | q, c) = p head (z | h). Thus, the model probability for an answer is: p(a | q, c) = z p head (z | q, c) ? p z (a | q, c). With this architecture, we can seamlessly integrate our multi-span approach into existing RC models. Specifically, a model can include both a single-span head and a multi-span head, dynamically deciding which span extraction method to utilize based on the input. 

 Empirical Evaluation Experimental setup As an encoder, we use the Hugging Face implementation of RoBERTa LARGE  (Wolf et al., 2019; , which produces the representations h. For DROP, we add the arithmetic and count heads from  Dua et al. (2019)  to handle non-span questions. Full details of the experimental setup are in Appendix A. 

 Results Table  1  shows development set results on the spanextraction questions of DROP  (Dua et al., 2019)  and QUOREF . We compare the previous best-performing multi-span models to a combination of our multi-span architecture (TASE: TAg-based Span Extraction) with the traditional single-span extraction (SSE), as well as to each separately. Comparison to previous models For a fair comparison with prior work on DROP, we also train our model initialized with BERT LARGE , as all prior work used it as an encoder. On DROP, TASEBIO+SSE (BERT LARGE ) outperforms all prior models that handle multi-span questions, improving by at least 3.2 EM points. On multi-span questions, we dramatically improve performance over BERT-CALC and MTMSN, while obtaining similar performance to NeRd. On QUOREF, compared to CorefRoBERTa LARGE  (Ye et al., 2020)  which uses the same method as MTMSN for multi-span extraction, we achieve a substantial improvement of over 20 EM on multi-span questions and an improvement of 4.5 EM and 3.2 F1 on the full development set, where the best results are achieved when using solely our multi-span architecture with IO-tagging. Comparing span extraction architectures Table 1 also shows that in both DROP and QUOREF, replacing the single-span extraction architecture with our multi-span extraction results in dramatic improvement in multi-span question performance, while single-span question performance is either maintained or improved. Furthermore, although combining both architectures tends to yield the best overall performance, 3 the improvement over using only our multi-span architecture is not substantial, suggesting that the multi-span architecture may be used by itself as a general span extraction method. Effects of tagging scheme Overall, the results are quite similar for the BIO and IO schemes. The slight advantage of IO could perhaps be explained by the fact that the model no longer requires distinguishing between B and I, in the presence of powerful contextualized representations.  Effect of marginalization To check whether marginalizing over all possibly-correct taggings is beneficial, we ran TASEBIO in a setup where only a single tagging is considered, namely where all occurrences of a gold answer span are tagged. Table  1  shows that this indeed leads to a moderate drop of up to 2 points in performance. 

 Test set results We ran TASEIO on the QUOREF test set. Our model obtains 79.7 EM and 86.1 F1, an improvement of 3.9 EM points and 3.3 F1 points over the state-of-the-art CorefRoBERTa LARGE . On DROP, our TASEIO+SSE model achieves 80.4 EM and 83.6 F1 on the entire test set (including nonspan questions). We note that the top 10 models on the DROP leaderboard (as of September 15, 2020) have all incorporated our multi-span head using our code base which has been public for a while. 

 Analysis Figure  2  shows that in both DROP and QUOREF the performance of TASEBIO decreases only moder-ately as the number of gold spans increases. This shows relative robustness to the number of answer spans. In addition, we can see that our architecture is quite accurate in predicting the correct number of spans, with a tendency for under-estimation. We analyzed the performance of the p head module in TASEBIO+SSE. A non-multi-span head is selected erroneously for 3.7% and 7.2% of the multispan questions in DROP and QUOREF respectively. The multi-span head is selected for 1.2% and 1.5% of the single-span questions in DROP and QUOREF respectively. However, this is reasonable as the multi-span head is capable of answering singlespan questions as well, and indeed it returned a single span in 45% of these cases on both datasets. We manually analyzed errors of TASEBIO+SSE on DROP, and detected 3 main failure cases: (1) questions where the answer is a span, but requires some numerical computation internally, (2) questions where the number of output spans is explicitly mentioned in the question but is not followed by the model, and (3) questions where a single contiguous span is unnecessarily split into two shorter spans. An example for each case is given in Appendix B. 

 Conclusion In this work, we cast the task of answering multi-span questions as a sequence tagging problem, and present a simple corresponding multispan architecture. We show that replacing the standard single-span architecture with our multispan architecture dramatically improves results on multi-span questions, without harming performance on single-span questions, leading to state-of-the-art results on QUOREF. In addition, integrating our multi-span architecture into existing models further improves performance on DROP, as is evident from the leading models on DROP's leaderboard. Our code can be downloaded from https://github.com/eladsegal/tag-based-multispan-extraction. . Training is done by minimizing cross entropy of the start and end indices of the gold span, and at test time the answer span is extracted by finding the indices (s, e): (s, e) = arg max s?e p start s p end e . 
