title
Evaluating Rewards for Question Generation Models

abstract
Recent approaches to question generation have used modifications to a Seq2Seq architecture inspired by advances in machine translation. Models are trained using teacher forcing to optimise only the one-step-ahead prediction. However, at test time, the model is asked to generate a whole sequence, causing errors to propagate through the generation process (exposure bias). A number of authors have suggested that optimising for rewards less tightly coupled to the training data might counter this mismatch. We therefore optimise directly for various objectives beyond simply replicating the ground truth questions, including a novel approach using an adversarial discriminator that seeks to generate questions that are indistinguishable from real examples. We confirm that training with policy gradient methods leads to increases in the metrics used as rewards. We perform a human evaluation, and show that although these metrics have previously been assumed to be good proxies for question quality, they are poorly aligned with human judgement and the model simply learns to exploit the weaknesses of the reward source.

Introduction Posing questions about a document in natural language is a crucial aspect of the effort to automatically process natural language data, enabling machines to ask clarification questions  (Saeidi et al., 2018) , become more robust to queries  (Yu et al., 2018) , and to act as automatic tutors  (Heilman and Smith, 2010) . Recent approaches to question generation have used Seq2Seq  (Sutskever et al., 2014 ) models with attention  (Bahdanau et al., 2014)  and a form of copy mechanism  (Vinyals et al., 2015; Gulcehre et al., 2016) . Such models are trained to generate a plausible question, conditioned on an input document and answer span within that document  (Zhou et al., 2018; Du et al., 2017; Du and Cardie, 2018; Yuan et al., 2017) . There are currently no dedicated question generation datasets, and authors have used the context-question-answer triples available in SQuAD  (Rajpurkar et al., 2016) . Only a single question is available for each context-answer pair, and models are trained using teacher forcing  (Williams and Zipser, 1989) . This lack of diverse training data combined with the one-stepahead training procedure exacerbates the problem of exposure bias  (Ranzato et al., 2015) . The model does not learn how to distribute probability mass over sequences that are valid but different to the ground truth; during inference, the model must predict the whole sequence, and may not be robust to mistakes during decoding. Recent work has investigated training the models directly on a performance based objective, either by optimising for BLEU score  (Kumar et al., 2018a)  or other quality metrics  (Yuan et al., 2017) . By decoupling the training procedure from the ground truth data, the model is able to explore the space of possible questions and learn to recover from suboptimal predictions during decoding. While the metrics used seem to be intuitively good choices, there is an assumption that they are good proxies for question quality which has not yet been confirmed. Our contributions are as follows. We perform fine tuning using a range of rewards, including a novel adversarial objective that directly estimates the probability that a question was generated or came from the ground truth data. We show that although fine tuning leads to increases in reward scores, the resulting models perform worse when evaluated by human workers. We also demonstrate that the generated questions exploit weaknesses in the reward models. Context although united methodist practices and interpretation of beliefs have evolved over time , these practices and beliefs can be traced to the writings of the church 's founders , especially john wesley and charles wesley ( anglicans ) , but also philip william otterbein and martin boehm ( united brethren ) , and jacob albright ( evangelical association ) . 

 Rewards Output Ground Truth Question who were two of the founders of the united methodist church ? No fine tuning which two methodist can be traced to the church 's founders ? LM according to the writings of the church 's founders , according to the writings of the church 's founders , [...] QA who in anglicans ? LM and QA who are the writings of the church 's founders ? Discriminator who founded the church 's founders ? Adversarial discriminator who were two western methodist practices ? LM, QA and adversarial discriminator who are the anglicans of the church ? Table  1 : Example generated questions for various fine-tuning objectives. The answer is highlighted in bold. The model trained on a QA reward has learned to simply point at the answer and exploit the QA model, while the model trained on a language model objective has learned to repeat common phrase templates. 

 Background Many of the advances in natural language generation have been led by machine translation (MT)  (Sutskever et al., 2014; Bahdanau et al., 2014; Gulcehre et al., 2016) . Previous work on question generation has made extensive use of MT techniques.  Du et al. (2017)  use a Seq2Seq based model to generate questions conditioned on context-answer pairs, and build on this work by preprocessing the context to resolve coreferences and adding a pointer network  (Du and Cardie, 2018) . Similarly, Zhou et al. (  2018 ) use a part-of-speech tagger to augment the embedding vectors. Both authors perform a human evaluation of their models, and show significant improvement over their baseline.  Kumar et al. (2018a)  use a similar model, but apply it to the task of generating questions without conditioning on a specific answer span.  Song et al. (2018)  use a modified context encoder based on multiperspective context matching  (Wang et al., 2016) .  Kumar et al. (2018b)  propose a framework for fine tuning using policy gradients and perform a human evaluation showing promising results. However, they use as rewards various similarity metrics that are still coupled to the ground truth. Yuan et al. (  2017 ) describe a Seq2Seq model with attention and a pointer network, with an additional encoding layer for the answer. They also describe a method for further tuning their model using policy gradients, with rewards given by an external language model and question answering (QA) system. Unfortunately they do not perform any hu-man evaluation to determine whether this tuning led to improved question quality. For the related task of summarisation,  Paulus et al. (2017)  propose a framework for fine tuning a summarisation model using reinforcement learning, with the ROUGE similarity metric used as the reward. 

 Experimental setup The task is to generate a natural language question, conditioned on a document and the location of an answer within that document. For example, given the input document "this paper investigates rewards for question generation" and answer "question generation", the model should produce a question such as "what is investigated in the paper?" 

 Model description We use the model architecture described by  Yuan et al. (2017) . Briefly, this is a Seq2Seq model  (Sutskever et al., 2014)  with attention  (Bahdanau et al., 2014)  and copy mechanism  (Vinyals et al., 2015; Gulcehre et al., 2016) .  Yuan et al. (2017)  also add an additional answer encoder layer, and initialise the decoder with a hidden state constructed from the final state of the encoder. Beam search  (Graves, 2012)  is used to sample from the model at inference time. We train the model using maximum likelihood before fine tuning. Our implementation achieves a BLEU-4 score  (Papineni et al., 2002)  of 13.5 on the test set used by  Du et al. (2017) , before fine tuning.   N L L B L E U Q A L M D i s c r i m i n a t o 

 Fine tuning Generated questions should be formed of language that is both fluent and relevant to the context and answer. Following  (Yuan et al., 2017) , we perform fine tuning on a trained model, using rewards given either by the negative perplexity under a LSTM language model, or the F1 score attained by a question answering (QA) system, or a weighted combination of both. The language model is a standard recurrent neural network formed of a single LSTM layer. For the QA system, we use QANet  (Yu et al., 2018)  as implemented by  Kim (2018) . 

 Adversarial training Additionally, we propose a novel approach by learning the reward directly from the training data, using a discriminator detailed in Appendix A. We generate questions for each context-answer pair in the training set using a generator trained by maximum likelihood, and train the discriminator to predict whether an input question was generated by our model, or originated from the training data. Keeping the discriminator fixed, we then fine-tune the generator, using as reward the probability esti-mated by the discriminator that a generated question was in fact real. In other words, the generator is rewarded for successfully fooling the discriminator. We also experiment with interleaving updates to the discriminator within the fine tuning phase, allowing the discriminator to become adversarial and adapt alongside the generator. The rewards described above are used to update the model parameters via the REINFORCE policy gradient algorithm  (Williams, 1992) . We teacher force the decoder with the generated sequence to reproduce the activations calculated during beam search, to enable backpropagation. All rewards are normalised with a simple form of PopArt  (Hasselt et al., 2016) , with the running mean ? R and standard deviation ? R updated online during training. We continue to apply a maximum likelihood training objective during this fine tuning. 

 Evaluation We report the negative log-likelihood (NLL) of the test set under the different models, as well as the corpus level BLEU-4 score  (Papineni et al., 2002)  of the generated questions compared to the ground truth. We also report the rewards achieved on the  test set, as the QA, LM and discriminator scores. For the human evaluation, we follow the standard approach in evaluating machine translation systems  (Koehn and Monz, 2006) , as used for question generation by  Du and Cardie (2018) . We ask three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer. 

 Results Table  2  shows the changes in automatic metrics for models fine tuned on various combinations of rewards, compared to the model without tuning. In all cases, the BLEU score reduces, as the training objective is no longer closely coupled to the training data. In general, models achieve better scores on the metrics on which they were fine tuned. Jointly training on a QA and LM reward results in better LM scores than training on only a LM reward; the LM score did not increase smoothly when used as the sole objective, and we believe the additional QA reward acts as a form of regularisation. We conclude that fine tuning using policy gradients can be used to attain higher rewards, as expected. Table  3  shows the human evaluation scores for a subset of the fine tuned models. The model fine tuned on a QA and LM objective is rated as significantly worse by human annotators, despite achieving higher scores in the automatic metrics. In other words, the training objective given by these reward sources does not correspond to true question quality, despite them being intuitively good choices. The model fine tuned using an adversarial discriminator has also failed to achieve better human ratings, with the discriminator model unable to learn a useful reward source. Although the training process was stable and robust to different initialisations, and the outputs do not appear to be significantly worse, we conclude that the discriminator was unable to learn a sufficiently useful distinction between generated and real questions, and the additional fine tuning procedure simply added unwanted noise to the model predictions. Table  1  shows an example where fine tuning has not only failed improve the quality of generated questions, but has caused the model to exploit the reward source. The model fine tuned on a LM reward has degenerated into producing a loop of words that is evidently deemed probable, while the model trained on a QA reward has learned that it can simply point at the location of the answer. This observation is supported by the metrics; the model fine tuned on a QA reward has suffered a catastrophic worsening in LM score of +226. Figure  1  shows the automatic scores against human ratings for all rated questions. The correlation coefficient between human relevance and automatic QA scores was 0.439, and between fluency and LM score was only 0.355. While the automatic scores are good indicators of whether a question will achieve the lowest human rating or not, they do not differentiate clearly between the higher ratings: training a model on these objectives will not necessarily learn to generate better questions. A good question will likely attain a high QA and LM score, but the inverse is not true; a sequence may exploit the weaknesses of the metrics and achieve a high score despite being unintelligible to a human. We conclude that fine tuning a question generation model on these rewards does not lead to better quality questions. 

 Conclusion In this paper, we investigate the use of external reward sources for fine tuning question generation models to counteract the lack of task-specific training data. We show that although fine tuning can be used to attain higher rewards, this does not equate to better quality questions when rated by humans. Using QA and LM rewards as a training objective causes the generator to expose the weaknesses in these models, which in turn suggests a possible use of this approach for generating adversarial training examples for QA models. The QA and LM scores are well correlated with human ratings at the lower end of the scale, suggesting they could successfully be used as part of a reranking or filtering system. We plan to research overgenerating questions and using the reward signals to rerank the outputs, thereby including the inductive bias the rewards represent without allowing the model to exploit them.  We use an architecture based on a modified QANet as shown in Figure  2 , replacing the output layers of the model to produce a single probability. Since the discriminator is also able to consider a full context-question-answer triple as input (as opposed to a context-question pair for the QA task), we fuse this information in the output layers. Specifically, we apply max pooling over time to the output of the first two encoders, and we took the mean of the outputs of the third encoder that formed part of the answer span. These three reduced encodings were concatenated, a 64 unit hidden layer with ReLU activation applied, and the output passed through a single unit sigmoid output layer to give the estimated probability that an input context-question-answer triple originated from the ground truth dataset or was generated. c r i m i n a t o r r e w a r d A d v e r s a r i a l d i s c r i m i n a t o r 
