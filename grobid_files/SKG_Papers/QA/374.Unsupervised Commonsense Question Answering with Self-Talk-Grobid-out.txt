title
Unsupervised Commonsense Question Answering with Self-Talk

abstract
Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pretrained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning  (Bruner, 1961) , our approach inquires language models with a number of information seeking questions such as "what is the definition of ..." to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zeroshot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the selftalk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.

Introduction Human level natural language understanding involves reading between the lines and relying on implicit background knowledge. Consider the sentence: Alice let Bob stand in front of her at the concert. Using physical and social commonsense -(i) Bob and Alice want to see the stage, and (ii) If Bob is taller, they would block Alice's view -one can infer that Alice is taller than Bob. Such examples are ubiquitous across natural language understanding (NLU) tasks such as reading comprehension  (Hirschman et al., 1999)  and recognizing textual entailment  (Dagan et al., 2013) , and even more so in tasks dedicated to commonsense reasoning such as the Winograd schema challenge  (Levesque et al., 2012) . Most current NLU models rely on pretrained language models (LMs; e.g.  Radford et al., 2019; Devlin et al., 2019; Raffel et al., 2020) . The standard practice is to fine-tune a pre-trained LM in a supervised manner on task-specific data. Alternatively, LM score is used to rank answer choices in a zero-shot setup . In both setups, pre-trained LMs yield improved performance upon prior methods, greatly due to the world knowledge that such LMs capture, having been trained on massive texts  (Petroni et al., 2019; Davison et al., 2019) . Despite the performance boost, LMs as knowledge providers suffer from various shortcomings: (i) insufficient coverage: due to reporting bias, many trivial facts might not be captured by LMs because they are rarely written about  (Gordon and Van Durme, 2013) . (ii) insufficient precision: the distributional training objective increases the probability of non-facts that are semantically similar to true facts, as in negation ("birds cannot fly";  Kassner and Sch?tze, 2020) . LMs excel in predicting the semantic category of a missing word, but might predict the wrong instance in that category (e.g., depending on the phrasing, BERT sometimes predicts red as the color of a dove). Finally, (iii) limited reasoning capabilities: it is unclear that LMs are capable of performing multiple reasoning steps involving implicit knowledge. To increase the coverage of high-precision world knowledge and facilitate multi-hop reasoning by making intermediate reasoning steps explicit, prior work incorporated KBs (e.g. ConceptNet;  Speer and Havasi, 2012)  and knowledge-informed models into LM-based models  (Xia et al., 2019; . In this paper, we study pre-trained LMs as an alternative to external KBs in providing knowledge Because Brett found an internship while in college but Ian was unable to, Brett found a job less quickly after graduation. The purpose of the internship is to help people find jobs. s 11 min i (s i2 ) Because Brett found an internship while in college but Ian was unable to, Ian found a job less quickly after graduation. The purpose of the internship is to help people find jobs. s 12 Because Brett found an internship while in college but Ian was unable to, Brett found a job less quickly after graduation. The definition of "job" is to be employed by someone. 

 s k1 Because Brett found an internship while in college but Ian was unable to, Ian found a job less quickly after graduation. The definition of "job" is to be employed by someone. to commonsense question answering tasks. We propose an unsupervised model that uses an LM as the answer scorer, and a (possibly different) LM as a knowledge source. We formulate the process of obtaining relevant knowledge as a self-talk, inquirybased discovery learning  (Bruner, 1961) , with the following steps: 1) seeking out knowledge by generating natural-language "clarification questions" conditioned on a given context, 2) generating their corresponding answers ("clarifications"), and 3) incorporating the clarifications as additional context. Our model does not rely on external knowledge or additional supervision. Yet, we show that on 4 out of 6 tasks it substantially improves upon a zero-shot baseline that relies on LM score alone and performs on par, and sometimes better than, models that use external knowledge sources. Integrating external knowledge warrants discerning relevant and helpful facts for solving a particular instance. LMs further require identifying that a clarification is factually-correct. We show that even among the clarifications that helped the prediction, humans perceived many as unhelpful or even incorrect, demonstrating that LM-based models often solve problems correctly for seemingly incorrect reasons. Our results call for future research on robust and correct knowledge integration to LM-based question answering systems. 

 Tasks We focused on the multiple-choice question answering tasks detailed below. Each instance consists of an optional context, an optional question, and several answer choices. 

 COPA: Choice of Plausible Alternatives  (Gordon et al., 2012) : Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. CommonSenseQA: commonsense Question Answering  (Talmor et al., 2019) : General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. 

 MC-TACO: Multiple Choice Temporal commonsense ): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors were selected in an adversarial way using BERT. 1 Social IQa: Social Interaction Question Answering  (Sap et al., 2019b) : Questions regarding social interactions, based on the ATOMIC dataset  (Sap et al., 2019a) . Contexts describe social interactions and questions refer to one of a few aspects (e.g. the subject's motivation, following actions, etc.). The answers were crowdsourced. PIQA: Physical Interaction Question Answering  (Bisk et al., 2020) : Questions regarding physical commonsense knowledge. Contexts are goals derived from an instruction website, typically involving less prototypical uses of everyday objects (e.g., using a bottle to separate eggs). The answers were crowdsourced, and an adversarial filtering algorithm was used to remove annotation artifacts. 2 WinoGrande  (Sakaguchi et al., 2020) : A largescale version of WSC that exhibits less bias thanks to adversarial filtering and use of placeholders instead of pronouns. As opposed to WSC that was curated by experts, WinoGrande was crowdsourced with a carefully designed approach that produces diverse examples which are trivial for humans. 

 Models A given instance consists of an optional context c, an optional question q, and answer choices: a k i=1 . We first describe the baseline model, which makes Taylor was doing her job so she put the money in the drawer.  

 What will Taylor do next? xWant As a result, Taylor wants to keep the money in the drawer. the prediction based on the instance alone ( ?3.1). 

 Job to earn money to keep the money in the drawer We then describe a knowledge-informed model that relies on external resources ( ?3.2). Finally, we discuss our self-talk model, which uses a pre-trained LMs to produce clarifications ( ?3.3). 

 LM-only Baseline We use a pre-trained language model LM s to score the plausibility of different text fragments. We experiment with the various LMs provided by the transformers package : GPT  (Radford et al., 2018) , GPT2  (Radford et al., 2019, all sizes) , a distilled GPT2 , and XLNet  (Yang et al., 2019, both sizes) . We assign each of the answer choices a i into the combination of the context and the question, and obtain opt i = combine(c, q, a i ). The combine function is computed differently for each task. For example, in COPA, where the question might be either about the cause or the effect of the context, we create the following texts for cause: "[context]. As a result, [choice]" and for effect: "  [context] . The cause for it was that [choice]". We denote the score of each answer choice as score(a i ) = CE(opt i ), where CE is cross-entropy loss defined as: CE(t 1 ...t n ) = ? 1 n n i=1 log 2 p LMs (t i | t 1 ...t i?1 ). We predict the a i with the lowest score as the correct answer, which is the most likely option according to LM s : y = argmin i score(a i ). 

 Baseline Model with External Knowledge In the setup illustrated in Figure  1 , each instance consists of an additional clarification list: CL = {cl 1 , ..., cl m }. Those are text fragments containing potentially relevant knowledge for solving the instance. For example, the clarification "The purpose of the internship is to help people find jobs" might help answering the question "which of Brett and Ian found a job less quickly after graduation?". We don't expect all the clarifications to be relevant and helpful for answering the main question. Instead, the model relies on the single clarification that increases its belief of a certain answer choice. Thus, the score of each answer choice is selected as the score of the text containing the clarification that most supports it, i.e., whose combination with it yields the minimal loss: score(a i ) = min cl?CL CE(opt i + cl). Again we predict y = argmin i score(a i ). We extract clarifications from the following sources, exemplified in Figure  2 . ConceptNet. Similarly to previous work, we extract relation paths between words from the context and the question, and words from the answer choices. Since we incorporate the knowledge into the model as text, we convert each ConceptNet relation to a natural language template as in  Davison et al. (2019) . We limit the path length to 2 edges in order to maintain high precision. Corpus. For pairs of words from the context and question and from the answer choices, we extract their joint occurrences (with minimum frequency of 100) in Google N-grams  (Brants and Franz, 2006) . This yields text fragments of up to 5 words rather than well-formed sentences, with the potential of describing the relationship between the two words  (Shwartz and Dagan, 2018) . COMeT. COMeT  is a knowledge base construction model trained on the ATOMIC resource  (Sap et al., 2019a)  which consists of everyday situations along with multiple commonsense dimensions such as their causes, effects, pre-and post-conditions, etc. We generate all the dimensions unless we can generate specific relations that are more likely to help. Specifically, in Social IQa, we heuristically try to understand which type of relation in COMeT the question asks for. In COPA, we use the pre-condition relations for cause questions (xIntent, xNeed) and the postcondition relations for effect questions (xEffect, Because Brett found an internship while in college but Ian was unable to, found a job less quickly after graduation. 

 Question Generation: Because Brett found an internship while in college but Ian was unable to, found a job less quickly after graduation. What is the purpose of 

 Answer Generation: the internship? 

 LM The purpose of is the internship The purpose of the internship is to help people find jobs. What is the purpose of 

 Question & Answer Prefixes What is the purpose of The purpose of is the internship? LM help people find jobs xReact, xWant, oEffect, oReact, oWant). When possible, we replace personX with the syntactic subject of the context or the question. 

 Self-talk Model Our proposed model makes the prediction identically to Figure  1 , but extracts the clarifications from pre-trained LMs. We treat the knowledge extraction from LMs as a process of self-asking clarification questions about the context and "discovering" their answers. Figure  3  exemplifies this process for WinoGrande with a generator language model LM g . For the sake of simplicity, the illustration depicts the process of generating a single pair of clarification question and answer. We start by generating multiple clarification questions conditioned on the context, by 1) concatenating one of several question prefixes, which we curated for each task (e.g. "What is the purpose of", see Table  6  in the appendix); and 2) generating 5 questions for each prefix using Nucleus sampling with p = 0.2, i.e., sampling from the top 20% tokens  (Holtzman et al., 2019) .  3  We limit the question length to up to 6 additional tokens. For each well-formed question that we obtained at the previous step, e.g. "What is the purpose of the internship?", we generate multiple answers using a similar method. Each question prefix corresponds to an answer prefix. We use the concatenation of the context, generated clarification question, and answer prefix as the prompt for generating an answer (clarification). We limit the answer length to 10 generated tokens, and use Nucleus sampling with p = 0.5. We generate 10 answers for each clarification question and keep all the well-formed clarifications. Note that the clarification questions themselves are only means to generate the clarifications, and they are not used by our model.  4  Since we did not train the clarification generator to ask sensical, relevant, and helpful questions, nor did we train the answer generator to generate coherent and factually correct answers, we can assume that some of the generated clarifications do not provide useful information to the model. 

 Results Table  2  displays the performance of the best model in each category according to the development accuracy. We report the performance of the following models: majority baseline, LM baseline (Baseline), LM-based model with external knowledge (Ext. Knowledge), Self-talk, supervised models from prior work when applicable (Pre. Sup), and human performance. Our zero-shot models are highlighted in purple. As expected, the overall performance is worse for the zero-shot models compared to the state-of-the-art supervised models, but they perform substantially better than the majority baselines on most tasks, with the exception of WinoGrande where they only slightly outperform it. Among the LM-based models, self-talk performs on par or within a few points from the external knowledge model. Best Knowledge Source. Among the knowledge informed models, COMeT achieves the best performance across tasks. This likely happens because COMeT can dynamically generate predictions for any context, while the other two knowledge sources are static and lack coverage. Table  1  shows the relative improvement in accuracy points compared to the zero-shot baseline, for each knowledge source averaged across LMs for each dataset. Interestingly, the relative improvement is fairly uniform across knowledge sources, but it varies substantially across tasks. While some tasks benefit from any added knowledge, others benefit from none. COMeT ConceptNet Google Ngrams GPT Distil-GPT2 GPT2 GPT2-M GPT2-L GPT2-XL We also experimented with combining the clarifications from all the knowledge sources, which didn't prove beneficial except for MC-TACO (where it added +7.9 points to the dev accuracy, bringing it to 66.7). We assume that some resources added noise, making the whole smaller than the sum of its parts. 

 Analysis While the performance on the end task serves as an extrinsic evaluation for the quality of the generated clarifications, we are also interested in evaluating it intrinsically. From preliminary experiments we know that there is a high ratio of noisy clarifications. We thus focus on and analyze two types of clarifications: useful ( ?5.1) and harmful ( ?5.2). 5 

 Useful Clarifications We define a clarification as useful if (a) it is the clarification with the best LM score in its instance (i.e., the clarification used in practice); and (b) the instance was incorrectly predicted by the zero-shot baseline but correctly predicted by the self-talk model. We sampled up to 50 useful clarifications for each combination of task and knowledge source, using the best performing LM (See Table  3  in the appendix for examples). We showed crowdsourcing workers an instance along with a clarification question and its answer, and asked them: 1) whether the question is grammatical, not entirely grammatical but understandable, or completely not understandable; and if the answer was anything but "completely not understandable", 2) whether the question is relevant, i.e. on topic with the instance. We asked the same questions about the answer, in addition to: 3) whether the answer is factually correct or likely true; and 4) whether the answer adds helpful information to solve the instance. The annotation task was carried out in Amazon Mechanical Turk. To ensure the quality of annotations, we required that the workers be located in the US, UK, or Canada, and have a 99% approval rate for at least 5,000 prior tasks. We aggregated annotation from 3 workers using majority vote. The annotations yielded moderate levels of agreement, with Fleiss' Kappa ? = 0.43  (Landis and Koch, 1977) . Among the different categories of annotations we measured pairwise accuracy, which ranged from 60.41% (the answer is factually correct) to 92.26% (the question is completely not understandable). COMET ConceptNet Distil-GPT2 GPT2 GPT2-M GPT2-XL GPT2-L GPT XLNet XLNet For the sake of brevity, we focus on the analysis of the answers to the clarification questions. The left part of Figure  5  shows that across tasks and resources, most clarifications are grammatical or at least understandable. Among the clarifications considered grammatical or understandable, the right part of the figure shows the percentage of clarifications considered relevant, correct, and helpful. Most clarifications were considered relevant to the context and factually correct, but only 40% on average were considered helpful. Considering that these are all clarifications that indeed helped the model, this is an interesting though not completely unexpected finding: the model utilizes knowledge that humans wouldn't consider as helpful.  Breaking down by knowledge source, Figure  4  shows the ratio of clarifications considered by humans as relevant (top), factually correct (middle), and helpful (bottom), for each task and knowledge source. XLNet performs worse on all measures. ConceptNet's clarifications are often judged as irrelevant likely because they are limited to a very specific type of clarification (the relationship between a pair of terms). It's not too surprising that clarifications generated by LMs were sometimes judged as factually incorrect. We also note that COMeT generated factually correct clarifications for Social IQa (which is based on ATOMIC, on which COMeT was trained), and ConceptNet generated factually correct clarifications for Common-SenseQA (which is based on ConceptNet). Table  3  demonstrates the types of knowledge in useful and relevant clarifications, showing that pre-trained LMs do particularly well in definitions. LM score by adding relevant lexical cues. A manual examination of a sample of answers judged as relevant but unhelpful revealed that 53.33% were answers for unhelpful questions, 20% were correct but unhelpful, 16.67% were factually incorrect, 10% were helpful to some extent (containing knowledge deemed too trivial by the annotators), and 10% had corresponding unanswerable instances.   

 Harmful Clarifications Symmetrically, we also study the harmful clarifications. A clarification is harmful if (a) it is the clarification with the best LM score in its instance; and (b) the instance was correctly predicted by the zero-shot baseline but incorrectly predicted by the self-talk model. We sampled up to 25 harmful clarifications from the predictions of the best setup (LM and knowledge source) for each task, and manually categorized the errors into the following types. 1. Irrelevant: the clarification was off topic. 2. Nonsensical or ungrammatical: the clarification was not a complete sentence, or had other grammar or meaning issues. 3. Relevant: the clarification contributed relevant knowledge but it wasn't enough for predicting the correct answer. 4. Factually Incorrect: the clarification made a factually incorrect statement, often in support of one of the distractors. 5. Correct: the clarification yielded an alternative correct answer for the main instance. 6. Restating the instance: the clarification repeated the context or the main question. 7. Wrong sense: the clarification interpreted a word from the instance in the wrong sense. 8. Dataset error: the instance is incorrect or lacks information required for answering it correctly. Figure  6  shows the percent of each error type across all the tasks and knowledge sources. The majority of clarifications are irrelevant, ungrammatical or nonsensical, or relevant but not helpful for making the correct prediction. We judged a non-negligible 12.4% of the clarifications as providing alternative correct answers, phrased differently from the gold answer. Table  4  provides an instance for each error type. 7 6 Related Work 

 External Knowledge in Neural Models Approaches for incorporating external knowledge into a neural model consist of several components: (1) the task addressed; (2) neural model; (3) knowledge sources; and (4) incorporation method. Most models target tasks that require commonsense knowledge, such as the story cloze test  (Mostafazadeh et al., 2016)  and machine comprehension tasks  (Ko?isk? et al., 2018; Ostermann Task  Know. Source Instance Clarification Irrelevant 

 PIQA GPT2-XL Q: how do you sit a baby in a restaurant? Q: What is the definition of "a good time"? Choices: place them in a booster seat., place them on the table. A: The definition of "a good time" is not the same as what constitutes an acceptable meal. 

 Nonsensical or ungrammatical 

 Social IQa XLNet C: Cameron went out of their way to help a friend who was in need of help. Q: How would Cameron feel after helping? Q: How would Cameron feel after helping? Choices: Cameron then frustrated., Cameron then happy., Cameron then annoyed. A: Cameron felt they were doing, but then he realized that. 

 Relevant WinoGrande GPT2-XL The children were not vaccinated, which was fine with Betty but annoyed Mary. believed they made kids autistic. Q: What does it mean to be "autistic"?" A: Be "autistic" means to have problems in social interaction and communication skills.    Talmor et al., 2019) . The neural component has recently shifted from biLSTM to transformer-based representations, specifically pre-trained LMs  (Devlin et al., 2019; . With respect to the knowledge source, the vast majority of papers rely on ConceptNet to extract relation paths between concepts and entities identified in the input  (Speer and Havasi, 2012 , see an example in Figure  2 ). Additional resources include WordNet  (Lin et al., 2017; Wang and Jiang, 2019) , retrieval or statistics mind from corpora  (Lin et al., 2017; Mitra et al., 2019; Joshi et al., 2020) , knowledge base embeddings , hand-crafted rules  (Lin et al., 2017; Tandon et al., 2018) , and tools such as sentiment analyzers  and knowledgeinformed LMs . The external knowledge is typically incorporated into the neural model by learning a vector representation of the symbolic knowledge (e.g. subgraphs from ConceptNet), and attending to it via attention mechanism when representing the inputs  (Bauer et al., 2018; Paul and Frank, 2019; Lin et al., 2019) . Alternative approaches include using the knowledge to score answer candidates and prune implausible ones  (Lin et al., 2017; Tandon et al., 2018) , and training in a multi-task setup via auxiliary tasks pertaining to knowledge  (Xia et al., 2019) . To the best of our knowledge, our method is the first to generate knowledge from pre-trained language models and incorporate it as external knowledge into a question answering model. Concurrently,  Latcinnik and Berant (2020)  used one language model to generate hypotheses and another language model as an answer scorer for Common-SenseQA. 

 Extracting Knowledge from LMs Pre-trained LMs such as GPT2  (Radford et al., 2019)  and BERT  (Devlin et al., 2019)  capture various types of world knowledge.  Petroni et al. (2019)  showed that such LMs can be used in a KB completion task over ConceptNet and Wikidata  (Vrande?i? and Kr?tzsch, 2014)  by converting KB relations into natural language templates and querying the LM for the missing part in the triplet (concept 1 , relation, concept 2 ). For instance, querying BERT for suitable substitutes to the mask in "Dante was born in  [MASK] " assigns the highest probability to Rome.  Davison et al. (2019)  similarly showed that BERT assigns higher scores to natural language fragments of true rather than fictitious ConceptNet triplets, and semi-automated the template creation by using GPT2 to score hand-crafted templates. While both works have shown somewhat promising results, other work showed that knowledge extracted from LMs is expectantly not always ac-curate. Specifically,  Kassner and Sch?tze (2020)  showed that negated facts are also considered likely by the LM, while  Logan et al. (2019)  pointed out that LMs may over-generalize and produce incorrect facts such as "Barack Obama's wife is Hillary". 

 Generating Questions and Explanations There are numerous research directions investigating automatic question generation  (Vanderwende, 2008) . Motivations vary from data augmentation to QA tasks  (Du et al., 2017; Dhingra et al., 2018; Du and Cardie, 2018; Sachan and Xing, 2018; Fabbri et al., 2020)  through conversational machine reading  (Saeidi et al., 2018; Pan et al., 2019) , simplifying questions to make them more easily answerable  (Buck et al., 2018; Talmor and Berant, 2018; Perez et al., 2020) , to using questions as means for other purposes such as sentence representation and summarization  (Guo et al., 2018; Potash and Suleman, 2019) . In particular, our work is pertinent to previous work in producing clarification questions and explanations.  Rao and Daum? III (2019)  worked on questions from forums (e.g. Stack Exchange). They proposed a model that generates clarification questions and corresponding answers for a given question, using the question's comments (clarification questions and answers) as supervision. Questionanswer pairs were scored based on how much relevant information they add to the context.  Shen et al. (2019)  developed an active learning framework for image captioning that learns to detect uncertainty about generated words and ask natural language questions to reduce its uncertainty. A visual question answering (VQA) model provides an answer which is then used to change the caption. The framework is trained with reinforcement learning, but the gold standard captions are used during a warmup steps and the VQA model is supervised.  Klein and Nabi (2019)  proposed a joint question generation and question answering framework. They fine-tuned GPT2 on a question answering dataset to generate a question and an answer span for a given passage, and trained BERT to answer the generated question given the passage.  Finally, Rajani et al. (2019)  proposed a model for Com-monSenseQA that generates explanations for its predictions. They collected human explanations and used them to fine-tune LMs to automatically generate explanations. These explanations were then added as additional inputs. The shortcoming of this approach is that it requires collecting specific human explanations for each new dataset. 

 Discussion and Conclusion We presented an unsupervised framework for multiple choice commonsense tasks that generates and integrates background knowledge from pre-trained LMs. On most tasks, it performs substantially better than the baseline and similarly to a model that had access to external knowledge resources. We have listed several shortcomings of using pre-trained LMs as knowledge providers: (i) insufficient coverage, (ii) insufficient precision, and (iii) limited reasoning capabilities. Despite their insufficient precision compared to a KB like Con-ceptNet, we showed that clarifications generated by LMs resulted in similar or superior empirical gains. Among the clarifications used in practice by the answer scorer, about 60% of those that yielded a correct prediction and 12% of those that yielded an incorrect prediction were judged by humans as factually correct. By design, our model makes a single additional reasoning step explicit, aiming to facilitate reasoning about implicit inferences. A preliminary experiment in which we incorporated clarification pairs to facilitate two hops got mixed results. An interesting future direction is to generate each clarification in response to the previous ones, in a dialogue setup  (Saeidi et al., 2018) . Another challenge is the "needle in a haystack" problem of the clarifications, and one way to address it is to develop a model that is capable of "introspection", specifically knowing what it doesn't know. A more structured knowledge generation might also make the combination of various knowledge sources more successful. Filling in knowledge gaps and making implicit intermediate reasoning steps explicit is imperative going forward. We hope that our framework will facilitate future research in this area. Our code and data will be made available upon publication. Our code and data is available at github.com/vered1986/self talk. 

 A Question and Answer Prefixes We came up with question and answer prefixes by experimenting with a few generic prefixes and observing what generally yields accurate answers. For example, we observed that LMs are not very good at causal and temporal relationships but are pretty good at definitions. For the datasets whose instances include questions (e.g. Social IQa) we also used the corresponding question prefixes. Table  6  presents the question and answer prefixes used for each task. " " in the answer prefix is replaced with the generated question (excluding the question mark), e.g. "What is the definition of a cat?" yields the answer prefix: "The definition of a cat is". The Social IQa templates correspond to COMeT dimensions. X is replaced with the syntactic subject of the sentence. 

 B Best Language Model Table  5  shows the average development accuracy of the LMs across the different knowledge sources. In general there is a preference to GPT-2, and in particular to the larger models, except for COPA in which the distilled version works best. A possible explanation might be that the language model distillation reduces the likelihood of rare words  (Tang and Lin, 2018) , which works well for the simple sentences in COPA. The XLNet models perform poorly, perhaps due to their smaller training corpus (16GB vs 40GB in GPT-2, both using web text). 

 GPT Distil-GPT2 GPT2 GPT2-M GPT2-L GPT2  Figure  7  shows, for each task and knowledge source, the ratio of useful clarifications that were considered by humans as either grammatical or at least understandable. The majority of the helpful clarifications are considered as grammatical. The XLNet models are slightly worse in terms of gram-maticality. For example, the clarification question "What are the properties of a you sharpen a pencil,?" and the answer "The properties of a you sharpen a pencil, are that it will not break or be dulled" generated for the PIQA instance "sharpen a pencil" by XLNet-base. Despite its grammar errors, the answer was still useful for a LM to determine the correct answer.  Figure  8  breaks down by task the type of errors found in the harmful clarifications. In Social IQa and CommonSenseQA, many alternative correct answers are generated, but this doesn't happen in WinoGrande, that by design only allows for one correct answer. Clarifications in MC-TACO are more than average irrelevant. In the future, it would be interesting to investigate whether this is due to inherent lack of temporal commonsense in LMs or due to misguided attempts to extract it. 

 C.2 Harmful Clarifications Figure  9  similarly breaks down the errors by knowledge source. All knowledge sources except for ConceptNet make incorrect statements, but LMs also tend to make nonsensical statements, especially XLNet. ConceptNet tends to generate irrelevant clarifications (about the relationship between two unimportant terms). Being a static resource, is was also insensitive to the word senses. Google Ngrams, the only other static knowledge source, didn't suffer from this issue. This is likely because a polysemous term x related to y in one of its senses wouldn't typically co-occur with y in its non-related senses  (Shwartz and Dagan, 2016) . Figure 1 : 1 Figure 1: Model illustration for WinoGrande. Each answer choice (Brett, Ian) is assigned to the concatenation of the context and a clarification. The score for each choice is the best LM score across clarifications (2 in this case). 

 d by go al Job is a type of work. You would work because you want money. Job to earn money. 

 Figure 2 : 2 Figure 2: Generating a single clarification using ConceptNet, Google Ngrams, and COMeT (Social IQa instance). 

 Figure 3 : 3 Figure 3: Generating a clarification with LM: 1) Generate a question, conditioned on the context (pink) and question prefix (yellow). 2) Generate an answer, conditioned on the context, generated question and a corresponding answer prefix. The clarification is a concatenation of the answer prefix and generated text (green). 

 Figure 6 : 6 Figure 6: Types of errors caused by the harmful clarifications across all tasks and knowledge sources. 

 Figure 7 : 7 Figure 7: Ratio of clarifications considered by humans as grammatical or understandable among the useful clarifications for each task and knowledge source. 

 Figure 8 : 8 Figure 8: Types of errors caused by the harmful clarifications, for each task, across all knowledge sources. 

 Figure 9 : 9 Figure 9: Types of errors caused by the harmful clarifications, for each knowledge source, across all tasks. 

 Table 1 : 1 Relative improvement upon the zero-shot baseline in terms of development accuracy, for each knowledge source averaged across LMs for each dataset. XLNet XLNet-L * *(Lin et al., 2020). 

 Ratio of clarifications considered as relevant (top), factually correct (middle), and helpful (bottom), among the useful and grammatical or understandable clarifications for each task and knowledge source. Answers in Social IQa were evaluated for helpfulness when the clarification question was different from the main question. -L WinoGrande 72.00 43.80 36.00 61.20 83.00 68.00 71.10 67.90 72.70 83.30 Social IQa 90.00 56.00 66.00 74.00 72.00 76.00 76.00 80.00 36.00 52.00 MC-TACO 66.00 12.50 26.30 46.80 62.00 56.00 54.00 43.80 50.00 33.30 PIQA 72.00 40.00 38.00 62.00 72.00 60.00 66.00 35.00 75.00 33.30 CSQA 66.00 55.20 44.40 48.70 66.00 72.00 64.00 100.00 - 48.10 WinoGrande 60.00 43.80 40.00 24.50 46.80 46.00 53.30 39.30 45.50 33.30 Social IQa 76.00 42.00 28.00 48.00 36.00 42.00 50.00 50.00 22.00 28.00 MC-TACO 60.00 12.50 42.10 46.80 48.00 60.00 54.00 29.20 40.60 33.30 PIQA 62.00 44.00 24.00 44.00 44.00 42.00 36.00 0.00 50.00 33.30 CSQA 48.00 86.20 50.00 51.30 54.00 62.00 58.00 80.00 - 51.90 WinoGrande 34.00 12.50 20.00 14.30 34.00 24.00 31.10 35.70 27.30 33.30 Social IQa - 20.00 - - - - - - - - MC-TACO 20.00 0.00 15.80 23.40 30.00 42.00 32.00 31.20 18.80 33.30 PIQA 28.00 6.00 14.00 16.00 30.00 26.00 24.00 5.00 25.00 33.30 CSQA 30.00 34.50 33.30 25.60 46.00 50.00 42.00 80.00 - 37.00 Figure 4: Grammatical Understandable Gibberish Relevant 64.94% Correct 60.47% Helpful 40.64% 0 25 50 75 Figure5: Human evaluation of the clarifications, aggregated across tasks and knowledge sources. Left: ratio of grammatical, not entirely grammatical but understandable, and completely not understandable clarifications. Right: percent of grammatical/understandable clarifications considered relevant, correct, and helpful. 

 Working on the elaborate task was taxing, it require extreme what? Q: What is the relationship between 'working' and 'concentration'? Choices: holding, concentration, energy, job, energy A: In order for working to happen, concentration needs to happen. When you travel you should what in case of unexpected costs? Q: What do they want as a result? Choices: go somewhere, energy, spend frivolously, fly in airplane, have money A: As a result, they want to make sure they have enough money. What does you want as a result? Choices: [...] antibacterial mouthwash into a small toilet bowl [...] soak your toothbrush [...] [...] antibacterial mouthwash into a small A: As a result, you wants to get rid of germs. cup [...] soak your toothbrush [...] What is the definition of 'zoo'? A: The definition of a 'zoo' is an area where people can see animals in captivity. Vacation is the opposite of work. Task Source Instance Clarification Preconditions CSQA MC- ConceptNet Q: COMeT C: [...] Mark Bailey didn't dream of becoming a judge. Q: GPT2-XL Q: How many years did it take for Mark to become a judge? Q: What happened first? A: The first thing that happened was he went to law school TACO Choices: 63 years, [...], 7 months, 180, 7 weeks, 7 years, [...], 7 hours and became an attorney. Social C: Carson was at a friends house and decided to play video games. Q: What does Carson need to do before this? IQa GPT2-M Q: What does Carson need to do before this? A: Before doing that, Carson first had to get his mother's permission Choices: know about their friends house, of though about playing before playing the game. video games, of thought about their friend Social Commonsense Wino C: Neil looked all over the neighborhood for their missing dog until Q: What does Neil feel as a result? Grande COMeT Samuel called and got their dog back. A: As a result, Neil feels happy to have the dog back. Choices: Neil, Samuel Definitions and Lexical Knowledge PIQA Q: Wino COMeT Q: how to sanitize a toothbrush Grande GPT2-L C: During the summer, I like visiting the zoo more than the aquar-ium because the is inside. Choices: zoo, aquarium Q: Social IQa ConceptNet Functions Q: What do professors primarily do? CSQA DistilGPT2 Choices: master physics, state facts, wear wrinkled tweed jackets, school students, teach courses C: Jan had been on vacation in Hawaii for a week. Last night, she finally returned home. Q: What will Jan want to do next? Choices: return to her family, Return to work, Leave for another vacation.Q: What is the relationship between 'vacation' and 'work'? A:Q: What is the main function of a professor's teaching career,? A: The main function of a professor's teaching career, is to teach students how they can improve their knowledge. 

 Table 3 : 3 Example of relevant useful clarifications and the type of knowledge they provide. 

 What happened after Islam became popular in the region? Choices: they drank liquor, it died off, it expanded even further, they drank alcohol, it died out, it died down Choices: Betty, Mary Factually Incorrect WinoGrande COMeT to work at the same company, and Derrick really liked working in Human Resources and William wanted subsequently offered a position. Q: What will Derrick want as a result? Choices: Derrick, William A: As a result, Derrick wants to go to work with them Correct CSQA Google Ngrams Q: What do people usually feel when falling in love? Choices: getting married, pain, happiness, getting married, suffering Q: -A: Suffering from unrequited love. Restating the instance CSQA COMeT might he do this? Q: Billy set aside a block of time for having fun after work. Why Q: What will Billy want as a result? Choices: happiness, stress relief, pleasure, ocean, may laugh A: As a result, they want to do something fun. Wrong Sense MC-TACO ConceptNet C: [...] Islam thrived as a strong, male-dominated religion of individuality [...] preaching brotherhood [...]. Q: What is the relationship between brotherhood and al-cohol? A: You are likely to find brotherhood in a fraternity house. You are likely to find alcohol in a fraternity house. Q: 

 Table 4 : 4 An example for each of the error types among the harmful clarifications. 

 Table 5 : 5 Average self-talk accuracy for each LM answer scorer, averaged across knowledge sources. -XL XLNet XLNet-L 

			 To make this task compatible with the other tasks, we only kept a single correct answer per instance, making our results not comparable to previously reported results.2 Word associations and dataset-specific features that are not informative for the task are identified by a strong baseline and removed (Gururangan et al., 2018; Zellers et al., 2018) . 

			 p = 0.2 is significantly lower than the standard value of p = 0.9 in the literature. We optimized for factual correctness, and our preliminary experiments have shown that lower p values produce texts that are more faithful to the LM training corpus, at the price of being more bland. 

			 In some datasets, an instance consists of a question. In this case, we can use the instance question as a "clarification" question and generate additional clarification questions similar to it. For example, the Social IQa context "Austin fought for Quinn's life, but they eventually died on the operating table.", the LM answers the question "Why did Austin do this?" directly with: "Austin did this because they wanted to keep him alive" (the correct answer is "Because Austin wanted to save Quinn"). 

			 We omitted COPA from the analysis due to its small size. 

			 See Figures8 and 9in the appendix for a breakdown of error types by task and knowledge source.
