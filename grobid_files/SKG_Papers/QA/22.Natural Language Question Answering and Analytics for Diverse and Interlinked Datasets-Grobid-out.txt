title
Natural Language Question Answering and Analytics for Diverse and Interlinked Datasets

abstract
Previous systems for natural language questions over complex linked datasets require the user to enter a complete and well-formed question, and present the answers as raw lists of entities. Using a feature-based grammar with a full formal semantics, we have developed a system that is able to support rich autosuggest, and to deliver dynamically generated analytics for each result that it returns.

Introduction In order to retrieve data from a knowledge base (KB), knowledge workers, such as physicians or financial analysts, often face the challenge of having to learn specific query languages (e.g., SQL and SPARQL 1 ). However, the fast pace of changing query languages to different types of KBs (e.g., Relational Databases, Triple Stores, NoSQL stores, etc.) makes it difficult for users to keep up with the latest developments of such query languages that allow them to access the data they need for their work. This situation prevents users without extensive computer training from effectively utilizing the available information in the KB. Developing userfriendly natural language interfaces will make it easier for non-technical users to access the information in the KB in an intuitive way. In this paper, we present a Natural Language Interface that allows users to query the underlying KBs with natural language questions. Unlike previous approaches, instead of asking the users to provide the entire question on their own, our system makes suggestions to help the users to complete their questions. Given a complete question, our system parses it to its First Order Logic (FOL) representation using a grammar derived from interlinked datasets; different translators are developed to further translate the FOL of a query into executable queries, including both SQL and SPARQL. Finally, our system generates dynamic analytics for the result sets in order to help users to gain a better understanding of the data. 

 Related Work Keyword-based search  (Ding et al., 2004; Tummarello et al., 2007; d'Aquin and Motta, 2011)  and faceted search  (Zhang et al., 2013;  have been frequently adopted for retrieving information from KBs. However, users have to figure out the most effective queries in order to retrieve relevant information. Furthermore, without appropriate ranking methods, users may be overwhelmed by the information available in the search results. Early Natural Language Interfaces (NLIs) required a handcrafted interface solution for each database thereby restricting its portability  (Green et al., 1961; Hendrix et al., 1978; Woods, 1973) . Recent research has focused more on developing open domain systems  (Kwiatkowski et al., 2013; Yao and Durme, 2014; Bordes et al., 2014) , but there remains a need for specialized NLIs  (Minock, 2005) . One unique feature of our system is to help users to build a complete question by providing suggestions according to a partial question and a grammar. Much of prior work translates a natural language question into SPARQL and retrieves answers from a triple store  (Lopez et al., 2005; Yahya et al., 2013; He et al., 2014) ; however, SPARQL queries have been criticized to have unsatisfying query response time. In this work, we maintain flexibility by first parsing a question into First Order Logic, which is further translated into both SQL and SPARQL. This enables us to easily adapt to new query languages and allows us to choose the most appropriate query language technology for a given use case. Finally, to the best of our knowledge, none of existing NLIs provide dynamic analytics for the results. Our system performs descriptive analytics and comparisons on various dimensions of the data, conducts sentiment analysis, and analyzes trends over time in the data. Such analytics would enable users to better conduct further analyses and derive insights from the data. This feature of our system is a clear advantage over other NLI systems that only retrieve a simple result list of documents/entities. 

 Overall Architecture Figure  1  shows the overall architecture of our proposed NLI system. Users can input their questions on the Web Interface and our Auto-suggestion component will guide the users in completing their questions. A complete question is then sent to the Question Understanding module again to be parsed into its first order logic representation with the grammar. As the next step, the FOL of a query is translated into an executable query with the Query Translation module. A translated query is then executed against an underlying knowledge base/graph for retrieving answers and generating corresponding analytics. Our system currently focuses on the following domains: Drugs, Organizations, Patents, People, Finance and News. The underlying knowledge base contains about 1 million entities and 12 million relationships. Rule 1 shows a lexical entry for the word drugs, indicating that its TYPE is drug, is plural, and has the following semantic: ?x.drug(x). Rule 2 specifies the verb develop, describing its tense (TNS) and indicating that it connects an organization and a drug via the TYPE feature. By utilizing the type constraints, we can then license the query companies developing drugs while rejecting nonsensical queries like rabbits develop drugs on the basis of the mismatch in semantic type. Furthermore, our grammar also covers wh-questions, e.g., what, which, how many, where, and nominal phrases and imperatives. Disambiguation relies on the presence of features on non-terminal syntactic nodes. We mark prepositional phrases (PPs) with features that determine their attachment preference. E.g., the PP for pain in how many companies develop drugs for pain? must attach to an NP rather than a VP; thus, it must attach to drugs rather than develop. Together with other features, we filter out many of the logically possible but undesired PP-attachments in queries with many modifiers. E.g., our approach is able to generate a single parse for companies headquartered in Germany developing drugs for pain or cancer. 

 Auto-suggestion Our NLI provides suggestions to help users to complete their questions. Unlike Google's query autocompletion that is based on query logs (Cornea and Weininger, 2014), our auto-suggestion utilizes the linguistic constraints encoded in the grammar. Our auto-suggestion is based on the idea of leftcorner parsing. Given a query segment qs (e.g., drugs, developed by, etc.), we find all grammar rules whose left corner f e on the right side matches the left side of the lexical entry of qs. We then find all leaf nodes in the grammar that can be reached by using the adjacent element of f e. For all reachable leaf nodes (i.e., lexical entries in our grammar), if a lexical entry also satisfies all the linguistic constraints, we then treat it as a valid suggestion. Specifically, for the query segment Drugs, according to our grammar, we could be looking for a verb as the next part of the question. In our lexicon, we may have many verbs, e.g., drive and developed by. Here, developed by is a valid suggestion because its semantic constraints match that of drugs. We continue our suggestions to the end of the user-entered query string, and never try to interpolate material either before or inside the string. In our current system, the automatically generated suggestions are ranked by considering their popularity. We associate each lexical entry with a node in a knowledge graph. This graph contains nodes for the entities corresponding to the lexical entries, further nodes for generic types such as Drug, Company and Technology, and yet further nodes for predicates such as developed by and granted to.The edges of the graph represent relations such as developed by and filed by. For ranking, the degree of a node is as a proxy for its quality. For example, if the node "Google" filed 10 patents and is also involved in 20 lawsuits, then its popularity will be 30. 

 Query Translation and Execution The interpreted FOL (Section 4) of a question is further analyzed by another parser (implemented with ANTLR  (Bovet and Parr, 2008) ) that parses FOL expressions. Figure  3  shows the parse tree of the FOL for the query Drugs developed by Merck. We then traverse this parse tree, and put all the atomic logical conditions and the logical connectors into a stack. When we finish traversing the entire tree, we pop the conditions out of the stack to build the query constraints; predicates in the FOL are also mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL). The following summarizes the translation from a natural language question to a SQL and SPARQL query via a FOL representation: Natural Language: ''Drugs developed by Merck'' First Order Logic (FOL) Representation: all x.(drug(x) ? (develop(id042,x) & type(id042,Company) & label(id042,Merck))) SQL Query: select drug. * from drug where drug.originator company = 'Merck' SPARQL Query (prefixes for RDF and RDFS omitted): PREFIX example: <http://www.example.com#> select ?x ?id123 ?id042 where { ?id042 rdfs:label 'Merck'. ?id042 rdf:type example:Company . ?x rdf:type example:Drug . ?id042 example:develops ?x . } We execute the SQL queries using Apache Spark  (Zaharia et al., 2010) , a distributed computing environment, thus providing us the potential to handle large-scale datasets. We run SPARQL queries with Jena  (Carroll et al., 2004) . If a question cannot be parsed into FOL or translated to SQL or SPARQL, we then treat it as a keyword query and retrieve the results from an inverted index built out of our data. 

 Analytics Instead of only retrieving a list of entities, our system provides several different types of analytics for different result sets. In many situations, the result is a set of records rather than one single entry. This provides us the opportunity to perform and provide further analyses of the result set for the users. Our system provides several types of analytics. Descriptive analytics summarize the facts in the result set. For instance, for the question "show me all drugs targeting pain", our system shows the distribution of all technologies used for such drugs in the result set. We also compare the drugs in the result set on different dimensions (e.g., diseases). Moreover, we compute trends via exponential smoothing for entities that have a temporal dimension. By linking entities from our KB to entity mentions in a large news corpus (14 million articles and 147 million sentences), we are able to perform additional analytics based on named entity recognition and sentiment analysis techniques. We adopted the Stanford CoreNLP toolkit  (Manning et al., 2014)  for recognizing person, organization, and location from the news corpus. Given an entity, we show its frequency count and how its sentiment may change over time. This information may provide further insights to users in order to support their own analysis. 

 Demonstration Script Outline Figure  2  shows the beginning of the sample query: companies developing drugs having an indication of . . . ? While the user is typing, a variety of possible extensions to the query are offered, and the user se-lects Hypertension (1). Our system shows a pie chart of each company's market share for hypertension drugs (2); we also show news mentions and sentiment analysis for the most discussed companies (3). For the demo, we will first motivate the use of natural language question answering for extracting information from complex, interlinked datasets. Next, we will demonstrate how the user can compose a variety of questions with auto-suggestion. Finally, we will walk through the generated analytics and various visualizations for different natural language questions in order to show how it allows the user to gain deeper insights into the data. 

 Conclusion and Future Work In this paper, we presented a Natural Language Interface for answering complex questions over linked data. Our system parses natural language questions to an intermediate logical representation based on a grammar derived from multiple interlinked datasets. Different translators are developed to translate a question from its FOL representation to SQL and SPARQL queries, which are then executed against an underlying knowledge graph/base for retrieving the answers and generating corresponding analytics. In future work, we intend to cover more domains and provide more complex analytics. We will also perform a thorough evaluation of our system. Figure Figure 1: System Architecture 
