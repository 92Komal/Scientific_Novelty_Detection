title
AMBIGQA: Answering Ambiguous Open-domain Questions

abstract
Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AMBIGQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AMBIGNQ, a dataset covering 14,042 questions from NQ-OPEN, an existing opendomain QA benchmark. We find that over half of the questions in NQ-OPEN are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AMBIGQA which we show benefit from weakly supervised learning that incorporates NQ-OPEN, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs. washington.edu/ambigqa.

Introduction In the open-domain setting, it can be difficult to formulate clear and unambiguous questions. For example, Figure  1  shows a Google search query  (Kwiatkowski et al., 2019)  that, perhaps surprisingly, has two possible interpretations given the evidence in Wikipedia. Although open-domain question answering (QA) systems aim to answer any factoid question  (Voorhees et al., 1999) , existing methods assume questions have a single welldefined answer. Nonetheless, ambiguity arises frequently in open-domain QA, where questions are written during information gathering (e.g., search queries) without knowledge of the answer. As we will see in Section 4, over 50% of the questions we sampled from a set of Google search queries are ambiguous. Furthermore, identifying ambiguities is difficult both for humans and machines. As shown in Figure  1 , ambiguity is a function of both the question and the evidence provided by a large text corpus. To study this challenge, we introduce AM-BIGQA (Answering Ambiguous Open-domain Questions), a new task which involves disambiguating and answering potentially ambiguous questions. Specifically, the model must (1) find a set of distinct, equally plausible answers to the question, and (2) provide minimal yet unambiguous rewrites of the question that clarify the interpretation which leads to each answer. Figure  1  shows two such disambiguated questions and their answers. To support the study of this task, we construct a dataset called AMBIGNQ using 14,042 questions from an open-domain version of NATURAL QUES-TIONS  (Kwiatkowski et al., 2019) , denoted NQ-OPEN. For each question, annotators search for, navigate, and read multiple Wikipedia pages to find as many answers as possible. The high prevalence of ambiguity makes the task difficult even for human experts; it is inherently difficult to know if you have found every possible interpretation of a question. Nonetheless, we are able to collect high quality data covering high levels of ambiguity (2.1 distinct answers per question on average) with high estimated agreement (89.0 F1) on valid answers. The types of ambiguity are diverse and sometimes subtle (Table  1 ), including ambiguous entity or event references, or ambiguity over the answer type; many are only apparent after examining one or more Wikipedia pages. To establish initial performance levels on this data, we present a set of strong baseline methods. We extend a state-of-the-art QA model  (Karpukhin et al., 2020)  with three new components: (1) set-based question answering with a sequence-tosequence model, (2) a question disambiguation model, and (3) a modification to democratic cotraining  (Zhou and Goldman, 2004 ) which leverages the partial supervision available in the full NQ-OPEN dataset. We also do an ablation study and qualitative analysis, which suggest there is significant room for future work on this task. To summarize, our contributions are threefold. 1. We introduce AMBIGQA, a new task which requires identifying all plausible answers to an open-domain question, along with disambiguated questions to differentiate them. 2. We construct AMBIGNQ, a dataset with 14,042 annotations on NQ-OPEN questions containing diverse types of ambiguity. 3. We introduce the first baseline models that produce multiple answers to open-domain questions, with experiments showing their effectiveness in learning from our data while highlighting avenues for future work. 

 Related Work Open-domain Question Answering requires a system to answer any factoid question based on evidence provided by a large corpus such as Wikipedia  (Voorhees et al., 1999; Chen et al., 2017) . Existing benchmarks use questions of various types, from open-ended information-seeking  (Berant et al., 2013; Kwiatkowski et al., 2019; Clark et al., 2019)  to more specialized trivia/quiz  (Joshi et al., 2017; Dunn et al., 2017) . To the best of our knowledge, all existing formulations assume each question has a single clear answer. Our work is built upon an open-domain version of NATURAL QUESTIONS  (Kwiatkowski et al., 2019) , denoted NQ-OPEN, composed of questions posed by real users of Google search, each with an answer drawn from Wikipedia. NQ-OPEN has promoted several recent advances in open-domain question answering  Asai et al., 2020; Min et al., 2019a,b; Guu et al., 2020; Karpukhin et al., 2020) .  Nonetheless, Kwiatkowski et al. (2019)  report that the answers to such questions are often debatable, and the average agreement rate on NQ-OPEN test data is 49.2%, 1 in large part due to ambiguous questions. In this work, we embrace this ambiguity as inherent to information seeking open-domain QA, and present the first methods for returning sets of answers paired with different interpretations of the question. Clarification Questions have been used to study question ambiguity in other settings. Research on community Q&A  (Braslavski et al., 2017; Daum? III, 2018, 2019)  studies finding underspecification in the question, but it does not find the answer to the original question. In recent work,  Xu et al. (2019)  study clarification of questions that are intentionally annotated with pre-specified entity reference ambiguities.  Aliannejadi et al. (2019)  and  Zamani et al. (2020)  use clarification questions to refine intents of simple query logs without immediately apparent information needs (e.g., single keywords like dinosaur 2 ). In contrast, we study open-domain factoid questions asked by real users: these present clear information needs, but carry diverse naturally occurring ambiguities (see Table  1 ). Furthermore, instead of prolonging the user's information-seeking session with clarification questions, our task formulation provides a complete and immediate solution with unambiguous rewrites of the original question. Question Rewriting is a novel, well-defined task which we propose for differentiating distinct answers. To the best of our knowledge, it has not been studied for resolving ambiguity; we are only aware of  Elgohary et al. (2019)  which use question rewriting to convert conversational questions into self-contained questions. 3 Task: AMBIGQA 

 AMBIGQA Setup Figure  1  depicts the AMBIGQA task. The input is a prompt question q, and the output is a list of n question-answer pairs (x 1 , y 1 ), . . . , (x n , y n ), where each y i is an equally plausible answer to q, and each x i is a minimally edited modification of q whose answer is unambiguously y i . We consider two subtasks. Multiple Answer Prediction. Given a question q, output a set of semantically distinct and equally plausible answers y 1 , . . . , y n , where n is unknown. Question Disambiguation. Given q and a set of answers y 1 , . . . , y n , generate disambiguated questions x 1 , . . . , x n , where each x i is a minimal edit of q which makes it unambiguous so that y i is a correct answer and all y j for all j = i are incorrect. When n = 1, this task is trivial, as x 1 = q. We choose to represent ambiguity with a set of disambiguated questions because it is well-defined, immediately human-interpretable, and allows for straightforward annotation of a wide range of ambiguities without complex guidelines. 

 Evaluation Metrics To evaluate model performance, we present several ways to compare a model prediction with m question-answer pairs (x 1 , y 1 ), . . . , (x m , y m ) with a gold reference set with n pairs (x 1 , ?1 ), . . . , (x n , ?n ). Since there may be more than one way to refer to a single answer (e.g., Michael Jordan and Michael Jeffrey Jordan) each gold answer ?i is a set of acceptable answer strings, where all ?i are disjoint. We assign each predicted question-answer pair (x i , y i ) a correctness score based on a string similarity function f valued in [0, 1]. c i = max 1?j?n I[y i ? ?j ]f (x i , xj ). Intuitively, c i considers (1) the correctness of the answer and (2) the similarity f (x i , xj ) between the predicted and reference question. We calculate F1 treating the c i as measures of correctness: prec f = i c i m , rec f = i c i n , F1 f = 2 ? prec f ? rec f prec f + rec f . We consider three choices of F f . F1 ans is the F1 score on answers only, where f always yields 1. This may be used without the question disambiguation step. F1 BLEU accounts for string similarity between questions, calculating f with BLEU  (Papineni et al., 2002) . F1 EDIT-F1 uses EDIT-F1 as f , where EDIT-F1 is a new measure that represents each disambiguated question by its added and deleted unigrams compared to the prompt question, and computes the F1 score between them. For example, consider the prompt question "Who made the play the crucible?", the reference "Who wrote the play the crucible?" and the prediction "Who made the play the crucible in 2012?". The gold edits 3 here are -made , +wrote while the predicted edits are +in , +2012 . Their EDIT-F1 is thus zero, even though the questions are similar. Unlike BLEU which we use to directly measure similarity to the gold question, this metric only gives credit for getting the key semantic differences correct between the original question and the clarification. 4 Data: AMBIGNQ 

 Data Collection We construct AMBIGNQ using prompt questions from NQ-OPEN and English Wikipedia as the evidence corpus. We use Amazon Mechanical Turk for crowdsourcing. The crucial annotation challenge is maximizing recall: finding all possible distinct answers to a question. This is difficult, as ambiguities are often only apparent after carefully searching the evidence for multiple possible answers. However, we can collect high quality data with high levels of ambiguity using careful worker selection and a two stage pipeline: generation and validation. Generation. Workers in the first stage are given a prompt question and a search box that uses the Google Search API restricted to English Wikipedia. Allowing annotators to find Wikipedia pages on their own closely approximates the real process people use to answer open-ended questions-an approach with no existing large-scale dataset.  4  Workers find all plausible answers to the question; when there are multiple, each answer is paired with a minimal edit of the prompt question which differentiates it from the other answers, in line with our task requirements. A distinct answer may be annotated as multiple possible spans (e.g., Michael Jordan and Michael Jeffrey Jordan). As a special case, some questions contain temporal deixis which depends on the time of writing, e.g., "When does the new family guy season come out?". To avoid unmanageably many answers, we Table  2 : Data statistics. For the number of QA pairs (# QAs), the minimum is taken when there are more than 1 accepted annotations. instruct workers to remove the time-dependence by rewriting the prompt question for up to three most recent events before Jan 1, 2018, e.g., "When does family guy season 16 come out?" (see Table  1 ). Validation. Workers in the validation stage review the annotations provided by multiple generators. Validators mark each generator's annotations as correct or incorrect, or provide a new set of question-answer pairs by combining the valid ones from each generator. They search Wikipedia as generators do, and are additionally given Wikipedia pages that generators viewed to speed up the process. Validation is skipped when annotated answers from all generators exactly match (37% of cases). Quality control. We recruit highly qualified workers through a qualification test (details in Appendix A). Although the task was difficult for most workers, we found that our highly qualified fulltime workers, given quick and detailed feedback on their work, produced high accuracy and recall. For development and test data, we use two generators and one validator per prompt question. For training data, we skip validation and only use one generator per question. Inter-annotator agreement. Evaluating generators against each other on the development set yields 60.8 F1 ans . All annotations passed validation for 76% of questions, while validators made changes (edits or exclusions) in the remaining 24%. The average F1 ans between co-authors and workers on a sample of 50 validations was 89.0%. This indicates that, despite the intrinsic difficulty and subjectivity of the task, humans agree on the boundary between valid and invalid answers in most cases. 

 Data Analysis The final dataset contains 14,042 annotated examples, split consistently with NQ-OPEN. As shown in Table  2 , over 50% of development and test examples contain multiple question-answer pairs. This  indicates a high rate of ambiguity in NQ-OPEN, even though previous work has studied it with the assumption that each question has a single answer. We also find a discrepancy between development and test; this is likely due to the way in which NQ-OPEN is constructed, which over-samples difficult questions in the test set (see Appendix B for details). The training set contains relatively fewer ambiguous examples (47%), presumably because using only one worker per training example yielded slightly lower recall. Types of ambiguity. Table  1  shows a breakdown of the types of ambiguity in AMBIGNQ. They are diverse, including ambiguity in entity references, event references, properties, and answer types, with a relatively uniform distribution between them. In comparison to  Xu et al. (2019) , who intentionally elicit questions with ambiguous entity references, our analysis shows that unintended ambiguity comes from diverse sources. In many cases, ambiguity is not apparent from the prompt question alone, but only after researching the question on Wikipedia, as evidenced by differences in model performance (Section 6.2). Annotator behavior. Figures  2a and 2b  show the number of unique Wikipedia pages and the number of search queries used by workers during annotation. More often than not, workers used multiple queries and navigated multiple Wikipedia pages, showing how our setup captures ambiguity in the retrieval step of open-domain question answering, which is missed in approaches that assume a prespecified evidence document. Distribution of edits. Figure  2c  shows unigram edits made to questions in the development data, where we remove stopwords except wh-words and group numeric values by the number of digits. Adding numerals such as years is common, as they can easily disambiguate entity or event references or remove time dependence. Wh-word changes are also common, especially for specifying the answer type (e.g., from who to which group; see Table  1 ). The distribution of edits is fairly long-tailed, with the 100 most frequent edits covering 36% of the total, and the top 1,000 covering 69%. 

 Model To set initial performance levels on AMBIGNQ, we present a baseline AMBIGQA model combining ideas from recent advances in open-domain QA  (Karpukhin et al., 2020)  and generation  (Lewis et al., 2020) . Given a prompt question q, our model predicts answers y 1 ..y n , and generates corresponding questions x 1 ..x n conditioning on q, the answers y 1 ..y n , and the evidence passages.   (Lewis et al., 2020) . Specifically, it conditions on the concatenation of q and the top passages in order up to 1024 tokens, and sequentially generates distinct answers token-by-token, separated by  [SEP] . We pretrain SPANSEQGEN on NQ-OPEN and finetune it on AMBIGNQ. We develop SPANSEQGEN primarily because  Karpukhin et al. (2020)  is designed for generating a single answer, but SPANSEQGEN also boosts the // Train C sequence-to-sequence QA models 6: for i ? {1..C} do 7: ?i ? train( Dfull) 8: DL ?Dfull 9: for (q j , y j ) ?Dpartial do 10: // Get predictions by using yj as prefix 11: ? j ? {? | ? = y j , and 12: |{i | ? ? ?i(q j |y j ), 1 ? i ? C}| > C 2 13: } 14: if | ? j | > 0 then 15: // Add it as a multiple answer case 16: Dfull? DL ? {(q j , {y j } ? ? j )} 

 17: else if ?i = 1..C, |?i(x j ) ? {y j }| = 0 then 18: // Add it as a single answer case 19: Dfull? DL ? {(q j , {y j })} performance on NQ-OPEN (41.5?42.2 on the test data). We include ablations on different approaches and models in Section 6.2. Question Disambiguation. We design a question disambiguation (QD) model based on BART. The model generates each question x i (i = 1..n) conditioning on the concatenation of q, the target answer y i , other answers y 1 ..y i?1 , y i+1 ..y n , and the top passages as used by SPANSEQGEN. We pretrain on NQ-OPEN to generate questions given an answer and passage, and then finetune it on the full task data in AMBIGNQ. We include ablations on different variants of the model in Section 6.2. Co-training with weak supervision. Given the prevalence of unlabelled ambiguity in NQ-OPEN, we introduce a method that treats the NQ-OPEN annotations as weak supervision and learns to discover potential ambiguity in the data. We modify a democratic co-training algorithm  (Zhou and Goldman, 2004)  as described in Algorithm 1. We iteratively grow the training set Dfull from AMBIGNQ (D full ) with silver data from NQ-OPEN (D partial ) predicted by a majority of a set C of SPANSEQGEN models trained on Dfull . The key step is injecting the known answer y j from NQ-OPEN as a prefix to SPANSEQGEN's output during prediction. In each step, if a majority of C predict an additional answer, we assume we have found a false negative and add the result to the training set Dfull . If all models predict no additional answer, we add the example to Dfull with y j as a single answer. 

 Experiments We describe the baseline models used in our experiments, followed by results and ablations. Implementation details and hyperparameters of all models are provided in Appendix D. 6.1 Baselines DISAMBIG-FIRST. This baseline disambiguates the prompt question without any context from plausible answers or reference passages. Specifically, it implements the following pipeline: (1) Feed the prompt question q into a BERT-based binary classifier to determine whether it is ambiguous. (2) If q is ambiguous, pass it into a BART-based model which generates a sequence of disambiguated questions x 1 ..x n (n > 1), separated by [SEP]; otherwise, consider only x 1 = q. (3) Feed each x i into a state-of-the-art model on NQ-OPEN  (Karpukhin et al., 2020)  to produce its answer y i . Thresholding + QD. We also include a model based on  Karpukhin et al. (2020) , with thresholding for multiple answer prediction and our question disambiguation (QD) model.  Karpukhin et al. (2020)  outputs a likelihood score for each span; we obtain y 1 ..y n by taking valid spans with likelihood larger than a hyperparameter ?. The model is trained to maximize the marginal likelihood of any span in the gold answer set ?1 .. ?n . As with SPANSEQGEN, we pretrain on NQ-OPEN and finetune on AM-BIGNQ. We then produce disambiguated questions using our BART-based QD model (Section 5). 

 Results Table  3  reports the performance of our baselines; example model outputs are provided in Table  5 . Main results. We first find that DISAMBIG-FIRST is significantly worse than other models. In particular, classification accuracy on whether the prompt question is ambiguous is 67%, close to the majority baseline (60%). When the model does identify an ambiguous question, its rewrites often look reasonable on the surface, but do not match the facts. For instance, in example 1 of not as great as we expected. This suggests two things. First, thresholding may be a surprisingly effective baseline for outputting multiple answers, even though the answers must compete with each other for probability mass in order to surpass the threshold ?. Second, maximizing likelihood in a sequence-to-sequence model like SPANSEQGEN may not produce well-calibrated results. For instance, the model seems to suffer due to variation in the length of the output sequence, outputting shorter sequences on average (3.0 tokens) than gold (6.7). 5 This leads to low recall when there are multiple answers; our best model achieves a precision of 49.6 and recall of 25.3 for its F1 ans of 31.7 on such questions. Overall, SPANSEQGEN achieves reasonable F1 ans scores. F1 ans on examples with multiple question-answer pairs (multi) are lower, indicating that predicting all plausible answers is more challenging than predicting a single answer, as expected. SPANSEQGEN also obtains the best performance in F1 BLEU and F1 EDIT-F1 , although their absolute values are low in general; we discuss this in our question disambiguation ablations below. There is a substantial difference in performance between development and test overall, likely due to distributional differences in the original questions in NQ-OPEN; detailed discussion is in Appendix B. Effect of co-training. The last two rows of Table 3 reports the effect of our co-training method. As co-training requires multiple trained models, we compare with a naive ensemble. While we see gains from ensembling alone, an ensemble trained with the co-training method achieves the best performance on all metrics. This result demonstrates the potential of jointly using AMBIGNQ and partial supervision from NQ-OPEN. Ablations on question disambiguation. Table 4 reports results of an ablation experiment on question disambiguation (QD). Among our ablations, we include models without the prompt question or untargeted answers as input, and a naive baseline that always outputs the prompt question. We report the metrics both in the scenarios of the full task and the gold answers given, to see the performance dependent on and independent from multiple answer prediction, respectively.  6  Simply copying the prompt question gives high F1 BLEU , which is natural since the questions were disambiguated using minimal edits. This justifies using F1 EDIT-F1 to evaluate semantic differences from the prompt question. In addition, we find that Prompt question #1: Where was snow white and the huntsman filmed? Reference: Q: Where were beach scenes for snow white and huntsman predominantly filmed? / A: Marloes Sands Beach Q: Where was principal photography for snow white and huntsman filmed? / A: United Kingdom Q: Where was castle in snow white and huntsman filmed? / A: Gateholm island Prediction of DISAMBIG-FIRST: (F1ans=0.40, F1EDIT-F1=0.00) Q: Where was snow white and the huntsman filmed in 2017? / A: Marloes Sands Beach Q: Where was snow white and the huntsman filmed during the filming of Season 1 of the TV series? / A: Marloes Sands Beach Prediction of SPANSEQGEN: (F1ans=0.80, F1EDIT-F1=0.69) Q: Where was snow white and huntsman principal photography filmed / A: United Kingdom Q: Where were beach scenes for snow white and huntsman mostly filmed / A: Marloes Sands Beach Prompt question #2: When was the city of new york founded? Reference: Q: When was city of new york founded by dutch and initially called new amsterdam? / A: 1624 Q: When was city of new york under english control and renamed to new york? / A: 1664 Prediction of SPANSEQGEN: (F1ans=1.00, F1EDIT-F1=0.67) Q: When was city of new york city founded with dutch protection? / A: 1624 Q: When was city of new york city founded and renamed with english name? / A: 1664 Table  5 : Model predictions on samples from the development data. (#1) DISAMBIG-FIRST generates questions that look reasonable on the surface but don't match the facts. SPANSEQGEN produces the reasonable answers and questions, although not perfect. (#2) SPANSEQGEN produces correct answers and questions. 

 Reference has multiple answers Multiple answer prediction is correct 2% Multiple answer prediction is partially correct ? 40% Multiple answer prediction is incorrect 14% Reference has one answer Over-generated predictions 2% Correct single answer prediction 26% Incorrect single answer prediction 12% Reference is incorrect 4% our QD model conditioned on all available context is better than other variants in overall metrics. Performance is low overall, even given the gold answers, highlighting the challenge of the task. We think there are two major reasons. First, maximizing the likelihood of the output sequence can miss the importance of edits to the prompt question, leading the QD model to miss the information that is most important to differentiate one answer from the others. Second, there is a lack of annotated data, especially for question disambiguation which does not benefit from weakly supervised learning with NQ-OPEN; future work can explore how to maximize the use of supervision from other available data. It is also worth noting that the metric may miss edits that are semantically correct, but phrased differently (see Table  7 : Zero-shot performance on multiple answer prediction of the models trained on NQ-OPEN. We report Exact Match (EM) on NQ-OPEN and F1 ans on AMBIGNQ. 

 Zero-shot results Since AMBIGNQ provides an evaluation set with explicit sets of multiple answers, we can also test if models trained on partial supervision only (NQ-OPEN) are capable of producing full answer sets. In fact, the problem of ambiguity already exists in previous QA tasks, and a single labeled answer can be viewed as a sample from a multi-modal distribution of answers. This setting is important for modeling in domains where single-answer datasets are available but full annotations like in AMBIGNQ are not. To this end, we present a zero-shot setting where a system predicts multiple distinct answers without using AMBIGNQ training data. We include four NQ-OPEN models including ours, consisting of diverse approaches and model architec-tures, as baselines. These models, when trained on NQ-OPEN, may be made to predict multiple answers via thresholding as described in Section 6.1. 7 Table  7  reports zero-shot performance. Although SPANSEQGEN outperforms  Karpukhin et al. (2020)  in the standard setting, it is worse in zero-shot F1 ans (multi), potentially because thresholding exacerbates the problems that SPANSEQGEN has with long sequences (Section 6.2). 

 Error Analysis Table  6  reports an analysis of predictions by SPANSEQGEN with co-training, based on 50 random samples from the development data; examples can be found in the Appendix (Table  10 ). When there are multiple reference answers, the model rarely gets all correct answers, although often generates a subset of them. In 15 out of 20 partially correct cases, the model produces only one answer, consistent with the under-generation we found in Section 6.2. In four out of those 15 cases, the model prediction is arguably the most likely answer, 8 but in the other 11 cases, it hard to argue for one answer over the other(s). It is also worth noting that accuracy on examples with a single answer is quite high, being correct in 13 out of 20 cases. This estimated accuracy on unambiguous questions is higher than state-of-the-art levels on NQ-OPEN (42 EM), suggesting that NQ-OPEN may substantially underestimate performance due to the prevalence of unmarked ambiguity. Together with our experimental results, this seems to indicate that recall of multiple answers is one of the primary challenges in AmbigQA. 

 Conclusion & Future Work We introduced AMBIGQA, a new task that involves providing multiple possible answers to a potentially ambiguous open-domain question, and providing a disambiguated question corresponding to each answer. We constructed AMBIGNQ, a dataset with 14,042 annotations on NQ-OPEN questions. Our analysis shows the dataset contains diverse types of ambiguity, often not visible from the prompt question alone. We also introduced a first base-line model for producing multiple answers to opendomain questions, with experiments showing its effectiveness in learning from our data while highlighting possible areas for improvement. Future research developing on AmbigQA models may include explicitly modeling ambiguity over events and entities or in the retrieval step, as well as improving performance on the difficult problems of answer recall and question disambiguation. Furthermore, future work may build on the AmbigQA task with more open-ended approaches such as (1) applying the approach to QA over structured data (such as ambiguous questions that require returning tables), (2) handling questions with no answer or ill-formed questions that require inferring and satisfying more complex ambiguous information needs, and (3) more carefully evaluating usefulness to end users. 
