title
Question Difficulty Estimation in Community Question Answering Services *

abstract
In this paper, we address the problem of estimating question difficulty in community question answering services. We propose a competition-based model for estimating question difficulty by leveraging pairwise comparisons between questions and users. Our experimental results show that our model significantly outperforms a PageRank-based approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. This implies the possibility of predicting question difficulty from the text of question descriptions.

Introduction In recent years, community question answering (C-QA) services such as Stackoverflow 1 and Yahoo! Answers 2 have seen rapid growth. A great deal of research effort has been conducted on CQA, including: (1) question search  (Xue et al., 2008; Duan et al., 2008; Suryanto et al., 2009; Zhou et al., 2011; Cao et al., 2010; Zhang et al., 2012; Ji et al., 2012)   ;  (2) answer quality estimation  (Jeon et al., 2006; Bian et al., 2009; Liu et al., 2008) ; (3) user expertise estimation  (Jurczyk and Agichtein, 2007; Zhang et al., 2007; Bouguessa et al., 2008; Pal and Konstan, 2010; ; and (4) question routing  (Zhou et al., 2009; Li and King, 2010; . However, less attention has been paid to question difficulty estimation in CQA. Question difficulty estimation can benefit many applications: (1) Experts are usually under time constraints. We do not want to bore experts by routing every question (including both easy and hard ones) to them. Assigning questions to experts by matching question difficulty with expertise level, not just question topic, will make better use of the experts' time and expertise  (Ackerman and McDonald, 1996) . (  2 )  Nam et al. (2009)  found that winning the point awards offered by the reputation system is a driving factor in user participation in CQA. Question difficulty estimation would be helpful in designing a better incentive mechanism by assigning higher point awards to more difficult questions. (3) Question difficulty estimation can help analyze user behavior in CQA, since users may make strategic choices when encountering questions of different difficulty levels. To the best of our knowledge, not much research has been conducted on the problem of estimating question difficulty in CQA. The most relevant work is a PageRank-based approach proposed by  Yang et al. (2008)  to estimate task difficulty in crowdsourcing contest services. Their key idea is to construct a graph of tasks: creating an edge from a task t 1 to a task t 2 when a user u wins task t 1 but loses task t 2 , implying that task t 2 is likely to be more difficult than task t 1 . Then the standard PageRank algorithm is employed on the task graph to estimate PageRank score (i.e., difficulty score) of each task. This approach implicitly assumes that task difficulty is the only factor affecting the outcomes of competitions (i.e. the best answer). However, the outcomes of competitions depend on both the difficulty levels of tasks and the expertise levels of competitors (i.e. other answerers). Inspired by , we propose a competition-based approach which jointly models question difficulty and user expertise level. Our approach is based on two intuitive assumptions: (1) given a question answering thread, the difficulty score of the question is higher than the expertise score of the asker, but lower than that of the best answerer; (2) the expertise score of the best answerer is higher than that of the asker as well as all other answerers. Given the two assumptions, we can determine the question difficulty score and user expertise score through pairwise comparisons between (1) a question and an asker, (2) a question and a best answerer, (3) a best answerer and an asker, and (4) a best answerer and all other non-best answerers. The main contributions of this paper are: ? We propose a competition-based approach to estimate question difficulty (Sec. 2). Our model significantly outperforms the PageRank-based approach  (Yang et al., 2008)  for estimating question difficulty on the data of Stack Overflow (Sec. 3.2). ? Additionally, we calibrate question difficulty scores across two CQA services to verify the effectiveness of our model (Sec. 3.3). ? Most importantly, we demonstrate that different words or tags in the question descriptions indicate question difficulty levels. This implies the possibility of predicting question difficulty purely from the text of question descriptions (Sec. 3.4). 

 Competition based Question Difficulty Estimation CQA is a virtual community where people can ask questions and seek opinions from others. Formally, when an asker u a posts a question q, there will be several answerers to answer her question. One answer among the received ones will be selected as the best answer by the asker u a or voted by the community. The user who provides the best answer is called the best answerer u b , and we denote the set of all non-best answerers as S = {u o 1 , ? ? ? , u o M }. Assuming that question difficulty scores and user expertise scores are expressed on the same scale, we make the following two assumptions: ? The difficulty score of question q is higher than the expertise score of asker u a , but lower than that of the best answerer u b . This is intuitive since the best answer u b correctly responds to question q that asker u a does not know. ? The expertise score of the best answerer u b is higher than that of asker u a and all answerers in S. This is straightforward since the best answerer u b solves question q better than asker u a and all nonbest answerers in S. Let's view question q as a pseudo user u q . Taking a competitive viewpoint, each pairwise comparison can be viewed as a two-player competition with one winner and one loser, including (1) one competition between pseudo user u q and asker u a , (2) one competition between pseudo user u q and the best answerer u b , (3) one competition between the best answerer u b and asker u a , and (4) |S| competitions between the best answerer u b and all non-best answers in S. Additionally, pseudo user u q wins the first competition and the best answerer u b wins all remaining (|S| + 2) competitions. Hence, the problem of estimating the question difficulty score (and the user expertise score) is cast as a problem of learning the relative skills of players from the win-loss results of the generated twoplayer competitions. Formally, let Q denote the set of all questions in one category (or topic), and R q denote the set of all two-player competitions generated from question q ? Q, i.e., R q = {(u a ? u q ), ( u q ? u b ), (u a ? u b ), (u o 1 ? u b ), ? ? ? , (u o |S| ? u b )}, where j ? i means that user i beats user j in the competition. Define R = ? q?Q R q (1) as the set of all two-player competitions. Our problem is then to learn the relative skills of players from R. The learned skills of the pseudo question users are question difficulty scores, and the learned skills of all other users are their expertise scores. TrueSkill In this paper, we follow  and apply TrueSkill to learn the relative skills of players from the set of generated competitions R (Equ. 1). TrueSkill  (Herbrich et al., 2007)  is a Bayesian skill rating model that is developed for estimating the relative skill levels of players in games. In this paper, we present a two-player version of TrueSkill with no-draw. TrueSkill assumes that the practical performance of each player in a game follows a normal distribu-tion N(?, ? 2 ), where ? means the skill level of the player and ? means the uncertainty of the estimated skill level. Basically, TrueSkill learns the skill levels of players by leveraging Bayes' theorem. Given the current estimated skill levels of two players (priori probability) and the outcome of a new game between them (likelihood), TrueSkill model updates its estimation of player skill levels (posterior probability). TrueSkill updates the skill level ? and the uncertainty ? intuitively: (a) if the outcome of a new competition is expected, i.e. the player with higher skill level wins the game, it will cause small updates in skill level ? and uncertainty ?; (b) if the outcome of a new competition is unexpected, i.e. the player with lower skill level wins the game, it will cause large updates in skill level ? and uncertainty ?. According to these intuitions, the equations to update the skill level ? and uncertainty ? are as follows: ?winner = ?winner + ? 2 winner c ? v ( t c , ? c ) , ( 2 ) ? loser = ? loser ? ? 2 loser c ? v ( t c , ? c ) , ( 3 ) ? 2 winner = ? 2 winner ? [ 1 ? ? 2 winner c 2 ? w ( t c , ? c )] , (4) ? 2 loser = ? 2 loser ? [ 1 ? ? 2 loser c 2 ? w ( t c , ? c )] , (5) where t = ? winner ? ? loser and c 2 = 2? 2 + ? 2 winner + ? 2 loser . Here, ? is a parameter representing the probability of a draw in one game, and v(t, ?) and w(t, ?) are weighting factors for skill level ? and standard deviation ? respectively. Please refer to  (Herbrich et al., 2007)  for more details. In this paper, we set the initial values of the skill level ? and the standard deviation ? of each player the same as the default values used in  (Herbrich et al., 2007) . 

 Experiments 

 Data Set In this paper, we use Stack Overflow (SO) for our experiments. We obtained a publicly available data set 3 of SO between July 31, 2008 and August 1, 2012. SO contains questions with various topics, such as programming, mathematics, and English. In this paper, we use SO C++ programming (SO/CPP) and mathematics 4 (SO/Math) questions for our main experiments. Additionally, we use the data of Math Overflow 5 (MO) for calibrating question difficulty scores across communities (Sec. 3.3). The statistics of these data sets are shown in Table  1  To evaluate the effectiveness of our proposed model for estimating question difficulty scores, we randomly sampled 300 question pairs from both SO/CPP and SO/Math, and we asked experts to compare the difficulty of every pair. We had two graduate students majoring in computer science annotate the SO/CPP question pairs, and two graduate students majoring in mathematics annotate the SO/Math question pairs. When annotating each question pair, only the titles, descriptions, and tags of the questions were shown, and other information (e.g. users, answers, etc.) was excluded. Given each pair of questions (q 1 and q 2 ), the annotators were asked to give one of four labels: (1) q 1 ? q 2 , which means that the difficulty of q 1 was higher than q 2 ; (2) q 1 ? q 2 , which means that the difficulty of q 1 was lower than q 2 ; (3) q 1 = q 2 , which means that the difficulty of q 1 was equal to q 2 ; (4) Unknown, which means that the annotator could not make a decision. The agreements between annotators on both SO/CPP (kappa value = 0.741) and SO/Math (kappa value = 0.873) were substantial. When evaluating models, we only kept the pairs that annotators had given the same labels. There were 260 SO/CPP question pairs and 280 SO/Math question pairs remaining. 

 Accuracy of Question Difficulty Estimation We employ a standard evaluation metric for information retrieval: accuracy (Acc), defined as follows: Acc = the number of correct pairwise comparisons the total number of pairwise comparisons . We use the PageRank-based approach proposed by  Yang et al. (2008)  as a baseline. As described in Sec. 1, this is the most relevant method for our problem. Table  2  gives the accuracy of the baseline and our Competition-based approach on SO/CPP and SO/Math. From the results, we can see that (1) the proposed Competition-based approach significantly outperformed the PageRank-based approach on both data sets; (2) PageRank-based approach only achieved a similar performance as randomly guessing. This is because the PageRank-based approach only models the outcomes of competitions affected by question difficulty. However, the outcomes of competitions depend on both the question difficulty levels and the expertise levels of competitors. Our Competition-based approach considers both these factors for modeling the competitions. The experimental results demonstrate the advantage of our approach. Acc@SO/CPP Acc@SO/Math PageRank 50.38% 48.93% Competition 66.54% 71.79% Table  2 : Accuracy on SO/CPP and SO/Math. 

 Calibrating Question Difficulty across CQA Services Both MO and SO/Math are CQA services for asking mathematics questions. However, these two services are designed for different audiences, and they have different types of questions. MO's primary goal is asking and answering research level mathematics questions 6 . In contrast, SO/Math is for people studying mathematics at any level in related fields 7 . Usually, the community members in MO are not interested in basic mathematics questions. If 6 http://mathoverflow.net/faq 7 http://area51.stackexchange.com/ proposals/3355/mathematics a posted question is too elementary, someone will suggest moving it to SO/Math. Similarly, if a posted question is advanced, the community members in SO/Math will recommend moving it to MO. Hence, it is expected that the ratio of difficult questions in MO is higher than SO/Math. In this section, we examine whether our competition-based model can identify such differences. We first calibrate the estimated question difficulty scores across these two services on a same scale. The key idea is to link the users who participate in both services. In both MO and SO/Math, users can specify their home pages. We assume that if a user u 1 on MO and a user u 2 on SO/Math have the same home page URL, they should be linked as one natural person in the real world. We successfully linked 633 users. They provided 18, 196 answers in SO/Math among which 10, 993 (60.41%) were selected as the best answers. In contrast, they provided 8, 044 answers in MO among which 3, 215 (39.97%) were selected as the best answers. This shows that these users reflect more competitive contests in MO. After the common users are linked, we have a joint data set of MO and SO/Math. Then, we can calibrate the estimated question difficulty scores across the two services by performing the competition-based model on the joint data set. Figure  1  shows the distributions of the calibrated question difficulty scores of MO and SO/Math on the same scale. As expected, we observed that the ratio of difficult questions in MO was higher than SO/Math. Additionally, these two distributions were significantly different (Kolmogorov-Smirnov Test, p-value < 0.05). This demonstrates that our competition-based model successfully identified the difference between questions on two CQA services. 

 Analysis on the Question Descriptions In this section, we analyze the text of question descriptions on the scale of question difficulty scores estimated by the competition model. Micro Level We first examine the frequency distributions of individual words over the question difficulty scores. Figure  3  shows the examples of four words in SO/CPP. We observe that the words 'list' and 'array' have the lowest mean of difficulty scores, compared to the words 'virtual' and 'gcc'. This is reasonable, since 'list' and 'array' are related to basic concepts in programming language, while 'virtual' and 'gcc' are related to more advanced topics. It can be observed that the order of the means of the difficulty scores of these words are well aligned to our learning process. Macro Level We evenly split the range of question difficulty scores into n buckets, and we grouped the questions into the n buckets according to which bucket their difficulty scores were in. Then, we had n question buckets and each bucket corresponded to a word distribution of questions. Let variable X denote the distance between the difficulty scores in two question buckets (which is the difference between the average difficulty scores of questions in the two buckets), and variable Y denote the Jensen-Shannon distance between word distributions in two question buckets. We examined the correlation between vari-X and variable Y . The experimental results showed that the correlation between these two variables were strongly positive. Specifically, the correlation coefficient on SO/CPP was 0.8129 and on SO/Math was 0.7412. In other words, when the distance between the difficulty scores of two buckets become larger, the two word distributions in the two buckets become less similar, and vice versa. We further visualized the word distribution in each question bucket. We set n as 3, and we had three question buckets: (1) easy questions; (2) normal questions; and (3) hard questions. Figure  3 .4 plots the tag clouds of SO/Math questions in the three buckets. The size of tags is proportional to the frequency of tags in each bucket. We observed that (1) the tag 'homework' and 'calculus' become smaller from easy questions to hard questions; (2) the tag 'set-theory' becomes larger. These observations also reflect our learning process. The above experimental results show that different words or tags of question descriptions reflect the question difficulty levels. This implies the possibility of predicting question difficulty purely from the text of question descriptions. 

 Conclusion and Future Work In this paper, we address the problem of estimating question difficulty in CQA services. Our proposed competition-based model for estimating question difficulty significantly outperforms the PageRankbased approach. Most importantly, our analysis shows that the text of question descriptions reflects the question difficulty. In the future, we would like to explore predicting question difficulty from the text of question descriptions. We also will investigate non-technical areas, where there might be no strongly distinct notion of experts and non-experts. Figure 1 : 1 Figure 1: The distributions of calibrated question difficulty scores of MO and SO/Math. 
