title
Kea: Expression-level Sentiment Analysis from Twitter Data

abstract
This paper describes an expression-level sentiment detection system that participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter. Our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive, negative or neutral. The proposed approach helps to understand the relevant features that contribute most in this classification task.

Introduction In recent years, Twitter has emerged as an ubiquitous and an opportune platform for social activity. Analyzing the sentiments of the tweets expressed by an international user-base can provide an approximate view of how people feel. One of the biggest challenges of working with tweets is their short length. Additionally, the language used in tweets is very informal, with creative spellings and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, such as, RT for "re-tweet" and #hashtags, which are a type of tagging for tweets. Although several systems tackle the task of analyzing sentiments from tweets, the task of analyzing sentiments at term or phrase-level within a tweet has remained largely unexplored. This paper describes the details of our expressionlevel sentiment detection system that participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter  (Wilson et al., 2013) . The goal is to mark expressions (a term or short phrases) in a tweet with their contextual polarity. This is challenging given the fact that the entire length of a tweet is restricted to just 140 characters. We describe the creation of an SVM classifier that is used to classify the contextual polarity of expressions within tweets. A feature set derived from various linguistic features, parts-of-speech tagging and prior sentiment lexicons was used to train the classifier. 

 Related Work Sentiment detection from Twitter data has attracted much attention from the research community in recent times  (Go et al., 2009; Pang et al., 2002; Pang and Lee, 2004; Wilson et al., 2005; T. et al., 2012) . However, most of these approaches classify entire tweets by their overall sentiment (positive, negative or neutral). The task at hand is to classify expressions with their contextual sentiment. Most of these expressions can be found in sentiment lexicons already annotated with their general polarity, but the focus of this task is to detect the polarity of that expression within the context of the tweet it appears in, and therefore, given the context, the polarity of the expression might differ from that found in any lexicon. One of the primary goals of this task is to facilitate the creation of a corpus of tweets with sentiment expressions marked with their contextual sentiments. Wilson, Wiebe and Hoffman  (Wilson et al., 2005 ) explored the challenges of contextual polarity of sentiment expressions by first determining whether an expression is neutral or polar and then disambiguating the polarity of the polar expressions. Nasukawa and Yi  (Nasukawa and Yi, 2003)  classified the polarity of target expressions using manually developed patterns. Both these approaches, however, experimented with general webpages and online reviews but not Twitter data. 

 Task Setup This paper describes the task of recognizing contextual sentiments of expressions within a tweet. Formally, given a message containing a marked instance of a word or a phrase, the task is to determine whether that instance is positive, negative or neutral in that context. A corpus of roughly 8343 twitter messages was made available by the task organizers, where each tweet included an expression marked as positive, negative or neutral. Also available was a development data set containing 1011 tweets with similarly marked expressions. The data sets included messages on a broad range of topics such as a mixture of entities (e.g., Gadafi, Steve Jobs), products (e.g., kindle, android phone), and events (e.g., Japan earthquake, NHL playoffs). Keywords and hashtags were used to identify and collect messages relevant to the selected topic, which were then annotated using Mechanical Turk. Further details regarding the task setup may be found in the task description paper  (Wilson et al., 2013) . The evaluation consisted of classifying 4435 expressions in a Twitter data set. Furthermore, to test the generalizability of the systems, the task organizers provided a test data set consisting of 2334 SMS messages, each containing a marked expression, for which no prior training data set was made available. 

 System Description Our aim by participating in the SemEval-2013 Sentiment Analysis in Twitter task was to investigate what features are most useful in distinguishing the different polarities. The various steps of building our system are described in detail as follows. 

 Tokenization Tweets are known for being notoriously noisy due to their length restricted to just 140 characters which forces users to be creative in order to get their messages across. This poses an inherent challenge when analyzing tweets which need to undergo some sig-nificant preprocessing. The first step includes tokenizing the words in the tweet. Punctuation is identified during the tokenization process and marked for inclusion as one of the features in the feature set. This includes Twitter-specific punctuation such as "#" hashtags, specific emoticons such as ":)" and any URL links are replaced by a "URL" placeholder. 

 n-gram features Each expression consists of one or more words, with the average number of words in an expression in the training data set found to be 2. We derive lower-case unigram and bigram as well as the full string features from the expressions which are represented by their frequency counts in the feature set. The n-grams were cleaned (stripped of any punctuation) before being included in the feature set as they were observed to provide better results than noisy n-grams. Note that the presence of punctuation did become a part of the feature set as described in 4.3. We also experimented with word-splitting, especially found in hashtags (e.g., #iamsohappy); however, contrary to our initial supposition, this step resulted in poorer results overall due to word-splitting error propagation and was therefore avoided. 

 POS tagging For tagging the various parts-of-speech of a tweet, we use the POS tagger  (Gimpel et al., 2011)  that is especially designed to work with English data from Twitter. The tagging scheme encompasses 25 tags (please see  (Gimpel et al., 2011)  for the full listing), including some Twitter-specific tags (which could make up as much as 13% of all tags as shown in their annotated data set) such as "#" hashtag (indicates topic/category for tweet), "@" at-mention (indicates another user as a recipient of a tweet), "RT" re-tweets and URL or email addresses. The punctuation (such as ":-)", ":b", "(:", amongst others) from the n-grams is captured using the "emoticon" and "punctuation" tags that are explicitly identified by this POS tagger trained especially for tweets. Table  1  shows an example using a subset of two POS tags for an expression (# Adj. and # Emoticon denotes the number of adjectives and emoticons respectively). Other POS tags include nouns (NN), verbs (VB) and so on. Features incorporating the information about the parts-of-speech of the expres- sion as well as the tweet denoted by their frequencies produced better results than using a binary notation. Hence frequency counts were used in the feature set. 

 Prior sentiment lexicon A prior sentiment lexicon was generated by combining four already existing polarity lexicons including the Opinion Lexicon  (Hu and Liu, 2004) , the Sen-tiWordNet  (Esuli and Sebastiani, 2006) , the Subjectivity Clues database  (Wilson et al., 2005)  and the General Inquirer  (Stone and Hunt, 1963) . If any of the words in the expression are also found in the prior sentiment lexicon, then the frequencies of such prior positive and negative words are included as features in the feature set. 

 Other features Other features found to be useful in the classification process include the length of the expression as well as the length of the tweet. A sample of the feature set is shown in Table  1 . 

 Classifier During development time, we experimented with different classifiers but in the end, the Support Vector Machines (SVM), using the polynomial kernel, trained over tweets from the provided train and development data outperformed all the other classifiers. The final feature set included four main features plus the n-grams as well as the features depicting the presence or absence of a POS in the expression and the tweet. 

 Experiments and Discussion The task organizers made available a test data set composed of 4435 tweets where each tweet contained an instance of an expression whose sentiment was to be detected. Another test corpus of 2334 SMS messages was also used in the evaluation to test how well a system trained on tweets generalizes on other data types. The metric for evaluating the systems is Fmeasure. We participated in the "constrained" version of the task which meant working with only the provided training data and no additional tweets/SMS messages or sentences with sentiment annotations were used. However, other resources such as sentiment lexicons can be incorporated into the system. Table  2 , which presents the results of our submission in this task, lists the F-score of the positive, negative and neutral classes on the Twitter test data, whereas Table  3  lists the results of the SMS message data. As it can be observed from the results, the negative sentiments are classified better than the positive ones. We reckon this may be due to the comparatively fewer ways of expressing a positive emotion, while the negative sentiment seems to have a much wider vocabulary (our sentiment lexicon has 25% less positive words than negative). Whereas the positive class has a higher precision, the negative class seems to have a more notable recall. The most striking observation, however, is the extremely low F-score for the neutral class. This may be due to the highly skewed proportion (less than 5%) of neutral instances in the training data. In future work, it will be interesting to see how balancing out the proportions of the three classes affects the classification accuracy. We also ran some ablation experiments on the provided Twitter and SMS test data sets after the submission. Table  4  reports the findings of experiments where, for example, "-prior polarities" indicates a feature set excluding the prior polarities. The metric used here is the macro-averaged F-score of the positive and the negative class. The baseline measure implements a simple SVM classifier using only the words as unigram features in the expression. Interestingly, contrary to our hypothesis dur-ing development time, using the POS of the entire tweet was the least helpful feature. Since this was an expression level classification task, it seems that using the POS features of the entire tweet may misguide the classifier. Unsurprisingly, the prior polarities turned out to be the most important part of the feature set for this classification task as it seems that many of the expressions' contextual polarities remained same as their prior polarities. 

 Class Precision    Table 1 : 1 Esperance will be without star player Youssef Msakni for the first leg of the Champions League final against Al Ahly on Saturday. #AFRICA Sample feature set for an expression (denoted in bold) Prior Polarity Length POS in Expression POS in Tweet n-grams Pos. Neg. Exp. Tweet #Adj. #Emoticon #Adj. #NN "without" "star" "without star" ... 0 0 3 23 0 0 1 13 1 1 1 ... 

 Table 2 : 2 Submitted results: Twitter test data Class Precision Recall F-score Positive 0.85 0.39 0.53 Negative 0.59 0.96 0.73 Neutral 0.18 0.06 0.09 Macro-average 0.6327 

 Table 3 : 3 Submitted results: SMS test data Twitter SMS Baseline 0.821 0.824 Full feature set (submitted) 0.639 0.632 -Prior polarities 0.487 0.494 -Lengths 0.612 0.576 -POS expressions 0.646 0.615 -POS tweets 0.855 0.856 

 Table 4 : 4 Macro-averaged F-score results using different feature sets6 ConclusionThis paper presented the details of our system which participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter. An SVM classifier was trained on a feature set consiting of prior polarities, various POS and other Twitter-specific features. Our experiments indicate that prior polarities from sentiment lexicons are significant features in this expression level classification task. Furthermore, a classifier trained on just tweets can general-ize considerably well on SMS message data as well. As part of our future work, we would like to explore what features are more helpful in not only classifying the positive class better, but also distinguishing neutrality from polarity.
