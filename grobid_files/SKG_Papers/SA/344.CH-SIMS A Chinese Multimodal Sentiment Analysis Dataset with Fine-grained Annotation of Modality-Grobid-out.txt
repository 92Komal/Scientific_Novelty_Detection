title
CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality

abstract
Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single-and multimodal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis. Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The full dataset and codes are available for use at https://github.com/ thuiar/MMSA.

Introduction Sentiment analysis is an important research area in Natural Language Processing (NLP). It has wide applications for other NLP tasks, such as opinion mining, dialogue generation, and user behavior analysis. Previous study  (Pang et al., 2008; Liu and Zhang, 2012)  mainly focused on text sentiment analysis and achieved impressive results. However, using text alone is not sufficient to determine the speaker's sentimental state, and text can be misleading. With the booming of short video applications, nonverbal behaviors (vision and audio) are introduced to solve the above shortcomings  (Zadeh et al., 2016; . In multimodal sentiment analysis, intra-modal representation and inter-modal fusion are two im- * * Corresponding Author portant and challenging subtasks  (Baltru?aitis et al., 2018; Guo et al., 2019) . For intra-modal representation, it is essential to consider the temporal or spatial characteristics in different modalities. The methods based on Convolutional Neural Network (CNN), Long Short-term Memory (LSTM) network and Deep Neural Network (DNN) are three representative approaches to extract unimodal features  Zadeh et al., , 2018a . For inter-modal fusion, numerous methods have been proposed in recent years. For example, concatenation , Tensor Fusion Network (TFN) , Lowrank Multimodal Fusion (LMF)  (Liu et al., 2018) , Memory Fusion Network (MFN)  (Zadeh et al., 2018a) , Dynamic Fusion Graph (DFG)  (Zadeh et al., 2018b) , and others. In this paper, we mainly consider late-fusion methods that perform intramodal representation learning first and then employ inter-modal fusion. An intuitive idea is that the greater the difference between inter-modal representations, the better the complementarity of intermodal fusion. However, it is not easy for existing late-fusion models to learn the differences between different modalities, further limits the performance of fusion. The reason is that the existing multimodal sentiment datasets only contain a unified multimodal annotation for each multimodal segment, which is not always suitable for all modalities. In other words, all modalities share a standard annotation during intra-modal representation learning. Further, these unified supervisions will guide intra-modal representations to be more consistent and less distinctive. To validate the above analysis, in this paper, we propose a Chinese multimodal sentiment analysis dataset with independent unimodal annotations, CH-SIMS. Figure  1  shows an example of the annotation difference between our proposed dataset and the other existing multimodal datasets. SIMS has 2,281 refined video clips collected from different movies, TV serials, and variety shows with spontaneous expressions, various head poses, occlusions, and illuminations. The CHEAVD  (Li et al., 2017)  is also a Chinese multimodal dataset, but it only contains two modalities (vision and audio) and one unified annotation. In contrast, SIMS has three modalities and unimodal annotations except for multimodal annotations for each clip. Therefore, researchers can use SIMS to do both unimodal and multimodal sentiment analysis tasks. Furthermore, researchers can develop new methods for multimodal sentiment analysis with these additional annotations. Based on SIMS, we propose a multimodal multitask learning framework using unimodal and multimodal annotations. In this framework, the unimodal and multimodal tasks share the feature representation sub-network in the bottom. It is suitable for all multimodal models based on late-fusion. Then, we introduce three late-fusion models, including TFN, LMF, and Late-Fusion DNN (LF-DNN), into our framework. With unimodal tasks, the performance of multimodal task is significantly increased. Furthermore, we make a detailed discus-sion on multimodal sentiment analysis, unimodal sentiment analysis and multi-task learning. Lastly, we verify that the introduction of unimodal annotations can effectively expand the difference between different modalities and obtain better performance in inter-modal fusion. In this work, we provide a new perspective for multimodal sentiment analysis. Our main contributions in this paper can be summarized as follows: ? We propose a Chinese multimodal sentiment analysis dataset with more fine-grained annotations of modality, CH-SIMS. These additional annotations make our dataset available for both unimodal and multimodal sentiment analysis. ? We propose a multimodal multi-task learning framework, which is suitable for all latefusion methods in multimodal sentiment analysis. Besides, we introduce three late-fusion models into this framework as strong baselines for SIMS. ? The benchmark experiments on the SIMS show that our methods learn more distinctive unimodal representations and achieve state-ofthe-art performance. 

 Related Work In this section, we briefly review related work in multimodal datasets, multimodal sentiment analysis, and multi-task learning. 

 Multimodal Datasets To meet the needs of multimodal sentiment analysis and emotion recognition, researchers have proposed various of multimodal datasets, including IEMOCAP  (Busso et al., 2008) , YouTube  (Morency et al., 2011) , MOUD  (P?rez-Rosas et al., 2013) , ICT-MMMO  (W?llmer et al., 2013) , MOSI  (Zadeh et al., 2016) , CMU-MOSEI  (Zadeh et al., 2018b)  and so on. In addition,  

 Multi-task Learning Multi-task learning aims to improve the generalization performance of multiple related tasks by utilizing useful information contained in these tasks  (Zhang and Yang, 2017) . A classical method is that different tasks share the first several layers and then have task-specific parameters in the subsequent layers  (Liu et al., 2015; Zhang et al., 2016b) . Based on this method, we design a multimodal multi-task learning framework for verifying the practicality and feasibility of independent unimodal annotations. 

 CH-SIMS Dataset In this section, we introduce a novel Chinese multimodal sentiment analysis dataset with independent unimodal annotations, CH-SIMS. In the following subsections, we will explain the data acquisition, annotation, and feature extraction in detail. 

 Data Collection Comparing with unimodal datasets, the requirements of multimodal datasets are relatively high. A fundamental requirement is that the speaker's face and voice must appear in the picture at the same time and remain for a specific period of time. In this work, to acquire video clips as close to life as possible, we collect target fragments from movies, TV series, and variety shows. After getting raw videos, we use video editing tools, Adobe Premiere Pro 1 , to crop target segments at the frame level, which is very time-consuming but accurate enough. Moreover, during the data collection and cropping, we enforce the following constraints: ? We only consider mandarin and are cautious with the selection of materials with the accent. ? The length of clips is no less than one second and no more than ten seconds. ? For each video clip, no other faces appear except for the speaker's face. Finally, we collect 60 raw videos and acquire 2,281 video segments. SIMS has rich character background, wide age range, and high quality. Table 1 shows the basic statistics for SIMS. 2 

 Annotation We make one multimodal annotation and three unimodal annotations for each video clip. In addition to the increase in workload, the mutual interference between different modalities is more confused. To avoid this problem as much as possible, we claim every labeler can only see the information in the current modality when annotating. Besides, conducting four annotations at the same time is not permitted. More precisely, every labeler makes unimodal annotation first and then performs multimodal annotation, which of the order is text first, audio second, then silent video, and multimodal last. For each clip, every annotator decides its sentimental state as -1 (negative), 0 (neutral) or 1 (positive). we have five independent students in this field making annotations. Then, in order to do both regression and multi-classifications tasks, we average the five labeled results. Therefore, the final labeling results are one of {-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0}. We further divide these values into 5 classifications: negative {-1.0, -0.8}, weakly negative {-0.6, -0.4, -0.2}, neutral {0.0}, weakly positive {0.2, 0.4, 0.6} and positive {0.8, 1.0}. The histogram in the left of Figure  2  shows the distribution of sentiment over the entire dataset in four annotations. We can see that negative segments are more than positive segments. The main reason is that actors in film and television dramas are more expressive in negative sentiments than positive ones. The confusion matrix in the right of Figure  2  indicates the annotations difference between different modalities, which is computed as: D ij = 1 N N n=1 (A n i ? A n j ) 2 (1) where i, j ? {m, t, a, v}, N is the number of all samples, A n i means the n th label value in modal i. From the confusion matrix, we can see that the difference between A and M is minimal, and the difference between V and T is maximal, which is in line with expectations. Because audio contains text information, closer to multimodal while the connection between video and text is sparse. Furthermore, we provide the other attribute annotations, including speakers' age and gender. And we use sentimental annotations only in our following experiments. 

 Extracted Features The extracted features for all modalities are as follows (we use the same basic features in all experiments): Text: All videos have manual transcription, including the Chinese and English versions. We use Chinese transcriptions only. We add two unique tokens to indicate the beginning and the end for each transcript. And then, pre-trained Chinese BERTbase word embeddings are used to obtain word vectors from transcripts  (Devlin et al., 2018) . It is worth noting that we do not use word segmentation tools due to the characteristic of BERT. Eventually, each word is represented as a 768-dimensional word vector. Audio: We use LibROSA (McFee et al., 2015) speech toolkit with default parameters to extract acoustic features at 22050Hz. Totally, 33dimensional frame-level acoustic features are extracted, including 1-dimensional logarithmic fundamental frequency (log F0), 20-dimensional Melfrequency cepstral coefficients (MFCCs) and 12dimensional Constant-Q chromatogram (CQT). These features are related to emotions and tone of speech according to  (Li et al., 2018) .     Text SubNet Audio SubNet Video SubNet Feature Fusion Network FC FC FC FC ? Y t < l a t e x i t s h a 1 _ b a s e 6 4 = " u B D p O 1 u e g T I N U k X e 5 q 6 J R p / L R v E = " > A A A B 6 n i c b V C 7 S g N B F L 0 b X z G + o o K N z W I Q r M J u L L Q M s b F M 0 D w k W c L s Z D Y Z M j u 7 z N w V w p J P s L F Q x N b W v / A L 7 G z 8 F i e P Q h M P X D i c c y / 3 3 u P H g m t 0 n C 8 r s 7 K 6 t r 6 R 3 c x t b e / s 7 u X 3 D x o 6 S h R l d R q J S L V 8 o p n g k t W R o 2 C t W D E S + o I 1 / e H V x G / e M 6 V 5 J G 9 x F D M v J H 3 J A 0 4 J G u n m r o v d f M E p O l P Y y 8 S d k 0 L 5 q P b N 3 y s f 1 W 7 + s 9 O L a B I y i V Q Q r d u u E 6 O X E o W c C j b O d R L N Y k K H p M / a h k o S M u 2 l 0 1 P H 9 q l R e n Y Q K V M S M T K E U M X N r T Y d E E U o m n R y J g R 3 8 e V l 0 i g V 3 f N i q W b S q M A M W T i G E z g D F y 6 g D N d Q h T p Q 6 M M D P M G z J a x H 6 8 V 6 n b V m r P n M I f y B 9 f Y D I f i R Z Q = = < / l a t e x i t > Y m < l a t e x i t s h a 1 _ b a s e 6 4 = " a H T w e l 4 y S h E m O z j 3 9 M 1 Z J T F l T 5 Y = " > A A A B 6 3 i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b F M w D w k L m F 2 M p s M m Z l d Z m a F s O Q X b C w U s b X 0 L / w C O x u / x d k k h S Y M l W S 2 K E w 4 M h H K H k d 9 p i g x f G w J J o r Z W x E Z Y o W J s f F k I X i L L y + T V q X s n Z c r D Z t G D W b I w z G c w B l 4 c A F V u I Y 6 N I H A E B 7 g C Z 4 d 4 T w 6 L 8 7 r r D X n z G c O 4 Q + c t x 9 M k Z F y < / l a t e x i t > Y a < l a t e x i t s h a 1 _ b a s e 6 4 = " V H X i 6 T X k O U Z j q Z L 7 g H K L V k 4 d y 1 A = " > A A A B 6 n i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b F M 0 D w k W c L s Z D Y Z M j O 7 z M w K Y c k n 2 F g o Y m v r X / g F d j Z + i 5 N H o Y k H L h z O u Z F C v D C K f j X C f R N M Z k i P u 0 b a n E g m o / n Z 4 6 R q d W 6 a E w U r a k Q V P 1 9 0 S K h d Y j E d h O g c 1 A L 3 o T 8 T + v n Z j w 0 k + Z j B N D J Z k t C h O O T I Q m f 6 M e U 5 Q Y P r I E E 8 X s r Y g M s M L E 2 H R y N g R v 8 e V l 0 i g V v f N i q W b T q M A M W T i G E z g D D y 6 g D N d Q h T o Q 6 M M D P M G z w 5 1 H 5 8 V 5 n b V m n P n M I f y B 8 / Y D B S y R U g = = < / l a t e x i t > Y v < l a t e x i t s h a 1 _ b a s e 6 4 = " b z T Z x d a t E N u 4 b X n W o p 0 7 y t Y U G f Q = " > A A A B 6 n i c b V C 7 S g N B F L 3 j M 8 Z X V L C x G Q y C V d i N h Z Y h N p Y J m o c k S 5 i d z C Z D Z m e X m d l A W P I J N h a K 2 N r 6 F 3 6 B n Y 3 f 4 u R R a O K B C 4 d z 7 u X e e / x Y c G 0 c 5 w u t r K 6 t b 2 x m t r L b O 7 t 7 + 7 m D w 7 q O E k V Z j U Y i U k 2 f a C a 4 Z D X D j W D N W D E S + o I 1 / M H 1 x G 8 M m d I 8 k n d m F D M v J D 3 J A 0 6 J s d L t f W f Y y e W d g j M F X i b u n O R L x 9 V v / l 7 + q H R y n + 1 u R J O Q S U M F 0 b r l O r H x U q I M p 4 K N s + 1 E s 5 j Q A e m x l q W S h E x 7 6 f T U M T 6 z S h Z O 4 B T O w Y V L K M E N V K A G F H r w A E / w j A R 6 R C / o d d a 6 g u Y z R / A H 6 O 0 H J Q C R Z w = = < / l a t e x i t > 

 Text Input Audio Input Video Input Vision: Frames extracted from the video segments at 30Hz. We use the MTCNN face detection algorithm  (Zhang et al., 2016a)  to extract aligned faces. Then, following  Zadeh et al. (2018b) , we use MultiComp OpenFace2.0 toolkit  (Baltrusaitis et al., 2018)  to extract the set of 68 facial landmarks, 17 facial action units, head pose, head orientation, and eye gaze. Lastly, 709-dimensional frame-level visual features are extracted in total. 

 Multimodal Multi-task Learning Framework In this section, we describe our proposed multimodal multi-task learning framework. Shown as Figure  3 , based on late-fusion multimodal learning framework , we add independent output units for three unimodal representations: text, audio, and vision. Therefore, these unimodal representations not only participate in feature fusion but are used to generate their predictive outputs. For the convenience in following introduction, in text, audio and vision, we assume that L u , D u i , D u r , where u ? {t, a, v}, represent the sequence length, initial feature dimension extracted by section 3.3 and representation dimension learned by unimodal feature extractor, respectively. The batch size is B. 

 Unimodal SubNets Unimodal subNets aim to learn intra-modal representations from initial feature sequences. A universal feature extractor can be formalized as: R u = S u (I u ) (2) where I u ? R B?L u ?D u i , R u ? R B?D u r . S u (?) is the feature extractor network for modal u. In this work, following ;  Liu et al. (2018) , we use a Long Short-Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  network, a deep neural network with three hidden layers of weights W a and a deep neural network with three hidden layers of weights W v to extract textual, acoustic and visual embeddings, respectively. 

 Feature Fusion Network Feature fusion network aims to learn inter-modal representation with three unimodal representations, formulated as: R m = F (R t , R a , R v ) (3) where R t , R a , R v ? R B?D u r are the unimodal representations. F (?) is the feature fusion network and R m is the fusion representation. In this work, for full comparison with existing works, we try three fusion methods: LF-DNN, TFN  and LMF  (Liu et al., 2018) . 

 Optimization Objectives Except for the training losses in different tasks, we sparse the sharing parameters via L2 norm, which aims to select intra-modal features. Therefore, our optimization objectives is: min 1 N t Nt n=1 i ? i L(y n i , ?n i ) + j ? j ||W j || 2 2 (4) where N t is the number of training samples, i ? {m, t, a, v}, j ? {t, a, v}. L(y n i , ?n i ) means the training loss of n th sample in modality i. W j is the sharing parameters in modality j and multimodal tasks. ? i is the hyperparameter to balance different tasks and ? j represents the step of weight decay of subNet j, respectively. Lastly, we use a three-layer DNN to generate outputs of different tasks. In this work, we treat these tasks as regression models and use the L1 loss as training loss in Equation  4 . 

 Experiments In this section, we mainly explore the following problems using SIMS: (1) Multimodal Sentiment Analysis: We evaluate the performance of multimodal multi-task learning methods comparing with the other methods. learning with unimodal annotations and set up multimodal baselines for SIMS. (2) Unimodal Sentiment Analysis: We analyze the performance in unimodal tasks with unimodal or multimodal annotations only. The aim is to validate the necessary of multimodal analysis and set unimodal baselines for SIMS. (3) Representations Differences: We use t-SNE to visualize the unimodal representations of models with or without independent unimodal annotations. The aim is to show that the learned unimodal representations are more distinctive after using unimodal annotations. 

 Baselines In this section, we briefly review our baselines used in the following experiments. Early Fusion LSTM. The Early Fusion LSTM (EF-LSTM)  (Williams et al., 2018)  concatenates initial inputs of three modalities first and then use LSTM to capture long-distance dependencies in a sequence. Later Fusion DNN. In contrast with EF-LSTM, the Later Fusion DNN (LF-DNN) learns unimodal features first and then concatenates these features before classification. Memory Fusion Network. The Memory Fusion Network (MFN)  (Zadeh et al., 2018a)   word-level alignment in three modalities. However, this is not easy for SIMS because we haven't found a reliable alignment tool of Chinese corpus. In this work, we follow  Tsai et al. (2019)  to use CTC  (Graves et al., 2006)  as an alternative. Low-rank Multimodal Fusion. The Low-rank Multimodal Fusion (LMF)  (Liu et al., 2018)  model learns both modality-specific and cross-modal interactions by performing efficient multimodal fusion with modality-specific factors. Tensor Fusion Network. The Tensor Fusion Network (TFN)  explicitly models view-specific and cross-view dynamics by creating a multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions across three modalities. Multimodal Transformer. The Multimodal Transformer (MULT)  (Tsai et al., 2019)  using the directional pairwise crossmodal attention to realize the interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another.  

 Experimental Details In this section, we introduce our experimental settings in detail, including dataset splits, hyperparameters selection, and our evaluation metrics. Dataset Splits. We shuffle all video clips in random first and then divide train, valid and, test splits by multimodal annotations. The detailed split results are shown in Table  3 . Hyper-parameters Selection. Due to the different sequence lengths in different segments, it is necessary that fixing sequence length for the specific modality. Empirically, we choose the average length plus three times the standard deviation as the maximum length of the sequence. Besides, for all baselines and our methods, we adjust their hyperparameters using grid search with binary classification accuracy. For a fair comparison, in each experiment, we select five same random seeds  (1, 12, 123, 1234, and 12345)    

 Results and Discussion In this section, we present and discuss the experimental results of the research questions introduced in Section 5. 

 Comparison with Baselines. We compare three new methods with the aforementioned baselines. In this part, we only consider the multimodal evaluation results though new methods are multi-task. Results are shown in Table  2 . Compared with single-task models, multi-task models have better performance in most of evaluation metrics. In particular, all three improved models (MLF-DNN, MLFM, and MTFN) have promotion significantly compared to corresponding original models (LF-DNN, LFM, and TFN) in all evaluation metrics except for Acc-5. The above results demonstrate that the introduction of independent unimodal annotations in multimodal sentiment analysis can significantly improve the performance of existing methods. Also, we find that some methods, such as MULT, that perform well on existing public datasets while they are not satisfactory on SIMS. It further illustrates that designing a robust, cross-lingual multimodal sentiment analysis model is still a challenging task, which is also one of our motivations for proposing this dataset. 

 Unimodal Sentiment Analysis. Due to the independent unimodal annotations in SIMS, we conducted two sets of experiments for unimodal sentiment analysis. In the first set of experiments, we use real unimodal labels to verify the model's ability of performing unimodal sentiment analysis. In the second set of experiments, we use multimodal labels instead of unimodal labels to verify the ability of predicting the true emotions of speakers when there is only unimodal information. Results are shown in Table  4 . Firstly, in the same unimodal task, the results under unimodal labels are better than those under multimodal labels. But the former cannot reflect the actual sentimental state of speakers. Secondly, under multimodal annotations, the performance with unimodal information only is lower than using multimodal information in Table  2 . Hence, it is inadequate to perform sentiment analysis using unimodal information only due to the inherent limitations of unimodal information. 

 Representations Differences. Another motivation for us to propose CH-SIMS is that we think the unimodal representation differences will be greater with independent unimodal annotations. We use t-SNE  (Maaten and Hinton, 2008)    

 Ablation Study In this section, we compare the difference in the effects of combining different unimodal tasks on multimodal sentiment analysis. We aim to further explore the influence on multimodal sentiment analysis with different unimodal tasks. Furthermore, we reveal the relationship between multi-task learning and multimodal sentiment analysis. We conducted multiple combination experiments to analyze the effects of different unimodal subtasks on the main multimodal task. In this part, we only report the results in MLF-DNN. Results are shown in Table  5 . The results show that in the case of partial absence of three unimodal subtasks, the performance of the multimodal task has not significantly improved, or even damaged. Two factors may cause an adverse effect in multimodal learning, including the consistency between different unimodal representations and the asynchrony of learning in different tasks. The former means that unified annotations guide the representations to be similar and lack complementarity in different modalities. The latter means that the learning process in different tasks is inconsistent.   5 : (%) Results for multimodal sentiment analysis with different tasks using MLF-DNN. "M" is the main task and "T, A, V" are auxiliary tasks. Only the results of task "M" are reported. "M, A" as an example, the sub-network of subtask "A" is supervised by multimodal loss and unimodal loss. In contrast, subtask "T" and subtask "V" are supervised by their unimodal loss only. It means the "A" is learned twice while the "T" and the "V" are learned once only during an training epoch. Therefore, the introduction of unimodal tasks will reduce the consistency of the representation and strengthen the complementarity, but will also cause the asynchrony. As more unimodal tasks are introduced, the positive effects of the former gradually increase, and the negative effects of the latter gradually decrease. Finally, when all unimodal tasks are added, the negative effect of the latter is almost dis-appearing. Finally, the performance of the model with tasks "M, T, A, V" reaches a peak. 

 Conclusion In this paper, we propose a novel Chinese multimodal sentiment analysis dataset with independent unimodal annotations and a multimodal multi-task learning framework based on late-fusion methods. We hope that the introduction of CH-SIMS will provide a new perspective for researches on multimodal analysis. Furthermore, we conduct extensive experiments on discussing unimodal, multimodal, and multi-task learning. Lastly, we summarize our overall findings as follows: ? Multimodal labels cannot reflect unimodal sentimental states always. The unified multimodal annotations may mislead the model to learn inherent characteristics of unimodal representations. ? With the help of unimodal annotations, models can learn more differentiated information and improve the complementarity between modalities. ? When performing multi-task learning, the asynchrony of learning in different subtasks may cause an adverse effect on multimodal sentiment analysis. In the future, we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy, including early-and middle-fusion. Figure 1 : 1 Figure 1: An example of the annotation difference between CH-SIMS and other datasets. For each multimodal clip, in addition to multimodal annotations, our proposed dataset has independent unimodal annotations. M: Multimodal, T: Text, A: Audio, V: Vision. 
