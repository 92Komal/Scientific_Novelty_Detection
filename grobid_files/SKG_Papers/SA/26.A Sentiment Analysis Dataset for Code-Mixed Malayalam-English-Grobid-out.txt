title
A Sentiment Analysis Dataset for Code-Mixed Malayalam-English

abstract
There is an increasing demand for sentiment analysis of text from social media which are mostly code-mixed. Systems trained on monolingual data fail for code-mixed data due to the complexity of mixing at different levels of the text. However, very few resources are available for code-mixed data to create models specific for this data. Although much research in multilingual and cross-lingual sentiment analysis has used semi-supervised or unsupervised methods, supervised methods still performs better. Only a few datasets for popular languages such as English-Spanish, English-Hindi, and English-Chinese are available. There are no resources available for Malayalam-English code-mixed data. This paper presents a new gold standard corpus for sentiment analysis of code-mixed text in Malayalam-English annotated by voluntary annotators. This gold standard corpus obtained a Krippendorff's alpha above 0.8 for the dataset. We use this new corpus to provide the benchmark for sentiment analysis in Malayalam-English code-mixed texts.

Introduction The Internet gave users the opportunities to express an opinion on any topic in the form of user reviews or comments. The comments are usually of an informal style, mostly in social media forums such as Youtube, Facebook, and Twitter, which opens up the ground for mixing languages in the same conversation for multilingual communities. Some people with different linguistic backgrounds and cultures mark their impressions about a subject with the individual feeling in mixed language as not all are comfortable with a single language alone  (Scotton, 1982; Tay, 1989; Suryawanshi et al., 2020b) . This unplanned switching between more than one language in the same conversation for the speaker's convenience is referred to as code-mixing  (Androutsopoulos, 2013; Chakravarthi et al., 2019a; Chakravarthi et al., 2020) . Even though many languages have their own scripts, social media users use nonnative script, usually Roman script,  (Saint-Jacques, 1987; Rosowsky, 2010)  for convenience in some part of the world, like India. This causes difficulties in finding the languages involved and also makes it hard to execute various existing natural language processing tasks, as these were developed for a single language  (Bali et al., 2014; Solorio et al., 2014) . Malayalam is one of the Dravidian languages spoken in the southern region of India with nearly 38 million Malayalam speakers in India and other countries  (Thottingal, 2019) . Malayalam is a deeply agglutinating language  (Sreelekha and Bhattacharyya, 2018) . The Malayalam script is the Vatteluttu alphabet extended with symbols from the Grantha alphabet. It is an alphasyllabary (abugida), a writing system that is partially "alphabetic" and partially syllable-based  (Krishnamurti, 2003; Lalitha Devi, 2019; Chakravarthi et al., 2019c) . Still, social media users use Roman script for typing due to it being easier to input. There is a lot of code-mixed data between Malayalam and English among the YouTube comments we surveyed. Monolingual datasets are available for Indian languages for various research aims  (Agrawal et al., 2018) . However, there are few attempts to make datasets for Malayalam code-mixed text. Thus traditional NLP tasks fail in this scenario due to the absence of a proper dataset. To create resources for a Malayalam-English code-mixed scenario, we collected comments of various Malayalam movie trailers from YouTube. Malayalam code-mixed sample text from the proposed dataset is shown below with the corresponding English glosses. ? Malayalam-English: Innaleyaaane kandath super Padam.....ellarum familyaaayi poyi kananam super abinayam English: "Watched yesterday only this super movie... everyone go and watch the movie with the family..super acting..". The English words 'super' and 'family' intra-sententially code mixed  (Barman et al., 2014)  with the Malayalam language. Also, the word 'familyaayi' is a new word combining both English and Malayalam, which is another kind of code-mixing called Intra-word switching that happens at the word level  (Das and Gamb?ck, 2014) . In this case, 'with family' is together said as a single word following Malayalam morphology. Although the main word 'family' is in English and as the sentence is in Malayalam, the new word takes Malayalam morphology. This comment can be considered as a positive comment from the viewer of the trailer of a Malayalam movie as it is clear that he enjoyed the movie and also recommend that movie to other viewers of the trailer. ? Malayalam-English: enthu oola trailer aanu ithu. poor dialogue delivery. English: "What a useless trailer is this? Poor dialogue delivery." Figure  1 : Data collection process. This is an example of inter-sentential code-mixing  (Barman et al., 2014) . 'Oola' a slang word for 'useless' which is popular among the youth of Kerala.The viewer expressed strong dislike against the whole trailer and one aspect is 'poor dialogue delivery'. This comment has been marked as a negative comment as the disapproval of the trailer is evident. Sentiment analysis is a topic of greater interest recently since business strategies can be enhanced with insights obtained from the opinion about the product or subject of interest from the users  (Balage Filho et al., 2012; Suryawanshi et al., 2020a) . As mentioned earlier, the greater part of comments in social media are code-mixed. The conducive nature of such platforms invits all users from different stratus of society to express their opinion about a subject with their own feeling. Hence it is true that the real sentiments about the subject can be extracted from the analysis of code-mixed data. Even with this massive enthusiasm for user-opinions, there is not much effort taken to analyse the sentiment of code-mixed content in under-resourced languages. The contribution of this paper is that we release the gold-standard code-mixed dataset for Malayalam-English annotated for sentiment analysis and provide comprehensive results on popular classification methods. To the best of our knowledge, this is the first code-mixed dataset for Malayalam sentiment analysis. Our code implementing these models along with the dataset is available freely for research purposes 1 . 

 Related Work The sentiment analysis task has become increasingly important due to the explosion of social media, and extensive research has been done for sentiment analysis of monolingual corpora such as English  (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019) , Russian  (Rogers et al., 2018) , German  (Cieliebak et al., 2017) , Norwegian  (Maehlum et al., 2019)  and Indian languages  (Agrawal et al., 2018; Rani et al., 2020) . There have been two traditional approaches to solve sentiment analysis problem such as lexicon-based, and machine learning approaches  (Habimana et al., 2019) . With the increasing popularity of lexicons in the field of sentiment analysis since 1966, new lexicons namely Word-Net  (Fellbaum, 1998) , WordNet-Affect  (Valitutti, 2004; Chakravarthi et al., 2018; Chakravarthi et al., 2019b) , Sen-1 https://github.com/bharathichezhiyan/MalayalamMixSentiment tiNet  (Poria et al., 2012) , and SentiWordNet  (Esuli and Sebastiani, 2006)  were primarily used. Although being famous for their simplicity, both traditional machine learning and lexicon-based approaches are not efficient when applied on user-generated data, due to the dynamic nature of such data. This is where deep learning approaches take the spotlight for being efficient in adapting to dynamic user-generated data. In the advent of transfer learning, GloVe  (Pennington et al., 2014) , Word2Vec  (Mikolov et al., 2013b) , fastText  (Bojanowski et al., 2017a)  comes with their pros and cons. Malayalam  (Nair et al., 2014; Sarkar and Chakraborty, 2015; Se et al., 2015; Se et al., 2016; Mouthami et al., 2013)  has official status in India and other countries. Several research activities on sentiment analysis and events are focused on Malayalam due to their population and use of this language. However, sentiment analysis on Malayalam-English is very low, and data are not easily available for the research. Code-mixed data contains informal language with numerous accidental, deliberate errors, mixing of language and grammatical mixing, which makes previous corpora and methods less suitable to train a model for sentiment analysis in code-mixed data. In the past few years, there have been increasing efforts on a variety of task using code-mixed text. However, the number of a freely available code-mixed dataset  (Ranjan et al., 2016; Jose et al., 2020)  are still limited in number, size, availability. For few languages, such as English-Hindi  (Joshi et al., 2016; Patra et al., 2018; Priyadharshini et al., 2020) , English-Spanish  (Solorio et al., 2014)    

 Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Malayalam-English and to ensure that enough data are available for research purposes. We used youtube-comment-scraper tool 2 to download the comments from YouTube. First, we collected 116,711 sentences for Malayalam from YouTube post comments. We collect the comments from the movie trailers of 2019 based on the YouTube search results for keyword "Malayalam movie 2019". Many of the comments that we downloaded were either fully in English or mixed. Therefore, we filtered out non-code-mixed corpus bases on language identification at comment level with the langdect library 3 . That is if the comment is fully in one language than we discarded that comment since monolingual resources are available for these languages. Comments in Malayalam script was also discarded. We preprocessed the comments by removing the emoji's, and sentence length longer than 15 or less than 5 words since sentence more than 15 words will be difficult for annotators. After cleaning, we got 6,738 sentences for Malayalam-English code-mixed post comments. 

 Annotation Setup For annotation, we adopted the approach taken by Mohammad (2016) and each sentence was annotated by a minimum of three annotators according to the following schema: ? Positive state: There is an explicit or implicit clue in the text suggesting that the speaker is in a positive state, i.e., happy, admiring, relaxed, and forgiving. ? Negative state: There is an explicit or implicit clue in the text suggesting that the speaker is in a negative state, i.e., sad, angry, anxious, and violent. ? Mixed feelings: There is an explicit or implicit clue in the text suggesting that the speaker is experiencing both positive and negative feeling: Comparing two movies ? Neutral state: There is no explicit or implicit indicator of the speaker's emotional state: Examples are asking for like or subscription or questions about the release date or movie dialogue. This state can be considered as a neutral state. ? Not in intended language: For Malayalam if the sentence does not contain Malayalam then it is not Malayalam. We anonymized sensitive elements that may result in the problem of confidentiality in the YouTube comments. We created Google Forms, in which we collected the annotator's email so the annotator can annotate only once. We collected gender, education and medium of schooling information to know the diversity of the annotators, and we informed the annotators about the use of the data for finding the diversity of annotators. The annotators were given a choice to quit the annotation whenever they are uncomfortable with annotation. Each Google Form has to set contain a maximum of 100 sentences. The annotation of each corpus was performed in three phases. First, each sentence was annotated by two annotators. were collected if both annotators agreed, in the case of conflict, a third annotator annotated the sentence. In the third step, if all the three annotators did not agree, then two more annotators annotated the sentences. 

 Annotators Once the Google form was ready, we sent it out to an equal number of male and females to annotate. In the end, six annotators volunteered to annotate all of who are Malayalam-English bilingual proficiency and ready to take up the task seriously. From Table  1 , we can see that four female and two male voluntarily annotated our forms. All of them were postgraduates. Though among the annotators only one did schooling in native (Malayalam) medium and others in English medium, we ensured it would not affect the task as all of them are fully proficient at using this language. 

 Corpus Statistics Table  2  shows the corpus statistics of Malayalam-English code-mixed dataset. As is shown, this huge corpus at the end has 70,075 tokens, where 19,992 are unique. There are 6,739 comments and 7,743 distinct sentences in our codemixed sentiment dataset. On average, there are ten tokens per sentence, and there is at least one sentence per post. As mentioned before, the whole data has been categorized into five groups viz: positive, negative, neutral, mixed feeling, non-Malayalam. The distribution of data each category is detailed in Table  3 . Out of 6,739 posts, 2,811 comments have a positive polarity which is the most frequent category here. If there is no indication of the speaker's emotional state about the subject in the post, the post belongs to a neutral state which is the second-largest category with 1,903 posts here. This may be due to the increasing trend of asking for likes to their comments by the users. We split the corpus retaining 20 percentage that is 1,348 for test, 10 percentage for validation that is 674 for validation, and remaining for training. 

 Inter Annotator Agreement While labelling, the corpus linguist has to decide independently to which category the comment to be added following the guidelines provided strictly. It could be inferred that the guidelines for annotation were clearly understood by all the annotators if they made the same annotations freely. Because of this existence of more than one annotator to label the same set of data, it is necessary to have a metric to compare those annotation qualities. This motivates the use of Though computationally complex, we used Krippendorff's alpha (?) a prominent method among the numerous approaches developed to measure the degree of agreement between annotators. Krippendorff's alpha (?) is more relevant in our case as it is not affected by missing data, takes care of varying sample sizes, categories, numbers of raters and can also be employed to any measurement levels like nominal, ordinal, interval, ratio. Since more than two people have done the annotation task here and the same peoples annotate not all sentences, Krippendorff's alpha (?) fits here more. We used nltk 4 for calculating Krippendorff's alpha (?). Our annotation produced an agreement of 0.890 using nominal metric and 0.911 using interval metric. 

 Difficult Examples While annotating, a few of the comments were ambiguous about sensing the right feelings from the viewers. Hence the task of annotation for sentiment analysis seemed difficult. The problems include the comparison of the movie with movies of same or other industries, expression of opinion of different aspects of the movie in the same sentence. Below shows a few examples of such comments and detailed how we resolved those issues. ? Kanditt Amala Paul Aadai Tamil mattoru version aanu ennu thonnunu 4 https://www.nltk.org/ "It looks like another version of amala paul's Tamil movie aadai". Here the viewer doubts the Malayalam movie 'Helen' is similar to the Tamil movie 'Aadai'.Though that movie 'Aadai' was a positively reviewed movie by viewers and critics, we cannot generalize and assume this comment also as positive only because of this comparison. Hence we add it to the category of 'mixed feeling'. ? Evideo oru Hollywood story varunnilleee. Oru DBT. "Somewhere there is a Hollywood storyline...one doubt." This is also a comparison comment of that same movie 'Helen' mentioned above. Nevertheless, here the difference is that it is compared with the whole Hollywood standard, which is accepted worldwide. Hence it is marked as a positive comment. ? Trailer pole nalla story undayal mathiyarinu. "It was good enough to have a good story like the trailer". Here viewer mentioned about two aspects of that movie viz: 'trailer' and 'story'. He appreciates the trailer but at the same time doubt about the story. This comment we considered as a positive comment as it is clear that he enjoyed the trailer and also shows strong optimism for that particular movie. 

 Benchark Systems Traditional machine learning algorithm such as Logistic regression (LR), Support vector machine (SVM), Decision tree (DT), Random Forest (RF), Multinomial Naive Bayes (MNB), K-nearest neighbours (KNN) have been used on the newly annotated English-Malayalam dataset to show the insights about the dataset. The input features are the Term Frequency Inverse Document Frequency (TF-IDF). This approach makes these models trained only on this dataset without taking any pre-trained embeddings. We also show the result in the deep learning-based models. This is due to the dynamic nature of the data as it is hard to derive a pattern just by using the handcrafted features, which later could be feed inside the algorithms such as logistic regression (LR), support vector machines (SVM). To provide a simple baseline, we implemented four models, which includes Dynamic Meta-Embeddings DME  (Kiela et al., 2018)   2018), 1D Dimensional Convolution 1DConv  (Zhou et al., 2016) , Bidirectional Encoder Representations for Transformers BERT  (Devlin et al., 2018) . We evaluated our dataset based on the precision, recall and F-score of these baselines. We used sklearn 5 , the micro average is calculated globally by counting the total true positives, false negative and false positive. The macro average compute the metric independently for each class and then take the unweighted mean. The macro average does not take imbalance into account. A weighted average calculated for each label like macro, and find their average 5 https://scikit-learn.org/ weighted by support. For our test, there are 2,075 positive examples, 424 negative, 173 neutral, 377 mixed feelings, and 100 non-Malayalam examples. This is a variation of macro to include label imbalance. This may cause F-score not be between precision and recall. We combined fastText  (Bojanowski et al., 2017b ) and word2vec  (Mikolov et al., 2013a)  in DME and CDME baselines. The fastText and word2vec were trained on our codeswitched dataset. In DME, we are combining the mentioned embeddings by doing a weighted sum. On the otherhand CDME is using self-attention based mechanism on top of DME to make the embeddings context-dependent. 1DConv makes use of a 1D convolution filter to represent each word with the context of the neighbouring word in the range of the kernel. In this convolutional neural network (CNN)  (Kalchbrenner et al., 2014 ) approach, we are trying to capture the standout features from the text. BERT makes use of encoder-decoder architecture with an attention mechanism which increases the flexibility to read sequence both (left to right and vice versa) ways. From the results shown in the Table  4 , all the machine learning algorithms succeed in classifying all the classes except SVM. A recall of 1.00 and precision around 0.13 for non-Malayalam class shows that all the classes have been labelled as non-Malayalam irrespectively. Other than SVM, LR, DT, RF shows considerable macro average score for precision, recall and F1-score. However, MNB and KNN achieve higher macro-averaged precision at the expense of the lower recall values. As mentioned earlier, deep learning models are using pre-trained embeddings. The use of fastText in combination with word2vec for DME and CDME gives both local as well as global context. 1DConv shows the better macro-averaged score in precision, recall and F1-score, BERT, on the other hand, fails to identify "Mixed feeling" class. However, DME and CDME succeed in identifying all the classes. 

 Conclusion In this paper, we have presented the Malayalam-English corpus a code-mixed corpus of YouTube comments annotated for sentiment analysis. This annotation project aims to allow researches to enable research on code-mixed sentiment analysis, as well as provide useful data for codemixed research. We also provide an inter-annotator agreement score in terms of Kripendorff's alpha and baseline results, as well as making the corpus available to the research community.  
