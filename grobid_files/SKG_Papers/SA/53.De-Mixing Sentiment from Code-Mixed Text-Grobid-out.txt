title
De-Mixing Sentiment from Code-Mixed Text

abstract
Code-mixing is the phenomenon of mixing the vocabulary and syntax of multiple languages in the same sentence. It is an increasingly common occurrence in today's multilingual society and poses a big challenge when encountered in different downstream tasks. In this paper, we present a hybrid architecture for the task of Sentiment Analysis of English-Hindi code-mixed data. Our method consists of three components, each seeking to alleviate different issues. We first generate subword level representations for the sentences using a CNN architecture. The generated representations are used as inputs to a Dual Encoder Network which consists of two different BiLSTMs -the Collective and Specific Encoder. The Collective Encoder captures the overall sentiment of the sentence, while the Specific Encoder utilizes an attention mechanism in order to focus on individual sentiment-bearing sub-words. This, combined with a Feature Network consisting of orthographic features and specially trained word embeddings, achieves state-of-the-art results -83.54% accuracy and 0.827 F1 score -on a benchmark dataset.

Introduction Sentiment Analysis (SA) is crucial in tasks like user modeling, curating online trends, running political campaigns and opinion mining. The majority of this information comes from social media websites such as Facebook and Twitter. A large number of Indian users on such websites can speak both English and Hindi with bilingual proficiency. Consequently, English-Hindi code-mixed content has become ubiquitous on the Internet, creating the need to process this form of natural language. Code-mixing is defined as "the embedding of linguistic units such as phrases, words and mor- * Equal Contribution phemes of one language into an utterance of another language"  (Myers-Scotton, 1993) . Typically, a code-mixed sentence retains the underlying grammar and script of one of the languages it is comprised of. Due to the lack of a formal grammar for a codemixed hybrid language, traditional approaches don't work well on such data. The spelling variations of the same words according to different writers and scripts increase the issues faced by traditional SA systems. To alleviate this issue, we introduce the first component of our model -it uses CNNs to generate subword representations to provide increased robustness to informal peculiarities of code-mixed data. These representations are learned over the code-mixed corpus. Now, let's consider the expression: x but y, where x and y are phrases holding opposite sentiments. However, the overall expression has a positive sentiment. Standard LSTMs work well in classifying the individual phrases but fail to effectively combine individual emotions in such compound sentences  (Socher et al., 2013) . To address this issue, we introduce a second component into our model which we call the Dual Encoder Network. This network consists of two parallel BiL-STMs, which we call the Collective and Specific Encoder. The Collective Encoder takes note of the overall sentiment of the sentence and hence, works well on phrases individually. On the other hand, the Specific Encoder utilizes an attention mechanism which focuses on individual sentiment bearing units. This helps in choosing the correct sentiment when presented with a mixture of sentiments, as in the expression above. Additionally, we introduce a novel component, which we call the Feature Network. It uses surface features as well as monolingual sentence vector representations. This helps our entire system work well, even when presented with a lower amount of training examples as compared to established approaches. We perform extensive experiments to evaluate the effectiveness of this system in comparison to previously proposed approaches and find that our method outperforms all of them for English-Hindi code-mixed sentiment analysis, reporting an accuracy of 83.54% and F1 score of 0.827. An ablation of the model also shows the effectiveness of the individual components. 

 Related Work Mohammad et al. (  2013 ) employ surface-form and semantic features to detect sentiments in tweets and text messages using SVMs. Keeping in mind a lack of computational resources,  Giatsoglou et al. (2017)  came up with a hybrid framework to exploit lexicons (polarized and emotional words) as well as different word embedding approaches in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements  (Araque et al., 2017) . Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted  . Shallow parsing of code-mixed data curated from social media has also been tried  (Sharma et al., 2016) . Work has also been done to support word level identification of languages in code-mixed text  (Chittaranjan et al., 2014) .  Sharma et al. (2015)  tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data.  Pravalika et al. (2017)  used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level  (Joshi et al., 2016) , or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentiment space  (Choudhary et al., 2018) . 

 Model Architecture An overview of this architecture can be found in Figure  3 . Our approach is built on three components. The first generates sub-word level representations for words using Convolutional Neural Networks (CNNs). This produces units that are more atomic than words, which serve as inputs for a sequential model. In Section 3.2, we describe our second component, a dual encoder network made of two Bidirectional Long Short Term Memory (BiLSTM) Networks that: (1) captures the overall sentiment information of a sentence, and (2) selects the more important sentiment-bearing parts of the sentence in a differential manner. Finally, we introduce a Feature Network, in Section 3.3, built on surface features and a monolingual vector representation of the sentence. It augments our base neural network to boost classification accuracy for the task. 

 Generating Subword Representations Word embeddings are now commonplace but are generally trained for one language. They are not ideal for code-mixed data given the transcription of one script into another, and spelling variations in social media data. As a single character does not inherently provide any semantic information that can be used for our purpose, we dismiss characterlevel feature representations as a possible choice of embeddings. Keeping in mind the fact that code-mixed data has innumerable inconsistencies, we use characters to generate subword embeddings  (Joshi et al., 2016) . This increases the robustness of the model, which is important for noisy social media data. Intermediate sub-word feature representations are learned by filters during the convolution operation.   Let S be the representation of the sentence. We generate the required embedding by passing the characters of a sentence individually into 3 layer 1-D CNN. We perform a convolution of S with a filter F of length m, before adding a bias b and applying a non-linear activation function g. Each such operation generates a sub-word level feature map f sw . 

 Sub-word Representations 

 Max Pooling 

 Sub-word Level Convolutions f sw [i] = g((S[:, i : i + m ? 1] * F ) + b) (1) Here, S[:, i : i + m ? 1] is a matrix of (i) th to (i + m ? 1) th character embeddings and g is the ReLU activation function. Thereafter, a final maxpooling operation over p feature maps generates a representation for individual subwords: sw i = max(f sw [p * (i : i + p ? 1)]) (2) A graphical representation of the architecture can be seen in Figure  1  3 

 .2 Dual Encoder We utilize a combination of two different encoders in our model. 

 Collective Encoder The collective encoder, built over a BiLSTM, aims to learn a representation for the overall sentiment of the sentence. A graphical representation of this encoder is in Figure  2(A) . It takes as input the subword representation of the sentence. The last hidden state of the BiLSTM i.e. h t , encapsulates the overall summary of the sentence's sentiment, which is denoted by cmr c . 

 Specific Encoder The specific encoder is similar to the collective encoder, in that it takes as input a subword representation of the sentence and is built over LSTMs, with one caveat -an affixed attention mechanism. This allows for selection of subwords which contribute the most towards the sentiment of the input text. It can be seen in Figure  2(B) . Identifying which subwords play a significant role in deciding the sentiment is crucial. The specific encoder generates a context vector cmr s to this end. We first concatenate the forward and backward states to obtain a combined annotation (k 1 , k 2 , . . . , k t ). Taking inspiration from  (Yang et al., 2016) , we quantify the significance of a subword by measuring the similarity of an additional hidden representation u i of each sub-word against a sub-word level context vector X. Then, a normalized significance weight ? i is obtained. u i = tanh(W i k i + b i ) (3) ? i = exp(u T i X) exp(u T i X) (4) The context vector X can be looked at as a highlevel representation of the question "is it a significant sentiment-bearing unit" evaluated across the sub-words. It is initialized randomly and learned jointly during training. Finally, we calculate a vector cmr s as a weighted sum of the sub-word annotations. cmr s = ? i k i (5) Using such a mechanism helps our model to adaptively select the more important sub-words from the less important ones. 

 Fusion of the Encoders We concatenate the outputs obtained from both these encoders and use it as inputs to further fully connected layers. Information obtained from both the encoders is utilized to come up with a unified representation of sentiment present in a sentence, cmr sent = [cmr c ; cmr s ] (6) where cmr sent represents the unified representation of the sentiment. A representation of the same can be found in Figure  3 . 

 Feature Network We also use linguistic features to augment the neural network framework of our model. 

 ? Capital words: Number of words that have only capital letters ? Extended words: Number of words with multiple contiguous repeating characters. ? Aggregate positive and negative sentiments: Using SentiWordNet  (Esuli and Sebastiani, 2006)  for every word bar articles and conjunctions, and combining the sentiment polarity values into net positive aggregate and net negative aggregate features. ? Repeated exclamations and other punctuation: Number of sets of two or more contiguous punctuation. ? Exclamation at end of sentence: Boolean value. ? Monolingual Sentence Vectors: Bojanowski et al. (  2017 )'s method is used to train word vectors for Hindi words in the code-mixed sentences. sw 1 sw 2 sw 3 sw t ? h t cmr c = h t ? h 1 ? h 2 ? h 3 

 Sub-word Representations BiLSTM Layer sw 1 sw 2 sw 3 sw t cmr s = t ? i = 1 ? i k i ? k 1 

 Sub-word Representations BiLSTM Layer    (Wang and Manning, 2012)  62.5% 0.5375 MNB (Uni+Bigram)  (Wang and Manning, 2012)  66.36% 0.6046 MNB (Tf-Idf)  (Wang and Manning, 2012)  63.53% 0.4783 Lexicon Lookup  (Sharma et al., 2015)  51.15% 0.252 Char-LSTM  (Joshi et al., 2016)  59.8% 0.511 Subword-LSTM  (Joshi et al., 2016)  69.7% 0.658 FastText  46.39% 0.505 SACMT  (Choudhary et al., 2018)  77.3% 0.759 CMSA (Proposed) 83.54% 0.827  

 Representation of the Specific Sentiment of the Sentence ? h t ? h 1 ? h 2 ? h 3 

 Representation of the Collective Sentiment of the Sentence ? k 2 ? k 3 ? k t ? k 1 ? k 2 ? k 3 ? k t 

 CMSA The entire model is shown in Figure  3 . Sub-word representations are fed into both the specific and the collective encoder. The outputs of the encoders are concatenated with each other, and further with the result of the Feature Network. Subsequently, these are passed through multiple fully connected layers to make the prediction. This architecture allows us to capture sentiment on the morphemic, syntactic and semantic levels simultaneously and learn which parts of a sentence provide the most value to its sentiment. This system enables us to combine the best of neural networks involving attention mechanisms with surface and semantic features that are traditionally used for sentiment analysis. 

 Experiments And Results 

 Dataset We use the dataset released by  (Joshi et al., 2016) . It is made up of 3879 code-mixed English-Hindi sentences which were collected from public Facebook pages popular in India. 

 Baselines We compare our approach with the following: ? SVM  (Pang and Lee, 2008) : Uses SVMs with ngrams as features. ? NBSVM  (Wang and Manning, 2012) : Uses Naive Bayes and SVM with ngrams as features. ? MNB  (Wang and Manning, 2012) : Uses Multinomial Naive Bayes with various features. ? Lexicon Lookup  (Sharma et al., 2015) : The code-mixed sentences are first transliterated from Roman to Devanagari. Thereafter, sentiment analysis is done using a lexicon based approach. ? Char-LSTM and Sub-word-LSTM  (Joshi et al., 2016) : Character and sub-word level embeddings are passed to LSTM for classification. ? FastText : Word Embeddings utilized for classification. It has the ability to handle OoV words as well. ? SACMT  (Choudhary et al., 2018) : Siamese networks are used to map code-mixed and standard sentences to a common sentiment space. 

 Results and Analysis From Table  1 , it is clear that that CMSA outperforms the state-of-the-art techniques by 6.24% in accuracy and 0.068 in F1-score. We observe that sentence vectors (FastText) alone are not suitable for this task. A possible reason for this is the presence of two different languages. There is a significant difference in accuracy between Subword-LSTM and Char-LSTM, as seen in Table  1 , which confirms our assumption about subword-level representations being better for this task as compared to character-level embeddings. Amongst the baselines, SACMT performed the best. However, mapping two sentences into a common space does not seem to be enough for sentiment classification. With a combination of differ-ent components, our proposed method is able to overcome the shortcomings of each of these baselines and achieve significant gains. We experiment with different encoders used in the dual encoder network. From Table  2 , we can see that CMSA > Collective Encoder > Specific Encoder, for both accuracy and F1 score. A combination of the two encoders provides a better sentiment classification model which validates our initial hypothesis of using two separate encoders.  From Table  3 , we see the performance trend as follows: CMSA > Dual Encoder > Feature Network. Although the feature network alone does not result in better overall performance, it is still comparable to Char-LSTM (Table  2 ). However, the combination of feature network with dual encoder helps in boosting the overall performance.  

 Effect of different Encoders 

 Effect of Different Model Components 

 Effect of Training Examples 

 Conclusion We propose a hybrid approach that combines recurrent neural networks utilizing attention mechanisms, with surface features, yielding a unified representation that can be trained to classify sentiments. We conduct extensive experiments on a real world code-mixed social media dataset, and demonstrate that our system is able to achieve an accuracy of 83.54% and an F1-score of 0.827, outperforming state-of-the-art approaches for this task. In the future, we'd like to look at varying the attention mechanism in the model, and evaluating how it performs with a larger training set. sw3 
