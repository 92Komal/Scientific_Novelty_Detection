title
Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding

abstract
Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspectbased reviews by sentiment polarity. In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples. Existing methods are either designed only for one of the sub-tasks, neglecting the benefit of coupling both, or are based on topic models that may contain overlapping concepts. We propose to first learn sentiment, aspect joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data. Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets 1 .

Introduction With the vast amount of reviews emerging on platforms like Amazon and Yelp, aspect-based sentiment analysis, which extracts opinions about certain facets of entities from text, becomes increasingly essential and benefits a wide range of downstream applications  (Bauman et al., 2017; Nguyen et al., 2015) . Aspect-based sentiment analysis contains two sub-tasks: Aspect extraction and sentiment polarity classification. The former identifies the aspect S1: Eye-pleasing with semiprivate booths, place for a date. S2: Mermaid Inn is an overall good restaurant with really good seafood. (good, ambience) (good, food) Figure  1 : Two sample restaurant reviews. Pure aspect words are in red and wavy-underlined, and general opinion words are in blue and framed in boxes. Words implying both aspects and opinions (which we define as joint topics) are underlined and in purple. covered in the review, whereas the latter decides its sentiment polarity. Various methods have been proposed for the task. Neural network models  Xu et al., 2018)  have outperformed rule-based models  (Hu and Liu, 2004; Zhuang et al., 2006) , but they require large-scale fine-grained labeled data to train, which can be difficult to obtain. Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised  (He et al., 2017; Liao et al., 2019)  or weaklysupervised setting  (Angelidis and Lapata, 2018; Karamanolakis et al., 2019) , without using any annotated documents. In this work, we study the weakly-supervised setting, where only a few keywords are provided for each aspect and sentiment. We show two sample restaurant reviews in Fig.  1  together with their expected output-aspect and sentiment labels. With a closer look at these two example reviews, we observe that S2 includes a general opinion word "good" and a pure aspect word "seafood", which are separate hints for sentiment and aspect classification respectively. S1, on the other hand, does not address the target with plain and general words, but instead use more specific words like "semi-private" and "date" which are uniquely used when people feel good about the ambience instead of other aspects. Humans can interpret these unique and fine-grained terms as hints for a joint topic of good, ambience , but this is hard for models that are solely trained for one sub-task. If a model can automatically learn the semantics of each joint topic of sentiment, aspect , it will be able to identify representative terms of the joint topics such as "semi-private" which provide information for aspect and sentiment simultaneously, and will consequently benefit both aspect extraction and sentiment classification. Therefore, leveraging more fine-grained information by coupling the two subtasks will enhance both. Several LDA-based methods consider learning joint topics  (Zhao et al., 2010; Wang et al., 2015; Xu et al., 2012) , but they rely on external resources such as part-of-speech (POS) tagging or opinion word lexicons. A recent LDA-based model  (Garc?a-Pablos et al., 2018)  uses pre-trained word embedding to bias the prior in topic models to jointly model aspect words and opinion words. Though working fairly well, topic models are generative models and do not enforce topic distinctivenesstopic-word distribution can largely overlap among different topics, allowing topics to resemble each other. Besides, topic models yield unstable results, causing large variance in classification results. We propose the JASen model for Joint Aspect-Sentiment Topic Embedding. Our general idea is to learn a joint topic representation for each sentiment, aspect pair in the shared embedding space with words so that the surrounding words of topic embeddings nicely describe the semantics of a joint topic. This is accomplished by training topic embeddings and word embeddings on in-domain corpora and modeling the joint distribution of usergiven keywords on all the joint topics. After learning the joint topic vectors, embedding-based predictions can be derived for any unlabeled review. However, these predictions are sub-optimal for sentiment analysis where word order plays an important role. To leverage the expressive power of neural models, we distill the knowledge from embeddingbased predictions to convolutional neural networks (CNNs)  (Krizhevsky et al., 2012)  which perform compositional operations upon local sequences. A self-training process is then conducted to refine CNNs by using their high-confident predictions on unlabeled data. We demonstrate the effectiveness of JASen by conducting experiments on two benchmark datasets and show that our model outperforms all the baseline methods by a large margin. We also show that our model is able to describe joint topics with coherent term clusters. Our contributions can be summarized as follows: (1) We propose a weakly-supervised method JASen to enhance two sub-tasks of aspect-based sentiment analysis. Our method does not need any annotated data but only a few keywords for each aspect/sentiment. (2) We introduce an embedding learning objective that is able to capture the semantics of fine-grained joint topics of sentiment, aspect in the word embedding space. The embedding-based prediction is effectively leveraged by neural models to generalize on unlabeled data via self-training. (3) We demonstrate that JASen generates high-quality joint topics and outperforms baselines significantly on two benchmark datasets. 

 Related Work The problem of aspect-based sentiment analysis can be decomposed into two sub-tasks: aspect extraction and sentiment polarity classification. Most previous studies deal with them individually. There are various related efforts on aspect extraction  (He et al., 2017) , which can be followed by sentiment classification models  (He et al., 2018) . Other methods  (Garc?a-Pablos et al., 2018)  jointly solve these two sub-tasks by first separating target words from opinion words and then learning joint topic distributions over words. Below we first review relevant work on aspect extraction (Sec 2.1) and then turn to studies that jointly extract aspects and sentiment polarity (Sec 2.2). 

 Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules  (Hu and Liu, 2004; Zhuang et al., 2006) , which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering. While CNN  (Xu et al., 2018)  and RNN  based models have shown the powerful expressiveness of neural models, they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck. Various unsupervised approaches are proposed to model different aspects automatically. LDAbased methods  (Brody and Elhadad, 2010; Chen et al., 2014)  model each document as a mixture of aspects (topics) and output a word distribution for each aspect. Recently, neural models have shown to extract more coherent topics. ABAE  (He et al., 2017)  uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms. CAt  (Tulkens and van Cranenburgh, 2020)  introduces a single head attention calculated by a Radial Basis Function (RBF) kernel to be the sentence summary. The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often do not well align with user's interested aspects, and additional human effort is needed to map topics to certain aspects, not to mention some topics are irrelevant of interested aspects. Several weakly-supervised methods address this problem by using a few keywords per aspect as supervision to guide the learning process. MATE (Angelidis and Lapata, 2018) extends ABAE by initializing aspect embedding using weighted average of keyword embeddings from each aspect. ISWD  (Karamanolakis et al., 2019)  co-trains a bagof-word classifier and an embedding-based neural classifier to generalize the keyword supervision. Other text classification methods leverage pre-trained language model  (Meng et al., 2020b)  to learn the semantics of label names or metadata  to propagate document labels. The above methods do not take aspect-specific opinion words into consideration. The semantic meaning captured by a sentiment, aspect joint topic preserves more fine-grained information to imply the aspect of a sentence and thus can be used to improve the performance of aspect extraction. 

 Joint Extraction of Aspect and Sentiment Most previous studies that jointly perform aspect and sentiment extraction are LDA-based methods.  Zhao et al. (2010)  include aspect-specific opinion models along with aspect models in the generative process.  Wang et al. (2015)  propose a restricted Boltzmann machine-based model that treats aspect and sentiment as heterogeneous hidden units.  Xu et al. (2012)  adapt LDA by introducing sentimentrelated variables and integrating sentiment prior knowledge. All these methods rely on external resources such as part-of-speech (POS) tagging or opinion word lexicons. A more recent study that shares similar weakly-supervised setting with ours is W2VLDA  (Garc?a-Pablos et al., 2018) . They apply Brown clustering  (Brown et al., 1992)  to separate aspect-terms from opinion-terms and construct biased hyperparameters ? and ? by embedding similarity. Despite the effectiveness of topic models, they suffer from the drawback of not imposing discriminative constraints among topics-topic-word distribution can largely overlap among different topics, allowing redundant topics to appear and making it hard to classify them. We empirically show the advances of our method by capturing discriminative joint topic representations in the embedding space. 

 Problem Definition Our weakly-supervised aspect-based sentiment analysis task is defined as follows. The input is a training corpus D = {d 1 , d 2 , . . . , d |D| } of text reviews from a certain domain (e.g., restaurant or laptop) without any label for aspects or sentiment. A list of keywords l a for each aspect topic (denoted as a ? A) and l s for each sentiment polarity (denoted as s ? S) are provided by users as guidance. For each unseen review in the same domain, our model outputs a set of s, a labels. 

 Model Figure  2  shows the workflow of JASen. Our goal is to generate a set of sentiment, aspect predictions for each review. We first learn an embedding space to explicitly represent the semantics of the topics (including both pure aspect/sentiment and joint sentiment, aspect ones) as embedding vectors, which are surrounded by the embeddings of the representative words of the topics. We also impose discriminative regularization on the embedding space to push different topics apart. To model the local sequential information which is crucial for sentiment analysis, we use CNN as the classifier by pre-training it on pseudo labels given by the cosine similarity between document embeddings and topic embeddings, and self-training it on unlabeled data to iteratively refine its parameters. Below we introduce the details of JASen. 

 Joint-Topic Representation Learning We learn the representations of words and topics on the in-domain corpus by following two principles: (1) distributional hypothesis  (Sahlgren, 2008)  and (2) topic distinctiveness. The first principle is achieved by an adaptation of the Skip-Gram model  (Mikolov et al., 2013)    Mermaid Inn is an overall good restaurant with really good seafood. 

 d: < l a t e x i t s h a 1 _ b a s e 6 4 = "  6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t = < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > food good, food bad, food = < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > Topic Distribution P (w j |w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > P (d|w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > P (t|w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > objectives. Fig.  3  provides the overview of our embedding learning objectives with an example. Modeling Local and Global Contexts. We learn word embeddings based on the assumption that words with similar contexts have similar meanings, and define contexts to be a combination of location contexts  (Mikolov et al., 2013)  and global contexts  (Meng et al., 2019; Liao et al., 2019; Meng et al., 2020a) . The local context of a word w i refers to other words whose distances are h words or less from w i . To maximize the probability of seeing the local context of a word w i , we use the following objective: L l = ? w i 0<|j?i|?h log P (w j |w i ), (1) where P (w j |w i ) ? exp(v j u i ), and u i , v j are the center and context word embeddings. The global context  (Meng et al., 2019; Liao et al., 2019)  of a word w i refers to the document d where a word appears, based on the motivation that similar documents contain similar-meaning words. We use the following objective for global context: L g = ? d?D w i ?d log P (d|w i ), (2) where P (d|w i ) ? exp(d u i ). Regularizing Pure Aspect/Sentiment Topics. To endow the embedding space with discriminative power over the aspect/sentiment categories for better classification performance, we regularize the aspect topic embeddings t a and sentiment topic embeddings t s so that different topics are pushed apart. For example, the word "good" in Fig.  3  is a keyword for the sentiment topic good, and we aim to place t good close to the word embedding of "good" in the embedding space while away from other topic embeddings (i.e., t bad ). To achieve this, we maximize the probability of using each topic keyword to predict its representing topic: L A reg = ? a?A w i ?la log P (t a |w i ), (3) L S reg = ? s?S w i ?ls log P (t s |w i ), (4) where l a , l s are the keyword lists for aspect a and sentiment s, respectively; P (t|w i ) ? exp(u i t). Eqs. (  3 ) and (4) empower the embedding space for classification purpose, that is, words can be "classified" into topics based on embedding similarity. For good initializations of t a and t s , we use the average word embedding of user-provided keywords for each aspect and sentiment. Regularizing Joint Sentiment, Aspect Topics. Now we examine the joint case, where |S|?|A| topics are regularized. We connect the learning of joint topic embeddings with pure aspect/sentiment topics by exploring the relationship between marginal distribution and joint distribution: P (t a |w i ) = s?S P t s,a w i , (5) P (t s |w i ) = a?A P t s,a w i . (6) As an example, Fig.  3  shows that the marginal probability of the keyword "good" belonging to the sentiment topic "good" is equal to the probability sum of it belonging to good, food , good, ambience and good, service . The objective for regularizing joint topics L joint can be derived by replacing P (t a |w i ) in Eq. (  3 ) with Eq. (  5 ) and P (t s |w i ) in Eq. (  4 ) with Eq. (  6 ). We also notice that general opinion words such as "good" (or pure aspect words such as "seafood") are equally irrelevant to the aspect (or sentiment) dimension, so we use a uniform distribution U to regularize their distribution over all the classes on the irrelevant dimension: L A cross = s?S w i ?ls KL (U, P (t a |w i )) , (7) L S cross = a?A w i ?la KL (U, P (t s |w i )) . (8) Putting the above objectives altogether, our final embedding learning objective is: L = L l +? g L g +? r (L reg +L joint +L cross ), (9) where L reg = L A reg +L S reg , and the same for L joint and L cross . For all the regularization terms, we treat them equally by using the same weight ? r , which shows to be effective in practice. 

 Training CNNs for Classification Word ordering information is crucial for sentiment analysis. For example, "Any movie is better than this one" and "this one is better than any movie" convey opposite sentiment polarities but have the exactly same words. The trained embedding space mainly captures word-level discriminative signals but is insufficient to model such sequential information. Therefore, we propose to train convolutional neural networks (CNNs) to generalize knowledge from the preliminary predictions given by the embedding space. Specifically, we first pre-train CNNs with soft predictions given by the cosine similarity between document embeddings and topic embeddings, and then adopt a self-training strategy to further refine the CNNs using their highconfident predictions on unlabeled documents. Neural Model Pre-training. For each unlabeled review, we can (1) derive one distribution over the joint topics by calculating the cosine similarity between the document representation d and t s,a , (2) derive separate distributions over sentiment and aspect variables using cosine similarity with marginal topics t a and t s , or (3) combine (  1 ) and (  2 ) by adding the two sets of cosine scores. We find empirically that the last method achieves the best result, i.e., the distribution of a test review d over the aspect and sentiment categories is computed as: P (a|d) ? exp T ? cos(ta, d) + s?S cos(t s,a , d) |S| , (10) P (s|d) ? exp T ? cos(ts, d) + a?A cos(t s,a , d) |A| , (11) where d is obtained by averaging the word embeddings in d, and T is the temperature to control how greedy we want to learn from the embedding-based prediction. We train two CNN models separately for aspect and sentiment classification by learning from the two distributions in Eqs. (  10 ) and (  11 ). We leverage the knowledge distillation objective  (Hinton et al., 2015)  to minimize the cross entropy between the embedding-based prediction p d and the output prediction q d of the CNNs: H(p d , q d ) = ? t P (t|d) log Q(t|d). (12) Neural Model Refinement. The pre-trained CNNs only copy the knowledge from the embedding space. To generalize their current knowledge to the unlabeled corpus, we adopt a self-training technique to bootstrap the CNNs. The idea of self-training is to use the model's high-confident predictions on unlabeled samples to refine itself. Specifically, we compute a target score  (Xie et al., 2016)  for each unlabeled document based on the predictions of the current model by enhancing highconfident predictions via a squaring operation: target(P (a|d)) = P (a|d) 2 /f a a ?A P (a |d) 2 /f a , where f a = d?D P (a|d). Since self-training updates the target scores at each epoch, the model is gradually refined by its most recent high-confident predictions. The self-training process is terminated when no more samples change label assignments after the target scores are updated. The resulting model can be used to classify any unseen reviews. 

 Evaluation We conduct a series of quantitative and qualitative evaluation on benchmark datasets to demonstrate the effectiveness of our model.   

 Experimental Setup Datasets: The following two datasets are used for evaluation: ? Restaurant: For in-domain training corpus, we collect 17,027 unlabeled reviews from Yelp Dataset Challenge 2 . For evaluation, we use the benchmark dataset in the restaurant domain in SemEval-2016  (Pontiki et al., 2016)  and SemEval-2015  (Pontiki et al., 2015) , where each sentence is labeled with aspect and sentiment polarity. We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem (otherwise a set of keywords can be added to describe it). ? Laptop: We leverage 14,683 unlabeled Amazon reviews under the laptop category collected by  (He and McAuley, 2016)  as in-domain training corpus. We also use the benchmark dataset in the laptop domain in SemEval-2016 and SemEval-2015 for evaluation. Detailed statistics of both datasets are listed in Table  1 , and the aspects along with their keywords are in Table  2 . Preprocessing and Hyperparameter Setting. To preprocess the training corpus D, we use the word tokenizer provided by NLTK 3 . We also use a phrase mining tool, AutoPhrase  (Shang et al., 2017) , to extract meaningful phrases such as "great wine" and "numeric keypad" such that they can capture complicated semantics in a single text unit. We use the benchmark validation set to fine-tune the hyperparameters: embedding dimension = 100, local context window size h = 5, ? g = 2.5, ?  

 Quantitative Evaluation We conduct quantitative evaluation on both aspect extraction and sentiment polarity classification. Compared Methods. Our model is compared with several previous studies. A few of them are specifically designed for aspect extraction but do not perform well on sentiment classification. So we only report their results on aspect extraction. For fair comparison, we use the same training corpus and test set for each baseline method. For weaklysupervised methods, they are fed with the same keyword list as ours. ? CosSim: The topic representation is averaged by the embedding of seed words trained by Word2Vec on training corpus. Cosine similarity is computed between a test sample and the topics to classify the sentence. ? ABAE  (He et al., 2017) : An attention-based model to unsupervisedly extract aspects. An autoencoder is trained to reconstruct sentences through aspect embeddings. The learned topics need to be manually mapped to aspects. ? CAt (Tulkens and van Cranenburgh, 2020): A recent method for unsupervised aspect extraction. A single head attention is calculated by a Radio Basis Function kernel to be the sentence summary. ? W2VLDA (Garc?a-Pablos et al., 2018): A stateof-the-art topic modeling based method that leverages keywords for each aspect/sentiment to jointly do aspect/sentiment classification. ? BERT  (Devlin et al., 2019) : A recent proposed deep language model. We utilize the pre-trained BERT (12-layer, 768 dimension, uncased) and implement a simple weakly-supervised method that fine-tunes the model by providing pseudo    

 Effect of Number of Keywords We study the effect of the number of keywords. In Figure  4  we show the macro-F1 score of aspect extraction using different number of keywords for each aspect on Laptop dataset. The trend clearly shows the model performance increases when more keywords are provided. Moreover, when only one keyword is provided (only the label name), JASen still has a stable performance and a large gap over the ablation without learning joint topic embedding, implying that learning joint topic semantics is especially powerful in low resource setting.  

 Joint Topic Representation Visualization To understand how the joint topics are distributed in the embedding space, along with the aspect and sentiment topics, we use PCA  (Jolliffe, 2011)  for dimension reduction to visualize topic embedding trained on the Restaurant corpus in Fig.  5 . An interesting observation is that, some aspect topics (e.g., ambience) lie approximately in the middle of their joint topics ("good, ambience" and "bad, ambience"), showing that our embedding learning objective understands the joint topics as decomposition of their "marginal" topics, which fits with our goal to learn fine-grained topics. 

 Case Studies We list several test samples along with their ground truth and model predictions in Table  6 and Table  7 . Some conflicting cases between ours and the ground truth are rather ambiguous. For example, the ground truth of the second example in Table  6  is (good, location), but we still think given that the review mentions "the outdoor atmosphere" and uses terms like "sitting on the sidewalk" and "cool evening", it is more relevant to ambience than location, as is output by our full model. The gold aspect label for the second and the third reviews in Table  7  are both "company", but apparently these two sentences are talking about two different aspects: the product itself and the service of the company. Though the output of our model, "os" and "support" for these two sentences may not be the most precise prediction, at least our model treats them as two different aspects. 

 Conclusion In this paper we propose to enhance weaklysupervised aspect-based sentiment analysis by learning the representation of sentiment, aspect joint topic in the embedding space to capture more fine-grained information. We introduce an Figure 2 : 2 Figure 2: Overview of our model JASen. We first leverage the in-domain training corpus and user-given keywords to learn joint topic representation in the word embedding space. The marginal probability of keywords belonging to an aspect/sentiment can be summed up by the joint distribtution over sentiment, aspect joint topics. Embeddingbased prediction on unlabeled data are then leveraged by neural models for pre-training and self-training. 
