title
Combining Sentiment Lexica with a Multi-View Variational Autoencoder

abstract
When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.

Introduction Sentiment lexica provide an easy way to automatically label texts with polarity values, and are also frequently transformed into features for supervised models, including neural networks  (Palogiannidi et al., 2016; Ma et al., 2018) . Indeed, given their utility, a veritable cottage industry has emerged focusing on the design of sentiment lexica. In practice, using any single lexicon, unless specifically and carefully designed for the particular domain of interest, has several downsides. For example, any lexicon will typically have low coverage compared to the language's entire vocabulary, and may have misspecified labels for the domain. In many cases, it may therefore be desirable to combine multiple sentiment lexica into a single representation. Indeed, some research on unifying such lexica has emerged  (Emerson and Declerck, 2014; Altrabsheh et al., 2017) , borrowing ideas from crowdsourcing  (Raykar et al., 2010; Hovy et al., 2013) . However, this is a non-trivial task, because lexica can use binary, categorical, or continuous scales to quantify polarity-in addition to different interpretations for each-and thus cannot easily be combined. In Fig.  1 , we show an example of the same word labeled using different lexica to illustrate the nature of the challenge. To combine sentiment lexica with disparate scales, we introduce SentiVAE, a novel multiview variant of the variational autoencoder (VAE)  (Kingma and Welling, 2014) . SentiVAE, visualized as a graphical model in Fig.  2 , differs from the original VAE in two ways: (i) it uses a Dirichlet latent variable (rather than a Gaussian) for each word in the combined vocabulary, and (ii) it has multiple emission distributions-one for each lexicon. Because the latent variables are shared across the lex- ica, we are able to derive a common latent representation of the words' polarities. The resulting model is spiritually related to a multi-view learning approach  (Sun, 2013) , where each view corresponds to a different lexicon. Experimentally, we use SentiVAE to combine six commonly used Englishlanguage sentiment lexica with disparate scales. We evaluate the resulting representation via a text classification task involving nine Englishlanguage sentiment analysis datasets. For each dataset, we transform each text into an average polarity value using either our representation, one of the six commonly used sentiment lexica, or a straightforward combination thereof. We then train a classifier to predict the overall sentiment of each text from its average polarity value. We find that our representation outperforms the individual lexica, as well as the straightforward combination for some datasets. Our representation is particularly efficacious for datasets from domains that are not well-supported by standard sentiment lexica.  1  The existing research that is most closely related to our work is SentiMerge  (Emerson and Declerck, 2014) , a Bayesian approach for aligning sentiment lexica with different continuous scales. SentiMerge consists of two steps: (i) aligning the lexica via rescaling, and (ii) combining the rescaled lexica using a Gaussian distribution. The authors perform token-level evaluation using a single sentiment analysis dataset where each token is labeled with its contextually dependent sentiment. Because SentiMerge can only combine lexica with continuous scales, we do not include it in our evaluation. 

 Sentiment Lexica and Scales We use the following commonly used Englishlanguage sentiment lexica: SentiWordNet  (Baccianella et al., 2010) , MPQA  (Wilson et al., 2005) , SenticNet 5  (Cambria et al., 2014) , Hu-Liu (Hu and 1 Our representation and code are available at https:// github.com/ahoho/SentiVAE.  Liu, 2004) , GI  (Stone et al., 1962) , and VADER  (Hutto and Gilbert, 2014) . Descriptive statistics for each lexicon are shown in Tab. 1. Each word in SentiWordNet is labeled with two real values, each in the interval [0, 1], corresponding to the strength of positive and negative sentiment (e.g., the label (0 0) is neutral, while the label (1 0) is maximally positive). Each word in VADER is labeled by ten different human evaluators, with each evaluator providing a polarity value on a nine-point scale (where the midpoint is neutral), yielding a 10-dimensional label. MPQA, Hu-Liu, and GI all use binary scales. Lastly, each word in SenticNet is labeled with a real value in the interval [?1, 1], where 0 is neutral. 

 SentiVAE We first describe a figurative generative process for a single sentiment lexicon d ? D, where D is a set of sentiment lexica. Imagine there is a true (latent) polarity value z w associated with each word w in the lexicon's vocabulary. When the lexicon's creator labels that word according to their chosen scale (e.g., thumbs-up or thumbs-down, a real value in the interval [0, 1]), they deterministically transform this true value to their chosen scale via a function f ( ? ; ? d ). 2 Sometimes, noise is introduced during this labeling process, corrupting the label as it leaves the ethereal realm and producing the (observed) polarity label x w d . They then add this potentially noisy label to the lexicon. Given a lexicon of observed polarity labels, the latent polarity values can be inferred using a VAE. The original VAE posits a generative model of observed data X and latent variables Z: P (X , Z) = P (X | Z) P (Z). Inference of Z then proceeds by approximating the (intractable) posterior P (Z | X ) with a Gaussian distribution, factorized over the individual latent variables. A parameterized encoder function compresses X into Z, while a parameterized decoder function reconstructs X from Z. SentiVAE extends the original VAE model to combine multiple lexica with disparate scales, producing a common latent representation of the polarity value for each word in the combined vocabulary. Generative process. Given a set of sentiment lexica D with a combined vocabulary W, Senti-VAE posits a common latent representation z w of the polarity value for each word w ? W, where z w is a three-dimensional categorical distribution over the sentiments positive, negative, and neutral. ? w z w ? w d ? d x w d w ? W d ? D The generative process starts by drawing each latent polarity value z w from a three-dimensional Dirichlet prior, parameterized by ? w = (1, 1, 1): z w ? Dir(? w ). (1) If the word is uncontroversial, 3 we spur this prior somewhat using the number of lexica in which the word appears c(w). Specifically, we add c(w) to the parameter for the sentiment associated with that word in the lexica, e.g., ? SUPERB = (1 + c(SUPERB), 1, 1). This has the effect of regularizing the inferred latent polarity value toward the desired distribution over sentiments. Having generated z w , the process proceeds by "decoding" z w into each lexicon's chosen scale. First, for each lexicon d ? D, z w is deterministically transformed via neural network f ( ? ; ? d ) with a single 32-dimensional hidden layer, parameterized by lexicon-specific weights ? d : ? w d = f (z w ; ? d ). (2) The value ? w d is then used to generate the (observed) polarity label x w d for that lexicon: x w d ? P d (x w d | ? w d ). (3) The dimensionality of ? w d and the emission distribution P d are lexicon-specific. For SentiWordNet, P d 3 We say that a word is uncontroversial if there is strong agreement across the sentiment lexica in which it appears. Even without this spurring, the inferred latent representation typically separates into the three sentiment classes, but performance on our text classification task is somewhat diminished. 

 Dataset Source Inference. Inference involves forming the posterior distribution over the latent polarity values Z given the observed polarity labels X . Because computing the normalizing constant P (X ) is intractable, we instead approximate the posterior with a family of distributions Q ? (Z), indexed by variational parameters ?. Specifically, we use Q ? (Z) = w?W Q ? w (z w ) = w?W Dir(? w ). (4) To construct ? w , we first define a neural network g(?; ? d ), with a single 32-dimensional hidden layer, which "encodes" x w d into a threedimensional vector. The output of this neural network is then transformed via a softmax as follows: ? w d = softmax g(x w d ; ? d ) (5) ? w = 1 + d?D ? w d . (6) The intuition behind ? w can be understood by appealing to the "pseudocount" interpretation of Dirichlet parameters. Each lexicon contributes exactly one pseudocount, divided among positive, negative, and neutral, to what would otherwise be a symmetric, uniform Dirichlet distribution. As a consequence of this construction, words that appear in more lexica will have more concentrated Dirichlets. Intuitively, this property is appealing. We optimize the resulting ELBO objective  (Blei et al., 2017)  with respect to the variational parameters via stochastic variational inference (Hoffman in the Pyro framework  (Bingham et al., 2018) . The standard reparameterization trick used in the original VAE does not apply to models with Dirichletdistributed latent variables, so we use the generalized reparameterization trick of  Ruiz et al. (2016) . 

 Experiments and Results To evaluate our approach, we first use SentiVAE to combine the six lexica described in ?2. For each word w in the combined vocabulary, we obtain an estimate of z w by taking the mean of Q ? w (z w ) = Dir(? w )-i.e., by normalizing ? w . We compare this representation to using ? w directly, because ? w contains information about Sen-tiVAE's certainty about the word's latent polarity value. We evaluate our common latent representation via a text classification task involving nine English-language sentiment analysis datasets: IMDB  (Maas et al., 2011) , Yelp  (Zhang et al., 2015) , SemEval 2017 Task 4 (SemEval, Rosenthal et al. (  2017 )), multi-domain sentiment analysis (MultiDom,  Blitzer et al. (2007) ), and PeerRead  (Kang et al., 2018)  with splits ACL 2017 and ICLR 2017  (Kang et al., 2018) . Each dataset consists of multiple texts (e.g., tweets, articles), each labeled with an overall sentiment (e.g., positive). Descriptive statistics for each dataset are shown in Tab. 2. For the datasets with more than three sentiment labels, we consider two versions-the original and a version with only three (bucketed) sentiment labels. For each dataset, we transform each text into an average polarity value using either our representation, one of the six lexica, 4 or a straightforward combination thereof, where the polarity value for  4  We bucket the upper four and lower four points of VADER's nine-point scale, to yield a three-point scale. Without this bucketing, our representation outperforms VADER on four of the nine datasets. We do not bucket VADER when using it in SentiVAE or in the straightforward combination. each word in the (combined) vocabulary is a 16dimensional vector that consists of a concatenation of polarity values. (Unlike SentiVAE, this concatenation does not yield a single sentiment lexicon that retains scale coherence, while achieving maximal coverage over words.) Specifically, we replace each token with its corresponding polarity value, and then average the these values  (Go et al., 2009; ?zdemir and Bergler, 2015; Kiritchenko et al., 2014) . We then use the training portion of the dataset to learn a logistic regression classifier to predict the overall sentiment of each text from its average polarity value. Finally, we use the testing portion to compute the accuracy of the classifier. Results. The results in Tab. 3 show that our representation using ? w outperforms the individual lexica for all but one dataset, and that our representation using the mean of Q ? w (z w ) outperforms them for six datasets. This is likely because Senti-VAE has a richer representation of sentiment than any individual lexicon, and it has greater coverage over words (see Tab. 4). The results in Tab. 5 support the former reason: even when we limit the words in our representation to match those in an individual lexicon, our representation still outperforms the individual lexicon. Unsurprisingly, our representation especially outperforms lexica with unidimensional scales. We also find that our representation outperforms the straightforward combination for datasets from domains that are not well supported by the individual lexica (see Tabs. 1 and 2 for lexicon and dataset sources, respectively). By combining lexica from different domains, our representation captures a general notion of sentiment that is not tailored to any specific domain. 

 Conclusion We introduced a generative model of sentiment lexica to combine disparate scales into a common Table  5 : Classification accuracies for a 10% validation portion of two of the datasets. The first row, labeled SentiVAE, contains the classification accuracy for our representation using ? w . Subsequent (lexicon-specific) rows compare our representation (SV), restricted to the vocabulary of that lexicon, to the lexicon itself (Lex). latent representation, and realized this model with a novel multi-view variational autoencoder, called SentiVAE. We then used SentiVAE to combine six commonly used English-language sentiment lexica with binary, categorical, and continuous scales. Via a downstream text classification task involving nine English-language sentiment analysis datasets, we found that our representation outperforms the individual lexica, as well as a straightforward combination thereof. We also found that our representation is particularly efficacious for datasets from domains that are not well-supported by standard sentiment lexica. Finally, we note that our approach is more general than SentiMerge  (Emerson and Declerck, 2014) . While SentiMerge can only combine sentiment lexica with continuous scales, SentiVAE is designed to combine lexica with disparate scales. Figure 1 : 1 Figure 1: A depiction of the "encoder" portion of Sen-tiVAE. The word peppy has polarity values of 0.65 and pos in the SenticNet and Hu-Liu lexica, respectively. These values are "encoded" into two three-dimensional vectors, which are then summed and added to (1, 1, 1) (not shown) to form the parameters of a Dirichlet over the latent representation of the word's polarity value. 
