title
Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification

abstract
Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations. To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision. Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision. Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training. Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment. Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weaklysupervised baselines, and is also comparable to the supervised method.

Introduction Document-level multi-aspect sentiment classification (DMSC) is a fine-grained sentiment analysis task, aiming to predict the sentiments of aspects in a document consisting of several sentences. In previous studies, neural models have shown to be effective for improving DMSC with the help of large amounts of aspect-level annotations  (Chen et al., 2017; Xue and Li, 2018; Chen and Qian, 2019;   A great location to stay at. The room was ordinary the bathroom looked like an after thought, the shower was extremely small. Also the front desk clerks provided minimum service. Besides, the price is very expensive. 

 Review overall:-Ratings room: -location:+ service: 1 value: 1 Figure 1: A review example with sentiment labels.  Wang et al., 2020) . Despite the advantages, the acquisition of aspect-level sentiment annotations remains a laborious and expensive endeavor. Fortunately, the overall document-level sentiment annotations are relatively easy to obtain thanks to the widespread online reviews with overall star ratings. Therefore, it is practically meaningful to perform DMSC by weak supervision from document-level sentiment signals. However, this problem is far from solved. To the best of our knowledge, there is no neural model that is able to achieve DMSC with only document-level signals. There are mainly two challenges need to be settled. First, the granularity between aspect-level sentiment and document-level sentiment is quite different. It is unclear how to properly model the relation between them, in order to transfer knowledge from document-level to aspect-level. Second, the relevant text of aspect-level is unobserved. Without any constraint, a vanilla weakly supervised model would be easy to overfit to document-level signals in terms of both sentiment and attended text, despite each aspect often has its unique relevant text and different sentiment (as shown in Figure  1 ). However in this case, no matter the given aspect is location, room, service, or value, a vanilla model would pay more attention to the words "great", "ordinary", "small", "minimum" and "expensive", and transfer the negative sentiment from documentlevel to all aspects. As a result, the sentiment towards location is wrongly learned as negative, which should be positive instead. Accordingly, we propose a diversified multiple instance learning network (D-MILN) to achieve DMSC with only document-level sentiment supervision. We novelly formularize this problem as multiple instance learning (MIL;  Keeler and Rumelhart 1991)  to model document-level sentiment as a combination of aspect-level sentiments. The aspects are regarded as instances and their sentiment distributions are predicted by an attention-based classifier, while the document is regarded as a bag and its sentiment distribution is computed as a combination of the aspect-level sentiment distributions. Thus, we provide a framework for learning aspectlevel classifier by optimizing the document-level predictions. Meanwhile, in order to avoid the overfiting to document-level signals, we further propose two kinds of diversified regularization. Diversified textual regularization is applied to guide the aspectlevel sentiment classifier to select aspect-relevant snippets. Diversified sentimental regularization is leveraged to control the variance among aspectlevel sentiments. Overall, our contributions are summarized as follows: ? We propose a novel diversified multiple instance learning neural network, which properly models the relation between aspectlevel and document-level sentiment, and thus achieves DMSC with merely document-level supervision. ? Two kinds of diversified regularization are introduced to alleviate the key challenge of overfitting document-level signals and to improve the aspect-level sentiment classification performance. ? Comprehensive experiments are conducted on the BeerAdvocate and TripAdvisor benchmark datasets. The results verify the necessity and advantages of both our framework and diversified regularizations. Meanwhile, our D-MILN outperforms previous weakly supervised methods significantly and is also comparable to the supervised method with thousands of labeled instances per aspect. 

 Related Work Document-level multi-aspect sentiment classification In previous studies, DMSC is usually done by supervised learning methods  (Lei et al., 2016; Yin et al., 2017; Wang et al., 2019) , where aspect-level annotations should be provided. However, human annotation of aspectlevel sentiment is laborious and expensive, therefore, some researches focus on weakly supervised DMSC. This approach can be further categorized into knowledge-supervised and document-level supervised methods. As for knowledge-supervised methods,  Zeng et al. (2019)  propose to use aspectopinion word pairs as knowledge for supervision. The aspect-level sentiment classification is achieved by accomplishing another relevant objective: to predict an opinion word when given an aspect. However, their model heavily depends on the performance of dependency parsing and manually designed rules. As for document-level supervised methods,  Wang et al. (2010 Wang et al. ( , 2011  propose to use the document-level sentiment as supervision which is similar to ours. Specifically, they propose a probabilistic graphical model for the task, which assumes the overall rating is generated based on a weighted sum of the latent aspect ratings. However, this non-neural network model adopts bagof-words representations which are insufficient at capturing the order of words and complex semantics. Furthermore, their model fails to consider the problem of overfitting to document-level signals. Multiple Instance Learning Multiple instance learning is a form of weakly supervised learning where instances are arranged in bags and a label is provided for the entire bag  (Keeler and Rumelhart, 1991) .  Most MIL methods (Zhou et al., 2009; Wei et al., 2014; Pappas and Popescu-Belis, 2017; Hau?mann et al., 2017; Tu et al., 2019; Ilse et al., 2018; Wang et al., 2018; Wang and Wan, 2018)  focus on the bag-level performance and there are also a few methods focusing on the instance-level performance. Apart from the loss defined on the bag level,  Kotzias et al. (2015)  also introduces a regularization based on the instance similarities into the objective function.  Peng and Zhang (2019)  assigns the bag-level label to instances under the i.i.d assumption and directly define the loss function on the instance-level label prediction. Some works propose to apply MIL to sentencelevel sentiment classification task.  Kotzias et al. (2015) ;  Angelidis and Lapata (2018a)   Wan (2018) and  Angelidis and Lapata (2018b)  propose to train the sentence-level sentiment classifier with document-level annotations. For these works, the content for each instance (i.e. words in the sentence) is already given. However, for DMSC task, the relevant text snippets for a given aspect, which are crucial for determining the sentiment, are not provided in advance. This makes the DMSC task much different and challenging to apply MIL. Besides, these works never consider the overfitting to bag-level supervision. To the best of our knowledge, this is the first work to apply MIL to DMSC task. 

 Methodology We first briefly introduce the problem we work on. Given a review, our task is to predict the sentiments of aspects in the review. Formally, we denote the review document as d which contains I words {w 1 , w 2 , ? ? ? , w I }, the sentiment label for the document as l d , and the set of J aspects mentioned in the document as timent distribution p d is computed as a weighted sum of aspect-level sentiment distributions. The diversified sentimental regularization as shown in Figure  2 (a) is applied on the aspect-level sentiment distributions to alleviate the overfitting to document-level sentiment. The diversified textual regularization as shown in Figure  2 (b) is applied on the attention weights to encourage the aspect-level classifier to select aspect-relevant snippets. {a 1 , a 2 , ? ? ? , a J }. Same as Yin et al. (2017), each aspect a j is represented by K aspect-related keywords, {a j 1 , a j 2 , ? ? ? , a j K }, 

 Aspect-level Sentiment Distribution In this section, we introduce our aspect-level attention-based sentiment classifier. Aspect encoding We first apply a one-layer MLP on the top of word embedding of each aspectrelated keyword a j k : q j k = tanh(W q e j k + b q ) (1) where e j k is the word embedding of a j k , W q and b q are parameters of the one-layer MLP. Then the final representation of aspect a j is calculated as q j = k c k q j k , where c k encodes the importance of each keyword for the given aspect: c k = exp(w c ? q j k ) k exp(w c ? q j k ) (2) and w c is the parameter to learn. Document encoding We first convert the words in the given document into a sequence of embed- ding vectors E = [e 1 , e 2 , ? ? ? , e I ]. Usually, the sentiments are expressed through phrases in the document  (Fei et al., 2004) . For example, "a lovely room" expresses a positive sentiment towards the aspect room. Since one-dimension convolutional layers can serve as linguistic feature detectors to extract specific patterns of n-grams  (Kalchbrenner et al., 2014) , we apply several one-dimension convolutional layers on top of the word embeddings and obtain the final contextual features for the input words: H = [h 1 , h 2 , ? ? ? , h I ]. Aspect-specific representations We obtain the aspect-specific representation by a weighted sum of contextual features: r a j = I i=1 ? i j h i (3) where ? i j encodes the importance of word w i to determine the sentiment towards aspect a j . ? i j is calculated through attention mechanism: ? i j = exp(q j T W a h i ) i exp(q j T W a h i ) (4) where W a is a bilinear term to capture the relevance between q j and h i . Prediction The aspect-specific representation is then used to predict the aspect-level sentiment distribution p a j by: p a j = sof tmax(W p r a j + b p ) (5) where W p and b p are parameters of the softmax layer. 

 Document-level Sentiment Distribution Since only document-level supervision is provided, we could not directly use the aspect-level sentiment distribution p a j for optimization. In order to connect aspect-level sentiment with document-level sentiment, we compute document-level sentiment distribution as a weighted sum of aspect-level distributions. Thus, by optimizing the document-level predictions, the parameters of the aspect-level sentiment classifier are learned through back propagation. Specifically, the document-level distribution is as following: p d = J j=1 ? j p a j (6) where ? j encodes the importance of aspect a j for determining the sentiment of the overall document. To obtain ? j , we first average the aspect representations: r d = 1 J J j=1 r a j (7) then we use attention mechanism to derive ? j : ? j = exp(v T r tanh(W r [r a j ; r d ] + b r )) j exp(v T r tanh(W r [r a j ; r d ] + b r )) (8) where [r a j ; r d ] is the concatenation of r a j and r d , W r , b r and v r are parameters of the attention mechanism. . After obtaining document-level sentiment distributions, we train the model with respect to document-level sentiment labels and introspectively, the aspect-level sentiment classifier is learned through back propagation. 

 Diversified Regularizations The aspect-level sentiment classifier simply learned in such a way suffers from the overfitting to document-level supervision signals. Firstly, given different aspects, the aspect-level sentiment classifier tends to focus on the same snippets, which actually express the document-level sentiment. Secondly, the predicted aspect-level sentiments tend to be overly consistent with the document-level sentiment. Diversified Textual Regularization To alleviate the first problem, diversified textual regularization is proposed to encourage the sentiment classifier to select aspect-relevant snippets with distant supervision. The main idea is that the aspect-level classifier should pay more attention to the words which co-occur with the given aspect in a same sentence. Specifically, given an aspect a j , a distantlylabeled word selection vector s j is leveraged to guide the attention weight vector ? j in Equation 4. To obtain s j , we first initialize the weights of all words in the document to be 0. Secondly, we find the sentences which contain any keywords of the given aspect 2 . Then we set the weights of words in these sentences to be 1. Finally, we normalize the weight vector. The diversified textual regularization is defined as the KL-divergence betweens ? j and s j : L d?text = KL(s j ||? j ) = i s i j log s i j ? i j (9) Furthermore, there exist sentences which describe multiple aspects. As in most of these sentences, the parts related to different aspects are non-overlapping, we also apply orthogonal regularization  (Lin et al., 2017; Hu et al., 2018)  to guide the attention weights in a fine granularity: L ortho = j j =j ? j ? ? j (10) Minimizing the dot product between two attention weight vectors will force orthogonality between them, so that different aspects attend on different parts of the sentence with less overlap. Diversified Sentimental Regularization Given a document, some of its aspects often have different sentiments from the document-level sentiment. But simply fitting the document-level supervision leads the sentiments of all aspects to be same with the document-level sentiment. To tackle this problem, we propose diversified sentimental regularization to control the variance among aspect-level sentiment distributions. The variance is computed as follows: L d?senti = 1 J J j=1 (p a j (l d ) ? p u (l d )) 2 p u (l d ) = 1 J J j=1 p a j (l d ) (11) where p a j (l d ) is the probability of class l d for aspect a j . By maximizing L d?senti , the model allows the aspect-level sentiment distributions to be different, so that for some aspects, their sentiments could be different from the document-level sentiment l d . Furthermore, instead of using cross-entropy loss, we propose to leverage hinge loss to control the fitting degree of the document-level sentiment distribution p d to the ground truth label l d . The hinge loss is defined as follows: L doc = max(t ? p d (l d ), 0) (12) where p d (l d ) is the probability of the ground-truth label l d , t ? (0.5, 1.0] is the probabilistic margin, which gives the tolerance to diverse aspect-level sentiment distributions. 

 Final Objective Function The final objective function of D-MILN is a combination of document-level loss and diversified regularizations. To minimize clutter, we describe the objective function for a single document: L = L doc + ? m L d?text + ?L ortho + ?L d?senti (13 ) where ?, ?, ? are the hyper-parameters, m is the number of training steps. In diversified textual regularization, the distant supervision is relatively "hard" on the attention weights, which may hurt the generalization of D-MILN, so we further introduce a decay factor ? ? (0, 1). With the increase of training steps (m), the weight of textual diversified regularization will decrease to zero such that the model will be allowed to achieve better generalization. ? controls the sentimental diversity among aspects. For ? < 0, the sentimental diversity is encouraged. For ? > 0, the sentimental diversity is discouraged. 

 Experiments 

 Datasets We evaluate our model on TripAdvisor  (Wang et al., 2010)  and  BeerAdvocate (McAuley et al., 2012)  benchmark datasets, which contain seven predefined aspects (value, room, location, cleanliness, check in/front desk, service, and business) and four predefined aspects (feel, look, smell, and taste) respectively. We run the same preprocessing steps as  Zeng et al. (2019) . The original ratings of Tri-pAdvisor and BeerAdvocate datasets are converted to binary scales, namely, positive or negative. The exploration on fine-grained sentiment classification remains for future work. The number of reviews with negative overall sentiment and that with positive overall sentiment are balanced. Table  1  shows the statistics of the two datasets. Both datasets are split into train/development/test sets with proportions 8:1:1. The development set is used to tune the hyper-parameters for all methods. We use accuracy as the evaluation metric. Note that both aspectlevel and document-level sentiment annotations are provided in the datasets, but our D-MILN only uses document-level annotations for training. 

 Implementation Details We adopt the pre-trained uncased GloVe 300dimensional word embeddings  (Pennington et al., 2014) , which are set to be trainable during the training process 3 . In document encoding, we apply three one-dimension convolutional layers with kernel widths of 3, 5, and 7 respectively 4 . The number of filters is 200 for each convolutional layer. Batch normalization is applied on the output of the convolutional layers. The dimension of all hidden layers is 200. Dropout is applied on the embedding layer and the final representations of aspects and document words with dropout rate being 0.4. The values of ?, ?, ? in Equation 13 are 0.999, 0.1 and ?0.1 respectively. The probabilistic margin t is 0.7. The batch size is set to be 64. Parameter optimization is performed using Adam (Kingma and Ba, 2014) with learning rate being 0.001. We run experiments on one Tesla V100 16GB GPU and each epoch takes several minutes. Our model has 438K parameters, not including word embeddings. 

 Compared Methods Here, we compare our method with a variety of baselines, which can be divided into three categories. (1) Weakly supervised baselines. We use these baselines to show the advancement of D-MILN in terms of weak supervision. (2) MIL baselines. We novelly formulate weakly supervised DMSC as MIL for the first time. By comparing with several simple MIL methods, we also hope to see the necessity of D-MILN. (3) Supervised baseline. Finally, we compare D-MILN with supervised baselines to analyse the performance gap with supervised methods. 

 Weakly Supervised Baselines Assign-O, which directly uses the overall sentiment of a review in the test set as the prediction for its aspects. LRR  (Wang et al., 2010) , which is a probabilistic graphical model (non-neural model) that regards the aspect-level sentiments as latent variables and assumes the document-level sentiment is generated based on a weighted sum of the latent aspect sentiments. LRR only requires document-level annotations. VWS-DMSC  (Zeng et al., 2019) , which is previous state-of-the-art weakly supervised approach for DMSC. VWS-DMS uses aspect-opinion word pairs as supervision. The sentiment of an aspect is treated as a latent variable and is used to predict the opinion word of the given aspect. VWS-DMSC also uses document-level sentiment labels to train a document encoder. 

 MIL Baselines Vanilla-MILN, which is derived by removing key components from D-MILN. Specifically, in Vanilla-MILN, the loss function is cross-entropy loss and the diversified regularizations are not applied. Identity-MILN, which sets the aspect-level sentiment of training data to be identical with document-level labels, and directly trains the aspect-level attention-based sentiment classifier introduced in Section 3.1. Explicit-MILN, of which the relevant snippets for each aspect are firstly extracted by an iterative method adopted in  Wang et al. (2010) , then a CNNbased text classifier is applied on the extracted snippets to predict the aspect-level sentiment under the MIL framework. 

 Supervised Baselines AB-DMSC, which is the attention-based aspectlevel sentiment classifier introduced in Section 3.1. We directly train this classifier with entire aspectlevel sentiment annotations. AB-DMSC serves as an upper bound to our model. AB-DMSC-{500, 1000, 2000, 5000}, which is the AB-DMSC model trained with {500, 1000, 2000, 5000} labeled instances per aspect. Since the sampled labeled data may vary for different trials, we perform five trials of random sampling and report both mean and standard deviation of the results. N-DMSC  (Yin et al., 2017) , which is the stateof-the-art supervised neural model. N-DMSC is also trained with entire aspect-level sentiment annotations.  

 Results and Analysis Table  2  shows the main results. It contains three blocks, corresponding to the three categories of systems.We compare D-MILN with them as follows. (1) Weakly Supervised Baselines. Our model achieves the best performance comparing with previous weakly supervised baselines. From Assign-O, we can see that directly transferring the documentlevel sentiment to aspects gives a poor result, showing the difficulty and necessity of finding a way to properly model the relation between documentlevel sentiment and aspect-level sentiment. Our model outperforms the traditional probabilistic graphical model LRR with a substantial margin, which demonstrates the necessity of utilizing neural networks to capture deep semantic features. Our model also outperforms previous SOTA VWS-DMSC significantly. VWS-DMSC relies on the extracted aspect-opinion word pairs, but we find that there are no typical opinion words for some aspects in the corpus (e.g. look in BeerAdvocate). Besides, in VWS-DMSC, the document-level supervision is only used to train a document encoder, which ignores the relationship between aspects and documents. As our D-MILN only relies on documentlevel signals, this further confirms that D-MILN properly models the relation between aspect-level and document-level sentiment. (2) MIL Baselines. D-MILN significantly outperforms all MIL baselines with a substantial margin. Meanwhile, we find simple MIL baselines often fail to improve performance against previous work (LRR and VWS-DMSC), showing the difficulty of achieving weakly-supervised DMSC by MIL. Furthermore, from Vanilla-MILN, we can conclude that locating aspect-relevant snippets and overcoming the overfitting to document-level supervision are two challenges to improve the performance of MIL on DMSC. Compared with Identity-MILN, it suggests that our method could reduce the noises brought from the document-level supervision signals. Compared with Explicit-MILN, it suggests that our method could effectively select aspect relevant snippets. (3) Supervised Baselines. we first find that AB-DMSC is comparable with N-DMSC, which demonstrates that our aspect-level sentiment classifier could serve as a strong supervised baseline model. Our D-MILN is comparable with AB-DMSC-2000. To analyse the performance gap between D-MILN and AB-DMSC, we conduct a case study, which is contained in Appendix A.3, to qualitatively evaluate the aspect-level attention-based sentiment classifiers. 

 Ablation Study To demonstrate the effectiveness of each component of D-MILN, we conduct an ablation study and list the results in Table  3 . "-keywords" means simply using the aspect term rather than its keywords to interact with the document. "-hinge loss" means replacing the hinge loss in Equation 12 by cross-entropy loss. "-d-senti" means removing diversified sentimental regularization. "-d-text" means removing diversified textual regularization. We can see that extending a single aspect term with a list of aspect relevant keywords can improve the classification performance on both datasets. The orthogonal regularization is much more useful in the TripAdvisor dataset, which indicates there are more sentences containing multiple aspects. By employing the diversified sentimental regularization, the overfitting problem of document-level signals can be alleviated and thus improves the classification performance. When removing the diversified textual regularization, the results are much worse than removing other components, demonstrating locating the aspect-relevant snippets is crucial for correctly predicting the aspect-level sentiments. 

 Effectiveness of Diversified Textual Regularization To further demonstrate the effectiveness of diversified textual regularization, we display the KLdivergence between attention weight distributions of different aspect pairs in Figure  3 . The attention weight distribution, which is calculated by Equation  4 , indicates the importances of document words to the given aspect. Large KL-divergences indicate that the aspect-level classifier selects distinct snippets for different aspects. For Vanilla-MILN, the KL-divergences are relatively small, which indicates that the model focuses on similar snippets for different aspects. For Vanilla-MILN+dtext, on which the diversified textual regularization is applied, the KL-divergences become larger and are similar with that of AB-DMSC, which is trained with aspect-level annotations and produces the most proper attention weights among the three models. Such results indicate that diversified textual regularization encourages the aspect-level sentiment classifier to select aspect-relevant snippets. 

 Hinge Loss for Diversified Sentimental Regularization We further demonstrate that hinge loss is more compatible than cross-entropy loss with diversified sentimental regularization. In Figure  4 , we display the variances, which is calculated by Equation  11 , among aspect-level sentiment distributions when different loss functions are adopted. The horizontal axis ? denotes the weight of the diversified sentimental regularization. When ? turns to 0.0, which means the diversified sentimental regularization is not applied, we find that the variance is relatively small for both hinge loss and cross entropy loss, which indicates that the predicted aspect-level sentiments are over consistent with document-level ones. When ? turns to ?0.1, which means the diversity of sentiments is encouraged, the variance under hinge loss grows significantly than crossentropy loss, which verifies that by applying hinge loss, the diversity among aspect-level sentiments could be controlled more effectively. 

 Conclusion In this paper, we propose a diversified multiple instance learning network to achieve DMSC with only document-level supervision. We formulate this problem as multiple instance learning, so as to model the relation between aspect-level sentiment and document-level sentiment. In order to guarantee the proper transfer from document-level supervision to aspect-level prediction, we further propose diversified textual regularization and diversified sentimental regularization. Through experiments on two benchmark datasets, we verify that our D-MILN can properly capture the interaction between aspect-level and document-level, and achieve new SOTA on weakly supervised DMSC. Detailed comparisons also show the necessity and effectiveness of our diversified regularizations. In the future, we plan to further improve D-MILN with aspect-level annotations and find appropriate way to combine D-MILN with pre-training methods  (Tian et al., 2020) .  Review very unwelcoming staff -downright unfriendly while the room be lovely , the staff be very unfriendly and discourteous . we be very easygoing people . and experienced traveller . however , the staff be very unwilling to answer basic question unk airport unk and restaurant recommendation . one woman behind the desk just seem to be angry all the time . while i love barcelona -this hotel experience be very unk to unk . definitely not a service orient hotel . lovely be love to and very people -and be hotel restaurant definitely be . this unk however i , 
