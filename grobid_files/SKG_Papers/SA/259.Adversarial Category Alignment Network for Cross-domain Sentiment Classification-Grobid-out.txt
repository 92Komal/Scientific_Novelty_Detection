title
Adversarial Category Alignment Network for Cross-domain Sentiment Classification

abstract
Cross-domain sentiment classification aims to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. Most existing adversarial learning methods focus on aligning the global marginal distribution by fooling a domain discriminator, without taking category-specific decision boundaries into consideration, which can lead to the mismatch of category-level features. In this work, we propose an adversarial category alignment network (ACAN), which attempts to enhance category consistency between the source domain and the target domain. Specifically, we increase the discrepancy of two polarity classifiers to provide diverse views, locating ambiguous features near the decision boundaries. Then the generator learns to create better features away from the category boundaries by minimizing this discrepancy. Experimental results on benchmark datasets show that the proposed method can achieve stateof-the-art performance and produce more discriminative features.

Introduction Sentiment classification aims to automatically identify the sentiment polarity (i.e., positive or negative) of the textual data. It has attracted a surge of attention due to its widespread applications, ranging from movie reviews to product recommendations. Recently, deep learningbased methods have been proposed to learn good representations and achieved remarkable success. However, the performances of these works are highly dependent on manually annotated training data while annotation process is time-consuming and expensive. Thus, cross-domain sentiment classification, which aims to transfer knowledge learned on labeled data from related domains * Equal contribution ? Corresponding author (called source domain) to a new domain (called target domain), becomes a promising direction. One key challenge of cross-domain sentiment classification is that the expression of emotional tendency usually varies across domains. For instance, considering reviews about two sorts of products: Kitchen and Electronics. One set of reviews would contain opinion words such as "delicious" or "tasty", and the other "rubbery" or "blurry", to name but a few. Due to the small intersection of two domain words, it remains a significant challenge to bridge the two domains divergence effectively. Researchers have developed many algorithms for cross-domain sentiment classification in the past. Traditional pivot-based works  (Blitzer et al., 2007; Yu and Jiang, 2016)  attempt to infer the correlation between pivot words, i.e., the domainshared sentiment words, and non-pivot words, i.e., the domain-specific sentiment words by utilizing multiple pivot prediction tasks. However, these methods share a major limitation that manual selection of pivots is required before adaptation. Recently, several approaches  (Sun et al., 2016; Zellinger et al., 2017)  focus on learning domain invariant features whose distribution is similar in source and target domain. They attempt to minimize the discrepancy between domain-specific latent feature representations. Following this idea, most existing adversarial learning methods  (Ganin et al., 2016;  reduce feature difference by fooling a domain discriminator. Despite the promising results, these adversarial methods suffer from inherent algorithmic weakness. Even if the generator perfectly fools the discriminator, it merely aligns the marginal distribution of the two domains and ignores the category-specific decision boundaries. As shown in Figure  1  (left), the generator may generate ambiguous or even mismatched features near the decision boundary, thus hindering the performance of adaptation. To address the aforementioned limitations, we propose an adversarial category alignment network (ACAN) which enforces the category-level alignment under a prior condition of global marginal alignment. Based on the cluster assumption in  (Chapelle et al., 2009) , the optimal predictor is constant on high density regions. Thus, we can utilize two classifiers to provide diverse views to detect points near the decision boundaries and train the generator to create more discriminative features into high-density region. Specifically, we first maximize the discrepancy of the outputs of two classifiers to locate the inconsistent polarity prediction points. Then the generator is trained to avoid these points in the feature space by minimizing the discrepancy. In such an adversarial manner, the ambiguous points are kept away from the decision boundaries and correctly distinguished, as shown in Figure  1  (right). We evaluate our method on the Amazon reviews benchmark dataset which contains data collected from four domains. ACAN is able to achieve the state-of-the-art results. We also provide analyses to demonstrate that our approach can generate more discriminative features than the approaches only aligning global marginal distribution  (Zhuang et al., 2015) . 

 Related Work Sentiment Classification: Deep learning based models have achieved great success on sentiment classification  (Zhang et al., 2011) . These models usually contain one embedding layer which maps each word to a dense vector, and different network architectures then process combined word vectors to generate a representation for classification. According to diverse network architectures, four categories are divided including Convolutional Neural Networks (CNNs)  (Kalchbrenner et al., 2014; Kim, 2014) , Recurrent Neural Networks (RNNs)  (Yang et al., 2016; Zhou et al., 2016b) , Recursive Neural Networks (RecNNs)  (Socher et al., 2013)  and other neural networks  (Iyyer et al., 2015) . Domain Adaption: The fundamental challenge to solve the domain adaptation lies here is that data from the source domain and target domain have different distributions. To alleviate this difference, there are many pivot-based methods  (Blitzer et al., 2007; He et al., 2011; Gouws et al., 2012; Yu and Jiang, 2016; Ziser and Reichart,   2018) which try to align domain-specific opinion (non-pivot) words through domain-shared opinion (pivot) words as the expression of emotional tendency usually varies across domains, which is a major reason of the domain difference. However, selecting pivot words for these methods first is very tedious, and the pivot words they find may not be accurate. Apart from pivot-based methods, denoising auto-encoders  (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014)  have been extensively explored to learn transferable features during domain adaption by reconstructing noise input. Despite their promising results, they are based on discrete representation. Recently, some adversarial learning methods  (Ganin et al., 2016; Li et al., , 2018  propose to reduce this difference by minimizing the distance between feature distributions. But these methods solely focus on aligning the global marginal distribution by fooling a domain discriminator, which can lead to the mismatch of category-level features. To solve this issue, we propose to further align the categorylevel distribution by taking the decision boundary into consideration. Some recent works with classlevel alignment have been explored in computer vision applications  (Saito et al., 2017 (Saito et al., , 2018 . Semi-supervised learning: Considering the target samples as unlabeled data, our work is somehow related to semi-supervised learning (SSL). SSL has several critical assumptions, such as cluster assumption that the optimal predictor is constant or smooth on connected high density regions  (Chapelle et al., 2009) , and manifolds assumption that support set data lies on low-dimensional manifolds  (Chapelle et al., 2009; Luo et al., 2017) . Our work takes these assumptions to develop the approach.  are n t unlabeled target domain examples. In our proposed method, we denote G as a feature encoder that extracts features from the input sentence. Then two classifiers F 1 and F 2 map these features to soft probabilistic outputs p 1 (y|x) and p 2 (y|x) respectively. The goal is to train a model to classify the target examples correctly with the aid of source labeled data and target unlabeled data. To achieve this, we first train G, F 1 and F 2 to obtain global marginal alignment. This step reduces the distance between two domains but generates ambiguous target features near the decision boundary. Thus, F 1 and F 2 are adjusted to detect them by maximizing prediction discrepancy. After that, G is trained to generate better features avoiding appearing near the decision boundary. The method also regularizes G by taking the target data samples into consideration. In this way, we can achieve the category alignment. The proposed Adversarial Category Alignment Network (ACAN) is illustrated in Figure 2. The detailed training progress is described in Appendix D. 

 Marginal Distribution Alignment To solve the domain adaption problem, we first consider minimize the classification error on the source labeled data for two classifiers: L cls = ? 1 n s ns i=1 K j=1 y (s) i (j) log y (s) 1i (j) ? 1 n s ns i=1 K j=1 y (s) i (j) log y (s) 2i (j) y 1i =F 1 (G(x (s) i )) y 2i = F 2 (G(x (s) i )) (1) where K denotes the number of different polarities. In addition, similar to  (Zhuang et al., 2015) , our method tries to explicitly minimize the distance between the embedding features from both the source and the target domains. We adopt the Kullback?Leibler (KL) to estimate the distribution divergence: L kl = n i=1 g s (i) log g s (i) g t (i) + n i=1 g t (i) log g t (i) g s (i) g s = 1 n s ns i=1 G(x (s) i ), g s = g s ||g s || 1 g t = 1 n t nt i=1 G(x (t) i ), g t = g t ||g t || 1 (2) where g s , g t ? R D , || ? || 1 denotes L1 normalization. In this way, the latent network representations of two domains are encouraged to be similar. In other words, the marginal distribution is forced to be aligned. 

 Category-level Alignment Diverse Views: Considering the marginal distribution alignment, there could be some ambiguous features near the decision boundary, which are easy to be incorrectly categorized into a specific class. If we alter the boundary of classifier F 1 and F 2 , the samples closer to the decision boundary would have larger change. To explore these samples, we use F 1 and F 2 to provide diverse guidance. We define a discrepancy between probabilistic outputs of the two classifiers p 1 (y|x) and p 2 (y|x). The formula is: L dis = E x?Dt [d(p 1 (y|x), p 2 (y|x))] (3) where d(p 1 (y|x), p 2 (y|x)) defines the average absolute difference for K classes, which is: d(p 1 (y|x), p 2 (y|x)) = 1 K K i=1 |p 1 i (y|x)?p 2 i (y|x)| (4) Specifically, we first fix the generator G and train the classifiers F 1 ,F 2 to detect points near the decision boundary by maximizing their discrepancy. The objective is as follows: max F 1,F 2 E x?Dt [ 1 K K i=1 |p 1 i (y|x) ? p 2 i (y|x)|] (5) Then, this discrepancy is minimized by optimizing G in order to keep these points away from the decision boundary and categorized into correct classes. The objective is as follows: min G E x?Dt [ 1 K K i=1 |p 1 i (y|x) ? p 2 i (y|x)|] (6) This adversarial step is repeated in the whole training process so that we can continuously locate non-discriminative points and classify them correctly, forcing the model to achieve categorylevel alignment on two domains. 

 Training Steps The whole training procedure can be divided into three steps. In the first step, we consider both minimizing the classification error and marginal distribution discrepancy to achieve global marginal alignment. The loss function of this step can be written as: L 1 = L cls + ? 1 L kl (7) In the second step, we consider increasing the difference of two classifiers F 1 and F 2 for the fixed G, thus the ambiguous features can be located by the diverse views. The loss function is defined as below: L 2 = L cls ? ? 2 L dis (8) L cls is used here to ensure the stability of the training process. ? 2 is a hyper-parameter controlling the range of classifiers. In the third step, the difference of two classifiers should be reduced for the fixed F 1 and F 2 : L 3 = L cls + ? 3 L dis (9) L cls and ? 3 used here are similar to the second step. We repeat this step n times to balance the generator and two classifiers. After each step, the corresponding part of the network parameters will be updated. Algorithm 1 describes the overall training procedure. Algorithm 1 Training procedure of ACAN Require: D s , D t , G, F 1 , F 2 Require: ? 1 , ? 2 , ? 3 , iteration number n for i ? [1, max?epochs] do for minibatch B (s) , B (t) ? D (s) , D (t) do compute L cls on x i ? B (s) , y i ? B (s) compute L kl on x i ? B (s) , x j ? B (t) L 1 = L cls + ? 1 L kl update G, F 1 , F 2 by minimizing L 1 compute L cls on x i?B (s) , y i?B (s) compute L dis on x i?B (t) , x i?B (t) L 2 = L cls ? ? 2 L dis fix G, update F 1 , F 2 by minimizing L 2 . for j ? [1, n] do compute L cls on x i ? B (s) , y i ? B (s) compute L dis on x i ? B (t) , x i ? B (t) L 3 = L cls + ? 3 L dis fix F 1 , F 2 , update G by minimizing L 3 . end for end for end for 

 Generator Regularizer To further enhance the feature generator, we introduce to regularize G with the information of unlabeled target data. Generally, the mapping of G(?) can been seen a low-dimensional feature of the input. According to the manifolds assumption  (Chapelle et al., 2009) , this feature space is expected to be low-dimensional manifold and linearly separable. Inspired by  (Luo et al., 2017) , we consider the connections between data points to regularize G(?) in the feature space. Specifically, the regularizer is formulated as follows: R(G) = x?Dt l G (x i , x j ) (10) here l G is to approximate the semantic similarity of two feature embeddings. Possible options include triplet loss  (Wang et al., 2016) , Laplacian eigenmaps  (Belkin and Niyogi, 2003)  etc. After exploring many tricks, we find below is optimal which is also used by  (Luo et al., 2017) : l G = d 2 i,j s ij = 1 max(0, m?d i,j ) 2 s ij = 0 (11) where d i,j is L2 distance between data points, m is a predefined distance, and s ij indicates whether x i and x j belong to the same class or not. Eq. 10 serves as a regularization that encourages the output of R(G) to be distinguishable among classes. It is applied on target data and integrated in the framework in the third training step, weighted by ? 4 . During the training, the underlying label of x i is estimated by taking the maximum posterior probability of the two classifiers. 

 Theoretical Analysis In this subsection, we provide a theoretical analysis of our method, which is inspired by the theory of domain adaptation in  (Ben-David et al., 2010) . For each domain, there is a labeling function on inputs X, defined as f : X ? [0, 1]. Thus, the source domain is denoted as D s , f s and the target domain as D t , f t . We define a hypothesis function h: X ? [0, 1] and a disagreement function: (h 1 , h 2 ) = E[|h 1 (x) ? h 2 (x)|] (12) Then the expected error on the source samples s (h, f ) of h is defined as: s (h) = s (h, f s ) = E x?Ds [|h(x)?f s (x)|] (13) Also for the target domain, we have t (h) = s (h, f t ) = E x?Dt [|h(x) ? f t (x)|] (14) As is introduced in (Ben-  David et al., 2010) , the probabilistic bound of the error of hypothesis h on the target domain t (h) is defined as: ?h ? H, t (h) ? s (h) + 1 2 d H?H (D s , D t ) + ? (15) where the expected error t (h) is bounded by three terms: (1) the expected error on the source examples s (h); (2) the divergence between the distributions D s and D t ; (3) the combined error of the ideal joint hypothesis ?. First, the training algorithm is easy to minimize s (h) with source label information. Second, ? is expected to be negligibly small and can be usually disregarded. Therefore, the second term d H?H (D s , D t ) is important quantitatively in computing the target error. Regarding d H?H (D s , D t ), we have d H?H (D s , D t ) = 2 sup h,h ?H | s (h, h ) ? t (h, h )| =2 sup h,h ?H |E x?Ds [|h(x)?h (x)|]?E x?Dt [|h(x)?h (x)|]| (16) where h and h are two sets of hypotheses in H. As we have sufficient labeled source examples to train, h and h can have consistent and correct predictions on the source domain data. Thus, d H?H (D s , D t ) is approximately calculated as E x?Dt [|h(x) ? h (x)|]. In our model, the hypothesis h can be decomposed into the feature extractor G and the classifier F using the notation ?. Thus d H?H (D s , D t ) can be formulated as: sup F 1,F 2 E x?Dt [|F 1 ? G(x) ? F 2 ? G(x)|] (17) For fixed G, sup can be replaced by max. Therefore, F 1 and F 2 are trained to maximize the discrepancy of their outputs and we expect G to minimize this discrepancy. So we obtain min G max F 1,F 2 E x?Dt [|F 1 ? G(x) ? F 2 ? G(x)|] (18) The maximization of F 1 and F 2 is to provide diverse views, to find ambiguous points near the decision boundary, and the minimization of G is to keep these points away from the decision boundary. To optimize Eq. 18, we assist the model to capture the whole feature space on the target domain better and achieve lower errors. 

 Experiments 

 Data and Experimental Setting We evaluate the proposed ACAN on the Amazon reviews benchmark datasets collected by  Blitzer (2007)  positive and 1000 negative reviews for each domain, as well as a few thousand unlabeled examples, of which the positive and negative reviews are balanced. Following the convention of previous works  (Zhou et al., 2016a; Ziser and Reichart, 2018; He et al., 2018) , we construct 12 cross-domain sentiment classification tasks. In our transferring task, we employ a 5-fold crossvalidation protocol, that is, in each fold, 1600 balanced samples are randomly selected from the labeled data for training and the rest 400 for validation. The results we report are the averaged performance of each model across these five folds. 

 Training Details and Hyper-parameters In our implementation, the feature encoder G consists of three parts including a 300-dimensional word embedding layer using GloVe  (Pennington et al., 2014) , a one-layer CNN with ReLU activation function adopted in  (Yu and Jiang, 2016; He et al., 2018)  and a max-over-time pooling through which final sentence representation is obtained. Specifically, the convolution filter and the window size of this one-layer CNN are 300 and 3 separately. Similarly, the classifier F 1 and F 2 can be decomposed into one dropout layer and one fully connected output layer. For the fully connected layer, we constrain the l2-norm of the weight vector, setting its max norm to 3. For the implementation of generator regularizer, we apply doubly stochastic sampling approximation due to the computational complexity. The margin m is set to 1 in this procedure. During training period, ? 1 , ? 2 , ? 3 , ? 4 , and n are set to 5.0, 0.1, 0.1, 1.5, 2. Similar to  (He et al., 2018) , we parametrize ? 4 as a dynamic weight exp[?5(1 ? t max?epochs ) 2 ]? 4 . This is to minimize the effort of the regularizer as the predictor is not good at the beginning of training. We train 30 epochs for all our experiments with batch-size 50 and dropout rate 0.5. RMSProp  (Tieleman and Hinton, 2012)  optimizer with learning rate set to 0.0001 is used for all experiments. 

 Methods for Comparison We consider the following approaches for comparisons (The URLs of previous methods code and data we use are in Appendix A): SVM  (Fan et al., 2008) : This is a non-domainadaptation method, which trains a linear SVM on the raw bag-of-words representation of the labeled source domain. AuxNN  (Yu and Jiang, 2016) : This method uses two auxiliary tasks to learn sentence embeddings that works well across two domains. For fair comparison, we replace the neural model in this work with our CNN encoder. DANN  (Ganin et al., 2016) : This method exploits a domain classifier to minimize the discrepancy between two domains via adversarial training manner. we replace its encoder with our CNNbased encoder. PBLM  (Ziser and Reichart, 2018) : This is a representation learning model that exploits the structure of the input text. Specifically, we choose CNN as the task classifier. DAS  (He et al., 2018) : This method employs two regularizations: entropy minimization and self-ensemble bootstrapping to refine the classifier while minimizing the domain divergence. Baseline: Our baseline model is a non-adaptive CNN similar to  (Kim, 2014) , trained without using any target domain information, which is a variant of our model by setting ? 1 , ? 2 , ? 3 , ? 4 to zeros. ACAN-KL: ACAN-KL is a variant of our model which minimizes the distance between the features of two domains by minimizing the KL divergence. (set ? 2 = ? 3 = ? 4 = 0) ACAN-KM: ACAN-KM introduces the adversarial category mapping based on ACAN-KL without the regularizer. (set ? 4 = 0). ACAN: It is our full model. 

 Results Table  1  shows the classification accuracy of different methods on the Amazon reviews, and we can see that the proposed ACAN outperforms all other methods generally. It is obvious to see that SVM performs not well in domain transferring task, beaten by Baseline. We can notice that exploring the structure of the input text (AuxNN and PBLM) brings some improvements over Baseline. However, these two pivot-based methods present relatively lower ability than DAS, which jointly minimizes global feature divergence and refines classifier. Compared to DAS, our proposed ACAN can improve 0.19% on the average accuracy. This can be explained by that we deal with the relationship between target features distribution and classifier more precisely. Finally, we conduct experiments on the variants of the ACAN. It is clear that the performances of Baseline, ACAN-KL, ACAN-KM and ACAN present a growing trend in most cases. Compared with ACAN-KL, ACAN achieves large gain from 80.48% to 82.15%, showing the effectiveness of category-level alignment. 

 Case Study To better understand the results of different models, we conduct experiments on task B ? E. For each sentiment polarity, we first extract the most related CNN filters according to the learned weights of the output layer in classifier F 1 . Since all listed models use a window size of 3, the outputs of CNN with the highest activation values correspond to the most useful trigrams. As shown in Table  2 , we identify the top trigrams from 10 most related CNN filters on the target domain. It is obvious that Baseline and ACAN-KL are more likely to capture the domainindependent words, such as "pointless", "disappointing" and "great". Thus, the performance of these two models drops much when applied to the target domain. Besides, DAS can capture more words of the target domain, but it is limited to nouns with less representativeness, such as "receiver", "product" and etc. Compared to them, ACAN is able to extract the domain-specific words like "flawlessly" and "rechargeable". These results are consistent with the accuracy of each model's predictions. We also conduct experiments on the tasks B ? K and K ? D. Due to the space limitations, the results are presented in Appendix B. 

 Visualization of features For more intuitive understanding of the differences between the global marginal alignment and category alignment, we further perform a visualization of the feature representations of the ACAN-KL and ACAN model for the training data in the source domain and the testing data in the target domain for the K?E task. As can be seen in Figure  3 , global marginal alignment causes ambiguous 

 Method Negative Sentiment Positive Sentiment Baseline audio-was-distorted, is-absolutely-pointless, *-very-disappointing, waste-of-money, was-point-most, an-unsupported-config, an-extremely-disappointed, author-album-etc, cure-overnight-headphones, aa-rechargable-batteries wep-encryption-detailed, totally-wireless-headset, best-!-i, love-it-!, again-period-!, beautifully-great-price, awesome-accurate-sound, beautifully-designed-futuristic, wonderful-product-*, glad-i-purchased 

 ACAN-KL totally-useless-method, audio-was-distorted, *-very-weak, *-very-disappointing, extra-ridiculous-buttons, hopeless-mess-no, now-as-useless, waste-of-cash, is-absolutely-pointless, manual-is-useless gift-i-love, uniden-cordless-telephone, a-journey-to, totally-wireless-headset, your-own-frequencies, a-gift-excellent, exceptional-being-rechargeable, gorgeous-picture-excellent, with-wireless-security, beautifully-designed-futuristic DAS receiver-was-faulty, defective-product-i, is-useless-i, do-not-waste, did-not-work, very-poor-quality, the-crappy-keyboard, just-too-weak, is-absolutely-pointless, very-stupid-design is-an-excellent, excellent-monitor-with, is-very-nice, truly-excellent-headphones, an-incredible-sound advanced-technology-incredible, this-is-an, !-highly-recommended show-very-easy, picture-is-fabulous   features locating between two clusters while category alignment effectively projects these points into clusters, thus leading a more robust classification result. We also conduct experiments on the tasks B ? E and B ? K. Due to the space limitations, the results are presented in Appendix C. 

 Model Analysis In this part, we provide analysis to our proposed ACAN variants. In Figure  4 , we show the comparison between Baseline and ACAN under a setting that some labeled target data are randomly selected and mixed with training data. Here, we present results on two transferring tasks while a similar tendency can be observed in other pairs. With an increase in the number of randomly selected labeled target data, the difference between the two models gradually decreases and ACAN also progressively obtains better results. These trends indicate that our ACAN is more effective with no or little-labeled target data and can further benefit from more labeled target data. In Figure  5 , we can easily observe that ACAN continuously shows better results during the whole training process among four settings. After some epochs, ACAN-KL starts presenting lower testing accuracy than Baseline. One possible reason is that those categories which are initially well aligned between the source and target may be incorrectly mapped because of ignoring category-level feature distribution. This observation can prove our motivation in some degree. 

 Conclusion In this paper, we propose a novel approach, which utilizes diverse view classifiers to achieve category-level alignment for sentiment analysis. Unlike previous works, we take the decision boundary into consideration, thus classifying the target samples correctly into the corresponding category. Experiments show the proposed ACAN significantly outperforms state-of-the-art methods on the Amazon benchmark. In future we would like to adapt our method to other domain adaptation tasks and consider more effective alternatives for the generator regularizer. 
