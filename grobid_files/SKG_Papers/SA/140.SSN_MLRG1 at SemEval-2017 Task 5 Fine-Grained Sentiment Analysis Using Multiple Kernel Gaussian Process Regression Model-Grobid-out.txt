title
SSN MLRG1 at SemEval-2017 Task 5: Fine-Grained Sentiment Analysis Using Multiple Kernel Gaussian Process Regression Model

abstract
The system developed by the SSN MLRG1 team for Semeval-2017 task 5 on fine-grained sentiment analysis uses Multiple Kernel Gaussian Process for identifying the optimistic and pessimistic sentiments associated with companies and stocks. Since the comments on the same companies and stocks may display different emotions depending on time, their properities like smoothness and periodicity may vary. Our experiments show that while single Kernel Gaussian Process can learn some properties well, Multiple Kernel Gaussian Process are effective in learning the presence of different properties.

Introduction Sentiments have been widely studied as they play an important role in human intelligence, rational decision making, social interaction, perception, memory, learning and creativity  (Pang and Lee, 2008; Strapparava and Mihalcea, 2008; Maas et al., 2011; Li et al., 2015) . The ability to discern and understand human sentiments is critical for making interactive human-like computer agents, and requires the use of machine learning approaches  (Alm et al., 2005) . 

 Gaussian Process Gaussian Process (GP) is a Bayesian nonparametric approach to machine learning. A Gaussian Process is a collection of random variables, any infinite number of which have a joint Gaussian distribution  (Rasmussen and Williams, 2006) . Using a Gaussian process, we can define a distribution over functions f (x), f (x) ? GP (m(x), k(x, x )) (1) where m(x) is the mean function, usually defined to be zero, and k(x, x ) is the covariance function (or kernel function) that defines the prior properties of the functions considered for inference. Gaussian Process has the following main advantages  (Cohn and Specia, 2013; Cohn et al., 2014) . ? The kernel hyper-parameters can be learned via evidence maximization. ? GP provides full probabilistic prediction, and an estimate of uncertainty in the prediction. ? Compared to SVMs which need unbiased datasets for good performance, GPs do not usually suffer from this problem. ? GP can be easily extended and incorporated into a hierarchical Bayesian model. ? GP works really well when combined with kernel models. ? GP works well for small datasets too. 

 Gaussian Process Regression The Gaussian Process regression framework assumes that, given an input x, output y is a noise corrupted version of a latent function evaluation. In a regression setting, we usually consider a Gaussian likelihood, which allows us to obtain a closed form solution for the test posterior  (Ebden, 2008) . Gaussian Process model, as they are applied in machine learning, is an attractive way of doing non-parametric Bayesian modeling for a supervised learning problem. GP-based modeling has the ability to learn hyper-parameters directly from data by maximizing the marginal likelihood. Like other kernel methods, the Gaussian Process can be optimized exactly, given the values of their hyper-parameters and this often allows a fine and precise trade-off between fitting the data and smoothing. A practical implementation of Gaussian Process Regression (GPR)  (Rasmussen and Williams, 2006)  is outlined in the following algorithm: Algorithm: Predictions and log-marginal likelihood for GP regression. Input: X (training inputs) , y (training targets), k (covariance function), ? 2 n (noise level), x * (test input). Output: Predictive mean, variance and logmarginal likelihood. 1. L := cholesky(K + ? 2 n I) 2. ? := L T \(L\y) 3. f * := k * T ? 4. v := L\k * 5. V [f * ] := k(x * , x * ) ? v T v 6. log p(y|X) := ? 1 2 y T ? ? i log L ii ? n 2 log 2? 7. return f * (mean), V [f * ] (variance), log p(y|X) (log-marginal likelihood) 

 Multiple Kernel Gaussian Process The heart of every Gaussian process model is a covariance kernel. The kernel k directly specifies the covariance between every pair of input points in the dataset. The particular choice of covariance function determines the properties such as smoothness, length scales, and amplitude, drawn from the GP prior. Therefore, it is an important part of GP modelling to select an appropriate covariance function for a particular problem. Multi Kernel Learning (MKL) -using multiple kernels instead of a single one -can be useful in two ways: ? Different kernels correspond to different notions of similarity, and instead of trying to find which works best, a learning method does the picking for us, or may use a combination of them. Using a specific kernel may be a source of bias which is avoided by allowing the learner to choose from among a set of kernels. ? Different kernels may use inputs coming from different representations, possibly from different sources or modalities.  (Gonen and Alpaydin, 2011; Wilson and Adams, 2013)  explain how multiple kernels definitely give a powerful performance.  (Gonen and Alpaydin, 2011 ) also describes in detail various methodologies to combine kernels.  (Wilson and Adams, 2013)  introduces simple closed form kernels that can be used with Gaussian Processes to discover patterns and enable extrapolation. The kernels support a broad class of stationary covariances, but Gaussian Process inference remains simple and analytic. We studied the possibility of using multiple kernels to explain the relation between the input data and the labels. While there is a body of work on using Multi Kernel Learning (MKL) on numerical data and images, yet applying MKL on text is still an exploration. We have used Exponential kernel and Multi-Layer Perceptron kernel together with Squared Exponential kernel, and found the combinations to give better results. The text data used in sentiment analysis is collected over a period of time. Comments on the same topic may exhibit different emotions, depending on the time it was made, and hence their properties, such as smoothness and periodicity, also vary with time. Since any one kernel learns only certain properties well, multiple kernels will be effective in detecting the presence of different emotions in the data. The MKL algorithms use different learning methods for determining the kernel combination function. It is divided into five major categories: Fixed rules, Heuristic approaches, Optimization approaches, Bayesian approaches and Boosting approaches. The combination of kernels in different learning methods can be performed in one of the two basic ways, either using linear combination or using non-linear combination. Linear combination seems more promising  (Gonen and Alpaydin, 2011) , and have two basic categories: unweighted sum (i.e., using sum or mean of the kernels as the combined kernel) and weighted sum. Non-linear combination use non-linear functions of kernels, namely multiplication, power, and exponentiation. We have studied the fixed rule linear combination in this work which can be represented as k(x, x ) = k 1 (x, x )+k 2 (x, x )+. . .+k n (x, x ). (2) For training, we have used one-step method together with the simultaneous approach. One-step methods, in a single pass, calculate both the parameters of the combination function, and those of the combined base learner; and the simultaneous approach ensures that both sets of parameters are learned together. 

 System Overview The system comprises of the following modules: data extraction, preprocessing, feature vector generation, and multi-kernel Gaussian Process model building. The algorithm for preprocessing of the data and feature vector building is outlined below: Algorithm: Preprocess the data and generate feature vectors. Input: Input dataset. Output: Dictionary with the key -value pair and BoW Feature vector. begin 

 Perform lemmatization using WordNet Lemmatizer from the NLTK tool kit. 2. Perform tokenization using the wordpunct tokenize function of the NLTK toolkit. 3. Set the integer value for the train variable. 4. Build data dictionaries for training sentences. 5. Build a data dictionary with words mapped to their indices. 6. Generate feature vectors for the train sets that encode a BoW representation. 7. Build a dictionary with the key-value pairs. The key is the emotion and the value is a matrix where rows are BoW vectors.  

 end The Multi-Kernel Gaussian Process model is implemented using linear combination method which takes the unweighted sum of the kernels. 

 Comparison Using Different Kernels The output submitted for the task was based on the linear combination of Squared Exponential kernel and Exponential kernel. 

 Kernels The Squared Exponential (SE) kernel, sometimes called the Gaussian or Radial Basis Function (RBF), has become the default kernel in GPs. To model the long term smooth-rising trend we use a Squared Exponential covariance term. k(x, x ) = ? 2 exp ? (x ? x ) 2 2l 2 . ( 3 ) where ? 2 is the variance and l is the length-scale. The usage of Exponential kernel is particularly common in machine learning and hence is also used in GPs. They perform tasks such as statistical classification, regression analysis, and cluster analysis on data in an implicit space. k(x, x ) = ? 2 exp ? (x ? x ) 2l 2 (4) The Multi-Layer Perceptron kernel has also found use in GP as it can learn the periodicity property present in the dataset; its k(x, x ) is given by 2? 2 ? sin ?1 (? 2 w x T x + ? 2 b ) ? 2 w x T x + ? 2 b + 1 ? 2 w x T x ? 2 b + 1 (5) where ? 2 is the variance, ? 2 w is the vector of the variances of the prior over input weights and ? 2 b is the variance of the prior over bias parameters. The kernel can learn more effectively because of the additional parameters ? 2 w and ? 2 b . 

 Performance Evaluation Other combinations of the kernel were also tried after submission. One such kernel used for experimentation purpose was Multi-Layer Perception Kernel. The results of the Single Kernel and Multi-Kernel GP on subtask 1 dataset are collated in Table  1 . The results of the Single Kernel and The evaluation considered 70% of the dataset for training and 30% for testing. The greater the Cosine Similarity (CS) and the Pearson Score (PS), and the smaller the Mean Absolute Error (MAE), the better the performance of the system. The tables show that MKGP(R+M), Multi Kernel Gaussian Process with sum of Squared Exponential and Multi-Layer Perceptron kernels, performs better. 

 Official Evaluation The systems developed were evaluated based on Cosine Similarity measure. Our system ranked fifth position with Cosine Similarity of 0.7347 for subtask 1 and fifteenth position with Cosine Similarity of 0.6657 for subtask 2. 

 Conclusion In this paper, we have presented a Multi Kernel Gaussian Process(MKGP) regression model for fine-grained sentiment analysis of financial microblogs and news. We used Bag of Words input feature vectors as input and fixed rule multi kernel learning to build GP model and found it to perform better than single kernel learning. The results can be further enhanced by using different feature generation approaches and multi kernel learning approaches. Table 1 1 : A performance comparison based on Cosine Similarity (CS), Pearson Score (PS) and Mean Absolute Error (MAE) for subtask 1 dataset Model CS PS MAE SGP 0.6942 0.6694 0.2003 MKGP(R+E) 0.7044 0.6809 0.1965 MKGP(R+E+M) 0.7099 0.6864 0.1931 MKGP(R+M) 0.7106 0.6872 0.1930 Multi-Kernel GP on subtask 2 dataset are shown in Table 2. The kernel combinations used in Ta- ble 1 and Table 2 are
