title
NLANGP at SemEval-2016 Task 5: Improving Aspect Based Sentiment Analysis using Neural Network Features

abstract
This paper describes our system submitted to Aspect Based Sentiment Analysis Task 5 of SemEval-2016. Our system consists of two components: binary classifiers trained using single layer feedforward network for aspect category classification (Slot 1), and sequential labeling classifiers for opinion target extraction (Slot 2). Besides extracting a variety of lexicon features, syntactic features, and cluster features, we explore the use of deep learning systems to provide additional neural network features. Our system achieves the best performances on the English datasets, ranking 1st for four evaluations (Slot 1 for both restaurant and laptop domains, Slot 2, and Slot 1 & 2).

Introduction Sentiment analysis and opinion mining have gained increasing interests in recent years due to the continuous growing of user-generated content on the Internet. Traditionally, the primary focus of the research has been on the detection of the overall sentiment of a sentence or paragraph. However, such approach is unable to handle conflicting sentiment for different aspects of the same entity. Hence, a more fine-grained approach, known as Aspect-Based Sentiment Analysis (ABSA), is proposed. The goal is to correctly identify the aspects of entities and the polarity expressed for each aspect. The SemEval-2016 Aspect Based Sentiment Analysis (SE-ABSA16) task is a continuation of the same task in 2015  (Pontiki et al., 2015) . Besides sentence-level ABSA (Subtask 1), it provides datasets to allow participants to work on text-level ABSA (Subtask 2). In addition, additional datasets in languages other than English are available  (Pontiki et al., 2016) . We participate in Subtask 1 of SE-ABSA16, where we submitted results for Slot 1 (aspect category classification), Slot 2 (opinion target extraction), and Slot 1 & 2 (assessing whether a system correctly identifies both Slot 1 and Slot 2) for the English datasets. Our work is based on our previous machine learning system described in  Toh and Su (2015) , enhanced using additional features learned from neural networks. For Slot 1, we treat the problem as a multi-class classification problem where aspect categories are predicted via a set of binary classifiers. The one-vs-all strategy is used to train a binary classifier for each category found in the training data. Each classifier is trained using a single layer feedforward network. We enhance the system by adding neural network features learned from a Deep Convolutional Neural Network system. For Slot 2, we treat the problem as a sequential labeling task, where sequential labeling classifiers are trained using Conditional Random Fields (CRF). The output of a Recurrent Neural Network system is used as additional features. To generate Slot 1 & 2 predictions, the predictions of Slot 1 and Slot 2 are combined. The remainder of this paper is organized as follows. In Section 2, the features used in our system are described. Section 3 presents the detailed machine learning approaches. Section 4 and Section 5 show the official evaluation results and feature ablation results respectively. Finally, Section 6 summa-rizes our work. 

 Features Our system used a variety of features which are briefly described in the following subsections. Most of the features used are the same as the features used in  Toh and Su (2015) . 

 Word Each word in a sentence is used as a feature. Additional word context is used for different slots: for Slot 1, all word bigram context found in a sentence are also used; for Slot 2, the previous word and next word context are also used. 

 Name List Two name lists of opinion targets are generated from the training data of the restaurant domain. One list contains opinion targets that frequently occur in the training data. The other list contains words that often occur as part of an opinion target in the training data. 

 Head Word For each word, the head word is extracted from the sentence parse tree and is used as a feature. 

 Word Embeddings Word embeddings have shown previously to be beneficial to opinion target extraction, requiring only minimal feature engineering effort  (Liu et al., 2015) . We trained word embeddings from two unlabeled datasets: the Multi-Domain Sentiment Dataset containing product reviews from Amazon  (Blitzer et al., 2007)  1 , and the user reviews found in the Yelp Phoenix Academic Dataset 2 . Additional word embeddings are also generated from the concatenation of the above two datasets. Two different approaches are used to train the word embeddings. The first approach uses the gensim 3 implementation of the word2vec tool  (Mikolov et al., 2013)  4 . We experiment with different vector sizes, window sizes, minimum occurrences and subsampling thresholds. The second approach uses the GloVe tool  (Pennington et al., 2014)    5  . By varying the minimum count, window size and vector size, different embedding files are generated. The best embedding files to use are selected using 5-fold cross validation. 

 Word Cluster We further processed the embedding files described in Section 2.4 by generating K-means clusters from them. Specifically, the K-means clusters are generated using the K-means implementation of Apache Spark MLlib 6 . Different cluster sizes are tried out and the best cluster files are selected using 5-fold cross validation. 

 Double Propagation Name List Besides using the training data to generate name lists, we used the unsupervised Double Propagation (DP) algorithm  (Qiu et al., 2011)  to generate candidate opinion targets and collect them into a list. We adjust the logical rules stated in  to derive our own propagation rules written in Prolog. The SWI-Prolog 7 is used as the solver. One issue with our rules is that it can only identify single-word targets. Thus, we check each identified target and include any consecutive noun words right before the target. 

 Approaches This section describes our approaches used to generate the predictions for the different slots. The machine learning system is based on our previous work  (Toh and Su, 2015)  and is extended to use additional neural network features. 

 Aspect Category Classification (Slot 1) For each category found in the training data, a binary classifier is trained using the Vowpal Wabbit tool 8 , which provides the implementation of the single layer feedforward network algorithm that we use. Besides using the features reported previously, we enhance our existing system by using additional features from a deep learning system described below. The deep learning system is based on the Deep Convolutional Neural Network (CNN) architecture described in  Severyn and Moschitti (2015) . The architecture we use is shown in Figure  1 . 

 Max-Pooling Layer A sentence matrix S ? R |s|?d is built for each input sentence s, where each row i is a vector representation of the word i in the sentence. The sentence length |s| is fixed to the maximum sentence length of the dataset so that all sentence matrices have the same dimensions. (Shorter sentences are padded with row vectors of 0s accordingly.) Each row vector of the sentence matrix is made up of columns corresponding to different input features (e.g. word embedding feature, name list feature, etc.) concate-nated together 9 . The input sentence matrix S is then passed through a series of network layer transformations, described in the following subsections. 

 Convolutional Layer We apply a convolution operation between the input sentence matrix S and a filter matrix F ? R m?d of context window size m, resulting in a column vector c ? R |s| . The filter matrix F will slide (with a stride of 1) along the row dimension of S, generating a value for each word in the sentence. Instead of a single filter matrix, n filter matrices are applied to  9  Categorical features are converted to one-hot encodings. the sentence matrix S, resulting in a convolutional feature matrix C ? R |s|?n . To learn non-linear decision boundaries, each element of C passes through the hyperbolic tangent tanh activation function. 

 Max-Pooling Layer The output matrix C is then passed to the maxpooling layer. This layer will return the maximum value of each column. 

 Hidden Dense Layer A hidden dense layer with h hidden units is applied to the output of the pooling layer, using Rectified Linear Unit (ReLU) as the activation function. 

 Softmax Layer A softmax layer receives the output of the previous dense layer and computes probability distribution over the possible categories. We include an additional category for the case where the sentence contains no aspect category. Since a sentence may contain more than one category, we output the categories whose output probability value is greater than a threshold t. 

 Network Training and Regularization The stochastic gradient descent (SGD) algorithm is used to train the CNN network, using the backpropagation algorithm to compute the gradients. We run SGD for e epochs, where a batch size of b sentences is used. The categorical cross-entropy is used as the loss function. To prevent overfitting, the loss function is augmented with a L2 regularization term (l 2 ) for the parameters of the network. The Adadelta update function (with a specific decay rate ? and constant ) is used to control the learning rate. The specific values used for the hyperparameters of the network are tuned using 5-fold crossvalidation. The context window size m is set to 5. The number of filter matrices n is set to 300. The probability threshold t is set to 0.2. The number of hidden units h is set to 100. The number of epochs e is set to 50 and 100 for the restaurant and laptop domain respectively. The L2 regularization term l 2 is set to 0.01. The Adadelta decay rate ? and constant is set to 0.95 and 1e ?6 respectively.  

 Slot 1 Features Besides the features described in Section 2, the probability output of the CNN system is used as additional features to our multi-class classification system. The CNN system is trained on the following input features: Word Embeddings, Name List (only for the restaurant domain) and Word Cluster. We performed 5-fold cross-validation experiments to obtain performances of the system after adding each feature group. Table  1  shows the experimental results. We also include the 5-fold cross-validation performances if we only use the CNN system output for evaluation (last row). For both domains, the CNN system achieves better performances than the multi-class classification system without the neural network features. However, the best performances are achieved when we used the CNN probability output as additional features to the multi-class classification system. This suggests our approach of combining two different machine learning systems is a feasible approach for the task. 

 Opinion Target Extraction (Slot 2) We treat opinion target extraction as a sequential labeling task. The sequential labeling classifiers are trained using Conditional Random Fields (CRF). Such approach is similar to previous work that achieves state-of-the-art performances  (Toh and Su, 2015) . The implementation of CRF is provided by the CRFsuite tool  (Okazaki, 2007) . Similar to our previous work, for different evaluations involving Slot 2, we train different models. For Slot 1 & 2 evaluation (multi setting), the explicit opinion targets may be classified under more than one category. Thus, a separate CRF model is trained for each category C found in the training data, where each model is trained using the corresponding BIO labels: "B-C", "I-C" and "O" (corresponding to start of an opinion target, continuation of an opinion target and outside respectively). For Slot 2 evaluation (single setting), only the target span is required. Thus, all categories are collapsed into a single category (e.g. "TARGET"). A single CRF model is trained using the labels "B-TARGET", "I-TARGET" and "O". We also enhance our existing CRF system by using the output of a Recurrent Neural Network (RNN) system as additional features. Specifically, we implement the Bidirectional Elman-type RNN model described in  Liu et al. (2015)  10 . Such a model allows long-range dependencies from the future as well as from the past to be captured, which are beneficial for sequential labeling tasks. The last layer of the model is a fully connected softmax layer to allow the model to output probabilities. 

 Network Training and Regularization The RNN network is trained using SGD for 20 epochs, using Nesterov momentum with a learning rate of 0.05 and momentum of 0.9 and a batch size of 100 sentences. The categorical cross-entropy is used as the loss function, with L2 penalty of 0.01 for regularization. The number of hidden cell units for both directions is set to 250.  

 Restaurant 

 Slot 2 Features Besides the features described in Section 2, the probability output of the RNN system is used as additional features to our CRF system. The RNN system is trained on the following input features: Word Embeddings, Name List and Word Cluster. We performed 5-fold cross-validation experiments to obtain performances of the system after adding each feature group. Table  2  shows the experimental results. We tune the system for two different settings: Slot 2 predictions used for Slot 1 & 2 evaluation (multi setting), and Slot 2 predictions used for Slot 2 evaluation (single setting). 

 Slot 1 & 2 To generate the predictions for Slot 1 & 2 evaluation, we combine Slot 1 and Slot 2 predictions together. First, we use all Slot 2 predictions used for Slot 1 & 2 evaluation (multi setting). This covers the cases for explicit targets. To include NULL targets, we check the Slot 1 predictions for categories that are not found in the Slot 2 predictions above. These categories are assumed to belong to NULL targets.   

 Results We participated in both unconstrained and constrained settings for the English datasets. Table  3  presents the official results of our submission. For comparison, the top three performing systems and baseline results are included  (Pontiki et al., 2016) . As shown from the table, our system is ranked 1st for all four evaluations we participated (Slot 1 for both restaurant and laptop domains, Slot 2 and Slot 1 & 2 for the English datasets). Similar to previous observation, the constrained systems achieved lower results than the corresponding unconstrained systems, demonstrating the use of external resources are helpful for the task. 

 Feature Ablation The feature ablation experimental results are shown in Table  4  (Slot 1) and Table  5  (Slot 2). The neural network features contributed the most performance gains. However, using the Name List and Word Cluster features do not seem to be particularly effective on the testing data: There are negligible or negative performance gains for Slot 1. As these two features are also used in the CNN system, it may be redundant to include them again in the multiclass classification system. In addition, the neural network features may have become the dominant features during training, affecting the usefulness of other features. Further investigation may be needed to identify better ways of combining the different machine learning systems together. For example, instead of adding neural network probability output to our multi-class classification system, we could instead add our classifier probability output as additional features to our CNN system. 

 Conclusion In this paper, we describe our system used in classifying aspect categories (Slot 1) and extracting opinion targets (Slot 2). We explore the use of deep learning systems to provide additional neural network features to our existing system. Our system is ranked 1st in the four evaluations on the English datasets. In future, we hope to perform better feature engineering and explore how our deep learning systems can be further enhanced for the task.   Figure 1 : 1 Figure 1: The architecture of our Convolutional Neural Network. 
