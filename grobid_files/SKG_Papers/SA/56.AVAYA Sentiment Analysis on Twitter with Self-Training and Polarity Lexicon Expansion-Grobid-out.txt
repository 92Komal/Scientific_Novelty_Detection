title
AVAYA: Sentiment Analysis on Twitter with Self-Training and Polarity Lexicon Expansion

abstract
This paper describes the systems submitted by Avaya Labs (AVAYA) to SemEval-2013 Task 2 -Sentiment Analysis in Twitter. For the constrained conditions of both the message polarity classification and contextual polarity disambiguation subtasks, our approach centers on training high-dimensional, linear classifiers with a combination of lexical and syntactic features. The constrained message polarity model is then used to tag nearly half a million unlabeled tweets. These automatically labeled data are used for two purposes: 1) to discover prior polarities of words and 2) to provide additional training examples for self-training. Our systems performed competitively, placing in the top five for all subtasks and data conditions. More importantly, these results show that expanding the polarity lexicon and augmenting the training data with unlabeled tweets can yield improvements in precision and recall in classifying the polarity of non-neutral messages and contexts.

Introduction The past decade has witnessed a massive expansion in communication from long-form delivery such as e-mail to short-form mechanisms such as microblogging and short messaging service (SMS) text messages. Simultaneously businesses, media outlets, and investors are increasingly relying on these messages as sources of real-time information and are increasingly turning to sentiment analysis to discover product trends, identify customer preferences, and categorize users. While a variety of corpora ex-ist for developing and evaluating sentiment classifiers for long-form texts such as product reviews, there are few such resources for evaluating sentiment algorithms on microblogs and SMS texts. The organizers of SemEval-2013 task 2, have begun to address this resource deficiency by coordinating a shared evaluation task for Twitter sentiment analysis. In doing so they have assembled corpora in support of the following two subtasks: Task A -Contextual Polarity Disambiguation "Given a message containing a marked instance of a word or phrase, determine whether that instance is positive, negative or neutral in that context." Task B -Message Polarity Classification "Given a message, classify whether the message is of positive, negative, or neutral sentiment. For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen." This paper describes the systems submitted by Avaya Labs for participation in subtasks A and B. Our goal for this evaluation was to investigate the usefulness of dependency parses, polarity lexicons, and unlabeled tweets for sentiment classification on short messages. In total we built four systems for SemEval-2013 task 2. For task B we developed a constrained model using supervised learning, and an unconstrained model that used semi-supervised learning in the form of self-training and polarity lexicon expansion. For task A the constrained system utilized supervised learning, while the unconstrained model made use of the expanded lexicon from task B. Output from these systems were submitted to all eight evaluation conditions. For a complete description of the data, tasks, and conditions, please refer to  Wilson et al. (2013) . The remainder of this paper details the approaches, experiments and results associated with each of these models. 

 Related Work Over the past few years sentiment analysis has grown from a nascent topic in natural language processing to a broad research area targeting a wide range of text genres and applications. There is now a significant body of work that spans topics as diverse as document level sentiment classification  (Pang and Lee, 2008) , induction of word polarity lexicons  (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2006; Mohammad and Turney, 2011)  and even election prediction  (Tumasjan et al., 2010) . Efforts to train sentiment classifiers for Twitter messages have largely relied on using emoticons and hashtags as proxies of the true polarity  (Barbosa and Feng, 2010; Davidov et al., 2010b; Pak and Paroubek, 2010; Agarwal et al., 2011; Kouloumpis et al., 2011; Mohammad, 2012) . Classification of word and phrase sentiment with respect to surrounding context ) has yet to be explored for the less formal language often found in microblog and SMS text. Semi-supervised learning has been applied to polarity lexicon induction  (Rao and Ravichandran, 2009) , and sentiment classification at the sentence level  (T?ckstr?m and Mc-Donald, 2011)  and document level  (Sindhwani and Melville, 2008; He and Zhou, 2011) ; however to the best of our knowledge self-training and other semi-supervised learning has seen only minimal use in classifying Twitter texts  (Davidov et al., 2010a; Zhang et al., 2012) . 

 System Overview Given our overarching goal of combining polarity lexicons, syntactic information and unlabeled data, our approach centered on first building strong constrained models and then improving performance by adding additional data and resources. For both tasks, our data-constrained approach combined standard features for document classification conj ? conj conjunction pobj ? prep preposition pcomp ? prepc preposition prep|punct|cc ? ? with dependency parse and word polarity features into a weighted linear classifier. For our dataunconstrained models we used pointwise mutual information for lexicon expansion in conjunction with self-training to increase the size of the feature space. 

 Preprocessing and Text Normalization Our systems were built with ClearTK  (Ogren et al., 2008)  a framework for developing NLP components built on top of Apache UIMA. Our preprocessing pipeline utilized ClearTK's wrappers for  ClearNLP's (Choi and McCallum, 2013)  tokenizer, lemmatizer, part-of-speech (POS) tagger, and dependency parser. ClearNLP's ability to retain emoticons and emoji as individual tokens made it especially attractive for sentiment analysis. POS tags were mapped from Penn Treebank-style tags to the simplified, Twitter-oriented tags introduced by  Gimpel et al. (2011) . Dependency graphs output by ClearNLP were also transformed to the Stanford Collapsed dependencies representation (de Marneffe and Manning, 2012) using our own transformation rules (table  1 ). Input normalization consisted solely of replacing all usernames and URLs with common placeholders. 

 Sentiment Resources A variety of our classifier features rely on manually tagged sentiment lexicons and word lists. In particular we make use of the MPQA Subjectivity Lexicon  as well as manually-created negation and emoticon dictionaries 1 . The negation word list consisting of negation words such as no and not. Because tokenization splits contractions, the list includes the sub-word token n't as well as the apostrophe-less version of 12 contractions (e.g. cant, wont, etc . . . ). To support emoticon-specific features we created a dictionary, which paired 183 emoticons with either a positive or negative polarity. 6 Message Polarity Classification 

 Features Polarized Bag-of-Words Features: Instead of extracting raw bag-of words (BOW), we opted to integrate negation directly into the word representations following the approaches used by  Das and Chen (2001)  and  Pang et al. (2002) . All words between a negation word and the first punctuation mark after the negation word were suffixed with a NOT tag -essentially doubling the number of BOW features. We extended this polarized BOW paradigm to include not only the raw word forms but all of the following combinations: raw word, raw word+PTB POS tag, raw word+simplified POS tag, lemma+simplified POS tag. Word Polarity Features: Using a subjectivity lexicon, we extracted features for the number of positive, negative, and neutral words as well as the net polarity based on these counts. Individual word polarities were inverted if the word had a child dependency relation with a negation (neg) label. Constrained models use the MPQA lexicon, while unconstrained models use an expanded lexicon that is described in section 6.2. Emoticon Features: Similar to the word polarity features, we computed features for the number of positive, negative, and neutral emoticons, and the net emoticon polarity score. Microblogging Features: As noted by  Kouloumpis et al. (2011) , the emotional intensity of words in social media messages is often emphasized by changes to the word form such as capitalization, character repetition, and emphasis characters (asterisks, dashes). To capture this intuition we compute features for the number of fully-capitalized words, words with characters repeated more than 3 times (e.g. booooo), and words surround by asterisks or dashes (e.g. *yay*). We also created a binary feature to indicate the presence of a winning score or winning record within the target span (e.g. Oh yeah #Nuggets 15-0). Part-of-Speech Tag Features: Counts of the Penn Treebank POS tags provide a rough measure of the content of the message. Syntactic Dependency Features: We extracted dependency pair features using both standard and collapsed dependency parse graphs. Extracted head/child relations include: raw word/raw word, lemma/lemma, lemma/simplified POS tag, simplified POS tag/lemma. If the head node of the relation has a child negation dependency, the pair's relation label is prefixed with a NEG tag. 

 Expanding the Polarity Lexicon Unseen words pose a recurring challenge for both machine learning and dictionary-based approaches to sentiment analysis. This problem is even more prevalent in social media and SMS messages where text lengths are often limited to 140 characters or less. To expand our word polarity lexicon we adopt a framework similar to the one introduced by Turney (2002). Turney's unsupervised approach centered on computing pointwise mutual information (PMI) between highly polar seed words and bigram phrases extracted from a corpus of product reviews. Instead of relying solely on seed words for polarity, we use the constrained version of the message polarity classifier to tag a corpus of approximately 475,000 unlabeled, English language tweets. These tweets were collected over the period from November 2012 to February 2013. To reduce the number of noisy instances and to obtain a more balanced distribution of sentiment labels, we eliminated all tweets with classifier confidence scores below 0.9, 0.7, and 0.8 for positive, negative and neutral instances respectively. Applying the threshold, reduced the tweet count to 180,419 tweets (50,789 positive, 59,029 negative, 70,601 neutral). This filtered set of automatically labeled tweets was used to accumulate co-occurrence statistics between the words in the tweets and their corresponding sentiment labels. These statistics are then used to compute word-sentiment PMI (equation 1), which is the joint probability of a word and sentiment cooccurring divided by the probability of each of the events occurring independently. A word's net polarity is computed as the signum (sgn) of the difference between a its positive and negative PMI values (equation 2). It should be noted that polarities were deliberately limited to values of {-1, 0, +1} to ensure consistency with the existing MPQA lexicon, and to dampen the bias of any single word.  Words with fewer than 10 occurrences, words with neutral polarities, numbers, single characters, and punctuation were then removed from this PMIderived polarity dictionary. Lastly, this dictionary was merged with the dictionary created from the MPQA lexicon yielding a final polarity dictionary with 11,740 entries. In cases where an entry existed in both dictionaries, the MPQA polarity value was retained. This final polarity dictionary was used by the unconstrained models for task A and B. Unconstrained Model: In addition to using the expanded polarity dictionary described in 6.2 for feature extraction, the unconstrained model also makes use of automatically labeled tweets for self-training  (Scudder, 1965) . In contrast to preparation of the expanded polarity dictionary, the self-training placed no threshold on the examples. Combining the selflabeled tweets, with the official training and development set yielded a new training set consisting of 485,112 examples. Because the self-labeled instances were predominantly tagged neutral, the LI-BLINEAR cost parameters were adjusted to heavily discount neutral while emphasizing positive and neutral instances. The size and cost of training this model prevented extensive parameter tuning and instead were chosen based on experience with the constrained model and to maximize recall on positive and negative items. Final parameters for the unconstrained model were cost c = 1 and category weights w positive = 2, w negative = 5, and w neutral = 0.1. 

 Contextual Polarity Disambiguation 

 Features The same base set of features used for message polarity classification were used for the contextual polarity classification, with the exception of the syntactic dependency features. To better express the incontext and out-of-context relation these additional feature classes were added: Scoped Dependency Features: Because this task focuses on a smaller context within the message, collapsed dependencies are less useful as the compression may cross over context boundaries. Instead the standard syntactic dependency features described above were modified to account for their relation to the context. All governing relations for the words contained within the contact were extracted. Relations wholly contained within the boundaries of the context were prefixed with an IN tag, whereas those that crossed outside of the context were prefixed with an OUT tag. Additionally counts of IN and OUT relations were included as features. Dependency Path Features: Like the single dependency arcs, a dependency path can provide additional information about the syntactic and semantic role of the context in the sentence. Our path features consisted of two varieties: 1) POS-path and 2) Sentiment-POS-path. The POS-path consisted of the PTB POS tags and dependency relation labels for all nodes between the head of the context and the root node of the parent sentence. The Sentiment-POS-path follows the same path but omits the dependency relation labels, uses the simplified POS tags and appends word polarities (POS/NEG/NTR) to the POS tags along the path. For example given the bold-faced context in the sentence: @User Criminals killed Sadat, and in the process they killed Egypt. . . they destroyed the future of young & old Egyptians.. the extracted POS-path feature would be: {NNP} dobj <{VBD} conj <{VBD} ccomp <{VBD} root <{TOP} while the Sentiment-POS path would be: {?/pos}{V/neg}{V/neg}{V/neg}{TOP}. Paths with depth greater than 4 dependency relations were truncated to reduce feature sparsity. In addition to these detailed path features, we include two binary features to indicate if any part of the path contains subject or object relations. 

 Model Parameters and Training Like with message polarity classification, the contextual polarity disambiguation systems rely on LI-BLINEAR's L2 regularized logistic regression for model training. Both constrained and unconstrained models use identical parameters of cost c = 1 and weights w positive = 1, w negative = 2, and w neutral = 1. They vary only in the choice of polarity lexicon. The constrained model uses the MPQA subjectivity lexicon, while the unconstrained model uses the expanded dictionary derived via computation of PMI, which ultimately differentiates these models through the variation in the sentiment path and word polarity features. 

 Experiments and Results In this section we report results for the series of Sentiment Analysis in Twitter tasks at SemEval 2013. Please refer to refer to  Wilson et al. (2013)  for the exact details about the corpora, evaluation conditions, and methodology. We submitted polarity output for the Message Polarity Classification (task B) and the Contextual Polarity Disambiguation (task A). For each task we submitted system output from our constrained and unconstrained models. As stated above, the constrained models made use of only the training data released for the task, whereas the unconstrained models trained on additional tweets. Each subtask had two test sets one comprised of tweets and the other comprised of SMS messages. Final task 2   

 Error Analysis To better understand our systems' limitations we manually inspected misclassified output. Table  4  lists errors representative of the common issues uncovered in our error analysis. Though some degree of noise is expected in sentiment analysis, we found several instances of annotation error or ambiguity where it could be argued that the system was actually correct. The message in #1 was annotated as neutral, whereas the presence of the word "yay" suggests an overall positive polarity. The text in #2 could be interpreted as positive, negative or neutral depending on the author's disposition. Unseen vocabulary and unexpected usages were the largest category of error. For example in #3 "crashed" means to attend without an invitation instead of the more negative meaning associated with car accidents and airplane failures. Although POS features can disambiguate word senses, in this case more sophisticated features for word sense disambiguation could help. While the degradation in performance between the Tweet and SMS test sets might be explained by differences in medium, errors like those found in #4 and #5 suggest that this may have more to do with the dialectal differences between the predominantly American and British English found in the Tweet test set and the Colloquial Singaporean English (aka Singlish) found in the SMS test set. Error #6 illustrates both how hashtags composed of common words can easily become a problem when assigning a polarity to a short context. Hashtag segmentation presents one possible path to reducing this source of error. 

 Conclusions and Future Work The results and rankings reported in section 8 suggest that our systems were competitive in assigning sentiment across the varied tasks and data conditions. We performed particularly well in disambiguating contextual polarities finishing second overall on the Tweet test set. We hypothesize this performance is largely due to the expanded vocabulary obtained via unlabeled data and the richer syntactic context captured with dependency path representations. Looking forward, we expect that term recall and unseen vocabulary will continue to be key challenges for sentiment analysis on social media. While larger amounts of data should assist in that pursuit, we would like to explore how a more iterative approach to self-training and lexicon expansion may provide a less noisy path to attaining such recall. ) = sgn(P M I(word, positive)? P M I(word, negative)) 

 6. 3 3 Model Parameters and Training Constrained Model: Models were trained using the LIBLINEAR classification library (Fan et al., 2008). L2 regularized logistic regression was chosen over other LIBLINEAR loss functions because it not only gave improved performance on the development set but also produced calibrated outcomes for confidence thresholding. Training data for the constrained model consisted of all 9829 examples from the training (8175 examples) and development (1654 examples) set released for SemEval 2013. Cost and label-specific cost weight parameters were selected via experimentation on the development set to maximize the average positive and negative F 1 values. The c values ranged over {0.1, 0.5, 1, 2, 5, 10, 20, 100} and the label weights w polarity ranged over {0.1, 1, 2, 5, 10, 20, 25, 50, 100}. Final parameters for the constrained model were cost c = 1 and weights w positive = 1, w negative = 25, and w neutral = 1. 

 Table 1 : 1 Collapsed Dependency Transformation Rules 

 Table 3 : 3 Contextual Polarity Disambiguation (Task A) Results System P Positive R F P Negative R F P Neutral R F Favg +/- Rank Tweet NRC-Canada (top) AVAYA-Unconstrained 0.751 0.655 0.700 0.608 0.557 0.582 0.665 0.768 0.713 0.641 0.814 0.667 0.733 0.697 0.604 0.647 0.677 0.826 0.744 0.690 AVAYA-Constrained 0.791 0.580 0.669 0.593 0.509 0.548 0.636 0.832 0.721 0.608 1 5 12 Mean of submissions 0.687 0.591 0.626 0.491 0.456 0.450 0.612 0.663 0.615 0.538 - NRC-Canada (top) 0.731 0.730 0.730 0.554 0.754 0.639 0.852 0.753 0.799 0.685 1 SMS AVAYA-Constrained AVAYA-Unconstrained 0.609 0.659 0.633 0.494 0.637 0.557 0.814 0.710 0.759 0.595 0.630 0.667 0.648 0.526 0.581 0.553 0.802 0.756 0.778 0.600 4 5 Mean of submissions 0.512 0.620 0.546 0.462 0.518 0.456 0.754 0.578 0.627 0.501 - Table 2: Message Polarity Classification (Task B) Results System P Positive R F P Negative R F P Neutral R F Favg +/- Rank Tweet NRC-Canada (top) AVAYA-Unconstrained 0.892 0.905 0.898 0.834 0.865 0.849 0.539 0.219 0.311 0.874 0.889 0.932 0.910 0.866 0.871 0.869 0.455 0.063 0.110 0.889 AVAYA-Constrained 0.882 0.911 0.896 0.844 0.843 0.843 0.493 0.225 0.309 0.870 1 2 3 Mean of submissions 0.837 0.745 0.773 0.745 0.656 0.677 0.159 0.240 0.115 0.725 - GUMLTLT (top) 0.814 0.924 0.865 0.908 0.896 0.902 0.286 0.050 0.086 0.884 1 SMS AVAYA-Unconstrained 0.815 0.871 0.842 0.853 0.896 0.874 0.448 0.082 0.138 0.858 AVAYA-Constrained 0.777 0.875 0.823 0.859 0.852 0.856 0.364 0.076 0.125 0.839 3 4 Mean of submissions 0.734 0.722 0.710 0.807 0.663 0.698 0.144 0.184 0.099 0.704 - 

 Eric Decker catches his second TD pass from Manning. This puts Broncos up 31-7 with 14:54 left in the 4th. 3 -/ So, crashed a wedding reception and Andy Lee's bro was in the bridal party. How'd you spend your Saturday night? #longstory 4 -+ Aiyo... Dun worry la, they'll let u change one... Anyway, sleep early, nite nite... S G Message / Context 1 + / Going to Helsinki tomorrow or on the day after tomorrow,yay! 2 / + 5 + - Sori I haven't done anything for today's meeting.. pls pardon me. Cya guys later at 10am. 6 + - these PSSA's are just gonna be the icing to another horrible monday. #fmlll #ihateschool 

 Table 4 : 4 Example Classification Errors: S=System, G=Gold, +=positive, ?=negative, /=neutral. Bold-faced text indicates the span for contextual polarities.evaluation is based on the average positive and negative F-score. Task B results are listed in table 2, and task A results are shown in table 3. For comparison these tables also include the top-ranked system in each category as well as the mean scores across all submissions. 

			 http://leebecker.com/resources/semeval-2013
