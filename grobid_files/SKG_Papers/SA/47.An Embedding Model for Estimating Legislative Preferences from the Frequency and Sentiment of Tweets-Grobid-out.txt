title
An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets

abstract
Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators' political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting. Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets. To illustrate our method, we model legislators' attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets. We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018. We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.

Introduction Legislator preferences are typically estimated as general measures of ideology using roll-call votes on legislation. However, such measures fail to capture aspects of preferences not reflected in legislation, such as attitudes towards a sitting president. For instance, Sen. Bob Corker (R-TN) famously referred to the Trump White House as an "adult day-care center," John McCain (R-AZ) said Trump "is often poorly informed,"and Jeff Flake (R-AZ) called him a "danger to a democracy," yet all of these Republican Senators cast more than 80% of their legislative votes in line with president  (Silver and Bycoffe, 2019) . Generally, the political science research recognizes that the public's views of the president have spillover effects on evaluations of legislators, which incentivizes strategic communication about the president. For example, Senate Majority Leader Mitch McConnell recently encouraged Republican senators in vulnerable re-election campaigns to distance themselves from Trump. Understanding legislators' attitudes toward the president enables greater understanding and measurement of such strategic communications. Furthermore, these attitudes also matter for understanding the president's ability to pass his legislative agenda. In this paper, we propose a new method for estimating legislator preferences from the frequency and sentiment of their tweets using a novel combination of spatial models based on item response theory and the modeling of count data. We use this method to estimate legislator preferences about Donald Trump using tweets by members of Congress and Donald Trump in the 15-month period following election day in November 2016. In our model, legislator embeddings interact with embedding representations of Donald Trump himself, constructed from a neural network using the text (and timing) of his tweets during the same time frame. Thus, our model leverages the text feature extraction capabilities of neural networks and incorporates the legislator sentiment in tweets about Trump as well as the strategic decision about whether and when to tweet about him. We quantitatively assess the quality of our learned legislator representations by demonstrating the model's predictive performance on a test set of tweets, and we also compare our model-obtained embeddings to DW-NOMINATE scores, traditional measures of legislator ideology.  1  Our analysis not only validates the modeling approach but also highlights that attitudes towards Trump are not being entirely captured by legislative voting behavior. More broadly, a method for estimating domain-specific preferences, rather than general ideological ideal points, broadens the range of hypotheses than can be tested by political researchers. 

 Measuring Legislator Preferences The predominant method of measuring legislator preferences over the past half-century has been the modeling of the ideal point of a legislator from recorded votes on policy legislation. These ideal points constitute a spatial model for legislative behavior, as both legislators and policies are represented in a low-dimensional Euclidean space  (Poole and Rosenthal, 1997; Clinton et al., 2004) . Such ideal points are interpreted as measures of ideological preferences and have been used to test hypotheses on topics such as political polarization, political representation, and cross-institutional relationships  (Tausanovitch and Warshaw, 2018) . A key limitation of initial methods for estimating ideal points was the inability to perform out-ofsample predictions. Thus, they could not be used to predict votes on new legislation. To address this shortcoming,  Gerrish and Blei (2011)  extended the ideal point model by placing legislation into a "political space" based upon the latent topics of the legislation's text and perform prediction using these topics.  Xing et al. (2017)  use a nonparametric Bayesian model to incorporate constituency data into a factor model for legislative roll calls and text, with the text again being analyzed using a topic model. Further efforts to incorporate bill text using topic models come from  Wang et al. (2010 ), Gerrish and Blei (2012 ), Nguyen et al. (2015 , and  Gu et al. (2014) . The incorporation of text into ideal point modeling is not limited to legislators:  Sim et al. (2016)  model U.S. Supreme Court behavior using a generative model for amicus briefs. Efforts to incorporate text into vote prediction were improved by moving to an embedding paradigm rather than topic models.  Kraft et al. (2016)  incorporate word embeddings into a model for vote prediction by representing a piece of legislation as the average of its word embeddings and further representing legislators using ideal vectors as a multi-dimensional extension to ideal points.  Kornilova et al. (2018)  augment bill text with bill metadata (i.e., bill sponsor information) to improve the predictive capabilities of legislator embeddings, and use a convolutional neural network (CNN,  Kim (2014) ), rather than the average over bill word em-beddings, to model bill text. While tweets have increasingly been used to measure political preferences of the mass public  (Wang et al., 2016; Preot ?iuc-Pietro et al., 2017) , little attention has been paid to the potential of using tweets to measure legislators' preferences. One notable exception,  Barbera (2015) , uses the structure of social networks on Twitter to learn ideological positions of both political elites and the general public, but does not incorporate information from the tweets themselves. As all legislators in the U.S. House and Senate now use Twitter to communicate with constituents on a wide variety of topics, we recognize an opportunity to observe nuances in legislator preferences not captured by broader ideological measures that rely on roll call votes. Here we focus specifically on legislators' attitudes toward the sitting president. While attitudes toward the president are among the most frequently measured aspects of public opinion, there is currently no method for explicitly measuring these preferences among legislators. We develop an embedding model that jointly predicts the frequency and sentiment of legislator tweets about Donald Trump. Similar to the  Kraft et al. (2016)  modeling of legislator votes in response to the text of legislation, here legislator tweets are considered as a response to text features extracted from Donald Trump's tweets. Whereas embedding models for vote prediction analyze only one outcome of legislator behavior (i.e., the vote itself), our embedding model is trained to predict multiple outcomes of legislator behavior in both tweet counts and content in the form of sentiment. Moreover, because our model does not rely on votes casts by legislators, it could be used to estimate preferences among a wider range of political actors (e.g., candidates, cabinet members) on a variety of domains, and with texts other than tweets. 

 Tweet Dataset We obtained all publicly-available tweets by members of Congress from TweetCongress, a Sunlight Foundation initiative. We restricted the sample to only those tweets that contained any of a specific set of terms related to Donald Trump (in addition to his Twitter handle): "Donald Trump," "Trump," "realDonaldTrump," "MAGA," (an acronym for Trump's campaign slogan "Make America Great Again") "whitehouse," "WhiteHouse," "POTUS," (acronym for "President of the United States"), and "potus." Of these, we further restricted the tweets to span in time from November 2016 to February 2018, when the data was collected. This culling process yielded 29,696 tweets from 451 legislators. The model also incorporates tweets from Trump, which we obtained from the website www.trumptwitterarchive.com. For each day included in the dataset, the text of all tweets by Donald Trump was agglomerated and preprocessed by removing excess whitespace and lowercasing all letters. The text was tokenized and each word-token mapped to an integer identifier, with a vocabulary mapping of 2783 words. For each day, we obtain a sequence of integers representing the words composing the text of Donald Trump's tweets from that day, and these are the inputs to the model described in Section 4.3. Figure  1  plots the number of tweets by Republicans, Democrats, and Trump over time for the period we examine. There were only 13 days for which Trump did not tweet (2.79%), and the most tweets that he sent in a single day was 32. The most tweets by a Democrat in a single day was 99, while the most tweets by a Republican in a single day was 25. The variation in tweets across time highlights one of the key features of the model-the incorporation of not only the sentiment of tweets about Trump by also the number of daily tweets. Of the 29,696 Trump-related legislator tweets, a subset of 4,661 tweets were randomly selected to be manually labeled with respect to their sentiment about Trump, using a three-point "positive," "negative," "neutral" scale based on the text of the tweet 2 from November 2016 to February 2018. We divided the tweets temporally by day into disjoint training, validation, and test sets, such that all tweets from each day were randomly assigned to one of the three sets. The training, validation, and test sets contain 70%, 10%, and 20% of all days, respectively. Underpinning our model is the assumption of a latent political space of dimension K. In this space, we learn a set of "day embeddings" (or "Trump embeddings") that interact with a set of legislator embeddings. For a particular day t, let ? t ? R K be a vector that represents Donald Trump on that day. Indexing legislators by i ? {1, 2, . . . , N}, we endow a legislator i with a vector v i ? R K as well as a bias term b i ? R, which captures a legislator's propensity to react to Trump regardless of how he presents himself via Twitter. While the legislator embeddings are learned as free parameters of the model, the Trump embeddings are constructed using the text of Donald Trump's tweets. We now describe how we use legislator and Trump embeddings to predict tweet counts and sentiment. 

 Tweet Count Model Understanding legislator-president interactions requires understanding not only the sentiment of legislators' remarks about the president, but also whether and how often they remark about him. This distinguishes, for instance, a legislator who criticizes Trump every week from one who criticizes Trump only once during his tenure. Even when the sentiment expressed in these two legislators' tweets is identical, the fact that one legislator expresses that sentiment more frequently likely reflects a more negative attitude toward Trump. Furthermore, since we model tweets as a response to a daily representation of Trump, modeling counts reveals legislators who respond in concert with each other and may share similar preferences. Let x it be the number of tweets that legislator i sends about Donald Trump on day t. We consider two distributions with which to construct our tweet count model: Poisson and Negative Binomial. While the former offers simplicity, the latter is more flexible and suitable for overdispersed data because of its additional parameter. In Section 5, we compare the Poisson and Negative Binomial model performances. We will parameterize the Negative Binomial using (p it , r): x it ? NegBin(p it , r), p it = ?(? t v i ) (1) where ?(?) is the sigmoid function defined by ?(x) = 1 1+exp(?x) , which is used to transform the input onto (0, 1) to represent a probability. The remaining parameter r is learned as a common free parameter for all legislators and days. For the Poisson case, we model the rate parameter of the distribution as the exponential of the dot-product between the Trump and legislator embeddings. This choice ensures the rate parameter is non-negative while also modeling an "interaction" between legislators and Trump. We train the count model by minimizing the negative log-likelihood (NLL) of the training data under either of the assumed distributions. We denote the total count-loss over a training set X tr as: L count = x it ?Xtr NLL count (x it ; ? t , v i ) (2) 

 Tweet Sentiment Model Let y it be an ordinal variable that encodes the sentiment legislator i expresses in a tweet about Donald Trump on day t. We consider an ordinal model to account for the possible gradations of approval. Assuming L sentiment levels, the model is parameterized by a set of cutpoints, C = {c 0 < c 1 ? c 2 ? ? ? ? ? c L?1 < c L } , where c 0 and c L are defined to be ? and ?, respectively. The remaining cutpoints are learned during model training. Let z it ? R be a latent variable underlying the ordinal response. Then for a thresholded ordinal model, the predicted sentiment takes value l for which: c l?1 < z it < c l . Under a cumulative link model (CLM) 3 for ordinal regression, the predicted probability of a particular sentiment level l is: p(y it = l|z it ; C) = ?(c l ?z it )?(c l?1 ?z it ) (3) where again ?(?) is the sigmoid function. The latent variable z it is a function of the attributes of legislator i and of Trump at day t. As with the count model, we seek to employ a map that captures the interaction between the legislator and Trump embeddings, and thus we employ a weighted inner product. Additionally, we expect that legislators maintain a concrete bias towards Trump, which we include in the term b i for each legislator. Thus, we obtain the variable z it through the following map: z it = g(v i , ? t , b i ) = ? t H g v i + b i (4) where H g ? R K?K is a learned weight matrix. As with the count model, the sentiment model is trained by optimizing the negative log-likelihood of the sentiment-labeled tweets in the training set. With the predicted probability of the correct label, p(y it = l), given by equation 3, and the set of all labeled tweets in the training set being Y tr then the total loss for the sentiment model is given by: L sent = y it ?Ytr l?{1,2,...L} ?I(y it = l)log p(y it = l) (5) where I(?) denotes the indicator function, in which I(?) = 1 when the argument is true and 0 otherwise. 

 Trump Embedding Construction In the ideal point/vector models that consider roll call data, legislator behavior is a response to policies as captured by the text of bills. As we seek an alternative to legislation as a method of measuring preferences, we rely instead on Twitter behavior but similarly construct embeddings that legislators respond to. Since Donald Trump is our entity of investigation, we use the text of his tweets to construct such embeddings. To map Donald Trump's tweet text to a political embedding representation, we employ a Simple Word-Embedding Model (SWEM),  (Shen et al., 2018) . SWEMs rely upon word embeddings  (Bengio et al., 2003; Mikolov et al., 2013)  and pooling operations to encode the compositionality of text without the heavy parameterization required of such models as recurrent neural networks (RNNs, see  Socher et al., 2011)  or CNNs  (Kalchbrenner et al., 2014; Kim, 2014) . Endowing each word-token u i in a lexicon with an embedding w i ? R d , we may represent a sequence of n words as a matrix of stacked embeddings: {w 1 , . . . , w L } = W ? R n?d . To extract the most salient features from every word-embedding dimension, we employ a max-pooling operation, which amounts to a column-wise maximum of matrix W. Supposing that W t contains the embeddings from all Donald Trump tweets on day t, then we will denote ? t ? R d as the max-pooled vector, These text features are subsequently mapped to the daily Trump vector by an affine transformation: ? t = M? t + a (6) where M ? R d?K and a ? R K are a weight matrix and bias vector that are shared by all days t. This transformation can be made more flexible by introducing a non-linear activation function, ?(?), such as the rectified linear unit (ReLU). This nonlinear "hidden" layer is described by: ? t = M 2 ?(M 1 ? t + a 1 ) + a 2 (7) where an additional weight matrix and bias vector have been appended. 

 Model Training & Parameter Learning The parameters in the model to be learned include the legislator embeddings and biases, the word embeddings, the parameters of the maps to count and ordinal variables, and the parameters of the map from text features to Trump embeddings. We refer to this collection as ?. The optimization objective is the combination loss of the negative-log likelihood of the count and ordinal models: L(?) = ?L count + (1 ? ?)L ord (8) where L count and L ord are given by equations 2 and 5, respectively, and ? is a hyperparameter that controls the relative importance of the two component losses. The construction of equation 8 allows the researcher to only admit tweet count information by setting ? = 1 and only admit tweet sentiment information by setting ? = 0; a balance may be achieved by choosing ? ? (0, 1). The Adam algorithm (Kingma and Ba, 2015) is used for gradient-based optimization of 8 with a learning rate of ? = 10 ?4 . 

 Predictive Results To demonstrate the efficacy of our model for legislator tweeting behavior with respect to President Donald Trump, we first show that the construction of Trump embeddings from the language of his own tweets provides an informational signal for legislators to react to. We train our model using the days for the training set and present the predictive results for days in the test set. Since the model seeks to capture two aspects of legislator tweeting behavior, we evaluate the model using two metrics: the negative-log likelihood of the count model and the mean-absolute-error (MAE) of the sentiment model. Overall model performance is also captured by the total loss of the model, which is the weighted negative-log likelihood of both the count and sentiment models, equation 8. MAE is used rather than accuracy to account for the ordinal nature of the sentiment model. The hyperparameter ? controls the balance between the two components of our model, counts and sentiment. We present our results for three settings of ?, which allows us to analyze the two components of our model separately before analyzing the joint model. A full description of the process used to tune hyperparameters and a comparison of the model with linear and nonlinear text maps can be found in Appendix B. For all results presented here, we set K = 2, and use a linear text map. The number of epochs for which the model was trained varies depending on model setting, but in all cases each training batch comprises 128 tweets. The model was implemented in TensorFlow  (Abadi et al., 2015)  and trained on a single NVIDIA Titan X GPU. Code can be found on the author's Github at: github.com/gspell/CongressionalTweets. 

 ? = 1 (only count model): When ? = 1, only the loss from the part of the model that handles tweet counts contributes to the total loss in equation 8. We present the final negative log-likelihood of the count model for both the Poisson and Negative Binomial models described in Section 4.1, and for both the case in which the text of Donald Trump's tweets is used to construct his daily embedding representation and the case in which the Trump embeddings are free parameters of the model. For the negative binomial model, the model was trained for 75 epochs, which was the amount of training required to perform best on the validation (rather than test) set of tweets. The Poisson model was trained for 100 epochs while using the text and 2000 epochs without text. The predictive results are shown in Modeling legislator tweet counts using the Negative Binomial distribution achieves superior performance to modeling using the Poisson distribution, as the Negative Binomial can better accomodate the overdispersion in the tweet counts. Additionally, using the text of Donald Trump's tweets to construct his daily embedding that legislator embeddings interact with provides significantly better results than neglecting the text and allowing the Trump embeddings to be free parameters of the model. This effect is more pronounced for the Poisson distribution, but is present for the Negative Binomial model as well. Indeed, this aspect of the model is to be expected, since the model is evaluated on days of which there are no examples in the training set. Without using the text, there is no way for the model to represent an "unseen" day. 

 ? = 0 (only sentiment model): When ? = 0, only the loss from the part of the model that handles legislator tweet sentiment contributes to the total loss in equation 8. We present the final model loss -which is the negative loglikelihood of the sentiment model -as well as the model MAE. Again, we show results for the case in which Trump's tweet text is used to construct embeddings and the case in which the text is not used. We also toggle an additional model setting for analysis: the inclusion of the legislator bias term, b i , from equation 4. We adjust the number of epochs to 150 for training with text. We train the model without text for 1000 and 3000 epochs, including and excluding the legislator bias term, respectively. The results are presented in Table  3  In addition to the MAE of the ordinal model, we note that the model accuracy -which is more intuitive but less exact than MAE -is 88.4% for the best performing model, when both the legislator bias and Trump's tweet text are used. Note that when the text of Donald Trump's tweets is used, the model performs as well with respect to MAE with the inclusion of the legislator bias as without it. Additionally, when the bias term is included but Trump's text is excluded, the model is able to achieve better performance than when both the text and bias term are excluded. In fact, for the case of no Trump text and no legislator bias, the model is incapable of achieving test MAE better than how it performs upon initialization. We note that while the model does train, performance on the test (and validation) never improves in that case. Table  3  suggests that the legislator bias (when present) accounts for much of the model's ability to predict legislator tweet sentiment, since the model achieves decent results even when no Trump text is used to construct meaningful Trump embeddings to interact with the trained legislator embeddings. Without the bias term, the interaction between Trump and legislator embeddings is the only means toward predicting tweet sentiment, which is why the necessity of text is so critical in that case. 

 ? = 0.03 (both counts & sentiment): For any other value of ? ? (0, 1), the total loss in equation 8 will have contributions from both the count and sentiment losses, and thus both aspects of the model are trained jointly. Using the validation set, we determined that setting ? = 0.03 achieves a good balance between both the count and sentiment parts of the model 4 , obtaining a good MAE without neglecting modeling of the counts. Given the considerations discussed for ? = 0, 1, we only examine the Negative Binomial count model and the inclusion of the legislator bias term. When the model was trained using the text of Donald Trump's tweets, it was trained for 200 epochs, while it was trained for 1500 epochs when the text was not used, and the runtimes were 3.12 and 12.9 minutes, respectively. The joint model performance is shown in Table  4  As with the cases for ? = 0, 1, we have found that for our final model configuration with ? = 0.03, model predictive performance is superior when Donald Trump's tweet text is used to construct his daily embedding representation. Additionally, the MAE on the test set for ? = 0.03 is less than the MAE for the case that ? = 0 when only the sentiment model is trained. This demonstrates that the inclusion of tweet count information mitigates sentiment prediction as well, since more information is being used to model legislators. 

 Legislator Embeddings Training our legislator tweeting model yields a key byproduct: the legislator embeddings. As with previous spatial representations of legislator preferences, our model enables the visualization of the positions of legislators in space. In Figure  2  we plot the two dimensions of legislator embeddings from the model presented in Table  4 .  5  The most noticeable characteristic of the embeddings is how they separate legislators across party lines into Democrats and Republicans, even though party affiliations were not incorporated into the model. In the first dimension, senators are perfectly separated by party with the exception of five Democrats who have lower values on the first 4 See Appendix B.2 for a discussion of choosing ?.  In the figure we also see that embeddings are not simply an artifact of the number of tweets about Trump authored by the legislator, nor whether the legislator is a member of the House or Senate. Legislators with more extreme values of Twitter sentiment relative to other members of their party can be found in both chambers of Congress and range from having authored fewer than 100 tweets about Trump to over 500.  6  Another initial validating characteristic of the embeddings is the clustering of prominent Republican senators who have been publicly critical of Trump. We examine the spatial positions of Republican senators whom a 2017 Washington Post analysis identified as critical of the President based on their responses to controversial events in Trump's presidency, such as Trump's firing of FBI Director James Comey and response to the Charlottesville protests, as well as overall rhetoric used when discussing Trump  (Lewis et al., 2017) . The positions Considering which Democratic legislators are interspersed near the cluster of Republicans in the two-dimensional embedding space is also informative. The two most extreme Democratic outliers were Angus King, an Independent Senator from Maine who caucuses with the Democratic party but has openly considered caucusing with the Republican party and Joe Manchin, a notably conservative Democratic senator in whose state Trump won 68.5% of the vote. We observe fewer outliers among Republicans. Among the most extreme outliers, are Ileana Ros-Lehtinen and Carlos Curbelo, whose districts Hillary Clinton won in 2016 by 19.6 and 16.3 percentage points, respectively. Ros-Lehtinen, in particular, tweeted many scathing responses to Trump regarding his controversial stance on immigration. We next compare the embeddings to an existing measure of general legislator preferences. Figure  3  illustrates the relationship between the first dimension of DW-NOMINATE -a canonical measure of legislator ideology in political science -and the first dimension of our learned legislator embeddings. Generally, legislators who are ideolog-ically conservative have lower embedding values, whereas liberals have higher values. At the same time, this comparison does identify legislators who are more or less critical of Trump than might be expected based on ideology alone, thereby offering new empirical leverage to scholars examining the behavior and attitudes of legislators. Figure  3  also compares our legislator embeddings to an approximation of how legislators might feel toward Trump: the proportion of time that they vote in line with him during the period in which legislator tweets were collected. This metric was calculated using a dataset published by Fivethirtyeight and includes only legislation on which the Trump administration publicly expressed a clear position  (Silver and Bycoffe, 2019) . While this measure is limited by many of the same constraints as other vote-based measures (e.g., DW-NOMINATE), it is the closest existing measure of legislators' attitudes toward Trump. In the right panel of Figure  3 , we observe little variation in the extent to which legislators vote with Trump, particularly for Republicans. Indeed, many of the President's most prominent critics frequently voted with the president during this time period. For instance, John McCain voted with Trump 85% of the time, Bob Corker voted with Trump 84% of the time, and Jeff Flake voted with Trump 83% of the time. Meanwhile, we observe far more variation in legislator embeddings among both Republicans and Democrats. In Ap-pendix C, we further compare our embeddings to alternative measures of legislative preferences: Campaign Finance Scores  (Bonica, 2018)  and Trump vote-share in a legislator's constituency during the 2016 presidential election. 

 Conclusion In this paper, we modeled legislator tweeting behavior towards Donald Trump, predicting the frequency and sentiment of their tweets. The proposed model yields embedding representations for legislators that we interpret as measures of legislator attitudes towards Trump. Our application suggests that ideal points estimated from roll call votes can miss this critical aspect of political preferences for members of Congress. Whereas legislative voting might recover ideological similarities and differences with the president, it is not well suited to measure attitudes toward the president orthogonal to policy preferences, such as criticisms of his rhetoric and tone. To address this shortcoming and obtain representations of legislators' attitudes toward Trump, we have proposed a model that assigns a vector to each legislator based on the content of their tweets about Trump. We similarly represent Donald Trump with a vector for each day he tweets, constructed using the text of his daily tweets. Legislator vectors and Trump vectors interact to produce predictions of both the sentiment of legislator tweets about Donald Trump and the number of tweets produced each day. From this model we obtain representations of legislators that capture their attitudes toward the president. Our model's predictive performance is robust to a variety of settings and achieves sentiment predictive performance of 0.127 mean-absolute-error and 89.3% accuracy, demonstrating its capability to predict legislator tweeting behavior. When visualizing the two dimensions of learned legislator embeddings we find that the model separates legislators across party lines (despite not being trained on the party of legislators) and groups together Republican senators who are well-known critics of Trump (despite overwhelmingly voting with him on legislation). Though our model demonstrates the capability of representing legislators' attitudes toward Trump and performs well with respect to predicting tweet counts and sentiment based upon Donald Trump's tweets, our method has some limitations. For one, as is the case for Rheault and Cochrane (2020), our model is not able to produce uncertainty bounds, as deriving uncertainty measures from neural networks remains an open area of research without a clear solution within the field of machine learning.  7  An avenue for improving the model is to allow it to capture legislators' dynamic attitudes toward Trump over time. While legislator attitudes are currently modeled as static embeddings, allowing each legislator's embedding to change over time would enable the exploration of temporal dynamics and hypothesis testing about when legislators are more likely to tweet negatively about Trump, what factors contribute to a legislator's decision to tweet about Trump, and how the Trump's tweets interact with legislator's tweets over time. While our aims in this paper were to develop a method of modeling attitudes toward Trump beyond legislative policy preferences, this method can be used to test a wide range of hypotheses about modern U.S. politics. Legislator embeddings can be used to explore how legislators appeal to different audiences, such as party leaders and constituents. The method presented here could similarly be used to evaluate how members of Congress are punished and rewarded in elections for their criticism of praise of the president. Moreover, because our model does not rely on roll call votes, it can also be used to model attitudes by any of the growing number of political elites using Twitter, such as non-incumbent political candidates, state legislators, and pundits. Possible extensions of this work could investigate enriching Trump vectors by incorporating other sources of text, such as White House press releases and speeches. While we restrict ourselves to Twitter data in this paper to maintain consistency across the sources of data for vectors representing Trump and legislators, the incorporation of auxiliary text data could provide additional context. 
