title
Classification Using Group LSTM for Emotion Intensity prediction

abstract
This paper describes a system attended in the SemEval-2018 Task 1 "Affect in tweets" that predicts emotional intensities. We use Group LSTM with an attention model and transfer learning with sentiment classification data as a source data (SemEval 2017 Task 4a). A transfer model structure consists of a source domain and a target domain. Additionally, we try a new dropout that is applied to LSTMs in the Group LSTM. Our system ranked 8th at the subtask 1a (emotion intensity regression). We also show various results with different architectures in the source, target and transfer models.

Introduction Sentiment analysis is one of the most famous Natural Language Process (NLP) task. In this study, we perform a task that predicts emotional intensities of anger, joy, fear and sadness with tweet messages, where intensity values range from 0 to 1. This task is competed at SemEval-2018 Task 1  (Mohammad et al., 2018) . In previous studies, neural networks with word embedding and affective lexicons were widely used  (Goel et al., 2017; He et al., 2017) . Also, many studies employed support vector regression  (Duppada and Hiray, 2017; Akhtar et al., 2017) . Transfer learning was recently proposed as an effecive approach to have higher performance, when data is not abundant. Using a pre-trained deep-learning model with an abundant data set has been popular and shows good results in various tasks  (Donahue et al., 2014; Conneau et al., 2017) . Especially in a medical image task, it is very efficient because of lacks of medical data  (Tajbakhsh et al., 2016) . Just as humans can learn new things better with their past knowledge, neural networks can also be trained on target domains by transferring knowledge from the source domain. We make a transfer model that can be divided into a source model and a target model. The source model is constructed based on the paper  (Baziotis et al., 2017) . The model of this paper uses LSTM with attention. However, we introduce Group LSTM (GLSTM)  (Kuchaiev and Ginsburg, 2017)  with a new dropout. After then, we make the target model with LSTM. In the result section, we provide comparison of LSTM and GLSTM in the source model, and results of various pre-trained word embeddings with target model. Finally, we discuss about the result of the transfer model that is a combined model with the source and target models. 

 System Description 

 Data and Label For transfer learning, we use a source data provided by SemEval 2017 Task4 (a)  (Rosenthal et al., 2017) . The task of the source domain is to classify sentences to positive, negative and neutral sentences. Training data is 44,613 sentences (10% are used as a development set), and test data is 12,284 sentences for the source model evaluation. For transfer learning in this study, all training and test data are used as training data. For the target domain, training data is about 2,000 sentences for each emotion. Although the main task is regression prediction, we change it as distribution prediction  (Tai et al., 2015) . In this way, we deal it as a classification problem. Intensity scores y are changed to labels t satisfying:  , 2, 3, 4, 5]  and y' = 4y t i = ? ? ? ? ? y' ? y' if i = y' + 1 y' ? y' + 1 if i = y' 0 otherwise where i = [1 Size of the final output is 5. For example, if an intensity score y is 0.7, label t is [0, 0, 0.2, 0.8, 0]. With given r = [0, 0.25, 0.5, 0.75, 1], label y can be obtained again by dot product with t and r (0.7 = 0.2*0.5 + 0.8*0.75). 

 Text preprocessing To normalize words and remove noise in sentences, we use ekphrasis library  (Baziotis et al., 2017) . It helps to apply social tokenizer, spell correction, word segmentation and various preprocessing. We normalize time and number, and omit URL, email and user tag. Annotations are added on hashtags, emphasized and repeated words. We annotate them as a group because hashtags are gathered in many cases (see Table  1 ). Lastly, emoticons are changed to words that represent emoticons. 

 #letsdance #dancinginthemoonlight #singing ? hashtag lets dance dancing in the moonlight singing /hashtag  

 Word embedding We try five pre-trained word embeddings to choose the best one for the target model. Two are trained with GloVe  (Pennington et al., 2014)  using different data sets: one 1 is trained with very large data in Common crawl, and the other 2 is made with tweets  (Baziotis et al., 2017) . Other word embedding methods are fastText 3  (Bojanowski et al., 2016) , word2vec 4  (Mikolov et al., 2013)  and LexVec 5  (Salle et al., 2016) . LexVec is the mixed version of GloVe and word2vec. Dimensions of them are all 300. Among them, GloVe with tweet is used for the source and transfer models. Emoji can be good features but most of emoji ideograms are not contained in embedding vocabulary. Hence, we change a emoji to a phrase with python 'emoji' library. For example, is decoded to "Smiling Face with Open Mouth and Smiling Eyes". Because it is quite long, embedding vectors of emoji are changed to mean of vectors of each decoded words. In this way, we reduce Out-Of-Vocabulary and prevent the sentence from lengthening. 

 LSTM and GLSTM Recurrent Neural Network (RNN) works well in a sequence model like language by addressing its arbitrary length  (Tai et al., 2015) . However, RNN is difficult to be optimized because of a gradient vanishing problem. To solve it, LSTM suggested a cell state and gates as bridges to control the flow of error  (Hochreiter and Schmidhuber, 1997) . GLSTM is just a group of several LSTMs, where outputs of LSTMs are concatenated. The idea is that LSTM can be divided into several sub-LSTMs  (Kuchaiev and Ginsburg, 2017) . This model has some advantages compared to the original LSTM. The number of parameters is reduced with a preserving feature size. Also, it can be parallelized and computation times are reduced because the computation of each sub-LSTM is independent. 

 Dropout To avoid overfitting and achieve generality, we use three types of dropout. One is normal dropout between layers  (Srivastava et al., 2014) . If a shape of the layer is sequential, dropout mask is shared on sequential axis. Another dropout is inside cells of LSTM. In the each LSTM cell, the same dropout mask is applied on hidden values that come from the previous cell  (Zaremba et al., 2014) . Applying different dropout masks for each cell can mislead memory and information. With the same dropout mask, however, LSTM cell can dropout nodes consistently so that the model can forget or memorize information stably. The last one is dropout between sub-LSTMs. To get more generality, we dropped several LSTMs in GLSTM. For example, if GLSTM consist of five sub-LSTMs, we dropped two LSTMs and only use the rest three LSTMs. 3 Model structure 

 Source model For the source model, Glove with tweets is used as input vectors of the embedding layer. After embedding layer, two GLSTM layers are stacked. GLSTM is made of 5 LSTMs with 40 feature size. Additionally, we concatenate forward and backward GLSTM to be bidirectional. So hidden size of each recurrent layer is 400 ( = 5 ?40 ? 2). Next is an attention layer, which calculates importance of each time step. Attention mechanism shows good performance on sequential tasks like machine translation  (Bahdanau et al., 2014)  and sentiment analysis  (Baziotis et al., 2017) . It helps to concentrate position related to emotion. Attention values are calculated: e t = W h h t + b t a t = exp(e t ) l i exp(e i ) , a t = 1 Calculated attention values are multiplied by each current hidden state and they are all added up. Passing through the attention layer, the output becomes non-sequential representation vectors. It enters a fully connected softmax layer as a final classification layer, where the size of the layer is 3. 

 Target model Unlike the source model, a normal bi-LSTM is used with 100 feature size. After then, attention and output layers are stacked. The size of output layer is 5. For transfer learning, outputs of several layers on the source model are used as additional features. The LSTM layer on the target model takes as input the concatenation of the embedding layer and the first LSTM layer output of the source model. After the attention layer, in a similar way, outputs of the attention and the final layers on the source model are concatenated and entered into the final layer as input. 

 Regularization At the embedding layer, Gaussian noise is applied with sigma = 0.2. It helps models to be robust by avoiding overfitting on specific features of words. Dropouts are used everywhere between layers with probability p = 0.3 except before the final layer. Before the final layer, p = 0.5 dropout is applied. Additionally, LSTM dropout was applied on every LSTM layers with p = 0.3. The probability of dropout at GLSTM on the source model is 0.3. Also, we use L2 normalization. It prevents weights to be large values by adding weight penalty to loss. We set up it with 0.001 for the source model and 0.0001 for the target model. 

 Training For the source and target models, categorical cross-entropy is used as a loss function. For updating weights, we apply the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 0.001. During training the transfer model, since we want to preserve target model weight parameters with a little updating, we decrease gradient flow of backpropagation from the source model to the target model by 0.05 times (see large arrows on Figure  1 ). Because there are many parameters on the final model, we take that constraint to prevent overfitting. 

 Result and discussion 

 GLSTM Figure  2  shows the result of GLSTM and normal LSTM on the source model for Sentiment Classification (SemEval 2017 Task 1a). We tried various feature sizes. The number of sub-LSTM in GLSTM is fixed to 5 and the feature size of each sub-LSTM is changed. As the sizes of features increase, the performances of GLSTM increase. On the other hand, although the performances of LSTM gradually improve with larger feature sizes, it starts to decrease rapidly after 100. Thus, we infer that GLSTM with dropout is more effective on overfitting than LSTM with larger feature size. Based on this result, we use GLSTM for the source model. 

 Various Embedding We tested five different word embedding vectors using the target model to choose the best embedding. To compare the performances of embeddings, the embedding layers was not trained  (static). Note that we did not use transfer learning in this experiment. Table  2  shows Pearson correlation between the given emotion intensities and predicted intensities by the models on the development set. Tweet GloVe had the best score and Common GloVe showed the second best score. Hence, we decided to do transfer learning with Tweet GloVe and Common GloVe. 

 Transfer Our main task results are described in Table  3 . There are four models. Tweet Glove and Common GloVe were picked from the conclusion of 4.2, and we performed two approaches: training the embedding layer or not (non-static or static)  (Kim, 2014) . Tweet GloVe with static showed the best performance as a single model and it is almost same to non-static. However, the non-static method had a higher score than the static for Common GloVe embedding. In addition, the ensemble model by averaging all single models showed better performance than the single models. We also found that compared to the scores without trans-fer learning on dev set (Table  2 ), there were significant performance improvements when transfer learning used in Table  3 . 

 Conclusion This paper described the system submitted to SemEval-2018 Task 1: Affect in tweets and analysis of various models. Various embedding vectors were tried and we chose Tweet GloVe with static. The main method is LSTM with attention and transfer learning that uses sentiment classification as source domain. In future work, we will perform transfer learning with labeled data sets such as SNLI or SST data sets. Also, training tagging or tree parsing can be used for transfer learning. Figure 1 : 1 Figure 1: Structure of models. For the transfer model, connections between source and target models are used. Large arrows are paths of reduced gradient flow during backpropagation. 
