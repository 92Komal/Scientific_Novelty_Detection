title
Corpus Creation for Sentiment Analysis in Code-Mixed Tamil-English Text

abstract
Understanding the sentiment of a comment from a video or an image is an essential task in many applications. Sentiment analysis of a text can be useful for various decision-making processes. One such application is to analyse the popular sentiments of videos on social media based on viewer comments. However, comments from social media do not follow strict rules of grammar, and they contain mixing of more than one language, often written in non-native scripts. Non-availability of annotated code-mixed data for a low-resourced language like Tamil also adds difficulty to this problem. To overcome this, we created a gold standard Tamil-English code-switched, sentiment-annotated corpus containing 15,744 comment posts from YouTube. In this paper, we describe the process of creating the corpus and assigning polarities. We present inter-annotator agreement and show the results of sentiment analysis trained on this corpus as a benchmark.

Introduction Sentiment analysis has become important in social media research  (Yang and Eisenstein, 2017) . Until recently these applications were created for high-resourced languages which analysed monolingual utterances. But social media in multilingual communities contains more code-mixed text  (Barman et al., 2014; Chanda et al., 2016; Pratapa et al., 2018a; Winata et al., 2019a) . Our study focuses on sentiment analysis in Tamil, which has little annotated data for code-mixed scenarios  (Phani et al., 2016; Jose et al., 2020) . Features based on the lexical properties such as a dictionary of words and parts of speech tagging have less performance compared to the supervised learning  (Kannan et al., 2016)  approaches using annotated data. However, an annotated corpus developed for monolingual data cannot deal with code-mixed usage and therefore it fails to yield good results  (AlGhamdi et al., 2016; Aguilar et al., 2018)  due to mixture of languages at different levels of linguistic analysis. Code-mixing is common among speakers in a bilingual speech community. As English is seen as the language of prestige and education, the influence of lexicon, connectives and phrases from English language is common in spoken Tamil. It is largely observed in educated speakers although not completely absent amongst less educated and uneducated speakers  (Krishnasamy, 2015) . Due to their pervasiveness of English online, code-mixed Tamil-English (Tanglish) sentences are often typed in Roman script  (Suryawanshi et al., 2020a; Suryawanshi et al., 2020b) . We present TamilMixSentiment 1 , a dataset of YouTube video comments in Tanglish. TamilMixSentiment was developed with guidelines following the work of Mohammad 1 https://github.com/bharathichezhiyan/TamilMixSentiment  (2016)  and without annotating the word level language tag. The instructions enabled light and speedy annotation while maintaining consistency. The overall inter-annotator agreement in terms of Kripendorffs's ?  (Krippendorff, 1970)  stands at 0.6. In total, 15,744 comments were annotated; this makes the largest general domain sentiment dataset for this relatively low-resource language with code-mixing phenomenon. We observed all the three types of code-mixed sentences --Inter-Sentential switch, Intra-Sentential switch and Tag switching. Most comments were written in Roman script with either Tamil grammar with English lexicon or English grammar with Tamil lexicon. Some comments were written in Tamil script with English expressions in between. The following examples illustrate the point. ? Intha padam vantha piragu yellarum Thala ya kondaduvanga. -After the movie release, everybody will celebrate the hero. Tamil words written in Roman script with no English switch. In this work we present our dataset, annotation scheme and investigate the properties and statistics of the dataset and information about the annotators. We also present baseline classification results on the new dataset with ten models to establish a baseline for future comparisons. The best results were achieved with models that use logistic regression and random forest. The contribution of this paper is two-fold: 1. We present the first gold standard code-mixed Tamil-English dataset annotated for sentiment analysis. 2. We provide an experimental analysis of logistic regression, naive Bayes, decision tree, random forest, SVM, dynamic meta-embedding, contextualized dynamic meta-embedding, 1DConv-LSTM and BERT on our code-mixed data for sentiment classification. 

 Related Work Recently, there has been a considerable amount of work and effort to collect resources for code-switched text. However, code-switched datasets and lexicons for sentiment analysis are still limited in number, size and availability. For monolingual analysis, there exist various corpora for English  (Hu and Liu, 2004; Wiebe et al., 2005; Jiang et al., 2019) , Russian  (Rogers et al., 2018) , German  (Cieliebak et al., 2017) , Norwegian  (Maehlum et al., 2019)  and Indian languages  (Agrawal et al., 2018; Rani et al., 2020) . When it comes to code-mixing, an English-Hindi corpus was created by  (Sitaram et al., 2015; Joshi et al., 2016; Patra et al., 2018) , an English-Spanish corpus was introduced by  (Solorio et al., 2014; Vilares et al., 2015; Vilares et al., 2016) , and a Chinese-English one  (Lee and Wang, 2015)  was collected from Weibo.com and English-Bengali data were released by Patra et al.  (Patra et al., 2018) . Tamil is a Dravidian language spoken by Tamil people in India, Sri Lanka and by the Tamil diaspora around the world, with official recognition in India, Sri Lanka and Singapore  (Chakravarthi et al., 2018; Chakravarthi et al., 2019a; Chakravarthi et al., 2019b; Chakravarthi et al., 2019c) . Several research activities on sentiment analysis in Tamil  (Padmamala and Prema, 2017)  and other Indian languages  (Ranjan et al., 2016; Das and Bandyopadhyay, 2010; A.R. et al., 2012; Phani et al., 2016; Prasad et al., 2016; Priyadharshini et al., 2020; Chakravarthi et al., 2020)  are happening because the sheer number of native speakers are a potential market for commercial NLP applications. However, sentiment analysis on Tamil-English code-mixed data  (Patra et al., 2018)  is under-developed and data tare not readily available for research. Until recently, word-level annotations were used for research in code-mixed corpora. Almost all the previous systems proposed were based on data annotated at the word-level. This is not only time-consuming but also expensive to create. However, neural networks and metaembeddings  (Kiela et al., 2018)  have shown great promise in code-switched research without the need for word-level annotation. In particular, work by  Winata et al. (2019a)  learns to utilise information from pre-trained embeddings without explicit word-level language tags. A recent work by  Winata et al. (2019b)  utilised the subword-level information from closely related languages to improve the performance on the code-mixed text. As there was no previous dataset available for Tamil-English (Tanglish) sentiment annotation, we create a sentiment dataset for Tanglish with voluntary annotators. We also show the baseline results with a few models explained in Section 5.  

 Corpus Creation and Annotation Our goal was to create a code-mixed dataset for Tamil to ensure that enough data are available for research purposes. We used the YouTube Comment Scraper tool 2 and collected 184,573 sentences for Tamil from YouTube comments. We collected the comments from the trailers of a movies released in 2019. Many of the them contained sentences that were either entirely written in English or code-mixed Tamil-English or fully written in Tamil. So we filtered out a non-code-mixed corpus based on language identification at comment level using the langdetect library 3 . Thus if the comment is written fully in Tamil or English, we discarded that comment since monolingual resources are available for these languages. We also identified if the sentences were written in other languages such as Hindi, Malayalam, Urdu, Telugu, and Kannada. We preprocessed the comments by removing the emoticons and applying a sentence length filter. We want to create a code-mixed corpus of reasonable size with sentences that have fairly defined sentiments which will be useful for future research. Thus our filter removed sentences with less than five words and more than 15 words after cleaning the data. In the end we got 15,744 Tanglish sentences. 

 Annotation Setup For annotation, we adopted the approach taken by Mohammad (  2016 ), and a minimum of three annotators annotated each sentence in the dataset according to the following schema shown in the Figure  1 . We added new category Other language: If the sentence is written in some other language other than Tamil or English. Examples for this are the comments written in other Indian languages using the Roman script. The annotation guidelines are given in English and Tamil. As we have collected data from YouTube we anonymized to keep the privacy of the users who commented on it. As the voluntary annotators' personal information were collected to know about the them, this gives rise to both ethical, privacy and legal concerns. Therefore, the annotators were informed in the beginning that their data is being recorded and they can choose to withdraw from the process at any stage of annotation. The annotators should actively agree to being recorded. We created Google Forms in which we collected the annotators' email addresses which we used to ensure that an annotator was allowed to label a given sentence only once. We collected the information on gender, education and medium of instruction in school to know the diversity of annotators. Each Google form has been set to contain a maximum of 100 sentences. Example of the Google form is given in the Figure  1 . The annotators have to agree that they understood the scheme; otherwise, they cannot proceed further. Three steps complete the annotation setup. First, each sentence was annotated by two people. In the second step, the data were collected if both of them agreed. In the case of conflict, a third person annotated the sentence. In the third step, if all the three of 2 https://github.com/philbot9/youtube-comment-scraper 3 https://pypi.org/project/langdetect/ them did not agree, then two more annotators annotated the sentences.  

 Gender 

 Annotators To control the quality of annotation, we removed the annotator who did not annotate well in the first form. For example, if the annotators showed unreasonable delay in responding or if they labelled all sentences with the same sentiment or if more than fifty annotations in a form were wrong, we removed those contributions. Eleven volunteers were involved in the process. All of them were native speakers of Tamil with diversity in gender, educational level and medium of instruction in their school education. Table  1  shows information about the annotators. The volunteers were instructed to fill up the Google form, and 100 sentences were sent to them. If an annotator offers to volunteer more, the next Google form is sent to them with another set of 100 sentences and in this way each volunteer chooses to annotate as many sentences from the corpus as they want. We send the forms to an equal number of male and female annotators. However, from Table  1 , we can see that only two female annotators volunteered to contribute. 

 Corpus Statistics Corpus statistics is given in the Table  2 . The distribution of released data is shown in  

 Inter Annotator Agreement We used Krippendorff's alpha (?)  (Krippendorff, 1970)  to measure inter-annotator agreement because of the nature of our annotation setup. This is a robust statistical measure that accounts for incomplete data and, therefore, does not require every annotator to annotate every sentence. It is also a measure that takes into account the degree of disagreement between the predicted classes, which is crucial in our annotation scheme. For instance, if the annotators disagree  : ? = 1 ? D o D e (1) D o is the observed disagreement between sentiment labels by the annotators and D e is the disagreement expected when the coding of sentiments can be attributed to chance rather than due to the inherent property of the sentiment itself. D o = 1 n c k o ck metric ? 2 ck (2) D e = 1 n(n ? 1) c k n c . n k metric ? 2 ck (3) Here o ck n c n k and n refer to the frequencies of values in coincidence matrices and metric refers to any metric or level of measurement such as nominal, ordinal, interval, ratio and others. Krippendorff's alpha applies to all these metrics. We used nominal and interval metric to calculate annotator agreement. The range of ? is between 0 and 1, 1 ? ? ? 0. When ? is 1 there is perfect agreement between annotators and when 0 the agreement is entirely due to chance. Our annotation produced an agreement of 0.6585 using nominal metric and 0.6799 using interval metric. 

 Difficult Examples In this section we talk about some examples that were difficult to annotate. 1. Enakku iru mugan trailer gnabagam than varuthu -All it reminds me of is the trailer of the movie Irumugan. Not sure whether the speaker enjoyed Irumugan trailer or disliked it or simply observed the similarities between the two trailers. 2. Rajini ah vida akshay mass ah irukane -Akshay looks more amazing than Rajini. Difficult to decide if it is a disappointment that the villain looks better than the hero or a positive appreciation for the villain actor. 3. Ada dei nama sambatha da dei -I wonder, Is this our sampath? Hey!. Conflict between neutral and positive. 4. Lokesh kanagaraj movie naalae.... English Rap....Song vandurum -If it is a movie of Lokesh kanagaraj, it always has an English rap song. Ambiguous sentiment. According to the instructions, questions about music director, movie release date and remarks about when the speaker is watching the video should be treated as neutral. However the above examples show that some comments about the actors and movies can be ambiguously interpreted as neutral or positive or negative. We found annotator disagreements in such sentences. 

 Benchmark Systems In order to provide a simple baseline, we applied various machine learning algorithms for determining the sentiments of YouTube posts in code-mixed Tamil-English language. 

 Experimental Settings 

 Logistic Regression (LR): We evaluate the Logistic Regression model with L2 regularization. The input features are the Term Frequency Inverse Document Frequency (TF-IDF) values of up to 3 grams. 

 Support Vector Machine (SVM): We evaluate the SVM model with L2 regularization. The features are the same as in LR. The purpose of SVM classification algorithm is to define optimal hyperplane in N dimensional space to separate the data points from each other. 

 K-Nearest Neighbour (K-NN): We use KNN for classification with 3,4,5,and 9 neighbours by applying uniform weights. 

 Decision Tree (DT): Decision trees have been previously used in NLP tasks for classification. In decision tree, the prediction is done by splitting the root training set into subsets as nodes, and each node contains output of the decision, label or condition. After sequentially choosing alternative decisions, each node recursively is split again and finally the classifier defines some rules to predict the result. We used it to classify the sentiments for baseline. Maximum depth was 800 and minimum sample splits were 5 for DT. The criterion were Gini and entropy. 

 Random Forest (RF): In random forest, the classifier randomly generates trees without defining rules. We evaluate the RF model with same features as in DT. 

 Multinominal Naive Bayes (MNB): Naive-Bayes classifier is a probabilistic model, which is derived from Bayes Theorem that finds the probability of hypothesis activity to the given evidence activity. We evaluate the MNB model with our data using ?=1 with TF-IDF vectors. 

 1DConv-LSTM: The model we evaluated consists of Embedding layer, Dropout, 1DConv with activation ReLU, Max-pooling and LSTM. The embeddings are randomly initialized. 

 BERT-Multilingual: Devlin et al. (  2019 ) introduced a language representation model which is Bidirectional Encoder Representation from Transforms. It is designed to pre-train from unlabelled text and can be fine-tuned by adding last layer. BERT has been used for many text classification tasks  (Tayyar Madabushi et al., 2019; Ma et al., 2019; Cohan et al., 2019) . We explore classification of a code-mixed data into their corresponding sentiment categories. 

 DME and CDME: We also implemented the Dynamic Meta Embedding  (Kiela et al., 2018)  to evaluate our model. As a first step, we used Word2Vec and FastText to train from our dataset since dy-namic meta-embedding is an effective method for the supervised learning of embedding ensembles. 

 Experiment Results and Discussion The experimental results of the sentiment classification task using different methods are shown in terms of precision in Table  4 , recall in Table  5 , and F-score in Table  6 . We used sklearn 4 for evaluation. The micro-average is calculated by aggregating the contributions of all classes to compute the average metric. As shown in the tables, all the classification algorithms perform poorly on the code-mixed dataset. Logistic regression, random forest classifiers and decision trees were the ones that fared comparatively better across all sentiment classes. Surprisingly, the classification result by the SVM model has much worse diversity than the other methods. Applying deep learning methods also does not lead to higher scores on the three automatic metrics. We think this stems from the characteristics of the dataset. The classification scores for different sentiment classes appear to be in line with the distribution of sentiments in the dataset. The dataset is not a balanced distribution. Table  3  shows that out of total 15,744 sentences 67% belong to Positive class while the other sentiment classes share 13%, 5% and 3% respectively. The precision, recall and F-measure scores are higher for the Positive class while the scores for Neutral and Mixed feeling classes were disastrous. Apart from their low distribution in the dataset, these two classes are difficult to annotate for even human annotators as discussed in Section 4. In comparison, the Negative and Other language classes were better. We suspect this is due to more explicit clues for negative and non-Tamil words and due to relatively higher distribution of negative comments in the data. Since we collected the post from movie trailers, we got more positive sentiment than others as the people who watch trailers are more likely to be interested in movies and this skews the overall distribution. However, as the code-mixing phenomenon is not incorporated in the earlier models, this resource could be taken as a starting point for further research. There is significant room for improvement in code-mixed research with our dataset. In our experiments, we only utilized the machine learning methods, 4 https://scikit-learn.org/ but more information such as linguistic information or hierarchical meta-embedding can be utilized. This dataset can be used to create a multilingual embedding for code-mixed data  (Pratapa et al., 2018b) . 

 Conclusion We presented, to the best of our knowledge, the most substantial corpus for under-resourced code-mixed Tanglish with annotations for sentiment polarity. We achieved a high inter-annotator agreement in terms of Krippendorff ? from voluntary annotators on contributions collected using Google form. We created baselines with gold standard annotated data and presented our results for each class in Precision, Recall, and F-Score. We expect this resource will enable the researchers to address new and exciting problems in code-mixed research.  
