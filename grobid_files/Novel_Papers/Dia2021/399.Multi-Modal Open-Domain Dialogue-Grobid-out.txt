title
Multi-Modal Open-Domain Dialogue

abstract
Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size  (Adiwardana et al., 2020; Roller et al., 2020) . However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot  (Roller et al., 2020)  in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.

Introduction An important goal of artificial intelligence is the construction of open-domain conversational agents that can engage humans in discourse. Indeed, the future of human interaction with AI is predicated on models that can exhibit a number of different conversational skills over the course of rich dialogue. Much recent work has explored building and training dialogue agents that can blend such skills throughout natural conversation, with the ultimate goal of providing an interesting and engrossing experience for humans  Shuster et al., 2019b) . Coupled with the advancement of * Joint First Authors. large-scale model training schemes, such models are becoming increasingly human-like and engaging  Adiwardana et al., 2020; Roller et al., 2020) . In order to better approach human-like ability, however, it is necessary that agents can converse with both textual and visual context, similarly to how humans interact in the real world; indeed, communication grounded in images is naturally engaging to humans  (Hu et al., 2014) . Recent efforts have gone beyond classical, fact-based tasks such as image captioning or visual question answering  (Antol et al., 2015; Das et al., 2017a)  to produce models that can respond and communicate about images in the flow of natural conversation  (Shuster et al., , 2019b . In this work, we explore the extension of largescale conversational agents to image-based dia-logue. We combine representations from imagebased models that have been trained on object detection tasks  (Lu et al., , 2019  with representations from Transformers with billions of parameters pre-trained on massive (text-only) dialogue datasets, to produce responses conditioned on both visual and textual context. To ensure that our model retains the ability to engage in regular, text-based conversation, we include in our training procedure multi-tasking with datasets expressly designed to instill conversational skills in the model . We find that our best resulting models are as proficient in text-only conversation as the current best reported dialogue models, with respect to both performance on the relevant datasets and human evaluations of preference. Concatenating image feature embeddings to the input of our model's encoder leads to better performance than concatenating the embeddings to the encoder's output, and using spatially based image embeddings performs better than single-vector embeddings. Simultaneously, our model significantly outperforms recent strong multi-modal dialogue models when in an image-dialogue regime; we measure several metrics via pairwise human judgments using ACUTE-Eval  (Li et al., 2019b)  to show that our model is not only more preferred by humans but can also discuss and reference visual context throughout a conversation. See Figure  1  for one sample cherrypicked conversation with our model, with random and lemon-picked conversations in Figures  2 and  3 . One important avenue we explore with our best models is safety -that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area  Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Sch?fer and Burtenshaw, 2019; Zhang et al., 2018a ), yet we note that safety in the context of image-dialogue is relatively less explored. In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset . Notably, after tuning the model to reduce toxicity and gender bias, we find that human preference for this model does not diminish. The training procedure and initial pre-trained model weights will be made publicly available to allow for fully reproducible results. 

 Related Work 

 Multi-Modal Models and Tasks Rich Representations Modeling multi-modal inputs, i.e. in visual + textual contexts, is a wellresearched area. Much of the existing literature explores similar architectures to our setup, i.e., using standard Transformer-based models to jointly encode text and images  (Li et al., 2019a; Kiela et al., 2019) . Others have explored modifications to the standard self-attention scheme in Transformers by incorporating additional co-attention  (Lu et al., 2019; Tan and Bansal, 2019)  or cross-attention  (Stefanini et al., 2020)  layers. These models have primarily been used for generating rich joint representations of images and text for use in downstream tasks, and primarily focus on the encoding aspect. Visual Dialogue/Caption Generation Many tasks have been designed to measure the ability of a model to produce text in the context of images. Specifically, COCO Captions  (Chen et al., 2015)  and Flickr30k  (Young et al., 2014)  require a model to produce a caption for a given image. A variety of sequence-to-sequence  (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018)  and retrievalbased  (Gu et al., 2018; Faghri et al., 2018; Nam et al., 2016)  models have been applied to these tasks, however they do not go beyond the one-turn text generation expected for captioning an image. Other recent architectures have explored text generation  (Wang et al., 2020; Park et al., 2020)  in the context of the Visual Dialog  (Das et al., 2017b)  task; however, this task is primarily used to measure the ability to answer questions about an image in the flow of a natural conversation, which differs somewhat from the open-domain dialogue task. Further still, there have been recent forays into open-domain natural dialogue in the context of images, e.g. in the Image-Chat  and Image-grounded Conversations  (Mostafazadeh et al., 2017 ) tasks. Again, retrieval-based (Shuster et al., 2020 Ju et al., 2019)  and sequence-tosequence  (Shuster et al., 2019b  models have been used to conduct dialogue in this regime. 

 Multi-Task Training / Using Pre-Trained Representations Our multi-modal model is constructed from models pre-trained in other, related domains; specifically, we seek to fuse the resulting weights of large-scale, uni-modal pre-training to achieve good performance on downstream, multi-modal tasks. Adapting pre-trained representations to later downstream tasks has been shown to be successful in NLP  (Peters et al., 2019; Devlin et al., 2019)  and dialogue in particular  (Roller et al., 2020; Mazar? et al., 2018) , while large-scale multi-modal pretraining has been shown to be effective in other downstream multi-modal tasks  Singh et al., 2020b) . Our work does not contain multi-modal pre-training in itself, but rather we explore "domain-adaptive pretraining"  (Gururangan et al., 2020)  or "intermediate task transfer"  (Pruksachatkun et al., 2020) , in which pre-trained representations are "adapted" to a certain domain via an intermediate training step, before training/evaluating on the requisite downstream tasks. We also employ multi-task training, to both help generalize the applicability of the model and improve its performance on downstream tasks/evaluations; this has been shown recently to help in both image-based  (Singh et al., 2020b; Ju et al., 2019; ) and text-based  (Shuster et al., 2019b; Roller et al., 2020)  tasks. 

 Comparison to Existing Models In this work, we compare our best resulting model to several existing models in the literature. BlenderBot: the 2.7-billion-parameter Transformer sequence-to-sequence model from  Roller et al. (2020) , known as "BST Generative 2.7B model" in that work, pre-trained on 1.5B comments from a third-party Reddit dump hosted by pushshift.io  (Baumgartner et al., 2020) . We refer to this model as "BlenderBot". DialoGPT: a GPT-2-based model trained on 147M exchanges from public-domain social-media conversations . Meena: a 2.6B-parameter Transformer sequence-to-sequence model trained on 341GB of conversations  (Adiwardana et al., 2020) . Dodeca: the Image+Seq2Seq model from do-decaDialogue  (Shuster et al., 2019b) , a Transformer sequence-to-sequence model in which the encoder is passed pre-trained image features from the ResNeXt-IG-3.5B model  (Mahajan et al., 2018) . We use their model fine-tuned on Image-Chat (and we refer to this model as "Dodeca"). 2AMMC: a retrieval model in which multiple Transformers are attended over in order to make use of a combination of ResNeXt-IG-3.5B and Faster R-CNN image features . We specifically use the 2AMMC model from  Ju et al. (2019)  because that model has the best test-set performance on Image-Chat in that work. 

 Model Architectures The inputs to our models are visual and/or textual context, where applicable. We explore different ways to encode images, and we additionally compare ways of combining (fusing) the image and text representations before outputting a response. 

 Image Encoders Converting an image from pixels to a vector representation is a well-researched problem, and thus we explore using two different image encoders, using features taken from ResNeXt  (Mahajan et al., 2018)  and Faster R-CNN  (Ren et al., 2017) , to determine the best fit for our tasks. See Appendix A for a description of these image encoders. 

 Multi-Modal Architecture To jointly encode visual and textual context, we use a modification of a standard Transformer sequenceto-sequence architecture  (Vaswani et al., 2017) , whereby we experiment with different ways of fusing the image and text representations to generate an output sequence. Our Transformer model architecture follows that of  Roller et al. (2020) , with 2 encoder layers, 24 decoder layers, 2560dimensional embeddings, and 32 attention heads, and the weights are initialized from a 2.7-billion parameter model pre-trained on 1.5B comments from a third-party Reddit dump hosted by pushshift.io  (Baumgartner et al., 2020)  to generate a comment conditioned on the full thread leading up to the comment. From this base model, we explore two possible fusion schemes. 

 Late Fusion The late fusion method is the same as in  Shuster et al. (2019b) , whereby the encoded image is projected to the same dimension as the text encoding of the Transformer encoder, concatenated with this output as an extra "token" output, and finally fed together as input to the decoder. Early Fusion We additionally experiment with an earlier fusion scheme to allow greater interaction between the image and text in the sequenceto-sequence architecture. In a similar fashion to VisualBERT  (Li et al., 2019a)  and multi-modal Bitransformers  (Kiela et al., 2019) , we concatenate the projected image encoding from the visual input with the token embeddings from the textual input, assign each a different segment embedding, and jointly encode the text and image in the encoder.  1  The encoder thus performs full self-attention across the textual and visual context, with the entire output used as normal in the sequence-to-sequence architecture. As our resulting model can be seen as a multimodal extension to the BlenderBot model  (Roller et al., 2020) , we refer to it as "Multi-Modal BlenderBot" (MMB). 

 Training Details When training the model, we fix the weights of the pre-trained image encoders, except the linear projection to the Transformer output dimension, and fine-tune all of the weights of the Transformer encoder/decoder. 

 Domain-Adaptive Pre-Training During training, the vast majority of trainable model weights are initialized from a large, 2.7B parameter Transformer pre-trained solely on textual input. As our end goal is to achieve improved performance on multi-modal tasks, we found that training first on domain-specific/related data was helpful in order to adapt the Transformer model to an image setting. Following  (Singh et al., 2020b) , we experimented with pre-training on COCO Captions  (Chen et al., 2015)  -a dataset of over 120k images with 5 captions each, resulting in over 600k utterances -in which the model is trained to generate a caption solely from image input. We additionally explored multi-tasked training with COCO Captions and on the same third-party Reddit dump hosted by pushshift.io  (Baumgartner et al., 2020)  as the one used in pre-training the Transformer model, to see whether it was necessary to ensure the model did not stray too far from its ability to handle pure textual input. See Appendix C for more details. 

 Fine-tuning Datasets The goal of our resulting model is to perform well in a multi-modal dialogue setting; thus, we fine-tune the model on both dialogue and imagedialogue datasets. For dialogue-based datasets, we consider the same four as in  Roller et al. (2020) : ConvAI2  (Dinan et al., 2020b) , Empa-theticDialogues (ED)  (Rashkin et al., 2019) , Wiz-ard of Wikipedia (WoW)  (Dinan et al., 2019c) , and BlendedSkillTalk . To model image-dialogue, we consider the Image-Chat dataset . We give a brief description of the five datasets in Appendix B; more information can be found in  Roller et al. (2020)  and . In the fine-tuning stage, we consider two different regimes: one in which we multi-task train on the five datasets together, and one in which we train on Image-Chat alone. While the latter regime is useful in exploring upper bounds of model performance, our main goal is to build a model that can display the requisite skills of an appealing conversationalist (empathy, personalization, knowledge) while also having the ability to respond to and converse about images; thus, we are more interested in the former training setup. See Appendix C for more details. 

 Experiments 

 Automatic Evaluations 

 Results on Pre-Training Datasets To fully understand the effects of various training data and image features, as well as multi-modal fusion schemes, we measure model perplexity on the COCO and pushshift.io Reddit validation sets. We are primarily interested in performance on COCO Captions, as the model has already been extensively pre-trained on the pushshift.io Reddit data. The full results are shown in Table  10  in the appendix, and we leave extensive discussion of the results to Appendix D. Notably, we find that training on COCO Captions exclusively yields the best performance on that task, with spatially-based image features yielding better performance than single vector representations. Additionally, our early fusion scheme outperforms the late fusion scheme holding all other variables constant. 

 Results on Fine-Tuned Datasets We conduct the same ablation setups for training on the dialogue and image-and-dialogue datasets as we did in the domain-adapative pre-training setup; the ablation results for multi-tasking all of the datasets are in Table  11 , while results for finetuning on Image-Chat alone are in Table  12  (each in the appendix). Results are summarized in Table  1 , and we note some interesting conclusions here, with further details in Appendix E. First, overloading the Trans- Table  2 : Test performance of existing models on the datasets considered, compared to MMB (specifically, the "MMB Style" model discussed in Section 5.2.1), in terms of F1, BLEU-4 (B), and ROUGE-L (R) scores. * indicates that gold knowledge was utilized in the WoW task. former encoder/decoder to incorporate image features does not hinder performance on dialogue datasets (as seen via multi-tasked training), and in fact domain-adaptive pre-training improves downstream performance on Image-Chat. In terms of architecture choices, we find that our early fusion architecture improves performance on Image-Chat across all ablation regimes, with Faster R-CNN features yielding the best performance. 

 Final Test Results Following the ablation analyses, we decide to compare our best multi-tasked and single-tasked trained model (with respect to the fine-tuning datasets), where we use Faster R-CNN image features and an early fusion scheme, to existing models in the literature. For this comparison, we consider additional metrics that can be computed on the actual model generations: F1, BLEU-4 and ROUGE-L. We generate model responses during inference with the same generation scheme as in  Roller et al. (2020)  -beam search with beam size of 10, minimum beam length of 20, and tri-gram blocking within the current generation and within the full textual context. The test perfor-mance of our best multitask model on the various datasets compared to existing models from Section 2.3 is shown in Table  2 , with full test results in Table  13  in Appendix F. We first note that the Dodeca model performs well across the board, and indeed has the highest ROUGE-L, BLEU-4, and F1 scores for the three text-only datasets. Higher BLEU-4 scores can be attributed to specifying a smaller minimum generation length, as forcing the BlenderBot models to generate no less than 20 tokens hurts precision when compared to reference labels -we verified this by generating with a smaller minimum length (5 tokens) and saw a 20% increase in BLEU-4 on Image-Chat for Multi-Modal BlenderBot. Higher ROUGE-L scores can additionally be attributed to specifying a larger minimum generation length; this was also verified by generating with a higher minimum length (50 tokens) where we saw nearly a 40% increase in ROUGE-L score. Nevertheless, we do not report an exhaustive search over parameters here for our model, and instead compare it to BlenderBot with the same settings next. When compared to its predecessor, text-only BlenderBot, MMB performs nearly the same on all four text-only datasets, indicating that MMB has not lost its proficiency in text-only dialogue. Additionally, when comparing performance on Image-Chat to models trained on multi-modal data, MMB outperforms Dodeca in terms of F1 score (13.1 vs. 12.9) and outperforms 2AMMC on all three metrics. For the 2AMMC model, these metrics are computed under the assumption that the model's chosen response (from a set of candidate responses collated from the Image-Chat training set) is the "generated" response. 

 Human Evaluations 

 Human/Model Chats Without Images We compare MMB to BlenderBot by having crowdsourced workers chat with our models, over 50 conversations per model. (Henceforth we refer to this model as "MMB Style" to reflect the fact that it was exposed to Image-Chat styles during training: see Appendix B for a description of these styles.) Each conversation consists of 7 turns per speaker, with the human speaking first by saying "Hi!", following the convention of  Adiwardana et al. (2020) . No Image-Chat style is given to MMB Style at the beginning of these conversations, matching its training setup in which no style was given when training on dialogue datasets. Table  14  shows that human ratings of these conversations, including how often they contain issues such as contradiction and repetitiveness, are similar between models. We then perform ACUTE-Evals  (Li et al., 2019b ) on the collected conversations of MMB Style and BlenderBot in order for crowdsourced raters to directly compare conversations from different models in an A/B setting. For each comparison, we ask each rater to compare conversations on one of two metrics, following  Li et al. (2019b) : the Preference metric asks, "Who would you prefer to talk to for a long conversation?", and the Humanness metric asks, "Which speaker sounds more human?". Results are shown in Table  3 : raters choose conversations from one model over the other roughly equally, with no statistically significant differences among models. See Appendix G.2 for reasons that raters give for choosing one model over another. In Table  4 , we also compare MMB Style to two other baseline models, DialoGPT and Meena. Raters are significantly more likely to prefer MMB Style over both of these models with respect to both the preference and humanness metrics. 

 Human/Model Chats About Images We measure MMB Style's ability to chitchat about what it perceives visually by collecting roughly 50 multi-modal conversations between a human and the MMB Style model, for which each conversation discusses an image taken from the test set of Image-Chat. 2 Image-Chat styles are divided into three categories, "positive", "neutral", and "negative" (Appendix B): only Image-Chat images for which the first speaker has a "positive" or "neutral" style are used, and thus images for which the first speaker has a "negative" style are filtered out. For each conversation, the image is first shown to both the human and the model. Then, the model responds to the image, and the human responds to the model to carry the conversation forward. The conversation continues for 6 human utterances and 7 model utterances total. As a comparison, we also collect similar conversations between humans and two previous models trained on Image-Chat data, Dodeca and 2AMMC.  Among the three models, 2AMMC alone is a retrieval model: it retrieves its response from the set of utterances in the Image-Chat training set. Examples of the three models' initial responses to an image are in Table  5 . We then run ACUTE-Evals to ask raters to compare these models' conversational skills on the Preference, Humanness, and Image-response metrics, where the Image-response metric asks, "Who talks about the image better?" The same image is used for both sides of each A/B comparison between conversations. Ratings are shown in  der bias: for instance, there is no safeguard against it misgendering a person in an image, and many common text datasets are known to contain gender bias  (Dinan et al., 2019a (Dinan et al., , 2020a , which may lead to bias in models trained on them. To remedy this, we train a version of the MMB Style model in which we examine the label of each training example to determine whether it contains female or male words, and then a string representing that classification is appended to the example's context string  (Dinan et al., 2019a) , for input to the model. At inference time, the string representing a classification of "no female or male words" is appended to the context, nudging the model to generate a response containing no gendered words. The fraction of utterances produced by this model that still contain gendered words is shown in Table  7 . Compared to the gold response, the original BlenderBot, and MMB Style, this degendered MMB model (which we call "MMB Degendered") reduces the likelihood of producing an utterance with male word(s) by roughly a factor of 9 and of producing an utterance with female word(s) by roughly a factor of 4, given a context from the ConvAI2 validation set. ACUTE-Evals in Table  3  show that this degendering does not lead to a significant drop in the preference for or humanness of the model's responses during a conversation. 

 Removing Dependence on Style Since each of the images that MMB Style saw during training was associated with an Image-Chat style, it relies on an input style during inference in order to be able to discuss an image. However, this results in a model whose utterances will necessarily strongly exhibit a particular style. (For example, see the "Playful" MMB Style response in Table  21 : constricting the model to respond playfully to all images could seem rather contrived and perhaps unlike typical human speech.) To avoid this, we train a version of MMB Style where, for 75% of all images seen during training, the accompanying style is replaced with the string "positive/neutral" or "negative", depending on which list the style was a part of. Thus, during inference, the string "positive/neutral" can be used in lieu of a specific style string in order to produce responses that are unlikely to be negative and that do not consistently display strong adherence to a specific style. We refer to this model as the "MMB Positive" model, or "MMB DegenPos" if it was trained with degendering in addition as in Section 6.1. Table  22  in the appendix shows that these models exhibit little increase in perplexity, with the increase likely due to the loss of specificity provided by a concrete style. The MMB DegenPos model exhibits the same level of degendering as the base MMB Degendered model (Table  7 ), and ACUTE-Evals show that these models exhibit no detectable loss of ability to talk about an image (Table  8 ). See Appendix H.1 for an ablation of MMB Positive in which a model is not shown images at all. 

 Safety The data contains real-world conversations from the Internet; and (3) the Image-Chat dataset has negative styles to better capture the range of human styles. All of these factors could lead to an unsafe response given a multi-modal context. To mitigate this problem, we first measure our models' toxicity using an openly available blocklist 3 and an offensive language classifier presented in . We define the term "toxicity" to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validation set, with a fixed style trait to control the generation, presenting results for different choices of fixed trait. We first evaluate our model in the first round of the Image-Chat validation set. The results in Table  9  indicate that positive styles reduce the level of toxicity by a large margin for both metrics (classifier and blocklist). The results also align well with our previous experiments on degendering, as toxicity is reduced across all styles after applying the degendering process. After degendering, we can considerably improve our model's safety by enforcing that it uses positive styles. We also evaluate our model in the second round of the conversation and collect the statistics based on the first round style, as shown in Table  23 . This result suggests that even if the model is controlled with a positive style, it is less safe when responding to negative conversations.  

 Example 

 Ethical Considerations In this work we present conversational agents that maintain dialogue in a multi-modal setting. Our intention is to ultimately build agents that can meaningfully engage humans in dialogue; in such a setting, humans who chat with our models would benefit from having a chat partner who is personable, knowledgeable, empathetic, and visually perceptive. Our experiments and human evaluations lead us to believe that our models are preferred to alternatives, and beneficial interactions should take place when pairing our models with human conversational partners. It is clear, however, that conversational language can contain offensive statements. Indeed, if no measures or precautions are taken during model training or deployment, conversational models can produce offensive statements as well -this should come as no surprise given the nature of the pretraining data (i.e., Internet chat forums) ), yet it is perhaps even more important in a multi-modal setting, where otherwise safe text can be viewed as offensive given the right (or, in this case, wrong) visual context. As we note in our introduction, safety in opendomain dialogue is a well-researched (and far from solved) issue, and despite our work not focusing specifically on generating safe conversations, we make some efforts to address safety concerns in Section 6.3. As mentioned above, the main goal of this work is to explore and measure the conversational ability of various multi-modal dialogue architectures. Nevertheless, we acknowledge that safety is a major element of human-model discourse, and we note that we dedicate a substantial portion of the paper towards exploring how certain safety mechanisms impact how humans interact with our models. In particular, we can identify several potential ethical failure modes that might arise if this model were used in an irresponsible manner. First, if we were to release this model to the general public as-is without any safety measures in place (such as the ones we discuss above), bad actors could either attempt to find specific dialogues/images for which our model delivers an unsafe response or else deploy this model in a setting that exploits any safety weaknesses in the model. We also acknowledge the potential for remaining bias in the model's responses along demographic lines such as gender, although we address the question of gender bias by degendering our model in Section 6.1. 

 A Details of Image Encoders We test the following image encoders in our MMB models: ResNeXt WSL We first experiment with image representations obtained from pre-training a ResNeXt 32x48d model on nearly 1 billion public images  (Mahajan et al., 2018) , with subsequent fine-tuning on the ImageNet1K dataset  (Russakovsky et al., 2015)  4 . The output of this model is a 2048-dimensional vector, and we refer to these representations as "ResNeXt WSL" features. ResNeXt WSL Spatial One can also take the output of the image encoder prior to its final fullyconnected layer to obtain "spatial" image features, resulting in a 2048?7?7-dimensional vector. We explore results with these features as well, and refer to them as "ResNeXt WSL Spatial". Faster R-CNN Finally, we consider Faster R-CNN features  (Ren et al., 2017) , using models trained in the Detectron framework ; specifically, we use a ResNeXt-152 backbone trained on the Visual Genome dataset  (Krishna et al., 2016)  with the attribute head  (Singh et al., 2020a)  5 . The Faster R-CNN features are 2048?100-dimensional representations, and we refer to these features as "Faster R-CNN". 

 B Dataset Descriptions ConvAI2 The ConvAI2 dataset  (Dinan et al., 2020b)  is based on the Persona-Chat  (Zhang et al., 2018b)  dataset, and contains 140k training utterances in which crowdworkers were given prepared "persona" lines, e.g. "I like dogs" or "I play basketball", and then paired up and asked to get to know each other through conversation. 

 EmpatheticDialogues (ED) The EmpatheticDialogues dataset  (Rashkin et al., 2019)  was created via crowdworkers as well, and involves two speakers playing different roles in a conversation. One is a "listener", who displays empathy in a conversation while conversing with someone who is describing a personal situation. The model is trained to act like the "listener". The resulting dataset contains 50k utterances. two speakers discussing a given topic in depth, comprising 194k utterances. One speaker (the "apprentice") attempts to dive deep on and learn about a chosen topic; the other (the "wizard") has access to a retrieval system over Wikipedia, and is tasked with teaching their conversational partner about a topic via grounding their responses in a knowledge source. 

 BlendedSkillTalk (BST) BlendedSkillTalk  is a dataset that essentially combines the three above. That is, crowdworkers are paired up similarly to the three previous datasets, but now all three "skills" (personalization, empathy, and knowledge) are at play throughout the dialogue: the speakers are tasked with blending the skills while engaging their partners in conversation. The resulting dataset contains 74k utterances. 

 Image-Chat (IC) The Image-Chat dataset  contains 200k dialogues over 200k images: crowdworkers were tasked with discussing an image in the context of a given style, e.g. "Happy", "Cheerful", or "Sad", in order to hold an engaging conversation. The resulting dataset contains over 400k utterances. For each conversation in the dataset, the two speakers are each assigned a style in which that speaker responds, and these styles are optionally fed into models as part of the input, alongside the dialogue context. There are 215 styles in total, and styles are divided into 3 categories, "positive", "neutral", and "negative". 6 

 C Additional Training Details Domain-adaptive Pre-training During domainadaptive pre-training, we trained the model on 8 GPUs for 10k-30k SGD updates, using earlystopping on the validation set. The models were optimized using Adam (Kingma and Ba, 2014), with sweeps over a learning rate between 5e-6 and 3e-5, using 100 warmup steps. Fine-tuning In this stage, we train the models on 8 GPUs for around 10k train updates using a similar optimization setup as in the domain-adaptive pretraining stage.  

 D Automatic Evaluations on Pre-Training Datasets Training Data We first note that, regardless of image fusion and image feature choices, we see the best performance on COCO Captions by simply fine-tuning exclusively on that data. This is an expected result, though we do see that in nearly every scenario the decrease in perplexity is not large (e.g. 5.23 for Faster R-CNN early fusion multi-tasking, down to 4.83 with just COCO Captions). Image Features Across all training setups, we see that using spatially-based image features (ResNeXt WSL Spatial, Faster R-CNN) yields better performance than just a single vector image representation (ResNeXt WSL). This difference is particularly noticeable when training with COCO and pushshift.io Reddit, where with Faster R-CNN features the model obtains an average ppl of 9.13 over the two datasets, while with ResNeXt WSL features the model only obtains 10.1 ppl. We find that using Faster R-CNN features additionally outperforms using ResNeXt WSL Spatial features, where using the latter obtains an average of 10.0 ppl over the two datasets. Image Fusion Finally, holding all other variables constant, we find that using our early fusion scheme yields improvements over using a late fusion scheme. E.g., with Faster-R-CNN features in 6 Lists of positive, neutral, and negative styles are from http://ideonomy.mit.edu/essays/traits. html, following  Shuster et al. (2019a) . the COCO-only setup, we see a decrease in perplexity from 5.21 to 4.83; with ResNeXt WSL Spatial image features, we see perplexity differences ranging from 0.3 to 0.9 depending on the training data. 

 E Ablation Results on Fine-Tuned Datasets Text-Only Datasets First, we look at the performance of our models on the text-only datasets. The second-to-last column in ity on Image-Chat compared to 12.92 and 12.87 respectively for single-task training on Image-Chat (see Table  12 ). Image Fusion Finally, we note as before that using our early fusion technique improves performance on Image-Chat across all ablation regimes. While the average perplexity across the dialogue datasets is best when using late image fusion, we obtain the best image chat perplexity when performing early image fusion.   14 ). 

 F Final Test Results 

 G Additional Human Evaluations Similar human ratings at the end of conversations between a human and a model about an image show that MMB Style beats Dodeca and 2AMMC on measures of engagingness, humanness, and the ability to talk about an image by a large margin (Table  15 ). 

 G.2 Reasons for ACUTE-Eval Ratings For ACUTE-Evals comparing pairs of human/model conversations from different models, crowdsource workers are asked to select among 10 checkboxes to explain their preference for one conversation over another. Workers are able to select multiple checkboxes. Results for ACUTE-Evals on the preference metric are shown in Tables  16, 17 , and 18. 

 G.3 ACUTE-Evals on the models' first response to an image On ACUTE-Evals comparing two models' initial responses to the same image, we find that crowdsource raters choose both the MMB Style and 2AMMC models' responses significantly more often than those of Dodeca (Table  19 ). We also find no significant difference in the rate at which MMB Style image responses are chosen compared to the same model fine-tuned only on Image-Chat and not on dialogue datasets (Table  20 ), which implies that multitasking on dialogue datasets does not degrade the ability to effectively respond to an image. See Table  21  for additional example responses of models to images.    

 H Additional Analyses of Safety and Gender Bias See Table  22  for a table of perplexies of all MMB model variants. Table  23  displays measurements of safety in the second rounds of Image-Chat conversations dependent on whether first round exhibited a positive or negative style. 

 H.1 Analyzing Dependence on Image We also train a no-image ablation model, otherwise equivalent to MMB Positive, for which Image-Chat images are removed during both training and inference: crowdsource workers prefer the image responses of MMB Positive to those of this ablation model 80% to 20% (Table  24 ). For this ablation, style was removed from the context (replaced with the string "positive/neutral") to prevent the ablation model from being aided by this information. 

 I Example Conversations and Failure Modes Figure  1  in particular demonstrates a successful conversation: the model is clearly able to interpret what is in the image (a teddy bear and a road), and it is able to thoughtfully and creatively combine these two subjects in the conversation for several turns. Figure  2  provides several more example conversations: in all of these, the model is able to both discuss the image and use it as a catalyst for further conversation, although occasionally with contradiction and forgetfulness issues as seen in  Roller et al. (2020) . (For instance, the model contradicts itself on whether it has any pets and forgets who is planning to make a fancy dinner.) Last, we show a few hand-picked examples of poor conversations in Figure  3 : in these, the model fails to identify the contents of the images, identifying them both as buildings, although this may reflect a difference in the prevalence of (for example) buildings vs. roller coasters in the training sets. Despite the human nudging the model about what the images actually convey, the model does not demonstrate that it has corrected its initial misidentification in later turns. This could perhaps be remedied by an increase in image training data, by further advancements in the integration of image features with this BlenderBot-based sequence-to-sequence model, or perhaps by training specifically on data in which one partner learns about the contents of an image over time.       Figure 1 : 1 Figure 1: Cherry picked conversation between a paper author (right) and our MMB DegenPos model (left). More sample conversations are in the appendix. 

 MMB I would love to take my wife here for our anniversary. It would be so romantic. Dodeca What a beautiful view! 2AMMC Oh what a great honeymoon spot with the lovely view of the mountains.(Style) Maternal (Mother-like) MMB I would love to take my children here to show them the beauty of the earth and its inhabitants. Dodeca I would love to take my kids here. 2AMMC I would like to hide my kids safe in that cavern from a storm. 

 Figure 2 : 2 Figure 2: Randomly picked author examples. Paper author (right speaker) talking to the MMB DegenPos model (left speaker). Conversations are mostly fluent, with occasional contradictions. 

 Figure 3 : 3 Figure 3: Lemon-picked author examples. Paper author (right speaker) talking to the MMB DegenPos model (left speaker): misidentifying the subject of the image (top); misidentifying the subject of the image and not being able to learn from the chat partner's feedback (bottom). 

 Table 23 : 23 Toxicity of MMB variants as assessed with different control variables. We evaluate on the second round of the Image-Chat validation set. Column "Pos C" shows the safety classifier metric when conditioning on a positive style for the round-1 utterance, and "Pos B" shows the same thing for the blocklist metric. The following two columns show the same metrics when the round-1 utterance has a negative style. MMB Positive With image vs. Without image 80 * 20 *Table 24: ACUTE-Evals show that the MMB Positive model is significantly better at responding to an image than an equivalent model not shown any images during training or inference. 

 Table 1 : 1 Model perplexity measured on the validation data of the datasets described in Section 4.2, across various image features, training data (including domain-adaptive pre-training), and image fusion techniques, where BST + refers to the four text-only dialogue datasets (ConvAI2, ED, WoW, and BST). Performance on the first turn of Image-Chat is measured to highlight model performance when only given visual context. We note that using Faster R-CNN image features results in the best average performance, as well as the best performance on Image-Chat. Image Training Image ConvAI2 ED WoW BST IC 1st IC Text All Features Data Fusion Turn Avg. Avg. None 12.31 10.21 13.00 12.41 32.36 21.48 11.98 13.88 None BST + None 8.74 8.32 8.78 10.08 38.94 23.13 8.98 14.76 BST + + IC 8.72 8.24 8.81 10.03 16.03 13.21 8.95 9.83 BST + + IC Late 8.71 8.25 8.87 10.09 16.20 13.27 8.98 9.84 ResNeXt WSL BST + + IC Early 8.80 8.32 8.79 10.17 15.16 12.99 9.02 9.81 BST + + IC + COCO Late 8.79 8.36 9.00 10.21 16.00 13.31 9.09 9.93 BST + + IC + COCO Early 8.91 8.38 8.99 10.29 14.64 12.85 9.14 9.88 BST + + IC Late 8.70 8,24 8.92 10.07 13.97 12.48 8.98 9.68 Faster R-CNN BST + + IC Early 8.81 8.33 8.81 10.15 13.66 12.43 9.03 9.71 BST + + IC + COCO + Reddit Late 8.75 8.31 8.93 10.14 13.83 12.49 9.03 9.73 BST + + IC + COCO + Reddit Early 8.78 8.31 8.85 10.15 13.51 12.36 9.02 9.69 Model ConvAI2 ED WoW Seen BST IC F1 B R F1 B R F1 B R F1 B R F1 B R DialoGPT 11.4 0.1 8.5 10.8 0.3 8.2 8.6 0.1 5.9 10.5 0.1 7.6 6.2 0.1 5.2 (Zhang et al., 2020) Dodeca 21.7 5.5 33.7 19.3 3.7 31.4 38.4* 21.0* 45.4* - - - 12.9 2.1 24.6 (Shuster et al., 2019b) 2AMMC - - - - - - - - - - - - 9.3 0.1 11.0 (Ju et al., 2019) BlenderBot 18.4 1.1 22.7 19.1 1.4 24.2 18.8 2.3 17.5 17.8 1.0 19.2 9.2 0.1 12.3 (Roller et al., 2020) Multi-Modal BlenderBot 18.4 1.1 22.6 19.2 1.5 24.5 18.6 2.2 17.4 17.8 1.0 19.3 13.1 0.4 18.0 (ours) 

 Table 5 : 5 Example outputs from MMB Style, Dodeca fine-tuned on Image-Chat, and 2AMMC, on images from Shuster et al. (2020). The Image-Chat style fed to the models is shown above the models' responses. Additional examples are in the appendix in Table21. Loss % MMB Dodeca 2AMMC Prefer MMB Style Dodeca 30  *  2AMMC 34  * 70  *  62  * 66  *  38  * Win % Human MMB Style Dodeca 30  *  2AMMC 42  * 70  *  49 58  *  51 Image MMB Style Dodeca 39  *  2AMMC 48 61  *  56 52 44 

 Table 6 : 6 ACUTE-Evals on human/model conversations with images. MMB Style significantly outperforms Dodeca and often 2AMMC on various metrics. 

 Table 6 : 6 MMBStyle performs significantly better than Dodeca and 2AMMC on the preference and humanness metrics, and it performs significantly better than Dodeca on the image-response metric. See Appendix G.3 for similar ACUTE-Eval results that compare models' initial responses to an image.We would like to reduce the ways in which the MMB Style model could potentially display gen- 6 Analysis of Safety and Gender Bias 6.1 Degendering Models 

 Table 7 : 7 The frequency of utterances containing gendered words is greatly reduced for degendered models (MMB Degendered, MMB DegenPos), given contexts from ConvAI2 and the same generation parameters as in Roller et al. (2020) . Loss % Style Degen Pos DP Win % MMB Style MMB Degendered 46 MMB Positive 51 54 52 49 56 48 52 41 MMB DegenPos 44 48 59 

 Table 8 : 8 ACUTE-Evals on the models' first response to an image show no significant differences in how well MMB models can respond to the image, even if the model is degendered or was trained to not require concrete Image-Chat styles. 

 Conversations/Failure Cases 2, and 3. See Appendix I for a discussion of what in these conversations tends to work well, as well as common failure modes. 7 Conclusion In this work, we explored a necessary component of open-domain dialogue models preferred by hu- mans: the ability to perceive and converse in the context of what is seen. We showed that we can match prior work in text-only dialogue in both auto- mated metrics and preference/humanness metrics, and our best model surpasses existing models in multi-modal dialogue. Finally, we demonstrated that we do not sacrifice human preference for our model by incorporating safety components into it. We show several handpicked examples of conversa- tions with our MMB DegenPos model in Figures 1, 

 Table 10 : 10 Model performance, measured via perplexity on validation data, on domain-adaptive pre-training datasets, comparing various image features and image fusion techniques. The top three rows involve multi-task training on COCO Captions and pushshift.io Reddit, while the bottom three rows involve single task training on COCO Captions only. We note that early fusion with Faster R-CNN features yields the best performance on COCO Captions. Image Features Image Fusion COCO (ppl) pushshift.io Reddit (ppl) Average COCO & pushshift.io Reddit training data ResNeXt WSL Late 11.11 13.80 12.45 Early 6.69 13.50 10.10 ResNeXt WSL Spatial Late 7.43 13.00 10.22 Early 6.53 13.46 10.00 Faster R-CNN Late 5.26 13.17 9.21 Early 5.23 13.15 9.13 COCO training data only ResNeXt WSL Late 5.82 19.52 12.67 Early 6.21 21.30 13.76 ResNeXt WSL Spatial Late 6.51 16.50 11.51 Early 6.19 18.77 12.48 Faster R-CNN Late 5.21 17.88 11.55 Early 4.83 18.81 11.82 

 Table 11 11 shows the aver- 

 Table 11 : 11 Ablation analysis of the impact of various image features, training data (including domain-adaptive pretraining), and image fusion techniques on the datasets described in Section 4.2, where BST + refers to the four text-only dialogue datasets (ConvAI2, ED, WoW, and BST). The numbers shown are model perplexities measured on each of the datasets' validation data. Performance on the first turn of Image-Chat is also measured to highlight model performance when only given visual context. We note that using Faster R-CNN image features results in the best average performance, as well as the best performance on Image-Chat. Image Training Image IC First Turn IC Features Data Fusion None None None 32.36 21.48 Image Chat 28.71 13.17 IC Late 14.80 12.83 IC Early 16.00 13.21 IC + COCO + Reddit Late 16.73 13.92 ResNeXt WSL IC + COCO + Reddit Early 15.71 13.53 IC + COCO Late 14.70 12.95 IC + COCO Early 14.62 12.92 IC Late 15.34 13.01 IC Early 15.27 13.00 ResNeXt WSL IC + COCO + Reddit Late 15.09 12.95 Spatial IC + COCO + Reddit Early 15.55 13.50 IC + COCO Late 15.02 12.95 IC + COCO Early 14.62 12.87 IC Late 13.99 12.51 IC Early 13.76 12.42 IC + COCO + Reddit Late 13.75 12.43 Faster R-CNN IC + COCO + Reddit Early 13.44 12.29 IC + COCO Late 13.82 12.48 IC + COCO Early 13.56 12.37 

 Table 12 : 12 Ablation analysis of the impacts of various image features, training data (including domain-adaptive pretraining), and image fusion techniques when training on the Image-Chat dataset alone (i.e., ignoring the text-only dialogue datasets). As in Table11, we note that Faster R-CNN features yield the best results on Image-Chat. that domain-adaptive pre-training indeed improves Image Features Again, we see that using Faster performance on Image-Chat. This difference is R-CNN features leads to dramatic improvements highlighted even more when we measure perfor- compared to using the ResNeXt WSL features (spa- mance on the first turn of Image-Chat, in which the tial or otherwise), yielding 12.36 perplexity on model must generate a response given no textual Image-Chat compared to 12.85 and 12.87 perplex- context: 15.16 to 14.64, 15.34 to 14.76, and 13.66 ity with ResNeXt WSL (non-spatial and spatial re- to 13.51. We note a similar trend in Table 12. spectively) during multi-tasking, and 12.29 perplex- 

 Table 14 : 14 Per-turn annotations and mean engagingness ratings of human/model conversations without images, for MMB Style, MMB Degendered, and the original BlenderBot. MMB Style and BlenderBot perform roughly equivalently on these metrics, with a small drop from degendering. Ranges given are plus/minus one standard deviation. 

 Table 16 : 16 Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB variants with BlenderBot during ACUTE-Evals on the preference metric. Conversations do not include images. MMB Style DialoGPT std. beam DialoGPT min beam 20 Meena Contradicts themselves less 8% 9% 14% 11% Better English 32% 53% 46% 37% Repeats themselves less 13% 3% 13% 13% More on-topic 37% 33% 43% 32% Makes more sense 47% 38% 47% 43% More detailed / less vague 35% 17% 34% 34% More knowledgeable 33% 28% 30% 25% Better listener / more inquisitive 34% 17% 25% 29% More entertaining/witty/thoughtful 17% 14% 21% 20% Other 1% 2% 1% 1% 

 Table 17 : 17 Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB Style to existing text-only models during ACUTE-Evals on the preference metric. Conversations do not include images. Models and generation parameters are as in Table4. 

 Table 18 : 18 Fraction of the time that crowdsource workers select a particular reason for choosing one human/model conversation over another when comparing MMB Style to other multi-modal models during ACUTE-Evals on the preference metric. Conversations are started by the model responding to an image. Models and generation parameters are as in Table6. Loss % MMB Dodeca 2AMMC Win % MMB Style Dodeca 35  *  2AMMC 51 65  *  61  * 49 39  * 

 Table 19 : 19 ACUTE-Evals on the image-response metric show that MMB Style and 2AMMC significantly outperform Dodeca fine-tuned on Image-Chat. ACUTE-Evals are measured on the models' first response to an image only. MMB Style Multi-task vs. FT Image-Chat 48 52 

 Table 20 : 20 ACUTE-Evals show no significant difference on the image-response metric for MMB Style vs. an equivalent model only fine-tuned on Image-Chat and no dialogue datasets. ACUTE-Evals are measured on the models' first response to an image. 

 Table 22 : 22 Perplexities of MMB Style, MMB Degendered, MMB Positive, and MMB DegenPos on the validation set. For Image-Chat, styles are used in the context for all models, for consistency. (MMB Positive and MMB DegenPos observed styles for 25% of Image-Chat examples during training.) BST Conv ED WoW IC Avg Style 10.15 8.78 8.31 8.88 12.36 9.70 Degen 10.14 8.76 8.21 9.01 12.58 9.74 Pos 10.15 8.76 8.27 8.95 12.55 9.74 DP 10.36 8.97 8.34 9.41 12.65 9.95 Style Pos C Pos B Neg C Neg B Cheerful 2.41 0.00 3.81 0.09 Style Relaxed Angry 3.87 67.07 0.00 0.22 6.47 62.62 0.09 0.27 Cruel 77.57 1.42 73.67 0.83 Cheerful 1.50 0.00 3.19 0.09 Dgen Relaxed Angry 2.55 53.90 0.00 0.33 4.43 51.64 0.04 0.31 Cruel 58.28 0.95 57.00 0.84 Pos Pos/Neu Negative 30.96 7.00 0.00 0.22 12.98 31.05 0.22 0.09 Dgen Pos/Neu Negative 25.86 4.56 0.04 0.26 8.86 25.42 0.18 0.27 

			 Unlike in those works, the output of the encoder is then passed to a decoder, as in the late fusion case. 

			 We select only images that fall under a CC-BY license and do not contain recognizable people. 

			 https://github.com/LDNOOBW 

			 https://pytorch.org/hub/ facebookresearch_WSL-Images_resnext/ 5 https://github.com/facebookresearch/ vilbert-multi-task
