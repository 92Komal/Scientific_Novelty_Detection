title
DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings

abstract
Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a contextaware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman's correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.

Introduction Sentence embeddings are used with success for a variety of NLP applications  and many prior methods have been proposed with different learning schemes. ; Logeswaran and Lee (2018);  Hill et al. (2016)  train sentence encoders in a self-supervised manner with web pages and books.  Conneau et al. (2017) ; ;  Reimers and Gurevych (2019)  propose to learn sentence embeddings on the supervised datasets such as SNLI  (Bowman et al., 2015)  and MNLI  (Williams et al., 2018) . Although the supervised-learning approaches achieve better performance, they suffer from high cost of annotation in building the training dataset, which makes them hard to adapt to other domains or languages. Recently, learning sentence embeddings from dialogues has begun to attract increasing attention. Dialogues provide strong semantic relationships among conversational utterances and are usually easy to collect in large amounts. Such advantages make the dialogue-based self-supervised learning methods promising to achieve competitive or even superior performance against the supervised-learning methods, especially under the low-resource conditions. While promising, the issue of how to effectively exploit the dialogues for this task has not been sufficiently explored.  propose to train an input-response prediction model on Reddit dataset  (Al-Rfou et al., 2016) . Since they build their architecture based on the single-turn dialogue, the multi-turn dialogue history is not fully exploited.  Henderson et al. (2020)  demonstrate that introducing the multi-turn dialogue context can improve the sentence embedding performance. However, they concatenate the multi-turn dialogue context into a long token sequence, failing to model intersentence semantic relationships among the utterances. Recently, more advanced methods such as  (Reimers and Gurevych, 2019)  achieve better performance by employing BERT  (Devlin et al., 2019)  as the sentence encoder. These works have in common that they employ a feed-forward network with a non-linear activation on top of the sentence en-coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presents two limitations: (1) It yields a large gap between training and evaluating, since the semantic textual similarity is commonly measured by the element-wise distance metrics such as cosine and L2 distance. (2) Concatenating all the utterances in the dialogue context inevitably introduces the noise as well as the redundant information, resulting in a poor result. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle these issues. We hold that the semantic matching relationships between the context and the response can be implicitly modeled through contrastive learning, thus making it possible to eliminate the gap between training and evaluating. To this end, we introduce a novel matching-guided embedding (MGE) mechanism. Specifically, MGE first pairs each utterance in the context with the response and performs a token-level dot-product operation across all the utterance-response pairs to obtain the multi-turn matching matrices. Then the multi-turn matching matrices are used as guidance to generate a context-aware embedding for the response embedding (i.e. the context-free embedding). Finally, the context-aware embedding and the context-free embedding are paired as a training sample, whose label is determined by whether the context and the response are originally from the same dialogue. Our motivation is that once the context semantically matches the response, it has the ability to distill the context-aware information from the context-free embedding, which is exactly the learning objective of the sentence encoder that aims to produce context-aware sentence embeddings. We train our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) , the Jing Dong Dialogue Corpus (JDDC)  (Chen et al., 2020) , and the E-commerce Dialogue Corpus (ECD)  (Zhang et al., 2018) . To evaluate our model, we introduce two types of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) task. Here we do not adopt the standard semantic textual similarity (STS) task  (Cer et al., 2017)  for two reasons: (1) As revealed in  (Zhang et al., 2020) , the sentence embedding performance varies greatly as the domain of the training data changes. As a dialogue dataset is always about several certain domains, evaluating on the STS benchmark may mis-lead the evaluation of the model. (2) The dialoguebased sentence embeddings focus on context-aware rather than context-free semantic meanings, which may not be suitable to be evaluated through the context-free benchmarks. Since previous dialoguebased works have not set up a uniform benchmark, we construct two evaluation datasets for each dialogue corpus. A total of 18,964 retrieval samples and 4,000 sentence pairs are annotated by seven native speakers through the crowd-sourcing platform 1 . The evaluation results indicate that DialogueCSE significantly outperforms the baselines on the three datasets in terms of both MAP and Spearman's correlation metrics, demonstrating its effectiveness. Further quantitative experiments show that Dia-logueCSE achieves better performance when leveraging more dialogue context and remains robust when less training data is provided. To sum up, our contributions are threefold: ? We propose DialogueCSE, a dialogue-based contrastive learning approach with MGE mechanism for learning sentence embeddings from dialogues. As far as we know, this is the first attempt to apply contrastive learning in this area. ? We construct the dialogue-based sentence embedding evaluation benchmarks for three dialogue corpus. All of the datasets will be released to facilitate the follow-up researches. ? Extensive experiments show that Dia-logueCSE significantly outperforms the baselines, establishing the state-of-the-art results. 2 Related Work 

 Self-supervised Learning Approaches Early works on sentence embeddings mainly focus on the self-supervised learning approaches.  train a seq2seq network by decoding the token-level sequences of the context in the corpus.  Hill et al. (2016)  propose to predict the neighboring sentences as bag-of-words instead of step-by-step decoding.  Logeswaran and Lee (2018)  perform sentence-level modeling by retrieving the ground-truth sentence from candidates under the given context, achieving consistently better performance compared to the previous token-level modeling approaches. The datasets used in these works are typically built upon the corpus of web pages and books . As the semantic connections are relatively weak in these corpora, the model performances in these works are inherently limited and hard to achieve further improvement. Recently, the pre-trained language models such as BERT  (Devlin et al., 2019)  and GPT  (Radford et al.)  yield strong performances across many downstream tasks . However, BERT's embeddings show poor performance without fine-tuning and many efforts have been devoted to alleviating this issue.  Zhang et al. (2020)  propose a self-supervised learning approach that derives meaningful BERT sentence embeddings by maximizing the mutual information between the global sentence embedding and all its local context embeddings.  Li et al. (2020)  argue that BERT induces a non-smooth anisotropic semantic space. They propose to use a flow-based generative module to transform BERT's embeddings into isotropic semantic space. Similar to this work,  Su et al. (2021)  replace the flow-based generative module with a simple but efficient linear mapping layer, achieving competitive results with reported experiments in BERT-flow. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in this area.  Giorgi et al. (2020) ;  Meng et al. (2021)  incorporate the data augmentation methods including the word-level deletion, reordering, substitution, and the sentencelevel corruption into the pre-training of deep Transformer models to improve the sentence representation ability, achieving significantly better performance than BERT especially on the sentence-level tasks  Cer et al., 2017; Conneau and Kiela, 2018) .  Gao et al. (2021)  apply a twice independent dropout to obtain two same-source embeddings from a single sentence as input. Through optimizing their cosine distance, SimCSE achieves remarkable gains over the previous baselines.  Yan et al. (2021)  empirically study more data augmentation strategies in learning sentence embeddings, and it also achieves remarkable performance as SimCSE. In this work, we propose the MGE mechanism to generate a context-aware embedding for each candidate response based on its context-free embedding. Different from previous methods built upon the data augmentation strategies, MGE leverages the context to accomplish this goal without any text corruption. For dialogue,  train a siamese transformer network with single-turn inputresponse pairs extracted from Reddit. Such architecture is further extended in  (Reimers and Gurevych, 2019)  by replacing the transformer encoder with BERT.  Henderson et al. (2020)  propose to leverage the dialogue context to improve the sentence embedding performance. They concatenate the multi-turn dialogue context into a long word sequence and adopt a similar architecture as  to model the context-response matching relationships. Our work is closely related to their works. We propose a novel dialogue-based contrastive learning approach, which directly models the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 

 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets  (Bowman et al., 2015; Williams et al., 2018) .  Conneau et al. (2017)  demonstrate the superior performance of the supervised learning model on both the STS-benchmark  (Cer et al., 2017)  and the SICK-R tasks  (Marelli et al., 2014) . Based on this observation,  further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task  (Henderson et al., 2017; , and the NLI classification task, achieving significant improvement over InferSent.  Reimers and Gurevych (2019)  employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI datasets, achieving the new state-of-the-art performance. 

 Problem Formulation Suppose that we have a dialogue dataset D = {S i } K i=1 , where S i = {u 1 , ? ? ? , u k?1 , r, u k+1 , ? ? ? , u t } is the i-th dia- logue session in D with t turn utterances. r is the response and C i = {u 1 , ? ? ? u k?1 , u k+1 , ? ? ? , u t } is the bi-directional context around r. We omit the subscript i in the following paragraph and use S, C instead of S i , C i for brevity. To generate the contrastive training pairs, we introduce two embedding matrices for r, named context-free embedding matrix and context-aware  embedding matrix. Specifically, we first encode r as an embedding matrix R. Since R is encoded independently of the dialogue context, it is treated as the context-free embedding matrix. Then we generate a corresponding embedding matrix R based on R according to the guidance of C. R is treated as the context-aware embedding matrix. As C and r are derived from the same dialogue, ( R, R) naturally forms a positive training pair. To construct a negative training pair, we first sample an utterance r from a dialogue randomly selected from D. r is encoded as the context-free embedding matrix R based on which a context-aware embedding matrix R is generated through the completely identical process. ( R , R ) is treated as a negative training pair. For each response r, we generate a positive training pair (since there is only one ground-truth response for each context) and multiple negative training pairs. All the training pairs are then passed through the contrastive learning module. It is worth to mention that there is no difference between sampling the response or the context as they are symmetrical in constructing the negative training pairs. But we prefer the former as it is more straightforward and in accordance with the previous retrieval-based works for dialogues. With all the training samples at hand, our goal is to minimize their contrastive loss, thus fine-tuning BERT as a context-aware sentence encoder. 

 Our Approach Figure  1  shows the model architecture. Our model is divided into three stages: sentence encoding, matching-guided embedding, and turn aggregation. We describe each part as below. 

 Sentence Encoding We adopt BERT  (Devlin et al., 2019)  as the sentence encoder. Let u represent a certain utterance in C. u and r are first encoded as two sequences of output embeddings, which is formulated as: {u 1 , u 2 , ? ? ? , u n } = BERT(u), (1) {r 1 , r 2 , ? ? ? , r n } = BERT(r), (2) where u i , r j represent the i-th and the j-th output embedding derived from u and r respectively. n is the maximum sequence length of both input sentences. ?i, j ? 1, 2, ? ? ? , n, the shapes of u i and r j are 1 ? d, where d is the dimension of BERT's outputs. We stack {u 1 , u 2 , ? ? ? , u n } and {r 1 , r 2 , ? ? ? , r n } to obtain the context-free embedding matrices ? and R, whose shapes are both n ? d. 

 Matching-Guided Embedding The matching-guided embedding mechanism performs a token-level matching operation on ? and R to form a matching matrix M, which is formulated as: M = ? R T ? d , (3) Then it generates a refined embedding matrix R based on the context-free embedding matrix R, which is given by: R = M R (4) R is a new representation of r from the perspective of the utterance u. Note that as u is only a single turn utterance in C, we generate t ? 1 refined embedding matrices for r in total. 

 Turn Aggregation After obtaining all of the refined embedding matrices across turns, we consider two strategies to fuse them to obtain the final context-aware embedding matrix R. The first strategy adopts a weighted sum operation based on the attention mechanism, formulated by: R = i ? i Ri , (5) where i ? {1, ? ? ? , k ? 1, k + 1, ? ? ? , t} and Ri is the refined embedding matrix corresponding to the i-th turn utterance in the context. The attention weight ? i is decided by: ? i = exp(FFN( Ri )) j exp(FFN( Rj )) , ( 6 ) where FFN is a two-layer feed-forward network with ReLU (Nair and Hinton, 2010) activation function. We denote this strategy as I 1 . The second strategy I 2 directly sums up all the refined embeddings across turns, which is defined as: R = 1 t ? 1 i Ri , (7) For the negative sample r , we apply the same procedure to generate the context-free embedding matrix R and the context-aware embedding R . Each context-aware embedding matrix is then paired with its corresponding context-free embedding matrix to form a training pair. As mentioned in the introduction, MGE holds several advantages in modeling the contextresponse semantic relationships. Firstly, the tokenlevel matching operation acts as a guide to distill the context-aware information from the contextfree embedding matrix. Meanwhile, it provides rich semantic matching information to assist the generation of the context-aware embedding matrix. Secondly, MGE is lightweight and computationally efficient, which makes the model easier to train than the siamese-network-based models. Finally and most importantly, the context-aware embedding R shares the same semantic space with R, which enables us to directly measure their cosine similarity. This is the key to successfully model the semantic matching relationships between the context and the response through contrastive learning. 

 Learning Objective We adopt the NT-Xent loss proposed in  (Oord et al., 2018)  to train our model. The loss L is formulated as: L = ? 1 N N i=1 log e sim( Ri , Ri )/? M j=1 e sim( Rj , Rj )/? , ( 8 ) where N is the number of all the positive training samples and M is the number of all the training pairs associated with each positive training sample r. ? is the temperature hyper-parameter. sim(?, ?) is the similarity function, defined as a token-level pooling operation followed by the cosine similarity. Once the model is trained, we take the mean pooling of BERT's output embeddings as the sentence embedding. 

 Experiments We conduct experiments on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) , the Jing Dong Dialogue Corpus (JDDC)  (Chen et al., 2020) , and the Ecommerce Dialogue Corpus (ECD)  (Zhang et al., 2018) . Each utterance in these three datasets is originally assigned with an intent label, which is further leveraged by us in the heuristic strategy to construct the evaluation datasets. JD 2 . Although the dataset collected from the realworld scenario is quite large, it contains much noise which brings great challenges for our model. The E-commerce Dialogue Corpus is a large-scale dialogue dataset collected from Taobao 3 . The released dataset takes the form of the response selection task. 

 Experimental Setup 

 Training We recover it to the dialogue sessions by dropping the negative samples and splitting the context into multiple utterances. We pre-process these datasets by the following steps: (1) We combine the consecutive utterances of the same speaker. (2) We discard the dialogues with less than 4 turns in JDDC and ECD since such dialogues are usually incomplete in practice. 

 Evaluation We introduce the semantic retrieval (SR) and the dialogue-based STS (D-STS) tasks to evaluate our model. For the SR task, we construct evaluation datasets by the following steps: (1) we sample a large number of sentences with the intent labels as candidates. (2) the candidates are annotated with binary labels indicating whether the given sentence and its intent label are consistent. The inconsistent instances are directly discarded from the candidates. (3) for each sentence, we retrieval 100 sentences through BM25  (Robertson and Zaragoza, 2009)  from the candidates, and assign each candidate sentence a label by whether its intent is consistent with the target sentence. We limit the number of positive samples to a maximum of 30 and keep approximately 7k, 7k, and 4k samples for MDC, JDDC, and ECD respectively. For the D-STS task, we sample the sentence pairs from the dialogues following the heuristic strategies proposed by  (Cer et al., 2017)  to ensure there are enough semantically similar samples. The heuristic strategies include unigram-based and w2v-based KNN retrieval methods and random sampling from the candidates with the same intent labels. The sentence pairs are further annotated through the crowd-sourcing platform, with five degrees ranging from 1 to 5 according to their semantic relevance. We use the median number of annotated results as the semantic relevance degrees, obtaining 1k, 2k, and 1k sentence pairs for MDC, JDDC, and ECD respectively. All annotations are carried out by seven native speakers. For the SR task, we adopt the Mean average precision (MAP) and the Mean reciprocal rank (MRR) metrics. Following previous works, we adopt Spearman's correlation metric for the D-STS task to assess the quality of the dialogue-based sentence embeddings. 

 Baselines We evaluate our model against the two groups of baselines: self-supervised learning methods and dialogue-based self-supervised learning methods. The former is not designed for dialogues while the latter is. 

 Self-supervised learning methods In this line, we consider the BERT-based methods, which include BERT  (Devlin et al., 2019) , domain-adaptive  BERT (Gururangan et al., 2020) , BERT-flow  (Li et al., 2020) , and BERT-whitening  (Su et al., 2021) . "Domain-adaptive BERT" means that we run continue pre-training with the dialogue datasets. BERT-flow and BERT-whitening are two BERT-based variants that transform BERT's sentence embedding to the isotropic semantic space. For BERT, we use the [CLS] token embedding (denoted as BERT-CLS) and the average of the sequence output embeddings (denoted as BERT-avg) as the sentence embedding, and the same is true for domain-adaptive BERT. It should be noted that in related sentence embedding researches, domainadaptive BERT is rarely considered since the training datasets are relatively small. Fortunately, the large-scale dialogue datasets allow us to explore whether the domain-adaptive pre-training is helpful for our tasks. We also adopt the average of GloVe word embeddings  (Pennington et al., 2014 ) (denoted as Avg. GloVe) as the sentence embedding to compare with our results. 

 Dialogue-based self-supervised learning methods In this line, we mainly consider the siamesenetworks commonly applied in dialogue-based researches. Considering none of the previous works  Henderson et al., 2020)  employs the pre-trained language model as encoder, we re- implement two BERT-based siamese-network models according to their original approaches. The first baseline SiameseBERT s is a siamese-network which shares the architecture with  Reimers and Gurevych, 2019) . It is equipped with a non-linear activation function in the matching layer to model the heterogeneous matching relationships between the context and the response 4 . The second baseline SiameseBERT m has the similar architecture as  (Henderson et al., 2020) . It flattens the multi-turn context and takes the token sequence as input. There is also an MLP layer on top of the sentence encoders. 

 Implementation Details Our approach is implemented in Tensorflow  (Abadi et al., 2016)  with CUDA 10.0 support. For all datasets, we continue pre-training BERT for approximately 0.5 epochs to improve its domain adaption ability as well as keeping the general domain information as much as possible. During the continue pre-training stage, we use a masking probability of 0.15, a learning rate of 2e-5, a batch size of 50, and a maximum of 10 masked LM predictions per sequence. During the contrastive learning stage, we freeze the bottom 6 layers of BERT to prevent catastrophic forgetting which simultaneously en-ables the model to be trained with larger batch size. Such a setting achieves the best performance in our experiments. The batch size, the learning rate, and the number of context turns are set to 20, 5e-5, and 3 respectively. The maximum sequence length is set to 100, 50, 50 for JDDC, MDC, and ECD for both continue pre-training stage and contrastive learning stage. All models are trained on 4 Tesla V100 GPUs. 

 Evaluation Results Table  2  shows the main experimental results on the three datasets. From the There are even larger improvements between Di-alogueCSE and the domain-adaptive baselines including BERT(adapt) and its variants. We attribute this improvement to two main reasons: First, by introducing contrastive learning, DialogueCSE eliminates the gap between training and evaluating, gaining significant improvements on both SR and D-STS tasks. Second, DialogueCSE models the semantic relationships in each utterance-response pair, which distills the important information at turn-level from the multi-turn dialogue context and achieves better performance. Moreover, by comparing the performances of DialogueCSE I 1 and DialogueCSE I 2 , we find that the weighted sum aggregation strategy surprisingly brings a significant deterioration on all metrics. We consider that this is because the weighted sum operation breaks down the turn-level unbiased aggregation process. Since the attention mechanism tends to provide shortcuts for the model to achieve its learning objective, the long-tail utterances in the context may be partially ignored, thus leading to a decline in embedding performance. We hold that we can completely dismiss the weighted sum aggregation strategy in DialogueCSE since the tokenlevel matching operation in MGE has implicitly served this role. We also notice that BERT(adapt) achieves significantly better performance than the original BERT, especially on JDDC and ECD. It demonstrates the importance of continued pre-training with the indomain training data. Without such procedure, the in-domain data can't be fully exploited, making it difficult for the model to achieve satisfactory performance. This also indicates that the MLM pre-training task is indeed a powerful task to learn effective sentence embeddings from texts, especially when the domain training data is sufficient. 

 Discussion We conduct comparison and hyper-parameter experiments in the following section to study how our model performs with different numbers of turns, data scales, temperature hyper-parameter, and numbers of negative samples. 

 Comparison with Baseline In this section, we choose SiameseBERT m as a comparison method. MAP and Spearman's correlation metrics are adopted in these experiments. Impact of turn number. Figure  2  shows the performance of our model and the baseline under different numbers of turns on all datasets. From the results, we observe that our model is indeed benefited from the multi-turn dialogue context, and it exhibits consistently better performance than the baseline. The performance of our model increases as the turn number increases until it approximately arrives at 3. When the turn number goes bigger, the performance of both models begins to drop. We believe that in this case, adding more dialogue context will bring too much noise. Since MGE acts as a noise filter at both token and turn level, it makes the model more robust when using more context turns. Impact of data scale. We further explore whether our model is robust when fewer training samples are given. we select JDDC and ECD in this experiment since they are large-scale and topically diverse, which is suitable for simulating a few-shot learning scenario. Figure  3  shows the performances of our model and the baseline under different numbers of training dialogues. As the figure reveals, the performance gaps between our model and the baseline are even larger when fewer training dialogue sessions are given. Particularly, when using only a few dialogues, our model can achieve even superior performance over the SiameseBERT trained on larger datasets, especially on the D-STS task. We think this is reasonable since the siamese-networks introduce a large amount parameters to model the semantic matching relationships, while our model accomplishes this goal without introducing any additional parameters. 

 Hyper-parameter Evaluations We further conduct experiments on JDDC and EDC to study how our model is influenced by the temperature ? and the number of negative samples. The MDC dataset is excluded here since the semantics of its utterances are highly centralized around a few top intents. Impact of temperature. Table  3  shows the experimental results with different ? values. We find that the Spearman's correlations increase monotonically as ? increases until 0.1 for JDDC and 0.2 for ECD, then they begin to drop. The MAP metrics also increase as ? increases until 0.1 for both datasets, but they remain stable as ? varies from 0.1 to 0.5. We consider this is due to the coarsegrained nature of the SR task. When ? approaches 0.1, our model can gradually distinguish among different fine-grained semantics, thus achieving better performance on both SR and D-STS tasks. As ? continues to increase, the model forces the sentence embeddings to be closer, resulting in a decrease in Spearman's correlation. However, as all positive samples in the candidates have identical labels, such degradation may not be fully reflected through the ranking metric (e.g. MAP) or even be covered as the number of retrieved positive samples changes. Impact of negative samples. We vary the number of negative samples for each positive sample within {1, 4, 9, 19}. Table  4  shows the experimental results, from which we find that both metrics improve slightly when the number of negative samples increases. Considering the similar observation in  (Gao et al., 2021; Yan et al., 2021) , we conclude this phenomenon may be related to the discrete nature of language. Specifically, as the generation of the sentence embeddings in our approach is guided and constrained by the token-level interaction mechanism, our model is more robust than the other contrastive learning approaches and is even effective when only one negative sample is provided. 

 Conclusion In this work, we propose DialogueCSE, a dialoguebased contrastive learning approach to learn sentence embeddings from dialogues. We also propose uniform evaluation benchmarks for evaluating the quality of the dialogue-based sentence embeddings. Evaluation results show that DialogueCSE achieves the best result over the baselines while adding no additional parameters. In the next step, we will study how to introduce more interaction information to learn the sentence embeddings and try to incorporate the contrast learning method into the pre-training stage. Figure 1 : 1 Figure 1: Model architecture. (1) We use BERT to encode the multi-turn dialogue context and the responses, all of the BERT encoders share the same parameters. (2) The matching-guided embedding (MGE) mechanism performs the token-level matching between each utterance and a response, generates multiple refined embeddings across turns. (3) All refined embedding matrices are aggregated to form a context-aware embedding matrix, which is further pooled along the sequence dimension. 
