title
Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State Tracking

abstract
Zero-shot cross-domain dialogue state tracking (DST) enables us to handle task-oriented dialogue in unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot description enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes dialogue context and slots with a pre-trained self-attentive encoder, and generates slot values in an auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information across slots to facilitate cross-domain knowledge transfer. Experimental results on the MultiWOZ dataset show that our proposed method significantly improves existing stateof-the-art results in the zero-shot cross-domain setting.

Introduction Task-oriented dialogue systems are designed to assist users in performing daily activities, such as restaurant booking, travel planning, and online shopping. These virtual assistants provide natural language interfaces to services and online APIs . Based on users' needs, these systems frequently require support for new domains. However, the current state-ofthe-art systems require a substantial amount of indomain data to properly model a new domain. The data-collection process is both expensive and timeconsuming, and thus it is very important to study methods that can build robust and scalable dialogue systems using little to no in-domain data. The dialogue state tracking (DST) is an essential component of task-oriented dialogue systems that tracks users' requirements over multi-turn conversations. A popular formulation of the dialogue state is in the form of a list of slot-value pairs. In DST, tracking unseen slots in a new domain, a.k.a. zeroshot domain adaptation, is a significant challenge, since the model has never seen in-domain training samples. There are two main lines of work to tackle this problem. The first proposes domain transferable models using copy mechanisms or ontology graph information  Zhou and Small, 2019) . A limitation of such models is that they may not fully leverage pre-trained language models due to the specialized model architecture. The second line of work uses slot-descriptions as input to the model to facilitate the slot understanding . However, the provided slot descriptions are collected by crowd sourced human annotators and might be inconsistent among different domains. In general, the optimal approach for constructing slot descriptions in zero-shot settings remains unexplored. In this work, we tackle the challenge of zeroshot cross-domain DST via leveraging large scale pre-trained sequence-to-sequence (seq2seq) models and with effective encoding of slot descriptions. We first introduce a generative DST model called T5DST, which models the relation of a slot and its dialogue context with a self-attentive encoder, and generates the slot value with a decoder in an autoregressive manner. This simple design allows us to effectively incorporate a pre-trained seq2seq model (e.g., T5  (Raffel et al., 2020) ) without any task-specific modification. To further enhance the model's cross-domain transferability, we propose Slot Type Informed Descriptions that capture the shared information of different slots. Experimental results on the MultiWOZ benchmark  (Budzianowski et al., 2018)  suggest that 1) our model achieves significantly higher joint goal accuracy compared to existing results in zero-shot cross domain DST; 2) models using the proposed slot description formulation substantially outperform those using other slot description variants. Our contributions are summarized as the following: ? We propose a simple yet novel generative DST model based on T5 that significantly improves existing zero-shot cross-domain DST results; ? We investigate the effectiveness of different slot description formulations. To the best of our knowledge, this is the first work that comprehensively studies the effectiveness of slot descriptions in zero-shot cross-domain DST. 

 Related Work Dialogue State Tracking has been of broad interest to the dialogue research community  (Williams and Young, 2007; Williams et al., 2014; Heck et al., 2020; Wu et al., 2020; . Current state-of-the-art models  Heck et al., 2020; Hosseini-Asl et al., 2020; Ye et al., 2021;  trained with extensive annotated data have been shown promising performance in complex multi-domain conversations  (Budzianowski et al., 2018) . However, collecting large amounts of data for every  domain is costly and inefficient. To address this issue, several methods  Zhou and Small, 2019)  have proposed for transferring prior knowledge of existing domains to new ones. On the other hand,  Campagna et al. (2020)  proposed an abstract dialogue model that leverages the ontology and in-domain templates to generate a large amount of synthesized data for domain adaptation. Different from their method, in this paper, we utilize a pre-trained seq2seq model and slot descriptions for cross-domain DST without any in-domain data. Slot Description has been shown to be a promising technique in cross domain semantic parsing  (Bapna et al., 2017; Shah et al., 2019; Namazifar et al., 2020) . To encourage this line of research in DST as well, MultiWOZ2.1  (Eric et al., 2019)  provides a further annotation for slot descriptions.  incorporated slot descriptions for facilitating cross domain DST, while  Gao et al. ( , 2020  formulated DST as a question answering problem by casting a slot name into questions. However, these works did not show the effectiveness of slot descriptions, by comparing the performance of models with and without them. There is no study on how to construct slot descriptions. In this paper, we aim to fill this research gap by providing an empirical study on the different slot description formulations. 

 Methodology 

 T5DST The design of our model follows the basis of generative question answering models. As illustrated in Figure  1 , given a dialogue history which consists of an alternating set of utterances from two speakers, denoted as C t = {U 1 , R 1 , . . . , R t?1 , U t }, we add the "user:" and "system:" prefixes to the user and system utterance respectively. Then all the utterances and slot names s i are concatenated into a single sequence, i.e., user:U 1 . . .system:R t?1 user:U t [sep] s i . The sequence is used as the input to the encoder, and the decoder generates the corresponding slot value v i : v i = Seq2seq(C t , s i ). (1) The learning objective of this generation process is minimizing the negative log-likelihood of v i given C t and s i , that is, L = ? n i log p(v i |C t , s i ), (2) where n is the number of slots to be tracked. We initialize the model parameters with T5 (Raffel et al., 2020), an encoder-decoder Transformer with relative position embeddings  (Shaw et al., 2018)  pre-trained on a massive amount of English text. We denote our model as T5DST. To incorporate slot descriptions into T5DST, we replace the slot name with its corresponding slot description as the model input. 

 Slot Type Informed Descriptions Although different slots may have distinguishing names, they can share the same slot type. As shown in Table  1 , the slot type of hotel-stars and restaurant-book people are both number slots, while hotel-internet and hotel-parking are both boolean slots. In light of these observations, we hypothesize that adding slot type information to the slot description facilitates the knowledge transfer among different slots. We construct a template for each slot type that follows "[slot type] of [slot] of the [domain]". We denote such a slot description as Slot Type. More details are available in Appendix A.1. 

 Experiments 

 Dataset and Evaluation We evaluate the proposed method on the Mul-tiWOZ 2.0 dataset  (Budzianowski et al., 2018) , which has 7 domains. We use the pre-processing and evaluation setup from , where restaurant, train, attraction, hotel, and taxi domains are used for training, as the test set only contains these 5 domains. In the zero-shot cross-domain experiments, the models are first trained with four domains and then evaluated on the test-set of the unseen domain. Joint goal accuracy is used to evaluate the performance of the models. The generated dialogue states are considered to be correct if and only if all of the predicted values exactly match the oracle values. 

 Implementation We implement T5DST 1 based on the T5small (60M parameters) model which has 6 encoder-decoder layers and the hidden size d model = 512. All models are trained using an AdamW  (Loshchilov and Hutter, 2018)  optimizer with the initial learning rate of 0.0001. In all crossdomain zero-shot experiments, we train the models with batch size 128 for 5 epochs. For the few-shot Table  3 : Few-shot experimental results in MultiWOZ 2.0. We evaluate our proposed model with 1%, 5%, and 10% in-domain data, against TRADE  and DSTQA  (Zhou and Small, 2019) . experiments, the models are first trained on 4 domains for 5 epochs then fine-tuned with 1%, 5% and 10% of target domain data for 10 epochs. For full shot training, we train our model for at most 10 epochs with batch size 64 and early stop according to the loss in the validation set. Other hyperprameters are same as zero-shot cross-domain setting. We use 8 NVIDIA V100 GPUs for all of our experiments. We use greedy decoding in test time. 

 Baselines 

 Models TRADE. Transferable dialogue state generator  which utilizes copy mechanism to facilitate domain knowledge transfer. SUMBT. Slot-utterance matching belief tracker  based on the language model BERT  (Devlin et al., 2018) . DSTQA. Dialogue state tracking via question answering 2 over ontology graph  (Zhou and Small, 2019) . 

 SimpleTOD++. SimpleTOD (Hosseini-Asl et al., 2020) uses a single causal language model GPT2  (Radford et al., 2019)  to generate the dialogue states. To adapt this model to a zero-shot cross-domain setting, we also provide the slot name as the model input. We denote this model as SimpleTOD++. 

 Slot Description Variants Human. Human annotated slot descriptions collected in MultiWOZ2.1  (Eric et al., 2019)  and used in MultiWOZ2.2 . Naive. Simple transformation of the slot name from "domain-slot" to "[slot] of the [domain]". Slot Value. Following recent works  (Zhang et al., 2019; , slots are divided into categorical and non-categorical slots. For categorical slots, we incorporate the candidate values into the slot description, i.e., "[slot] of the [domain] is [value-1] or [value-2]?". The order of values is random. For non-categorical slots, their descriptions are the same as aforementioned Naive. Question. Similar to  (Gao et al., , 2020 , we reformulate the slot into a natural language question, i.e., "What is the [slot] of the [domain] that is the user interested in?". 

 Results & Discussion 

 Zero-Shot Cross-Domain The results of the zero-shot cross domain experiments are shown in Table  2 . Overall, T5DST achieves significantly higher performance in terms of averaged joint goal accuracy compared to the three baseline models TRADE, SUMBT, and Sim-pleTOD++. These results demonstrate that our model can effectively capture the slot-context relation, and thus generalize better in unseen domains. Replacing slot-names with human annotated slot descriptions does not bring improvement to the zero-shot performance. This might because of the diverse and inconsistent human descriptions among different domains. For example, the human descriptions of attraction-area and restaurantarea are "area to search for attractions" and "area or place of the restaurant" respectively. Such inconsistent descriptions increase the challenge on slot understanding in the zero-shot learning setting. the model using naive slot descriptions gives similar performance to the one that uses original slot names. The two approaches lead to similar semantic representation of the slots. In contrast, incorporating slot values hurts the learning, leading to a lower joint goal accuracy in the restaurant domain. We observe that even though adding value candidates improve some of the categorical slots (e.g., restaurant-area 68.35% ? 82.25% slot accuracy), it hurts the unseen non-categorical slots (e.g., restaurant-food 40.63% ? 26.10% slot accuracy). These non-categorical slots are usually the  bottlenecks of joint goal accuracy. Finally, models trained with question style descriptions improves the performance in some domains, but fails in the others. Our proposed slot type informed descriptions consistently improves the zero-shot performance of T5DST in all the domains. It produced an average of 2% joint goal accuracy improvement compared to human labeled and naive description formulations. This result indicates that slot type information may better capture the shared property (e.g., time, location) among different slots, thus facilitating the domain knowledge transferring for DST. Figure  3  and 4 show the slot accuracy of models using Naive and Slot Type description. Compared to naive description, we obverse significant gain of time slots (e.g., arrive by and leave at), location slots (e.g., departure and destination), and number slots (e.g., book stay and book people) by adding slot type information. We conjecture that explicit information about the target value (i.e., slot type) is important in the low resource condition when the model does not have enough data to capture the semantic meaning of a new slot. 

 Few-Shot Cross-Domain We further conduct experiments in few-shot crossdomain settings, as in  Zhou and Small, 2019) , where the models are first trained on 4 domains then fine-tuned with 1%, 5% and 10% of target domain data. As shown in Table  3 , our model outperforms the DSTQA model in 4 out of 5 domains. Moreover, our approach is more practical in a real-world learning scenario as it does not require the supervision of a full ontology graph. We also conduct the full shot experiments and compare our model with previous methods. The reults are reported in Appendix A.2. 

 Conclusion In this paper, we propose leveraging large scale pretrained models with an effective slot description formulation to tackle the zero-shot cross-domain DST challenge. Specifically, we propose T5DST, a novel generative DST model based on the T5 language model, and incorporate Slot Type Informed Descriptions to facilitate cross-domain knowledge transfer. In the evaluation on the MultiWOZ dataset, our approach substantially improves existing results in both the zero-shot and few-shot settings. 
