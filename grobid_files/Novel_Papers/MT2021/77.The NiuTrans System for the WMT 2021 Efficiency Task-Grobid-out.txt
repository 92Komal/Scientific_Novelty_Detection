title
The NiuTrans System for the WMT21 Efficiency Task

abstract
This paper describes the NiuTrans system for the WMT21 translation efficiency task 1 . Following last year's work, we explore various techniques to improve the efficiency while maintaining translation quality. We investigate the combinations of lightweight Transformer architectures and knowledge distillation strategies. Also, we improve the translation efficiency with graph optimization, low precision, dynamic batching, and parallel pre/postprocessing. Putting these together, our system can translate 247,000 words per second on an NVIDIA A100, being 3? faster than our last year's system. Our system is the fastest and has the lowest memory consumption on the GPU-throughput track. The code, model, and pipeline will be available at NiuTrans.NMT 2 .

Introduction Large and deep Transformer models have dominated machine translation (MT) tasks in recent years  (Vaswani et al., 2017; Edunov et al., 2018; Raffel et al., 2020) . Despite their high accuracy, these models are inefficient and difficult to deploy  (Wang et al., 2020a; Hu et al., 2021; . Many efforts have been made to improve the translation efficiency, including efficient architectures  (Li et al., 2021a,b) , quantization  (Bhandare et al., 2019; , and knowledge distillation  Lin et al., 2021a) . This work investigates efficient Transformers architectures and optimizations specialized for different hardware platforms. In particular, we study deep encoder and shallow decoder Transformer models and optimize them for both GPUs and CPUs. Starting from an ensemble of three deep Transformer teacher models, we train various student models via sequence-level knowledge distil-1 http://statmt.org/wmt21/ efficiency-task.html 2 https://github.com/NiuTrans/NiuTrans. NMT lation (SKD)  (Hinton et al., 2015; Kim and Rush, 2016)  and data augmentation  (Shen et al., 2020) . We find that using a deep encoder (6 layers) and a shallow decoder (1 layer) gives reasonable improvements in speed while maintaining high translation quality. We improve the student model's efficiency by removing unimportant components, including the FFN sub-layers and multi-head mechanism. We also explore other model-agnostic optimizations, including graph optimization, dynamic batching, parallel pre/postprocessing, 8-bit matrix multiplication on CPUs, and 16-bit computation on GPUs. Section 2 describes the training procedures of the deep teacher models. Then, Section 3 presents various optimizations for reducing the model size, improving model performance and efficiency. Finally, Section 4 details the accuracy and efficiency results of our submissions for the shared efficiency task. 

 Model Overview Following  Hu et al. (2020) ,  and  Lin et al. (2021a) , we use the SKD method to train our models. Our experiments also show that the SKD method can obtain better performance than the word-level knowledge distillation (WKD) method, similar to  Kim and Rush (2016) . Therefore, all of student models are optimized by using the interpolated SKD method  (Kim and Rush, 2016) , and trained on data generated from the teacher models. 

 Deep Transformer Teacher Models Recently, researchers have explored deeper models to improve the translation quality  Dehghani et al., 2019; Wang et al., 2020b) . Inspired by them, we employ deep Transformers as the teacher models. More specifically, we train three teachers with different configurations, including Deep-30, Deep-12-768, and Skipping Sublayer-40. We also utilize   . The number of encoder layers is 40 and model's other setups are same as . We adopt the relative position representation (RPR)  (Shaw et al., 2018)  to further improve the teacher models and set the key's relative length to 8. 

 Lightweight Transformer Student Models Although the ensemble teacher model delivers excellent performance, our goal is to learn lightweight models. The natural idea is to compress knowledge from an ensemble into the lightweight model using knowledge distillation  (Hinton et al., 2015) . We employ sequence-level knowledge distillation on the ensemble teacher model described in Section 2.1. Seqence-level Knowledge Distillation The SKD will make a student model mimic the teacher's behaviors at the sequence level. Moreover, the method considers the sequence-level distribution specified by the model over all possible sequences t ? T . Following  Kim and Rush (2016) , the loss function of SKD method for training students is L SKD ? ? t?T 1{t = ?} log p(t | s) (1) = ? log p(t = ? | s) (2) where 1{?}is the indicator function, ? is the output of teacher model using beam search, s symbolizes the source sentence and p(?|?) denotes the conditional probability. We use the ensemble teacher model to generate multiple translations of the raw English sentences. In particular, we collect the 5best list for each sentence against the original target to create the synthetic training data. However, we select only 12 million synthetic data to train our student models to reduce training costs. We find that student models will not have better performance when increasing the number of training data. Fast Student Models As suggested in  Hu et al. (2020) , the bottleneck of translation efficiency is the decoder part. Hence, we accelerate the decoding by reducing the number of decoder layers and removing multi-head mechanism 3 . Inspired by  Hu et al. (2021) , we design the lightweight Transformer student model with one decoder layer. We further remove the multi-head mechanism in the decoder's attention modules. Table  1  shows that the Transformer student model with one decoder layer and one decoder attention head can achieve similar translation quality to the baseline. Therefore, we train four different student models based on the Transformer architecture with one decoder layer and one decoder attention head. Those student models are described in detail in the Table  2 . Besides, experiments show that adding more encoder layers cannot improve the performance when the student model has 12 encoder layers. Therefore, our submissions have 12 encoder layers at most. 

 Data and Training Details Our data is constrained by the condition of the WMT 2021 English-German news translation task 4 , and we use the same data filtering method as  Zhang et al. (2020) . We select 20 million pairs to train our teacher models after filtering all official released parallel datasets (without official synthetic datasets). The data is tokenized with Moses tokenizer  (Koehn et al., 2007)  Encoded (BPE)  (Sennrich et al., 2016)  with 32K merge operations using a shared vocabulary. After decoding, we remove the BPE separators and detokenize all tokens with Moses detokenizer  (Koehn et al., 2007) . 

 Teacher Models Training We train three teacher models using newstest19 as the development set with Fairseq  (Ott et al., 2019) . We share the source-side and target-side embeddings with the decoder output weights. We use the Adam optimizer (Kingma and Ba, 2015) with ? 1 = 0.9, ? 2 = 0.997 and = 10 ?8 as well as gradient accumulation due to the high GPU memory footprints. Each model is trained on 8 TITAN V GPUs for up to 11 epochs. The learning rate is decayed based on the inverse square root of the update number after 1,6000 warm-up steps, and the maximum learning rate is 0.002. After training, we average the last five checkpoints in the training process for all models. Similar to  Zhang et al. (2020) , we train our teacher models with a round of back-translation with 12 million monolingual data selected from the News crawl and News Commentary. We train three De?En models with the same method and model setup to generate pseudo-data. Table  3  shows the results of all teacher models and their ensemble, where we report SacreBLEU  (Post, 2018)  and the model size. Our final ensemble teacher model can achieve a BLEU score of 33.4 on newstest20. 

 Student Models Training The training settings for student models are the same for the teacher models, except its learning rate is 0.0007 and warmupupdates is 8000. In addition, we also use the cutoff method  (Shen et al., 2020)  to boost our student models 5 and we train our student model with 21 epochs. Table  2  shows the results of all student models. Our student model yields a significant speedup (2?-2.6?) with modest sacrifice in terms of BLEU (0.2-0.9 on newstest20). 

 Interpretation of Results After training the final student models, we evaluate their BLEU scores on the English-German newstest20, newstest19, and newstest18 before any inference optimization. Results show that the student models can achieve very similar performance to the teachers. For instance, the Student-12-1-512 model delivers a loss of 0.2 BLEU score compared to the ensemble of teacher models. 

 Optimizations for Decoding Our optimizations for decoding are implemented with NiuTensor 6 . The optimizations can be divided into three parts, including optimizations for CPUs, GPUs, and device-independent techniques. 

 Optimizations for GPUs For the GPU-based decoding, we mainly explore dynamic batching and FP16 inference. Dynamic Batching Unlike the CPU version, the easiest way to reduce the translation time on GPUs is to increase the batch size within a specific range. We implement a dynamic batching scheme that maximizes the number of sentences in the batch while limiting the number of tokens. This strategy significantly accelerates the inference compared to a fixed batch size when the sequence length is short. FP16 Inference Since the Tesla A100 GPU supports calculations under FP16, our systems execute almost all operations in 16-bit floating-point. To escape overflow, we convert the data type before and after the softmax operation in the attention modules. We also reorder some operations for numerical stability. For instance, we apply the scaling operation (dived by ? d k ) to the query instead of the attention weights. To accelerate our systems further, we replace the vanilla layer normalization with the L1-norm . Also, we find that removing the multi-head mechanism (by setting the head to 1) in the student models significantly improves the throughput without performance loss. 

 Optimizations for CPUs We employ the Student-6-1-512 and Student-3-1-512 models as our CPU submissions. Two methods are discussed to speed up the decoding for our CPU systems. The Use of MKL We use the Intel Math Kernel Library  (Wang et al., 2014)  to optimize our NiuTensor framework, which helps our systems to make the full use of the Intel architecture and to extract the maximum performance. 

 8-bit Matrix Multiplication with Packing We implement 8-bit matrix multiplication using the open-source library FBGEMM  (Khudia et al., 2021) . Following  Kim et al. (2019) , we quantize each column of the weight matrix separately with different scales and offsets. Scale and offsets for weight matrix are calculated by: b scale [j] = 14? j 255 (3) b zeropoint [j] = 127 ? (x j + 7? j ) b scale [j] (4) where ? j and xj refers to average and standard deviation for the j-th column. The quantization parameters for the input matrix is calculated by: a scale = x max ? x min 255 (5) a zeropoint = 255 ? x max a scale (6) where x max and x min are the maximum and minimum values of the matrix respectively. With FBGEMM API, we also execute the packing operation to change the layout of the matrices into a form that uses the CPU more efficiently. We prequantize and pre-pack all the weight matrices to avoid repeated operation during inference. where x max and x min are the maximum and minimum values of the matrix, respectively. We also execute the packing operation to change the layout of the matrices into a form that uses the CPU more efficiently. We pre-quantize and pre-pack all the weight matrices to avoid repeated operation during inference. 

 Other Optimizations Furthermore, we explore other device-independent methods to optimize our systems. Those methods help our systems to achieve obvious speed-up without translation precision loss. Graph Optimization A neural net can be represented by a directed acyclic graph (DAG), where the nodes represent tensors and the connections represent operations. We optimize our system by simplifying the computational graph of the models. The optimizations for the graph are detailed as follows: ? Computation optimization. We prune all redundant operations and reorder some operations in the computational graph. For instance, we remove the log-softmax operation in the output layer when using greedy search. We also extract the transpose operations from matrix multiplications to the begin of decoding. ). When the GPU system is running, it will use all free CPUs on the device. ? Memory optimization. We reuse all possible nodes to minimize the memory consumption. We also reduce the memory allocation or movement with an efficient memory pool. Moreover, we sort the source sentences in descending order of length and detect the peak memory footprint before decoding. Parallel Execution We use the GNU Parallel  (Tange, 2011)  for our systems to perform tasks in parallel. More specifically, we split the standard input into several lines and deliver them via the pipeline. The method is used to accelerate pre-processing, post-processing, and decoding on CPUs. We also find that the system decoding speed/memory is strongly correlated with the number of lines per task. To find the best number of lines for each run, we measure the time cost in different setups against the number of lines. Figure  1  shows that 2000 is a relatively good choice, and the Student-6-1-512 model can translate 100,000 sentences in 102.6s on CPUs under this setup. Better Decoding Configurations As aforementioned, our GPU versions use a large batch size, but the batch size on the CPU is much smaller. To be more clear, there is sentence batch (sbatch) and word batch (wbatch) in our systems, and they restrict the number of sentences and number of words in a mini-batch to not be greater than sbatch and wbatch, respectively. In our GPU systems, we set the sbatch/wbatch to 3072/64000. For our CPU systems, the number of processes is managed by the Parallel tool, which is more efficient and accurate. Moreover, We use one MKL thread for each process and set the sbatch/wbatch to 128/2048. 

 Greedy Search In the practice of knowledge distillation, we find that our systems are insensitive to the beam size. It means that the translation quality is good enough even using greedy search in all submissions. Fast Data Preparation We use the fastBPE 7 , a faster C++ version of subword-nmt 8 , to speed the BPE process. Moreover, we also use the fastmosestokenizer 9 for tokenization. 

 Results after Optimizations Figure  2  plots the Student-6-1-512 model's performance with different decoding optimizations. All results show that our optimizations can significantly speed up our system without losing BLEU. What is interesting about the BLEU is that we can achieve additional improvements of 0.4/0.1 BLEU points on the GPU/CPU through decoding optimizations in all our experiments. We also measure other models after decoding optimizations and find their performance is similar to the Student-6-1-512 model. 

 Submissions and Results 

 Submissions For the GPU track submissions, our GPU systems are compiled with CUDA 11.2. We set the num-ber of decoder layers and the number of our decoder attention head to 1 as described in Section 2.2 for all our GPU systems. We see a speedup of more than 6? on the GPU system created by Student-12-1-512 model and a slight decrease of only 0.2 BLEU on the newstest20 compared to the deep ensemble model. The system is named as Base-GPU-System in following part. We continue to reduce the number of encoder layers for more accelerations, and the GPU system with Student-6-1-512 model reduces the translation time by onefour with only six encoder layers compared to the Base-GPU-System. Our fastest GPU system consists of three encoder layers and one decoder layer, which achieves 31.5 BLEU on the newstest20 with GPU and 1.6? speedup compared to the Base-GPU-System. We also employ the Student-6-1-0 model to create a GPU system that can achieve the 1.3? speedup compared to Base-GPU-System. Our systems are compiled in the 11.2.1-devel-centos7 docker image, an NVIDIA open-source image 10 . We copy the executables, dependence tools, and model files to the 11.2.1-base-centos7 docker image (final submission). In this way, we ensure all of our system docker images can be executed by the organizers successfully and reduce the docker images size. For the CPU track submissions, we use the test machine, which has 18 virtual cores. Our CPU version is compiled with MKL static library, and the executable file is 23MiB. Also, we use the 8bit matrix multiplication with packing to speed the matrix multiplication in the network. We use the Student-3-1-512 and Student-6-1-512 models in our CPU systems, and they respectively achieve 31.5 and 32.8 BLEU on newstest20. For our CPU docker images, we use the base-centos7 docker image 11 to deploy our CPU MT systems. Furthermore, all submissions are tested with different cases, including dirty data, empty input, and very long sentences. The test results show that our systems can run successfully with exceptional inputs. 

 Results Our systems for the GPU-throughput track are the fastest overall submissions. Specifically, the Student-3-1-512 system can translate about 250 thousand words per second and achieve 25.5 BLEU on newstest21. We attribute this to the comparison of the performance of our teacher model on WMT21. In the CPU track, our system also has competitive performance. Our fastest CPU system created by Student-3-1-512 model can translate about 48 thousand words per real second via 36 CPU cores and can achieve 25.5 BLEU. We find that reducing the number of encoder layers for student model achieves lower BLEU scores at a similar speed for our CPU systems. Moreover, we compare the cost-effective of GPU and CPU decoding in terms of millions of words translated per dollar according to the official evaluation results. We find that highly-effective GPU decoding is about to out-compete CPU-bound decoding in terms of cost-effective. Noteworthy, our GPU system with Student-3-1-512 model can translate 300M words per dollar with acceptable quality. Also, all of our GPU systems have the lowest RAM consumption (about 4 GB) to official test compared with the submissions of other participants. 

 Conclusion We have described our systems for the WMT21 shared efficiency task. We have explored various efficient Transformer architectures and optimizations specialized for both CPUs and GPUs. We have shown that a lightweight decoder and proper optimizations for different hardware can significantly accelerate the translation process with slight or no loss of translation quality. Our fastest GPU system with three encoder layers and one decoder layer is 11? faster than the deep ensemble model and lose 1.9 BLEU points. 7LPH&RVWVHFFigure 1 : 1 Figure 1: Results on Student-6-1-512 model. The time cost is measured on an Intel Xeon Gold 6240 CPU with 100,000 lines of raw English sentences with an averaged length of 18 words. 
