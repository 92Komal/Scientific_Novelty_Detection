title
Synchronous Syntactic Attention for Transformer Neural Machine Translation

abstract
This paper proposes a novel attention mechanism for Transformer Neural Machine Translation, "Synchronous Syntactic Attention," inspired by synchronous dependency grammars. The mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side selfattentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. The experiments show that the proposed method improves the translation performance on WMT14 En-De, WMT16 En-Ro, and AS-PEC Ja-En (up to +0.38 points in BLEU).

Introduction The Transformer Neural Machine Translation (NMT) model  (Vaswani et al., 2017)  has achieved state-of-the-art performance and become the focus of many NMT studies. One of its characteristics is the self-attention mechanism, which computes the strength of relationships between two words in a sentence. Transformer NMT has been improved by extending the self-attention mechanism to incorporate syntactic information  (Wang et al., 2019b; Omote et al., 2019; Deguchi et al., 2019; Wang et al., 2019a; Bugliarello and Okazaki, 2020) . In particular,  Deguchi et al. (2019)  and  Wang et al. (2019a)  have proposed dependencybased self-attentions, which are trained to attend to the syntactic parent for each token under constraints based on the dependency relations, for capturing sentence structures. Existing syntaxbased NMT models, including their ones, use only monolingual syntactic information on either side or both. By contrast, synchronous grammars such as synchronous context-free grammars and synchronous dependency grammars, which are defined in two languages and generate sentence structures aligned across them, have been introduced into many SMT models with the result of improving their translation performances  (Jiang et al., 2009; Ding and Palmer, 2005; Chiang, 2005; Zhang et al., 2006) . Figure  1  shows an example of the dependency structures of source and target language sentences and their alignments 1 . Inspired by synchronous dependency grammars, we aim to improve the performance of Transformer NMT by incorporating the main idea of the synchronous dependency grammars (i.e., synchronizing sentence structures across two languages). As far as we know, neither the synchronous dependency grammars themselves nor their basic idea has yet been incorporated into NMT. This paper proposes a novel attention mechanism for Transformer NMT, called "Synchronous Syntactic Attention," which captures sentence structures aligned across two languages by the aligned self-attentions on the source-and targetside. The mechanism uses encoder-decoder attentions to map source-side syntactic self-attentions into a target language space based on  Garg et al. (2019) 's observation that encoder-decoder attentions represent the alignments of source and target words. The mechanism is trained to maintain consistency between source-and target-side syntactic self-attentions according to an objective loss function that incorporates the difference between the target-side syntactic self-attentions and the mapped source-side syntactic self-attentions. We use dependency-based self-attention  (Deguchi et al., 2019)  as source-and target-side syntactic self-attentions. 

 Transformer NMT Model The Transformer NMT model  (Vaswani et al., 2017)  is an encoder-decoder model composed of the encoder that encodes source tokens f = (f 1 , f 2 , . . . , f I ) into hidden vectors and the decoder that generates target tokens e = (e 1 , e 2 , . . . , e J ) from the outputs of the encoder. The encoder and decoder consist of N enc encoder layers and N dec decoder layers, respectively. Both the encoder layers and decoder layers are composed of multiple sub-layers, each of which includes a self-attention layer and a feed forward layer. The decoder layers additionally apply an encoder-decoder attention layer between the selfattention layer and the feed forward layer. The self-attention and encoder-decoder attention are calculated by a multi-head attention mechanism. The multi-head attention MHA(Q, K, V ) maps the d emb -dimension embedding space into H subspaces of the d k (= d emb H ) dimension and calculates attention in each subspace as shown in Equations 1 to 3: MHA(Q, K, V ) = [M 1 ; . . . ; M H ]W M , (1) M h = A h V h , A h = softmax ( Q h K ? h ? d k ) , (2) Q h = QW Q h , K h = KW K h , V h = V W V h , ( 3 ) where W Q h , W K h , W V h ? R d emb ?d k and W M ? R d emb ?d emb are parameter matrices. In the selfattention, the previous layer's output is used as Q, K, and V . In the encoder-decoder attention, the previous layer's output is used as Q and the last encoder layer's output is used as K and V . Note that, in training, the decoder's self-attention masks future tokens. 

 Dependency-Based Self-Attention This section describes dependency-based selfattention (DBSA)  (Deguchi et al., 2019) , which is the baseline of our syntactic self-attention. DBSA captures dependency structures by extending the multi-head self-attention of the l dep -th layer of the encoder or decoder. Let h be one of head of the l dep -th encoder layer's self-attention or the l dep -th decoder layer's self attention. An attention weight matrix A h , where each value indicates the dependency relationship between two words, is calculated by using the bi-affine operation in Equation  4 : A h = softmax ( Q h U K ? h ? d k ) , U ? R d k ?d k . (4) In A h , the probability of token q being the head of token t in a source/target sentence S (i.e., P (q = head(t)|S)) is modeled as A h [t, q]. Then, a weighted representation matrix M h , which includes dependency relationships in the source sentence or target sentence, is obtained by multiplying A h and V h (i.e., M h = A h V h ). Finally, M h is concatenated with the other heads and mapped to a d emb -dimensional matrix. In the decoder-side DBSA, future information is masked to prevent attending to unpredicted tokens in inference. The Transformer NMT model with DBSA learns translation and dependency parsing at the same time by minimizing the objective function L = L t + ? dep L dep , where L t is the translation loss and L dep is computed in Equation  5 : L dep = ? I ? i=1 logP (head(f i ) | f ) ? J ? j=1 logP (head(e j ) | e). (5) ? dep > 0 is a hyperparameter to control the influence of the dependency parsing loss L dep . DBSA has been extended to deal with subword tokens. For details, see the original paper by  Deguchi et al. (2019) . 

 Proposed Method: Synchronous Syntactic Attention This section proposes a novel attention mechanism for Transformer NMT, "Synchronous Syntactic Attention," which captures sentence structures aligned across source and target languages. A Transformer NMT model with the proposed attention is trained according to the objective function presented below as Equation  6 : L = L t + ? dep L dep + ? sync L sync , ( 6 ) where L sync is the loss to keep consistency between source-side and target-side syntactic self-  E n g li s c h ? E n g li s c h ? D mapped D ? D Figure 2: An example of synchronous syntactic attention attention (i.e., DBSA) and ? sync is a hyperparameter to control the influence of L sync . In particular, L sync is the differences between the encoder's self-attention, which is mapped into target language space by the encoder-decoder attention, and the decoder's self-attention. Let E and D be the attention matrix A h of the l dep -th encoder layer's syntactic self-attention and that of the l dep -th decoder layer's syntactic self attention, respectively. The proposed method first maps E into the target language space by the encoder-decoder attention as shown by Equation 7: D mapped = CEC ? , ( 7 ) where D mapped is the mapped encoder's syntactic self attention matrix, and C is the encoder-decoder attention weight matrix of the l sync -th decoder's layer. Then, D mapped is masked to prevent attending to future tokens, and a softmax function is applied to the masked D mapped as follows in Equation 8: D ? = softmax(mask(D mapped )). (8) Next, the proposed method computes the mean squared error between D ? and D as L sync as follows in Equation  9 : L sync = ? t,q (D ? t,q ? D t,q ) 2 . ( 9 ) Figure  2  shows an example of the synchronous syntactic attention. The value in each cell indicates an attention score (i.e., an element of an attention weight matrix), and the darker cell represents a higher attention score. In all matrices, each row represents an attention distribution for each token (i.e., scores are normalized in a row direction). As can be seen in Figure  2 , the English encoder's syntactic self-attentions E is mapped into the German encoder's syntactic self-attentions D ? using the encoder-decoder attentions C and C ? . Then, the loss between the German encoder's syntactic self-attentions D ? and the German decoder's syntactic self-attentions D is measured. When calculating the loss, the values of the masked elements in D ? and D, such as D Sprichst,du and D du,Englisch? , are assigned to zero. 

 Experiments 

 Setup We compared the proposed model with a conventional Transformer NMT model and a Transformer NMT with DBSA (Transformer+DBSA), which do not synchronize between source-and targetside self attentions, to confirm the effectiveness of the proposed synchronous syntactic attention. The Transformer base model  (Vaswani et al., 2017)  was used as the baseline model. We evaluated translation performance in the WMT14 En-De translation task, WMT16 En-Ro translation task, and WAT ASPEC Ja-En translation task  (Nakazawa et al., 2016) . In ASPEC Ja-En, we used the first 1.5 million translation pairs of the training data in training. We used Moses Tokenizer to tokenize English, German, and Romanian sentences and KyTea  (Neubig et al., 2011)  to tokenize Japanese sentences. Byte Pair Encoding (BPE) was applied to create subword tokens. We used dependency structures generated by Stanza  (Qi et al., 2020)  for English, German, and Romanian sentences, and EDA 2 for Japanese sentences as the supervisions in the training of source-and target-side DBSA (i.e., calculation of L dep in Transformer+DBSA and the proposed model). Note that Stanza and EDA are not used in testing. The details of the dataset and preprocessing are shown in the Appendix. All models were trained for 100,000 updates. We used label smoothed cross entropy  (Szegedy et al., 2016)  as the L t of the objective function and set label smoothing ? to 0.1. In the proposed model, the hyperparameter ? sync was tuned for each development set and set to 0.5 for WMT14 En-De, 0.1 for WMT16 En-Ro, and 10.0 for AS-PEC Ja-En. In all experiments, ? dep and l dep were set to 0.5 and 1, respectively. l sync was set to 5 according to  Garg et al. (2019)  Import@@ ance of retrieval and optimum treatment of exacerbation factors is emphasized .  alignment performance of the encoder-decoder attention in the penultimate layer is the best among all layers. In decoding, we used beam search with length penalty and set the beam size to 4. The details of the hyperparameters are shown in the Appendix. 

 Results Table  1  shows the experiment results. In the table, "DBSA" and "SyncAttn" indicate Transformer NMT with DBSA and Transformer NMT with the proposed synchronous syntactic attention, respectively. Translation performance was evaluated by BLEU  (Papineni et al., 2002) . As Table  1  illustrates, the proposed model Syn-cAttn outperforms the baseline models Transformer and on all the tasks. In particular, SyncAttn improved by 0.38, 0.20, and 0.27 BLEU points in the WMT14 En-De, WMT16 En-Ro, ASPEC Ja-En tasks, respectively, compared to DBSA. These results demonstrate the effectiveness of our synchronous syntactic attention. 

 Case Study This section compares translation examples of the baseline model DBSA and the proposed model SyncAttn to show the effectiveness of the synchronous syntactic attention. Figure  4  shows translation examples of the two models for the Ja- Input ? DBSA Importance of retrieval and optimum treatment of exacerbation factors is emphasized. 

 SyncAttn Importance of retrieval of exacerbation factors and optimum treatment are emphasized. 

 Reference The importance of finding out exacerbation factors and optimum treatment are emphasized. En task. The bold words are the differences between the translations by the two models. As can be seen in Figure  3 , in both models, the encoder's self-attentions correctly find that "? (factors)" attends to "? (of )". However, DBSA does not correctly find the head of "factors" on the English side, while SyncAttn does. This is because Syn-cAttn synchronizes the source-and target-side dependency structures between "?" and "factors" identified by the encoder-decoder attentions while DBSA does not. Figure  3  and 4 show that the correct analysis for the target-side dependency structures led to the correct translation. 

 Related Work The main characteristic of Transformer NMT is attention mechanisms (i.e., self-attentions and encoder-decoder attentions). Some researches have analyzed and/or improved the attention mechanisms of Transformer NMT. For instance,  Tang et al. (2018b)  analyzed encoder-decoder attentions in terms of word sense disambiguation, and  Tang et al. (2018a)  analyzed self-attentions in terms of subject-verb agreement and word sense disambiguation.  Raganato and Tiedemann (2018)  and  Voita et al. (2019)  revealed the behaviors of attention heads in terms of dependency relations. Namely,  Raganato and Tiedemann (2018)  observed that specific attention heads of the en-coder's self-attentions mark syntactic dependency relations.  Voita et al. (2019)  found that the confident heads play linguistically-interpretable roles like dependency relations.  Garg et al. (2019)  proposed a method for jointly learning to produce translations and alignments with a single Transformer model and showed that encoder-decoder attentions emulate word alignments. Based on their observations, our method maps the encoder's syntactic self-attentions into the target language space by using encoder-decoder attentions.  Shaw et al. (2018)  extended a self-attention mechanism to encode the relative positions between two words in a sentence.  Omote et al. (2019)  and  Wang et al. (2019b)  proposed a selfattention mechanism to encode relative positions on source-side dependency trees. Some researchers proposed syntax-aware selfattentions that are trained using dependency-based constraints.  For instance, et al. (2019a)  and  Bugliarello and Okazaki (2020)  proposed sourceside dependency-aware Transformer NMT.  Wang et al. (2019a)  created a constraint based on dependency relations between tokens to encoder self-attentions.  Bugliarello and Okazaki (2020)  also proposed Parent-Scaled Self-Attention, which multiplies an attention weight matrix by scores based on dependency relations.  Deguchi et al. (2019)  proposed DBSA, which is applicable to both the encoder's and decoder's self-attentions and is extended to subword units. We used DBSA to implement source-and target-side syntactic attentions in Transformer NMT. The main difference from the above-mentioned studies is that our work focuses on the incorporation of bilingual syntactic information into NMT. Harada and Watanabe (2021) incorporated synchronous phrase structure grammar into NMT. Specifically, they proposed a syntactic NMT model that induces latent phrase structure and synchronizes the source-and target-side sentence structures. The difference with our model is that we synchronize dependency structures while they synchronize phrase structures. 

 Conclusions In this paper, we proposed a novel attention mechanism for Transformer NMT, "Synchronous Syntactic Attention," which captures sentence structures aligned across source and target languages by aligned self-attention. The synchronous at-tention mechanism trains syntactic self-attentions (DBSA) under a constraint that minimizes the loss between encoder's and decoder's self attentions, where the encoder's self attentions are mapped into the target language space by encoder-decoder attentions. Since this method relies only on the constraint induced from the encoder's and decoder's self-attentions and encoder-decoder attentions, it does not require additional model parameters. The experiments show that the proposed method improves Transformer NMT's translation performance (up to a 0.38 BLEU point improvement). Figure 1 : 1 Figure 1: An example of dependency structures and alignments 

 Figure 3 : 3 Figure 3: Dependency structures of the examples in Figure 4 

 Figure 4 : 4 Figure 4: Translation examples of DBSA and SyncAttn in the ASPEC Ja-En task 

 's finding that the ? ? ? ? ? ? ? ? ? ? ? ? ? ? exacerbation factors of retrieval and optimum treatment of importance importance are emphasized emphasized emphasized 

 Table 1 : 1 Experimental results (BLEU(%)) WMT14 WMT16 ASPEC Model En?De En?Ro Ja?En Transformer 27.23 23.83 28.94 DBSA 27.31 24.13 29.57 SyncAttn 27.69 24.33 29.84 

			 In this paper, an arrow is drawn from a head to its dependent. 

			 http://www.ar.media.kyoto-u.ac.jp/ tool/EDA 

			 http://lotus.kuee.kyoto-u.ac.jp/ WAT/evaluation/index.html#automatic_ evaluation_systems.html
