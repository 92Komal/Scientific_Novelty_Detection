title
The University of Maryland, College Park Submission to Large-Scale Multilingual Shared Task at WMT 2021

abstract
This paper describes the system submitted to Large-Scale Multilingual Shared Task (Small Task #2) at WMT 2021. It is based on the massively multilingual open-source model FLO-RES101_MM100 model, with selective finetuning. Our best-performing system reported a 15.72 average BLEU score for the task.

Introduction Massively multilingual models such as Facebook's M2M-100  (Fan et al., 2020)  model provide an attractive approach to scaling Machine Translation to many language pairs by sharing encoder-decoder parameters across languages. By not centering English in its training process, M2M-100 improves translation quality substantially (by over 10 BLEU points) compared to the best single systems of WMT before 2020 on the "large-scale Many-to-Many dataset for 100 languages"  (Fan et al., 2020) . However, translation quality for low-resource languages still leaves much room for improvement. We address the Large-Scale Multilingual Machine Translation Shared Task (Small Track #2) at WMT 2021, by fine-tuning the FLO-RES101_MM100 model for the languages in the Shared task. We consider different fine-tuning configurations, with a goal to minimize the computational and data resources required. First, we consider the impact of finetuning on datasets of different sizes, and surprisingly show that finetuning with the smaller dataset gives better performance for some language pairs. Second, we consider selectively dropping layers during fine-tuning to reduce the computational cost of working with a Transformer model with millions of parameters. We adopt a structure dropout technique, LayerDrop, which has been shown to have a regularization effect and to effectively reduce model size for inference  (Fan et al., 2019) , as well as to reduce training * These authors contributed equally to this work time while preserving decoding quality  (Zhang and He, 2020) . We have used LayerDrop so that our model can run on large datasets for low resource language pairs. Our best performing system is fine-tuned on the large MultiCCAligned training data and yields a sentence-piece BLEU score (the official Shared task metric) of 15.72 on the Shared task test set. However, a model fine-tuned on smaller amounts of data (bible-uedin) approaches that result, with a BLEU score of 15.10. This paper describes the submitted models, as well as experiments with Lay-erDrop configuration, which show that dropping the top layers does not help BLEU. 

 Shared Task Data Training Our training data is provided by the Shared task organizers and is drawn from the publicly available open-source multilingual parallel corpus (OPUS) data repository for the languages of the Shared task  (Tiedemann, 2012) . It consists of the MultiCCAligned large dataset which supports 112 languages  with English as the pivot language. The bible-uedin dataset  (Christodouloupoulos and Steedman, 2015)  is comparatively smaller than the MultiCCAligned dataset and is supported by 102 languages based on translations from the Bible. Table  1  reflects the statistics for the datasets from 23 different language pairs (3 from bible-uedin with a size of 125 MB and 15 from MultiCCAligned with a size of 16 GB) considering only the 6 languages in the Shared task which are Indonesian, Javanese, Tamil, Tagalog, Malay, and English. MultiCCAligned takes up more than 50% of the dataset while bible-uedin takes less than 0.2%. We preprocess the data using the "Sentence-Piece" module  (Kudo and Richardson, 2018)  for tokenization and byte-pair encoding, and remove duplicate samples. Evaluation Sets The Shared task evaluates models on three distinct datasets: dev, devtest and test. They are all drawn the FLORES-101 benchmark for Many-to-Many multilingual translation  (Goyal et al., 2021) . It consists of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated into 101 languages by professional translators through a carefully controlled process. The Shared task uses a subset of six languages including English from FLORES-101. The languages are: Javanese (jav), Indonesian (ind), Malay(msa), Tagalog (tgl), Tamil (tam), and English (eng). The dev and devtest sets are both 2.8MB in size. These datasets were evaluation test set and were therefore held out from our fine-tuning experiments. The test set were inaccessible to the Shared Task participants.  

 Model Configurations This section describes our base model and the various fine-tuning configurations considered. 

 Base Model Figure  1  shows a FLORES101_MM100 model with the original encoder and decoder.  

 Finetuning Strategies Hyper-parameters Table  2  gives the list of the hyper-parameter settings we use for all finetuning in our experiments. Since batch size and learning rate affect finetuning, we experimented with two different learning rates, 3e ?5 and 3e ?7 , on the smaller dataset (bible-uedin). Changing the learning rate from 3e ?5 to 3e ?7 boosts the BLEU score of bible-uedin fine-tuned model to 15.10. 

 Batch Size 4 Loss Data We compare the impact of using each of the datasets decribed in Section 2 to fine-tune the models: bible-uedin and MultiCCAligned. Activation function In addition to using the standard ReLu activation function, we experiment with the GELU nonlinearity, which weighs inputs by their percentile, rather than gates inputs by their sign as in ReLUs. Compared to ReLU or leaky ReLU, GELU has the theoretical advantage of being differentiable for all values of x. LayerDrop  Fan et al. (2019)    by selectively dropping the last three layers  (9, 10, 11)  in the encoder and the decoder. We compare this approach to fine-tuning all layers in our model without LayerDrop. 

 Results 

 Aggregate Results The Shared task evaluates the performance of models using a sentence-piece BLEU (spBLEU) score, aggregated across all language pairs tested. We report results using this metric and to it as BLEU in this section. Table  3  reports the BLEU score of our models finetuned with the different datasets on the three Shared task evaluation sets. From Table  3 , we can see that the model finetuned with MultiCCAligned obtains higher BLEU scores across the board compared to the model finetuned with bible-uedin. On the test set, it obtains a BLEU score of 15.72. However, the model fine-tuned on bible-uedin, is only about .6 BLEU point behind (15.10 BLEU), despite being only about 1 340 in size comparing to the MultiCCAligned. These results suggest that amount of data is not the most important factor when selecting a dataset for fine-tuning. Table  4  shows the BLEU scores obtained with fine-tuning configurations which vary in the activation function used and in the use of the LayerDrop technique for reducing model size. The best results are obtained with the standard settings: fine-tuning with the ReLU activation and no LayerDrop. Lay-erDrop degrades translation quality substantially, which suggests that it is not a promising strategy to reduce the computational cost of neural MT. 

 Per Language Results In addition to aggregate results, we report BLEU scores per language pair in Figure  2  for each of the main experimental conditions considered. Since our main motivation is to improve the performance of the model for lowresource languages, we would like to fill the gap between the languages with a higher score and the languages with a lower score, i.e. to see more dark blue squares in the Figure  .  Comparing the score break down of the MultiCCAligned model and the bible-uedin model, the latter one performs better on almost all translations to Tamil and Tagalog; for example, there is a 2.33 improvement on eng-tam and a 6.49 improvement on eng-tgl. Some translations from Tamil also show improvements, 1.3 on tam-eng, while the only improvement from Tagalog is 1.31 on tgl-tam. However, bible-uedin model performs worse on 19 out of all 30 language pairs and has a lower average. 

 Submitted System Configuration The submitted system is fine-tuned with the Multi-CCAligned dataset for all the language pairs mentioned in Table  1 . The hyper-parameters are set as described in Table  2  with learning rate 3e ?05 . This system uses ReLu as the activation function and keeps all the original layers in the encoder and decoder. The fine-tuning is done for 10 epochs. 

 Conclusion We described the University of Maryland submission to the Large-Scale Multilingual Shared Task (Small Task #2) at WMT 2021. We considered several fine-tuning configurations on top of the massively multilingual FLORES101_MM100, and find that using MultiCCAligned data and a standard model configuration give the best result. We also show that finetuning on the much smaller Bibleuedin dataset approaches our best result, with a BLEU score of 15.10. Selecting appropriate finetuning data thus plays a significant role in the quality of the final model, and the amount of data alone is a suboptimal selection criterion. Dropping the last three layers of the encoder and decoder decreased the translation quality. Future work is needed to determine how to reduce the computational needs of large-scale multilingual MT. Figure 1 : 1 Figure 1: Baseline FLORES101_MM100 architecture 
