title
RTM Super Learner Results at Quality Estimation Task

abstract
We obtain new results using referential translation machines (RTMs) with predictions mixed to obtain a better mixture of experts prediction. Our super learner results improve the results and provide a robust combination model.

Introduction Quality estimation task in WMT21  (Specia et al., 2021)  (QET21) address machine translation (MT) performance prediction (MTPP), where translation quality is predicted without using reference translations, at the sentence-level (Tasks 1, 2, and 3) and with classification of sentences into containing a critical error or not (Task 3). Task 1 predicts the sentence-level direct assessment (DA) in 11 language pairs categorized according to the MT resources available: ? high-resource, English-German (en-de), English-Chinese (en-zh), and Russian-English (en-ru), ? medium-resource, Romanian-English (roen) and Estonian-English (et-en), ? low-resource, Sinhalese-English (si-en) and Nepalese-English (ne-en), and ? no-resource, English-Czech (en-cs), English-Japanese (en-ja), Pashto-English (ps-en), and Khmer-English (km-en) for zero-shot prediction. en-ru contains sentences from both Wikipedia and Reddit articles while others use only Wikipedia sentences with 7000 sentences for training, 1000 for development, 1000 for test QET in 2020, and 1000 for testing at QET21. The target to predict in Task 1 is z-standardised DA scores, which changes the range from [0, 100] for DA scores to  [3.178, ?7.542 ] in z-standardized DA scores.  The target to predict in Task 2 is sentence HTER (human-targeted translation edit rate) scores  (Snover et al., 2006) . We participated in sentence-level subtasks. Table  1  lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the referential translation machine (RTM)  (Bic ?ici and Way, 2015; Bic ?ici, 2020)  models (M for million). In zero-shot prediction, we use all of the training instances made available to the task in all 7 translation directions. We tokenize and truecase all of the corpora using Moses'  (Koehn et al., 2007)  processing tools. 1 Language models (LMs) are built using kenlm  (Heafield et al., 2013) . 

 RTM for MTPP We use RTM models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, text data selected close to the task instances in bilingual training settings or monolingual LM settings. Interpretants are text data that provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. With the enlarging parallel and monolingual corpora made available by WMT 2 , the capability of the interpretant datasets selected to provide context for the training and test sets improve with parallel feature weight decay (parfwd) instance selection  (Bic ?ici, 2019) . RTMs use parfwd for instance selection and for machine translation performance prediction system (MTPPS)  (Bic ?ici et al., 2013; Bic ?ici and Way, 2015)  to obtain the features, where additional features from word alignment are added. Figure  1  depicts RTMs and explains the model building process. We treated all of Tasks 1, 2, and 3 as bilingual tasks where parallel corpora are obtained from WMT translation task.  3  The related monolingual or bilingual datasets are used during feature extraction. The machine learning models we use include ridge regression (RR), support vector regression (SVR)  (Boser et al., 1992) , gradient tree boosting, extremely randomized trees  (Geurts et al., 2006) , and multi-layer perceptron  (Bishop, 2006)  in combination with feature selection (FS)  (Guyon et al., 2002)  and partial least squares (PLS)  (Wold et al., 1984)  where most of these models can be found in scikit-learn.  4  We use RR to estimate the noise level for SVR, which obtains accuracy with 5% error compared with estimates obtained with known noise level (Cherkassky and Ma, 2004) and set = ?/2. We use Pearson's correlation (r), mean absolute error (MAE), root mean squared error (RMSE), relative absolute error (RAE), relative MAE (MAER), and mean RAE relative (MRAER) as evaluation metrics  (Bic ?ici and Way, 2015) . Our best non-mixed results are in Table  2 . Official evaluation metric is r P . 

 Mixture of Experts Models We use prediction averaging  (Bic ?ici, 2018)  to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions, ? with evaluation metrics indexed by j ? J and weights with w: w j,i = w j,i 1?w j,i ? ? ? k = 1 k k i=1 ? ? ?i MEAN ? ? ?j,w j k = 1 k i=1 w j,i k i=1 w j,i ? ? ?i ? ? ?k = 1 |J| j?J ? ? ?j,w j k MIX (1) MEAN is the averaged results and MIX is the weighted average. We assume independent predictions and use p i /(1 ? p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble  (Kuncheva and Rodr?guez, 2014) . We use the MIX prediction only when we obtain better results on the training set. We select the best model using r and mix the r P MAE RMSE results using r, RAE, MRAER, and MAER. We filter out those results with higher than 0.95 relative evaluation metric scores. We also use generalized ensemble method (GEM) as an alternative to MIX to combine using weights and correlation of the errors, C i,j , where GEM achieves smaller error than the best combined model  (Perrone and Cooper, 1992) : ?GEM = L i=1 w i ? i (x) = y + L i=1 w i i C i,j = E[ i , j ] = (? i (x) ? y) T (? i (x) ? y) w i = L j=1 C i,j L k=1 L j=1 C k,j Super learner  (Polley and van der Laan, 2010 ) is a stacking model on a library of L learning models that are V -fold cross-validated on the training set and constructs an V ? L level 1 dataset. Theoretical results show that as the number of different predictors in the ensemble increase, the ensemble result gets closer to the oracle result  (Dudoit and van der Laan, 2005) . The function that minimize the empirical risk on the validation set will achieve lower error than the function that minimize the overall risk:  (Vapnik, 1998) . Model combination (Figure  2 ) selects top k combined predictions and adds them to the set of predictions where the next layer can use another model combination step or just pick the best model according to the results on the training set. We use a two layer combination where the second layer is a combination of all of the predictions obtained. The last layer is an arg max. 1 m m i=1 L(? * , y i ) ? 1 m m i=1 L( ?, y i ) ? 0 Our test set results using super learner are in Table 3. Before model combination, we further filter prediction results from different machine learning models based on the results on the training set to decrease the number of models combined and improve the results. A criteria that we use is MREAR ? 0.95 since MRAER computes the mean relative RAE score, which we want to be less than 1. In general, the combined model is better than the best model in the set. Super learner improve the results (Table  3 ). The baseline deepQuest  (Ive et al., 2018)  use bidirectional gated recurrent unit type recurrent neural networks to model QET. RTM + deepQuest combination results in Task 2 use linear interpolation of RTM and deepQuest results with weights 0 ? ? ? 1 and 1 ? ? respectively as well as polynomial function fits to find the best combination model optimized on the development set. The most common function fit found is f (x) = r P MAE RMSE  a x + bx 3 + cx 2 + dx + e (Table  4 ). Task 3 results are in Table  5 . 

 Conclusion Referential translation machines pioneer a language independent approach and remove the need to access any task or domain specific information or resource and can achieve good results in automatic, accurate, and language independent prediction of translation scores. We present RTM ensemble results with super learner.  0.2 M 3.5 M en-cs 63000 1000 bilingual 0.2 M 3.5 M en-ja 63000 1000 bilingual 0.2 M 3.5 M km-en 63000 1000 bilingual 0.2 M 3.5 M ps-en 63000 1000 bilingual 0.2 M 3.5 
