title
Like Chalk and Cheese? On the Effects of Translationese in MT Training

abstract
We revisit the topic of translation direction in the data used for training neural machine translation systems, focusing on a real-world scenario with known translation direction and imbalances in translation direction: the Canadian Hansard. According to automatic metrics, we observe that using parallel data that was produced in the "matching" translation direction (authentic source, translationese target) improves translation quality. In cases of data imbalance in terms of translation direction, we find that tagging the translation direction of training data can close the performance gap. We perform a human evaluation that differs slightly from the automatic metrics, but nevertheless confirms that for this French-English dataset that is known to contain high-quality translations, authentic or tagged mixed source improves over translationese source for training.

Introduction Prior work in statistical machine translation (SMT) highlighted potential benefits of making use of information about the translation direction of training data  (Kurokawa et al., 2009) . When text is translated, there is an authentic source (the language in which the text was originally produced), and its translation, which in contrast can be described as translationese. Thus when considering translation direction in machine translation, training data can be described as consisting of authentic source, translationese source, or a mix. 1 Backtranslated data produced by machine translation may be thought of as an extreme case of translationese source  (Marie et al., 2020) , but because the quality and types of errors that occur in machine translation are quite different from those that occur in human translation, it is worth examining translation direction of human translation separately from MT-based data augmentation. In Figure  1  we show a fairly dramatic example of the kinds of translation quality differences that can occur when building MT systems using authentic source as opposed to translationese source. Recent work in neural machine translation (NMT) has revisited this issue, motivating the automatic detection of (human) translationese by showing improved performance on several metrics when training translation direction matches the testing translation direction  (Sominsky and Wintner, 2019) , examining domain and backtranslation along with the translation direction of test sets  (Bogoychev and Sennrich, 2019) , and evaluating the treatment of predicted translation direction as separate languages in a multilingual-style NMT system through human and automatic metrics  (Riley et al., 2020) .  1  For the purposes of this paper, we will set aside the situation where both sides of the text consist of translationese, translated from one or more other pivot languages. 

 Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 -20, 2021, Volume 1: MT Research Track Source Les producteurs de fromage au Qu?bec sont des fleurons dont on est fiers. 

 Reference We are proud of our exceptional Quebec cheese producers. MT  (Authentic Src.)  We are proud of the success of cheese producers in Quebec. MT (Translationese Src.) The cheese producers in Quebec are proud flowers. We focus this work on a particular real-world scenario, where translation direction is known, and translation (whether human, machine, or computer aided) is expected to be performed from authentic source language text. This is, in fact, a fairly common scenario (i.e., parliamentary, legal, medical, patent, etc. translation), and we highlight one such case as an example: the Canadian Hansard (House of Commons), which consists of transcripts of parliamentary speech, alongside their translations. These proceedings are published in French and English, and it is indicated whether the authentic source was French or English. 2 There is also an imbalance in translation direction; most of the text of the Hansard was originally spoken in English and transcribed and then translated into French. Given that the text is formal and falls within the parliamentary domain, it is appropriate to build or adapt translation systems using the existing Hansard as training data, for use in translating future Hansard text (i.e., in a computer aided translation setting), which raises questions about how to make the best use of the available text and the metadata regarding source language. In this work, we focus on translating original (authentic) source language text. We examine the following questions: Q1: What effect does translation direction of training data have on system output? Q2: Can tagging source side translationese in the training data (i.e., adding a special token like "<translationese>" to the start of translationese source sentences) improve translation of authentic source language test data? Q3: In a moderate resource setting (approx. 3.7 million sentence pairs), what effect does the proportion of source side translationese (from 0% to 100%) in the training data have? We experiment and evaluate these using automatic metrics and a small human judgment task, looking at both French-English and English-French translation directions. With regard to Q1, we find that systems trained exclusively with Authentic source data outperform by a large margin those trained exclusively with Translationese source data, even with twice as much training data. Combining Authentic and Translationese source does not always produce significantly better systems, compared to using Authentic source only, but tagging Translationese source in the training and tuning data (Q2) can improve performance, especially in situations where there is more Translationese source than Authentic source data. In general, translation quality increases as the percentage of Authentic source training data increases (Q3): below 50%, tagging Translation source data can help bridge the gap, but the importance of tagging decreases as the percentage of Authentic source training data increases. 

 Data We use parallel English-French (EN-FR) text from the Canadian Hansard, House of Commons. Our corpus contains transcripts of debates from 1986 to 2016. Earlier parts of this dataset are available from LDC  (Ma, 1999) , more recent transcripts are publicly available from the Canadian Parliament website.  3  This data is known to have high translation quality. It is annotated with direction of translation (the original language, FR or EN, as spoken in the House is known); we omit all lines marked as unspecified.  4  The question of domain is always intertwined with the question of translation direction. Here we hope to minimize that by confining our work to the parliamentary domain; we expect that the level of formality and style of parliamentary speech is relatively consistent, even across languages (certainly more so than it would be if compared between news data and parliamentary speech). Nevertheless, we acknowledge that there will remain differences within this domain; i.e., Members of Parliament may speak more frequently about topics related to their own constituencies or about different topics over time. We also sample our data with an eye toward temporal aspects for this reason. The full dataset (from which we select our training, development, and test data) is unbalanced in terms of original language: 10,091,250 lines (68.5%) were originally spoken in English, while 3,699,822 lines (25.1%) were originally spoken in French (the remaining 933,996 lines, 6.3%, were labeled as unspecified). In order to run experiments on the proportion of source-side translationese used, we are limited by the size of the smaller sub-corpus, the Authentic-FR language data. We sample data for validation and testing (2k and 8k lines, respectively), with Authentic-EN source data used for translation into FR and vice versa. The validation and testing data are randomly sampled sentences from recent data (Nov. 1 to Dec. 15, 2016), while training contains older data. This mimics a real-world scenario, where translators (potentially using computer aided translation) might post-edit or interactively translate new text using the output of machine translation systems build on older text. By drawing the test sentences from a separate portion of the Hansard as the training data, we guarantee that test sentence performance is not inflated due to having included neighboring context in the training data; rather, the test data performance should be representative of realistic performance on new and previously unseen Hansard data. For Q3, we subsample Authentic-EN parallel text once, to match the Authentic-FR training data in size, also attempting to match it in date distribution (which we expect may also serve as a proxy for matching topic distributions).  5  For the experiments that consider between 0% and 100% source side Translationese, we then subsample this Authentic-EN subsample and the Authentic-FR data. We preprocess the data using open-source normalization and tokenization scripts from PortageTextProcessing. 6 Specifically, we applied clean-utf8-text.pl (removing control characters, standardization, etc.), followed by fix-slashes.pl (heuristically adding whitespace around slashes), and tokenization with utokenize.pl -noss -lang=$lang. We then train joint 32k byte-pair encoding (BPE) subword vocabularies on the training data  (Sennrich et al., 2016), 7  and apply them to train, development, and test. 

 Models We build Transformer  (Vaswani et al., 2017 ) models using Sockeye-1.18.115  (Hieber et al., 2018) , with 6 layers, 8 attention heads, network size of 512 units, and feedforward size of 2048 units. We have changed the default gradient clipping type to absolute, used source-target soft-max weight tying, an initial learning rate of 0.0002, batches of ?8192 tokens/words, maximum sentence length of 200 tokens, optimizing for BLEU, checkpoint intervals of 4000, and early stopping after 32 checkpoints without improvement. Decoding used beam size 5. Training used 4 NVIDIA V100 GPUs. 

 Experiments 

 Challenges and Evaluation We measure system quality through automatic metrics: BLEU  (Papineni et al., 2002)  and chrF  (Popovi?, 2015) , both of which we computed using SacreBLEU  (Post, 2018) . We show BLEU score 95% confidence intervals using bootstrap resampling  (Koehn, 2004)  with 1000 iterations of sampling the full test set (with replacement). When the confidence intervals are nonoverlapping, we can claim statistically significant differences between the systems, but when they overlap we cannot directly make claims about statistical significance or the lack thereof. We also perform pairwise bootstrap resampling, again with 1000 iterations, in order to evaluate whether improvements from one system to another are statistically significant  (Koehn, 2004) . Recent work has noted that BLEU score can effectively be gamed by producing more translationese-like text  (Riley et al., 2020) , improving automatic metric scores while decreasing quality according to human ratings.  Mathur et al. (2020)  observe that small improvements in metric scores may not always result in corresponding improvements in human judgments. We address this by complementing BLEU with another metric (chrF) and doing a manual (human) analysis of translation quality. For the human evaluation, we asked annotators to perform two sets of three-way ranking tasks on a sample of test sentences produced by three different systems. We then computed average rankings of the three systems based on the human judgments. 8 Annotators viewed a source sentence, its reference translation, and were asked to rank three translations of it based on which output they found to be the best translation (semantically, grammatically, and fluencywise). 9 There was also a free text box for optional comments. The ranking was performed using LimeSurvey, 10 and the three sentences were displayed in a random order. All annotators first completed 100 annotations for interannotator agreement; we expected this to be quite low. We measured interannotator agreement using Cohen's kappa coefficient (?), as in  Bojar et al. (2013) : ? = P (A) ? P (E) 1 ? P (E) where P(A) is the proportion of times that pairs of annotators agree on the relative ranking of pairs of systems, and P(E) is the proportion of times that they would agree by chance.  11  We find overall ? = 0.25 for EN-FR translations and ? = 0.28 for FR-EN. Such values of ? are typically interpreted as indicating "fair" agreement  (Landis and Koch, 1977) . If we convert the rankings into the task of labeling the best system, annotator agreement increases: ? = 0.31 (EN-FR) and ? = 0.31 (FR-EN). The agreement on which is the worst system is even stronger: ? = 0.34 8 Annotators were adult L1/fluent speakers of the target language with knowledge (ranging from conversational to fluent) of the source language, including the authors and colleagues, five for French, four for English; all volunteer. No personally identifying information was collected. 9 Annotators were only asked to judge sentence tuples where there were at least two unique translations of the sentence; exact matches ranked consecutively were scored as ties (such that the final ranking could be either: 1-2-3, 1-1-2, or 1-2-2). This explains why average ranks don't always sum to 6, as would be expected if all ranks were exclusive. 21 annotations where exact matches were ranked non-consecutively were dropped (out of a total of 1800 annotations, this is approximately 1%). 10 https://www.limesurvey.org/ 11 ? is calculated excluding comparison of identical system outputs. (EN-FR) and ? = 0.39 (FR-EN). The example sentence pair in Figure  1  is an extreme one, showing the worst effects of translationese training. In their qualitative assessments, annotators noted that this was a challenging ranking task, as the sentences they were judging often differed by only a few tokens; several annotators expressed a wish for a mechanism for marking ties. In many cases this was an issue of three high-quality outputs, though there were also examples of three equally-poor outputs. As with BLEU scores, we compute 95% confidence intervals around the average rankings using bootstrap resampling of the human ranking data  (Koehn, 2004)  with 1000 iterations of sampling the full annotated sets with replacement. We also perform pairwise bootstrap resampling for significance. In the following sections, we discuss both the automatic and the human rankings in greater detail, including the matter of statistical significance (via confidence intervals and pairwise bootstrap resampling), where the human and automatic metrics agree and disagree, and what trends we observe that do not rise to the level of statistical significance but which may still merit future work. 

 Q1: Translation Direction We first examine the effects of translation direction in our realistic setting, considering three systems built with three different training sets: Authentic source only, Translationese source only, and finally their combination (Mixed; all available data). As we evaluate by translating Authentic source data, we expect that training on Authentic source data should be better than training on Translationese source data. confidence intervals) and chrF on the test data. The Human column reports the average ranking of the system (1 is the best, 3 is the worst). The Lines column shows the number of lines used in training the system. Table  1  shows the results. As expected, in both translation directions, using Authentic source data for training outperforms using Translationese source data (by a difference of 4.8 BLEU in the EN-FR direction and by a difference of 4.0 in the FR-EN direction). This is particularly striking in the FR-EN direction: despite using more than twice as much training data (10.0M lines as compared to 3.7M), the Translationese source condition lags well behind the Authentic source condition by all metrics. We conclude that the Translationese source system is significantly worse than the Authentic source and Mixed source systems, as evidenced by the non-overlapping 95% confidence intervals and the fact that 100% of pairwise bootstrap resampling iterations found the Translationese to be worse than either system it was paired with. The performance of training with the Mixed data is very comparable to training with only Authentic data. In the EN-FR direction, there is a difference of 0.2 BLEU in favor of the Mixed training data, while in the FR-EN direction there is a very small difference of 0.08 BLEU. According to pairwise bootstrap resampling of BLEU scores, the EN-FR Mixed system is significantly better than the Authentic only system (p < 0.05, with the Mixed system performing better in 95.7% of resampling instances). In the FR-EN direction, the BLEU difference between Mixed and Authentic is not statistically significant. In the EN-FR direction, chrF also shows a small gain for the Mixed training data, while in the FR-EN direction, the Authentic source has a very small advantage. Figure  2 : Confidence intervals (shaded for visibility) for average human-annotated rank (rank 3 is worst and rank 1 is best) for systems corresponding to Table  1 . We turn to human evaluation, where for systems in Table  1 , we have 395 (EN-FR) and 498 (FR-EN) annotations respectively. We found that the human rankings agreed with the automatic metrics in terms of which system was consistently worst: the Translationese source system. As evidenced by the distinctly non-overlapping 95% confidence intervals in Figure  2  and via pairwise bootstrap resampling with p = 0.05, human judgments (like automatic metrics) judge the Translationese source model to be significantly worse than each of the other two. This result contrasts with  Riley et al. (2020) . Annotators disagreed slightly with automatic metrics in terms of ranking Authentic source and Mixed source, but we note that the differences between those scores (both automatic and human) were quite small. For EN-FR, automatic scores the Mixed source best by 0.2 BLEU and 0.001 chrF, while human judgments scored Authentic source systems as best by an average rank difference of 0.07. For FR-EN, BLEU had Authentic and Mixed source tied, while chrF had Authentic source edging out Mixed by a difference of 0.001; human rankings favored the Mixed by 0.04. While these results are not statistically significant (for human rankings), they do raise questions about the effects of the ratio of Authentic and Translationese source data, which we examine in more detail in Section 4.4. When testing the above systems on Translationese source test data, we observe results similar to the ones discussed here: in that setting, systems trained on Translationese source perform better than systems trained on Authentic or Mixed data. However, since our primary interest is in the more realistic task setting of translating Authentic source data, we do not further discuss these results here. We note that the data is unbalanced, with much more Authentic English source than Authentic French source, due to the distribution of language as spoken in the House of Commons. The fact that using Authentic source training data performs better when translating Authentic source test data than Translationese source data (even when there is much more Translationese source data) indicates that translation direction does matter. 

 Q2: Tagging Translation Direction Having observed through automatic and human metrics that the translation direction does matter, we turn to the question of tagging translation direction (with a special "<translationese>" tag at the start of the source sentence for source-Translationese sentences), to see if this will enable the Mixed data systems to make better use of all available information. Tagging has been shown to be effective in multilingual  (Johnson et al., 2017; Rikters et al., 2018)  and multidomain  (Kobus et al., 2017)  systems, as well as when using backtranslated data  (Caswell et al., 2019) . All of these systems for Q2 make use of the full 13.7M line training set. . We indicate if a system is tagged through the addition of "Tagged" in the system name, while untagged systems are unmarked. The untagged systems here are the same Mixed systems shown in Table  1 . 

 EN?FR Table  2  shows our results. The effect of tagging is stronger in the FR-EN translation direction, where simply adding tags results in a BLEU score increase of 0.6 (chrF increase of 0.005). We recall that the Authentic FR source data is much smaller than the Authentic EN source, so we hypothesize that tagging allows the system to take better advantage of the two types of data. In the other translation direction, where Authentic EN source already comprises the majority of the training data, we observe minimal changes when applying tagging. The Mixed (untagged) and initial Tagged Mixed experiments are performed with a validation set that consists only of Authentic source data. This raises the question of whether that is adequate to make the most of the information contained in the tags, or whether using a Tagged Mixed validation set (with 1461 lines Authentic-EN source, and 539 lines Authentic-FR source) might be better. We refer to this system that uses the Tagged Mixed validation set as "Tagged Mixed+mixdev" in Table  2 . In the FR-EN direction, we see an additional 0.3 BLEU improvement when using the Tagged Mixed+mixdev (0.002 chrF improvement). In the other direction, we see a small 0.1 BLEU improvement and no corresponding change in chrF. In the EN-FR direction, where Authentic data was already the majority, we do not find any significant BLEU score differences between the various tagged and untagged systems. However, in the FR-EN direction, both the Tagged Mixed and Tagged Mixed+mixdev systems are found to be significantly better in terms of BLEU than the Mixed (untagged) system, according to pairwise bootstrap resampling (with 100% of samples showing this to be the case). Paired bootstrap resampling also finds that in the FR-EN direction the Tagged Mixed+mixdev system is significantly better in terms of BLEU than the Tagged Mixed system (p < 0.05, with 98.6% of the samples showing this result). Human evaluation provides additional insight. For Table  2  systems, we collected rankings for 391 (EN-FR) and 495 (FR-EN) source sentences and their translation triplets, respectively. We first note that in both translation systems, we observe the same pattern: Tagged Mixed is ranked best, followed by Tagged Mixed+mixdev, with Mixed (untagged) ranked worst. In the English-French direction, none of the average human system rankings differ significantly, which is unsurprising given how close they are to one another, as shown in Figure  3a . This matches the automatic metrics and our intuitions: Authentic English source makes up the majority of the Mixed training data, and we already observed that Authentic and Mixed translation systems performed quite similarly in this direction. In the French-English direction, shown in Figure  3b , we do not find a significant difference in human rankings between the the two tagged systems (Tagged Mixed and Tagged Mixed+mixdev). However, based on pairwise bootstrap resampling, the human annotators rank both tagged systems (Tagged Mixed and Tagged Mixed+mixdev) significantly higher than the (untagged) Mixed system. This is partially in agreement with the results on BLEU, but may merit more exploration. The significant improvement in human ranking by adding tagging (FR-EN) suggests that in a scenario where the Authentic source data makes up a minority of the training data, it is beneficial to add direction tags. When Authentic data makes up the majority of the training data, it does not hurt to add direction tags, but it does not appear to significantly help. We examine this in a controlled experiment in Section 4.4. In our previous experiments, we maintained a fixed ratio of Authentic and source Translationese, matching the true distribution of our dataset. We now examine what happens when we vary the ratio of Authentic to Translationese source data, maintaining a fixed corpus size. This is a moderate resource setting with 3.7M lines.  12  We vary the proportion of Authentic training data from 0% to 100% (by steps of 12.5%) and build translation systems in both directions, both tagged and untagged, using Authentic source validation sets. As we see in Figure  4 , translation quality on Authentic source test data increases as the percentage of Authentic source training data increases. Below 50%, tagging clearly helps bridge the gap, but the importance of tagging decreases as the percentage of Authentic source training data increases. This trend matches our earlier intuitions. In the English-French direction (Figure  4a ), the gap is greatest at 12.5% (i.e., tagging provides the most additional benefit), and shrinks as it approaches in 100%. In the French-English direction (Figure  4b ), the story is similar, though the two approaches appear to converge around 50%.  13  Thus we would argue that tagging translation direction is worth considering in situations where the "matching" translation direction (Authentic source) makes up the minority of the data, though it may still have some benefits at higher percentages. 

 Q3: Proportion of Source Translationese 

 Conclusion We have shown that in a moderate-resource setting with high-quality translations in training data, training on Authentic-source data or Tagged Mixed-source data is preferred over training on Translationese-source or Mixed (untagged) source data, by both automatic metrics and human judgments. This is in contrast with the findings of  Riley et al. (2020) , who found that BLEU scores could be "gamed" to produce higher scores with translationese-like output, while being judged to be worse by human annotators. This raises questions for future work, such as whether Translationese training effects may vary depending on the quality of the parallel text, the proportion of Translationese data, and the size of the training data, or whether differences in experimental setup and human annotation may also come into play. Future work could examine these issues across a wider range of language pairs and domains, as well as directly comparing known translation direction with automatically predicted translation direction. Figure 1 : 1 Figure 1: Example output of French-English MT trained on Authentic-source (authentic French, translationese English) and Translationese-source (translationese French, authentic English). 
