title
NVIDIA NeMo's Neural Machine Translation Systems for English ↔ German and English ↔ Russian News and Biomedical Tasks at WMT21

abstract
This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English ? German (En ? De) and English ? Russian (En ? Ru) are built on top of a baseline transformer-based sequence-to-sequence model  (Vaswani et al., 2017) . Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English ? Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacre-BLEU score of 39.5 on the WMT'20 En ? De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru ? En and En ? Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions.

Introduction We take part in the WMT'21 News Shared Task for English ? German, English ? Russian, and the Biomedical Shared Task for English ? Russian. Our systems are implemented in the NVIDIA NeMo 1 framework  (Kuchaiev et al., 2019) . They build on baseline sequence-to-sequence transformer models  (Vaswani et al., 2017)  in the following ways: 1) Checkpoint averaging, 2) Model scaling up to 1B parameters, 3) Data augmentation with 1 https://github.com/NVIDIA/NeMo large-scale backtranslation  (Edunov et al., 2018)  of monolingual Newscrawl data and sequence-level knowledge distillation from a right-to-left factorized model  (Zhang et al., 2019b) , 4) Finetuning models on in-domain news data from WMT test sets made available in previous years, 5) Ensembling models trained on different subsets of the overall data 6) Shallow fusion decoding with transformer language models  (Gulcehre et al., 2015)  7) Noisy channel re-ranking of beam search candidate hypotheses  (Yee et al., 2019) . Overall, we find each of these components results in a small improvement in BLEU scores with backtranslation results being mixed depending on the language direction and whether the test data contains translationese inputs. Using a combination of these techniques, we achieve 39.5 sacre-BLEU scores on the En ? De WMT'20 test set, outperforming the best BLEU scores from last year's competition of 38.77. Training our En ? Ru biomedical task submission from scratch using a biomedical vocabulary and similar model improvements to those used for our news task submission, we report a sacreBLEU score of 40.3 on En ? Ru and 43.8 on Ru ? Enh on the WMT'20 Biomedical Shared Task test dataset. This improves over the best submissions from last year's competition 2 of 39.6 and 43.3 on En ? Ru and Ru ? En respectively. 

 Datasets We participated in the constrained data track at this year's news and biomedical competitions and used all the parallel corpora provided by the WMT Shared Tasks for both En ? De and En ? Ru. We used the provided English, German, and Russian monolingual Newscrawl data for backtranslation and training our autoregressive transformer language models. We filter out monolingual Newscrawl data only based on minimum and maximum length criteria, but perform more aggressive filtering of our parallel data described in Section 2.1. 

 Parallel Corpus Filtering We use a combination of the following data filtering steps for all parallel corpora (including pseudo parallel corpora generated via backtranslation and distillation) except for the Biomedical Shared Task provided data. ? Language ID Filtering -We use the fastText  (Joulin et al., 2016)  language ID classifier 3 to remove training examples that aren't in the appropriate language. ? Length and Ratio Filtering -We filter out examples where a sentence in either language is longer than 250 tokens before BPE tokenization and where the length ratio between source and target sentences exceeds 1.3. ? Bicleaner -Bitexts that were assigned a Bicleaner (Ram?rez-S?nchez et al., 2020) score of < 0.6 were removed. On the news shared task, we keep 60M parallel sentences for En ? De and 26M sentences for En ? Ru after filtering. 

 Biomedical Task Data Our parallel biomedical domain data included a mix of all the En ? Ru parallel training data given by shared task organizers and biomedically relevant examples selected from the provided En ? Ru news task data. We trained two biomedical domain binary classifiers, one for English and one for Russian. The classifiers were composed of two task-specific fully connected layers on top of pre-trained BERT Base  (Devlin et al., 2018)  or RuBERT Base  (Kuratov and Arkhipov, 2019)  for English and Russian respectively. The positive examples were sourced from the WMT'20 Biomedical Shared Task train set. The negative examples were randomly sampled from the parallel En ? Ru news data given for the WMT'21 news task. An equal amount of 45K examples were used for both the positive and negative classes. We ran our English biomedical domain classifiers on the English half of all approximately 26M parallel En ? Ru WMT'21 news training data. We saved all sentences with predicted biomedical domain probabilities over 50%, collecting around 560k examples. We then ran our Russian classifier on the Russian counterparts to the 560k predicted in domain English sentences. We averaged the classifier scores from the English and Russian domain classifiers and used this average score as our final selection criteria. We set a cut-off threshold of .90 resulting in 208K parallel examples classified from the news domain data. We combined this with the 46k parallel biomedical examples provided for the task, resulting in a total of 256,037 parallel training examples. 

 Data Pre-processing and Post-processing We normalize punctuation 4 and tokenize 5 examples with the Moses toolkit. For En ? De, we train a shared BPE tokenizer with a vocab of 32k tokens using the YouTokenToMe 6 library. For En ? Ru, we train language-specific BPE tokenizers with a vocab of 16k tokens each. For the En ? Ru Biomedical translation task, we learn a separate BPE tokenizer solely on our Biomedical Task Data described in 2.2. We use BPE-dropout  (Provilkov et al., 2019)  of 0.1 for both language pairs and tasks. We post-process En ? De model generated translations to replace quotes with their German equivalents -" and ". 

 System overview Our systems build on the Transformer sequenceto-sequence architecture  (Vaswani et al., 2017) . In the subsequent subsections, we discuss model scaling, checkpoint averaging, data augmentation with backtranslation and right-to-left distillation, model finetuning, ensembling, shallow fusion decoding with LMs, and noisy channel re-ranking. 

 Model Configurations We experiment with three different model configurations -Large, XLarge, and XXLarge. The Large configuration corresponds to the "Transformer Large" variant from  Vaswani et al. (2017)  and the XLarge and XXLarge scale that base configuration along depth and width. The exact specifications are in Table  1 . Following  Kasai et al. (2020) , we keep the number decoder layers fixed at 6 and scale only the depth of the encoder to 24 layers for the "XLarge" configuration. For stable optimization of deep transformers, we use the "pre-LN" transformer block  (Xiong et al., 2020) . When scaling to 1 billion parameters (XXLarge), we only increase hidden and feedforward dimensions of the model.  

 Large XLarge XXLarge 

 Checkpoint Averaging Over the course of training, we save the top-k checkpoints that obtain the best sacreBLEU scores on a validation set. The final model parameters are obtained by averaging the parameters corresponding to these checkpoints. ? avg = 1 k k i=1 ? i ? avg are the model parameters after checkpoint averaging and ? 1 . . . ? k are the individual checkpoints being averaged. Empirically, we didn't observe a difference between averaging the last k checkpoints versus the top-k checkpoints. The former is however more common and implemented in libraries such as fairseq  (Ott et al., 2019) . 

 Data Augmentation with Backtranslation & Right-to-left model distillation We follow  Edunov et al. (2018)  in backtranslating monolingual Newscrawl data with noise introduced via topk sampling (k=500). For En ? De, we backtranslate ~250M sentences and filter translations based on the process described in Section 2.1. We observed fairly significant drops in BLEU score when using backtranslated data for En ? Ru and did not apply any data augmentation for this language pair. We use the XLarge model configuration trained only on the News Task provided parallel corpus to generate translations. We also train an XLarge model for En ? De and De ? En on the News Task provided parallel data where the output sequence is factorized from right-to-left. Translations of the training dataset with topk sampling (k=500) using these models are generated and added to the overall training set. When adding only backtranslated text or data generated from right-to-left factorized models, we use a 2:1 ratio of parallel to pseudo-parallel (model generated) data. When training with a combination of both, we use a 6:3:1 ratio of parallel, right-toleft generated, and backtranslated data. We skew data sampling in this way since training on right-toleft generated data showed better performance on recent WMT test sets as opposed to backtranslation which did better on old test sets that contained translationese inputs (see Tables  2 and 3 ). 

 Mixed Domain Training For the biomedical task submission, we experiment with different mixed domain training approaches  (Zhang et al., 2019a) . We train on the concatenated combination of news task and biomedical task dataup-sampling the proportion of biomedical data to make up 30% or 50% of the data-parallel examples seen during training. We also train models on concatenated data with no up-sampling and with purely news task data. The base models trained on exclusively news task data still use the biomedical vocabulary tokenizer. 

 Model Finetuning For our news task submission, we finetuned models on an in-domain parallel corpus consisting of WMT provided test datasets from past years (WMT'08 -WMT'19 for En ? De comprising ~32k examples) for both En ? De and En ? Ru. We finetuned our biomedical task base models on the 250k parallel sentences obtained via the process described in Section 2.2. Models are finetuned for 1-2 epochs using a fixed tuned learning rate and the top-k checkpoints on a validation dataset (new-stest2020 for the News Shared Task) are averaged. 

 Ensembling Given k different models for a particular language direction trained with the same tokenizer, we ensemble them at inference by averaging their probability distributions over the next token. P (y t |y <t , x; ? 1 . . . ? k ) = 1 k k i=1 P (y t |y <t , x; ? i ) Where P (y t |y <t , x) is the probability distribution over the target token y t given all previously generated target tokens y <t and the input sequence x. ? 1 . . . ? k are the k different models being ensembled. Beam search scores are computed using these averaged probabilities at each time step. In practice, we ensemble models trained on different subsets of the available data. For En ? De, we ensemble a total of 6 models trained on different subsets of the data. Example: News Task provided bitext only, the addition of backtranslated and/or data from right-to-left factorized models and finetuned models. For En ? Ru, we ensemble a total of 3 identical XLarge models trained with different random seeds on the main parallel corpus. For the En ? Ru biomedical task, we ensemble 4 finetuned models whose base configurations were trained with different mixed domain sampling ratios. Specifically, each translation direction includes an ensemble of models initially trained on mixed domain data with 50% up-sampling of biomedical data, concatenated biomedical and news data with no up sampling, exclusively news data, and exclusively news data with right-to-left distillation. 

 Shallow Fusion Decoding with Language Models Aside from backtranslation, another way to leverage large amounts of monolingual data is via training language models. We train language-specific 16-layer transformer language models at the sentence level, which is architecturally similar to Radford et al.  (2019) . They are trained on Newscrawl and use the same tokenizers as our NMT systems. When generating translations, we decode jointly with our NMT system ? s?t and a target side language moel ? t  (Gulcehre et al., 2015) . The score of a partially decoded sequence on the beam S(y 1...n ) of length n is given by the following recurrence S(y 1...n |x; ? s?t , ? t ) = S(y 1...n?1 |x; ? s?t , ? t ) + log P (y n |y <n , x; ? s?t ) + ? sf log P (y n |y <n ; ? t ) where the empty sequence has a score of 0. We tuned the LM importance coefficient ? sf on a validation dataset and found a value between 0.05 -0.1 to work well in practice. 

 Noisy Channel Re-ranking We re-rank the beam search candidates produced by our ensemble model generated with or without shallow fusion using a neural noisy channel model  (Yee et al., 2019) . The noisy channel model computes the score of any translation S(y i |x) on the beam based on a forward (source-to-target) model, a reverse (target-to-source), and a target language model. The best translation after re-ranking is given by arg max i S(y i |x) = log P (y i |x; ? ens s?t ) +? ncr log P (x|y i ; ? t?s ) + log P (y i ; ? t ) Forward log probabilities are given by an ensemble of source-to-target models ? ens s?t . We experimented with using an ensemble of target-tosource translation models to compute log P (x|y i ) but didn't observe any empirical benefits and so all reported results use only a single reverse model ? t?s for noisy channel re-ranking. We generate 15 candidates via beam search and tune ? ncr on a validation dataset and found a value between 0.5 -0.7 to work well in practice. 

 Training & Optimization All En ? De models were trained for up to 450k updates using the Adam optimizer (Kingma and Ba, 2014) with ? 1 = 0.9, ? 2 = 0.98 and Inverse Square Root Annealing  (Vaswani et al., 2017)  with 30k warm-up steps and a maximum learning rate of 4e-4. En ? Ru models were trained for up to 150k updates with 7k warmup steps. We use label smoothing of 0.1 and a dropout of 0.1 on intermediate activations including attention scores to regularize our models. The "Large" models were trained on NVIDIA DGX-1 machines with 8 32G V100 GPUs. We use a batch size of 16k tokens per GPU for an effective batch size of 128k tokens. The "XLarge" models were trained on 64 GPUs split across 4 NVIDIA DGX-2 nodes with 16 32G V100 GPUs each. These models use an effective batch size of 256k tokens. Finally, our "XXLarge" models were trained on 256 GPUs across 16 DGX-2 nodes with an effective batch size of 512k tokens.  

 News Task Submission In this Section, we present results for our News Shared Task submission. Tables  2 and 3  contain ablations for En ? De and while Tables  4 and 5  has ablations for En ? Ru. Each of the components we describe improves BLEU scores except for backtranslation and scaling our models to 1B params. Both show mixed results on En ? De -scores improve significantly on WMT'14 and WMT'18 test sets when adding backtranslated data (possibly because these test sets contain translationese inputs) but hurts or does not improve performance on WMT'19 and WMT'20 test sets. Our 1B parameter model does significantly better on WMT'14, but worse on WMT'19 and is comparable to the 500M parameter model on  WMT'18 and WMT'20  Backtranslation significantly hurt performance in initial experiments on En ? Ru so we exclude it from our ensemble. The impact of ensembling, finetuning, and shallow fusion are fairly similar to En ? De. Additionally, we also report an "Oracle BLEU" score in Tables  4 and 5  where we compute BLEU scores by cheating and picking the translation on our beam that has the highest sentence BLEU score with respect to the reference. This is a useful indicator of how much there is to gain by re-ranking the beam search candidates. 

 Biomedical translation task submission We present our Biomedical Shared Task submission in this section. Building on lessons learned from our news task ablation studies, we opted to use the Transformer-XLarge architecture, and average all of the intermediate model checkpoints which helps reduce model variance as a consequence of finetuning. Tables  6 and 7  show our results as we iterated on improving our models. We trained our BPE tokenizer on biomedical data to mitigate character-level segmentation of words unique to the biomedical domain. We found this had a minimal effect on model performance. This could be because the majority of our parallel biomedical data was selected from news task training data, and thus biomedical words were already adequately accounted for by the news task model's tokenizer. We found that up-sampling in-domain Unsurprisingly, finetuning base models on biomedical domain data improved BLEU scores for all models. In-domain finetuning helped models initially trained on news task data overcome performance gaps between themselves and models that had seen a higher amount of biomedical data during training. Neither shallow fusion nor noisy channel re-ranking improved model performance after ensembling for the Ru ? En direction. Both techniques individually improved En ? Ru performance but failed to do so in combination. Ensembling our models led to an additional performance boost and allowed us to reach our maximum En ? Ru BLEU score of 40.3 and Ru ? En BLEU score of 43.8. These scores show a 0.9 and 0.5 improvement over last year's best score of 39.4 and 43.3  (Bawden et al., 2020)  respectively. 

 Conclusion We present Neural Machine Translation Systems for the En ? De News Task and En ? Ru News and Biomedical shared tasks implemented in the NeMo framework  (Kuchaiev et al., 2019) . Our systems build on the Transformer sequence-tosequence model to include backtranslated text and data from right-to-left factorized models, ensembling, finetuning, mining biomedically relevant data using domain classifiers, shallow fusion with LMs, and noisy channel re-ranking. These achieve competitive performance to submissions from previous years. 
