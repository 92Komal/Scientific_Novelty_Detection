title
Huawei AARC's Submissions to the WMT21 Biomedical Translation Task: Domain Adaption from a Practical Perspective

abstract
This paper describes Huawei Artificial Intelligence Application Research Center's neural machine translation systems and submissions to the WMT21 biomedical translation shared task. Four of the submissions achieve state-of-the-art BLEU scores based on the official-released automatic evaluation results (EN?FR, EN?IT and ZH?EN). We perform experiments to unveil the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with overfitting and under-translation are also discussed.

Introduction General-purpose machine translation systems have limited capability in addressing domain-specific tasks  (Koehn and Knowles, 2017) , for example, the WMT biomedical translation shared task, due to the low availability for high-quality in-domain data. In our WMT20 submission, various domain adaption technologies  (Bawden et al., 2019 (Bawden et al., , 2020  have been applied including practical approaches finetuning on general-purpose models, back-translation  (Sennrich et al., 2016)  and leveraging in-domain dictionaries  (Peng et al., 2020b) . Despite achieving state-of-the-art (SOTA) BLEU scores for most of the submissions, few efforts were put in place to disclose the practical insights associated with these techniques. This year, the Artificial Intelligence Application Research Center (AARC) participate in the WMT21 biomedical translation task for eight language directions between English and other four languages  (French, German, Italian, and Chinese) . The baseline model is an in-house general-purpose NMT model built upon the transformer-big architecture  (Vaswani et al., 2017) . Apart from presenting an overview of the proposed biomedical Neural * Co-first authors. Machine Translation (NMT) system, we investigate the practical insights of the involved domain adaptation techniques, including finetuning order, terminology dictionaries, and ensemble decoding. Issues associated with overfitting to in-domain data and under-translation are also discussed. 

 The Data In this section we detail the bilingual and monolingual data used in this shared task (Table  1 ). 

 Bilingual Data In-domain bilingual data In all directions, we use the in-domain data (IND) provided by the shared task organizers to finetune the base model.  1  The IND data consists of WMT-released bitexts from Pubmed, UFAL 2 and Medline.  3  We notice that the official release of IND data suffers from issues of misalignment between source and target sentences, and missing target sentences. The translation of a source sentence may be misplaced in a different line or even appeared in multiple lines on the target side. Moreover, a source sentence may have not been translated into in a target sentence. A data processing pipeline is developed to address the issues mentioned above (depicted in 3.4). The test data is the official release of the WMT19 shared task. Augmented Bilingual Data We collect indomain data from TAUS 4 for the English-French, English-Italian and English-Chinese language pairs (depicted in Master's and Doctoral Dissertations, we align the data on the sentence level by using a model proposed by Ac ?arc ?ic ?ek et al. (  2020 ). This is done by finetuning a RoBERTa  (Liu et al., 2019)  filter model on the TAUS dataset and selecting the source-target sentence pairs above a normalized log-probability threshold of 90%. General-domain bilingual data We observe that finetuning the base model with IND data alone may incur sub-optimal BLEU scores. A conjecture is that the test data has a different distribution to that of the IND data. We present a case to show that finetuning the base model on a mixture of general domain data (OOD) and IND data can produce minor improvements (shown in 4.2). 

 Monolingual Data A batch of monolingual Medline data in English (IND-BT.) dated before July 2018 has been collected and back-translated for data augmentation. The official released IND data from WMT is also back-translated. The models used for backtranslation are from our last year's competition  (Peng et al., 2020b) . 

 The Approaches The proposed systems are finetuned using the following methods. All models are trained on one Tesla V100 GPU, taking approximately 8-20 hours depending on the volumes of data involved. 

 Leveraging In-domain Dictionary Leveraging domain-specific dictionaries is a viable solution for domain adaptation in NMT  (Peng et al., 2020a,b)  to enhance IND data coverage. We collect lexicons from SNOMED-CT 5 , DOPPS 6 , WFOT 7 and generate a terminology dictionary which is subsequently attached to the end of training data. Terminology is further entended to cover COVID-19 related terms obtained from Neulab. 8 

 Ensemble Ensembling methods is a machine learning technique that aggregates several base models to generate one optimal predictive model  (Garmash and Monz, 2016) . We choose the top two models to ensemble in an attempt to produce a more general NMT model. 

 Architecture To train the in-domain NMT model, we choose the in-house NMT system trained on general domain data as a baseline built upon the transformer-big architecture. LazyAdam optimizer is used during the training phase with a learning rate of 1e ?5 and a warm-up period of 16,000 steps. The dropout ratio is set to 0.1, and the batch size for training and validation is 6,144 and 32 tokens, respectively. The width of the beam search is 4. Early stopping is applied to the training.  

 Data Processing Several pre-processing techniques are introduced to ensure the quality of the data. ? First, we perform punctuation normalization to standardize their formats using Moses library  (Koehn et al., 2007) . ? Then we carry out a primary data cleaning process to remove nonstandard sentences, including those with special characters, weblinks, extra spaces, and other bad cases. ? According to the length of the sentence after segmentation and the proportion of rare words, we remove bitexts with more rare words in the sentences. We further clean the data by skipping those sentence pairs with more than 100 subwords or less than one subword. The bitexts with a source and target sentence length ratio of more than 2.5 are excluded. A language detection tool 9 is used to filter out bitexts with abnormal language patterns, i.e., sentences with undesirable langid. ? An alignment model trained by fast-align  (Dyer et al., 2013)  10 is used to score the corpus to remove misaligned parallel sentences. After decoding, post-processing is performed to detokenize subwords and remove undesirable spaces between special characters and numbers, i.e., converting "rs = 0.9148" into "rs=0.9148". 

 Experimental Results and Analysis The base systems are trained with OOD data and finetuned using IND data enhanced with monolingual data to produce the submitted results. We 9 https://github.com/aboSamoor/polyglot 10 https://github.com/clab/fast align extract the OK-aligned data from the last two years (WMT19 and WMT20) and produce the test data to evaluate the NMT models. The BLEU scores are calculated using the MTEVAL script from Moses  (Koehn et al., 2007) . Results are shown in Table  2 . The final two rows demonstrate the results of our submissions this year and the best official records released by the organizers. 

 Finetuning Order Does Matter We identify the order of training is crucial in the experiment. We perform the experiment under the following three training strategies: 1. Strategy 1 (S1): the baseline is finetuned on the back-translation (BT) pseudo parallel corpus, followed by another finetuning using IND data. 2. Strategy 2 (S2): the baseline is finetuned using the IND data, followed by another finetuning using the BT data. 

 Strategy 3 (S3 ): the baseline is finetuned using a mixture of BT and IND data. Table  5  presents the results of this comparative study for French?English translation direction. It can be observed that finetuning order generates significantly different BLEU scores, with Strategy 1 achieving a BLEU score +8.89 higher than that from Strategy 2. We follow the training strategy 1 in WMT21 shared task to this end. 

 OOD Data Mixed Finetuning We observe that finetuning the base model with IND data alone (particularly with a limited amount of IND data) may result in sub-optimal BLEU scores. This may indicate overfitting to the training data, which has a different distribution to the test data. We perform a series of experiments to   disclose this issue. As shown in Tables  6 and 7 , finetuning with a mixture of OOD and IND data generates minor improvements. Interestingly, the experiment results are sensitive to the amount of OOD data involved. Future work is planned to look into this issue in detail. 

 The Effect of Terminology Dictionaries In this section, we perform an ablation study to show the effectiveness of terminology dictionaries. The IND dictionaries are appended to bitexts as a part of the corpus to train NMT models.   

 Ensemble Decoding Ensemble decoding is applied to improve the generality of the NMT model by averaging the logarithmic probabilities of a decoded token. It can be observed from Table  4  that ensemble decoding is marginally effective compared to well-learned NMT models. This finding is consistent with that obtained from . 

 Under-translation with Overfitting Under-translation occurs when the NMT model fails to decode a portion of the input sentence. One of Chinese?English models under-translates a particular sentence of the WMT21 test data. For example, as shown in Table  8 , "? ?" of the input has been left untranslated. After increasing the width of the beam search, under-translation can be avoided. In our opinion, under-translation may be caused by noisy IND data, in which the learned self-attentions are not differentiable during decoding. By ensembling the affected model with the baseline, we successfully rectify the problem. sentence example input The disease duration ranged from 2 weeks to 60 months (median, 4 months), and the affected segment was C All the patients were followed up 3 to 42 months (median, 12 months). prediction ?2? input The median age of the 30 patients was 56.5 (28-80) years old, among them, 25 patients were primary plasma cell leukemia, and 5 patients were secondary plasma cell leukemia. prediction 30?56.5(28 input ?10?OS? ?100%?60.6%(P=0.0007)? prediction The 10-year os rate was 100% and 60.6% respectively (p=0.0007).  

 Conclusion This paper depicts Huawei's neural machine translation systems and submissions to the WMT21 biomedical shared task. We have achieved stateof-the-art BLEU scores for four of eight language pairs (EN?FR, EN?IT and ZH?EN) based on the official-released results. We also explore practical issues for the involved domain adaptation techniques, including the effects of finetuning order, terminology dictionaries, and ensemble decoding on enhancing the performances of cross-domain NMT. We have discussed issues associated with overfitting and under-translation. Table 1 1 as IND-Aug.) to address the in-domain data scarcity issue. For English-Chinese data, after collecting a portion of abstracts of China 
