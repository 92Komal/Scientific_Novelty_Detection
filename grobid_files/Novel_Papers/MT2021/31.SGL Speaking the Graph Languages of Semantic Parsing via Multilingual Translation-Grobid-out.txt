title
SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation

abstract
Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https: //github.com/SapienzaNLP/sgl.

Introduction Being able to associate natural language text with well-defined and machine-actionable meaning representations, i.e. the task of semantic parsing (SP), is one of the holy grails in Natural Language Processing (NLP) and Understanding  (Navigli, 2018) . Considering how a breakthrough in this direction would empower NLP systems to explictly make sense of natural language, the evergrowing interest semantic parsing has been receiving really comes as no surprise. Graph-based formalisms such as Abstract Meaning Representation  (Banarescu et al., 2013, AMR) , Elementary Dependency Structures  (Oepen and L?nning, 2006, EDS) , Prague Tectogrammatical Graphs  (Haji? et al., 2012, PTG) , Universal Conceptual Cognitive Annotation  (Abend and Rappoport, 2013, UCCA) , inter alia, are emerging as the de facto standard for general-purpose meaning representations and have shown potential in Machine Translation  (Song et al., 2019) , Text Summarization  (Hardy and Vlachos, 2018) , Human-Robot Interaction  (Bonial et al., 2020) , and as evaluation metrics  (Sulem et al., 2018; Xu et al., 2020b) . These formalisms propose encoding meaning through directed graphs, however, each of them builds upon different linguistic assumptions, aims to target different objectives and, at a more practical level, assigns different functions to nodes and edges. For instance, while AMR uses nodes to encode concepts and edges to express the semantic relations between them, UCCA proposes using text tokens as terminal nodes and building graphs on top of them. As a result of this heterogeneous landscape, often referred to as framework-specific balkanization  (Oepen et al., 2020) , graph-based semantic parsing has seen a proliferation of framework-specific solutions. However, approaches capable of competitively scaling across formalisms represent a natural desideratum, and recent works have started to explore this direction, examining the usage of multi-task learning in different architectures  (Hershcovich et al., 2018; Oepen et al., 2019) , or casting different formalisms under a unified framework where models can be trained to perform graph transduction  (Zhang et al., 2019b) . Nevertheless, despite achieving promising results, research in this direction has been hindered by the general lack of training data that afflicts semantic parsing. Indeed, due to the inherent complexity of this task, annotated corpora are still scarce and prohibitively expensive to expand. In this work, we set ourselves to address these issues and, in particular, we propose Speak the Graph Languages (SGL), a many-to-many seq2seq architecture which we show to competitively scale across formalisms and across languages.  1  The key idea is to train a seq2seq model with a Multilingual Neural Machine Translation (MNMT) objective, where, given an input text and an identifier denoting the desired output formalism, a single shared model has to learn to translate towards the corresponding linearized graph. We use AMR and UCCA as our cases in point to show the effectiveness of this framework. In particular, we show that, once the learning procedure also considers large parallel corpora coming from Machine Translation (MT), this configuration becomes an effective approach for framework-independent parsing via a single model. Even more interestingly, this model scales surprisingly well to cross-lingual parsing and is capable of navigating through translation paths like IT ? AMR, 2 which it has never seen during training. The contributions of this work are therefore as follows: ? We reframe semantic parsing towards multiple formalisms and from multiple languages as multilingual machine translation; ? On AMR parsing, our framework achieves competitive performances, surpassing most of its current competitors once paired with a pre-trained Transformer; ? We outperform all current alternatives in crosslingual AMR parsing without ever seeing non-English to AMR examples at training time and push the current state of the art even further once we include these examples; ? On UCCA parsing, we reach competitive results, outperforming a strong BERT-powered baseline  (Hershcovich and Arviv, 2019) . We release our code and our models for research purposes. 

 Related Work Our work is mainly concerned with semantic parsing in UCCA and AMR, considering also the cross- 1 By across languages, we mean that the model is capable of performing cross-lingual semantic parsing as defined for AMR by  Damonte and Cohen (2018) . Unless otherwise specified, we will follow this perspective throughout the paper. 2 IT stands for Italian. lingual setting introduced by  Damonte and Cohen (2018)  for the latter. Semantic Parsing Arguably among the formalisms that have drawn the most interest, AMR has seen the emergence of a rich yet dedicated literature, with recent approaches that can be roughly clustered into two groups. On the one hand, several graph-based solutions have been proposed  (Lyu and Titov, 2018; Zhang et al., 2019a,b; Zhou et al., 2020; Cai and Lam, 2020) ; among these solutions,  Zhou et al. (2020)  show the effectiveness of enhancing an aligner-free parser with latent syntactic information, whereas  Cai and Lam (2020)  present an iterative method to build and refine AMR graphs incrementally. On the other hand, translation-based approaches, where seq2seq models are trained to translate from natural language text to linearized graphs, have been shown to reach competitive performances, despite the scarcity of training data  (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019) . Continuing this latter direction and arguably closest to our work,  Xu et al. (2020a)  and  Bevilacqua et al. (2021)  show that these models, once paired with adequate pre-training, can behave on par or better than dedicated and more sophisticated graph-based alternatives, surpassing the performances of  Cai and Lam (2020) . In particular, similarly to our work,  Xu et al. (2020a)  leverage a multilingual framework inspired by  Johnson et al. (2017)  and explore the possibility of pre-training on a range of related tasks, including MT; however, their focus is limited to showing the effectiveness of transfer learning from related tasks to English AMR parsing. Conversely, here we show that the benefits of multilingual seq2seq frameworks are not limited to English TEXT-to-AMR but, rather, that they enable astonishing performances on unseen translation paths such as IT ? AMR and competitive results on other frameworks, using UCCA as our case in point. In this sense, we continue the recent cross-framework trend formally started by the shared task of  Oepen et al. (2019) , exploring the possibility of using translation-based approaches for framework-independent parsing, as opposed to the transition-based parsers proposed in that seminal work. Our findings are in line with the recent results reported by  Oepen et al. (2020)  and, in particular, by  Ozaki et al. (2020) , where the authors cast semantic parsing in multiple formalisms as translation towards a novel Plain Graph Notation  [ <root_0> L [ <L_0> T [ After ] ] H [ <H_0> P [ <P_0> T [ graduation ] ] A * [ <A_0> T [ John ] ] ] U [ <U_0> T [ , ] ] ] H [ <H_1> A [ <A_1> R [ <R_0> T [ to ] ] C [ <C_0> T [ Paris ] ] ] A <A_0> P [ <P_1> T [ moved ] ] ] (d) UCCA Linearization Figure  1 : AMR and UCCA graphs, along with their linearizations, for the sentence "After graduation, John moved to Paris". To ease readability, linearizations are shown with newlines and indentation; however, when fed to the neural model, they are in a single-line single-space format. (PGN) they devise. However, whereas they train different independent models for each framework, we explore the possibility of using a single multilingual model. Cross-lingual AMR While most of the research effort in the AMR community has been focused on English only, the seminal work of  Damonte and Cohen (2018)  gave rise to an interesting new direction, i.e. exploring the extent to which AMR can act as an interlingua. The authors introduced a new problem, cross-lingual AMR parsing, and defined it as the task of recovering, given a sentence in any language, the AMR graph corresponding to its English translation. Using an adapted version of the transition-based parser originally proposed by  Damonte et al. (2017)  and training it on silver data generated through annotation projection, they examined whether AMR graphs could be recovered starting from non-English sentences. Even though their models fell short when compared to MT alternatives, 3 their work showed promising results and suggested that, despite translation divergences, AMR could act effectively as an interlingua. Annotation projection has been focal in subsequent work as well.  Blloshmi et al. (2020)  propose an aligner-free cross-lingual parser, thus disposing of the need for word alignments in the annotation projection pipeline; their parser manages to outperform MT alternatives when both annotation projection and these baselines have access to comparable amounts of data. Conversely,  Sheth et al. (2021)  leverage powerful contextualized word embeddings to improve the foreign-text-to-English-AMR alignments, surpassing all previous approaches and, most importantly, the yet-unbeaten MT baselines that have access to larger amounts of data. We stand out from previous research and show that, as a matter of fact, annotation projection techniques are not needed to perform cross-lingual AMR parsing. By jointly training on parallel corpora from MT and the EN ? SP data we have, we find that a multilingual model can navigate unseen translation paths such as IT ? AMR effectively, outperforming all current approaches by a significant margin; yet, annotation projection is naturally beneficial and, when its training data are taken into account as well, SGL pushes performances even further. 

 Speak the Graph Languages (SGL) In this section, we describe SGL, our proposed approach to graph-based semantic parsing. We first explain the graph linearizations we employ for AMR and UCCA, along with their delinearizations ( ?3.1). We then describe the seq2seq modelling approach we use ( ?3.2) and, finally, we present our multilingual framework ( ?3.3). 

 Graph Linearizations We now describe how we convert the considered meaning representations into translatable text sequences (linearization), along with their reverse process (delinearization). For AMR parsing, as in van Noord and Bos (2017), we first simplify AMR graphs by removing variables and wiki links. We then convert these stripped AMR graphs into trees by duplicating coreferring nodes. At this point, in order to obtain the final linearized version of a given AMR, we concatenate all the lines of its PENMAN notation (Goodman, 2020) together, replacing newlines and multiple spaces with single spaces (Figure  1a and  1b ). Conversely, delinearization is performed by assigning a variable to each predicted concept, per-forming Wikification, 4 restoring co-referring nodes and, where possible, repairing any syntactically malformed subgraph.  5  For both phases, we use the scripts released by van Noord and Bos (2017).  6  For UCCA parsing, we employ a Depth-First Search (DFS) approach: starting from the root, we navigate the graph, using square brackets to delimit subgraph boundaries and special variables to denote terminal and non-terminal nodes; remote edges are denoted by a special modifier appended to their labels, while re-entrancies, that is, edges whose target is a node already seen, are handled by simply entering the respective variable (Figure  1c and 1d ). Similarly to AMR, delinearization is performed by back-parsing this sequence into a UCCA graph, repairing malformed subgraphs when possible; 7 additionally, as terminal nodes are anchored in UCCA, we remove those whose anchoring is impossible. The linearization and delinearization scripts for this schema are released along with the rest of our code. 

 Sequence-to-sequence Modelling We employ neural seq2seq models based upon the Transformer architecture  (Vaswani et al., 2017) . This architecture is essentially composed of two building blocks, namely, a Transformer encoder and a Transformer decoder. The encoder is a stack of N identical layers, each made up of two sublayers: the first is a multi-head self-attention mechanism, while the second is a position-wise fully connected feed-forward network. The decoder follows a similar architecture, presenting, however, an additional sub-layer that performs multi-head attention over the output of the encoder. Within this work, we use two different kinds of Transformer architecture, Cross and mBART . Cross is a randomly initialized Transformer closely following the architecture depicted by  Vaswani et al. (2017) , except for a significant difference: we leverage a factorized embedding parameterization  (Lan et al., 2020) , that is, we decompose the large vocabulary embedding matrix into two smaller matrices. While the first of these represents the actual embedding matrix and projects one-hot vectors into an embedding space whose dimension is lower than the Transformer hidden size, the second one takes care of projecting these intermediate representations towards the actual Transformer hidden space. This technique significantly reduces the number of parameters and, within the context of our experiments, did not show any significant performance penalty. On the other hand, mBART is a multilingual Transformer pre-trained in many languages over large-scale monolingual corpora. As AMR and UCCA are naturally not included among the supported languages in the vocabulary, we apply an architectural change to mBART and increase its vocabulary with two new language ids. More specifically, we augment its embedding matrix by adding two additional vectors, which we randomly initialize as in  Tang et al. (2021) . 

 Multilingual Framework In order to empower our models to support translation from and towards multiple languages, we employ a data-driven approach: we replace the start token of the decoder with a special tag specifying the language the encoder representations should be unrolled towards. Figure  2  shows an example of this schema. It is worth pointing out that, while for Cross we do not feed the source language to the encoder, when using the mBART model we follow its input format and do provide it. Once data have been tagged according to this schema, we train a many-to-many translation model on both the semantic parsing and Englishcentric parallel corpora. 8 Considering that our focus is on semantic parsing, we perform oversampling on the AMR and UCCA datasets. Furthermore, when considering the parallel corpora from MT, we flip the training direction with probability 0.5, hence allowing our model to see at training time both the X ? EN and EN ? X training directions; we argue that this stochastic flip benefits our models in multiple ways: ? As EN ? X shares the source language with both EN ? AMR and EN ? UCCA, this results in positive transfer; ? As AMR, UCCA and EN are significantly related, X ? EN also results in positive transfer (similar target language); ? Finally, X ? EN allows our model to navigate unseen translation paths (i.e. zero-shot) such as IT ? AMR and thus tackle tasks like crosslingual AMR parsing. 

 Experimental Setup We assess the effectiveness of our proposed approach by evaluating its performance on all translation paths where the target language is a graph formalism, the only exception being X ? UCCA, with X any language but English. This choice is motivated by the fact that, differently from AMR where cross-lingual AMR aims to produce Englishbased meaning representations  (Damonte and Cohen, 2018) , UCCA builds graphs on top of its tokens which are, consequently, inherently in the same language as the input text ; we leave exploring this direction to future work. 

 Models We choose to use both Cross, a randomly initialized Transformer, and mBART, a multilingual pretrained Transformer, to better grasp the effects of this joint multilingual framework in different regimes. In particular, we explore the following configurations: ? models trained only on a single semantic parsing task (AMR or UCCA parsing) and without considering any parallel data, denoted by Cross st and mBART st ; ? models trained on both semantic parsing tasks and the MT data, denoted by Cross mt and mBART mt . Furthermore, so as to explore whether the training schedules we use result in underfitting for AMR and UCCA, we also consider Cross f t mt and mBART f t mt , that is, Cross mt and mBART mt fine-tuned with a training schedule biased towards the semantic parsing formalism that is being considered. 9 

 Datasets and Preprocessing AMR For AMR parsing, we use AMR-2.0 (LDC2017T10) and its recently released expansion, AMR-3.0 (LDC2020T02), amounting, respectively, to 39 260 and 59 255 manually-created sentence-graph pairs. Cross-Lingual AMR We use Abstract Meaning Representation 2.0 -Four Translations  (Damonte and Cohen, 2020)  to investigate the performance of SGL on cross-lingual AMR parsing. This corpus contains translations of the sentences in the test set of AMR-2.0 in Chinese (ZH), German (DE), Italian (IT) and Spanish (ES). UCCA We replicate the setting of the CoNLL 2019 Shared Task  (Oepen et al., 2019) . We train our models using the freely available 10 UCCA portion of the training data; this corpus amounts to 6 572 sentence-graph pairs, drawn from the English Web Treebank (2012T13) and English Wikipedia articles on celebrities. As no official development set was included in the data release, following Hershcovich and Arviv (2019), we reserve 500 instances and use them as the validation set. To the best of our knowledge, the full evaluation data have not been released yet and, therefore, we compare with state-of-the-art alternatives and report results only on The Little Prince, a released subset consisting of 100 manually-tagged sentences sampled from the homonymous novel. Parallel Data We use English-centric parallel corpora in four languages, namely, Chinese, German, Italian and Spanish; we employ Mul-tiUN  (Tiedemann, 2012)  for Chinese and Spanish, ParaCrawl  (Espl? et al., 2019)  for German, and Europarl  (Tiedemann, 2012)  for Italian. We perform a mild filtering over all the available parallel sentences and then take the first 5M out of these. 11 Preprocessing We do not perform any preprocessing or tokenization, except for the graph linearizations explained in ?3.1 and Chinese simplification. 12 Instead, we directly apply subword tokenization with a Unigram Model  (Kudo, 2018) . When working with Cross in a single-task setting on AMR or UCCA, we follow Ge et al. (  2019 ) and use a vocabulary size of 20k subwords; instead, when working in the multilingual setting, we increase this value to 50k so as to better accommodate the increased amount of languages. Conversely, when using mBART, we always use the original vocabulary consisting of 250k subwords. 

 Evaluation We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 , a metric that computes the overlap between two graphs. Furthermore, in order to have a better picture of the systems' performances, we also re-port the fine-grained scores as computed by the evaluation toolkit 14 of  Damonte et al. (2017) . For UCCA parsing, we employ the official evaluation metric 15 of the shared task, conceptually similar to the Smatch score. 

 Results We now report the results SGL achieves focusing on the following translation paths: i) EN ? AMR ( ?5.1); ii) X ? AMR, with X any language among Chinese, German, Italian and Spanish ( ?5.2); iii) EN ? UCCA ( ?5.3). 

 AMR Parsing We report the Smatch and fine-grained scores that SGL and its current state-of-the-art alternatives attain on AMR-2.0 in   Xu et al. (2020a) . Considering the similarity between the two approaches, this difference is likely caused by the increased number of tasks our model is asked to handle. Once we replace Cross with mBART, all performances rise significantly. In particular, even mBART st , a single-task variant with no additional data, outperforms all its alternatives except for SPRING and SPRING bart  (Bevilacqua et al., 2021) , highlighting the potential of fully pre-trained Transformer language models for translation-based approaches. mBART mt and mBART f t mt push performances further up, showing that the MT data are beneficial even in this pretrained setting and that the multi-task training set, which enables a single shared model to scale across formalisms and languages, is not detrimental to English AMR parsing. However, arguably more interesting is the comparison between the performances of mBART models and SPRING, which, in contrast, builds upon the English-only BART . In particular, as SPRING bart outperforms even mBART f t mt , this finding suggests that, as expected, BART is more suitable than mBART when dealing with English AMR. However, as we show in ?5.2, our choice is beneficial for cross-lingual AMR parsing and results in an interesting trade-off. Finally, we also evaluate SGL on AMR-3.0 and report the results of Cross f t mt , mBART st and mBART f t mt when trained on this dataset (Figure 1 bottom). Overall, we witness a similar trend compared to AMR-2.0. 

 Cross-lingual AMR Parsing We now show the performances of SGL on crosslingual AMR parsing in terms of Smatch score over Chinese (ZH), German (DE), Italian (IT) and Spanish (ES). For comparison, we report the results of the systems proposed by  Damonte and Cohen (2018, AMREAGER) ,  Blloshmi et al. (2020, XL-AMR)  and  Sheth et al. (2021) ; along with their best systems, we also show the strongest MT baseline reported in Damonte and Cohen (2018, AMREAGER M T ) and the zero-shot configuration explored in  Blloshmi et al. (2020  falling short only when compared to the recent work of  Sheth et al. (2021) ; in particular, it surpasses the strong AMREAGER M T baseline. The most interesting aspect of this result is that Cross f t mt attains these performances without ever seeing at training time any X ? AMR translation path; this is in marked contrast with all previous literature and with the systems we report in Table  2 . This finding clearly highlights the effectiveness of transfer learning and, by extension, of our proposed framework in this setting. Secondly, the performances mBART st achieve are astounding under multiple perspectives. First, to the best of our knowledge, it is the first reported result of AMR systems achieving competitive performances on cross-lingual AMR parsing in a fully zero-shot configuration: mBART st is fine-tuned solely on EN ? AMR and then applied directly to X ? AMR translation; especially when compared to XL-AMR ? , the only similar approach we are aware of, the gap is significant. Second, among the languages we consider, the case of Chinese is especially interesting as it appears to require constrained decoding in order to work properly: in particular, we restrict the model to generate only subwords whose characters belong to the English alphabet.  16  If we were to perform ZH ? AMR parsing with no additional decoding machinery, as for the other languages, performances would be significantly lower, with mBART st attaining only 31.9. This performance drop is caused by the model leaving some nodes of the graph untranslated, i.e. named entities left written in Chinese (? rather than Obama), which disrupts the auto-regressive nature of the decoding procedure and, besides, eventually results in a penalized Smatch score. Finally, despite the larger amount of pre-training mBART has been exposed to, its bigger capacity and better Smatch score on English, mBART st still falls short when compared to Cross f t mt , highlighting the benefits of seeing related translation directions at training time. , XL-AMR ? ). mBART mt pushes the bar further up, with performances on German, Spanish and Italian that are now only roughly 10 points behind their English counterparts. As mBART mt significantly outperforms mBART st , this result shows that, despite the massive pretraining, parallel data are still beneficial for cross-lingual AMR. Moreover, differently from English AMR, mBART f t mt does not yield an improvement and, in fact, performances slightly drop on average. While the scores mBART mt attains are already unprecedented, it is natural to wonder whether annotation projection (AP) might yield a further beneficial effect. To this end, similarly to  Blloshmi et al. (2020) , we translate the input sentences of AMR-2.0 into the four languages under consideration 17 and build a training set for each language by pairing the translated sentence with the original AMR graph. We further fine-tune mBART f t mt , including also these new datasets among the training data. This model, which we denote by mBART f t mt + AP, surpasses further mBART mt , clearly underlining the beneficial effect of this technique. Finally, following  Sheth et al. (2021) , we also report the results of SGL when evaluated on the machine-translated test set; 18 similarly to their findings, we observe that, as the mismatch between the training and test set is reduced, our parser performs better in this setting than on the human-translated one. 

 UCCA Parsing We report in Table  3  the performance of SGL on UCCA parsing. We compare our approach with the original multi-task baseline  (Oepen et al., 2019)  and 3 transition-based parsers that participated; in particular, we report the score of  Che et al. (2019) , the system that ranked first in both all-framework and UCCA parsing. First of all, we note the result of Cross st ; while its performance is far below the score  Che et al. (2019)  achieve, it still outperforms the original proposed baseline by more than 10 points. Furthermore, to the best of our knowledge, apart from the recent works proposed in the latest shared task of  Oepen et al. (2020) , this is the first reported result of translation-based approaches on UCCA parsing. Once plugged into our multilingual framework, UCCA benefits from transfer learning to an even greater extent than AMR parsing, likely owing to the smaller amount of training data: Cross mt and especially Cross f t mt significantly reduce the gap between SGL  and Che et al. (2019) , with Cross f t mt outperforming the multi-task transition-based approach of  Hershcovich and Arviv (2019) . The usage of mBART pushes up the system's performance further, with mBART st achieving 77.0 and mBART mt 79.9; differently from AMR, mBART f t mt suffers from overfitting and its performance is actually lower than that of mBART mt . Even though these scores are lower than those of  Che et al. (2019) , we argue that such results are still incredibly promising as they demonstrate the effectiveness of SGL in tackling cross-framework semantic parsing. Indeed, these results show that multilingual translation-based approaches allow for a single model to jointly accommodate different formalisms, each potentially linearized according to a different linearization scheme. Furthermore, we believe there is a significant margin for improvement on both the linearization used and the model; for instance, we did not consider node ids such as <root_0> as special tokens, but instead had the unigram tokenizer handle them as if they were normal words. Finally, we wish to point out that direct comparability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs; in particular, we limit ourselves to two frameworks only and leverage resources (the parallel corpora from MT) whose usage was forbidden to the shared task participants.  19  Nevertheless, we believe that their results are needed here to better contextualize the performances SGL obtains. 6 Analysis: is MT the one helping? Although the performances of Cross mt are remarkable, mBART st achieves competitive results on cross-lingual parsing and fares even better on English parsing. While mBART st admittedly features a massive amount of pre-training, this pre-training is over monolingual corpora and, as such, the model has never seen any parallel data. We therefore wonder to what extent the parallel nature of the additional MT data we use is crucial for Cross mt . To answer this question, we treat our MT corpora as monolingual data by sampling, for each instance, either the source or target side and converting the translation task into a denoising one: given an instance EN ? IT, we sample either EN or IT with equal probability, denoting the result by Z, and convert the instance into g(Z) ? Z, where g is a noising function that corrupts the input text. We follow  and choose a noising function that masks 35% of the words by random sampling a span length from a Poisson distribution (? = 3.5). Applying this noisification scheme to the MT data, we train a model identical to Cross mt and denote it by Cross N mt . As shown in Table  4 , in this data regime, the parallel nature is crucial both for English and, especially, for cross-lingual parsing. While Cross N mt does yield a significant boost over Cross st , when  19  Allowed resources are specified at: http://svn. nlpl.eu/mrp/2019/public/resources.txt compared instead to Cross mt , it is 4 points behind on UCCA parsing and only half way on AMR parsing. The difference is even more marked in the cross-lingual setting, where Cross N mt simply does not work. 

 Conclusion In this work, we presented SGL, a novel framing of semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation. That is to say, given a sentence and the desired output formalism, a many-to-many neural model has to learn to translate from the input sentence to the corresponding linearized graph. Within this framework, we show that we can address the paucity of annotated data that afflicts semantic parsing effectively by performing the learning procedure jointly on large parallel corpora coming from MT, and leveraging the power of pre-trained Transformer language models. Using AMR and UCCA as our cases in point, we report competitive performances on their parsing, especially once pre-trained models enter the picture. Furthermore, we find that the benefit MT data provide goes beyond merely improving Englishcentric parsing, yielding astonishing performances on cross-lingual AMR parsing as well, and allowing SGL to outperform all existing approaches by a large margin. Most interestingly, differently from all previous literature, this result is attained without ever explicitly seeing at training time the translation paths the model is tested upon. Once we use annotation projection and include these data as well, performances rise even further, attaining unprecedented results. As future work, thanks to the nimbleness with which we can add new languages, we plan to assess the scalability of this framework as more formalisms are taken into account. 
