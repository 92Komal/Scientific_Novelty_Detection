title
Neural Machine Translation for Tamil-Telugu Pair

abstract
The neural machine translation approach has gained popularity in machine translation because of its context analysing ability and its handling of long-term dependency issues. We have participated in the WMT21 shared task of similar language translation on a Tamil-Telugu pair with the team name: CNLP-NITS. In this task, we utilized monolingual data via pretrain word embeddings in transformer model based neural machine translation to tackle the limitation of parallel corpus. Our model has achieved a bilingual evaluation understudy (BLEU) score of 4.05, rank-based intuitive bilingual evaluation score (RIBES) score of 24.80 and translation edit rate (TER) score of 97.24 for both Tamil-to-Telugu and Telugu-to-Tamil translations respectively.

Introduction Machine translation (MT) works as an interface that handles language ambiguity concerns via automatic translation between two different languages. Neural machine translation (NMT) attains state-ofthe-art results for both high and low-resource language pairs translation  (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Laskar et al., 2020a) . The NMT utilizes an artificial neural network to predicts the likelihood of a sequence of words. But NMT requires a sizeable parallel corpus to get effective MT output, challenging for low-resource pair translation. In this WMT21 shared task, we have participated on a similar language pair translation task of Tamil-Telugu pair using NMT. We aim to utilize similarity features among such a similar language pair and monolingual data to overcome the less availability of parallel corpus. The transformer model  (Vaswani et al., 2017)  based NMT is considered in this work, since it outperforms RNN based NMT. Moreover, NMT performance can be enhanced utilizing monolingual data  (Weng et al., 2019; Wu et al., 2019; Ramachandran et al., 2017; Vari? and Bojar, 2019; Qi et al., 2018) . To evaluate the performance of our system's output, WMT21 organizer used standard evaluation metrics, namely, BLEU  (Papineni et al., 2002) , RIBES  (Isozaki et al., 2010)  and TER  (Snover et al., 2006)  which are reported in Section 4. 

 Related Work There are limited works on the Tamil-Telugu pair  (Chakravarthi et al., 2021) . The literature survey found similar works on Indian similar language pairs, such as Hindi-Nepali  and Hindi-Marathi  (Laskar et al., 2020b)  at WMT19 and WMT20. Both  (Laskar et al., 2020b  used transformer model based NMT. Moreover,  Ramachandran et al. (2017) ;  Vari? and Bojar (2019) ;  Qi et al. (2018)  pre-trained methods are incorporated in NMT to utilize advantage of monolingual data for low-resource pairs translation. In this work, GloVe  (Pennington et al., 2014)  pretrained word embeddings are used in transformer model  (Vaswani et al., 2017)  based NMT for both Tamil-to-Telugu and Telugu-to-Tamil translation. 

 System Description Our system mainly consists of the following parts: data preprossessing, model training and testing. These have been described in the following subsections. The dataset description is presented in Section 3.1. For our system, we have used the OpenNMT-py toolkit  (Klein et al., 2017)    

 Data Preprocessing The OpenNMT-py toolkit is used to preprocess the parallel data and then generates a vocabulary of size: 50002 for the source-target sentences by tokenizing and indexing in a dictionary. It was done in both ways independently, considering Tamil as source and Telugu as target and then with Telugu as source and Tamil as the target to train models for both ways for translation in either direction. We have used GloVe  (Pennington et al., 2014)  to pretrain on the monolingual corpora to obtain word vectors. These word vectors are specifically used in the form of word embeddings in the transformer model during the training process. 

 System Training After the data preprocessing, the pre-trained embeddings and parallel dataset are used for training our model for both Tamil-to-Telugu and Telugu-to-Tamil. We have adopted a transformer model to implement both of the trained models separately. The transformer model consists of a self-attention mechanism, encoder, and decoder layers. The selfattention comes into play, where the relevancy of one word to other words of the sentence is represented as an attention vector that contains the context between words in that sentence. Multiple such attention vectors are calculated, and the weighted average is taken so that the interactions with other words are captured properly rather than their value. More specifically, the embeddings are converted into three spaces: query, key, and value. The dot product of its query vector and all the key vectors are calculated for every embedding. Since the hidden state of the previous embedding is not needed in calculating the current embedding's hidden state, the self-attention can be done in parallel for all embeddings. Thus, it can be run in parallel for all embeddings simultaneously. This speeds up the training and translation process a lot. Now, the target sentences are passed to the decoder layers similarly to the encoders and then passed to the self-attention block. The difference is that in attention layers, the next word of the target sentence is masked so that the word will be predicted using previous results for learning. It is called a masked multi-head attention block. The attention vectors thus produced and the outputs from the encoder layers are then passed to another attention block called encoderdecoder attention block. The attention vectors for every word in the sentences are the output. Then we pass it through a feed-forward network for making output acceptable for further layers. Our transformer model consists of six layers for both encoders and decoders and eight attention heads. We used adam optimizer with a learning rate 0.001 and a drop-out of 0.1 for normalization. The rest of the parameters were selected as the default configuration of the toolkit. This configuration is used for both models, the Tamil-to-Telugu and vice versa. 

 System Testing The obtained trained models are used in system testing, where the test data is used to obtain the predicted translation for both Tamil-to-Telugu and vice-versa independently.  

 Result Our system's outputs were submitted to the organizer for evaluation. Consequently, the results of the shared task on "Similar Language Translation" were announced separately for Tamil-to-Telugu 3 and Telugu-to-Tamil 4 . The ranking of the systems is mainly based on BLEU score, while the RIBES and TER scores are also given. Our team name is CNLP-NITS. For the Tamil-to-Telugu translation system, we achieved 4th rank with a BLEU score of 4.05 and 6th rank with a BLEU score of 4.05 for the Telugu-to-Tamil translation. The results of our system are reported in the Table  2 . The system performance is identical for both translation directions. We need to perform a human evaluation in future work to identify the test set and predicted output are identical or not. 

 Conclusion and Future Work This work reports our system description along with results, which we have participated in the WMT19 shared task of similar language pair: Tamil-Telugu. Both direction of translations, transformer model based NMT is used and utilized monolingual data through pre-trained word embeddings. We will investigate multilingual NMT approach in future to improve such low-resource translation quality. for the data preprocessing, training and testing. Corpus Type Sentences Tokens Tamil Telugu Source Train 40147 588919 625308 Parallel Validation 1261 25443 25844 WMT21 Organizer Test 1735 33911 35895 Monolingual Tamil Telugu 31542481 47877462 488507451 574131374 IndicNLP Table 1: Dataset Statistics 3.1 Dataset The parallel corpus for Tamil-Telugu pair is pro- vided by the WMT21 organizer 1 . It consists of 40147, 1261, 1735 sentence pairs for train, vali- dation and test set. Apart from this, we also col- lected Monolingual data from the IndicNLP 2 cor- pus. It consists of 31542481 Tamil sentences and 47877462 Telugu sentences. This monolingual corpus is specifically used for deriving pretrained embeddings to use in the model. The dataset statis- tics are described in the table 1. 
