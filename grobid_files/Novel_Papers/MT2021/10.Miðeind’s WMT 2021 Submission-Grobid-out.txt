title
Mi√∞eind's WMT 2021 submission

abstract
We present Mi?eind's submission for the English?Icelandic and Icelandic?English subsets of the 2021 WMT news translation task. Transformer-base models are trained for translation on parallel data to generate backtranslations iteratively. A pretrained mBART-25 model is then adapted for translation using parallel data as well as the last backtranslation iteration. This adapted pretrained model is then used to re-generate backtranslations, and the training of the adapted model is continued.

Introduction Our work on machine translation for Icelandic has been going on for a couple of years as a part of the government sponsored Icelandic Language Technology Programme  (Nikul?sd?ttir et al., 2020) . By building on state-of-the-art solutions we have developed an open and effective translation system between Icelandic and English. To achieve this, we collect parallel Icelandic and English texts which are filtered for good quality alignments. We also collect monolingual text for backtranslations. We follow tried and tested methods in neural machine translation using iterative backtranslation  (Edunov et al., 2018)  and adapt the multilingual denoising autoencoder model mBART-25  (Liu et al., 2020)  for translation. 

 Datasets We used several parallel and monolingual datasets, both publicly available and created in-house. 

 Parallel data The parallel data used are ParIce  (Steingr?msson et al., 2020)  and the JW300 corpus  (Agi? and Vuli?, 2019) . In addition we used a parallel student theses and dissertation abstracts corpus, IPAC, generated in-house and sourced from the Skemman reposi-tory 1 as described in  (S?monarson and Snaebjarnarson, 2021) . A breakdown of the data is shown in Table  1 .  Following  (Pinnis, 2018)  we apply simple heuristic filters to the parallel data, mainly for capturing OCR and PDF errors, and correcting or removing character encoding errors after deduplication. Filters include but are not limited to: empty sequence removal, length cut-offs, character whitelists, mismatch in case and symbols between languages, editdistances between source and target, normalizing of punctuation, and ad-hoc regular expressions for Icelandic specific OCR/PDF errors. For a more in-depth description see  (J?nsson et al., 2020) . 

 Corpus Other potential parallel datasets are ParaCrawl  (Ba?n et al., 2020)  and CCMatrix  (Schwenk et al., 2021) . Manual review of a couple of hundred randomly chosen lines from ParaCrawl revealed that the data quality is quite low for Icelandic, many lines are machine translated or badly aligned. We therefore did not include ParaCrawl. CCMatrix did not exist when the project started and we have not taken the time to review and integrate it although a quick inspection indicates that the quality is higher than in ParaCrawl. 

 Data used for backtranslation We collected and translated monolingual data for backtranslations, made available in  (S?monarson et al., 2020) , mostly building on the work in  (Edunov et al., 2018) . The English sentences (44.7m) are retrieved from the Wikipedia, Newscrawl and Europarl corpora. The Icelandic sentences (31.3m) are sourced from the Icelandic Gigaword Corpus  (Steingr?msson et al., 2018)   3 Training of small transformer models Our earlier models were trained using the transformer-base configuration described in  (Vaswani et al., 2017)  as implemented in Google's Tensor2Tensor (TensorFlow-based)  (Vaswani et al., 2018)  library. For later models we switched to Facebook's Fairseq  (Ott et al., 2019)  library. An improved translation task was implemented in Fairseq to include BPE dropout; it is available in the greynirseq 2 library. The transformer-base models were trained iteratively and used to generate new backtranslations. We stopped when each language direction had been trained on backtranslations that were produced by a model that had itself seen backtranslations at training time. We compared tagged and untagged backtranslations, sampling versus beam search and different mixing ratios (upsampling rate) between parallel and backtranslated data. Using tagged backtranslations as opposed to no tag showed an improvement from 16.5 to 17.5 BLEU 3 after the first iteration over the IPAC development set, while using no backtranslations gave 15.0, so we proceeded to use tagged translations. We use the IPAC test set to measure BLEU since it was available, has a large range of topics (although maybe not a large range of style) and is very unlikely to be accidentally included in the training data. The IPAC data is out of distribution with the rest of the training data but we do not consider that to be a problem since our goal is a general purpose model. The WMT dev set did not exist at the time. 

 Model We used a joint BPE vocabulary of size 16k and shared input-output embedding matrices. We pretokenized the input using tokenizer 4 for the Icelandic side and spaCy  (Honnibal et al., 2020)  for the English side. A beam width of 4 was used for beam search during backtranslation. Each training iteration took approximately one week on a single GTX 1080 graphics card. We were pleasantly surprised with how far we got with only this modest hardware. 

 Translation mixing ratio selection and beam noise We assessed the impact of the ratio of synthetic backtranslation data to authentic parallel data on translation performance. Best results were obtained with a 1:2 ratio of authentic to synthetic data, using IPAC (held out from training) for evaluation. For noising the backtranslation beam outputs, we follow  (Edunov et al., 2018)  and used within-k permutation of whole words (with k=3), whole-word masking, and word dropout. Using sampling and noised beam outputs yielded comparable results. 

 Adapting mBART-25 for translation The mBART-25  (Liu et al., 2020 ) (610M parameters) language model is far larger than the Transformer-base model (110M parameters). It was pretrained on 25 languages, including English and Swedish, but not Icelandic. We adapt it for translation from Icelandic to English and vice versa, using the same human-derived parallel translation data as for the transformer-base model along with the synthetic backtranslated corpus in a ratio of 1:2. We do not use any pre-tokenization and inherit the BPE sententencepiece vocabulary from mBART-25 (of size 250k) with the addition of an Icelandic language marker that was randomly initialized. We use the same hyperparameters as in  (Liu et al., 2020)  and the implementation from Fairseq  (Ott et al., 2019) . The models are trained until their performance on the development sets plateaus. The initial learning rate was set to 3e-4. Sixteen 32GB nVidia V100 GPUs connected with Infiniband were used for training. The effective batch size was around 10k sequences and the training took around 4 days of wall clock time per model. Subsequently, these trained models were used to generate improved backtranslations. We then continued training the first iteration of our models with the new backtranslated data for another 30,000 steps for the Icelandic-English direction, and 36,000 steps for the English-Icelandic direction. The same training configurations were maintained as for the earlier runs. 

 Dir. Steps The benefit of continuing training of the mBARTderived models ranges from 0.6 to 3.1 BLEU as shown in Table  4 . BLEU performance is shown for both the newstest2021 development set as well as our cleaned-up dataset with sentence pairs from the EEA regulation corpus. Note that we do not finetune prior to evaluation nor do we perform checkpoint averaging. 

 Conclusion We have shown how a small team with modest resources can adapt state-of-the-art methods to a medium resource language and achieve competitive results on machine translation between English and Icelandic. The trained models are available for translation at https://velthyding.is and will be made available at the open CLARIN-IS 5 repository. While a formal human comparison of the current models to the popular Google Translate service has not been performed, hundreds of monthly active users choose our solutions for translation between Icelandic and English. 
