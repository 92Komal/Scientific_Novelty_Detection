title
Learning to Rewrite for Non-Autoregressive Neural Machine Translation

abstract
Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named REWRITENAT to explicitly learn to rewrite the erroneous translation pieces. Specifically, REWRITENAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consistency of data distribution with iterative decoding, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that REWRITE-NAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, REWRITENAT can obtain competitive results with autoregressive translation on WMT14 En?De, En?Fr and WMT16 Ro?En translation benchmarks 1 .

Introduction State-of-the-art neural machine translation (NMT) systems use autoregressive decoding where the decoder generates a target sentence word by word, and the generation of the latter words depends on previously generated ones  (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) . Instead of sequential decoding as in the autoregressive translation (AT), non-autoregressive neural machine translation (NAT)  (Gu et al., 2018; Guo et al., 2019; Ma et al., 2019; , Figure  1 : Illustration of the difference in masking words between (a) conventional masked LM-based NAT  (Ghazvininejad et al., 2019)  and (b) our proposed REWRITENAT. Instead of using inefficient heuristic rules which perhaps mask correct words in some case (e.g., y 1 y 4 ), REWRITENAT utilizes an additional locator module to learn to explicitly distinguish erroneous translation pieces (e.g., ?2 ?3 ), annotated as special symbol (i.e.,  [MASK] ). 2019;  Ghazvininejad et al., 2020a; Ding et al., 2021a,b)  generates the whole target sentence simultaneously. To enable parallel decoding, NAT imposes a conditional independence assumption among words in target sentences, yielding significantly faster inference speed than AT. However, since intrinsic dependencies within target sentence are omitted, NAT suffers from severe inconsistency problem , leading to inferior translation quality, especially when capturing highly multimodal distribution of target translations  (Gu et al., 2018) . Towards tackling above fundamental problem, iterative decoding  (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019; Guo et al., 2020b; Ghazvininejad et al., 2020b)  is proposed to improve NAT by repeatedly refining previously generated translation. Instead of enforcing NAT to generate accurate translation by one-pass decoding, these approaches are expected to revise incorrect translation pieces through several refinements  (Xia et al., 2017; Geng et al., 2018) . With the introduction of iterative decoding, NAT further boosts translation quality, bridging performance gap between NAT and AT models. However, existing iterative NAT models expose the weakness in distinguishing the erroneous words. The dominant approach to identify the mistakes is mask-predict algorithm  (Ghazvininejad et al., 2019; Guo et al., 2020b) , which employs inefficient heuristic rules to roughly choose the least confident words as the erroneous. In some case, mask-predict may mistake to rewrite correct words while maintain erroneous ones, acting as noises to make a negative impact on subsequent iterations. Without explicitly classifying translated words into wrong or right, the translations decode in constant number of iterations, hindering the further improvement of inference speed. Besides, decoder inputs of prevailing iterative NAT models  (Kasai et al., 2020; Guo et al., 2020b)  almost come from the ground-truth during training, while target sentences generated at different refinement steps are taken as decoder inputs in inference, creating a discrepancy that can hurt performance. In this paper, we propose an architecture named REWRITENAT, which explicitly learns to rewrite erroneous translation pieces. Specifically, we introduce a locator module to locate incorrect words within previously generated translation. The located words will be masked out and revised by the revisor module in subsequent refinement. We frame learning to rewrite, comprised of two steps: locate and revise, as an iterative training procedure, where locate and revise operations are supervised by comparing the generated translation with the groundtruth. Towards keeping the consistency with iterative decoding, iterative training is utilized to further improve the training procedure. Experimental results on several typical machine translation datasets demonstrate that REWRITENAT achieves consistent improvement over iterative decoding baselines, but with substantially less decoding time. Further analysis show that REWRITENAT prefers to generate the "easy" words at the early decoding iteration, and leaves the more complicated choice later. 

 Background 

 Autoregressive Machine Translation Autoregressive neural machine translation (AT) draws much attention due to its convenience and effectiveness on various machine translation tasks  (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) . Given a source sentence X = {x 1 , ? ? ? , x T } and target sentence Y = {y 1 , ? ? ? , y T }, AT decomposes translation distribution p AT (X|Y ) into a chain of conditional probabilities in a unidirectional manner: p AT (X|Y ) = T t=1 p(y t |y <t , X) (1) where y <t represents the set of generated tokens before time-step t. Besides, T and T is the length of the source and the target sequence, respectively. Sine AT generates translation in an autoregressive manner, it suffers from low inference speed. 

 Non-Autoregressive Machine Translation Towards alleviating this issue, NAT  (Gu et al., 2018)  removes sequential dependencies within target sentence, and generates target words, simultaneously. NAT models conditional probabilities p NAT (Y |X) of translation from X to Y as a product of conditionally independent per-step distributions: p NAT (Y |X) = p(T |X) T t=1 p(y t |X) (2) Since each target word y t only depends on the source sentence X, the target distributions p(y t |X) can be computed in parallel at inference time. Nevertheless, this desirable property of parallel decoding comes at the cost that the translation quality is largely sacrificed. Since the intrinsic dependencies within target sentence (y t depends y <t ) are abandoned from decoder input, NAT shows its weakness in exploiting inherent sentence structure for prediction. Hence, NAT has to figure out such target-side information by itself, merely conditioned on source-side information. In contrast, AT produces current target word, conditioned on previously generated words, which provides strong target side context information. Consequently, with less and weaker information, NAT suffers from inferior translation quality. 

 Architecture As depicted in Figure  2 , our proposed REWRITE-NAT literally consists of three major components: an encoder, a revisor and a locator. The encoder utilizes transformer encoder, comprisedof N e transformer blocks  (Vaswani et al., 2017) , to convert source sentence into the contextual representations, similar to previous work  (Gu et al., 2018)  Figure  2 : Architecture of our proposed REWRITENAT model, which consists of three major components: an encoder, a revisor and a locator. The encoder is utilized to convert the source sentence into contextual representations. During the decoding, the revisor converts the erroneous words annotated as "[MASK]" into the correct ones, while the incorrect words within previously generated hypothesis are distinguished, by classifying the words into two classes: revise and keep. Given previously located hypothesis, M refinements, each of which utilizes a revisor and a following locator refine the hypothesis, are applied to obtain the final translation. We take an instance from English?German translation as example, where source sentence is "Thank you .". REWRITENAT applies two refinements into the initial hypothesis, merely comprised of "[MASK]". Subsequently, the decoding terminates since the locator categorizes the entire sequence into keep, meaning that any word is not required to be revised. revisor and locator, composing into an decoder, are employed to revise and locate the incorrect words within previously generated translation, respectively. We will elaborate the revisor and locator in the following. 

 Revisor Given altered translation Y r by the locator, the revisor is utilized to convert erroneous pieces into the correct, conditioned on source sentence. Particularly, it's expected to speculate about correct words in positions annotated as "[MASK]", under the context of the remaining translation. Notably, the revisor treats an input merely consisting of "[MASK]" as initial input, meaning that the whole input is required to be revised. Given the hypothesis Y r = {y r 1 , ? ? ? , y r T }, we leverage a stack of transformer blocks  (Vaswani et al., 2017; Gu et al., 2018)  to generate the corresponding representations H r = {H r 1 , ? ? ? , H r T }, with the glimpse at source representations H e : H r = TransformerStack r (Y r , H e ) (3) where TransformerStack r (?) represents the stack of N r transformer blocks with respect to the revisor. Subsequently, the generated representations H r with respect to special symbol "[MASK]" are fed to a classifier ? r to generate the target words as follows: ? r (r t |y r t , X) = softmax(W r h r t + b r ) (4) where W r and b r are trainable parameters, and represent weight matrix and bias vector, respectively. The generated words by ? r are treated as the substitute of the incorrect words annotated as "[MASK]", yield the revised translation Y l = {y l 1 , ? ? ? , y l T } as follows: y l t = r t , if y r t = [MASK] y r t , otherwise (5) where Y l is fed to the locator. 

 Locator Given previously generated translation as input, we employ the locator to distinguish incorrect words within entire sequence, conditioned on source sentence. Using the locator, each word within translation can be categorized into two types: revise (1) and keep (0). According to resulted classification, it is required to alter previous translation into another format, which is then fed to the revisor. In details, the words annotated as "revise" are substituted by special symbol, denoted as "[MASK]", while the remaining hold. Given previously generated translation Y l = {y l 1 , ? ? ? , y l T } to be located, a stack of transformer blocks  (Vaswani et al., 2017; Gu et al., 2018)  are utilized to transform input translation Y l into a sequence of hidden states H l = {h l 1 , ? ? ? , h l T }, conditioned on source contextual representations H e : H l = TransformerStack l (Y l , H e ) (6) where TransformerStack l (?) represents the stack of N l transformer blocks with respect to the locator. Using induced hidden states H l as input, an additional classifier ? l is employed to decide whether previously generated word y l t at step t is required to be revised, and calculated as follows: ? l (l t |y l t , X) = softmax(W l h l t + b l ) (7) where W l and b l are trainable parameters, and represent weight matrix and bias vector, respectively. Using the classifier ? l , input translation Y l can be converted into an annotation sequence L = {l 1 , ? ? ? , l T }. Subsequently, dependent on the annotation L, the translation Y l is altered into Y r = {y r 1 , ? ? ? , y r T } as follows: y r t = [MASK], if l t = revise y l t , otherwise (8) where Y r is treated as input of the revisor. 

 Training and Inference 

 Training Towards maintaining the consistency of data distribution with iterative decoding at inference time, iterative training strategy is utilized to train REWRITENAT to learn the ability to rewriting, as described in Algorithm 1. During training, at m-th refinement including revise and locate operations, we compare previously-generated translations (i.e., ? r m and ? l m ) with ground-truth (i.e., Y ) to distinguish erroneous translation pieces, and construct two types of supervised signals (i.e., q( ? r m ) and z( ? l m )) to instruct the learning of revisor and locator modules, respectively. With the introduction of iterative training with M refinements, training objective L(?) can be formalized as: Generate ? l m using ? r (?| ? r m , X) as Eq. 5 L(?) = M m=1 q( ? r m ) log ? r ? (Y | ? r m , X) revisor objective + log ? l ? (z( ? l m )| ? l m , X) locator objective (9) 7: L r m ? q( ? r m ) log ? r ? (Y | ? r m , X) 8: Generate ? r m using ? l (?| ? l m , X) as Eq. 8 9: L l m ? log ? l ? (z( ? l m )| ? l m , X) 10: end for 11: L ? M m=1 (L r m + L l m ) 12: Update model parameters ? ? ? + ? ? L 13: until convergence where the translations ? r m and ? l m are generated at m-th refinement step depending on output distributions of the revisor ? l (?| ? l m?1 , X) and locator ? r (?| ? r m , X), respectively. During training, generated translation ? r m and ? l m have same length with the ground-truth Y . When calculating revisor objective, we use q( ? r ) as a weight vector to merely concentrate on optimizing at the incorrect words (annotated as [MASK] in ? r ) but omit the losses with respect to correctly-generated ones: q t ( ? r ) = 1, if ? r t = [MASK] 0, otherwise (10) The locator target z( ? l ) is a vector meaning that the positions where translation ? l is different from ground-truth Y should be categorized into revise (1), while the remaining are mapped into keep (0): z t ( ? l ) = 1, if ? l t = Y t 0, otherwise (11) 

 Inference During training REWRITENAT generates the translations with same length as the ground-truth, while in inference we apply REWRITENAT over a sequence of "[MASK]" with a length predicted by length classifier  (Lee et al., 2018)    (2018) , where newsdev2016 and newstest2016 are taken as development and test sets. For WMT17 En?Zh translation, we pre-process the dataset following  Hassan et al. (2018) . We treat newsdev2017 as the development set and newstest2017 as the test set. The datasets are tokenized into subword units using BPE  (Sennrich et al., 2016) . We evaluate performance with BLEU  (Papineni et al., 2002)  for all language pairs, except for En?Zh, where we use SacreBLEU  (Post, 2018)  5 . Distillation Knowledge distillation  (Kim and Rush, 2016; Hinton et al., 2015)  is utilized to train the NAT models due to its effectiveness of alleviating multimodality  (Gu et al., 2018)  using the generated translation by TRANS-FORMER (TRANSFORMER-BIG for WMT14 En?De and En?Fr as well as WMT17 En?Zh, TRANSFORMER-BASE for WMT16 En?Ro) as a substitute for target-side ground-truth  (Ghazvininejad et al., 2019) . Hyperparameters We follow most of the standard hyperparameters for TRANSFORMER-BASE  (Vaswani et al., 2017) : 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We follow the weight initialization schema from BERT  (Devlin et al., 2019) , and sample weights from N (0, 0.02), set biases to zero, and set layer normalization parameters to ? = 0 and ? = 1. For regularization, we use dropout (En?De and En?Ro: 0.3, En?Fr: 0.1, En?Zh: 0.25), 0.01 L 2 weight decay, and smoothed cross validation loss with ? = 0.1. we adopt the Adam optimizer (Kingma and Ba, 2015) using ? 1 = 0.9, ? 2 = 0.98, = 1e ?8 . The learning rate is scheduled using inverse_sqrt with a maximum learning rate 0.0005, and 10,000 warmup steps except for TRANSFORMER which sets warmup steps as 4000. All the models are run on 8 Tesla V100 GPUs for 300,000 updates with an effective batch size of 128,000 tokens apart from En?Fr where we make 500,000 updates to account for the data size. During decoding, we use a beam size of b = 5 for autoregressive decoding, while length beam  (Ghazvininejad et al., 2019)  is applied to obtain the translation with respect to non-autoregressive counterpart. 

 Main Results Table  1  shows the results of REWRITENAT, together with a series of purely NAT baselines and representative iterative NAT approaches   3 : Average number of iterations ("Iters.") and performance ("BLEU") with repsect to REWRITENAT on large-scale WMT17 En?Zh and WMT14 En?Fr datasets. 

 Decoding Speed As shown above, REWRITENAT can obtain substantial improvements than strong iterative NAT baselines while reducing the number of iterations. Here we compare them in terms of speedup with respect to TRANSFORMER, as depicted in Figure  3 . It can be clearly observed that REWRITENAT can obtain same performance but with substantially higher speedup than iterative NAT baselines. When maximum iteration is set as 2 (i.e., T = 2), REWRITE-NAT obtains competitive result to CMLM and TRANSFORMER with b = 1 (i.e., 27.03 vs. 27.05) but with higher speedup (i.e., 7.02?). The performance of REWRITENAT benefits much from the growth of T until T = 4. Particularly, REWRITE-NAT with T = 4 achieves comparable result (27.77 vs. 27.82, 3.86?) with TRANSFORMER with b = 5. Furthermore, REWRITENAT with T = 4 outperforms the strongest SMART (i.e., 27.56 vs. 27.77) but using about half of decoding time. Afterwards, performance gain is relatively subtle but with a slight decrease of speedup due to dynamic halting. 

 Analysis 

 Word Repetitions With decoupling the sequential dependencies among target sentence, NAT shows the serious weakness in modeling highly multimodal distributions  (Gu et al., 2018) , often manifest as word repetitions  in generated translations. Towards evaluating the multi-modality, we follow  Ghazvininejad et al. (2019)  to measure the percentage of consecutive repetitive words as a proxy metric. As shown in Table  4 , the proportion of repetitive words with respect to REWRITENAT is significantly lower than most relevant CMLM baseline, especially when decoding using single iteration (-6.05%). Simultaneously, REWRITE-NAT can achieve substantial performance over CMLM. These results demonstrate the superiority of REWRITENAT over CMLM in alleviating word repetitions. 

 Iters. CMLM REWRITENAT BLEU Reps BLEU Reps T = 1 18.05 16.72% 21.17 10.67% T = 2 22.91 5.40% 27.03 1.95% T = 3 24.99 2.03% 27.54 0.97% T = 4 25.94 1.07% 27.76 0.59% Table  4 : The performance ("BLEU") and percentage of repetitive words ("Reps") when decoding with a different number of iterations on WMT14 En?De test set. Notably, with respect to REWRITENAT, T denotes the max number of iterations taken during decoding. 

 Effect of Weight Sharing Towards evaluating the effectiveness of weight sharing between revisor and locator modules, we conduct some experiments to make the further analysis. As shown in  

 Iterations vs. Length As described above, compared with previous iterative NATs, the number of iterations taken during decoding significantly decreases with respect to our proposed REWRITENAT. Towards exploring the impact of length, we compare the number of required iterations and the length of target sentences, as illustrated in Figure  4 . It's clearly observed that REWRITENAT can properly choose the number of iterations accordingly. In general, as the length of target sentences grows, REWRITENAT also requires more iterations to produce the translation. 

 Analysis on Part-of-Speech Despite proving the effectiveness, we doubt whether the number of iterations has any prefer- ence towards different Part-of-Speechs 6 . For each Part-of-Speech 7 , we calculate average percentage of required iterations to produce the words with respect to different Part-of-Speechs. As shown in Figure  5 , REWRITENAT tends to generate punctuation words (i.e., PUNC) early in decoding. Subsequently, nouns are next easiest to predict. Conditioned on generated nouns, other Part-of-Speechs (e.g., CONJ, ADJ, DET, ADV, PREP), which often act as modifiers, prefers to come out in the generated translation. Finally, the most difficult for REWRITENAT is to generate verbs (i.e., VERB) and particles (i.e., PRT). These observations are consistent with easy-first generation hypothesis: early decoding iterations mostly generate words which are the easiest to predict based on input data  (Emelianenko et al., 2019) . 

 Case Study As illustrated in Figure  6 , we present a translation example to compare REWRITENAT with CMLM. The number of maximum decoding iterations is set as 10. We can observe that REWRITENAT can generate the reasonable translation with 3 decoding iterations and terminate the decoding due to the locator module, automatically. In addition, the erroneous translation pieces (e.g., "are children children") can be accurately distinguished. In contrast, strong CMLM baseline shows their weakness at tackling the incorrect ones. Consequently, 6 STANFORD CORENLP TOOLKIT  (Manning et al., 2014 ) is utilized to annotate translation output with Part-of-Speechs. CMLM generally spend more decoding iterations than REWRITENAT, but achieving inferior performance. These results confirm the effectiveness and efficiency of the proposed REWRITENAT. 7 Related Work  Gu et al. (2018)  first proposed NAT to generate the translation in parallel, boosting the inference speed. Towards mitigating the performance degradation, a series of works were proposed to strengthen the capacity of capturing the dependencies among output words, including adding a lite autoregressive module  (Kaiser et al., 2018; , training with well-designed objectives  (Guo et al., 2019; Libovick? and Helcl, 2018; Shao et al., 2020; Ghazvininejad et al., 2020a; Du et al., 2021) , modeling with latent varibles  (Ma et al., 2019)  and mimicking hidden states of autoregressive teacher . Despite above improvements, decoding inconsistency can still be observed in the translation. Towards eliminating the errors, iterative decoding  (Xia et al., 2017; Geng et al., 2018)  was proposed to employ multiple iterations to polish previously generated translation. As an early alternative,  Lee et al. (2018)  corrected the original non-autoregressive output by passing it multiple times through a denoising autoencoder. Instead of generating in discrete space of sentences, continuous latent variables were utilized to improve iterative refinements . Subsequently,  Ghazvininejad et al. (2019)  introduced mask-predict, which first generate target words non-autoregressively, and then repeat- 

 SOURCE Den Kindern stehen regionale Handwerker von 11 bis 17 Uhr helfend zur Seite . CMLM 1?8 Regional craftsmen are at their children from 11 a.m. to 5 p.m . 9 Regional craftsmen are assist their children from 11 a.m. to 5 p.m . 10 Regional craftsmen are helping the children from 11 a.m. to 5 p.m . REWRITENAT 1 Regional craftsmen are children children children from 11 a.m. to 5 p.m . 2 Regional craftsmen are assist the children from 11 a.m. to 5 p.m . 3 Regional craftsmen will assist the children from 11 a.m. to 5 p.m . Figure  6 : An example from the WMT14 De?En translation that illustrates how REWRITENAT, together with CMLM generate text with iterative decoding. The translation pieces to be revised in next iteration are annotated as strikethrough, and the erroneous ones within the final translation are underlined. Notably, we remove the BPE tokens in the generated translation, leading to the unreasonable words (e.g., cra@@ fts@@ from ? craftsfrom). edly mask out and re-generate the subset of words that model is least confident about  (Ghazvininejad et al., 2020b; Guo et al., 2020b) . However, a serious drawback is that previous iterative NAT approaches exposes fundamental weakness in distinguishing erroneous translation pieces. Precisely, previous iterative NAT models based on mask-predict utilizes heuristic rules to consider the least confident words as the ones to be revised, but it struggles to perfectly make correct classifications simply relying on the probability distribution of the generated translation. Despite LevT  (Gu et al., 2019)  can alleviate the issue to some extent by adopting two basic operations (i.e.,insert and delete), a serious discrepancy in input data distribution between training and decoding exists due to the utilization of iterative strategy into decoding but not training. Towards address above issues, REWRITENAT adopts an additional locator module specialized to distinguish the erroneous translation pieces, and iterative training strategy is utilized to maintain the consistency of data distribution with iterative decoding. 

 Conclusion In this work, we propose an architecture named REWRITENAT, which explicitly learns to rewrite the erroneous translation pieces, and iterative training is utilized to train this architecture. Extensive experimental results demonstrate REWRITENAT can achieve remarkable improvement over previous iterative NAT models, but with significantly less decoding iterations. The further analysis reveals that the generation orders of REWRITENAT measured by the percentage of decoding iterations are consistent with easy-first hypothesis. Algorithm 1 1 Iterative Training to REWRITENAT 1: Input: Parallel training dataset (X , Y), revisor module ? r ? , locator module ? l ? , maximum refinement steps M , learning rate ? 2: repeat 3: Sample sentence pair (X, Y ) from (X , Y) 4: Initialize ? r 1 as a sequence of [MASK] with same length as Y 5:for m ? 1 to M do 6: 
