title
WeChat Neural Machine Translation Systems for WMT21

abstract
This paper introduces WeChat AI's participation in WMT 2021 shared news translation task on English?Chinese, English?Japanese, Japanese?English and English?German. Our systems are based on the Transformer  (Vaswani et al., 2017)  with several novel and effective variants. In our experiments, we employ data filtering, large-scale synthetic data generation (i.e., back-translation, knowledge distillation, forward-translation, iterative in-domain knowledge transfer), advanced finetuning approaches, and boosted Self-BLEU based model ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 casesensitive BLEU scores on English?Chinese, English?Japanese, Japanese?English and English?German, respectively. The BLEU scores of English?Chinese, English?Japanese and Japanese?English are the highest among all submissions, and that of English?German is the highest among all constrained submissions.

Introduction We participate in the WMT 2021 shared news translation task in three language pairs and four language directions, English?Chinese, English?Japanese, and English?German. In this year's translation tasks, we mainly improve the final ensemble model's performance by increasing the diversity of both the model architecture and the synthetic data, as well as optimizing the ensemble searching algorithm. Diversity is a metric we are particularly interested in this year. To quantify the diversity among different models, we compute Self-BLEU  from the translations of the models on the valid set. To be precise, we use the translation of one model as the hypothesis and the translations of other models as references to calculate an aver- * Equal contribution. age BLEU score. A higher Self-BLEU means this model is less diverse. For model architectures  (Vaswani et al., 2017; , we exploit several novel Transformer variants to strengthen model performance and diversity. Besides the Pre-Norm Transformer, the Post-Norm Transformer is also used as one of our baselines this year. We adopt some novel initialization methods  (Huang et al., 2020)  to alleviate the gradient vanishing problem of the Post-Norm Transformer. We combine the Average Attention Transformer (AAN)  and Multi-Head-Attention  (Vaswani et al., 2017)  to derive a series of effective and diverse model variants. Furthermore, Talking-Heads Attention  (Shazeer et al., 2020)  is introduced to the Transformer and shows a significant diversity from all the other variants. For the synthetic data generation, we exploit the large-scale back-translation  (Sennrich et al., 2016a)  method to leverage the target-side monolingual data and the sequence-level knowledge distillation  (Kim and Rush, 2016)  to leverage the source-side of bilingual data. To use the source-side monolingual data, we explore forward-translation by ensemble models to get general domain synthetic data. We also use iterative in-domain knowledge transfer  to generate in-domain data. Furthermore, several data augmentation methods are applied to improve the model robustness, including different token-level noise and dynamic top-p sampling. For training strategies, we mainly focus on scheduled sampling based on decoding steps  (Liu et al., 2021b) , the confidence-aware scheduled sampling  (Mihaylova and Martins, 2019; Duckworth et al., 2019; Liu et al., 2021a) , the target denoising  method and the Graduated Label Smoothing  for in-domain finetuning. For model ensemble, we select high-potential candidate models based on two indicators, namely model performance (BLEU scores on valid set) and model diversity (Self-BLEU scores among all other models). Furthermore, we propose a search algorithm based on the Self-BLEU scores between the candidate models with selected models. We observed that this novel method can achieve the same BLEU score as the brute force search while saving approximately 95% of search time. This paper is structured as follows: Sec. 2 describes our novel model architectures. We present the details of our systems and training strategies in Sec. 3. Experimental settings and results are shown in Sec. 4. We conduct analytical experiments in Sec. 5. Finally, we conclude our work in Sec. 6. 

 Model Architectures In this section, we describe the model architectures used in the four translation directions, including several different variants for the Transformer  (Vaswani et al., 2017)  . 

 Model Configurations Deeper and wider architectures are used this year since they show strong capacity as the number of parameters increases. In our experiments, we use multiple model configurations with 20/25-layer encoders for deeper models and the hidden size is set to 1024 for all models. Compared to our WMT20 models , we also increase the decoder depth from 6 to 8 and 10 as we find that gives a certain improvement, but deeper depths give limited performance gains. For the wider models, we adopt 8/12/15 encoder layers and 1024/2048 for hidden size. The filter sizes of models are set from 8192 to 15000. Note that all the above model configurations are applied to the following variant models. 

 Transformer with Different Layer-Norm The Transformer  (Vaswani et al., 2017)  with Pre-Norm  (Xiong et al., 2020)  is a widely used architecture in machine translation. It is also our baseline model as its performance and training stability is better than the Post-Norm counterpart. Recent studies  Huang et al., 2020)  show that the unstable training problem of Post-Norm Transformer can be mitigated by modifying initialization of the network and the successfully converged Post-Norm models generally outperform Pre-Norm counterparts. We adopt these initialization methods  (Huang et al., 2020)    

 Average Attention Transformer We also use Average Attention Transformer (AAN)  as we used last year to introduce more model diversity. In the Average Attention Transformer, a fast and straightforward average attention is utilized to replace the self-attention module in the decoder with almost no performance loss. The context representation g i for each input embedding is as follows: g i = F F N ( 1 i i k=1 y k ) (1) where y k is the input embedding for step k and i is the current time step. F F N (?) denotes the position-wise feed-forward network proposed by  Vaswani et al. (2017) . In our preliminary experiments, we observe that the Self-BLEU  scores between AAN and Transformer are lower than the scores between the Transformer with different configurations. 

 Weighted Attention Transformer We further explore three weighting strategies to improve the modeling of history information from previous positions in AAN. Compared to the average weight across all positions, we try three methods including decreasing weights with position increasing, learnable weights and exponential weights. In our experiments, We observe exponential weights perform best among all these strategies. The exponential weights context representation g i is calculated as follows: c i = (1 ? ?)y i + ? ? c i?1 (2) g i = F F N (c i ) (3) where ? is a tuned parameter. In our previous experiments, we test different alpha, including 0.3, 0.5, and 0.7, on the valid set and we set the alpha to 0.7 in all subsequent experiments as it slightly outperform the others. In the experiments, Mixed-AAN not only performs better but also shows strong diversity compared to the vanilla Transformer. With four Mixed-AAN models, we reach a better ensemble result than the result with ten models which consist of deeper and wider standard Transformer. We will further analyze the effects of different architectures from performance, diversity, and model ensemble in Sec. 5.1 

 Talking-Heads Attention In Multi-Head Attention, the different attention heads perform separate computations, which are then summed at the end. Talking-Heads Attention  (Shazeer et al., 2020)  is a new variation that inserts two additional learned linear projection weights, W l and W w , to transform the attention-logits and the attention scores respectively, moving information across attention heads. The calculation formula is as follows: Attention(Q, K, V ) = sof tmax( QK T ? d k W l )W w V (4) We adopt this method in both encoders and decoders to improve information interaction between attention heads. This approach shows the most remarkable diversity among all the above variants with only a slight performance loss. 

 System Overview In this section, we describe our system used in the WMT 2021 news shared task. We depicts the overview of our NMT system in Figure  2 , which can be divided into four parts, namely data filtering, large-scale synthetic data generation, in-domain finetuning, and ensemble. The synthetic data generation part further includes the generation of general domain and in-domain data. Next, we proceed to illustrate these four parts. 

 Data Filtering We filter the bilingual training corpus with the following rules for most language pairs: ? Normalize punctuation with Moses scripts except Japanese data. ? Filter out the sentences longer than 100 words or exceed 40 characters in a single word. ? Filter out the duplicated sentence pairs. ? The word ratio between the source and the target words must not exceed 1:4 or 4:1. ? Filter out the sentences where the fast-text result does not match the origin language. ? Filter out the sentences that have invalid Unicode characters. Besides these rules, we filter out sentence pairs in which Chinese sentence has English characters in En-Zh parallel data. The monolingual corpus is also filtered with the n-gram language model trained by the bilingual training data for each language. All the above rules are applied to synthetic parallel data. 

 General Domain Synthetic Data Generation In this section, we describe our techniques for constructing general domain synthetic data. The general domain synthetic data is generated via large-scale back-translation, forward-translation and knowledge distillation to enhance the models' performance for all domains. Then, we exploit the iterative in-domain knowledge transfer  in Sec 3.3, which transfers in-domain knowledge to the vast source-side monolingual corpus, and builds our in-domain synthetic data. In the following sections, we elaborate the above techniques in detail. 

 Large-scale Back-Translation Back-translation is the most commonly used data augmentation technique to incorporate the target side monolingual data into  NMT (Hoang et al., 2018) . Previous work  (Edunov et al., 2018)  has shown that different methods of generating pseudo corpus has a different influence on translation quality. Following these works, we attempt several generating strategies as follows: ? Beam Search: Generate target translation by beam search with beam size 5. ? Sampling Top-K: Select a word randomly from top-K (K is set to 10) words at each decoding step. ? Dynamic Sampling Top-p: Selected a word at each decoding step from the smallest set whose cumulative probability mass exceeds p and the p is dynamically changing from 0.9 to 0.95 during data generation. Note that we also use Tagged Back-Translation  (Caswell et al., 2019)  in En?De and Right-to-Left (R2L) back-translation in En?Ja, as we achieve a better BLEU score after using these methods. 

 Knowledge Distillation Knowledge Distillation (KD) has proven to be a powerful technique for NMT  (Kim and Rush, 2016; Wang et al., 2021)  to transfer knowledge from the teacher model to student models. In particular, we first use the teacher models to generate synthetic corpus in the forward direction (i.e., En?Zh). Then, we use the generated corpus to train our student models. Notably, Right-to-Left (R2L) knowledge distillation is a good complement to the Left-to-Right (L2R) way and can further improve model performance. 

 Forward-Translation Using monolingual data from the source language to further enhance the performance and robustness of the model is also an effective approach. We use the ensemble model to generate high quality forward-translation data and obtain a stable improvement in En?Zh and En?De directions. 

 Iterative In-domain Knowledge Transfer Since in-domain knowledge transfer  delivered a massive performance boost last year, we still use this technique in En?Ja and En?De this year. It is not applied to En?Zh because no significant improvement is observed. We guess the reason is that the in-domain finetuning in the En?Zh direction does not bring a significant improvement compared to the other directions. And in-domain knowledge transfer is aiming at enhancing the effect of finetuning, so this does not have a noticeable effect in the English-Chinese direction. We first use normal finetuning in Sec. 3.5 to equip our models with in-domain knowledge. Then, we ensemble these models to translate the source monolingual data into the target language. We use 4 models with different architectures and training data as our ensemble model. Next, we combine the source language sentences with the generated in-domain target language sentences as pseudoparallel corpus. Afterwards, we retrain our models with both in-domain pseudo-parallel data and general domain synthetic data. 

 Data Augmentation Once the pseudo-data is constructed, we further obtain diverse data by adding different noise. Compared to previous years' WMT competitions, we implement a multi-level static noise approach for our pseudo corpus: ? Token-level: Noise on every single subword after byte pair encoding. ? Word-level: Noise on every single word before byte pair encoding. ? Span-level: Noise on a continuous sequence of tokens before byte pair encoding. The different granularities of noise make the data more diverse. The noise types are random replacement, random deletion and random permutation. We apply the three noise types in a parallel way for each sentence. The probability of enabling each of the three operations is 0.2. Furthermore, an on-the-fly noise approach is applied to the synthetic data. By using on-the-fly noise, the model is trained with different noises in every epoch rather than all the same along this training stage. 

 In-domain Finetuning A domain mismatch exists between the obtained system trained with large-scale general domain data and the target test set. To alleviate this mismatch, we finetune these convergent models on small scale in-domain data, which is widely used for domain adaption  (Luong and Manning, 2015; Li et al., 2019) . We take the previous test sets as in-domain data and extract documents that are originally created in the source language for each translation direction  (Sun et al., 2019) . We also explore several advanced finetuning approaches to strengthen the effects of domain adaption and ease the exposure bias issue, which is more serious under domain shift. Target Denoising . In the training stage, the model never sees its own errors. Thus the model trained with teacher-forcing is prune to accumulated errors in testing  (Ranzato et al., 2016) . To mitigate this training-generation discrepancy, we add noisy perturbations into decoder inputs when finetuning. Thus the model becomes more robust to prediction errors by target denoising. Specifically, the finetuning data generator chooses 30% of sentence pairs to add noise, and keeps the remaining 70% of sentence pairs unchanged. For a chosen pair, we keep the source sentence unchanged, and replace the i-th token of the target sentence with (1) a random token of the current target sentence 15% of the time (2) the unchanged i-th token 85% of the time. Graduated Label-smoothing . Finetuning on a small scale in-domain data can easily lead to the over-fitting phenomenon which is harmful to the model ensemble. It generally appears as the model over confidently outputting similar words. To further preventing overfitting of in-domain finetuning, we apply the Graduated Label-smoothing approach, which assigns a higher smoothing penalty for high-confidence predictions, during in-domain finetuning. Concretely, following the paper's setting, we set the smoothing penalty to 0.3 for tokens with confidence above 0.7, zero for tokens with confidence below 0.3, and 0.1 for the remaining tokens. Confidence-Aware Scheduled Sampling. Vanilla scheduled sampling  simulates the inference scene by randomly replacing golden target input tokens with predicted ones during training. However, its critical schedule strategies are only based on training steps, ignoring the real-time model competence. To address this issue, we propose confidence-aware scheduled sampling  (Liu et al., 2021a) , which quantifies real-time model competence by the confidence of model predictions. At the t-th target token position, we calculate the model confidence conf (t) as follow: conf (t) = P (y t |y <t , X, ?) (5) Next, we design fine-grained schedule strategies based on the model competence. The fine-grained schedule strategy is conducted at all decoding steps simultaneously: y t?1 = y t?1 if conf (t) ? t golden ?t?1 else (6) where t golden is a threshold to measure whether conf (t) is high enough (e.g., 0.9) to sample the predicted token ?t?1 . We further sample more noisy tokens at highconfidence token positions, which prevents scheduled sampling from degenerating into the teacher forcing mode. y t?1 = ? ? ? ? ? y t?1 if conf (t) ? t golden ?t?1 if t golden < conf (t) ? t rand y rand if conf (t) > t rand (7) where t rand is a threshold to measure whether conf (t) is high enough (e.g., 0.95) to sample the random target token ?rand . 

 Scheduled Sampling Based on Decoding Steps. We propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors  (Liu et al., 2021b) . Namely, we gradually increase the selection probability of predicted tokens with the growth of the index of decoded tokens. At the t-th decoding step, the probability of sampling golden tokens g(t) is calculated as follow: Add m index to candidate list C 9: end while 10: return C ? Linear Decay: g(t) = max( , kt + b), where is the minimum value, and k < 0 and b is respectively the slope and offset of the decay. ? Exponential Decay: g(t) = k t , where k < 1 is the radix to adjust the decay. ? Inverse Sigmoid Decay: g(t) = k k+e t k , where e is the mathematical constant, and k ? 1 is a hyperparameter to adjust the decay. Following our preliminary conclusions  (Liu et al., 2021b) , we choose the exponential decay and set k to 0.99 by default. 

 Boosted Self-BLEU based Ensemble (BSBE) After we get numerous finetuned models, we need to search for the best combination for ensemble model. Ordinary random or greedy search is oversimplified to search for a good model combination and enumerate over all combinations of candidate models is inefficient. The Self-BLEU based pruning strategy  we proposed in last year's competition achieve definite improvements over the ordinary ensemble. However, diversity is not the only feature we need to consider but the performance in the valid set is also an important metric. Therefore, we combine Self-BLEU and valid set BLEU together to derive a Boosted Self-BLEU-based Ensemble (BSBE) algorithm. Then, we apply a greedy search strategy in the top N ranked models to find the best ensemble models. See algorithm 1 for the pseudo-code. The algorithm takes as input a list of n strong single models M, BLEU scores on valid set for each model B, average Self-BLEU scores for each model S, the number of models n and the number of ensemble models c. The algorithm return a list C consists of selected models. We calculate the weighted score for each model as line 2 in the pseudo-code. The weight calculated in line 3 is a factor to balance the scale of Self-BLUE and valid set BLEU. Then the list C initially contains the model m top has a highest weighted score. Next, we iteratively re-compute the average Self-BLEU between the remaining models in |M ? C| and selected models in C, based on which we select the model has minimum Self-BLEU score into C. In our experiments, we save around 95% searching time by using this novel method to achieve the same BLEU score of the Brute Force search. We will further analyze the effect of Boosted Self-BLEU based Ensemble in section 5.2. 

 Experiments And Results 

 Settings The implementation of our models is based on Fairseq 1 for En?Zh and EN?De, and OpenNMT 2 for En?Ja. All the single models are carried out on 8 NVIDIA V100 GPUs, each of which has 32 GB memory. We use the Adam optimizer with ? 1 = 0.9, ? 2 = 0.998. The gradient accumulation is used due to the high GPU memory consumption. The batch size is set to 8192 tokens per GPU and we set the "update-freq" parameter in Fairseq to 2. The learning rate is set to 0.0005 for Fairseq and 2.0 for OpenNMT. We use warmup step = 4000. We calculate sacreBLEU 3 score for all experiments which is officially recommended. 

 Dataset The statistics of all training data is shown in Table  1 . For each language pair, the bilingual data is the combination of all parallel data released by WMT21. For monolingual data, we select data from News Crawl, Common Crawl and Extended Common Crawl, it is then divided into several parts, each containing 50M sentences. For general domain synthetic data, we use all the target monolingual data to generate backtranslation data and a part of source monolingual data (about 80 to 100 million for different languages) to get forward translation data. For the in-domain pseudo-parallel data, we use the entire source monolingual data and bilingual data. All the test and valid data from previous years are used as in-domain data. We use the methods described in Sec. 3.1 to filter bilingual and monolingual data. 

 Pre-processing and Post-processing English and German sentences are segmented by Moses 4 , while Japanese use Mecab 5 for segmentation. We segment the Chinese sentences with an in-house word segmentation tool. We apply punctuation normalization in English, German and Chinese data. Truecasing is applied to English? Japanese and English?German. We use byte pair encoding BPE  (Sennrich et al., 2016b)  with 32K operations for all the languages. For the post-processing, we apply de-truecaseing and de-tokenizing on the English and German translations with the scripts provided in Moses. For the Chinese translations, we transpose the punctuations to the Chinese format. 

 English?Chinese The results of En?Zh on newstest2020 are shown in Table  2 . For the En?Zh task, filtering out part of sentence pairs containing English characters in Chinese sentences shows a significant improvement in the valid set. After applying large-scale Back-Translation, we obtain +2.0 BLEU score on the baseline. We further gain +0.62 BLEU score after applying knowledge distillation and +0.24 BLEU from Forward-Translation. Surprisingly, we observe that adding more BT data from different In preliminary experiments, we select the best performing models as our ensemble combinations obtaining +0.4 BLEU score. On top of that, even after searching hundreds of models, no better results are obtained. With BSBE strategies in Sec. 3.6, a better model combination with less number of models are quickly searched, and we finally achieve 50.94 BLEU score. Our WMT2021 English?Chinese submission achieves a Sacre-BLEU score of 36.9, which is the highest among all submissions and chrF score of 0.337. 

 English?Japanese The results of En?Ja on newstest2020 are shown in Table  2 . For the En?Ja task, we filter out the sentence pairs containing Japanese characters in the English side and vice versa. The Back-Translation and Knowledge Distillation improve the baseline from 35.78 to 36.66. Adding more BT data further brings in 0.56 improvements. The improvement by finetuning is much larger than other directions, which is 5.32 BLEU. We speculate that this is because there is less bilingual data for English and Japanese than for other languages, and the test results for Japanese are char level BLEU so this direction is more influenced by the in-domain finetuning. Two In-domain knowledge transfers improve BLEU score from 37.22 to 43.69. Normal finetune still provides 0.54 improvements after in-domain knowledge transfer. Then, we apply advanced finetuning methods to further get 0.19 BLEU improvements. Our final ensemble result outperforms baseline 9.57 BLEU. 

 Japanese?English The Ja?En task follows the same training procedure as En?Ja. From Table  2 , we can observe that Back-Translation can provide 1.11 BLEU improvements from baseline. Knowledge Distillation and more BT data can improve the BLEU score from 20.82 to 22.11. The finetuning improvement is 3.8 which is slightly less than the En?Ja direction but still larger than En?Zh and En?De. We also apply two-turn in-domain knowledge transfer and further boost the BLEU score to 25.89. After normal finetuning, the BLEU score achieves 26.27. The advanced finetuning methods provide a slight improvement on Ja?En. After ensemble, we achieve 28.24 BLEU in newstest2020.  

 MODEL 

 English?German The results of En?De on newstest2020 are shown in Table  2 . After adding back-translation, we improve the BLEU score from 33.28 to 35.28. Knowledge Distillation further boosts the BLEU score to 36.58. The finetuning further brings in 2.63 improvements. After injecting the in-domain knowledge into the monolingual corpus, we get another 0.31 BLEU gain. We apply a post-processing procedure on En?De. Specifically, we normalize the English quotations to German ones in German hypotheses, which brings in 1.3 BLEU improvements. 

 Analysis To verify the effectiveness of our approach, we conduct analytical experiments on model variants, finetune methods, and ensemble strategies in this section. 

 Effects of Model Architecture We conduct several experiments to validate the effectiveness of Transformer  (Vaswani et al., 2017)  variants we used and list results in Table  3 . We also investigate the diversity of different variants and the impacts on the model ensemble. The results is listed in Table  4 and Table 5    

 Effects of Boosted Self-BLEU based Ensemble To verify the superiority of our Boosted Self-BLEU based Ensemble (BSBE) method, we randomly select 10 models with different architecture and training data. For our submitted system, we search from over 500 models. We use a greedy search algorithm  (Deng et al., 2018)  as our baseline. The greedy search greedily selects the best performance model into candidate ensemble models. If the selected model provides a positive improvement, we keep it in the candidates. Otherwise, it is added to a temporary model list and still has a weak chance to be reused in the future. One model from the temporary list can be reused once, after which it is withdrawn definitely. We compare the results of greedy search, BSBE and Brute Force and list the ensemble model BLEU and the number of searches in Table  6 . Note that n is the number of models, which is 10 here. For BSBE, we need to get the translation result of every model to calculate the Self-BLEU. After that, we only need to perform the inference process once. 

 Effects of Advanced Finetuning In this section, we describe our experiments on advanced finetuning in the four translation directions. As shown in  

 Conclusion We investigate various novel Transformer based architectures to build robust systems. Our systems are also built on several popular data augmentation methods such as back-translation, knowledge distillation and iterative in-domain knowledge transfer. We enhance our system with advanced finetuning approaches, i.e., target denoising, graduated label smoothing and confidence-aware scheduled sampling. A boosted Self-BLEU based model ensemble is also employed which plays a key role in our systems. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3 case-sensitive BLEU scores on English?Chinese, English?Japanese, Japanese?English and English?German, respectively. The BLEU scores of English?Chinese, English?Japanese and Japanese?English are the highest among all submissions, and that of English?German is the highest among all constrained submissions. would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. to our training flows to stabilize the training of deep Post-Norm Transformer. Our experiments have shown that the Post-Norm model has a good diversity compared to the Pre-Norm Model and slightly outperform the Pre-Norm Model. We will further analyze the model diversity of different variants in Sec. 5.1. 
