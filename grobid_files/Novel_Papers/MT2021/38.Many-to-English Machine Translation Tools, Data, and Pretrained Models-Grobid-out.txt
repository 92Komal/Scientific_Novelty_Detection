title
Many-to-English Machine Translation Tools, Data, and Pretrained Models

abstract
While there are more than 7000 languages in the world, most translation research efforts have targeted a few high resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTDATA, NLCODEC, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages. 1

Introduction Neural machine translation (NMT)  (Bahdanau et al., 2015; Vaswani et al., 2017)  has progressed to reach human performance on select benchmark tasks  (Barrault et al., 2019 (Barrault et al., , 2020 . However, as MT research has mainly focused on translation between a small number of high resource languages, the unavailability of usable-quality translation models for low resource languages remains an ongoing concern. Even those commercial translation services attempting to broaden their language coverage has only reached around one hundred languages; this excludes most of the thousands of languages used around the world today. Freely available corpora of parallel data for many languages are available, though they are hosted at various sites, and are in various forms. A challenge for incorporating more languages into MT models is a lack of easy access to all of these datasets.While standards like ISO 639-3 have been established to  1  Demo website: http://rtg.isi.edu/many-eng. Video demo: https://youtu.be/NSY0-MvO1KE. bring consistency to the labeling of language resources, these are not yet widely adopted. In addition, scaling experimentation to several hundred languages on large corpora involves a significant engineering effort. Simple tasks such as dataset preparation, vocabulary creation, transformation of sentences into sequences, and training data selection becomes formidable at scale due to corpus size and heterogeneity of data sources and file formats. We have developed tools to precisely address all these challenges, which we demonstrate in this work. Specifically, we offer three tools which can be used either independently or in combination to advance NMT research on a wider set of languages (Section 2): firstly, MTDATA, which helps to easily obtain parallel datasets (Section 2.1); secondly, NL-CODEC, a vocabulary manager and storage layer for transforming sentences to integer sequences, that is efficient and scalable (Section 2.2); and lastly, RTG, a feature-rich Pytorch-backed NMT toolkit that supports reproducible experiments (Section 2.3). We demonstrate the capabilities of our tools by preparing a massive bitext dataset with more than 9 billion tokens per side, and training a single multilingual NMT model capable of translating 500 source languages to English (Section 3). We show that the multilingual model is usable either as a service for translating several hundred languages to English (Section 4.1), or as a parent model in a transfer learning setting for improving translation of low resource languages (Section 4.2). 

 Tools Our tools are organized into the following sections: ing an ID for datasets, we establish a clear way of communicating the exact datasets used for MT experiments, which helps in reproducing the experimental setup. By offering a unified interface to datasets from many heterogeneous sources, MT-DATA hides mundane tasks such as locating URLs, downloading, decompression, parsing, and sanity checking. Some noteworthy features are: ? Indexer: a large index of publicly available parallel datasets. ? Normalizer: maps language names to ISO-639-3 codes which has representation space for 7800+ languages. 2 ? Parsers: parses heterogeneous data formats for parallel datasets, and produces a simple plain text file by merging all the selected datasets. https://github.com/thammegowda/mtdata Listing 1 shows an example for listing and getting datasets for German-English. In Section 3.1, we use MTDATA 3 to obtain thousands of publicly available datasets for a large many-to-English translation experiment. 2 https://iso639-3.sil.org 3 At the time of writing, v0.2.8 

 NLCODEC NLCODEC is a vocabulary manager with encodingdecoding schemes to transform natural language sentences to and from integer sequences. Features: ? Versatile: Supports commonly used vocabulary schemes such as characters, words, and bytepair-encoding (BPE) subwords  (Sennrich et al., 2016) . ? Scalable: Apache Spark 4  (Zaharia et al., 2016)  backend can be optionally used to create vocabulary from massive datasets. ? Easy Setup: pip install nlcodec ? Open-source: https://github.com/isi-nlp/nlcodec/ When the training datasets are too big to be kept in the primary random access memory (RAM), the use of secondary storage is inevitable. The training processes requiring random examples lead to random access from a secondary storage device. Even though the latest advancements in secondary storage technology such as solid-state drive (SSD) have faster serial reads and writes, their random access speeds are significantly lower than that of RAM. To address these problems, we include an efficient storage and retrieval layer, NLDB, which has the following features: ? Memory efficient by adapting datatypes based on vocabulary size. For instance, encoding with vocabulary size less than 256 (such as characters) can be efficiently represented using 1-byte unsigned integers. Vocabularies with fewer than 65,536 types, such as might be generated when using subword models  (Sennrich et al., 2016)  require only 2-byte unsigned integers, and 4-byte unsigned integers are sufficient for vocabularies up to 4 billion types. As the default implementation of Python, CPython, uses 28 bytes for all integers, we accomplish this using NumPy  (Harris et al., 2020) . This optimization makes it possible to hold a large chunk of training data in smaller RAM, enabling a fast random access. ? Parallelizable: Offers a multi-part database by horizontal sharding that supports parallel writes (e.g., Apache Spark) and parallel reads (e.g., distributed training). ? Supports commonly used batching mechanisms such as random batches with approximatelyequal-length sequences. NLDB has a minimal footprint and is part of 308 the NLCODEC package. In Section 3, we take advantage of the scalability and efficiency aspects of NLCODEC and NLDB to process a large parallel dataset with 9 billion tokens on each side. 

 RTG Reader translator generator (RTG) is a neural machine translation (NMT) toolkit based on Pytorch  (Paszke et al., 2019) . Notable features of RTG are: ? Reproducible: All the required parameters of an experiment are included in a single YAML configuration file, which can be easily stored in a version control system such as git or shared with collaborators. ? Implements Transformer  (Vaswani et al., 2017) , and recurrent neural networks (RNN) with crossattention models  (Bahdanau et al., 2015; Luong et al., 2015) . ? Supports distributed training on multi-node multi-GPUs, gradient accumulation, and Float16 operations. ? Integrated Tensorboard helps in visualizing training and validation losses. ? Supports weight sharing  (Press and Wolf, 2017) , parent-child transfer  (Zoph et al., 2016) , beam decoding with length normalization  (Wu et al., 2016) , early stopping, and checkpoint averaging. ? Flexible vocabulary options with NLCODEC and SentencePiece  (Kudo and Richardson, 2018)  which can be either shared or separated between source and target languages. ? Easy setup: pip install rtg ? Open-source: https://isi-nlp.github.io/rtg/ 3 Many-to-English Multilingual NMT In this section, we demonstrate the use of our tools by creating a massively multilingual NMT model from publicly available datasets. 

 Dataset We use MTDATA to download datasets from various sources, given in Table  1 . To minimize data imbalance, we select only a subset of the datasets available for high resource languages, and select all available datasets for low resource languages. The selection is aimed to increase the diversity of data domains and quality of alignments. Cleaning: We use SACREMOSES 5 to normalize   (2013, 2014, 2015, 2016, 2017, 2018) ;  Barrault et al. (2019 Barrault et al. ( , 2020  Table  1 : Various sources of MT datasets. Unicode punctuations and digits, followed by word tokenization. We remove records that are duplicates, have abnormal source-to-target length ratios, have many non-ASCII characters on the English side, have a URL, or which overlap exactly, either on the source or target side, with any sentences in held out sets. As preprocessing is computeintensive, we parallelize using Apache Spark. The cleaning and tokenization results in a corpus of 474 million sentences and 9 billion tokens on the source and English sides each. The token and sentence count for each language are provided in Figure  1 . Both the processed and raw datasets are available at http://rtg.isi.edu/many-eng/data/v1/. 6 

 Many-to-English Multilingual Model We use RTG to train Transformer NMT  (Vaswani et al., 2017)   Figure  1 : Training data statistics for the 500 languages, sorted based on descending order of English token count. These statistics are obtained after de-duplication and filtering (see Section 3.1). The full name for these ISO 639-3 codes can be looked up using MTDATA, e.g. mtdata-iso eng . 512,000 and 64,000 BPE types as source and target vocabularies, respectively. The decoder's input and output embeddings are shared. Since some of the English sentences are replicated to align with many sentences from different languages (e.g. the Bible corpus), BPE merges are learned from the deduplicated sentences using NLCODEC. Our best performing model is trained with an effective batch size of about 720,000 tokens per optimizer step. Such big batches are achieved by using mixed-precision distributed training on 8 NVIDIA A100 GPUs with gradient accumulation of 5 minibatches, each having a maximum of 18,000 tokens. We use the Adam optimizer (Kingma and Ba, 2014) with 8000 warm-up steps followed by a decaying learning rate, similar to  Vaswani et al. (2017) . We stop training after five days and six hours when a total of 200K updates are made by the optimizer; validation loss is still decreasing at this point. To assess the translation quality of our model, we report BLEU  (Papineni et al., 2002; Post, 2018)  7 on a subset of languages for which known test sets are available, as given in Figure  2 , along with a comparison to Zhang et al. (  2020 )'s best model. 8 

 Applications The model we trained as a demonstration for our tools is useful on its own, as described in the following sections.    (Zhang et al., 2020) . Despite having four times more languages on the source side, our model scores competitive BLEU on most languages with the strongest system of  Zhang et al. (2020) . The tests where our model scores lower BLEU have shorter source sentences (mean length of about three tokens). 

 Readily Usable Translation Service Our pretrained NMT model is readily usable as a service capable of translating several hundred source languages to English. By design, source language identification is not necessary. Figure  2  shows that the model scores more than 20 BLEU, which maybe be a useful quality for certain downstream applications involving web and social media content analysis. Apache Tika  (Mattmann and Zitting, 2011) , a content detection and analysis toolkit capable of parsing thousands of file formats, has an option for translating any document into English using our multilingual NMT model.  9  Our model has been packaged and published to DockerHub, 10 which can be obtained by the following command: IMAGE=tgowda/rtg-model:500toEng-v1 docker run --rm -i -p 6060:6060 $IMAGE # For GPU support: --gpus '"device=0"' The above command starts a docker image with HTTP server having a web interface, as can be seen in Figure  3 , and a REST API. An example interaction with the REST API is as follows: curl --data "source=Comment allez-vous?"\ --data "source=Bonne journ?e"\ http://localhost:6060/translate { "source": [ "Comment allez-vous?", "Bonne journ?e" ], "translation": [ "How are you?", "Have a nice day" ] } 

 Parent Model for Low Resource MT Fine tuning is a useful transfer learning technique for improving the translation of low resource languages  (Zoph et al., 2016; Neubig and Hu, 2018;  9 https://cwiki.apache.org/confluence/display/ TIKA/NMT-RTG 10 https://hub.docker.com/    (Kudo and Richardson, 2018)  and HuggingfaceTokenizers  (Wolf et al., 2020)  are the closest alternatives in terms of features, however, modification is relatively difficult for Python users as these libraries are implemented in C++ and Rust, respectively. In addition, SentencePiece uses a binary format for model persistence in favor of efficiency, which takes away the inspectability of the model state. Retaining the ability to inspect models and modify core functionality is beneficial for further improving encoding schemes, e.g. subword regularization  (Kudo, 2018) , BPE dropout  (Provilkov et al., 2020) , and optimal stop condition for subword merges  (Gowda and May, 2020) . FastBPE is another efficient BPE tool written in C++. 12 Subword-nmt  (Sennrich et al., 2016)  is a Python implementation of BPE, and stores the model in an inspectable plain text format, however, it is not readily scalable to massive datasets such as the one used in this work. None of these tools have an equivalent to NLDB's mechanism for efficiently storing and retrieving variable length sequences for distributed training. RTG: Tensor2Tensor  (Vaswani et al., 2018 ) originally offered the Transformer  (Vaswani et al., 2017)  implementation using Tensorflow  (Abadi 12  https://github.com/glample/fastBPE  et al., 2015) ; our implementation uses Pytorch  (Paszke et al., 2019)  following Annotated Transformer  (Rush, 2018) . OpenNMT currently offers separate implementations for both Pytorch and Tensorflow backends  (Klein et al., 2017 (Klein et al., , 2020 . As open-source toolkits evolve, many good features tend to propagate between them, leading to varying degrees of similarities. Some of the available NMT toolkits are: Nematus  (Sennrich et al., 2017) , xNMT . Marian NMT  (Junczys-Dowmunt et al., 2018) , Joey NMT  (Kreutzer et al., 2019) , Fairseq  (Ott et al., 2019) , and Sockey  (Hieber et al., 2020 ). An exhaustive comparison of these NMT toolkits is beyond the scope of our current work.  Johnson et al. (2017)  show that NMT models are capable of multilingual translation without any architectural changes, and observe that when languages with abundant data are mixed with low resource languages, the translation quality of low resource pairs are significantly improved. They use a private dataset of 12 language pairs; we use publicly available datasets for up to 500 languages.  assemble a multi-parallel dataset for 58 languages from TEDTalks domains, which are included in our dataset.  Zhang et al. (2020)  curate OPUS-100, a multilingual dataset of 100 languages sampled from OPUS, including test sets; which are used in this work. Tiedemann (2020) have established a benchmark task for 500 languages including single directional baseline models.  Wang et al. (2020)  examine the language-wise imbalance problem in multilingual datasets and propose a method to address the imbalance using a scoring function, which we plan to explore in the future. 

 Multilingual NMT 

 Conclusion We have introduced our tools: MTDATA for downloading datasets, NLCODEC for processing, storing and retrieving large scale training data, and RTG for training NMT models. Using these tools, we have collected a massive dataset and trained a multilingual model for many-to-English translation. We have demonstrated that our model can be used independently as a translation service, and also showed its use as a parent model for improving low resource language translation. All the described tools, used datasets, and trained models are made available to the public for free. 
