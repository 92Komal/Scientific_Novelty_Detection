title
NICT Kyoto Submission for the WMT'21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection

abstract
This paper presents the NICT Kyoto submission for the WMT'21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of correlation coefficient and F-score show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.

Introduction This paper describes the NICT Kyoto submission to the WMT'21 Quality Estimation (QE) shared task. We participated in Task 3 "Critical Error Detection" involving four language pairs, namely English-Chinese, English-Czech, English-Japanese and English-German. A critical error is defined as a translation error falling into one of the following five categories: toxicity, health or safety risk, named entity, sentiment polarity and number or unit deviation.  1  The objective of the task is to classify a sequence pair, composed of a sentence in the source language and its automatic translation in the target language, in a binary fashion whether it contains or not at least one of the five types of critical errors. This task differs from the other QE tasks as not all translation errors should be detected but only critical ones. Labels were produced by majority vote over three annotators for each pair leading to two possible classes: ERR (or class 1) when at least one critical error is spotted and NO (or class 0) when no critical errors are present. Our approach relies mainly on QE model pretraining leveraging a large amount of synthetic data produced using parallel corpora and MT systems. Because annotating translations for critical error is costly, we propose to pretrain a model on translation quality scores computed with automatic metrics. To capture multiple translation error granularities during pretraining, we employ multiple metrics and evaluate their performance individually on the downstream task. Additionally, we pretrain the QE model jointly on all WMT QE shared tasks language pairs as a data augmentation method. Transfer learning is then conducted for each language pair by finetuning the pretrained model on the downstream task with the officially released training data annotated with critical errors. The remainder of this paper is organized as follows. In Section 2, we introduce our approach involving multimetric and multilingual pretraining. In Section 3, the data, tools and training procedure are presented, followed by the experimental results and their analysis in Section 4, before the conclusion in Section 5. 

 Multimetric & Multilingual Pretraining Multilingual pretrained masked language models (LMs) were shown to perform well in several downstream natural language processing tasks  (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020) . Starting from an XLM-R checkpoint  (Conneau et al., 2020) , we performed continued (or intermediate) training  (Phang et al., 2018; Rubino and Sumita, 2020)  with large amount of automatically translated source language texts (thereafter called synthetic data), replacing the masked LM objective with QE oriented ones. Because XLM-R is multilingual and all languages in this model share a common vocabulary of sub-words, we decided to conduct QE pretraining on the 11 language pairs from all subtasks of WMT'21 QE. These language pairs all share English, whether on the source or target side, and this method can be seen as a data augmentation approach to increase vocabulary coverage. The objective of QE Task 3 is to classify sentence pairs in a binary fashion. Formally, given a source sequence s and its translation t, we want to learn a function f : f ? (s, t) ? y where y ? {0, 1} is the class associated with the sequence pair (s, t) and ? represents the model parameters. While finetuning a pretrained model on the official QE task 3 data allows us to directly learn model parameters approximating y given (s, t), we do not have such classes for synthetic data. We decided to use MT automatic metric scores as objective instead, assuming that critical error classes could correlate with translation quality scores at least in extreme cases (e.g. no translation errors also means no critical errors). Several automatic metrics are used by the research community to evaluate the performance of MT systems by measuring translation accuracy against a human-produced reference at different granularity levels. We opted for metrics capturing quality information at the character (chrF  (Popovi?, 2017) ), token (TER  (Snover et al., 2006) ) and token n-gram (BLEU  (Papineni et al., 2002) ) levels. For the latter, the smoothed sentence-level BLEU was chosen  (Chen and Cherry, 2014) . In addition to sentence-level metrics, token-level binary tags were also extracted following the usual procedure to determine post-editing effort  (Specia et al., 2020) .  2  To allow for sentence-level QE predictions, we added a feed-forward layer on top of XLM-R for each of the three metrics employed without parameter sharing, following: ?s = tanh (?(h)W s1 + b s1 ) W s2 + b s2 (1) where ?s ? R 1 is the sentence-level score, W s1 ? R d?d , b s1 ? R d , W s2 ? R d?1 and b s2 ? R 1 are parameters of the model with dimensionality d = 1, 024, ? is a pooling function and h ? R n?d is the set of contextual embeddings corresponding to the n tokens in (s, t). The pooling function is the class token added at the beginning of each input sequence. For token-level predictions, we used a linear transformation from contextual embeddings to two-dimensional output (for binary token-level classes): ?t = softmax (hW t + b t ), with ?t ? R n?2 are token-level scores, W t ? R d?2 and b t ? R 2 are the parameter matrix and bias. Parameters of the model are learned with minibatch stochastic gradient descent based on losses computed for sentence-level and token-level predictions. For the former loss, we used mean squared error, while cross-entropy was used for the latter. All losses are linearly summed with equal weights before back-propagation. The parameters of the classification and regression heads are optimized along with XLM-R. 

 Data and Tools This section presents the data used in our experiments, including the synthetic data produced for pretraining and the official QE task 3 corpora, along with the tools required to train our models and the procedure employed for both pretraining and finetuning. 

 Datasets In order to gather as much data as possible for many language pairs, we collected all parallel data from the QE shared tasks (from all subtasks). Additionally, we retrieved parallel data from the WMT news translation task  (Barrault et al., 2020)  and from OPUS  (Tiedemann, 2016) .  3  The source side of these parallel corpora was translated using publicly available neural MT models based on the Transformer architecture  (Vaswani et al., 2017) . For Estonian-English (et-en), Nepalese-English (neen), Romanian-English (ro-en), Russian-English (ru-en), Sinhala-English (si-en), English-German (en-de) and English-Chinese (en-zh), we used the MT systems made available by the shared task organizers, 4 while for English-Czech (en-cs), English-Japanese (en-ja), Khmer-English (km-en) and Pashto-English (ps-en), we used the mBART50 model  (Liu et al., 2020; Tang et al., 2020) .  5  Statistics about the synthetic corpora after translation are presented in Table  1 , along with the official QE data for Task 3 released by the shared task organizers. After deduplicating and cleaning the synthetic corpora produced to conduct QE pretraining, the total amount of data reached 72.3M triplets (source, translation and reference sentences). 

 Tools Data preprocessing was conducted using the tokenizer and truecaser from the Moses distribution  (Koehn et al., 2007) , except for Chinese, Japanese, Nepalese and Sinhala, for which the tokenization was conducted using jieba, 6 KyTea 7 and FLORES  (Goyal et al., 2021)  respectively. To compute the sentence-level and token-level scores, we used automatic metrics implementations available in the tools SacreBLEU  (Post, 2018)  for BLEU and chrF and tercom  (Snover et al., 2006)  for TER and token-level classes. The XLM-R checkpoint used was the xlmroberta-large from HuggingFace Transformers library  (Wolf et al., 2020) . We used in-house Pytorch  (Paszke et al., 2019)   GPUs for the former step and 1 GPU for the latter. 

 Training Procedure Model pretraining on synthetic data was conducted for one epoch (approx. 500k updates) with batches of 128 source and target sequences for a total training time of 3 days. The AdamW optimizer  (Loshchilov and Hutter, 2019)  was used with ? 1 = 0.9, ? 2 = 0.999 and = 1 ? 10 ?6 , while the weight decay was set to 0. A linear learning rate warmup was used during the first 50k updates to reach a maximum value of 5 ? 10 ?6 , which remained without decay until the end of the first epoch. The dropout rates were set to 0.1 for both the embeddings and the transformer blocks (feedforward and attention layers). A total of four models were pretrained with different random seeds before being finetuned on the official QE Task 3 data. To conduct finetuning, we added a classification layer on top of XLM-R following: ?e = softmax(tanh (?(h)W e1 + b e1 ) W e2 + b e2 ) (2) where ?e ? R 2 is the sentence-level probability distribution over the two classes, During finetuning, which lasted 40 minutes per model, we used the validation set to select the best performing models according to the Matthews correlation coefficient (MCC), which is the main metric chosen by the shared task organizers for the final evaluation. One model per seed was selected and a total of four models were ensembled to produce our final submission to the shared task. W e1 ? R d?d , b e1 ? R d , W e2 ? R d?2 

 Results and Analysis We present in this section the main results obtained on the official shared task test set as reported by the organizers, followed by an analysis with ablation study and various pretraining objectives.  

 Shared Task Results The official results reported by the shared task organizers are presented in Table  2 . We compare our final ensemble results, obtained with four models trained on different seeds, to our baseline, obtained with a single model. We also include the official baseline provided by the shared task organizers. All our submissions outperform the official baseline and our ensembles reach the highest performance according to the correlation score and F-measure. One exception, however, is for the English-Japanese language pair. Despite several attempts to improve our ensembling method for this pair, we could not improve over our baseline. A comparison with other shared task participants in terms of MCC and F1 scores shows that our submissions were ranked first for English-Czech and English-German, third for English-Chinese and sixth for English-Japanese. We assume that the smaller amount of synthetic data, as well as a possible preprocessing mismatch between the official data and our synthetically generated corpora, could be the reason behind the low performance of the two latter language pairs. More precisely, the data preprocessing pipeline for English, German and Czech are commonly based on the Moses tokenizer and truecaser, and it is possible to infer the parameters used with these tools by looking at the official training data released for the task. For Chinese and Japanese, however, due to the lack of details given by the shared task organizers, it was not possible to use the same preprocessing tools and parameters with certainty. 

 Impact of Pretraining Steps While our approach relied on a two-step process, QE pretraining on synthetic data followed by finetuning on the task specific training set, we still made use of a pretrained XLM-R model by initiating QE pretraining from a checkpoint. Overall, three steps are thus required to obtain the results presented in Table  2 . XLM-R and QE pretraining, as well as producing synthetic data, are the most computationally expensive steps, whereas finetuning is relatively cheap to perform due to the small amount of task specific data. Therefore, we performed an ablation study aiming at evaluating the impact of each pretraining step and ran two sets of experiments following the same experimental setup employed for our main submission to the shared task. For the first set of experiments, no pretraining of XLM-R was conducted, meaning that we did not start QE pretraining from an existing checkpoint, but instead randomly initialized XLM-R parameters and ran QE pretraining from scratch (this setup is noted No Checkpoint). For the second set of experiments, we finetuned the XLM-R checkpoint directly on the task specific data, without conducting QE pretraining. This alleviates the need to produce large amount of synthetic QE data (this setup is noted No QE Pretraining). We conducted an additional set of experiments based on XLM-R and QE pretraining without finetuning on the official training set but the obtained results were subpar compared to the baseline, due to the randomly initialized parameters of the classification layer (see eq. (  2 )) which was not tuned for the task following this configuration. We present the results of the two ablation experiments in Table  3 . While combining both the use of a pretrained XLM-R with masked LM and QE pretraining on synthetic data leads to the best results on the four language pairs, No QE Pretraining performs better than the No Checkpoint configuration. These results emphasize the usefulness of large selfsupervised LM pretraining. The amount of data used for QE pretraining is smaller compared to the large quantity of monolingual and parallel data used to train xlm-roberta-large, which could be an explanation for the difference in downstream performances according to the MCC and F1 metrics. 

 Impact of Pretraining Objectives As an additional analysis, we propose to evaluate the impact of different metrics used as pretraining objectives on the downstream critical error detection task. Several independent QE pretraining were conducted for this purpose: one for each sentencelevel translation quality metrics, one for the combination of sentence-level metrics and finally one for word-level metrics which includes source, target and gap error predictions as described in Section 2. The finetuning step for each pretrained model is identical, only the learning objective during pretraining differs. The results obtained on the validation set for the critical error detection task are presented in Table  4 . Based on MCC scores, using sentence-level metrics during pretraining is not leading to the best downstream performance compared to using wordlevel metrics or combining both sentence and wordlevel quality indicators. From the three sentencelevel metrics used as learning objectives during pretraining, TER and BLEU outperform chrF. For English-German and English-Chinese, using wordlevel metrics outperforms the combination of all metrics, while it is the opposite for English-Czech and English-Japanese. These results show that the optimal quality indicator for QE pretraining depends on the language pair and the translation direction, and should therefore be considered as a hyper-parameter to be optimized. However, due to the costly nature of large model pretraining, combining multiple translation quality indicators in a multi-task learning fashion appears to be an efficient solution, in addition to using masked LM pretrained model as shown in the results presented in Section 4.2. 

 Conclusion This paper presented the NICT Kyoto submission for the WMT'21 QE Task 3 "Critical Error Detection". Our submissions were ranked first for two out of four language pairs. Our approach relies mainly on model pretraining with large amount of synthetic data, followed by finetuning on the official data released for the shared task. We proposed a novel QE pretraining approach which allows for a multimetric learning objective based on relatively cheap to compute MT automatic metrics. An analysis of each automatic metric used during QE pretraining shows the complementarity of metrics both at level of sentences and words. The ablation study emphasized the usefulness of both self-supervised and QE pretraining. Future work focuses on exploring additional metrics and their performance on various downstream QE tasks. and b e2 ? R 2 are parameters of the model with d = 1, 024. The pooling function ? is the same as the one employed during pretraining presented in Section 2. Due to the class imbalance of the critical error dataset, we used the weighted cross-entropy loss function to finetune our models. The weight given to the error class (the least populated) was tuned on the validation set in a grid-search manner, with integer values ranging from 1 to 8. 
