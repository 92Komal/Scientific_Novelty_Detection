title
Findings of the WMT Shared Task on Machine Translation Using Terminologies

abstract
Language domains that require very careful use of terminology are abundant and reflect a significant part of the translation industry. In this work we introduce a benchmark for evaluating the quality and consistency of terminology translation, focusing on the medical (and COVID-19 specifically) domain for five language pairs: English to French, Chinese, Russian, and Korean, as well as Czech to German. We report the descriptions and results of the participating systems, commenting on the need for further research efforts towards both more adequate handling of terminologies as well as towards a proper formulation and evaluation of the task.

Introduction Language domains that require very careful use of terminology are abundant. The need to adequately translate within such domains is undeniable, as shown by e.g. the different WMT shared tasks on biomedical translation. More interestingly, as the abundance of research on domain adaptation shows, such language domains are (a) not adequately covered by existing data and models, while (b) new (or "surge") domains arise and models need to be adapted, often with significant downstream implications: consider the new COVID-19 domain and the large efforts for translation of critical information regarding pandemic handling and infection prevention strategies. In the case of newly developed domains, while parallel data are hard to come by, it is fairly straightforward to create word-or phrase-level terminologies, which can be used to guide professional translators and ensure both accuracy and consistency. This shared task 1 replicated such a scenario, and invited participants to explore methods to incorporate terminologies into either the training or the 1 http://statmt.org/wmt21/terminology-task.html inference process, in order to improve both the accuracy and consistency of MT systems on a new domain. 

 Shared Task Details The shared task focused on five language pairs, with systems evaluated on: ? English to French ? English to Chinese ? English to Russian ? English to Korean ? Czech to German The last three language pairs were "surprise" language pairs. This shared task construction follows a three-phase approach to ensure the generalizability of the findings, inspired by other multilingual shared tasks  (Vylomova et al., 2020) . In this setting, only part of the evaluation language pairs (or languages) are revealed from the beginning (the Development Phase). In this elongate period (a couple of months), the participants are provided with data in some language pairs to develop their methods. The second phase is the Generalization phase, which is a short time period (two to three weeks in this task's case), in which additional (surprise) language settings are revealed, only giving the shared task participants enough time to deploy a system, as opposed to allowing them enough time to also perform extensive optimization on the datasets. The final stage is the Evaluation phase, in which the test data are released and the methods are evaluated on these held-out data. The goal of this 3-stage approach (with both development and surprise language pairs) is to avoid approaches that overfit on language selection, and instead evaluate the more realistic scenario of needing to tackle the new domain in a new language in a limited amount of time. The surprise language pairs were announced 3 weeks before the start of the evaluation campaign. The organizers provided training/development data and terminologies for the above language pairs. Test sets were released at the beginning of the evaluation period. The participating teams were invited to participate in any or all of the language pairs. 

 Data Training The shared task primarily focused on a constrained submission setting, in which the participants could only use any parallel or monolingual data listed in previous versions of WMT shared tasks to train their systems. Some pre-trained systems listed at the shared task announcement (mBERT, XLM, XLM-R, mBART, mT5, M2M100) were also allowed, but should be disclosed by the participants. We note that the training data allowed come from a "general" domain, as opposed to e.g. highly specialized biomedical data, which in theory should be more helpful for this setting. Terminologies The shared task focused on adapting MT systems to the health domain in general, with a particular interest in the surge COVID-19 domain. The terminologies for the English to French, Chinese, Russian, and Korean language pairs were taken from the publicly available TICO-19 project  (Anastasopoulos et al., 2020) , a multiorganizational project that created data to aid translators and evaluate MT systems on the COVID-19 domain. The terminologies were created by linguists at Google and Facebook in consultation with domain experts, providing translations for about 600 terms in each language. The terminologies are publicly available.  2  The Czech-German medical terminology was generated automatically from Wikipedia. We considered all Wikipedia titles corresponding to the category Health care or to one of its subcategories, and all titles linked from the text. The list of (sub)categories was manually filtered to only include relevant articles. We treated all page titles as terms and relied on the Wikipedia language links to provide their translations. Furthermore, we used redirection links to obtain synonyms of both source and target terms. For all terminologies, we truecased the terms using a pretrained truecaser and manually checked the results. The Czech-German terminology was eventually further reduced to only include terms which occurred in the EMEA medical corpus. 2 https://tico-19.github.io/ Development and Test The development and test data for French, Chinese, and Russian were taken from the publicly available TICO-19 evaluation data. The organizers additionally created Korean translations of the English source-side sentences, which will be made available as part of the original TICO-19 datasets.  3  The primary source of the Czech-German development and test data is the EMEA 4 parallel corpus of the European Medicines Agency. We cleaned it using the Moses tools, searched for terms and their translations and tagged the occurrences. The surface forms used for the search were collected from a corpus of in-domain Wikipedia articles which includes links to the lemmatized Wikipedia titles/terms next to their inflected forms. Target options were retrieved from the terminology and enriched with surface forms. Out of all sentences with terms, we selected around 3.5k sentences for the dev set and 1.1k for the test set. The development and test sets were tagged automatically but the test set was manually corrected to get rid of the artifacts caused by the automatic generation. 

 Ensuring Terminology Consistency on the Evaluation Datasets It is worth noting that, originally, none of the development and test data were created under the constraints imposed by the specific terminologies we use. As such, we needed to ensure that the data 'complied' with the terminologies in order to guarantee a meaningful, accurate, and fair to the participants evaluation of the shared task's research questions. The TICO-19 project created the evaluation dataset independently of the terminologies.  5  In our preliminary analysis, we first searched for all terminology terms on the English side of the parallel data, also searching over the lemmatized versions of English sentences. The choice of starting from the English side is due to two reasons: (a) it reflects the actual translation direction the data was created with and that we evaluate on, (b) it reduces the rate of possible false negative/positive term matches due to the lack of morphological complexity of English. Having the source-side terms identified, we assume all of them should be translated according to the terminology. We then search the target side (both original and lemmatized) for the translation required by the terminology, and created a tag on the source-side term if we found an exact match. Last, we showed all sentences to professional translators, who were instructed to produce three types of annotations, for each source-side term. The first is a label describing whether (a) the automaticallyannotated source-side term should not be translated by the terminology i.e. it is not really a term, (b) the tagged exact match is correct, (c) the translation is compliant with the terminology even though there is not an exact match, (d) the tagged exact match is incorrect, or (e) the source term translation is applicable in the context, but not used. The second annotation is a tagged translation for any terms labeled as (a), (c), or (d), denoting exactly which part of the target-side corresponds to the source-side term. The third annotation is a tagged terminology-compliant translation, where if any source-side terms are labeled with (d) or (e), we ask the translators to rephrase the target side in order to make it compliant with the terminology. Table  1  shows example sentences from the dataset, along with their expected annotations from the translators. Below we provide the exact instructions given to the annotators, which also reference the same examples. [begin annotation instructions] About: This task is about determining if a translation is compliant with a terminology data base and perform inline annotations on the translations to mark the terms used. Annotators receive: Source side input, together with approximate terminology matches on the source side. Annotators return: For each term match, please annotate a Label: (a) does _ not _ apply: The terminology is not applicable in the context because of wrong meaning on the source side (Example 3). Please use a) if you think the translation should not comply with the terminology matched, irrespective of whether the translation uses it or not. (b) exact _ match _ correct: The term translation is found exactly as is in the target and its usage is correct (it fits the context and agrees grammatically with the sentence). (  Example 2) (c) variation _ correct: The translation is compliant with the terminology, however the term translation appears in a different form in the target (Examples 1 and 2). If only part of the term was preserved, use this label if this partial term is sufficient and completely preserves the meaning. Please use b) or c) if you think the translation is compliant with the terminology. (d) incorrect: The term is found in the target, as an exact match or as a variant, but it is used incorrectly, either semantically or grammatically: e.g. the term use does not convey the required meaning, there is a wrong inflection or other grammatical disagreement. (e) not _ used: The term translation is applicable in the context, but not used (Example 2, 4). Make this only for clear omissions: everything else should be variation (correct or incorrect variation) Please use d) or e) if you think the translation is not compliant with the terminol-ogy, but it should. Tagged translation: For any terms that are labeled as a), c) or d) please add inline markup to identify the fragments of the translation that they match. For each source sentence, please generate a Tagged Terminology-compliant translation: if any of d) to e) apply to any term in the sentence, meaning the translation is not compliant with the terminology for at least one term, please provide an alternate translation that is compliant with the terminology w.r.t all the terms in the sentence. If there is no acceptable translation that would use the expected target term then you should annotate the target with a) does_not_apply. If all terms in sentence match a) b) or c), leave this empty. 

 [/end annotation instructions] Through this process, we ended up modifying 284 (9.25%), 251 (8.17%), 450 (14.65%), and 809 (26.34%) sentences in the French, Chinese, Russian, and Korean datasets respectively, in order to make them terminology-compliant. Last, the Czech-German terminologies were directly derived from the parallel data hence they implicitly directly reflect the underlying data, so there was no need for the aforementioned process. 

 Evaluation The evaluation of the shared task used several metrics, focusing on both translation accuracy and terminological consistency. ? Translation accuracy was evaluated with standard reference-based MT metrics (BLEU, chrF, BERTscore, COMET). In light of recent work  (Kocmi et al., 2021) , we rank systems according to the COMET metric. ? we also performed terminology-targeted evaluation (to evaluate for consistency). We use the metrics outlined by Alam et al. (  2021 ), namely exact-match term accuracy, 1-TERm score, and window overlap accuracy. We rank systems according to term exact-match accuracy. Briefly, the lemmatized exact-match term accuracy is an accuracy score that searches for exact term translation matches (of the terminology required output) over either the lemmatized or the original hypothesis. The window overlap accuracy identifies the translation of the term, and then scores its context, to measure how well a translated term is placed in the hypothesis. Last, the 1-TERm score is a modification of the TER metric  (Snover et al., 2006) , biased to assign higher edit cost weights for words belonging to a term (and then simply reversed so that a higher score is better). We refer the reader to  Alam et al. (2021)  for further discussion of the metrics and supporting arguments for their use. Last, we evaluate whether differences between systems are statistically significant using paired bootstrap resampling  (Koehn, 2004) , over sentencelevel COMET and exact-match accuracy scores. Based on this information we cluster statisticallyinsignificantly-different (i.e. similarly performing) systems when we produce their final rankings. Winning submissions will be the ones that are Pareto-optimal along the two evaluation metrics that a good but also terminology-compliant system should maximize: exact-match accuracy (which captures terminology consistency) and COMET (which captures general translation quality). As such, there is the possibility that each language pair will have multiple winning submissions. 

 Participants and System Descriptions We received a total of 43 submissions from 9 teams. Below we provide a short description of each submission. CUNI  (Jon et al., 2021b)  Authors competed on En-Fr language pair. The terminology constrains are inserted as done in  (Jon et al., 2021a) . The target translation of specific terms is appended to the source sentence as a suffix and separated by a special token (if multiple constraints occur for a single sentence, an additional token separator is added). In order to have more training data of this form, synthetic constraints are added by sampling random token subsequences from the target sentence and appending them to the source sentence as described earlier. Note that since no modification is done on target side of the parallel data, no post-processing of the MT output is needed. As NMT systems trained from this pre-processed data sometimes fail to generate inflection in the translation output, terminology tokens appended to the source are lemmatized for both training and inference which brings improvements over the different shared task metrics. Huawei (HW-TSC)  (Wang et al., 2021b)  Authors submitted output of an unconstrained system to En-Zh language pair. They train a Transformer big architecture on both out-of-domain and indomain (biomedical) data. Parallel data in biomedical domain is augmented using more resources from TAUS 6 and back-translation of monolingual in-domain data is also applied. For the terminology shared-task, authors applied the system created for the biomedical translation shared task (described in  (Wang et al., 2021b) ) without any specific adaptation except appending the terminology dictionary to the end of training data. No separate paper was submitted for the terminology task. Kakao Enterprises (KEP)  (Bak et al., 2021)  Authors submitted to En-Fr, En-Zh, En-Kr, Cz-De. A detailed data cleaning is performed, removing between 6% and 14% of the data. In-domain data is back-translated (only for En-Fr and En-Kr) and is selected by a a combination of keywords spotting and domain similarity, measured as perplexity of an in-domain language model. A first model is obtained by adding to that synthetic language pairs obtained by verbalizing the terminology database. The only language pair were this verbalization does not yield improvement is Cz-De, whose terminology was automatically constructed. Models obtained in this manner were submitted for En-Zh, En-Kr, Cz-De. For En-Fr additional techniques are used: as those obtained the highest COMET score we detail them there. The final system for that language pair is trained inspired by techniques from (Bergmanis and Pinnis, 2021a;  Dinu et al., 2019) , but without modifying the model architecture. The source data is modified by adding immediately after a source term the corresponding target lemma, separated by special tokens. The model is pre-trained on randomly selected verbs and nouns, and fine-tuned using the terminology ontology. Interestingly, the pre-trained model -while improving Exact Match with respect to the baselines -degrades all other metrics. That degradation is however recovered and even improved when fine-tuning. For En-Ko and Cs-De ensemble models were used. Lingua Custodia (LC)  (Ailem et al., 2021a)  The team participated in En-Fr, En-Ru and En-Zh tasks. They build on top of  (Ailem et al., 2021b)  by inserting the terminology as constraints in the source sentence. Such constraints represent special tags around the detected source term followed by the target term from the terminology, the original source term is masked. Presence of such constraints at training encourages the model to copy the correct term translation. In case where multiple translations are proposed by the terminology, the one which is present in the target sentence is chosen at training time. At inference time the translation is selected at random. In order to enforce learning signal, the team enriched parallel data with backtranslation of monolingual data that contains terminology. Authors show that the proposed method allows to improve significantly for standard MT evaluation metrics, as well as terminology oriented metrics (Alam et al., 2021) over the standard baseline without terminological constraints. PROMT  (Molchanov et al., 2021)  The team submitted two systems (En-Fr and En-Ru), both of which are transformer models implemented on MarianMT  (Junczys-Dowmunt et al., 2018) . The first approach uses a rule-based system (SmartMT) to modify the neural system's output, which extracts rules only for noun phrases. If the desired output of a source term is not found in the NMT output, the rule-based system identifies the term's current translation and its morphological analysis (case and number) in order to substitute it with the terminology-provided translation in the desired inflection. The second approach is an adaptation of  (Dinu et al., 2019)  to MarianNMT toolkit. Each source terminological term is followed by its translation using special tokens to signal these terminological entries in the text (and impose a softconstraint to the translation system). Model is retrained from such pre-processed data. Data augmentation is also performed to create more synthetic data with terminology markup. Both approaches are rather close in performance. SPECTRANS  (Ballier et al., 2021)  This team sumbitted to En-Fr language pair. They experimented with 2 open source NMT toolkits JoyeNMT  (Kreutzer et al., 2019)  and OpenNMT  (Klein et al., 2017) . After the first experiments with Europarl they retained OpenNMT which gave better performance. Their best runs were trained on Com-monCrawl augmented with terminological data. They provided qualitative analysis of terminologyrelated translations and discuss the limitations of the terminologies provided for the task. SYSTRAN  (Pham et al., 2021)  This participant submitted to En-Fr language pair and proposed two methods to incorporate terminology. The first approach, based on  (Michon et al., 2020) , replaces source and target terminological terms by placeholders including a unique identifier plus morphological information (masculine/feminine and singular/plural). In a variant of this method, the source terminology word form is also incorporated in the source stream. At training time, NMT model is learnt on such pre-processed data and a postprocessing step recovers the word tokens from the placeholders after inference. The second approach (which lead to better performance) consists in learning a copy behaviour for terminological tokens at training time: terminology translations are inserted in the source sentence either by appending the target term (its surface or lemma form) to its source version, or by directly replacing the original term with the target one. A NMT system is trained on such pre-processed data and no post-process for recovering terminology tokens is needed at inference as target side of parallel data remains untouched. For both approaches however, a grammatical error correction is applied to the MT hypotheses in order to limit morphology errors. The impact of such post-processing on BLEU is positive, although small. TermMind  (Wang et al., 2021a)  The team submitted to En-Zh task. Similar to  (Ailem et al., 2021a)  they build on top of  (Ailem et al., 2021b)  by inserting terminological constraints in the training data. In the case where multiple translations are available they augment source sentence with all possible translations (which is different from  (Ailem et al., 2021a)  who kept only one translation). In order to strengthen the learning signal participants extend given terminologies with biphrases extracted from parallel data and integrate the constraints for those biphrases as well. Finally, they used backtranslation, fine tuning on pseudo in-domain data and ensembling to strengthen the baseline model. Ensembling methods seem to lead to the best results. 

 TILDE (Bergmanis and Pinnis, 2021c) The team participated to En-Fr, En-Ru and Cz-De language pairs. They focused primarily on terminology filtering, outlining several notable shortcomings of the Shared Task's terminologies, most of which are due to the use of terminologies intended for human translators (as opposed to terminologies created specifically for integration with MT systems). They devise two strategies for selecting among multiple target candidates for a source term, finding that an alignment-based technique outperforms the option of always selecting the first terminology entry. The MT systems are transformerbased using MarianMT, also integrating the method of Bergmanis and Pinnis (2021b) for incorporating terminology constraints in a soft manner. 

 Results and Discussion The results and rankings for English-French are listed in Table  2  and for English-Chinese in Table 3. The results for the surprise language pairs are in Table  4  for English-Russian and Table  5  for English-Korean and Czech-German. In the English-French translation task, there are two winning submissions. Two ProMT submissions ranked first according to exact-match accuracy (along with a CUNI submission), but the ProMT.soft submission is statistically significantly better than the other two with respect to COMET, hence it is one of the winning submission. The second winning submission is the one by KEP, which ranks first according to COMET, but also according to 1-TER, which indicates that it might strike a good balance between general translation quality and term consistency. In the English-Chinese translation task there is a single winning submission, the one by TermMind (system 2), which ranks first according to both metrics. We note that another submission (HW-TSC) is statistically significantly better than all submissions in all metrics except for 1-TERm, but this submission is an unconstrained one, and hence it is excluded from the rankings. In English-Russian the ProMT submission ProMT.soft is the clear winner, ranking as the single best system according to exact-match accuracy, as well as one of the two best systems according to COMET. Interestingly, the other system that ranks first according to COMET (ProMT.smartnd.v2) ranks first according to 1-TERm score, but also last according to exact-match accuracy, denoting perhaps an orthogonality between the goals of terminological consistency and general translation quality, where prioritizing one over the other leads to performance drops along the other dimension. Last, the submissions by KEP are the winning ones for English-Korean and Czech-German. For the former language pair it was the only submitted system (see discussion on potential reasons), while for Czech-German it ranked for best system according to exact match accuracy with the other submission (by TildeMT), but was significantly better according to COMET. Although TildeMT used a more sophisticated approach to the terminology translation, the KEP team had a stronger baseline and used ensembling which significantly increased both general translation quality and the term accuracy. 

 General Quality It was pointed out by Bergmanis and Pinnis (2021c) that a majority of terms from the terminologies were represented in the training corpora, which could lead to an underestimation of the importance of terminology in the metrics. The results show that using terminology constrains leads to an improvement over the baselines trained without it, but the effect would be more substantial if the training corpora were filtered to exclude sentences with terms. Perhaps a future iteration of the shared task could include an explicitly novel domain, although how well such a domain indeed exists or is even possible in the age of big data where our models can be trained on a large part of the Internet is debatable. An alternative is to carefully filter the training corpora to remove sentences with the terms, to create a truly challenging domain adaptation with terminologies setting. 

 Terminology Consistency The discussion of the Shared Task taught us that narrow terminology with unambiguous translations is more suitable for terminology-focused machine translation than a broader and more universal terminology with several target options. Unlike human translators who naturally choose from translation alternatives, it is difficult for a MT system to filter out noisy or inappropriate word forms. While a narrow terminology can ensure a proper and exact translation of terms, e.g. when translating a lecture with several special terms known in advance, we believe that a broad terminology can serve for more general domain adaptation using existing lexical resources. We note that several participating teams highlighted this issue, e.g.  Ballier et al. (2021) ; Bergmanis and Pinnis (2021c). The TICO terminologies in a few cases included additional comments aimed at translators who are    

 Development vs Surprise Language Pairs The participants had significantly more time to develop systems for English-French and English-Chinese, as opposed to the other three surprise language pairs. This is reflected partly on the total submitted systems in each language pair, where English-Korean and Czech-German received only 1 and 2 submissions respectively. We hypothesize that another explanation for this lies in the much more low-resource setting of these two language pairs, which generally tend to lead to lower quality systems, which might in turn discourage the participants. A second potential explanation could lie in the general cohort of participants, which is largely comprised of teams from industry (the only exception is the CUNI team that is an academic one). Perhaps the two low-resource language pairs are simply translation directions that the participating institutions are less interested in -which we take as an indication for the importance of including such lessresearched, low-resource, under-served language pairs in future iterations of this shared task, to encourage research in languages and language pairs beyond those with the most obvious commercial value. 

 Czech-German Analysis We believe that even with the automatically generated resources this task provided an important insight into translation of terms between two linguistically different and morphologically rich languages such as German and Czech. When analyzing the results, we focused on the phenomenon of nominal compounding in German. A natural translation of terms into German often results in a compound of a term and a general word, e.g. Hormonproduktion (production of hormones), or two terms, e.g. Plasmaprotein (plasma protein). Compounding is an important aspect of terminology-based translation to German that the model should have the capacity to create compounds from terminology entries. The automatic metrics favor translations into two separate words, even though a compound is often more natural. We analyzed how candidate translations handled concrete cases; see Table  6  for an example. Out of 262 sentences with this phenomenon in the reference, the correct compound word was generated in 112 and 133 cases by the TildeMT and KEP systems, respectively. Both systems generate compounds from terms, although the former was trained with terminology constraints and the latter only saw the terms during explicit training on the terminology entries. 

 Related Work Phrase-based statistical MT systems  (Koehn et al., 2003)  allowed for fine-grained control over the system's output by design, e.g. by incorporating domain-specific dictionaries into the phrase table, or by forcing translation choices for certain words or phrases. On the other hand, the currently stateof-the-art approach of neural machine translation (NMT) does not inherently allow for such control over the system's output. Some approaches  incorporate dictionaries through interpolation of the decoder's probability with a lexical probability based on source-side attention matches  (Arthur et al., 2016) . Perhaps the most common paradigm is constrained decoding  (Hokamp and Liu, 2017; Anderson et al., 2017; Post and Vilar, 2018, inter alia) , where the terminology matches are presented as hard constraints that the beam search must satisfy. Constrained decoding is not without disadvantages: it can be computationally expensive and it is often brittle when applied in realistic conditions  (Dinu et al., 2019) . To this end, some works  (Dinu et al., 2019; Bergmanis and Pinnis, 2021b; Exel et al., 2020; Niehues, 2021)  introduced approaches where the terminological constraints are provided as input to the NMT as additional annotations inline with the source sentence. As such, these can be considered as "soft" constraints, as there is no guarantee that the NMT system will indeed produce an output containing them. In any case, the best practice for incorporating terminological constraints in NMT is both underresearched and still not settled yet, especially in the case of morphologically rich languages, underlying the need for this shared task. 

 Conclusion We presented the results of the first edition of the WMT21 shared task on MT using Terminologies. For the purposes of the task we created new evaluation datasets, annotated by professional translators for their terminology consistency, based on the TICO-19 data for English to French, Chinese, Russian, and Korean, as well as a dataset for Czech-German based on the EMEA corpus. The Shared Task received 43 submissions from 9 teams, 8 from industry and 1 from academia, underscoring the general applicability of our focus problem ('how best can we use a terminology in MT?') on real-world settings. Most submissions add soft or hard constraints on the source side that the MT learns to handle, as proposed in  (Dinu et al., 2019) , but other novel approaches include terminology filtering for selecting between multiple options provided by the terminology, or replacing terms with placeholders to be inserted after the MT has produced the output. We devised multiple terminology-targeted metrics and evaluated systems along both these metrics as well as general translation quality. In most cases we find that, encouragingly, one does not necessarily have to sacrifice general translation quality for terminology compliance, as long as the terminology is of adequate standards. In future iterations of the Shared Task, we will take into account the distinction between terminologies created for humans (which are abundant) and terminologies created specifically for MT systems which need to be created, and have different requirements/specifications that the former. In addition, we will attempt to consider a new domain, rather than focusing again on the biomedical domain and specifically COVID-19 (although this is a great example of a "surge" domain that immediately required that translation providers and MT engines adapt in order to handle translations of large volumes of text in this novel domain). :::::: krv?cen? do sval? nebo hematom. TGT Hirn :::::::: metastasen zeigten inkonsistente oder keine Fluoreszenz. . . . Muskel ::::: blutung oder H?matom. TildeMT Zerebrale :::::::: Metastasen zeigten eine inkonsistente oder keine Fluoreszenz. . . . :::::::: Blutungen in den Muskeln oder H?matom KEP Hirn :::::::: metastasen zeigten eine inkonsistente oder keine Fluoreszenz. . . . Muskel ::::: blutung oder H?matom. 
