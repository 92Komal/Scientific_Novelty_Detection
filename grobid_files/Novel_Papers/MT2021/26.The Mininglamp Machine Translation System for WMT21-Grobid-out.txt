title
The Mininglamp Machine Translation System for WMT21

abstract
This paper describes Mininglamp neural machine translation systems of the WMT2021 news translation tasks. We have participated in eight directions translation tasks for news text including Chinese?English, Hausa?English, German?English and French?German. Our fundamental system was based on Transformer architecture, with wider or smaller construction for different news translation tasks. We mainly utilized the method of back-translation, knowledge distillation and fine-tuning to boost single model, while the ensemble was used to combine single models. Our final submission has ranked first for the English?Hausa task.

Introduction This paper describes the Mininglamp submissions to the WMT2021 news translation tasks for eight directions including four highresource Chinese?English, German?English, two medium-resource French?German and two low-resource Hausa?English. Furthermore, all of our systems were built with constrained data sets. For this participation, we experimented with some smaller or wider Transformer  (Vaswani et al., 2017)  architectures to reach a reliable baseline based on different resource scales, sampling or beam search in back-translation to generate more suitable pseudo bilingual sentences. Particularly in the low-resource tasks, Hausa?English, the Transformer-Small neural machine translation was built for the baseline, we presented iterative between back-translation and fine-tuning pattern which significantly improve the BLEU score on the validation set, and it worked well on English?Hausa task. Due to time constraints, we did not experiment on Hausa?English task. This path could be an experiment in the future work. As for the data augmentation aspect, we experimented with several back-translation methods  (Sennrich et al., 2016a) , including the beam search, un-restricted sampling and sampling-topK  (Edunov et al., 2018) , to leverage the target-side monolingual data. We also applied knowledge distillation  (Freitag et al., 2017)  to leverage the source-side monolingual data. Our systems followed four main steps:1) data filtering and preprocessing, 2) back-translation to generate pseudo bilingual data, 3) knowledge distillation by monolingual data, 4) fine-tuning with in-domain. It should be emphasized that we used Marian 1  (Junczys-Dowmunt et al., 2018)  to implement only for Hausa?English baseline systems, and Fairseq 2  (Ott et al., 2019)  for the rest, include Hausa?English back-translation and knowledge distillation models. 

 System Overview 

 Data Filtering and Preprocessing In this section, we discuss the preprocessing, normalization and filter techniques carried out in an attempt, in order to reduce spurious uncertainty in the modeling problem. 

 Text Preprocessing Generally, we carried out the following text preprocessing steps prior to use in every model:   (Koehn et al., 2007)  (normalize-punctuation.perl) script except for every language pair. ? Segmentation: Chinese was segmented using the Jieba 4 segmentation tool, and tokenizer using Moses (tokenizer.perl) script for English, German, French and Hausa. For the Hausa tokenizer, we used English tokenizer instead. ? True-case: The word, at the start of a sentence, containing only an initial capital letter was replaced with the capitalized variant. That occurred most frequently in other positions of the English monolingual training data. Thus, in the previous sentence, the initial token would be "words" rather than "Words". We used Moses' script for true-case. ? Subword: The neural machine translation system is capable of open-vocabulary translation by representing rare and unseen words as a sequence of subword units. The model was trained based on subword-nmt 5 on the parallel training corpus. 

 Data Filtering For all language pairs, the data filtering process for the training bilingual corpus stayed to the principle with the following rules: ? Filter out the sentence pairs that contain blank lines either from the source side or the target side. ? Filter out the sentence pairs that the source side and the target side at the same. ? Filter out the sentences with the length ratio falling outside from 0.4 to 2.5. ? Filter out the sentences whose punctuation and foreign words taking more than 40 percent. ? Remove the sentences which are longer than 200 words, or exceed a single word with 30 characters. ? Filter out the sentences which contain HTML tags or duplicated translations. ? Filter out the sentences which its word ratio between the source and the target exceeds 1:2.5 or 2.5:1. ? Identify language and delete foreign languages. Filter parallel and monolingual data by language detection using cld2 6 . The rules described above were also employed when cleaning monolingual and back-translation data. In the monolingual data particularly there were some lines that include two or more sentences, we cut them into several sentences by writing a script. 

 Data Augmentation 

 Back-Translation Back-translation  (Sennrich et al., 2016a ) is an essential method to integrate the target side monolingual synthetic knowledge when building a stateof-the-art neural machine translation system. Especially for low-resource language tasks, it's indispensable to augment the training data by mixing the pseudo corpus with the parallel part. In that the target side, lexicon coverage was insufficient. The nucleus sampling  (Holtzman et al., 2020)  in back-translation to generate more suitable pseudo bilingual sentences. We attempted several data augmentation methods as follow, with different single technologies or combinations. ? Beam search: Generated target translation by beam search with beam 5. ? Sampling: Selected a word randomly from the whole distribution in each step, which increases the diversity of pseudo corpus with low precision, compared with beam search. ? Sampling Top-K: Selected a word in a restricted way that only top-K (we set K as 16) words could be chosen. 

 Forward Translation to Generate Synthetic Parallel Sentence For Chinese?English tasks. To generate a more diverse pseudo-parallel corpus, we use forwardtranslated to do generated synthetic parallel sentences on source monolingual data only by our own ensemble model. 

 Knowledge Distillation We used knowledge distillation  (Kim and Rush, 2016)  to do distillation on the original dataset. Specifically, we translated the source-side of the bilingual data using previously trained proposal models, and generated distilled candidates. We then trained models on filtered data along with the original bilingual data and back-translation data. 

 Iterative Back-translation and Fine-tuning A process which iterative twice between backtranslation and fine-tuning was implemented by following steps for the low-resource Hausa?English tasks. 

 Reranking For German?English, French?German tasks, we followed noisy-channel  (Yee et al., 2019)  reranking using one neural language model and three reverse translation models. 3 Experiment 

 Experiment Settings In order to demonstrate the experiments of the system, there some experiment details should be clarified. To train all of the models used in our system, we made use only of the constrained data sets provided to shared news translation task participants. On the other side, the baseline models were trained on parallel corpus only by cleaned corpus. In terms of model evaluation, the main indicator for the report was calculated according to sacreBLEU 7  (Post, 2018)  based on the results which has been removed parts of post-preprocessing such as removed BPE symbols, detruecased, detokenized, etc. The Transformer-Small was implemented based on Marian (Junczys-Dowmunt et al., 2018) as our baseline for Hausa?English tasks. For Chinese?English, German?English and French?German tasks, we implemented the Transformer-Big FFN-8192 based on Fairseq  (Ott et al., 2019)  as our baseline model. We used Adam optimizer (Kingma and Ba, 2014) during training, learning rate was 5e-4, ? 1 = 0.9, ? 2 = 0.98, weight decay was 0.0001, label smoothing was 0.1. Specifically, the learning rate warmed up over the 8,000 steps for pre-normalize architectures Transformer-Big FFN-8192 model. The system shuffled the 7 https://github.com/mjpost/sacrebleu training data before generating the training batch for each epoch, so the document context information was not considered in this case. FP16 was applied to accelerate training with few performance damage during the training process. 

 Chinese?English For Chinese?English system, our parallel corpus included CCMT, wikititles-v3, wikimatrix-v1, para-crawl-v7.1, news-commentary-v16 corpus. While Chinese were segmented by Jieba word segmentation toolkit, English was tokenized by Moses tokenizer script. Based on the result of data Filtering, we used 17 million Chinese?English parallel data corpus for training the baseline model. As the next step after the preprocessing, we trained BPE  (Sennrich et al., 2016b ) models which were learned with 32,000 merge operations for joined English and Chinese on the parallel data. We built separately vocabularies for each language, and the final vocabulary size of Chinese was 42K and English was 22K. Baseline train data we followed drop-BPE  (Provilkov et al., 2020) . We trained the Transformer-Big FFN-8192 model for Chinese?English. For back-translation, we selected 20 million News Crawl 2020 English monolingual data for Chinese?English task. All News Crawl Chinese monolingual data and selected 20 million Extended Common Crawl Chinese monolingual data were combined for English?Chinese task. Backtranslation data were combined by sampling top-16 and beam search. At the same time, there was a combination between back-translation data and parallel data corpus in order to train Chinese?English models. We selected 10 million Chinese and English sentences respectively for forward translation and knowledge distillation to generate synthetic parallel sentences. Our final submissions consisted of three Transformer-Big FFN-8192 models with different configurations, using the beam search with a beam size of 5, and set lenpen 2.0. Table  1  shows that the translation quality was improved by using the proposed techniques. 

 Hausa?English The parallel corpus for Hausa?English system included para-crawl-v8, wikititles-v3, Khamenei and Opus corpus, which was tokenized by Moses tokenizer script. It should be clear that Hausa used tokenizer by English mode. After the data filter- ing, we used 550 thousand Hausa?English parallel data corpus for training the baseline model. A joint BPE model was applied with 10,000 merge operations. Moreover, shared vocabularies were selected for Hausa?English language pairs. We used Marian trained Transformer-Small 8 model for Hausa?English baseline, with learning rate ranging from 0.0008 to 0.001, warmup steps fixing at 48,000. Three models(3e3d, 4e4d, 6e4d) were trained under different architectures on single 2080Ti GPU. For English?Hausa back-translation, the standard Transformer-Big model implemented in Fairseq. We selected 4.5 million Hausa monolingual data by data filtering and language detection, and 20 million English monolingual data from the News Crawl 2020 were filtered as the backtranslation dataset. Every time we handled the back-translation process, the beam search was applied. Then the back-translation and the fine-tune were executed twice. For Hausa?English, due to time constraints, it was limited to one backtranslation and fine-tune. In the fine-tuning stage, 200 sentences from the newsdev2021 were kept randomly as the validation set, and other sentences were attributed to fine-tune the model. Table  2  shows that the translation quality was improved by using the proposed techniques. Our final submissions consisted of two Transformer-Big models. 

 German?English For German?English task, the provided parallel sentences were completely joined together so as to get about 95 million sentence pairs. Then, sentences with lots of punctuation masks and nonalpha-number characters were removed, as well as the sentences whose length ratio was larger than 2. As a result, 52 million sentences were selected to be candidates. After that, BPE was learned jointly with 32k as the merge operations, and the size of the vocabulary was 32,168. The model's parameters for both directions were copied from the Transformer-Big in the paper "Attention is all you need"  (Vaswani et al., 2017) . Finally, we got three English?German models and two English?German models for ensembling and reranking. The language model used for reranking was trained with GPT-3 using data cleaned from news 2020. All the models were trained using Fairseq. The overview of our German?English system is listed in Table  3 .   

 French?German For French?German task, about 7 million sentences were left after removing the sentences with invalid characters or punctuations from the original parallel sentences. We trained the BPE codes with 32k as the merge operations. The final vocabulary size for German was 32,144 and for French was 32,176. We introduced forward translation in German?French direction using models trained from the original parallel dataset. In both directions, the models were based on the Transformer-Big as the basic architecture. At last, three French?German models and two German?French models, trained from forwardtranslation, were applied to ensembling and reranking. The language model used for reranking was trained with GPT-3 using data cleaned from news 2020. The models from this system were completely trained by Fairseq. Check the overview of our German?French systems in  

 Conclusions This paper described the Mininglamp submissions to the WMT2021 eight news translation tasks, and our main exploration was using more diversified architectures, back-translation, fine-tuning and ensemble. We used a similar data preprocess and filtering strategy for all the tasks, containing statistical information-based rules. And we experimented with back-translation by different decoding strategies, using the Transformer-Small model and iterative between back-translation and fine-tuning for low-resource. Table 3 : 3 SacreBLEU scores on newstest2016 German?English tasks. Learning rate for training is 0.001 and warmup steps are 4000. 

 Table 4 . 4 System de-fr fr-de baseline 30.9 27.6 + Knowledge Distillation 31.3 - + Ensemble 32.6 28.8 + Reranking 34.1 30.9 

 Table 4 : 4 SacreBLEU scores on newstest2019French?German tasks. 

			 https://github.com/marian-nmt/marian 2 https://github.com/pytorch/fairseq 3 https://github.com/moses-smt/ mosesdecoder 

			 https://github.com/fxsjy/jieba 5 https://github.com/rsennrich/ subword-nmt 

			 https://github.com/CLD2Owners/cld2 

			 The dimension of word embedding was 256, the dimension of the feed-forward network was 1024, multi-head was 4, encoder and decoder layer was 4.
