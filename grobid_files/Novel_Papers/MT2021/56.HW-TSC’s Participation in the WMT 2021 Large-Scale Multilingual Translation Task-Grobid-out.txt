title
HW-TSC's Participation in the WMT 2021 Large-Scale Multilingual Translation Task

abstract
This paper presents the submission of Huawei Translation Services Center (HW-TSC) to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Small Track #2, including 6 languages: Javanese (Jv), Indonesian (Id), Malay (Ms), Tagalog (Tl), Tamil (Ta) and English (En) with 30 directions under the constrained condition. We use Transformer architecture and obtain the best performance via multiple variants with larger parameter sizes. We train a single multilingual model to translate all the 30 directions. We perform detailed pre-processing and filtering on the provided large-scale bilingual and monolingual datasets. Several commonly used strategies are used to train our models, such as Back Translation, Forward Translation, Ensemble Knowledge Distillation, Adapter Fine-tuning. Our model obtains competitive results in the end.

Introduction This paper introduces our submission to the WMT 2021 Large-Scale Multilingual Translation Task. We participate in Small Track #2, including 6 languages: Javanese (Jv), Indonesian (Id), Malay (Ms), Tagalog (Tl), Tamil (Ta) and English (En) with 30 directions. We consider that the officially provided dataset has the acceptable size and quality and therefore only participate in the constrained evaluation. Our method is mainly based on previous works but with fine-grained data cleaning techniques and a multi-step multilingual training strategy. For each language pair, we perform multi-step data cleaning on the provided dataset and only keep a high-quality subset for training. At the same time, several training strategies are tested in a pipeline, including Backward  (Edunov et al., 2018)  and Forward  (Wu et al., 2019a)  Translation, Multilingual Translation  (Johnson et al., 2017) , Iterative Joint Training  (Zhang et al., 2018) , Ensemble Knowledge Distillation  (Freitag et al., 2017; Li et al., 2019) , Adapter Fine-Tuning , and Ensemble  (Garmash and Monz, 2016) . Based on the task requirements, we train a single multilingual model that translates all 30 directions. We refer to  (Johnson et al., 2017)  and employ language tags  (Wu et al., 2021) . By combining multiple strategies, our model achieves considerable quality improvements in all directions. Section 2 focuses on our data processing strategies while section 3 describes our training techniques, including model architecture and the iterative training strategy, etc. Section 4 explains our experiment settings and training processes and section 5 presents our experiment results. 

 Data 

 Data Source For all language pairs, we follow the constrained data requirements and take full advantage of the bilingual and monolingual training data available. Table  1  lists the data sizes of each language pair before and after filtering. 

 Data Pre-processing We conduct the following steps to pre-process the data: ? Filter out repeated sentences  (Khayrallah and Koehn, 2018; . ? Convert XML escape characters. ? Normalize punctuations using Moses  (Koehn et al., 2007) . ? Delete html tags, non-UTF-8 characters, unicode characters and invisible characters. ? Filter out sentences with mismatched parentheses and quotation marks; sentences of which punctuation percentage exceeds 0.3; sentences with the character-to-word ratio greater than 12 or less than 1.5; sentences of which the source-to-target token ratio higher than 3 or lowers than 0.3; and sentences with more than 120 tokens. Based on our experience in the industry, this strategy can reduce the low-level errors in model inference and the problem of missing translations. ? Apply langid  (Joulin et al., 2016b,a)  to filter sentences in other languages. ? Use fast-align  (Dyer et al., 2013)  to filter sentence pairs with poor alignment. ? Use LaBSE  (Feng et al., 2020)  to rank and filter the monolingual data. Data sizes before and after cleaning are listed in Table  1 . 

 Data Selection According to , highresource language pairs may squeeze the living space of low-resource language pairs. In other words, different data sizes across languages may lead to uneven translation quality in a multilingual model. Since we incorporate all 30 directions in one multilingual model, this issues should be addressed. We use temperature sampling strategy  (Zoph et al., 2016)  with T=5 to over-sample the low-resource language pairs. We train all 30 directions under the constrained condition. To improve the performance of backtranslation, we combine officially provided monolingual data with the monolingual data extracted from corresponding bilingual corpora. Data sizes are listed in Table  1 . The detailed bilingual data size after forward translation and sampling back translation (FTST) and over-sampling are listed in Table  3 . 

 System Overview 

 Model Transformer  (Vaswani et al., 2017)  has been widely used for machine translation in recent years, which has achieved good performance even with the most primitive architecture without much modifications. Therefore, we choose to start from Transformer-Deep  (Sun et al., 2019)  and consider it as a baseline. The detailed model parameters are as follow: 35-layer encoder, 3-layer decoder, 512 hidden units and a batch size of 4096. We used the Adam optimizer (Kingma and Ba, 2014) with ? 1 = 0.9 and ? 2 = 0.98, and the same warmup and decay strategy for learning rate as  (Vaswani et al., 2017) , with 4,000 warmup steps. During training, we employ label smoothing with a value of 0.1  (Szegedy et al., 2016) . For evaluation, we use beam search with  a beam size of 4 and length penalty ? = 0.6  (Wu et al., 2016) . 

 Data Augmentation Back-translation  (Edunov et al., 2018 ) is an effective way to enhance translation quality by using monolingual sentences to generate synthetic training parallel data. As described in  (Wu et al., 2019b) , similar to back translation, the monolingual corpus in source language can also be used to generate forward translation text with a trained MT model, and the generated forward and backward translation data can both be merged with the authentic bilingual data. This strategy can increase the data size to a large extent. We take full advantage of the officially provided monolingual data for data augmentation. In terms of back translation, we adopt top-k sampling for high-resource languages, and adopt beam search for low-resource languages. With regard to forward translation, we translate monolingual data using beam search. Through sampling, we ensure that the sizes of data generated by forward and back translation are relatively equal. In this paper, we refer to the combination of forward and sampling back translation as FTST.  et al. (2017)  propose a simple solution to use a single neural machine translation model to translate among multiple languages, and the model requires no change to the model architecture. Instead, the model introduces an artificial token at the beginning of the input sentence to specify the required target language. According to  (Wu et al., 2021) , we add "2XX" (XX indicates the target language, e.g. 2id) at the beginning of the source sentence. All languages use a shared vocabulary. We train the hybrid SentencePiece model  (Kudo and Richardson, 2018)  in conjunction with all 6 languages as the shared word segmentation system for all language pairs. We keep the vocabulary within 40k, including tokens of all 6 languages (En/Id/Jv/Ms/Ta/Tl). 

 Multilingual Strategy 

 Johnson Two mainstream methods about multilingual training are available: two models with XX?Multi and Multi?XX separately and a mono Multi?Multi model. According to  (Johnson et al., 2017) , Multi?XX performs better than Multi?Multi and XX?Multi in general. Multi?Multi model contains too many languages pairs (30 in this case), so conflicts and confusions may occur among language pairs in different directions. However, due to the requirements of the task, we need to provide a Multi?Multi model that includes all 30 directions. In our experiment, we divide 30 language pairs into five Multi?XX multilingual models as step 1. Than we use five Multi?XX multilingual models to conduct backtranslation and train a Multi?Multi model as step 2 and step 3, as shown in Figure  1 . 

 Iterative Joint Training Zhang  et al. (2018)  propose a new iterative joint training method, that is, using monolingual data from both source and target sides to train a sourceto-target (forward) model and a target-to-source (backward) model at the same time. The two models generate synthetic data for each other. The advantage of such method is that both of the two models gain improvement after each iteration with the synthetic data provided by the other, and then can generate synthetic data with higher quality. Such training procedure is repeated after the two models converge. 

 Language independence Adapter Fine-tuning Previous works demonstrate that fine-tuning a model with in-domain data could effectively improve the model performance. However, due to limitations of a multilingual translation model, once the model is trained, when fine-tuning one of the language pairs, the performance of others will go worse. Thanks to the finding of Adapter , we are able to fine-tune each language pair without impacting the performance of others. In the experiment, we set the adapter size to 512 and fine-tune the model on the bilingual data for each language pair in 30 directions with 3,000 tokens per batch for one epoch. 

 Ensemble Knowledge Distillation (EKD) Ensemble Knowledge Distillation  (Freitag et al., 2017; Li et al., 2019)  improves the performance of a student model by distilling knowledge from a group of trained teacher models. Comparing with some soft label distillation methods, the EKD for NMT is relatively straightforward, which can be implemented by training the student models on the combination of the original training set and the translation from the ensembled teacher model on the training set. In our experiments, we ensemble models as the teacher model to translate the FLORES dev set, and use the translation results to further fine-tune models. 

 Ensemble Model ensemble is a widely used technique in previous WMT workshops  (Garmash and Monz, 2016) , which can improve the performance by combining the predictions of several models at each decoding step. els with different architectures to further improve system performances. 4 Experiment Settings 

 Settings We use the open-source fairseq  (Ott et al., 2019)  for training and SentencePieceBLEU to measure system performances. Each model is trained using 8 GPUs. The architectures and main parameters we used are described in section 3.1. Marian  (Junczys-Dowmunt et al., 2018)  is used for decoding during inference. 

 Training Process We employ iterative training and phase-based data augmentation. Figure  1  shows our training process in details. The specific steps are as follows: 1) Process data using methods described in section 2.2. Train one Multi?Multi model as baseline and five Multi?XX models as forward models and backward models. 2) Generate back translation and forward translation data. Mix the data with parallel training data and train second round five Multi?XX models. 3) Generate back translation and forward translation data using models trained in step 2. Mix data with bilingual training data and train three Multi?Multi models. 4) Average the last eight checkpoints of each model and adapter fine-tune it with bilingual data. Ensemble models to produce the final system. 

 Results and analysis We use methods described in Section 2.2 for data processing. Model architecture mentioned in Section 3.1 is employed to increase system diversity. On the basis of Multi?Multi baselines model, we use FTST data augmentation to further enhance model performance. Table  2  lists the results of our experiment on FLORES dev set and devtest set  (Goyal et al., 2021) . Comparing with the baseline model, the first round FTST Multi?XX models leads to 1.3 BLEU increase on average for the 30 directions. Further, the second round FTST achieves 1.2 BLEU increase on average. We fine-tune the model using bilingual data with adapter and achieve 0.8 BLEU increase on average. Finally, ensemble further leads to 0.5 BLEU increase. When submitting the final results, because of time limits, we only finish round-two FTST. As for model inference, there is a problem with our fairseq architecture, resulting in poor model quality that seriously affects the FTST results. The final model we submitted achieves 28.64 BLEU on FLORES dev and 28.34 BLEU on FLORES devtest. After the submission, we fixed the problem and continued our experiments, eventually achieving 30.7 BLEU on on FLORES dev and 30.9 BLEU on FLORES devtest. The detailed experiment results are listed in Table  4 . In our experiment, due to the inference problem mentioned above, we have not seen much performance improvements. The low quality of model inference leads to poor FT results, which made no contributions to the model. And even worse, it offsets the gain brought by BT results to the model. We also found that the Multi?en model does surpass the Multi?Multi model in quality, which is the same as the results observed by the industry. 

 Conclusion This paper presents the submissions of HW-TSC to the WMT 2021 Large-Scale Multilingual Translation Task. We perform experiments with a series of pre-processing and training strategies. The effectiveness of each strategy is demonstrated. We finally achieve competitive results. Figure 1: This figure shows the training process for the WMT 2021 Large-Scale Multilingual Translation Task, which consists of three stages. In stage 1, one Multi?Multi model as baseline and five Multi?XX models are trained. In stage 2, the synthetic data by forward and sampling back translation (FTST) is used to train the second round Multi?XX models. In stage 3, second round synthetic FTST data is used to train three Multi?Multi models. Finally, adapter fine-tune and model ensemble are used to enhance the performance. train Multi2Multi Baseline Model Bilingual Corpus train train train FT data FT data Multi2Multi Model Multi2Multi Model Multi2Multi Multi2XX Model Multi2XX Model Multi2XX Model Multi2XX Model Multi2XX Model Multi2XX Model Multi2XX Model Model Mono Corpus train train Adapter Fine-Tune Ensemble Official BT data BT data Provided Corpus stage 1 stage 2 stage 3 
