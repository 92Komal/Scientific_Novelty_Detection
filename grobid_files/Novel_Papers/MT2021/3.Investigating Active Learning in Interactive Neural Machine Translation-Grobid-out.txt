title
Investigating Active Learning in Interactive Neural Machine Translation

abstract
Interactive-predictive translation is a collaborative iterative process, where human translators produce translations with the help of machine translation (MT) systems interactively. Various sampling techniques in active learning (AL) exist to update the neural MT (NMT) model in the interactive-predictive scenario. In this paper, we explore term based (named entity count (NEC)) and quality based (quality estimation (QE), sentence similarity (Sim)) sampling techniques -which are used to find the ideal candidates from the incoming data -for human supervision and MT model's weight updation. We carried out experiments with three language pairs, viz. German-English, Spanish-English and Hindi-English. Our proposed sampling technique yields 1.82, 0.77 and 0.81 BLEU points improvements for German-English, Spanish-English and Hindi-English, respectively, over random sampling based baseline. It also improves the present state-of-the-art by 0.35 and 0.12 BLEU points for German-English and Spanish-English, respectively. Human editing effort in terms of number-of-words-changed also improves by 5 and 4 points for German-English and Spanish-English, respectively, compared to the state-of-the-art.

Introduction Neural machine translation (NMT) requires a significantly large amount of in-domain data for building the robust systems. Absence of sufficient training samples often result in the generation of erroneous output samples. Post-editing could be an effective solution in this situation, where human interference may help to rectify the errors in the output samples. However, there are two problems, viz. (i) post-editing a large number of output samples is time consuming and not very efficient in terms of productivity and (ii) not including all the post-edited examples might pose the risk of encountering the same mistakes in future. Hence, there is a necessity that instead of post-editing all the output samples, we explore effective sampling techniques for selecting important samples for post-editing, and further these post-edited samples are used to update the model's parameter following an active learning technique that makes the translation model learns from these (new) samples. Interactive MT (IMT) is viewed as an effective mean to increase the productivity in the translation industry. In principle, IMT aims to reduce human effort in automatic translation workflows by employing an iterative collaborative strategy with its two most important components, the human agent and the MT engine. As of today, NMT models  (Bahdanau et al., 2015; Vaswani et al., 2017)  represent state-of-the-art in MT research. This has led researchers to test interactive-predictive protocol on NMT too. Papers  (Knowles and Koehn, 2016; Peris et al., 2017)  that pursued this line of research suggest that NMT is superior than phrase-based statistical MT  (Koehn et al., 2003) . So use of interactive NMT (INMT) for output sample correction can significantly reduce the overall translation time and active learning strategy can use human corrected samples for adapting the underlying NMT model so that in future, the model does not repeat previous errors and improves the translation quality. The contributions of our current work are stated as follows: ? We propose term based (NEC) and quality based (QE and Sim) sampling techniques that provide us with the ideal source samples which are first post-edited using interactive NMT (INMT) and then used to update the Transformer  (Vaswani et al., 2017)  based NMT model. ? With the help of the proposed sampling techniques, we significantly reduce human efforts in correcting the hypothesis in terms of token replacements using this proposed INMT model. 

 Related Work In a case, where an MT model is not providing high quality translation due to low resource or out-of-domain scenarios, it could be beneficial to update the model with new samples while preserving the previous knowledge too. There has been some works which deal with the large input data streams but generally adopt the incremental learning approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature  (Levenberg et al., 2010; Denkowski et al., 2014) , authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model.  Turchi et al. (2017)  applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data.  Nepveu et al. (2004) ; Ortiz-Mart?nez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning  (Olsson, 2009; Settles, 2009) . This unbounded data stream scenario was explored by  Haffari et al. (2009) ;  Bloodgood and Callison-Burch (2010) , where a pool of data was edited and the SMT model was updated using this data.  Gonz?lez-Rubio et al. (2011)  used the stream data to update the SMT model. Further, interactive paradigm of SMT was introduced in Gonz?lez-  Rubio et al. (2012); Gonz?lez-Rubio and Casacuberta (2014) . Later, the NMT became more prominent and efficient in the interactive paradigm of MT  (Knowles and Koehn, 2016; Peris et al., 2017) .  Peris and Casacuberta (2018)  explored the application of active learning and IMT on the NMT model. They performed the experiments over the attention based encoder-decoder NMT model  (Bahdanau et al., 2015) . To handle the Source aunque nunca jugu? un juego de beber basado en el tema nazi . Reference never played a Nazi themed drinking game though . Initial Hypothesis never played a Nazi drinking play there . Hypo-1 never played a Nazi themed play though . Hypo-2 never played a Nazi themed drinking though . Hypo-3 never played a Nazi themed drinking game though . Table  1 : Hypothesis correction and translation in INMT process. Here, Hyposhows the step by step correction by user to achieve reference/desired sentence incoming and unlabelled data stream, they introduced the sampling techniques which are majorly attention and alignment based. We explore the sampling criteria on the basis of lexical properties (term-based) and semantic properties (quality-based). We observe the impact of the proposed sampling techniques over the Transformer based NMT. 

 Interactive Neural Machine Translation In INMT  (Knowles and Koehn, 2016; Peris et al., 2017) , human translators correct errors in automatic translations in collaboration with the MT systems. Here, users read tokens of the generated hypothesis from left to right and modifies (insert/replace) his/her choice of words in the hypothesis generated by the NMT model. From the start index to the right most token position where the user make change is considered as the 'validated prefix'. After the user makes any change, the model regenerates a new hypothesis by preserving the validated prefix and new tokens next to it. Multiple attempts of token replacements may be required by a user to get the desired output as shown by an example in Table  1 . For an input-output sentence pair [x, y], where x = (x 1 , x 2 , ..., x m ) being a sequence of input tokens and y = (y 1 , y 2 , ..., y n ) being a sequence of output tokens, the probability of the ith translated word y i is calculated as in Eq. (1): p(y i |y 1 , ..., y i?1 , x) = f (y i?1 , s i , c i ) (1) Here, s i and c i are the i th decoder hidden state and context vector, respectively. As shown in Eq. (1), in NMT, during decoding, next predicted output y i depends on model's previous output y 1 , ..., y i?1 . In INMT, y i will be generated by considering y * 1 , ..., y * i?1 as the previous tokens, where y * i?1 is actually the token of user's choice at sequence position i ? 1. Eq. (2) shows the conditional probability of generating y i in the INMT scenario. p(y i |y * 1 , ..., y * i?1 , x) = f (y * i?1 , s i , c i ) (2) 4 Sampling Techniques From Figure  1 , we see that the sampling module selects and recommends the incoming inference samples to the INMT for supervision. The purpose of a sampling technique is to filter out the ideal candidate from the incoming inference samples for which the trained NMT model is most uncertain and by supervising that sample it should increase the NMT performance using the technique of AL. Let S be the input sentences for inference, B be the block of sentences that are taken from S iteratively. From the block B, C a chunk, the size of which depends on the percentage (%) of the samples from B are taken, is used to be supervised from the human. We take the size of B as 10,000 samples and the chunk size from B can be 20, 40, 60 and 80%.   Testset 59,975 (newscommentary) 51,613 (newscommentary) 47,999 (ILCI corpus)   Table 2 : Size of the corpora used for the experiments sampling (which labels those instances for which the model is least certain about the correct output to be generated) and query-by-committee (QbC) (where a variety of models are trained on the labeled data, and vote on the outputs of unlabeled data; label those instances which the committee disagrees the most). Hence, the objective of the sampling techniques as mentioned below is to select from the unbounded data stream S, those sentences S ( ?S) which are worth to be used to update the parameters p of the NMT model. 

 Random Sampling (RS) In RS, samples from the unlabelled block are taken without any criteria or uncertainty metric. Even though random sampling has no logically involved concept still it is expected to produce good and diverse samples from this sampling. We consider random sampling as the baseline for the proposed sampling techniques. 

 Quality Estimation (QE) Quality estimation (QE) is the process of evaluating the MT outputs without using gold-standard references. This requires some kind of uncertainty measure which indicates the confidence that the model has in translating the sentence. It uses human translation edit rate (HTER) score evaluation metric. The HTER score is generally used to measure human effort in editing (insert/replace/delete) the generated hypothesis  (Specia et al., 2018) . we use this as a confidence score of the translation model. A high HTER scored translation can be seen as a bad translation which requires more human effort for editing and a low HTER scored translation can be seen as a good translation which requires less human effort for editing. We did QE sampling using the Openkiwi toolkit  (Kepler et al., 2019) . Openkiwi provides the pre-trained QE models for language pairs (like English-German). We use one of the pre-trained models to obtain the HTER (uncertainty measure or score s i ) for every sentence S i in the S data stream. In our case, the high HTER score is the sampling criteria. For every input sentence, this tool takes two inputs which are source sentence and translation of the source sentence generated by the initial NMT model and gives us the estimated HTER score. For a test sentence S i in S where (1 ? i ? |S|) (|S| = number of sentences in S), quality estimation (QE) pre-trained model takes S i and its generated translation T i , and returns the corresponding HTER score HT ER i . 

 Sentence Similarity (SS) Here, we calculate the similarity between the source sentence and its round trip translation (source-to-target and again target-to-source translation)  (Moon et al., 2020) . We explore the similarity based sampling criteria since the quality of the round trip translation depends on the two intermediate translations i.e. forward translation (source-to-target) and back-translation (target-to-source). In case of a weak NMT model (i.e. MT system that does not generate high quality translations; e.g. say in low resource scenario or translating out-of-domain data), it is unlikely that a generated round-trip translation would be closer to the source sentence. As for the RTT setup, we had to train forward-and back-translation models. In this case, a low similarity score is the criteria for sampling. We calculated similarity between sentences in the following manner: (1). similarity between the semantic form of the sentences and (2). similarity between the lexical (surface) form of the sentences. 

 Similarity Based on Nearest Sentence Embedding (Sim emb ) On completion of RTT, the RTT-ed sentence may be different from the original source sentence but semantically similar to it, which is not captured by surface level metrics such as BLEU. In fact, we need information about the semantics of both source and back translation. 'Similarity based on sentence embedding' (Sim emb ) as the name itself suggests, this sampling technique uses a cosine similarity measure based on sentence embeddings. For every input sentence, two embeddings are generated: 1) embedding of the source sentence and 2) embedding of the RTTed sentence of the source sentence. These embeddings are generated using S-BERT  1 Reimers and Gurevych (2019) . Sentences having the least similarity scores in the block are sampled and supervised by the user. 

 Similarity based on Edit distance between sentences (Sim f uzzy ) This similarity is a surface level similarity method and it does not take into account the semantics of the source and back translated sentences. In this sampling technique the similarity measure/score is based on the 'levenshtein-distance' between the source sentence and the round-trip translation of the source sentence. For every test sentence the similarity score (Sim f uzzy ) between the sentence and round-trip translation is calculated using 'fuzzywuzzy' toolkit 2 which is based on the levenshtein-distance and generates a score between 0-100 (0 and 100 are the lowest and highest similarity level). The sentences having the least score in the block are considered for supervision. 

 Named Entity Counting (NEC) The NMT model suffers with the vocabulary restriction problem due to the limitation over the decoder side vocabulary size  (Sennrich et al., 2016) . Named entities (NEs) are open vocabularies and it is not possible for the NMT model to have all the NEs in the decoder vocabulary. Therefore, we considered presence of NEs as one of the sampling criteria. In other words, we took inability of the NMT model to translate the NEs perfectly into account for sampling. We count the NE tokens in each source sample of the incoming inference data and the sentences having the most number of NE tokens in the block are considered as "difficult to translate" by the NMT model, and hence filtered for supervision. We use Spacy 3 named entity recognizer (NER) for marking NEs in sentences from English, German and Spanish languages. 

 Query-by-committee (QbC) Here, we combine the opinions of the random and the proposed sampling techniques to filter out the input samples for human supervision. Like  Peris and Casacuberta (2018) , we use a voted entropy function as in Eq. (  3 ) to calculate the highest disagreement among the sampling techniques for a sample x. In the given Eq. (  3 ), #V(x) is the number of sampling techniques voted for x to be supervised. C denotes the number of all the sampling techniques participating in the voting process. C QbC (x) = ?#V (x) |C| + log #V (x) |C| (3) 

 Attention Distraction Sampling (ADS) Attention distraction sampling (ADS) is introduced by  Peris and Casacuberta (2018) . Attention based NMT distributes the weights over the source tokens based on their contribution in generating a target token. If the system finds the translation of a sample uncertain then the attention probability distribution features like the uniform distribution. It shows that NMT model is having difficulty in distributing weights over the source tokens based on their contribution in target generation. The samples having highest distraction are selected for active learning. The kurtosis of weights given by the attention model while generating y i is calculated to measure the attention distraction. Kurt(y i ) = 1 |x| |x| j=1 (? i,j ? 1 |x| ) 4 ( 1 |x| |x| j=1 (? i,j ? 1 |x| ) 2 ) 2 (4) Here, ? i,j is the attention weight between the j-th source word and i-th target word. Note that, the fraction 1 |x| is equivalent to the mean of the attention weights of the word y i . Finally, The kurtosis values for all the target words are used to obtain the attention distraction score. 

 Dataset We carried out experiments on three language pairs using three benchmark datasets. Table  2  shows the statistics of training, development and test sets used for our experiments. In order to measure performance of the proposed sampling techniques, we use different domain datasets for training and testing. For German-English and Spanish-English, we use Europarl corpus  (Koehn, 2005)  for training and News-Commentary (NC) corpus for testing. This gives us a clear indication whether the translation models trained over Europarl corpus are able to adapt over the sampled examples from NC corpus using active learning. Similarly, for English-Hindi translation, we use the IITB corpus  (Kunchukuttan et al., 2018)  for training which is a combination of sentences from government sites, ted talks, administration books etc. As for evaluation, we use the ILCI corpus  (Jha, 2010)  which is a combination of sentences from the health and tourism domain. 

 Experimental Setup Our experiments were based on the Transformer NMT model  Vaswani et al. (2017) . We used 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer  (Kingma and Ba, 2015)  was used for training with 8,000 warm up steps. We used the BPE  (Sennrich et al., 2016)  with a vocabulary size of 40K. Models were trained with OpenNMT toolkit 4  (Klein et al., 2020)  with batch size of 2,048 tokens till convergence and checkpoints were created after every 10,000 steps. During inference, beam size is set to 5. We measured BLEU (calculated with multi-bleu.pl script)  (Papineni et al., 2002)  of the trained models on the test sets. 

 Results and Analysis We evaluate the impact of the proposed sampling techniques for active learning in NMT in two different ways. Firstly, we test whether the proposed techniques help the NMT model to improve its translation performance in terms of the BLEU score. Secondly, in order to see whether the proposed techniques are able to reduce the human efforts (number of token correction required) in correcting the hypothesis, we compare the performance of the proposed   

 Effect on Translation Quality We consider the random sampling-based method as a baseline model. By increasing the amount of the supervised samples of the block recommended by the proposed sampling techniques with 20, 40, 60 and 80%, we observed changes in the BLEU score. The BLEU scores presented are calculated based on a single block of 10,000 sentences. Table  3  shows the BLEU scores for different translation directions. We also present the charts (see Figure  2 ) to illustrate the effect of the sampling techniques on the translation quality of the NMT model for the specific translation directions using AL. As can be seen from Figure  2 , for English-to-German translation, the initial BLEU score of the trained NMT model before active learning was 23.28. By adapting the trained NMT to the new samples recommended by the random sampling, the BLEU score increases upto 25.31 (when 80% of the samples of block are supervised) which is 2.03 BLEU points improvement over the initial score. Compared to the random sampling, the proposed sampling techniques QE, Sim emb , Sim f uzzy and NEC yield  26.17, 26.90, 26.68 and 26.84  BLEU scores, respectively, by supervising 80% of the samples in the block. Here, we can see that Sim emb performs the best and achieves 26.90 which is 1.59 BLEU more than that we obtain with the random sampling method (baseline). We also tested a combined opinion of sampling techniques (i.e. QbC) and it outperformed the other methods and produced 27.13 BLEU points, which is a 1.82 BLEU improvement over the one that we obtained after applying the random sampling method. For German-to-English translation, we observed the BLEU score of 24.08 without using any active learning. The baseline INMT system (i.e. based on random sampling method) brought about 27.05 BLEU points on the test set. The INMT system with sentence-similarity sampling feature (i.e. Sim emb ) surpassed the baseline by 0.94 BLEU points. Furthermore, the QbC method outperforms all the other sampling methods, and with this, we achieve 28.13 BLEU points (an improvement of 1.08 points over the random sampling technique) on the test set. In case of English-to-Hindi translation, the initial BLEU score was observed to be 25.76. Here, NEC was found to be the best performing sampling method.  with this method statistically significantly outperforms the baseline INMT system (built on the random sampling method), and we obtain an improvement of 0.81 BLEU points over the baseline. The statistical significance test is performed using the bootstrap resampling method  Koehn (2004) . For Spanish-to-English translation, the initial BLEU score was found to be 38.76. The baseline sampling strategy provided us with 40.87 BLEU points on the test set. As in Englishto-German, QbC is found to be the best performing sampling method, and provides us a gain of 0.81 BLEU points over the baseline. It is to be noted that for Spanish-to-English translation, Sim emb also yields the comparable score to that of one by QbC. Furthermore, in Figure  2 , we demonstrate the performance of different sampling techniques in AL for the German-to-English, English-to-German, English-to-Hindi and Spanishto-English translation. The x-axis of the graphs in Figure  2  represents the amount (%) of the samples supervised in the block and the y-axis represents the BLEU scores. For English-to-Hindi, the baseline INMT model (i.e. random sampling) produces 26.83 BLEU points on the test set, which corresponds to an absolute improvement of 1.07 BLEU points over the vanilla  NMT system (i.e. 25.76 BLEU points) . NEC is found to be the best-performing sampling technique, and yields 27.64 BLEU points with an absolute improvement of 0.82 BLEU points over the baseline (random sampling). As for Spanish-to-English translation, we see that Sim emb significantly outperforms the random sampling by 0.77 BLEU points. Furthermore, for English-to-German, English-to-Hindi and Spanish-to-English, the respective best-performing sampling techniques, which are our proposed methods, bring about gains over ADS  (Peris and Casacuberta, 2018 ) by 0.35, 0.06 and 0.12 BLEU scores. These improvements are very small and except English-to-German, the re- maining two improvements are not statistically significant 5 . However, in the next section, we will see that our proposed sampling techniques outperform ADS significantly in terms of human effort reduction. 

 Effect on Human Effort We check if the proposed sampling techniques in AL are helpful to reduce the human effort in correcting (supervising) the generated hypothesis. For interaction between the user and the MT system, we used an INMT system which generates the hypothesis based on the NMT models adapted over the samples recommended by the sampling techniques in AL. Due to the high cost of involving humans in the performance evaluation, we measure the human effort in a reference-simulated environment, where the reference sentences are considered as the user's choice of sentences. The idea is to correct the hypothesis until it matches the reference sentence. Using different sampling techniques, we aimed at improving the translation quality of the NMT system. We recorded performance of the INMT system in terms of the model's ability to predict the next word at decoding. Every time the user modified hypothesis is fed to the NMT model, the model predicts next correct token based on the modifications made by the user. We calculate the model's accuracy in predicting the next words using a commonly-used metric: word prediction accuracy (WPA) metric. WPA is the ratio of the number of correct tokens predicted and the total number of tokens in the reference sentences  Peris et al. (2017) . Higher the WPA scores of the NMT model means the lesser human efforts in correcting the hypothesis. We also calculated human efforts using another metric: word stroke ratio (WSR). WSR is the ratio of the number of tokens corrected by the user and the total number of tokens present in the reference sentences  Knowles and Koehn (2016) . In our case, we investigated whether the proposed sampling techniques are able to reduce human efforts in translation (i.e. lower WSR and higher WPA scores are better). Table  4  shows WPA scores of our INMT systems in different translation tasks. Here, we showed the WPA scores only when 80% of the samples in the block are supervised. We considered random sampling as the baseline and compared it with the QbC since we found that it is the best performing approach out of all proposed sampling techniques (i.e. Sim, NEC, Fuzzy) as far as WPA is concerned. In sum, the interactive-predictive translation setup with QbC surpassed the baseline setup by 5.67%, 4.85%, 8.32% and 4.24% accuracies in terms of WPA for the English-to-German, German-to-English, English-to-Hindi and Spanish-to-English translation tasks, respectively. In Figure  3 , we show WSR scores obtained by the different sampling techniques. As above, we considered varying sizes of sentences for supervision, i.e. 20, 40, 60 and 80% of the samples are supervised in a block. We calculated average number of total tokens replaced in the hypotheses generated by the NMT models adapted over the samples recommended by the sampling techniques. The x-axis of the graphs shows the % of samples supervised and y-axis shows  the average number of tokens replaced. As can be seen from the graphs, for English-to-German translation, QbC achieves statistically significantly absolute improvement of 1.82 BLEU points over the baseline. As for English-to-Hindi and Spanish-to-English, NEC and Sim emb yield 0.81 and 0.77 BLEU improvements over the baseline. We also observed the reduction of human efforts in terms of word stroke ratio (WSR). For English-to-German, English-to-Hindi and Spanish-to-English, we achieve a reduction in WSR of 9%, 23% and 10% over the baseline. We also present the scores that were shown in graphs in Table  3 . We see that for English-to-German translation, QbC performs the best with respect to WSR reduction. For German-to-English, QbC and Sim emb are found to be the best-performing strategies. For English-to-Hindi and Spanish-to-English, along with the QbC, the second best-performing sampling techniques are NEC and Sim emb , respectively. Unlike German-to-English and Spanish-to-English, for English-to-Hindi, Sim emb is not the best-performing method. We observed that there may be some reasons for this: (i) morphological richness of Hindi, and (ii) syntactic divergence of English and Hindi languages. These might introduce more challenges in RTT in case of Sim emb . We also compared the amount of human effort reduction by the proposed techniques and ADS. For English-to-German, English-to-Hindi and Spanish-to-English translation, we observed the reduction in WSR by 5, 7 and 4 points, respectively, over the ADS. 

 Conclusion In this paper, we have explored the applicability of various sampling techniques in active learning to update the NMT models. We select the incoming source samples using the sampling techniques, correct them in an interactive NMT scenario and subsequently update the trained NMT model using the corrected parallel samples. It helps the model to adapt over the new parallel samples which results in improving the translation quality and reducing the human ef-fort for further hypothesis correction. We proposed term based (NEC) and quality based (QE, Sim emb , Sim f uzzy ) sampling techniques to pick the source samples from a large block of input sentences for correction and subsequently updating the NMT models. Since it is not feasible for a human to supervise (modify) a large set of input data coming for the translation, the proposed sampling techniques help to pick and recommend the suitable samples from large input data to the user for supervision. We measure the impact of sampling techniques by two criteria: first, improvement in translation quality in terms of BLEU score and second, reduction in human effort (i.e. number of tokens in generated outputs needed to correct). We performed experiments over three language pairs i.e. English-German, English-Spanish and English-Hindi. We use different domain data for training and testing the NMT model to see if the NMT model trained over the data from one domain can successfully adapt to the different domain data. We empirically showed that the proposed term and quality based sampling techniques outperform the random sampling and outperformed the attention distraction sampling (ADS) method 9 Acknowledgement Figure 1 : 1 Figure 1: A pipeline showing the flow of data through sampling module, model updation through active learning. 
