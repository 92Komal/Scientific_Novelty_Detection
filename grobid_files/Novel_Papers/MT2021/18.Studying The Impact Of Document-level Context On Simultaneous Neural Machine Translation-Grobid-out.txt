title
Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation

abstract
In a real-time simultaneous translation setting, neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences, making them harder to translate, leading to poor translation quality. Previous research has shown that document-level NMT, comprising of sentence and context encoders and a decoder, leverages context from neighbouring sentences and helps improve translation quality. In simultaneous translation settings, the context from previous sentences should be even more critical. To this end, in this paper, we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the Asian Language Treebank (ALT) and OpenSubtitles2018 corpora, where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does, in fact, benefit from context but is unable to effectively leverage it, especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.

Introduction Neural machine translation (NMT)  (Bahdanau et al., 2015; Luong et al., 2016)  is an end-to-end approach known to give the state of the art results for a variety of language pairs. In standard NMT, the entire source language sentence is fed to the model, and once the entire target language sentence is generated, it is presented to the user. However, in a real-time translation setting, translation models are expected to present translated words or phrases as they are generated. Furthermore, waiting for the entire source language sentence adds to the latency, and therefore an optimal solution is to have a model that can start generating target language words right after the first few source language words are available for translation. This is known as simultaneous NMT (SNMT) and is known for its poor translation quality, especially in lowresource settings. The concept of waiting for k words or tokens before generating target language words or tokens is known as wait-k SNMT  (Ma et al., 2019) . In this paper, we work with the Transformer architecture as the standard NMT model, consisting of a bidirectional encoder and unidirectional decoder. The decoder is able to attend to all source language tokens when generating target language tokens. However, in the case of the wait-k SNMT model, the standard encoder and decoder are replaced with their SNMT equivalents, which are a unidirectional encoder and a modified decoder, respectively. The decoder can only look at i + k ? 1 encoder tokens when predicting the i th token. We are aware of a previous work that has shown that using an image as an additional modality can help improve translation quality in a wait-k setting when k is a small value around 1 to 4  (Imankulova et al., 2020; Caglayan et al., 2020) . The additional image modality provides the model with a form of context which helps disambiguate hard-to-translate phenomena, especially when needed information is not available yet during translation. An additional image modality may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality  (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017) . The simplest documentlevel NMT architecture involves using an additional encoder that encodes the context sentences, following which the encoded context is used to augment the representation of the sentence to be translated  (Zhang et al., 2018) . Just like using an image as a modality helps enrich the encoding of the sentence with additional disambiguation information, the context sentences might also contain such useful information. We already know that in an SNMT setting, due to partial sentences being translated, the amount of context available to the decoder is limited, and thus leveraging the context sentences should significantly boost SNMT translation quality. This motivated us to combine document-level NMT with SNMT leading to document-level SNMT. Our document-level SNMT architecture is simple, where we have a sentence encoder, context encoder, and a decoder except that the sentence encoder and decoder are wait-k SNMT equivalents of the standard encoder and decoder. We experiment with a high-resource OpenSub-titles2018 dataset for English?Russian and Russian?English translation and a low-resource ALT document-level dataset for English?Japanese and Japanese?English translation. Our observations show that document-level context helps improve translation slightly in both settings but not by a large margin. We then perform a statistical and manual analysis of the translations where we observe that while SNMT models definitely benefit from context, they are unable to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 

 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with fixed or adaptive policies  (Zheng et al., 2019b) . Adaptive policy decides whether to READ another source word or WRITE a target word in one model  (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015) . Most dynamic models with adaptive policies  (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a Zheng et al., ,c, 2020a  focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile,  Ma et al. (2019)  proposed a simple wait-k method with fixed policy, where the decoder starts generating the target language tokens the moment k source language tokens are available. However, their model for simultaneous translation relies only on the source sentence. This research concentrates on the wait-k approach leveraging document-level information from previous context sentences. Document-level NMT leverages context beyond the current sentence in order to improve translation quality  (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Voita et al., 2018 Voita et al., , 2019 Zheng et al., 2020b; Fernandes et al., 2021) . Document-level NMT models can be implemented as a post-processing model or context-aware model. The post-processing models use an additional module to use context on generated translations  (Xiong et al., 2019; Voita et al., 2019) . However, post-processing generated translations may lead to higher latency, which is counter-intuitive in a simultaneous translation scenario. On the other hand, context-aware models leverage additional context during translation. For example,  Tiedemann and Scherrer (2017)  proposed to simply concatenate the previous sentences in both the source and target side to the input to the system.  Jean et al. (2017) ;  Bawden et al. (2018) ;  Zhang et al. (2018)  use separate context encoder for a few previous source sentences. Similarly, we also use a separate context encoder to extract document-level information. However, we incorporate document-level information into SNMT in order to improve translation quality, where only information from the source sentence is insufficient during translation. 

 Methods 

 Background: Wait-k Simultaneous NMT The most straightforward approach for SNMT is the wait-k approach  (Ma et al., 2019)  with a fixed policy. As tokens are fed to the encoder one at a time, we have to rely on a unidirectional encoder that cannot attend to future tokens. Once the encoder has been fed k tokens, the decoder starts generating a token at a time. This means that at the i th decoding step, the encoder and decoder can only see the first k + i ? 1 encoder token representations. Once the whole input sentence is available, wait-k behaves like regular NMT except with a unidirectional encoder. Different from  (Ma et al., 2019)  we have a unidirectional encoder, so when a new source token arrives, the encoder representations for the previous tokens are not updated. This can have a minor impact on the overall translation quality, but this paper aims to understand how context affects SNMT. 

 Background: Document-level NMT Suppose X, X c and Y are the source sentence, context sentences, and the target sentence. In this paper, we work with SNMT, and hence X c only consists of past sentences, which for simplicity we concatenate into a single long context sentence 1 . Document-level NMT involves using X and X c together for translation. In the case when only X and Y are available, X is fed to an encoder (E), leading to a sentence encoding E(X). This sentence encoding is then attended to by the decoder in order to produce the translation Y ? = D(E(X)). When X c is available we encode it using a context encoder (E c ) leading to context encoding E c (X c ) which is then used for translation along with E(X) as Y ? = D(E(X), E c (X c )). It is a common practice to share the parameters of the sentence and context encoders. A key component of document-level NMT is the incorporation of E c (X c ) into the framework by combining it with E(X). This paper considers two simple approaches, which we dub as "multi-source" (MS) and "context-attention" (CA). 

 MS: Multi-Source Based Context Incorporation This method treats the context as an additional source of information similar to the setting in multi-source NMT  (Zoph and Knight, 2016; Dabre et al., 2017) . In multi-source NMT, the decoder is modified to attend to multiple source sentences, and this approach should help incorporate context into the decoding process. For vanilla NMT, the cross attention mechanism of the decoder takes in E(X) and produces a weighted representation, the attention, A. Given the context encoding E c (X c ) we additionally compute the context attention A c . We combine A and A c into A comb , the context augmented attention, using a simple gating mechanism as A comb = ? * A + (1 ? ?) * A c where ? = sigmoid(W comb * [A : A c ]). [:] indicates concatenation of representations along the hidden layer axis. W comb is the weight matrix of size   [2h, h]  where h is the model's hidden size. ? is a weight that can help interpolate A and A c to determine the balance between them. 

 CA: Context Attention Based Context Incorporation This method is same as the one in  Voita et al. (2018) . Where the multi-source approach involves combining E(X) and E c (X c ) in the decoder by combining the attentions obtained from them (A and A c ), this approach combines E(X) and E c (X c ) into a single E comb (X, X c ) which is then fed to the decoder. Thus, the decoder sees one encoder representation instead of two. To combine E(X) and E c (X c ), E(X) is fed to a self-attention layer which gives E sa (X) and E c (X c ) is fed to a cross-attention layer where EX is the query and E c (X c ) is the key/value which gives E ca,c (X c ). By doing so, E sa (X) and E ca,c (X c ) have the same shape and can be combined via the gating mechanism in the previous section into E comb (X, X c ). Apart from these two combination methods, there are several others  (Libovick? et al., 2018)  which we will explore in the future. 

 Our Method: Document-level SNMT Document-level NMT can be easily extended to document-level SNMT by enforcing the SNMT constraint on the sentence encoder E and the sentence cross-attention mechanism A. No such constraints are placed on the context encoder E c . Refer to Figure  1  for a simple overview of our method. It shows that at the i th decoding step, the decoder and encoder can access the context representations fully but only k + i ? 1 source sentence representations. 

 Experimental Settings We describe experimental settings aimed at helping verify the degree to which document context helps improve translation quality in a simultaneous translation setting. 

 Datasets and preprocessing We experimented with English?Russian and Russian?English translation using a corpus created by  (Voita et al., 2018) , derived from the OpenSubtitles2018 corpus, consisting of 1.5M training sentences where each sentence has 3 sentences as context. The development and test sets consist of 10,000, 4 sentence documents leading to a total of 40,000 sentences which can have up to 3 context sentences. This dataset belongs to the spoken language domain, where we expect that document context should be very helpful in improving translation quality. Given that Russian has flexible word order, missing information in an incomplete source sentence can be complemented via the context. We also experimented with the low-resource Asian Language Treebank (ALT )dataset  (Riza et al., 2016) , which contains sentence level aligned document pairs split into training/development/test sets of 18,088/1,000/1,018 lines spanning 1,698/98/97 documents, respectively. We experimented with English?Japanese and Japanese?English translation. Japanese has subject-object-verb word order, whereas English has subject-verbobject, so we expect document context to be helpful whenever the object or verb-related information is missing for incomplete sentences in an SNMT setting. Regarding preprocessing, we segmented the Japanese source sentences using MeCab, and our NMT implementation handles other preprocessing, such as subword tokenization. When providing document context sentences to our models, we concatenate previous N context sentences to form a single long sentence before feeding it to the model along with the sentence to be translated. Naturally, the first sentence of the document will have no context sentence, which we designate with a special token < EM P T Y >. 

 Implementation and Training Details We modified the Transformer  (Vaswani et al., 2017)  implementation in tensor2tensor v1.15.4 2 , which has an internal subword segmentation mechanism. We set the separate source and target subword vocabulary sizes of 8,000 for the ALT dataset and 32,000 for the OpenSubtitles2018 dataset. We use hyperparameters of the "transformer base" model for English?Russian and Russian?English translation whereas for English?Japanese and Japanese?English translation we use the "transformer base single gpu" model hyperparameters. The "transformer base" models are trained on 8 NVIDIA V100 GPUs, whereas the"transformer base single gpu" models are trained on a single NVIDIA V100 GPU. We save and evaluate our models on the development set every 1000 batches with BLEU  (Papineni et al., 2002)  as the evaluation metric. We train our models till the BLEU score does not increase for ten consecutive evaluations. We average the last ten saved checkpoints and then decode the model. As we work in a simultaneous translation setting, greedy search makes sense as tokens should be output one at a time 3 . 

 Models Compared We train and compare the following types of full sentence and wait-k SNMT models for both datasets: 1. Non-contextual models: where the document context is not used 2. Contextual models: which use up to N previous sentences as context. N = 1 for English?Japanese 4 and N = 1, 2, 3 for English?Russian. 

 Results We describe the results of our experiments in resource-rich and resource-poor settings. 2 https://github.com/tensorflow/tensor2tensor/tree/v1.15.4 3 It's possible to consider a sophisticated beam search method, but that is beyond the scope of this paper.  4  In reality, we had experimented with N = 2, but found out that the translation quality, measured in BLEU, dropped. We suspect that this is because either the model ends up paying unnecessary attention to the context or that the low-resource setting hinders the model from learning how to utilize context effectively. Ultimately we feel that N = 1 is a practical choice for the ALT dataset because it contains sentences with around 20 words on average. The longer the context sentence, the more computations the cross attention mechanism has to make, which slows decoding, which is ultimately what we are trying to avoid via SNMT while incorporating context. We were able to consider all 3 context sentences for English?Russian because each sentence was substantially smaller, which does not impact decoding time as badly. In the future, we can consider sparse attention mechanisms such as locality sensitive hashing, which is used in the Reformer  (Kitaev et al., 2020)  Table  2 : BLEU scores for English?Japanese and Japanese?English translation using the ALT corpus. Results are presented for full sentence and SNMT models using either no context or up to 1 context sentence (CS = 0, 1). CT indicates the document context incorporation technique which can be MS (Multi-Source) or CA (Context Attention). For each type of model (full sentence or wait-k) for a language pair, we mark the best scores in bold. 

 Resource Rich English?Russian translation Table  1  gives the BLEU scores for English?Russian translation. 

 Non-contextual: Full Sentence versus SNMT models Regarding the baselines, it is clear that the SNMT models with small wait-k's give poor translation quality as compared to the full sentence models. Increasing the value of wait-k naturally improves the translation quality, where a value of k = 8 leads to results that are within 1 BLEU of the results of the full sentence models. Given that the average sentence length for the Russian-English dataset is approximately 8 words, it makes sense that K = 8 would give the best results. 

 Context incorporation technique: Multi-Source (MS) versus Context Attention (CA) The results show that there is no clear answer as to which of MS or CA is superior, which makes both viable solutions for incorporating context into the NMT model. For the remainder of the results section, the BLEU scores we quote will be for the MS approach. Looking at the results, it will be clear that the trends in the improvement of translation quality by incorporating context are similar regardless of the use of MS or CA. 

 Non-contextual versus Contextual Full-Sentence models Next, when context sentences are used for full sentence translation for Russian?English, the quality for when up to 1, 2, and 3 previous sentences as context are used is  35.2, 35.5, and 35.7, respectively.  Compared to a baseline score of 34.9, the improvements are 0.3, 0.6, and 0.8 BLEU. Similarly, for English?Russian, compared to a baseline score of 26.7, using up to 1, 2, and 3 previous sentences as context lead to translation quality improvements of 0.3, 0.5, and 0.5, respectively. We performed statistical significance testing  (Koehn, 2004)  which showed that all improvements are significant 5 at p < 0.05. This shows that context certainly helps in a spoken language domain, and as the number of context sentences grows, the translation quality also grows steadily. 

 Non-contextual versus Contextual SNMT models Comparing the wait-k non-contextual model against contextual models using up to N context sentences shows that, once again, context is helpful in an SNMT setting. When using up to 3 context sentences, for wait-k values of 1, 2, 4, 6 and 8, the BLEU score improvements over their non-contextual counterparts are 0.6, 0.5, 0.8, 0.9, 1.0, respectively, for Russian?English translation. Similarly for the reverse direction the improvements are 0.3, 0.3, 0.2, 0.3, 0.6. One important observation is that the improvements are almost proportional to the value of wait-k. As we wait for more source language tokens, the impact of the previous sentences as context seems to be higher. This makes sense because the importance of the context is determined using a gating mechanism, and the more information we have about the current sentence, the better the gating mechanism will be at determining what part of the context should be used. Finally note the maximum gain for SNMT models using up to 3 context sentences which is 1.0 for Russian?English and 0.6 for English?Russian. Compared to the full sentence models, the corresponding gains are 0.8 and 0.5. Previously we have seen that a difference of 0.1 BLEU is sufficient for it to be statistically significant, which means that SNMT models experience significantly larger improvements in translation quality when compared to their full sentence counterparts.  

 Resource Poor English?Japanese translation Table  2  gives the BLEU scores for English?Japanese translation. Looking at the absolute BLEU scores shows that context does lead to minor improvements in translation quality regardless of a full sentence or SNMT models. Unfortunately, the improvements are not statistically significant. Although we do not show it here, using additional context sentences led to a drop in translation quality. We suppose that this may be either due to the low-resource nature of the ALT dataset or perhaps there are not many cases where context should be helpful. Note that our context NMT model takes a weighted average of the attentions of the current and the context sentence, and so the translation quality may degrade if there are very few cases where context is needed. To this end, we decided to perform a statistical and manual analysis of the models for English?Japanese translation. 

 Analysis 

 Translation of Context-Aware Tokens We investigate whether SNMT performance is improved by using contextual information. Therefore, we created context-aware parallel data in which the target sentence contains the tokens related to the previous target sentence. For example, given the context source sentence "The 2008 Taipei Game Show, organized by the Taipei Computer Association (TCA), ended on Monday, and was different from shows of past years.", and the source sentence "This could be seen in the gaming population, industry, and exhibition arrangements." in a simultaneous manner, the generated target sentence should be "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?". Here, "?" means "game" and it is a token related to the context. The context sentence contains information about the game, and this can help translate "?" that appears at the beginning of the target sentence, where it is not available yet from the source sentence (e.g., k < 6). We randomly investigated such sentence pairs from the Table  3 : Translation examples generated by non-contextual models as well as the contextual models using one previous sentence as context and the multi-source (MS) context incorporation method. Sentences in parentheses are the English meanings of the translation results. test data of WAT data and extracted 50 of them. Using BLEU and accuracy, calculated by the sum of correctly translated sentences that include the token that needs context to be translated, divided by the number of sentences, we evaluate whether the performance of the SNMT model is improved by using the context. Figure  2  shows that BLEU and accuracy results for contextual models, using up to one or two previous sentences 6 as context, for created context-aware parallel data. In BLEU, it can be seen that the results are almost the same between the non-contextual and the contextual models. On the other hand, the results of accuracy differ between the non-contextual and the contextual models. In particular, accuracy is improved by considering the context at k = 1, 2, and 4. From this result, it can be seen that tokens related to the context can be translated by considering the context in SNMT. Our analysis also leads us to believe that it is difficult for BLEU to evaluate the improvement due to the context because BLEU was not designed in that way. This shows that there is a need for context-aware evaluation mechanisms. 

 Examples of Translations In order to understand how the translation quality is improved by using context, we analyze the following translations: Table  3  shows the translation examples generated by non-contextual as well as the contextual models using one previous sentence as context and the multi-source (MS) context incorporation method. The "Saudis" contained in the context sentence is thought to be helpful when translating "?" which means "Saudi Arabia" in the source sentence. If k is 4 or less, "Saudi Arabia" will not be seen by the decoder. Since the translation result of k = 8 and the full sentence is the same, it can be seen that the effect of the missing words is almost eliminated when is large in wait-k. "Saudi Arabia" was not translated with k = 2 without context, but it was correctly translated using the contextual model. From this translation example, we can see that the context helps to translate the words related to it. However, given that the overall corpus level BLEU does not show a large amount of improvement, we suspect that the current context incorporation mechanisms are not good at determining when the context should and should not be used. This means that we need to design better context relevance mechanisms. 

 Conclusion We proposed wait-k document-level simultaneous NMT to complement the information of incomplete input during the translation process. Our proposed method is to replace the source encoder and target language decoder with wait-k equivalents while keeping the context encoder. The experimental results show that the proposed method slightly improves the translation quality in high-resource settings but not by appreciable amounts in low-resource settings. The analysis showed that wait-k models are more context-aware and rely on context whenever it should be helpful. However, the current model is unable to successfully determine when the context should be used, preventing the successful utilization of context. This indicates that we need to investigate further more effective ways to utilize the previous sentences in the document as context. Our human evaluation was also rather limited, and in the future, we plan to conduct a human evaluation to determine which kind of context-aware phenomena (pronoun disambiguation, word sense disambiguation) our approaches can address. Figure 1 : 1 Figure 1: A simplified overview of our simultaneous document level NMT model which uses previous source sentences as context. 
