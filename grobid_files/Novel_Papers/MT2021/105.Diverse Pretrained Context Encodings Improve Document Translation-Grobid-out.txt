title
Diverse Pretrained Context Encodings Improve Document Translation

abstract
We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pretrained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pretrained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context. Our best multicontext model consistently outperforms the best existing context-aware transformers. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR.

Introduction Generating an adequate translation for a sentence often requires understanding the context in which the sentence occurs (and in which its translation will occur). Although single-sentence translation models demonstrate remarkable performance  (Chen et al., 2018; Vaswani et al., 2017; Bahdanau et al., 2015) , extra-sentential information can be necessary to make correct decisions about lexical choice, tense, pronominal usage, and stylistic features, and therefore designing models capable of using this information is a necessary step towards fully automatic high-quality translation. A series of papers have developed architectures that permit the broader translation model to condition on extra-sentential context  Miculicich et al., 2018) , operating jointly on multiple sentences at once  (Junczys-Dowmunt, 2019) , or indirectly conditioning on target side document context using Bayes' rule  (Yu et al., 2020b) . While noteworthy progress has been made at modeling monolingual documents  (Brown et al., 2020) , progress on document translation has been less remarkable, and continues to be hampered by the limited quantities of parallel document data relative to the massive quantities of monolingual document data. One recurring strategy for dealing with this data scarcity-and which is the basis for this work-is to adapt a sentence-level sequence-to-sequence model by making additional document context available in a second stage of training  (Maruf et al., 2019; Miculicich et al., 2018; Haffari and Maruf, 2018) . This two-stage training approach provides an inductive bias that encourages the learner to explain translation decisions preferentially in terms of the current sentence being translated, but these can be modulated at the margins by using document context. However, a weakness of this approach is that the conditional dependence of a translation on its surrounding context given the source sentence is weak, and learning good context representations purely on the basis of scarce parallel document data is challenging. A recent strategy for making better use of document context in translation is to use pretrained BERT representations of the context, rather than learning them from scratch . Our key architectural innovation in this paper is an architecture for two-staged training that enables jointly conditioning on multiple context types, including both the source and target language context. Practically, we can construct a weak context representation from a variety of different contextual signals, and these are merged with the source sentence encoder's representation at each layer in the transformer. To examine the potential of this architec-ture, we explore two high-level research questions. First, using source language context, we explore the relative impact of different kinds of pretraining objectives on the performance obtained (BERT and PEGASUS), the amount of parallel document training data required, and the size of surrounding context. Second, recognizing that maintaining consistency in translation would seem to benefit from larger contexts in the target language, we compare the impact of source language context, target language context, and context containing both. Our main findings are (1) that multiple kinds of source language context improves performance of document translation over existing contextual representations, especially those that do not use pretrained context representations; (2) that although fine-tuning using pretrained contextual representations improves performance, large performance is strongly determined by the availability of contextual parallel data; and (3) that while both source and target language context provide benefit, source language context is more valuable, unless the quality of the target language context translations is extremely high. 

 Model Description Our architecture is designed to incorporate multiple sources of external embeddings into a pretrained sequence-to-sequence transformer model. We execute this by creating a new attention block for each embedding we wish to incorporate and stack them. We then insert this attention stack as a branching path in each layer of the encoder and decoder. The outputs of the new and original paths are averaged before being passed to the feed forward block at the end of the layer. Details are discussed below ( ?2.4), and the architecture is shown in Figure  1 . The model design follows the adapter pattern  (Gamma et al., 1995) . The interface between the external model and translation model takes the form of an attention block which learns to perform the adaptation. The independence between the models means that different input data can be provided to each, which enables extra information during the translation process. In this work, we leverage this technique to: (1) enhance a sentence-level model with additional source embeddings; (2) convert a sentence-level model to a document-level model by providing contextual embeddings. Like BERTfused , we use pretrained masked language models to generate the external embed-dings. 

 Pre-Trained Models We use two kinds of pretrained models: BERT  (Devlin et al., 2019)  and PEGASUS . Although similar in architecture, we conjecture that these models will capture different signals on account of their different training objectives. BERT is trained with a masked word objective and a two sentence similarity classification task. During training, it is provided with two sentences that may or may not be adjacent, with some of their words masked or corrupted. BERT predicts the correct words and determining if the two sentences form a contiguous sequence. Intuitively, BERT provides rich word-in-context embeddings. In terms of machine translation, it's reasonable to postulate that BERT would provide superior representations of the source sentence and reasonable near sentence context modulation. On the other hand, we expect it to fail to provide contextual conditioning when the pair of sentences are not adjacent. This shortcoming is where PEGASUS comes in. PEGASUS is trained with a masked sentence objective. During training, it is given a document that has had random sentences replaced by a mask token. Its task is to decode the masked sentences in the same order they appear in the document. As a result, PEGASUS excels at summarization tasks, which require taking many sentences and compressing them into a representation from which another sentence can be generated. In terms of providing context for document translation, we conjecture that PEGASUS will be able to discover signals across longer ranges that modulate output. 

 Embedding Notation To keep track of the type of embeddings being incorporated in a particular configuration, we use the notational convention Model Side (Inputs). ? Model: B for BERT, P for PEGASUS, and D for Document Transformer . ? Side: s for the source and t for the target language. ? Inputs: c for the current source (or target), i.e., x i , p for the previous source (target), and n for the next one. Note that 3p means the three previous sources (targets), (x i?3 , x i?2 , x i?1 ). ? When multiple embeddings are used, we include a ? to indicate the order of attention operations. We can thus represent the BERT-fused document model proposed by  as B s (p,c) since it passes the previous and current source sentences as input to BERT. 

 Enhanced Models The core of this work is to understand the benefits that adding a diverse set of external embeddings has on the quality of document translation. To this effect, we introduce two new models that leverage the output from both BERT and PEGASUS: Multi-source := B s (c) ? P s (c) Multi-context := B s (p,c) ? B s (c,n) ? P s (3p,c,3n) There are a few ways to integrate the output of external models into a transformer layer. We could stack them vertically after the self-attention block  or we could place them horizontally and average all of their outputs together like MAT  (Fan et al., 2020) . Our preliminary experiments show that the parallel attention stack, depicted in Figure  1 , works best. Therefore, we adopt this architecture in our experiments. 

 Parallel Attention Stack If we let A = B s (p,c), B = B s (c,n), and C = P s (3p,c,3n) refer to the output of the external pretrained models computed once per translation example, then the Multi-context encoder layer is defined as R = AttnBlock(E ?1 , E ?1 , E ?1 ) S a = AttnBlock(A, A, E ?1 ) S b = AttnBlock(B, B, S a ) S = AttnBlock(C, C, S b ) T = DropBranch(R , S ) training 1 2 ? (R + S ) otherwise E = LayerNorm(FeedForward(T )) + T The intermediate outputs of the attention stack are S a ? S b ? S . To reproduce BERT-fused, we remove S a and S b from the stack and set S directly to AttnBlock(A, A, E ?1 ). We use attention block to refer to the attention, layer normalization, and residual operations,  (Fan et al., 2020)  is defined as AttnBlock(K, V, Q) = LayerNorm(Attn(K, V, Q)) + Q While drop-branch DropBranch(M, N) = 1(u ? .5) ? M + 1(u < .5) ? N where u ? Uniform(0, 1) and 1 is the indicator function. 3 Experiment Setup 

 Datasets We evaluate our model on three translation tasks, the NIST Open MT Chinese-English task, 1 the IWSLT'14 English-German translation task, 2 and the WMT'14 English-German news translation task.  3  Table  1  provides a breakdown of the type, quantity, and relevance of the data used in the various dataset treatments. NIST provides the largest amount of in domain contextualized sentence pairs. IWSLT'14 and WMT'14 are almost an order of magnitude smaller. See Appendix A for preprocessing details. NIST Chinese-English is comprised of LDC distributed news articles and broadcast transcripts. We use the MT06 dataset as validation set and MT03, MT04, MT05, and MT08 as test sets. The validation set contains 1,649 sentences and the test set 5,146 sentences. Chinese sentences are frequently underspecified with respect to grammatical features that are obligatory in English (e.g., number for nouns, tense on verbs, and dropped arguments), making it a common language pair to study for document translation. IWSLT'14 English-German is a corpus of translated TED and TEDx talks. Following prior work , we use the combination of dev2010, dev2012, tst2010, tst2011, and tst2012 as the test set which contains 6,750 sentences. We randomly selected 10 documents from the training data for validation. We perform a data augmentation experiment with this dataset by additionally including news commentary v15. We denote this treatment as IWSLT+ and consider this to be out of domain data augmentation. 

 1302 

 PEGASUS Enc BERT Encoder layer Decoder layer WMT'14 English-German is a collection of web data, news commentary, and news articles. We use newstest2013 for validation and newstest2014 as the test set. For the document data, we use the original WMT'14 news commentary v9 dataset. We run two document augmentation experiments on this dataset. The first, denoted as WMT+, replaces news commentary v9 with the newer news commentary v15 dataset. The second augmentation experiment, denoted as WMT++, builds on the first by additionally incorporating the Tilde Rapid 2019 corpus. The Rapid corpus is comprised of European Commission press releases and the language style is quite different from the style used in the News Commentary data. For this reason, we consider Rapid to be out of domain data for this task. 

 Training We construct enhanced models with additional attention blocks and restore all previously trained parameters. We randomly initialize the newly added parameters and exclusively update these during training. For a given dataset, we train a model on all the training data it is compatible with. This means that for document-level models, only document data is used, while for sentence-level models both document and sentence data is used. In our work, this distinction only matters for the WMT'14 dataset where there is a large disparity between the two types of data. Transformer models are trained on sentence pair data to convergence. For NIST and IWSLT'14 we use transformer base while for WMT'14 we use transformer big. We use the following vari- The ratio of in domain vs out of domain data per training batch was tuned on the validation set for each treatment. We used the dataset descriptions to determine the domain. For example, IWSLT'14 is a dataset of translated TED talks so we considered News Commentary data which is composed of translated news articles to be out of domain for this task. Dataset In Domain Out Domain Sent Doc Sent Doc NIST 1.45M 1.45M - - IWSLT 173K 173K - - IWSLT+ 173K 173K 345K 345K WMT 4.7M 200K - - WMT+ 4.85M 345K - - WMT++ 4.85M 345K 1.63M 1.63M ants of BERT from Google Research GitHub: 4 BERT-Base Chinese on NIST, BERT-Base Uncased on IWSLT'14, and BERT-Large Uncased (Whole Word Masking) on WMT'14. We pretrain three PEGASUS base models for the languages en, de, and zh using the Multilingual C4 dataset as detailed in TensorFlow's dataset catalog.  5  When training our models, we only mask a single sentence per training example and do not include a masked word auxiliary objective. We use the public PEGASUS large 6 on the English side of WMT'14, for everything else, we use our models. See Appendix B for batch size and compute details. 

 Evaluation To reduce the variance of our results and help with reproducibility, we use checkpoint averaging. We select the ten contiguous checkpoints with the highest average validation BLEU. We do this at two critical points: (1) with the transformer models used to bootstrap enhanced models; (2) before calculating the validation and test BLEU scores we report. We use the sacreBLEU script  (Post, 2018)  7 on our denormalized output to calculate BLEU. 

 Results In this section, we present our main results and explore the importance of each component in the multi-context model. Additionally, we investigate the performance impact of document-level parallel data scarcity, the value of source-side versus targetside context, and the importance of target context quality. Table  2  compares our Multi-source and Multicontext models to baselines of related prior work, transformer  (Vaswani et al., 2017) , document transformer , and the BERT-fused model for machine translation . We see that a multi-embedding model outperforms all the single embedding models in each of the datasets we try. However, the best multiembedding configuration varies by dataset. We find that incorporating target-side context does not improve performance beyond using source-side context alone. We will present our ablation studies in the subsequent sections to further shed light on the causes of this pattern of results. To preserve the value of test set, we report results on the validation set for these experiments. 

 Source Context vs. Target Context In some language pairs, the source language is underspecified with respect to the obligatory information that must be given in the target language. For example, in English every inflected verb must have tense and this is generally not overtly marked in Chinese. In these situations, being able to condition on prior translation decisions would be valuable. However, in practice, the target context is only available post translation, meaning there is a risk of cascading errors. In this section, we seek to answer two questions: (1) how does the quality of target context affect document-level translation; (2) whether incorporating high-quality target context into source only models adds additional value. To answer the first question, we evaluate the target context model P t (3p,3n) using various translations as context. Table  3  shows the BLEU scores achieved by the target context models on the validation set. The lowest quality context comes from using the output of the baseline transformer model to furnish the context (valid BLEU of 48.76); the middle level comes from a model that conditions on three views of source context (valid BLEU of 52.8) and the third is an oracle experiment that uses a human reference translation. We see that the Table  3 : The value of using context on the target side of a translation is dependent on its quality. We test this in the limit by providing oracle context, which uses one of the references as context. We report BLEU scores on the validation set. The numbers in the second column are the BLEU scores of the translations used as the context, indicating the quality of the context. BLEU score improves as the quality of the target context improves; however, the impact is still less than the Multi-context source model-even in the oracle case! Next, we explore whether leveraging both source and target context works better than only using source context. To control for the confounding factor of target context quality, we remove one of the references from the validation dataset and use it only as context. We believe this provides an upper bound on the effect of target context for two reasons: (1) it's reasonable to assume that, at some point, machine translation will be capable of generating human quality translations; (2) even when this occurs, we will not have access to the style of a specific translator ahead of time. For these reasons, we calculate BLEU scores using only the three remaining references. We can see in  

 Context Ablation To assess the importance of the various embeddings incorporated in the Multi-context model, we perform an ablation study by adding one component at a time until we reach its full complexity. Table  5  shows the study results. We can see that much of the improvement comes from the stronger sentencelevel model produced by adding BERT's encoding of the source sentence-a full 2.25 BLEU improvement. The benefit of providing contextual embeddings is more incremental, yet consistent. Adding the previous sentence gives us 0.44 BLEU, adding additional depth provides another .49, and including the next sentence adds .37. Finally, adding PEGASUS' contextual embedding on top of all this results in a boost of .49. Holistically, we can assign 2.45 BLEU to source embedding enrichment and 1.59 to contextual representations. 

 Data Scarcity NIST is a high resource document dataset containing over 1.4M contextualized sentence pairs. In  this section, we investigate to what extent the quantities of parallel documents affect the performance of our models. To do so, we retrain enhanced models with subsets of the NIST training dataset. It is important to note that the underlying sentence transformer model was not retrained in these experiments meaning that these experiments simulate adding document context to a strong baseline as done in  Lopes et al. (2020) . Figure  2  shows the BLEU scores of different models on the NIST validation set with respect to the number of contextualized sentences used for training. We can see that it requires an example pool size over 300K before these models outperform the baseline. We conjecture that sufficient contextualized sentence pairs are crucial for document-level models to achieve good performance, which would explain why these models don't perform well on the IWSLT'14 and WMT'14 datasets. Further, this pattern of results helps shed light on the inconsistent findings in the literature regarding the effectiveness of document context models. A few works  (Kim et al., 2019; Lopes et al., 2020)  have found that the benefit provided by many document context models can be explained away by factors other than contextual conditioning. We can now see from Figure  2  that these experiments were done in the low data regime. The randomly initialized context model needs around 600K training examples before it significantly outperform the baseline, while the pretrained contextual models reduce this to about 300K. It is important to note that none of the conextual models we tried outperformed the baseline below this point. This indicates that data quantity is not the only factor that matters but it is a prerequisite for the current class of document context architectures. 

 Document Data Augmentation We further validate our hypothesis about the importance of sufficient contextualized data by experimenting with document data augmentation, this time drawing data from different domains. We augment the IWSLT dataset with news commentary v15, an additional 345K document context sentence pairs, and repeat the IWSLT experiments. During training, we sample from the datasets such that each batch contains roughly 50% of the original IWSLT data. To ensure a fair comparison, we first finetune the baseline transformer model on the new data, which improves its performance by 1.61 BLEU. We use this stronger baseline as the foundation for the other models and show the results in Table  6 . Although Multi-context edges ahead of Multi-source, the significance lies in the relative impact additional document data has on the two classes of models. The average improvement of the sentence-level models is 1.58 versus the 1.98 experienced by the document models.  Huo et al. (2020)  observed a similar phenomenon when using synthetic document augmentation. This further emphasizes the importance of using sufficient contextualized data when comparing the impact of various document-level architectures, even when the contextualized data is drawn from a new domain. 

 Three Stage Training WMT'14 offers an opportunity to combine the insights gained from the aforementioned experiments. This dataset provides large quantities of sentence pair data and a small amount of document pair data. Not surprisingly, both BERT-fused 8 and Multi-context struggle in this environment. On the other hand, Multi-source benefits from the abundance of sentence pair data. In order to make the most of the training data, 

 1306 Impact of Data Scarcity    we add a third stage to our training regime. As before, in stage one, we train the transformer model with the sentence pair data. In stage two, we train the Multi-source model also using the sentence pair data. In stage three, we add an additional P s (3p,3n) attention block to the Multi-source model and train it with document data. We perform two document augmentation experiments. In the first, we replace news commentary v9 with v15. In the second, we train on a mix of news commentary v15 and Tilde Rapid 2019. The optimal mix was 70% and 30% respectably, which we found by tuning on the validation dataset. For each of the augmentation experiments, we created new Multi-source baselines by fine-tuning the original baseline on the new data. When training these new baselines we only updated the parameters in the B s (c) and P s (c) attention blocks. In contrast, when training the treatment models, we froze these blocks and only updated the parameters in the P s (3p,3n) block. In this way, both the new baselines and treatments started from the same pretrained Multi-source model, were exposed to the same data, and had only the parameters under investigation updated. We see in Table  7  that this method can be used to provide the document-level model with a much stronger sentence-level model to start from. As we saw in the previous data augmentation experiments ( ?4.4), document augmentation helps the documentlevel model more than the sentence-level model. It is interesting to note that out of domain document data helps the document-level model yet hurts the sentence-level model. 9 

 Related Work This work is closely related to two lines of research: document-level neural machine translation and representation learning via language modeling. Earlier work in document machine translation exploits the context by taking a concatenated string of adjacent source sentences as the input of neural sequence-to-sequence models ( Scherrer, 2017). Follow-up work adds additional context layers to the neural sequence-to-sequence models in order to have a better encoding of the context information  Miculicich et al., 2018, inter alia) . They vary in terms of whether to incorporate the source-side context  (Bawden et al., 2018; Miculicich et al., 2018)  or target-side context  (Tu et al., 2018) , and whether to condition on a few adjacent sentences  (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Miculicich et al., 2018)  or the full document (Haffari and  Maruf, 2018; Maruf et al., 2019) . Our work is similar to this line of research since we have also introduced additional attention components to the transformer. However, our model is different from theirs in that the context encoders were pretrained with a masked language model objective. There has also been work on leveraging monolingual documents to improve document-level machine translation. Junczys-Dowmunt (2019) creates synthetic parallel documents generated by backtranslation  (Sennrich et al., 2016; Edunov et al., 2018)  and uses the combination of the original and the synthetic parallel documents to train the document translation models.  Voita et al. (2019)  train a post-editing model from monolingual documents to post-edit sentence-level translations into document-level translations.  Yu et al. (2020b,a)  uses Bayes' rule to combine a monolingual document language model probability with sentence translation probabilities. Finally, large-scale representation learning with language modeling has achieved success in im-proving systems in language understanding, leading to state-of-the-art results on a wide range of tasks  (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; McCann et al., 2017; Chronopoulou et al., 2019; Lample and Conneau, 2019; Brown et al., 2020) . They have also been used to improve text generation tasks, such as sentence-level machine translation  (Song et al., 2019; Edunov et al., 2019;  and summarization  (Zhang et al., 2019 Dong et al., 2019) , and repurposing unconditional language generation  (Ziegler et al., 2019; de Oliveira and Rodrigo, 2019) . Our work is closely related to that from , where pretrained largescale language models are applied to documentlevel machine translation tasks. We advance this line of reasoning by designing an architecture that uses composition to incorporate multiple pretrained models at once. It also enables conditioning on different inputs to the same pretrained model, enabling us to circumvent BERT's two sentence embedding limit. 

 Conclusion We have introduced an architecture and training regimen that enables incorporating representations from multiple pretrained masked language models into a transformer model. We show that this technique can be used to create a substantially stronger sentence-level model and, with sufficient document data, further upgraded to a document-level model that conditions on contextual information. Through ablations and other experiments, we establish document augmentation and multi-stage training as effective strategies for training a document-level model when faced with data scarcity. And that source side context is sufficient for these models, with target context adding little additional value. ? PEGASUS De small -Generated subword vocabulary of 34K tokens from the WMT'14 dataset. ? Transformers -Generated joint subword vocabulary of 34K tokens for NIST & WMT'14 and 20K for IWSLT'14. 
