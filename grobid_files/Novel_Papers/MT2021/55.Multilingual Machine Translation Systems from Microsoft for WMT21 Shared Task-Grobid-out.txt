title
Multilingual Machine Translation Systems from Microsoft for WMT21 Shared Task

abstract
This report describes Microsoft's machine translation systems for the WMT21 shared task on large-scale multilingual machine translation. We participated in all three evaluation tracks including Large Track and two Small Tracks where the former one is unconstrained and the latter two are fully constrained. Our model submissions to the shared task were initialized with DeltaLM 1 , a generic pre-trained multilingual encoder-decoder model, and finetuned correspondingly with the vast collected parallel data and allowed data sources according to track settings, together with applying progressive learning and iterative backtranslation approaches to further improve the performance. Our final submissions ranked first on three tracks in terms of the automatic evaluation metric.

Introduction Recently, multilingual neural machine translation has attracted lots of attention because it enables one model to translate between multiple languages  (Dong et al., 2015; Johnson et al., 2017; Arivazhagan et al., 2019; Dabre et al., 2020; Philip et al., 2020; Lin et al., 2021) . To improve the performance of the multilingual translation models, there are various approaches on the training methods  (Aharoni et al., 2019; Wang et al., 2020a,c) , the model structures  (Wang et al., 2018; Gong et al., 2021; Zhang et al., 2021a) , and the data augmentation  (Tan et al., 2019; Pan et al., 2021) . M2M  (Fan et al., 2020)  leverages the large-scale data mined from the web data and explore the strategies to scale the model size and train the model effectively. Meanwhile, the multilingual pre-trained language models have proven beneficial for the multilingual machine translation models. mBART  (Liu et al., 2020)  pre-trains a multilingual model with the multilingual denoising objective to improve the multilingual machine translation. 1 https://aka.ms/deltalm In this work, we explore the effects of different advanced approaches for multilingual machine translation models, especially on the large-scale dataset. We first explore the way to leverage the pre-trained language models that have been trained with large-scale monolingual data. We use the public available DeltaLM-Large checkpoint to initialize the model. DeltaLM  (Ma et al., 2021 ) is a multilingual pre-trained encoder-decoder model, which has been proven useful for multilingual machine translation. We further explore the training methods and the data augmentation to improve the model. For efficient training, we apply progressive learning  (Li et al., 2020; Zhou et al., 2021; Zhang et al., 2021b)  to our model that continue-trains a shallow model into a deep model. Specifically, we first train a model with 24 encoder layers, and then continuetrain it by adding 12 layers on the top of the encoder. As for the data augmentation, we implement iterative back-translation  (Hoang et al., 2018; Dou et al., 2020)  that back-translates the data for multiple rounds. Due to the limits of time and GPU memories of the shared task, we do not explore other approaches like mixture-of-experts (MOE) and model ensemble. We participated in all three tracks including Large Track, Small Track #1, and Small Track #2. Our final submissions are fine-tuned from DeltaLM with the allowed data sources according to the track settings, followed by progressive learning and iterative back-translation. The submissions on three tracks all rank first in terms of the automatic evaluation metric. 

 Data Large Track The monolingual and bilingual data are collected from multiple sources, including CCAligned  (El-Kishky et al., 2020 ), CCMatrix (Schwenk et al., 2021 ,  OPUS-100 (Zhang et al., 2020) , JW300  (Agic and Vulic, 2019)   (Tiedemann, 2012), WMT2021 news track 2 , multilingual track data 3 , and our in-house data. To improve the translation quality of non-English languages, we construct dual-pseudo parallel data (or dual-pseudo data briefly) in which the source and target sides per each sentence pair are translated from the same monolingual English sentence respectively. The Wikipedia English monolingual sentences are translated to other 70 languages by leveraging various machine translation models including in-house MT models, M2M  (Fan et al., 2020) , the multilingual model of small tracks, and our intermediate multilingual MT model. Finally, the training data was split into three parts: the bitext data (1.7B parallel sentences from 394 language pairs), the back-translation (1.4B parallel sentences from 45 language pairs), and the dual-pseudo data (8.7B parallel sentences of 70 languages from 4830 language pairs). Figure  1  lists the statistics of the bilingual training data size of 102 languages. Small Track #1 We use the constrained monolingual and bilingual data of 6 languages (Croatian, Hungarian, Estonian, Serbian, Macedonian, and English) provided by the shared task. According to the statistics, the bitext data contains 273M sentence pairs of all translation directions. Inspired by the previous work, we leverage the multilingual iterative back-translation method with one single multilingual model to generate parallel pseudo data. For En?X and X?En directions, we generate the back-translation data of 390M sentence pairs. As for X?Y directions, we generate the dual-pseudo data of 1.18B sentence pairs, where X and Y stand for any two non-English languages. Small Track #2 The monolingual and bilingual corpora of 6 languages (Javanese, Indonesian, Malay, Tagalog, Tamil, and English) provided by the shared task are used for the multilingual model training, containing 98M bilingual data, 256M generated back-translation data, and 860M generated dual-pseudo data. 

 Large-scale Data Augmentation In this section, we introduce details about how to perform the iterative back-translation method  (Hoang et al., 2018)  to augment data. We use different models for data augmentation according to different tracks. For the small tracks, the multilingual models were trained over the constrained data sets to generate data. For the large track, we leverage the M2M model  (Fan et al., 2020) , the intermediate multilingual MT models, and in-house MT models to generate different language pairs' data respectively, so as to play their respective advantages to enhance the data generation quality. In practice, both the monolingual and bilingual corpora are effectively utilized in three ways: 1) For the back-translation data of X?En and En?X directions, we used the best model to generated X data accordingly by back-translating monolingual English Wikipedia data; 2) For the dual-pseudo data of X?Y directions, they are generated by back-translating the same English text to X and Y respectively. Alternatively, when the monolingual data of either X or Y is enough, we also directly perform back-translation between X and Y to obtain pseudo parallel data; 3) We try to augment existing bilingual corpora with the third language. Given the bilingual corpus (X 1 , Y 1 ), we generate pseudo parallel corpus of (  X 1 , Y 2 ) and (X 2 , Y 1 ) by  back-translating X 1 to X 2 and Y 1 to Y 2 , where X 2  and Y    

 Preprocessing Filtering To enhance the model performance, we remove the noisy sentence pairs with the incorrect language identification or character encoding. More specifically, we remove the sentences longer than 1024 words and truncate the sentence to 512 tokens. We also construct three corpora after tokenization with different length ratio limitations, i.e. {1.5, 2.0, 2.5, 3.0}, between the source and the target sentence. Our multilingual model is first trained on the entire noisy data set and then continually tuned on cleaner data with descending length ratio, where the number of training directions is also gradually reduced by removing noisy language pairs. Therefore, we can progressively fine-tune the multilingual model in an efficient way (noisy corpora ? clean corpora ? numerous directions ? selected directions ? shallow encoder layers ? deep encoder layers). Besides, to clean the backtranslation corpora, we remove the sentences containing unknown tokens ([UNK]). Regarding the language Sr (Serbian), those sentences comprised of Latin characters in training data were also discarded since we found that the validation sets use Cyrillic script for this language instead. Tokenization After data filtering, we use the Sen-tencePiece (Kudo and Richardson, 2018) to tokenize all raw training, validation, and test data sets, where the SentencePiece model is consistent with the one used for DeltaLM  (Ma et al., 2021) . We shuffled the whole training dataset before launching the training of multilingual models. The input sentence is prefixed with the language tag to indicate the translation direction. 

 Model and Training 

 DeltaLM We adopt the DeltaLM_large architecture as the backbone model for all our experiments, which has 24 Transformer encoder layers and 12 inter-leaved decoder layers with an embedding size of 1024, a dropout of 0.1, the feed-forward network size of 4096, and 16 attention heads. We directly initialize our model with the public available DeltaLM large checkpoint 4 . 

 Multilingual Fine-tuning The training data was split into the bitext corpora D b = {D 1 b , . . . , D u b }, the back-translation corpora D bt = {D 1 bt , . . . , D v bt } , and the dual-pseudo corpora D dp = {D 1 dp , . . . , D w dp }, where u, v, w represent the number of the corpora of different translation directions. The multilingual model with parameters ? is jointly trained over the corpora to optimize the combined objective as below: L M T = ? ? 1 u i=1 E x,y?D i b [? log P (y|x; ?)] ? ? 2 v i=1 E x,y?D i bt [? log P (y|x; ?)] ? ? 3 w i=1 E x,y?D i dp [? log P (y|x; ?)] (1) where x, y denote the sentence pair in the bilingual corpus. L M T is the combined translation objective of the multilingual model. ? 1 , ? 2 , ? 3 (? 1 + ? 2 + ? 3 = 1.0) are used to balance the training objectives of the bitext corpora, the back-translation corpora, and the dual-pseudo corpora. In this work, we first set ? 1 = 0.33, ? 2 = 0.33, ? 3 = 0.33 and then reset ? 1 = 0.6, ? 2 = 0.2, ? 3 = 0.2 to focus more on the bitext corpora avoiding the noise introduced by pseudo data. We follow the dynamic temperature-based datasampling strategy  (Fan et al., 2020; Wang et al., 2020b)  to ease the underrepresentation of lowresource languages. The probability of picking a language is proportional to its number of sentences D l , i.e., p l = D l i D i . We set the temperature T = 5 to rescale and control the distribution p 1 T l . It can balance the samples between the high-resource languages and the low-resource languages. 

 Progressive Learning We implement the progressive training method to train the model from shallow to deep  (Li et al., 2020) . The training process can be divided into two stages. In the first stage, the pre-trained DeltaLM model with 24 encoder layers and 12 decoder layers is directly adopted to initialize the multilingual translation model with the same architecture. The shallow translation model with 24 encoder layers and 12 decoder layers is fine-tuned on all available multilingual corpora. In the second stage, we increase the depth of the encoder from 24 layers to 36 layers, where the bottom 24 layers of the encoder are initialized with the shallow model's encoder and the top 12 layers are randomly initialized. Then we perform continue training. The deeper encoders enlarge the model's capacity, but no much extra decoding cost is introduced. 

 Training Details We train multilingual models with the Adam optimizer (Kingma and Ba, 2014) (? 1 = 0.9, ? 2 = 0.98). The learning rate is set as 1e-4 with a warmup step of 4, 000. The models are trained with the label smoothing with a ratio of 0.1. All experiments are conducted on 64 NVIDIA V100 or 32 A100 GPUs. The batch size is 1536 or 2048 tokens per GPU and the model is updated every 32 (for 64 V100 GPUs) or 64 (for 32 A100 GPUs) steps to simulate the large batch size. 

 Decoding To enhance the performance of low-resource language pairs for X?Y directions, we adopt the pivot-based translation method  (Kim et al., 2019) . We use English as the pivot language and employ a unified model to perform the pivot-based translation. When the performance of X?Y directions on the validation set is better than the pivot-based translation X?En ? En?Y, we directly translate the language X into Y . Otherwise, we translate them in the pivot way. This approach is used for the submission to Large Track and Small Track #2. As for Small Track #1, we do not use the pivot-based translation. 

 Evaluation Results Following the previous work  (Goyal et al., 2021) , we use the dev and the devtest of the FLORES-101 benchmark as our validation set and test set respectively. During the inference, the beam search strategy is performed with a beam size of 4 for the target sentence generation. We set the length penalty as 1.0 by default. The last N checkpoints (N = {1, 5, 10, 15, 20}) are averaged for evaluation and we select the best checkpoint based on the performance on the validation set. We report the SentencePiece-based BLEU using spBLEU 5 . 

 Large Track Given the unbalanced large-scale multilingual corpora, we use the hybrid strategy for the translation for Large Track. The pivot-based translation is more suitable for the low-resource translation direction between non-English languages since the corpora of X?Y are commonly scarce. Our model with the 36 encoder layers significantly outperforms the shallow counterpart with the 24 encoder layers, which indicates that using a deep encoder and shallow decoder is a good trade-off between the translation quality and the decoding speed. Table 1 shows that our model with the hybrid strategy gets the best performance with less inference cost than the pivot-based translation which costs double inference time compared to the direct translation. We build a massively multilingual neural machine model, which translates between any pair of 102 languages. In Figure  2  and Figure  3 , we reported the spBLEU scores of the shallow model with 24 encoder layers and 6 decoder layers and our best multilingual model with 36 encoder layers and 12 decoder layers in all translation directions, where the languages are ordered alphabetically by the language code. Nearly 30% translation directions adopt the pivot-based translation, where the zeroresource and low-resource translation directions lack of supervised training data tend to be chosen for pivot-based translation. 

 Small Track #1 In Table  2 , we compare the performance of M2M with our method in different architectures on any to English (X?En), English to any (En?X), and the translation between any non-English languages (X?Y). Both En?X and X?En contain 5 directions, while X?Y have 20 directions. Given the enormous bilingual and back-translation data of Small Track #1, we are able to perform the direct translation for all X?Y directions. Furthermore, we explore the deep encoder (36 encoder layers) and shallow decoder (12 decoder layers) considering the limited inference time. From Table  2 , we can observe that the largest model (36 encoder layers and 12 decoder layers) has a significant improvement of +9.41 BLEU points over the strong M2M baseline. Table  2 : Evaluation results of Small Track #1 for M2M and our method of 6 languages (Croatian, Hungarian, Estonian, Serbian, Macedonian, English) on the devtest of the FLORES-101 benchmark. DeltaLM + Zcode (Direct) denotes the strategy that we choose the direct translation for all translation directions, where the target language symbol is prefixed to the input sentence to indicate the translation direction. Our mutilingual translation model is only trained on the constrained corpora of 6 languages provided by the shared task. 

 Small Track #2 Compared to Small Track #1 (273M bilingual pairs), Small Track #2 contains smaller while more unbalanced training data (93M bilingual pairs). Therefore, we consider the hybrid strategy for the X?Y translation directions. We separately calculate the BLEU scores of the direct and the pivot-based translation on the validation set. For those directions satisfying BLEU direct (X, Y ) ? BLEU pivot (X, Y ), we employ the direct translation. Otherwise, we use the pivot-based translation direction by first translating the source language to English and then to the target language. According to Table  3 , DeltaLM + Zcode (Hybrid) outperforms both the direct and pivot-based translation by about +0.5 BLEU points. It confirms that the hybrid strategy is essential since the training data of the X?En and En?Y is easy to obtain while the X?Y is hard to obtain. The deep model with the 36 encoder layers and 12 decoder layers has comparable performance with the shallow model with the 24 encoder layers and 12 decoder layers, which may be caused by the overfitting problem on the low-resource directions. 

 Discussion on Progressive Learning Given the pre-trained model and large-scale parallel data, we adopt progressive learning as an alternative to fine-tune the multilingual model on the multilingual translation task. Our multilingual model is first trained on the large-scale noisy data and then continues to be tuned on the clean data (Noisy Data ? Clean Data), where the model is denoted as z. Since the training data of K languages in the dual-pseudo parallel data is generated by the same English monolingual data, we are able to adopt all possible K ? (K ? 1) training directions on the clean data. The performance of many translation directions is improved by the additional dual-pseudo data while the performance of other directions has been degraded compared to the initial model {, due to the poor quality of some languages in the dual-pseudo data. Therefore, only the part of K ? (K ? 1) training directions is selected to continue training the multilingual model (Numerous Directions ? Selected Directions), which we denoted as y. af am ar as ast az be bn bs bg ca ceb cs ku cy da de el en et fa fi fr ff ga gl gu ha he hi hr hu hy ig id is it jv ja kam kn ka kk kea km ky ko lo lv ln lt lb lg luo ml mr mk mt mn mi ms my nl no ne ns ny oc om or pa pl pt ps ro ru sk sl sn sd so es sr sv sw ta te tg tl th tr uk umb ur uz vi wo xh yo zh zt zu 8.4 1.7 1.7 1.1 2.2 1.5 0.9 2.3 2.9 3.3 3.4 3.5 3.2 0.1 4.2 3.9 3.4 2.7 7.9 2.8 2.6 2.7 4.0 1.4 1.8 3.7 1.4 2.7 2.9 3.0 3.0 3.3 1.4 4.4 4.8 2.8 3.4 2.8 3.9 3.0 1.3 2.4 1.8 0.9 1.5 1.2 3.0 2.6 2.6 3.3 2.9 1.9 3.3 1.4 1.4 1.6 3.0 4.1 1.1 1.3 3.6 2.3 3.7 3.4 1.9 1.6 2.7 2.9 1.1 1.4 1.3 2.9 4.4 1.7 3.5 2.4 3.1 3.1 2.4 0.7 1.5 3.3 2.3 3.9 3.4 2.1 2.1 2.0 3.5 2.6 3.5 2.7 0.8 1.4 1.9 3.2 1.4 1.6 0.0 2.9 1.5 2.8 18.6 8.2 6.3 5.1 5.8 5.3 5.7 8.8 8.9 12.9 10.8 12.4 10.1 0.1 15.5 12.2 10.8 9.0 25.5 9.8 9.1 8.4 13.9 1.5 13.2 11.7 4.9 7.4 10.0 10.2 9.8 9.8 3.8 7.4 15.1 9.1 9.8 7.5 12.0 4.4 4.3 7.8 5.0 0.9 5.3 3.3 8.5 4.5 9.5 11.5 9.0 4.9 5.5 2.2 7.4 5.1 11.2 17.0 3.5 3.0 15.5 5.8 9.7 9.5 6.5 3.8 7.4 8.8 3.7 4.9 5.3 7.9 13.4 4.4 11.9 9.2 9.1 9.5 6.5 2.4 2.2 10.2 9.9 11.5 11.4 9.2 7.8 7.8 14.2 8.5 11.0 9.1 1.0 6.1 8.9 14.5 2.6 9. directions on the FLORES-101 devtest set. The language x in the i-th row and language y in the j-th column denotes the translation direction from the language x to language y. For example, the cell of the 1-th row (af) and the 2-th column (am) represents the result of the translation direction af?am. The table shows the results of all translation directions of 102 languages. af am ar as ast az be bn bs bg ca ceb cs ku cy da de el en et fa fi fr ff ga gl gu ha he hi hr hu hy ig id is it jv ja kam kn ka kk kea km ky ko lo lv ln lt lb lg luo ml mr mk mt mn mi ms my nl no ne ns ny oc om or pa pl pt ps ro ru sk sl sn sd so es sr sv sw ta te tg tl th tr uk umb ur uz vi wo xh yo zh zt zu  Figure 1 : 1 Figure1: Dataset statistics of the bilingual data of the 102 languages. For better visualization, we apply the logarithmic function (base 10 logarithm) to the size of the training data. Each column denotes the data size of a language that was paired with the remaining 101 languages. For example, the first column denotes the number of bilingual sentence pairs that contain sentences from language hr. 

 2 are non-English languages. 

 Figure 2 : 2 Figure2: Evaluation results of our multilingual model (24 encoder layers and 6 decoder layers) on all translation directions on the FLORES-101 devtest set. The language x in the i-th row and language y in the j-th column denotes the translation direction from the language x to language y. For example, the cell of the 1-th row (af) and the 2-th column (am) represents the result of the translation direction af?am. The table shows the results of all translation directions of 102 languages. 

 , Tatoeba hr et zt zh ko hu id pl vi en ja cs pt el nl it ro ru fr de es tr ar th fi bg da sv sr sl he ms lv hi tl sk ta fa ml lt luo uk ur az no jv cy mk ka ca te am bn ga mr gu xh bs hy ky af is km as be ln kk ny sn mi pa ast mn gl tg mt ne ceb oc kn uz ha kea so ig lg sw my umb zu yo ku ps sd wo lo lb ff or om ns kam 

 Table 1 : 1 #Languages #Params #Layers Avg X?En Avg En?Y Avg X?Y Avg all Evaluation results of Large Track for M2M and our method of 102 languages on the devtest of the FLORES-101 benchmark. Avg X?En denotes the average score of directions between other languages to English. Avg X?En denotes the average score of directions between English and other languages. Avg X?Y denotes the average score of directions between non-English languages to other non-English languages. Avg all denotes the average result of all translation directions. #Languages #Params #Layers Avg X?En Avg En?Y Avg X?Y Avg all M2M (Fan et al., 2020) 102 102 175M 615M 6/6 12/12 15.43 20.03 12.02 16.21 5.85 7.66 6.00 7.86 102 711M 24/6 30.39 23.52 11.21 11.52 DeltaLM + Zcode (Direct) 102 862M 24/12 33.09 27.21 13.56 13.89 102 1013M 36/12 33.35 27.39 14.34 14.65 102 711M 24/6 31.32 24.04 14.74 14.99 DeltaLM + Zcode (Pivot) 102 862M 24/12 33.09 27.21 17.20 17.45 102 1013M 36/12 33.35 27.39 17.36 17.62 102 711M 24/6 31.32 24.04 14.76 15.01 DeltaLM + Zcode (Hybrid) 102 862M 24/12 33.09 27.21 17.27 17.52 102 1013M 36/12 33.35 27.39 17.44 17.70 M2M (Fan et al., 2020) 102 102 175M 615M 6/6 12/12 24.60 31.58 20.83 29.62 20.80 26.66 21.44 27.98 DeltaLM + Zcode (Direct) 6 6 862M 1013M 24/12 36/12 43.78 44.34 41.02 41.32 34.38 34.68 37.06 37.39 

 Table 3 : 3 To further enlarge the model's capacity, we extend the shallow model with 24 encoder layers to the deep model with 36 #Languages #Params #Layers Avg X?En Avg En?Y Avg X?Y Avg all Evaluation results of Small Track #2 for M2M and our method of 6 languages (Javanese, Indonesian, Malay, Tagalog, Tamil, English) on the devtest of the FLORES-101 benchmark. DeltaLM + Zcode (Hybrid) denotes the strategy that we choose the pivot-based translation (X?En, En?X) for low-resource X?Y directions and direct translation for high-resource X?Y directions. M2M (Fan et al., 2020) 102 102 175M 615M 6/6 12/12 18.95 24.67 15.16 19.14 9.49 12.11 12.01 15.38 DeltaLM + Zcode (Direct) 6 6 862M 1013M 24/12 36/12 43.12 43.56 39.78 39.04 28.69 28.60 32.94 32.83 DeltaLM + Zcode (Pivot) 6 6 862M 1013M 24/12 36/12 43.12 43.56 39.78 39.04 29.02 28.63 33.17 32.85 DeltaLM + Zcode (Hybrid) 6 6 862M 1013M 24/12 36/12 43.12 43.56 39.78 39.04 29.38 28.99 33.40 33.09 ID Large Track Avg all x DeltaLM + Zcode 14.65 y x -Shallow Model ? Deep Model 13.89 z y -Numerous Directions ? Selected Directions 13.09 { z -Noisy Data ? Clean Data 12.24 

 Table 4 : 4 Ablation study of the large track on devtest. DeltaLM + Zcode is fine-tuned on the multilingual translation task via progressive learning. Track Submission Name Avg all Large DeltaLM + Zcode (Microsoft) 16.63 Small #1 DeltaLM + Zcode (Microsoft-Small) 37.59 Small #2 DeltaLM + Zcode (Microsoft-Small) 33.89 encoder layers, where the top 12 encoder layers are initialized by random parameters (Shallow Model ? Deep Model). Putting them all together, we obtain the final model x DeltaLM + Zcode. Ta- ble 4 summarizes the results of the ablation study of these approaches. It shows that each approach has a significant contribution to the final model. This proves the effectiveness of progressive learning that can gradually improve performance in different as- pects. 7 Submissions Considering the trade-off between the decoding time and the performance, we submit the model (24 encoder layers and 12 decoder layers) with the hybrid strategy to both the Large Track and Small Track #2, while the deep model (36 encoder layers and 12 decoder layers) with the direct translation is submitted to Small Track #1. Table 5 summarizes the evaluation results of our model on the hidden test sets. According to the final results on the leader- board, DeltaLM + Zcode ranks first across three tracks. 8 Conclusion This paper describes Microsoft's submission to the large-scale multilingual machine translation of the WMT21 shared task. Our multilingual translation 

 Table 5 : 5 Submission results based on the hidden test sets of our method on three tracks, including Large Track, Small Track #1, and Small Track #2. model achieves substantial improvement over the baseline systems by fine-tuning the pre-trained language model DeltaLM. We further enhance the model performance with the progressive learning and the iterative back-translation methods. As a result, our submitted systems get the top evaluation results on three tracks, including Large Track, Small Track #1, and Small Track #2. Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, and Chao Chen. 2021b. Learning with feature-dependent label noise: A progressive ap- proach. In ICLR 2021. Baohang Zhou, Xiangrui Cai, Ying Zhang, and Xiaojie Yuan. 2021. An end-to-end progressive multi-task learning framework for medical named entity recog- nition and normalization. In ACL 2021, pages 6214- 6224. 

			 http://statmt.org/wmt21/ translation-task.html 3 http://data.statmt.org/wmt21/ multilingual-task/ 

			 https://aka.ms/deltalm 

			 https://github.com/ngoyal2707/ sacrebleu.git 

			 Figure3: Evaluation results of our multilingual model (36 encoder layers and 12 decoder layers) on all translation directions on the FLORES-101 devtest set. The language in the i-th row and language y in the j-th column denotes the translation direction from the language x to language y. For example, the cell of the 1-th row (af) and the 2-th column (am) represents the result of the translation direction af?am. The table shows the results of all translation directions of 102 languages.
