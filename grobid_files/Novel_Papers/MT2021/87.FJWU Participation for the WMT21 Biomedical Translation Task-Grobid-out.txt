title
FJWU participation for the WMT21 Biomedical Translation Task

abstract
In this paper we present the FJWU's system submitted to the biomedical shared task at WMT21. We prepared state-of-the-art multilingual neural machine translation systems for three languages (i.e. German, Spanish and French) with English as target language. Our NMT systems based on Transformer architecture, were trained on combination of indomain and out-domain parallel corpora developed using Information Retrieval (IR) and domain adaptation techniques.

Introduction Due to vast availability of multilingual information, Neural Machine Translation (NMT) systems have achieved remarkable growth over Statistical Machine Translation (SMT) systems. Although the amount of training resources has significantly increased in the past few years but availability of large in-domain parallel data is still a challenging task. Performance of NMT system may quickly degrade as soon as the application domain deviates from training domain. Domain adaptation  (Koehn and Schroeder, 2007 ) is a promising active research topic to enhance the translation quality when faced with data scarcity issues. In domain adaptation, initially large amount of parallel out-ofdomain corpora is utilized for training NMT models and then fine-tuning is performed on small indomain data for adapting to novel domains  (Freitag and Al-Onaizan, 2016; Luong and Manning, 2015) . Fine-tuning does not require building system from scratch, instead it is fast and efficient method of integrating in-domain data. An NMT model already trained on general domain data is further fine-tuned on in-domain data with less time and effort  (Chu et al., 2017; Hira et al., 2019) . Training MT systems on back-translated data is a proven domain adaptation method  (Abdul Rauf et al., 2020; Sennrich et al., 2015) , where synthetic parallel data is combined with original data to generate large indomain training corpus. In addition, information retrieval (IR) technique to extract relevant sentences from out-of-domain corpus has shown promising results to overcome data scarcity  (Naz et al., 2020) . NMT system incorporating multiple languages into single model is known as multilingual NMT (MNMT)  (Dabre et al., 2020) . Multilingual NMT systems are gaining popularity due to effective use of available resources and boosting translation quality with Translation Knowledge Transfer  (Pan and Yang, 2009) . In this paper, we present study on adapting MNMT systems (Multiway many-to-one) for translating English (EN) language from French (FR), German (DE) and Spanish (ES) using fairseq  (Ott et al., 2019)  implementation of Transformer model. Our main focus is to investigate the effect on EN translation in Biomedical domain using multilingual NMT systems. We have also explored the domain adaptation for fine-tuning of bilingual NMT models into multilingual NMT models using outdomain and in-domain corpora. Furthermore, we show the effectiveness of utilizing in-domain data generated through IR techniques  (Naz et al., 2020)  by training a NMT system on combined parallel in-domain data. We also compare in-domain multilingual and bilingual models. The remainder of this paper is organized as follows. Section 2 introduces the literature review followed by corpus processing in Section 3. Section 4 presents experiments and results. In Section 5, we conclude the findings of our work. 

 Literature Review MNMT models tend to acquire knowledge from more than one language which helps in generalization and in building systems for low resource languages. MNMT models may help in miti- gating the problem for resource poor languages  (Dabre et al., 2020) , where limited training data is available. Tubay and Costa-juss ? (2018) submitted their NMT systems for English translation with multi-source similar languages including Portuguese, French and Spanish showing improvement of 6 BLEU points over single source NMT system. Soares and Krallinger (2019) also built NMT systems using two of the Romance languages, Spanish and Portuguese for translating into English language. For domain adaptation in NMT, fine-tuning models on in-domain parallel text is a common and effective approach  (Peng et al., 2020) . We assume that, the same can be used for training multilingual (many-to-one) NMT models.  Chu and Dabre (2019)  focused on fine-tuning MNMT models for domain adaptation, they initially trained different MNMT models using single domain and then further fine-tune on multi-domain corpora with mixed (combination of out-domain and in-domain) corpora. 3 Corpus Pre-processing   (Moore, 2002) . ? EDP are the in-domain texts of scientific publications available for FR/EN language pair only  (Neves et al., 2018) . ? EMEA provides in-domain biomedical parallel corpus of documents related to medicinal products  (Tiedemann, 2012) . We used corpora provided for DE/EN, ES/EN and FR/EN language pairs. ? Scielo in-domain corpus provided by WMT comprises of abstracts and titles in biological and health sciences domain  (Neves et al., 2016) . We used datasets provided for FR/EN and ES/EN language pairs. We also used Scielo full text corpus of scientific articles as general domain data set provided for ES/EN language pair  (Soares et al., 2018) . ? UFAL Medical Corpus provides various indomain medical texts and out-domain corpus sources including dictionaries  (Jimeno Yepes et al., 2017) . We included corpora provided for DE/EN, ES/EN and FR/EN language pairs. ? United Nations (UN) parallel corpus comprises of official records in general domain  (Ziemski et al., 2016)  . We used sources provided for ES/EN and FR/EN language pairs. News Commentary 2 and WikiPedia 3 in-domain IR corpora are used. These corpora are extracted using data selection based on IR approach  (Abdul-Rauf et al., 2016)   for retrieving related biomedical domain sentences. From experiments conducted by  (Naz et al., 2020) , corpora with top-2 best sentences gave good results in training NMT models for biomedical domain. Medline18 and 19 testsets are used as development set. We used Medline20 testset provided by WMT20  (Bawden et al., 2020)  as initial test sets to determine quality of our translation models. Preprocessing of data include tokenization and learning joint Byte Pair Encoding (BPE)  (Sennrich et al., 2016)  using sentencepiece 4 with a vocabulary size of 32K over in-domain corpus and encoding all available corpora with learned BPE. 

 Experiments and Results In this section we present details of experimentation along with training configurations. 

 Training and Parameters We employed Fairseq toolkit to train MNMT systems for (German, Spanish, French) ? English translation. We used Transformer architecture and followed similar configuration parameters for out systems as reported in original paper  (Vaswani et al., 2017) . Batch size of 4K words and Adam optimizer was used in all experiments. Training was done till convergence and stopped if no improvement was noted in BLEU scores on development sets for 2-3 consecutive checkpoints. Fine-tuned models were trained for 150K steps unless early stopping is employed based on bleu score convergence. 4 https://github.com/google/ sentencepiece 

 NMT Models We have categorized our experiments into 3 classes based on the corpora and training technique used. I) Multilingual models trained using all in-domain corpus and fine-tuned on Medline and IR. II) Multilingual models trained on all out-domain corpus and fine-tuned on all in-domain and Medline corpus. III) Bilingual models trained on all in-domain corpus. Results of all experiments are depicted in Table  2 . BLEU score for all models is calculated using Sacrebleu  (Post, 2018)  on Medline20 test-set for German-English, Spanish-English and French-English. For System I: ? M 1: this is trained on all in-domain parallel corpus with a total size of 3.71M (DE-EN), 2.77M (ES-EN), 1.78M (FR-EN) sentences. Best BLEU score of 39.12 on Medline20 testset was achieved for ES?EN as it has high rate of Medline sentences (66K) as compared to FR?EN (46K) and DE?EN (18K). ? M 2: this model derived from M 1 by further tuning it on Medline corpus for domain adaptation which resulted in significant increase in BLEU score of +3.56 for FR?EN as compared to previous model (M 1). An increase of +0.26 BLEU for DE?EN and +0.28 BLEU for ES?EN is achieved. ? M 3: M 2 was further fine-tuned on IR corpus for FR?EN language pair but we observe no significant improvements in term of BLEU score on out test-set for all combinations of languages. IR corpus was extracted from News commentary and Wikipedia parallel corpora. Apparently these corpora are far in language jargon from the traditional Medline tests, so we see no apparent gain. It is pertinent to note that IR data was only available for FR-EN thus the in-domain training corpus was used for other language pairs. For System II: ? M 4: this model is trained on all out-domain parallel corpus with a total size of 3.  

 Conclusion In this paper we have described our system submissions at WMT21 biomedical shared translation task under FJWU's submission. For our submission we trained multilingual NMT systems for German, Spanish and French languages with English as target language. We focused on utilizing in-domain and out-domain parallel corpora and domain adaptation techniques for training multilingual NMT systems. We showed that, domain adaptation using fine-tuning of multilingual NMT model can be a reasonable alternative to achieve good translation quality for novel domains. Table 1 : 1 Corpus DE/EN ES/EN FR/EN In-domain training data UFAL 2.6 M 631 K 2.6 M Scielo Health - 124 K 9 K Scielo Biological - 581 K - EDP - - 3 K Medline Titles - 285 K 612 K Medline Abstracts 18 K 66 K 46 K EMEA 1.10 M 1.09 M 1.09 M In-domain IR training data News Commentary-IR2 - - 65 K WikiPedia-IR2 - - 84 K Out-domain training data UFAL 30.9 M 74.8 M 73.3 M UFAL Dictionary 733 K 544 K 744 K Scielo - 433 K - UN - 21.9 M 25.8 K Development data Medline18 321 239 311 Medline19 439 437 400 Test data Medline20 409 466 479 Sentence Pairs Used for Training, Development and Testing of MNMT models (K stands for "Thousand" and M stands for "Million") 
