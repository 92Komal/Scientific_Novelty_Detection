title
TenTrans High-Performance Inference Toolkit for WMT2021 Efficiency Task

abstract
The paper describes the TenTrans's submissions to the WMT 2021 Efficiency Shared Task. We explore training a variety of smaller compact transformer models using the teacherstudent setup. Our model is trained by our self-developed open-source multilingual training platform TenTrans-Py 1 . We also release an open-source high-performance inference toolkit 2 for transformer models and the code is written in C++ completely. All additional optimizations are built on top of the inference engine including attention caching, kernel fusion, early-stop, and several other optimizations. In our submissions, the fastest system can translate more than 22,000 tokens per second with a single Tesla P4 while maintaining 38.36 BLEU on En-De newstest2019. Our trained models and more details are available in TenTrans-Decoding competition examples 3 .

Introduction We participate in the GPU throughput track of the Workshop on Machine Translation (WMT) 2021 Efficiency Shared Task. The efficiency task aims at exploring the different techniques for training and optimizing GPU models for high throughput while preserving the highest possible accuracy. While we do not pay more attention to training techniques, we apply a variety of optimizations to improve the computation efficiency of our GPU models in the inference phase. In terms of the training phase, we trained a variety of smaller compact student models using the common teacher-student training approach  (Hinton et al., 2015; Kim and Rush, 2016)  on our opensource multilingual training platform TenTrans-Py. All of them are based on the deep transformer which has proven more effective and has lower training costs than the wide transformer models  (Wang et al., 2019) . For the inference phase, our strategy for the shared task includes attention caching, kernel fusion, early-stop, and several other optimizations. All of these optimizations are employed in a high-optimized and C++-based inference engine TenTrans-Decoding. The paper is structured as follows: Section 2 describes the data preparation and the training details, then Section 3 presents the variety of ours optimizations to improve decoding efficiency. The detailed accuracy and efficiency results are shown in Section 4. Finally, we conclude our work in Section 5. 

 Teacher-student Training To train smaller compact student models, the teacher-student training approach  (Hinton et al., 2015; Kim and Rush, 2016 ) is adopted. First, a large model (the teacher) is trained on all available bilingual data, included synthetic data generated by the back-translation  (Sennrich et al., 2015a)  method. Multiple model ensembles are also typically used to build stronger teacher systems. Then, all our small optimized models (the student) are created using sequence-level knowledge distillation  (Kim and Rush, 2016)  and trained on data generated from the teacher model. The sequencelevel knowledge distillation is a common technique that has proven successful for reducing the size of neural models, especially in NMT tasks. 

 Deep Transformer Transformer networks  (Vaswani et al., 2017)  are the current state-of-the-art in many machine translation tasks, and the deep transformer  (Wang et al., 2019)   model, we use the Pre-Norm strategy  (Wang et al., 2019) . The layer normalization  (Ba et al., 2016)  is applied to the input of every sub-layer which the computation sequence could be expressed as: layer normalization ? multi-head attention / feedforward ? residual-add. All of our models are based on deep transformer architecture. 

 Teacher & Student Models The different model configurations for both teacher and student models are presented in Table  1 . We train a teacher model and three student model variant with a different number of encoder layers N enc , decoder layers N dec , hidden size d model , and feedforward network size d f f . We adopt a deep encoder and a shallow decoder architecture of all student models, and the number of decoder layers is set to 1 by default. All of our models tie source embedding, target embedding, and softmax weights. 

 Data and Training Details Dataset Following the shared task setup, we limit our training data to the WMT 2021 English-German translation task. The bilingual data used in the English-German task includes all the available corpora provided by WMT 2021: Europarl v10, ParaCrawl v7.1, News Commentary, Wiki Titles v3, Tilde Rapid corpus and WikiMatrix. For monolingual data, we only use NewsCrawl2020, Europarl v10, and News Commentary for backtranslation. Data preprocessing Then, we normalize punctuation and tokenize all data with the Moses tokenizer  (Koehn et al., 2007) . For the bitext datasets, we remain sentences no longer than 200 words as well as sentence pairs with a source / target length ratio between 0.3 and 2.0. The fast-align tools  (Dyer et al., 2013)  are applied to further obtain a cleaned and high-quality parallel corpus. For the monolingual dataset, the sentences with words between 4 and 200 are remained. See with 32K split operations for subword segmentation  (Sennrich et al., 2015b) . Student training First, we train the teacher model on all available bilingual data, including synthetic data through the back-translation method, and we use English-German newstest2019 as the development set. We ensemble four best models for building a stronger teacher. Then, the English part of the bilingual data is translated by the teacher model and the resulting synthesized parallel data is used to train the student models. Table  1  shows their evaluation scores on newstest2019 of different models. The results correlate well with the expectation that more model parameters lead to better performance. Our distillation student models show strong competitiveness even when the number of parameters is greatly reduced. 

 GPU Inference Optimizations 

 Implementation: TenTrans-Decoding TenTrans-Decoding is an open-source highoptimized inference engine for transformer models and the code is written in C++. TenTrans-Decoding's goal is to offer a lightweight and rapid deployment of high-performance service solutions for executing models. All additional optimizations are built on top of the inference engine. 

 Attention Caching We apply the common technique of caching linear projections in Transformer decoder layers. More specifically, we cache the linear transformations for keys and values before cross-attention layers and each step of decoder self-attention layers. 

 Kernel Fusion To reduce kernel launching overhead and enhance the GPU computation efficiency, we implement many kernel fusion techniques for our Transformer models. ? Add_bias_residual_layerNormalization For the layer normalization between two General Matrix Multiplications (GEMMs), we reorganize the AddBias kernel, residual network, and LayerNormalization kernel into a single one. ? Add_bias_ReLU In the Feed-Forward network layers of the Transformer model, the AddBias kernel and ReLU kernel are fused into one. ? Add_bias_residual For the output of every encoder or decoder layer, we fuse the AddBias kernel and residual network. ? Fused_multihead_attention In addition to the fusion techniques above, we also fuse the attention layer by packing GEMMs and bias to further improve the computation efficiency.     3 : The decoding speed (source tokens per second) and SacreBLEU scores on newstest2019 for student-tiny-20_1. The speed is measured by a single Tesla P4 GPU and the beam size is 4. x weightQ/K/V -> Q / K / V (GEMM 0, 1, 2) add bias to Q / K / V softmax(QK / sqrt(size_per_head)) x V x weightQ -> Q encoder_output x weightK/V -> K / V (GEMM 4, 5, 6) add bias to Q / K / V softmax(QK / sqrt(size_per_head)) x V GEMM7 

 Early-stop In batch decoding, the number of decoding ending steps between sentences is different. The early-stop strategy which optimizes kernel function is adopted to avoid redundant computation. For sentences that have been decoded in batch, there is no additional computation for these sentences until the whole batch has been decoded. 

 Sorted Batch & Greedy Search In addition to the methods above, we sort all input sentences from shortest to longest, and the batch size is 128 in our settings. The sorting makes the batches contain sentences of similar sizes which reduces the amount of padding and increases the computation efficiency. During decoding, we use greedy search instead of beam search since we find the distillation model are insensitive to the beam size. We skip the final softmax layer and simply get the maximum from the output logits. 

 Optimization Results Table  3  shows the impact of different inference optimizations when decoding the Student-tiny-20_1 student transformer model. TenTrans-Decoding leads to a 2.62x speedup than the TenTrans-Py baseline without any inference optimizations. Combine all the inference optimizations mentioned above, it can achieve a 7.23x speedup with no accuracy loss over the baseline. Table  4  presents all of our submissions and we only participate in the GPU-throughput track. As details in Table  4 , we report our model configuration, model size, and metric for translation, including SacreBLEU scores on newstest2019 and the real translation time cost. All of our systems are tested on a single Tesla P4 GPU. All student models follow a deep encoder and a shallow decoder architecture, the number of decoder lay- In this version, we do not pay more attention to the model size, memory footprint, and low precision inference (e.g., FP16). All operations on the model are based on FP32 floating-point numbers. In the future version, we plan to optimize these points mentioned above. 

 Conclusion This presents the TenTrans's submissions to the 2021 Efficiency Shared Task of WMT. We show the deep encoder and shallow decoder student models that training with sequence-level distillation can achieve a competitive performance both in speed and accuracy compared with the teacher baseline. To further improve computation efficiency, we combine several optimizations including attention caching, kernel fusion, early-stop and sorted batch. Finally, our fastest student model achieves a speedup of 3.58x times, while only has one-sixth parameters of the teacher baseline. In the future, we will apply low-precision inference (e.g., FP16) and more kernel fusion techniques to improve the computation efficiency of our GPU systems. Furthermore, we will continue to explore a more efficient teacher-student training approach to obtain compact student models with competitive performance both in quality and speed. Figure 1 1 Figure1details the kernel fusion techniques of a transformer decoder layer. The computation graph of a transformer can be reorganized into a more compact graph by fusing all the kernels between two GEMMs into a single one. 
