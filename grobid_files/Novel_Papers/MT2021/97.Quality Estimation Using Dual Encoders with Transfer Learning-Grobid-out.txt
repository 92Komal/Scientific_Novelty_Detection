title
Quality Estimation Using Dual Encoders with Transfer Learning

abstract
This paper describes POSTECH's quality estimation systems submitted to Task 2 of the WMT 2021 quality estimation shared task: Word and Sentence-Level Post-editing Effort. We aim to improve the stability of recently proposed quality estimation models, which usually have a single encoder based on the selfattention mechanism to simultaneously process both of the two input data: a source sequence and its machine translation; considering that such models are not propped up by pre-trained language models' monolingual word representations, which are generally accepted as reliable representations for various natural language processing tasks. Therefore, our model first uses two pre-trained monolingual encoders and then exchanges their output information through two additional cross attention networks. According to the official leaderboard, our systems outperform the baseline systems in terms of the Matthews correlation coefficient for machine translations' wordlevel quality estimation and in terms of the Pearson's correlation coefficient for sentencelevel quality estimation by 0.4126 and 0.5497 respectively.

Introduction Quality estimation (QE) is the task of estimating the quality of given machine translations without regard to their reference translations  (Blatz et al., 2004; Specia et al., 2009) . As reference translations are generally unavailable in real life, QE should help to treat output texts of machine translation (MT) systems. QE can be categorized into several subtasks, and this round of the WMT QE task has three subtasks, yet we focus on Task 2: Word and Sentence-Level Post-editing Effort. In Task 2, while sentence-level QE aims to predict the Human-Targeted Translation Edit Rate (HTER,  Snover et al. 2006) , which measures the edit distance between an MT output (mt) and its human post-edited text (pe), word-level QE aims to predict OK-BAD tags for three sequences of tokens: the sequence of words in a source text (src) depending on whether they are correctly translated referring to mt; the sequence of words in mt depending on their correctness; and <GAP> tokens, which each represent the gap between two adjacent words, depending on the existence of any missing words . As other recent QE models do, our method also applies transfer learning, considering that pretrained language models (LM) have been successfully applied to various natural language processing (NLP) tasks including QE; many previous studies  Hu et al., 2020; Wu et al., 2020; Lee, 2020; Moura et al., 2020; Nakamachi et al., 2020; Rubino, 2020)  that apply pre-trained LMs to QE have adopted multilingual or crosslingual LMs such as multilingual-BERT  (Pires et al., 2019) , XLM  (Conneau and Lample, 2019) , and XLM-R  (Conneau et al., 2020)  to process the two input data src and mt. Such cross-lingual LMs have a single Transformer  (Vaswani et al., 2017)  encoder using only the self-attention mechanism to create vector representations of the input data and predict the labels. However, it appears possible to further improve the stability of those models, considering that they are not propped up by pre-trained LMs' monolingual word representations, which are generally accepted as reliable representations for various NLP tasks. With this background, we propose a QE model that has two separate pre-trained encoders that each produce monolingual representations of src and mt, respectively. On top of each encoder, we add a cross attention network for the learning of the cross-lingual context between src and mt; these networks will produce two sets of cross-lingual representations for QE. We conduct simple experiments to compare the performance of our systems and ensembles of them with that of the baseline systems and that of other submitted systems for   (Clark et al., 2020)  Task 2. Experimental results imply that although our systems do not always outperform the baseline systems, they do in terms of the Matthews correlation coefficient (MCC) for mt's word-level QE and in terms of the Pearson's correlation coefficient (PCC) for sentence-level QE by 0.4126 and 0.5497 respectively. 

 Related Work Because our model does not confine its monolingual encoders to specific pre-trained LMs, all pretrained LMs can be considered relevant. Among them, most of the recently proposed pre-trained LMs are denoising autoencoders, of which the pretraining task is usually to select about 15% of tokens in unlabeled input sequences and apply the attention mechanism to those tokens  (Yang et al., 2019)  or is to mask certain tokens  (Devlin et al., 2019)  and then restore them. However, in our experiments, our systems use ELECTRA  (Clark et al., 2020) . ELECTRA introduces "replaced token detection" as an additional pre-training task and let the language model learn to distinguish between real input tokens and specious but artificially generated tokens. In detail, when the generator network predicts the tokens in the masked positions, some of the predicted tokens are corrupted, and then this output sequence is fed into a Transformer-based discriminator network, which predicts whether each token in the fed sequence is the same as the original one or is a replaced one (Figure  1 ). We suppose that this process and QE are similar to each other in that both of them predict the soundness of the given tokens, so ELECTRA would be one of the most appropriate pre-trained LMs for our QE model's monolingual encoders, especially for Task 2. 

 Model Description Our model uses two ELECTRAs: one ELECTRA 1 that is pre-trained with English corpora and the other ELECTRA 2 pre-trained with German corpora. Figure  2  depicts the overall structure of our model. 

 Dual Monolingual Encoders Our model has dual encoders: a pre-trained English ELECTRA processing src and a pre-trained German ELECTRA processing mt. These encoders will produce reliable monolingual representations of src and mt respectively to provide these refined representations to the upper cross attention networks. Because unlike other pre-trained QE models that have a single Transformer encoder being fed with the concatenation of src and mt, our model lets the two different encoders process the two input data respectively, we exclude the segment embeddings, which are used to distinguish one language from another, and assign different positional embeddings to each input data. In addition, for sentence-level QE, mt's special token <CLS> is used to predict the HTER. 

 Cross Attention Networks We attach a cross attention network to each pretrained encoder; it learns the cross-lingual context information by using the encoders' refined monolingual representations of the two input data. Although the structure of a cross attention network is identical to that of the encoders, the cross attention networks are not pre-trained, so we train them after the random initialization of their parameters. We find that applying transfer learning to the cross attention networks is not available due to the absence of pre-trained language models that are pre-trained to perform cross attention on cross-lingual input data by using one side as a query vector and the other side as both a key vector and a value vector just as the Transformer decoder performs multihead attention on the output of the Transformer encoder. 

 Sentence-Level QE To predict the HTER for sentence-level QE we employ the final hidden vector m <CLS> of the mt-side cross attention network, which is the final representation of the <CLS> token, as the representation of the mt sequence as a whole. After this representation passes through double linear layers with the GELU  (Hendrycks and Gimpel, 2016)  activation function, the HTER of the given mt sentence is estimated as follows. l = W h m <CLS> + b 0 ?HTER = w T h GELU(l) + b 1 (1) We have trainable parameters W h ? R H?H , w h ? R H , b 0 ? R H , and b 1 ? R; H denotes hidden vectors' dimension. We use the mean squared error of this estimator, that is, the difference between the estimated HTER ?HTER and the ground truth HTER value y HTER , as the training loss L HTER = MSE(? HTER , y HTER ). (2) 

 Word-Level QE src-Side Prediction We use the final hidden vector s (i) (i ? {1, ..., |S|}, where |S| is the number of tokens in the tokenized src sequence) of the srcside cross attention network corresponding to each token in src to predict OK or BAD in the token's position. After each of these representations passes through a linear layer, the word-level probability of the corresponding token being OK or BAD is predicted with a sigmoid activation function: P (i) s = sigmoid(w T s s (i) ), (3) where w s ? R H is a trainable parameter. We use the binary cross-entropy loss function; we also introduce an extra hyperparameter k s to prevent our model from being overfitted because the statistics of the ratio between the number of OK tags and that of BAD tags in our training data (Table 1) can misguide the model to have the tendency of outputting OK even when it should output BAD. The src-side loss is as follows: L src = 1 |S| |S| i=1 k s y (i) s logP (i) s + (1 ? y (i) s )log(1 ? P (i) s ) , (4) where y (i) s is a ground-truth OK-BAD tag. mt-Side Prediction We use the final hidden vector m (i) (i ? {1, ..., |M |}, where |M | is the number of tokens in the tokenized mt sequence) of the mt-side cross attention network corresponding to each token in mt to predict OK or BAD in the token's position. We estimate the probabilities of the word tokens P (i) m = sigmoid(w T m m (i) ), (5) where w m ? R H is a trainable parameter. We also use the final hidden vector m (j) (j ? {1, ..., |M |+1} including the vector in the position of the last <SEP> token to predict OK or BAD for the last <GAP> token. We estimate the probabilities of the <GAP> tokens P (j) g = sigmoid(w T g m (j) ), (6) where w g ? R H is a trainable parameter. The mt-side prediction loss equals the sum of the losses for word tokens and <GAP> tokens: L mt = L m + L g , (7) where L m = 1 |M | |M | i=1 k m y (i) m logP (i) m + (1 ? y (i) m )log(1 ? P (i) m ) , (8) and L g = 1 (|M | + 1) |M |+1 j=1 k g y (j) g logP (j) g + (1 ? y (j) g )log(1 ? P (j) g ) , (9) y (i) m and y (j) g being ground-truth OK-BAD tags for a word token and a <GAP> token respectively and hyperparameters k m and k g being introduced for the same reason why we introduce k s . Finally, we define the word-level loss and the overall QE loss of our model as follows. L word = L src + L mt (10) L QE = L word + L HTER (11) 4 Experiments 

 Datasets In our experiments, we used the eSCAPE  (Negri et al., 2018)   between src and mt, and TER. Then, we created a tuple of labels (T src , T word mt , T gap mt , the TER  (Snover et al., 2006 )) for each triplet 3 . Finally, we tokenized and truncated both of the artifical data and the WMT 2021 official data by using a pre-trained tokenizer based on WordPiece  (Wu et al., 2016) . 

 QE Pre-training After obtaining about three million of artificial triplets, we made the final QE pre-training data by joining the artificial training data and the official human-labeled data together; especially, we augmented the quantity of the latter by replication to allow our systems to learn from both kinds of training data relatively more evenly during the QE pre-training. Our systems learn to predict all kinds of labels jointly (L QE , Eqn. 11) considering the close correlation among the subtasks in Task 2. We used 1,000 triplets in the WMT 2021's official development dataset as validation data. 

 Fine-Tuning We used only the WMT 2021 human-labeled data for fine-tuning. In contrast with the QE pretraining, we fine-tuned our systems to each subtask: the prediction of the sentence-level task (L HTER , Eqn. 2) and the word-level task (L word , Eqn. 10) Considering the overproportion of OK tags in our training data (Table  1 ), we set a large k s , k m , and k g ( ? 3.2.2) in our experiments. 

 Ensemble Learning Besides single fine-tuned systems, we also made ensembles of our best fine-tuned systems, each of which has a different random seed from that of the others. In detail, after fine-tuning several single systems with different random seeds, for each seed, we picked out the top two systems, each of which is different from the other in certain variable training conditions such as how its cross attention networks have been randomly initialized in that instance, in terms of their performance on our validation dataset. Finally, we averaged the weights of the systems element-wisely for better generalization and made the ensembles. 

 Hyperparameters We used ELECTRA-base  (Clark et al., 2020) s as pre-trained monolingual LMs for our dual monolingual encoders 4 . In the QE pre-training, we used get_schedule_with_warmup 5 as our learning rate scheduler with 3,000 warm-up steps. We used the AdamW  (Loshchilov and Hutter, 2018)  optimizer that has a weight decay with ?=0.5, ? 1 =0.9, ? 2 =0.999, and =1e-8, together with gradient clipping. Setting a batch size of 64 for both the QE pre-training and fine-tuning, we set a learning rate of 1e?5 and a tuple of (k s = 1, k m = 1, k g = 3) for the QE pre-training and a learning rate of 5e?5 and a tuple of (k s = 2, k m = 2, k g = 4) for the   fine-tuning, respectively. We validated the performance of our systems on our validation set every 5,000 steps during the QE pre-training and every 200 steps during the fine-tuning, respectively; we applied early stopping with a patience of 30. 

 Results In comparison with our single system, our ensembles report an improved PCC, mt-side words MCC, and mt-side <GAP>s MCC of about 0.5497, 0.4296, and 0.1225 respectively (Table  2 ). Compared to other systems submitted to the WMT 2021 English-German QE Task 2, our systems outperform the baseline systems in terms of the sentence-level PCC (Table  3 ) and the mt-side words MCC (Table 4). Our systems are inferior to the baseline systems in terms of the src-side MCC and the mt-side <GAP>s MCC by a narrow margin (Table  4 ). However, because our systems have a smaller number of parameters than other submitted systems, we expect that it is possible to improve the performance of our systems by adopting larger pre-trained LMs such as ELECTRA-large  (Clark et al., 2020) . 

 Conclusion We model our systems submitted to Task 2 of the WMT 2021 QE shared task on our proposed model, which uses dual pre-trained monolingual encoders and two additional cross attention networks to pro-cess the two input data src and mt more effectively considering that the latest Transformer-based QE models are not propped up by pre-trained monolingual word representations. We expect that the cross attention networks enable the two pre-trained monolingual encoders to exchange cross-lingual information without losing their stability and to learn the subtasks of Task 2 jointly and also separately. Experimental results partially supports this expectation: according to the official leaderboard, our systems outperform the baseline systems in terms of the mt-side words MCC and the sentence-level PCC by 0.4126 and 0.5497 respectively, although they do not in terms of the src-side MCC and the mt-side <GAP>s MCC. Neverhteless, it appears possible to improve the performance of our systems by adopting larger pre-trained LMs, and thus, our future work will explore such aspects and other related new methods. Figure 1 : 1 Figure 1: A diagram depicting the training task of ELECTRA (Clark et al., 2020)   
