title
QEMind: Alibaba's Submission to the WMT21 Quality Estimation Shared Task

abstract
Quality Estimation, as a crucial step of quality control for machine translation, has been explored for years. The goal is to investigate automatic methods for estimating the quality of machine translation results without reference translations. In this year's WMT QE shared task, we utilize the large-scale XLM-Roberta pre-trained model and additionally propose several useful features to evaluate the uncertainty of the translations to build our QE system, named QEMind. The system has been applied to the sentence-level scoring task of Direct Assessment and the binary score prediction task of Critical Error Detection. In this paper, we present our submissions to the WMT 2021 QE shared task and an extensive set of experimental results have shown us that our multilingual systems outperform the best system in the Direct Assessment QE task of WMT 2020.

Introduction Quality estimation (QE) aims to predict the quality of a machine translation (MT) system's output without any access to ground-truth translation references or human intervention  (Blatz et al., 2004; Specia et al., 2009 Specia et al., , 2018 . Automatic methods for QE are highly appreciated in MT applications when we expect to efficiently obtain the quality indications for a larget amount of machine translation outputs in a short time, or even at run-time. This paper describes Alibaba's submissions to the WMT 2021 Quality Estimation Shared Task. We developed a novel QE system, called QEMind, that have been applied to two tasks this year, the sentence-level direct assessment (DA) and binary score prediction of Critical Error Detection (CED). Common approaches in the previous years heavily focus on human-crafted rule-based feature engineering mode such as QuEst++  (Specia et al., 2015) . The features extracted are usually fed into traditional machine learning algorithms such as a support vector regression for the sentence-level scoring or a sequence-labeling model with conditional random fields for the word-level labeling respectively. With the development of neural networks applied in machine translation and other NLP tasks, a neural predictor-estimator framework for QE was proposed and achieved better results in WMT 2017 and WMT 2018 QE shared tasks  (Fan et al., 2019; Kim et al., 2017) . This framework extensively requires a pre-training procedure with a large amount of parallel corpora in the predictor mode and stacks a downstream estimator mode with additional layers for a supervised regression or classification task. Since 2019, state-of-the-art (SOTA) QE systems  (Kepler et al., 2019; Ranasinghe et al., 2020)  have hit record high with transfer learning by leveraging SOTA pre-trained NLP neural network models, for example, mBERT  (Pires et al., 2019)  and XLM-Roberta  (Conneau et al., 2019) . Till then, only "black-box" QE methods had been mainly used in WMT QE shared tasks. Furthermore, with the accessibility to the NMT systems, some "glass-box" QE features have been explored and verified to bring improvements upon "black-box" approaches  (Moura et al., 2020) . In addition,  Fomicheva et al. (2020)  have showed that useful information that are extracted from the MT systems performs good correlation with human judgements of quality. Inspired by these works, we propose more useful features in this paper, among which, some are derived from the NMT systems and others are created via utilizing the masked language model of XLM. We develop our QE systems by incorporating all the features that can potentially evaluate the uncertainty of the machine translations into a supervised QE model based on the transfer learning from XLM-Roberta. We evaluate our method on the Direct Assessment QE tasks of WMT 2020 and WMT 2021 and our experiment results demonstrate the efficiency and versatility of the features we have proposed on the quality estimation in different language pairs. 

 XLM-Roberta 

 Task & Data Set We participate the sentence-level Direct Assessment task and Critical Error Detection tasks of this year's QE shared task. (1) For the DA task, we merge 7000 and 1000 labeled data in the training and development data sets as our training set and treat the test20 data set as our development set for each of the seven language pairs. However, for the four zero-shot language pairs, we only have the blind test sets. (2) For the CED task, we observed that the distributions of two classes, NOT and ERR, are extremely unbalanced for all four language pairs. Therefore, we simply up-sample the samples with ERR labels to get a relatively balanced training set. This strategy of data augmentation has also been empirically verified to be valid. 

 Methodology In this section, we provide a complete view of our uncertainty feature enhanced approach, including: (1) The overall framework of QEMind is carried out in Section 3.1: how uncertainty features are combined with a pre-trained multilingual language model to enhance transfer learning; (2) Uncertainty features used in QEMind are described in Section 3.2: how uncertainty features are defined and extracted for translation quality estimation; (3) Strategies we applied in the WMT QE shared task to further improve the system's performance, such as data augmentation and model ensemble, are explained in Section 3.3. 

 QEMind Framework QEMind follows the general transfer learning procedure while allowing extra meta features to enhance the model. We concatenate the source text and machine translation and feed them into the pre-trained XLM-Roberta model to get the output representation of the special [CLS] token. Afterwards, the output representation is combined with the normalized uncertainty features described in Section 3.2. They are fed into a simple linear regression/classification layer to predict the continuous or binary quality score. The architecture of our feature enhanced model is shown in Figure  1 . This model is equivalent to TransQuest's  (Ranasinghe et al., 2020)  when no extra feature is used. Considering the size of the training set is small, we have not added extra parameters, such as bottleneck adapter layers used in  Moura et al. (2020) , to fuse uncertainty features and the output from XLM-Roberta.  Fomicheva et al. (2020)  proposed several "glassbox" features extracted from the NMT model. Estimating translation quality with these features achieves state-of-the-art results as an unsupervised approach. However, the performances of this approach are still far below those of the supervised model from transfer learning  (Ranasinghe et al., 2020) .  Moura et al. (2020)  combined limited "glass-box" features with the hidden state of a bottle-neck adapter layer attached on the output from the XLM-Robert, and the results indicate that these features can bring slight but significant improvements to the transfer learning model.  Wang et al. (2021)  proposed more unsupervised "glassbox" and "black-box" QE features and investigated further on the contributions of each one to the QE model's performance via a feature-enhanced supervised model. 

 Uncertainty Features Inspired by their work, we explored deeply in the aspect of uncertainty quantification to obtain uncertainty features in this section to enhance the transfer learning model. First, we extend "glass-box" features in  Fomicheva et al. (2020)  to the Decoding Probability Features and the Monte Carlo Dropout Features. And then, the Noised Data Features are proposed similar to the Monte Carlo Dropout Features. Decoding Probability Features. For autoregressive sequence generating models like Transformers  (Vaswani et al., 2017) , the decoding probability at each step can be extracted from the softmax layer directly in a "glass-box" setting: P (x,t,?) step = log P (y t |y <t , x, ?) (1) where x represents the input source text and y is the output machine translation. P step is a probability sequence with the same length of the generated sequence y. Three statistical indicators of P step can be used to estimate the uncertainty of the output: expectation, standard deviation, and the combined ratio of them: E(P step |x, ?) = 1 T T t=1 P (x,t,?) step (2) ?(P step |x, ?) = E(P 2 step |x, ?) ? E 2 (P step |x, ?) (3) Combo(P step |x, ?) = E(P step |x, ?) ?(P step |x, ?) (4) Intuitively, larger expectation, smaller deviation and larger combined ratio of P step indicate lower uncertainty and higher quality. P step is an extended version of the T P feature in  Fomicheva et al. (2020)  and the expectation of P step is the same as T P . Monte Carlo Dropout Features. Monte Carlo (MC) Dropout sampling, that has been exploited in  Gal and Ghahramani (2016) , is an efficient "glassbox" approach to estimate uncertainty. It enables random dropout on neural networks during inference and the predictive probabilities through different sampling paths are used to obtain measures of uncertainty  (Fomicheva et al., 2020) . The output sequences ? sampled across stochastic forwardpasses by MC dropout with different sampled model parameters ? can be different as well. If y is a high-quality output with low uncertainty, the Monte Carlo sampled outputs ? should be close to y and the variance of ? should be low. Hence, two measurements of sampling based on text similarity are carried out here: M C-Sim = Sim(y, ?i ) (5) M C-Sim-Inner = 1 N N j=1 Sim(? i , ?j ) (6) where ?i is the i-th sample of ?, and  1 ? i ? N . The expectation, standard deviation, and combined ratio of M C-Sim, M C-Sim-Inner and M C-P step are calculated over all MC dropout samples and will be used as "glass-box" uncertainty features.  One crucial point in designing this type of features is how to generate noised input x. One solution is Algorithm 1 Generate Noise Input with "Post-Editing" Require: input x = {x t |t = 1, 2, ..., T }, hyperparameters R, p i , p d . 1: Initialize x mask = x 2: for r = 1, ..., R do 3: x mask = randomly delete tokens from x mask with probability p d 4: x mask = randomly insert special <mask> tokens into x mask with probability p i 5: end for 6: x = M LM (x mask ), where M LM is a pretrained masked language model. 7: return x a "black-box" way that takes the advantage of the masking strategy of the pre-trained XLM-Roberta. Basically, we can mask some words in the source text and get a noised source text by the predictions from the pre-trained model in the masked positions. This simple approach only conducts substitutions on x with the [mask] token, but it limits the diversity of the noised sample inputs. To enrich the variety of x, we adjust the imitation learning algorithm in  Wang et al. (2020)  to a simplified version to obtain noised input x. We "post-edit" the input x by randomly deleting tokens and inserting masks for several rounds to get x mask . Then, the pre-trained XLM-R is used as a masked language model to predict the tokens in the masked positions of x mask to get the post-edited x. Pseudo codes of this "post-editing" algorithm is provided in Algorithm 1. 

 Strategies Multilingual Training. Considering zero-shot language pairs in the DA task, we mix up all seven language pairs' training data to fine-tune the XLM-Roberta model and predict on the whole test set including zero-shot language pairs. We have tried two different ways of mixing up training data from different language pairs to fine-tune XLM-Roberta: (1) source sentence + translation sentence; (2) English sentence + non-English sentence. Our experimental results demonstrate that multilingual models usually perform better than bilingual models trained on a single language pair, but there is no prominent difference in performance of the two different multilingual strategies. We keep both multilingual models and bilingual models for model ensemble. Data Augmentation. Two data augmentation strategies are applied for the CED task. First, considering the imbalance between positive and negative samples in the CED dataset, we up-sample the data withERR labels in each language pair to obtain a balanced dataset. Secondly, inspired by examples provided by the organizer, we have also tried to replace the original machine translation with a back-translated sentence and hope that the gap between the source sentence and the back-translated sentence can provide insights of the detection of potential critical errors. The back translations come from the released ML50 multilingual translation model  (Tang et al., 2020) . Model Ensemble. For the DA task, models trained with different multilingual strategies and different uncertainty features are ensembled by averaging predicted scores. While for the CED task, we average classification probability outputs from models trained with different data augmentation strategies and uncertainty features to obtain ensemble results. We apply a greedy ensemble strategy. First, all models are sorted by their performance on the development sets. Then, upon the best single model, we take one more model into the ensemble at each step until there is no more performance gain on the development sets or the maximum step is reached. We set the maximum step to avoid overfitting on the development sets. 

 Experiments 

 Model Settings We follow the model settings of Transquest  (Ranasinghe et al., 2020)  to fine-tune our QE model based on the XLM-Roberta large model with a classification/regression head on a single P100 GPU. The training batch size is set to 8 and the training process takes about 2 hours to convergence. For the DA task, the total number of parameters of QE-Mind with uncertainty features is 560981507; if no uncertainty features are used, it is 560941571. And for the CED task, the numbers of parameters with and without uncertainty features are 560982532 and 560942596 respectively. 

 Experiments of DA task We conduct all experiments and evaluate our model on last year's test sets to optimize model configurations for each language pair. In particular, the model performed best on all seven language pairs in average is selected to generate submissions for     et al. (2020) , which is the winner system of last year's DA task. QEMind-Bi and QEMind-Multi are models without uncertainty features, between which, the difference is that the model is trained on bilingual data or mixed multilingual data. QEMind-Multi + UNC is the complete QEMind model enhanced by various uncertainty features described in Section 3.2. Finally, predictions from bilingual models, multilingual models, and uncertainty features enhanced models are ensembled following Section 3.3, marked as QEMind Ensemble in the table. Results on the DA test sets of WMT 2020 show that: (1) multilingual strategies work well on this task, especially for high-resource language pairs; (2) the uncertainty features enhanced multilingual model achieves the highest performance among all single models, which verifies that these uncertainty features are useful to all language pairs and can be fused in multilingual models. (3) ensemble of multiple models of different settings can further improve the performance of QEMind systems. We pick the best single and ensemble models for each language pair and produce predictions on the newly released blind test sets of WMT 2021, including the 4 zero-shot language pairs. Results of Pearson's correlations are shown in Table  2  and Table  3 . 

 Experiments of CED task We test different strategies and uncertainty features on the CED development sets. Brief results of Matthews correlations (MCC) on development sets are shown in Table  4 . All models are trained on up-sampled training data of each language pair. From the results observations, compared to QE-Mind, which only applies up-sampling on the training data, the strategies of back-translation (QEMind + BK) and uncertainty features (QEMind + UNC) can achieve comparable or better performances. The ensemble of all these models makes a significant improvement. Similar to the DA task, the best single and ensemble models are picked to generate our final submissions. Results on test sets of this year are listed in Table  5 . 

 Conclusion This paper introduces our machine translation quality estimation model, QEMind, for the sentencelevel Direct Assessment and Critical Error Detection tasks of WMT 2021. We propose novel features to estimate the uncertainty of machine translations and incorporate them into the transfer learning from the large-scale pre-trained model, XLM-Roberta. Besides, three important strategies are particularly utilized for improving the QE system's performance such as multilingual training, data augmentation and model ensemble. Our system has achieved the first ranking in average Pearson correlation across all languages, including the zeroshot ones in the multilingual DA task of WMT 2021. Figure 1 : 1 Figure 1: Structure of the uncertainty quantification feature-enhanced model. 
