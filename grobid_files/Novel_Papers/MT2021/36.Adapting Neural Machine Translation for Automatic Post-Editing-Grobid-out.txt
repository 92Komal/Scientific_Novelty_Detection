title
Adapting Neural Machine Translation for Automatic Post-Editing

abstract
Automatic post-editing (APE) models are used to correct machine translation (MT) system outputs by learning from human post-editing patterns. We present the system used in our submission to the WMT'21 Automatic Post-Editing (APE) English-German (En-De) shared task. We leverage the state-of-the-art MT system  for this task. For further improvements, we adapt the MT model to the task domain by using WikiMatrix (Schwenk et al., 2021) followed by fine-tuning with additional APE samples from previous editions of the shared task 17,18)  and ensembling the models. Our systems beat the baseline on TER scores on the WMT'21 test set.

Introduction Automatic Post-Editing (APE) is the task of automatically correcting machine translation (MT) outputs. Along with fixing systematic errors in MT outputs, APE models can adapt general purpose MT systems to new domains and provide better translations to reduce the human post-editing effort . APE has seen significant progress with Transformer based models  (Yang et al., 2020; Lopes et al., 2019; Chatterjee et al., 2019 Chatterjee et al., , 2020  dominating the landscape as opposed to the earlier Statistical Machine Translation (SMT) based models  (Simard et al., 2007; B?chara et al., 2012)  and RNN based sequence-tosequence models  (Junczys-Dowmunt and Grundkiewicz, 2017) . To track this progress, WMT has been conducting APE shared tasks since 2015 on different data domains and language pairs  (Bojar et al., 2015 (Bojar et al., , 2016 (Bojar et al., , 2017 Chatterjee et al., , 2019 Chatterjee et al., , 2020 . WMT 2021's shared task focused on English-German and English-Chinese language pairs. We participated in the English-German sub-task and describe our submission in this paper. Participants * Work done as intern at Amazon Prime Video were provided a training set with 7000 instances and a development set with 1000 instances. Each dataset consisted of source, machine-translation, post-edit triplets. The source sentences came from the English Wikipedia, the MT outputs were generated with a black-box state-of-the-art MT system and the post-edits were created by professional translators correcting MT outputs. The test set consisted of 1000 pairs of source and MT outputs for which the participants had to submit the post-edits generated by their systems. The task organisers provided two additional synthetic post-editing datasets -'artificial training data'  (Junczys-Dowmunt and Grundkiewicz, 2016)  and 'eSCAPE corpus'  and permitted using additional data to train the model. TER scores  (Snover et al., 2006)  and BLEU  (Papineni et al., 2002)  scores were used as primary and secondary evaluation metrics respectively. Last year's entries primarily focused on transfer learning  (Yang et al., 2020; Lee, 2020;  and novel data augmentation techniques  (Lee et al., 2020b,a; . The winning submission  (Yang et al., 2020)  was based on finetuning a pre-trained machine translation model for the APE task. We take a similar line of approach by leveraging an existing state-of-the-art machine translation model. We first fine-tune an MT model on WikiMatrix  (Schwenk et al., 2021 ) -a mined bitext from Wikipedia -to bridge the domain gap, followed by further tuning to the APE task with post-editing samples. To deal with the limited training data, we exploit APE data from the previous editions of the WMT shared tasks. We describe the details of our experiments in Section 3 with gains and observations from individual tuning steps mentioned above. 

 Related Work The last year's WMT'20 APE shared task saw methods using transfer learning with data augmentation techniques perform well.  Yang et al. (2020)  fine-tune state-of-the-art transformer-based MT system on APE data using bottleneck adapter layers  (Houlsby et al., 2019)  to avoid overfitting. They additionally use outputs from an external MT system as input to the model and converged to ensembling to achieve 66.89 BLEU score on the WMT'20 development set to make it to the top of the final leaderboard. Data augmentation techniques where post-edits are synthesized to augment human-edited data was shown to be effective in the last year's submissions for addressing the training data limitation. However, data augmentation must be done carefully to prevent a mismatch between the error distributions in gold and synthetic data  (Yang et al., 2020) .  use data augmentation along with dual conditional cross-entropy model  (Junczys-Dowmunt, 2018)  based filtering to ensure data quality, model adaptation to target domain, and ensembling to achieve 56.06 BLEU on the development set and the second rank on the leaderboard. Similarly,  Lee et al. (2020b)  performed data augmentation by creating a novel noising scheme to synthesize four kinds of errors for APE training, namely, insertion, deletion, substitution and shifting/reordering noise to attain 53.77 BLEU score. The other submissions to the WMT'20 task used variations of the language models to generate edits.  Lee et al. (2020a)  trained a model by jointly optimizing losses for masked language and translation language models while Lee (2020) tailored a language model to make corrections by replacing poor quality words to improve the overall sentence-level quality. These two submissions were able to get 55.67 and 53.82 BLEU scores respectively on the WMT'20 development set. In comparison, our model is a pre-trained MT model adapted to the target domain and further fine-tuned on the APE data. These improvements give us about five absolute points gain over the no post-editing baseline (that returns MT output without changes) on the BLEU score to arrive at 55.85 which is competitive with all but one of last year's submissions on the WMT'20 development set.  

 Dataset Train 

 Method We describe our baseline model followed by the details of domain and task adaptation in this section. 

 Baseline translation model Limited by availability of training data, we used transfer learning approach (as is common in related tasks with few samples, see Ruder et al. (  2019 )) beginning with a pre-trained MT model. We used the MT models from FAIR's WMT'19 submission 1  that is an ensemble trained for the News Translation task using fairseq  library. It takes a single source sentence as input and returns translation in the target language. To use this model for the APE task, we concatenated the source and the machine-translation with a special token to make the input. Thus, we fine-tune the NMT model on the APE dataset with source <sep> machine-translation as input and post-edited reference as the output. 

 Pre-training on domain-specific data FAIR's WMT'19 NMT model was trained on Newscrawl and Commoncrawl datasets while the source of this year's APE data is Wikipedia. To fix the domain mismatch in NMT model's training data and our task, we fine-tune the NMT model on WikiMatrix  (Schwenk et al., 2021)  before finetuning the model with APE data. WikiMatrix is mined from Wikipedia using the multi-lingual sentence embeddings from the LASER toolkit  (Artetxe and Schwenk, 2019) . We ensure that the model is fine-tuned on only high-quality parallel data by using a higher threshold of 1.1 for extracting parallel sentences (rather than the default 1.04) to get 64k parallel sentences. 

 Fine-tuning on APE data To further address the data limitation, we use samples from earlier editions of the APE shared task;  WMT'16, WMT'17 and WMT'18  domain of the data in the previous editions of this shared task challenge is different from the current one, we preferred using this data over synthetic APE data similar to  (Yang et al., 2020) . We prefer this because unlike in WMT datasets where the post-edits are human revisions of the MT output, synthetic APE datasets have post-edited sentences independent of the MT output, causing the error patterns and data distributions to vary significantly. Hence, we combine the WMT'16, WMT'17 and WMT'18 datasets to get 45k source, machinetranslation and post-edit triplets. We present the details of the data in Table  1 . 

 Results and conclusion We report the results of our model on the WMT'21 development and test set. We use BLEU scores  (Papineni et al., 2002)  2 for quality estimates relative to a human reference and TER scores  (Snover et al., 2006)  for quantifying human post-editing effort. We report improvements over the Do Nothing baseline. This baseline refers to the system that returns the base machine translation output as the post-edit without any changes. We submitted the best performing single model and the ensemble model in Table  2  for evaluation. In Table  3  we present the results reported by the organizers for baseline, our model fine tuned on WMT'16-18 + WMT'21 (model A) and our ensemble model (A + B). The Do Nothing baseline from last year  (Chatterjee et al., 2020)  was reported at 50.21 BLEU score and this year it is reported at 71.07 BLEU score. These numbers suggest that the baseline machine translation engine used in this year's task proved to be of very high quality for the dataset used; leaving very little room for APE models to improve the translation similar to the observation made in . This is the only logical conclusion we could draw since the data used last year and this year are the same with human post-editing re-done. Using data from previous years' tasks clearly improves both BLEU and TER scores on the development set. While fine-tuning on WikiMatrix data itself has not led to improvements on the development set, it helps improve performance when used in ensemble with the other model. The model A beats the baseline on TER metric by 0.31 points on the test set while both our model A and ensemble system manage to outperform previous year's best entry. Further extending this work, we wish to study more carefully the impact of adaptation by switching the order of domain and task adaptation, effect of noise in training sample by tuning threshold  (Wieting and Gimpel, 2018) , and evaluate if synthetic data can be selectively augmented for greater metric gains. Table 1 : 1 WMT APE shared task data for En-De Dev Test Domain WMT'16 12000 1000 2000 IT WMT'17 11000 - 2000 IT WMT'18 13442 1000 3023 IT WMT'21 7000 1000 1000 Wikipedia 

 Table 2 : 2 . Although the Results on the WMT 2021 APE development set. Higher BLEU and lower TER is better. The "Ensemble" model is the ensemble of the two best performing single models (the ones with 69.12 and 69.34 BLEU scores). Model BLEU? TER? Do Nothing 68.79 19.06 MT fine-tuned on WMT'21 68.74 18.45 MT fine-tuned on (WMT'16-18 + WMT'21) (A) 69.34 18.27 MT fine-tuned on WikiMatrix and further on (WMT'16-18 + WMT'21) (B) 69.12 18.34 Ensemble (A + B) 69.38 18.18 Model BLEU? TER? Do Nothing 71.07 18.05 Model (A) 70.54 17.74 Ensemble (A + B) 70.50 17.85 

 Table 3 : 3 Results on the WMT 2021 APE test set. Higher BLEU and lower TER is better. The Model (A) is the one described in the same from Table2and the "Ensemble" model is the ensemble of the two best performing single models. 

			 transformer.wmt19.en-de 

			 calculated using multi-bleu.perl script from the Moses toolkit (Koehn et al., 2007)
