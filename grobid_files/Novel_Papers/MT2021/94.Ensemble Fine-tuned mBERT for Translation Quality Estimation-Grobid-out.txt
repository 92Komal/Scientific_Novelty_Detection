title
Ensemble Fine-tuned mBERT for Translation Quality Estimation

abstract
Quality Estimation (QE) is an important component of the machine translation workflow as it assesses the quality of the translated output without consulting reference translations. In this paper, we discuss our submission to the WMT 2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that challenge participants to predict the HTER score for sentence-level post-editing effort. Our proposed system is an ensemble of multilingual BERT (mBERT)-based regression models, which are generated by fine-tuning on different input settings. It demonstrates comparable performance with respect to the Pearson's correlation and beats the baseline system in MAE/ RMSE for several language pairs. In addition, we adapt our system for the zero-shot setting by exploiting target language-relevant language pairs and pseudo-reference translations.

Introduction Progress in machine translation (MT) has accelerated due to the introduction of deep learning based approaches, dubbed as neural machine translation (NMT)  Sutskever et al., 2014; . Several metrics (e.g., BLEU  (Papineni et al., 2002) , METEOR  (Agarwal and Lavie, 2008) ) are used to automatically evaluate the quality of the translations outputted by the NMT systems. However, these evaluation metrics require comparing the NMT outputs against human-prepared reference translations, which cannot be readily obtained. To tackle this predicament, recently quality estimation (QE)  (Blatz et al., 2004;  has emerged as an alternative evaluation approach for NMT systems. QE obviates the need for human judgements and hence can be efficiently integrated into the dynamic translation pipeline in the industry setting. * work done during internship at IQVIA QE is performed at different granularity (e.g., word, sentence, document)  (Kepler et al., 2019a) ; in this work we focus on the sentence-level postediting effort task, which predicts the quality of the translated sentence as a whole in terms of the number of edit operations that need to be made to yield a post-edited translation, termed as HTER  (Snover et al., 2006) . Sentence-level QE using neural approaches is generally treated as a supervised regression problem involving mainly two steps. In the first step, an encoder is used to learn vector representation/s of the source and translation sentences. While in the second step, the learned representations are passed through a sigmoid output layer to estimate the HTER score. These two steps can be performed either with a single model in an end-toend fashion (e.g., Bi-RNN  (Ive et al., 2018) ), or using two separate models (e.g., POSTECH  (Kim et al., 2017) ). The different QE systems vary in their choice of the encoder, which range from RNNbased to Transformer-based models. In this work, we leverage the fine-tuning capability of a Transformer-based encoder, namely the mBERT  (Devlin et al., 2018)  pre-trained model. Alongside the standard practice of feeding both the source and target (i.e., translation) sentences as the input sequence  (Kepler et al., 2019a; Kim et al., 2019) , we also explore other input settings based on only the target-side sentences (i.e., monolingual context). To this end, our final QE system is an ensemble of several mBERT models 1 , each generated by fine-tuning on a different input combination comprising the source and/or target sentences. We experiment with the following three input settings: (1) both source and target, (2) just target and (3) both target and a randomly-sampled target sentence in the data forming the input se-quence. Empirical analysis on 6 language pairs shows that the ensemble model is able to perform better than the individual fine-tuned models. Moreover, we provide experimental results for zero-shot QE, where training data for the test language pair is not available. This we tackle by improvising on the available training/dev data that match the target language of the test language pair and also by generating the pseudo-reference translations in that language. 

 Data We use the WMT21 QE Shared Task 2 sentencelevel data  (Specia et al., 2021; Fomicheva et al., 2020a,b)  for the following 7 language pairs: English-German (En-De), Romanian-English (Ro-En), Estonian-English (Et-En), Nepalese-English (Ne-En), Sinhala-English (Si-En), Russian-English (Ru-En) and Khmer-English (Km-En). Source-side data for each language pair includes sentences from Wikipedia articles, with part of the data gathered from Reddit articles for Ru-En. To obtain the translations, state-of-the-art MT models  (Vaswani et al., 2017)  built using fairseq toolkit  were used. The label for this task is the HTER score for the source-translation pair. Annotation was performed first at the word-level with the help of TER 2 tool. The word-level tags were then aggregated deterministically to obtain the sentence-level HTER score. The training, development, test and blind test data sizes for each language pair (except Km-En) are 7K, 1K, 1K and 1K instances respectively. As Km-En language pair was introduced for zero-shot prediction, only the test data containing 990 source and translation sentences was provided. 

 Our Approach A key innovation in recent neural models lies in learning the contextualized representations by pretraining on a language modeling task. One such model, the multilingual BERT (mBERT) 3 , is a transformer-based masked language model that is pre-trained on monolingual Wikipedia corpora of 104 languages with a shared word-piece vocabulary. Training the pre-trained mBERT model for a supervised downstream task, aka finetuning, has dominated performance across a wide spectrum of NLP tasks  (Devlin et al., 2018) . Our proposed approach leverages this fine-tuning capability of mBERT so as to form the component models in the ensemble QE system (Section 3.3). That is, each component model is a re-purposed mBERT that is fine-tuned for the sentence-level HTER score prediction task on one of the three input settings discussed in Section 3.2. 

 Fine-tuning mBERT for Regression mBERT's model architecture is similar to BERT 4 and contains the following parameter settings: 12 layers, 12 attention heads and 768 hidden dimension per token. However, the only difference is that mBERT is trained on corpora of multiple languages instead of just on English. This enables mBERT to share representations across the different languages and hence can be conveniently used for all language pairs in the WMT21 data. We first load the pre-trained mBERT model 5 and use its weights as the starting point of finetuning. The pre-trained mBERT is then trained on QE-specific input sequences (Section 3.2) for a few epochs such that the constructed sequence X is consumed by mBERT to output the contextualized representation h = (h CLS , h x 1 , h x 2 , ..h x T , h SEP ). Here, [CLS] is a special symbol that denotes downstream classification and [SEP ] is for separating non-consecutive token sequences. Considering the final hidden vector of the [CLS] token as the aggregate representation, it is then passed into the output layer with sigmoid activation to predict the HTER score: y = sigmoid(W ? h CLS + b) (1) W is a weight matrix for sentence-level QE finetuning that is trained along with all the parameters of mBERT end-to-end. 

 Input Settings We construct the input sequence for each language pair in the following three ways: SRC-MT: Given a source sentence s = (s 1 , s 2 , ...s N ) from a source language (e.g., English) and its translation t = (t 1 , t 2 , ...t M ) from a target language (e.g., German), we concatenate them together as X = ([CLS], t 1 , t 2 , ...t M , [SEP ], s 1 , s 2 , ...s N , [SEP ]) to form the input sequence. 

 MT: The target sentence is only used to form the input sequence, X = ([CLS], t 1 , t 2 , ...t M , [SEP ]). 

 MT-MT': Given the translation t for a source sentence s, we randomly sample another translation t' = (t 1 , t 2 , ...t K ) from the training data having HTER label close to t 6 . Although the source sentences for t and t' are different, we assume the additional monolingual context would help mBERT learn the correlating QE-specific features between t and t' for the target-side language. The resultant input sequence is X = ([CLS], t 1 , t 2 , ...t M , [SEP ], t 1 , t 2 , ...t K , [SEP ]). We fine-tune each of these mBERT models using AdamW optimizer  (Kingma and Ba, 2014; Loshchilov and Hutter, 2017)  for two epochs with a batch size of 32 and a learning rate of 2e ?5 . 

 Ensemble Model To take advantage of the individual strengths of the three mBERT component models fine-tuned the aforementioned input settings, we combine their HTER score predictions by training an ensemble model. In particular, we experiment with three different ensemble models -Gradient Boosting (Friedman, 2001), AdaBoost  (Freund and Schapire, 1997)  and Average. For Gradient Boosting and AdaBoost we use the implementation in scikit-learn 7 with 10-fold cross validation. The settings for Gradient Boosting are: number of estimators 600, learning rate 0.01, minimum number of samples 3 and other default settings. We use the default settings for AdaBoost. In Average ensembling, we average the HTER score predictions by the three mBERT models. Our system submission to WMT21 is based on Gradient Boosting as it gave the best performance on the test data, as shown in Table  1 . 

 Zero-Shot QE Performing sentence-level QE in the zero-shot setting presents a unique challenge as the QE system is expected to predict HTER scores for sentences in a test language pair (e.g., Km-En) without having been trained on any instances from that test  

 Avg AdaBoost GradBoost Pearson's 0.266 0.458 0.473 Spearman's 0.249 0.436 0.443 language pair. We address this by training on language pairs in the WMT21 QE data that match the target-side language (i.e., En) in the test language pair. The reason we focus on the target-side language is because the component mBERT models in the proposed ensemble QE system are finetuned on monolingual input sequences in the targetside language, which could potentially help the QE system generalize on the unseen test language pair. We consider the training and development data for the following language pairs in WMT21 QE data: Ro-En, Si-En, Et-En. Additionally, we augment this data by generating pseudo-references in the target language. A pseudo-reference (Scarton and Specia, 2014) is a translation for a source sentence that is outputted by a different NMT system than the one that produced the actual translations (e.g., transformer-based translation system proposed in  (Vaswani et al., 2017) ) and has shown to improve sentence-level QE performance (Soricut and Narsale, 2012). We use Google Translate 8 to get the pseudo-references in En for the Ro, Si and Et source sentences. The HTER scores for the translation and pseudo-reference pairs are then obtained using the TER tool. We train the ensemble QE system on the combined WMT21 QE data and the pseudo-reference parallel data, and test on the unseen test language pair. 

 Baseline The baseline QE system (BASELINE) set by the WMT21 organizers this year is the Transformerbased Predictor-Estimator model  (Kepler et al., 2019b; Moura et al., 2020) . XLM-RoBERTa is used as the Predictor for feature generation. The baseline system is fine-tuned on the HTER scores and word-level tags jointly. 

 Results Table 2 presents the experimental results of mBERT fine-tuned on the SRC-M T , M T and M T -M T  input settings, as well as the performance of the ensemble of the three mBERT models, which we call ENSBRT. First, comparing among the three input settings, it seems that mBERT exhibits competitive results even when it does not have knowledge of the source-side text in the M T and M T -M T settings, in particular for the following language pairs -En-DE, Si-En, Ne-En. While the ensemble mBERT model, ENSBRT, outperforms the independent counterparts for all the language pairs. This shows that the ensemble method can help to balance out the weakness of any component model, thereby benefiting the sentence-level QE task overall. We also visualize ENSBRT's predictions against the ground truth HTER scores in Figure  1 . Table  3  compares the QE performance between the BASELINE and ENSBRT in terms of Pearson's correlation, RMSE and MAE on the WMT21 blind test set, for which the ground truth HTER scores were not available at the time. We submitted results for 6 language pairs (En-De, Ro-En, Ru-En, Si-En, Et-En, Ne-En) in the normal QE setting and one language pair (Km-En) for zero-shot prediction. ENSBRT demonstrates comparable performance to the BASELINE for Pearson's and outperforms it in either MAE or RMSE for the following language pairs: En-De, Ru-En, Et-En and Ne-En. 

 Conclusion In this work, we describe the ENSBRT system submission to the WMT21 QE Shared Task. ENSBRT is based on fine-tuning the multilingual BERT pretrained model for sentence-level translation quality score prediction. We explore three different input settings for fine-tuning which include either bilingual or monolingual context, and combine the predictions of the three models using ensemble methods as our final system. Furthermore, zeroshot QE is facilitated by using labeled data for existing language pairs and pseudo-references that align with the target language of the unseen test data.   ENSBRT (i.e., predicted (red) ) against the gold labels (i.e., original (blue)) for 6 language pairs on the test set. X-axis represents each data point and Y-axis is the HTER score. The closer the corresponding red line and blue dot are to each other the better, as we expect the HTER prediction to be same as or close to the ground truth. Figure 1 : 1 Figure 1: Visualization comparing HTER score predictions byENSBRT (i.e., predicted (red)) against the gold labels (i.e., original (blue)) for 6 language pairs on the test set. X-axis represents each data point and Y-axis is the HTER score. The closer the corresponding red line and blue dot are to each other the better, as we expect the HTER prediction to be same as or close to the ground truth. 
