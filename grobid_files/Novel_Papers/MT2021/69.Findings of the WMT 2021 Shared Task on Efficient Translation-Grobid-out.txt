title
Findings of the WMT 2021 Shared Task on Efficient Translation

abstract
The machine translation efficiency task challenges participants to make their systems faster and smaller with minimal impact on translation quality. How much quality to sacrifice for efficiency depends upon the application, so participants were encouraged to make multiple submissions covering the space of tradeoffs. In total, there were 53 submissions by 4 teams. There were GPU, single-core CPU, and multi-core CPU hardware tracks as well as batched throughput or single-sentence latency conditions. Submissions showed hundreds of millions of words can be translated for a dollar, average latency is 5-20 ms, and models fit in 7.5-150 MB.

Introduction The efficiency task complements the collocated news task by challenging participants to make their machine translation systems computationally efficient. This is the fourth edition of the task, expanding upon previous editions  (Heafield et al., 2020; Hayashi et al., 2019; Birch et al., 2018) . Participants built English?German machine translation systems following the constrained data condition of the 2021 Workshop on Machine Translation news translation task. For translation quality measurement, we use the same news-focused WMT21 test set and human evaluation protocol as the news task. However, human assessment was conducted separately from the evaluation of the news task submissions. Submissions are made as Docker containers so we can consistently measure their performance in terms of quality, speed, memory usage, and disk space. We run the containers in three different hardware environments: one GPU, one CPU core, and multiple CPU cores. Systems were tested for throughput by providing 1 million sentences upfront to allow batching and parallelization. We also tested for latency with a program that drip-feeds one input sentence, waits for the translation, and then provides the next input sentence. There were five conditions in total: GPU Batch (for throughput), GPU Latency, 1 CPU Core Batch, 1 CPU Core Latency, and 36 CPU cores Batch. We did not measure latency in a multi-core CPU setting because the test hardware has 36 cores and overhead for 36 threads is larger than the cost of arithmetic for the small tensors in optimized models. Participants were free to choose which conditions to participate in. The condition was passed to the Docker container as command line arguments. Table  1  shows the four participants and the conditions they submitted to. Machine translation is used in a range of settings where users might choose different trade-offs between quality and efficiency. For example, a highfrequency trading system might prefer the lowest latency at the expense of quality given that the output will only be read by a machine. Conversely, in a post-editing scenario the personnel costs outweigh many computational costs. Therefore there is not a single best system, but a range of options that trade between quality and efficiency. We emphasize the Pareto frontier: the fastest systems at each level of quality, or the smallest systems at each level of quality. To explore the Pareto frontier, participants were encouraged to make multiple submissions covering the range of trade-offs. In total, 53 combinations of models, hardware, and batching were benchmarked. 

 2 Hardware We chose modern hardware to encourage exploiting new hardware features. The GPU is an NVidia A100 from the Oracle Cloud BM.GPU4.8 instance. The instance has eight GPUs and we limited Docker to using only one GPU. The GPU machine has an AMD EPYC 7542 CPU with all cores allowed. In practice, most submissions used only one core while NiuTrans's submissions used the CPU cores to parallelize preprocessing and postprocessing. The CPU-only condition used a dual-socket Intel Xeon Gold 6354 from Oracle Cloud BM.Optimized3.36 with a total of 36 cores. For the single-core CPU track, we reserved the entire machine then ran Docker with -cpuset-cpus=0. In the 36-core CPU track, participants were free to configure their own CPU sets and affinities. The Oracle Cloud machines are bare metal servers, meaning there was no shared tenancy, no virtualization, and the test machines were otherwise quiescent. 

 Input Text To amoritize loading time, avoid starving highly parallel submissions, and reduce the ability to cheat, we benchmark systems on 1 million sentences of input. The test set is hidden inside these 1 million sentences, shuffled with filler sentences. Many filler sentences are drawn from parallel corpora to check that systems are in fact translating all sentences, though we do not consider scores on noisy corpora reliable enough to report. The composition of this set changes each year and is decided after the submission deadline. Filler data was gathered from parallel corpora and gender bias challenge sets: WMT news test sets from 2008 through 2021  (Barrault et al., 2020) , the additional test inputs in WMT 2021, Khresmoi summary test v2  (Du?ek et al., 2017) , IWSLT 2019  (Jan et al., 2019) , SimpleGen  (Renduchintala et al., 2021) , WinoMT  (Stanovsky et al., 2019) , TED 2020  (Reimers and Gurevych, 2020) , and Tilde RAPID 2019  (Rozis and Skadin , ?, 2017) . We capped sentence lengths at 150 space-separated tokens, except for the WMT 2021 test set to preseve the ability to evaluate with it. Because WMT 2020 includes excessively long segments that are actually concatenated sentences, we also added sentence split versions of WMT 2020 and WMT  2021, though the difference on WMT 2021 was minor. Source sentences were concatenated, deduplicated, and shuffled. The Tilde RAPID corpus was clipped to make a total of 1 million deduplicated lines. Counts are shown in Table  2 . Input text and tools to extract test sets from system outputs are available at http://data.statmt.org/heafield/ wmt21-testdata.tar.xz . The input file has 1,000,000 lines, 19,951,184 space-separated words, and 124,257,215 bytes (most of which are characters since the file is English in UTF-8). This is an average of 20 words per sentence compared to 15 words per sentence the previous year  (Heafield et al., 2020)  due to raising the cap from 100 to 150 tokens per sentence and the lengthy text in the RAPID corpus. Teams were responsible for their own tokenization and detokenization. We provided raw UTF-8 English input text with one sentence per line. 

 Metrics 

 Cost Time was measured with wall (real) time reported by time and CPU time reported by the kernel for the process group. We do not measure loading time because it is small compared to translating 1 million sentences, some tools load lazily, and it is easily gamed by padding loading time. Peak RAM consumption was measured using memory.max_usage in bytes from the kernel for the CPU and by polling nvidia-smi for the GPU. Swap was disabled. Participants were told to separate their Docker We report size of the model directory captured before the model ran. We also measured the total size of the Docker image (after compressing with xz), though participants were encouraged to prioritize shipping one container for multiple hardware conditions over the size of the container. 

 Quality Translation quality is measured on the WMT 2021 news test set. The automatic metrics are COMET  (Rei et al., 2020)  wmt20-comet-da from version 1.0.0rc6, BLEU from sacrebleu  (Post, 2018)  nrefs:3|case:mixed|eff:no|tok:13a |smooth:exp|version:2.0.0, and chrF also from sacrebleu. We use references A, C, and D because the organizers found postedited DeepL output in reference B. COMET does not natively support multiple references so we averaged as recommended by the authors.  1  We also averaged chrF across references. Results were presented to participants 2 who were encouraged to whittle down systems for a focused human evaluation. HuaweiTSC and TenTrans included all of their submissions. NiuTrans included their GPU submissions but not their CPU submissions that have lower automatic scores than Edinburgh's. This left GPU Batch and 1 Core Latency as the only conditions with multiple teams. Edinburgh kept systems that have competitors and are near the Pareto frontier. The number of submissions evaluated is shown in Table  3 . Out of 53 submissions, we ran direct assessment on 18. For human evaluation, as a source of the absolute quality measure we used document-level source-based direct assessments (DA)  (Graham et al., 2013; Cettolo et al., 2017)  following the procedure established at the WMT20 News Translation Task  (Barrault et al., 2020) . We also conducted contrastive evaluation using segment-level pairwise direct assessments  (Novikova et al., 2018; Sakaguchi and Van Durme, 2018) , because it can be a better discriminative tool for measuring relative quality difference between pairs of systems. We compared the 18 systems using source-based direct assessment and 58 pairs of systems with contrastive direct assessment. In total, we gathered 21,487 and 20,416 direct assessment scores in standard and contrastive campaigns respectively. All annotations were made by bilingual native German speakers with a translation or linguistics background. Annotations were collected using Appraise 3  (Federmann, 2018) . 

 Results All submissions are shown in Table  4 . Sourcebased direct assessment scores appear for the submissions in the focused human evaluation with the number of wins against other systems (including those in other conditions), raw direct accessment score, and z-score after standardizing annotator scores to mitigate differences in annotator scores. Scores were averaged ("Ave.") across sentences. Rows are sorted by COMET because only some submissions have human assessment. The system ranking based on the standard DA is presented in Table  5 . Systems are ordered by the number of respective wins against other systems and average DA z-score. Ordering solely by z-scores would produce three clusters with all systems within a cluster considered tied according to Wilcoxon rank-sum test with p < 0.05.   Figure  1  shows the trade-off between quality and speed of batched translation submissions. Since source-based DA is available for select GPU submissions, we include that comparison; the other plots rely on COMET to approximate quality. Each plot shows the Pareto frontier as a black staircase to highlight the best combinations of quality and speed. In Figure  2 , we combine GPU and 36 Core CPU speed by using Oracle Cloud pricing. The GPU is cheaper for throughput-oriented tasks that allow batching. 

 642 

 NVIDIA A100 GPU Batch Latency is shown in Figures  3 and 4 . HuaweiTSC and Edinburgh were the two participants and shared the Pareto frontier. While the GPU is cheaper for throughput, both CPU and GPU entries appear on the Pareto frontier for latency. In fact, the lowest latencies are achieved by singlecore CPU submissions, likely due to the overhead of launching small kernels on a GPU. Model sizes at rest on disk appear in Figures  5  and 6 . Participants were allowed to compress their models using their own tools and standard tools like xz. The entire Pareto frontier consists of Edinburgh submissions, resting partly on 4-bit integer compression. Docker image sizes, which include model and software, appear in Figure  7 . HuaweiTSC optimized their image size well. Conversely, some others opted to optimize other metrics and included large Linux installations. We compressed all docker images with xz before measuring.  8 . GPU memory consumption reflects batch size and some participants set a large batch size to maximize speed. Optimizing speed for multisocket CPU machines implies having a copy of the model in RAM close to each socket, so memory consumption is larger beyond simply having temporary space for more batches. Finally, participants may have sorted the entire 118 MB input file in RAM to form batches of equal length sentences. NiuTrans is the clear winner on GPU RAM consumption and curiously the clear loser on CPU RAM consumption. 

 Memory (RAM) consumption appears in Figure Many of the systems tied on standard DA and contrastive DA helps us pull them apart by directly comparing system outputs. Table  6  shows detailed results of contrastive DA including average scores, respective deltas between two systems and the outcome of significance testing. For groups of systems for which we evaluated each system from a group against each other system from the same group, we created separate rankings based solely on pairwise comparisons within the group, presented in Table  7 . 

 Conclusion and Future Tasks Using the highest quality system in this evaluation, translating 124,257,215 characters took 140 seconds on an A100 GPU that costs $3.05/hr in a cloud. That is $0.001/million characters. By comparison, Google Translate's cost is $20/million             6 : Results of the pairwise contrastive direct assessment human evaluation for each evaluated system pair. The stronger system on the left is considered better than the weaker system on the right according to the Wilcoxon rank-sum test with p < 0.05 for * , p < 0.01 for * * , p < 0.001 for * * * . line and the efficient task deadline was too short and some teams noted this reduced the conditions they participated in. In addition, scaffolding would reduce the barrier to participation. This could take the form of providing a trained high-quality model, providing distilled  (Kim and Rush, 2016)  training data, or even optimized models where only the toolkit code is optimized. Providing this scaffolding would effectively require the organizers to perform the full task before releasing it to participants. If the training and test data are renewed each year as a countermeasure to overfitting and a participant that cheated, this would require more time between the news task and release of the news test set references. German is a high resource language, which raises the computational cost of participation. A medium resource language would generally reduce training costs and explore whether results apply in this data condition. The next task should aim to recruit more participants and perhaps separate the organization from one of the participants. Speed on GPU with source-based direct assessment for select systems 
