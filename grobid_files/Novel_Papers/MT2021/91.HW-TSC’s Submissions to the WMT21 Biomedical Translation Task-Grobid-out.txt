title
HW-TSC's Submissions to the WMT21 Biomedical Translation Task

abstract
This paper describes the submission of Huawei Translation Service Center (HW-TSC) to WMT21 biomedical translation task in two language pairs: Chinese?English and German?English (Our registered team name is HuaweiTSC). Technical details are introduced in this paper, including model framework, data pre-processing method and model enhancement strategies. In addition, using the wmt20 OK-aligned biomedical test set, we compare and analyze system performances under different strategies. On WMT21 biomedical translation task, Our systems in English?Chinese and English?German directions get the highest BLEU scores among all submissions according to the official evaluation results.

Introduction We have witnessed great progress made by neural machine translations  (Bahdanau et al., 2015; Vaswani et al., 2017)  in recent years. However, domain adaptation remains to be a tough issue. As noted by  Koehn and Knowles (Koehn and Knowles, 2017) , translations by NMT systems in out-ofdomain scenarios are relatively poor, and highquality data in specific domains are difficult to obtain, which pose great challenges to certain translation tasks (e.g. biomedical translation). To address the domain adaptation issue, on one hand, we leverage data diversification  (Nguyen et al., 2020) , forward translation  and back translation  (Sennrich et al., 2016a; Edunov et al., 2018)  to generate synthetic in-domain corpora. On the other hand, fine-tuning  (Sun et al., 2019)  and ensemble  (Freitag et al., 2017; Li et al., 2019)  are used to further enhance system performances in the biomedical domain. We introduce our data strategy in section 2, and model architectures as well as model enhancement techniques in section 3. Section 4 presents experimental results of both language pairs on the wmt20 OK-aligned biomedical test set. Section 5 is a conclusion of our work. 

 Dataset 2.1 Data Source Our baseline model is trained with out-of-domain WMT21 news data. The sizes of bilingual and monolingual data for Chinese?English and German?English language pairs are shown in Table  1 . With regard to in-domain data, we use both the bilingual data and monolingual data provided by the WMT21 Biomedical Translation Shared task. For German?English task, we select Biomedical Translation and UFAL Medical Corpus as in-domain training data. Besides, 21.43M inhouse English monolingual data are used. For Chinese?English task, the used in-house data includes: 1.35M parallel data, 21.43M English monolingual data, and 36.11M Chinese monolingual data. Table  2  shows the details of data in the biomedical domain for German?English and Chinese?English tasks. 

 Data Pre-processing Our data pre-processing methods include: ? Filter out repeated sentences  (Khayrallah and Koehn, 2018; . ? Normalize punctuations using Moses  (Koehn et al., 2007) . ? Filter out sentences with repeated fragments. ? Filter out sentences with mismatched parentheses and quotation marks. ? Filter out sentences of which punctuation percentage exceeds 0.3. ? Filter out sentences with the character-toword ratio greater than 12 or less than 1.5. ? Filter out sentences with more than 120 words. ? Apply langid  (Joulin et al., 2017 (Joulin et al., , 2016  to filter sentences in other languages. ? Use fast-align  (Dyer et al., 2013)  to filter sentence pairs with poor alignment. It should be noted that for the German?English translation task, we employ joint SentencePiece model(SPM)  (Kudo and Richardson, 2018; Kudo, 2018)  for word segmentation, with the size of the vocabulary set to 32k. As for the Chinese?English translation task, Jieba tokenizer is used for Chinese word segmentation while Moses tokenizer for English word segmentation. Byte Pair Encoding (BPE)  (Sennrich et al., 2016b ) is adopted for Chinese and English sub-word segmentation. We train BPE models with 32,000 merge operations for both the source and target sides. 3 System overview 3.1 Model Our system uses Transformer  (Vaswani et al., 2017)  model architecture, which adopts full self-attention mechanism to realize algorithm parallelism, accelerate model training speed, and improve translation quality. Two Transformer deep-large model architectures are used in our experiments: ? Deep 25-6  (Wang et al., 2018; Li et al., 2019) : Based on the Transformer-base model architecture, the deep 25-6 model features 25-layer encoder, 6-layer decoder, 1024 dimensions of word vector, 4096-hidden-state, 16-head self-attention and layer normalization. We use the open-source Fairseq  (Ott et al., 2019)  for training. The main parameters are as follows: Each model is trained using 8 GPUs. The size of each batch is set as 2048, parameter update frequency as 32, learning rate as 5e-4  (Vaswani et al., 2017)  and label smoothing as 0.1  (Szegedy et al., 2016) . The number of warmup steps is 4000, and the dropout is 0.1. We also use the Adam optimizer (Kingma and Ba, 2015) with ?1 = 0.9, ?2 = 0.98. In the inference phase, The beam-size is 8, The length penalties for Chinese?English and German?English are set to 0.5, and the length penalties for the other two directions are set to 1.5. 

 Data augmentation Given the small size of in-domain bilingual data, how to generate more training data becomes a crucial issue for model performance enhancement in the biomedical field. We adopt three data augmentation methods: ? Data diversification  (Nguyen et al., 2020) : Data diversification is a simple but effective strategy to enhance the performance of NMT. It uses predictions from multiple forward and backward models and then combines the results with raw data to train the final NMT model. The method does not require additional monolingual data and is suitable for all types of NMT models. It is more efficient than knowledge distillation and dual learning, and exhibits strong correlation with model integration. In our Chinese?English and German?English systems, we use only the forward model and the backward model to create synthetic data and add the data to the original parallel corpora. ? Forward translation : Forward translation usually refers to using source language monolinguals to generate synthetic data through beam search decoding, and then add synthetic data to the training data so as to increase the training data size. Although merely using forward translation may not work well, forward translation can be used in conjunction with a back translation strategy, which also works better than using back translation alone. We do not use forward translation for the German?English system task due to the lack of high-quality in-domain German monolinguals. We then give up forward translation for the English?German direction because forward translation and back translation cannot be used jointly for better effects. Ultimately, we only adopt forward translation for our Chinese?English systems. ? Back translation  (Edunov et al., 2018) : Back translation translates target side monolingual data back to the source language so as to increase the training data size, which has been proved to be an effective method to improve neural machine translation performances. There are many methods for generating synthetic corpus through back translation. In a non-extremely low-resource scenario, sampling or noisy beam search decoding method is more effective than beam search or greedy search, and the synthetic data generated by sampling or noisy beam search decoding method may introduce more diversity to training data. In our experiment, sampling decoding is adopted. We use back translation for all directions expect English?German, due to the lack of high-quality in-domain German monolinguals. 

 Training strategy We first use in-domain training data to conduct incremental training with baseline models trained by WMT21 news data for domain transfer. Then, we use three monolingual enhancement strategies, data diversity, forward translation and back translation, to create synthetic data and add them to the in-domain training data to further expand the scale of the training data, and then perform incremental training again. In addition, we fine-tune our models with test sets from previous years of the same task in hope of further improving in-domain performances. Specifically, we ensemble multiple models to forward translate the source side of test sets to increase the size of the training data, and then add noise  (Meng et al., 2020)  to the target side of the training data to achieve a better fine-tuning effect. Finally, multiple models are ensembled to achieve better performance. Algorithm 1: Strategies for selecting ensemble models Input : The list of all NMT models to be selected M := [M 1 , ..., M n ], n is the Number of M , and the test Set T ; Output : The optimal model combination B := [M i , ..., M j ];  

 Ensemble For each translation task, we randomize two sets of training data and train four models using the two model architectures mentioned above. In the course of our experiments, we find that directly ensemble all models does not necessarily perform better on test set than a single model. To achieve a better ensemble effect, we design an algorithm, as shown in the algorithm 1. The core idea is to traverse all combinations of models and find the best one in the test set. The experiment results show that ensemble with the best combination found by the traverse strategy is much better than simply ensemble all models. In our experiment, the model combination that performs best on the wmt20 OK-aligned biomedical test set is used as the final submission. 

 Experimental result We train baseline models using WMT21 news data, then incrementally train them using medical bilingual corpora and synthetic data generated by data augmentation techniques, fine-tune models with previous years' test sets, and finally ensemble multiple models to produce submitted results. We benchmark our submissions using the WMT20 OKalign test set. BLEU scores are calculated using the MTEVAL script from Moses  (Koehn et al., 2007) . The results are shown in Table  3 and Table 4 . Our models outperform last year's official best results in three language directions. The tables mainly show the results of deep 35-6 models. Only in the last ensemble phase, multiple model architectures are used. we compare our results with the best official results from last year. We notice that our baseline models trained by WMT news data may also perform quite well in the biomedical field. For example, in German?English, Our baseline model is only 2.2 BLEU below last year's best result. 

 Chinese?English For Chinese?English task, we first train the baseline model on WMT21 news data.  

 German?English For German?English task, the model training strategy used is similar to that for Chinese?English task, except data augmentation techniques. As mentioned above, due to the lack of in-domain German monolingual data, we use data diversity and back translation strategies for German?English direction and only data diversity for English?German direction. The German?English experiment results are shown in Table  4 . Data augmentation results in significant performance improvements, with 1.1 BLEU and 1.7 BLEU on German?English and English?German respectively. Fine-tuning with previous years' test sets has also improved the quality of in-domain translations. On German?English, we fine-tune the model with wmt18 and wmt19 test sets and see an improvement of 1.1 BLEU. On English?German, fine-tuning leads to an increase of 0.4 BLEU. 

 System English?German German?English baseline 33.8 39.5 + biomedical corpus 34.9 (+1. Ensemble the model combinations found through the ergodic approach contribute to 0.7 BLEU increase for German?English and 0.6 BLEU for English?German. Ultimately, due to the lack of effective in-domain German monolingual data, we only surpass last year's official best results on German?English direction. 

 Conclusion This paper presents the submissions of HW-TSC to the WMT21 Biomedical Translation Task. We perform experiments with a series of pre-processing and training strategies. The effectiveness of each strategy is demonstrated by our experiment results. Combining with data augmentation strategies, incremental training with in-domain data on the basis of a baseline model from new domain can effectively improve in-domain translation quality. Our systems in English?Chinese and English?German directions get the highest BLEU scores among all submissions according to the official evaluation results. 1 Initialize the test set T 's maximum BLEU score maxbleu := 0; 2 Initialize the optimal model combination B := []; 3 for num ? range(1, n) do 4 Generate a list of model combination numlist, which is all possible combination of num models in M ; 5 for current model combination subnumlist ? numlist do 6 Calculate the current BLEU score curbleu of the current combined model on the test set T .; 
