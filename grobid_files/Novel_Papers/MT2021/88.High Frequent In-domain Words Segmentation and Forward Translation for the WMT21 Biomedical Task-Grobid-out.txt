title
High frequent in-domain word segmentation and forward translation for the WMT21 Biomedical task

abstract
This paper reports the optimization of using the out-of-domain data in the Biomedical translation task. We firstly optimized our parallel training dataset using the BabelNet in-domain terminology words. Afterward, to increase the training set, we studied the effects of the out-of-domain data on biomedical translation tasks, and we created a mixture of in-domain and out-of-domain training sets and added more in-domain data using forward translation in the English-Spanish task. Finally, with a simple bpe optimization method, we increased the number of in-domain subwords in our mixed training set and trained the Transformer model on the generated data. Results show improvements using our proposed method. 041 using Babelnet to include biomedical sentences 2) 042 Implementing subwords bpe optimization on the 043 train set to study the adaptation of out-of-domain

Introduction Domain adaptation is one of the known challenges in Machine Translation since NMT(neural machine translation) models are susceptible to the training data  (Koehn and Knowles, 2017) . To say, NMT models perform poorly for domain-specific translation when trained on large out-resource data  (Chu and Wang, 2018) . As a result, due to the limitations of specific domain data, domain adaptation strategies help NMT models by increasing the parallel corpora. There have been several tasks to address domain adaptation which recently, in  (Sato et al., 2020)  they proposed a vocabulary adaptation to fine-tune the embedding layers of the NMT model by projecting general word embeddings induced from monolingual data in a target domain onto a source-domain embedding space to improve translation score. On the other hand, augmenting bilingual training data with forwarding and backward translation improves the in-domain translation quality  (Nayak et al., 2020) . Inspired by mentioned ideas, in this work, we implemented our strategy by two essential steps: 1) collecting and augmenting data by forwarding translation and then tuning it Algorithm 1: Optimizing the parallel corpus using BabelNet; selecting sentences that contain at least one token in-domain word  

 Experiments Experiments illustrated in this section study the effects of the out-of-domain data on indomain(biomedical) translation task as well as the possibility of adapting it by performing a tuned subword-bpe segmentation algorithm 3 to improve the translation quality. We split this section into four parts which start with data collection and prepossessing. Then, we describe the training system and, finally, the evaluation scores of the competition. 

 Data collection We rely on the WMT21 official webpage to collect the (en/es) parallel in-domain data. Out of the provided resources, in particular, for the in-domain train set, we selected UFAL, Pubmed, Medline, IBECS  (Villegas et al., 2018)  and UNcorpus  (Ziemski et al., 2016)  along with the OPUS collection  (Tiedemann, 2012) . Next, we cleaned the data by removing empty lines, duplicates, and very short and long sentences. Also, to perform our experiments on out-of-domain data, we collected the parallel sentences provided from the same WMT21 official website. 

 Data preprocessing To prepare our data for training, we followed the standard pipelines by performing normalization, tokenization, and removing words that contain nonalphabetic characters using Moses  (Koehn et al., 2007) . Then, we removed concise and long sentences by keeping the thresh-hold between 2 and 30 words for each sentence and implemented the strategy described in section 2 to select in-domain sentences. As a report, we collected 6,855,049 in-domain and added 1,965,824 out-of-domain parallel data (English/Spanish). We also translated 1,558,834 in-domain UFAL monolingual English data to Spanish and added it to our bilingual corpus for retraining the en/es model. 

 Training on optimized segmented data Our method focuses on data preparation and investigates how the out-of-domain data affects the BLEU score. We imply that tuning the vocabulary of subwords would improve the accuracy of the indomain translation(biomedical) even though some of the data is out of the domain. The two crucial factors applied in our experiments are preprocessing the parallel corpus with BabelNet, and tuning the learning step of subwords to adapt out-of-domain data. following the strategy, four experiments have been done with two different trainsets, in-domain and mixture of in-domain and out-of-domain data: 1. In the first experiment, we used word-level data in both the source and target sides to evaluate the impact of out-of-domain usage in an in-domain task. 2. In the second experiment, we applied subword-bpe level on both source and target side with shared embeddings; however, the data were preprocessed by using Babelnet (described in section 2) to adjust the in-domain sentences in the train set for all the experiments. 3. We used the same strategy as the second experiment but with applying BPE-dropout  (Provilkov et al., 2019)  on both the source and target side of the data. 4. The last experiment was carried out by using tuned in-domain subword level data on both source and target sides as explained in the section 3. In all experiments, we trained baselines on wordlevel and subword-bpe level to measure the proposed methods. We selected a vocabulary size of 50k tokens and trained the data by the Transformer model with its default parameters using Open-nmt  (Klein et al., 2017)  neural machine translation framework. 

 Evaluation and results The evaluation has been done on WMT18 and WMT19 test sets based on the BLEU score. We compared the trained models with word-level, standard subword bpe level, bpe drop out and tuned subword bpe level of the parallel corpus in the trainset to follow our experiments. We also studied the results with three types of trainsets:   2. 2 2 In-domain forward translation 078 Considering a translation task of L1 ? L2, where 079 L1 has more significant monolingual data than L2, 080 a forward translation translates the L1 to L2 and 081 uses the translated L2 to recreate a synthetic paral-082 lel corpus. It has been widely reported that forward 083 and back translation brings significant results. (Bo-084 goychev and Sennrich, 2019). We benefited from 085 this fact and produced bilingual data from the En-086 glish source, which did not have any target or good 087 target parallel translation. However, to ensure the 088 availability of in-domain data, we first passed the 089 previous step on the available monolingual side. 090 Then we translated the source side using our MT 091 model and added bilingual data for retraining. Fi-092 nally, we merged the in-domain and out-of-domain 093 parallel corpus to achieve a bigger train set. 094 3 Subword BPE optimization 095 Byte Pair Encoding, or BPE, is a subword segmen-096 tation algorithm that encodes rare and unknown 097 words as sequences of subword units by merging 098 the most frequent consecutive byte pair into a new 099 subword (Sennrich et al., 2015). Since we enriched 100 the train set with out-of-domain data, We propose 101 "bpe-terms in-domain optimization" to handle open 102 vocabulary problems and enhancing the morphol-103 ogy when out-of-domain data is available. Con-104 sequently, increasing the frequency of in-domain 105 words in the subword bpe training raises the chance 106 of having in-domain words in the vocabulary. As a 107 result, out-of-domain data will not affect the quality 108 of the model on translating the in-domain words, 109 while they let the model learn on an enormous cor-110 pus. We performed this strategy by first learning 111 the subwords on 10x duplicated in-domain parallel sentences with a size of eight million mixed with smaller out-of-domain corpora (no duplication) and then applying the trained subword model on the standard-sized corpus. After that, we expect to have the biomedical in-domain words directly translated to the target language without breaking them into subwords. 

 ? in-domain 209 ? 209 fair mixture of in-domain and out-of-domain 210 sentences 211 ? an unfair mixture of in-domain and out-of-212 domain with more in-domain sentences 213 We started and continued each training until it 214 accomplished the best BLEU score on the valida-215 tion set. We realized that using bpe dropout in 216 the trainset gives worse results than the standard 217 bpe level in terms of the BLEU score. Also, as 218 expected, the worst results belong to word level 219 and hybrid wordlevel+subword level trainset. On 220 the other hand, using out-of-domain data in an in-221 domain task caused a dramatic drop in the BELU 222 score. In this regard, there was a slight improve-223 ment in BLEU score by increasing the frequency of 224 biomedical words in the mixture of in-domain and 225 out-of-domain trainset in both fair and unfair distri-226 bution of each domain sentence. For WMT21 com-227 petition, we selected the models which achieved 228 the highest scores in the wmt18 and wmt19 en2es 229 and es2en test sets. 230 Table 1 describes our (en2es) results on a mix-231 ture of 2.7 million in-domain + 1.7 million out-of-232 domain parallel sentences (described the data in the 233 section 2). As well, Table 2 shows the results on 234 2.7 million in-domain parallel sentences and also a 235 mixture of 8 million in-domain + 1.7 million out-236 of-domain parallel data (all of that data). Similarly, 237 we show the (es2en) results in the tables 3 and 4 238 5 Conclusion and future works 239 This work presented a method to adapt out-of-240 domain data in an in-domain(biomedical) task to 241 improve the BLEU score. We tuned the parallel 242 data by BabelNet, then found and increased the fre-243 quency of biomedical words in subword-learning 244 to raise the weight of in-domain words in the vo-245 cabulary. Our results obtained in a different mix-246 ture of datasets show that our method improves the 247 BLEU score compared with the standard subword-248 bpe approach. In the future, we plan to extend 249 our approach to more low-resource languages and 250 domains. Moreover, we plan to increase out-of-251 domain data and configure the frequency of in-252 domain words based on the domain type. 

 Table 1 : 1 en2es BLEU score results on hybrid dataset using different word segmentation approaches, word level, hybrid, standard bpe, bpe dropout and tuned subword bpe Dataset: 2.7m indomain+ 1.7m out-of-domain EXP type wmt18 wmt19 Word level indomain+out-of-domain 35.0 36.6 Word level Indomain+ subword level out-of-domain 34.5 36.1 Subword level indomain+ subword level out-of-domain (baseline) 35.6 42.4 10x freq subword indomain+subword out-of-domain (our approach) 39.8 42.7 bpe dropout indomain + bpe dropout out-of-domain 38.5 41.9 Dataset: 2.7m indomain Dataset: 8m in + 1.7m out EXP type wmt18 wmt19 wmt18 wmt19 subword bpe in domain (baseline) 39.8 42.1 40.1 42.8 10x freq subwords indomain (our approach) 39.9 42.2 39.2 43.0 bpe dropout 39.7 39.2 37.1 41.7 

 Table 2 : 2 en2es BLEU score results on solid indomain and eight million hybrid datasets using different word segmentation approaches, word level, hybrid, standard bpe, bpe dropout and tuned subword bpe Table3: es2en BLEU score results on hybrid dataset using different word segmentation approaches, word level, hybrid, standard bpe, bpe dropout and tuned subword bpe Dataset: 2.7m indomain+ 1.7m out-of-domain EXP type wmt18 wmt19 Word level indomain+out-of-domain NA NA Word level Indomain+ subword level out-of-domain NA NA Subword level indomain+ subword level out-of-domain (baseline) 38.1 43.23 10x freq subword indomain+subword out-of-domain (our approach) 39.6 43.3 Dataset: 2.7m indomain Dataset: 8m in + 1.7m out EXP type wmt18 wmt19 wmt18 wmt19 subword bpe in domain (baseline) 42.1 44.0 43.0 44.1 10x freq subwords indomain (our approach) 41.9 43.6 42.3 44.1 

 Table 4 : 4 es2en BLEU score results on hybrid indomain+out-of-domain dataset and unfair distribution.
