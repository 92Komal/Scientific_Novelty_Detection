title
Pruning Neural Machine Translation for Speed Using Group Lasso

abstract
Unlike most work on pruning neural networks, we make inference faster. Group lasso regularisation enables pruning entire rows, columns or blocks of parameters that result in a smaller dense network. Because the network is still dense, efficient matrix multiply routines are still used and only minimal software changes are required to support variable layer sizes. Moreover, pruning is applied during training so there is no separate pruning step. Experiments on top of English?German models, which already have state-of-the-art speed and size, show that two-thirds of feedforward connections can be removed with 0.2 BLEU loss. With 6 decoder layers, the pruned model is 34% faster; with 2 tied decoder layers, the pruned model is 14% faster. Pruning entire heads and feedforward connections in a 12-1 encoder-decoder architecture gains an additional 51% speed-up. These push the Pareto frontier with respect to the trade-off between time and quality compared to strong baselines. In the WMT 2021 Efficiency Task, our pruned and quantised models are 1.9-2.7? faster at the cost 0.9-1.7 BLEU in comparison to the unoptimised baselines. Across language pairs, we see similar sparsity patterns: an ascending or U-shaped distribution in encoder feedforward and attention layers and an ascending distribution in the decoder.

Introduction Making transformer-based machine translation models  (Vaswani et al., 2017)  faster and smaller is a common requirement for server and mobile deployment. We focus on pruning methods that actually improve speed upon strong baselines. There is a variety of work on pruning individual parameters  (See et al., 2016; Brix et al., 2020) , structures like attention heads  (Voita et al., 2019; Behnke and Heafield, 2020) , and even whole layers . Unfortunately, much of the prior work on pruning does not report speed or makes inference slower:  Brix et al. (2020)  achieved no speed-up while  Yao et al. (2019)  report a 87.5% sparse model took 1.6? as long using cuSPARSE. However,  Gale et al. (2020)  point out that coefficient-sparse kernels like cuSPARSE are highly unoptimised. Even block-sparse kernels are 1.8? slower at 70% sparsity  (Gray et al., 2017)  though they did eventually achieve a 1.4? speed-up with "balanced pruning". We propose pruning entire rows or columns and even whole submatrices of a tensor, resulting in a smaller dense matrix. Because the inference problem remains dense, we sidestep the need for sparse kernels to improve speed. We use group lasso  (Yuan and Lin, 2006)  regularisation, which encourages groups to diminish together, during the usual training procedure.  Murray et al. (2019)  used group lasso to prune feedforward layers in their submission to the Efficient Translation Task at WNGT 2019  (Kim et al., 2019a) . Their submissions, which "eliminate more than 25% of the model's parameters while suffering a decrease of only 1.1 BLEU" were at best 6% faster than their baseline. When tuned for the same quality loss, our method reduces size by 66% and translates 52% faster. Moreover, their submissions were slower than higher-quality competitors by an order of magnitude, whereas our baselines are state-of-the-art. Too much work  (Gu et al., 2018; Lee et al., 2020; Wang et al., 2020b)  on efficiency compares a baseline unoptimized system with their optimized system, which is smaller or faster in exchange for some reduction in BLEU. What these papers fail to prove is that their method works better than existing methods that also make models smaller or faster in exchange for some reduction in BLEU like knowledge distillation  (Kim and Rush, 2016) , quantisation, reducing the number of layers, prior work on pruning, or simply training a smaller model. In other words, is the trade-off offered by their method any better than the trade-offs already available, regardless of the type of method? Stacking the exist-ing methods produces a variety of data points with different speed and quality. The Pareto frontier is the set of data points that a practitioner would choose from: no other data point is simultaneously faster (or smaller) and of higher quality. We argue that a new method's empirical justification should advance the Pareto frontier. In this work, we build upon and compare to strong baselines to show the frontier advances. To compare with the state-of-the-art in terms of speed and to investigate the impact this pruning makes on different languages, we build upon English?German, Spanish?English and Estonian?English student models trained with sequence-level knowledge distillation  (Kim and Rush, 2016) . We experiment with four architecture variations: a typical decoder with 6 layers and faster variations with shallow decoder of only 1-2 layers. We also include our experiments from the WMT 2021 Efficiency Shared Task. Our key findings show that: 1. It is possible to prune entire nodes from feedforward layers early during training, resulting in Pareto optimal architectures (quality vs speed). Similarly, pruning entire heads on the top of it results in even faster models. 2. Different language pairs exhibit similar structural sparsity patterns. 3. Pruning during training matches, and sometimes outperforms, retraining the pruned model from scratch. 4. Among the English?German Pareto optimal models, the notable examples include a model with a 6-layered decoder being 34% faster at the cost of 0.2 BLEU and a model with 12-1 encoder-decoder ratio gaining additional 51% speed-up costing 0.3 BLEU. 5. This type of pruning combined with quantisation gives a significant speed boost. Our models are 1.9-2.7? faster at the cost of 0.9-1.7 BLEU. 

 Related work Extensive research to reduce workload, compress and speed-up neural machine translation models includes methods such as knowledge distillation  (Kim and Rush, 2016) , quantisation  (Quinn and Ballesteros, 2018; Aji and Heafield, 2020) , layer approximation  (Kim et al., 2019b)  and pruning. For the best results, they can be stacked together to train the efficient state-of-the-art model. In their analysis,  claim that 85% of transformer neurons are redundant across the network. Using transfer learning, they find the minimal set of neurons that achieve optimum performance given the task. However, that method requires a fully pretrained model to perform a bruteforce search on it, making overall training time too long. Pruning techniques are usually split into two groups: unstructured and structured. Unstructured removes individual coefficients. It is straightforward to apply and yields good quality results simultaneously, which makes it popular. Unstructured magnitude pruning, while successfully applied to NMT  (See et al., 2016) , often needs retraining to recover from quality damage. Moreover, it also requires an efficient matrix multiplication routine to get any speed-up besides size compression. The latest research on combining lottery ticket hypothesis with other methods  (Brix et al., 2020)  sparsified NMT models by 70 to 90% while losing between 0.6 to 3.3 BLEU points in quality. They used a sparse matrix representation for compression but did not report any speed gains. On the other hand, structured pruning removes whole layers or groups of parameters, such as blocks  (Narang et al., 2017) , which makes it easier to optimise on hardware via a special block-sparse matrix multiply  (Gray et al., 2017) . We apply block sparsity, but the blocks are entire rows or columns so that the usual dense matrix multiply can be used with less overhead. Another line of work prunes entire attention heads from a model  (Voita et al., 2019; Behnke and Heafield, 2020) , which we also explore in our approach.  Yao et al. (2019)  combine unstructured sparsity with a light structure that aims to balance parallel workloads. They introduce a specialised kernel for their structured sparsity. Our workloads are easier to balance because they retain density. The idea that different levels of coarseness can be combined may also extend to prune coefficients, rows, columns, and layers simultaneously in future work.  Wang et al. (2020c)  parametrise weight matrices with low-rank factorisations and remove rank-1 components during training, which is said to better preserve linear transformation of uncompressed matrices. They report compressing a Transformer-XL language model by 90% with 1.6? speed-up during inference. Low-rank approximations preserve density, albeit at the cost of doing serial ma-trix multiplications.  Fan et al. (2019)  explored a structured dropout that allows users to prune models for inference. Unfortunately they fail short on NMT experiments. They call WMT14 en-de a 'competitive benchmark' which it has not been for many years. Most problematically, they use tokenised BLEU, which has been noted to be harmful and gives false 'boost' of multiple points on tokenised data. They do not specify the tokeniser or script they use either. Again, there do not include any report on speed or model sizes despite claiming to have much smaller models as a result.  Dodge et al. (2019)  used group lasso to sparsify a variant of RNN for text classification, which is an easier task to learn than NMT. They have to train until convergence twice, which we avoid. They provide no speed or model size analysis, suggesting that there is no improvement or proper implementation. Group lasso has also been previously used by  Wuebker et al. (2018)  to compress the delta between a base model and a domain adapted version of the model. They still have to run a full-size model in inference, so they have no overall speed gain. They also have to store the full base model; compression only refers to the delta. In contrast, our work makes the base model faster and smaller. The different goals also mean different groups: they focused on embeddings that update in domain adaptation while we focus on costly parts of the architecture. Though we use the same algorithm of group lasso, our method differs in several ways from  (Murray et al., 2019) . We prune submatrices in addition to rows and columns, though experiments on just rows and columns show better performance than theirs. They pruned only feedforward layers; we see more speed-up from feedforward layers and additionally prune attention. Finally, we use the normal Adam optimiser (Kingma and Ba, 2014) instead of proximal gradient descent  (Parikh and Boyd, 2014) . Empirically, we find turning regularisation off after some training is important to quality. Overall, we achieve a much better tradeoff between quality and speed/compression. Most of the methods above need either tuning or retraining, often multiple times. They are usually treated as techniques to compress already existing models. Still, there are ongoing research efforts on training a reduced model from start to finish in one go. For example,  Golub et al. (2018)  pruned weights with the lowest total accumulated gradients and reduced the memory footprint to allow training much larger models than possible on available hardware. Some methods prune immediately after initialisation, in either unstructured  (Lee et al., 2019)  or structured  (Wang et al., 2020a)  way. Our method is orthogonal and is integrated into a training scheme instead. Using regularisation to sparsify groups of parameters was introduced by Yuan and Lin (  2006 ) and has been since then built upon in the machine learning field  (Scardapane et al., 2017; Wen et al., 2016) . In this paper, we use group lasso in its simplest form to achieve structural sparsity in transformer layers, focusing on inference speed of machine translation. 

 Methodology To allow regularisation to remove parameters structurally, we need to define how we group parameters. Depending on which matrix it is, we treat parameters in its rows, columns or heads as one entity to be pruned together. Thus, we apply a group lasso over them. A bias term, if necessary, is treated as a part of regularised groups. We want such a sparsity pattern to emerge early into training so that there is no need to retrain or tune it later. 

 Group lasso regularisation Given a matrix w split into non-overlapping groups of parameters G, the group lasso is defined as: R(w) = G g=1 ? w g 2 = G g=1 ? |Gg| j=1 w j g 2 . (1) This penalty term applies L 2 norm over the parameters in each group to force them to go towards 0 together, with L 1 on top of it to enforce overall sparsification. ? is a scalar that orthonormalises groups of different sizes  (Simon and Tibshirani, 2012) , scaling by the number of elements in a group d g . If regularising only rows and columns, all groups are of the same size. However, in later experiments, we also regularise whole attention heads alongside individual feedforward connections. In the end, the penalty for each layer is added to the cost function and scaled by ? and averaged over words in a batch along with cross-entropy: 1 |B| ( x?B CE(x) + ? * w?W R(w)) . (2) Initially we experimented with 8x8 blocks in group lasso. However, this approach removed entire rows and columns that correspond to pruning connections. Figure  1  shows an example of this effect on parameter matrices. When an entire row or column is zero, it can be deleted to form a smaller dense matrix. If an input connection is ignored and not used elsewhere, it can be removed from the upstream matrix. If an output connection is constant, the constant can be folded into downstream bias. These optimisations are typically discovered automatically by regularisation. We mainly focus on pruning feedforward layers, but later include experiments that prune attention heads as well. 

 Setup 

 Data & architectures We concentrate on three language pairs: English?German, Spanish?English and Estonian?English. We use knowledge distillation  (Kim and Rush, 2016)  under teacher-student regime. In English?German, we follow the Workshop on Neural Generation and Translation 2020 Efficiency shared task (WNGT2020) 1 under the WMT 2019 data condition  (Barrault et al., 2019) . As a teacher, we use a WMT 2019 system submitted by Microsoft to news translation task (Junczys-Dowmunt, 2019). It is an ensemble of four deep transformer-big models  (Vaswani et al., 2017)  with 12 layers in encoder and decoder, embedding size of 1024 and feedforward size 4096 and 8 attention heads. For Spanish?English and Estonian?English, we use teachers provided by the Bergamot, 2 which is an ensemble of two similar architectures but with 6 layers instead. We start with strong student baselines, which are already very small and fast, closely following the latest trends set by WNGT2020. Students for all language pairs have an embedding dimension of 256 and feedforward of 1536, based on "tiny" architecture from  Kim et al. (2019a) . The attention has 8 heads in each layer except for decoder self-attention, which is replaced by a faster SSRU (Simpler Simple Recurrent Unit)  (Kim et al., 2019b) . The models use a shared vocabulary of 32,000 subword units generated by Senten-cePiece  (Kudo and Richardson, 2018)  and translate using shortlists of top 50 words. We tried different configurations of layers to investigate trade-offs between them and potential bottlenecks. We describe each architecture by layer number in encoder and decoder and whether the decoder layers are tied. Thus, we investigate the following architectures (chronologically): "6-2tied", "6-6", "12-1" and "6-2". Other training hyperparameters were Marian defaults for training a transformer base model.  3  We used dynamic batching, filling a 10GB workspace on each of 4 GPUs, resulting in about 71,000 words per batch in a "6-2tied" student and about 46,000 words per batch in a "6-6" student. As is more effective in the teacher-student regime, we did not use dropout or label smoothing. We use the Adam optimiser (Kingma and Ba, 2014). The English?German models were trained on 13M sentences of available parallel data, using the concatenated English-German WMT testsets from 2016-2018 as a development set. The Spanish?English students were trained on 242M sentences which included about 15M of mixed forward-and backtranslations. We used a WMT13 testset for development. Estonian?English students were trained on 132M sentences which included about 30M of mixed forward-and backtranslations, and WMT18/dev was used for development. We trained and decoded all our models using the Marian NMT toolkit  (Junczys-Dowmunt et al., 2018) . We evaluate quality and speed on 1 CPU core. In order to expand beyond BLEU and incentivise others to do the same, we additionally evaluate with chrF  (Popovi?, 2015)  and COMET 4  (Rei et al., 2020)  as well. We use SacreBLEU  (Post, 2018)  for BLEU and chrF. Training progressed until BLEU stopped improving for 20 consecutive validations. The checkpoint with the highest BLEU score was selected. 

 Training regime Our training regime for all of ours models has three phases: 1. Pretrain for 25k batches. 2. Train with a regulariser for 250k batches. 3. Remove rows/columns with a sum less than 1e?5, collapse a model and then train without regularisation until convergence. It is well known that initial transformer training is problematic and sensitive to model hyperparameters  (Nguyen and Salazar, 2019; Aji et al., 2019; Liu et al., 2020) . A transformer starts training with 1-2 BLEU and quickly jumps over to 15-30 or more within a short training period, then slows down. Thus, we start pruning after BLEU improvement slowed down to be less than 1 BLEU point in a single checkpoint. This way, we avoid any potential damage to a model during the critical initial period. In this case, we pretrain for 25k batches. Next, we had to decide how long to regularise our model to achieve a good trade-off between quality and sparsity levels. We started with regularising a model until convergence. As shown in Fig.  2 , most of the parameters are already pruned in the first half of the training. Since students require significantly more updates to train than standard models, we want to give a model enough time to sparsify and converge without any constraints. For this reason, we split an average training time into two halves: the first with pruning, the second without it with normal convergence. We found that switching the regulariser off at some point is less aggressive and allows a model to recover some of its lost quality. We chose 250k updates as a pivot as it is about halfway to when the model has begun stalling in Fig.  2 . After each step, we copy the latest checkpoint and start a fresh training round. Thus, all training hyperparameters are reset. We checked and found no additional advantage to our baselines by refreshing learning rate scheduling or Adam optimiser. We do so to avoid partially retraining the same settings during our development phase, but Brix et al.  Table  1 : The evaluation of English?German "6-2tied" students pruned using group lasso. (2020) found it beneficial in their pruning scheme. 

 Experiments 5.1 Pruning "6-2tied" models (English?German) We begin our experiments with the state-of-theart English?German student with a tied decoder  (Bogoychev et al., 2020) . This is their fastest architecture and we want to investigate how much further it can be pushed in that regard. In terms what is a typical difference in inference speed, a tiny distilled model is usually at least 20? faster than its teacher  (Germann et al., 2020) . We investigate two scenarios: pruning both the encoder and decoder (Tab. 1a) or pruning only the encoder (Tab. 1b). The models were trained with the regularisation term ? ? {0.3, 0.4, 0.5, 0.7, 1.0}. Since there is only one layer's worth of decoder parameters, the regularisation is reluctant to re-  move any parameters from it (Tab. 1a). A similar effect was observed by  Behnke and Heafield (2020) . Because only the encoder was pruned, the speedup is relatively small. Still, we successfully prune from half up to two-thirds of feedforward parameters with ?0.2 BLEU change with 9-14% faster inference. In the most extreme case, it gains 42% speed-up at the cost of 1.5 BLEU. To investigate whether this pruning just found a new type of architecture structure, we reinitialise and retrain the smaller pruned models from scratch with reduced dimensions. As seen in Tab. 2, the same models achieve noticeably worse translation quality when trained from the get-go in comparison to careful pruning. Next, we concentrate on pruning encoder only (Tab. 1b). The model with about 50% of feedforward parameters removed is 14% faster with no change to the overall quality.  the encoder at the loss of 1.2 BLEU. In both cases, only one-third of feedforward parameters is required to perform within a small margin of BLEU loss (?0.2 to ?0.3). Removing more than that results in progressively worse quality. 

 Pruning "6-6" models (English?German, Spanish?English) The models with "6-6" architecture were trained with ? = {0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 1.0} pruning both encoder and decoder. The results are presented in Tab. 4. Additionally, we show the number of remaining rows/columns left in each layer, along with sparsity and inference speed-up. The pruned models behave similarly to the smaller models in Tab. 1. The regularised models are of a better translation quality than the same architectures trained from scratch (Tab. 6). Similarly, it is possible to remove two-thirds of all feedforward parameters with -0.2 BLEU and +34% speed-up. Pruning more than that causes a notable step down in quality, which may not be worth aiming for since the "6-2tied" architectures outperform that loss. The sparsity pattern follows an ascending trend in both encoder and decoder layers. We repeat the experiments but this time with Spanish?English using the same "6-6" architecture. The models were trained with regularisation ? = {0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 1.0}. The results are presented in Tab 5.  The only differences between German and Spanish experiments are the languages involved and the scale of the training data: Spanish students trained on a 19? larger corpus. Experiments of such scale are still widely unexplored in machine translation, raising the question of whether known methods are beneficial in real-life scenarios. Most pruning papers use English?German models under WMT14 constraints  (Bojar et al.) , which is only 4.5M sentences  (See et al., 2016; Brix et al., 2020; Hsu et al., 2020) , sometimes branching into different languages such as Russian or French in a similar scope  (Voita et al., 2019; Kasai et al., 2020) . For that reason, we sampled 13M from 242M sentences (the same amount as English?German) and repeated the experiments. In the end, we came to similar conclusions, meaning that the Spanish subpar results are not related to architecture or data size. The reinitialised models (Tab. 6) on full dataset achieve comparable quality to their pruned counterparts. We conclude that in some cases, structural pruning serves as an architecture search method to find Pareto optimal quality-speed trade-off.  The WMT18 testset evaluation of Estonian?English "6-2" students pruned with group lasso on 1 CPU core with the same architectures trained from scratch (Reinit). 

 Pruning " proportions perform as good as 6-6. Pruning an already reduced decoder may cause a bottleneck that damages quality too much. However, if we shift most of the workload into an encoder, we can focus on pruning it exclusively. This time we prune attention layers as well. Pruning attention structurally is more tricky -you cannot remove individual connections easily due to how matrix multiplications perform their routine. The only option is to remove respective heads or an entire layer. To keep it simple, we regularise individual connections and remove an entire attention head if at least half of its connections are dead (its rows/columns < 1e ? 5). The results are in Tab. 7, with an extended version of it in the appendix. In terms of quality and speed-up, it outperforms other models presented so far. This type of pruning was not aggressive on attention, preferring to prune feedforward layers instead, indicating that attention connections perform more critical work in a model. At the small cost of 0.3 BLEU, the model is 51% faster than the baseline. 

 5 .4 Pruning "6-2" models with head lasso (Estonian?English) Finally, we train Estonian?English models, pruning both feedforward and attention layers across the whole model. We do not sweep parameters, choosing ? = 0.3. The results are in Tab. 8. This time we try two options: ? regularising individual connections and then removing heads with more than half of con-  nections inactive (rc+rc = rows/columns in FFN and attention both) ? regularising entire heads with group lasso (rc+heads = rows/columns in FFN, heads in attention) Due to how the penalty is scaled with ? in Eq. 1, the regularisation of entire heads is much more aggressive towards them, removing some layers entirely, which we skip during inference. Both pruning methods perform within a -0.1 to -0.3 BLEU difference compared to the same architecture trained from scratch. However, despite only 0.1 BLEU difference, the same model loses 2.3 COMET points, further validating the fact that training from scratch is subpar. Those results show that there is a potential in regularising larger struc-tures and even entire layers as a way of architecture searching. We leave the improvement of the method for future work. 

 Pareto trade-off (English?German) In this section, we look at the Pareto trade-off between the translation quality and speed for all our English?German models (Fig.  3 ). To be fair in our comparison, we trained several simpler baselines with uniformly smaller feedforward dimensions set to {768, 384, 192, 96}. For"12-1" we additionally set heads per layer to 4 to roughly reflect sparsity percentages of pruned models. We pit against each other the said baselines, the pruned models and their reinitialised counterparts. Naturally, the models with 6 decoder layers are slower but of a higher quality. However, it is better to switch to fewer decoder layers than to prune too far. Our experiments on "12-1" architecture show that its pruned models outperform all others (including all simpler baselines), being a leader in the Pareto frontier. 

 WMT2021 Efficiency Shared Task To put our method to the final test, we participated in WMT2021 Efficiency Task 5  (Behnke et al., 2021) . Under the task constraints, we trained, pruned and quantised 12-1.tiny and 12-1.micro architectures. We tried two pruning settings, following the directions set in Sect. 5.3 and 5.4: rowcol- lasso and head-lasso. Both prune feedforward and attention layers in the encoder. rowcol-lasso regularised individual connections and removed an entire attention head if at least half of its connections are dead. head-lasso applied lasso to a whole head submatrix. Due to the scale of the task, we had no opportunity to grid-search for the best pruning hyperparameters, thus the experiments are as close to 'out-of-the-box' usage as they can be. We used ? = 0.5 for both methods. The models were pretrained for 50k updates and regularised for 150k, after which the models were sliced and trained until convergence. The results are presented in Tab. 9. head-lasso left attention layers almost completely unpruned, focusing on removing connections from feedforward layers instead. rowcollasso was much more aggressive in both layers at the cost of quality. To further optimise the models, they were quantised to 8bit. However, we observe that the smaller a model is, the larger the quality drop after its quantisation. Additional finetuning allows us to recover at least partially from the quantisation damage. Evaluating on the latest testset WMT21, our pruned models are 1.2-1.7? faster at the cost of 0.6-1.3 BLEU. With quantisation, those models are 1.9-2.7? faster losing 0.9-1.7 BLEU in comparison to the unpruned and unquantised baselines. 

 Analysis To analyse sparse architecture patterns, we experiment with Spanish?English models. We prune individual connections in all attention and feedforward layers. In Fig.  4 , we present the distribution of remaining parameters for the baseline and the models regularised with ? = {0.1, 0.2, 0.3, 0.4}. Since the decoder self-attention is replaced with SSRU  (Kim et al., 2019b) , we only show two "pairs" of parameters: encoder self-attention and decoder context attention, with their feedforward counterparts. Both encoder layer types follow a similar sparsity pattern making a "U-shape", with the second and third ones being the most aggressively pruned. On the other hand, the decoder parameters are pruned less and less with each subsequent layer. This arrangement of parameters is identical to that exhibited by pruned attention heads in  Behnke and Heafield (2020) . In that paper, the attention in the encoder also prunes middle layers, and the context attention retains more heads in further layers. It strongly indicates that the decoder prefers to attend to itself first and confront context later. The Estonian architectures, in which we pruned entire attention heads, exhibit a roughly similar structure. For us, this is a strong signal that structural pruning with its architecture search may have a broader generalisation. 

 Conclusions This paper investigated the structural pruning of a transformer incorporated into a typical training routine. We focused on shredding nodes in feedforward layers and whole attention heads as training progresses. Our experiments on knowledgedistilled models with deep and shallow decoders have shown that this type of pruning leads to Pareto optimal architectures in quality and speed. Moreover, it converges in just one "pass" like a baseline since there is no need to repeat an entire or a part of the training. The resulting sparsity patterns are similar across different languages, with the first and middle layers being the most prioritised during pruning. On the other hand, our experiments on pruning both feedforward and attention layers reveal that some of them, such as the last context attention layer, distinctively avoid being pruned. Figure 1 : 1 Figure 1: An example of block-sparse matrices in the first layer of decoder (top) and encoder (bottom) pruned by group lasso regularisation. 

 Figure 2 : 2 Figure 2: An example of pruning FFN layers in an English?German tied model (? = 0.5). About "halfway" through, most parameters are already removed. 

 Figure 3 : 3 Figure 3: Pareto trade-off between average translation quality and average translation time for English?German students of different architectures. 

 Figure 4 : 4 Figure4: The distribution of feedforward and attention connections in Spanish?English "6-6" pruned students. 

 Table 3 3 : The evaluation of English?German "6-6" students pruned with group lasso on 1 CPU core, with the distribution of parameters left in each layer. 

 The most aggressive pruning removes almost all feedforward layers in Reg. ? ? Base 0.1 0.15 0.2 0.3 0.4 0.5 1.0 BLEU Pruned 38.5 38.7 38.6 38.3 37.7 37.6 37.4 36.8 Reinit -38.1 38.0 37.6 37.3 37.2 37.0 36.6 chrF Pruned 64.2 64.5 64.4 64.1 63.7 63.6 63.5 63.1 Reinit -64.0 63.9 63.7 63.6 63.4 63.3 63.0 COMET Pruned 54.8 55.7 54.7 53.8 52.9 52.8 51.9 49.6 Reinit -53.8 53.3 52.3 51.5 51.4 49.7 49.1 Table 4: The evaluation of English?German "6-6" students with pruned both encoder and decoder (Pruned), compared to the same architecture trained from scratch (Reinit). Reg. ? ? Base 0.1 0.15 0.2 0.3 0.4 0.5 1.0 BLEU 37.3 36.9 36.8 36.6 36.3 36.3 36.1 35.9 chrF 62.6 62.4 62.3 62.3 62.0 62.0 61.9 61.8 COMET 58.1 57.3 56.8 56.2 55.1 55.4 54.6 54.3 FFN sparsity 0% 48% 67% 77% 86% 91% 94% 98% Size (MB) 83 59 66 55 52 50 49 48 WPS 1407 1655 1811 1891 2017 2071 2112 2204 Speed-up 1.00 1.18 1.29 1.34 1.43 1.47 1.50 1.57 

 Table 5 : 5 The evaluation of Spanish?English "6-6" students pruned with group lasso on 1 CPU core averaged over WMT12-13. 

 Table 7 : 7 The evaluation of English?German "12-1" students pruned with group lasso on 1 CPU core. 

 Table 8 : 8 

 Table 9 : 9 8-bit model performance. BLEU score is calculated from WMT20. Speed is measured on a single core CPU with a mini-batch of 32. We experimented with two types of pruning. Head pruning removes entire heads. Row and column pruning removes entire rows or columns of matrices, resulting in a smaller matrix. BLEU COMET Sparsity WMT20 WMT21 WMT20 WMT21 Att. FFN Speed (s) 12-1.tiny 36.1 27.6 48.2 41.9 0% 0% 19.2 + head-lasso pruning 34.7 27.0 42.9 38.8 3% 75% 14.5 + 8bit quantisation 33.9 26.2 38.8 33.6 3% 75% 9.3 + 8bit finetuning 34.1 26.7 39.8 33.0 3% 75% 9.3 + rowcol-lasso pruning 33.8 26.3 39.3 34.2 68% 73% 11.6 + 8bit quantisation 32.9 25.6 33.7 28.7 68% 73% 6.9 + 8bit finetuning 32.9 26.0 35.7 31.3 68% 73% 7.1 12-1.micro 35.4 27.6 46.2 40.2 0% 0% 17.1 + head-lasso pruning 34.6 26.7 43.0 35.4 3% 72% 14.1 + 8bit quantisation 33.4 26.0 36.7 31.2 3% 72% 9.2 + 8bit finetuning 33.7 26.5 38.3 33.3 3% 72% 9.2 + rowcol-lasso pruning 34.3 26.4 40.7 35.1 60% 59% 12.0 + 8bit quantisation 32.7 25.5 34.2 29.1 60% 59% 7.5 + 8bit finetuning 33.3 25.9 35.2 30.5 60% 59% 7.5 

			 https://sites.google.com/view/wngt20 

			 https://github.com/browsermt/students 3 Available via --task transformer-base. 

			 We used the default 'wmt20-comet-da' metric model. 

			 http://www.statmt.org/wmt21/ efficiency-task.html
