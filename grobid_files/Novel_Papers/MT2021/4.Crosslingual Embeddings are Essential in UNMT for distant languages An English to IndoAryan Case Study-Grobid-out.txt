title
Crosslingual Embeddings are Essential in UNMT for Distant Languages: An English to IndoAryan Case Study

abstract
Recent advances in Unsupervised Neural Machine Translation (UNMT) have minimized the gap between supervised and unsupervised machine translation performance for closely related language-pairs. However, the situation is very different for distant language pairs. Lack of lexical overlap and low syntactic similarities such as between English and Indo-Aryan languages lead to poor translation quality in existing UNMT systems. In this paper, we show that initialising the embedding layer of UNMT models with cross-lingual embeddings shows significant improvements in BLEU score over existing approaches with embeddings randomly initialized. Further, static embeddings (freezing the embedding layer weights) lead to better gains compared to updating the embedding layer weights during training (non-static). We experimented using Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT approaches for three distant language pairs. The proposed cross-lingual embedding initialization yields BLEU score improvement of as much as ten times over the baseline for English-Hindi, English-Bengali, and English-Gujarati. Our analysis shows the importance of cross-lingual embedding, comparisons between approaches, and the scope of improvements in these systems.

Introduction Unsupervised approaches to training a neural machine translation (NMT) system typically involve two stages: (i) Language Model (LM) pre-training and (ii) finetuning of NMT model using Back-Translated (BT) sentences. Training a shared encoder-decoder model on combined monolingual data of multiple languages helps the model learn better cross-lingual representations  (Conneau et al., 2020; Wang et al., 2019) . Fine-tuning the pre-trained model iteratively using Back-translated sentences helps further align the two languages closer in latent space and also trains an NMT system in an unsupervised manner. Unsupervised MT has been successful for closely related languages  (Conneau and Lample, 2019; Song et al., 2019) . On the other hand, very poor translation performance has been reported for distant language pairs  (Kim et al., 2020a; Marchisio et al., 2020) . Lack of vocabulary overlap and syntactic differences between the source and the target languages make the model fail to align the two language representations together. Recently, few approaches  (Kulshreshtha et al., 2020; Wu and Dredze, 2020 ) take advantage of resources in the form of bilingual dictionary, parallel corpora, etc. to better align the language representations together during LM pre-training. In this paper, we explore the effect of initialising the embedding layer with cross-lingual embeddings for training UNMT systems for distant languages. We also explore the effect of static cross-lingual embeddings (embedding are not updated during training) v/s non-static cross-lingual embeddings (embedding are updated during training). We experiment with two existing UNMT approaches namely, MAsked Sequence-to-Sequence (MASS)  (Song et al., 2019)  and a variation of Denoising Auto-Encoder (DAE) based UNMT approach  (Artetxe et al., 2018c; Lample et al., 2018)  for English to IndoAryan language pairs i.e. English-Hindi, English-Bengali, English-Gujarati. The contribution of the paper is as follows: 1. We show that approaches initialized with cross-lingual embeddings significantly outperform approaches with randomly initialized embeddings. 2. We observe that the use of static cross-lingual embeddings leads to better gains compared to the use of non-static cross-lingual embeddings for these language-pairs. 3. We did a case study of UNMT for English-IndoAryan language pairs. For these languagepairs SOTA UNMT approaches perform very poorly. 4. We observed that DAE-based UNMT with crosslingual embeddings performs better than MASS-based UNMT with crosslingual embeddings for these language-pairs. The rest of the paper is organized as follows. In Section 2, we discuss the related work in detail. Then, we present our approach in Section 3. In Section 4, we outline the experimental setup and present the results of our experiments in Section 5. Finally, we conclude the paper and discuss future work in Section 6. 

 Related Work Neural machine translation (NMT)  (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015)  typically needs a lot of parallel data to be trained on. However, parallel data is expensive and rare for many language pairs. To solve this problem, unsupervised approaches to train machine translation  (Artetxe et al., 2018d; Lample et al., 2018; Yang et al., 2018)  was proposed in the literature which uses only monolingual data to train a translation system.  Artetxe et al. (2018c)  and  Lample et al. (2018)  introduced Denoising Auto-Encoder-iterative (DAE-iterative) UNMT which utilizes cross-lingual embeddings and trains a RNN-based encoderdecoder model  (Bahdanau et al., 2015) . Architecture proposed by  Artetxe et al. (2018d)  contains a shared encoder and two language-specific decoders while architecture proposed by  Lample et al. (2018)  contains a shared encoder and a shared decoder. In the approach by  Lample et al. (2018) , the training starts with word-by-word translation followed by denoising and backtranslation (BT). Here, noise in the input sentences in the form of shuffling of words and deletion of random words from sentences was performed. Conneau and Lample (2019) (XLM) proposed a two-stage approach for training a UNMT system. The pre-training phase involves training of the model on the combined monolingual corpora of the two languages using Masked Language Modelling (MLM) objective  (Devlin et al., 2019) . The pre-trained model is later fine-tuned using denoising auto-encoding objective and backtranslated sentences.  Song et al. (2019)  proposed a sequence to sequence pre-training strategy. Unlike XLM, the pre-training is performed via MAsked Sequence to Sequence (MASS) objective. Here, random n-grams in the input are masked and the decoder is trained to generate the missing n-grams in the pre-training phase. The pre-trained model is later fine-tuned using backtranslated sentences. Recently,  Kim et al. (2020b)  demonstrated that the performance of current SOTA UNMT systems is severely affected by language divergence and domain difference. The authors demonstrated that increasing the corpus size does not lead to improved translation performance. The authors hypothesized that existing UNMT approaches fail for distant languages due to lack of mechanism to bootstrap out of a poor initialization. Recently,  Chronopoulou et al. (2021)  trained UNMT systems with 2 language pairs English-Macedonian (En-Mk) and English-Albanian (En-Sq) in low resource settings. These pairs achieved BLEU scores ranging from 23 to 33 using UNMT baseline XLM  (Conneau and Lample, 2019)  and RE-LM  (Chronopoulou et al., 2020)  systems. They showed further improvement up to 4.5 BLEU score when initialised embedding layer with crosslingual embedding. However, they did not explore the effect of initialising embedding layers on MASS, DAE-pretrained, and DAE-iterative approaches. Moreover, they did not experiment with language-pairs for which UNMT approaches with randomly initialised embedding layers fail completely even after training with a sufficient amount of monolingual data. Additionally, there is some work on understanding multilingual language models and their effectiveness on zero-shot performance on downstream tasks  (Pires et al., 2019; Kulshreshtha et al., 2020; Liu et al., 2020; Wang et al., 2020; Wu and Dredze, 2020) . Here, the pre-trained multilingual language model is fine-tuned for the downstream NLP task in one language and tested on an unseen language (unseen during fine-tuning stage). While multilingual models have shown promising results on zero-shot transfer, the gains are limited for distant languages unless additional resources in the form of dictionary and corpora are used  (Kulshreshtha et al., 2020; Wu and Dredze, 2020) . Also, training a single model on unrelated languages might lead to negative interference  (Wang et al., 2020) . 

 Approaches In this section, we explain different approaches used in our experiments. We use MASS  (Song et al., 2019)  and DAE based iterative approach similar to  Lample et al. (2018)  as our baseline models. 

 MASS UNMT In MASS  (Song et al., 2019) , random n-grams in the input are masked and the model is trained to generate the missing n-grams in the pre-training phase. The pre-trained model is later fine-tuned using back-translated sentences. For every token, the input to the model is the summation of randomly initialised word embedding, positional encoding, and language code. 

 DAE UNMT DAE UNMT approach is similar to the MASS UNMT approach with the difference being the pre-training objective. Here, we add random noise to the input sentence before giving it as input and the model is trained to generate the entire original sentence. Here, noise in the input sentences in the form of shuffling of words and deletion of random words from sentences was performed. 

 Cross-lingual Embedding Initialization In both MASS and DAE UNMT approaches, the embedding layer is randomly initialized before the pre-training phase. We use Vecmap  (Artetxe et al., 2018a)    In input, we do not add language code here. Similar to MASS and DAE, we experiment with using static and non-static cross-lingual embeddings. 

 Experimental Setup We trained the models using 8 approaches for all language-pair out of which 3 approaches use DAE as LM pretraining, 3 approaches use MASS as LM pretraining, and the other two train DAE and BT simultaneously. 

 Dataset and Languages used We use monolingual data of 4 languages i.e. English (en), Hindi (hi), Bengali (bn), Gujarati (gu). While English is of European language family, the other three languages are of Indo-Aryan language family. These three Indian languages follow Subject-Object-Verb word order. However, for English the word order is Subject-Verb-Object. We organise this experiment for distant language pairs with word-order divergence. Therefore, we pair English language with one of these three Indic languages resulting in three language-pairs, i.e. en-hi, en-bn, en-gu. We use monolingual data provided by AI4Bharat  (Kunchukuttan et al., 2020)  dataset as training data. We use English-Indic validation and test data provided in WAT 2020 Shared task  (Nakazawa et al., 2020)    

 Preprocessing We have preprocessed the English corpus for normalization, tokenization, and lowercasing using the scripts available in Moses  (Koehn et al., 2007)  and the Indo-Aryan corpora for tokenization using Indic NLP Library  (Kunchukuttan, 2020) . For BPE segmentation we use FastBPE ? jointly on the source and target data with number of merge operations set to 100k. 

 Word Embeddings We use the BPE-segmented monolingual corpora to independently train the embeddings for each language using skip-gram model of Fasttext ?  (Bojanowski et al., 2017) . To map embeddings of the two languages to a shared space, we use Vecmap ? to obtain cross-lingual embedding proposed by  Artetxe et al. (2018b) . We report the quality of the cross-lingual embeddings in  

 Network Parameters We use MASS code-base ? and to tun our experiments. We train all the models with a 6 layer 8-headed transformer encoder-decoder architecture of dimension 1024. The model is trained using an epoch size of 0.2M steps and a batch size of 64 sentences (token per batch 3K)). We use Adam optimizer with beta 1 set to 0.9, and beta 2 to 0.98, with learning rate to 0.0001. We pre-training for a total of 100 epochs and fine-tune for a maximum of 50. However, we stop the training if the model converges before the max-epoch is reached. The input to the model is a summation of word embedding and positional encoding of dimension 1024. In all our models, we drop the language code at the encoder side. For MASS pre-training we use word-mass of 0.5. Other parameters are default parameters given in the code-base. We do not search for optimised parameters, instead, we are looking for approaches that give decent results on most hyperparameters as hyperparameter tuning is very expensive. 

 Evaluation and Analysis We report both BLEU scores as translation accuracy metric for these approaches. We additionally plot perplexity, accuracy, and BLEU scores for intermediate results of each model. 

 Result and Analysis In this section, we present the results from our experiments and present a detailed analysis of the same. ? https://github.com/glample/fastBPE ? https://github.com/facebookresearch/fastText ? https://github.com/artetxem/vecmap ? https://github.com/microsoft/MASS 

 Results The translation performance from our experiments is as shown in Table  4 . We compared BLEU scores between models where embedding layers were initialised with cross-lingual embeddings and models where embedding layers were randomly initialised. Initialising embedding layer with static cross-lingual embedding helps both MASS-based and DAE-based UNMT systems to learn better translations as seen from the table. Our results suggest that, freezing cross-lingual embeddings (static) during UNMT training results in better translation quality compared to the approach where cross-lingual embeddings are updated (non-static). BLEU scores suggest that DAE objective based models surpass MASS objective based models for these language pairs. Though DAE-iterative models produce lower BLEU scores than DAE Static or DAE Non-Static models, the former approach gives better BLEU scores in less number of iterations as shown in Fig.  3 . For completeness, we compare the BLEU scores of the best UNMT model, i.e. DAE Static, with the best reported BLEU scores in WAT 2020 Shared Task  (Nakazawa et al., 2020)  reported by  Yu et al. (2020)  on the same test data in the supervised setting. The supervised approach uses parallel data in a multilingual setting. Their models reached high accuracy by improving baseline multilingual NMT models with Fast-align, Domain transfer, ensemble, and Adapter fine-tuning methods. While our en-hi and en-gu models produce decent values of BLEU score, en-bn models produce low BLEU score. Intuitively, we assume language characteristics to be the reason behind it. 

 UNMT approaches en ? hi hi ? en en ? bn bn ? en en ? gu gu ? en MASS   (2020)  .   

 Analysis We analyse the performance of our models by plotting translation perplexities on the validation set. Moreover, we manually analyse translation outputs and discuss them in this section. 

 Quantitative Analysis In Fig.  1 , we observe that for both MASS (baseline MASS) and DAE (baseline DAE) the plot of translation perplexity over epoch of finetuning stage increases rather than decreasing. On the other hand, when cross-lingual word embeddings are used the validation set translation perplexity decreases. Among these embedding initialised models, we observe better convergence for models where embedding layers are frozen (static) than the models where embedding layers are updated (non-static). We also observe that the DAE-UNMT models converge better than MASS-UNMT models when initialized with cross-lingual embeddings.  

 Qualitative Analysis An example of a Hindi ? English translation produced by various approaches is presented in Fig.  2 . We observe the translation to be capturing the meaning of the source sentence when cross-lingual embeddings are used. However, we report some observations we found while analysing the translation outputs. 

 Lose of Phrasal Meaning We observe some translations where word meanings are prioritised over phrasal meaning. Fig.  4  shows such an example where dis-fluent translation is generated because of ignoring the phrasal meaning. Here, the model is unable to get the conceptual meaning of the sentence, instead translates words of the sentence literally. Word Sense Ambiguity In Fig.  5  model fails to disambiguate word sense resulting in wrong translation. English word 'fine' have different sense, i.e. beautiful and penalty. In this example, the model selects wrong sense of the word. Scrambled Translation For many instances like Fig.  6 , though the reference sentence and its corresponding generated sentences are formed with almost the same set of words, the sequence of words is different making the sentence lose its meaning. The error looks similar to the error addressed in  Banerjee et al. (2019) . 

 Conclusion We show that existing UNMT methods such as DAE-based and MASS-based UNMT models fail for distant languages such as English to IndoAryan language pairs (i.e. en-hi, en-bn, en-gu). However, initialising the embedding layer with cross-lingual embeddings before Language Model (LM) pre-training helps the model train better UNMT systems for distant language pairs. English Source their hearts and my heart beat to the same rhythm .   We also observe that static cross-lingual embedding better translation quality compared to non-static cross-lingual embeddings. For these distant language pairs, DAE objective based UNMT approaches produce better translation quality and converges better than MASS-based UNMT. 3. 4 4 DAE-iterative UNMT Artetxe et al. (2018c)  and Lample et al. (2018)  proposed an approach based on Denoising Auto-Encoder and Back-Translation. Their approach trained the UNMT in one stage. During training, they alternated between denoising and back translation objectives iteratively. They initialised the embedding layer with cross-lingual embeddings and trained an RNN-based encoder-decoder model (Bahdanau et al., 2015) . Architecture proposed by Artetxe et al. (2018d)  contains a shared encoder and two language-specific decoders while architecture proposed by Lample et al. (2018)  contains a shared encoder and a shared decoder, where all the modules are bi-LSTMs. We use Transformer-based architecture instead of bi-LSTM. 
