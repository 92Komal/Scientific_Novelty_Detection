title
A Comparison of Sentence-Weighting Techniques for NMT

abstract
Sentence weighting is a simple and powerful domain adaptation technique. We carry out domain classification for computing sentence weights with 1) language model cross entropy difference 2) a convolutional neural network 3) a Recursive Neural Tensor Network. We compare these approaches with regard to domain classification accuracy, and study the posterior probability distributions. Then we carry out NMT experiments in the scenario where we have no in-domain parallel corpora, and only very limited in-domain monolingual corpora. Here, we use the domain classifier to reweight the sentences of our out-of-domain training corpus. This leads to improvements of up to 2.1 BLEU for German to English translation.

Introduction Neural Machine Translation (NMT) outperforms phrase based SMT for settings with large amounts of parallel data. However, in general adding out-of-domain data during training does not particularly improve NMT translation quality and is sometimes even harmful. For SMT domain adaptation is well understood and can be classified into two main approaches: 1) model centric techniques adapt the training objective on instance level (e.g., sentence weighting or regularization) or model level (e.g., ensembling or language models), and 2) data centric techniques perform a sentence selection based on a score indicating the similarity between the sentence to be translated and in-domain data. We combine ideas from model centric and data centric approaches. We apply CNNs and Recursive Neural Tensor Networks (RNTNs) to compute domain scores for sentence weighting in NMT. We compare with a Cross-Entropy classifier (XenC) as a well established baseline. Our approach modifies the training objective so that every sentence pair is scaled by its individual weight, with sentences most similar to the in-domain data having most impact during training. Our classifier is trained on small amounts of in-domain and out-of-domain monolingual data. We then use the classifier to find useful sentences within the out-of-domain data, i.e., sentences which are similar to the in-domain data. We carry out intrinsic (classification) and extrinsic (MT) experiments applying sentence classification for domain adaptation. The scores obtained by the CNN and RNTN are strongly peaked in comparison to the cross-entropy classifier, which is important for the NMT sentence weighting. As the neural classifiers showed rather extreme probability score distributions in the intrinsic experiments, we studied various transformations of the scores which we use to find less peaked distributions. The resulting distributions showed less extreme behavior while preserving the strong classification ability. Applying our transformed scores to the task of sentence weighting for domain adaptation outperformed cross-entropy classifiers. In summary, the contributions of this paper are as follows: 1) Neural classifiers show high confidence separating in-and out-of-domain data, higher than a cross-entropy classifier, hence posterior probabilities are distributed closely around the extremes 0 and 1. 2) The CNN and RNTN classifiers don't differ much from each other with respect to their score distributions, both are strongly peaked. 3) The extreme scores need to be transformed in order to be applied as weights in NMT, and we show how to do this effectively. 4) We show that using transformed CNN scores as weights during NMT training is better than a cross-entropy based classifier, which was the previous state-of-the-art solution. 

 Sentence-Weighting Techniques In order to apply sentence weighting to the translation process, one first needs to come up with a method for scoring sentences with respect to how similar they are to in-domain data. Here we carry out a comparison between an established baseline (cross entropy) to the two different techniques based on neural networks that we have discussed (CNN and RNTN). 

 XenC: LM Cross-Entropy Difference Language model (LM ) cross-entropy difference scoring is a widely used technique for MT domain adaptation. The approach is implemented in the tool XenC  Rousseau (2013) . Here the difference between cross-entropy scores of sentences from the entire training corpus and the sentences of an in-domain corpus is computed. We applied monolingual cross-entropy difference as proposed by  (Moore and Lewis, 2010) , which is defined as H(P LM ) = ? 1 n n i=1 logP LM (w i | w i , . . . , w i?1 ) (1) where P LM is the probability of the word w i given the words w 1 to w i?1 for the language model LM . LM is estimated from the specified in-domain corpus. The formula is applied to all sentences in the training data for the NMT system, and is then interpreted as the sentence weight. XenC is not a neural system. It applies statistical computation of cross-entropy given an LM . The language model is a 4-gram model and Kneser-Ney smoothing is applied  Ney et al. (1994) . This approach is widely used throughout various papers and systems with regard to domain adaptation. It is mathematically relatively inexpensive and can therefore be computed very quickly even for extensive training corpora, without the need for GPU resources. These factors make it a suitable baseline for our comparisons to neural classification systems. 

 CNN Classifier Convolutional neural networks (CNN) perform very well on tasks like image and sentence classification. In our case, we are classifying sentences in two classes, in-domain and outof-domain. We applied a plain vanilla system by Yoon  Kim Kim (2014) , which consists of a simple CNN on top of pretrained word vectors. CNNs consist of layers with convolving filters learning local features. In this architecture one layer of convolution is applied on top of word vectors trained by  Mikolov et al. (2013)  on Google News. This approach performed well on several sentence classification tasks  (Kim, 2014) . Figure  1  shows this simple model architecture. A sentence of length n (shorter sentences are padded) is represented as the concatenation of its word vectors. Similar to computer vision tasks, filters are applied to words in a certain proximity to produce a new feature. c i = f (w ? x i:i+h?1 + b) (2) b is a bias term and f a non-linear activation function. The filter slides over the input sentence and therefore creates a feature map c = [c 1 , c 2 , . . . , c n?h+1 ] (3) Then max-over-time pooling is applied, ? = max{c}, to capture the most important feature for each feature map. Multiple filters are applied simultaneously and the max-pooling outputs form the penultimate layer. The last layer is a fully connected softmax layer to output the probability distribution over the labels. For regularization to reduce over-fitting and improve generalization, Dropout and constraining the l 2 ?norms of weight vectors is applied  Krizhevsky et al. (2012) . Dropout randomly drops out -i.e. setting to zero -a proportion p of hidden units (in this case in the last layer) during training. Given the output of the max-pooling layer z = [ ?1 , ?2 , . . . , ?m ], instead of y = w ? z + b (4) dropout uses y = w ? (z ? r) + b (5) with ? being element-wise multiplication and r ? R m a "masking" vector of bernoulli distributed random variables with probability p of being 1. Furthermore a threshold s for l 2 ?norms in introduced, rescaling w to ||w|| 2 = s if ||w|| 2 > s after a gradient descent step. 

 RNTN Classifier CNNs work on word vectors and filters, which aggregate local information within a sentence. This is less expressive than richer forms of sentence representation, e.g., parse trees, which take into account the grammatical structure. To deal with parse trees for sentiment classification  (Socher et al., 2013)  introduced a recursive deep model, the Recursive Neural Tensor Network (RNTN). The representations of sentences within recursive neural models apply to variable length and syntactic type and is used for classification. First, each sentence is parsed into a binary tree with leaf nodes being single words, represented by a vector. Then the parent vectors will be computed in a bottom-up fashion using compositionality functions g. The parent vectors themselves are recursively given as features to a classifier and their parents respectively. Each word is represented by a d dimensional word vector. These are fed into activation functions and ultimately used in sof tmax for classification. Recursive Neural Network. The simplest approach is the standard recursive neural network  (Goller and K?chler, 1996; Socher et al., 2011) . First, the parents whose children are already computed (i.e. both children are words) will be evaluated with an activation function f = tanh. Following equations are used to evaluate the parent nodes according to Figure  2a : p 1 = f W b c , p 2 = f W a p 1 (6) where W ? R dx2d is the main learning parameter. Matrix-Vector RNN. MV-RNNs are linguistically motivated in a sense that most of the parameters are linked with words and that the composition function depends on the actual words being combined. Each word and subphrase are represented as a vector and a matrix, which are combined in the composition function. Each word's matrix initially is a dxd identity matrix with Gaussian noise. These matrices will be trained to optimise classification. Each sentence and subphrase is represented by a list of (vector, matrix) pairs and its parse tree. Following the same example from Figure  2a , the computation is as follows: p 1 = f W Cb Bc , P 1 = f W M B C , (7) while the parent pair (p 2 , P 2 ) is computed using (p 1 , P 1 ) and (a, A). The vectors are fed into the softmax function for classifying each subphrase.  Recursive Neural Tensor Network. Since MV-RNNs combine vectors with matrices, the number of parameters becomes very large, also depending on vocabulary size. A fixed number of parameters would be more desirable. The standard recursive neural network has to be extended for this purpose, because there, different from the MV-RNN, the input vectors only interact with each other implicitly. In search for a single, more powerful composition function to perform better and aggregate meaning from subphrases, they proposed the Recursive Neural Tensor Network. The output for a tensor product h ? R d is computed as follows h = b c T V [1:d] b c ; h i = b c T V i b c , (8) where V  [1:d]  ? R 2dx2dxd is the tensor that defines multiple bilinear forms. The RNTN uses a definition very similar to the standard recursive neural network for computing p 1 : p 1 = f b c T V [1:d] b c + W b c (9) The tensor V can directly relate input vectors and its slices can be interpretated as capturing specific types of composition, with a static number of parameters. 3 Intrinsic Evaluation: Domain Classification 

 Data We study the interesting task of translation using limited in-domain monolingual corpora and larger out-of-domain parallel corpora, which is a realistic scenario. All classifiers were trained on 30k medical in-domain and 30k out-domain sentences, selected from the UFAL corpus.  1  This training data was the same for all three classifiers to allow comparison. The RNTN requires a certain input format, so the sentences were pre-processed by the Stanford Parser and brought into the necessary parse tree format. For intrinsic evaluation, the classifiers were applied to gold standard test data. Newstest 2017 was used as out-of-domain data, whereas the medical HimL test set 2 was used as in-domain data. Both test sets contain about 2k sentences. The trained classifiers were applied to the test sets, in the next section we analysed the classification errors and compared the respective probability score distribution. Figure  3  and Table  1  show the scoring outputs for in-and out-domain test data. These histograms indicate how many sentences in the test set where assigned a certain score with bins of width 0.05. An output of 1 means high confidence for in-domain data and 0 means high confidence for out-domain data. 

 Evaluation on When comparing the results for the CNN and the RNTN, the differences are rather small, without obvious difference in shape of their distributions. We see a dominating peak at the correct side of the spectrum, which shows these classifiers have a high degree of confidence in their decisions. This peak diminishes rather quickly to then have a second minor peak around the other end of the spectrum. This shape looks different for the cross entropy scoring. It resembles a bell curve with its mean slightly skewed towards the correct side of the spectrum. This shows a relatively unclear decision boundary between in-and out-of-domain data, since most of the sentences are scored rather in the middle between the two extremes. These results should be taken with a grain of salt, as it is difficult to define pure in-domain and out-of-domain data. Discussions in the European Parliament (as found in the Europarl corpus) can revolve around medical topics, while being labeled as out-of-domain. Patient information as found in the data by the Health in my Language (HimL) project can include phrases of a more general nature, while being labeled in-domain. Such effects are not taken into account in our work. We applied the classifiers to the source (German) side of the NMT training data, leading to scores that can be used as weights during training the NMT system. Figure  4  shows the distribution of the scores for the CNN and the Cross Entropy classifier. Since we do have English data for the same 30K sentences, we also looked at this classification problem, but the graphs are very similar, so they are not presented. The similarity of English and German suggests that our work may apply well to other languages. The scores by the XenC classifier look similar to a normal distribution, with its mean around 0.5-0.6. Most of the sentences are scored with similar values, indicating an average importance during learning. There are few outliers, overall the distribution is rather narrow with a low standard deviation. The scores by the CNN classifier look significantly different. Instead of the expected normal distribution, most of the weights are below 0.1 with a few scores above 0.95. This means that the classifier is very confident in it's decisions. This high level of confidence is also visible in Figure  3 . 

 Extrinsic Evaluation: Neural Machine Translation In this section we first present our score transformations, and then we present the experiments and results. In initial experiments (which we present in detail later), we found that without applying score transformations instance weighting training of NMT models does not converge. During sentence weighting, the probability score from the classifiers is multiplied with the learning rate. As mentioned previously, the high classification confidence in neural classifiers lead to a vast majority of sentences scored very close to 0, setting the learning rate during training very low. This restricts the Transformer to only learn fully on a small subset of its original training data. We suppose the rather extreme original probability scores let the NMT starve for data. For the purpose of sentence weighting, the data distributions from the classifier outputs are problematic in a sense that they put most of the mass to the borders of the distribution, i.e., almost all of the scores are very close to 0 or 1. This impacts the sentence weighting techniques significantly, since a score that is almost 0 effectively excludes these sentences from the data set. We therefore applied several score transformations to obtain a normalized score distribution, as we describe next. Parabolic Transformation. The first approach is to multiply each of the scores with a linear function to increase the very low scores and decrease the very high scores. Here we chose a simple linear function by taking an educated guess without doing further hyperparameter optimisation. For every score x we applied the function f (x) = x * (?4.2 * x + 5) (10) which results in a parabola with its peak around x = 0.5. A parabola in this shape increases low scores and decreases high scores. Its parameters were an educated guess, leading to competitive results in preliminary experiments. Sigmoidal Transformation. The second approach is to limit the scores into a certain interval using a sigmoid function. We tried different hyperparameters indicating different intervals according the following function ? * 1/(1 + exp(?6 * (x ? 0.5))) + (1 ? ?)/2 (11) indicating the interval [0.5 ? ?/2, 0.5 + ?/2]. These functions are shown in Figure  5a , leading to a normalised distribution on the NMT training data shown in Figure  5b . Quantile Transformation. The previous approaches lead to narrower and flatter data distributions. As a third approach, we made the distribution completely uniform. The second attempt was to "normalise" the quantiles by considering the negatively classified (0-0.5) and the positive (0.5-1) sentences separately and then performing the quantile transformation on both subsets individually. Both categories were transformed into quantiles according to their own distribtion and then transformed back into the respective interval.  

 Experiments and Results For our translation experiments we applied Marian  (Junczys-Dowmunt et al., 2018)  because of its ability to incorporate sentence weighting. It offers a transformer  (Vaswani et al., 2017)  implementation that closely follows the original architecture. This setup is shown to achieve state-of-the-art results. Marian is C++ based, which makes it very time efficient. We assume a scenario with a sufficient amount of parallel out-of-domain data, but only a small amount of monolingual in-domain data on the source side. We use the classifiers we trained before. 3M out-of-domain sentences (of which 2M are from Europarl, see the UFAL corpus web page) from the UFAL corpus are used for training NMT. We report on two wellknown MT test sets (Cochrane and NHS24) which are both from the medical domain. Table  2  gives an overview of all performed experiments. A baseline transformer model (Table  2 , row 1) was trained without any domain specific adaptation. Since we assume we have 30K of monolingual in-domain data, we wanted to evaluate whether giving the NMT system access to this data could be effective. Since we had a translation of this 30K available, we actually fine-tuned on parallel data (i.e., we assumed perfect translation of the 30K, so this is an upper bound of the gains that could be obtained). The results (row 2) show that this is too little data to make much of a difference in translation quality (0.2 to 0.4 BLEU gains), which is not surprising given the very large out-of-domain corpus. The strong results we present below are qualitatively different from having access to a small amount of in-domain data to train on (even small amounts of in-domain parallel data). The results for the the XenC classifier (row 3) serve as a stronger baseline for our results with the neural classifiers. We also tried to directly apply the scores from the neural classifiers, but this led to bad or unstable models that did not coverge (not shown in table). Too many sentences are scored too close to 0, letting their impact vanish, not allowing the training to converge. As discussed earlier and shown in Figure  4  for the CNN, most of the probability mass of the CNN's score distribution is concentrated at the extremes, 0 and 1, leading to many sentences having nearly no impact during training (this is similar for the RNTN as well). This is similar to training with too little data, as weighting a sentence very close to 0 skips the sentence. These effects can be repaired by adding +1 to the classifier scores (rows 4-6), leading to improvements over the baseline for all trained systems, especially for the two neural classifiers. Further experiments focused on the CNN because it outperforms the RNTN and is simpler. Following this we looked at score transformations. The scores from the CNN were manipulated by various sigmoidal transformations (rows 7-9), as its results in the first experiments looked most promising. As the qualitative analysis already showed in Figure  5b , after the sigmoidal transformation the CNN scores look more natural. The experiment results indicate that this transformation also lead to major improvements (rows 7-9), producing the best result (row 8) among our experiments, an improvement over the baseline of 2.1 BLEU. The sigmoid transformation keeps the CNN's ability to clearly distinguish between in-domain and out-domain sentences from the test sets -much clearer than XenC. Another possibility of combining the CNN's classifying power and the XenC's natural score distribution, is averaging their scores  (rows 10,11) . This also lead to improvements over the baseline but could not beat the CNN in combination with the sigmoidal transformation (row 8). 

 MT Finally, as adding +1 to the scores improved the results for all classifiers, we also applied +1 to the previously described transformations  (rows 12-17) . This still lead to minor improvements over the baseline system, but was harmful to the CNN and its sigmoidal transformation. In summary we saw that classifier outputs might be too extreme in their distribution, which can be normalised by transformations to even outperform baseline approaches. Neural classifiers show stronger abilities to distinguish between in-domain and out-of-domain data than cross-entropy based classifiers, resulting in higher BLEU scores when applied in sentence weighting. 

 Related Work Domain adaptation strategies can be separated into four categories: data selection, data generation, instance weighting and model interpolation  Chu and Wang (2018) . We focus our discussion on data selection and instance weighting, as these are closely related to our approach. Data-centric methods. Models are trained using in-domain and out-of-domain data to evaluate out-of-domain data and compute a similarity score. Using a cut-off threshold on these scores the training data can be selected. Language Models Moore and Lewis (2010);  Axelrod et al. (2011); Duh et al. (2013)  or joint models  Cuong and Sima'an (2014) ;  Durrani et al. (2015)  can traditionally be applied to score corpora. Recently convolutional neural networks (CNN)  Chen et al. (2016)  were used. Our work has similarities to this work but uses instance weighting rather than data selection. In settings where the amount of parallel training corpora is not sufficient, generating pseudo-parallel sentences by information retrieval  Utiyama and Isahara (2003) , self-enhancing  Lambert et al. (2011)  or parallel word embeddings  Marie and Fujita (2017) . Aside from generating sentences, other approaches generate monolingual n-grams  Wang et al. (2014)  or parallel phrase pairs  Chu (2015) . In general, data-centric methods (data selection and data generation) are not SMT specific and can be directly applied to NMT. However, because these methods are not directly related to NMT's training criterion, they only lead to minor improvements  Wang et al. (2017a) . Model-centric methods. Instance Weighting is a technique from SMT and was introduced to NMT as well  Wang et al. (2017b) . An in-domain language model was trained to measure the similarity between sentences and the in-domain data via cross-entropy. The weights are then integrated into the training objective. We improve on their work by using state-of-the-art neural classifiers and showing that they are more effective than cross-entropy. Two works that are closer to our work are  Wang et al. (2018)  and  Chen et al. (2017) . In  Wang et al. (2018)  they generate sentence embeddings for all in-domain sentences and then measure the distance between every sentence and the in-domain core. The underlying assumption is that the core of all in-domain sentence embeddings is a typical representative and proximity in their sentence embeddings indicates being part of the same domain. This approach is appropriate when we have in-domain parallel text, but we study a different scenario, with no access to in-domain parallel text, which means the encoder has no access to in-domain training examples. In  Chen et al. (2017)  a domain classifier is incorporated into the NMT system, using features from the encoder to distinguish between in-domain and out-of-domain data. The classifier probabilities are used to weight sentences with regard to their similarity to in-domain data, when training the neural network. Scaling the loss function is similar to multiplying the learning rate with the instance weight. The classifier and NMT are trained at the same time, whereas we chose an approach with pretrained neural classifiers which are trained on a small amount of monolingual data (the scenario we study) with no access to parallel in-domain data. Finally, while some previous work we have mentioned did look at various ways to use domain classification, such previous work has not focused on how to weight the classifier probabilities for effective use in NMT, which we showed is important for obtaining translation quality improvements, particularly when using neural classifiers which can be overconfident. 

 Conclusion Neural classifiers have high confidence when separating in-domain from out-of-domain data, leading to a strong decision boundary. Classification results are good, but the boundary was too drastic, resulting in a poor score distribution with most mass near 0 and 1. This can be fixed by adding +1, keeping sentences with a low score as they are and giving a bonus to sentences with a higher score. The scores from, e.g., a CNN, can be transformed by a sigmoid function, making the score distribution more natural while keeping its strong decision boundary. Cross-entropy approaches lead to a poor score distribution. Sigmoid CNN scores performed best. Our MT experiments showed that neural classifiers can be used to score out-of-domain data effectively. Our work showed that simple transformations of classifier outputs are necessary. The use of the transformed scores by applying sentence weighting on the NMT training data improves translation quality. Our research shows that results from CNNs trained on domain classification achieve significant domain adaptation effects in NMT. It was important to carry out light-weight score transformations. We outperformed baseline experiments by up to 2.1 BLEU points. Figure 1 : 1 Figure 1: CNN model architecture. 
