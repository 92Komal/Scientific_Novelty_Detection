title
Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders

abstract
Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoderdecoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.

Introduction Multilingual pretrained encoders (MPE) such as mBERT  (Wu and Dredze, 2019) , XLM  (Conneau and Lample, 2019) , and XLM-R  (Conneau et al., 2020)  have shown remarkably strong results on zero-shot cross-lingual transfer mainly for natural language understanding (NLU) tasks, including named entity recognition (NER), question answering (QA) and natural language inference (NLI). These methods jointly train a Transformer  (Vaswani et al., 2017)   In the zero-shot cross-lingual NMT transfer task, the model is trained with parallel dataset of only one language pair (such as De-En) and a multilingual pretrained encoder. The trained model is tested on many-to-one language pairs (like Fi/Hi/Zh-En) in a zero-shot manner. Monolingual text of the to-be-tested source languages is not available in this task. masked language modeling task in multiple languages. The pretrained model is then fine-tuned on a downstream NLU task using labeled data in a single language and evaluated on the same task in other languages. With this pretraining and finetuning approach, the MPE is able to generalize to other languages that even do not have labeled data. Given that MPE has achieved great success in crosslingual NLU tasks, a question worthy of research is how to perform zero-shot cross-lingual transfer in the NMT task by leveraging the MPE. Some work  (Zhu et al., 2020; Yang et al., 2020; Weng et al., 2020; Imamura and Sumita, 2019)  explores approaches to improve NMT performance by incorporating monolingual pretrained Transformer encoder such as BERT  (Devlin et al., 2019) . However, simply replacing the monolingual pretrained encoder in previous studies with MPE does not work well for cross-lingual transfer of NMT (see baselines in Table  2 ). Others propose to fine-tune the encoder-decoder-based multilingual pretrained model for cross-lingual transfer of NMT  Lin et al., 2020) . It is still unclear how to conduct cross-lingual transfer for NMT model with existing multilingual pretrained encoders such as XLM-R. In this paper, we focus on a Zero-shot crosslingual(X) NMT Transfer task (ZeXT, see Figure  1 ), which aims at translating multiple unseen languages by leveraging an MPE. Different from unsupervised or multilingual NMT, only an MPE and parallel dataset of one language pair such as German-English are available in this task. The trained model is directly tested on many-to-one test sets in a zero-shot manner. We propose a Simple cross-lingual(X) Transfer NMT model (SixT) which can directly translates languages unseen during supervised training. We initialize the encoder and decoder embeddings of SixT with the XLM-R and propose a two-stage training schedule that trades off between supervised performance and transferability. At the first stage, we only train the decoder layers, while at the second stage, all model parameters are jointly optimized except the encoder embedding. We further improve the model by introducing a position disentangled encoder and a capacity-enhanced decoder. The position disentangled encoder enhances cross-lingual transferability by removing residual connection in one of the encoder layers and making the encoder outputs more language-agnostic. The capacity-enhanced decoder leverages a bigger decoder than vanilla Transformer base model to fully utilize the labelled dataset. Although trained with only one language pair, the SixT model alleviates the effect of 'catastrophic forgetting'  (Serra et al., 2018)  and can be transferred to unseen languages. SixT significantly outperforms mBART with an average improvement of 7.1 BLEU on zeroshot any-to-English translation across 14 source languages. Furthermore, with much less training computation cost and training data, the SixT model gets better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines. 1 

 Problem Statement The zero-shot cross-lingual NMT transfer task (ZeXT) explores approaches to enhance the crosslingual transferability of NMT model. Given an MPE and parallel dataset of a language pair l s -tol t , where l s and l t are supported by the MPE, we aim to train an NMT model that can be transferred to multiple unseen language pairs l i z -to-l t , where l i z = l s and l i z is supported by the MPE. The learned NMT model is directly tested between the unseen language pairs l i z -to-l t in a zero-shot manner. Different from multilingual NMT  (Johnson et al., 2017) , unsupervised NMT  (Lample et al., 2018)  or zero-resource NMT through pivoting , neither the parallel nor monolingual data in the language l i z is directly accessible in the ZeXT task. The model has to rely on the offthe-shelf MPE to translate from language l i z . The challenge to this task is how to leverage an MPE for machine translation while preserving its crosslingual transferability. In this paper, we utilize XLM-R, which is jointly trained on 100 languages, as the off-the-shelf MPE. The ZeXT task calls for approaches to efficiently build a many-to-one NMT model that can translate from 100 languages supported by XLM-R with parallel dataset of only one language pair. The trained model could be useful for translating resource-poor languages. It can further extend to scenarios where datasets of more language pairs are available. In addition, while currently the cross-lingual transferability of different MPEs is mainly evaluated on cross-lingual NLU tasks, the ZeXT task provides a new perspective for the evaluation, which can hopefully facilitate the research on MPEs. 

 Approach 

 Initialization and Fine-tuning Strategy For downstream tasks like cross-lingual NLI/QA, only an output layer is added to the pretrained encoder at the fine-tuning stage. In contrast, an entire decoder is added on top of the MPE when the model is adapted to NMT task. The conventional strategy that fine-tunes all parameters reduces the cross-lingual transferability in the pretrained encoder due to the catastrophic forgetting effect. Therefore, we make an empirical exploration on how to initialize and fine-tune the NMT model with an MPE. The NMT model can be divided into four parts in our method: encoder embedding, encoder layers, decoder embedding, and decoder layers. With an MPE, each part can be trained with one of the following methods, namely, ? Rand: randomly initialized and trained; ? Fix: initialized from the MPE and fixed; ? FT: initialized from the MPE and trained. We compare different fine-tuning strategies for these modules in a greedy manner.   3 )), the encoder layers (Strategy (4)-(  5 )), the decoder embedding (Strategy (  6 )-(  7 )) and the decoder layers (Strategy (  8 )-(  10 )) with MPE sequentially. Each time we compare the strategy of 'FT' and 'Fix' which fine-tunes the corresponding module or keeps it fixed, respectively. Since Strategy (  8 )-(  9 ) use a larger decoder than the rest ones due to decoder layer initialization, we add Strategy (10) whose decoder size is the same as Strategy (  8 )-(  9 ) for fair comparison. The best BLEU is bold and underlined. the encoder embedding, the encoder layers, the decoder embedding, and the decoder layers, sequentially. The details of experimental settings are in the Section 4.1. From the results shown in Table 1, we observe that it is the best to initialize the encoder embedding, the encoder layers and the decoder embedding with XLM-R and keep their parameters frozen, while randomly initializing the decoder layers (see Figure  2 ). More discussions are in the Section 4.2. Two-stage training Since we freeze the encoder and only train the decoder layers, the model is able to perform translation while preserving the transferability of the encoder. However, freezing most of the parameters limits the capacity of the NMT model, especially when the training data goes large. Therefore, we propose a second training stage to further improve the translation performance by jointly fine-tuning all parameters except encoder embedding of the NMT. 2 Since the decoder has been well adapted to the encoder at the first stage, we expect the model can be slightly fine-tuned to improve the translation capacity without losing the 2 According to our preliminary experiment, the average BLEU is 0.2 lower when the encoder embedding is also learned at the second stage. Besides, freezing encoder embedding leads to higher computational efficiency. transferability of the encoder. 

 Model The training strategy and generalization objective of our model are different from vanilla Transformer. This motivates us to propose a new model that can further improve on zero-shot translations. The proposed model consists of a position disentangled encoder and a capacity-enhanced decoder, which aims at enhancing the cross-lingual transferability of the encoder and fully utilizing the labelled data, respectively. Position disentangled encoder The representations from XLM-R initialized encoder have a strong positional correspondence to the source sentence. The word order information inside is language-specific and may hinder the cross-lingual transfer from supervised source language to unseen languages. Inspired by  Liu et al. (2021) , we propose to relax this structural constraint and make the encoder outputs less position-and languagespecific. More specifically, at the second stage, we remove the residual connection after the selfattention sublayer in one of the encoder layers i during training and inference.  3  The other encoder layers remain the same. The hidden states in this i th encoder layer are calculated as the following pseudo code: h[i] = SelfAttn(h[i-1]) h[i] = LayerNorm(h[i]) # No residual connection here h[i] = h[i] + LayerNorm(FFN(h[i])) where SelfAttn is the encoder self-attention sublayer, FFN is the feed-forward sublayer and LayerNorm is the layer normalization.  Liu et al. (2021)  aim at training a language-agnostic encoder for NMT using parallel corpus from scratch. Compared with them, our method shows that it's possible to make a pretrained multilingual encoder more language-agnostic by relaxing the position constraint during fine-tuning. Capacity-enhanced decoder Some previous work  (Zhu et al., 2020; Yang et al., 2020)  incorporates BERT into NMT and configures the decoder size as  Vaswani et al. (2017) . For example, to train an NMT on Europarl De-En training dataset, the default decoder configuration is Transformer base  (Gu et al., 2018; Currey et al., 2020) . However, our model relies more on the decoder to learn from the labeled data, as the encoder is mainly responsible for cross-lingual transfer. This is also reflected in our training strategy: at the first stage only the decoder parameters are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Model settings We use the XLM-R base model as the off-the-shelf MPE. The model is implemented on fairseq toolkit  (Ott et al., 2019) . We set Transformer encoder the same size as the XLM-R base model. For the decoder, we use the same hyper-parameter setting as the encoder. We denote model with such configuration as SixT and use this configuration for our NMT models through the paper unless otherwise stated. The encoderdecoder attention modules are randomly initialized. We remove the residual connection at the 11-th (penultimate) encoder layer, which is selected on the validation dataset. For the empirical exploration in Table  1 , we use two model configurations. For Strategy (1)-(  7 ) where decoder layers are trained from scratch, we use a smaller decoder denoted as BaseDec. This model configuration is denoted as SixT small. For the rest strategies, we follow the configuration of SixT and denote its decoder as BigDec. Table  12  in Appendix presents the details of different model configurations. Training and evaluation The Adam optimizer  (Kingma and Ba, 2015)  with ? 1 = 0.9 and ? 2 = 0.98 is used for training. We use label smoothing with value 0.1. The learning rate is 0.0005 and warmup step is 4000 at the first stage. For the second stage, we set the learning rate as 0.0001 and do not use warmup. All the drop-out probabilities are set to 0.3. We use eight GPUs and the batch size is set as 4096 tokens per GPU. Maximum updates number is 200k for the first stage and 30k for the second stage. We use beam search (beam size is 5) and do not tune length penalty. We evaluate the results with sacrebleu 5 . If not specified, the best checkpoint is selected by zero-shot cross-lingual transfer performance on the validation set for all experiments. We refer the reader to Section B in Appendix for more training details. Baselines We compare our model with vanilla Transformer and five conventional methods to apply pretrained Transformer encoder on NMT task. The pretrained encoders in these methods are replaced with XLM-R base for fair comparison. ? Vanilla Transformer. The encoder is with the same size of XLM-R base, the decoder uses the size of BaseDec. All model parameters are randomly initialized. ? +XLM-R fine-tune encoder  (Conneau and Lample, 2019) . The encoder is initialized with XLM-R. All parameters are trained. ? +XLM-R fine-tune all  (Conneau and Lample, 2019) . All parameters except those of cross attention module are initialized with XLM-R and directly fine-tuned. ? +XLM-R as encoder embedding  (Zhu et al., 2020) . The XLM-R output is leveraged as the encoder input of the NMT. The XLM-R model is fixed during training. ? +Recycle XLM-R for NMT  (Imamura and Sumita, 2019) . The method initializes the encoder with XLM-R and only trains decoder at the first step. Then all are trained at the second step. ? XLM-R fused model  (Zhu et al., 2020) . The XLM-R output is fused into encoder and decoder separately with attention mechanism. The encoder embedding is initialized from XLM-R to facilitate 5 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.5.0 transfer. The parameters of XLM-R are frozen during training. 

 Results The results of the empirical exploration in the Section 3.1 are shown in Table  1 . Since Strategy (  8 )-(  9 ) use a larger decoder than the rest ones, we add Strategy (10) whose decoder size is the same as Strategy (  8 )-(  9 ) for fair comparison. Overall, we observe that it is best to use a big decoder and initialize the decoder embedding and all encoder parameters with XLM-R, and to train the decoder layers from scratch (Strategy (  10 )). To verify the effect of a capacity enhanced decoder in the ZeXT task, we train vanilla Transformer with the same size of Strategy (7) (with BaseDec) and Strategy (10) (with BigDec) using the same training corpus.  6  The vanilla Transformer model with BaseDec and BigDec obtains a BLEU score of 23.5 and 22.9 on the De-En test set, respectively. The big decoder improves the performance of SixT, but fails to improve that of vanilla Transformer. This proves the effectiveness of BigDec to improve the zero-shot translation performance of our model. Table  2  illustrates the performance of the proposed SixT comparing with the baselines. SixT gets 18.3 average BLEU and improves over the best baseline by 5.4 average BLEU, showing that SixT successfully learns to translate while preserving the cross-lingual transferability of XLM-R. For all language pairs, SixT obtains better transferring scores. In contrast, vanilla Transformer can hardly transfer and the other baselines do not well transfer to the distant languages. In addition to zero-shot performance, SixT also achieves the best result on De-En test set. Note that the best checkpoint is selected with zero-shot validation set for all methods. Previous work  (Conneau et al., 2020; Hu et al., 2020)  mainly uses XLM-R for cross-lingual transfer on NLU tasks. The experiments demonstrate that XLM-R can be also utilized for zero-shot neural machine translation if it is fine-tuned properly. We leave the exploration of cross-lingual transfer using XLM-R for other NLG tasks as the future work. 

 Ablation Study We conduct an ablation study with the proposed SixT on the Europarl De-En training set, as shown in Table  3 . Overall, SixT obtains the best zero-shot translation results, demonstrating the importance of all three components. From the results of (1) to (3), TwoStage and BigDec along improve the zeroshot translation performance by 0.8 and 0.4 average BLEU over (1), respectively. However, combining them together brings a significant improvement of 2.6 average BLEU over (1). This indicates that TwoStage and BigDec are complementary to each other, thus it is important to use them together. The results of (  6 )?(5) confirms our claim: without using BigDec, the performance of SixT drops by 1.8 average BLEU. We also observe that the supervised task (De-En) improves with TwoStage and BigDec (from results of (1) to (  4 )) while degrades with Resdrop (see results of (2)?(  5 ) and (  4 )?(  6 )). This is expected since Resdrop helps to build a more language-agnostic encoder. Although Resdrop degrades supervised performance, it improves zero-shot translation. The zero-shot performance is related with both supervised performance and model transferability. By either enhancing the supervised performance (with TwoStage and BigDec) or the model transferability (with Resdrop), the overall performance of zero-shot translation can be improved. 

 Analysis Comparison with multilingual NMT In this part, we compare SixT with mBART , CRISS  (Tran et al., 2020)  and m2m-100  (Fan et al., 2020)  on any-to-English test sets. mBART is a strong pretrained multilingual encoderdecoder based Transformer explicitly designed for NMT. We follow their setting and directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages  (Aharoni et al., 2019; , while SixT is a many-to-one NMT model. (small) model are reported. To compare with these models, we train a manyto-one SixT large model with WMT19 German-English training data, which only consists of 41 million sentences pairs. It only requires a pretrained XLM-R large model and do not contain any data in other languages. We remove the residual connection after the self-attention sublayer of the 23-th (penultimate) encoder layer. The dataset and model configuration details are in Table  9  and 12 in the appendix. From the results in Table  4 , the SixT large model is significantly better than mBART and slightly better than CRISS and m2m-100. The averaged BLEU across all languages is 7.1, 0.5 and 1.4 higher than mBART, CRISS and m2m-100 7 , respectively. The SixT model has larger model size, nevertheless, the results of SixT are impressive given that SixT does not use any monolingual or parallel texts except German-English training data. The performance gain over mBART shows that with proper fine-tuning strategy, the pretrained multilingual encoder has better cross-lingual transfer ability on NMT tasks. In addition, with large-scale German-English parallel data, the SixT model transfers well  7  The 1.2B m2m-100 model is larger than our model (737M parameters) and gets 2.2 more average BLEU than SixT. to distant resource-poor languages like Ne and Si, which indicates a promising approach to translate resource-poor languages. The SixT performance might be further improved with the data of more languages pairs. We leave this as future work. Language transfer v.s. language distance In this part, we explore the relationship between the cross-lingual transfer performance and the language distance. We train the SixT models on different supervised language pairs including De-En, Es-En, Fi-En, Hi-En and Zh-En, and then directly apply them to all test sets, as seen in We observe that the cross-lingual transfer generally works better when the SixT model is trained on source languages in the same language family. The performance on Ko-En is one exception, where Hi-En achieves the best transfer performance. We also notice that the vocabulary overlapping (even character overlapping) between Hindi and Korean is low, showing that significant vocabulary sharing is not a requirement for effective transfer. When trained on 3.5 million Hi-En sentence pairs, SixT obtains promising results on the Ne-En and Si-En translation, with a BLEU score of 16.7 and 9.6, respectively. As comparison, The vanilla Transformer supervised with FLoRes training set only receives 14.5 and 7.2 BLEU score  on the same test sets. Therefore, another approach to translate resource-poor languages is to train SixT on similar high-resource language pairs. As a comparison, we train vanilla Transformer configured as Transformer big 9 without MPE initialization with the same training sets and validation sets. The poor zero-shot cross-lingual performance of vanilla Transformer indicates that the XLM-R initialized encoder is essential and can produce language-agnostic representations. 

 Performance on the supervised language pair To study whether the SixT model gains the crosslingual transfer ability at the cost of performance degradation on the supervised language pair, we compare the vanilla Transformer big model 10 and SixT model on the supervised translation task. The performance of SixT is lower than that of vanilla Transformer when more than 20M parallel sentences are available, but it gets better performance with fewer parallel sentences. The Hindito-English is an exception where SixT has lower BLEU. When large amount of bi-text data is given, the SixT model size is expected to be increased to fully digest the bi-text. For example, if we re- Performance with other target language To build many-to-one NMT model with other target language, we train two SixT models on WMT16 En-De and WMT19 En-De, respectively. We use Fi-De as validation language pair and Fr/Cs/Ru/Nl-De as test language pairs. From the results shown in Table  8 , SixT can obtain reasonable transferring scores to unseen source languages when target language is not English. Again, the results confirm that the cross-lingual transfer ability improves with larger training data. 

 Related Work Zero-shot cross-lingual transfer learning Multilingual pretrained models, such as mBERT  (Wu and Dredze, 2019) , XLM-R  (Conneau et al., 2020) , mBART , and mT5  (Xue et al., 2021) , have achieved success on zero-shot crosslingual transfer for various NLP tasks. The models are pretrained on large-scale multilingual corpora with a shared vocabulary. After pretrained, it is fine-tuned on labeled data of downstream tasks in one language and directly tested in other languages in a zero-shot manner. While multilingual pretrained models with encoder-decoder-based architecture  Chi et al., 2020)  work well on cross-lingual transfer for NLG tasks, multilingual pretrained encoders  (Wu and Dredze, 2019; Conneau and Lample, 2019; Conneau et al., 2020)  are mainly applied to cross-lingual NLU tasks  (Hu et al., 2020) . In this work, we explore how to fine-tune an off-the-shelf multilingual pretrained encoder for zero-shot cross-lingual transfer in neural machine translation, a typical NLG task. Pretrained models for NMT Some previous works  (Imamura and Sumita, 2019; Conneau and Lample, 2019; Yang et al., 2020; Weng et al., 2020; Ma et al., 2020; Zhu et al., 2020)  explore methods to integrate pretrained language encoders into the NMT model to improve supervised translation performance. For instance,  Zhu et al. (2020)  propose BERT-fused model, in which they first use BERT to extract representations for an input sentence, and then fuses the representations into both the encoder and decoder via the attention mechanism. Another line of works  Song et al., 2019; Lin et al., 2020)  propose novel encoder-decoder-based multilingual pretrained language models and fine-tune such models for NMT. For example,  propose mBART, an encoder-decoder-based Transformer explicitly designs for NMT and demonstrate that mBART can be fine-tuned for supervised and zero-shot NMT. Different from them, we leverage MPE for zeroshot translation instead of supervised translation. Among the previous works,  Wei et al. (2021)  is the most similar with ours. They fine-tune their MPE on NMT with a two-stage strategy. However, their work focuses on improving the MPE for a more universal representation across languages and lacks in-depth study of cross-lingual NMT. In contrast, we aim at leveraging an MPE for machine translation while preserving its ability of cross-lingual transfer. 

 Conclusion In this paper, we focus on the zero-shot crosslingual NMT transfer (ZeXT) task which aims at leveraging an MPE for machine translation while preserving its ability of cross-lingual transfer. In this task, only a multilingual pretrained encoder such as XLM-R and one parallel dataset such as  Figure1: In the zero-shot cross-lingual NMT transfer task, the model is trained with parallel dataset of only one language pair (such as De-En) and a multilingual pretrained encoder. The trained model is tested on many-to-one language pairs (like Fi/Hi/Zh-En) in a zero-shot manner. Monolingual text of the to-be-tested source languages is not available in this task. 
