title
Kakao Enterprise's WMT21 Machine Translation using Terminologies Task Submission

abstract
This paper describes Kakao Enterprise's submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the En?Fr language direction. Furthermore, we explore various methods such as backtranslation, explicitly training terminologies as additional parallel data, and in-domain data selection.

Introduction We participate in the WMT21 Machine Translation using Terminologies Task in four language directions, English?French (En?Fr), English?Chinese (En?Zh), English?Korean (En?Ko) and Czech?German (Cs?De). 

 Task description The recent COVID-19 pandemic has raised the urgency to translate and distribute the latest medical information worldwide. However, despite recent advances in neural machine translation (NMT), translation in such emerging domains remains a challenge, as it is unaffordable to collect fair amounts of quality in-domain parallel data in a short time. As an alternative, word-or phrase-level dictionaries of key terms are relatively easier to obtain. These dictionaries are prevalent in commercial settings, where customers specify domain-specific jargon that human translators can attend to. However, incorporating pre-specified dictionaries effectively into NMT models is a non-trivial problem, as NMT *Work done during the author's internship at Kakao Enterprise. is inherently trained without explicit constraints compared to statistical approaches. In this context, the shared task of Machine Translation using Terminologies is held in five language directions at WMT21. The task assumes a realistic scenario where parallel and monolingual data are abundant in generic domains (e.g., news, web crawl), but only hundreds of word-or phrase-level term dictionaries are available in the domain of interest -COVID-19. Technically, this poses a challenge as we must impose terminology constraints without hurting general translation quality, while only 1.5% of parallel data contain the provided terminologies. Additional issues such as the 1 : N mapping of term translations further complicate the problem. Evaluating MT systems in specialized domains diverge from general MT evaluation in that overall translation quality may not ensure the translation accuracy of domain-specific terms. This potential gap calls the need for evaluation metrics that directly assess the consistent use of terms. Concretely, three metrics proposed in  Alam et al. (2021)  are employed in this task -Exact-Match Accuracy, Window Overlap, and Terminology-biased Translation Edit Rate (TERm). The suggested metrics complement general translation accuracy measured by standard MT metrics (BLEU, chrF, BERTscore, COMET) by validating whether terms are translated faithfully according to the dictionary. Specifically, human-labeled COVID-19 related term dictionaries are released in four language directions (En?Fr, En?Zh, En?Ko, En?Ru), with around 600 terms for each direction. Exceptionally, the dictionary for Cs?De is constructed automatically and consists of 5,601 parallel terms. 

 Related work Word-or phrase-level constraints have often been introduced to NMT via constrained decoding to reinforce specific tokens in the output sequence. Combined with terminology dictionaries, constrained decoding integrates the target side terms as decoding-time constraints  (Hokamp and Liu, 2017; Anderson et al., 2017; Post and Vilar, 2018) . Subsequent work has shown that adding inline annotations to the source sentence as soft constraints can improve performance and time complexity when employed with additional source factor streams  (Dinu et al., 2019; Bergmanis and Pinnis, 2021) . Similarly, a merging approach by adding markers without modifying the model has also proven to be effective  (Wang et al., 2019) . 

 Data 

 Cleaning Both monolingual and parallel corpora of all languages are preprocessed according to the following pipeline. First, we remove non-utf8 or nonprintable characters. Second, we unescape HTML characters such as &gt;. Finally, we normalize variations in spaces and punctuation marks. All cleaning steps are done with Moses scripts  (Koehn et al., 2007) . We also use the Moses tokenizer, but only for European languages (En, Fr, Cs, De) since Asian languages (Zh, Ko) require language-specific tokenizers that consider the characteristics of each language. 

 Filtering Web-crawled data are notorious for being noisy. To prevent defective data from undermining performance, we filter both parallel and monolingual data with diverse methods. Bi-text We filter the provided parallel data with several heuristics. We first eliminate pairs that contain empty lines or identical content in both source and the target side. We filter pairs that contain overly long sentences (250 words) or excessively long words (50 characters). The pairs that have a word count ratio larger than four are also omitted. We refer to previous literature to set statistical thresholds of each rule. Lastly, we only use pairs of which both sides are identified as the correct language with a language identification tool. Specifically, we use fastText  (Joulin et al., 2016 (Joulin et al., , 2017 . In addition, for En?Ko, we filter out mislabeled bi-text which we found manually, that seemed as byproducts of web-crawl in the source or target side. For instance, the pattern "YYYY? MM En-Fr En-Zh En-Ko Cs-De Parallel 158M 62M 13M 15M + Filter 149M - 12M 13M Table  1 : Dataset sizes of parallel corpora before and after filtering in each language pair. For En-Zh, we did not apply rule-based filtering. ? DD? ? ", which means "Confirmed in YYYY/MM/DD", was found instead of the correct labels in 20,909 samples. The final dataset sizes are shown in Table  1 . Mono-text We used monolingual text for two language pairs (En?Ko, En?Fr) to augment existing parallel corpora via back-translation  (Sennrich et al., 2016a) . The back-translation procedure is described in Section 3.2. For En?Ko, we do not apply any filtering schemes as the size of the Korean monolingual corpus is small (14M sentences). On the contrary, for En?Fr, using the entire French monolingual corpora (8.5B) for backtranslation is unwieldy, considering the time and computation required to infer all samples. Hence, we filter the corpus and select in-domain, COVID-19 related data to maintain a reasonable size for inference and training. We filter French monolingual data in three steps. First, we roughly filter the data with rule-based methods that are similar to those of bi-text filtering. Second, we choose sentences that contain terms in the terminology dictionary (8.5B ? 725M). Lastly, we use the Moore and Lewis (Moore and Lewis, 2010) method to find samples that are more similar to the term-related samples. Specifically, we train an in-domain language model with sentences that contain terminologies from the En-Fr parallel corpus. A general-domain language model is also trained with samples chosen randomly from the En-Fr parallel corpus. For both models, we use KenLM  (Heafield, 2011)  to train 5-gram language models with modified Kneser-Ney smoothing. Finally, top-k sentences with the highest scores are chosen (725M ? 160M). 

 Approaches 

 Baseline We explore two baseline approaches that differ by their training data. First, models are trained with solely the parallel data described in 2.2. This baseline does not utilize the terminology dictionary. Second, we take a na?ve approach to leverage the term dictionary -including the provided terms as additional parallel data to train the model. For 1 : N mappings of term translations, we flatten them into N distinct pairs. We refer to this approach as the "explicit" model in the following sections as we "explicitly" augment the training dataset with terminology dictionaries. 

 Back-translation We incorporate back-translated monolingual data for two language directions: En?Fr and En?Ko.  1  We train reverse translation models (Fr?En, Ko?En) with the same parallel corpora and training configuration used to train our baseline models covered in Section 4.2. Back-translated samples are inferred with beam search of beam size 4, and a length penalty of 0.6. For En?Fr, we use back-translated corpora for Exact Target Annotation fine-tune. We revisit the details of this procedure in Section 3.4. For En?Ko, we train the back-translation model from scratch using both parallel and backtranslated text. During training, we upsample the parallel corpus twice as frequently as the backtranslated text. 

 Target Lemma Annotation To integrate terminology constraints, we employ Target Lemma Annotation (TLA) of  Bergmanis and Pinnis (2021) , which helps the model learn how to copy-and-inflect inline annotations. At training time, we randomly select target lemmas and inject them into the source sentence behind the corresponding source word(s). Specifically, we adopt a simple approach where we modify the input data but not the model. This differs from the method described in Bergmanis and Pinnis (2021), which uses additional input streams to denote the annotated tokens. In detail, we introduce three special tokens <b>, <t>, and </t> which respectively indicate the start of annotated source tokens, the start of target lemma tokens and the end of target lemma tokens. An example is shown in Table  2 . Following the training data annotation procedure of  Bergmanis and Pinnis (2021) , we first lemma-  At test time, we provide soft terminology constraints by annotating source terms with their corresponding target terms retrieved from the terminology dataset. Terminology entries are identified with the longest word-sequence match in the source sentence. If there exist several target terms for one source term, we randomly select one candidate. 

 Exact Target Annotation (Fine-tune) We adopt Exact Target Annotation (ETA) designed by  Dinu et al. (2019)  to fine-tune the TLA model pre-trained as in Section 3.3. ETA injects the exact target-side translation of a terminology entry into the source sentence using inline annotations. Note that we utilized the whole terminology dataset during training, unlike  Dinu et al. (2019) , since the task allows the use of the terminology dataset at training time. While TLA learns to copy-and-inflect general words, our terminology dataset is domain-specific. We aim to fill the domain gap by constructing fine-tuning data in which terminology entries are present on both the source and target sides. As a result, 750K samples from the parallel data and 10M samples from the back-translated data are selected. We upsample the parallel corpus by eight times. Another discrepancy between training and test time annotation in TLA is that TLA engages a single target word to the corresponding source word(s), whereas many of the actual terms are multi-word expressions in both source and target sides. We expect ETA fine-tune to alleviate the problem since ETA annotates target terms in verbatim. The pretrain-finetune phases are outlined with their motivation in Figure  1 . Specifically, we follow the annotation strategy of  Dinu et al. (2019) , where we annotate only when both the source side term t s and the target side term t t are present. When a sentence contains multiple matches overlapping each other, we keep the longest match. The difference between Dinu et al. (  2019 ) and our method is that we annotate with three special tokens as described in Section 3.3. Instead of randomly deciding whether to annotate or not, we annotate all matches. We then combine the annotated data with its original data and use it for training with a proportion of 1:1. The annotation procedure at test time is also equivalent to Section 3.3. 

 Experiments 

 Evaluation setting Evaluation of the models is done using the evaluation script 2 and the development dataset, both provided by the task organizers. We select the best models by considering all metrics provided by the evaluation script. For evaluation, we tokenize our outputs so that they resemble the tokenization setup of the development dataset. For En?Fr and Cs?De, we use the Moses toolkit  (Koehn et al., 2007) . For En?Zh, we apply the Jieba tokenizer.  3  Before submitting the test set translations, we handle rare target-side tokens decoded as <unk> by simple substitutions, which we found to work 2 https://github.com/mahfuzibnalam/ terminology_evaluation 3 https://github.com/fxsjy/jieba well during evaluation even without incorporating external methods such as word alignments. When the number of <unk> tokens are equal on both sides, we copy the original source-side tokens to the target slots in the same order. After replacing rare tokens, outputs are detokenized using the Moses toolkit  (Koehn et al., 2007) . 

 Experimental details For En?Fr and Cs?De, we pre-tokenize the data using the Moses toolkit  (Koehn et al., 2007) . We use sentencepiece  (Kudo and Richardson, 2018)  to learn a joint byte pair encoding (BPE) with vocabulary size 40K (En?Fr) and 32K (Cs?De). For En?Ko, We pre-tokenize Korean sentences with Mecab  (Kudo, 2005)  without space tokens as suggested in  Park et al. (2021)  and use sentencepiece to learn a BPE model with vocabulary size 32K for each language side. For En?Zh, we first convert characters possibly in traditional Chinese to simplified Chinese text using hanziconv 4 and. Then, we pre-tokenize the data using the Jieba tokenizer 3 . We then use subword-nmt  (Sennrich et al., 2016b)  to train BPE on combined Chinese and English corpus and build separated vocabularies. The final vocabulary size is 44K for Chinese and 32K for English. For all language directions, we employ the Transformer architecture  (Vaswani et al., 2017)  implemented in fairseq  (Ott et al., 2019) . The specific training and generation configurations can be found in Appendix A. Since TLA relies on the word-aligner's performance, we did not apply TLA pre-training and ETA fine-tuning for En?Ko and En?Zh. Given that both are linguistically distant language pairs, we assumed that the word-aligner's performance would not be sufficient enough to guarantee improvements from TLA. We start ETA fine-tuning from the TLA checkpoint saved at 750,000 steps for En?Fr and 200,000 steps for Cs?De, chosen based on BLEU scores and Exact Match Accuracy. To evaluate the TLA and ETA fine-tuned models, we run annotation using the terminology tags provided with the development dataset, which is different from the test annotations described in 3.3. For En?Ko and Cs?De, we use an ensemble of models that utilize back-translation, explicit training, and data augmentation.  configurations are detailed in Appendix B. 

 Results Table  3  reports the evaluation results of the four language pairs that we participated in. 

 English?French The TLA model improves Exact Match Accuracy but shows deteriorated performance on all other metrics compared to the baseline. Notably, the degradation stems from the test-annotation method -test scores are comparable to the baseline when tested with raw text (without test-annotation) on the same TLA model. On the other hand, under the same testannotation condition, the ETA fine-tuned model recovers the performance loss and even boosts the BLEU score, Exact Match Accuracy, and the 1-TERm score compared to both the baseline and the TLA model. TLA + ETA fine-tune outperforms the baseline by 0.33 points, 4.65%, and 0.24% on BLEU, Exact Match, and 1-TERm, respectively. In addition, we run a simple ablation experiment by using only bi-text data during ETA fine-tuning: TLA + ETA fine-tune (bi-text only). The results are indistinguishable from the original TLA + ETA fine-tune, which is fine-tuned with data from both bi-text and mono-text. This result supports that the performance gain stems not only from the use of monolingual data, which was unseen during TLA pre-training. 

 English?Chinese We compare two approaches -baseline and explicit, and observe that adding the term pairs explicitly to training improves both general translation performance (+0.73 BLEU) and term consistency (+2.29% 1-TERm) compared to the baseline. 

 English?Korean Back-translation yields performance gains across all metrics with considerable improvements, particularly in BLEU and 1-TERm. Accuracy and 1-TERm. Finally, our ensemble model that combines these approaches demonstrates the best performance across all metrics, raising the BLEU score by 2.52 points, Exact Match Accuracy by 4.2%, Window Overlap by 0.43% and 0.54% for windows 2 and 3 respectively, and 1-TERm by 4.88 points. 

 Czech?German We discover that the explicit model does not bring significant gains compared to the baseline model. This trend contradicts other language directions, where we observed at least modest improvements over their respective baselines. We suspect the differences lie in how the terminologies are generated; Cs?De terminologies are constructed automatically, whereas, for other language directions, the terminologies were annotated manually. Our ensemble model improves upon the baseline model by 1.5 BLEU points, 1.6% Exact Match Accuracy, 1.84% and 1.74% Window Overlap for window sizes 2 and 3, and 1.1 points in 1-TERm. We also attempted to apply TLA pre-training + ETA fine-tuning to Cs?De as done in En?Fr. In our preliminary experiments, while some metrics improved, we observed Exact Match Accuracy deteriorate after 1,000 steps of TLA training, unlike En?Fr, possibly due to the automatic creation pipeline of Cs?De terminologies. Therefore, we did not further explore this direction during our task participation. However, subsequent experiments after the deadline revealed that TLA, when followed by ETA fine-tuning, has its advantages in finding a balance between BLEU and Exact Match Accuracy, supporting our findings in En?Fr. 

 Official task results We present our official submission results in Table 4. Despite the trade-off between general translation quality (COMET) and term consistency (Ex-act Match Accuracy), our approach strikes at the right balance between the two criteria for En?Fr. Out of 22 submissions in this direction, our system ranks 1st in COMET. According to Exact Match Accuracy, our system performs roughly comparable to the best system, ranking 4-6th. For En?Zh, our system ranks 8th in both metrics out of 8 submissions. For En?Ko, our submission is the only submission. For Cs?De, our submission ranks 1st in terms of COMET and 1st-2nd for Exact Match Accuracy out of 2 submissions. 

 Conclusion We participate in four language directions for the shared task WMT21 Machine Translation Terminologies. To this end, we explore various techniques, including back-translation, explicitly training with term pairs along with other parallel data, and in-domain data selection to improve translation performance in the COVID-19 domain. In particular, for En?Fr and Cs?De, we find that TLA outperforms the baseline in terms of Exact Match Accuracy by leveraging terminology constraints. However, all other metric scores (BLEU, 1-TERm) plummeted, implying that the overall translation quality was compromised. We recover this performance loss by introducing a new techniquefine-tuning with ETA, and achieve significant improvements in both general translation quality and terminology consistency. We leave it to future work to validate our approach in other languages and reveal the factors behind the benefits of ETA finetuning precisely, hopefully, to discover a more suitable design to impose terminology constraints. Original SourceEN and are you having any of the following symptoms with your chest pain? Annotated Source EN and are you having any of the following <b> symptoms <t> sympt?mes </t> with your chest pain? Target FR et avez-vous l'un de sympt?mes suivants en plus de vos douleurs thoraciques ? Table2: An example of using special tokens for inline annotations. Inline annotations are marked in bold. <b>, <t>, </t> denote the start of the annotated source tokens, the start of the target lemma tokens, and the end of the target lemma tokens. 
