title
Efficient Machine Translation with Model Pruning and Quantization

abstract
We participated in all tracks of the WMT 2021 efficient machine translation task: single-core CPU, multi-core CPU, and GPU hardware with throughput and latency conditions. Our submissions combine several efficiency strategies: knowledge distillation, a simpler simple recurrent unit (SSRU) decoder with one or two layers, lexical shortlists, smaller numerical formats, and pruning. For the CPU track, we used quantized 8-bit models. For the GPU track, we experimented with FP16 and 8-bit integers in tensorcores. Some of our submissions optimize for size via 4-bit log quantization and omitting a lexical shortlist. We have extended pruning to more parts of the network, emphasizing component-and block-level pruning that actually improves speed unlike coefficientwise pruning.

Introduction This paper describes the University of Edinburgh's submission to Sixth Conference on Machine Translation (WMT2021) Efficiency Task 1 , which measures performance on latency and throughput on both CPU and GPU, in addition to translation quality. Our submission focused on the trade-off between these metrics and quality. Our submission builds upon the work of last year's submission  (Bogoychev et al., 2020) . We trained our models in a teacher-student setting  (Kim and Rush, 2016) , using Edinburgh's En-De system submitted to the WMT2021 news translation task as the teacher model. For the students, we used a Simpler Simple Recurrent Unit (SSRU)  (Kim et al., 2019)  decoder, used a target vocabulary shortlist, and experimented with pruning the student models by removing component-and blocklevel parameters to improve speed. We further experimented with quantizing into smaller numerical 1 http://statmt.org/wmt21/ efficiency-task.html formats, including fixed point 8-bit quantization on the CPU, and both 8-bit and log based 4-bit quantization on the GPU, as well as post-quantization fine-tuning of 4-bit quantized models. For running our experiments, we improved upon the Marian  (Junczys-Dowmunt et al., 2018)  machine translation framework by incorporating speed ups for 8-bit matrix multiplication operations, optimizations for pruning neural network parameters on Intel CPUs, and exploring tensorcores on the GPU. 

 Efficiency Shared Task The WMT21 efficiency shared task consists of two sub-tasks: throughput and latency. Systems should translate English to German under the constrained conditions of the WMT21 news task. For each task, systems are provided 1 million lines of raw English input with at most 150 space-separated words. The throughput task receives this input directly. The latency task, introduced this year, is fed input one sentence at a time, waiting for the translation output before providing the next sentence. Throughput is measured on multi-core CPU or GPU system, and latency is measured on singlecore CPU or GPU systems. The CPU-based evaluations use an Intel Ice Lake system via Oracle Cloud BM.Optimized3.36, while the GPU-based use a single A100 via Oracle Cloud BM.GPU4.8. Entries to both tasks are measured on quality, approximated via BLEU score  (Papineni et al., 2002) , speed, model size, Docker image size, and memory consumption. We did not optimise specifically for the latency task beyond configuring the relevant batch sizes to one. We used Ubuntu 20.04 based images for our systems, with standard Ubuntu for CPU-only systems and NVIDIA's Ubuntubased CUDA-11.4 docker for GPU-capable systems. Docker images were created using multistage builds, with model disk size reduced by compression with xzip. 

 Training teacher models We used Edinburgh's En?De systems submitted to the WMT 2021 news translation task as teacher models  (Chen et al., 2021) . We trained transformerbig models  (Vaswani et al., 2017) , using a shared 32K SentencePiece  (Kudo and Richardson, 2018)  vocabulary, built in three stages: corpus filtering, back-translation and fine-tuning. The models achieved 29.90 and 51.78 BLEU on En?De and De?En WMT 2021 test respectively (scored by the task organizers, with multiple references). We used sequence-level knowledge distillation  (Kim and Rush, 2016)  to synthesize forward, backward, and backward-forward translations using the teachers. We filtered the synthesized parallel data using handcrafted rules 2 , followed by removing bottom 5% according to cross-entropy per word on the generated side using KenLM  (Heafield et al., 2013) . 

 Knowledge distillation We ran experiments using different combinations of teacher-synthesized corpora. One variant included all of the synthesized data: parallel, monolingual backward and forward as well as backwardforward (Aji and  Heafield, 2020b) . Another variant excludes only the fully-synthetic monolingual backward-forward data, while the final variant used parallel data only. All student models were trained using a validation set consisting of the subset of sentences in the English-German WMT test sets from 2015-2019 that were originally in English. Training concluded after reaching 20 consecutive validations without an improvement in BLEU score. The student models used the same shared vocabulary as the teacher ensemble. During decoding, we used a lexical shortlist  (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014)  of the top 50 most probable alignments, combined through a union with the top 50 most frequent vocabulary items. Other than this, we used the default training hyperparameters from Marian for the transformer-base model. Each of the student models used transformer encoders  (Vaswani et al., 2017)  and RNN-based decoders with Simpler Simple Recurrent Unit (SSRU)  (Kim et al., 2019) . Several different architectures were explored; these differ in the number of encoder and decoder blocks as well as in the sizes of the embedding and FFN layers. Further to this, some of our transformer architectures use a modified attention matrix of shape (d emb , n head ?d head ) rather than the typical (d emb , d emb ). In all cases we use 8 transformer heads per layer, and set d head = 32 across all modified attention models. The student architectures are summarized in Table 1. A baseline comparison of student models trained on all synthesized data can be seen in Table 1. 

 Pruning Attention is a crucial part of the transformer architecture, but it is also computationally expensive. Research has shown that many heads can be pruned after training; with further work suggesting that pruning during training can be less damaging to quality. Feedforward layers are also expensive and could be reduced. Among many experiments, we applied group lasso regularisation to sparsify and prune 12-1.tiny and 12-1.micro architectures. We follow the directions set by  Behnke and Heafield (2021) . We tried two pruning settings: rowcol-lasso and headlasso. Both prune feedforward and attention layers in the encoder. rowcol-lasso regularised individual connections (rows and columns) and removed an entire attention head if at least half of its connections are dead. head-lasso applied lasso to a whole head submatrix. Due to the scale of the task, we had no opportunity to grid-search for the best pruning hyperparameters, thus the experiments are as close to 'out-of-the-box' usage as they can be. We control pruning with ? = 0.5 for both methods. The models were pretrained for 50k updates and regularised for 150k, after which the models were sliced and trained until convergence. The results are presented in Tab. 2. head-lasso left attention layers almost completely unpruned, focusing on removing connections from feedforward layers instead. rowcollasso was much more aggressive in both layers at the cost of quality.  Behnke and Heafield (2021)  have shown that group lasso pruning results in a better quality model than training the same exact architecture from scratch. To further optimise the models, they were quantised to work within 8bit representation. However, we observe that the smaller a model is, the larger the quality drop after its quantisation. Additional finetuning allows us to recover at least partially from the quantisation damage. Evaluating on the latest testset WMT21, our pruned models are 1.2-1.7? faster at the cost of 0.6-1.3 BLEU. With quantisation, those models are 1.9-2.7? faster losing 0.9-1.7 BLEU in comparison to the unpruned and unquantised baselines. 

 Fixed Point 8-bit Quantization Quantizing fp32 models into 8-bit integers is a known strategy to reduce decoding time, specifically on CPU, with a minimal impact on quality  (Kim et al., 2019; Bhandare et al., 2019; Rodriguez et al., 2018) . This year's submission closely follows the quantization scheme of last year's work  (Bogoychev et al., 2020) . Quantization entails computing a scaling factor to collapse the range of values to  [?127, 127] . For parameters, this scaling factor is computed offline using the maximum absolute value but activation tensors change at runtime. This year, we changed from computing a dynamic scaling factor on the fly for activations to computing a static scaling factor offline. We decoded the WMT16-20 datasets and recorded the scaling factor ?(A i ) = 127/max(|A i |) for each instance A i of an activation tensor A. Then, for production, we fixed the scaling factor for activation tensor A to the mean scaling factor plus 1.1 standard deviation: ?(A) = ?({?(A i )}) + 1.1 * ?({?(A i )}). These scaling factors were baked into the model file so that statistics were not computed at runtime. Quantization does not extend to the attention layer, which is still computed in fp32. The reason being is that in the attention layer, both the A and B matrices of the GEMM operation would need to be quantized at runtime, which makes the quantization too expensive. We note that we only perform the GEMM operations in 8-bit integers. 

 Log 4-bit Quantization We further quantize the models with log based 4bit quantization (Aji and  Heafield, 2020a) . In this case, model weights are represented in a 16 unique quantization centers in a form of S * 2 k . S is a scaling factor that is optimized to minimize the MSE of the quantized weight to the actual weight. Following Aji and Heafield (2020a), we only perform 4-bit quantization on non-bias layers. Unfortunately, the hardware used is not designed to perform native 4-bit operations. Therefore, our 4-bit quantization experiment is used solely for model compression purposes, in which we can reduce the model size to be 8x smaller. To perform inference, we de-quantize the 4-bit model back to fp32 representation, therefore does not achieve any speed up over the vanilla fp32 models. 

 Quantization fine-tuning Quantizing models degrades the quality, especially on smaller architectures. Therefore, after applying quantization, we fine-tune the model under the quantized weight. We find that lowering the learning rate to 0.0001 yields better model quality. Moreover, for 4-bit models, we also find that doubling the warm-up duration helps. Our 8-bit quantization models mainly aim for speed improvement. Therefore, we apply 8-bit quantization to pruned models to further boost the speed. As shown in Table  2 , 8-bit inference achieves significant speedup. However, fine-tuning is necessary to restore the quality degradation.  We apply 4-bit quantization solely for size efficiency. Therefore, we quantize non-pruned models since they give better size to quality trade-off, compared to pruned models. The performance of 4-bit models can be seen in Table  3 . 4 Software improvements 

 CPU We built our work using the Marian machine translation framework, making some improvements on top of the submission from last year: We used predominantly intgemm 3 for our 8-bit GEMM operations, including for the shortlisted output layer. All parameter matrices are quantized to 8-bit offline and the activations get quantized dynamically before a GEMM operation. We only perform the GEMM operation and the following activation in 3 https://github.com/kpu/intgemm 8-bit integer mode. Right after a GEMM operation, the output is de-quantized back to fp32. More formally we perform dequantize(?(A * B + bias)), where the addition of the bias, the activation function 4 ?, and the de-quantization are applied in a streaming fashion to prevent a round trip to memory. Furthermore we make use of Intel's DNNL 5 for our pruned models, as it performs better than intgemm for irregular sized matrices. Unfortunately, DNNL doesn't support streaming de-quantization, bias addition or activation function application. For the CPU_ALL throughput track, we swept configurations of multiple processes and threads on the platform, settling on 4 processes with 9 threads each. The input text is simply split into 4 pieces and parallelized  (Tange, 2011)  over processes. The mini-batch sizes did not impact performance substantially and 32 was chosen as the mini-batch size. The Hyperthreads available on the platform were not put into use as the compute on each was saturated by the efficient threads. Each process is bound to 9 cores assigned sequentially and to the memory domain corresponding to the socket with those cores using numactl. Output from the data parallel run is then stitched together to produce the final translation.   (Heafield et al., 2020) . 

 GPU For our GPU submission we built up on top of last year's submission, applying experimental GPU optimisations on top of the marian-dev master tree 6 and exploring tensorcore 7 applicability using CUT-LASS.  8  Tensorcores can in theory drastically increase the performance of our computations and were enabled for all of our fp16 experiments. Tensorcores can also improve speed when doing 8-bit integer operations, so we implemented 8-bit integer GPU decoding similar to our CPU scheme. We found that shortlisting doesn't improve the performance, so we didn't use it. We found that while fp16 decoding works fairly well and delivers good performance improvements for decoding, especially when using a really large mini-batch size. We performed a large parameter sweep on a RTX 3090, as shown on Table  4 . Unfortunately, we found no setting in which tensorcore 8-bit integer decoding outperforms the fp16 baseline, likely due to the overhead of quantisating the activations beforehand. 

 Conclusion We participated in all tracks of the WMT 2021 efficiency tracks and we submitted multiple systems that have different trade-offs between speed and translation quality. We performed ample hyperparameter tuning and exploration in order to take advantage of GPU tensorcores for decoding, but unfortunately we couldn't beat our optimised fp16 baseline. For the CPU submission we used 8bit 6 https://github.com/marian-nmt/ marian-dev/pull/743 7 https://developer.nvidia.com/blog/ programming-tensor-cores-cuda-9/ 8 https://github.com/NVIDIA/cutlass integer decoding and a combination of pruned and non-pruned system, together with a lexical shortlist in order to reduce the computational cost of the largest GEMM in decoding -the output layer. Table 1 : 1 Architectures for the different student models. The number of encoder/decoder layers are reported with the size of the embedding, attention and FFN layers, the total number of parameters, the model size on disk, quality in both BLEU and COMET as well as speed on WMT21 testset. The first and second groups use a modified attention matrix shape, with second group consisting of tied models. The third group uses the typical shape attention matrices. Depth Dimensions BLEU COMET Model Enc Dec Emb. FFN Att. Heads Params. Size WMT20 WMT21 WMT20 WMT21 Speed (s) teacher x 3 6 6/6/8 1024 4096 1024 16 619.0M 1.59GB 38.3 28.8 56.8 50.8 - 12-1.large 12 1 1024 3072 256 8 130.5M 498MB 37.6 28.7 54.0 47.7 92.2 12-1.base 12 1 512 2048 256 8 51.1M 195MB 36.7 28.2 50.7 44.1 38.9 12-1.tiny 12 1 256 1536 256 8 22.0M 85MB 36.1 27.6 48.2 41.9 19.2 12-1.micro 12 1 256 1024 256 8 18.6M 72MB 35.4 27.6 46.2 40.2 17.1 8-4.tied.tiny 8 4 256 1536 256 8 17.8M 69MB 35.7 27.8 50.3 43.9 30.4 6-2.tied.tiny 6 2 256 1536 256 8 15.7M 61MB 34.9 27.4 47.4 42.1 18.6 6-2.base 6 2 512 2048 512 8 42.7M 163MB 37.7 28.7 54.3 48.5 56.2 6-2.tiny 6 2 256 1536 256 8 16.9M 65MB 35.8 27.4 50.2 44.5 19.2 
