title
Tune In: The AFRL WMT21 News-Translation Systems

abstract
This paper describes the Air Force Research Laboratory (AFRL) machine translation systems and the improvements that were developed during the WMT21 evaluation campaign. This year, we explore various methods of adapting our baseline models from WMT20 and again measure improvements in performance on the Russian-English language pair.

Introduction As part of the 2021 Conference on Machine Translation (wmt, 2021) news-translation shared task, the AFRL human language technology team participated in the Russian-English portion of the competition. We experiment with OpenNMT-tf 1  (Klein et al., 2018)  and Marian 2  (Junczys-Dowmunt et al., 2018)  transformer  (Vaswani et al., 2017)  models trained as part of our WMT20  (Gwinnup and Anderson, 2020)  efforts and apply varying continuedtraining and fine-tuning approaches  (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) , including a new method to select a fine-tuning set from a separate, larger corpus not used in training. We submit an OpenNMT-based transformer system fine-tuned on newstest test sets from 2014-2017 as our primary entry, and a Marian-based transformer system fine-tuned on newstest test sets from 2014-2018 as a contrast. 

 Data and Preprocessing Since most of our efforts focus on fine-tuning existing models this year, we reuse the training corpus from our WMT20 systems which includes the following parallel corpora: Commoncrawl  (Smith et al., 2013) , Yandex 3 , UN v1.0  (Ziemski et al., 2016) , Paracrawl 4  (Espl? et al., 2019) , Wikimatrix  (Schwenk et al., 2019) , and backtranslated data from our WMT17 system  (Gwinnup et al., 2017)  as well as Edinburgh's WMT17 system  (Sennrich et al., 2017)  yielding a raw corpus of over 76.3 million lines. The new Russian-English version 8 Paracrawl corpus is reserved for tuning set selection as described in Section 2.3. 

 Data Preparation We re-use the fastText  (Joulin et al., 2016b,a)  based language ID filtered corpus with an ID threshold of 0.8 as described in  Gwinnup and Anderson (2020) , shown in Table  1 , allowing us to make concrete progress comparisons to last year's systems. 

 Data Augmentation with Speech Recognition-like output In order to build a larger pool of training data, we have created Automatic Speech Recognition (ASR) -like training data for the Russian-English translation task. Whereas written text can include upper and lowercase characters, punctuation, special symbols, and numbers written using digits, transcripts produced by ASR systems are typically uncased with no punctuation, no special symbols, and numbers written as spoken (e.g., 4.1% rendered as "four point one percent"). In previous experiments on an English-German spoken language translation task  (Ore et al., 2020) , we found that we could get an improvement in BLEU score by formatting the MT training data such that the source language text matched the output format of our ASR system, while leaving the target language text unmodified. We applied a similar procedure to the Russian side of the Russian-English training corpus using the text2norm.pl script from ru2sphinx. the original training data, effectively doubling the size of the corpus. 

 Selecting Tuning Sets from Representative Data We performed experiments involving automatic selection of fine-tuning corpora. Given a monolingual application corpus, we wish to test the possibility of selecting an appropriate fine-tuning set to improve a general-purpose neural MT system's performance on that application corpus. We anticipate such techniques to be of increasing importance, especially for high-value application corpora, as computational costs of subcorpus selection and fine-tuning continue to decrease. 

 Method We performed subselection as in  Erdmann and Gwinnup (2019) , which can flexibly incorporate a text quality metric and multiple parallel text corpora. In short, this algorithm tries to simultaneously optimize the quality of the subset's text and its coverage of the vocabulary present in given application corpora. Of special note is our use of clustering to select data. We hierarchically applied the MAPPER algorithm  (Singh et al., 2007)  to cluster sentence vectors of a monolingual corpus. The clusters deemed useful were then used to assign fuzzy clustering to the application corpus and the corpus from which we subselect. This clustering information was included as one of the text corpora. 

 Application The application corpus we used was the Russian side of newstest2019 and newstest2020, totalling 6777 lines. The pool of possible parallel text for subselection we took to be the given 12.6M-line subset of Russian-English version 8 ParaCrawl corpus with LASER score at least 1.1. For subselection algorithms, we first preprocessed the Russian text, applying a 90k-element joint BPE. We used the algorithm in  Erdmann and Gwinnup (2019)  to subselect a corpus, using 3-grams in the vocabulary coverage. As a text quality metric in this algorithm we used either the provided Bicleaner scores  (S?nchez-Cartagena et al., 2018; Ram?rez-S?nchez et al., 2020)  or the word-averaged scores provided by OpenNMT's scoring functionality, using the untuned OpenNMT model we developed for this year's task. In order to provide meaningful comparisons with our baseline fine-tuning set of newstest2014-2018, we matched its size by always subselecting a fine-tuning set with fifteen thousand lines. Fine-tuning was performed using a singlemodel Marian-based untuned MT system as a baseline. Sentence vector clustering was learned using a 570M-line monolingual Russian corpus built from the concatenation of monolingual CommonCrawl  (Smith et al., 2013)  data provided by WMT organizers as part of our WMT18 efforts towards pretraining word embeddings. The word vectors were trained using word2vec  (Mikolov et al., 2013)  on this corpus, after applying a 90k-element joint BPE. These embeddings have a dimensionality of 512 to match our Marian transformer-base system configuration as described in  Gwinnup et al. (2018) . A randomly-chosen 100k-line subset of the corpus was used to find the clustering. Several methods of converting word vectors to sentence vectors were considered, and we empirically chose a "softened sum" of the word vectors w i as the sentence vector s: s = w i log(1 + number of words in sentence) . Clusters were considered to be useful if they covered between 1% and 5% of this corpus. In this case there were 19 such clusters, having between 1000 and 5000 representatives each. These clusters were found to have qualitative meaning to a Russian linguist: clusters with relatively high representation in our application corpus tended to be news-like, and clusters with relatively high representation in ParaCrawl tended to be noisier. We computed membership of a given sentence vector in a fuzzy clustering sense, with weight of cluster i defined as z i = (min distance/distance i ) 4 where we use Euclidean distance, and the minimum is taken over all 19 clusters. Although the exact form is empirical, note that the weight has a maximum of unity at the closest cluster and that a cluster will get lower weight if it is farther from the sentence vector. This fuzzy clustering was computed once using k-means (distance is to cluster mean) and once using single-linkage (distance is to nearest member) clustering. These two membership clusters were then averaged. Coverage of the clusters was encouraged by including the clustering as another text corpus in our standard algorithm (Erdmann and Gwinnup, 2019) -each sentence vector was converted into a 100-word "sentence," where each cluster's "word" appeared a number of times relative to the magnitude of its weight in the line's clustering 6 . Naturally, coverage of these clustering words was computed using only unigrams. 

 Results Table  2  shows the results of our fine-tuning experiments. The "clustering" and "metric" columns designate whether clustering was incorporated and whether Bicleaner ("Bic") or NMT scoring ("NMT") was used as the text quality metric. We see consistent gains over the untuned set, even on newstest2021, which was not used in the selection. The three subselection methods produced similar results on the three test sets. Fine-tuning with our selected sets did not  6  For example, using a 10-word sentence for brevity, this process would convert the fuzzy cluster membership vector [0.2, 0.0, 0.8, 1.0] into the sentence "0 2 2 2 2 3 3 3 3 3". produce consistent improvement over our baseline fine-tuning using newstest2014-2018. Compared to this baseline fine-tuning, the new sets improved performance on newstest2019 (roughly +0.7 BLEU), but they lowered performance on newstest2020 (roughly ?0.7 BLEU) and the unseen newstest2021 (roughly ?1.1 BLEU). Our generated fine-tuning sets did not show a consistent benefit for this task, so they were not used in our submission systems. Without further information, we cannot attribute the quality of the results to the method, the quality of data in ParaCrawl, or other causes. Our method generates a pseudo in-domain set for an unknown application domain, using only source-side data of the application corpus. This generated set can be used for fine-tuning, training, or other purposes in natural language processing. We believe that such techniques warrant further investigation, especially for an application corpus where the domain is unknown or human-curated parallel data are unavailable. 

 Machine Translation Systems 

 OpenNMT-tf The OpenNMT-tf system trained for this task used the configuration for a big deep transformer network. We used the following network hyperparameters: ? 1024 embedding size with SentencePiece  (Kudo and Richardson, 2018)  using a model with a vocabulary size of 40K trained on this ru-en corpus of 16,805,109 bi-text. This was one of our WMT20 submitted systems (Systems 3 and 4 in Table  3 ). Additionally the corpus was processed as described in Section 2.2 to resemble ASR output and the resulting data was combined with the above for a final count of 33,610,218 bitext. The network was trained for 10 epochs of this training data using a batch size of 3124 and an effective batch size of 49984 using the lazy Adam (Kingma and Ba, 2015) optimizer with beta1=0.9, beta2=0.998 and learning rate 2.0. This a system that had been originally trained for speech translation application but showed improvements in text translation as well. The final submitted system continued training an additional 2 epochs using the unfiltered data described in Table  1 . This was done to try to take advantage of the larger data set and not having the computational resources or time to train a new system with with the larger data set in time for submission deadline. The output was an average of the last 8 checkpoints of training. Checkpoints were saved every 5000 steps. The system was then tuned with three epochs of newstest data from years 2014-2017 (Systems 5 and 6 in Table  3 ). 

 Marian Our Marian systems utilize the transformer architecture in the transformer-base configuration. We use the WMT14 newstest2014 test set for validation during training and the following network hyperparameters: ? 512 embedding size ? 2048 hidden units ? 6 layer encoder ? 6 layer decoder ? 8 transformer heads ? Tied embeddings for source, target and output layers ? Layer normalization ? Label smoothing ? Learning rate warm-up and cool-down We experimented with tuning these systems with the concatenation of WMT newstest sets from 2014-2018 yielding a tuning corpus of 14,820 parallel sentences. For each of the five separate transformer models trained for the Marian transformerbase ensemble systems in  Gwinnup and Anderson (2020) , continued training was performed for 10 epochs on the concatenated tests sets. An ensemble of the five resulting tuned models is then used to decode newstest sets from 2019-2021. Resulting scores reported by SacreBLEU are shown as Row 2 in Table  3 , while the baseline, untuned ensemble is shown as Row 1. We note gains between +2.0 and +3.5 BLEU as measured by SacreBLEU over the baseline ensemble system depending on test set. 

 Experimental Results Results reported here and in Table  3  for Marian systems were scored with SacreBLEU  (Post, 2018)  while results for OpenNMT systems were score with mult-bleu-detok.perl from the Moses toolkit  (Koehn et al., 2007) . Internal comparisons between the two scoring methods have been in agreement. All scores are on detokenized cased output. The primary submission system was the OpenNMT-tf configuration described in section 3.1 and shown in Table  3  as onmt+asr-tune. It resulted in official scores of  38 .83 BLEU-A, 39.56 BLEU-B, 0.64 chrf-all, 0.63 chrf-A, and 0.64 for chrf-B on the 2021 test-set. Post evaluation a model with the OpenNMT-tf configuration described in section 3.1 was trained on all the unfiltered data (approx. 76M million bi-text). The results are shown in Table  3  as onmtlarge. The baseline onmt-large system was approx-imately +1 BLEU better that the baseline onmtasr system while the onmt-asr system which continued training with two epochs of the large data set and tuned with newstest2014-2017 (onmt-+asrtune) was +2.5 BLEU better than the baseline onmtlarge system which was trained with 10 epochs and comparable to the onmt-large system tuned with newstest2014-2017. Experiments were conducted on both onmt+asr and onmt-large with tuning sets comprised of different combinations of the supplied news test sets from 2014 to 2019. Tune7 is news test sets from  2014-2017 (11,820 bi-text) , tune8 is news test sets from 2014-2018 (14,820 bi-text), and tune9 is news test sets from  2014-2019 (16,820 bi-text) . Systems were tuned for three epochs using these tuning sets. Generally performance dropped off or decreased slightly with more than 3 epochs of tuning. To be consistent across systems and tuning sets we are only reporting results for 3 epochs. As can be seen in Table  3  all three tuning sets provided significant improvements over the baseline systems, generally in the range of +3.5 BLEU on test 2021. For onmt+asr there was little difference in tuning with tune7 or tune8 whereas tune9 was approximately +0.4 BLEU better than those two. For onmt-large tune7 did not provide as much benefit as tune8 and tune9 which were basically the same, less than 0.1 BLEU difference between the two. 

 Conclusion While our two submission systems employ a standard method of fine-tuning to adapt models towards a test set, we find that our methods to sample a similarly-sized tuning corpus from a larger body of text while only using information about the source side of that data yields a reasonable improvement in translation quality. Such a technique could be useful in adapting translation models to specific domains where only the source language of a text source is available. Using actual in-domain data, such as the provided news development sets, for fine-tuning provide a substantial gain in translation quality. Such data is not always available and thus other selection techniques as described in Section 2.3 come into play. Future work will investigate combining the two approaches to see if additional gains can be obtained. The authors would like to thank Emily Conway and Braeden Bowen for their assistance in human   Table 2 : 2 Tuning sets and resultant BLEU scores. ? 4096 hidden units ? 12 layer encoder ? 12 layer decoder ? 16 transformer heads ? dropout 0.3 ? attention dropout 0.1 ? feed forward network dropout 0.1 ? embeddings for source, target and output lay- ers were not tied ? Layer normalization ? Label smoothing 0.1 ? Learning rate warm-up 8000 steps The corpus used for the initial model con- sisted of commoncrawl, paracrawl v1, and news- commentary-v13 from wmt19 and was processed 
