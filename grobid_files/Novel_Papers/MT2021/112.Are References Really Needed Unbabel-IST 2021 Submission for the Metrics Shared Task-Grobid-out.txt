title
Are References Really Needed? Unbabel-IST 2021 Submission for the Metrics Shared Task

abstract
In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task. With this year's focus on Multidimensional Quality Metric (MQM) as the ground-truth human assessment, our aim was to steer COMET towards higher correlations with MQM. We do so by first pretraining on Direct Assessments and then finetuning on z-normalized MQM scores. In our experiments we also show that referencefree COMET models are becoming competitive with reference-based models, even outperforming the best COMET model from 2020 on this year's development data. Additionally, we present COMETINHO, a light-weight COMET model that is 19x faster on CPU than the original model, while also achieving state-of-theart correlations with MQM. Finally, in the "QE as a metric" track, we also participated with a QE model trained using the OPENKIWI framework leveraging MQM scores and word-level annotations. 1   
 Crosslingual Optimized Metric for Evaluation of Translation hosted at: https://github.com/ Unbabel/COMET

Introduction In this paper, we present the joint contribution of Unbabel and IST to the WMT 2021 Shared Task on Metrics. We participated in the segment-level and system-level tracks, as well as the "QE as a Metric" task. Similar to our participation last year  (Rei et al., 2020b) , most of the models are based on the COMET framework 1  (Rei et al., 2020a) . In last year's shared task  (Mathur et al., 2020) , COMET along with other trainable metrics such as PRISM  (Thompson and Post, 2020)  and BLEURT  (Sellam et al., 2020)  showed superior correlations with the Direct Assessments (DA) collected for the News Translation Shared Task. This year, we build on top of the models used last year to take into account that human assessments will be carried out using variants of the Multidimensional Quality Metric (MQM)  (Lommel et al., 2014)  framework and no longer based on DA  (Graham et al., 2013) . For this reason, we extended our training dataset to include DA evaluations from WMT ranging 2015 to 2020, with the exception of en-de and zh-en for which we do not include the 2020 data given that the same is included in the MQM development data  (Freitag et al., 2021) . Finally, we fine-tuned these new models on the znormalized MQM scores provided for this year's shared task. One of the remaining redeeming qualities of automated metrics such as BLEU  (Papineni et al., 2002)  is that they are incredibly light-weight. Despite the higher correlation with human judgement, trainable metrics tend to be slower to run. In an effort to close this gap we present COMETINHO, a light-weight model based on the COMET framework that replaces the original XLM-R large encoder with MiniLMv2  (Wang et al., 2020) . This model is approximately 19x faster at inference time compared to the original COMET model  (Rei et al., 2020a)  and maintains state-of-the-art correlations with MQM in reference-based evaluations. For the "QE as a metric" track, we show that reference-free evaluation models can reach surprisingly high correlations with human judgements and are competitive with their corresponding referencebased models. Last year we also participated with a similar model in the Metrics Shared Task, but here we elaborate in more detail on the primary differences between this model architecture and other COMET models. Finally, and for the first time, we submit and describe a reference-free model that in addition to learning from MQM scores also makes use of word-level error annotations. This is possible this year given the shift in evaluation method from DA the main goal of this project is to develop a car for the blind. Table  1 : Example of word-level OK and BAD tags produced by our OPENKIWI model trained with word-level annotation spans. This translation received an overall sentence score of 0.2 and the model was able to identify that the words "blind driving" are translation errors giving a good insight on why the sentence score is low. to MQM. This model uses the OPENKIWI 2 architecture and its word-level tagging feature to predict OK/BAD word tags along with a sentence-level quality score. 

 The COMET Framework For a more comprehensive description of the COMET architecture we direct the reader to the original paper  (Rei et al., 2020a) . Below we will highlight some relevant features that contrast with the COMET reference-free model (COMET-QE). In COMET we encode segment-level representations using the pretrained, cross-lingual, model XLM-RoBERTa  (Conneau et al., 2020) . Even though we encode the source, the hypothesis, and the reference (i.e. the human curated translation of the source) separately, their embeddings are mapped into a shared feature space. Subsequently, we obtain combined features using the three embeddings (s, h, and r, for the source, hypothesis, and reference, respectively): h s , h r, |h ? s|, and |h ? r|. These features, concatenated to r and h and the resulting vector is the input to a feedforward regressor. 

 Reference-free COMET The architecture of the COMET model used in the "QE as a metric" task (COMET-QE) is very similar to the main COMET model  (Rei et al., 2020a)  briefly described above and RUSE  (Shimanaka et al., 2018) . The biggest difference being that in the COMET-QE model the reference is not used and, consequently, the combination of features used as input to the feed-forward regressor are also different from reference-based COMET. In this case, the combined features are simply: h s and |h ? s|; the final vector to the feed-foward regressor being the concatenation of the latter features together with h and s. A schematic representation is shown in Figure  1 . 3 Lightweight COMET: COMETINHO Our light-weight version of the original COMET model is almost an exact replica in terms of architecture save that we replaced the underlying pre-trained encoder with MiniLMv2  (Wang et al., 2020)  which is a distilled version of XLM-R large  (Conneau et al., 2020) . This distilled model is made available by HuggingFace Transformers  (Wolf et al., 2020) : nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large Our COMETINHO models are 19x faster on CPU and 14.3x times faster on GPU than COMET models based on XLM-R large. Also, in terms of disk footprint, these models are 5x smaller 3 . 

 The OPENKIWI Framework When using the MQM framework for the calculation of the quality score, human annotators seek to identify and annotate error spans at the wordlevel, as well as the severity of those errors. We leveraged these word-level annotations using the OPENKIWI framework  (Kepler et al., 2019) , by transforming each word into an OK or BAD tag. In the OPENKIWI architecture, in contrast with COMET-QE, source and hypothesis are jointly encoded. A sentence pair representation is then obtained using average pooling over the hypothesis word embeddings and then used as features to a feed-forward regression layer that learns to produce a sentence level score. At the same time, the word embeddings from the hypothesis are used to predict OK/BAD tags and therefore, the model is trained in a multitask setting (regression and sequence labelling). 

 Corpora In this year's shared task the organisers provided a development set with MQM annotations for the ende and zh-en participating systems on WMT20  (Freitag et al., 2021) . Apart from the official development data we used all the Direct Assessments available from previous years. 

 Multi-dimensional Quality Metric Corpus In this corpus, for each language pair, each translation was annotated by 3 raters from a pool of 6. Following what is a common practice for the DA's we convert the segment-level scores of each annotator into a z-normalized score and the final translation quality score is an average of the 3 z-scores. Also, because the sign of these MQM annotations is the opposite of the Direct Assessments we invert the score. Subsequently we generate a train and test split leaving 20% of the documents for each language pair for testing. This results in a total of 11230 en-de training samples and 15600 zh-en training samples, with testsets of 2950 and 4400 samples, respectively. All results reported in this paper are with respect to the above train and test split. The documents contained in each split are listed in the Appendix of this paper. Annotators are not always consistent and the annotations of one annotator might differ from another . With this in mind, we decided to calculate the Kendall's Tau correlation between all annotators as a measure of inter-annotator agreement (Figure  2 ). The interannotator Kendall Tau can then be used as a ceiling effect for the developed metrics which ideally should behave as an additional annotator. For training of the OPENKIWI model described herein we used proprietary MQM data from the customer support domain, covering several industries such as tech industry and travel industry. This data is composed by 1.1M (source, hypothesis) pairs with corresponding MQM annotations from 38 language pairs mostly out-of-english. 

 Direct Assessments Each year, the WMT News Translation shared task organisers collect human judgements in the form of Direct Assessments. Those assessments are then used in the Metrics task to measure the correlation between metrics and therefore decide which metric works best. In recent years researchers have been using these annotations to create trainable metrics that regress on these scores  (Shimanaka et al., 2018; Sellam et al., 2020; Rei et al., 2020a) . We follow the same approach and use Direct Assessments ranging from 2015 to 2020 for training. The collective corpora contain a total of 33 language pairs including low-resource languages such as English-Tamil (en-ta) and a total of 795269 tuples with source, hypothesis, reference and direct assessment z-score. The only exception to this data is that we did not include the en-de and zh-en assessment from 2020 because they overlap with the MQM development data described in section 5.1. 

 Segment-level task The COMET framework is highly flexible and easy to adapt to different types of human judgements  (Rei et al., 2020a) . This year we first pre-trained on the DA collected from 2015 to 2020 except for ende and zh-en as described above. Like in  Glushkova et al. (2021)   model (COMET-DA). During our experiments we tested two ensembling techniques; averaging the different model predictions and averaging the parameters from the 5 models. Those two approaches had similar results but in the end we decided to use the later one for performance. Subsequently, we fine-tuned each of the 5 models on the MQM data provided as development for another epoch. As before, we performed weight averaging to obtain an ensemble of those models (COMET-MQM). In both the pre-training and finetuning we only perform 1 training epoch in order to ensure that the final models are able to generalise to many language pairs and do not overfit to the News domain. This is especially important since the MQM dataset only contains en-de and zh-en. For COMETINHO, as previously mentioned, we used the distilled version of XLM-R (MiniLMv2), available through Hugging Face, and we followed the same training recipe where we pre-train the model using DA's for 1 epoch and then we adapt the model to the MQM data for another epoch. 

 System-level task For the System-level task we compute the systemlevel score for each system by averaging the segment-level scores obtained. This follows the same approach used to compute system-level scores based on segment-level human annotations such as DA's and MQM which means that a met-ric that achieves strong segment-level correlation should also achieve strong system-level performances. 

 QE as a Metric Task We trained a reference-free model (COMET-QE) in the same way we did for reference-based COMET models described in section 6. As described in section 2.1, the primary difference between the two models is the inclusion or exclusion of the source as input. 

 Experimental Results 

 Segment-level task Reference-based segment-level correlations on the en-de and zh-en testsets are shown in Table  2 . We used both Pearson and Kendall Tau correlation metrics to evaluate our models. As baselines we used lexical metrics such as CHRF  (Popovi?, 2015)  and BLEU  (Papineni et al., 2002) , an embedding-based metric BERTSCORE  (Zhang et al., 2020)  and three trainable-metrics; BLEURT  (Sellam et al., 2020) , PRISM  (Thompson and Post, 2020)  and COMET-DA (2020)  (Rei et al., 2020b) . The fact that the COMET-DA (2021) gives higher correlations than the COMET-DA (2020) shows that adding more training data and combining checkpoints trained on different seeds already provides a boost in performance. However, fine- tuning on the MQM development data was the most significant addition to previous work: the COMET-MQM (  2021 ) model increased on average more than 0.1 Pearson correlation. This improvement is consistent with regard to the two COMETINHO models (with COMETINHO-MQM having notably higher correlations than COMETINHO-DA). Nevertheless, the fact that COMETINHO-DA has competitive or state-of-the-art performance with all the other metrics such as BLEURT, PRISM, and BERTSCORE, while also being much faster, presents an ideal opportunity for future work to investigate the incorporation of trainable metrics into the training objectives of MT systems. For reference-free metrics, the fine-tuning on the MQM data, on average, gave a boost in performance (the only exception being the Pearson correlation for the en-de where COMET-QE-DA has a slightly higher correlation than COMET-QE-MQM). Overall, it is somewhat surprising that COMET-QE-* (2021) and COMET-* (2021) show relatively comparable correlations, suggesting that using the reference as input for MT evaluation might be less useful than expected and could feasibly become redundant. This surprising result was also reported by  Kocmi et al. (2021)  and is especially important since curating reference sentences is usually costly and time consuming and can introduce undesired bias in the evaluation . Finally, the OPENKIWI model has competitive correlations when looking to other trainable metrics and to COMET models that were not fine-tuned on the MQM development data. This add further weight to the suggestion above that references might not add substantial value to MT evaluation. Its performance is even more surprising when considering the fact that this model was train with data from a completely different domain. It is worth highlighting that the Kendall's Tau correlations for all models (with exception of the two reference-based COMETINHO models) are in the range obtained for correlations between different annotators, for en-de, Figure  1 . This further validates the value of our models. 

 System-level task System-level results are presented in Table  3  where we report a Kendall Tau correlation defined as follows: ? = Concordant ? Discordant Concordant + Discordant (1) where Concordant defined as the number of times a metric agrees with humans that a given system x is better than a given system y and Discordant is the opposite. These decisions are the computed for all combinations of systems in the testset. Due to the low number of systems and the relative proximity of the ground-truth MQM system scores we also compare metrics on their ability to distinguish human references from MT outputs. With reference to table 7 in the appendix we note that, for zh-en, all 8 MT systems demonstrate comparable performance but that there is a clear separation of human translations. For that reason Table  3  also presents the Kendall Tau correlations considering only "Human" systems against MT systems where we can observe that reference-free metrics achieve better performance. This results confirms the finding from last year's shared task  (Mathur et al., 2020)  where COMET-QE was highlighted as being the only metric able to differentiate human translations from MT. 

 Related work Classic n-gram matching MT evaluation metrics such as BLEU  (Papineni et al., 2002)  have been adopted by the MT community as a primary form of MT evaluation, yet, in the recent years of the WMT Metrics shared task  (Bojar et al., 2017; Ma et al., 2018 Ma et al., , 2019 Mathur et al., 2020)  these classic metrics have been outperformed first by embeddingbased alternatives and more recently by trainable metrics based on pre-trained models. With the rise of word embeddings  (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019) , metrics such as BLEU2VEC  (T?ttar and Fishel, 2017)  and MEANT 2.0  (Lo, 2017)  replaced the typical word/n-gram matching by fuzzy matches based on distributional word representations. These metrics appeared for the first time at the WMT Metrics task in 2017 with MEANT 2.0-SRL achieving the highest results at segment-level. In 2018 and 2019 YISI-1  (Lo, 2019) , a successor of MEANT 2.0  (Lo, 2017) , was among the winners of the WMT Metrics task. YISI-1  (Lo, 2019)  mostly takes advantage of BERT embeddings  (Devlin et al., 2019)  to create soft alignments between hypothesis and reference. Trainable metrics started as simple regressions based on lexical features (e.g BLEND  (Ma et al., 2017) ) but nowadays these metrics also use embeddings to extract features that are then used to regress on quality assessments. The first of such metrics were RUSE  (Shimanaka et al., 2018)  and ESIM  (Mathur et al., 2019)  which were based on RNN encoders and worked mostly for English. In 2020, BLEURT  (Sellam et al., 2020)  and COMET  (Rei et al., 2020a)  were proposed. Both metrics used pre-trained transformer based encoders to extract sentence-level features that are then passed to a regression model; the difference is that COMET also extracts features for the source segment which was something overlooked by predecessor metrics. In the 2020 Metrics Shared task both COMET and BLEURT achieved some of the highest correlations with human judgements and shared the podium with PRISM  (Thompson and Post, 2020)  11 Conclusions In this paper we present the Unbabel-IST's contribution to the WMT 2021 Metrics shared task which for the first time, introduced evaluation using MQM. Our specific contributions include; the fine-tuning of Direct Assessment based models on MQM data which yields impressive gains on the described test sets and a new, lightweight COMET model which achieves comparable performance to its predecessors. Such a light model can provide interesting opportunities for future work into the incorporation of modern metrics into MT training. Finally, but perhaps our most important contributions; we further validate the observations in  (Kocmi et al., 2021)  that QE as a metric is becoming competitive as an alternative to reference-based evaluation, and, we show that a word-level QE system can be successfully trained on MQM annotations and be competitive with current trainable metrics while providing some intuition about "what" is wrong with a specific translation. 

 A Appendix A.1 COMET Hyper-Parameters In Table  5  is an excerpt of the training configuration used for training the COMET-DA model and Table  5  for the COMET-QE-DA. Then these models are finetuned for 1 extra epoch with same hyperparameters except the learning_rate that is decreased to 1.0e ? 05 and the nr_frozen_epochs which we increase to 1 to completely freeze the encoder model. 

 A.2 OPENKIWI Hyper-Parameters The hyperparameters used for the OpenKiwi model are expressed in  Figure 1 : 1 Figure 1: The COMET-QE model follows the dual encoder architecture proposed in RUSE (Shimanaka et al., 2018) but replacing the reference translation with the source sentence. 

 Figure 2 : 2 Figure2: Kendall Tau Correlations between the en-de annotators used to develop the shared task development set(Freitag et al., 2021). 

 Table 2 : 2 Segment-level correlations on the en-de and zh-en testset. we trained 5 models for 1 epoch each using 5 different seeds and created an ensembled 

 Table 3 : 3 System-level Kendall's Tau (? ) correlations for all system combinations (on the left) and Human vs MT (on the right). All systems Human vs MT en-de en-zh en-de en-zh N? Comparisons 45 45 21 16 Kendall Avg Kendall Avg BLEU 0.378 0.311 0.345 0.095 0.077 0.086 Baselines CHRF BERTSCORE (F1) PRISM 0.444 0.422 0.433 0.143 0.000 0.072 0.356 0.356 0.356 0.143 0.000 0.072 0.444 0.422 0.433 0.143 0.077 0.110 COMET-DA (2020) 0.822 0.533 0.678 0.714 0.231 0.473 Ref. based COMET-DA (2021) COMET-MQM (2021) COMETINHO-DA COMETINHO-MQM 0.844 0.489 0.667 0.761 0.231 0.496 0.867 0.778 0.823 0.762 0.875 0.819 0.533 0.378 0.456 0.238 0.000 0.119 0.355 0.311 0.333 0.095 0.000 0.048 Ref. Free COMET-QE-DA (2021) COMET-QE-MQM (2021) 0.933 0.800 0.867 1.000 1.000 1.000 0.778 0.778 0.778 0.667 0.938 0.803 OPENKIWI 0.822 0.733 0.778 0.762 0.769 0.766 

 Table 4 : 4 Table 4 and follows the configurations proposed in the sample file of the github repository 4 . Hyperparameters for OPENKIWI MQM model 4 https://github.com/Unbabel/OpenKiwi/ blob/master/config/xlmroberta.yaml nr_frozen_epochs 0.3 keep_embeddings_frozen True optimizer AdamW encoder_learning_rate 1.0e-05 learning_rate 3.1e-05 layerwise_decay 0.95 encoder XLM-RoBERTa pretrained_model xlm-roberta-large pool avg layer mix dropout 0.15 batch_size 4 gradient_accumulation_steps 4 hidden_sizes [3072, 1024] System epochs 1 batch_size 2 Encoder hidden_size 1024 Decoder bottleneck_size 1024 dropout 0.05 hidden_size 1024 Optimizer class_name adam encoder_learning_rate 0.0001 learning_rate_decay 1.0 learning_rate_decay_start 0 learning_rate 0.0001 Trainer training_steps 2180 early_stop_patience 10 validation_steps 0.5 gradient_accumulation_steps 4 gradient_max_norm 1.0 

 Table 5 : 5 Hyper-parameters for fine-tuning Referencebased COMET model on Direct Assessments. nr_frozen_epochs 0.3 keep_embeddings_frozen True optimizer AdamW encoder_learning_rate 1.0e-05 learning_rate 3.1e-05 layerwise_decay 0.95 encoder XLM-RoBERTa pretrained_model xlm-roberta-large pool avg layer mix dropout 0.15 batch_size 4 gradient_accumulation_steps 4 hidden_sizes [2048, 1024] epochs 1 

 Table 6 : 6 Hyper-parameters for fine-tuning Referencefree COMET model on Direct Assessments. en-de zh-en System MQM System MQM Human-B.0 0.794 Human-A.0 3.114 Human-A.0 0.933 Human-B.0 3.149 Human-P.0 1.547 Huoshan_Translate.919 5.077 Tohoku-AIP-NTT.890 2.043 Tencent_Translation.1249 5.163 OPPO.1535 2.284 OPPO.1422 5.309 Tencent_Translation.1520 2.333 THUNLP.1498 5.389 Online-B.1590 2.516 DeepMind.381 5.442 eTranslation.737 2.530 WeChat_AI.1525 5.469 Huoshan_Translate.832 2.600 DiDi_NLP.401 5.484 Online-A.1574 3.189 Online-B.1605 5.512 

 Table 7 : 7 System-level Ranking and corresponding MQM scores for the test split described in section 5.1 

			 OpenKiwi hosted at: https://github.com/ Unbabel/OpenKiwi 

			 Contrastive inference times were tested using a 2.3 GHz Intel Core i5 for CPU, and using a Nvidia T4 for GPU.
