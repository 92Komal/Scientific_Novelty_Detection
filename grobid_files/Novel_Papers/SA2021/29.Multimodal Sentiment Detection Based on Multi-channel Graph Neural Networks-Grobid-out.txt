title
Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks

abstract
With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for imagetext sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multichannel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.

Introduction The tasks of extracting and analyzing sentiments embedded in data have attracted substantial attention from both academic and industrial communities  (Zhang et al., 2018; Yue et al., 2018) . With the increased use of smartphones and the bloom of social media such as Twitter, Tumblr and Weibo, users can post multimodal tweets (e.g., text, image, and video) about diverse events and topics to convey their feelings and emotions. Therefore, multimodal sentiment analysis has become a popular research topic in recent years  (Kaur and Kautish, 2019; Soleymani et al., 2017) . As shown in Fig.  1 , sentiment is no longer expressed by a pure modality in the multimodal scenario but rather by the com- Two posts express the user's positive sentiment from multimodal data that has global characteristics, including the "have a fun/nice day" phrase, the ocean scene, and the beach scene. bined expressions of multiple modalities  (e.g., text, image, etc.) . In contrast to unimodal data, multimodal data consist of more information and make the user's expression more vivid and interesting. We focus on multimodal sentiment detection for image-text pairs in social media posts. The problem of image-text mismatch and flaws in social media data, such as informality, typos, and a lack of punctuation, pose a fundamental challenge for the effective representation of multimodal data for the sentiment detection task. To tackle this challenge,  Xu et al. (2017; 2017)  constructed different networks for multimodal sentiment analysis, such as a Hierarchical Semantic Attentional Network (HSAN) and a Multimodal Deep Semantic Network (MDSN).  Xu et al. (2018)  and  Yang et al. (2020)  proposed a Co-Memory network (Co-Mem) and a Multi-view Attentional Network (MVAN) models, respectively, introducing memory networks to realize the interaction between modalities. The above methods treat each image-text post in the dataset as a single instance, and feature dependencies across instances are neglected or modeled implicitly. In fact, social media posts have specific global co-occurring characteristics, i.e., co-occurring words, objects, or scenes, which tend to share similar sentiment orientations and emotions. For example, the co-occurrences of the words "have a fun/nice day" and of the bright scenes "ocean/beach" in the two images in Fig.  1  imply a strong relationship between these features and positive sentiment. How to more effectively make use of the feature co-occurrences across instances and capture the global characteristics of the data remain a great challenge. We propose a Multi-channel Graph Neural Networks model with Sentiment-awareness (MGNNS) for multimodal sentiment analysis that consists of three stages. (i) Feature extraction. For text modality, we encode the text and obtain a text memory bank; for image modality, we first extract objects and scenes and then capture the image' semantic features from a multiview perspective. (ii) Feature representation. We employ a Graph Neural Network (GNN) for text modality based on the global shared matrices, i.e., one text graph based on word co-occurrence is built based on the whole dataset. Specifically, we first connect word nodes within an appropriate small window in the text. After that, we update the node representation by itself as well as neighbor nodes. For image modality, it is believed that different views of an image, such as the beach (Scene view) and person (Object view) in Fig.  1 (a), can reflect a user's emotions  (Xu and Mao, 2017) . The existing literature usually models the relationship between the scenes and objects within an image, failing to capture the rich co-occurrence information from the perspective of the whole dataset. In contrast, we explicitly build two graphs for scenes and objects according to the co-occurrences in the datasets and propose Graph Convolutional Network (GCN) models over the two graphs to represent the images. In general, to tackle the isolated feature problem, we build multiple graphs for different modalities, with each GNN acting as a channel, and propose a Multi-channel Graph Neural Networks (Multi-GNN) module to capture the in-depth global characteristics of the data. This multi-channel based method can provide complementary representation from different sources  (George and Marcel, 2021; George et al., 2019; Islam et al., 2019) . (iii) Feature fusion. Previous studies usually directly connect multimodal representations, without considering multimodal interactions  (Wang et al., 2020a; Xu, 2017; Xu and Mao, 2017) . In this stage, we realize the pairwise interaction of text and image modalities from different channels through the use of the Multimodal Multi-head Attention Interaction (MMAI) module and obtain the fusion representation. Our main contributions are summarized as follows: ? We propose a novel MGNNS framework that models the global characteristics of the dataset to handle the multimodal sentiment detection task. To the best of our knowledge, we are the first to apply GNN to the image-text multimodal sentiment detection task. ? We construct the MMAI module from different channels to realize in-depth multimodal interaction. ? We conduct extensive experiments on three publicly available datasets, and the results show that our model outperforms the stateof-the-art methods. 2 Related Work 

 Multimodal Sentiment Analysis For convenience, multimodal polarity analysis and emotion analysis are unified to form multimodal sentiment analysis. Traditional machine learning methods are adopted to address the multimodal sentiment analysis task  (P?rez-Rosas et al., 2013; You et al., 2016)   

 Graph Neural Network The Graph Neural Network has achieved promising results for text classification, multi-label recognition, and multimodal tasks. For text classification, a novel neural network called Graph Neural Network (GNN), and its variants have been rapidly developed, and their performance is better than that of traditional methods, such as Text GCN  (Yao et al., 2019) , TensorGCN  (Liu et al., 2020) , and TextLevelGNN  (Huang et al., 2019) . The GCN is also introduced in the multi-label image recognition task to model the label dependencies  (Chen et al., 2019) . Recently, Graph Convolutional Network has been applied in different multimodal tasks, such as Visual Dialog  (Guo et al., 2020; Khademi, 2020) , multimodal fake news detection  (Wang et al., 2020a) , and Visual Question Answering (VQA)  (Hudson and Manning, 2019; Khademi, 2020) .  Jiang et al. (2020)  applied a novel Knowledge-Bridge Graph Network (KBGN) in modeling the relations among the visual dialogue cross-modal information in fine granularity.  Wang et al. (2020a)  proposed a novel Knowledge-driven Multimodal Graph Convolutional Network (KMGCN) to model semantic representations for fake news detection. However, the KMGCN extracted visual words as visual information and did not make full use of the global information of the image.  Khademi (2020)  introduced a new neural network architecture, a Multimodal Neural Graph Memory Network (MN-GMN), for VQA, which model constructed a visual graph network based on the bounding-boxes, which produced overlapping parts that might provide redundant information. For the image-text dataset, we found that certain words often appear in a text post simultaneously, and different objects or scenes within an image have specific co-occurrences that indicate certain sentiments. We explicitly model these global characteristics of the dataset through the use of a multichannel GNN. 

 Proposed Model Fig.  2  illustrates the overall architecture of our proposed MGNNS model for multimodal sentiment detection that consists of three modules: the encoding module, the Multi-GNN module, and the multimodal interaction module. We first encode text and image input into hidden representations. Then, we introduce GNN from different channels to learn multiple modal representations. In this paper, the channels are the Text-GNN (TG) module, the Image-GCN-Scene (IGS) module, and the Image-GCN-Object (IGO) module. Finally, we realize the in-depth interactions between different modalities by multimodal multi-head attention. 

 Problem Formalization The goal of our model is to identify which sentiment is expressed by an image-text post. Given a set of multimodal posts from social media, P = {(T 1 , V 1 ), ..., (T N , V N )}, where T i is the text modality and V i is the corresponding visual information, N represents the number of posts. We need to learn the model f : P ? L to classify each post (T i , V i ) into the predefined categories L i . For polarity classifica- tion, L i ? {P ositive, N eutral, N egative}; for emotion classification, L i ? {Angry, Bored, Calm, F ear, Happy, Love, Sad }. 

 Encoding For text modality, we first encode words by GloVe  (Pennington et al., 2014)  to obtain the embedding vector and then obtain the text memory bank, M t , by BiGRU  (Cho et al., 2014) : M t = f BiGRU (Embedding(T )), M t ? R L t ?2d t , (1) where T is a text sequence, L t is the maximum length of a padded text sequence, and d t is the dimension of hidden units in the BiGRU. For image modality, we extract image features from both the object and scene views to capture sufficient information. We believe that there are interdependencies between different objects or scenes in an image. To explicitly model this co-occurrence, we first extract objects O = {o 1 , ..., o l o } by YOLOv3  (Farhadi and Redmon, 2018) , and extract scenes S = {s 1 , ..., s l s } by VGG-Place  (Zhou et al., 2017) . Finally, we obtain the object and scene memory banks with the pretrained ResNet  (He et al., 2016) . Thus, if an input image V has a 448?448 resolution and is split into 14?14 = 196 visual blocks of the same size, then each block is represented by a 2,048-dimensional vector. Note that we delete the stopwords during data preprocessing so that the words "a" and "the" do not have connections. M x = f x ResN et (V ), M x ? R L x ?d x , (2) 

 Multi-channel Graph Neural Networks In this subsection, we present our proposed Multi-GNN module. As Fig.  2  shows, this module consists of the TG channel (middle), the IGO channel (right), and the IGS channel (left). Text GNN: As shown in the middle of Fig.  2 , motivated by  (Huang et al., 2019) , we learn text representation through the Text Level GNN. For text with l t words T = {w 1 , ..., w k , ..., w l t }, where the k th word, w k , is initialized by glove embedding r t k ? R d , d = 300. We build the graph of the textbased vocabulary of the training dataset, which is defined as follows: N t = {w k |k ? [1, l t ]}. (3) We build edges between w k and w j when the number of co-occurrences of two words is not less than 2. E t = {e t k,j |w k ? [w 1 , w l t ]; w j ? [w k?ws , w k+ws ]}, (4 ) where N t and E t are the set of nodes and edges of the text graph, respectively. The word representations in N t and the edge weights in E t are taken from global shared matrices built based on vocabulary and the edge set of the dataset, respectively. That is, the representations of the same nodes and weights of the edges are shared globally. e t k,j is initialized by point-wise mutual information (PMI)  (Wang et al., 2020a)  and is learned in the training process. ws is the hyperparameter sliding window size, which indicates how many adjacent nodes are connected to each word in the text graph. Then, we update the node representation based on its original representations and neighboring nodes by the message passing mechanism (MPM)  (Gilmer et al., 2017) , which is defined as follows: A t k = max j?N ws k e t kj r t k , (5) r t k = ?r t k + (1 ? ?)A t k , (6) where A t k ? R d is the aggregated information from neighboring nodes from node k?ws to k+ws, and max is the reduction function. ? is the trainable variable that indicates how much original information of the node should be kept, and r t k ? R d is the updated representation of node k. Finally, we can calculate the new representation of text T as follows: T = l t k=1 r t k (7) Image GCN: In this module, we explicitly model interdependence within l x scenes or objects by IGX, as shown on the left and right sides of Fig.  2 , respectively. The graph of the image is defined as follows: N x = {x p |p ? [1, l x ]}, (8) where N x ? R C x is the set of nodes of IGX; x or X ? {Object, Scene}, C x = 80 when x = Object, and C x = 365 when x = Scene. To build the edges of IGX, we first build the global shared co-occurrence matrix-based dataset: E x = {e x p,q |p ? [1, l x ] , q ? [1, l x ]}, (9) where E x ? R C x ?C x is the co-occurrence matrix; edge weight e x p,q indicates the co-occurrence times of x p and x q in the dataset. Then, we calculate the conditional probability for node p as follows: P x p,q = e x p,q /N x p , when q = p (10) where N x p denotes the occurrence times of x p in the dataset. Note that P x p,q = P x q,p . As mentioned by  (Chen et al., 2019) , the simple correlation above may suffer several drawbacks. We further build the binary co-occurrence matrix: B x p,q = 1, if P x p,q ? ? 0, if P x p,q ? ? , ( 11 ) where ? is the hyperparameter used to filter noisy edges. It is obvious that the role of the central node is different from that of neighboring nodes, so we need to further calculate the weight of the edge: R x p,q = 1 ? ?, if p = q ?/ C x q=1 B x p,q , if p = q , ( 12 ) where R x ? R C x ?C x is the weighted cooccurrence matrix, and hyperparameter ? indicates the importance of neighboring nodes. Finally, we input node N x and edge R x of the image into the graph convolutional network. Like in (Kipf and Welling, 2016), every layer can be calculated as follows: H x L+1 = h( R x H x L W x L ), (13) where H x L ? R C x ?d x , H x L+1 ? R C x ?d x , W x L ? R d x ?d x , and R x ? R C x ?C x is the normalized representation of R x ; h(?) is a non-linear operation. When L = 1, H x 1 is the word-embedding vector of N x . The MMAI module illustrates the process of multimodal interaction from four channels, X ? {Object, Scene}. We take the interaction process between text and image scene channels as an example to demonstrate this for convenience. The dotted arrows are the outputs of the other two channels after the interactions. By stacking multiple GCN layers, we can explicitly learn and model the complex interdependence of the nodes. Then, we obtain the image representation with objects or scenes dependencies: I x = M axP ooling(M x )(H x L+1 ) T , I x ? R C x . (14) But, we cannot capture the relationship between nodes and sentiments. Therefore, we learn the sentiment-awareness image representation through multi-head attention  (Vaswani et al., 2017) . Att = sof tmax( QK T ? d k )V, (15) EI x = M H(Q, K, V ) = Concat(head 1 , ..., head H )W O where head h = Att(QW Q h , KW K h , V W V h ), ( 16 ) where M H(?) is multi-head attention; W Q h ? R d?d k , W K h ? R d model ?d k , W V h ? R d model ?dv , and W O ? R Hdv?d ; and H = 5, d model = 300, d k = d v = 60. Q ? R l s ?d is a sentiment embedding matrix built based on the label set l s = 3 for polarity classification and l s = 7 for emotion classification; K = V = I x W I , W I ? R C x ?d model , K, V ? R d model . 

 Multimodal Interaction Motivated by the Transformer  (Vaswani et al., 2017)  prototype, we design a Multimodal Multihead Attention Interaction (MMAI) module that can effectively learn the interaction between text modality and image modality by multiple channels, as shown in Fig.  3 . We employ the MMAI to obtain the Text guided Image-X representations and Image-X guided Text representations, X ? {Object, Scene}. For the Text-guided Image-X attention, O T gX N +1 = LN (M H(Q = H T gX N , K = V = M x ) + H T gX N ), (17) H T gX N +1 = LN (F F N (O T gX N +1 ) + O T gX N +1 ), (18) where LN (?) is layer normalization, and F F N (?) is the feed-forward network. When N = 1, H T gX 1 = T , as in Eq. 7. For the Image-X-guided Text attention, O XgT N +1 = LN (M H(Q = H XgT N , K = V = M t ) + H XgT N ), (19) H XgT N +1 = LN (F F N (O XgT N +1 ) + O XgT N +1 ), ( 20 ) when N = 1, H XgT 1 = EI x , as in Eq. 16. For M H, H = 4, d model = 512, d k = d v = 128. The fused multimodal representation is as follows: R m = [H T gO N ? H T gS N ? H OgT N ? H SgT N ], where ? is a concatenation operation. 

 Sentiment Detection Finally, we feed the above fused representation, R m , into the top fully connected layer and employ the softmax function for sentiment detection. L m = sof tmax(w s R m + b s ), L m ? R l s , (21) where w s and b s are the parameters of the fully connected layer. 

 Experiments We conduct experiments on three multimodal sentiment datasets from social media platforms, MVSA-Single, MVSA-Multiple  (Niu et al., 2016) , and TumEmo  (Yang et al., 2020) , and compare our MGNNS model with a number of unimodal and multimodal approaches.  amount of image-text data crawled from Tumblr 2 . The statistics of these datasets are given in Appendix A; and for a fair comparison, we adopt the same data preprocessing method as that of Yang  (Yang et al., 2020) . The corresponding details are shown in Appendix B. 

 Experimental Setup 

 Parameter 

 MVSA- * TumEmo Learning rate 4e ? 5 5e ? 5 ws 4 5 Object-? 0.4 0.4 Scene-? 0.3 0.5 ? 0.2 0.2 L x 2 2 N T gX 1 1 N XgT 1 1 Table  2 : Parameter settings of the different datasets. We adopt the cross-entropy loss function and Adam optimizer. In the process of extracting objects and scenes, we reserve the objects with the probability greater than 0.5 and the top-5 scenes, respectively. The other parameters are listed in Table 2, * ? {Single, M ultiple}. We use Accuracy (Acc) and F1-score (F1) as evaluation metrics. All models are implemented with PyTorch. 

 Baselines We compare our model with multimodal sentiment models with the same modalities and the unimodal baseline models. Unimodal Baselines: For text modality, CNN  (Kim, 2014)  and Bi-LSTM  (Zhou et al., 2016)  are well-known models for text classification tasks, and BiACNN  (Lai et al., 2015)  incorporates the CNN and BiLSTM models with an attention mechanism for text sentiment analysis. TGNN  (Huang et al., 2019)  is a text-level graph neural network for text classification. For image modality, OSDA (Yang    (Xu and Mao, 2017 ) is a deep semantic network with attention for multimodal sentiment analysis. Co-Mem  (Xu et al., 2018 ) is a co-memory network for iteratively modeling the interactions between multiple modalities. MVAN  (Yang et al., 2020)  is a multi-view attentional network that utilizes a memory network for multimodal emotion analysis. This model achieves state-of-the-art performance on image-text multimodal sentiment classification tasks. 

 Experimental Results and Analysis The experimental results of the baseline methods and our model are shown in Table  3 , where MGNNS denotes that our model is based on multichannel graph neural networks 3 . We can make the following observations. First,  3  The source codes are available for use at https:// github.com/YangXiaocui1215/MGNNS. our model (MGNNS) is competitive with the other strong baseline models on the three datasets. Note that the data distribution of MVSA- * is extremely unbalanced. Thus, we reproduce the MVAN model with ACC and Weighted-F1 metrics instead of the Micro-F1 metric used in the original paper, which is more realistic. Second, the multimodal sentiment analysis models perform better than most of the unimodal sentiment analysis models on all three datasets. Moreover, the segmental indictors are difficult to capture for images owing to the low information density, and the sentiment analysis on the image modality achieves the worst results. Finally, the TGNN unimodal model outperforms the HSAN multimodal model, indicating that the GNN has excellent performance in sentiment analysis. 

 Ablation Experiments We conduct ablation experiments on the MGNNS model to demonstrate the effectiveness of different modules. Table  4  shows that the whole MGNNS model achieves the best performance among all models. To show the performance of the Multi-GNN module, we replace the Text-GNN with the CNN, as well as the Image-GCN with the pretrained ResNet. The removal of the MMAI module (w/o MMAI) and Multi-GNN module (w/o MGNN) adversely affect the model results, which indicates that these modules are useful for multimodal sentiment analysis. By replacing the MMAI module with the CoAtt  (Lu et al., 2016)   (+CoAtt), the model performance is found to be slightly worse than that of the MGNNS module. This further illustrates the importance of multimodal interactions and the superiority of the MMAI module. When one of the object views (w/o Object) or scene views (w/o Scene) is removed, the performance of the model declines, which indicates that both views of the image are effective for multimodal sentiment analysis. 

 Transferability Experiment In the Multi-GNN module, we build multiple graphs for different modalities based on the dataset. For different datasets, the graphs built by the unimodal model are different. However, can graph capture from one dataset (e.g., MVSA-Single) have positive effects on other datasets (e.g., TumEmo)? In this subsection, we will verify the transferability of the model through experiments. As Table  5  shows, the following conclusions can be drawn: (i) Regardless of the modality, such as text or image, compared to introducing the graph constructed based on own dataset, the experimental results calculated based on graphs transferred from other datasets are worse. This is mainly because each dataset has unique global characteristics, the experimental results based on transferred graphs are slightly worse. (ii) However, due to the commonality of datasets when expressing the same emotions, the results of the transferred models are not completely worse. For example, the same scenes and objects can appear in different images in different datasets simultaneously for image modalities. Therefore, graphs from different datasets have transferability and can be used for other datasets. (iii) For different datasets, the experimental results of "X2Y-Text" are worse than those of "X2Y-Image". That is, the text graph has worse transferability. The reason for this may be that text graphs with various nodes are created based on the vocabulary of different datasets. Two situations in the transferred text graph will seriously affect the results: fewer nodes will lose information, and more nodes will provide redundant information. (iv) When the dataset gap is relatively wide, the transferability of text graphs is worse. For example, from the larger datasets transfer to the smallest dataset, including T2S-Text and M2S-Text, experimental results show a drop of 2.45% and 2.69%, respectively; from the smaller datasets transfer to the most largest dataset, including S2T-Text and M2T-Text, experimental results show a significant drop of 4.81% and 4.09%, respectively. 

 Hyperparameter Settings Hyperparameter ws: To obtain adequate information from neighboring nodes in the TGNN, we conduct experiments under different settings for hyperparameter ws in Eq. 4, the related results of which are shown in Fig.  4 . The best ws selection varies among different datasets since the average text length of TumEmo is longer compared to other data. The TGNN cannot obtain sufficient information from neighboring nodes with ws values that are too small, while larger values may degrade the performance due to the redundant information provided by neighboring nodes.    For "Z" modality, "X2Y-Z" represents that the graph that is built based on the "X" dataset is transfered to the "Y" dataset, where Z ? {Text, Image}, X ? {MVSA-Single, MVSA-Multiple, TumEmo}, and Y ? {MVSA-Single, MVSA-Multiple, TumEmo}. For example, "M2S-Text" represents that the text graph that is built based on the MVSA-Multiple dataset is transferred to the MVSA-Single dataset. Hyperparameter ?: We vary the values of hyperparameter ? in Eq. 11 for the binary cooccurrence matrix from different views, the results of which are shown in Fig.  5 . We find that the best ? value is different for different views in different datasets. For MVSA- * , the smaller ? value can reserve more edges to capture more information since the scene co-occurrence matrix is sparser than that in the object view. For TumEmo with a large amount of data, preserving the top-5 scenes produces many noise edges, so the value of scene-? is greater than that of MVSA- * .   Hyperparameter ?: As Fig.  6  shows, the model receives the best performance for the three datasets when ? is 0.2. When ? is smaller, the neighboring nodes do not receive enough attention; in contrast, their own information is not fully uti-lized.   

 Conclusions This paper proposes a novel model, MGNNS, that is built based on the global characteristics of the dataset for multimodal sentiment detection tasks. As far as we know, this is the first application of graph neural networks in image-text multimodal sentiment analysis. The experimental results on publicly available datasets demonstrated that our proposed model is competitive with strong baseline models. In future work, we plan to construct a model that adopts the advantages of the GNN and pretrained models such as BERT, VisualBERT, and etc. We want to design a reasonable algorithm to characterize the quality of the objects and scenes selected from the image and further improve the representation ability of the model. Figure 1: Multimodal posts with global characteristics. Two posts express the user's positive sentiment from multimodal data that has global characteristics, including the "have a fun/nice day" phrase, the ocean scene, and the beach scene. 

 Figure 2 : 2 Figure 2: The framework of the proposed Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for multimodal sentiment detection. The channels are Text-GNN (TG) for text modality, Image-GCN-Scene (IGS) for image scene modality, and Image-GCN-Object (IGO) for image object modality.Note that we delete the stopwords during data preprocessing so that the words "a" and "the" do not have connections. 

 Figure3: The MMAI module illustrates the process of multimodal interaction from four channels, X ? {Object, Scene}. We take the interaction process between text and image scene channels as an example to demonstrate this for convenience. The dotted arrows are the outputs of the other two channels after the interactions. 

 and MVSA-Multiple are two different scale image-text sentiment datasets crawled from Twitter 1 . 

 Figure 4 : 4 Figure 4: Acc comparisons with different values of ws. MS is MVSA-Single, MM is MVSA-Multiple, and T is TumEmo. 

 Figure 5 : 5 Figure 5: Acc comparisons with different ? values. 

 Figure 6 : 6 Figure 6: Acc comparisons with different ? values. 

 Table 1 : 1 Statistics of the different datasets. TumEmo is a multimodal weak- supervision emotion dataset containing a large 1 https://twitter.com 

 Table 3 : 3 Experiment results of Acc and F1 on three datasets. ? represents the reproductive operation. et al., 2020) is an image sentiment analysis model based on multiple views. Note that the SGN, OGN, and DuIG are variants of our model and rely only on image modality. SGN and OGN are the image graph convolutional neural networks based on scenes and objects for image sentiment analysis, respectively. DuIG is the image graph convolutional neural network with dual views, e.g., Object and Scene. Muiltimodal Baselines: HSAN (Xu, 2017) is a hierarchical semantic attentional network based on image captions for multimodal sentiment analysis. MDSN 

 Table 4 : 4 module Ablation experiment results. Datasets Model Acc F1 w/o MGNN 0.7010 0.6847 w/o MMAI 0.7108 0.6879 MVSA-Single +CoAtt w/o Scene 0.7255 0.6986 0.7304 0.6988 w/o Object 0.7034 0.6900 MGNNS 0.7377 0.7270 w/o MGNN 0.7019 0.6752 w/o MMAI 0.7128 0.6792 MVSA-Multiple +CoAtt w/o Scene 0.7210 0.6849 0.7170 0.6797 w/o Object 0.7110 0.6848 MGNNS 0.7249 0.6934 w/o MGNN 0.6553 0.6547 w/o MMAI 0.6370 0.6347 TumEmo +CoAtt w/o Scene 0.6624 0.6606 0.6618 0.6593 w/o Object 0.6592 0.6584 MGNNS 0.6672 0.6669 

 Table 5 : 5 Transferability experiment results of Acc and F1 on different datasets. S, M and T denote MVSA-Single, MVSA-Multiple, and TumEmo, respectively. 

			 http://tumblr.com
