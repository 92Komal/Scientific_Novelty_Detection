title
Video Question Answering with Phrases via Semantic Roles

abstract
Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fillin-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language-bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public.

Introduction Given a video, Video Question Answering (VidQA) requires a model to provide an answer to a video related question. However, existing works treat VidQA as an N-way (N ?1k) classification task across a fixed set of phrases. Models trained under such formulations are strictly restricted in their recall rate, generalize poorly, and have severe limitations for end-user applications. In this work, we introduce Video Question Answering with Phrases (VidQAP) which treats VidQA as a fill-in-the-phrase task. Instead of a question, the input to VidQAP consists of a query expression with a query-token. Then, given a video, VidQAP requires replacing query-token with a sequence of generated words. To generate a query, we leverage video descriptions and assign semantic roles to each phrase in these descriptions. Replacing a particular semantic-role with a query token produces a query-answer pair. We illustrate this in Figure  1  (details in Section 3.1). ) or masking-out strategy (Q3). However, such QA has a theoretical recall upper bound when the correct answer is not among the choice list. In comparison, we propose a free-form text generation task which do not suffer such limitation (Q4-Q7) While free-form answer generation is highly desirable, evaluating them is non-trivial due to two main challenges. First, existing language generation metrics like BLEU  (Papineni et al., 2002)  or BERTScore  (Zhang* et al., 2020)  operate on sentences rather than phrases. When applied to short phrases, in the absence of context, even close matches like "A person" and "The man" would be falsely rejected due to no n-gram overlap or poor contextual embeddings. Second, natural language questions often have strong language priors making it difficult to ascertain if the model retrieved information from the video. To propose a reasonable evaluation metric, we With this key insight, we propose relative scoring: using the description as reference sentence, we compute the metrics once by replacing the querytoken once with the predicted answer phrase and once with an empty-string. The model's performance is measured by the relative improvement from the predicted answer compared to the empty string. In particular, substituting the answer phrase in the query expression allows the computing the contextual embeddings required by BERTScore. To mitigate the language-bias issue, we emulate the procedure proposed by  (Goyal et al., 2017)  where for a given question, another image (or video in our case) is retrieved which has a different answer for the same question. To retrieve such a video, we use a contrastive sampling method  (Sadhu et al., 2020)  over the dataset by comparing only the lemmatized nouns and verbs within the semantic roles (SRLs). We then propose contrastive scoring to combine the scores of the two answer phrases obtained from the contrastive samples (details on evaluation in Section 3.2). To investigate VidQAP, we extend three visionlanguage models namely, Bottom-Up-Top-Down  (Anderson et al., 2018) , VOGNet  (Sadhu et al., 2020)  and a Multi-Modal Transformer by replacing their classification heads with a Transformer  (Vaswani et al., 2017)  based language decoder. To facilitate research on VidQAP we construct two datasets ActivityNet-SRL-QA (ASRL-QA) and Charades-SRL-QA and provide a thorough analysis of extended models to serve as a benchmark for future research (details on model framework in Section 3.3 and dataset creation in Section 4.1). Our experiments validate the merits of mov-ing away from N-way classification, and further show even among sequence generation models there exists a large disparity in performance across semantic-roles (i.e. queries for some roles can be answered very easily compared to other roles). Moreover, certain roles hardly benefit from visionlanguage models suggesting room for improvement. Finally, we investigate the effects of relative scoring and contrastive scoring for VidQAP with respect to BertScore. Our contributions in this work are two-fold: (i) we introduce VidQAP and propose a systematic evaluation protocol to leverage state-of-art language generation metrics and reduce language bias (ii) we provide extensive analysis and contribute a benchmark on two datasets evaluated using three vision-language models. Our code and dataset are publicly available. 1 

 Related Works Question Answering in Images has received extensive attention in part due to its end-user applicability. Key to its success has been the availability of large-scale curated datasets like VQA v2.0  (Goyal et al., 2017)  for visual question answering and GQA (Hudson and Manning, 2019) for relational reasoning. To address the strong language priors, the datasets are balanced by retrieving images which given the same question lead to a different answer. However, these procedures cannot be extended for VidQA since crowd-sourcing to retrieve videos is expensive and there exists no scene-graph annotations for videos. In this work, we perform the retrieval using lemmatized nouns and verbs of the semantic roles labels obtained from video descriptions to balance the dataset. Question Answering in Videos: has garnered less attention compared to ImageQA. A major bottleneck is that there is no principled approach to curating a VidQA dataset which reflects the diversity observed in ImageQA datasets. For instance, naively crowd-sourcing video datasets leads to questions about color, number which is same as ImageQA datasets and doesn't reflect any spatialtemporal structure. To address this issue, TGIF-QA  (Jang et al., 2017)  and ActivityNet-QA  (Yu et al., 2019)  use a question-template to enforce questions requiring spatio-temporal reasoning but forgo the question diversity. An orthogonal approach is to combine VidQA with movie scripts  (Tapaswi et al., 2016)  or subtitles . However, this severely restricts the domain of videos. Moreover, recent works have noted that language-only baselines often outperform vision-language baselines  (Jasani et al., 2019; Zellers et al., 2019) . A separate line of related research has focused on scene-aware dialogue  (Alamri et al., 2019) . Instead of a single annotator providing both questions and answers, the annotation procedure follows a two-player game setup with one player asking a question and the other player answering with the roles switching after each turn. However, the evaluation method utilizes recall metrics which require the set of phrases to be known apriori. As a result, it doesn't strictly measure the performance of free-form generation but rather how well the ground-truth answer is ranked given a competing set of phrases which is analogous to multiple-choice questions. Automatic Question Generation: Due to the above limitations, the dominant approach to create large-scale VidQA dataset has been automatic question generation from existing video descriptions which can be easily crowd-sourced. Our proposed formulation of using SRLs to generate queryexpressions falls in this category. Prior works include VideoQA  (Zeng et al., 2017) , MSR-VTT-QA and MSVD-QA  (Xu et al., 2017)  which use a rule based question generator  (Heilman and Smith, 2009)  to convert descriptions to questions and Movie-Fill-in-the-Blanks  (Maharaj et al., 2017)  which mask outs at most one word which could be a noun, adjective or verb in a sentence. In comparison, our method poses VidQAP as fill-in-blanks but with phrases, explicitly asks questions about actions, and the answer phrases are not constrained to a fixed set. As a result of this increased space of phrases, methods on existing datasets cannot be directly applied to VidQAP. To enable further research, we contribute two datasets ASRL-QA and Charades-SRL-QA. In Table  1  we compare these with existing VidQA datasets. SRL in Vision: has been explored in the context of human object interaction  (Gupta and Malik, 2015) , situation recognition  (Yatskar et al., 2016) , and multi-media extraction  (Li et al., 2020) . Most related to ours is the usage of SRLs for grounding  (Silberer and Pinkal, 2018)  in images and videos  (Sadhu et al., 2020) . Our work builds on  (Sadhu et al., 2020)  in using SRLs on video descriptions, however, our focus is not on grounding. Instead, we use SRLs primarily as a query generation tool and use the argument as a question directive. 

 Design Considerations for VidQAP The VidQAP task is conceptually simple: given a video and a query expression with a query-token, a model should output an answer phrase that best replaces the query-token. This leads to three main design considerations: (i) How to generate a queryexpression from existing resources (Section 3.1) (ii) How to evaluate the answer phrases returned by a model (Section 3.2) (iii) What modeling framework choices enable VidQAP (Section 3.3). 

 Using SRLs to Generate Queries for VidQAP We first briefly describe semantic-role labels (SRLs) 2 . Then we detail how SRLs are used to create VidQAP queries. Query Generation Using SRLs: Semantic Role Labels (SRLs) provide a high-level label to entities extracted from a sentence in the form of who (ARG0), did what (V) to whom (ARG1)  (Strubell et al., 2018) . Other roles such as to whom / using what (ARG2) and where (LOC) are also common. As a pre-processing step, we assign SRLs to video descriptions using a state-of-art SRL labeler  (Shi and Lin, 2019) . A particular description could consist of multiple verbs, in which case, we consider each verb and its associated SRLs independently. For a particular semantic-role, we substitute the corresponding phrase with a query token to generate the query expression. The replaced phrase is the corresponding answer. Using this method we  are able to generate multiple queries from a single description. An added merit of using SRLs is that query phrases are centered around "verb-phrases" which are highly relevant to the video content. Generating queries using every SRL is not beneficial as some SRLs are more concerned with phrasing of the language rather than the video. For instance, in the phrase "Players are running around on the field", if we mask out the word "around" (DIR), it can be answered without looking at the video. To address the above issue, we confine our description phrases to a fixed set of semantic-roles namely: ARG0, ARG1, V, ARG2, ARGM-LOC. Only those phrases which belong to the above set of SRLs may appear in the query-expression or as an answer phrase. We further remove phrases which have only two arguments as these are too ambiguous to fill. Figure  2  illustrates these steps. While using a slot for each slot could potentially limit the vocabulary used in each slot (for instance, the vocabulary set for <Q?ARG1> could be limited to a small number of objects), empirically we don't find this to be the case (see Appendix A.3 for detailed statistics). As a result, VidQAP is no simpler than VidQA task. We also remark that generating queries need not be strictly limited to masking out a single SRL and one could easily mask multiple SRLs in the same description. However, we find two problems: first, for many cases, the output of masking multiple SRLs becomes exceedingly similar to video description task; second, using contrastive scoring (described in Section 3.2) for multiple SRLs be- 

 Query Expression: A person <Q-V> exercise equipment. 

 Reference (Ground Truth): A person moves exercise equipment. 

 Hypothesis (Prediction): A person lifts exercise equipment. 

 Baseline (Empty String): A person exercise equipment. "moves" is the ground-truth answer and "lifts" is a model's prediction. Relative Metric compares the relative improvement from using the model's prediction as compared to an empty string. ! = B(Ref, Base), ? = B(Ref, Hyp), " = B(Ref, Ref) B r (Ref, Hyp) = ? !! " !! Relative Metric Score A person holding <Q-ARG1> in their hands Answer: a dog Answer: a hair dryer comes considerably more involved. As a result, in this work, we focus on using a single SRL and keep the generalization to include multiple SRL queries for future work. 

 Evaluating Answer Phrases A key challenge in VidQAP is the lack of any standard protocol to evaluate free-form generated phrases. A simple way is to adopt metrics like BLEU  (Papineni et al., 2002) , ROUGE  (Lin, 2004) , METEOR  (Banerjee and Lavie, 2005) , and CIDER  which are already used for captioning in images and videos. However, these metrics suffer from limited generalization: BLEU, ROUGE, and CIDER require exact n-gram matches. While this is fine for captioning where longer phrases average out errors, answers phrases are typically much smaller than a complete sentence. This leads to many near-correct answers receiving very low scores. This issue is resolved to a certain extent for captioning by learned metrics like BERTScore  (Zhang* et al., 2020)  which utilize contextual embeddings obtained from large pretrained models like BERT  (Devlin et al., 2019)  and RoBerta . However, answer phrases are usually short and don't provide meaningful contextual embeddings. In the extreme case when the answer is a single word, for instance when the query is about a Verb, these embeddings turn out to be very noisy leading to large number of false-positives. Relative Scoring: To enable usage of contextual embeddings, we propose evaluating the relative improvement of the generated answer phrase compared to the ground-truth phrase. We denote the input query expression as Q, the ground-truth answer is A gt ,and the predicted answer is A pred . Let Q(X) denote Q with the question tokens replaced by X. Then for a given metric B, we compute the relative metric B r as (see Figure  3   for illustration) Ref =Q(A gt ), Hyp=Q(A pred ), Base=Q("") B r (A gt , A pred ) = B(Ref,Hyp)?B(Ref,Base) B(Ref,Ref )?B(Ref,Base) (1) Note that B(Ref, Ref )=1 for BLEU, METEOR, ROUGE, BERTScore but not for CIDEr. The empty-string baseline in Eqn 1 could be replaced with predictions from any model trained for this task. In this work, we restrict to only emptystring baseline due to two desirable properties: its computational simplicity and it being agnostic to models and datasets. We further observe that Eqn 1 is very similar to the re-scaling proposed in BERTScore. However, in BertScore re-scaling aims at making the score more readable and doesn't change the relative ranking of the hypothesis. In our case, Eqn 1 plays two roles: first, it allows computing the contextual embeddings because the answers are now embedded inside a complete phrase, second while the ranking is not affected for a particular query, the score would be different across queries and hence affect the overall relative metric. Contrastive Scoring: Visual Question Answering suffers from heavy language priors, and as a result, it is often difficult to attribute whether the image or video played a role in the success. For images,  (Goyal et al., 2017)  resolved this by balancing the dataset where they crowd-sourced the task of collecting an image that has a different answer for the same question. However, such a crowdsourcing method is difficult to extend to videos since searching for videos requires a much longer time. This is further complicated by accepting answer phrases compared to single word. We simulate the balancing process using the contrastive sampling method used in  (Sadhu et al., 2020) . Specifically, for a given video-query-answer (V 1 , Q 1 , A 1 ) tuple we retrieve another video-queryanswer (V 2 , Q 2 , A 2 ) tuple which share the same semantic role structure as well as lemmatized noun and verbs for the question, but a different lemmatized noun for the answer. At test time, the model evaluates the question separately, but the evaluation function requires both answers to be correct. Since our answers comprise of phrases, the notion of correctness is not absolute (unlike say accuracy metric). Thus, we put a threshold t below which the answer is deemed incorrect. Mathematically, let S i =B r (A gt i , A pred i ) be the relative score for sample i, and we are given sample j is a contrastive example for sample i. Then the contrastive score (CS i ) for sample i at a threshold T CS would be CS i = max(S i 1[S j > T CS * B(Ref j , Ref j )], 0) (2) Here 1[] is the indicator variable which is 1 if the expression within brackets is True, otherwise 0. The max operator ensures the scores don't become negative. For our experiments, we use T CS =0 which requires that the answer for the contrastive sample should be better than an empty string. We further use the contrastive samples to compute a consistency metric. For sample i, the consistency Cons i for a threshold T cons is given by Cons i = 1[(S i ? T cons ) * (S j ? T cons ) > 0] (3) As such, Consistency requires the model to be either correct or incorrect for both the original and the contrastive sample. Combined Metric at a Glance: Given metric B, for a given sample i and contrastive sample j 1. Compute relative metric (Eqn 1) for i, j 2. Compute contrastive score (Eqn 2) 

 Optionally compute Consistency (Eqn 3) We use the prefix "R-" such as in R-B to denote both relative scoring and contrastive scoring is being computed. We report Consistency for BertScore with T cons =0.1 We note that, by construction, the relative scoring (Eqn 1) is positively correlated with human judgment, as the closer, the hypothesis is to the reference, the higher would the score be. The contrastive scoring is a metric used to prevent the model from guessing the correct answer by exploiting language biases and instead use the video to give a suitable prediction. Since humans don't have the ability to exploit such biases, it is difficult to relate to human evaluation.  

 Model Framework Models for VidQAP require a language encoder to encode the question, a visual encoder to extract video features, a multi-modal module to jointly learn over vision-language space and a decoder to generate a sequence of words. Inputs include query expression {w} L i=1 (L is number of words), video segment features for F 1 frames and optionally k RCNN features for F 2 frames. In either case, frames are sampled uniformly from the video segment time-span. While the models differ in their encoding scheme, our language decoder model (Transformer based) used to generate the output answer phrase is kept same across all models with QAP suffix. Lang-QAP: is a language-only (video-blind) model using only the query input. It uses Transformer based encoder to encode the query into q ? R L?d . The decoder subsequently uses the last layer output of the encoder (Figure5-(a)). BUTD-QAP: Bottom-up-Top-Down (Anderson et al., 2018) is a popular approach for image question answering as well as captioning. It first computes attention between the question and the RCNN visual features to generate an attended visual feature, which is then used with the question to produce an output answer. Here, we replace the RCNN features with the segment features (v ? R F 1 ?d ). We can also include RCNN features by projecting them to same dimension as segment features and then concatenate them along the frame-axis (v ? R (F 1 +F 2 * k)?d ). For language features, we use the [CLS] token representation from the last layer of the language encoder used in Lang-QAP. The output using the language and visual features is ( m ? R d ) passed to the decoder (Figure  5(b) ). VOG-QAP: VOGNet (Sadhu et al., 2020) has been proposed for grounding objects in videos given a natural language query. Following the architecture, we first derive phrase encoding which corresponds to a single SRL i.e. q ? R S?d (S is number of semantic roles). These phrase features are concatenated with the visual features (same as those used in BUTD-QAP (i.e. v)) to get multimodal features m[l, i]=[v i ||q l ] and then reshaped to get m ? R S * F ?d . These multi-modal features are subsequently passed to decoder to generate the output sequence (Figure  5 (c) ). MTX-QAP: Recently, transformer models pretrained on large-scale paired image-text data have become popular. Even in the absence of pretraining, such architectures can achieve competitive performance . In the context of videos, ActBert (Zhu and Yang, 2020) has been proposed. We create a similar architecture to ActBert but we replace their proposed Tangled-Transformer with a vanilla Transformer 3 . Specifically, we jointly encode the language and visual features in a single transformer and feed the output to the decoder (Figure  5 (d) ). LangCL and MTxCL: Apart from QAP models, we also consider their phrase classification counterparts where the decoder is replaced with a N-way classifier (two-layered MLP in our case) across a fixed set of phrases. For our experiments, we used N =1k phrases for LangCL and N ?{1k, 10k} for MTxCL. 

 Experiments We briefly discuss the dataset creation process (Section 4.1), followed by experimental setup (Section 4.2). We then summarize our results (Section 4.3) and discuss key-findings. We provide implementation details, qualitative visualizations of our dataset, metrics and trained models in the appendix. 

 Dataset Creation We create two datasets ASRL-QA and Charades-SRL-QA derived from ActivityNet-Captions  (Krishna et al., 2017)  and Charades  (Sigurdsson et al., 2016)  respectively. There are three key steps to create QA datasets from descriptions: (i) assign semantic-roles to the descriptions (ii) perform co-reference resolution so that the questions are self-contained (iii) obtain lemmatized nouns and verbs to perform contrastive sampling. For semantic-role labeling, we use  (Shi and Lin, 2019) . For co-reference resolution, we use the co-reference resolution model provided by allennlp library  (Gardner et al., 2017)  which uses the model by  (Lee et al., 2017)  but replaces the GloVe  (Pennington et al., 2014)  embeddings with SpanBERT embeddings    4  . Since Charades primarily involves videos with a single person, we discard questions involving ARG0. We limit to using a single description per video to avoid repetitive questions. We re-use the same train split for both datasets. For ASRL-QA, test set of ActivityNet is not public and Charades only has a test set but no official validation set. Thus, we split the existing validation set by video names and create the validation and test sets. For both validation and test splits, we remove those questions for which no contrastive sample was found as it indicates data-biases. 

 Experimental Setup Dataset Statistics: ASRL-QA has 35.7k videos and 162k queries split into train, validation and test sets with 30.3k, 2.7k, 2.7k videos and 147k, 7.5k, 7.5k queries. We observe that the size of validation and test sets are proportionately smaller compared to their respective train sets. This is because only queries with corresponding contrastive sample are included while no such filtering is done for the train set (?95k queries in train set have a contrastive pair). Charades-SRL-QA contains 9.4k videos and 71.7k queries split across train, validation and test 4 https://demo.allennlp.org/coreference-resolution sets with 7.7k, 0.8k, 0.8k videos and 59.3k, 6.1k, 6.2k queries. Despite its smaller size, the size of validation, test sets of Charades-SRL-QA is comparable to ASRL-QA as Charades is curated with the goal of diversifying subject, verb, object tuples. Supplementary material provides further details on the dataset statistics and visualizations. Evaluation Metrics: As discussed in Section 3.2, we report the combined metric (i.e. metrics prefixed with "R-") for the commonly used generation metrics: BLEU, METEOR, ROUGE, CIDEr and BertScore (implementations from  (Chen et al., 2015; Zhang* et al., 2020) ). For BLEU, we report the sentence level BLEU-2. All reported results are test set results using the model which performs best on validation set. 

 Results and Discussions Table  2  compares performance of the proposed VidQAP models with N-way classification baselines (denoted with suffix "CL") on ASRL-QA and Charades-SRL-QA. Comparing Metrics: It is evident that compared to other metrics, R-BertScore shows a higher relative improvement. This is because BertScore allows soft-matches by utilizing contextual embeddings obtained from a pre-trained BERT  (Devlin et al., 2019)  or Roberta  model. Comparison Across Datasets: We find that performance on both datasets follow very similar trends across all metrics. Charades-SRL-QA has slightly higher scores compared to ASRL-QA likely because it has lesser data variations (Charades is mostly confined indoor videos) suggesting findings on either dataset would transfer. Comparison within N-way Classification: We notice that when 1k fixed set of phrases are used classification models show very limited performance. Allowing 10k phrases gives a significant improvement in performance on Charades-SRL-QA (12 points on R-BS) however this doesn't translate to ASRL-QA. This is because ASRL-QA contains many more probable phrases (29K compared to 8K) in their respective training sets. We also notice that increasing the number of phrases vocabulary coincides with decreasing consistency. Comparing Free-from Answer Generation (QAP) with N-way Classification (CL): We investigate the advantages of using a decoder network to generate phrases compared to an N-way classification over a fixed set of phrases (denoted  with the suffix "CL" and number of phrases used in parenthesis). Table  2  shows that both Lang-QAP and MTX-QAP outperform their classification counterparts, namely Lang-CL and MTX-CL on both datasets. This implies the free-form generation are not limited to simply generating the most frequently appearing phrases in the training set, thereby showing its effectiveness. ASRL-QA Charades-SRL-QA R-BS Cons R-B@2 R-R R-M R-C R-BS Cons R-B@2 R-R R-M R-C LangCL ( Comparison Across Models: We find that multi-modal models outperform language-only baseline. However, the improvement over language baseline is small. The reason for the small gap is elucidate in Table  3  where we report R-BertScore for every considered SRL. We find a large disparity in performance depending on the SRL. Most strikingly, multi-modal models perform worse than language-only model on ARG0 and V. For ARG0, the strong performance of the Lang-QAP arises because most of the time the agent who causes an action is a human. Therefore answer phrases having simply "A man" or "A woman" or "A person" leads to reasonable performance. This additionally suggests that grounding "who" is performing the action remains non-trivial. The more surprising result is the strong performance of Lang-QAP on V which is consistent across both datasets despite using contrastive sampling. There are two likely causes. First, the distinction between verbs is not as strict as object nouns, i.e. even similar verbs are classified as a separate verb diminishing the returns of contrastive sampling. For instance, "jumping" and "hoping" have different lemma and thus considered distinct verbs but R-BS would treat them as similar even if the specific action would be classified "jumping" rather than "hoping". Second, SRLs such as ARG1 confines the set of possible verbs. For instance, if the object is "glass", only limited verbs such as "drink", "hold" are probable. On the remaining arguments namely ARG1, ARG2, and LOC, multi-modal models show a steady improvement over language-only baseline ranging from 1?10%. However, the performance in absolute terms remains very low. As such, our proposed task VidQAP remains extremely challenging for current multi-modal models. Evaluation Metric Scores: In Table  4  we record the BertScore computation in three parts: directly computing over the answer phrases, performing relative scoring, finally performing contrastive scoring with different thresholds. We observe that for V, naive computation leads to absurdly high scores. This is because verbs consist of a single word which means the embeddings are not contextual. This is remedied by relative scoring and is further controlled by combining with contrastive sampling. Further note that relative scoring operates differently based on the SRLs. For instance, it increases the score for ARG0 and ARG1 where the answers more often paraphrased the ground-truth questions while for ARG2 and LOC, it decreases the score due to incorrect matches. While contrastive scoring is aimed at reducing language-only bias and as such should always reduce the relative score, we observe increased score in ARG2 for both Lang-QAP and MTX-QAP. This is caused by the max function which restricts the lower-limit to be 0. Effect of Region Boxes: As noted earlier, the visual features can also include region features extracted from an object detector like  FasterRCNN (Ren et al., 2015) . In Table  5  we record the effect of including regional features. In particular, we use the GT5 setting used in  (Sadhu et al., 2020)  where 5 region proposals are used from 10 frames uniformly sampled from the video segment. Interestingly, MTX-QAP under-performs than both BUTD-QAP and VOG-QAP on ARG0. A possible reason is that the transformer is unable to effectively reason over both language and vision over such a large range of inputs. 

 Conclusion In this work, we introduce Video Question Answering with Phrases (VidQAP) where we pose VidQA as a fill-in-the-phrase task. Given a video and query expression, a model needs to compose a sequence of words to answer. We then propose a method to leverage semantic roles from video descriptions to generate query expressions and outline a robust evaluation protocol. This involves computing the relative improvement of the prediction answer compared to an empty string followed by a contrastive sampling stage which reduces language-only biases. We then contribute two datasets ASRL-QA and Charades-SRL-QA to facilitate further on VidQAP and benchmark them with three visionlanguage models extended for our proposed task. 
