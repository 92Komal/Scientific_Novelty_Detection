title
NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis

abstract
While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand, which are likely more relevant than the most frequent words. The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach.

Introduction Pre-training of neural networks with a language model (LM) or masked language model (MLM) objective on large amounts of non-domain-specific texts has given a significant boost of performance in almost all natural language processing tasks. While 16GB of texts were shown to BERT  (Devlin et al., 2019)  and ten times more to RoBERTa  (Liu et al., 2019)  during pre-training, the further training of these models with the MLM objective on domainspecific texts before fine-tuning to the target task was shown to further improve the final results  (Sun et al., 2019; Gururangan et al., 2020) . This technique is called the domain or task adaptation, depending on the degree of similarity of the data for adaptation to the target dataset. While initial pretraining is extremely expensive, it does not depend on the final task and can be performed only once. However, domain or task adaptation is done for each domain or task individually and is still quite resource-demanding, requiring hundreds of thousands of training steps or several GPU days, unlike final fine-tuning, which can often be done in a few GPU hours  (Sun et al., 2019) . In this work, we propose a method for more efficient MLM adaptation. We have noticed that the standard MLM spends most of the training time on learning to restore the most frequent words like determiners or auxiliary verbs hidden (masked) from its input. While such training examples may be useful for learning English grammar, their domination during the adaptation phase seems to be wasteful for many final tasks. Since the final task and the dataset are already known in this phase, we propose to undersample such examples in favor of examples with targets related to the final task. This relatedness is estimated using a Naive Bayes classifier. Hence, we call our modified objective Naive Bayes Masked Language Model (NB-MLM). We hypothesize that hiding from the model and asking it to restore mostly features that are important for the final task will likely result in faster adaptation. Additionally, the absence of simple features and the requirement to restore them may teach the model to exploit more sophisticated and implicit features relevant to the final task. We evaluate the proposed method on two datasets for sentiment analysis. It is one of the most popular tasks in natural language processing  (Feldman, 2013)  and an excellent playground for the comparison of adaptation methods due to the large amount of labeled and unlabeled user reviews of different products available. In particular, we consider the task of classifying the binary sentiment polarity of a given review. Our experiments show that the NB-MLM objective can significantly reduce adaptation time while achieving the same final performance or help to improve performance given the same amount of time for adaptation. 1 

 Related Work Pre-training Transformer networks with the MLM objective is proposed in  (Devlin et al., 2019)  for the BERT model and is shown to outperform the more traditional LM objective, though the similar task of predicting a word from its left and right context was used with different architectures earlier  (Mikolov et al., 2013; Melamud et al., 2016) . RoBERTa enhances BERT by pre-training longer on ten times larger corpora, getting rid of the next sentence prediction (NSP) task during pre-training, and selecting different target words to be masked and predicted in each epoch (dynamic masking). Various approaches to further pre-training of BERT on domain or task-specific data are compared in  (Sun et al., 2019) , while Gururangan et al. (  2020 ) carry out a similar investigation with RoBERTa. They try various options of data sources for adaptation: texts only from the target dataset (called task adaptation or within-task pre-training), larger datasets from the same domain (called domain adaptation or in-domain pretraining), and datasets from different domains (called cross-domain pre-training). They find the task adaptation, which is a computationally cheapest option, to be a surprisingly good solution. In their experiments, it often outperforms the domain adaptation and is only marginally worse than com-bining both methods. However, due to the large amount of data used in domain adaptation,  Gururangan et al. (2020)  train the MLM only for one or very few epochs. We find that our method leveraging large data more efficiently makes the domain adaptation comparable to the task adaptation, and their combination is significantly better than each of them. Our idea of employing Naive Bayes weights is inspired by the NB-SVM model  (Wang and Manning, 2012; Mesnil et al., 2014) , which scales bag-ofngrams vectors with Naive Bayes classifier weights and then trains linear SVM or logistic regression classifiers on them. It proved to be a very strong baseline, often outperforming both linear and more sophisticated models from that time. 

 MLM Objectives for Adaptation Uniform MLM. For each input example, the standard MLM objective, as proposed by  Devlin et al. (2019) , samples 15% of the input positions (subwords) for calculating the loss. The positions are sampled from the uniform distribution without replacement: P (pos) ? 1. Then 80% of the tokens on sampled positions are masked (replaced with a [MASK] token), 10% are replaced with some random tokens from the uniform distribution over the vocabulary, and 10% are left intact. 

 NB-MLM. As an alternative, we propose sampling 15% of positions from a non-uniform distribution that gives higher probabilities to positions that contain subwords with high feature importance f i(w): P (pos) ? exp(f i(w pos )/T ), where the temperature T is the hyperparameter allowing to balance between uniform sampling and determin- istic selection of positions that contain the most important features. For binary classification, the feature importance is estimated using the Naive Bayes classifier weights as follows: f i(w) = |logP (w|1) ? logP (w|0)|. Thus, those features that are much more probable in one class than in another receive the highest scores. Similar to the method proposed by  Wang and Manning (2012) , the probabilities are estimated by the multinomial Naive Bayes model with additive smoothing (alpha = 0.1). Additionally, the scores are set to zero for those features that occurred in less than m examples to avoid the overrepresentation of unreliable features. As an example, Figure  1  shows the words that the model is most frequently asked to predict during the task adaptation on the IMDB movie reviews dataset (T = 0.1, m = 5 for NB-MLM). Evidently, NB-MLM learns to predict words relevant to sentiment analysis more often than the standard MLM. Along with the uniform and NB-based distributions, during the preliminary experiments, we tried other options, which are described and compared in Appendix D. However, only NB-MLM outperformed the uniform baseline. 

 Experiments and Results During the preliminary experiments described in Appendix A, we found that our method helps for both BERT and RoBERTa models. However, the latter model achieved significantly better performance. Therefore, we describe the results for RoBERTa in the rest of the paper. For domain adaptation (denoted as DAPT), we employed the Amazon Reviews dataset  (McAuley et al., 2015)  with duplicates removed. We removed reviews shorter than 500 characters and split the rest into the training and validation sets of 21M and 10K reviews correspondingly. The valida- tion set was used to calculate perplexity during MLM training. For task adaptation (denoted as all-TAPT), we used all texts (without labels) from the target dataset, i.e. IMDB  (Maas et al., 2011)  or Yelp  (Zhang et al., 2015)  2 . For IMDB, we employed the split of  Gururangan et al. (2020)  to make the results of our experiments directly comparable with their results. We used the binary classification version of Yelp  (Zhang et al., 2015) . For validation, we randomly selected 5K positive and 5K negative examples. For domain and task adaptation, we used the batch size of 1024, while classifiers were fine-tuned with the batch size of 32. Based on our preliminary experiments, we set the learning rate of 2e-4 for the domain adaptation, 1e-4 for the task adaptation, and 1e-5 for final fine-tuning. Following  Gururangan et al. (2020) , we performed domain adaptation for one epoch on the Amazon dataset (20K steps, 38h on one V100 GPU) and task adapta- Figure  2  shows how the final classification accuracy improves during the task and domain adaptation. Our NB-MLM model significantly helps for domain adaptation on IMDB. For task adaptation, the difference is much smaller and fits into two standard deviations. Still, on average, the NB-MLM seems to provide a consistent improvement throughout the adaptation. For Yelp, the improvements from NB-MLM are also small but consistent. Table 1 compares our models and the previously published results on the test sets. In order to apply McNemar's test for statistical significance, instead of averaging across all runs of each model with different random seeds, we have to take predictions of a particular run. Thus, for each of our models, we selected the run with the median performance (for the even number of runs, the one just above the median) and reported its performance in the table. For IMDB, the domain adaptation with NB-MLM obtains results similar to the Uniform MLM in 5x fewer training steps and data (only 20% of the data is seen during the first 4K steps). When trained for one epoch, it improves the results by more than 0.3%, which is also statistically significant. For task adaptation, the NB-MLM gives a much smaller improvement. Similarly to the results of  Gururangan et al. (2020) , in our experiments, the task adaptation with the Uniform MLM outperforms the domain adaptation that employs much more data by almost 0.5%. We suppose that this is due to the small proportion of relevant examples sampled by the Uniform MLM, which require many repetitions to learn from. Probably, training domain adaptation for hundreds of epochs, similarly to task adaptation, can fix this problem, but this is not feasible for large datasets and moderate computation resources. More efficient domain adaptation with NB-MLM, which focuses on targets that are likely relevant for the final task, reduces this difference to 0.2%. Finally, using the domain adaptation followed by the task adaptation results in the best final performance. In this scenario, NB-MLM gives 0.2% improvement for short adaptation and 0.1% for long adaptation. For Yelp, the metrics are higher, and the differences are smaller but still consistent. 

 Conclusion We proposed a technique for the more efficient domain and task adaptation of MLMs. It is especially helpful for leveraging large data efficiently during the domain adaptation, resulting in significantly shorter adaptation time or better performance.  To verify our hypothesis, in the preliminary experiments, we tried improving the results of the ITPT (withIn-Task Pre-Training) method  (Sun et al., 2019) . Since no code for this paper was available at that time, we implemented this method using the Transformers library  (Wolf et al., 2020)    4  , which closely followed the details and hyperparameters specified in the paper but adopted recommendations from more recent models by not using NSP prediction and exploiting dynamic masking. Since no official development set is available for the IMDB dataset  (Maas et al., 2011)  and the split is not specified in the paper, for early stopping during classifier fine-tuning and NB-MLM hyperparameters selection, we employed our own split 5 . Note that this split was used only for preliminary experiments; later, we switched to the split of  Gururangan et al. (2020) . For adaptation, we used the whole dataset, excluding half of the development set to measure the validation perplexity. 
