title
XOR QA: Cross-lingual Open-Retrieval Question Answering

abstract
Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity-where languages have few reference articles-and information asymmetry-where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TYDI QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving crosslingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington. edu/xorqa/.

Introduction Information-seeking questions-questions from people who are actually looking for an answerhave been increasingly studied in question answering (QA) research. Fulfilling these information needs has led the research community to look further for answers: beyond paragraphs and articles toward performing open retrieval 1 on large-scale document collections ). Yet  1  We use open retrieval-instead of open domain-to refer to models that can access answer context from large document collections. We avoid using open domain due to its double meaning as "covering topics from many domains." 

 ?[Japanese] (What did Ron Paul major in during undergraduate?) ? (Biology) Paul went to Gettysburg College, where he was a member of the Lambda Chi Alpha fraternity. He graduated with a B.S. degree in Biology in 1957. the bulk of this work has been exclusively on English. In this paper, we bring together for the first time information-seeking questions, open-retrieval QA, and multilingual QA to create a multilingual open-retrieval QA dataset that enables crosslingual answer retrieval. While multilingual open QA systems would benefit the many speakers of non-English languages, there are several pitfalls in designing such a dataset. First, a multilingual QA dataset should include questions from non-English native speakers to represent real-world applications. Questions in most recent multilingual QA datasets  (Lewis et al., 2020; Artetxe et al., 2020; Longpre et al., 2020)  are translated from English, which leads to English-centric questions such as questions about American sports, cultures and politics. Second, it is important to support retrieving answers in languages other than the original language due to information scarcity of low-resource languages  (Miniwatts Marketing Group, 2011) . Moreover, questions strongly related to entities from other cultures are less likely to have answer content in the questioner's language due to cultural bias (information asymmetry, Callahan and Herring, 2011). For example, Fig.  1  shows that the Japanese Wikipedia article of an American politician, Ron Paul, does not have information about his college degree perhaps because Japanese Wikipedia editors are less interested in specific educational backgrounds of American politicians. In this paper, we introduce the task of crosslingual open-retrieval question answering (XOR QA) which aims at answering multilingual questions from non-English native speakers given multilingual resources. To support research in this area, we construct a dataset (called XOR-TYDI QA) of 40k annotated questions and answers across 7 typologically diverse languages. Questions in our dataset are inherited from TYDI QA  (Clark et al., 2020) , which are written by native speakers and are originally unanswerable due to the information scarcity or asymmetry issues. XOR-TYDI QA is the first large-scale cross-lingual open-retrieval QA dataset that consists of information-seeking questions from native speakers and multilingual reference documents. XOR-TYDI QA is constructed with an annotation pipeline that allows for cross-lingual retrieval from large-scale Wikipedia corpora ( ?2). Unanswerable questions in TYDI QA are first translated into English by professional translators. Then, annotators find answers to translated queries given English Wikipedia using our new model-in-theloop annotation framework that reduces annotation errors. Finally, answers are verified and translated back to the target languages. Building on the dataset, we introduce three new tasks in the order of increasing complexity ( ?3). In XOR-RETRIEVE, a system retrieves English Wikipedia paragraphs with sufficient information to answer the question posed in the target language. XOR-ENGLISHSPAN takes one step further and finds a minimal answer span from the retrieved English paragraphs. Finally, XOR-FULL expects a system to generate an answer end to end in the target language by consulting both English and the target language's Wikipedia. XOR-FULL is our ultimate goal, and the first two tasks enable researchers to diagnose where their models fail and develop under less coding efforts and resources. We provide baselines that extend state-of-theart open-retrieval QA systems  Karpukhin et al., 2020)  to our multilingual retrieval setting. Our best baseline achieves an average of 18.7 F1 points on XOR-FULL. This result indicates that XOR-TYDI QA poses unique challenges to tackle toward building a real-world open-retrieval QA system for diverse languages. We expect that our dataset opens up new challenges to make progress in multilingual representation learning. 

 The XOR-TYDI QA Dataset Our XOR-TYDI QA dataset comprises questions inherited from TYDI QA  (Clark et al., 2020)  and answers augmented with our annotation process across 7 typologically diverse languages. We focus on cross-lingual retrieval from English Wikipedia because in our preliminary investigation we were able to find answers to a majority of the questions from resource-rich English Wikipedia, and native speakers with much annotation experience were readily available via crowdsourcing in English. 

 XOR-TYDI QA Collection Our annotation pipeline proceeds with four steps: 1) collection of questions from TYDI QA without a same-language answer which require cross-lingual reference to answer ( ?2.1.1); 2) question translation from a target language to the pivot language of English where the missing information may exist ( ?2.1.2); 3) answer retrieval in the pivot language given a set of candidate documents ( ?2.1.3); 4) answer verification and translation from the pivot language back to the original language ( ?2.1.4). Fig.  2  shows an overview of the pipeline. 

 Question Selection Our questions are collected from unanswerable questions in TYDI QA. A question is unanswerable in TYDI QA if an annotator cannot select a passage answer (a paragraph in the article that contains an answer). We randomly sample 5,000 questions without any passage answer annotations (unanswerable questions) from the TYDI QA training data, and split them into training (4,500) and development (500) sets. We use the development data from TYDI QA as our test data, since the TYDI QA's original test data is not publicly available. 2 We choose 7 languages with varying amounts of Wikipedia data out of the 10 non-English languages based on the cost and availability  (Q L , A L ) In-language Cross-lingual 1. Question Selection (Q L , A L ) 

 XOR-TyDiQA Figure  2 : Overview of the annotation process for XOR-TYDI QA. of translators: 3 Arabic, Bengali, Finnish, Japanese, Korean, Russian and Telugu. 

 Question Translation We use a professional translation service, Gengo, 4 to translate all collected questions into English. Since named entities are crucial for QA, we instruct translators to carefully translate them by searching for common English translations from English Wikipedia or other external sources. We perform manual quality assessment by native speakers on 50 translation samples, finding that more than 95% are correct. Note that while these translations are a part of the annotation procedure (due to the inherently cross-lingual nature of this task), they are not provided to models during evaluation. 

 Answer Retrieval in English We use Amazon Mechanical Turk to retrieve answers to translated English questions given English Wikipedia articles. Annotators are instructed to select passage answers (gold paragraphs) and minimal answer spans as in  Clark et al. (2020) . To annotate answers to information-seeking queries, previous work first identifies relevant Wikipedia articles using Google Search, and then annotators attempt to find answers there.  Asai and Choi (2020)  show that in information-seeking QA datasets many questions were annotated as "unanswerable" due to two systematic errors: retrieval error where the search engine failed to retrieve a relevant article and answer annotation error where the annotator overlooks answer content. Importantly, these two types of annotation errors present a tradeoff: if we retrieve many articles, retrieval errors will be reduced at the expense of answer annotation errors because annotators have to find answer context among many candidate articles. Collaborative model-in-the-loop. To find a middle ground in the tradeoff, we introduce a collaborative model-in-the-loop framework that uses Google Search and a state-of-the-art paragraph ranker. We first run Google Search to retrieve as many as top 10 Wikipedia articles, resulting in 387 paragraphs per question on average. We score them with Path Retriever  and present the five highest scoring paragraphs. Annotators are asked to skim these five paragraphs first; if they cannot find any answer content, they are asked to read the rest of the paragraphs, where the Wikipedia section headings guide their reading. To incentivize workers to find answers beyond the pre-selected ones, we carefully communicate with workers and send additional rewards to annotators who actively read the rest of the paragraphs and find answers for questions that other annotators may overlook. We found about 70% of the answers from the 5 paragraphs and 30% from the rest of the paragraphs in the top 10 articles. This means that while our paragraph ranking was effective, the annotators did not fully rely on it, thereby mitigating the influence of the passage ranking model on the dataset. See Appendix ?B.1 for annotation interface details. Quality control for QA annotation. We first recruit MTurkers with a high approval rate (? 96%) located in English-speaking countries, and all workers first annotate the same qualification batch. We assess the quality of those submissions and select high-quality annotators.  

 Answer Verification and Translation We verify the annotated answers and translate those answers back to the target languages (cross-lingual data). Finally, we mix the annotated cross-lingual data with the same-language data from TYDI QA to reflect the actual question distributions from native speakers (in-language data). Answer verification. We trained undergraduate students who are native English speakers to verify the annotated paragraphs and short answers. Only 8% of the answers were marked as incorrect through the verification phase and were later corrected by our pool of high-quality crowdworkers who yielded less than 1% annotation error. Answer translation. We again use Gengo to translate answers from English back to the original languages. We give translators further instructions to normalize answers such that they are consistent with answers in TYDI QA. For example, some languages use their own unique set of numerals rather than Arabic numerals to represent numeric answers (e.g., Bengali numerals, Chinese numerals in Japanese text). The details of the answer translation process are described in Appendix ?B.4. Note that because of the cost of answer translations, we conduct this answer translation process for evaluation sets only. 

 2.2 The XOR-TYDI QA Corpus Dataset statistics. 5 Table  1  shows the percentages of the questions annotated with short answers in the original TYDI QA and our XOR-TYDI QA, and Table  2  shows statistics of XOR-TYDI QA. As seen in Table  1 , cross-lingual retrieval significantly increases the answer coverage in all languages by up to 40% (Bengali), and consequently we found answers for more than 50% of the origi- Table  2 : Dataset size of the XOR-TYDI QA corpus (answered data). Cross-lingual data comes from our reannotated questions that did not originally have samelanguage answers in TYDI QA. In-language data are taken directly from answerable questions in TYDI QA. nal information-seeking questions in 6 out of the 7 languages.  6  This result confirms the effectiveness of searching multilingual document collections to improve the answer coverage. Detailed statistics of the numbers of long answers, short answers, and unanswered questions are in Appendix ?B.5. We also release the 30k manually translated questions for our training set, which could be used to train multilingual models or machine translation models. Qualitative examples. Table  3  illustrates that finding relevant articles from multilingual document collections is important to answer questions asked by users with diverse linguistic and cultural backgrounds. The first question is unanswerable in Korean Wikipedia, but there is a clear description about who was the prime minister of France at the time in English Wikipedia. The second example shows English Wikipedia sometimes contains rich information about a target language-specific topic (e.g., economy in Krasnodar, a city in Russia). Those examples demonstrate the effectiveness of searching for answers in another language with more abundant knowledge sources. In the last question of Table  3 , on the other hand, only the Wikipedia of the target language can provide the answer. XOR QA allows for both retrieval paths. Comparison with other datasets. Table  4  compares XOR-TYDI QA and existing multilingual QA datasets. XOR-TYDI QA has three key properties that are distinct from these QA benchmarks. First, since all questions are inherited from TYDI QA, they are information-seeking questions written by L Original Question: Q L (Q en ) Passage Answer: P en or P L 

 Minimal Answer in English: A en Final Answer:  native speakers, and better reflect native speakers' interests and their own linguistic phenomena. This distinguishes XOR-TYDI QA from translationbased datasets such as MLQA  (Lewis et al., 2020)  and MKQA  (Longpre et al., 2020) . Second, our dataset requires cross-lingual retrieval unlike other multilingual datasets such as TYDI QA or XQuAD  (Artetxe et al., 2020) , which focus on samelanguage QA. Lastly, questions in XOR-TYDI QA require open retrieval from Wikipedia, whereas MLQA-R and XQuAD-R  (Roy et al., 2020)  limit the search space to matching each question with the predetermined 21k/31k sentences. A L Ko 1993? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? (Who 

 XOR QA Tasks and Baselines We introduce three new tasks (Fig.  3 ): XOR-RETRIEVE, XOR-ENGLISHSPAN, and XOR-FULL with our newly collected XOR-TYDI QA dataset and construct strong baselines for each task. XOR-FULL defines our goal of building a multilingual open-retrieval QA system that uses both cross-lingual and in-language questions from XOR-TYDI QA. To diagnose where models fail and to allow researchers to use the data with less coding effort or computational resource, we also introduce the first two intermediate tasks that only use the crosslingual data (Table  2 ). We denote the target language by L i . We also denote the English Wikipedia collection by W eng and the Wikipedia collection in each target language L i by W i . We experiment with baselines using black-box APIs as a reference, but we encourage the community to use white-box systems so that all experimental details can be understood. Nonetheless, we release the intermediate results from those external APIs to make our results reproducible. All of the white-box system results can be reproduced using our codebase. 

 XOR-RETRIEVE: Cross-lingual Paragraph Retrieval Task. Given a question in L i and English Wikipedia W eng , the task is to retrieve English paragraphs for the question. Finding evidence paragraphs from large-scale document collections like Wikipedia is a challenging task, especially when a query and documents are in different languages and systems cannot perform lexical matching. Evaluation. Different open-retrieval QA models use different units for retrieval. To make fair comparisons across various models, we measure the recall by computing the fraction of the questions for which the minimal answer is contained in the top n tokens selected. We evaluate with n = 2k, 5k: R@2kt and R@5kt (kilo-tokens).  Translate baselines. We first translate queries into English, and then paragraphs are retrieved in a monolingual way. For query translation, we train transformer machine translation (MT) models on publicly available corpora for easy replication. We also run Google's online machine translation service (GMT). This is not completely reproducible as these systems get constantly updated; Multilingual baselines. Alternatively, we can directly apply a multilingual pretrained model to retrieve paragraphs. We initialize and train a DPR encoder with multilingual BERT to enable multilingual document retrieval . 

 XOR-ENGLISHSPAN: L-to-English Open-Retrieval QA Task. Given a question in L i and English Wikipedia W eng , a system retrieves paragraphs from W eng and extracts an answer. This task is equivalent to existing open-retrieval QA tasks  (Chen et al., 2017) , except that the query is not in English. This task involves challenging cross-lingual retrieval and question answering on the L i query and English evidence paragraphs. Evaluation. We use Exact Match (EM) and F1 over the annotated answer's token set following prior work  (Rajpurkar et al., 2016) . Baselines. Our pipeline uses a machine reading model to find a minimal span that answers the question given paragraphs selected from the previous XOR-RETRIEVE step. In particular, for the translate baselines, we use the same approach as state-ofthe-art models  Karpukhin et al., 2020)  that jointly predicts a span and a relevance score of each paragraph to the question. For the multilingual baseline where queries are not automatically translated during evaluation, we build a reader model with multilingual BERT. 

 XOR-FULL: Round Trip Task. Given a question in target language L i and Wikipedia in both English and L i (W eng and W i ), a system is required to generate an answer in L i . In this task, a system does not know a priori in which language we can find information that the user is seeking. Note that the XOR-FULL evaluation data includes both cross-lingual and in-language data, while XOR-RETRIEVE and XOR-ENGLISHSPAN only use cross-lingual data during evaluation. Evaluation. Some answers in XOR-FULL are translated from English so the same spans may not exist in the target language's Wikipedia. For this reason, we use token-level BLEU scores  (Papineni et al., 2002)  over a ground-truth token set in addition to F1 and EM. The same tokenizer is applied to ground-truth and predicted answers to compute token-level F1 and BLEU. Baselines. Unlike the previous two tasks, evidence paragraphs can be found both in the target language and English, and a system has to output final answers based on the most plausible paragraphs. In this work, we introduce a simple multi-lingual baseline that first looks for answers in the target language and then English if no answers are found in the target language. Specifically, we apply monolingual retrieval (i.e., BM25, Google Custom Search) for W i and a multilingual machine reading model based on XLM-RoBERTa  (Conneau et al., 2020)  to find in-language answers in the target language (monolingual model; the bottom half of Fig.  3 ). If no answers are found by the monolingual model, we apply an XOR-ENGLISHSPAN baseline and translate English answers into the target language (the top half of Fig.  3 ). 

 Experiments and Analysis We present results from the baselines discussed above. We find that the three XOR QA tasks present challenges even for the strong models. 

 Experimental Setup For training, we first finetune the retrieval and machine reading models with the Natural Questions data  (Kwiatkowski et al., 2019)  and then further finetune on our XOR-TYDI QA data. For the BM25 retrieval baseline, we use ElasticSearch 8 to store and search documents using BM25 similarities. For both Path Retriever and DPR, we run the official open-source code. For our MT systems, we train base-sized (large for Russian) autoregressive transformers  (Vaswani et al., 2017)  on parallel corpora from OPUS  (Tiedemann and Nygaard, 2004) , Mul-tiUN  (Ziemski et al., 2016) , or WMT19  (Barrault et al., 2019) . All data are encoded into subwords by BPE  (Sennrich et al., 2016)  or SentencePiece  (Kudo and Richardson, 2018) . We use the fairseq library  (Ott et al., 2019) . Additional experimental details and full lists of hyperparameteres are available in Appendix ?C. We only evaluate questions having answers and do not give credit to predicting "no answers" as in prior open-retrieval work . For XOR-RETRIEVE and XOR-ENGLISHSPAN, we use cross-lingual data only and both cross-lingual and in-language data for XOR-FULL. 

 XOR-RETRIEVE Experiments Table  5  shows the R@5kt (as defined in ?3.1) for different retrieval and query translation systems. English translations of the questions used during the dataset collection as an upper bound of translate baselines. The best R@5kt macro-averaged over the 7 languages comes from running DPR on human translations: 72.1. Machine translation systems achieve averages of 67.2 (GMT) and 50.0 (our MT) again with DPR. The discrepancy between human and machine translation suggests that even state-of-the-art translation systems struggle to translate questions precisely enough to retrieve an evidence paragraph. Although the difference between GMT and our MT systems shows the effectiveness of industrial MT systems (large parallel data, model architecture, etc.), there remains a substantial performance gap from human translation. The translate baselines outperform the multilingual approach apart from Telugu, where our MT suffers from small parallel data (114k sentences), and as a result the multilingual approach performs better. BM25 substantially underperforms the other two models across the board. DPR generally achieves similar performance, if not better, compared to Path Retriever despite the fact that Path Retriever was used in our annotation ( ?2.1.3). As we found that these patterns persisted in all the following experiments, we will only report results with DPR. 

 XOR-ENGLISHSPAN Experiments Table  6  shows the performance of the baseline models in XOR-ENGLISHSPAN. The average macro F1 score with queries translated by human translators is 38.2, substantially higher than that of MT-based models: 32.9 and 20.5 F1 points for GMT and our MT respectively. This suggests that errors in automatic query translation affect later layers in the pipeline. The multilingual approach consistently underperforms translation-based methods, similarly to XOR-RETRIEVE. Telugu was an exception. The multilingual baseline significantly outperforms the translation-based approach with our MT system (14.4 vs. 3.6 F1 points). Query translation errors propagate to and directly impact downstream QA tasks in the languages with limited parallel data for MT training, and machine translation-based approaches may perform poorly. This encourages the research community to explore multilingual pretrained models to build a robust multilingual open-retrieval QA system for low-resource languages. Similar to the original TYDI QA dataset, the performance on XOR-ENGLISHSPAN varies across languages, which can be partially explained by the differing sets of questions  (Clark et al., 2020) . The best baseline achieves 39.5 in Arabic compared to 23.5 F1 points in Japanese, which may come from differences in question difficulty as well as how the models are trained for each language. 

 XOR-FULL Experiments Table  7  presents results on the XOR-FULL task. The first pipeline, which uses GMT, Google Search (GS), and DPR, yields the best average performance: 18.7 F1, 12.1 EM, and 16.8 BLEU points. This indicates that systems like GMT and GS, which are typically trained on large data, are effective. Yet, we encourage the community to experiment on top of open systems such that all experimental details can be fully reported and understood. Replacing GMT with our MT (second row) results in a large performance drop in Bengali (6.6 vs. 19.0 F1 points) and Telugu (1.7 vs. 13.6). Further replacing GS with BM25 retrieval in the target languages (third row) causes a large performance drop in all languages (e.g., 9.7 vs. 16.4 in Korean). Consistent with the previous tasks, the multilingual approach shown in the forth row underperforms the translation-based counterpart (15.7 vs. 18.7 F1 points on average). Similar baselines perform considerably better in prior open-retrieval QA datasets, such as MKQA (30 EM points,  Longpre et al., 2020)  and NQ questions (40 F1,  Karpukhin et al., 2020) . This gap illustrates the multidimensional challenge of XOR-TYDI QA. 

 Further Analysis Effects of translation performance on overall QA results. Table  8  compares the query translation BLEU scores and the final QA F1 performance of the translation-based baseline with three different MT systems in XOR-ENGLISHSPAN: GMT, Our MT, and Helsinki  (Tiedemann and Thottingal, 2020) . GMT significantly outperforms the other two baselines, demonstrating that its training setup may yield large improvements in these languages; similarly, in cases where additional parallel training data is not available, multilingual models may remain strong modeling tools. On the other hand, it is noteworthy that high BLEU scores do not always lead to better QA performance. In Bengali and Finnish, while Helsinki achieves a considerably better BLEU score than our MT (33.0 vs. 30.8 in Bengali and 29.8 vs. 27.4 in Finnish), our MT is 3.9 and 1.3 F1 points better in downstream XOR-ENGLISHSPAN, respectively. See Appendix ?D.3 for an example of translation errors resulting in QA errors. Those results suggest that the BLEU score is not always indicative of the downstream performance and that evaluating MT performance in the context of XOR QA would be important for improvements of multilingual QA systems. Single language Wikipedia ablations in XOR-FULL. To assess our models' ability to benefit from multilingual collections, we try restricting the retrieval target to single language Wikipedia: English W eng only or target language W i only. In W eng only, the best system, which applies GMT and DPR, underperforms the best pipeline that uses both W i,eng in all languages except for Finnish and Japanese. Similarly, the W i only setting generally underperforms the best W i,eng pipeline. These results illustrate the importance of searching multilingual collections. See Table  15  for the full results. 

 Related Work Multilingual QA Much recent effort has been made to create non-English QA datasets to over-555 Our XOR-TYDI QA is also closely related to QA@CLEF 2003-2008  (Magnini et al., 2003 (Magnini et al., , 2004 Vallin et al., 2005; Magnini et al., 2006; Giampiccolo et al., 2007; Forner et al., 2008) ; both QA@CLEF and XOR-TYDI QA attempt to develop and evaluate multilingual QA systems. Nevertheless, there are three crucial differences. First, our XOR-TYDI QA has a large number of questions that are required for training current state-of-the-art QA models like DPR, while QA@CLEF only has 200 evaluation questions for each language without training data  (Forner et al., 2010) . Secondly, the languages tested in QA@CLEF are all European languages, with the one exception of Indonesian; XOR-TYDI QA includes typologically diverse languages. Lastly, the task setup of QA@CLEF 2003-2008 is either monolingual-questions and documents are written in the same non-English language-or crosslingual-the source and target languages are prespecified  (Forner et al., 2010) . In XOR QA, questions are asked in a target language but a system does not know in which language it can find an answer in a non-parallel Wikipedia collection. Those differences from QA@CLEF tasks better simulate real-world scenarios and introduce new challenges that have yet to be extensively studied. Cross-lingual Information Retrieval Crosslingual Information Retrieval (CLIR) is the task of retrieving relevant documents when the document collection is in a different language from the query language (Hull and Grefenstette, 1996). The retrieval component in XOR QA is closely related to CLIR, but differs in several critical ways. First, since the end goal of XOR QA is QA, XOR QA queries always take question forms rather than search key words. Further, while CLIR typically retrieves documents from a single (low-resource) language  (Zhang et al., 2019) , XOR QA considers documents from both English and the query language. In many applications, we do not know a priori in which language we can find target information. Lastly, our document collection is orders of magnitude bigger than typical CLIR benchmarks  (Sasaki et al., 2018; Zhang et al., 2019) . 

 Conclusion We presented the task of XOR QA, in which a system retrieves and reads documents across languages to answer non-English information-seeking questions. We introduced a new large-scale XOR QA dataset, XOR-TYDI QA, with 40k newly annotated open-retrieval questions that cover seven typologically diverse languages. Our experiments showed that XOR-TYDI QA is a challenging benchmark that can benefit from further effort in both QA and multilinguality communities. 
