title
Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer

abstract
We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. Highquality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, nondeep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019)   dataset present a significant improvement on the F1 scores over the best existing CPC approaches.

Introduction Comparative Preference Classification (CPC) is a natural language processing (NLP) task that predicts whether a preference comparison exists between two entities in a sentence and, if so, which entity wins the game. For example, given the sentence: Python is better suited for data analysis than MATLAB due to the many available deep learning libraries, a decisive comparison exists between Python and MATLAB and comparatively Python is preferred over MATLAB in the context. The CPC task can profoundly impact various real-world application scenarios. Search engine users may query not only factual questions but also comparative ones to meet their specific information needs  (Gupta et al., 2017) . Recommendation * These authors contributed equally. providers can analyze product reviews with comparative statements to understand the advantages and disadvantages of the product comparing with similar ones. Several models have been proposed to solve this problem.  Panchenko et al. (2019)  first formalize the CPC problem, build and publish the CompSent-19 dataset, and experiment with numerous general machine learning models such as Support Vector Machine (SVM), representation-based classification, and XGBoost. However, these attempts consider CPC as a sentence classification while ignoring the semantics and the contexts of the entities  (Ma et al., 2020) . ED-GAT  (Ma et al., 2020)  marks the first entityaware CPC approach that captures long-distance syntactic relations between the entities of interest by applying graph attention networks (GAT) to dependency parsing graphs. However, we argue that the disadvantages of such an approach are clear. Firstly, ED-GAT replaces the entity names with "entityA" and "entityB" for simplicity and hence deprives their semantics. Secondly, ED-GAT has a deep architecture with ten stacking GAT layers to tackle the long-distance issue between compared entities. However, more GAT layers result in a heavier computational workload and reduced training stability. Thirdly, although the competing entities are typically connected via multiple hops of dependency relations, the unordered tokens along the connection path cannot capture either global or local high-quality semantic context features. In this work, we propose a Sentiment Analysis Enhanced COmparative classification Network (SAECON), a CPC approach that considers not only syntactic but also semantic features of the entities. The semantic features here refer to the context of the entities from which a sentiment analysis model can infer the sentiments toward the entities. Specifically, the encoded sentence and entities are fed into a dual-channel context fea-ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence  (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; . For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneficial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferred entity usually receives a positive sentiment while its rival gets a relatively negative one. These sentiments can be easily extracted by the strong ABSA models. The contrast between the sentiments assigned to the compared entities provides a vital clue for an accurate CPC. Secondly, the ABSA models are designed to target the sentiments toward phrases, which bypasses the complicated and noisy syntactic relation path. Thirdly, considering the scarcity of the data resource of CPC, the abundant annotated data of ABSA can provide sufficient supervision signal to improve the accuracy of CPC. There is one challenge that blocks the knowledge transfer of sentiment analysis from the ABSA data to the CPC task: domain shift. Existing ABSA datasets are centered around specific topics such as restaurants and laptops, while the CPC data has mixed topics  (Panchenko et al., 2019)  that are all distant from restaurants. In other words, sentences of ABSA and CPC datasets are drawn from different distributions, also known as domains. The difference in the distributions is referred to as a "domain shift"  (Ganin and Lempitsky, 2015; He et al., 2018)  and it is harmful to an accurate knowledge transfer. To mitigate the domain shift, we design a domain adaptive layer to remove the domainspecific feature such as topics and preserve the domain-invariant feature such as sentiments of the text so that the sentiment analyzer can smoothly transfer knowledge from sentiment analysis to comparative classification. 

 Related Work 

 Comparative Preference Classification CPC originates from the task of Comparative Sentence Identification (CSI)  (Jindal and Liu, 2006) . CSI aims to identify the comparative sentences.  Jindal and Liu (2006)  approach this problem by Class Sequential Mining (CSR) and a Naive Bayesian classifier. Building upon CSI,  Panchenko et al. (2019)  propose the task of CPC, release CompSent-19 dataset, and conduct experimental studies using traditional machine learning approaches such as SVM, representation-based classification, and XGBoost. However, they neglect the entities in the comparative context  (Panchenko et al., 2019) . ED-GAT  (Ma et al., 2020) , a more recent work, uses the dependency graph to better recognize longdistance comparisons and avoid falsely identifying unrelated comparison predicates. However, it fails to capture semantic information of the entities as they are replaced by "entityA" and "entityB". Furthermore, having multiple GAT layers severely increases training difficulty. 

 Aspect-Based Sentiment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification  (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014)  while neural network-based approaches employ variants of  RNN (Nguyen and Shirai, 2015; Aydin and G?ng?r, 2020) , LSTM  (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019) , GAT  GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020) . More recent works 1 widely use complex contextualized NLP models such as BERT  (Devlin et al., 2019) .  transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classification. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative sentiments. Therefore, the performance of a CPC model can be enhanced by the ABSA techniques.  i T j o I B H 8 = " > A A A C g 3 i c b V F N S 8 N A E N 1 E q z V + R T 1 6 C Z a K o N Q k F P U i V L 1 4 r G A / o A 1 h s 9 m 0 S z f Z s L s R S s g f 8 W d 5 8 9 + 4 a X P Q m o G d f f v m D T M 7 E 6 S U C G n b 3 5 q + t d 3 Y 2 W 3 u G f s H h 0 f H 5 s n p U L C M I z x A j D I + D q D A l C R 4 I I m k e J x y D O O A 4 l G w e C n j o w / M B W H J u 1 y m 2 I v h L C E R Q V A q y j c / 2 9 h 3 D O V c 5 Y z 2 N I Z y j i D N n w r 1 C B g N x T J W V z 4 v / J z e O L W s W 8 O K W q 2 o 1 x b G J j V T 6 b 7 Z s j v 2 y q z / w K l A C 1 T W 9 8 2 v a c h Q F u N E I g q F m D h 2 K r 0 c c k k Q x a p I J n A K 0 Q L O 8 E T B B M Z Y e P l q h o X V V k x o R Y y r k 0 h r x f 7 O y G E s y g a V s p y R 2 I y V Z F 1 s k s n o w c t J k m Y S J 2 h d K M q o J Z l V L s Q K C c d I 0 q U C E H G i e r X Q H H K I p F q b o Y b g b H 7 5 P x i 6 H e e u 4 7 5 1 W 7 3 n a h x N c A 4 u w B V w w D 3 o g V f Q B w O A N K B d a r e a r T f 0 a 9 3 V u 2 u p r l U 5 Z + C P 6 Y 8 / v t X C T A = = < / l a t e x i t > h g,1 < l a t e x i t s h a 1 _ b a s e 6 4 = " o G r H J + h v 1 k L K w R C c F B 7 + u g G 5 F 5 w = " > A A A C g 3 i c b V F N S 8 N A E N 1 E q z V + R T 1 6 C Z a K o N Q k F P U i V L 1 4 r G A / o A 1 h s 9 m 0 S z f Z s L s R S s g f 8 W d 5 8 9 + 4 a X P Q m o G d f f v m D T M 7 E 6 S U C G n b 3 5 q + t d 3 Y 2 W 3 u G f s H h 0 f H 5 s n p U L C M I z x A j D I + D q D A l C R 4 I I m k e J x y D O O A 4 l G w e C n j o w / M B W H J u 1 y m 2 I v h L C E R Q V A q y j c / 2 9 h 3 D O V c 5 Y z 2 N I Z y j i D N n w r 1 C B g N x T J W V z 4 v / J z e O L W s W 8 O K W q 2 o 1 x b G J j V T Q t 9 s 2 R 1 7 Z d Z / 4 F S g B S r r + + b X N G Q o i 3 E i E Y V C T B w 7 l V 4 O u S S I Y l U k E z i F a A F n e K J g A m M s v H w 1 w 8 J q K y a 0 I s b V S a S 1 Y n 9 n 5 D A W Z Y N K W c 5 I b M Z K s i 4 2 y W T 0 4 O U k S T O J E 7 Q u F G X U k s w q F 2 K F h G M k 6 V I B i D h R v V p o D j l E U q 3 N U E N w N r / 8 H w z d j n P X c d + 6 r d 5 z N Y 4 m O A c X 4 A o 4 4 B 7 0 w C v o g w F A G t A u t V v N 1 h v 6 t e 7 q 3 b V U 1 6 q c M / D H 9 M c f w F r C T Q = = < / l a t e x i t > h g,2 < l a t e x i t s h a 1 _ b a s e 6 4 = " 5 J l 0 k J   n q n 7 S 0 C e 7 8 k x d B p 1 F 3 z + u N p 7    n q n 7 S 0 C e 7 8 k x d B p 1 F 3 z + u N p 7     A J E d 3 m A m m m H 1 K 9 G p 2 O q p g = " > A A A C k X i c b V H J T s M w E H X C X q A E O H K J q C p x Q F V S V S w H U I E L E h c Q l F Z q q 8 h x J 6 1 V Z 5 E 9 Q V R R / 4 f v 4 c b f 4 J Q c o G Q k j 5 / f v L H H M 3 4 i u E L H + T L M l d W 1 9 Y 3 N r c r 2 z m 5 1 z 9 o / e F V x K h l 0 W C x i 2 f O p A s E j 6 C B H A b 1 E A g 1 9 A V 1 / e p f H u 2 8 g F Y + j F 5 w l M A z p O O I B Z x Q 1 5 V k f d f D c i n Z N 7 S r 1 Q U h x w q j I b u b 6 4 M d i p G a h 3 r L J 3 M v E q V v K N k t Y V a p V 5 d o S b p w r B w j v 6 A f Z 8 9 x z P K v m N J y F 2 f + B W 4 A a K e z R s z 4 H o 5 i l I U T I B F W q 7 z o J D j M q k T M B + v J U Q U L Z l I 6 h r 2 F E Q 1 D D b N H R u V 3 X z M g O Y q l X h P a C / Z 2 R 0 V D l 1 W p l 3 j G 1 H M v J s l g / x e B i m P E o S R E i 9 v N Q k A o b Y z s f j z 3 i E h i K m Q a U S c = " > A A A C l n i c b V H J T s M w E H X C H r Y C F y Q u E V U l D q h K K s R y Q S x C 5 V g E L U h t F T n u p L V w F t k T R B X l k / g Z b v w N T t s D t B n J 4 + c 3 b + T x s 5 8 I r t B x f g x z a X l l d W 1 9 w 9 r c 2 t 7 Z r e z t d 1 S c S g Z t F o t Y v v l U g e A R t J G j g L d E A g 1 9 A a / + + 3 1 R f / 0 A q X g c v e A 4 g X 5 I h x E P O K O o K a / y V Q P P t X R q 6 G T V e i H F E a M i u 8 3 1 w Y / F Q I 1 D v W W j 3 M v E q V v K N k p Y V a p V 5 d o S b j h V I n y i H 2 T P u e d Y T U 9 5 l a p T d y Z h L w J 3 B q p k F i 2 v 8 t 0 b x C w N I U I m q F J d 1 0 m w n 1 G J n A n I r V 6 q I K H s n Q 6 h q 2 F E Q 1 D 9 b G J r b t c 0 M 7 C D W O o V o T 1 h / 3 Z k N F T F y F p Z 2 K b m a w V Z V u u m G F z 2 M x 4 l K U L E p h c F q b A x t o s / s g d c A k M x 1 o A y y f W s N h t R S R P q z d 3 M j n V y R I 7 J C X H J B b k h j 6 R F 2 o Q Z B 8 a V c W f c m 4 f m t f l g N q d S 0 5 j 1 H J B / Y b Z + A W s N y U U = < / l a t e x i t > G s < l a t e x i t s h a 1 _ b a s e 6 4 = " Y Z z Z 4 4 2 x g e K g u s p v S z d s F 4 a y k / c = " > A A A C l n i c b V H J T s M w E H X C H r Y C F y Q u E V U l D q h K K s R y Q S x C 5 V g E L U h t F T n u p L V w F t k T R B X l k / g Z b v w N T t s D t B n J 4 + c 3 b + T x s 5 8 I r t B x f g x z a X l l d W 1 9 w 9 r c 2 t 7 Z r e z t d 1 S c S g Z t F o t Y v v l U g e A R t J G j g L d E A g 1 9 A a / + + 3 1 R f / 0 A q X g c v e A 4 g X 5 I h x E P O K O o K a / y V Q P P t X R q 6 G T V e i H F E a M i u 8 3 1 w Y / F Q I 1 D v W W j 3 M v E q V v K N k p Y V a p V 5 d o S b j h V I n y i H 2 T P u e d Y T U 9 5 l a p T d y Z h L w J 3 B q p k F i 2 v 8 t 0 b x C w N I U I m q F J d 1 0 m w n 1 G J n A n I r V 6 q I K H s n Q 6 h q 2 F E Q 1 D 9 b G J r b t c 0 M 7 C D W O o V o T 1 h / 3 Z k N F T F y F p Z 2 K b m a w V Z V u u m G F z 2 M x 4 l K U L E p h c F q b A x t o s / s g d c A k M x 1 o A y y f W s N h t R S R P q z d 3 M j n V y R I 7 J C X H J B b k h j 6 R F 2 o Q Z B 8 a V c W f c m 4 f m t f l g N q d S 0 5 j 1 H J B / Y b Z + A W s N y U U = < / l a t e x i t > G s < l a t e x i t s h a 1 _ b a s e 6 4 = " 5 J l 0 k J A J E d 3 m A m m m H 1 K 9 G p 2 O q p g = " > A A A C k X i c b V H J T s M w E H X C X q A E O H K J q C p x Q F V S V S w H U I E L E h c Q l F Z q q 8 h x J 6 1 V Z 5 E 9 Q V R R / 4 f v 4 c b f 4 J Q c o G Q k j 5 / f v L H H M 3 4 i u E L H + T L M l d W 1 9 Y 3 N r c r 2 z m 5 1 z 9 o / e F V x K h l 0 W C x i 2 f O p A s E j 6 C B H A b 1 E A g 1 9 A V 1 / e p f H u 2 8 g F Y + j F 5 w l M A z p O O I B Z x Q 1 5 V k f d f D c i n Z N 7 S r 1 Q U h x w q j I b u b 6 4 M d i p G a h 3 r L J 3 M v E q V v K N k t Y V a p V 5 d o S b p w r B w j v 6 A f Z 8 9 x z P K v m N J y F 2 f + B W 4 A a K e z R s z 4 H o 5 i l I U T I B F W q 7 z o J D j M q k T M B + v J U Q U L Z l I 6 h r 2 F E Q 1 D D b N H R u V 3 X z M g O Y q l X h P a C / Z 2 R 0 V D l 1 W p l 3 j G 1 H M v J s l g / x e B i m P E o S R E i 9 v N Q k A o b Y z s f j z 3 i E h i K m Q a U S a 5 r t d m E S s p Q D 7 G i m + A u f / k / e G 0 2 3 L N G 8 6 l V a 9 8 W 7 d g k R + S Y n B C X n J M 2 u S e P p E O Y U T V a x p V x b R 6 a l 2 b b L L S m U e Q c k j 9 m P n w D h / D H y w = = < / l a t e x i t > S 0 < l a t e x i t s h a 1 _ b a s e 6 4 = " 5 J l 0 k J A J E d 3 m A m m m H 1 K 9 G p 2 O q p g = " > A A A C k X i c b V H J T s M w E H X C X q A E O H K J q C p x Q F V S V S w H U I E L E h c Q l F Z q q 8 h x J 6 1 V Z 5 E 9 Q V R R / 4 f v 4 c b f 4 J Q c o G Q k j 5 / f v L H H M 3 4 i u E L H + T L M l d W 1 9 Y 3 N r c r 2 z m 5 1 z 9 o / e F V x K h l 0 W C x i 2 f O p A s E j 6 C B H A b 1 E A g 1 9 A V 1 / e p f H u 2 8 g F Y + j F 5 w l M A z p O O I B Z x Q 1 5 V k f d f D c i n Z N 7 S r 1 Q U h x w q j I b u b 6 4 M d i p G a h 3 r L J 3 M v E q V v K N k t Y V a p V 5 d o S b p w r B w j v 6 A f Z 8 9 x z P K v m N J y F 2 f + B W 4 A a K e z R s z 4 H o 5 i l I U T I B F W q 7 z o J D j M q k T M B + v J U Q U L Z l I 6 h r 2 F E Q 1 D D b N H R u V 3 X z M g O Y q l X h P a C / Z 2 R 0 V D l 1 W p l 3 j G 1 H M v J s l g / x e B i m P E o S R E i 9 v N Q k A o b Y z s f j z 3 i E h i K m Q a U S a 5 r t d m E S s p Q D 7 G i m + A u f / k / e G 0 2 3 L N G 8 6 l V a 9 8 W 7 d g k R + S Y n B C X n J M 2 u S e P p E O Y U T V a x p V x b M Y = " > A A A C p X i c b V F L b 9 N A E F 6 7 0 K Y G S l q O v V i J L D i g y I 4 q 2 m O A A z 2 A 1 E B e U h K t 1 p t x s s r 6 o d 0 x I r L 8 z / g V 3 P g 3 r B M L Q e O R d v b b b 7 7 R z i P M p N D o + 7 8 t + + T J 0 9 O z 1 r n z 7 P m L i 5 f t y 6 u J T n P F Y c x T m a p Z y D R I k c A Y B U q Y Z Q p Y H E q Y h t u P V X z 6 H Z Q W a T L C X Q b L m K 0 T E Q n O 0 F C 0 / d M D G j j G 9 Y 1 z v E X M c M O Z L N 6 X 5 h G m c q V 3 s b m K T U k L + T Z o Z P s N r G 7 U 6 m Z t A 7 c + K B F + Y B g V 3 0 r q O 9 4 n q p 2 / B X 4 u q a b t r t / z 9 + Y e g 6 A G X V L b A 2 3 / W q x S n s e Q I J d M 6 3 n g Z 7 g s m E L B J Z T O I t e Q M b 5 l a 5 g b m L A Y 9 L L Y T 7 l 0 P c O s 3 C h V 5 i T o 7 t l / M w o W 6 6 o D o 6 y K 1 I 9 j F d k U m + c Y 3 S 0 L k W Q 5 Q s I P H 0 W 5 d D F 1 q 5 W 5 K 6 G A o 9 w Z w L g S p l a X b 5 h i H M 1 i H T O E 4 H H L x 2 D S 7 w X v e v 3 h T X f w o R 5 H i 1 y T D n l D A n J L B u S e P J A x 4 V b H u r e G 1 l f 7 t f 3 F H t m T g 9 S 2 6 p x X 5 D + z 6 R / J Z c 9 W < / l a t e x i t > L s < l a t e x i t s h a 1 _ b a s e 6 4 = " D l u B z d I C L J p v 7 d R 3 q G V 4 a R r l P 9 U = " > A A A C p X i c b V F L T 9 t A E F 6 b l o f b U k O P X C y i C A 5 V Z E e o c O R x K I c i Q U t C p C R a r d f j Z J X 1 Q 7 t j R G T 5 n / E r e u u / 6 T q J E B C P t L P f f v O N d h 5 h L o V G 3 / 9 n 2 R s f P m 5 u b e 8 4 n z 5 / 2 f 3 q 7 u 3 3 d V Y o D j 2 e y U w N Q q Z B i h R 6 K F D C I F f A k l D C Q z i 7 q u M P j 6 C 0 y N J 7 n O c w T t g k F b H g D A 1 F 3 e c 2 0 M A x r m u c 0 x 4 l D K e c y f K i M o 8 w k 5 G e J + Y q p x U t 5 f e g k e 0 2 s L p R q 5 u 1 D d x k q U R 4 w j A u / 1 T U d 9 o / q X Z e C v x V 0 Y i 6 L b / j L 8 x b B 8 E K t M j K b q n 7 d x R l v E g g R S 6 Z 1 s P A z 3 F c M o W C S 6 i c U a E h Z 3 z G J j A 0 M G U J 6 H G 5 m H L l t Q 0 T e X G m z E n R W 7 C v M 0 q W 6 L o D o 6 y L 1 O 9 j N d k U G x Y Y n 4 1 L k e Y F Q s q X H 8 W F 9 D D z 6 p V 5 k V D A U c 4 N Y F w J U 6 v H p 0 w x j m a x j h l C 8 L 7 l d d D v d o I f n e 7 d S e v 8 c j W O b X J A D s k x C c g p O S f X 5 J b 0 C L c O r W v r z v p t H 9 k 3 9 r 3 d X 0 p t a 5 X z j b w x m / 4 H s q n P R w = = < / l a t e x i t > L d < l a t e x i t s h a 1 _ b a s e 6 4 = " O N G 7 f 0 T B L 2 + g o O 5 O J j w S U v o 2 p t g = " > A A A B 9 H i c b V D L S g M x F L 2 p r 1 p f V Z d u g k V w V W a K q M u i G x c u K t g H t E P J p J k 2 N J M Z k 0 y h D P 0 O N y 4 U c e v H u P N v z L S z 0 N Y D g c M 5 9 3 J P j h 8 L r o 3 j f K P C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a u k o U Z Q 1 a S Q i 1 f G J Z o J L 1 j T c C N a J F S O h L 1 j b H 9 9 m f n v C l O a R f D T T m H k h G U o e c E q M l b x e S M y I E p H e z / q 0 X 6 4 4 V W c O v E r c n F Q g R 6 N f / u o N I p q E T B o q i N Z d 1 4 m N l x J l O B V s V u o l m s W E j s m Q d S 2 V J G T a S + e h Z / j M K g M c R M o + a f B c / b 2 R k l D r a e j b y S y k X v Y y 8 T + v m 5 j g 2 k u 5 j B P D J F 0 c C h K B T Y S z B v C A K 0 a N m F p C q O I 2 K 6 Y j o g g 1 t q e S L c F d / v I q a d W q 7 m W 1 9 n B R q d / k d R T h B E 7 h H F y 4 g j r c Q Q O a Q O E J n u E V 3 t A E v a B 3 9 L E Y L a B 8 5 x j + A H 3 + A P y r k j 8 = < / l a t e x i t > L c < l a t e x i t s h a 1 _ b a s e 6 4 = " Z 6 U A G + l R X x J j W R P n X O w t K v t + q H o = " > A A A C B X i c b V D L S s N A F L 2 p r x p f U Z e 6 C J a C q 5 I U U Z d F Q V y 4 q G A f 0 I Y w m U 7 b o Z N J m J k I J X T j x l 9 x 4 0 I R t / 6 D O / / G S R t E W w 8 M n H v O v c y 9 J 4 g Z l c p x v o z C 0 v L K 6 l p x 3 d z Y 3 N r e s X b 3 m j J K B C Y N H L F I t A M k C a O c N B R V j L R j Q V A Y M N I K R p e Z 3 7 o n Q t K I 3 6 l x T L w Q D T j t U 4 y U l n z r s N w N k R p i x N K b i Y / N n + p K V 7 5 V c i r O F P Y i c X N S g h x 1 3 / r s 9 i K c h I Q r z J C U H d e J l Z c i o S h m Z G J 2 E 0 l i h E d o Q D q a c h Q S 6 a X T K y Z 2 W S s 9 u x 8 J / b i y p + r v i R S F U o 7 D Q H d m S 8 p 5 L x P / 8 z q J 6 p 9 7 K e V x o g j H s 4 / 6 C b N V Z G e R 2 D 0 q C F Z s r A n C g u p d b T x E A m G l g z N 1 C O 7 8 y Y u k W a 2 4 p 5 X q 7 U m p d p H H U Y Q D O I J j c O E M a n A N d W g A h g d 4 g h d 4 N R 6 N Z + P N e J + 1 F o x 8 Z h / + w P j 4 B j x N m G s = < / l a t e x i t > F c < l a t e x i t s h a 1 _ b a s e 6 4 = " H Z k R L i V z B / G c j X + a q C / J 7 a E K c 2 0 = " > A A A C F H i c b V D L S s N A F J 3 4 r P E V d e l m s B Q E o S R F 1 G V R E B c u K t g H t C F M J p N 2 6 G Q S Z i Z C C f 0 I N / 6 K G x e K u H X h z r 9 x 0 g a s r Q c G z j n 3 X u b e 4 y e M S m X b 3 8 b S 8 s r q 2 n p p w 9 z c 2 t 7 Z t f b 2 W z J O B S Z N H L N Y d H w k C a O c N B V V j H Q S Q V D k M 9 L 2 h 1 d 5 v f 1 A h K Q x v 1 e j h L g R 6 n M a U o y U t j z r p N K L k B p g x L L b s Y f N X 3 m d y 1 k V e F b Z r t o T w E X i F K Q M C j Q 8 6 6 s X x D i N C F e Y I S m 7 j p 0 o N 0 N C U c z I 2 O y l k i Q I D 1 G f d D X l K C L S z S Z H j W F F O w E M Y 6 E f V 3 D i z k 5 k K J J y F P m 6 M 1 9 S z t d y 8 7 9 a N 1 X h h Z t R n q S K c D z 9 K E w Z V D H M E 4 I B F Q Q r N t I E Y U H 1 r h A P k E B Y 6 R x 

 SAECON In this section, we first formalize the problem and then explain SAECON in detail. The pipeline of SAECON is depicted in Figure  1  with essential notations. 

 Problem Statement CPC Given a sentence s from the CPC corpus D c with n tokens and two entities e 1 and e 2 , a CPC model predicts whether there exists a preference comparison between e 1 and e 2 in s and if so, which entity is preferred over the other. Potential results can be Better (e 1 wins), Worse (e 2 wins), or None (no comparison exists). ABSA Given a sentence s from the ABSA corpus D s with m tokens and one entity e , ABSA identifies the sentiment (positive, negative, or neutral) associated with e . We denote the source domains of the CPC and ABSA datasets by D c and D s . D c and D s contain samples that are drawn from D c and D s , respectively. D c and D s are similar but different in topics which produces a domain shift. We use s to denote sentences in D c ? D s and E to denote the entity sets for simplicity in later discussion. |E| = 2 if s ? D c and |E| = 1 otherwise. 

 Text Feature Representations A sentence is encoded by its word representations via a text encoder and parsed into a dependency graph via a dependency parser  (Chen and Manning, 2014) . Text encoder, such as GloVe  (Pennington et al., 2014)  and BERT  (Devlin et al., 2019) , maps a word w into a low dimensional embedding w ? R d 0 . GloVe assigns a fixed vector while BERT computes a token 2 representation by its textual context. The encoding output of s is denoted by S 0 = {w 1 , . . . , e 1 , . . . , e 2 , . . . , w n } where e i denotes the embedding of entity i, w i denotes the embedding of a non-entity word, and w i , e j ? R d 0 . The dependency graph of s, denoted by G s , is obtained by applying a dependency parser to s such as Stanford Parser  (Chen and Manning, 2014)  or spaCy 3 . G s is a syntactic view of s  that is composed of vertices of words and directed edges of dependency relations. Advantageously, complex syntactic relations between distant words in the sentence can be easily detected with a small number of hops over dependency edges  (Ma et al., 2020) . 

 Contextual Features for CPC Global Semantic Context To model more extended context of the entities, we use a bidirectional LSTM (BiLSTM) to encode the entire sentence in both directions. Bi-directional recurrent neural network is widely used in extracting semantics . Given the indices of e 1 and e 2 in s, the global context representations h g,1 and h g,2 are computed by averaging the hidden outputs from both directions. ? ? ? h g,i , ? ? ? h g,i = BiLSTM(S 0 )[e i .index], i = 1, 2 h g,i = 1 2 ? ? ? h g,i + ? ? ? h g,i , h g,i ? R dg . Local Syntactic Context In SAECON, we use a dependency graph to capture the syntactically neighboring context of entities that contains words or phrases modifying the entities and indicates comparative preferences. We apply a Syntactic Graph Convolutional Network (SGCN)  (Bastings et al., 2017;  to G s to compute the local context feature h l,1 and h l,2 for e 1 and e 2 , respectively. SGCN operates on directed dependency graphs with three major adjustments compared with GCN (Kipf and Welling, 2017): considering the directionality of edges, separating parameters for different dependency labels 4 , and applying edge-wise gating to message passing. GCN is a multilayer message propagation-based graph neural network. Given a vertex v in G s and its neighbors N (v), the vertex representation of v on the (j + 1)th layer is given as h (j+1) v = ? ? ? u?N (v) W (j) h (j) u + b (j) ? ? , where ?(?) denotes an aggregation function such as mean and sum, W (j) ? R d (j+1) ?d (j) and b (j) ? R d (j+1) are trainable parameters, and d (j+1) and d  (j)  denote latent feature dimensions of the (j + 1)th and the jth layers, respectively. SGCN improves GCN by considering different edge directions and diverse edge types, and assigns different parameters to different directions or labels. However, there is one caveat: the directionalitybased method cannot accommodate the rich edge type information; the label-based method causes combinatorial over-parameterization, increased risk of overfitting, and reduced efficiency. Therefore, we naturally arrive at a trade-off of using direction-specific weights and label-specific biases. The edge-wise gating can select impactful neighbors by controlling the gates for message propagation through edges. The gate on the jth layer of an edge between vertices u and v is defined as g (j) uv = ? h (j) u ? ? (j) duv + ? (j) luv , g (j) uv ? R, where d uv and l uv denote the direction and label of edge (u, v), ? duv and ? (j) luv are trainable parameters, and ?(?) denotes the sigmoid function. Summing up the aforementioned adjustments on GCN, the final vertex representation learning is h (j+1) v = ? ? ? u?N (v) g (j) uv W (j) duv h (j) u + b (j) luv ? ? . Vectors of S 0 serve as the input representations h (0) v to the first SGCN layer. The representations corresponding to e 1 and e 2 are the output {h l,1 , h l,2 } with dimension d l . 

 Sentiment Analysis with Knowledge Transfer from ABSA We have discussed in Section 1 that ABSA inherently correlates with the CPC task. Therefore, it is natural to incorporate a sentiment analyzer into SAECON as an auxiliary task to take advantage of the abundant training resources of ABSA to boost the performance on CPC. There are two paradigms for auxiliary tasks: (1) incorporating fixed parameters that are pretrained solely with the auxiliary dataset; (2) incorporating the architecture only with untrained parameters and jointly optimizing them from scratch with the main task simultaneously  (Li et al., 2018; He et al., 2018; Wang and Pan, 2018) . Option (1) ignores the domain shift between D c and D s , which degrades the quality of the learned sentiment features since the domain identity information is noisy and unrelated to the CPC task. SAECON uses option (2). For a smooth and efficient knowledge transfer from D s to D c under the setting of option (2), the ideal sentiment analyzer only extracts the textual feature that is contingent on sentimental information but orthogonal to the identity of the source domain. In other words, the learned sentiment features are expected to be discriminative on sentiment analysis but invariant with respect to the domain shift. Therefore, the sentiment features are more aligned with the CPC domain D c with reduced noise from domain shift. In SAECON, we use a gradient reversal layer (GRL) and a domain classifier (DC)  (Ganin and Lempitsky, 2015)  for the domain adaptive sentiment feature learning that maintains the discriminativeness and the domain-invariance. GRL+DC is a straightforward, generic, and effective modification to neural networks for domain adaptation  (Kamath et al., 2019; Gu et al., 2019; Belinkov et al., 2019; Li et al., 2018) . It can effectively close the shift between complex distributions  (Ganin and Lempitsky, 2015)  such as D c and D s . Let A denote the sentiment analyzer which alternatively learns sentiment information from D s and provides sentimental clues to the compared entities in D c . Specifically, each CPC instance is split into two ABSA samples with the same text before being fed into A (see the "Split to 2" in Figure  1 ). One takes e 1 as the queried aspect the other takes e 2 . A(S 0 , G s , E) = h s,1 , h s,2 if s ? D c , h s if s ? D s . h s,1 , h s,2 , and h s ? R ds . These outputs are later sent through a GRL to not only the CPC and ABSA predictors shown in Figure  1  but also the DC to predict the source domain y d of s where y d = 1 if s ? D s otherwise 0. GRL, trainable by backpropagation, is transparent in the forward pass (GRL ? (x) = x). It reverses the gradients in the backward pass as ?GRL ? ?x = ?I. Here x is the input to GRL, ? is a hyperparameter, and I is an identity matrix. During training, the reversed gradients maximize the domain loss, forcing A to forget the domain identity via the backpropagation and mitigating the domain shift. Therefore, the outputs of A stay invariant to the domain shift. But as the outputs of A are also optimized for ABSA predictions, the distinctiveness with respect to sentiment classification is retained. Finally, the selection of A is flexible as it is architecture-agnostic. In this paper, we use the LCF-ASC aspect-based sentiment analyzer proposed by Phan and Ogunbona (2020) in which two scales of representations are concatenated to learn the sentiments to the entities of interest. 

 Objective and Optimization SAECON optimizes three classification errors overall for CPC, ABSA, and domain classification. For CPC task, features for local context, global context, and sentiment are concatenated: h e i = [h g,i ; h l,i ; h s,i ], i ? {1, 2}, and h e i ? R ds+dg+d l . Given F c , F s , F d , and F below denoting fullyconnected neural networks with non-linear activation layers, CPC, ABSA, domain predictions are obtained by ?c = ?(F c ([F(h e 1 ); F(h e 2 )])) (CPC only), ?s = ?(F s (h s )) (ABSA only), ?d = ?(F d (GRL(A(S 0 , G s , E)))) (Both tasks), where ? denotes the softmax function. With the predictions, SAECON computes the cross entropy losses for the three tasks as L c , L s , and L d , respectively. The label of L d is y d . The computations of the losses are omitted due to the space limit. In summary, the objective function of the proposed model SAECON is given as follows, L = L c + ? s L s + ? d L d + ?reg(L2), where ? s and ? d are two weights of the losses, and ? is the weight of an L2 regularization. We denote ? = {? s , ? d , ?}. In the actual training, we separate the iterations of CPC data and ABSA data and input batches from the two domains alternatively. Alternative inputs ensure that the DC receives batches with different labels evenly and avoid overfitting to either domain label. A stochastic gradient descent based optimizer, Adam  (Kingma and Ba, 2015) , is leveraged to optimize the parameters of SAECON. Algorithm 1 in Section A.1 explains the alternative training paradigm in detail. 

 Experiments 

 Experimental Settings Dataset CompSent-19 is the first public dataset for the CPC task released by  Panchenko et al. (2019) . It contains sentences with entity annotations. The ground truth is obtained by comparing the entity that appears earlier (e 1 ) in the sentence with the one that appears later (e 2 ). The dataset is split by convention  (Panchenko et al., 2019; Ma et al., 2020)    1 . Three datasets of restaurants released in  Se-mEval 2014 (Pontiki et al., 2014 Xenos et al., 2016)  are utilized for the ABSA task. We join their training sets and randomly sample instances into batches to optimize the auxiliary objective L s . The proportions of POS, NEU, and NEG instances are 65.8%, 11.0%, and 23.2%. Note. The rigorous definition of None in CompSent-19 is that the sentence does not contain a comparison between the entities rather than that 

 Performances on CPC Comparing with Baselines We report the best performances of baselines and SAECON in Table 2. SAECON with BERT embeddings achieves the highest F1 scores comparing with all baselines, which demonstrates the superior ability of SAE-CON to accurately classify entity comparisons. The F1 scores for None, i.e., F1(N), are consistently the highest in all rows due to the data imbalance where None accounts for the largest percentage. Worse data is the smallest and thus is the hardest to predict precisely. This also explains why models with higher micro-F1 discussed later usually achieve larger F1(W) given that their accuracy values on the majority class (None) are almost identical. BERT-based models outperform GloVe-based ones, indicating the advantage of contextualized embeddings. In later discussion, the reported performances of SAECON and its variants are based on the BERT version. The performances of the GloVe-based SAECON demonstrate similar trends. 

 Model Micro. F1(B) F1(W) F1(N) Majority 68.95 0.0 0.0 81.62 SE-Lin 79.31 62.71 37.61 88.42 SE-XGB 85.00 75.00 43.00 92.00  Table  2 : Performance comparisons between the proposed model and baselines on F1 scores (%). "-B" and "-G" denote different versions of the model using BERT  (Devlin et al., 2019)  and GloVe  (Pennington et al., 2014)  as the input embeddings, respectively. All reported improvements over the best baselines are statistically significant with p-value < 0.01.   5 . The Original (OR) row is a control experiment using the raw CompSent-19 without any weighting or augmentation. 

 Ablation Studies The optimal solution is the weighted loss (WL vs. the rest). One interesting observation is that data augmentation such as flipping labels and upsampling cannot provide a performance gain (OR vs. FL and OR vs. UP). Weighted loss performs a bit worse on F1(N) but consistently better on the other metrics, especially on Worse, indicating that it effectively alleviates the imbalance issue. In practice, static weights found via grid search are assigned to different labels when computing the cross entropy loss. We leave the exploration of dynamic weighting methods such as the Focal Loss  (Lin et al., 2017)   Alternative Training One novelty of SAECON is the alternative training that allows the sentiment analyzer to learn both tasks across domains. Here we analyze the impacts of different batch ratios (BR) and different domain shift handling methods during the training. BR controls the number of ratio of batches of the two alternative tasks in each training cycle. For example, a BR of 2 : 3 sends 2 CPC batches followed by 3 ABSA batches in each iteration. Figure  3a  presents the entire training time for ten epochs with different BR. A larger BR takes shorter time. For example, a BR of 1:1 (the leftmost bar) takes a shorter time than 1:5 (the yellow bar). Figure  3b  presents the micro-F1 scores for different BR. We observe two points: (1) The reported performances differ slightly; (2) Generally, the performance is better when the CPC batches are less than ABSA ones. Overall, the hyperparameter selection tries to find a "sweet spot" for effectiveness and efficiency, which points to the BR of 1:1. Micro-F1 As a result, SAECON achieves the best performance especially on F1(W), ?GRL gets the second, and the "option (1)" gets the worst. These demonstrate that (1) the alternative training (blue vs. green) for an effective domain adaptation is necessary and (2) there exists a positive correlation between the level of domain shift mitigation and the model performance, especially on F1(W) and F1(B). A better domain adaptation produces higher F1 scores in the scenarios where datasets in the domain of interest, i.e., CPC, is unavailable.  

 Case Study In this section, we qualitatively exemplify the contribution of the sentiment analyzer A. Table 6 reports four example sentences from the test set of CompSent-19. The entities e 1 and e 2 are highlighted together the corresponding sentiment predicted by A. The column "Label" shows the ground truth of CPC. The "?" column computes the sentiment distances between the entities. We assign +1, 0, and ?1 to sentiment polarities of CPC sentences with sentiment predictions by A Label ? S1: This is all done via the gigabit [Ethernet:POS] interface, rather than the much slower [USB:NEG] interface. Better +2 S2: Also, [Bash:NEG] may not be the best language to do arithmetic heavy operations in something like [Python:NEU] might be a better choice. POS, NEU, and NEG, respectively. ? is computed by the sentiment polarity of e 1 minus that of e 2 . Therefore, a positive distance suggests that e 1 receives a more positive sentiment from A than e 2 and vice versa. In S1, sentiments to Ethernet and USB are predicted positive and negative, respectively, which can correctly imply the comparative label as Better. S2 is a Worse sentence with Bash predicted negative, Python predicted neutral, and a resultant negative sentiment distance ?1. For S3 and S4, the entities are assigned the same polarities. Therefore, the sentiment distances are both zeros. We can easily tell that preference comparisons do not exist, which is consistent with the ground truth labels. Due to the limited space, more interesting case studies are presented in Section A.4. 

 Conclusion This paper proposes SAECON, a CPC model that incorporates a sentiment analyzer to transfer knowledge from ABSA corpora. Specifically, SAECON utilizes a BiLSTM to learn global comparative features, a syntactic GCN to learn local syntactic information, and a domain adaptive auxiliary sentiment analyzer that jointly learns from ABSA corpora and CPC corpora for a smooth knowledge transfer. An alternative joint training scheme enables the efficient and effective information transfer. Qualitative and quantitative experiments verified the superior performance of SAECON. For future work, we will focus on a deeper understanding of CPC data augmentation and an exploration of weighting loss methods for data imbalance. 
