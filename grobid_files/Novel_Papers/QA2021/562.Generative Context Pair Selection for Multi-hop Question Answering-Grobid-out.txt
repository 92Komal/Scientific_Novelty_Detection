title
Generative Context Pair Selection for Multi-hop Question Answering

abstract
Compositional reasoning tasks such as multihop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, our proposed generative passage selection model, while being comparable to the state-of-the-art answering performance, has a better performance (4.9% higher than baseline) on an adversarial held-out set that tests the robustness of model's multi-hop reasoning capabilities.

Introduction Recently many reading comprehension datasets like HotpotQA  (Yang et al., 2018)  and Wiki-Hop  (Welbl et al., 2018)  that require compositional reasoning over several disjoint passages have been introduced. This style of compositional reasoning, also referred to as multi-hop reasoning, first requires finding the correct set of passages relevant to the question and then finding the answer span in the selected set of passages. These dataset are often collected via crowdsourcing, which makes the training and evaluation of such models heavily reliant on the quality of the collected held-out sets. Crowdsourced datasets, however, often present only a partial picture of the underlying data distribution. Learning complex latent sequential decisions, like in multi-hop reasoning, to answer a given question under such circumstances is marred by numerous biases, such as annotator bias  (Geva et al., 2019) , label bias  (Dua et al., 2020; Gururangan et al., 2018) , survivorship bias  (Min et al., 2019b; Jiang and Bansal, 2019) , and ascertainment bias  (Jia and Liang, 2017) . As a result, testing model performance on such biased held-out sets becomes unreliable as the models exploit these biases and learn shortcuts to get the right answer but without learning the right way to reason. Consider an example from HotpotQA in Figure  1 , where the latent entity "Virgina Commonwealth University" can be used by the model  (Jiang and Bansal, 2019)  to bridge the two relevant passages (highlighted in green) from the original dev set and correctly predict the answer "1838". However, upon adding an adversarial context (highlighted in pink) to the pool of contexts, the model prediction changes to "1938" implying that the model did not learn the right way to reason. This is because the discriminatively trained passage selector exploits lexical cues like "founded" in the second passage and does not pay attention to the complete question. The absence of such adversarial contexts at training allows the model to find incorrect reasoning paths. In this work, we propose a generative context pair selection model that reasons through the data generation process of how a specific question could have been generated given pair of passages. We show that our proposed passage selection module has a better performance on the original (+2.2%) and the adversarial dev set (+4.9%) that tests the model's reasoning abilities (unlike the original dev set which is marred by bias). We use a generic answering model and show that while being comparable in end-to-end performance with close to state-of-the-art systems on the original dev set, our model provides a better performance on the adversarial set. Any advances in the answering model can be applied in a straightforward manner to a generative passage selector to further improve performance. 

 Generative Passage Selection Given a set of contexts C = {c 0 , ..., c N }, the goal of multi-hop question answering is to combine information from C to an identify answer span a for a given question q. Let ? = {(c i , c j ) = c i j : c i ? C, c j ? C)} be the set of all possible context pairs that can be formed from C. Existing models for multi-hop question answering  (Tu et al., 2020;  consist of two components: a discriminative passage selection and an answering model. Passage selection identifies which pairs of contexts are relevant for answering the given question, i.e., it estimates p(c i j | q, ?). This is followed by the answering model to extract the answer span given a context pair and the question (p(a | q, c i j )). These are combined as follows: p(a | q, ?) = c i j p(a | q, c i j )p(c i j | q, ?) (1) The discriminative passage selector learns to select a set of contexts by conditioning on the question representation. This learning process does not encourage the model to pay attention to the entire question, which can result in ignoring parts of the question, and thus, learning spurious correlations. For prediction, best context pair c * i j is used by the answering module to get the answer, a * = argmax p(a | q, c * i j ). As shown by  Min et al. (2019a) , using the top scoring reasoning chain to answer the question is often sufficient and does not require marginalization over multiple chains. 1 

 Proposed Model We propose a joint question-answering model that learns p(a, q | ?) instead of p(a | q, ?). This model can be factorized into a generative passage selector and a standard answering model as: p(a, q | ?) = c i j p(a | q, c i j )p(q|c i j )p(c i j |?) (2) A prior p(c i j |?) over the context pairs establishes a measure of compatibility between passages in a particular dataset. The conditional generation model p(q|c i j ) estimates the likelihood of generating the given question from a selected pair of passages. Finally, a standard answering model p(a | q, c i j ) learns a likely answer distribution given a question and context pair. The first two terms (prior and conditional generation) can be seen as a generative model that selects a pair of passages from which the question could have been constructed. The answering model can be instantiated with any existing SOTA model, like graph neural network  (Tu et al., 2020; Shao et al., 2020)  and entity-based chain reasoning . The process at prediction is identical to that with discriminative passage selection, except that the context pairs are scored by taking the entire question into account, c * i j = argmax c i j p(q|c i j )p(c i j |?). 

 Model Learning For learning the generative model, we train the prior, p(c i j |?) and the conditional generation model p(q | c i j , ?) jointly. First, the prior network projects the concatenated contextualized representation, r i j , of starting and ending token of concatenated contexts (c i ; c j ), from the encoder to obtain un-normalized scores, which are then normalized across all context-pairs via softmax operator. The loss function tries to increases the likelihood of gold context pair over all possible context pairs. r i j = encoder(c i ; c j ) (3) s i j = W 1?d (r i j [start]; r i j [end]) (4) The conditional question generation network gets contextual representations for context-pair candidates from the encoder and uses them to generate the question, via the decoder. The objective function increases the likelihood of the question for gold context pairs and the unlikelihood  (Welleck et al., 2020)  for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(?) = |question| t=1 log p(q t | q <t , c gold ) + n?|neg.pairs| |question| t=1 log (1 ? p(q t | q <t , c n )) (5) 

 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA  (Yang et al., 2018)  and Wik-iHop  (Welbl et al., 2018) . We use a pre-trained T5  (Raffel et al., 2019)     (Tu et al., 2020) . Table  1  compares end-to-end original dev set performance of answering model when combined with a standard (79.5 F 1 ) and a generative passage selector (81.9 F 1 ) for HotpotQA. Table  2  shows minor improvements in passage accuracy on using generative selector in WikiHop. This shows that generative passage selecetor is able to find (latent) entity connections between context pairs that are consistent with the complete question and not just parts of it. 

 Adversarial Evaluation We use an existing adversarial set  (Jiang and Bansal, 2019)  for HotpotQA to test the robustness of model's multi-hop reasoning capabilities given a confusing passage. This helps measure, quantitatively, the degree of biased correlations learned by the model. In Table  1 , we show that the standard discriminative passage selector has a much higher performance drop (?4%) as compared to the generative selector (?1%) on adversarial dev set  (Jiang and Bansal, 2019) , showing that generative selector is less biased and less affected by conservative changes  (Ben-David et al., 2010)  to the data distribution. While the end to end QA performance of our model is comparable (? 0.3 F1) to  Fang et al. (2020)  on the original dev-set, on the adversarial set our method is better than Fang et al. (2020) (? 1.2 F1). Table  3  shows that the decoder of generative passage selector was able to generate multi-hop style questions from a pair of contexts. 

 Context pairs vs. Sentences Some context selection models for HotpotQA use a multi-label classifier that chooses top-k sentences  (Fang et al., 2020; Clark and Gardner, 2018)  which result in limited inter-document interaction than context pairs. To compare these two input types, we construct a multi-label sentence classifier p(s|q, C) that selects relevant sentences. This classifier projects a concatenated sentence and question representation, followed by a sigmoid, to predict if the sentence should be selected. This model has a better performance over the context-pair selector but is more biased (Table  4 ). We performed similar experiments with the generative model, where we train a generative sentence selection model by first selecting a set of sentences with a gumbel softmax (prior) and then generating The Vermont Catamounts men's soccer team represents the University of Vermont in all NCAA Division I men's college soccer competitions. The team competes in the America East Conference. Original Question, q: the vermont catamounts men's soccer team currently competes in a conference that was formerly known as what from 1988 to 1996? Generated Questions: p(q | c i j , ?) the vermont catamounts men's soccer team competes in what collegiate athletic conference affiliated with the ncaa division i, whose members are located mainly in the northeastern united states? the vermont catamounts men's soccer team competes in a conference that was known as what from 1979 to 1988? the vermont catamounts men's soccer team competes in a conference that was known as what from 1988 to 1996? the question given the set of sentences. Given that the space of set of sentences is much larger than context pairs, the generative sentence selector does not have good performance (Table  4 ). Since sentence selection helped improve performance of the discriminative passage selector, we add an auxiliary loss term to our generative passage selector that also predicts the relevant sentences in the context pair when generating the question (p(q, s|c i j , ?)), in a multi-task manner. We see slight performance improvements by using relevant sentences as an additional supervision signal. 

 Related work Many recent passage selection models for Hot-potQA and Wikihop's distractor style setup employ discriminative context selectors given the question  (Tu et al., 2020; Fang et al., 2020; Shao et al., 2020) . The high performance of such passage selectors can be attributed to existing bias in Hot-potQA  (Jiang and Bansal, 2019; Min et al., 2019b) , which allows shallow lexical overlap of question with a single context to result in the correct answer. Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop  (Das et al., 2019) . With the release of datasets like SearchQA  (Dunn et al., 2017) , TriviaQA  (Joshi et al., 2017) , and NaturalQuestions  (Kwiatkowski et al., 2019) , a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question  (Seo et al., 2018;  for obtaining the relevant answers. In generative question answering, a few works  (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020)  have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification  (Duan et al., 2017) , fact checking  (Fan et al., 2020) , data augmentation  Serban et al., 2016) , pedagogical systems  (Lindberg et al., 2013) , and dialog systems  (Yanmeng et al., 2020)  etc. 

 Conclusion We proposed a generative formulation of context pair selection for multi-hop question answering. By encouraging this selection model to explain the entire question, it is less susceptible to bias, performing substantially better on adversarial data than existing discriminative methods. Our proposed model is simple to implement and can be used with any existing (or future) answering model; we will release code to support this integration. Since context pair selection scales quadratically with the number of contexts, it is not ideal for scenarios that involve a large number of possible contexts. However, it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations. With more reasoning steps, selecting relevant documents given only the question becomes challenging, increasing the need for inter-document interaction. An easy way to reduce the computation cost is to consider only a set of top-k contexts and perform a two-stage coarse-to-fine passage selection. The generative story presented in the paper may not work for question types beyond the datasets considered, for eg., in case of multiple (>1) bridge entities or more than two contexts. However, we demonstrate that this idea works for most common reasoning types that are central in current multihop reasoning datasets. The code is available at https://github.com/dDua/JointQA 
