title
Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning

abstract
Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with selfcontained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy. 1

Introduction Automatic generation of question-answer pairs (QA pairs) is a widely studied problem, primarily used to improve the performance of question answering systems via data augmentation  (Alberti et al., 2019; Shakeri et al., 2020) . However, question generation has also recently garnered interest in the context of conversational agents, where suggested questions (SQs) (i.e., You can also ask...) have emerged as a promising approach to drive multi-turn dialogues by educating customers about the agent capabilities and guiding users along dialogue trajectories with more engaging content  (Yin et al., 2020; Nouri et al., 2020) . As an example, consider a news chatbot engaged in a dialogue regarding COVID-19 vaccine developments producing the SQ {Q: How effective is the Pfizer-BioNTech vaccine?} paired with the answer  1  Code and Data will be made available at https:// github.com/amazon-research/SC2QA-DRIL {A: Pfizer/BioNTech vaccine is around 91% effective at preventing COVID-19, according to updated trial data. Experts fear new variants of COVID-19 from South Africa and Brazil may be resistant to existing vaccines and treatment.} Firstly, SQs of this form mitigates the user burden regarding the necessity of both deep subject knowledge to ask good questions and awareness of the agent question answering capabilities to expect good answers. Secondly, the agent can look-ahead when selecting SQs to bias toward confidently correct answers and content expected to lead to further follow-up questions and general system engagement. Targeting the SQ problem in news chatbot scenarios (e.g.,  (Laban et al., 2020) ), this work examines QA pair generation corresponding to a news article summary paired with a self-contained question. Table  1  shows an example of the task. SQs based on these summary-centric QA pairs act as implicit article recommendations, complementing SQs focusing on passage-level extracted answers or factoid information. QA pairs generated for this purpose must satisfy several criteria including: (1) questions are self-contained (i.e., users need not read the corresponding articles nor require significant additional domain knowledge to unambiguously understand the questions  (Yin et al., 2020) ), (2) questions are summary-centric (questions capture the gists of the corresponding articles), (  3 ) answers correctly answer the questions, and (4) answers are brief but sufficient such that users can confidently trust the results. Additionally, to support different settings (e.g., screened device, mobile device, voice-only), we explore QA pair generation for varying application-specific answer length requirements. To satisfy these requirements, we first collect a corpus of suitable QA pairs, accomplished by curating a set of news articles with well-formed questions as their titles and for which we can confidently generate variable length summaries as an-Article: President Biden's infrastructure plan calls for an unprecedented boost in federal aid to the nation's passenger rail system, seeking to address Amtrak's repair backlog, extend service to more cities and modernize the network in the Northeast Corridor. The American Jobs Plan announced Wednesday calls for $80 billion for rail -money that could be crucial in taking passenger service to cities such as Las Vegas and Nashville, and expand operations across large metropolitan areas such as Atlanta and Houston. "President Biden's infrastructure plan is what this nation has been waiting for," Amtrak chief executive William J. Flynn said, while echoing Biden's push to rebuild and improve... Suggested Question: What does President Biden's infrastructure plan mean for Amtrak? Short Answer: The federal funding would help Amtrak accomplish long-needed upgrades to tracks, tunnels and bridges in the Northeast. Long Answer: The American Jobs Plan announced Wednesday calls for $80 billion for rail. The federal funding would help Amtrak accomplish long-needed upgrades to tracks, tunnels and bridges in the Northeast, the nations busiest rail corridor. Amtrak has a $45.2 billion backlog of projects that it says are needed to bring its assets to a state of good repair. Table  1 : The suggested QA pair generation task. Given an article, we generate a self-contained and summarycentric question and a length-constrained answer. The question captures the gist of the article and can be understood without reading the corresponding article. swers. Observing that the summary generation ? question generation pipeline suffers from exposure bias  (Ranzato et al., 2016) , we propose a novel differential reward imitation learning (DRIL) training method that samples summary answers and reconstructs questions exclusively based on the hidden states of the answer decoder. Generated summaries are capable of directly reconstructing the questions, making them more likely the answers to the questions, and generate questions more closely related to the gists of the articles. We empirically validate the model with automated and human evaluations. In this paper, we study QA pair generation corresponding to variable length article-summarizing answers paired with self-contained and summarycentric questions. Our contributions include: (1) We collect a new QA dataset targeted for producing SQs in a news chatbot. (2) We propose a QA pair generation model where both questions and answers are well-formed, questions capture the central gists of articles, and answers are succinct while containing sufficient supporting context. (3) We propose a novel differentiable reward imita-tion learning (DRIL) method which shows better performance over maximum likelihood estimation (MLE) and reinforcement learning (RL) for QA pair generation. (4) We perform extensive empirical evaluations to quantify DRIL-based QA pair generation improvements. 

 Related Works Question-only Generation (QG). Both heristicbased  (Heilman and Smith, 2010)  and neural models  (Du et al., 2017; Zhou et al., 2017; Sun et al., 2018)  have been applied to QG. Usually, neural QG models are given contexts containing answers beforehand, contrasting with our goal of jointly generating QA pairs. Tuan et al. (2020);  Song et al. (2018) ;  Zhao et al. (2018)  proposed to generate questions from long text and wider contexts, which is related to our method for QG using summaries. However, these wider contexts are only used to improve QG for the specified answer spans and do not attempt to capture the central gists of articles. Question and Answer Generation (QG+AG). QG+AG generates QA pairs jointly  Alberti et al., 2019; Du and Cardie, 2018, 2017; Subramanian et al., 2018; Wang et al., 2019; Krishna and Iyyer, 2019) , frequently with two independent steps: identify question-worthy answer spans followed by generating answer-aware questions. Recent works train neural models to generate QA pairs  (Shakeri et al., 2020; Lee et al., 2020)  using QA datasets such as SQuAD  (Rajpurkar et al., 2016)  and Natural Questions  (Kwiatkowski et al., 2019)  modulo the goal of generating self-contained questions paired with succinct but sufficient articlesummarizing answers. Applications of QG and QG+AG. QG and QG+AG have been used for applications including data augmentation for QA systems  (Alberti et al., 2019; Shakeri et al., 2020) , information seeking in chatbots  (Qi et al., 2020a; Laban et al., 2020) , document understanding  (Krishna and Iyyer, 2019) , educational practice and assessment  (Le et al., 2014) , and online shopping  (Yu et al., 2020) . Training Mechanism for Sequence Prediction. Sequence prediction models are commonly trained with MLE. However, MLE can lead to degeneration  (Holtzman et al., 2019)  caused by exposure bias  (Ranzato et al., 2016) . Many algorithms  (Yu et al., 2017; Lamb et al., 2016; Song et al., 2020; Welleck et al., 2019)  have been proposed to mitigate exposure bias. Our DRIL method not only mitigates exposure bias, but also optimizes for a differentiable reward function that is aligned with the end goal. Please refer to Section 4.2 for comparison between DRIL and existing algorithms. 3 (SC) 2 QA: A Self-Contained and Summary-Centric QA Dataset While multiple QA datasets exist to train a QG or AG model, none specifically fit the goal of this paper. QA pairs in SQuAD  (Rajpurkar et al., 2018) ,  NewsQA (Trischler et al., 2017) , and Natural Questions (NQ)  (Kwiatkowski et al., 2019)  are not designed to capture the article gists, and a significant number of questions in SQuAD and NewsQA are not self-contained. A key observation enabling this work is that many news articles have questions as their titles (e.g. How has the Biden administration helped student loan borrowers?) that can be used to train a SQ generation model since these questions usually correspond to the central gists of the news articles and are designed to be understood without reading the articles. However, two challenges remain: (1) clickbait titles need to be filtered, and 2) these questions are not paired with summary-centric answers. Therefore, we developed the following data collection procedure to produce (SC) 2 QA, our selfcontained summary-centric QA dataset. 

 Question-Article Pairs Collection Starting with a curated URL list of news websites, we collected all articles between September 2020 to March 2021 with a title that starts with a predefined list of words (e.g., Where, What, How) and ends with a question mark. We then define a set of rules to filter out ill-formed and clickbait titles (details in Appendix A). Finally, we remove any questions that appear in the articles to ensure we don't learn to copy the questions when present. In total, we collected 39,460 such question-article pairs. 

 {Question, Article, Summary, Length Constraint} 4-Tuples Collection Given collected question-article pairs, we must pair them with suitable answers to produce QA pairs. From a preliminary study, we observed that ? 70% of title questions can be answered by summaries of the corresponding articles. As a result, we set out to augment the question-article dataset with generated summaries as pseudo ground truth answers using following three-step procedure: Step 1 (Define desired answer lengths): One of our goals is to generate well-formed answers that are succinct while containing sufficient supporting context. Therefore, we generate summaries with varying brevity. Analyzing the average number of tokens for the first 1, 2 and 3 sentences of the CNN/DailyMail summaries  (Hermann et al., 2015) , we define three buckets of varying answer lengths: (0, 30], (30, 50] and (50, 72] BPE tokens. Step 2 (Generate summary): For each article and desired length bucket, we use three SoTA summarization models  (PEGASUS (Zhang et al., 2020) , BART  (Lewis et al., 2020), and CTRLSum (He et al., 2020 )) fine-tuned on CNN/DailyMail to generate three candidate summaries -enforcing summary length via control of EOS token generation. Unfinished sentences are removed and the length bucket is reassigned if needed. Step 3 (Filter-out incorrect summary answers): Not all questions can be answered by the generated summaries since: (1) even the ground truth summary may not be a correct answer to the question and (2) summaries generated by SoTA models may not be good. To identify if a candidate summary answers the question, we train a QA pair classifier using the 4 million question-snippet pairs MSMARCO dataset  (Bajaj et al., 2016) . For each article and length bucket, we select the candidate summary that has the highest score predicted by the trained classifier. In total, we produce 53,746 4-Tuples of {Question, Article, Summary, Length Constraint}. For additional details and dataset statistics, please refer to Appendix A. 

 Models for QA Pair Generation In this section, we propose a family of QA pair generation models that are trained on the data collected in Section 3. Let D denote a document (news article), S denote a summary, Q denote a question, L denote a length bucket indicator (LB0, LB1 or LB2), and <s> and </s> denote the special BOS and SEP tokens respectively. 

 Base D?S?Q Model (D-S) Our base model is shown in Figure  1 , consisting of two transformer-based encoder-decoder models  (Vaswani et al., 2017)  where one performs answer generation (AG) and the other question generation (QG the document, and decodes a length-constrained summary: f ? a enc : L + D ? c a enc f ? a dec : S 0:T ?1 , c a enc ? S 1:T where ? a enc and ? a dec are the encoder and decoder parameters, c a enc is the sequence of hidden states at the last encoder layer, S 1:T is the ground truth summary, and S 0:T ?1 is the decoder input (S 1:T offset by one timestamp and prepended by a BOS token). The AG model is trained using MLE: L(? a enc , ? a dec ) = ? N n=1 log p(S (n) |L (n) + D (n) ) where (n) represents the n-th training instance. QG is also trained via MLE, mapping an input summary to a question: f ? q enc : S ? c q enc f ? q dec : Q 0:T ?1 , c q enc ? Q 1:T L(? q enc , ? q dec ) = ? N n=1 log p(Q (n) |S (n) ) During inference, when decoding summary answers, we again control the generation of EOS to fall into the range specified by the desired length bucket. We remove any unfinished sentences at the end unless after the truncation the answer is shorter than the minimum length of the length bucket. We use a pre-trained BART model  (Lewis et al., 2020)  to initialize ? a enc , ? a dec , ? q enc and ? q dec . We name this base model D-S since the AG model takes the document (D) as input and the QG model takes the summary (S) as input. In Section 4.3 we will describe multiple variants of this model. 

 Optimizing Answer Generation by Differentiable Rewards When using MLE to train the base model, the decoder input at timestep t is the ground truth token at timestep t ? 1, sometimes called teacherforcing  (Williams and Zipser, 1989 ) and known to suffer from exposure bias  (Ranzato et al., 2016)  due to the mismatch between training and inference. That is, during inference the decoder input is the predicted token instead of the ground truth token of the last timestep, causing errors from each timestamp to accumulate during generation. It has been shown that neural text generation models trained with MLE lead to generic and repetitive outputs  (Welleck et al., 2019; Holtzman et al., 2019) . Additionally, we usually want to optimize generation metrics (e.g., ROUGE) and human feedback directly instead of optimizing training data likelihood. To mitigate these concerns, we can sample decoder output during training and calculate the loss of the sampled output. Several works use RL to achieve this for text generation  (Stiennon et al., 2020; Ziegler et al., 2019; Yu et al., 2017)  and directly optimize for preferred metrics. However, RL is not sample efficient and difficult to tune in text generation tasks due to sparse rewards. For example, Hosking and Riedel (2019) have shown that applying RL to QG do not improve human evaluation metrics. Meanwhile, we observe that when generating a summary as the answer of a QA pair, we want to generate a summary that can better reconstruct the ground truth question without the article since: (1) a summary that can reconstruct a question is more likely to be able to answer that question and (2) a summary that better reconstructs the ground truth question leads to a generated question that is closer to the gist of the article. Moreover, the AG model is conditioned on the length bucket to control the levels of brevity, meaning that when the maximum allowed answer length is short, the question reconstruction will enforces the AG model to generate succinct but informative answers with respect to the question given the selected brevity level. We validate these assumptions in Section 5. We now propose the differentiable reward imita- Figure  2 : Training of answer generation (AG) of the D-S-DRIL model. The input to the AG decoder is either S 0:T ?1 or <s>. When the input is S 0:T ?1 , the AG decoder uses teacher-forcing to predict S 1:T , and the gradients back-propagate from S 1:T to the AG decoder and AG encoder (the red dash arrow on the middle left), which is similar to the AG of the D-S model. However, when the input is <s>, the AG decoder samples a summary S 1:T , and the answer decoder hidden states are used to reconstruct the question Q 1:T . The gradients back-propagate from Q 1:T to the AG decoder and AG encoder (the red dash arrow on the top right). This reinforces the model to generate summaries that can reconstruct the questions. tion learning (DRIL) method for training the AG model as shown in Figure  2 . During training, the AG model performs vanilla sampling to generate a summary: f ? a enc : L + D ? c a enc f ? a dec : BOS, c a enc ? c a dec , S where c a dec is the sequence of hidden states at the last layer of the decoder, and S is the sampled summary. This differs from teacher-forcing since summaries are sampled in training. We then use another transformer-based decoder to reconstruct the question: f ? r dec : Q 0:T ?1 , c a dec ? Q 1:T noting that this decoder only depends on the hidden states of the AG decoder (not L + D). This forces the model to reconstruct the question only from the summary. The gradient can back-propagate from the question to the hidden states of the AG decoder c a dec and AG encoder c a enc such that the question reconstruction loss will guide AG. To ensure generated summary fluency, we also add the MLE loss from the base model. Overall, the AG model's loss function is given by: L(? a enc , ? a dec ,? r dec ) = ? N n=1 ? log p(Q (n) |S (n) , L (n) + D (n) ) + (1 ? ?) log p(S (n) |L (n) + D (n) ) In our experiments, ? = 0.3 performs the best on the validation set. Finally, while we apply DRIL to the training of the AG model, the QG model remains the same as the base model. We do not use the question reconstruction decoder ? r dec as our QG model because its encoder input c a dec is a unidirectional representation and hence not preferred. We call this QA pair generation model D-S-DRIL. Connection with RL, Unlikelihood  (Welleck et al., 2019) ,  SeqGAN (Yu et al., 2017) , and Professor-forcing  (Lamb et al., 2016) , etc. These methods mitigate exposure bias to some degree by calculating the loss from sampled sequences during training. Unlikelihood training penalizes the likelihood of undesired sampled sequences. Seq-GAN and Professor-forcing both calculate the loss using a discriminator which learns to distinguish between the generated and ground truth sequences. They don't optimize an extrinsic reward function.  Caccia et al. (2019)  show that Language GANs suffer from mode collapse and do not outperform MLE on the quality and diversity evaluation. Seq-GAN uses RL optmization and thus suffers from aforementioned issues. Our DRIL method, on the other hand, learns to optimize a differentiable reward function that aligns with the end goal, and has lower gradient variance compared with RL. We empirically compare RL with DRIL in Section 5. Beyond this work, DRIL can be applied to other sequence prediction problems. For example, in step-by-step instruction following such as ALFRED tasks  (Shridhar et al., 2020) , DRIL can optimize the current step's action trajectory such that it can reconstruct the next K instructions. The intuition is if the current step's action trajectory is correct, then the agent should be able to follow the ground truth actions in the next steps to fulfill the tasks. From this perspective, DRIL is similar to SQIL  (Reddy et al., 2020) , which avoids drifting away from the demonstrations over long horizons by encouraging trajectories that return to demonstrated states when encountering unseen states. In conversational AI, Hosseini-Asl et al. (2020) proposed to fine-tune a GPT-2 model to generate system responses turn-by-turn. DRIL can optimize response generation at each turn such that the response and dialogue context can reconstruct the next K turns' user and system response with a similar intuition: a correct system response will increase the likelihood of the ground truth in future turns. It avoids drifting away from demonstrations and mitigates exposure bias. 

 Base Model Variants In this section, we specify additional baseline QA pair generation models. Similar to the base D-S model, these models are based on transformer encoder-decoder architectures. The differences between these models are the encoder and decoder inputs during training and inference as summarized in Table  2 . Models are named by the encoder input of the AG and QG models joined with a '-'. D-D is similar to D-S except that QG takes the document (D) rather than the summary (S) as encoder input. QD-D generates question-conditioned answers, such that the AG model becomes a questionanswering model. D-SD is an extension of D-S and D-D such that the encoder of the QG model takes the concatenation of S and D. D-S-DRIL optimizes the AG model of D-S using DRIL. D-S-RL optimizes the AG model of D-S using RL, and the reward function is defined as the negative question reconstruction loss calculated by the QG model of D-S. For further details, refer to Appendix B. 

 Experiments We conduct experiments to answer 3 research questions: (1) How good are the QA pairs generated by each algorithm?, (2) Can DRIL outperform MLE and RL on QA pair generation?, and (3) Is our (SC) 2 QA dataset preferable compared with existing public QA datasets for QA pair generation? For each generated QA pair, we are interested in evaluating the following 3 questions: (1) Does the length-constrained summary answer the question?, (2) Does the question capture the article gist?, (3) Is the question self-contained? We specify automated metrics and human evaluations to quantify the answers to these research questions. 

 Automated Metrics ROUGE-L (R-L) and BLEU. ROUGE-L and BLEU evaluate generated summaries/questions with respect to reference summaries/questions in the validation set. QA Pair Classifier Scores (QACS). We need to measure how well the generated summaries answer the generated questions despite not having ground truth answers. Using the trained QA pair classifier from Section 3, we propose QACS, which is the average of classifier predicted scores on the generated QA pairs. The pseudo upper and lower bounds of QACS are 0.359 and 0.046 based on the average classifier predicted scores of the positive and negative QA pairs in our human evaluation. 

 Human Evaluation We conduct human evaluation on Amazon Mechanical Turk. We designed 7 annotation tasks (ATs). Please refer to Appendix C for detailed human evaluation setup. Here we describe 4 ATs for which we are most concerned: AT-1 shows a QA pair and asks Without referring to the answer or the article, are you able to understand the question? (Is the question self-contained?), AT-2 follows AT-1 and asks Does the passage in the Answer text box answers the question?, AT-5 shows the corresponding article and asks Does the question in the Question text box capture the gist of the Article?. For these three tasks, annotators select either TRUE or FALSE. AT-6 shows an article and a list of questions generated by different models and asks Which Question shown above best captures the gist of the Article? 

 Baseline We evaluate D-S and its variants in Table  2 . Beyond that, we evaluate the following baselines. QA-Gen 2S: This is the state-of-the-art model for QA pairs generation for improving QA systems. We train QAGen 2S on our dataset, which is similar to QD-D except that there is no length control on the answers. CTRLSum: We use a pretrained CTRLSum model to generate questiondependent summaries. Questions are generated by the QG model of QD-D. QA Transfer: We train a question-answering model on the NewsQA dataset to answer the generated questions. Questions are generated by the QG model of QD-D. This is to verify if a pre-trained question-answering model is sufficient to answer the questions in our dataset. D-S-NewsQA and Natural Questions (D-S-NQ): These two models are similar to D-S, except that the QG models are trained on NewsQA and NQ, respectively. This is to verify if (SC) 2 QA is better than other existing QA datasets for QG tasks. Refer to Appendix B for implementation details.  D-D L + D S D Q L + D S' S' Q' D-SD L + D S S + D Q L + D S' S' + D Q' QD-D Q + L + D S D Q Q' + L + D S' D Q' D-S-DRIL L + D S/S' S Q L + D S' S' Q' D-S-RL L + D S/S' S Q L + D S' S' Q' QAGen 2S D + Q S D Q D + Q' S' D Q' Table  2 : A summary of models (D-S and its variants) we proposed for QA pair generation. Q' and S' denote the question and answer generated during inference, respectively. QAGen 2S  (Shakeri et al., 2020)     reuters.com, foxnews.com, cnn.com, cbsnews.com, nbcnews.com, nydailynews.com . We filtered out articles that use questions as titles, and removed all questions in the articles. In total we collect 7,698 test examples. Unlike validation set, there are no ground truth questions or answers in the test set. 

 Quality of Generated Answers In this section we measure the quality of answers, particularly, whether they answer the corresponding questions. In Table  3 , we show the ROUGE-L score of predicted summaries on the validation set, and QACS and AT-2 accuracy on the test set, resulting in the following observation. Models that generate questions based on answers have higher QACS and AT-2 accuracy than models that generate answers based on questions. Recall that during inference, D-S, D-D, D-S-DRIL and D-S-RL first generate summaries as answers and then generate questions based on the answers (see Table  2 ). These algorithms perform much better than QD-D, CTRLSum, QAGen 2S and QA Transfer which first generate questions and then generate answers to these questions. For example, D-S achieves 51.2%, 39.6%, and 23.4% higher AT-2 accuracy than QAGen 2S in each of the 3 length buckets respectively. This observation is consistent in both QACS and AT-2 accuracy. Meanwhile, QD-D achieves the best ROUGE-L scores while the QACS and AT-2 accuracy are significantly lower than D-S (e.g., AT-2 accuracy is 33.9% lower than D-S in length bucket 0). All these observations show that, to ensure the generated questions and answers match with each other, we should generate questions from answers rather than the opposite direction. This is especially true on our dataset, because the ground truth answers of our dataset are summaries, which are generated without conditioning on the questions (modulo examples generated by the CTRLSum in Section 3). 5.6 Quality of Generated Questions 5.6.1 Results on (SC) 2 QA Dataset In this section, we evaluate the quality of generated questions, particularly, whether the questions capture the gists of articles. From Section 5.5 we already observed that only D-D, D-S, D-S-DRIL, and D-S-RL can generate high quality answers. Therefore, here we only focus on these four models (refer to Appendix C and Section 5.7 for results on other models). The results are shown in Table  4 . We report ROUGE-L/BLEU score of predicted questions on the validation set. Questions are predicted from predicted summaries instead of ground truth summaries, which is consistent with inference on the test set where we also don't have ground truth summaries. We also report AT-5 accuracy on test set and make the following observations. DRIL and RL reinforce AG with question reconstruction loss and thus better reconstruct ground truth questions on validation set and better capture gists of articles on test set.  while, we assume that in our dataset the ground truth questions capture the gists of articles, this means that, by optimizing question reconstruction loss, D-S-DRIL can generate questions that better capture the gists of articles. This is validated by the results on AT-5 accuracy. D-S-DRIL has about 6% and 3% higher AT-5 accuracy than D-S on length bucket 0 and 1, respectively. D-S-DRIL has lower AT-5 accuracy than D-S on length bucket 2, likely because when the maximum allowed summary length is long, there is sufficient information to reconstruct the questions even without the reconstruction loss. D-S-DRIL also shows better performance compared with D-S-RL, indicating the advantage of differentiable question reconstruction loss over the non-differentiable question reconstruction reward. AT-6 shows one article and a list of questions generated by D-D, D-S, D-S-DRIL, and D-S-RL. Annotators select the question that best captures the gist of the displayed article. Figure  3  shows the percentage of each model selected. We can see that questions generated by D-S-DRIL are preferred in length bucket 0 and 1, which is consistent with our results in Table  4 . 5.6.2 (SC) 2 QA v.s. Exiting QA Datasets In this section, we evaluate if (SC) 2 QA is better than existing publicly available QA datasets for QG. We compare with D-S-NewsQA and D-S-NQ. NewsQA and NQ datasets are designed for question-answering but not QG specifically. Similar to (SC) 2 QA, NewsQA is in news domain but without explicitly self-contained questions. For example, the question "what are they going to address?" in the NewsQA dataset is incomprehensible without reading the article due to lack of pronoun resolution. The human evaluation results are shown in Figure  4 , leading to the following observation. QG models trained on NewsQA and Natural Questions cannot generate self-contained questions that capture gists of articles due to the limitations of the datasets, while QG models trained on (SC) 2 QA can. We can see that the QG model trained on NewsQA achieves about 50% lower AT-1 accuracy than the other two models, indicating that it cannot generate self-contained questions. Moreover, QG models trained on NewsQA and Natural Questions achieve 73.55% and 60.03% lower accuracy on AT-5 (averaged over 3 length buckets) compared with the QG model trained on (SC) 2 QA, even though all models generate questions from summaries. We observe that D-S-NewsQA tends to ask trivial questions such as the name of a person. D-S-NQ also fails to identify the focus of a summary. For example, in the summary "Michael Jordan has two brothers and  two sisters. He grew up playing basketball and baseball against his older brother.", D-S-NQ generates "Who is Michael Jordan's brother playing against?". However, the summary focus is Michael Jordan rather than his brother. We discuss such cases further in the qualitative analysis section.  Our (SC) 2 QA dataset performs better than NewsQA and Natural Questions on both AT-1 and AT-5 human evaluation in all three answer length buckets. length bucket 0 length bucket 1 length bucket 2 R-L/BLEU AT-5 Accuracy R-L/BLEU AT-5 Accuracy R-L/BLEU AT- 

 Overall QA Pair Quality We report the joint accuracy of {AT-1, AT-2, AT-5}, defined by the proportion of QA pairs that are answered TRUE for all three ATs and treat it as a metric for the overall QA pair quality, reporting results in Table  5  with the following observations. D-S-DRIL performs significantly better than the best performing baselines. The best performing baselines are QA Transfer in length bucket 0 and QAGen 2S in length bucket 1 and 2. We observe that D-D, D-S, D-S-DRIL and D-S-RL all surpass them by a large margin. Particularly, D-S-DRIL outperforms them by 31.51%, 34.82% and 22.92% in length bucket 0, 1 and 2, respectively. DRIL consistently outperforms RL and MLE. We can see from Table  5  that D-S-DRIL outperforms D-S and D-S-RL by 3.22% and 2.80%, respectively (averaged over 3 length buckets). The results are consistent on human annotations (AT-2 in Table  3 , AT-5 in Table  4 , AT-6 in Figure  3 , and joint accuracy in Table  5 ), and automated metrics (QACS in Table  3  and ROUGE-L/BLEU scores in Table  4 ). This further shows the advantage of DRIL over MLE and RL, indicating that DRIL can efficiently reinforce AG to generate better QA pairs. 

 Qualitative Analysis We also conduct qualitative analyses on generated QA pairs. Please refer to Appendix D for details. 

 Conclusion This paper proposes a model for generating QA pairs with self-contained and summarycentric questions and length-constrained articlesummarizing answers. The target application is suggested questions for conversational news recommendation system. We collect a new dataset, (SC) 2 QA, which contains news articles with questions as titles paired with summaries of varying length. We further propose differential reward imitation learning (DRIL) to efficiently mitigate exposure bias encountered with MLE. Empirically, it is shown that DRIL outperforms multiple alternative baseline neural architectures on automated and human evaluations. 
