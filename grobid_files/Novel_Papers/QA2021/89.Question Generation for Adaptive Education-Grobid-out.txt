title
Question Generation for Adaptive Education

abstract
Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how finegrained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately predicts the probability of a student answering a question correctly, and generalizes to questions not seen in training. We then use LM-KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty. Our results show we succeed at generating novel, wellcalibrated language translation questions for second language learners from a real online education platform.

Introduction Online education platforms can increase the accessibility of educational resources around the world. However, achieving equitable outcomes across diverse learning needs benefits from systems that are adaptive and individualized to each student  (Doroudi and Brunskill, 2019) . Traditionally, adaptive education methods involve planning over a pool of pre-made questions  (Atkinson, 1972; Hunziker et al., 2018) . These are naturally limited by the diversity and coverage of the pool, as well as the scaling capacity of curriculum planning algorithms. Recent approaches, such as procedural generation for personalized programming games  (Valls-Vargas et al., 2017) , are limited to well-specified small domains. We address these limitations by leveraging recent success in deep generative models, in particular language models (LMs). Many educational activities involve sequential data, such as language translation, reading compre-  

 <Y> Figure  1 : Example input and outputs for our LM-based knowledge tracing model (middle) and question generation model (bottom) for an online reverse language translation task (top). A question in this task consists of a target phrase for the student, in this case a Spanish learner, to translate (e.g. "the woman"). hension, algebra, and deductive logic. Meanwhile, pre-trained LMs can effectively handle sequences from a wide range of modalities  (Madani et al., 2020; Polu and Sutskever, 2020) . In this work, we focus on natural language sequences, where recent progress in language modeling has shown great success at capturing abstract properties of language  (Hewitt and Manning, 2019; Liu et al., 2019) . Specifically, we show how pre-trained LMs can be easily leveraged to adaptively generate questions for a given student and target difficulty in a reverse translation task, using difficulty at answering questions as a proxy for more complex future learning objectives. We introduce an LM-based knowledge tracing model (LM-KT) to predict students' difficulty on novel questions (e.g. target phrases to translate). We show that LM-KT is well-calibrated, allowing us to pose the learning problem for the question generator: given a student state, generate a question that will achieve a target difficulty, according to LM-KT. We evaluate both LM-KT and question generation models on real users and responses from Duolingo 1 , a popular online second-language learning platform. 

 Background & Related Works There exists a rich body of work on precisely modeling student "ability" and learning. For example, Item Response Theory (IRT) seeks to model individual student ability based on their responses to different questions, creating a strong factorization between students and test items  (Lord, 1980; Hambelton and Jodoin, 2003) . Meanwhile, Computer Adaptive Testing (CAT) techniques are used to determine a fixed student ability as quickly as possible by selecting test items based on information utility  (Weiss and Kingsbury, 1984; Thissen and Mislevy, 2000; Settles et al., 2020) . However, these methods, which have been used to develop efficient standardized tests, do not necessarily optimize a student's learning experience  (Mu et al., 2018) . We instead focus on tracking each student's evolving knowledge, choosing questions to target difficulty. Knowledge Tracing (KT) seeks to model a student's knowledge state from their answer history in order to help individualize exercise sequences  (Corbett and Anderson, 1995) . This draws inspiration from traditional education curriculum practices, such as distributed spacing of vocabulary  (Bloom and Shuell, 1981)  and mixed review in mathematics  (Rohrer, 2009) . To address simplifying assumptions in earlier KT approaches, such as discrete knowledge representations,  Piech et al. (2015)  introduced Deep Knowledge Tracing (DKT), which uses RNNs to enable more complex knowledge representations for students. Recently, SAINT+  (Shin et al., 2020)  showed state-of-the-art performance on the popular EdNet KT task using a Transformer model to capture temporal information across activities, motivating our use of Transformer LMs. 

 Controllable Text Generation aims to steer LMs towards desired attributes. Examples include using reinforcement learning to control quality metrics  (Ranzato et al., 2016) , adjusting sampling weights to control for poetry style  (Ghazvininejad et al., 2017) , and learning to condition on valence or domain-specific codes  (Keskar et al., 2019; Peng et al., 2018) . To the best of our knowledge, we are 

 Method Given any autoregressive language model (e.g.  GPT-2 (Radford et al., 2019) , we can fine-tune a LM-KT model (p ? KT ) to predict whether an individual student will correctly answer the next question. If this model has well-calibrated uncertainty, we can use its predicted probability of a correct answer as a proxy for the difficulty of a question to a student. We then train a question generation model (p ? QG ) to generate a new question conditioned on a student and desired target difficulty. Question Representation Unlike standard DKT, which treats questions as IDs or simple handcrafted features, we represent questions fully in text (e.g. "she eats" in Figure  1 ). This is a key contribution of our work, required by our eventual goal of generating questions in text, and allows the model to leverage similarity across linguistic features. We thus represent a question q as a sequence of words, with prefix and suffix tokens: q i = <Q> w i 1 w i 2 w i 3 ... w i n <A> Student State We represent a student as a temporally-evolving sequence of questions and their responses. As in much previous KT work, we represent the student response as simply correct/incorrect, with special tokens <Y> and <N>. A student's current state is thus represented as a sequence of all past question and response pairs: s j = q j 1 a j 1 q j 2 a j 2 ... q j m a j m , a i ? {<Y>,<N>} LM-KT Given the sequential nature of student learning over time, we can easily frame knowledge tracing as an autoregressive language modeling task. Given a dataset D of students s 1 , s 2 , ..., s |D| , we employ the standard training objective of finding the parameters ? KT that minimizes L KT = ? |D| i=1 |x (i) | t=1 logp ? KT (x (i) t |x (i) <t ) (1) where x (j) = (x (j) 1 , ...., x (j) |x| ) is the entire sequence tokens corresponding to student s j , consisting of all their past questions and answers. Using the softmax output of the LM-KT model (p ? KT ), we estimate a student's (inverse) difficulty in answering a specific question as d qs = p ? KT (<Y>|s, q). We find that p ? KT is well-calibrated (Section 4.2), yielding a good proxy for the true question difficulty. Question Generation We frame question generation as finetuning a new autoregressive LM. Given random samples of students and questions from a held-out set not used to train LM-KT, we can construct a new dataset D consisting of s i d i <G> q i sequences, where <G> is a special generation token and d i = p ? KT (<Y>|s i , q i ) is the continuous difficulty value assigned by LM-KT. We learn a linear layer to map the continuous input difficulty into a difficulty control vector c d of dimension matching the LM word-embeddings, which we append to the token embeddings. Unlike LM-KT, we train our question generation model p ? QG to minimize the loss only on the question text, which only appears after the<G> token. If t g is the token index of <G>, then our modified loss is: L QG = ? |D | i=1 |x (i) | t=tg+1 logp ? QG (x (i) t |x (i) <t ) (2) where sequence x (j) contains the full s j d j <G>q j sequence. At test time, we generate tokens w 1 ...w n conditioned on the s j d j <G> prefix. 

 Experiments Our method generalizes to any education activity that can be represented with text sequences. Due to the availability of real student learning data, we focus on a reverse language translation task, where a student translates phrases from their native language (e.g. English, "she eats") to the second language they are learning (e.g. Spanish, "ella come"). 

 Experimental Details We use the 2018 Duolingo Shared Task on Second Language Acquisition Modeling  (Settles et al., 2018)  dataset, which contains questions and responses for Duolingo users over the first 30 days of learning a second language. While the original task's goal was to identify token-level mistakes, we collapse these errors into binary (correct / incorrect) per-question labels. We use the provided train/dev/test splits for users learning Spanish and French. We create separate held-out sets from the test set to evaluate the LM-KT and question generation models. For both models, we finetune separate GPT-2  (Radford et al., 2019)    

 Results: Student Modeling We evaluate LM-KT two ways: first, its ability to predict if an individual student will answer a novel question correctly on a held-out test set of real Duolingo student responses. Second, how wellcalibrated these predictions are, which is crucial to our later use of LM-KT for question generation. Table  1  compares AUC-ROC on a held-out test set for our LM-KT model with standard DKT, which uses question IDs instead of text, and a baseline that ignores the student state, only using the question text representation. This question only baseline would perform well if the Duolingo dataset largely consisted of universally "easy" and "difficult" questions, independent of individual student. Our results show that incorporating the student state is crucial for accurately predicting Duolingo user responses, and including question text also leads to a significant improvement. LM-KT outperforms Standard DKT especially on novel questions-a necessary generalization ability for generation. Finally, we measure the calibration of our LM-KT models for both Spanish and French (from En-glish) learners, which is the crucial property for our downstream generation task. We bin our test data by predicted question difficulty, and plot the fraction of true correct answers in each bin. Figure  2  shows that LM-KT is well-calibrated, for both Spanish and French, meaning the predicted difficulty matches the empirically observed proportion of correct answers. 

 Results: Question Generation We evaluate four different aspects of our question generation model: (i) successful control for difficulty, (ii) novelty, (iii) fluency, and (iv) latency. Difficulty Control To explore whether our question generation model indeed depends on target difficulty and the individual student, we first measure the model's perplexity on a held-out test set of Duolingo questions, compared to permutation baselines. Table  2  (top) shows that perplexity is lower for true student / target difficulty inputs than when either or both of these are permuted. The target difficulty values in this analysis were defined by the LM-DKT model. We can remove this dependence by using the actual student responses from Duolingo: we set the target difficulty to 1 if the student was correct and 0 otherwise. Table  2  (bottom) shows our model prefers questions paired with these "true correctness" targets than paired with random ones. To evaluate how well our generation model achieves target difficulties, we take 15 unseen students and generate 30 questions for each of 9 input difficulties (0.1-0.9). We then use LM-KT (a wellcalibrated proxy for true difficulty) to measure the difficulty of these generated questions for each student. Figure  3  shows that we are able to achieve fine-grained control over target difficulty for both Spanish and French students, with an average Root-Mean Squared Error (RMSE) of .052 across all students and target difficulties. Adding a sampling penalty  (Keskar et al., 2019)  increases the variance in difficulty (RMSE .062) in exchange for more novel and diverse questions, as discussed next. Novelty and Fluency By leveraging a pretrained language model's ability to manipulate structure, we can generate novel questions not present in the entire Duolingo question set (See Table  3 ). Across 4,050 questions generated for Spanish learners, we found that with a repetition penalty  (Keskar et al., 2019) , around 43% of all questions, and 66% of high difficulty (d = 0.1) required to rank all questions in the pool, varying its size (Figure  4 ). On one NVIDIA Titan XP GPU, we find that, averaged across all target difficulties, our question generation model takes half the time to achieve the same quality as pool selection. The gap increases when trying to sample harder questions ( d <0.5) -even a pool size of 1000 does not have sufficient difficult questions, likely due to a skew in the Duolingo question set. Additional controls, such as for style or topic, can easily be combined with our generation method, but would make pool selection exponentially more complex. Pool Sampling (all targets) Pool Sampling (difficult targets only) Generation (all targets) Generation (difficult targets only) Figure  4 : Pool selection (for one student) suffers worse question quality vs. latency trade-off than question generation, especially for sampling difficult questions. 

 Conclusion Our work is a first step toward showing that sequence-based models combined with domain knowledge, such as pre-trained LMs, can be leveraged for adaptive learning tasks. We show how to use modern LMs to generate novel reversetranslation questions that achieve a target difficulty, allowing adaptive education methods to expand beyond limited question pools. Limitations of our approach include the compute constraints of large LMs and training data availability. More detailed student data will be crucial to future model development. For instance, while most publicly available education datasets do not include the full student responses (e.g. full translation response in Duolingo), such information could significantly improve the performance of our LM-KT model. Other future directions include exploring non-language domains, such as math or logic exercises, and controlling for auxiliary objectives such as question topic. Finally, designing appropriate user studies to evaluate our method is a complex yet critical next step to determine its suitability in a real-world education setting. Our techniques allows control for individual student difficulty, but it leaves open the question of optimal curriculum design using difficulty-directed question generation. 
