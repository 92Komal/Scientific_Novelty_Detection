title
ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering

abstract
We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve the transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard 1 and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework. 2 * The first three authors contributed equally. This work was conducted during Shuang and Qian's internship at Microsoft Research Asia. 1 https://dki-lab.github.io/GrailQA/ 2 https://aka.ms/ReTraCk

Introduction Knowledge base question answering (KBQA) is an important task in natural language processing that aims to satisfy users' information needs based on factual information stored in knowledge bases. Over the years, it has attracted a great deal of research attention from academia and industry. Early KBQA systems are generally rule-based. They rely on predefined rules or templates to parse questions into logical forms  (Cabrio et al., 2012; Abujabal et al., 2017) , suffering from coverage and scalability problems. Recently, researchers usually focus more on neural semantic parsing approaches. These data-driven parsing methods  (Yih et al., 2015; Jia and Liang, 2016; Dong and Lapata, 2016; Liang et al., 2017; Gu et al., 2021)  significantly improve the state-of-the-art (SOTA) performance on KBQA tasks. Although various neural semantic parsing methods have been proposed for KBQA, there are few works investigating how to leverage the advantages of SOTA models to build a comprehensive system, and how to fit the system with practical application purpose (e.g., balancing effectiveness and efficiency). To investigate, we identify two key issues hindering the development of KBQA systems. On the one hand, there is a lack of a generic and extensible framework for KBQA. For example, the popular SEMPRE 3 toolkit  (Berant et al., 2013)  provides infrastructures to develop statistical semantic parsers for KBQA with rich features, but its performance and scalability are inferior to recent neural semantic parsing methods. The TRANX toolkit 4 (Yin and Neubig, 2018) employs a transition-based neural semantic parser to model the logical form generation procedure as a sequence of tree-constructing actions under grammar specification. However, TRANX does not include the essential retriever components used in grounding, and thus does not support KBQA by now. On the other hand, recent neural semantic parsing methods mostly emphasize performance on benchmark datasets while neglecting the efficiency (speed) dimension. This limits the understanding of how designed approaches fit into real applications. For example, the popular query graph generation methods generate and rank a set of query graphs  (Yih et al., 2015; Maheshwari et al., 2019; Lan and Jiang, 2020) . Since all query graph candidates keep in line with the knowledge base (KB) structure, these methods take full advantage of the KB. However, they suffer from poor efficiency due to the large number of candidates and heavily querying on KB. To verify that, we performed a preliminary study on available SOTA models  5, 6, 7, 8, 9, 10  . According to our study, these models either have difficulties in supporting interactive online services, or limit the candidate space for specific datasets, which makes them difficult to apply in practice. To this end, we present ReTraCk, a practical framework for large scale KBQA. We hope Re-TraCk can help standardize the KBQA model design process and lower the barrier of entry for new practitioners. ReTraCk is designed with the following principles in mind: ? Flexibility ReTraCk employs a modular architecture, which decouples the dependencies among components as much as possible to enable quick integration of novel components. For example, our system supports two different kinds of schema retrievers, namely dense schema retriever and neighbor schema retriever 11 . ? Efficiency ReTraCk falls into the transduction family, which is fast during the generation process. Besides, we retrieve entities and relevant schema items (relations and types) in parallel by leveraging the recent advance of entity linking  (Orr et al., 2021)  and dense retrieval  (Wu et al., 2020; Karpukhin et al., 2020) . Our system can interact with users timely, demonstrating the efficiency of the proposed ReTraCk framework.   (Liang et al., 2017) , ontologylevel checking  (Chen et al., 2018) , real execution  (Wang et al., 2018)  and the novel virtual execution. The experimental results verify the significant effectiveness of our proposed checker. Notably, the checker is also flexible enough to be easily extended with new mechanisms. Finally, ReTraCk achieves state-of-the-art performance on GrailQA and achieves highly competitive performance on WebQuestionsSP. 

 ReTraCk Framework Given an input question q, ReTraCk parses the question into a logical form which can be deterministically converted into a SPARQL query to retrieve answers from the knowledge base K. Generally K consists of two parts: an ontology O ? T ?R?T , which defines the schema structure, and the fact triples F ? E ? R ? (E ? T ? L). Here, T is the set of types, R is the set of relations, E is the set of entities, and L is the set of literals. As shown in Fig.  1 , ReTraCk consists of three components: retriever, transducer and checker. The retriever consists of an entity linker, which links explicit entity mentions to corresponding entities, and a schema retriever, which retrieves relevant schema items (types and relations) mentioned either explicitly or implicitly in the question. Given the retrieved KB items (entities, types, and relations), the transducer employs a grammar-based decoder to generate the logical form with syntax correctness guarantees. Meanwhile, the transducer interacts with the checker to discourage generating programs that are semantically inconsistent with KB. To make ReTraCk more accessible and interpretable for end users, we build a user interface. As shown in Fig.  2 , users can type a question in the text box. The interface then displays retrieved  KB items, a graph visualization of predicted logical forms, generated SPARQL query and predicted answer (s). The schema items selected by our transducer are shaded. Besides, users can refer to more information of any KB item by clicking on the subsequent "Detail". Next, we will introduce each component in detail. 

 Retriever Entity Linker The entity linker used in this work follows the entity linking pipeline described in  Gu et al. (2021) . It firstly detects entity mentions using a BERT-based NER system, then generates candidate entities along with their prior score based on an alias map mined from the KB and FACC1  (Gabrilovich et al., 2013) . As for entity disambiguation, we implement a prior baseline which selects the most popular entity based on the prior score. Besides, we also implement an alternative model by leveraging BOOTLEG  (Orr et al., 2021)  enriched with the prior features 12 . Due to space limitations, the model details and its comparison with the entity linker used in  Gu et al. (2021)  are put in the Appendix. Schema Retriever As schema items are not always mentioned explicitly in the question and their vocabularies are much fewer than entities 13 , we leverage the dense retriever framework  (Mazar? et al., 2018; Humeau et al., 2020; Wu et al., 2020)  to obtain the related types and relations. To be specific, we train a bi-encoder architecture  (Wu et al., 2020)  such that related schema items are close to the question embedding. This architecture allows for fast real-time inference, as it is able to cache the encoded candidates. We use two independent BERT-base encoders  (Devlin et al., 2019)  to represent the input question e q and candidate schema items e s by extracting the upper most layer representation corresponding to the [CLS] token. The matching score for each pair (q g , s i ) is calculated by the dot-product: s(q g , s i ) = e qg ? e s i . (1) Given a question q, we retrieve the top k schema items with the highest scores during inference time. 

 Transducer Following previous work  (Guo et al., 2018 (Guo et al., , 2019 ) -especially the s-expression design principle  (Gu et al., 2021) , we design a set of grammar rules for the logical form. As shown in Table  1 , there are two kinds of grammars in our definition: knowledgeagnostic grammar and knowledge-specific grammar. To incorporate these predefined grammar rules, we introduce a question encoder and a grammar-based decoder  (Liu et al., 2020) . Question Encoder To capture contextual information in a question, we apply a Bidirectional Long Short-Term Memory Neural Network (BiLSTM)  (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997)  as our question encoder. For each   (rel1, rel2) {(e1, e2) | (e1, e) ? rel1 and (e, e2)    token q i in q, we obtain its contextual representation as h E i = [h ? ? E i ; h ? ? E i ], where the forward hidden state h ? ? E i is computed by passing the word embedding of q i into a forward LSTM. The backward hidden state is computed similarly. 

 Grammar-based Decoder Once the question representation is prepared, the grammar-based decoder starts to produce the target logical form step by step with attention on the question. Our decoder regards each logical form as a structure and outputs its corresponding grammar rule/action 14 sequence a = (a 1 , ? ? ? , a K ). At each decoding step, a nonterminal (e.g., set) is expanded using one of its valid grammar rules. For example, at time step k, the LSTM decoder LSTM ? ? D accepts the embedding of the previous output ? a (a k?1 ) as input and updates its hidden state as:  14  We use grammar rule and action interchangeably. h ? ? D k = LSTM ? ? D [? a (a k?1 ); c k?1 ], h ? ? D k?1 , (2) where c k?1 is the context vector obtained by attending on each encoder hidden state h E i . As for ? a , it behaves differently for knowledge-agnostic grammar rules and knowledge-specific grammar rules. For knowledge-agnostic grammar rules, ? a returns a trainable global embedding. For knowledgespecific grammar rules, ? a returns its related KB item representation, obtained by averaging over all word representations. When predicting a k , the probability of selecting the action ? follows: P (a k = ?)?exp ? a (?) tanh([h ? ? D k ;c k ]W o ) , (3) where W o is a learned matrix. BERT Encoding Motivated by the success of pretrained language models on cross-domain textto-SQL tasks  (Hwang et al., 2019) , we augment our model with BERT  (Devlin et al., 2019) . First, we concatenate the questions with all retrieved KB items as input for BERT to strengthen the connection between them. Then, we replace the word embeddings mentioned above with deep contextual representations from the last layer of BERT of each question token and each KB item, respectively. In a case where the total number of words in the retrieved KB items exceeds the maximum length constraint of BERT, we split these KB items into different blocks and encode them with the question separately  (Gu et al., 2021) . 

 Checker Inspired by previous work  (Liang et al., 2017; Chen et al., 2018; Wang et al., 2018) , we design a pluggable module named checker to improve the decoding process by leveraging semantics of KB. Instance-level Checking relies on the KB linkage information at the instance level (i.e., entities and their connected relations), which means that instance-level checking only deals with cases where the current action is a child node of action set? join ent (rel, ent) in the abstract syntax tree (AST). As illustrated in Fig.  4 , when expanding the nonterminal ent, any retrieved KB entity can return a valid grammar rule such as ent?m.04bmk or ent?m.04vd3. However, only m.04vd3 can pass the instance-level checking, since other candidates do not share direct links with the decoded relation tv.tv episode segment.subjects.  Ontology-level Checking performs checking with the help of KB linkage information at the ontology level (i.e., types and bridging relations). Taking the right subtree presented in Fig.  4  as an example, when expanding the second rel, we employ ontology-level checking to determine its valid semantic scope. According to the semantics of the grammar rule set? join rel (rel 1 , rel 2 ), the type set of the head entity in rel 2 must overlap with the type set of the tail entity in rel 1 , by which the candidate rel?tv.tv program.number of episodes is selected. Although ontology-level checking applies to more situations than instance-level, it is weaker in terms of checking effectiveness and needs constraints of high coverage. Real Execution When decoding reaches the end, an action sequence can be converted into a logical form, and finally into a SPARQL query. As depicted in Fig.  4 , the real execution simply takes the final SPARQL query and tries to execute it over KB. If the query cannot be executed successfully, or the result is empty, it means that the corresponding action sequence cannot meet the executable requirement. In practice, we utilize the real execution to check all complete action sequence candidates searched by the beam search procedure, until an action sequence passes checking. with previous work, we use F1 and Hits@1 as evaluation metrics on WebQSP. 

 Virtual Execution 

 Implementation Details We implemented our model based on PyTorch  (Paszke et al., 2019)  and AllenNLP  (Gardner et al., 2018) . With respect to BERT, we utilize the uncased BERT-base model from the Transformers library  (Wolf et al., 2020) . In training, we employed the Adam optimizer  (Kingma and Ba, 2015) . The learning rate is set to 1e-3, except for BERT, which is set to 2e-5. Our model training time on a single Tesla V100 is approximately 20h 16 . As for dense retriever, on GrailQA dataset, we retrieve top-100 type items and top-150 relation items. On WebQSP dataset, we retrieve top-200  16  Due to space limitation, we put the detailed hyperparameters setting in the Appendix. type items and top-500 relation items. 

 Baseline Models We compare our model with previous state-of-theart models on GrailQA  (Lan and Jiang, 2020; Gu et al., 2021)  and WebQSP  (Liang et al., 2017; Sun et al., 2019; Saxena et al., 2020; Lan and Jiang, 2020) . Notably, both TRANSDUCTION and RANKING models proposed by  Gu et al. (2021)  on GrailQA can be based on either GloVe  (Pennington et al., 2014)  or BERT  (Devlin et al., 2019) . We compare with them under all settings. 

 Results We test ReTraCk with two configurations, with or without Checker. As shown in Table  2 , Re-TraCk significantly outperforms the previous SOTA model BERT + RANKING (F1 +7.3, EM +7.5 ) and achieves an improvement (F1 +28.5, EM + 24.8) over the previous best transduction-based model BERT + TRANSDUCTION on GrailQA. Table  3  shows model performance on WebQSP. Given predicted entities, our model outperforms previous models (except for QGG  (Lan and Jiang, 2020) ) and even outperforms these models with oracle entities: GRAFT-Net, PullNet, and Embed-KGQA. Given oracle entities, the performance of our model further boosts to 74.7 F1, which shows the potential gains with a better entity linker. While most SOTA models constrain their answer space by assuming a fixed number of hops, we conduct experiments on both datasets without such assumptions, which simulates real world scenarios. QGG works well on WebQSP by accessing the KB via SPARQL when generating the query graph at each step. However, as noted in  Gu et al. (2021) , extending QGG to consider 3-hops relations on GrailQA will take a few months to train, which is time consuming. It works poorly on GrailQA Golden: (AND exhibitions.exhibition (JOIN (R exhibitions.exhibition curator.exhibitions cu rated) (JOIN exhibitions.exhibition curator.exhibitions curated m.064dsyn))) Query: how is surface density measured in international system of units? Predict: (AND measurement unit.unit of density (JOIN measurement unit.unit of density.measure ment system m.0c13h)) 

 ? Golden: (AND measurement unit.unit of surface density (JOIN measurement unit.unit of surface density.measurement system m.0c13h)) under 2-hop assumption. By removing the checker module, the performance drops 21.1 and 14.1 F1 points on GrailQA and WebQSP respectively, which demonstrates the significant effectiveness of the checker. Except for QGG mentioned above, GrailQA RANKING model takes an average 115.5 seconds 17 to process one query, which is not applicable for online systems. In contrast, ReTraCk takes only 1.62 seconds per query on average at its current implementation which demonstrates its efficiency. 

 Case Study To demonstrate ReTraCk's capability, we show three typical examples from the development set of GrailQA dataset in Table  4 . In the first case, ReTraCk accurately links two mentions (don slater and editor in chief ) in the query to corresponding entities (m.05ws t6 and m.02wk2cy) in Freebase. It also retrieves all necessary schema items (three relations and one type) via schema retriever. The transducer equipped with checker accurately understands the meaning of query and compose the complex logical form with five operators. The predicted logical form is exactly the same as the golden logical form. As for the second case, Re-TraCk parses the query to a logical form which is semantically equivalent to the golden logical form, which demonstrates the existence of program alias. As for the third case, ReTraCk ignores the seman-17 Data are derived from https://github.com/dki-lab/ GrailQA tics conveyed by the word surface in the query, and selects wrong schema item unit of density instead of unit of surface density. This example shows that our model sometimes only captures part of the semantics in the query and misses some span information. 

 Conclusion We present ReTraCk, a semantic parsing framework for KBQA. ReTraCk is flexible and efficient, achieving strong results on two distinct KBQA datasets. We hope that ReTraCk will be beneficial for future research efforts towards developing better KBQA systems.  
