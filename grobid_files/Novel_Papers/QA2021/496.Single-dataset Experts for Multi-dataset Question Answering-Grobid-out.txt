title
Single-dataset Experts for Multi-dataset Question Answering

abstract
Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over-or under-fitting different subdistributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules  (Houlsby et al., 2019)  that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems. 1

Introduction The goal of reading comprehension is to create computer programs that can answer questions based on a single passage of text. Many reading comprehension datasets have been introduced over the years, and prior work has explored ways of training one network on multiple datasets to get a model that generalizes better to new distributions  (Talmor and Berant, 2019; Fisch et al., 2019; Khashabi et al., 2020) . Our goal is to build a multidataset model that performs well on the training distributions and can also serve as a strong starting  1  Our code and models are available at https:// github.com/princeton-nlp/MADE.   (left) . To transfer to a new dataset (right), we either average the parameters of the adapters, or fine-tune the ensemble on the target dataset and take the weighted average at the end (Section 3). point for transfer learning to new datasets. Multidataset training provides a way to model the regularities between datasets but it has the following shortcomings. First, multi-task models are liable to over-or under-fit different tasks  (Gottumukkala et al., 2020) , which can result in worse in-distribution accuracy. Second, given a particular target dataset, multi-dataset models might achieve worse transfer performance compared to a specialized model trained on a more similar source dataset. ? L ? 1 ? 1 i ? 1 j ? 1 k q i q j q k a i a j a k ? L?1 i ? L?1 j ? L?1 k ? i ? j ? k ? ? ? L ? ? 1 ? ? 1 q tgt ? ? L?1 ? ? a tgt Our idea is to combine the benefits of singleand multi-dataset by training a collection of singledataset experts that share an underlying Transformer model (Figure  1 ). This system is based on adapters  (Houlsby et al., 2019) , lightweight task-specific modules interleaved between the layers of a pre-trained Transformer (e.g., BERT; . The standard use of adapters is as a parameter-efficient alternative to fine-tuning: task-specific adapters are trained separately on top of a frozen Transformer, which means the adapters cannot directly learn cross-task regularities. We instead first train a shared Transformer in a multi-adapter setup before refining adapters for individual datasets, which we call Multi-Adapter Dataset Experts (MADE). Our intuition is that the shared parameters encode regularities across different reading comprehension tasks while the adapters model the sub-distributions, resulting in more accurate and robust specialized models that transfer better to a variety of target datasets. We apply this approach to a range of extractive question answering datasets from the MRQA 2019 shared task  (Fisch et al., 2019) , training MADE on six in-domain datasets and evaluating generalization and few-shot transfer learning to six outof-domain datasets. The resulting system outperforms single-and multi-dataset models in terms of in-domain accuracy, and we find that a simple approach to transfer learning works well: averaging the parameters of the MADE adapters results in a single model that gets better zero-shot generalization and few-shot transfer performance compared to both baselines as well as a state-of-theart multi-dataset QA model, UnifiedQA  (Khashabi et al., 2020) . Our experiments illustrate the benefits of modeling both cross-dataset regularities and dataset-specific attributes, and the trained models offer a strong and versatile starting point for new question-answering models. 

 Related Work Prior work has addressed multi-dataset reading comprehension by fine-tuning a pre-trained Transformer language model  simultaneously on examples from multiple datasets  (Talmor and Berant, 2019; Fisch et al., 2019) . Several works explore different multi-task sampling schedules, as a way of mitigating training set imbalances  (Xu et al., 2019; Gottumukkala et al., 2020) . Another line of work focuses on training models to answer a wider variety of question types, including UnifiedQA  (Khashabi et al., 2020) , a T5 model  (Raffel et al., 2020)  trained on datasets with different answer formats, such as yes/no and multiple-choice, using a unified text-to-text format. Adapters  (Houlsby et al., 2019; Rebuffi et al., 2018)  are task-specific modules interleaved between the layers of a shared Transformer.  Stickland and Murray (2019)  trained task adapters and the Transformer parameters jointly for the GLUE benchmark  but achieved mixed results, improving on small datasets but degrading on larger ones. Subsequent work has used a frozen, pre-trained Transformer and trained task adapters separately. Researchers have explored different methods for achieving transfer learning in this setting, such as learning to interpolate the activations of pre-trained adapters  (Pfeiffer et al., 2021) . 

 Method 

 Problem Definition The objective of reading comprehension is to model the distribution p(a | q, c), where q, c, a ? V * represent a question, supporting context, and answer respectively and consist of sequences of tokens from a vocabulary V. For simplicity, we focus on extractive reading comprehension, where every question can be answered by selecting a span of tokens in the context, but the approach is generic and can be extended to other formats. We make the standard assumption that the probability of context span c i...j being the answer can be decomposed into the product of p(start = i | q, c) and p(end = j | q, c). We consider a collection of source datasets S and target datasets T , where each dataset D ? S ? T consists of supervised examples in the form (q, c, a). The goal is to train a model on S that achieves high in-domain accuracy and transfers well to unseen datasets in T , either zero-shot or given a small number of labeled examples. 

 Multi-dataset Fine-tuning The standard approach to multi-dataset reading comprehension is to fit a single model to examples drawn uniformly from the datasets in S: arg min ?,? E D i ?S [E q,c,a?D i [? log p ?,? (a | q, c)]] , where ? refers to the parameters of an encoder model (usually a pre-trained Transformer like BERT; , which maps a question and context to a sequence of contextualized token embeddings, and ? denotes the classifier weights used to predict the start and end tokens. The objective is approximated by training on mixed mini-batches with approximately equal numbers of examples from each dataset  (Fisch et al., 2019; Khashabi et al., 2020) , although some researchers have investigated more sophisticated sampling strategies  (Xu et al., 2019) . For example,  Gottumukkala et al. (2020)  introduce dynamic sampling, sampling from each dataset in inverse proportion to the model's current validation accuracy.   2020 ) on the MRQA datasets. We compare MADE results at the end of joint optimization (w/o adapter tuning), and after freezing the Transformer and tuning the adapters separately (w/ adapter tuning). See Section 4.2 for details. 

 MADE Our approach is to explicitly model the fact that our data represent a mixture of datasets. We decompose the model parameters into a shared Transformer, ?, and dataset-specific token classifiers ? = ? 1 , . . . , ? |S| and adapters ? = ? 1 , . . . , ? |S| (Figure  1 ). We use a two-phase optimization procedure to fit these parameters. In the joint optimization phase, we jointly train all of the parameters on the source datasets: arg min ?,?,? E D i ?S [E q,c,a?D i [? log p ?,? i ,? i (a | q, c)]] After validation accuracy (average F1 scores of the source datasets) stops improving, we freeze ? and continue adapter tuning, refining each pair of (? i , ? i ) separately on each dataset. 

 Zero-shot generalization We use a simple strategy to extend MADE to an unseen dataset: we initialize a new adapter and classifier (? , ? ) by averaging the parameters of the pre-trained adapters and classifiers ? 1 , . . . , ? |S| and ? 1 , . . . , ? |S| , and return the answer with the highest probability under p ?,? ,? (a | q, c). Alternatively, we also consider an ensemble approach, averaging the token-level probabilities predicted by each adapter (this is less appealing as we need to run the model |S| times). Transfer learning We also consider a transfer learning setting, in which a small number of labeled examples of a target domain (denoted D tgt ) are provided. We explore two ways to build a single, more accurate model. The first is to initialize (? , ? ) as a weighted average of pre-trained adapters, ? = 1 |S| |S| i=1 ? i ? i , and classifiers ? = 1 |S| |S| i=1 ? i ? i , using D tgt to estimate the mixture weights. For each i, we set the mixture weight ? i to be proportional to the exponential of the negative zero-shot loss on the training data: ? i ? exp E q,c,a?Dtgt [log p ?,? i ,? i (a | q, c)] , and then tune ? and (? , ? ) on the target dataset. The second approach is to first jointly tune ?, ?, and ? on D tgt , maximizing the marginal likelihood: E q,c,a?Dtgt ? ? log 1 |S| |S| i=1 p ?,? i ,? i (a | q, c) ? ? , and then take the weighted average of the parameters, calculating the mixture weights ? i as above but using the loss of the fine-tuned adapters on a small number of held-out examples from D tgt . After training, both approaches result in a single model that only requires running one forward pass through (?, ? , ? ) to make a prediction. 

 Experiments 

 Setup We use the datasets from the MRQA 2019 shared task  (Fisch et al., 2019) , which are split into six large in-domain datasets, 2 and six small out-ofdomain datasets.  3  Dataset statistics are in Appendix A.1. We use the RoBERTa-base model  with the default adapter configuration from  Houlsby et al. (2019) , which adds approximately 1.8M parameters to the ~128M in RoBERTa-base (1%).   Single-dataset adapters / zero-shot F1  B i o A S Q D R O P D u o R C R A C E R e l E x T e x t b o o k Q A A v g . B i o A S Q D R O P D u o R C R A C E R e l E x T e x t b o o k Q A A v g . 

 In-domain Performance First we train MADE on the six training datasets and compare in-domain accuracy with single-and multi-dataset fine-tuning and standard adapter training (freezing the Transformer parameters). For context, we also compare with a method from recent work, dynamic sampling  (Gottumukkala et al., 2020) , by sampling from each dataset in proportion to the difference between the current validation accuracy (EM+F1) on that dataset and the best accuracy from single-dataset training. We train all models by sampling up to 75k training and 1k development examples from each dataset, following  Fisch et al. (2019) . More details are in Appendix A.2. Table  1  shows that MADE scores higher than both single-and multi-dataset baselines. Both phases of MADE training-joint optimization followed by separate adapter tuning-are important for getting high accuracy. Jointly optimizing the underlying MADE Transformer improves performance compared to single-dataset adapters, sug-gesting that joint training encodes some useful cross-dataset information in the Transformer model. Adapter tuning is important because the multidataset model converges on different datasets at different times, making it hard to find a single checkpoint that maximizes performance on all datasets (see Appendix Figure  3  for the training curve). Some of the improvements can also be attributed to the adapter architecture itself, which slightly outperforms fine-tuning in most datasets. Dynamic sampling does not improve results, possibly because the datasets are already balanced in size. 

 Zero-shot Generalization Table  2  shows the results of applying this model to an unseen dataset (zero-shot). We compare two methods for using MADE-averaging the parameters or ensembling the predictions-with the multi-dataset model from Section 4.2, an ensemble of single-dataset adapters, and the pre-trained UnifiedQA-base  (Khashabi et al., 2020) .  4  We compare MADE with and without the second phase of separate adapter-tuning. Surprisingly, averaging the parameters of the different MADE adapters results in decent predictor. Parameter averaging works best for MADE without adapter-tuning, possibly because the parameters diverge during adapter tuning and no longer interpolate well.  5  Ensembling the separately-tuned MADE adapters gives the best performance, at an additional computational cost. Figure  2  compares the zero-shot accuracy obtained by the different MADE and single-dataset adapters. The two sets of adapters show similar patterns, with some adapters generalizing better than others, depending on the target, but all of the MADE adapters generalize better than the corresponding single-dataset adapters. This performance gap is considerably bigger than the gap in in-domain performance (Table  1 ), further illustrating the benefit of joint optimization. 

 Transfer Learning Finally, we compare two ways of using MADE for transfer learning: either averaging the adapter parameters and then fine-tuning the resulting model (pre avg.), or first fine-tuning all of the adapters and then taking the weighted average (post avg.). 4 UnifiedQA was trained on different datasets with a different architecture, but represents an alternative off-the-shelf model for QA transfer learning. We compare to UnifiedQAbase because the encoder has approximately the same number of parameters as RoBERTa-base.  5  We also tried applying this approach to single-dataset adapters but got an average F1 of 7%. In both cases, we also back-propagate through the Transformer parameters. We reserve 400 examples from each target dataset to use as a test set (following  Ram et al., 2021)  and sample training datasets of different sizes, using half of the examples for training and the other half as validation data for early stopping and to set the mixture weights for averaging the adapter parameters. The results are in Table  3 . On average, MADE leads to higher transfer accuracy compared to the baselines, especially for the smaller sizes of datasets, showing that an ensemble of robust singledataset experts is a good starting point for transfer learning. The post-average method performs about the same as averaging at initialization in the lower-data settings, and better with K = 256. All models struggle to learn on two datasets, DuoRC and TextbookQA, which have long contexts and distant supervision, which might represent more challenging targets for few-shot learning. We also experimented with single-dataset adapters and with a frozen Transformer, which perform worse; detailed results are in Appendix B.2. 

 Conclusion MADE combines the benefits of single-and multidataset training, resulting in better in-domain accuracy and transfer performance than either multidataset models or ensembles of single-dataset models, especially in low resource settings. For future work we plan to explore ensembling methods for better zero-shot prediction and interpolating MADE weights for better transfer learning. 
