title
Multiplex Graph Neural Network for Extractive Text Summarization

abstract
Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity & natural connection), nor model intra-sentential relationships (e.g, semantic & syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.

Introduction Numerous documents from a variety of sources are uploaded to the Internet or database everyday, such as news articles  (Hermann et al., 2015) , scientific papers  (Qazvinian and Radev, 2008)  and electronic health records  (Jing et al., 2019) . How to effectively digest the overwhelming information has always been a fundamental question in natural language processing  (Nenkova and McKeown, 2011) . This question has sparked the research interests in the task of extractive text summarization, which aims to generate a short summary of a document by extracting the most representative sentences from it. Most of the recent methods  (Cheng and Lapata, 2016; Narayan et al., 2018; Luo et al., 2019; Wang et al., 2020a; Mendes et al., 2019;  formulate the task of extractive text summarization as a sequence labeling task, where the labels indicate whether a sentence should be included in the summary. To extract sentence features, existing approaches generally use Recurrent Neural Networks (RNN)  (Yasunaga et al., 2017; Nallapati et al., 2017; Mendes et al., 2019; Luo et al., 2019; Cheng and Lapata, 2016) , Convolutional Neural Networks (CNN)  (Cheng and Lapata, 2016; Luo et al., 2019; Narayan et al., 2018)  or Transformers  (Zhong et al., 2019; Liu and Lapata, 2019a) . Endeavors have been made to develop models to capture various sentence-level relations. Early studies, such as LexRank  and TextRank  (Mihalcea and Tarau, 2004) , built similarity graphs among sentences and leverage PageRank  (Page et al., 1999)  to score them. Later, graph neural networks e.g., Graph Convolutional Network (GCN) (Kipf and Welling, 2016) have been adopted on various inter-sentential graphs, such as the approximate discourse graph  (Yasunaga et al., 2017) , the discourse graph  (Xu et al., 2020)  and the bipartite graph between sentences and words  (Wang et al., 2020a; Jia et al., 2020) . Albeit the effectiveness of the existing methods, there are still two under-explored problems. Firstly, the constructed graphs of the existing studies only involve one type of edges, while sentences are often associated with each other via multiple types of relationships (referred to as the multiplex graph in the literature  (De Domenico et al., 2013; Jing et al., 2021a) ). Two sentences with some common keywords are considered to be naturally connected (we refer this type of graph as the natural connection graph). For example, in Figure  1 , the first and the last sentence exhibit a natural connection (green) via the shared keyword "City". Although the two sentences are far away from each other, they can be jointly considered as part of the summary since the entire document is about the keyword "City". However, such a relation can barely be captured by traditional encoders such as RNN and CNN. Two sentences sharing similar meanings are also considered to be connected (we refer this type of graph as the semantic graph). In Figure  1 , the second and the third sentence are semantically similar since they express a similar meaning (yellow). The semantic similarity graph maps the semantically similar sentences into the same cluster and thus helps the model to select sentences from different clusters, which could improve the coverage of the summary. Different relationships provide relational information from different aspects, and jointly modeling different types of edges will improve model's performance  Park et al., 2020; Jing et al., 2021b; Yan et al., 2021; Jing et al., 2021c) . Secondly, the aforementioned methods fall short in taking advantage of the valuable relational information among words. Both of the syntactic relationship  (Tai et al., 2015; He et al., 2017)  and the semantic relationship among words  (Kenter and De Rijke, 2015; Wang et al., 2020b; Varelas et al., 2005; Wang et al., 2021;  have been proven to be useful for the downstream tasks, such as text classification  (Kenter and De Rijke, 2015; Jing et al., 2018) , information retrieval  (Varelas et al., 2005 ) and text summarization . We summarize our contributions as follows: ? To exploit multiple types of relationships among sentences and words, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN). 

 ? Based on Multi-GCN, we propose a Multiplex Graph based Summarization (Multi-GraS) framework for extractive text summarization. ? We evaluate our approach and competing methods on the CNN/DailyMail benchmark dataset and the results demonstrate our models' effectiveness and superiority. 

 Methodology We first present Multi-GCN to jointly model different relations, and then present the Multi-GraS approach for extractive text summarization. 

 Multiplex Graph Convolutional Network Figure  2c  illustrates Multi-GCN over a multiplex graph with initial node embedding X and a set of relations R. Firstly, Multi-GCN learns node embeddings H r of different relations r ? R separately, and then combines them to produce the final embedding H. Secondly, Multi-GCN employs two types of skip connections, the inner and the outer skip-connections, to mitigate the over-smoothing  (Li et al., 2018)  and the vanishing gradient problems of the original GCN (Kipf and Welling, 2016). More specifically, we propose a Skip-GCN with an inner skip connection to extract the embeddings H r for each relation. The updating functions for the l-th layer of the Skip-GCN are defined as: ?(l) r = GCN (l) r (A r , H (l?1) r ) + H (l?1) r (1) H (l) r = ReLU( ?(l) r W (l) r + b (l) r ) (2) where A r is the adjacency matrix for the relation r; W (l) r and b (l) r denote the weight and bias. Note that H (0) r = X is the initial embedding, and H r is the output after all Skip-GCN layers. Next, we combine the embedding of different relations {H r } r?R by the following equations: H = tanh(cat({H r } r?R )W + b) (3) where cat denotes the concatenation operation and W and b denote the weight and bias of the project block in Figure  2c . Finally, we use an outer skip connection to directly connect X with H: H = H + X (4) 

 The Multi-GraS model The overview of the proposed Multi-GraS is illustrated in Figure  2a . Multi-GraS is comprised of three major components: the word block, the sentence block, and the sentence selector. The word block and the sentence block share a similar "Initialization -Multi-GCN -Readout" structure to extract the sentence and document embeddings. The sentence selector picks the most representative sentences as the summary based on the extracted embeddings.  

 The Word Block The architecture of a word block is illustrated in Figure  2b .  

 The Sentence Block Given a document with M sentences {s m } M m=1 , the sentence block takes the sentence embeddings {e sm } M m=1 as inputs, and generates a document embedding d through a Bi-LSTM, a Multi-GCN and a pooling module. Essentially, the architecture of the sentence block resembles the word block, thus we only elaborate the construction of graphs for sentences. In this paper, we consider the natural connection and the semantic relation between sentences. The semantic similarity between s m and s m is the ab-solute value of dot product between x sm and x s m , and thus the semantic similarity graph A sems can be constructed by A sems [m, m ] = |x T sm ? x s m |. For the natural connection, if two sentences share a common keyword, then we consider they are naturally connected. Such a relation helps to cover more sections of a document by connecting faraway sentences (not necessarily semantic similar) via their shared keywords, as shown in Figure  1 . A nat [m, m ] = w?W tfidf (sm,w) ? tfidf (s m ,w) , (5) where tfidf (sm,w) is the tfidf score of the keyword w within s m ; W is the set of keywords. 

 Sentence Selector The sentence selector first scores the sentences {s m } M m=1 and then selects the top-K sentences as the summary. The model design for scoring the sentences follows the human reading comprehension strategy  (Pressley and Afflerbach, 1995; Luo et al., 2019) , which contains reading and postreading processes. The reading process extracts rough meaning of s m : o m = tanh(W Reading h sm + b Reading ). (6) The post-reading process further captures the auxiliary contextual information -document embedding e d and the initial sentence embedding e sm : o m = tanh(W P ost [o m , e d , e sm ] + b P ost ). (7) The final score for s m is given by: p m = ?(W p o m + b p ), (8) where ?() denotes the sigmoid activation. When ranking the sentences {s m } M m=1 , we follow  (Paulus et al., 2018; Liu and Lapata, 2019b; Wang et al., 2020a)  and use the tri-gram blocking technique to reduce the redundancy. 

 Experiments 

 Experimental Setup 

 Datasets We evaluate our propose model on the benchmark CNN/DailMail  (Hermann et al., 2015)  dataset. This dataset is a combination of the CNN and Daily-Mail datasets, which contains 287, 227, 13, 368 and 11, 490 articles for training, validating and testing respectively. For the DailyMail dataset  (Hermann et al., 2015) , the news articles were collected from the DailyMail website. Each article contains a story and highlights, and the story and highlights are treated as the document and the summary respectively. The dataset contains 219, 506 articles, which is split into 196, 961/12, 148/10, 397 for training, validating and testing. For the CNN dataset  (Hermann et al., 2015) , the news articles were collected from the CNN website. Each article is comprised of a story and highlights, where the story is treated as the document and highlights are considered as the summary. The CNN dataset contains 92, 579 articles in total, 90, 266 are used for training, 1, 220 for validation and 1, 093 for testing. 

 Comparison Methods For the task of extractive text summarization, we compare the proposed Multi-GraS method with the following methods in three categories: (1) deep learning based methods: NN-SE  (Cheng and Lapata, 2016) , LATENT  (Zhang et al., 2018) , NeuSUM , JECS  (Xu and Durrett, 2019)  and EXCONSUMM Extractuve  (Mendes et al., 2019) ; (2) reinforcement-learning based methods: RE-FRESH  (Narayan et al., 2018) , BanditSum  (Dong et al., 2018) , LSTM+PN+RL  (Zhong et al., 2019)  and HER  (Luo et al., 2019) ; (3) graph based methods: TextRank  (Mihalcea and Tarau, 2004)  and HSG  (Wang et al., 2020a) . 

 Implementation Details The vocabulary size is fixed as 50, 000 and the pre-trained Glove embeddings  (Pennington et al., 2014)  are used for the input word embeddings. For both of the word block and the sentence block, the Initialization modules employ two-layer Bi-LSTMs. The Multi-GCN modules use two-layer Skip-GCNs. We fix all the hidden dimensions as 300. We use the Stanford CoreNLP   natural connection graphs, we filter out the stop words, punctuation, and the words whose document frequency is less than 100. During training, we use the Adam optimizer (Kingma and Ba, 2014), and the learning rates for CNN, DailyMail, and CNN/DailyMail datasets are set to be 0.0001, 0.0005, and 0.0005, respectively. When generating summaries, we select the top-2 and top-3 sentences for the CNN and DailyMail datasets, respectively. 

 Oracle Label Extraction The summaries of the documents are the highlights of the news written by human experts, hence the sentence-level labels are not provided. Given a document and its summary, we follow  (Wang et al., 2020a; Liu and Lapata, 2019b; Mendes et al., 2019; Narayan et al., 2018)  to identify the set of sentences (or oracles) of the document which has the highest ROUGE scores with respect to its summary. 

 Evaluation Metrics We evaluate the quality of the summarization by the ROUGE scores  (Lin, 2004) , including R-1, R-2 and R-L for calculating the unigram, bigram and the longest common sub-sequence overlapping between the generated summarization and the groundtruth summary. In addition to automatic evaluation via ROUGE, we follow  (Luo et al., 2019; Wu and Hu, 2018)  and conduct human evaluation to score the quality of the generated summaries. 

 Overall Performance The ROUGE  (Lin, 2004)  scores of all comparison methods are presented in Table  1 . Within baseline methods, HSG achieves the highest performance, which indicates that considering graph structures could improve performance. We also observe that Multi-GraS outperforms all of the comparison methods and it achieves 0.21/0.38/0.26 performance increase on R-1/R-2/R-L scores.  

 Ablation Study Firstly, as shown in Table  2 , tri-gram blocking and contextual information within the sentence selector help improve model's performance. Then we study the influence of the Multi-GCN within the sentence block and the word block separately. To do so, we remove the Multi-GCN within the sentence block (Multi-GraS word ) and further remove the Multi-GCN within the word block (LSTM). By comparing LSTM, Multi-GraS word and Multi-GraS, it can be observed that Multi-GCN in both sentence and word blocks significantly improve the performance. Next, we study the influence of the components within Multi-GCN. Table  2  indicates that the inner and outer skip connections play an important role in Multi-GCN. Besides, jointly considering different relations is always better than considering one relation alone. Finally, for the Initialization module in the word and sentence blocks, LSTM performs better than Transformer  (Vaswani et al., 2017) . 

 Human Evaluation We randomly select 50 documents along with the summaries obtained by HSG, Multi-GraS, Multi-GraS word , LSTM as well as the oracle summaries. Three volunteers (proficiency in English) rank the summaries from 1 to 5 in terms of the overall quality, coverage and non-redundancy. The human evaluation results are presented in Table  3 : oracle ranks the highest, Multi-GraS ranks higher than HSG.     

 Sensitivity Experiments To check the performance on the number of selected sentences, we conduct a sensitivity experiment for both CNN and DailyMail datasets. The results in Figure  3  show that the Multi-GraS performs the best when the number of the selected sentences is 2 for the CNN dataset and 3 for the DailyMail dataset. 

 Conclusion In this paper, we propose a novel Multi-GCN to jointly model multiple relationships among words and sentences. Based on Multi-GCN, we propose a novel Multi-GraS model for extractive text summarization. Experimental results on the benchmark CNN/DailyMail dataset demonstrate the effectiveness of the proposed methods. Figure 1 : 1 Figure 1: An example document: There are two different relationships among sentences: the semantic similarity (yellow) and the natural connection (green). Sentences 2, 3, 21 are the oracle sentences. 
