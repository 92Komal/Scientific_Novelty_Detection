title
Enriching and Controlling Global Semantics for Text Summarization

abstract
Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.

Introduction Automatic text summarization corresponds to text understanding and text generation processes. In general, there are two main approaches to perform this task. Extractive systems  (Liu, 2019; Narayan et al., 2020; Jia et al., 2020)  highlight salient words or sentences from the source text and form the final summary by concatenating them. On the other hand, abstractive methods  (See et al., 2017; Zou et al., 2020)  switch among generating new words, choosing phrases from the source document, and rephrasing them. Abstractive summarization, which is the focus of this paper, is usually more advanced and closer to human-like interpretation. Recently, abstractive summarization studies  (Lewis et al., 2019; Chen and Yang, 2020)  are dominated by Transformer-based DOCUMENT: While Richie Benaud rose from the suburbs to captain Australia, he will be remembered forever for his mastery of commentating. The champion leg spinner turned cricket commentating into an art form, earning him the title of 'the Voice of Cricket.' His commentary was understated, measured and often extremely funny, and were perfectly timed. Scroll down for video. 84-year-old cricket commentator Richie Benaud has passed away after a battle with skin cancer . His sayings from the hundreds of Test and One Day cricket matches he commentated on across the world were often what fans remembered from important moments. His signature one liners soon dropped to a simple word. 'Marvellous...' will forever be linked to the cricket legend. On commentating, Benaud said: 'My mantra is -put your brain into gear and if you can add to what's on the screen then do it, otherwise shut up.' He once described the scene on the field: 'From our broadcast box you can't see any grass at all, it is simply a carpet of humanity.' On captaincy, and he was one of the best Test captains Australia ever had, Benaud was modest: 'The hallmark of a great captain is the ability to win the toss, at the right time.' The former leg spinner turned cricket commentating into an art form, giving him the title 'the Voice of Cricket'. But he cautioned that description with: 'Captaincy is 90 per cent luck and 10 per cent skill. But don't try it without that 10 per cent.' [...] GOLD SUMMARY: Cricket commentator Richie Benaud has passed away after cancer battle . The 84-year-old will be remembered for his mastery of commentating . The former leg spinner earned himself the title of the 'Voice of Cricket'. His trademark line was 'Marvellous'. PEGASUS: The champion leg spinner turned cricket commentating into an art form, earning him the title of 'the Voice of Cricket'. His commentary was understated, measured and often extremely funny, and were perfectly timed. Our model: 84-year-old cricket commentator Richie Benaud has passed away after a battle with skin cancer. The champion leg spinner earned the title of 'the Voice of Cricket'. His commentary was understated, measured and often extremely funny. His trademark word, 'Marvellous...' will forever be linked to the cricket legend. Table  1 : An example of summarization outputs. architecture  (Vaswani et al., 2017) . Despite good performance in large scale datasets, Transformerbased summarization models have been proven to have the tendency to favor encoding short-range dependencies , i.e., whenever there is one word from the input generated in the summary, the model tends to continue generating the nearby words due to their high attention scores to the previous generated word. As such, if the main content of the document is out of reach from the generated word, the final summary can miss that key information. For example, in Table  1 , PEGASUS, a state-of-the-art Transformed-based model, failed to capture one key information of the document, i.e., "84-year-old cricket commentator Richie Benaud has passed away after a battle with skin cancer". To understand this phenomenon, we visualize the attention scores in the model during the generation process. As shown in Figure  1 , when the model generates "commentary", the main subject of the blue sentence, it tends to point to and generate nearby words such as "his", "understated", "funny", etc. due to their high attention scores while words in the further range such as "Richard", "Benaud", "pass", and "away" receive little weight. Consequently, although PEGASUS generates a grammatically correct summary, the summary lacks the key content which describes the death of "Richie Bernaud". To avoid missing key points in summarizing, one solution is to furnish the models with global semantics by using probabilistic topic models such as LDA  (Narayan et al., 2018) , Poisson factor analysis , or inner hidden states . Nevertheless, traditional topic models were shown to be inefficient in scalability for large-scale datasets  (Hoffman et al., 2013; Rezende and Mohamed, 2015)  and have limited capability of describing documents  (Ding et al., 2018) . To overcome the above problems, we propose a novel method that integrates neural topic model into summarization architecture. Specifically, we aim to utilize the posterior distribution learned from the neural topic model as an approximation of global semantics of the document and from that, provide a signal for summarization model to have a better understanding of overall document. However, there is one critical question: how can we match the neural topic model's posterior distribution with the true posterior as it has been proven in improving the performance of variational inference  (Rezende and Mohamed, 2015) ? To this end, we propose a method to adapt normalizing flow in the neural topic model to have a better approximation of true distribution and integrate it into the summarization model. Integrating flow mechanism to better approximate the true posterior has been proven to improve performance for variational inference  (Rezende and Mohamed, 2015)  as well as for downstream tasks such as image synthesis  (Kingma et al., 2016) , etc. However, to the best of our knowledge, there is no study to investigate the benefit of flow mechanism for the abstractive summarization task. On the other hand, even though rich global semantics is beneficial, there are recent studies showing that the redundant amount of global semantics may cause harm to hidden representation since it introduces detrimental noise to the model  (Tenney et al., 2019; Li et al., 2020) . Therefore, we propose a novel contextualized gating mechanism to control the flow of global semantics and maintain important information of the hidden states in the main summarization model. The contributions of our paper can be summerized as follows: ? We propose a novel architecture which takes the global semantics into consideration when performing abstractive summarization. 2 Related Work 

 Transformer-based Text Summarization Transformer  (Vaswani et al., 2017)  and its variants have demonstrated high efficiency in text summarization.  (Liu and Lapata, 2019)  first use to perform extractive summarization.  (Zhong et al., 2020)  propose using Siamese BERT to score among summaries extracted from the source document, exploring the rich semantic space that those summaries are projected onto.  (Narayan et al., 2020)  combine HiBERT and structured transformers to extract the document incrementally to form the final summary. For abstractive approaches, ) develop a pretraining scheme well-suited for abstractive summarization. Other frameworks uniting language understanding and text generation such as BART  (Lewis et al., 2019) , UniLM  (Dong et al., 2019) , T5  (Raffel et al., 2019) ,  (Tuan et al., 2020) , and MASS  (Song et al., 2019)  provide further standards for future works. Unified system such as BottomUp  (Gehrmann et al., 2018)  extracts salient phrases and then generates the summary based upon the extracted content.  (Subramanian et al., 2019)  further improve with their decoder as a Transformer language model. 

 Topic-aware Summarization Models Various works integrate global semantics of topic model into the sequential information. One method is to attend topic vectors with the hidden states, only choosing entries with high document-level representations  (Zheng et al., 2020) .  design three modules to incorporate topic information to attentive heads, provide topical embedding, and form document-related representations. Other works integrate topical information into convolutional-based models  (Narayan et al., 2018; Wang et al., 2018) .  Ailem et al. 2019  have their pointer-generator conditioned on both the input document and the latent vector.  (Fu et al., 2020)  study how to effectively assist deep-learning summarization frameworks with external global information. Arguing that each paragraph in the document possesses a separate subtopic, they propose to merge topic information hierarchically with the dense word embedding. Unfortunately, there is still limited effort controlling the effect of global semantics on the contextualized representations and enriching the global semantics for summarization performance. 

 Methodology The overall architecture of our approach is given in Figure  2 . It comprises of a topic-oriented encoder, a topic-oriented decoder, and a flow-based neural topic model. Formally, given a document as input, we process it into a sequence of tokens X = {x i }, and the bagof-word (BoW) representation x bow . X is taken as the input for the text summarization module, while x bow serves as the input for the neural topic model. 

 Flow-based Neural Topic Model The architecture of the neural topic model (NTM) takes inspiration from  (Miao et al., 2017)  based on variational autoencoder  (Kingma and Welling, 2013) . In this work, we adapt the normalizing flow to the neural topic model to better grasp the global semantic patterns of the document. BoW Encoder. In particular, the input x bow is first encoded into a latent variable z by a topic encoder. Each input is passed to obtain the prior mean ? and prior standard deviation ? ? = f M LP (x bow ), ? = f 1 (?), log ? = f 2 (?) (1) where f M LP is a non-linear transformation with a tanh activation function; f 1 and f 2 are two linear transformations with bias. To obtain the topic distribution, we draw the latent variable z ? N (?, ? 2 ). Flow. Different from conventional neural topic model, a flow is applied to map the latent vector to a more complicated distribution. Formally, flow is a chain of transformations f 1 , f 2 , . . . , f K which are all invertible and have the Jacobian easy to compute. z K = f 0 ? f 1 ... ? f K (z) (2) BoW Decoder. Given the new topic vector, the BoW decoder retains the original input x bow by generating x bow . We take the following procedure to simulate the reconstruction of x bow ? Topic mixture ? = softmax(f ? (z K )) ? For each word w ? x bow , draw w ? softmax(f ? (?)) where f * (?) is a ReLU-activated non-linear transformation. The weight matrix of f ? is chosen as the topic-word distribution (? 1 , ? 2 , ..., ? K ). We proceed to employ the topic mixture ? to guide the text summarization process. 

 Neural Topic Modeling for Transformer Text summarization model is passed a source document X = {x i } N i=1 and its task is to predict the target summary Y = {y j } M j=1 . In this setting, the document D has N tokens and the summary S has M tokens (M < N ). Decoder S' Y S H' Encoder X H VAE Encoder ? ? f 1 VAE Decoder Flow-based Neural Topic Model f 2 f K x bow x' bow 

 Gating Gating 

 Topicoriented Encoder 

 Topicoriented Decoder Figure 2: Our overall architecture Our model inherits the Transformer-based architecture. Particularly, it consists of an encoder and decoder. The encoder learns the context of the source text, and the decoder then predicts the target summary, by learning the context of the generated tokens and attending over encoder hidden states. In our case, we make both the encoder and decoder conditioned on the latent topic yielded by the neural topic model. Topic-oriented Encoder We add the special token "CLS" to the beginning of the input. At each iteration, the encoder outputs a localized representation H = {h i } N i=1 for each token in the source docu- ment X h CLS , h 1 , ..., h N = Encoder(x CLS , x 1 , ..., x N ) (3) This explores the relationship among the input tokens (the encoder), or discovering the context each token stays in. We relate the context of each word to the main topic of the document by modulating the i-th hidden state h i h i = g(h i , ?) (4) where g is a function used to introduce the global semantics to the hidden representations which we will discuss later as the contextualized gating mechanism in section 3.3 Topic-oriented Decoder We also make "CLS" the first input of the decoder. The decoder bridges the summary Y and document X, creating target hidden states S = {s j } M j=1 aligned with the source text. Because of the uni-directionality of the text summarization task, the decoder must work in a left-to-right fashion s j = Decoder(y CLS , y 1 , y 2 , ..., y j?1 , {h i } N i=1 ) (5) Similar to the Encoder, we seek to inject the semantics of the topic model into the output hidden state. s j = g({h i } N i=1 , s j , ?) (6) 

 Contextualized Gating Mechanism Because a specified amount of semantic meaning, whether it is local or global, has been embedded in the contextualized representations, it is reasonable to only append sufficient information to the calculated hidden states to maximize the efficiency of the topical information. We adapt the gating mechanism of  (Cho et al., 2014)  to achieve this goal. In our contextualized gating mechanism, we approximate the necessary amount of global semantics based on the obtained hidden states. Encoder Gating For the encoder, we take the hidden representation of "CLS" token to control the amount of additional global information ? E = Sigmoid(W E h CLS + b E ) (7) where W E ? R d?d , and d is the dimension of the hidden representation. We form the topic-aware hidden state by merging it with the topic mixture and mapping it onto a topical space u i = [h i , ?] (8) c i = f enc_topic (u i ) (9) where f enc_topic is a non-linear transformation. The topic-oriented encoder hidden state of every token is the fusion of the topic-aware and the original representation. h i = ? E c i + (1 ? ? E )h i (10) Decoder Gating The amount of topic mixture used for the decoder is controlled by both encoder and decoder hidden state ? D = Sigmoid(W D 1 h CLS + W D 2 s CLS + b D ) (11) where W D 1 ? R d?d , W D 2 ? R d?d . This switching probability is used to modulate the decoder hidden state, which follows the same computation with the encoder gating. t j = [s j , ? dec ] (12) e j = f dec_topic (t j ) (13) s j = ? D e j + (1 ? ? D )s j (14) 

 Training Objective Our framework favors end-to-end learning of neural topic modeling and text summarization. In this section, we formally define the objective functions for the two modules. For our neural topic model, the objective function is derived from the evidence lower bound  (Blei et al., 2017) . We adapt the change of variables in normalizing flow that determine the distribution of the variable at the end of the flow to the loss of neural topic model L NTM = log p(x, z) ? log q(z|x) = ? log q(z 0 ) + K i=1 log det ?f i ?z i?1 + log p(x|z K ) + log p(z K ) (15) where p(z K ) denotes the prior distribution constructed by the flow; p(x|z K ) stands for the loglikelihood of the document; log q K (z K ) denotes the approximate posterior distribution. Detailed derivation is available in Appendix. For text summarization, we minimize the crossentropy loss L sum = ? M j=1 log p(y j |{x i } N i=1 , y i<j ) (16) where N and M are the length of the document X and summary Y , respectively. The entire framework is trained with the linear combination of two loss functions L sum and L NTM L = L sum + ?L NTM ( 17 ) where ? is the hyperparameter balancing the effect of neural topic model on the training process. 4 Experimental Setup 

 Datasets We evaluate our proposed method on five benchmark datasets: CNN/DailyMail (CNN/DM)  (Hermann et al., 2015) , XSum  (Narayan et al., 2018) , Reddit TIFU , arXiv, and PubMed  (Cohan et al., 2018) . The datasets possess various styles and varying lengths. CNN/DM is constructed by collecting news articles written by CNN and DailyMail journalists. For each article, its highlights are chosen as the summary. We use the non-anonymized version and follow the conventional training/validation/test split in  (Hermann et al., 2015) . XSum comprises of 226,711 news articles, each of which is linked with a one-sentence summary. Our preprocessing and training/validation/test split is analogous to  (Narayan et al., 2018) . Reddit TIFU includes 120K informal posts from the online discussion forum Reddit, strictly following the rule of constructing an expressive "TL;DR" summary. In this work, the long subset of the dataset is applied for performance evaluation. arXiv, PubMed are two long-document datasets of scientific publications. For each document, the abstract is chosen to be the summary. We present the statistics of datasets in Table  2 .   , integrate and jointly finetune with the flow-based neural topic model on downstream datasets. Following , during test time, our beam search is conducted with a beam size of 8, and top-3 checkpoints are selected based on their evaluation loss in the validation set, and we average their results on the test set. More detailed settings can be found in Appendix. 

 Comparisons As baselines, we compare our proposed architecture against a wide variety of previous studies: ?  PTGEN (See et al., 2017) : a pointer-generator baseline that allows switching between generating words from the vocabulary and copying words from the source.   

 Automatic Evaluation We use the automatic metrics of ROUGE scores  (Lin, 2004) . In Table  3 , 4, 5, 6, and 7, we report the unigram overlap (ROUGE-1), bigram overlap (ROUGE-2) to assess informativeness, and longest common subsequence (ROUGE-L) for the fluency of the generated summary. Our model outperforms prior works on five standard datasets. For CNN/DailyMail, we achieve an absolute improvement of 0.35 in ROUGE-1, 0.48 in ROUGE-2, and 0.28 in ROUGE-L over PEGASUS. Furthermore, our model obtains better results than the previous topic-aware model BART + TA in ROUGE-2 with 0.6 points. This shows that our methods can generate summaries that include important content in the document. On the XSum dataset, which is more abstractive than CNN/DailyMail  (Bommasani and Cardie, 2020) , our gain is even more pronounced. Compared with BART + TA, we achieve 3.8 absolute improvement in ROUGE-1, 2.4 in ROUGE-2, and 3.8 in ROUGE-L. For Reddit TIFU, in which most of the source texts and the target summaries are informal, our model outperforms PEGASUS by 1.3 in ROUGE-1, 0.4 in ROUGE-2, and 1.5 in ROUGE-L. These results show that global semantics is capable of helping the model generate better target summaries. For arXiv and PubMed dataset, we also achieve improvement over the baseline PEGASUS, which is designed specifically for abstractive text summarization. In particular, for arXiv dataset, we gain an increase of 0.71 in ROUGE-1, 2.48 in ROUGE-2, and 1.46 in ROUGE-L. For PubMed dataset, the increase is 1.7 in ROUGE-1, 1.3 in ROUGE-2, and 0.83 in ROUGE-L. Since the automatic metric does not fully reveal the true quality of the model, we conduct a human evaluation for further assessment. To achieve that goal, we design two tests in order to elicit human judgements in two ways. 

 Human Evaluation In the first experiment, we presented summaries of PEGASUS , BART  (Lewis et al., 2019) , our model, and the gold summary, then asked four professional English speakers to rate the summaries from worst to best in terms of informativeness, faithfulness, topic coherence, and fluency. We randomly sampled 100 summaries from 100 documents of CNN/DailyMail test set. The score of a system is equal to the percentage of times it was selected as the best minus the percentage of times it was chosen as the worst. In the second experiment, we applied the question answering (QA) paradigm. For each document, we create two independent questions which emphasizes the key information of the text. Participants would read and answer those questions as best as they could. The score for one system is the percentage of questions the participants answer correctly. Ten professional English speakers were asked to participate in two assessments. The results in table  9  show that our generated summaries favor human judgements, and are more likely to maintain the important content in the original text than other systems' summaries. The Fleiss' Kappa scores with overall agreement percentages of the first and second human evaluation experiments were denoted in Table  9 . As shown in the      (Lewis et al., 2019)  To study the effect of our topic-oriented module on other abstractive Transformer-based model, we integrate our flow-based neural topic model and contextualized gating into BART  (Lewis et al., 2019) . In particular, we continue to finetune on CNN/DailyMail, XSum, and arXiv dataset, given the pretrained checkpoint. As can be seen in Table  10 , 11, 12, our topic-oriented module is able to improve the performance, showing general effectiveness on other Transformer-based architecture. 

 Analysis on Neural Topic Model and Traditional Topic Model To substantiate our hypothesis that neural topic model does enhance the summarization performance in large-scale datasets, we have conducted experiments to combine the Transformerbased summarization module with traditional topic model, i.e. Latent Dirichlet Allocation (LDA) and Poisson Factor Analysis (PFA) on CNN/DailyMail and PubMed. We denoted the results in Table  13  and Table  14 . As it can be seen, neural topic models, particularly our proposed model, significantly outperform approaches of traditional topic models on abstractive summarization. It is inherent that latent vector is useful for text summarization, as shown in section 5.1. Here we study whether jointly training with summarization module helps the topic model produce humaninterpretable topics. Coherence Score Comparison We decide to evaluate the topic models with the automatic C V measure. Following  (Zeng et al., 2018; , we pick the top 10 words from each topic and average C V scores of all topics. The results are reported on two summarization datasets, CNN/DailyMail and XSum. To conduct the comparisons, we take LDA and LSA as probabilistic baselines, as they are notable and well-known for human interpretability. For both baselines, we execute 1000 iterations to assure convergence. As Table  16  shows, our model outperforms traditional topic models, which implies that jointly training neural topic model and text summarization creates human-understandable topics. Sample Topics To further assess the quality of the topics learned by our system, we continue to extract some sample words (Table  6 ) indicating the context around "liverpool chelsea" discovered by the model trained on CNN/DailyMail dataset. As can be realized, the topics pertaining to probabilis-tic topic models such as LSA and LDA contain some mixed topic words. Conversely, our neural topic models trained with the text summarization module produce the topic which looks more coherent. In particular, our words refer to the context which involves the teams competing in the football championship of England, such as "arsenal", "tottenham", etc. and related factors, for instance, "balls", "prize", "winning", etc.  In this section, we proceed to study the impact that (1) The integration of normalizing flow and (2) The contextualized gating mechanism have on the text summarization performance. 

 Impact of the contextualized gating mechanism It can be seen that plainly incorporating the global semantics into the model makes the performance improvement drop strongly. As shown in table 17, the ROUGE-1 score's decreases more than 2 points compared with models we apply contextualized gating. We hypothesize that in numerous cases, the effect of global semantics overwhelm the benefits of contextualized representations. Impact of integrating normalizing flow In this ablation, we eliminate the normalizing flow from the neural topic modeling. As shown in Table  17 , without the normalizing flow, the improvement that the latent vector brings is downgraded, nearly 0.4 of ROUGE-1 for using contextualized gating and 0.53 of ROUGE-1 in non-gating case . We hypothesize that the plain neural topic model does not give a sufficiently expressive global semantics as the neural topic model using normalizing flow. 

 Case Studies Table  1  shows a case study on the summarization results of PEGASUS and our models. While PE-GASUS model misses the key information related to the death of "Richie Benauld", our model successfully include it in the final summarization. It shows the effectiveness of our model in capturing key information in the document, thanks to the contribution of neural topic model and gating mechanism. Remarkably, our model is also able to rephrase "signature one liners" as "trademark word" when describing Richie Benauld's famous quote, not just copying the words in the original document. More case studies can also be found in Appendix. 

 Conclusion In this paper, we propose a method to utilize global semantics for text summarization task. In particular, we aim to fit the global semantics to expressively describe the documents. Moreover, we find that maintaining the information in the original contextualized representations is also beneficial for the summarization performance. We outperform other state-of-the-art models on five benchmark datasets. 
