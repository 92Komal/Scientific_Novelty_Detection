title
A Bag of Tricks for Dialogue Summarization

abstract
Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning about the situation, and informal language understanding. Using a pretrained sequence-to-sequence language model, we explore speaker name substitution, negation scope highlighting, multi-task learning with relevant tasks, and pretraining on in-domain data. Our experiments show that our proposed techniques indeed improve summarization performance, outperforming strong baselines.

Introduction The nature of dialogue poses additional challenges to summarizers beyond what is required when processing structured, single-speaker documents  (Zhu and Penn, 2006) . Given that dialogues typically represent an interaction between many speakers, a summarizer model must keep track of the different lines of thoughts of individual speakers, distinguish salient from non-salient utterances, and finally produce a coherent, monologue summary of the dialogue. Dialogues usually include unfinished sentences where speakers were interrupted or repetitions, where a speaker expresses their thoughts more than once and possibly in different styles. Moreover, a single dialogue could touch on many topics without a clear boundary between the different topics. All the aforementioned phenomena certainly add to the difficulty of the task  (Zechner and Waibel, 2000; Zechner, 2002; Chen and Yang, 2020) . Our work focuses on SAMSum  (Gliwa et al., 2019) , which is a dialogue summarization dataset comprised of ~16K everyday dialogues with their * Work done during an internship at Amazon. human-written summaries. As our backbone model, we use BART  (Lewis et al., 2020) , a stateof-the-art pretrained encoder-decoder language model that is suitable for sequence-to-sequence tasks. Table  1  shows an example of a summary generated using BART  (Lewis et al., 2020) , finetuned on SAMSum. Clearly, a level of reasoning is required to make sense of the conversation, which BART fails to do and therefore produces an incorrect summary. We propose a combination of techniques to tackle a set of dialogue summarization challenges. The first challenge is having multiple speakers (generally, more than 2), where it becomes harder for the model to keep track of different utterances and determine their saliency. The second challenge is multiple negations, which is thought by  Chen and Yang (2020)  to pose some difficulty to dialogue understanding. The third of these challenges is reasoning, where the model is required to reason about the dialogue context, and infer information that is not explicitly expressed. The last challenge is informal language. Since we focus on random, everyday conversations, these are usually filled with non-standard language (abbreviations, social media terms, etc.). 

 The contributions in this work are: ? We propose a set of novel techniques to address four dialogue summarization challenges: multiple speakers, negation, reasoning and informal language. Our techniques include name substitution, negation scope highlighting, multi-task learning with relevant tasks, and pretraining on in-domain corpora. ? We show impressive improvements on the summarization performance using three of these, outperforming very strong baselines.  

 Reference: Orion is grieving after the death of her rat. Table  1 : Example from SAMSum  (Gliwa et al., 2019)  of a dialogue and its generated summaries using two BART models: vanilla and multi-tasked. The summary generated by the vanilla model indicates that the rat is the cheater, pointing to a lack of commonsense reasoning on the model side. The output of our multi-tasked model (section 3.5) clearly shows better understanding of the dialogue. 

 Related Work Early work on dialogue summarization focused more on extractive than abstractive techniques for summarization of meetings  (Murray et al., 2005; Riedhammer et al., 2008)  or random conversations  (Murray and Renals, 2007) . In the context of meeting summarization,  Shang et al. (2018)  proposed an unsupervised graph-based sentence compression approach for meeting summarization on the AMI  (McCowan et al., 2005)  and ICSI  (Janin et al., 2003)  benchmarks.  Goo and Chen (2018)  leveraged hidden representations from a dialogue act classifier through a gated attention mechanism to guide the summary decoder. More recently,  Gliwa et al. (2019)  proposed SAMSum, a benchmark for abstractive everyday dialogue summarization.  modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 

 Challenges We now present our four techniques for dialogue summarization: name substitution (section 3.3), negation scope highlighting (section 3.4), multi-task learning on common sense tasks (section 3.5), and pretraining on an in-domain dialogue corpus (section 3.6). 

 Experimental Setup For all our experiments, we use BART large architecture  (Lewis et al., 2020) .  1  All our experiments are run using fairseq  (Ott et al., 2019) . 

 Baselines We compare our techniques to two summarization baselines: ? Vanilla BART: Fine-tuning the original BART large checkpoint model on SAMSum. ? Multi-view Seq2Seq (Chen and Yang, 2020) : This is based on BART, as well, but during the summarization, the model considers multiple views, each of which defines a certain structure for the dialogue. We compare to their best model which combines topic and stage views. 

 Multiple Speakers We hypothesize that uncommon (less frequent in the original pretraining data) or new names could be an issue to a pretrained model, especially if such names were seen very few times, or not at all, during pretraining. Such issues could specifically show up in multi-participant conversations, and could introduce co-reference issues when generating the summary. As a simple technique to alleviate this, we preprocess SAMSum by replacing speaker names with more common, frequent names, ones that the model is more likely to have seen during pretraining. Since we are dealing with English dialogue summarization, we use a list 2 of common English names and replace each speaker name with a randomly sampled same-gender name from this list. Since the name list is divided by gender (male or female), we use gender guesser 3 to replace with a same-gender name. To avoid modifying the ground truth summaries and to ensure a fair comparison with other models, the original name is replaced back into the generated summary before evaluation. Table  2  compares the performance of this technique to fine-tuning BART on the original SAM-Sum data. We observe ROUGE improvements on both validation and test sets of SAMSum. In addition, we perform an analysis of the performance with respect to the number of participants per dialogue. Figure  1  plots the summarization performance against the number of speakers. We can see that conversation with more participants (7, 8, 12) exhibit higher ROUGE boost than conversations with fewer speakers (2, 3, 4). In other words, we observe that the more participants in the summary, the more effect this technique has. Notably, the average number of speakers per dialogue in SAM-Sum is only ~2.4. and we expect name substitution to work even better with datasets that have many more speakers per dialogue. 

 Negation Understanding Chen and Yang (2020) argue that negations represent a challenge for dialogues. We experiment with marking negation scopes in the input dialogues before feeding them to BART. To do that, we fine-tune a RoBERTa base model on the CD-SCO dataset from SEM Shared Task 2012 for negation scope prediction  (Morante and Blanco, 2012) . Then, we mark negation scope using two designated special tokens to mark the start and the end of the negation scope. For example, the sentence "I don't know what to do" becomes "I don't <NEG> know what to do <\NEG>" after negation scope highlighting. We initialize the embeddings of the special tokens <NEG> and <\NEG> randomly. Results are shown in Table  3 . While we expected to see a performance boost due to negation scope highlighting, we actually saw a performance drop except on ROUGE-L on the test set. To understand why, we investigate the negation challenge dialogues put together in  (Chen and Yang, 2020) . We found that in all examples, negation did not seem to be a problem, and that BART was able to handle multiple negations very well. Therefore marking negation scopes could have introduced unneeded noise into the model, causing the observed performance drop. 

 Reasoning Reasoning is often necessary for dialogue summarization  (Chen and Yang, 2020) , especially in cases where there is missing information or implicit assumptions regarding the situation. Unfortunately, it is difficult for the model to learn to conduct such reasoning by relying only on the reference summaries (this difficulty is exacerbated by the fact that SAMSum is of a relatively small size). Multi-task learning (MTL) enables knowledge transfer across relevant tasks. For instance  Li et al. (2019)  improved their summarization performance by jointly learning summarization and topic segmentation. Also,  Konar et al. (2020)  improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : ? Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table  7  in the Appendix A). We use the ROC stories dataset  (Mostafazadeh et al., 2016) . ? Commonsense Generation: Generative commonsense reasoning (Lin et al., 2020) is a task involving generating an everyday scenario description given basic concepts. We assume such task could help the model reason more about conversations, which is certainly needed in many dialogues (see example 3 in Table  7  in Appendix A). ? Commonsense Knowledge Base Construction: The task here is to generate relation triplets similar to  (Bosselut et al., 2019) . More specifically, we train our model to predict relation objects given both relation and subject. We use ConceptNet  (Liu and Singh, 2004) . Table  4  shows the summarization performance after multi-task fine-tuning of BART. We also show the results of combining ROC and CommonGen with SAMSum. It is clear that MTL gives a performance boost in almost all cases, outperforming the vanilla BART and the Multi-view SS baseline on both the development and test sets. It is worth noting that due to the small size of both validation and test splits (~800 dialogues), it is difficult to test the statistical significance of these results. 

 Informal Language We hypothesize that pretrained language models, BART in our case, are not well-adapted to the dia-   (Gliwa et al., 2019) . Data Val Test R-1 R-2 R-L R-1 R-2 R- Figure  1 : ROUGE values against the number of participants per dialogue on the development set of SAMSum. Performance boost is more clear in dialogues with more participants logue domain. Therefore, we adapt BART to dialogue inputs by further pretraining of BART on a dialogue corpus and with dialogue-specific objectives. 4 

 Pretraining Corpora We consider the following 2 corpora for further pretraining of BART: PersonaChat (140K utterances) , and a collection of 12M Reddit comments. We experiment with both whole word masking and span masking (masking random contiguous tokens). Our experimental setup is described in the Appendix in section B.1. Table  5  shows the results of fine-tuning BART pretrained on dialogue corpora.  5  The best model (PersonaChat, word masking) outperforms the vanilla BART on all metrics and the Multiview SS baseline on test set ROUGE-2 and ROUGE-L. We can see that in general, BART pretrained on PersonaChat is better than pretraining on both PersonaChat and Reddit, which is surprising since more pretraining data usually means better performance. This could be explained by the dissimilarity between Reddit comments and the dialogues in SAMSum. We can also see that whole word masking performs slightly better than span masking. Based on these results, it is obvious that further pretraining on in-domain corpora can be helpful when dealing with inputs of special nature such as dialogues. Also, we can see that pretraining using dialoguespecific objectives is performing well (on either PersonaChat only or with Reddit), and even outperforming random span masking on the validation set. This certainly shows that task-specific pretraining could be beneficial. At last, we combine pretraining with MTL by fine-tuning a pretrained model in a multi-task learning fashion. Table  6  compares this to separate pretraining and MTL. We can see that pretraining on PersonaChat and fine-tuning on both SAMSum and ROC gives the best performance over the validation set, outperforming all other settings. On the test set, it is performing very well but slightly outperformed by multi-tasking with ROC in both ROUGE-2 and ROUGE-L. Lastly, we combine named substitution with the best model here and the results are also shown in Table  6 . We observe that name substitution does not give a performance boost when used in combination with pretraining and MTL. 

 Conclusion In this paper, we explored different techniques to improve dialogue summarization performance by addressing different challenges to the task individually. The proposed techniques included name substitution, negation scope highlighting, multitask learning with relevant tasks, and pretraining on in-domain corpora. On one hand, our experiments on three challenges showed the effectiveness of our proposed techniques which outperformed strong baselines on the task. On the other hand, our proposed technique to handle multiple negations performed poorly and by analyzing the outputs on  49.22 26.47 47.80 48.65 25.20 47.08 SAMSum + negation scope marked 48.61 25.45 47.82 48.59 24.96 47.32  Table  3 : Summarization performance on SAMSum when highlighting negation scope.  Table  5 : Summarization performance on SAMSum when BART is pretrained on an in-domain corpus. We also include results when using additional dialogue-specific pretraining objectives (See Appendix B.2).   49.97 26.94 48.88 48.87 25.70 47.72  Table  6 : Summarization performance on SAMSum when BART is pretrained on an in-domain corpus and then fine-tuned in a multi-task fashion. Data Val Test R-1 R-2 R-L R-1 R-2 R-L Original SAMSum Tasks Val Test R-1 R-2 R-L R-1 R-2 R-L Pretraining Corpus Val Test R-1 R-2 R-L R-1 R-2 R-L Tasks Val Test R-1 R-2 R-L R-1 R-2 R-L negation-intensive dialogues, we found that multiple negations do not represent a challenge for dialogue summarization systems. 
