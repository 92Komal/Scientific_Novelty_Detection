title
Nutri-bullets Hybrid: Consensual Multi-document Summarization

abstract
We present a method for generating comparative summaries that highlights similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health -rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization -while being equally fluent. 1 Transformer (baseline) * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as mozambican grass , are safe to eat with no serious side effects . * Whole -grain cereals may protect against obesity , diabetes and certain cancers. However , more research is needed . * Whole grains , such as blueberries , are likely safe to eat with no serious side effects . * Whole grains are safe to eat. However , people with type 2 diabetes should avoid whole grains . * Whole grains are lower in carbs than whole grains , making them a good choice for people with type 2 diabetes. Our Method * Whole grains has been shown to lower weight gain and improve various type 2 diabetes risk factors . * Whole grains has been shown to lower insulin resistance and improve various cancer risk factors . * Whole grains has been linked to several other potential health benefits , such as improved CVD risk , eyesight , and memory. However , more studies are needed to draw stronger conclusions. * There is some evidence , in both animals and humans , that whole grains can reduce mortality by regulating the hormone ghrelin.

Introduction Articles written about the same topic rarely exhibit full agreement. To present an unbiased overview of such material, a summary has to identify points of consensus and highlight contradictions. For instance, in the healthcare domain, where studies often exhibit wide divergence of findings, such comparative summaries are generated by human experts for the benefit of the general public. 2 Ideally, this capacity will be automated given a large number of relevant articles and continuous influx of new ones that require a summary update to keep it current. However, standard summarization architectures cannot be utilized for this task since the amount of comparative summaries is not sufficient for their training. In this paper, we propose a novel approach to multi-document summarization based on a neural interpretation of traditional concept-to-text generation systems. Specifically, our work is inspired by the symbolic multi-document summarization system of  (Radev and McKeown, 1998)  which produces summaries that explicitly highlight agreements, contradictions and other relations across input documents. While their system was based on human-crafted templates and thus limited to a narrow domain, our approach learns different components of the generation pipeline from data. To fully control generated content, we frame the task of comparative summarization as concept-totext generation. As a pre-processing step, we ex-5214 tract pertinent entity pairs and relations (see Figure  1 ) from input documents. The Content Selection component identifies the key tuples to be presented in the final output and establishes their comparative relations (e.g., consensus) via aggregation operators. Finally, the surface realization component utilizes a text-infilling language model to translate these relations into a summary. Figure  1  exemplifies this pipeline, showing selected key pairs (marked in bold), their comparative relation -Contradiction (rows 1 &3 and rows 4&5 conflict), and the final summary.  3  This generation architecture supports refined control over the summary content, but at the same time does not require large amounts of parallel data for training. The latter is achieved by separately training content selection and content realization components. Since the content selection component operates over relational tuples, it can be robustly trained to identify salient relations utilizing limited parallel data. Aggregation operators are implemented using simple deterministic rules over the database where comparative relations between different rows are apparent. On the other hand, to achieve a fluent summary we have to train a language model on large amounts of data, but such data is readily available. In addition to training benefits, this hybrid architecture enables human writers to explicitly guide content selection. This can be achieved by defining new aggregation operators and including new inference rules into the content selection component. Moreover, this architecture can flexibly support other summarization tasks, such as generation of updates when new information on the topic becomes available. We apply our method for generating summaries of Pubmed publications on nutrition and health. Typically, a single topic in this domain is covered by multiple studies which often vary in their findings making it particularly appropriate for our model. We perform extensive automatic and human evaluation to compare our method against state-of-the-art summarization and text generation techniques. While seq2seq models receive competent fluency scores, our method performs stronger on task-specific metrics including relevance, content faithfulness and aggregation cognisance. Our method is able to produce summaries that receive an absolute 20% more on aggregation cognisance, an absolute 7% more on content relevance and 7% on faithfulness to input documents than the next best baseline in traditional and update settings. 

 Related Work Text-to-text Summarization Neural sequence-tosequence models  (Rush et al., 2015; Cheng and Lapata, 2016; See et al., 2017)  for document summarization have shown promise and have been adapted successfully for multi-document summarization  Lebanoff et al., 2018; Baumel et al., 2018; Amplayo and Lapata, 2019; Fabbri et al., 2019) . Despite producing fluent text, these techniques may generate false information which is not faithful to the original inputs  (Puduppully et al., 2019; Kry?ci?ski et al., 2019) , especially in low resource scenarios. In this work, we are interested in producing faithful and fluent text cognizant of aggregation amongst input documents, where few parallel examples are available. Recent language modeling approaches  (Devlin et al., 2018; Stern et al., 2019; Shen et al., 2020; Donahue et al., 2020)  can also be extended for text completion. Our work is a text-infilling language model where we generate words in place of relation specific blanks to produce a faithful summary. Prior work  (Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018)  on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common  (Guu et al., 2017; Li, 2018; Shah et al., 2019) . In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases.  McKeown and Radev (1995) ;  Radev and McKeown (1998) ;  Barzilay et al. (1998)  introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems  (Wiseman et al., 2018; Puduppully et al., 2019; Sharma et al., 2019; Wenbo et al., 2019)  these approaches capture document consensus and aggregation cognisance. While the neural approaches alleviate the need for human intervention, they do need an abundance of parallel data, which are typically from one source only. Hence, modern techniques do not deal with input documents' consensus in low resource settings. 

 Method Our goal is to generate a text summary y for a food from a pool of multiple scientific abstracts X. In this section, we describe the framework of our Nutribullets Hybrid system, illustrated in Figure  2 . 

 Overview We attain food health entity-entity relations, for both input documents X and the summary y, from entity extraction and relation classification modules trained on corresponding annotations (Table  2 ). Notations: For N input documents, we collect X G = {G x p } N p=1 , a database of entity-entity rela- tions G x p . G p = (e k 1 , e k 2 , r k ) K k=1 is a set of K tuples of two entities e 1 , e 2 and their relation r. r represents relations such as the effect of a nutrition entity e 1 on a condition e 2 (see Table  2 ).  4  We have raw text converted into symbolic data. Similarly, we denote the corpus of summaries as  Y = {(y m , G y m , O y m ) M m=1 }, 

 Content Selection and Aggregation Our content selection model takes a mini-database of entity-entity relation tuples X G as input, and outputs the key tuples C and the aggregation operator O. Content selection and aggregation consists of two parts -(i) identifying key content P (C|X G ) and (ii) subsequently identifying the aggregation operator O using C, X G . volves selecting important, diverse and representative tuples from a database. While clustering and selecting from the database tuples is a possible solution, we model our content selection as a finite Markov decision process (MDP). This allows for an exploration of different tuple combinations while incorporating delayed feedback from various critical sources of supervision (similarity with target tuples, diversity amongst selected tuples etc). We consider a multi-objective reinforcement learning algorithm  (Williams, 1992)  to train the model. Our rewards (Eq. 2) allow for the selection of informative and diverse relation tuples. The MDP's state is represented as s t = (t, {c 1 , . . . , c t }, {z 1 , z 2 , ..., z m?t }) where t is the current step, {c 1 , . . . , c t } is the content selected so far and {z 1 , z 2 , ..., z m?t } is the remaining entityentity relation tuples in the m-sized database. The action space is all the remaining tuples plus one special token, Z ? {STOP}.  5  The number of actions is equal to |m ? t| + 1. As the number of actions is variable yet finite, we parameterize the policy ? ? (a|s t ) with a model f which maps each action and state (a, s t ) to a score, in turn allowing a probability distribution over all possible actions using softmax. At each step, the probability that the policy selects z i as a candidate is: 

 Content Selection Identifying key content in- ? ? (a = z i |s t ) = exp(f (t, ?i , ? i * )) m?t+1 j=1 exp(f (t, ?j , ? j * )) (1) where c i * = arg max c j (cos( ?i , ?j )) is the selected content closest to z i , ?i and ? i * are the encoded dense vectors, cos(u, v) = u?v ||u||?||v|| is the cosine similarity of two vectors and f is a feedforward neural network with non-linear activation functions that outputs a scalar score for each action a. The selection process starts with Z. Our module iteratively samples actions from ? ? (a|s t ) until selecting STOP, ending with selected content C and a corresponding reward. We can even allow for the selection of partitioned tuple sets by adding 5 STOP and NEW LIST get special embeddings. an extra action of "NEW LIST", which allows the model to include subsequent tuples in a new group. We consider the following individual rewards: ? R e = c?C cos( ? 1c , ? 1y ) + cos( ? 2c , ? 2y ) is the cosine similarity of the structures of the selected content C with the structures present in the summary y (each summary structure accounted with only one c), encouraging the model to select relevant content. ? R d = 1[max i,j (cos( ?j , ?i )) < ?] computes the similarity between pairs within selected content C, encouraging the selection of diverse tuples. ? r p is a small penalty for each action step to encourage concise selection. The multi-objective reward is computed as R = w e R e +w d R d ? |C|r p , (2) where w e , w d and r p are hyper-parameters. During training the model is updated based on the rewards. During inference the model selects an ordered set of key and diverse relation tuples corresponding to appropriate health conditions. Consensus Aggregation Identifying the consensus amongst the input documents is critical in our multi-document summarization task. We model the aggregation operator of our Content Selection using simple one line deterministic rules as shown in Table  1 . The rules are applied to the key C entity-entity relation pairs in context of X G . In our example in Figure  1 , O is Contradiction because of rows 1&3 and rows 4&5 (rows 1&3 only would also make it Contradiction). 

 Surface Realization The surface realization model P (y|O, C), performs the critical task of generating a summary guided by both the entity-entity relation tuples C and the aggregation operator O. The model allows for robust, diverse and faithful summarization compared to traditional template and modern seq2seq approaches.  We propose to model this process as a prototypedriven text infilling task. The entities from C are used as fixed tokens with relations as special blanks in between these entities. This is prefixed by a prototype summary corresponding to O. For the example shown in Figure  2 , we concatenate using |SEN | a randomly sampled contradictory summary "Kale contains substances ... help fight cancer ... but the human evidence is mixed ." to C "<blank> pears <controls> ovarian cancer <de-creases> breast cancer <blank>". The infilling language model produces text corresponding to relations between entities while maintaining an overall structure which is cognizant of O.  6  The model is trained on the few sample summaries from the training set using G y m and O y m to produce y m . Providing aggregation and content guidance during generation alleviates the lowresource issue. 

 Summary and Update Setting In this section we describe the setting of summary updates. In a real world setting, we would often receive new input documents such as scientific studies about the same subject which necessitate a change in an old summary. In context of our food and health summarization task, the goal is to update an old summary about a food and health condition on receiving results from new scientific studies from Pubmed. Our model can accommodate this scenario fairly easily. We describe the minor changes to the Content Selection and Aggregation and Surface Realization models for such a setting. We are provided an original summary and can extract it's content C and can also construct the mini-database X G from the text of the new documents. We identify the aggregation between the new studies' X G and original summary's content C first. Depending on the aggregation identified,  6  Summaries in our training data are labelled with O y m as belonging to one of the four categories of Under-reported, Population Scoping, Contradiction or Agreement to accommodate such training. corresponding content C is selected from X G . For instance, in case of a contradiction, we are keen on identifying content leading to this contradiction. The subsequent Surface Realization is dependent on O, the selected C and the C present in the original summary (P (y|O, C + C )). 

 Experiments Dataset We utilize a real world dataset for Food and Health summaries, crawled from https:// www.healthline.com/nutrition  (Shah et al., 2021) . The HealthLine dataset consists of scientific abstracts as inputs and human written summaries as outputs. The dataset consists of 6640 scientific abstracts from Pubmed, each averaging 327 words. The studies in these abstracts are cited by domain experts when writing summaries in the Healthline dataset, forming natural pairings of parallel data. Individual summaries average 24.5 words and are created using an average of 3 Pubmed abstracts. Each food has multiple bullet summaries, where each bullet typically talks about a different health impact (hydration, diabetes etc). We assign each food article randomly into one of the train, development or test splits. Entity tagging and relation classification annotations are provided for the Pubmed abstracts and the healthline summaries. Settings: We consider three settings. 1. Single Issue: We use the individual food and health issue summaries as a unique instance of food and single issue setting. We split 1894 instances 80%,10%,10% to train, dev and test. 

 Multiple Issues: We group each food's article Pubmed abstract inputs and multiple summary outputs as a single parallel instance. 464 instances are split 80%,10%,10% to train, dev and test. 

 Summary Update: We consider two kinds of updates -new information is fused to an existing summary and new information contradicts an existing summary. For fusion we consider single issue summaries that have multiple conditions from different Pubmed studies (bananas + low blood pressure from one study and bananas + heart health from another study). We partition the Pubmed studies to stimulate an update. The contradictory update setting is where we artificially introduce conflicting results in the input document set so that the aggregation changes from Agreement to Contradictory. We have a total of 103 test instances. All models are trained atop of Single issue data. Evaluation We evaluate our systems using the following automatic metrics. Rouge is an automatic metric used to compare the model output with the gold reference  (Lin, 2004) . KG(G) computes the number of entity-entity pairs with a relation in the gold reference, that are generated in the output.  7  This captures relevance in context of the reference. KG(I), similarly, computes the number of entityentity pairs in the output that are present in the input scientific abstracts. This measures faithfulness with respect to the input documents. Aggregation Cognisance (Ag) measures the accuracy of the model in producing outputs which are cognizant of the right aggregation from the input, (Under-reported, Contradiction or Agreement). We use a rule-based classifier to identify the aggregation implied by the model output and compare it to the actual aggregation operator based on the input Pubmed studies. In addition to automatic evaluation, we have human annotators score our models on relevance and fluency. Given a reference summary, relevance indicates if the generated text shares similar information. Fluency represents if the generated text is grammatically correct and written in well-formed English. Annotators rate relevance and fluency on a 1-4 likert scale  (Albaum, 1997) . We have 3 annotators score every data point and report the average across the scores. Baselines In order to demonstrate the effectiveness of our method, we compare it against text2text and  7  We run entity tagging plus relation classification on top of the model output and gold summaries. We match the gold (e g i , e g j , r g ) tuples using word embedding based cosine similarity with the corresponding entities in the output structures (e o i , e o j , r o ). A cosine score exceeds a threshold of 0.7 is set (minimize false positives) to identify a match. Implementation Details Our policy network is a three layer feedforward neural network. We use a Transformer  (Vaswani et al., 2017)  implementation for Surface Realization. We train an off-the-shelf Neural CRF tagger  (Yang and Zhang, 2018)  for entity extraction. We use BERT  (Devlin et al., 2018)  based classifiers to predict the relation between two entities in a text trained using crowdsourced annotations from  (Shah et al., 2021) . Futher implementation details can be found in A. 

 Results In this section, we describe the performance of our Nutribullet Hybrid system and baselines on summarization and summary updates. We report empirical results , human evaluation and present sample outputs, highlighting the benefits of our method. Single and Multi-issues Summarization: We describe the results on the task of generating summaries. Table  3  presents the automatic evaluation results for the food and single issue summarization task. High KG(I) and KG(G) scores for our method indicate that the generated text is faithful to input entities and relevant. In particular, a high Aggregation Cognisance (Ag) score indicates that our model generates summaries which are cog-    4 ). Since a lot of these patterns are learned from the human summaries, Transformer receives a high Rouge score. However, as in the low resource regime, the baseline does not completely capture the content and aggregation, it fails to get a very high KG(G) or Ag score. A similar trend is observed for the other baselines too, which in this low resource regime produce a lot of false information, reflected in their low KG(I) scores. Human evaluation, conducted by considering scores,on a 1-4 Likert scale, from three annotators for each instance, shows the same pattern. Our model is able to capture the most relevant information, when compared against the gold summaries while producing fluent summaries. The Transformer baseline produces fluent summaries, which are not as relevant. The performance is poorer for the Copy-gen, Entity Data2text and GraphWriter models. In the multi-issues setting, the baselines access the gold annotations with respect to the input documents' clustering. Our model conducts the extra task of grouping the selected tuples, using the "New List" action. Our model performs better than the baselines on both the KG(I) and KG(G) metrics as seen in Table  5 . Again, the pattern of producing very similar and repetitive sentences hurts the baselines. They fail to cover different issues and tend to produce false information, in this low resource setting. Our model scores an 7% higher on KG(G) and 17% higher on KG(I) compared to the next best performance, in absolute terms. Table  4  shows the comparison between the outputs produced by our method and the Transformer baseline on the benefits of whole-grains. Our method conveys more relevant, factual and organized information in a concise manner.  Summary Update: We study the efficacy of our model to fuse information in existing summaries on receiving new Pubmed studies. As the KG(G) metric in 6 shows, our model is able to select and fuse more relevant information. Table  7  shows two examples of summaries on flaxseeds where our model successfully fuses new information.  evaluation results to demonstrate the efficacy of maintaining Aggregation Cognisance (Ag), which is critical when updating summaries on receiving contradictory results. The high performance in this update setting demonstrates the Surface Realization model's ability to produce aggregation cognizant outputs, in contrast to the baselines that do not learn this reasoning in a low resource regime. Analysis: Information Extraction and Content Aggregation Information extraction is the critical first step performed for the input documents in order to get symbolic data for content selection and aggregation. To this end, we report the performance of the information extraction system, which is composed of two models -entity extraction and relation classification. As reported in Table  8 , the entity extraction model, a crf-based sequence tagging model, receives a token-level F1 score of 79%. The relation classification model, a BERT based text classifier, receives an accuracy of 69%. The performance of the information extraction models is particularly important for the content aggregation sub-task. In order to analyse this quantitatively, we perform manual analysis of the 179 instances in the dev set and compare them to the system identified aggregation -information extraction followed by the deterministic rules in Table  1 . Given the simplicity of our rules, system's 78% accuracy in Table  8  is acceptable. Deeper analysis shows that the performance is lowest for Population Scoping and Contradiction with an accuracy of 52% and 56% respectively. The performance of Population Scoping being low is down predominantly to the simplicity of the rules. Most mistakes occur when the input studies are review studies that don't mention any population but analyze results from several past work. Contradiction suffers because of the information extraction system and stronger models for the same should be able to alleviate the errors.  

 Conclusion While modern models produce fluent text in multidocument summarization, they struggle to capture the consensus amongst the input documents. This inadequacy -magnified in low resource domains, is addressed by our model. Our model is able to generate robust summaries which are faithful to content and cognizant of the varying consensus in the input documents. Our approach is applicable in summarization and textual updates. Extensive experiments, automatic and human evaluation underline its impact over state-of-the-art baselines. Figure 1 : 1 Figure 1: We consider the database extracted from four Pubmed studies on Pears and Cancer. The key facts (bold) and consensus (contradiction) are realized in the text generated by our model. 
