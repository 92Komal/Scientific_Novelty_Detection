title
StreamHover: Livestream Transcript Summarization and Annotation

abstract
With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams.

Introduction One of the most powerful communication mediums is livestreaming. New platforms such as YouTube Live, Twitch, Instagram Live and TikTok encompass a variety of topics, ranging from video games to social media to professional sports. We are particularly interested in livestreams that are distinguished by three characteristics: Excessive length, the recordings could last from several minutes to several hours; Verbal communication, the use of natural language is the primary means of communication, in contrast to gestures or facial expressions; Informal nature, the streamers' language is mostly informal and unplanned, unlike news broadcasts. Without an effective mechanism to summarize such streamed content, livestreaming platforms may not fully meet the needs of their customers. Our goal in this work is to create a text preview of the streamed content. When a user hovers over the thumbnail or scrolls past a video, they are shown a preview of the content. We present a dataset of over 500 hours of video footage, which were streamed live on a social media platform (behance.net) created to showcase and discover creative work. Figure  1  shows an example of the streams, where the artists showcase the use of Adobe Photoshop and Illustrator in designing holiday cards and posters. It is necessary to point out that video analysis is not suitable here, as the video only mirrors the artists' screen content. As a first step towards automatic creation of a text preview, we focus on identifying salient utterances to produce an extract from the livestream transcript. We make use of vector-quantized variational autoencoders (VQ-VAE; van den  Oord et al., 2017)  to identify salient utterances. The model has been applied successfully to opinion summarization that learns in-domain sentence representations  (Angelidis et al., 2021) , which is essential for adaptation of general-domain models. We refrain from using sequential methods for utterance selection. First, it is difficult to scale up sequential prediction to process transcripts that exceed the maximum allowed length, even with models that handle long text  (Beltagy et al., 2020; Zhao et al., 2020) . Second, sequential methods  (Narayan et al., 2018b; Xiao and Carenini, 2019)  may not give enough flexibility to select salient utterances on-the-fly when content is being streamed live, thus they are unsuitable for our case. There has been a shortage of annotated datasets that are necessary for livestream transcript summarization. We build a browser-based user interface for summary annotation that provides to the annotators a clip of the livestream recording alongside a synchronized display of the transcript. The interface allows annotators to conveniently label summary utterances and write an abstractive summary using their own words (Figure  3 ). With a total of 500 hours of annotated video footage, our dataset is notably larger than existing annotated corpora for transcript summarization  (Janin et al., 2003; Carletta et al., 2006) . We compare our summarization approach with strong baselines on the dataset and shed light on the task of livestream transcript summarization. Our contributions are as follows. ? We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. ? We present StreamHover, a unsupervised model based on VQ-VAE to identify salient utterances from livestream transcripts to form preview summaries. We evaluate the method across multiple dimensions and discuss its strengths and weaknesses. Empirical results show that our method outperforms strong summarization baselines. 1 

 Related Work Closed captions are often provided onscreen, turning streaming videos into text on an unprecedented scale  (Besik, 2020) . However, there are very few summarization studies that attempt to generate text previews of streaming videos to help users browse or refind information that has been watched before. Neural text summarizers have focused primarily on written text, including news articles, reviews, scientific papers and book chapters  (See et al., 2017 ; Tan 1 Our annotations and source code are available at https: //github.com/ucfnlp/streamhover et al., 2017;  Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bra?inskas et al., 2020; Ladhak et al., 2020; Song et al., 2021) . Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances are devoted to informal chit-chats in an attempt to connect with the audience (Figure  2 ). We investigate extractive rather than abstractive approaches as the latter are prone to generate hallucinated content that does not exist in the source text  (Cao et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) . The problem could be exacerbated by ungrammatical spoken utterances and transcription errors. Instead, we consider VQ-VAE, an unsupervised representation learning technique (van den  Oord et al., 2017; Jin et al., 2020; Angelidis et al., 2021)  for content extraction. Unsupervised training of the VQ-VAE model and its inference could potentially be performed at the same time, allowing important utterances to be extracted from a transcript segment on-the-fly during streaming, without interrupting the learning process. It is also easier to tailor the model to specific domains compared to contemporary extractive methods  (Yasunaga et al., 2017; Dong et al., 2018; Xu and Durrett, 2019; Wang et al., 2020) . Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs  (Papalampidi et al., 2020; Chen et al., 2021) , interviews  (Zhu et al., 2021) , multiparty meetings  (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020 Koay et al., , 2021 Zhong et al., 2021) , telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion  (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020)  as little information could be gleaned from videos that mirror the artists' desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality research. We describe our data annotation process in the following section. 

 Our Dataset We aim to create a large and representative corpus containing transcripts and summaries of streamed videos. We explore a leading social media platform (Behance.net) supported by Adobe Creative Cloud that features livestreams of creative work by artists and designers. The website boasts over 10 million users, who watch artists and designers livestream when they create. Our data are extracted from this website, containing a large quantity of streamed videos (>5,000), the length of which ranges from minutes to several hours. The streamers' language is unplanned, instead of rehearsed as that of TED talks  (Hernandez et al., 2018) . We obtain a total of 5,398 streamed videos. The metadata of a video includes its ID, duration, title, a short description and the transcript. Automatic transcription was provided by Microsoft Automatic Speech Recognition which helps make videos accessible to a wide audience. Each transcript contains a set of segments, each corresponds to about 30 seconds of audio. Each segment contains a set of utterances. 2 Figure  2  shows an example of the segments and utterances. The offset of the segment indicates the number of minutes since the beginning of the recording. When a user hovers over the thumbnail or scrolls past a video, we expect a textual summary to give a glimpse of the verbal content. This view of summarization leads us to annotate salient content across the video in an equally detailed manner. It naturally avoids lead bias that is ubiquitous in news  (Grenander et al., 2019) . We segment a video into 5-minute clips and annotate each clip for summary-worthy content. A clip contains an average of 51 utterances and 460 words. Due to time and budget constraints, we select 370 streamed video for summary annotation.  3  Table 1 provides a detailed comparison of our annotated corpus with previous datasets, including Switchboard  (Godfrey et al., 1992) ,  ICSI (Janin et al., 2003)  and AMI  (Carletta et al., 2006)  that contain both transcripts and human-annotated ex-   (Penn and Zhu, 2008) . With a total duration of over 500 hours, our dataset is notably larger than similar datasets. tractive/abstractive summaries. With a combined duration of 500 hours, our dataset is substantially larger than previously released datasets. We recruit 12 workers from Upwork.com and validate their language skills for summary annotation. Upwork is a freelancing platform that allows us to reach out to workers directly to ensure our instructions are fully understood. Each worker is asked to write a concise summary for a given clip using their own words (Task A) and identify summary utterances (Task B) using the graphical user interface (Figure  3 ), which shows a clip of the streamed video alongside a synchronized display of the transcript. Additionally, our guidelines suggest a good summary of Task A should have at least 100 characters and that of Task B should have between 50 and 80 words (?15% compression). As is the case with meeting summaries  (Janin et al., 2003) , a clip is annotated by a single worker owing to an expensive annotation process. The worker can also identify a clip to be chitchat, in which case it will not be annotated for summaries. Table  2  shows our dataset statistics. On average, a human abstract contains 3 sentences (36 words) and a human annotated extract contains 5.5 utterances (80 words). Moreover, summary utterances constitute 8.9% and 8.6% of all utterances in terms of number and duration. We study inter-annotator agreement by inviting three workers to each annotate 8 hours of video that contains a total of 96 clips. Using 10-second intervals as measuring units, 4 the Fleiss' Kappa on identifying summary utterances is 0.13. We note that the score is within the range of what is normally found in annotating speech transcripts for extractive summaries (0.1?0.3; Marge  et al., 2010), as annotating spoken text is a highly challenging task. We find that annotators tend to perceive the same region as salient but they may disagree as to which utterances should be included in the summary due to verbosity of spoken text. We refer the reader to  (Artstein and Poesio, 2008)  for interpretations and improvements to IAA. 

 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ? X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQ-VAE; van den  Oord et al., 2017)  to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization  (Angelidis et al., 2021)  and machine translation  (Prato et al., 2020) . We are interested in using the method to account for domain characteristics of livestreams, which showcase new and creative work of artists and designers on their use of Photoshop, Illustrator, and other tools. 5 VQ-VAE is a powerful framework for learning latent variable models using deep neural networks. It learns discrete vector representations for an utterance, which is then used to categorize the utterance along various dimensions. E.g., "Good morning Hi Everybody" suggests a greeting and opens up a dialogue; "I had probably 3 or 4 different customers on YouTube and ... on Facebook asked me how the heck do you burn an audio CD in Adobe Audition" engages the audience and introduces the  

 ' Figure  4 : Our summarizer embeds an input utterance using BERT, transforms BERT's semantic space to a set of latent codes, then reconstructs the utterance using the code embeddings. We identify summary utterances as those associated with prominent latent codes/topics. The model is trained using a dictionary learning algorithm for code embeddings (E) and backpropagation with a straight-through estimator for model parameters ?, ? and ?. main topic. The VQ-VAE method groups utterances based on their discrete representations and selects salient utterances to form a summary. We employ an embedding function Embed ? (?) to map an input utterance x into a semantically meaningful space. The space is subsequently discretized according to a codebook. To achieve this, we prefix x with a [CLS] token and append a [SEP] token, pass it into a BERT model, then obtain the vector corresponding to  [CLS]  as a pooled representation of the utterance, denoted by h ? R H (Eq. (  1 )). We use a ConvEncoder ? (?) with a set of D filters to convolve the input h. The output is a sequence of feature vectors [q 1 , ? ? ? , q H ] where q i ? R D (Eq. (  2 )). We define a codebook E = [e 1 , ? ? ? , e K ], where K is the number of latent codes and e k ? R D is the kth code embedding. The i-th feature q i is assigned to the latent code z i whose embedding e z i has the minimum Euclidean distance with it (Eq. (  3 )). Our method essentially discretizes the H-dimensional semantic space by producing latent codes {z i } H i=1 , one for each dimension of the semantic space. h = Embed ? (x) ? R H (1) [q 1 , ? ? ? , q H ] = ConvEncoder ? (h), q i ? R D (2) z i = arg max k ? q i ? e k 2 , i ? [H] (3) With the latent code embeddings [e z 1 , ? ? ? , e z H ], we seek to reconstruct the input utterance, which is achieved by generating a dense vector h using a ConvDecoder ? (?) (Eq. (  4 )). h is then fed to a Transformer decoder to reconstruct the original utterance x (Eq. (  5 )). In this process, the code embeddings serve as "topic vectors" that group dimensions of the semantic space into clusters relevant to the application domain. Our model parameters include those used by the BERT encoder and Transformer decoder (? and ?), the convolutional encoder and decoder that use tied parameters (?), and embeddings of the codebook E. h = ConvDecoder ? ([e z 1 , ? ? ? , e z H ]) ? R H (4) x = Generate ? ( h) (5) We next describe the loss function used to learn these parameters. The loss function of our model comprises of three parts, including a cross-entropy term between the original and reconstructed utterance XEnt(x, x) that optimizes the BERT embedder ?, Transformer generator ?, and convolutional encoder and decoder ?, as shown in Figure  4 . The gradients will, however, bypass the latent code embeddings due to the straight-through estimator  (Bengio et al., 2013) . To learn code embeddings in an end-to-end manner, we use a dictionary learning algorithm  (van den Oord et al., 2017)  that moves code embeddings e z i towards feature vectors q i by minimizing the l 2 -distance between the two vectors e z i ?sg(q i ) 2 2 , where sg(?) is a stop-gradient operator that constrains its operand to be a non-updated constant during backpropagation, i.e., it stops q i from being updated. As illustrated in Eq. (  6 ), we additionally apply a commitment loss to encourage the feature vector q i to commit to a code embedding. sg(e z i ) ? q i 2 2 prevents q i from deviating too much from the code embedding e z i . This loss term is associated with a coefficient ? ? [0, 1]. L(?) = XEnt(x, x) + i e z i ? sg(q i ) 2 2 + ? i sg(e z i ) ? q i 2 2 (6) At test time, we define summary utterances as those associated with prominent latent codes/topics. Given a set of N utterances, we obtain latent codes from the n-th utterance using Eq. (3), denoted by {z (n) i } H i=1 . This gives a total of N ? H codes from which we find prominent ones. They are denoted by P which contains a set of most frequently occurring codes. A score S(x n ) is assigned to utterance x n that computes how often it is associated with those prominent codes P. In Eq. (  7 ), H i=1 1[z (n) i = k] indicates the number of times the n-th utterance is assigned to code k, where k belongs to P. Finally, we extract K highest-scoring utterances to form an extractive summary of the input. 

 S(x n ) = k?P H i=1 1[z (n) i = k] (7) Our method draws on the convolutional encoder and decoder to transform BERT's semantic space to map each dimension to a latent code. The summary selection process is deterministic and our encoder takes full advantage of a large, pre-trained model to produce initial utterance representations. This design sets our method apart from that of  Angelidis et al. (2021) . Moreover, the method has the potential for modelling topic transitions between utterances to improve summarization of livestreams, which we leave for future work. 

 Experiments Dataset. Finding salient content from livestream transcripts is a "needle-in-the-haystack" problem. Our summarization dataset contains a total of 370 videos split into short clips of 5 minutes each. The annotators manually annotated 5,421 clips (?451 hours) with extractive and abstractive summaries. 582 clips (?49 hours) are removed because they are identified to contain only chit-chats. The dataset is divided into training, validation and test splits: ? 3,884 clips (320 videos / 323 hours) in training, ? 728 clips (25 videos / 61 hours) in validation, ? 809 clips (25 videos / 67 hours) in test split. We call our summarizer "StreamHover." When a user hovers their mouse over a video's timeline, a summary preview is shown and keeps updating. As a first attempt, StreamHover focuses on extracting salient utterances from individual clips instead of whole streams to encourage selected utterances to be mostly evenly distributed across the stream. When the content is provided live, the stream can be divided into short clips and our algorithm consumes one clip at a time to produce summaries on-the-fly. It is important to note that extracting summary utterances remains challenging even for modern neural summarizers. E.g.,  Kedzie et al. (2018)  reveal that summarizers may not effectively identify summary content without a dependency on intentional lead bias in news writing. Our setting is challenging as not only are there few utterances deemed to be summary-worthy but such utterances can occur anywhere in a video clip. Baselines. We compare StreamHover with stateof-the-art extractive and abstractive summarizers. The abstractive summarizers generate an abstract from the transcript of a clip without tuning.  6  These include BART-large,  BART-large-cnn (Lewis et al., 2020)  and T5  (Raffel et al., 2020) , which are some of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank  (Erkan and Radev, 2004)  and TextRank  (Mihalcea and Tarau, 2004)  are graph-based models that extract relevant sentences based on eigenvector centrality.  Sum-Basic (Vanderwende et al., 2007)  assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization method for speech transcripts: FluCovRank  (Shang et al., 2018)  groups utterances into clusters, generates an abstractive sentence from each cluster, then selects the best elements from abstractive sentences under a budget constraint. Finally, we compare our approach with the Quantized Transformer  (Angelidis et al., 2021) , which uses a clustering interpretation of the quantized space and two-step sampling algorithm to extract summary sentences from reviews. Settings. We use pretrained BERT-BASE as our embedder Embed ? (?). The model has 12 layers, 12 heads per layer and a hidden size (H) of 768. A 6layer Transformer decoder is used as the generator Generate ? (?) to reconstruct the original utterance. The model has 8 heads per layer, a hidden size of 768, and randomly initialized parameters. The convolutional encoder and decoder use a kernel size of 3. Because our embedder is pretrained and the remaining parameters are not, we divide them into two groups E={?} and R={?, ?}, then apply separate training schedules. Following Liu and Lapata (2019) we use two Adam optimizers: lr E = lr E ? min(step ?0.5 , step ? warmup ?1.5 E ), lr R = lr R ? min(step ?0.5 , step ? warmup ?1.5 R ) where the learning rate for the embedder lr E =7e  Table  5 : Results of human evaluation regarding fluency, informativeness and the overall quality of system summaries using Best-Worst Scaling. is smaller than that of the rest params lr R =4e ?2 . Its warmup period is longer: warmup E =3,000 for the embedder and warmup R =1,500 for the rest. It allows the pretrained embedder to be updated in a slower pace until other model parameters start to generate accurate gradients. All of our models are trained for 30 epochs on dual NVIDIA V100 GPUs with gradient accumulation every ten steps. We experiment with different numbers of filters, D = {64, 100, 128}, for the convolutional encoder and decoder. The number of latent codes are varied in K = {512, 1024, 2048}. The coefficient ? used for commitment loss is set to 0.25 (Eq. (  6 )). These hyperparameters are tuned on the validation set. We keep only utterances that contain >5 words in consideration. The final training set contains 168,111 utterances. The proportion of times a system is selected as the "Best." FluCovRank is omitted as it is <1%. 

 Results In Table  3 , we analyze the performance of extractive summarizers on identifying ground-truth summary utterances and report their precision, recall and F 1 -measure scores. We vary the length of their output to yield {3, 4, 5}-utterance summaries. In comparison, a ground-truth extract contains 5.5 utterances. The Lead-N baseline selects the first N utterances of a clip. It gives low scores because our data do not present strong lead bias as that of news articles. We find that StreamHover consistently outperforms other summarization systems across all lengths. Its length, when measured by number of words, is comparable to that of LexRank and Tex-tRank. The highest F 1 -score of 30.47% is achieved when StreamHover generates a 5-utterance summary for each 5-minute clip. This amounts to rendering one utterance per one-minute segment when a user scrolls past the video. In Table  4 , we compare extractive and abstractive summarizers and report ROUGE scores  (Lin, 2004)  that measure content overlap between system and reference summaries.  7  We use human abstracts as the reference. All extractive summarizers produce 5-utterance summaries and Oracle Extract contains ground-truth utterances. It places an upper bound on the performance of extractive summarizers. We observe that StreamHover yields the highest scores on both R-2 and R-L metrics. ? top left bottom / cloud studies today / find links to their original posts / hey jennifer saw the images / love the top left and bottom / info tab and i uploaded / colors are beautiful but im partial through colorful sky scenes / pretty large about 4000 by 4000 pixels / photo studies of today / moment LexRank ? I hope you guys are having a good day so far. ? So I'm going to be painting from these images and these beautiful photos are from various photographers. ? Those yeah well top right also is like very Contra high contrast that tends to like grab my attention when I look at the sheet but I would say top left and bottom right give me the most like happy feels. ? So yeah, if you guys want to grab the reference images, you can find them in the stream description below the individual images... 

 BART-Large ? Hello good morning everybody welcome to the stream. I hope you guys are having a good day so far. Is there a lot of buffering or are we doing alright? I got a little message that there was some connectivity issue. For a moment there, so I hope I hope it's OK. Yeah, I'll just keep going. So yeah, if you guys want to grab the reference images, you can find them in the stream description below the individual images... 

 Quantized Transformer ? Good to see you were going to be doing cloud studies today. ? The stream in the description. ? One of them is from Morguefile, One is from unsplash, well, two are from Unsplash and one is from pixels there a little bit from all over the place, but you can find the photographers below if you'd like. ? Hey Jennifer, saw the images. ? Let's see top left, bottom right... 

 StreamHover (Ours) ? So if anybody is interested in joining in, if you want to work on some skies for your landscapes for future landscapes, this is what we're going to be doing. ? One of them is from Morguefile, One is from unsplash, well, two are from Unsplash and one is from pixels there a little bit from all over the place, but you can find the photographers below if you'd like. ? Those yeah well top right also is like very Contra high contrast that tends to like grab my attention when I look at the sheet but I would say top left and bottom right give me the most like happy feels. ? So yeah, if you guys want to grab the reference images, you can find them in the stream description below the individual images... Table 6: Example system summaries for Digital Painting Studies with Maddy Bellwoar-Clouds. The BART summary is fluent but its content lacks specificity, as is the case for LexRank. The summary segments selected by FluCovRank are ungrammatical. StreamHover identifies on-topic and informative utterances related to digital painting. Their relevant spans of text are manually underlined for readability. We show example system summaries in Table  6 . The abstractive summaries generated by BART-Large are fluent but their content lacks specificity, so are the summaries produced by LexRank and Quantized Transformer. Particularly, QT does not seem to perform well on this task despite that the model has been retrained on livestream transcripts.  8  We believe this is partly because words and phrases tend to repeat themselves in review documents, and while spoken utterances are verbose, there is little repetition found in the transcripts. We observe that summary segments selected by FluCovRank are ontopic but they are ungrammatical and difficult to 8 https://github.com/stangelid/qt/blob/main/custom.md 

 Utterances C1 C2 C3 0 Hello good morning everybody welcome high foster highly art. x 1 Hi Lisa, welcome everyone. 2 I hope you guys are having a good day so far. 3 Good to see you were going to be doing cloud studies today. 4 So if anybody is interested in joining in, if you want to work on some skies for your landscapes for future landscapes, this is what we're going to be doing. x x 5 Photo studies of today. 6 So I'm going to be painting from these images and these beautiful photos are from various photographers. x 7 You can find links to their original posts below. 8 The stream in the description. x 9 One of them is from Morguefile, One is from unsplash, well, two are from Unsplash and one is from pixels there a little bit from all over the place, but you can find the photographers below if you'd like. x 10 Hey Jennifer, saw the images. 11 I really love the top left and bottom right. 12 The colors are beautiful but I'm partial through colorful Sky scenes. 13 Yeah, I totally agree. 14 Let's see top left, bottom right. 15 Those yeah well top right also is like very Contra high contrast that tends to like grab my attention when I look at the sheet but I would say top left and bottom right give me the most like happy feels. x ... interpret without context. In contrast, StreamHover can identify on-topic and informative utterances related to digital painting. We provide more examples in the supplementary materials. In Table  7 , we study the most prominent latent codes (C1-3) and their associated utterances. We define representative utterances as those frequently assigned to these codes (Eq. (  3 )). We observe that C1 usually contains a skewed number of utterances that are commonly seen in the data and not representative of the input; C2 contains lengthy but not necessarily summary-worthy utterances. In our experiments, we exclude C1/C2 before performing grid search on all codes to find the set of prominent codes: we use P=50 tuned on the valid set which is effective in helping identify summary utterances.  9  We conduct a human evaluation to assess how StreamHover compares to strong extractive and abstractive baselines. They are (a) LexRank  (Erkan and Radev, 2004) , (b) FluCovRank  (Shang et al., 2018)  and (c)  BART-Large (Lewis et al., 2020) ; the latter two are abstractive systems. Each evaluator is shown a video clip with a synchronized display of the transcript followed by four system summaries, shown in random order to remove any positional bias. The evaluator is asked to select the best and worst of the summaries according to each of these criteria: Fluency/Coherence: is the the summary well-presented, grammatically correct and easy to read? Informativeness: does the summary provide useful information about the video clip? Overall Quality: is the summary of good quality considering both content and linguistic aspects? We randomly sample 100 clips from the test set. Each clip and its summaries are judged by five evaluators that we recruit from Amazon mechanical turk.  10  Table  5  shows the performance of all systems measured by Best-Worst Scaling  (Kiritchenko and Mohammad, 2016) , where the score of a system is computed as the percentage of times it was selected as the best minus the worst. The range of scores is [-1,1]. Figure  5  shows how frequently a system is chosen to produce the "best summary." We observe that StreamHover achieves an overall score of 0.52 and it is selected as the best summary in over half of the times. 

 Conclusion We present StreamHover, a new framework for annotating and summarizing livestream transcripts. Our dataset contains over 500 hours of videos annotated with extractive and abstractive summaries. We explored an extractive method leveraging VQ-VAE to identify salient summary utterances and obtained strong results. Future work includes boosting summarization solutions to provide users a concentrated overview of streamed content.  
