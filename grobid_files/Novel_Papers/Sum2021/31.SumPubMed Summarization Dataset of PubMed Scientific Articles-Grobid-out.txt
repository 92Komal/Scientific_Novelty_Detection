title
SUMPUBMED: Summarization Dataset of PubMed Scientific Articles

abstract
Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SUMPUBMED, using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SUMPUBMED. SUMPUBMED is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SUMPUBMED. Thus, SUMPUBMED opens new avenues for the future improvement of models as well as the development of new evaluation metrics.

Introduction Most of the existing summarization datasets, i.e., CNN Daily Mail and DUC are news article datasets. That is, the article acts as a document, and the summary is a short (10-15 lines) manually written highlight (i.e., headlines). In many cases, these highlights have significant lexical overlap with the few lines at the top of the article. Thus, any model which can extract the top few lines, e.g., extractive methods, performs adequately on these datasets. However, the task of summarization is not merely limited to short-length news articles. One could also summarize long and complex documents such as essays, research papers, and books. In such cases, an extractive approach will most likely fail. For successful summarization on these documents, one needs to (a) find information from the distributed (non-localized) locale in the large text, (b) perform paraphrasing, simplifying, and shortening of longer sentences and (c) combine information from multiple sentences to generate the summary. Hence, an abstractive approach will perform better on such large documents. One obvious source that contains such complex documents is the MEDLINE biomedical scientific articles, which are publicly available. Furthermore, these articles are accompanied by abstracts and conclusions which summarize the documents. Therefore, we constructed a scientific summarization dataset from pre-processed PubMed articles, named SUMPUBMED. In comparison to the previous news-article based datasets, SUMPUBMED documents are longer, and the corresponding summaries cannot be extracted by selecting a few sentences from fixed locations in the document. The dataset, along with associated scripts, are available at https://github.com/vgupta123/ sumpubmed. Our contributions in this paper are: ? We created a new scientific summarization dataset, SUMPUBMED, which has longer text documents and summaries with non-localized information from documents. ? We analyzed the quality of summaries in SUMPUBMED on the basis of four parameters: readability, coherence, non-repetition, and informativeness using human evaluation. ? We evaluated several extractive, abstractive (seq2seq), and hybrid summarization models on SUMPUBMED. The results show that SUMPUBMED is more challenging compared to the earlier news-based datasets. ? Lastly, we showed that the standard summarization evaluation metric, ROUGE  (Lin, 2004) , correlates poorly with human evaluations on SUMPUBMED. This indicates the need for a new evaluation metric for the scientific summarization task. In Section 1, we provided a brief introduction. The remaining parts of the paper are organized as follows: in Section 2 we explain how SUMPUBMED was created. In Section 3, we explain how summaries were annotated by human experts. We then move on to experiments in Section 4. We next discuss the results and analysis in Section 5, followed by the related work in Section 6. Lastly, we move on to the conclusions in final Section 7. 

 SUMPUBMED Creation SUMPUBMED is created from PubMed biomedical research papers, which has 26 million documents. The documents are sourced from diverse literature, including MEDLINE, life science journals, and online books. For SUMPUBMED creation we took 33, 772 documents from Bio Med Central (BMC). BMC incorporates research papers related to medicine, pharmacy, nursing, dentistry, health care, health services, etc. The research documents in BMC contain two subsections: Front and Body. The front part of the document is basically the abstract and taken as the gold summary. The body part which is taken as the main document contains three subsections: background, results, and conclusion. Preprocessing The average word count in the PubMed scientific articles is around 4, 000 words for each document and 250 to 300 lines in every document. Therefore, to create SUMPUBMED, we performed extensive preprocessing so that nontextual content is removed and the overall text is reduced to a more manageable size. This extenstive pre-processing step is one of the main factors that sets SUMPUBMED apart from similar datasets  (Cohan et al., 2018) . During preprocessing, the non-textual content from the text was removed by: (a) replacing citations and digits in the content with <cit> and <dig> labels, (b) removing figures, tables, signatures, subscripts, superscripts, and their associated text (e.g., captions), and (c) removing the acknowledgments and references from the text. All the preprocessing was done on a sentence level utilizing the Python regex library. 1 After preprocessing, we convert the final document to an XML format and use the SAX parser to parse it. SAX vs DOM parser: In SAX, events are triggered when the XML is being parsed. When the parser is parsing the XML and encounters a tag starting (e.g., < something >), then it triggers the tagStarted event (actual name of the event might differ). Similarly, when the end of the tag is met while parsing (< /something >), it triggers tagEnded. Using a SAX parser implies one needs to handle these events and make sense of the data returned with each event. One could also use the DOM parser, 2 where no events are triggered while parsing. In DOM the entire XML is parsed, and a DOM tree (of the nodes in the XML) is generated and returned. In general, DOM is easier to use but has a huge overhead of parsing the entire XML before one can start using it; therefore, we use SAX instead. An example of the front part, body part, and the XML file formed from the pre-processed text is shown in https://github.com/vgupta123/ sumpubmed/blob/master/template.pdf. ? The Raw Text version is obtained after preprocessing when removing non-textual context is completed, followed by XML parsing. 

 Versions of SUMPUBMED ? In the Noun phrases version, we processed the raw text version further to ensure that the summary and the text have the same named entities. We found that standard Name Entity Recognition (NER)  (Finkel et al., 2005)  and Biomedical Named Entity Recognizer (ABNER)  (Settles, 2005)  fail to pick the scientific named entities correctly. Note that the main reason behind ABNER insufficiency is the presence of novel PubMed named entities that were not covered by any of the classes in the ABNER tool. Therefore, we use a simple heuristic of noun intersection between summary and main-text noun phrases to obtain plausible entity sets. This produced a shorter version of both the text and the summary than the original pair. The SUMPUBMED versions statistics is given in Table  1 . The SUMPUBMED overall creation pipeline is shown in Figure  1 . 3 Human Annotation of SUMPUBMED Inspired from work on human evaluation of summaries by  Friedrich et al. (2014) , we distributed 50 randomly chosen summaries from the noun-phrase versions of SUMPUBMED to 10 expert annotators (graduate NLP students) such that we have 3 annotation for each summary. We asked these humanannotators to rate the summaries on a scale of 1 to 10. We created different document files, each having 10 pairs of summaries where we randomly shuffled between reference and generated summaries with respect to the placement on the page (left or right). The annotators evaluated the summaries based on the following criteria: ? Non-Repetition and no factual Redundancy (Non-Re): There should not be redundancy in the factual information, and no repetition of sentences is allowed. ? Coherence (Coh): Coherence means "continuity of sense". The arguments have to be connected sensibly so that the reader can see consecutive sentences as being about one (or a related) concept. ? Readability (Read): Consideration of general readability criteria such as good spelling, correct grammar, understandability, etc. in the summaries. ? Informativeness, Overlap and Focus (IOF): How much information is covered by the summary. The goal is to find the common pieces of information via matching the same keywords (or key phrases), such as "Nematodes", across the summary. For overlaps, annotators compare the keywords' (or key-phrases) occurrence frequency and ensure the summaries are on the same topic. The average scores and standard deviations are shown in Table  2 . Annotators found that for readability, coherence, and non-repetitiveness, the quality of summaries is satisfactory. However, for informativeness and overlap, it is hard to evaluate summaries due to domain-specific technical terms.  

 ROUGE and Human Scores For the 50 summaries evaluated by expert annotators, we calculated the Pearson's correlation  (Pearson, 1895)  between ROUGE  (Lin, 2004)  scores (ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-L (R-L)) in terms of precision, recall and F1 score with the humanevaluated scores. ROUGE-n is an n-gram similarity measure that computes uni/bi/trigram and higher n-gram overlaps. In R-L, L refers to the Longest Common Subsequence (LCS) overlap: a subsequence of matching words with the maximal length that is common in both texts with the order of words being preserved. Pearson's correlation value (between ?1 and +1) quantifies the degree to which quantitative and continuous variables are related to each other. The Pearson's correlations values are shown in Table  3 . ROUGE scores assume that a high-quality summary generated by a model should have common words and phrases with a gold-standard summary. However, this is not always true because (a) there can be semantically similar meaning (synonymous) word usage, and (b) there can be the usage of text paraphrases (similar information conveyed) with a little lexical overlap in the reference summary text. Therefore, merely considering lexical overlaps to evaluate summary quality is not sufficient. A high ROUGE score may indicate a good summary, but a low ROUGE score does not necessarily indicate a bad summary. Furthermore, while summarizing large documents, humans tend to utilize different paraphrasing/words to convey the same meaning in a shorter form. Several studies by  Cohan and Goharian (2016); Dohare et al. (2017)  argue that ROUGE is not an accurate estimator of the quality of a summary for scientific input, e.g., biomedical text. Hence, a weak correlation of ROUGE scores with human ratings on SUMPUBMED, as reported in Table  3 , should not be a surprise. That is, all correlation values in Table  3  are close to zero, so we can conclude that Rouge scores are weakly related with human ratings on the SUMPUBMED. 

 Experiments We have used the noun phrase version of SUMPUBMED in the abstractive summarization settings and the Hybrid version of SUMPUBMED in the extractive and the hybrid settings, i.e., (extractive + abstractive) summarizations. We split the dataset into train (93%), test (3%), and validation (4%) sets. Before training, we wrote a script that first tokenizes all input files and then forms the vocabulary and chunked files for the train, test, and validation sets. This step converts the input into a suitable format for the seq2seq models. 

 Baseline Models We use the following models on SUMPUBMED for evaluation: We use extractive, abstractive, and hybrid (extractive + abstractive) automatic summarization methods to evaluate SUMPUBMED. Abstractive Methods We use several modifications of seq2seq with attention, as described below: Seq2Seq with Attention  (Nallapati et al., 2016) : The encoder is a single layer bidirectional LSTM, while the decoder is a single layer unidirectional LSTM. Both the encoder and decoder have same sized hidden states, with an attention mechanism over the source hidden states and a soft-max layer over the vocabulary to generate the words. We use the same vocabulary for both the encoding and the decoding phase. Seq2Seq with Pointer Generation Networks  (See et al., 2017) : The previous model has a computational decoder complexity because each time we have to apply the softmax over the entire vocabulary. The model also outputs an excessive number of UNK tokens (UNK is a special token utilized for out-of-vocabulary words) in the target summary. To address this issue, we use a pointer-generator network  (See et al. (2017) ) which integrates the basic seq2seq model (with attention) with a copying mechanism  (Gu et al. (2016) ). We call this model seq2seq for the rest of the paper. The seq2Seq model with Pointer Generation Networks and Coverage Mechanism (+cov)  (Mi et al., 2016) : The summaries generated by the model discussed before may show repetition, like generating the same arrangement of words multiple times (e.g., "this bioinformatic approach this bioinformatic approach..." ). This repetition of phrases is prominent when generating multi-line summaries. The solu- tion to the problem of redundancy in summaries in seq2seq models is the coverage mechanism of  Mi et al. (2016) . This model penalizes repeated word generations by keeping track of the hitherto covered parts using attention distribution. Criteria Prec Recall F1 R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L Non-Re -0. Extractive Methods There are several existing approaches to extractive summarization, mostly derived from LexRank  (Erkan and Radev, 2004) , and TextRank  (Mihalcea and Tarau, 2004 ). We use TextRank, which is an unsupervised approach for sentence extraction, and has been used successfully in many NLP applications  (Hulth, 2003) . 

 Hybrid Methods (Extractive + Abstractive) We also experimented with the hybrid approach for summarization. First, we used extractive summarization using the TextRank ranking algorithm. We then applied abstractive summarization on the extracted text. We used the pointer-generator networks, followed by the coverage mechanism for the abstractive summarization. In this setting, we have not perfomed any preprocessing before extractive summarization to decrease the length of the documents. The extractive summarization step makes the text length sufficient to apply the abstractive summarization step on it quite easily. 

 Experimental Settings While decoding seq2seq models (for abstractive and hybrid models), we use a beam search  (Medress et al., 1977)  with a beam width of 4.Note that, Beam search is a greedy technique which chooses the most likely token from all generated tokens at each step to obtain the best b sequences (the hyper-parameter b here represents the beam width). Beam search is shown to be better than generating the first sequence. We also experimented with varying target summary lengths (i.e., the number of decoding steps) for seq2seq models. We report both seq2seq models with and without coverage results for comparison. We considered ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L)'s precision, recall, and F1 score for evaluation. Hyper-parameters The hyper-parameters used for the seq2seq model is in We utilized tensorflow package 3 for models and ROUGE evaluation package pyrouge 4 for the evaluation metric.We use a single GeF orce GT X T IT AN X with 12GB GP U memory taking on average 5 to 6 days per model for model training. 

 Results and Analysis Results on SUMPUBMED for abstractive methods, i.e., seq2seq models (with and without coverage), the extractive method of TextRank, and the hybrid approach, i.e., TextRank + seq2seq (with and without coverage) are shown in Tables  6, 7 , and 8, respectively. We also evaluated the seq2seq models on news datasets (CNN/Daily Mail and DUC 2001) for comparison, as shown in Table  5 . Analysis: In all three approaches, abstractive in words, the chances of non-covered words in the output summary also increase. We notice in both Tables  6 and 8  that by adding the coverage (+cov) mechanism, the problem of repetition in summaries is solved to a great extent. The ROUGE scores also show improvement after applying coverage to pointer-generator networks. Thus, one can conclude that pointer generator networks effectively handle named entities and outof-vocabulary words, and the coverage mechanism is useful to avoid repetitive generation, which is essential for scientific summarization. In Table  9 , we note that in terms of Precision (Pr), the abstractive approach shows the best results. However, the Recall (Re) of the extractive summarization model is always better than abstractive and hybrid approaches. Furthermore, the R-1 Re (ROUGE-1 Recall) and R-L Re (ROUGE-L Recall) for the hybrid models are approximately similar to the abstractive models. We also provide a few qualitative example of summarization on CNN/DailyMail in Appendix Section A, on SUMPUBMED in Appendic Section B.   2019 ) released a large dataset of 1.3 million of U.S. patent documents along with human written summaries. However, the closest datasets to SUMPUBMED are released by  Cohan et al. (2018) ;  Kedzie et al. (2018) ;  Gidiotis and Tsoumakas (2019) . 

 Related Work 

 Comparison with SUMPUBMED: News datasets' summary is located at the top of the article for most examples. Social media datasets lack the scientific aspect, i.e., complex domain-specific vocabulary and non-localized distributed information of SUMPUBMED. Other works on the scientific datasets are by  Cohan et al. (2018) ;  Kedzie et al. (2018) ;  Gidiotis and Tsoumakas (2019) . The closest work to our approach is the PubMed dataset by  Cohan et al. (2018) . However, unlike SUMPUBMED, (a) no extensive preprocessing pipeline was applied to clean the text (b) a single version is released compared with SUMPUBMED's several versions with distinct properties (varying summary lengths, article lengths, and vocabulary sizes), (c) only level-1 section headings instead of the whole PubMed document are used, and (d) there is a lack of human evaluation to assess data quality. However,  Cohan et al. (2018)  do act as an powerful inspiration for our work. 

 Conclusion We created a non-news, SUMPUBMED dataset, from the PubMed archive to study how various summarization techniques perform on task of scientific summarization on domain specific scientific texts. These texts have essential information scattered throughout the whole text. In contrast, earlier datasets with news stories appear to mostly have useful information in the first few lines of the document text. We also conducted a human evaluation on aspects such as repetition, readability, coherence, and Informativeness for 50 summaries of 250 words. Each summary is evaluated by 3 different individuals on the basis of four parameters: readability, coherence, non-repetition, and informativeness. Due to the unavailability of any state-ofthe-art results on this new dataset, we built several baseline models (extractive, abstractive, and hybrid model) for SUMPUBMED. To check the significance of our results, we studied the effectiveness of ROUGE through Pearson's correlation analysis with human-evaluation and observed that many variants of ROUGE scores correlate poorly with human evaluation. Our results indicate that ROUGE is possibly not a proper metric for SUMPUBMED.  
