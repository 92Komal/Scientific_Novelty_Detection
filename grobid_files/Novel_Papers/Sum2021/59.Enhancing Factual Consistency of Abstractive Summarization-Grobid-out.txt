title
Enhancing Factual Consistency of Abstractive Summarization

abstract
Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASUM to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results 1 show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords.

Introduction Text summarization models aim to produce an abridged version of long text while preserving salient information. Abstractive summarization is a type of such models that can freely generate summaries, with no constraint on the words or phrases used. This format is closer to human-edited summaries and is both flexible and informative. Thus, there are numerous approaches to produce abstractive summaries  (See et al., 2017; Paulus et al., 2017; Dong et al., 2019; Gehrmann et al., 2018) . However, one prominent issue with abstractive summarization is factual inconsistency. It refers to the hallucination phenomenon that the summary sometimes distorts or fabricates the facts in the article. Recent studies show that up to 30% of the summaries generated by abstractive models contain such factual inconsistencies  (Kry?ci?ski et al., 2019b; Falke et al., 2019) , raising concerns about the credibility and usability of these systems.  1  We provide the prediction results of all models at https: //github.com/zcgzcgzcg1/FASum/. 

 Article Real Madrid ace Gareth Bale treated himself to a Sunday evening BBQ... The Welsh wizard was ... scoring twice and assisting another in an impressive victory... Cristiano Ronaldo scored five goals against Granada on Sunday ... BOTTOMUP ... The Real Madrid ace scored five goals against Granada on Sunday. The Welsh wizard was in impressive form for... 

 SEQ2SEQ ...Gareth Bale scored five and assisted another in an impressive win in Israel... 

 FASUM (Ours) ...Gareth Bale scored twice and helped his side to a sensational 9-1 win. Cristiano Ronaldo scored five goals against Granada on Sunday...   et al., 2018)  and SEQ2SEQ wrongly states that Bale scored five goals. Comparatively, our model FASUM generates a summary that correctly exhibits the fact in the article. And as shown in Section 4.6.1, our model achieves higher factual consistency not just by making more copies from the article. On the other hand, most existing abstractive summarization models apply a conditional language model to focus on the token-level accuracy of summaries, while neglecting semantic-level consistency between the summary and article. Therefore, the generated summaries are often high in tokenlevel metrics like ROUGE  (Lin, 2004)  but lack factual consistency. In view of this, we argue that a robust abstractive summarization system must be equipped with factual knowledge to accurately summarize the article. In this paper, we represent facts in the form of knowledge graphs. Although there are numerous Article The flame of remembrance burns in Jerusalem, and a song of memory haunts Valerie Braham as it never has before. This year, Israel's Memorial Day commemoration is for bereaved family members such as Braham. "Now I truly understand everyone who has lost a loved one," Braham said. Her husband, Philippe Braham, was one of 17 people killed in January's terror attacks in Paris... As Israel mourns on the nation's remembrance day, French Prime Minister Manuel Valls announced after his weekly Cabinet meeting that French authorities had foiled a terror plot... 

 BOTTOMUP Valerie Braham was one of 17 people killed in January 's terror attacks in Paris. France's memorial day commemoration is for bereaved family members as Braham. Israel's Prime Minister says the terror plot has not been done. 

 Corrected by FC Philippe Braham was one of 17 people killed in January's terror attacks in Paris. Israel's memorial day commemoration is for bereaved family members as Braham. France's Prime Minister says the terror plot has not been done. efforts in building commonly applicable knowledge graphs such as ConceptNet  (Speer et al., 2017) , we find that these tools are more useful in conferring commonsense knowledge. In abstractive summarization for contents like news articles, many entities and relations are previously unseen. Plus, our goal is to produce summaries that do not conflict with the facts in the article. Thus, we propose to extract factual knowledge from the article itself. We employ the information extraction (IE) tool OpenIE  (Angeli et al., 2015)  to extract facts from the article in the form of relational tuples:  (subject, relation, object) . This graph contains the facts in the article and is integrated in the summary generation process. Then, we use a graph attention network  (Veli?kovi? et al., 2017)  to obtain the representation of each node, and fuse that into a transformer-based encoder-decoder architecture via attention. We denote this model as the Fact-Aware Summarization model, FASUM. In addition, to be generally applicable for all existing summarization systems, we propose a Factual Corrector model, FC, to help improve the factual consistency of any given summary. We frame the correction process as a seq2seq problem: the input is the original summary and the article, and the output is the corrected summary. FC has the same architecture as UniLM  (Dong et al., 2019)  and initialized with weights from RoBERTa-Large . We finetune it as a denoising autoencoder. The training data is synthetically generated via randomly replacing entities in the ground-truth summary with wrong ones in the article. As shown in Table  2 , FC makes three corrections, replacing the original wrong entities which appear elsewhere in the article with the right ones. In the experiments, we leverage an indepen-dently trained BERT-based  (Devlin et al., 2018)  factual consistency evaluator  (Kry?ci?ski et al., 2019b) . Results show that on CNN/DailyMail, FA-SUM obtains 0.6% higher fact consistency scores than UNILM  (Dong et al., 2019)  and 3.9% higher than BOTTOMUP  (Gehrmann et al., 2018) . Moreover, after correction by FC, the factual score of summaries from BOTTOMUP increases 1.4% on CNN/DailyMail and 0.9% on XSum, and the score of summaries from TCONVS2S increases 3.1% on XSum. We also conduct human evaluation to verify the effectiveness of our models. We further propose an easy-to-compute modelfree metric, relation matching rate (RMR), to evaluate factual consistency given a summary and the article. This metric employs the extracted relations and does not require human-labelled summaries. Under this metric, we show that our models can help enhance the factual consistency of summaries. 

 Related Work 

 Abstractive Summarization Abstractive text summarization has been intensively studied in recent literature.  Rush et al. (2015)  introduces an attention-based seq2seq model for abstractive sentence summarization.  See et al. (2017)  uses copy-generate mechanism that can both produce words from the vocabulary via a generator and copy words from the article via a pointer.  Paulus et al. (2017)  leverages reinforcement learning to improve summarization quality. Gehrmann et al. (  2018 ) uses a content selector to over-determine phrases in source documents that helps constrain the model to likely phrases.  defines a pretraining scheme for summarization and produces a zero-shot abstractive summarization model.  Dong et al. (2019)  employs different masking techniques for both NLU and NLG tasks, resulting in the UNILM model.  employs denoising techniques to help generation tasks including summarization. 

 Fact-Aware Summarization Entailment models have been used to evaluate and enhance factual consistency of summarization.  co-trains summarization and entailment and employs an entailment-aware decoder.  Falke et al. (2019)  proposes using off-the-shelf entailment models to rerank candidate summary sentences to boost factual consistency.  Zhang et al. (2019b)  employs descriptor vectors to improve factual consistency in medical summarization.  Cao et al. (2018)  extracts relational information from the article and maps it to a sequence as an additional input to the encoder.  Gunel et al. (2019)  employs an entity-aware transformer structure for knowledge integration, and  Matsumaru et al. (2020)  improves factual consistency of generated headlines by filtering out training data with more factual errors. In comparison, our model utilizes the knowledge graph extracted from the article and fuses it into the generated text via neural graph computation. To correct factual errors,  uses pre-trained NLU models to rectify one or more wrong entities in the summary. Concurrent to our work,  Cao et al. (2020)  employs the generation model BART  to produce corrected summaries. Several approaches have been proposed to evaluate a summary's factual consistency  (Kry?ci?ski et al., 2019a; Goodrich et al., 2019; Maynez et al., 2020) .  Zhang et al. (2019a)  employs BERT to compute similarity between pairs of words in the summary and article. ;  Durmus et al. (2020)  use question answering accuracy to measure factual consistency.  Kry?ci?ski et al. (2019b)  applies various transformations on the summary to produce training data for a BERT-based classification model, FactCC, which shows a high correlation with human metrics. Therefore, we use FactCC as the factual evaluator in this paper. 

 Model 

 Problem Formulation We formalize abstractive summarization as a supervised seq2seq problem. The input consists of a pairs of articles and summaries: {(X 1 , Y 1 ), (X 2 , Y 2 ), ..., (X a , Y a )}. Each article is tokenized into X i = (x 1 , ..., x L i ) and each summary is tokenized into Y i = (y 1 , ..., y N i ). In abstrative summarization, the model-generated summary can contain tokens, phrases and sentences not present in the article. For simplicity, in the following we will drop the data index subscript. Therefore, each training pair becomes X = (x 1 , ..., x m ), Y = (y 1 , ..., y n ), and the model needs to generate an abstrative summary ? = (? 1 , ..., ?n ). 

 Fact-Aware Summarizer We propose the Fact-Aware abstractive Summarizer, FASUM. It utilizes the seq2seq architecture built upon transformers  (Vaswani et al., 2017) . In detail, the encoder produces contextualized embeddings of the article and the decoder attends to the encoder's output to generate the summary. To make the summarization model fact-aware, we extract, represent and integrate knowledge from the source article into the summary generation process, which is described in the following. The overall architecture of FASUM is shown in Figure  1 . 

 Knowledge Extraction To extract important entity-relation information from the article, we employ the Stanford OpenIE tool  (Angeli et al., 2015) . The extracted knowledge is a list of tuples. Each tuple contains a subject (S), a relation (R) and an object (O), each as a segment of text from the article. In the experiments, there are on average 165.4 tuples extracted per article in CNN/DailyMail  (Hermann et al., 2015)  and 84.5 tuples in XSum  (Narayan et al., 2018) . 

 Knowledge Representation We construct a knowledge graph to represent the information extracted from OpenIE. We apply the Levi transformation  (Levi, 1942)  to treat each entity and relation equally. In detail, suppose a tuple is (s, r, o), we create nodes s, r and o, and add edges s-r and r-o. In this way, we obtain an undirected knowledge graph G = (V, E), where each node v ? V is associated with text t(v). During training, this graph G is constructed for each batch individually, i.e. there's no shared huge graph. One benefit is that the model can take unseen entities and relations during inference. We then employ a graph attention network  (Veli?kovi? et al., 2017)  to obtain embedding e j for each node v j . The initial embedding of v j is given by the last hidden state of a bidirectional LSTM applied to t(v j ). In the experiment, we employ 2 graph attention layers. 

 Knowledge Integration The knowledge graph embedding is obtained in parallel with the encoder. Then, apart from the canonical cross attention over the encoder's outputs, each decoder block also computes cross-attention over the knowlege graph nodes' embeddings: ? ij = softmax j (? ij ) = exp(? ij ) j?V exp(? ij ) (1) ? ij = s T i e j , (2) u i = j?V ? ij e j , (3) where {e j } |V | j=1 are the final embeddings of the graph nodes, and {s i } t i=1 are the decoder block's representation of the first t generated tokens. 

 Summary Generation We denote the final output of the decoder as z 1 , ..., z t . To produce the next token y t+1 , we employ a linear layer W to project z t to a vector of the same size of the dictionary. And the predicted distribution of y t+1 is obtained by: p t+1 = ?(W z t ) (4) During training, we use cross entropy as the loss function L(?) = ? n t=1 y T t log(p t ), where y t is the one-hot vector for the t-th token, and ? represent the parameters in the network. 

 Fact Corrector To better utilize existing summarization systems, we propose a Factual Corrector model, FC, to improve the factual consistency of any summary generated by abstractive systems. FC frames the correction process as a seq2seq problem: given an article and a candidate summary, the model generates a corrected summary with minimal changes to be more factually consistent with the article. While FASum has a graph attention module in the transformer, preventing direct adaptation from pre-trained models, the FC model architecture adopts the design of the pre-trained model UniLM  (Dong et al., 2019) . We initiailized the model weights from RoBERTa-Large . The finetuning process is similar to training a denoising autoencoder. We use back-translation and entity swap for synthetic data generation. For example, an entity in the ground-truth summary is randomly replaced with another entity of the same type from the article. This modified summary and the article is sent to the corrector to recover the original summary. In the experiments, we gener-ated 3.0M seq2seq data samples in CNN/DailyMail and 551.0K samples in XSum for finetuning. We take 10K samples in each dataset for validation and use the rest for training. During inference, the candidate summary from any abstractive summarization system is concatenated with the article and sent to FC, which produces the corrected summary. 

 Experiments 

 Datasets We evaluate our model on benchmark summarization datasets CNN/DailyMail  (Hermann et al., 2015)  and XSum  (Narayan et al., 2018) . They contain 312K and 227K news articles and humanedited summaries respectively, covering different topics and various summarization styles. 

 Implementation Details We use the Huggingface's  (Wolf et al., 2019)  implementation of transformer in BART . We also inherit their provided hyperparameters of CNN/DailyMail and XSum for the beam search. The minimum summary length is 56 and 11 for CNN/Daily Mail and XSum, respectively. The number of beams is 4 for CNN/DailyMail and 6 for XSum. In FASUM, both the encoder and decoder has 10 layers of 10 heads for attention. Teacher forcing is used in training. We use Adam (Kingma and Ba, 2014) as the optimizer with a learning rate of 2e-4. The bi-LSTM to produce the initial embedding of graph nodes has a hidden state of size 64 and the graph attention network (GAT) has 8 heads and a hidden state of size 50. The dropout rate is 0.6 in GAT and 0.1 elsewhere. We use the subword tokenizer SentencePiece  (Kudo and Richardson, 2018) . The dictionary is shared across all the datasets. The vocabulary has a size of 32K and a dimension of 720. The correction model FC follows the UniLM  (Dong et al., 2019)  architecture initialized with weights from RoBERTa-Large . We fine-tune the model for 5 epochs with a learning rate of 1e-5 and linear warmup over the one-fifths of total steps and linear decay. During decoding, it uses beam search with a width of 2, and blocks trigram duplicates. The batch size during finetuning is 24. More details are presented in the Appendix. 

 Metrics To evaluate factual consistency, we re-implemented and trained the FactCC model  (Kry?ci?ski et al., 2019b) . The model outputs a score between 0 and 1, where a higher score indicates better consistency between the input article and summary. The training of FactCC is independent of our summarizer so no parameters are shared. More details are in the Appendix. We also employ the standard ROUGE-1, ROUGE-2 and ROUGE-L metrics  (Lin, 2004)  to measure summary qualities. These three metrics evaluate the accuracy on unigrams, bigrams and the longest common subsequence. We report the F1 ROUGE scores in all experiments. And the ROUGE-L score on validation set is used to pick the best model for both FASUM and FC. 

 Baselines The following abstractive summarization models are selected as baseline systems. TCONVS2S  (Narayan et al., 2018)  is based on topic modeling and convolutional neural networks. BOTTOMUP  (Gehrmann et al., 2018)  uses a bottom-up approach to generate summarization. UNILM  (Dong et al., 2019)  utilizes large-scale pretraining to produce state-of-the-art abstractive summaries. We train the baseline models when the predictions are not available in their open-source repositories. 

 Results As shown in Table  3 , our model FASUM 2 outperforms all baseline systems in factual consistency scores in CNN/DailyMail and is only behind UNILM in XSum. In CNN/DailyMail, FA-SUM is 0.6% higher than UNILM and 3.9% higher than BOTTOMUP in factual score. Statistical test shows that the lead is statistically significant with pvalue smaller than 0.05. The higher factual score of UNILM among baselines corroborates the findings in  Maynez et al. (2020)  that pre-trained models exhibit better factuality. But our proposed knowledge graph component can help the train-from-scratch FASUM model to excel in factual consistency. We conduct ablation study to remove the knowledge graph component from FASUM, resulting in the SEQ2SEQ model. As shown, there is a clear drop in factual score: 2.8% in CNN/DailyMail and 0.9% in XSum. This proves that the constructed knowledge graph can help increase the factual correctess of the generated summaries. It's worth noticing that the ROUGE metric does not always reflect the factual consistency, sometimes even showing an inverse relationship, a phenomenon observed in multiple studies  (Kry?ci?ski et al., 2019a; Maynez et al., 2020) . For instance, although BOTTOMUP has 0.69 higher ROUGE-1 points than FASUM in CNN/DailyMail, there are many factual errors in its summaries, as shown in the human evaluation. On the other hand, to make sure the improved factual correctness of our models is not achieved by simply copying insignificant information from the article, we conduct analysis on abstractiveness in Section 4.6.1 and human evaluation in Section 4.6.3. Furthermore, the correction model FC can effectively enhance the factual consistency of summaries generated by various baseline models, especially when the original summary has a relatively low factual consistency. For instance, on CNN/DM, the factual score of BOTTOMUP increases by 1.4% after correction. On XSum, after correction, the factual scores increase by 0.2% to 3.1% for all baseline models. Interestingly, FC can also boost the factual consistency of our FASUM model. Furthermore, the correction has a rather small impact on the ROUGE score, and it can improve the ROUGE scores of most models in XSum dataset. We check and find that FC only makes modest modifications necessary to the original summaries. For instance, FC modifies 48.3% of summaries generated by BOTTOMUP in CNN/DailyMail. These modified summaries contain very few changed tokens: 94.4% of these corrected summaries contain 3 or fewer new tokens, while the summaries have on average 48.3 tokens. In the appendix of supplementary materials, we show several examples of summaries given by FA-SUM and corrected by FC to demonstrate the improved factual consistency of summarization. 

 Insights 

 Novel n-grams It has been shown in  Durmus et al. (2020)  that less abstractive summaries are more factual consistent with the article. Therefore, we inspect whether our models boost factual consistency simply by copying more portions of the article. On XSum's testset, we compute the ratio of novel n-grams in summaries that do not appear in the article. Figure  2  shows that FASUM achieves the closest ratio of novel n-gram compared with reference summaries, and higher than BOTTOMUP and UNILM. This demonstrates that FASUM can produce highly abstractive summaries while ensuring factual consistency. 

 Relation Matching Rate While the factual consistency evaluator FactCC  (Kry?ci?ski et al., 2019b ) is based on pre-trained models, it requires finetuning on articles and labelled summaries. Furthermore, we empirically find that the performance of FactCC degrades when it is finetuned on one summary dataset and used to evaluate models on another dataset. Therefore, in this subsection, we design an easyto-compute model-free factual consistency metric, which can be used when ground-truth summaries are not available. As the relational tuples in the knowledge graph capture the factual information in the text, we compute the precision of extracted tuples in the summary. In detail, suppose the set of the relational tuples in the summary is R s = {(s i , r i , o i )}, and the set of the relational tuples in the article is R a . Then, each tuple in R s falls into one of the following three categories: 1. Correct hit (C): (s i , r i , o i ) ? R a ; 2. Wrong hit (W): (s i , r i , o i ) ? R a , but ?o = o i , (s i , r i , o ) ? R a , or ?s = s i , (s , r i , o i ) ? R a ; 3. Miss (M): Otherwise. We define two kinds of relation matching rate (RMR) to measure the ratio of correct hits: RMR 1 = 100 ? C C + W (5) RMR 2 = 100 ? C C + W + M (6) Note that this metric is different from the ratio of overlapping tuples proposed in  Goodrich et al. (2019) , where the ratio is computed between the ground-truth and the candidate summary. Since even the ground-truth summary may not cover all the salient information in the article, we choose to compare the knowledge tuples in the candidate summary directly against those in the article. An additional advantage of our metric is that it does not require ground-truth summaries to be available. We also compute factual consistency via natural language inference models following  Maynez et al. (2020) . We use the BERT-Large model finetuned on MNLI dataset  (Williams et al., 2018)  provided by fairseq . The model predicts the relationship between the article and summary to be one of the following: entailment, neutral and contradiction. We report the ratio of contradiction as predicted by the model in Table  4 . As shown, FA-SUM achieves the lowest ratio and FC helps further reducing conflicting facts in generated summaries. 

 Human Evaluation We conduct human evaluation on the factual consistency and informativeness of summaries. We randomly sample 100 articles from the test set of CNN/DailyMail. Then, each article and summary pair is labelled by 3 people from Amazon Mechanical Turk (AMT) to evaluate the factual consistency and informativeness. Each labeller gives a score in each category between 1 and 3 (3 being perfect). The kappa-ratio between reviewer scores is 0.32 for  factual consistency and 0.28 for informativeness. Here, factual consistency indicates whether the summary's content is faithful with respect to the article; informativeness indicates how well the summary covers the salient information in the article. As shown in Table  5 , our model FASUM achieves the highest factual consistency score, higher than UNILM and considerably outperforming BOTTOMUP. We conduct a statistical test and find that compared with UNILM, our model's score is statistically significant with p-value smaller than 0.05 under paired t-test. In terms of informativeness, our model is comparable with UNILM and outperforms BOTTOMUP. Finally, without the knowledge graph component, the SEQ2SEQ model generates summaries with both less factual consistency and informativeness. To assess the effectiveness of the correction model FC, we conduct a human evaluation of sideby-side summaries. In CNN/DailyMail, we randomly sample 100 articles where the summaries generated by BOTTOMUP are modified by FC. 3 labelers are asked whether the original or the corrected version is factually more correct. We collect all the feedbacks and compute the ratio of judgements for each case. To reduce bias, we randomly shuffle the two versions of summaries. We conduct similar evaluation on UNILM. As shown in Table  6 , the corrected summaries are significantly more likely to be judged as more factually correct for both baseline models. For example, 42.3% of the judgements think the corrected summaries are factually more correct, 42.7% think the corrected version neither improves nor worsens the factual consistency, while only 15.0% think that the corrected version becomes worse than the original BOTTOMUP summary. Therefore, FC can help boost the factual consistency of summaries from given systems. Finally, to evaluate the quality of the relation matching rate (RMR), we compute the correlation coefficient ? between the factual score given by human labelers and the RMR value. The result shows that ? = 0.43, indicating observable relationship between RMR and human evaluation results. 

 Conclusion In this paper, we extract factual information from the article to be represented by a knowledge graph. We then integrate this factual knowledge into the process of producing summaries. The resulting model FASUM enhances the ability to preserve facts during summarization, demonstrated by both automatic and human evaluation. We also present a correction model, FC, to rectify factual errors in candidate summaries. Furthermore, we propose an easy-to-compute model-free metric, relation matching rate, to measure factual consistency based on the overlapping ratio of relational tuples. For future work, we plan to integrate knowledge graphs into pre-training for more accurate and factually consistent summarization. Moreover, we will combine the internally extracted knowledge graph with an external knowledge graph (e.g. Con-ceptNet) to enhance the commonsense capability of summarization models. 
