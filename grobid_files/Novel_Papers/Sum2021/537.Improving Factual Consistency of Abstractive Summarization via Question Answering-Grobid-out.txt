title
Improving Factual Consistency of Abstractive Summarization via Question Answering

abstract
A commonly observed problem with the stateof-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training. Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation.

Introduction Recent advances in neural text generation have led to significant improvement in the quality of abstractive summarization  (Radford et al., 2019; Gehrmann et al., 2019; Lewis et al., 2019) . Despite this progress, there are still many limitations facing neural text summarization  (Kryscinski et al., 2019) , the most serious of which is the tendency to generate summaries that are not factually consistent with the input document; a factually consistent summary only contains statements that can be inferred from the source document. Recent studies show that about 30% of the summaries generated by neural network sequence-to-sequence (seq2seq) models suffer from fact fabrication  (Cao et al., 2018) . The standard training approach for seq2seq learning has been maximizing the log likelihood of the target given the input sequences (MLE). It has empirically performed well as a surrogate loss Input: ..."Klitschko doesn't have the legs, the power that he used to," said Lewis. "He has a chink in his armour after getting beat by Tyson Fury. Anthony Joshua is now taking that challenge, going after the man." ... 

 MLE: Anthony Joshua has a "chink in his armour" ahead of his world heavyweight title bout with Wladimir Klitschko, says former champion Lennox Lewis. CONSEQ: Wladimir Klitschko has a "chink in his armour" and is no match for British champion Anthony Joshua, says former world heavyweight champion Lennox Lewis. for evaluation metrics such as BLEU and ROUGE. This empirical success can be ascribed to the fact that both BLEU and ROUGE are directly linked to the n-gram overlap between the output and the target sequences, which can be efficiently learned via MLE. In contrast, metrics to capture factual consistency are much more elusive as they must take into account the relations among tokens in the context of an entire sequence. The widely used ROUGE score is inadequate to quantify factual consistency  (Kryscinski et al., 2019) . In fact, the lack of an effective (automatic) metric for factual consistency has been the major hurdle in improving abstractive summarization model training beyond MLE. Table  1  shows an example of a factually inconsistent summary generated by finetuning the BART-large model  (Lewis et al., 2019) , which is a transformer based seq2seq model pretrained on a large corpus with denoising objectives. Standard MLE training produces summaries with factual errors that, in addition to hallucinating facts, sometimes even contradict the input article. To make abstractive summarization models produce more factually consistent summaries, we need two critical components: an automatic evaluation metric for factual consistency and an effective training algorithm that maximizes factualness. Our main contributions lie in both areas. First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol . Evaluating QAGS is computationally expensive and ill-suited for being part of the model training process. Our proposed protocol achieves a 55x speedup while correlating closely with QAGS 1 . Second, we propose a new contrastive learning method that uses factualness as a training objective. We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation. 

 An Efficient Metric for Factual Consistency In order to improve factual consistency of summarization models, we must have a metric to quantify it. In addition, the metric needs to be computationally efficient so that we can incorporate it as part of the model training process. We first describe the QAGS protocol and then present our QUALS protocol. 

 Background on QAGS Given a summary and an input document, QAGS  scores the summary using a 4-steps pipeline: firstly, it extracts the named entities and noun phrases in the summary as  We show the QAGS pipeline in the top part of Figure  1 . QAGS has the advantage of being interpretable and is shown to correlate well with human evaluation. However, using QAGS directly as a part of the training process presents several challenges. First, QAGS requires three separate models for AE, QG and QA. In addition to the summarization model being trained, these models consume a significant amount of machine memory. Second, performing these three steps separately takes a significant amount of time. For good coverage in QAGS, multiple answers are extracted for a given summary and multiple questions are generated for each answer. This means the QA model needs to perform inference on an exploding number of inputs even for one summary. Indeed, QAGS evaluation on a training set would take 584 days on a single GPU. 2 

 QUALS (ours) In order to enable the use of a QA driven metric to maximize factual correctness during the training of summarization models, we propose QUALS (QUestion Answering with Language model score for Summarization), which is illustrated in the bottom part of Figure  1 . QUALS is an efficient metric that employs a single neural language model (QAGen), as proposed in  (Shakeri et al., 2020) , to generate both the questions and answers from the summary. In particular, given a summary, QAGen outputs a question-answer (q-a) pair jointly, separated by a special token <a> as shown in Figure  2 . Let LL summ (q, a) be the average log likelihood of generating the q-a pair from the given summary: LLsumm(q, a) = 1 Nq + Na ? ? Nq i=1 log pQAGen(q i |summ, q <i ) + Na i=1 log pQAGen(a i |summ, q, a <i ) , where N q and N a are the number of tokens for the question and answer, respectively. Note that we consider the log likelihood scores over both the question and answer tokens to account for factual consistency of both. To obtain good coverage and diverse q-a pairs, we use diverse beam search  (Vijayakumar et al., 2016)  to generate 60 q-a pairs for a given summary with 60 diverse beam groups and a diverse beam strength of 0.5. We then filter out low-quality q-a pairs by keeping only those with answers found in the input summary. When multiple q-a pairs share the same answer, we only select the pair with the highest LL summ (q, a). Then given the input document, we simply evaluate the average log likelihood of the QAGen model producing the same q-a pairs, denoted as LL doc (q, a). Formally, given a summary and input document, QUALS score is computed as follows: QU ALS(doc, summ) = 1 M M i=1 (LL doc (q i , a i ) ?LL summ (q i , a i )) , where M is the number of q-a pairs selected on the summary. There are two justifications for taking the difference between the log likelihood scores. 1. LL doc (q, a) alone only indicates the likelihood of the q-a pair given the document; subtracting LL summ (q, a) baselines it with the likelihood of generating the q-a pair given the summary. E.g. a low LL doc (q, a) does not necessarily imply factual inconsistency -it can be caused by the fact that the q-a pair itself is generated with low likelihood from the summary in the first place. 2. Documents may vary in style, vocabulary and topic, which lead to variations in log likelihood scores unrelated to factual consistency; LL doc (q, a) ? LL summ (q, a) can help normalize these domain-related shifts since both the document and summary share the same basic style, vocabulary and topic. 

 Improving Factual Consistency Through Contrastive Learning Although QUALS can be computed more efficiently, using it in the training process is not straightforward because one would need to backpropagate through generated sumaries and qa pairs. We present our CONSEQ (CONtrastive SEQ2seq learning) algorithm that can effectively maximize such metrics in training. To fix notation, x = x 1 , . . . , x m denotes a sequence of input tokens; y = y 1 , . . . , y n denotes a sequence of target output tokens; ? = ?1 , . . . , ?n denotes a sequence of generated tokens from a seq2seq model via sampling, i.e. ? ? p ? (?|x), where ? is the parameter of the model. Let r(?, x) be the evaluation (in our case the QUALS) metric that we aim to maximize. 

 CONSEQ First, we train an initial seq2seq model with parameters ? 0 using the original labeled training set {x  (i)  , y (i) } via MLE. Second, we collect ground truth labeled training target sequences y (i) as well as the sampled sequence ?(i) to form a set of candidate sequences S = {y  (i)  , ?(i) }. Third, we construct S + and S ? from S based on the evaluation scores r and minimize the following loss function from the initial parameters ? 0 : L contrast = ?E x,s?S + log p ? (s|x) L + contrast (1) ?E x,s?S ? log (1 ? p ? (s|x)) L ? contrast . Intuitively, S + consists of highly rewarded sequences (factually consistent summaries) and minimizing L + contrast forces the model to generate high score sequences; likewise, S ? consists of poorly rewarded sequences (factually inconsistent summaries) and minimizing L ? contrast forces the model to move away from low score sequences. We present the full method in Algorithm 1. 

 Comparison with REINFORCE: The typical approach to directly optimize a nondifferentiable evaluation score during training Algorithm 1: CONSEQ Input: Initial seq2seq (summarization) model weights ? 0 via MLE, input and target sequences {x  (i)  , y (i) }, evaluation metric r. Initialize k = 0; while not converged do Sample candidate sequences {? (i) } for input sequences {x (i) } and ? k ; Construct S + and S ? as described in Sec. 3; Minimize the contrastive loss in Eq. 1 to obtain ? k+1 ; k = k + 1; end is the REINFORCE algorithm  (Williams, 1992) . REINFORCE samples a sequence ? at each iteration and updates the model with the gradient (r(?, x) ? r(b, x)) ? log p ? (?|x), (2) where b is a baseline sequence, conditionally independent of ? given ?, x. To see the connection with CONSEQ, suppose the reward r is either 0 or 1. If r(?, x) = 1 and r(b, x) = 0, the sampled sequence ? is strongly rewarded compared to baseline and Eq. 2 reduces to ? log p ? (?|x). On the other hand, if r(?, x) = 0 and r(b, x) = 1, the sampled sequence is strongly discouraged and Eq. 2 reduces to ? ? log p ? (?|x), which pushes the model away from generating ?. This pulland-push effect is analogous to the L + contrast and L ? contrast terms in the loss Eq. 1 in CONSEQ. Note that the gradient updates of REINFORCE are entirely based on the sampled sequences. In contrast, CONSEQ takes advantage of the ground truth targets in addition to the sampled ones, which help avoid the instability of REINFORCE. Indeed, we implemented the REINFORCE algorithm with the BART-large model fine-tuned under MLE objective as initialization; we found that after a few hundred updates the summaries sampled from the model become unintelligible and our reward function fails to compute the scores (no meaningful q-a pairs can be generated based on the summaries). 

 CONSEQ + QUALS for Imposing Factual Consistency We The last step is to take the intersection of the examples between ?+ and ? to form S + and S ? , respectively. For example, we select a summary s from ?+ to be included in S + if and only if there exists a summary s in ? such that s and s correspond to the same input document. As a result of the above process, the contrastive loss in Eq. 1 can thus push the model from the inconsistent summary towards the consistent one for the same input document. Next, we describe two variants of the CONSEQ algorithm. Weighted loss: We can weight the losses in Eq. 1 using QUALS scores and minimize the following loss, assuming normalization of 0 ? r ? 1: L contrast = ?E x,s?S + r(s, x) log p ? (s|x) ? E x,s?S ? (1 ? r(s, x)) log (1 ? p ? (s|x)) , where r(s, x) is the QUALS score for summary s and input document x. Evaluation metrics: We use the ROUGE  (Lin, 2004)  to measure general summarizaiton quality. For factual consistency, we use the QAGS protocol (see Appendix for more details) as well as the FactCC model  (Kry?ci?ski et al., 2019)  downloaded directly from the official website.  4  In contrast to QAGS, FactCC is a BERT-based classification model that makes a binary prediction if the given claim sentence is factually consistent or not with the given input document. Implementation details: We use the Fairseq  (Ott et al., 2019)  implementation of BART-large  (Lewis et al., 2019)  for the summarization model as it is shown to achieve the state-of-the-art ROUGE scores for this task. We fine-tune the BART-large model with the standard learning rate of 3 ? 10 ?5 4 https://github.com/salesforce/factCC on XSUM and CNNDM respectively to establish the MLE baselines. We then initialize CONSEQ with the MLE baseline models. In CONSEQ we use a learning rate of 3 ? 10 ?6 . For evaluation, we generate summaries using beam search with beam sizes of 4 and 6 for CNNDM and XSUM, respectively. The generated summaries are limited to 55-140 and 10-60 tokens in lengths for CNNDM and XSUM, respectively. Our QAGen model in QUALS is also a BART-large model fine-tuned on the SQUAD  (Rajpurkar et al., 2016)  and NewsQA  (Trischler et al., 2017)  datasets. To construct the S + and S ? , we found that selecting the p = 30% and 50% leads to the best result on the validation set of XSUM and CNNDM, respectively, among the choices of p = 25, 30, 50, 75, 90. 

 QUALS Approximates QAGS We first verify that our proposed QUALS metric correlates well with QAGS. We evaluate both QUALS and QAGS on the same set of summaries generated by the MLE baseline model on the test set of documents in XSUM and CNNDM, respectively. The examples are grouped into bins based on the percentiles of the QUALS scores. We then plot the average QAGS score of the examples within each bin. As shown in Figure  3  (a more fine-grained plot is shown in Figure  4  of the Appendix), QUALS correlates very well with QAGS in both datasets. Since our method only relies on ranking QUALS scores in contrastive learning, monotonicity of QUALS with respect to QAGS is sufficient. 

 Results We compare our proposed method QUALS-CONSEQ to the state-of-the-art abstractive summarization model (BART-large MLE). In an ablation study, we check the effect of changing the QUALS metric as well as the effect of changing the CONSEQ algorithm. We summarize the results in Table  2  and Table  3 . We observe that our proposed method QUALS-CONSEQ (Q-C) achieves more than 4 points improvement in QAGS over the MLE baseline in XSUM and about 2 points improvement in CNNDM, where we also achieve a slightly better ROUGE over MLE. Improving ROUGE is not the goal of our paper; what we show is that we can significantly improve factual consistency of summaries without degrading ROUGE, as is common practice  (Kedzie and McKeown, 2019) . Next, we describe the various ablation settings. 1) In R-C (ROUGE-CONSEQ), we simply use the sum of ROUGE-1,2,L scores to evaluate the generated summaries against the ground truth summaries as the metric in constructing S + and S ? . In both Table  2  and 3 it results in poorer QAGS than the MLE baseline. This confirms the necessity of having an effective metric for factual consistency. Note that R-C even results in poorer ROUGE scores. We believe this is caused by the fact that ROUGE is already highly optimized by the MLE model and it is used as initialization for R-C; the "hard" examples where the MLE model couldn't produce good ROUGE scores may be inherently problematic (e.g. hallucination in the ground truth summary); focusing on these examples by R-C can therefore make the model weaker on other examples. 2) In Q-F1-C (QUALS-F1-CONSEQ), we make a modification to QUALS. Instead of measuring the factual consistency in terms of the log likelihood scores, we measure the F1 between generated answers from the summary and the input document in the QAGen model. In particular, given a summary as input, the QAGen model generates a q-a pair q, a. We then use the corresponding document as input to the QAGen model and force the decoder to generate the question tokens q and allow the QAGen to generate the answer tokens a . We then compute the F1 overlap score between a and a . This would be closer to the QAGS setting where explicit answers are generated and compared. We observe that in Table  3 , Q-F1-C achieves a slightly higher QAGS than Q-C. But overall Q-F1-C performs worse than Q-C. We believe this is due to the fact the log likelihood scores are softer than F1 and can potentially account for answers that are semantically similar. 3) In Q-C-W (QUALS-CONSEQ-Weighted), we use the weighted version of CONSEQ as described in Sec. 3. Since the QUALS score is a difference between log likelihood scores, it can have negative values. We evaluate the QUALS on the training examples to obtain an interval of its values and linearly normalize the QUALS as weights in the loss function. We observe that it improves the factual consistency over the MLE baseline but not as much as Q-C. 4) In Q-C-O (QUALS-CONSEQ-Online), we use the online version of CONSEQ as described in Sec. 3. We sample about 6 examples in a mini-batch and select 2 of them for S + and S ? per GPU with a total of 40 GPUs. We observe that it tends to achieve higher ROUGE scores but lower factual consistency scores compared to Q-C. 5) In Q-P (QUALS-Positive), we only use the positive summaries (S + ) and the positive loss L + contrast in Eq. 1 for training. We observe that it achieves lower factual consistency scores compared to Q-C and this shows that the negative loss in CONSEQ is useful to boost factual consistency. FactCC results: As shown in Table  3  for CNNDM, Q-C achieves over 4 points improvements in FactCC score over the MLE baseline. However, in Table  2  for XSUM, Q-C has about 1 point lower FactCC score than the MLE baseline. We investigated this issue and found that the ground truth summaries of the XSUM test set have a FactCC score of just 21.0, which means that only 21% of the ground truth summaries in XSUM are judged as factual according to FactCC. This suggests that the FactCC model is not well suited for making predictions on highly abstractive summaries. This is not surprising as the authors of FactCC mentioned in Sec. 3.1  (Kry?ci?ski et al., 2019)  that FactCC is built on the premise that "..the level of abstraction of generated summaries is low and models mostly paraphrase single sentences and short spans from the source". Unfortunately for XSUM, this premise does not hold. Comparison with Other Methods: There are 2 other methods in the literature  (Cao et al., 2018;  for improving factual consistency of summarization models. Both rely on information extraction (OpenIE) to extract relations and incorporate the relation representations into seq2seq models. The authors in  proposed a Fact-Aware Summarizer (FASum) and a Fact Corrector model. In Table  4  of their paper, the FASum achieves significantly lower ROUGE scores (30.28/10.03/23.76 and 40.53/17.84/37.4 for ROUGE-1/2/L on XSUM and CNNDM respectively). This indicates a significant gap in the summary quality. Even their best result, which is using Fact Corrector on UniLM  (Dong et al., 2019) , achieves lower ROUGE scores than BART-large MLE. Although the authors in  used FactCC as an evaluation metric, they did not use the official method to train FactCC; they used the ground truth summaries rather than sampled sentences from the input documents as positive examples. As a result, we are not able to compare the FactCC numbers reported in . Nevertheless, we can observe that there is little or no improvements for Fact Corrector on UniLM according to FactCC. We believe that this is because the recent large transformer-based, pre-trained seq2seq models such as UniLM and BART have significantly improved the summarization quality and it is much more challenging to improve even the factual consistency of these state-of-the-art models. In comparison, our results reported in Table  2  and Table  3  represent significant improvements. The authors in  (Cao et al., 2018)   the Gigaword corpus  (Rush et al., 2015)  and did not release their code so we were unable to compare to their method. However, given the recent progress in transformer-based seq2seq models, it is likely that our BART-large MLE baseline outperforms their RNN-based models. Again, we believe that it is much easier to improve factual consistency of a weak seq2seq model than that of a strong model (such as UniLM or BART-large) as shown in . 

 Human evaluation: We use Amazon SageMaker Ground Truth 5 to conduct human evaluation. We sample 100 examples from the test set of XSUM and CNNDM, respectively. In each task, we present an input document, together with the generated summaries from the BART-large MLE and QUALS-CONSEQ models. We ask the annotators to select which of the two summaries they prefer along 3 dimensions: factual consistency, informativeness and grammatical correctness. For each of these dimensions they can also choose "Equal" if they feel that both summaries are of similar quality. Our annotators consist of 10 data associates who are native English speakers whose background includes training in linguistic annotation. Each task is performed by 3 different annotators and we take the majority vote. We provide the detailed setup and instructions in the Appendix. The result of human evaluation is reported in Table  4 , showing the percentage of examples along these three dimensions. In both datasets, we observe that QUALS-CONSEQ clearly improves the factual consistency of the generated summaries compared to the BART-large MLE baseline. We notice that the improvement in informativeness is even greater. Fleiss's Kappa  (Fleiss et al., 1971)  shows fair agreement for factual consistency, informativeness and grammatical correctness choices (0.136/0.270/0.043 for XSUM and 0.237/0.202/0.206 for CNNDM). We note, however, that most disagreements occur when one annotator rates two summaries as equal and another rates one of the two as either better or worse. To measure this, we computed Fleiss's Kappa again, counting equal and either better or worse as equivalent (and better and worse as not equivalent). Here, our agreement is almost perfect (0.837/0.839/0.975 for XSUM and 0.945/0.816/0.967 for CNNDM). We thus see that annotators rarely directly contradict each other on rating one summary above or below another, but often have a hard time deciding when the two summaries are equal. 

 Qualitative Analysis We analyzed the human evaluation results and found several types of improvements/errors produced by QUALS-CONSEQ. Our model is able to rectify factual errors found in MLE such as 1) entity hallucination and errors (Example 1 and 2 in Table  5 ) and 2) relations and co-reference (see Table  1  and Example 3 in Table  5 ). QUALS-CONSEQ also made mistakes in cases where it was not sensitive to certain modifier phrases (extra "more than" in Example 2 in Table  5 ). More examples of generated summaries and q-a pairs are in the Appendix. Illustration of QUALS: We take an example to illustrate how QUALS captures the factual inconsistency of summaries. The BART-large MLE model generate a summary: The AirAsia flight 4U 9525 crash was the latest in a series of tragedies that have hit the aviation industry. The input document described the AirAsia crash but did not mention the flight number. In fact, "4U 9525" is the Germanwings flight that crashed in the French Alps. The model hallucinated the flight number because it appeared in several training examples that cover the Germanwings crash. Given the above summary, our QAGen model generates the following q-a pairs: Q1: What was the name of the flight that crashed? A1: 4U 9525. Q2: Which airlines flight crashed? A2: AirAsia. In Figure  5  in the Appendix we show the negative log likelihood per subword token on these q-a pairs conditioned on the summary (blue) and input document (orange). The answer to the first question is very likely according to the summary while extremely unlikely according to the input document, indicating factual inconsistency. On the other hand, "AirAsia" is factually consistent and the second q-a pair is likely according to the input document. The QUALS score for the two q-a pairs are ?2.615 and ?0.054, respectively. 

 Related work Several authors have pointed out the problem of factual inconsistency in abstractive summarization models  (Kryscinski et al., 2019; Cao et al., 2018; Durmus et al., 2020) . Besides QAGS  and FactCC  (Kry?ci?ski et al., 2019) , another possible approach to quantify factual consistency is to rely on Open Information Extraction (OpenIE) and dependency parsing tools to identify and match the relations in an input document and its summary  (Cao et al., 2018; . However, the underlying OpenIE tools are often not accurate enough to be used for this purpose. Our proposed CONSEQ algorithm is related to the unlikelihood training  (Welleck et al., 2019; Li et al., 2019)  as both have positive and negative loss terms. The key difference is that in unlikelihood training, the negative loss serves as a regularization term, weighted by a hyperparameter ?, in addition to the regular MLE training. In contrast, our CONSEQ is motivated from the REINFORCE algorithm and treats the positive and negative terms equally. Furthermore, while the unlikelihood training uses all the ground truth sequences equally in the regular MLE (positive) loss term, we construct the positive and negative sets by incorporating the reward function (e.g. QUALS) as discussed in Sec. 3. In another related work, factual consistency metrics at the entity level have been proposed  (Nan et al., 2021) . The authors also investigated several techniques such as data cleaning, multitask learning and entity-augmented decoding to improve entity level factual consistency scores of abstractive summarization models. In contrast, the QUALS metric that we propose is more general, not limited to entities. Another recent work tackles the hallucination problem in abstractive text summarization via post processing on the generated summary  (Chen et al., 2021) . Specifically, entities of the generated summaries are swapped with other named entities of the same type found in the original document to form a set of candidate summaries. The final summary is determined by a ranking model trained to prefer the factually consistent summaries. 

 Conclusion In this paper we proposed to improve the factual consistency of abstractive summarization models. We first proposed an efficient evaluation protocol called QUALS to measure factual consistency. We then proposed a contrastive learning algorithm for seq2seq models called CONSEQ to maximize QUALS during training. We demonstrated that our proposed method significantly improves the factual consistency of the current state-of-theart summarization model measured by automatic metrics as well as side-by-side human evaluation. In addition to improving factual consistency of summarization models, we believe that the CONSEQ algorithm can have a wider impact on training seq2seq models in general to incorporate non-differentiable evaluation metrics into model training. 
