title
Efficient Attentions for Long Document Summarization

abstract
The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose HEPOS, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with HEPOS, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GOVREPORT, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors. tokens with a batch size of 1, 70GB of memory is needed for encoder attentions, and 8GB for encoder-decoder attentions. 2 Our code is released at https://github.com/ luyang-huang96/LongDocSum. 3 GOVREPORT can be downloaded from https:// gov-report-data.github.io.

Introduction Long documents, such as scientific papers and government reports, often discuss substantial issues at length, and thus are time-consuming to read, let alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main topics, yet prior work has mostly focused on short texts (containing hundreds of words), e.g., news articles  (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019) . Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems  (Lewis et al., 2020; Zhang et al., 2019)  are built upon Transformer  (Vaswani et al., 2017) , which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents  1  . Solutions have been proposed to reduce  1  For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions  (Wang et al., 2020c; Zaheer et al., 2020 ) by selectively attending to neighboring tokens  (Beltagy et al., 2020; Child et al., 2019)  or relevant words  (Kitaev et al., 2020; Tay et al., 2020a ). Yet, these methods do not apply to encoder-decoder attentions in summarization models since they collaborate and dynamically pinpoint salient content in the source as the summary is decoded. Truncation is commonly used to circumvent the issue. However, training on curtailed content further aggravates "hallucination" in existing abstractive models  (Maynez et al., 2020) . We argue that summarizing long documents (e.g., with thousands of words or more) requires efficient handling of both types of attentions. To this end, we propose an efficient encoder-decoder attention with head-wise positional strides (HEPOS), where the attention heads follow a strided pattern and have varying starting positions. HEPOS reduces computational and memory costs while (1) maintaining the power of emphasizing important tokens, and (2) preserving the global context per head. HEPOS successfully doubles the processed input sequence size, when combined with any encoder. To the best of our knowledge, we are the first to study efficient encoder-decoder attentions and provide a systematic comparison of diverse encoder attentions for the task of summarization.  2  For evaluation, we collect a new large-scale dataset, GOVREPORT, consisting of about 19.5k U.S. government reports with expert-written abstractive summaries.  3  GOVREPORT has two important features: (1) It contains significantly longer documents (9.4k words) and summaries (553 words) than existing datasets, such as PubMed and arXiv  (Cohan et al., 2018)  (see Table  2 ); (2) Salient 1420 content is spread throughout the documents, as opposed to cases where summary-worthy words are more heavily concentrated in specific parts of the document. These properties make GOVREPORT an important benchmark for producing long document summaries with multiple paragraphs. We conduct experiments on GOVREPORT and scientific papers in PubMed and arXiv. First, when summarizing documents of the same length, HEPOS attention yields significantly better ROUGE scores than a non-trivial comparison that projects attentions into low-rank space  (Wang et al., 2020c) . Second, when trained on the same GPU, HEPOS attention, combined with sparse encoder attentions, is able to read more than 10K words and obtains significantly higher ROUGE scores on GOVREPORT and new state-of-the-art results on PubMed, compared with full encoder-decoder attention models which can process at most 5K input words. Human judges further rate the summaries generated by our models to be more informative and faithful. We further propose a new evaluation metric for faithfulness, inspired by APES  (Eyal et al., 2019) , a fill-in-the-blank QA metric for summary evaluation. With questions generated from references, our metric, APES src , compares QA answers by reading the source and the system summary. It is shown to be better correlated with human judgment than the original metric and an entailment-based scorer  (Kryscinski et al., 2020) . The rest of the paper is organized as follows. We describe efficient encoder attentions in prior work in ? 2, and formulate our proposed encoder-decoder attention in ? 3. The GOVREPORT data is presented in ? 4. We then share details on evaluation metrics ( ? 5) and experimental results ( ? 6). Additional related work is listed in ? 7, with conclusion in ?8. 

 Prior Work on Efficient Encoder Attentions Transformer models are built upon multi-head attentions in multiple layers. The attention is calcu- lated as Attention(Q, K, V) = softmax( QK T ? d k )V, where Q, K, and V are query, key, and value matrices, each consisting of n vectors for a document with n tokens, thus the quadratic memory footprint. Here, we present an overview of representative methods for efficient encoder self-attentions (henceforth "encoder attentions") that can be built upon large pre-trained seq2seq models, e.g., BART  (Lewis et al., 2020) . We follow the naming  convention of  Tay et al. (2020b) , and summarize their memory complexities and numbers of newly learned parameters in Table  1 . 

 Fixed Patterns Fixed patterns are used to limit the scope of attentions. In our experiments, in addition to windowbased attentions, we also combine them with global tokens, stride patterns, or random attentions. Sliding window attentions  (Beltagy et al., 2020)  aim to capture the local context, which is critical for language understanding  (Liu* et al., 2018; Child et al., 2019) . Concretely, each query token attends to w/2 neighboring tokens on both left and right, yielding a memory complexity of O(nw). Adaptive span is proposed by  Sukhbaatar et al. (2019)  to learn attention windows at different layers. This is implemented by learning a masking function for each head independently. In practice, the adaptive span attention has a complexity of O(n ?), where ? is the maximum values of predicted spans for all heads. Besides, it introduces O(1) new parameters for learning spans. Global tokens  (Beltagy et al., 2020)  are often added to sliding windows to let pre-selected tokens attend to the full sequence, to build global representations. Importantly, global attention operations are symmetric, i.e., a global token is also attendable to all tokens in the sequence. We select the first g tokens as global tokens, as leading sentences are often important for summarization. Memory complexity is O(2ng) due to the symmetric attentions. Stride patterns are proposed by  Child et al. (2019)  to capture long term interactions, where each query attends to every s-th token, with s as the stride size. It thus has a complexity of O(n 2 /s). Random attention is motivated by the fact that randomly constructed graphs with ?(n) edges can approximate the complete graphs spectrally  (Zaheer et al., 2020) .  Zaheer et al. (2020)  propose to allow each query to attend to r random keys, resulting in a complexity of O(nr). For efficient implementations, input tokens are first segmented into blocks. Tokens in the same block attend to tokens in another randomly selected block.  et al. (2020c)  show that self-attention matrices are low-rank. They propose Linformer that linearly projects key and value matrices into a lowdimensional space, e.g., from n to k, to achieve a O(nk) complexity. It also introduces O(n) new parameters for projection matrix learning. 

 Low-rank Methods 

 Wang 

 Learnable Patterns Recently, learnable sparse attentions are proposed to better capture both local and global contexts than attentions based on fixed patterns. Locality-sensitive hashing (LSH) attentions use a random-projection hashing function to hash similar queries and keys into the same buckets in l rounds  (Kitaev et al., 2020) . Attentions are then computed among tokens within each bucket. For bucket size b l , the complexity of LSH attention is O(lnb l ). Sinkhorn attentions first segment a sequence into blocks, which are then arranged by a learned Sinkhorn sorting network  (Tay et al., 2020a) . Given the new permutation, each query attends to b s tokens within the same block to maintain the local context and another b s tokens in a neighboring block to capture global interactions. Its complexity is O(2nb s ). 

 Other Attentions We also describe several notable methods that are not suitable for our experiments and excluded from this study: Recurrence over input segments are tailored for an autoregressive decoder only  (Dai et al., 2019) ; memory methods use a separate memory module to attend to full sequences  (Lee et al. , Figure  1 : A toy example of our HEPOS attention, with a stride of 2 and four attention heads. Dark colors indicate that heads 1 and 3 attend to the first and third tokens ("Job" and "home") in the input, heads 2 and 4 look at the second and fourth words ("in" and "care"). 2019), which share a similar theoretical foundation as global tokens; and kernel methods over attentions require training models from scratch  (Choromanski et al., 2020; Katharopoulos et al., 2020) . 3 Encoder-decoder Attention with Head-wise Positional Strides (Hepos) The efficient design of encoder-decoder attentions with head-wise positional strides (HEPOS) allows models to consume longer sequences. Concretely, our design is motivated by two observations: (1) Attention heads are redundant  (Voita et al., 2019) . (2) Any individual head rarely attends to several tokens in a row  (Clark et al., 2019) . Therefore, as illustrated in Fig.  1 , HEPOS uses separate encoderdecoder heads on the same layer to cover different subsets of source tokens at fixed intervals. Each head starts at a different position, and all heads collectively attend to the full sequence. Given a stride size of s h , for the h-th head, its attention value between decoder query q j (at step j) and encoder key vector k i (for the i-th input token) can be formulated as: a h ji = softmax(qjki), if (i ? h) mod s h = 0 0 otherwise (1) In HEPOS attention, each query token attends to n/s h tokens per head, yielding a memory complexity of O(mn/s h ), where m is the output length. For comparison, Linformer ( ? 2.2) can be straightforwardly adapted for encoder-decoder attentions by using decoder queries for attention calculation instead. We do not adapt pattern-based attentions ( ? 2.1 and ? 2.3), since they rely on local token grouping which makes it difficult to pinpoint salient content. 1422 

 GOVREPORT Dataset We introduce a new large-scale dataset, GOVRE-PORT, containing 19, 466 long reports published by U.S. Government Accountability Office (GAO) 4 to fulfill requests by congressional members, and Congressional Research Service (CRS) 5 , covering researches on a broad range of national policy issues. A human-written summary is provided along with each report. During data collection, we remove boilerplates from crawled files, and keep the section and paragraph structure of the documents and summaries. Additional data cleaning and processing details are in Appendix A. We obtain 12, 228 GAO reports and 7, 238 CRS reports of high quality evidenced by human inspection of 200 parsed reports. Collected GAO reports and CRS reports have on average 6.9 and 4.6 sections, respectively. We split train, validation and test set by publication date on each dataset, and end up with 17519 training samples, 974 validation documents, and 973 test samples. Notably, summaries of GAO reports are written by experts, and are often structured into three aspects in order: "Why GAO did this study"-motivation and problem(s) under discussion, "What GAO found"-findings of the report, and "What GAO recommends"suggestions and solutions to the problem(s). All but three GAO summaries include "What GAO Found". The percentages of GAO summaries that contain "Why GAO did this study" and "What GAO recommends" are 94.8% and 29.0%. For comparison, structured summaries are also observed on PUBMED  (Cohan et al., 2018)  samples. Though they do not contain explicit aspect labels, the summaries can often be broken down into "Introduction", "Methods", "Results", and "Conclusion" via keyword matching. Details about keyword choices for each aspect are provided in Table  11  in Appendix D. Comparison with Existing Long Document Summarization Datasets. In Table  2 , we compare GOVREPORT with several existing long document summarization datasets, including PUBMED and ARXIV  (Cohan et al., 2018)  that consist of scientific publications; BILLSUM  (Kornilova and Eidelman, 2019) , a collection of congressional bills; and BIGPATENT  (Sharma et al., 2019)    GOVREPORT 19,466 553.4 17.8 9409.4 19.0 7.3   Table 2:  Statistics of GOVREPORT and existing long document summarization datasets. Comp.: compression ratio, Den.: extractive fragment density  (Grusky et al., 2018) . All values are mean over the whole dataset except for the "# Doc" column. Documents and summaries in GOVREPORT are significantly longer.  U.S. patent documents. First, documents and summaries in GovReport are significantly longer than prior datasets. Next, we inspect the distribution of summary-worthy bigrams in the source by dividing each document into ten equisized partitions. For each partition, we count the occurrence of unique bigrams that also appear in the reference, accumulated from the start of the document to the end of the partition. Fig.  2  shows that key information is spread throughout documents in GOVREPORT, with new salient bigrams being steadily added as more content is consumed. For ARXIV and BIGPATENT, only about 10% of new salient bigrams are accumulated in the second half of the documents, reflecting the heavy positional bias in these two datasets. In contrast, in GovReport and BILLSUM, more than 18% of new summary-worthy bigrams appear in the later half of the articles, showing a more even distribution. A similar trend is observed on unigrams. However, BILLSUM has the shortest documents among the five datasets. 1423 5 Summary Evaluation with Cloze QA This work aims to evaluate whether processing more text improves both informativeness and faithfulness of abstractive summaries. In addition to ROUGE  (Lin, 2004)  and human evaluation, we extend existing QA-based metric  (Eyal et al., 2019)  and consider an entailment-based scorer. QA-based Evaluation. We present a new faithfulness evaluation metric by extending the APES score  (Eyal et al., 2019) . We follow APES to construct a set of cloze questions, {q}, from each reference summary by masking entities. Events, dates, and numbers are also masked, as they are prevalent in our data. Each masked phrase becomes the goldstandard answer a ref for a question q. We do not generate natural language questions  (Durmus et al., 2020; Wang et al., 2020a) , due to the lack of accurate question generation models for the domains of government reports and scientific papers. QA models are trained by reading a question and a context to label the answer span in the context. We construct context by greedily selecting sentences that maximize the improvement of ROUGE-2 recall when compared with the reference summary. If the answer a ref cannot be found in the context, the sample is excluded from training. We train all QA models by fine-tuning BERT  (Devlin et al., 2019)  to predict the answer span. To evaluate the faithfulness of a system summary, APES uses the QA model to read the summary and a question q to label an answer a sys . It calculates a unigram F1 score by comparing a sys and a ref . Different from APES, we further use the QA model to read the context (sentences selected from the source) and give an answer a cxt to the question q. We compute a unigram F1 by comparing a sys and a cxt , denoted as APES src . Given that existing summarization models rarely rewrite names or numbers correctly, our metric can better capture faithfulness by using a gold-standard answer constructed from the source article than from the human-written abstract. To extract entities and events, we deploy a state-of-the-art IE framework, OneIE  (Lin et al., 2020)  on GOVREPORT. On PubMed, we retrain OneIE on Genia 2011  (BioNLP, 2011 ) and 2013 (BioNLP, 2013 , and PubMed  datasets to extract domain-specific entities and events, such as entities of Gene and Disease. We additionally include numbers and dates extracted by spaCy  (Honnibal and Montani, 2017) . Entailment-based Evaluation. We further consider FactCC  (Kryscinski et al., 2020) , which evaluates factual consistency of a system summary by predicting an entailment score between the source and the summary. We reproduce their method on our datasets. Additional details for implementing the evaluation models and the entity extraction models are given in Appendix B. 

 Experimental Results In this section, we start with describing training details in ? 6.1. We then compare attention variants on documents of the same length ( ? 6.2) and study whether reading more text can generate more informative summaries ( ? 6.3). We further report human evaluation on summary informativeness and faithfulness as well as automatic faithfulness scores ( ? 6.4). Finally, we investigate whether automatic metrics correlate with human judgment ( ? 6.5). 

 Training Details We fine-tune BART  (Lewis et al., 2020)  for all experiments. We implement our models with Py-Torch  (Paszke et al., 2019)  and Fairseq  (Ott et al., 2019) . Additional position embeddings are initialized randomly for models that handle longer inputs. The learning rate is set to 1 ? 10 ?4 and learning rate warm-up is applied for the first 10,000 steps. Adafactor  (Shazeer and Stern, 2018)  optimizer with a gradient clipping of 0.1 is used. All models are trained on two Quadro RTX 6000 GPUs with 24GB memory or one Quadro RTX 8000 with 48GB memory. We set a batch size of 2 per step and accumulate gradient every 32 steps. During test, we adopt a beam size of 4 and a length penalty of 2  (Wu et al., 2016 ) on all datasets. 

 Comparing Attention Variants Comparisons. We first experiment with articles that are all truncated at 1024 tokens. For encoder attentions, we consider the following variants: (1) sliding WINDOW; (2) adaptive span (ADASPAN); (3) GLOBAL tokens; (4) STRIDE; (5) RANDOM tokens; (6) Linformer (LIN.); (7) locality sensitive hashing (LSH); and (8) SINKHORN. We ensure models are comparable by setting hyperparameters to satisfy w = ? = k = lb l = 2b s = 256, so that models have similar memory complexity. For LSH attentions, we select l = 4 rounds of hashing. Following prior work  (Zaheer et al. , 2020), we combine GLOBAL, STRIDE, and RAN-DOM with WINDOW and ADASPAN, where we set g = n 2 /s = r = 128 for a fair comparison. We adapt Linformer to encoder-decoder attentions to compare with HEPOS, where we use s h = n/k = 4 for all experiments. Finally, we report results using FULL, i.e., the original, encoder and encoderdecoder attentions. GovReport (new) PubMed System R-1 R-2 R-L R-1 R-2 R- Results. Among all encoder variants, learnable patterns perform the best, approaching the performance of full attentions on both GovReport and PubMed, as shown in Table  3 . Within learnable patterns, Sinkhorn attention consistently obtains better ROUGE scores. Moreover, combining techniques in fixed patterns is more effective than simply using window-based sparse attentions, though with an increased memory cost. For encoder-decoder attentions, HEPOS consistently yields higher ROUGE scores than Linformer on both datasets, using either full or Sinkhorn encoder. Notably, coupled with a Sinkhorn attention, our model's performance matches the variant using full encoder attention, implying the effectiveness of HEPOS on both identifying the salient content and capturing the global context. GovReport PubMed System (MAXLEN) R-1 R-2 R-L R-1 R-2 R-L Baselines PEGASUS ( 

 Reading More Input Boosts Informativeness We investigate whether processing more words generates more informative summaries. Comparisons include recent top-performing abstractive models: PEGASUS  (Zhang et al., 2019) , a large pre-trained summarization model with truncated inputs; TLM  (Pilault et al., 2020) , DANCER (Gidiotis and Tsoumakas, 2020), and SEAL  (Zhao et al., 2020) , all of them using hybrid extract-then-abstract methods; and BIGBIRD  (Zaheer et al., 2020) , which combines sliding window, global and random token attentions in the encoder. For encoder variants, we pick the best performing model from fixed patterns to be combined with full encoder-decoder attention, i.e., sliding window with stride (STRIDE), low-rank method (LIN.), and learnable patterns (LSH and SINKHORM). We then combine learnable patterns with HEPOS to support processing more text. All models consume as long an input as the memory allows. Results. Overall, models that read more text obtain higher ROUGE scores, according to results on Gov-Report and PubMed in Table  4 . First, different encoder variants with full encoder-decoder attentions attain better results than the full attentions baseline except Linformer. Second, adding HEPOS encoderdecoder attention almost doubles the words that can be processed and further improves the performance. This highlights the importance of handling both encoder attentions and encoder-decoder attentions efficiently. Notably, HEPOS with an LSH encoder achieves new state-of-the-art results on PubMed, outperforming BigBird which only uses sparse attentions on the encoder. We also report performances of our two best models with HEPOS on arXiv in Table  5 , and they outperform all competitive abstractive models. As can be seen from the sample summaries in Fig.  3 , our model that reads in 10k tokens generates more informative summary than the full attention model that only processes 1k tokens. Fig.  4  further shows that ROUGE-2 scores can be consistently lifted when reading more input, with similar trends observed on ROUGE-1 and ROUGE-L. More sample outputs are presented in Appendix C. 

 Reading More Input Improves Faithfulness Here we first show human evaluation results on informativeness and unfaithful errors in the generated summaries. We sample 100 documents from GovReport and PubMed (50 each) with structured references that are labeled with aspects as described in ? 4 and Appendix D. Each sample is evaluated by two fluent English speakers, who have cumulatively annotated tens of thousands of sentences for the same tasks before this work. Annotators are asked to label each summary sentence with an aspect and then decide whether it contains any type of error. Three types of unfaithful errors are considered: (i) hallucination-fabricating content not present in the input, (ii) deletion-incorrectly  summary covers important information of an aspect when compared with the reference. All system summaries and references are presented in a random order. Human evaluation guidelines and sample summaries for different aspects are included in Appendix D. Results. Overall, reading more text significantly improves informativeness as well as reduces fabricated content. From Table  6 , we observe that HEPOS attention, combined with a SINKHORN encoder, obtains better informativeness scores than comparisons that read in less text on both datasets. This echos results from automatic evaluation in the previous section. Moreover, both models that use efficient attentions reduce unfaithfulness, especially hallucination errors, when compared with the full attention model, which only reads 1024 tokens. As the models read more content, they learn to surface more factual and richer content in the summaries, as seen in Fig.  3 . Next, we explore if reading more helps correctly reflect the content in documents' later sections. We plot aspect-level human ratings of informativeness and unfaithful errors on PubMed and GovReport in Fig.  5  and Fig.  6 . We report percentages of sentences with unfaithful errors by majority voting (i.e., at least one error is found by both annotators in the sentence). As can be seen, our models consistently improve informativeness and reduce errors across sections, especially for "Results" and "Conclusions" on PubMed and "What GAO recommends" on GovReport-these sections often appear in the later part of the source documents.  Especially, we find that the full attention model tends to produce fabricated numbers in resultant summaries, whereas our models are able to correct them. Lastly, we report the entailment-based FactCC and QA scores APES and APES src for top performing models in Table  7 . The results again show that consuming longer input leads to more faithful summaries, though the differences are less pronounced. 

 Correlations between Human and Automatic Metrics Finally, we study whether the faithfulness evaluation metrics correlate with human judgment. As shown in Table  8 , on both government reports and scientific papers, QA metrics are better correlated with human ratings, with our newly pro- posed APES src being the stronger of the two. After inspection, we find that human-written summaries contain paraphrases or acronyms that APES cannot capture via strict lexical matching. For instance, for the question "Diabetes may worsen in patients", the reference answer is "death rate", whereas answers from the source and the system summary are both "mortality". APES src captures this, but not APES. 

 Additional Related Work Summarizing long inputs has been investigated in many domains, including books  (Mihalcea and Ceylan, 2007) , patents  (Trappey et al., 2009) , movie scripts (Gorinski and Lapata, 2015), and scientific publications  (Qazvinian and Radev, 2008) . However, the datasets are often too small to train neural models.  Cohan et al. (2018)  publish two large-scale datasets by collecting articles from ARXIV and PUBMED. Popular methods rely on extractive summarizers that identify salient sentences based on positional information  (Dong et al., 2020)  or combined global and local contexts  (Xiao and Carenini, 2019) , where each sentence is represented as aggregated word embeddings. However, extractive summaries are often redundant and in-coherent, highlighting the need for handling long documents via abstractive summarization. To that end, extract-then-abstract methods are proposed. For example,  Pilault et al. (2020)  first extract relevant sentences and then rewrite them into paper abstracts. Our work is in line with building end-to-end abstractive summarization models for long input.  Cohan et al. (2018)  design a hierarchical encoder to read different sections separately, and then use combined attentions over words and sections to generate the summary. Multiple agents are created to read segments separately, and then collaboratively write an abstract  (Celikyilmaz et al., 2018) . However, both work truncates articles to 2K words. Although efficient encoder attentions have been studied in  Zaheer et al. (2020)  for abstractive summarization, at most 3K tokens can be consumed by their models. Our HEPOS encoderdecoder attention are able to process more than 10K tokens, significantly improving summary informativeness and faithfulness. 

 Conclusion We investigate efficient attentions for long document summarization. We propose a novel encoderdecoder attention, HEPOS, based on head-wise positional strides that can effectively identify salient content. Models based on HEPOS attention can process at least twice as many words and produce more informative summaries with less unfaithful errors, according to both automatic evaluation and human evaluation. We further show that our new cloze QA metric better correlates with human judgment than prior faithfulness evaluation metrics. 
