topic,paper_ID,sentence_ID,info-unit,sub,pred,obj
translation,0,160,hyperparameters,number of hidden units,in,BiLSTM
translation,0,160,hyperparameters,number of hidden units,is,256
translation,0,160,hyperparameters,BiLSTM,is,256
translation,0,160,hyperparameters,number of layers,in,BiL-STM
translation,0,160,hyperparameters,BiL-STM,is,2
translation,0,160,hyperparameters,word embedding dimension,is,200
translation,0,160,hyperparameters,number of hidden units,in,MLP
translation,0,160,hyperparameters,MLP,is,100
translation,0,161,hyperparameters,Adam optimizer,with,epsilon 0.01
translation,0,161,hyperparameters,fixed learning rate,has,0.001
translation,0,162,hyperparameters,converges,within,4 epochs
translation,0,162,hyperparameters,Hyperparameters,has,Training
translation,0,5,model,"scene graph representation ( Johnson et al. , 2015 )",considers,objects
translation,0,5,model,Model,consider,"scene graph representation ( Johnson et al. , 2015 )"
translation,0,6,model,alternative but equivalent edgecentric view,of,scene graphs
translation,0,6,model,Model,introducing,alternative but equivalent edgecentric view
translation,0,7,model,two -stage pipeline,into,one
translation,0,7,model,generic dependency parsing,followed by,simple post-processing
translation,0,7,model,one,enabling,end-toend training
translation,0,7,model,Model,combine,two -stage pipeline
translation,0,33,model,our model,on top of,neural depen-dency parser implementation
translation,0,33,model,Model,build,our model
translation,0,34,results,carefully customized dependency parser,able to generate,high quality scene graphs
translation,0,34,results,high quality scene graphs,by learning from,data
translation,0,34,results,Results,show,carefully customized dependency parser
translation,0,171,results,customized dependency parsing model,achieves,average F-score
translation,0,171,results,average F-score,of,49.67 %
translation,0,171,results,significantly outperforms,by,5 percent
translation,0,171,results,parser,used in,SPICE
translation,0,171,results,parser,by,5 percent
translation,0,171,results,significantly outperforms,has,parser
translation,0,171,results,Results,see that,customized dependency parsing model
translation,0,181,results,relatively high,at,69.85 %
translation,0,181,results,Results,has,F-score
translation,0,196,results,Our parser,delivers,better retrieval performance
translation,0,196,results,better retrieval performance,across,three evaluation metrics
translation,0,196,results,Results,has,Our parser
translation,1,102,ablation-analysis,ablation,of,stack - Transformer components
translation,1,102,ablation-analysis,stack / buffer positions,seems clearly,detrimental
translation,1,102,ablation-analysis,detrimental,in,all scenarios
translation,1,102,ablation-analysis,Ablation analysis,Regarding,ablation
translation,1,102,ablation-analysis,Ablation analysis,Regarding,stack - Transformer components
translation,1,102,ablation-analysis,Ablation analysis,of,stack - Transformer components
translation,1,108,ablation-analysis,Modeling of the buffer,seems,more important
translation,1,108,ablation-analysis,more important,than,modeling of the stack
translation,1,108,ablation-analysis,Ablation analysis,has,Modeling of the buffer
translation,1,79,experimental-setup,minor modifications,over,MT model hyper-parameters
translation,1,79,experimental-setup,Experimental setup,implemented on,fairseq toolkit
translation,1,80,experimental-setup,crossentropy training,with,learning rate
translation,1,80,experimental-setup,crossentropy training,with,inverse square root scheduling
translation,1,80,experimental-setup,inverse square root scheduling,with,min
translation,1,80,experimental-setup,learning rate,has,5e ?4
translation,1,80,experimental-setup,Experimental setup,used,crossentropy training
translation,1,80,experimental-setup,Experimental setup,used,inverse square root scheduling
translation,1,81,experimental-setup,4000 warmup updates,with,learning rate 1e ?7
translation,1,81,experimental-setup,4000 warmup updates,maximum,3584 tokens per batch
translation,1,81,experimental-setup,1e ?9,has,4000 warmup updates
translation,1,81,experimental-setup,Experimental setup,has,1e ?9
translation,1,81,experimental-setup,Experimental setup,has,4000 warmup updates
translation,1,82,experimental-setup,label smoothing,reduced to,0.01 3
translation,1,82,experimental-setup,Adam parameters,has,0.9 and 0.98
translation,1,82,experimental-setup,Adam parameters,has,label smoothing
translation,1,82,experimental-setup,Experimental setup,has,Adam parameters
translation,1,85,experimental-setup,"RoBERTa - base ( Liu et al. , 2019 ) embeddings",without,fine-tuning
translation,1,85,experimental-setup,"RoBERTa - base ( Liu et al. , 2019 ) embeddings",averaging,wordpieces
translation,1,85,experimental-setup,wordpieces,to obtain,word representations
translation,1,85,experimental-setup,Experimental setup,used,"RoBERTa - base ( Liu et al. , 2019 ) embeddings"
translation,1,86,experimental-setup,Weight averaging,of,best 3 checkpoints
translation,1,86,experimental-setup,Experimental setup,has,Weight averaging
translation,1,88,experimental-setup,Models,trained for,fixed number of epochs
translation,1,88,experimental-setup,Models,selecting,best model
translation,1,88,experimental-setup,on validation,by,LAS or Smatch
translation,1,88,experimental-setup,best model,has,on validation
translation,1,88,experimental-setup,Experimental setup,trained for,fixed number of epochs
translation,1,88,experimental-setup,Experimental setup,has,Models
translation,1,91,experimental-setup,Training,took,at most 6h
translation,1,91,experimental-setup,at most 6h,on,Nvidia Tesla v100 GPU
translation,1,91,experimental-setup,Experimental setup,has,Training
translation,1,16,model,state modeling,in the context of,sequence- to-sequence Transformers
translation,1,16,model,action prediction,for,transition - based parsing
translation,1,16,model,Model,local versus global paradigms of,state modeling
translation,1,17,model,cross-attention mechanism,of,Transformer
translation,1,17,model,cross-attention mechanism,to provide,global parser state modeling
translation,1,128,model,sequence-tosequence Transformers,to encode,parser state
translation,1,128,model,parser state,for,transition - based parsing
translation,1,128,model,Model,modifications of,sequence-tosequence Transformers
translation,1,20,results,particularly large,for,smaller train sets and smaller model sizes
translation,1,20,results,Results,has,Gains
translation,1,97,results,stack - transformer,provides,around 2 points improvement
translation,1,97,results,stack - transformer,provides,0.5 points
translation,1,97,results,around 2 points improvement,against,Transformer
translation,1,97,results,Transformer,on,PTB and AMR2.0
translation,1,97,results,improvement,against,multi-task version ( a - c )
translation,1,97,results,LAS and Smatch,has,stack - transformer
translation,1,97,results,0.5 points,has,improvement
translation,1,97,results,Results,Comparing,LAS and Smatch
translation,1,107,results,most methods,perform,similarly
translation,1,107,results,different attention modifications ( e- h ),has,most methods
translation,1,107,results,Results,Comparing across,different attention modifications ( e- h )
translation,1,113,results,stack - Transformer,competitive against,recent works
translation,1,113,results,recent works,particularly for,AMR
translation,1,113,results,Results,has,stack - Transformer
translation,2,4,baselines,two semantic parsers,represent,factorizationand
translation,2,4,baselines,two semantic parsers,for,Elementary Dependency Structures ( EDS )
translation,2,127,model,multi-layer BiLSTM,to encode,tokens
translation,2,127,model,two softmax layers,to predict,concept-related labels
translation,2,127,model,Model,has,multi-layer BiLSTM
translation,2,12,results,factorization - based system,obtains,overall accuracy
translation,2,12,results,factorization - based system,out-performs,other submission systems
translation,2,12,results,overall accuracy,of,94.47
translation,2,12,results,94.47,in terms of,official MRP evaluation metrics
translation,2,12,results,other submission systems,by,large margin
translation,2,12,results,large margin,with respect to,prediction
translation,2,12,results,prediction,for,"labels , properties , anchors and edges"
translation,2,12,results,Results,has,factorization - based system
translation,2,15,results,composition - based system,reaches,score
translation,2,15,results,score,of,91.84
translation,2,15,results,Results,has,composition - based system
translation,2,227,results,Our factorization - based parser,achieves,relatively satisfactory performance
translation,2,227,results,relatively satisfactory performance,in,all basic evaluation items
translation,2,227,results,all basic evaluation items,except,top
translation,2,227,results,Results,has,Our factorization - based parser
translation,2,231,results,evalution results,of,our composition - based parser
translation,2,231,results,evalution results,are,not as good
translation,2,231,results,our composition - based parser,are,not as good
translation,2,231,results,not as good,as,factorization - based one
translation,2,231,results,Results,has,evalution results
translation,2,233,results,work rapidly and reliably,with,MRP accuracy
translation,2,233,results,of over 94 %,in,same - epoch - and - domain setup
translation,2,233,results,neural ERS parsers,has,work rapidly and reliably
translation,2,233,results,MRP accuracy,has,of over 94 %
translation,3,176,baselines,all arcs,longer than,maximum length
translation,3,176,baselines,maximum length,observed for,each head-modifier POS pair
translation,3,188,hyperparameters,maximum number of passes,in,cascade
translation,3,188,hyperparameters,cascade,is,five
translation,3,188,hyperparameters,Hyperparameters,has,maximum number of passes
translation,3,194,hyperparameters,each part- of-speech ( POS ) feature,with,additional feature
translation,3,194,hyperparameters,additional feature,using,coarse POS representations
translation,3,194,hyperparameters,Hyperparameters,replicate,each part- of-speech ( POS ) feature
translation,3,4,model,Model,has,Coarse - to-fine inference
translation,3,5,model,multi-pass coarse- to-fine architecture,for,dependency parsing
translation,3,5,model,multi-pass coarse- to-fine architecture,using,linear - time vine pruning
translation,3,5,model,Model,propose,multi-pass coarse- to-fine architecture
translation,3,12,model,linear- time vine pruning pass,build up to,higher - order models
translation,3,12,model,higher - order models,achieving,speed -ups
translation,3,12,model,speed -ups,of,two orders of magnitude
translation,3,12,model,speed -ups,while maintaining,state - of - the - art accuracies
translation,3,12,model,two orders of magnitude,while maintaining,state - of - the - art accuracies
translation,3,12,model,Model,start with,linear- time vine pruning pass
translation,3,22,model,multi-pass coarse- to-fine approach,where,initial pass
translation,3,22,model,initial pass,is,linear- time sweep
translation,3,22,model,linear- time sweep,tries to resolve,local ambiguities
translation,3,22,model,linear- time sweep,leaves,arcs
translation,3,22,model,arcs,beyond,fixed length b
translation,3,22,model,fixed length b,has,unspecified
translation,3,22,model,Model,propose,multi-pass coarse- to-fine approach
translation,3,6,results,"Our first - , second - , and third - order models",achieve,accuracies
translation,3,6,results,"Our first - , second - , and third - order models",exploring,fraction
translation,3,6,results,accuracies,comparable to,unpruned counterparts
translation,3,6,results,Results,has,"Our first - , second - , and third - order models"
translation,3,213,results,Our second - and third - order cascades,has,significantly outperform
translation,3,213,results,significantly outperform,has,ZHANGNIVRE
translation,3,213,results,Results,has,Our second - and third - order cascades
translation,3,214,results,transitionbased model,with,k = 8
translation,3,214,results,transitionbased model,increasing,k-best list size
translation,3,214,results,k = 8,is,very efficient and effective
translation,3,214,results,k-best list size,scales,much worse
translation,3,214,results,much worse,than employing,multi-pass pruning
translation,3,214,results,Results,has,transitionbased model
translation,3,217,results,unpruned models,perform,well
translation,3,217,results,unpruned models,scoring,comparably
translation,3,217,results,well,across,datasets
translation,3,217,results,comparably,to,top results
translation,3,217,results,top results,from,CoNLL -X competition
translation,3,217,results,Results,has,unpruned models
translation,3,218,results,speed increases,for,our cascades
translation,3,218,results,our cascades,with,almost no loss
translation,3,218,results,almost no loss,in,accuracy
translation,3,218,results,accuracy,across,all languages
translation,3,218,results,Results,see,speed increases
translation,4,95,ablation-analysis,content - based attention,all 8 layers of,network
translation,4,95,ablation-analysis,content - based attention,results in,development - set accuracy decrease
translation,4,95,ablation-analysis,development - set accuracy decrease,of,only 0.27 F1
translation,4,95,ablation-analysis,Ablation analysis,Disabling,content - based attention
translation,4,146,ablation-analysis,maximum parsing accuracy,using,our model
translation,4,146,ablation-analysis,Ablation analysis,demonstrate,long-distance dependencies
translation,4,156,ablation-analysis,tag embeddings,removed from,our model
translation,4,156,ablation-analysis,suffers,by,around 1 F1
translation,4,156,ablation-analysis,tag embeddings,has,performance
translation,4,156,ablation-analysis,performance,has,suffers
translation,4,4,model,LSTM encoder,with,self-attentive architecture
translation,4,4,model,LSTM encoder,lead to,improvements
translation,4,4,model,improvements,to,state- ofthe - art discriminative constituency parser
translation,4,4,model,Model,replacing,LSTM encoder
translation,4,16,model,parser,combines,encoder
translation,4,16,model,architecture,with,decoder
translation,4,16,model,decoder,customized for,parsing
translation,4,16,model,Model,introduce,parser
translation,4,17,model,self-attention,has,outperform
translation,4,17,model,outperform,has,LSTM - based approach
translation,4,8,results,Our parser,achieves,new state- ofthe - art results
translation,4,8,results,new state- ofthe - art results,for,single models
translation,4,8,results,single models,trained on,Penn Treebank
translation,4,8,results,93.55 F1,without the use of,external data
translation,4,8,results,95.13 F1,when using,pre-trained word representations
translation,4,8,results,Results,has,Our parser
translation,4,9,results,previous best-published accuracy figures,on,8 of the 9 languages
translation,4,9,results,8 of the 9 languages,in,SPMRL dataset
translation,4,9,results,outperforms,has,previous best-published accuracy figures
translation,4,26,results,deep contextualized representations,boost,parsing accuracy
translation,4,26,results,Results,find,deep contextualized representations
translation,4,26,results,Results,using,deep contextualized representations
translation,4,27,results,Our parser,achieves,93.55 F1
translation,4,27,results,Our parser,achieves,outperforming
translation,4,27,results,93.55 F1,on,Penn Treebank WSJ test set
translation,4,27,results,93.55 F1,when not using,external word representations
translation,4,27,results,Penn Treebank WSJ test set,when not using,external word representations
translation,4,27,results,all previous singlesystem constituency parsers,trained only on,WSJ training set
translation,4,27,results,outperforming,has,all previous singlesystem constituency parsers
translation,4,27,results,Results,has,Our parser
translation,4,28,results,pre-trained word representations,increases,parsing accuracy
translation,4,28,results,parsing accuracy,to,95.13 F1
translation,4,28,results,Results,addition of,pre-trained word representations
translation,4,29,results,previous best published results,on,8 of the 9 languages
translation,4,29,results,8 of the 9 languages,in,SPMRL 2013/2014 shared tasks
translation,4,29,results,Our model,has,outperforms
translation,4,29,results,outperforms,has,previous best published results
translation,4,29,results,Results,has,Our model
translation,4,122,results,true factoring of information,is,important
translation,4,130,results,our model,learns to use,combination of the two attention types
translation,4,130,results,combination of the two attention types,with,positionbased attention
translation,4,130,results,positionbased attention,being,most important
translation,4,130,results,Results,see that,our model
translation,4,131,results,content - based attention,is,more useful
translation,4,131,results,more useful,at,later layers
translation,4,131,results,later layers,in,network
translation,4,131,results,Results,see that,content - based attention
translation,4,135,results,strict windowing,yields,poor results
translation,4,135,results,40,causes,loss
translation,4,135,results,loss,in,parsing accuracy
translation,4,135,results,parsing accuracy,compared to,original model
translation,4,135,results,Results,has,strict windowing
translation,4,148,results,sideby - side comparison,of,strict and relaxed windowing
translation,4,148,results,strict and relaxed windowing,shows that,ability
translation,4,148,results,ability,to pool,global information
translation,4,148,results,global information,using,designated locations
translation,4,148,results,global information,insufficient to compensate for,small window sizes
translation,4,148,results,designated locations,always available in,relaxed scheme
translation,4,148,results,designated locations,insufficient to compensate for,small window sizes
translation,4,148,results,Results,has,sideby - side comparison
translation,4,190,results,Development set results,addition of,word embeddings
translation,4,190,results,word embeddings,to,model
translation,4,190,results,model,uses,character LSTM
translation,4,190,results,performance,for,some languages
translation,4,190,results,hurts,for,others
translation,4,190,results,character LSTM,has,mixed effect
translation,4,190,results,improves,has,performance
translation,4,190,results,Results,addition of,word embeddings
translation,4,190,results,Results,has,Development set results
translation,4,192,results,our test set result,exceeds,previous best-published numbers
translation,4,192,results,8 of the 9 languages,has,our test set result
translation,4,192,results,Results,On,8 of the 9 languages
translation,5,4,model,approach,decoding problem in,transition - based parsing
translation,5,5,model,series of partial parses,on,sentence
translation,5,5,model,series of partial parses,to locate,best candidate parse
translation,5,5,model,best candidate parse,using,confidence estimates
translation,5,5,model,confidence estimates,of,transition decisions
translation,5,5,model,transition decisions,as,heuristic
translation,5,5,model,heuristic,to guide,starting points
translation,5,24,model,selectional branching,by integrating,search strategy
translation,5,24,model,search strategy,based on,heuristic function
translation,5,24,model,Model,introduce,heuristic backtracking
translation,5,26,model,heuristic,based on,confidence of transition predictions
translation,5,26,model,Model,use,heuristic
translation,5,27,model,heuristic backtracking,with,cutoff
translation,5,41,model,algorithm,has,heuristic backtracking
translation,5,41,model,Model,introduce,algorithm
translation,5,42,model,novel cutoff approach,to further increase,speed
translation,5,42,model,Model,introduce,novel cutoff approach
translation,5,25,results,heuristic backtracking,maintains,property
translation,5,25,results,heuristic backtracking,allows,more fully explore
translation,5,25,results,good heuristic,has,heuristic backtracking
translation,5,25,results,more fully explore,has,space of possible transition sequences
translation,5,25,results,Results,When paired with,good heuristic
translation,5,105,results,decoding technique,tested with,varying numbers of beams
translation,5,105,results,accuracy,trended,upwards
translation,5,105,results,varying numbers of beams,has,both the predictions per sentence
translation,5,105,results,Results,has,decoding technique
translation,5,111,results,Dynamic beam search,performed,full beam search
translation,5,111,results,Dynamic beam search,as well as,full beam search
translation,5,111,results,Dynamic beam search,demonstrating,reduction
translation,5,111,results,reduction,in,predictions
translation,5,111,results,predictions,on par with,heuristic backtracking
translation,5,111,results,Results,has,Dynamic beam search
translation,5,113,results,Heuristic backtracking,with,cutoff
translation,5,113,results,Heuristic backtracking,reduced,transitions
translation,5,113,results,transitions,by,additional 50 %
translation,5,113,results,cutoff,has,outperformed
translation,5,113,results,outperformed,has,greedy decoding
translation,5,113,results,Results,has,Heuristic backtracking
translation,6,149,hyperparameters,AMR - to - text training,for,15 epochs
translation,6,149,hyperparameters,AMR - to - text training,takes,4.5h
translation,6,149,hyperparameters,15 epochs,takes,4.5h
translation,6,149,hyperparameters,15 epochs,takes,15h
translation,6,149,hyperparameters,4.5h,on,AMR 1.0
translation,6,149,hyperparameters,4.5h,on,AMR 2.0
translation,6,149,hyperparameters,15h,on,AMR 2.0
translation,6,149,hyperparameters,Hyperparameters,has,AMR - to - text training
translation,6,6,model,AMR parsing performance,including,generation of synthetic text and AMR annotations
translation,6,6,model,Model,explore,different ways
translation,6,16,model,trained parser,to iteratively refine,rule- based AMR oracle
translation,6,16,model,rule- based AMR oracle,to yield,better action sequences
translation,6,16,model,Model,explore,trained parser
translation,6,16,model,Model,use of,trained parser
translation,6,53,results,mining AMR,leads to,overall improvement
translation,6,53,results,overall improvement,of,up to 0.2 Smatch
translation,6,53,results,up to 0.2 Smatch,across,two tasks
translation,6,53,results,better Smatch,increasing,model performance
translation,6,53,results,Results,shows,mining AMR
translation,6,103,results,baseline system,close to,best published system
translation,6,103,results,best published system,with,better results
translation,6,103,results,best published system,with,worse
translation,6,103,results,better results,for,AMR1.0 ( + 0.8 )
translation,6,103,results,better results,for,AMR2.0 ( ?0.5 )
translation,6,103,results,worse,for,AMR2.0 ( ?0.5 )
translation,6,106,results,mining,shows,close to no improvement
translation,6,106,results,close to no improvement,in,individual results
translation,6,106,results,introduced methods,has,mining
translation,6,107,results,SynAMR,provides,largest gain ( 0.7/0.8 )
translation,6,107,results,largest gain ( 0.7/0.8 ),for,AMR1.0 / AMR2.0
translation,6,107,results,synTxt,provides,close to half
translation,6,107,results,Results,has,SynAMR
translation,6,108,results,combination of both methods,yields,improvement
translation,6,108,results,combination of both methods,only for,AMR1.0
translation,6,108,results,improvement,over,individual scores
translation,6,108,results,AMR1.0,with,0.9 improvement
translation,6,108,results,Results,has,combination of both methods
translation,6,109,results,improve,for,AMR2.0
translation,6,109,results,AMR2.0,attaining,1.1 improvement
translation,6,109,results,syn-Txt and synAMR,has,hurt
translation,6,109,results,hurt,has,results
translation,6,110,results,proposed approach,achieves,81.3 Smatch
translation,6,110,results,81.3 Smatch,in,AMR2.0
translation,6,110,results,Results,has,proposed approach
translation,6,115,results,transition - based approaches,provide,more uniform performance
translation,6,115,results,more uniform performance,across,categories
translation,6,115,results,presented self - learning methods,able to,improve
translation,6,115,results,improve,in,all categories
translation,7,118,baselines,baseline models,are,RNN model
translation,7,118,baselines,baseline models,are,bidirectional RNN ( BRNN ) model
translation,7,118,baselines,baseline models,are,BLSTM supertagging models
translation,7,118,baselines,BLSTM supertagging models,in,Vaswani et al . ( 2016 )
translation,7,118,baselines,BLSTM supertagging models,in,Lewis et al . ( 2016 )
translation,7,118,baselines,supertagging,has,baseline models
translation,7,118,baselines,Baselines,For,supertagging
translation,7,119,baselines,parsing,compared with,global beam-search shift-reduce parsers
translation,7,120,baselines,neural shift- reduce CCG parser baseline,is,beam-search shift-reduce parser
translation,7,120,baselines,Baselines,has,neural shift- reduce CCG parser baseline
translation,7,131,experimental-setup,hidden state size,is,256
translation,7,131,experimental-setup,attentional hidden layer,is,200
translation,7,131,experimental-setup,attentional hidden layer,has,x t
translation,7,131,experimental-setup,Experimental setup,size of,attentional hidden layer
translation,7,131,experimental-setup,Experimental setup,has,hidden state size
translation,7,132,experimental-setup,parsing model LSTMs,have,hidden state size
translation,7,132,experimental-setup,parsing model LSTMs,size of,action hidden layer ( b t
translation,7,132,experimental-setup,hidden state size,of,128
translation,7,132,experimental-setup,action hidden layer ( b t,is,80
translation,7,132,experimental-setup,Experimental setup,has,parsing model LSTMs
translation,7,133,experimental-setup,Pretrained word embeddings,for,all models
translation,7,133,experimental-setup,all models,are,100 - dimensional
translation,7,133,experimental-setup,all other embeddings,are,50 - dimensional
translation,7,133,experimental-setup,Experimental setup,has,Pretrained word embeddings
translation,7,134,experimental-setup,CCG lexical category and POS embeddings,on,concatenation
translation,7,134,experimental-setup,CCG lexical category and POS embeddings,on,Wikipedia dump parsed with C&C.
translation,7,134,experimental-setup,concatenation,of,training data
translation,7,134,experimental-setup,Experimental setup,pretrained,CCG lexical category and POS embeddings
translation,7,136,experimental-setup,training,used,plain non-minibatched stochastic gradient descent
translation,7,136,experimental-setup,plain non-minibatched stochastic gradient descent,with,initial learning rate ? 0 = 0.1
translation,7,136,experimental-setup,iterating,in,epochs
translation,7,136,experimental-setup,epochs,until,accuracy
translation,7,136,experimental-setup,no longer increases,on,dev set
translation,7,136,experimental-setup,accuracy,has,no longer increases
translation,7,136,experimental-setup,Experimental setup,For,training
translation,7,137,experimental-setup,learning rate schedule ? e = ? 0 /( 1 + ?e ),with,? = 0.08
translation,7,137,experimental-setup,? = 0.08,used for,e ? 11
translation,7,138,experimental-setup,clipped,whenever,norm
translation,7,138,experimental-setup,norm,exceeds,5
translation,7,138,experimental-setup,Experimental setup,has,Gradients
translation,7,139,experimental-setup,Dropout training,with,dropout rate
translation,7,139,experimental-setup,Dropout training,with,2 penalty
translation,7,139,experimental-setup,dropout rate,of,0.3
translation,7,139,experimental-setup,2 penalty,of,1 ? 10 ?5
translation,7,139,experimental-setup,Experimental setup,has,Dropout training
translation,7,114,experiments,experiments,on,"CCGBank ( Hockenmaier and Steedman , 2007 )"
translation,7,114,experiments,"CCGBank ( Hockenmaier and Steedman , 2007 )",with,standard splits
translation,7,5,model,neural shift-reduce parsing model,for,CCG
translation,7,5,model,neural shift-reduce parsing model,factored into,four unidirectional LSTMs
translation,7,5,model,neural shift-reduce parsing model,factored into,one bidirectional LSTM
translation,7,5,model,Model,describe,neural shift-reduce parsing model
translation,7,14,model,shift-reduce CCG parsing,based on,long short-term memories ( LSTMs
translation,7,18,model,global LSTM parsing model,by adapting,expected Fmeasure loss
translation,7,18,model,Model,present,global LSTM parsing model
translation,7,20,model,globally optimized model,can be leveraged with,greedy inference
translation,7,20,model,greedy inference,resulting in,deterministic parser
translation,7,20,model,deterministic parser,as accurate as,beam-search counterpart
translation,7,20,model,Model,show,globally optimized model
translation,7,21,results,standard CCGBank tests,combining,parser
translation,7,21,results,parser,with,attention - based LSTM supertagger
translation,7,21,results,clearly outperform,has,all previous shift-reduce CCG parsers
translation,7,21,results,Results,On,standard CCGBank tests
translation,7,142,results,baseline BLSTM model without attention,achieves,same level of accuracy
translation,7,142,results,same level of accuracy,as,Lewis et al . ( 2016
translation,7,142,results,same level of accuracy,as,baseline BLSTM model
translation,7,142,results,Results,has,baseline BLSTM model without attention
translation,7,155,results,full model LSTM -w+c+a+ p,surpasses,all previous shift-reduce models
translation,7,155,results,full model LSTM -w+c+a+ p,achieving,dev set accuracy
translation,7,155,results,dev set accuracy,of,86.56 %
translation,7,155,results,Results,has,full model LSTM -w+c+a+ p
translation,7,156,results,Category embeddings ( LSTM -w+c ),yielded,large gain
translation,7,156,results,Category embeddings ( LSTM -w+c ),further adding,POS embeddings ( LSTM -w+c+ a+ p )
translation,7,156,results,large gain,over using,word embeddings alone ( LSTM - w )
translation,7,156,results,action embeddings ( LSTM -w+c+ a ),provided,little improvement
translation,7,156,results,action embeddings ( LSTM -w+c+ a ),further adding,POS embeddings ( LSTM -w+c+ a+ p )
translation,7,156,results,POS embeddings ( LSTM -w+c+ a+ p ),gave,noticeable recall
translation,7,156,results,POS embeddings ( LSTM -w+c+ a+ p ),gave,F1 improvements
translation,7,156,results,F1 improvements,over,LSTM -w+c.
translation,7,156,results,+ 0.36 % ),over,LSTM -w+c.
translation,7,156,results,noticeable recall,has,+ 0.61 % )
translation,7,156,results,F1 improvements,has,+ 0.36 % )
translation,7,156,results,Results,has,Category embeddings ( LSTM -w+c )
translation,7,161,results,highest F1 ( 85.86 % ),on,dev set ( LSTM - BRNN
translation,7,161,results,highest F1 ( 85.86 % ),in comparison with,all previous shift- reduce models
translation,7,161,results,improvement,of,1.42 % F1
translation,7,161,results,1.42 % F1,over,greedy model
translation,7,162,results,baseline BLSTM supertagging model,for,parsing ( LSTM - BLSTM )
translation,8,26,ablation-analysis,contextualized representations,supervising,decoder attention
translation,8,26,ablation-analysis,contextualized representations,informing,decoder
translation,8,26,ablation-analysis,decoder attention,on,coverage
translation,8,26,ablation-analysis,decoder,on,coverage
translation,8,26,ablation-analysis,coverage,of,input
translation,8,26,ablation-analysis,input,by,attention mechanism
translation,8,26,ablation-analysis,downsampling,has,frequent program templates
translation,8,27,ablation-analysis,gap,in,exact match accuracy
translation,8,27,ablation-analysis,exact match accuracy,between,in-distribution and OOD
translation,8,27,ablation-analysis,in-distribution and OOD,reduced from,84.6 ? 62.2
translation,8,27,ablation-analysis,DROP,from,96.4 ? 77.1
translation,8,27,ablation-analysis,SQL,has,gap
translation,8,27,ablation-analysis,Ablation analysis,For,SQL
translation,8,167,ablation-analysis,SQL,adding,auxiliary attention supervision
translation,8,167,ablation-analysis,auxiliary attention supervision,to,SEQ2SEQ model
translation,8,167,ablation-analysis,program split EM,from,10.8 ? 18.5
translation,8,167,ablation-analysis,program split EM,combining with,ELMO
translation,8,167,ablation-analysis,ELMO,leads to,EM
translation,8,167,ablation-analysis,EM,of,20.3
translation,8,167,ablation-analysis,Ablation analysis,In,SQL
translation,8,168,ablation-analysis,ELMO and ATTNSUP,reduces,relative gap
translation,8,168,ablation-analysis,relative gap,from,84.6 ? 70.6
translation,8,168,ablation-analysis,84.6 ? 70.6,compared to,SEQ2SEQ
translation,8,173,ablation-analysis,attention - coverage,improves,program split EM
translation,8,173,ablation-analysis,program split EM,from,10.8 ? 17
translation,8,174,ablation-analysis,coverage,with,ELMO and ATTNSUP
translation,8,174,ablation-analysis,ELMO and ATTNSUP,leads to,our best results
translation,8,174,ablation-analysis,our best results,where,program split EM
translation,8,174,ablation-analysis,program split EM,reaches,25.4
translation,8,174,ablation-analysis,relative gap,drops from,84.6 ? 62.2
translation,8,174,ablation-analysis,Ablation analysis,Combining,coverage
translation,8,180,ablation-analysis,SEQ2SEQ models,improve,compositional generalization
translation,8,180,ablation-analysis,compositional generalization,increase,frequency of new programs
translation,8,180,ablation-analysis,compositional generalization,increase,invalid syntax errors
translation,8,180,ablation-analysis,Ablation analysis,for,SEQ2SEQ models
translation,8,196,ablation-analysis,development set,when moving from,program split
translation,8,196,ablation-analysis,program split,to,KB - free split
translation,8,196,ablation-analysis,average accuracy,drops from,14.5 ? 9.8
translation,8,196,ablation-analysis,development set,has,average accuracy
translation,8,196,ablation-analysis,program split,has,average accuracy
translation,8,196,ablation-analysis,KB - free split,has,average accuracy
translation,8,196,ablation-analysis,Ablation analysis,On,development set
translation,8,7,model,multiple extensions,to,attention module
translation,8,7,model,multiple extensions,aiming to improve,compositional generalization
translation,8,7,model,attention module,of,semantic parser
translation,8,7,model,Model,propose,multiple extensions
translation,8,24,model,aligning sub-structures,in,question
translation,8,24,model,aligning sub-structures,in,program
translation,8,24,model,supervising attention,based on,precomputed token alignments
translation,8,24,model,decoder attention,to cover,entire input utterance
translation,8,24,model,Model,propose,novel extensions to decoder attention ( ?3.3 )
translation,8,25,model,downsampling examples,from,frequent templates
translation,8,25,model,downsampling examples,to reduce,dataset bias
translation,8,25,model,Model,propose,downsampling examples
translation,8,166,results,substantial positive effect,on,compositional generalization
translation,8,166,results,compositional generalization,especially in,SQL
translation,8,166,results,attention supervision,has,substantial positive effect
translation,8,166,results,Results,shows,attention supervision
translation,8,169,results,DROP,reduces,relative gap
translation,8,169,results,attention supervision,improves,iid performance
translation,8,169,results,attention supervision,reduces,relative gap
translation,8,169,results,attention supervision,not lead to,additional improvements
translation,8,169,results,relative gap,for,GRAMMAR
translation,8,169,results,GRAMMAR,using,GloVe representations
translation,8,169,results,additional improvements,when combined with,ELMO
translation,8,169,results,DROP,has,attention supervision
translation,8,171,results,attention - coverage,improves,absolute performance
translation,8,171,results,attention - coverage,improves,compositional generalization
translation,8,171,results,compositional generalization,in,all cases
translation,8,171,results,Results,shows,attention - coverage
translation,8,172,results,best results,obtained without,attention coverage loss term
translation,8,172,results,best results,providing,coverage vector
translation,8,172,results,coverage vector,as,additional input
translation,8,172,results,additional input,to,decoder
translation,8,172,results,SQL,has,best results
translation,8,172,results,Results,in,SQL
translation,8,181,results,Grammarbased models,output,significantly more new programs
translation,8,181,results,Grammarbased models,output,less invalid syntax errors
translation,8,181,results,significantly more new programs,than,SEQ2SEQ models
translation,8,181,results,Results,has,Grammarbased models
translation,8,188,results,and attention over spans,increases,number of predictions
translation,8,188,results,number of predictions,that are,semantically close
translation,8,188,results,semantically close,to,target programs
translation,8,188,results,attentionsupervision,has,attention - coverage
translation,8,188,results,attentionsupervision,has,and attention over spans
translation,8,188,results,Results,adding,attentionsupervision
translation,8,189,results,correct typed variables,in,predictions
translation,8,189,results,correct typed variables,is,significantly higher
translation,8,189,results,significantly higher,when using,attention - supervision and attention - coverage
translation,8,189,results,attention - supervision and attention - coverage,compared to,baseline model
translation,8,189,results,Results,frequency of,correct typed variables
translation,8,190,results,errors,of,GRAMMAR model
translation,8,190,results,GRAMMAR model,closer to,target program
translation,8,190,results,target program,compared to,SEQ2SEQ
translation,8,190,results,Results,has,errors
translation,8,192,results,compositional generalization,in,DROP
translation,8,192,results,compositional generalization,is,harder
translation,8,192,results,harder,than,SQL datasets
translation,8,192,results,Results,show that,compositional generalization
translation,9,6,baselines,Earley algorithm,with,cubepruning
translation,9,6,baselines,cubepruning,for,AMR parsing
translation,9,6,baselines,AMR parsing,given,new sentences
translation,9,4,experiments,synchronous - graphgrammar- based approach,to,SemEval - 2016 Task 8
translation,9,4,experiments,synchronous - graphgrammar- based approach,to,Meaning Representation Parsing
translation,9,97,hyperparameters,cut variables,in,derivation forest
translation,9,97,hyperparameters,cut variables,initialized as,1
translation,9,97,hyperparameters,sampled uniformly,for,each node
translation,9,97,hyperparameters,Hyperparameters,During,sampling procedure
translation,9,98,hyperparameters,sampler,for,160 iterations
translation,9,98,hyperparameters,sampler,combine,grammar
translation,9,98,hyperparameters,Hyperparameters,run,sampler
translation,9,5,model,Synchronous Hyperedge Replacement Grammar ( SHRG ) rules,from,aligned pairs
translation,9,5,model,Synchronous Hyperedge Replacement Grammar ( SHRG ) rules,from,AMR graphs
translation,9,5,model,aligned pairs,of,sentences
translation,9,5,model,aligned pairs,of,AMR graphs
translation,9,5,model,Model,learn,Synchronous Hyperedge Replacement Grammar ( SHRG ) rules
translation,9,23,model,alignments,from,Ulf Hermjakob 's automatic aligner
translation,9,23,model,alignments,building,perceptron - based concept identifier
translation,9,23,model,perceptron - based concept identifier,where,boundary information
translation,9,23,model,boundary information,of,mapped frag- ments
translation,9,23,model,Model,using,alignments
translation,9,23,model,Model,building,perceptron - based concept identifier
translation,10,8,baselines,arc-standard algorithm,with,Swap action
translation,10,8,baselines,Swap action,for,general non-projective parsing
translation,10,85,experimental-setup,stack - prop and stacking models,include,lexical features
translation,10,85,experimental-setup,lexical features,in,input layer
translation,10,85,experimental-setup,input layer,of,neural networks
translation,10,85,experimental-setup,neural networks,using,64 dimension pre-trained word embeddings
translation,10,85,experimental-setup,64 dimension pre-trained word embeddings,concatenated with,64 - dimension character - based embeddings
translation,10,85,experimental-setup,Bi-LSTM,over,characters of a word
translation,10,85,experimental-setup,Experimental setup,For,stack - prop and stacking models
translation,10,86,experimental-setup,each language,include,pre-trained embeddings
translation,10,86,experimental-setup,pre-trained embeddings,for,100 K most frequent words
translation,10,86,experimental-setup,100 K most frequent words,in,raw corpora
translation,10,86,experimental-setup,Experimental setup,For,each language
translation,10,88,experimental-setup,word representations,learned using,Skipgram model
translation,10,88,experimental-setup,Skipgram model,with,negative sampling
translation,10,88,experimental-setup,Experimental setup,has,word representations
translation,10,89,experimental-setup,backoff character model,use,64 - dimension
translation,10,89,experimental-setup,character Bi-LSTM embeddings,in,input layer
translation,10,89,experimental-setup,input layer,of,network
translation,10,89,experimental-setup,64 - dimension,has,character Bi-LSTM embeddings
translation,10,89,experimental-setup,Experimental setup,For,backoff character model
translation,10,92,experimental-setup,POS tagger specific MLP,has,64 hidden nodes
translation,10,92,experimental-setup,parser MLP,has,128 hidden nodes
translation,10,92,experimental-setup,Experimental setup,has,POS tagger specific MLP
translation,10,93,experimental-setup,hyperbolic tangent,as,activation function
translation,10,93,experimental-setup,activation function,in,all tasks
translation,10,93,experimental-setup,Experimental setup,use,hyperbolic tangent
translation,10,95,experimental-setup,momentum SGD,for,learning
translation,10,95,experimental-setup,momentum SGD,with,minibatch size
translation,10,95,experimental-setup,learning,with,minibatch size
translation,10,95,experimental-setup,minibatch size,of,1
translation,10,95,experimental-setup,Experimental setup,use,momentum SGD
translation,10,96,experimental-setup,initial learning rate,set to,0.1
translation,10,96,experimental-setup,0.1,with,momentum
translation,10,96,experimental-setup,momentum,of,0.9
translation,10,96,experimental-setup,Experimental setup,has,initial learning rate
translation,10,97,experimental-setup,LSTM weights,initialized with,random orthonormal matrices
translation,10,97,experimental-setup,Experimental setup,has,LSTM weights
translation,10,98,experimental-setup,dropout rate,to,30 %
translation,10,98,experimental-setup,30 %,for,all the hidden states in the network
translation,10,98,experimental-setup,Experimental setup,set,dropout rate
translation,10,99,experimental-setup,models,trained for,up to 100 epochs
translation,10,99,experimental-setup,up to 100 epochs,with,early stopping
translation,10,99,experimental-setup,early stopping,based on,development set
translation,10,99,experimental-setup,Experimental setup,trained for,up to 100 epochs
translation,10,99,experimental-setup,Experimental setup,has,models
translation,10,100,experimental-setup,neural network models,implemented in,"DyNet ( Neubig et al. , 2017 )"
translation,10,7,hyperparameters,simple neural network architectures,rely on,long short - term memory ( LSTM ) networks
translation,10,7,hyperparameters,long short - term memory ( LSTM ) networks,for learning,task - dependent features
translation,10,7,hyperparameters,Hyperparameters,employ,simple neural network architectures
translation,10,6,model,segmentation,use,neural stacking
translation,10,6,model,neural stacking,for,joint learning
translation,10,6,model,joint learning,of,POS tagging and parsing tasks
translation,10,6,model,Model,use,neural stacking
translation,10,6,model,Model,has,segmentation
translation,10,9,model,neural stacking,as,knowledge transfer mechanism
translation,10,9,model,knowledge transfer mechanism,for,cross-domain parsing
translation,10,9,model,cross-domain parsing,of,low resource domains
translation,10,9,model,Model,use,neural stacking
translation,10,50,model,Joint POS tagging and Parsing,jointly model,POS tagging and parsing
translation,10,50,model,POS tagging and parsing,using,stack of tagger and parser networks
translation,10,50,model,Model,jointly model,POS tagging and parsing
translation,10,50,model,Model,has,Joint POS tagging and Parsing
translation,10,66,model,non-linear feed -forward network,to predict,labeled transitions
translation,10,66,model,labeled transitions,for,parser configurations
translation,10,66,model,Model,use,non-linear feed -forward network
translation,10,87,model,distributed word representations,for,each language
translation,10,87,model,learned separately,from,monolingual corpora
translation,10,87,model,monolingual corpora,collected from,Web to Corpus ( W2C )
translation,10,87,model,Model,has,distributed word representations
translation,10,10,results,Our system,shows,substantial gains
translation,10,10,results,substantial gains,against,UDPipe baseline
translation,10,10,results,UDPipe baseline,with,average improvement
translation,10,10,results,average improvement,of,4.18 %
translation,10,10,results,4.18 %,in,LAS
translation,10,10,results,4.18 %,across,all languages
translation,10,10,results,LAS,across,all languages
translation,10,10,results,Results,has,Our system
translation,10,21,results,beneficial,as,UDPipe POS tagger
translation,10,21,results,slightly lower performance,in,some languages
translation,10,21,results,training,has,separate POS tagger
translation,10,21,results,UDPipe POS tagger,has,slightly lower performance
translation,10,21,results,Results,observed,training
translation,10,104,results,74 out of 82 treebanks,obtained,average improvement
translation,10,104,results,average improvement,of,5.8 %
translation,10,104,results,5.8 %,in,LAS
translation,10,104,results,5.8 %,over,UDPipe baseline models
translation,10,104,results,LAS,over,UDPipe baseline models
translation,10,104,results,Results,For,74 out of 82 treebanks
translation,10,111,results,average improvement,of,3 %
translation,10,111,results,3 %,in,LAS
translation,10,111,results,LAS,over,UDPipe baseline
translation,10,112,results,our segmentation models,achieved,better ranking
translation,10,112,results,better ranking,than,our average ranking
translation,10,117,results,development sets,of,multiple domains
translation,10,117,results,multiple domains,of,English and French
translation,10,117,results,Results,on,development sets
translation,10,118,results,improvement,of,1 % to 2 %
translation,10,118,results,1 % to 2 %,using,eng ewt
translation,10,118,results,eng ewt,as,source domain
translation,10,118,results,quite high ( 2 % to 5 % ),using,fr gsd
translation,10,118,results,fr gsd,as,source domain
translation,10,118,results,Results,For,English domains
translation,10,118,results,Results,for,French
translation,10,119,results,ranking,on,treebanks
translation,10,119,results,treebanks,that use,neural stacking
translation,10,119,results,neural stacking,better than,our average
translation,11,79,baselines,two further variants,that replace,encoder and decoder
translation,11,79,baselines,encoder and decoder,with,transformers
translation,11,80,baselines,encoder,initialized with,"RoBERTa ( Liu et al. , 2019 )"
translation,11,80,baselines,encoder,initialized with,pretrained language model
translation,11,80,baselines,first variant,has,encoder
translation,11,80,baselines,"RoBERTa ( Liu et al. , 2019 )",has,pretrained language model
translation,11,7,experiments,"session - based , compositional taskoriented parsing dataset",of,20 k sessions
translation,11,7,experiments,20 k sessions,consisting of,60 k utterances
translation,11,9,experiments,new family of Seq2Seq models,for,session - based parsing
translation,11,9,experiments,new family of Seq2Seq models,achieve,better or comparable performance
translation,11,73,hyperparameters,two layers,of size,512
translation,11,73,hyperparameters,two layers,with,randomly initialized embeddings
translation,11,73,hyperparameters,randomly initialized embeddings,has,of size 300
translation,11,73,hyperparameters,Hyperparameters,consist of,two layers
translation,11,74,hyperparameters,base model,optimized with,LAMB
translation,11,74,hyperparameters,Hyperparameters,has,base model
translation,11,75,hyperparameters,learning rate,found separately for,each experiment
translation,11,75,hyperparameters,Hyperparameters,has,learning rate
translation,11,81,hyperparameters,decoder,is,randomly initialized 3 - layer transformer
translation,11,81,hyperparameters,decoder,is,4 attention heads
translation,11,81,hyperparameters,randomly initialized 3 - layer transformer,with,hidden size 512
translation,11,81,hyperparameters,randomly initialized 3 - layer transformer,with,4 attention heads
translation,11,81,hyperparameters,Hyperparameters,has,decoder
translation,11,83,hyperparameters,Both encoder and decoder,consist of,12 layers
translation,11,83,hyperparameters,12 layers,with,hidden size 1024
translation,11,83,hyperparameters,Hyperparameters,has,Both encoder and decoder
translation,11,6,model,semantic representation,for,task - oriented conversational systems
translation,11,6,model,semantic representation,represent,concepts
translation,11,6,model,concepts,such as,co-reference
translation,11,6,model,concepts,such as,context carryover
translation,11,6,model,concepts,enabling,comprehensive understanding
translation,11,6,model,comprehensive understanding,of,queries
translation,11,6,model,Model,propose,semantic representation
translation,11,72,model,base model,uses,two distinct stacked bidirectional LSTMs
translation,11,72,model,base model,uses,stacked unidirectional LSTMs
translation,11,72,model,two distinct stacked bidirectional LSTMs,as,encoder
translation,11,72,model,two distinct stacked bidirectional LSTMs,as,decoder
translation,11,72,model,two distinct stacked bidirectional LSTMs,as,decoder
translation,11,72,model,stacked unidirectional LSTMs,as,decoder
translation,11,72,model,Model,has,base model
translation,11,77,model,contextualized word vectors,by augmenting,input
translation,11,77,model,input,with,ELMo embeddings
translation,11,82,model,both encoder and decoder,with,"BART ( Lewis et al. , 2019 )"
translation,11,82,model,"BART ( Lewis et al. , 2019 )",has,sequence -tosequence pretained model
translation,11,149,model,sequenceto-sequence models,based on,pointergenerator architecture
translation,11,149,model,pointergenerator architecture,using,recurrent neural network and transformer architectures
translation,11,149,model,Model,family of,sequenceto-sequence models
translation,11,10,results,best known results,on,DSTC2
translation,11,10,results,DSTC2,by,up to 5 points
translation,11,10,results,up to 5 points,for,slot-carryover
translation,11,10,results,Results,improve,best known results
translation,11,111,results,previous state- ofthe - art results,on,ATIS
translation,11,111,results,previous state- ofthe - art results,comparable to,state - of- the - art
translation,11,111,results,ATIS,comparable to,state - of- the - art
translation,11,111,results,state - of- the - art,on,SNIPS
translation,11,111,results,proposed approach,has,outperforms
translation,11,111,results,outperforms,has,previous state- ofthe - art results
translation,11,112,results,decoupled model,to,RNNGs
translation,11,112,results,decoupled model,to,RNNG
translation,11,112,results,RNNGs,note that,single decoupled model
translation,11,112,results,single decoupled model,using,biLSTMs
translation,11,112,results,single decoupled model,using,transformers
translation,11,112,results,single decoupled model,able to outperform,RNNG
translation,11,112,results,transformers,with,RoBERTa or BART pretraining
translation,11,112,results,Results,Comparing,decoupled model
translation,11,113,results,outperforms,has,ensemble of seven RNNGs
translation,11,113,results,Results,has,decoupled model
translation,11,114,results,decoupled biLSTM,extended with,ELMo inputs
translation,11,114,results,decoupled biLSTM,able to outperform,transformer model
translation,11,114,results,ELMo inputs,able to outperform,transformer model
translation,11,114,results,transformer model,initialised with,RoBERTa pretraining
translation,11,114,results,Results,has,decoupled biLSTM
translation,11,119,results,RoBERTa - based model,benefit from,extra training data
translation,11,119,results,RoBERTa - based model,able to outperform,biLSTMbased model
translation,11,119,results,extra training data,able to outperform,biLSTMbased model
translation,11,119,results,biLSTMbased model,on,two datasets
translation,11,119,results,Results,shows that,RoBERTa - based model
translation,11,120,results,decoupled model,with,BART pretraining
translation,11,120,results,decoupled model,achieves,top performance
translation,11,133,results,outperforms,on,frame accuracy
translation,11,133,results,biLSTM model,on,frame accuracy
translation,11,133,results,biLSTM model,takes,lead
translation,11,133,results,lead,in terms of,raw slot carryover performance
translation,11,133,results,RoBERTa decoupled model,has,outperforms
translation,11,133,results,outperforms,has,biLSTM model
translation,11,134,results,BART,achieving,best overall performance
translation,11,134,results,outperforms,achieving,best overall performance
translation,11,134,results,BART,has,outperforms
translation,11,134,results,Results,has,BART
translation,12,73,experiments,constituency - based parsing,use,two models
translation,12,73,experiments,two models,trained by,Berkeley parser
translation,12,73,experiments,two models,one on,phrase-structure ( PS ) trees
translation,12,73,experiments,two models,one on,relational - realizational ( RR ) trees
translation,12,73,experiments,two models,one on,relational - realizational ( RR ) trees
translation,12,81,results,gold information,leads to,much higher scores
translation,12,81,results,Results,confirm,gold information
translation,12,84,results,Unlabeled TEDEVAL,shows,greater drop
translation,12,84,results,Unlabeled TEDEVAL,for,labeled TEDEVAL
translation,12,84,results,greater drop,when moving from,predicted to raw
translation,12,84,results,Results,for,labeled TEDEVAL
translation,12,84,results,Results,has,Unlabeled TEDEVAL
translation,13,141,ablation-analysis,performance,of,CURSOR
translation,13,141,ablation-analysis,further improved,to,68.21 %
translation,13,141,ablation-analysis,68.21 %,combination of,our data augmentation and ensemble method
translation,13,141,ablation-analysis,Ablation analysis,has,performance
translation,13,122,baselines,MiniDiver,based on,word reordering
translation,13,122,baselines,word reordering,reorders,words
translation,13,122,baselines,words,of,source sentences
translation,13,122,baselines,difference,in,POS sequence distribution
translation,13,122,baselines,POS sequence distribution,between,source and the target languages
translation,13,122,baselines,Baselines,has,MiniDiver
translation,13,123,experiments,La-graRelax,solves,word order difference problem
translation,13,123,experiments,word order difference problem,by using,Lagrangian relaxation
translation,13,123,experiments,Lagrangian relaxation,to force,constraints
translation,13,123,experiments,constraints,derived from,corpus-statistics
translation,13,123,experiments,constraints,yields,significant improvement
translation,13,123,experiments,corpus-statistics,in,inference time
translation,13,123,experiments,significant improvement,in,transfer parsing
translation,13,110,hyperparameters,pre-trained multilingual embeddings,from,Fast- Text
translation,13,110,hyperparameters,pre-trained multilingual embeddings,that project,word embeddings
translation,13,110,hyperparameters,word embeddings,from,different languages
translation,13,110,hyperparameters,word embeddings,using,offline transformation method
translation,13,110,hyperparameters,different languages,into,same space
translation,13,110,hyperparameters,same space,using,offline transformation method
translation,13,110,hyperparameters,Hyperparameters,leverage,pre-trained multilingual embeddings
translation,13,112,hyperparameters,POS - based language model,for,word reordering
translation,13,112,hyperparameters,POS - based language model,trained on,training set
translation,13,112,hyperparameters,training set,of,corresponding target language
translation,13,112,hyperparameters,POS tag dimension,set to,50
translation,13,112,hyperparameters,"number of layers l ? { 1 , 2 , 3 }",tuned on,development sets
translation,13,112,hyperparameters,development sets,of,5 non-held - out languages
translation,13,112,hyperparameters,hidden size h,has,", 100 }"
translation,13,112,hyperparameters,Hyperparameters,has,POS - based language model
translation,13,112,hyperparameters,Hyperparameters,has,"number of layers l ? { 1 , 2 , 3 }"
translation,13,5,model,words,in,each sentence
translation,13,5,model,each sentence,of,source language corpus
translation,13,5,model,rearranged,to meet,word order
translation,13,5,model,word order,in,target language
translation,13,5,model,Model,has,words
translation,13,18,model,CURSOR,to overcome,word order difference issue
translation,13,18,model,word order difference issue,in,crosslingual transfer
translation,13,18,model,CURSOR,has,Cross lingUal paRSing by wOrd Reordering
translation,13,25,model,near-optimal result,develop,population - based optimization algorithm
translation,13,25,model,Model,To find,near-optimal result
translation,13,26,model,algorithm,initialized with,population
translation,13,26,model,algorithm,iteratively produces,new generations
translation,13,26,model,new generations,by,specially designed genetic operators
translation,13,26,model,population,has,of feasible solutions
translation,13,26,model,Model,has,algorithm
translation,13,27,model,better solutions,by applying,"selection , crossover , and mutation subroutines"
translation,13,27,model,"selection , crossover , and mutation subroutines",to,individuals
translation,13,27,model,individuals,in,previous iteration
translation,13,27,model,each iteration,has,better solutions
translation,13,27,model,Model,At,each iteration
translation,13,57,model,POS - based neural language model,to guide,treebank reordering
translation,13,59,model,population - based optimization algorithm,combinatorial nature of,word reordering
translation,13,59,model,Model,designed,population - based optimization algorithm
translation,13,109,model,Graph decoder,utilizes,deep biaffine attentional scorer
translation,13,109,model,Stack decoder,is,top-down transition - based decoder
translation,13,109,model,Model,has,Graph decoder
translation,13,30,results,two dominant parsing paradigms,shows,our approach
translation,13,30,results,our approach,achieves,increase
translation,13,30,results,increase,of,1.73 %
translation,13,30,results,1.73 %,in,average UAS
translation,13,30,results,1.73 %,if,English
translation,13,31,results,improvement,rises to,4.12 %
translation,13,31,results,4.12 %,by,combination of our data augmentation and ensemble method
translation,13,31,results,RNN - Graph model,has,our approach
translation,13,31,results,2.5 %,has,in average UAS
translation,13,31,results,Results,for,RNN - Graph model
translation,13,33,results,Our approach,performs,exceptionally well
translation,13,33,results,exceptionally well,when,target languages
translation,13,33,results,target languages,quite different from,source one
translation,13,33,results,source one,in,word orders
translation,13,33,results,Results,has,Our approach
translation,13,130,results,improved,with,four different parsing models
translation,13,130,results,four different parsing models,trained on,corpora
translation,13,130,results,corpora,after,word reordering
translation,13,130,results,baseline,has,cross-lingual transfer performances
translation,13,130,results,Results,comparing to,baseline
translation,13,131,results,models,using,RNN encoder
translation,13,131,results,RNN encoder,benefit more than,others
translation,13,131,results,Results,has,models
translation,13,132,results,RNN - Graph model,enhanced by,our treebank reordering
translation,13,132,results,our treebank reordering,achieved,best average UAS
translation,13,132,results,best average UAS,of,66.6 %
translation,13,132,results,baseline,by,2.5 %
translation,13,132,results,Results,has,RNN - Graph model
translation,13,135,results,results,of,CURSOR
translation,13,135,results,CURSOR,achieved by,model
translation,13,135,results,model,based on,RNN - Graph architecture
translation,13,135,results,Results,of,CURSOR
translation,13,135,results,Results,has,results
translation,13,138,results,CURSOR,performs,better
translation,13,138,results,better,than,Mini-
translation,13,138,results,Diver,in,almost all languages
translation,13,138,results,Mini-,has,Diver
translation,13,139,results,CURSOR,achieves,slightly better results
translation,13,139,results,slightly better results,than,LagraRelax
translation,13,139,results,Results,has,CURSOR
translation,13,144,results,four different parsing models,show,CURSOR
translation,13,144,results,CURSOR,consistently improve,average UAS
translation,13,144,results,average UAS,across,30 target languages
translation,13,144,results,Results,with,four different parsing models
translation,13,166,results,outperforms,by,significant margin
translation,13,166,results,baseline,by,significant margin
translation,13,166,results,significant margin,in,cases
translation,13,166,results,CURSOR,has,outperforms
translation,13,166,results,outperforms,has,baseline
translation,13,182,results,transfer performance,of,CURSOR
translation,13,182,results,outperform,in,all languages
translation,13,182,results,Baseline,in,all languages
translation,13,182,results,further improve,has,transfer performance
translation,13,182,results,outperform,has,Baseline
translation,13,182,results,Results,show,ensemble method
translation,13,183,results,Ensembling CURSOR ( k = 3 ),with,Baseline
translation,13,183,results,Ensembling CURSOR ( k = 3 ),achieves,best performance
translation,13,183,results,Baseline,achieves,best performance
translation,13,183,results,best performance,establishing,new start- ofthe - art
translation,13,183,results,68.21 %,in,UAS
translation,13,183,results,58.04 %,in,LAS
translation,13,183,results,best performance,has,68.21 %
translation,13,183,results,Results,has,Ensembling CURSOR ( k = 3 )
translation,14,171,baselines,Malt - Parser,with,arc eager algorithm
translation,14,171,baselines,MSTParser,with,second-order projective model
translation,14,5,model,robust procedure,for,cross-experimental evaluation
translation,14,5,model,robust procedure,based on,deterministic unificationbased operations
translation,14,5,model,deterministic unificationbased operations,for harmonizing,different representations
translation,14,5,model,refined notion of tree edit distance,for evaluating,parse hypotheses
translation,14,5,model,parse hypotheses,relative to,multiple gold standards
translation,14,5,model,Model,develops,robust procedure
translation,14,20,model,all structures,brought into,single formal space of events
translation,14,20,model,single formal space of events,that neutralizes,representation peculiarities
translation,14,20,model,Model,first step,all structures
translation,14,191,results,every parser,appears to perform,at its best
translation,14,191,results,at its best,evaluated against,scheme
translation,14,191,results,Results,has,every parser
translation,15,68,experimental-setup,word embeddings,precomputed using,"word2vec ( Mikolov et al. , 2013 )"
translation,15,68,experimental-setup,word embeddings,precomputed using,word2vec
translation,15,68,experimental-setup,cbow,0 -,binary
translation,15,68,experimental-setup,min-count,has,10 - size
translation,15,68,experimental-setup,100 - window,has,10
translation,15,68,experimental-setup,10,has,negative
translation,15,68,experimental-setup,negative,has,5
translation,15,68,experimental-setup,iter,has,2
translation,15,68,experimental-setup,2,has,threads
translation,15,68,experimental-setup,threads,has,16
translation,15,68,experimental-setup,binary,has,0
translation,15,68,experimental-setup,Experimental setup,has,word embeddings
translation,15,235,results,performance,of,baseline UDPipe system
translation,15,235,results,baseline,is,relatively strong
translation,15,235,results,performance,has,baseline
translation,15,235,results,baseline UDPipe system,has,baseline
translation,15,252,results,Results,evaluates,"detection of tokens , syntactic words and sentences"
translation,15,263,results,four new teams,surpass,baseline
translation,15,263,results,LyS-FASTPARSE,being,best
translation,15,266,results,globally best system,falls back to,fourth rank
translation,15,266,results,C2L2 ( Cornell University ),employs,most successful strategy
translation,15,266,results,most successful strategy,for,underresourced languages
translation,15,266,results,globally best system,has,Stanford
translation,15,266,results,Results,has,globally best system
translation,16,141,ablation-analysis,residual dropout,observe,only a slight decrease
translation,16,141,ablation-analysis,only a slight decrease,in,overall performance
translation,16,141,ablation-analysis,Ablation analysis,When removing,position - wise feed -forward layer
translation,16,143,ablation-analysis,increase,in,performance
translation,16,143,ablation-analysis,performance,when removing,residual dropout only
translation,16,143,ablation-analysis,Ablation analysis,observe,increase
translation,16,145,ablation-analysis,position - wise feedforward layer and residual dropout,brings about,noticeable decrease
translation,16,145,ablation-analysis,noticeable decrease,in,performance
translation,16,145,ablation-analysis,Ablation analysis,removing,position - wise feedforward layer and residual dropout
translation,16,158,ablation-analysis,query vectors,by,query matrices
translation,16,158,ablation-analysis,query matrices,has,decreases
translation,16,158,ablation-analysis,decreases,has,performance
translation,16,159,ablation-analysis,similar decrease,in,performance
translation,16,159,ablation-analysis,performance,has,when removing concatenation
translation,16,164,ablation-analysis,significant decrease,in,performance
translation,16,164,ablation-analysis,performance,when replacing,concatenation
translation,16,164,ablation-analysis,concatenation,with,matrix projection
translation,16,166,ablation-analysis,our LAL,with,self-attention layer
translation,16,166,ablation-analysis,self-attention layer,with,equal number of attention heads
translation,16,166,ablation-analysis,Ablation analysis,replacing,our LAL
translation,16,130,experiments,English- language parsers,with,"2 , 3 , 4 , 6 , 8 , 12 and 16 self-attention layers"
translation,16,127,hyperparameters,output vectors,of,each label attention head
translation,16,127,hyperparameters,each label attention head,have,128 dimensions
translation,16,127,hyperparameters,both languages,has,"query , key and value vectors"
translation,16,127,hyperparameters,Hyperparameters,For,both languages
translation,16,129,hyperparameters,"large cased pre-trained XLNet ( Yang et al. , 2019 )",as,our embedding model
translation,16,129,hyperparameters,"large cased pre-trained XLNet ( Yang et al. , 2019 )",as,"base pre-trained BERT ( Devlin et al. , 2018 )"
translation,16,129,hyperparameters,our embedding model,for,our English - language experiments
translation,16,129,hyperparameters,"base pre-trained BERT ( Devlin et al. , 2018 )",for,Chinese
translation,16,129,hyperparameters,Hyperparameters,use,"large cased pre-trained XLNet ( Yang et al. , 2019 )"
translation,16,7,model,Model,introduce,Label Attention Layer
translation,16,215,model,attention heads,represent,labels
translation,16,215,model,proposed architecture,has,attention heads
translation,16,217,results,96.38 F1,for,constituency parsing
translation,16,217,results,97.42 UAS and 96.26 LAS,for,dependency parsing
translation,16,217,results,Results,In,English
translation,16,218,results,Chinese,achieves,89.28 LAS
translation,16,218,results,our model,achieves,92.64 F1
translation,16,218,results,our model,achieves,89.28 LAS
translation,16,218,results,Chinese,has,our model
translation,16,218,results,Results,In,Chinese
translation,16,239,results,LAL parser,establishes,new state - of - the - art results
translation,16,239,results,new state - of - the - art results,in,both languages
translation,16,239,results,new state - of - the - art results,improving,significantly
translation,16,239,results,significantly,in,dependency parsing
translation,16,239,results,Results,has,LAL parser
translation,17,108,results,Zhou Qiaoli - 3 system,achieved,best results
translation,17,108,results,best results,with,LAS
translation,17,108,results,LAS,of,61.84
translation,17,111,results,performances,of,top five results
translation,17,111,results,top five results,are,comparative ( p > 0.1 )
translation,17,111,results,rank sixth system,is,significantly ( p < 10 ?5 ) worse
translation,17,111,results,significantly ( p < 10 ?5 ) worse,than,top five results
translation,17,112,results,average LAS,for,all systems
translation,17,112,results,average LAS,was,54.22
translation,17,112,results,all systems,was,54.22
translation,17,112,results,Results,has,average LAS
translation,18,6,model,direct attachments between tokens farther apart,with,single transition
translation,18,19,model,non-local transitions,directly induce,attachments
translation,18,19,model,Model,introduce,novel transition system
translation,18,41,model,Arc-hybrid,combines,LArc
translation,18,41,model,Arc-hybrid,combines,RArc
translation,18,41,model,LArc,from,arc-eager
translation,18,41,model,RArc,from,arc-standard
translation,18,41,model,dependencies,has,bottom - up
translation,18,41,model,Model,has,Arc-hybrid
translation,18,82,results,arc-swift,is,equally accurate
translation,18,82,results,arc-swift,is,more robust
translation,18,82,results,equally accurate,as,existing systems
translation,18,82,results,existing systems,for,short dependencies
translation,18,82,results,more robust,for,longer ones
translation,18,82,results,Results,has,arc-swift
translation,18,88,results,average transition sequence length,per,sentence
translation,18,88,results,sentence,of,arc-swift
translation,18,88,results,arc-swift,is,77.5 %
translation,18,88,results,77.5 %,of,arc-eager
translation,18,88,results,PTB -SD development set,has,average transition sequence length
translation,18,88,results,Results,On,PTB -SD development set
translation,19,5,model,parser,not rely on,syntactic pre-parse
translation,19,5,model,parser,not rely on,heavily engineered features
translation,19,5,model,parser,uses,five recurrent neural networks
translation,19,5,model,five recurrent neural networks,as,key architectural components
translation,19,5,model,key architectural components,for estimating,AMR graph structure
translation,19,5,model,Model,has,parser
translation,19,90,results,Smatch F1 result,for,test dataset
translation,19,90,results,Smatch F1 result,was,56.0 %
translation,19,90,results,test dataset,was,66.1 %
translation,19,90,results,test dataset,was,56.0 %
translation,19,90,results,56.0 %,for,eval dataset
translation,19,90,results,Results,has,Smatch F1 result
translation,20,99,ablation-analysis,beam size,grows to,5
translation,20,99,ablation-analysis,increases,to,94.01 %
translation,20,99,ablation-analysis,94.01 %,is,2.3 % error reduction
translation,20,99,ablation-analysis,beam size,has,tagging accuracy
translation,20,99,ablation-analysis,5,has,tagging accuracy
translation,20,99,ablation-analysis,tagging accuracy,has,increases
translation,20,99,ablation-analysis,Ablation analysis,As,beam size
translation,20,82,experimental-setup,POS tags,of,training set
translation,20,82,experimental-setup,POS tags,generated using,10 - fold jack - knifing
translation,20,82,experimental-setup,dependency parsing,has,POS tags
translation,20,82,experimental-setup,Experimental setup,For,dependency parsing
translation,20,83,experimental-setup,Chinese,use,CTB 5.1
translation,20,83,experimental-setup,Experimental setup,For,Chinese
translation,20,89,experimental-setup,tagger and parser,implemented using,C ++.
translation,20,31,experiments,easy - first POS tagging,be improved significantly from,beam search
translation,20,31,experiments,easy - first POS tagging,be improved significantly from,global learning
translation,20,31,experiments,dependency parsing,be improved significantly from,beam search
translation,20,31,experiments,dependency parsing,be improved significantly from,global learning
translation,20,4,model,POS tagging algorithms,with,beam search
translation,20,4,model,POS tagging algorithms,with,structured perceptron
translation,20,4,model,Model,combine,easy - first dependency parsing
translation,20,4,model,Model,combine,POS tagging algorithms
translation,20,5,model,simple variant,of,early - update
translation,20,5,model,early - update,to ensure,valid update
translation,20,5,model,valid update,in,training process
translation,20,5,model,Model,propose,simple variant
translation,20,7,results,CTB,achieve,94.01 % tagging accuracy
translation,20,7,results,CTB,achieve,86.33 % unlabeled attachment score
translation,20,7,results,86.33 % unlabeled attachment score,with,relatively small beam width
translation,20,7,results,Results,On,CTB
translation,20,8,results,PTB,achieve,state - of - the - art performance
translation,20,8,results,Results,On,PTB
translation,20,32,results,CTB,achieve,94.01 % tagging accuracy
translation,20,32,results,94.01 % tagging accuracy,is,best result
translation,20,32,results,best result,for,single tagging model
translation,20,32,results,Results,on,CTB
translation,20,33,results,relatively small beam,achieve,86.33 % unlabeled score
translation,20,33,results,86.33 % unlabeled score,better than,state - of - the - art transition - based parsers
translation,20,34,results,PTB,achieve,good results
translation,20,34,results,good results,comparable to,state - of- the - art
translation,20,34,results,Results,On,PTB
translation,20,91,results,Chinese POS tagging,greatly benefit from,beam search
translation,20,91,results,English dependency parsing,greatly benefit from,beam search
translation,20,91,results,Chinese POS tagging,has,dependency parsing
translation,20,91,results,Results,see that,Chinese POS tagging
translation,20,94,results,drops linearly,growth of,beam width
translation,20,94,results,speed of both tagging and dependency parsing,has,drops linearly
translation,20,98,results,tagging accuracy,of,our greedy baseline
translation,20,98,results,tagging accuracy,already comparable to,state - of- the- art
translation,20,98,results,our greedy baseline,already comparable to,state - of- the- art
translation,20,98,results,CTB,has,tagging accuracy
translation,20,98,results,Results,On,CTB
translation,20,101,results,Parsing performances,on,PTB and CTB
translation,20,101,results,significantly improved,with,relatively small beam width ( s = 8 )
translation,20,101,results,Results,has,Parsing performances
translation,20,102,results,86.33 % uas,on,CTB
translation,20,102,results,1.54 % uas improvement,over,greedy baseline parser
translation,20,102,results,Results,achieve,86.33 % uas
translation,21,49,baselines,BILSTM -BERT,replaces,word embeddings
translation,21,49,baselines,word embeddings,with,frozen ( pre-trained ) BERT contextualized word embeddings
translation,21,49,baselines,Baselines,has,BILSTM -BERT
translation,21,51,baselines,BERT 's multi-layer multi-head self-attention architecture,with,pre-trained weights
translation,21,51,baselines,BERT -FT,has,BERT 's multi-layer multi-head self-attention architecture
translation,21,51,baselines,Baselines,has,BERT -FT
translation,21,70,baselines,BILSTM,reimplements,Zhang and Xue ( 2018a ) 's model
translation,21,70,baselines,Zhang and Xue ( 2018a ) 's model,in,"TensorFlow ( Abadi et al. , 2016 )"
translation,21,70,baselines,Baselines,has,BILSTM
translation,21,73,experimental-setup,"Adam ( Kingma and Ba , 2014 )",as,optimizer
translation,21,73,experimental-setup,"Adam ( Kingma and Ba , 2014 )",performed,coarse- to - fine grid search
translation,21,73,experimental-setup,coarse- to - fine grid search,for,key parameters
translation,21,73,experimental-setup,key parameters,such as,learning rate and number of epochs
translation,21,73,experimental-setup,key parameters,using,dev set
translation,21,73,experimental-setup,learning rate and number of epochs,using,dev set
translation,21,73,experimental-setup,Experimental setup,used,"Adam ( Kingma and Ba , 2014 )"
translation,21,73,experimental-setup,Experimental setup,performed,coarse- to - fine grid search
translation,21,75,experimental-setup,NVIDIA Tesla P100 GPUs,for training,models
translation,21,75,experimental-setup,Experimental setup,used,NVIDIA Tesla P100 GPUs
translation,21,50,experiments,BERTbase uncased model,trained on,English Wikipedia and the BookCorpus
translation,21,72,experiments,models,has,BILSTM - BERT and BERT - FT
translation,21,76,experiments,one epoch,takes,7.5 minutes
translation,21,76,experiments,one epoch,takes,0.8 minutes
translation,21,76,experiments,7.5 minutes,for,BERT - FT model
translation,21,76,experiments,0.8 minutes,for,BILSTM - BERT model
translation,21,76,experiments,single GPU,has,one epoch
translation,21,48,hyperparameters,word embeddings,pre-trained from,large corpus
translation,21,48,hyperparameters,Hyperparameters,has,word embeddings
translation,21,6,model,several variants,of,BERT - based
translation,21,6,model,BERT - based,has,temporal dependency parser
translation,21,6,model,BERT,has,significantly improves
translation,21,6,model,significantly improves,has,temporal dependency parsing
translation,21,6,model,Model,develop,several variants
translation,21,6,model,Model,show,BERT
translation,21,56,model,first model,adjusts,model architecture
translation,21,56,model,model architecture,to replace,word embeddings
translation,21,56,model,word embeddings,with,frozen BERT embeddings
translation,21,56,model,Model,has,first model
translation,21,23,results,BERT,improves,TDP performance
translation,21,23,results,TDP performance,in,all models
translation,21,23,results,TDP performance,with,best model
translation,21,23,results,best model,achieving,13 absolute F1 point improvement
translation,21,23,results,13 absolute F1 point improvement,over,our re-implementation
translation,21,74,results,fine-tuning,in,BERT - FT model
translation,21,74,results,BERT,in,BERT - FT model
translation,21,74,results,lower learning rate ( 0.0001 ),paired with,more epochs ( 75 )
translation,21,74,results,lower learning rate ( 0.0001 ),achieves,top performance
translation,21,74,results,lower learning rate ( 0.0001 ),compared to,learning rate 0.001
translation,21,74,results,more epochs ( 75 ),achieves,top performance
translation,21,74,results,learning rate 0.001,with,50 epochs
translation,21,74,results,50 epochs,for,BiLSTM models
translation,21,74,results,fine-tuning,has,BERT
translation,21,74,results,fine-tuning,has,lower learning rate ( 0.0001 )
translation,21,74,results,BERT,has,lower learning rate ( 0.0001 )
translation,21,74,results,BERT - FT model,has,lower learning rate ( 0.0001 )
translation,21,74,results,Results,observed,fine-tuning
translation,21,74,results,Results,when,fine-tuning
translation,21,83,results,BERT - based encoder ( BERT - FT ),resulted in,an absolute improvement
translation,21,83,results,an absolute improvement,of,13 absolute F1 points
translation,21,83,results,an absolute improvement,of,8 F1 points
translation,21,83,results,an absolute improvement,as much as,13 absolute F1 points
translation,21,83,results,an absolute improvement,as much as,8 F1 points
translation,21,83,results,13 absolute F1 points,over,BiLSTM re-implementation
translation,21,83,results,8 F1 points,reported results in,Zhang and Xue ( 2019 )
translation,21,83,results,Results,Finetuning,BERT - based encoder ( BERT - FT )
translation,22,117,ablation-analysis,Morphological features,help,parser
translation,22,117,ablation-analysis,Morphological features,resulting in,UAS 0.5 % absolute F1 score improvement
translation,22,117,ablation-analysis,UAS 0.5 % absolute F1 score improvement,For,tokenization
translation,22,117,ablation-analysis,word features,are,useful
translation,22,117,ablation-analysis,useful,made,1 % absolute F1 score improvement
translation,22,117,ablation-analysis,tokenization,has,word features
translation,22,117,ablation-analysis,Ablation analysis,For,tokenization
translation,22,117,ablation-analysis,Ablation analysis,has,Morphological features
translation,22,118,ablation-analysis,Lemma features,do not have,big effect
translation,22,118,ablation-analysis,big effect,on,parsing
translation,22,118,ablation-analysis,Ablation analysis,has,Lemma features
translation,22,122,ablation-analysis,Word cluster features,have,limited gains
translation,22,122,ablation-analysis,Ablation analysis,has,Word cluster features
translation,22,125,ablation-analysis,did not help,when,k
translation,22,125,ablation-analysis,k,is,small
translation,22,125,ablation-analysis,word cluster features,has,did not help
translation,22,125,ablation-analysis,Ablation analysis,noticed that,word cluster features
translation,22,5,baselines,structured linear classifiers,to learn,millions of sparse features
translation,22,14,experimental-setup,pipeline,for,each language
translation,22,14,experimental-setup,training portion,of,treebank
translation,22,14,experimental-setup,training portion,of,official word embeddings
translation,22,14,experimental-setup,official word embeddings,for,45 languages
translation,22,14,experimental-setup,each language,has,independently
translation,22,14,experimental-setup,Experimental setup,train,pipeline
translation,22,15,experimental-setup,system components,implemented in,C ++
translation,22,15,experimental-setup,Experimental setup,has,system components
translation,22,108,experimental-setup,k means,run,20 iterations
translation,22,108,experimental-setup,20 iterations,using,45 threads
translation,22,108,experimental-setup,45 threads,to quickly generate,clusters
translation,22,6,model,linear classifier,for,sentence boundary prediction
translation,22,6,model,linear classifier,for,tokenization
translation,22,6,model,linear classifier,for,part- of-speech tagging
translation,22,6,model,linear classifier,for,morph analysis
translation,22,6,model,linear classifier,for,tokenization
translation,22,6,model,linear classifier,for,part- of-speech tagging
translation,22,6,model,linear classifier,for,morph analysis
translation,22,6,model,linear chain conditional random fields ( CRFs ),for,tokenization
translation,22,6,model,linear chain conditional random fields ( CRFs ),for,part- of-speech tagging
translation,22,6,model,linear chain conditional random fields ( CRFs ),for,morph analysis
translation,22,6,model,Model,trained,linear classifier
translation,22,6,model,Model,trained,linear chain conditional random fields ( CRFs )
translation,22,7,model,second order graph based parser,learns,tree structure ( without relations )
translation,22,7,model,linear tree CRF,assigns,relations
translation,22,7,model,relations,to,dependencies
translation,22,7,model,dependencies,in,tree
translation,22,7,model,Model,has,second order graph based parser
translation,22,96,results,consistently outperforms,about,0.1 ? 0.2 %
translation,22,96,results,averaged perceptron,about,0.1 ? 0.2 %
translation,22,96,results,0.1 ? 0.2 %,for,all tasks
translation,22,96,results,k best MIRA,has,consistently outperforms
translation,22,96,results,consistently outperforms,has,averaged perceptron
translation,22,96,results,Results,found that,k best MIRA
translation,22,116,results,Character type features,useful for,sentence segmentation
translation,22,116,results,feature effect,has,Character type features
translation,22,116,results,Results,regarding,feature effect
translation,22,124,results,English development set,showed,two approaches
translation,22,124,results,two approaches,performed,quite the same
translation,22,124,results,0.15 % improvement,over,baseline
translation,22,124,results,Results,on,English development set
translation,22,132,results,Our system,ranked,15 th
translation,22,132,results,15 th,among,33 submissions
translation,22,132,results,Results,has,Our system
translation,23,24,ablation-analysis,data augmentation,in,training environment
translation,23,24,ablation-analysis,GAZP,has,outperforms
translation,23,24,ablation-analysis,outperforms,has,data augmentation
translation,23,24,ablation-analysis,Ablation analysis,show,GAZP
translation,23,192,ablation-analysis,? 60 %,of,synthesized examples
translation,23,192,ablation-analysis,cycle-consistency,has,effectively prunes
translation,23,192,ablation-analysis,effectively prunes,has,? 60 %
translation,23,192,ablation-analysis,significantly degrade,has,performance
translation,23,192,ablation-analysis,Ablation analysis,inclusion of,cycle-consistency
translation,23,5,model,GAZP,combines,forward semantic parser
translation,23,5,model,GAZP,selects,cycleconsistent examples
translation,23,5,model,forward semantic parser,with,backward utterance generator
translation,23,5,model,backward utterance generator,to synthesize,data ( e.g. utterances and SQL queries )
translation,23,5,model,data ( e.g. utterances and SQL queries ),in,new environment
translation,23,5,model,cycleconsistent examples,to adapt,parser
translation,23,5,model,Model,has,GAZP
translation,23,212,model,cycle-consistency,to prune,low-quality data
translation,23,212,model,cycle-consistency,keep,high-quality data
translation,23,212,model,high-quality data,for,adaptation
translation,23,212,model,Model,propose,cycle-consistency
translation,23,7,results,GAZP,improves,logical form and execution accuracy
translation,23,7,results,logical form and execution accuracy,of,baseline parser
translation,23,7,results,"Spider , Sparc , and CoSQL zero-shot semantic parsing tasks",has,GAZP
translation,23,7,results,Results,On,"Spider , Sparc , and CoSQL zero-shot semantic parsing tasks"
translation,23,23,results,GAZP,improves,logical form and execution accuracy
translation,23,23,results,logical form and execution accuracy,of,baseline parser
translation,23,23,results,logical form and execution accuracy,successfully adapting,existing parser
translation,23,23,results,baseline parser,on,all tasks
translation,23,23,results,Results,has,GAZP
translation,23,179,results,adaptation,by,GAZP
translation,23,179,results,GAZP,results in,consistent performance improvement
translation,23,179,results,consistent performance improvement,across,"Spider , Sparc , and CoSQL"
translation,23,179,results,consistent performance improvement,in terms of,"EM , EX , and FX"
translation,23,179,results,Results,shows,adaptation
translation,23,190,results,augmentation,in,training environment
translation,23,190,results,new environment,has,significantly outperforms
translation,23,190,results,significantly outperforms,has,augmentation
translation,23,190,results,Results,adaptation to,new environment
translation,23,190,results,Results,has,Our results
translation,23,195,results,EM consistency,shows,variant of cycleconsistency
translation,23,195,results,improves,has,performance
translation,23,195,results,Results,has,EM consistency
translation,23,196,results,EM consistency,performs,similarly
translation,23,196,results,similarly,to,execution consistency
translation,23,196,results,Results,has,EM consistency
translation,24,85,results,results,for,development set
translation,24,85,results,Results,for,development set
translation,24,92,results,measure Scope tokens,takes into account,tokens
translation,24,92,results,measure Scope tokens,takes into account,tokens
translation,24,92,results,tokens,included in,scope
translation,24,92,results,our system,provides,interesting outcomes ( around 63 % F1 and 73 % F1
translation,24,92,results,measure Scope tokens,has,our system
translation,24,92,results,scope match,has,our system
translation,24,92,results,annotate,has,tokens
translation,24,92,results,Results,for,measure Scope tokens
translation,24,94,results,negated events results,are,very low
translation,24,94,results,very low,around,17.46 % F1 and 22.53 % F1
translation,24,118,results,results,are,higher
translation,24,118,results,higher,around,73 % F1
translation,24,118,results,73 % F1,in,scope tokens
translation,24,118,results,55 %,in,full match scopes
translation,24,118,results,scope tokens detection,has,results
translation,24,118,results,Results,With respect to,scope tokens detection
translation,24,118,results,Results,are,higher
translation,25,5,baselines,MRS,has,Bi-Lexical Dependencies ( DP )
translation,25,19,baselines,Baselines,has,Biaffine attention decoder ( frameworkspecific )
translation,25,97,baselines,baseline biaffine attention model,based on,BiLSTM sentence encoder
translation,25,97,baselines,BiLSTM sentence encoder,without using,BERT
translation,25,98,baselines,Baselines,has,BERT + Biaffine
translation,25,100,baselines,Baselines,has,BERT + Multi-level Biaffine
translation,25,102,baselines,BERT + Biaffine + MTL,uses,multitask learning
translation,25,102,baselines,Baselines,has,BERT + Biaffine + MTL
translation,25,89,experimental-setup,Adam optimizer,to train,our biaffine attention models
translation,25,89,experimental-setup,Experimental setup,used,Adam optimizer
translation,25,6,model,unified parsing model,using,biaffine attention
translation,25,6,model,Model,propose,unified parsing model
translation,25,7,model,BERT - BiLSTM,for,sentence encoder
translation,25,7,model,BERT - BiLSTM,uses,BERT
translation,25,7,model,sentence encoder,uses,BERT
translation,25,7,model,BERT,to compose,sentence 's wordpieces
translation,25,7,model,BERT,applies,BiLSTM
translation,25,7,model,sentence 's wordpieces,into,word-level embeddings
translation,25,7,model,BiLSTM,to,word -level representations
translation,25,7,model,Model,has,BERT - BiLSTM
translation,25,8,model,biaffine attention decoder,determines,scores
translation,25,8,model,scores,for,edge 's existence and its labels
translation,25,8,model,edge 's existence and its labels,based on,biaffine attention functions
translation,25,8,model,biaffine attention functions,between,roledependent representations
translation,25,8,model,Model,has,biaffine attention decoder
translation,25,9,model,multi-level biaffine attention models,by combining,all the role-dependent representations
translation,25,9,model,all the role-dependent representations,appear at,multiple intermediate layers
translation,25,9,model,Model,present,multi-level biaffine attention models
translation,25,12,model,multi-task learning,explicitly make,shared common components
translation,25,12,model,shared common components,in,neural network architecture
translation,25,12,model,neural network architecture,across,different frameworks
translation,25,12,model,Model,To enable,multi-task learning
translation,25,13,model,MRP 2019,propose,unified neural model
translation,25,13,model,unified neural model,for,DM / PSD / UCCA frameworks
translation,25,13,model,biaffine attention,used in,"Manning , 2017 , 2018"
translation,25,13,model,Model,For,MRP 2019
translation,25,20,model,Role-dependent representations,for,each word
translation,25,20,model,Role-dependent representations,first induced from,sentence - level embeddings
translation,25,20,model,each word,first induced from,sentence - level embeddings
translation,25,20,model,sentence - level embeddings,of,BERT - BiLSTM encoder
translation,25,20,model,BERT - BiLSTM encoder,using,simple feed -forward layers
translation,25,20,model,Model,has,Role-dependent representations
translation,25,32,model,multi-level biaffine attention models,motivated by,multi-level architecture
translation,25,32,model,multi-level architecture,of,FusionNet
translation,25,32,model,FusionNet,in,machine reading comprehension task
translation,25,32,model,Model,present,multi-level biaffine attention models
translation,25,43,model,input sentence,fed to,word representation layer
translation,25,43,model,word representation layer,using,BERT
translation,25,43,model,sequence of word embedding vectors,given to,BiLSTM layer
translation,25,43,model,BiLSTM layer,to produce,sentence representation
translation,25,43,model,BERT - BiLSTM encoder,has,input sentence
translation,25,43,model,Model,In,BERT - BiLSTM encoder
translation,25,44,model,additional feed -forward layers,applied to obtain,role-dependent representations
translation,25,44,model,role-dependent representations,for,head and dependent roles
translation,25,44,model,biaffine attention,has,additional feed -forward layers
translation,25,44,model,Model,In,biaffine attention
translation,25,99,model,Model,uses,BERT - BiLSTM encoder
translation,25,101,model,Model,uses,BERT - BiLSTM encoder
translation,25,101,model,Model,uses,multi-level attention method
translation,25,119,model,Jeonbuk National University 's system,based on,unified biaffine attention models
translation,25,119,model,unified biaffine attention models,for,"DM , PSD , and UCCA frameworks"
translation,25,119,model,"DM , PSD , and UCCA frameworks",for,MRP 2019 task
translation,25,119,model,Model,presented,Jeonbuk National University 's system
translation,25,103,results,four variants,of,biaffine
translation,25,103,results,Results,"UF , LF , and Top on",three semantic graph frameworks
translation,25,104,results,BERT + Biaffine,performs,better
translation,25,104,results,better,than,Biaffine
translation,25,104,results,Biaffine,obtaining,increases
translation,25,104,results,increases,of,5 %
translation,25,104,results,5 %,for,UF and LF
translation,25,104,results,UF and LF,on,UCCA framework
translation,25,104,results,Results,has,BERT + Biaffine
translation,25,105,results,BERT + Multi-level Biaffine,does not achieve,any further improvements
translation,25,105,results,any further improvements,with respect to,Biaffine
translation,25,105,results,any further improvements,yielding,weak performances
translation,25,105,results,model,on,PSD and UCCA frameworks
translation,25,105,results,Results,has,BERT + Multi-level Biaffine
translation,25,106,results,BERT + Biaffine + MTL,achieves,small improvements
translation,25,106,results,small improvements,on,UCCA framework
translation,25,106,results,Results,has,BERT + Biaffine + MTL
translation,25,115,results,our system,shows,better performances
translation,25,115,results,better performances,over,baseline TUPA 's system
translation,25,115,results,better performances,except for,results of UCCA metrics
translation,25,115,results,Results,has,our system
translation,25,116,results,slightly improved performance,over,ERG
translation,25,116,results,slightly improved performance,in terms of,UF
translation,25,116,results,ERG,in terms of,UF
translation,25,116,results,UF,of,SDP metric
translation,25,116,results,ERG,has,our biaffine system
translation,25,116,results,top-performing system,has,our biaffine system
translation,25,116,results,Results,Comparing to,ERG
translation,25,117,results,published MRP metrics,of,best system
translation,25,117,results,performances,of,our system
translation,25,117,results,performances,are,1
translation,25,117,results,performances,about,1
translation,25,117,results,published MRP metrics,has,performances
translation,25,117,results,best system,has,performances
translation,25,117,results,Results,Comparing to,published MRP metrics
translation,26,168,ablation-analysis,all sub-components,significantly contributed to,model performance
translation,26,168,ablation-analysis,Ablation analysis,has,all sub-components
translation,26,169,ablation-analysis,decoding search space pruning strategies,including,generation in execution order
translation,26,169,ablation-analysis,decoding search space pruning strategies,including,static SQL correctness check
translation,26,169,ablation-analysis,decoding search space pruning strategies,are,effective
translation,26,169,ablation-analysis,effective,with,absolute E-SM improvements
translation,26,169,ablation-analysis,absolute E-SM improvements,ranging from,0.6 % to 2.6 %
translation,26,169,ablation-analysis,Ablation analysis,has,decoding search space pruning strategies
translation,26,174,ablation-analysis,BERT,critical to,performance
translation,26,174,ablation-analysis,performance,of,BRIDGE
translation,26,174,ablation-analysis,performance,of,base model
translation,26,174,ablation-analysis,performance,of,base model
translation,26,174,ablation-analysis,base model,by,more than three folds
translation,26,174,ablation-analysis,magnifying,has,performance
translation,26,174,ablation-analysis,Ablation analysis,has,BERT
translation,26,128,hyperparameters,"Adam-SGD ( Kingma and Ba , 2015 )",with,default parameters
translation,26,128,hyperparameters,mini-batch size,of,32
translation,26,128,hyperparameters,Hyperparameters,use,"Adam-SGD ( Kingma and Ba , 2015 )"
translation,26,129,hyperparameters,uncased BERT - base model,from,Huggingface 's transformer library
translation,26,129,hyperparameters,Hyperparameters,use,uncased BERT - base model
translation,26,130,hyperparameters,all LSTMs,to,1 - layer
translation,26,130,hyperparameters,Hyperparameters,set,all LSTMs
translation,26,130,hyperparameters,Hyperparameters,set,hidden state dimension n = 512
translation,26,131,hyperparameters,maximum,of,"50,000 steps"
translation,26,131,hyperparameters,learning rate,linearly shrink,0
translation,26,131,hyperparameters,5e ? 4,in,"first 5,000 iterations"
translation,26,131,hyperparameters,Hyperparameters,train,maximum
translation,26,132,hyperparameters,BERT,with,fine-tuning rate
translation,26,132,hyperparameters,fine-tuning rate,linearly increasing from,3e ? 5 to 8e ? 5
translation,26,132,hyperparameters,3e ? 5 to 8e ? 5,in,"first 5,000 iterations"
translation,26,132,hyperparameters,linearly decaying,to,0
translation,26,132,hyperparameters,Hyperparameters,fine- tune,BERT
translation,26,135,hyperparameters,beam size,of,256
translation,26,135,hyperparameters,256,for,leaderboard evaluation
translation,26,136,hyperparameters,experiments,uses,beam size
translation,26,136,hyperparameters,beam size,of,16
translation,26,4,model,Model,present,BRIDGE
translation,26,5,model,BRIDGE,represents,question and DB schema
translation,26,5,model,BRIDGE,in,tagged sequence
translation,26,5,model,question and DB schema,in,tagged sequence
translation,26,5,model,tagged sequence,where,subset of the fields
translation,26,5,model,subset of the fields,augmented with,cell values
translation,26,5,model,cell values,mentioned in,question
translation,26,5,model,Model,has,BRIDGE
translation,26,26,model,BRIDGE,represents,relational DB schema
translation,26,26,model,relational DB schema,as,tagged sequence
translation,26,26,model,tagged sequence,concatenated to,question
translation,26,26,model,Model,has,BRIDGE
translation,26,225,model,specific attention heads,to capture,DB connections
translation,26,225,model,graph structure,of,relational DBs
translation,26,225,model,graph structure,within,BRIDGE framework
translation,26,225,model,Model,regularizing,specific attention heads
translation,26,32,results,performs competitively,on,well studied Spider benchmark
translation,26,32,results,most of recently proposed models,with,more sophisticated neural architectures
translation,26,32,results,pointer- generator decoder,has,BRIDGE
translation,26,32,results,pointer- generator decoder,has,outperforming
translation,26,32,results,schema-consistency driven search space pruning,has,BRIDGE
translation,26,32,results,BRIDGE,has,performs competitively
translation,26,32,results,well studied Spider benchmark,has,Structure Acc
translation,26,32,results,Structure Acc,has,65.6 % dev
translation,26,32,results,Execution Acc,has,59.9 % test
translation,26,32,results,Execution Acc,has,top - 1 rank
translation,26,32,results,outperforming,has,most of recently proposed models
translation,26,32,results,Results,Combined with,pointer- generator decoder
translation,26,148,results,static checking approach,handles,complex SQL queries
translation,26,148,results,static checking approach,avoids,DB execution overhead
translation,26,148,results,Results,has,static checking approach
translation,26,150,results,k,from,1 to 2
translation,26,150,results,1 to 2,yield,marginal performance improvement
translation,26,150,results,changing,has,k
translation,26,153,results,significantly better,than,BRIDGE
translation,26,153,results,BRIDGE,by,E-SM
translation,26,153,results,E-SM,are,RYANSQL v2+BERT L
translation,26,153,results,E-SM,are,RAT -SQL v3 + BERT L
translation,26,153,results,two approaches,has,significantly better
translation,26,153,results,Results,has,two approaches
translation,26,155,results,BRIDGE,has,underperforms
translation,26,155,results,underperforms,has,RAT - SQL v3 +BERT
translation,26,155,results,Results,has,BRIDGE
translation,26,171,results,anchor texts,results in,absolute E-SM improvement
translation,26,171,results,absolute E-SM improvement,of,3 %
translation,26,171,results,Results,adding,anchor texts
translation,26,172,results,BRIDGE with and without anchor texts,shows,anchor text augmentation
translation,26,172,results,model performance,at,all hardness levels
translation,26,172,results,all hardness levels,especially in,hard and extra-hard categories
translation,26,172,results,anchor text augmentation,has,improves
translation,26,172,results,improves,has,model performance
translation,26,173,results,non-ground - truth tables,during,training
translation,26,173,results,diversity,of,DB schema
translation,26,173,results,Shuffling and randomly dropping,has,non-ground - truth tables
translation,26,173,results,training,has,significantly helps
translation,26,173,results,significantly helps,has,our ap- proach
translation,26,173,results,increases,has,diversity
translation,26,173,results,Results,has,Shuffling and randomly dropping
translation,26,176,results,performances,of,RAT -SQL v2 and RAT -SQL v2+BERT L
translation,26,176,results,improvement,with,BERT L
translation,26,176,results,BERT L,is,7 %
translation,26,176,results,performances,has,improvement
translation,26,210,results,BRIDGE,achieves,near perfect score
translation,26,210,results,near perfect score,on,some
translation,27,66,baselines,parsing system implementation,is,yap
translation,27,66,baselines,Baselines,has,parsing system implementation
translation,27,65,experiments,sentence segmentation and tokenization,up to and including,full morphological disambiguation
translation,27,65,experiments,sentence segmentation and tokenization,rely on,"UDPipe ( Straka et al. , 2016 )"
translation,27,65,experiments,full morphological disambiguation,for,all languages
translation,27,5,model,transitionbased parser,called,yap
translation,27,5,model,Model,based on,transitionbased parser
translation,27,95,results,baseline data-driven MA,learned from,treebank alone
translation,27,95,results,baseline data-driven MA,with,MA
translation,27,95,results,MA,backed with,external broad-coverage lexicon
translation,27,95,results,external broad-coverage lexicon,called,HebLex
translation,27,95,results,LAS results,arrive at,60.94 LAS
translation,27,95,results,LAS results,arrive at,outperforming
translation,27,95,results,results,obtained by,yap DEP
translation,27,95,results,gap,with,UDPipe full model
translation,27,95,results,baseline data-driven MA,has,LAS results
translation,27,95,results,outperforming,has,results
translation,27,95,results,Results,replace,baseline data-driven MA
translation,27,99,results,significant improvement,in,LAS
translation,27,99,results,significant improvement,obtaining,71.39
translation,27,99,results,71.39,without changing,parsing algorithms
translation,27,101,results,upper-bound,of,our parser
translation,27,101,results,our parser,given,completely disambiguated morphological input stream
translation,27,101,results,our parser,provides,LAS
translation,27,101,results,completely disambiguated morphological input stream,provides,LAS
translation,27,101,results,LAS,of,79.33
translation,27,101,results,79.33,few points above,best system ( Stanford )
translation,28,78,hyperparameters,overfitting problem,for,training
translation,28,78,hyperparameters,overfitting problem,use,dropout
translation,28,78,hyperparameters,training,on,relative small RST Treebank
translation,28,78,hyperparameters,dropout,of,0.5
translation,28,78,hyperparameters,Hyperparameters,To alleviate,overfitting problem
translation,28,6,model,first end-to - end discourse parser,jointly,parses
translation,28,6,model,parses,in,syntax and discourse levels
translation,28,6,model,first syntacto- discourse treebank,by integrating,Penn Treebank
translation,28,6,model,Penn Treebank,with,RST Treebank
translation,28,6,model,Model,propose,first end-to - end discourse parser
translation,28,13,model,first joint syntacto- discourse treebank,by unifying,constituency and discourse tree representations
translation,28,13,model,Model,propose,first joint syntacto- discourse treebank
translation,28,14,model,jointly parses,at,constituency and discourse levels
translation,28,14,model,Model,propose,first end-to - end incremental parser
translation,28,15,model,span-based parser,has,"Cross and Huang , 2016 )"
translation,28,15,model,parses,has,efficiently and robustly
translation,28,16,model,constituency and discourse trees,to facilitate,parsing
translation,28,16,model,parsing,at,both levels
translation,28,16,model,parsing,without,explicit conversion mechanism
translation,28,18,model,parses,at,constituency and discourse levels
translation,28,18,model,Model,propose,novel joint parser
translation,28,33,model,highlevel discourse parses,using,single joint parser
translation,28,7,results,joint syntactodiscourse parser,achieves,state - of- theart end-to - end discourse parsing accuracy
translation,28,7,results,constituency parser,has,joint syntactodiscourse parser
translation,28,21,results,end-to - end parser,has,outperforms
translation,28,21,results,outperforms,has,existing pipelined discourse parsing efforts
translation,28,21,results,Results,has,end-to - end parser
translation,28,86,results,our parser,in,end-to - end parsing
translation,28,86,results,endto-end,has,parsing
translation,28,86,results,endto-end,has,our parser
translation,28,86,results,parsing,has,our parser
translation,28,86,results,our parser,has,first outperforms
translation,28,86,results,first outperforms,has,state - of - the - art segmentator
translation,28,86,results,Results,in the perspective of,endto-end
translation,29,10,model,learning,of,dialog strategy
translation,29,10,model,learning,of,semantic parser
translation,29,10,model,dialog strategy,using,reinforcement learning
translation,29,10,model,semantic parser,for,robust natural language understanding
translation,29,10,model,only natural dialog interaction,for,supervision
translation,29,10,model,Model,integrate,learning
translation,29,102,model,POMDP,to model,dialog
translation,29,102,model,POMDP,learn,"policy ( Young et al. , 2013 )"
translation,29,102,model,"policy ( Young et al. , 2013 )",adapting,Hidden Information State model ( HIS )
translation,29,102,model,Hidden Information State model ( HIS ),to track,belief state
translation,29,102,model,belief state,as,dialog progresses
translation,29,102,model,Model,use,POMDP
translation,29,207,results,improvement,of,batchwise learning agent
translation,29,207,results,batchwise learning agent,over,agents performing only parser or only dialog learning
translation,29,207,results,agents performing only parser or only dialog learning,are,statistically significant
translation,29,207,results,dialog length,has,improvement
translation,29,207,results,Results,In,dialog length
translation,29,208,results,agent,performing,batchwise parser and dialog learning
translation,29,208,results,agents,performing,only parser
translation,29,208,results,agents,performing,only dialog learning
translation,29,208,results,batchwise parser and dialog learning,has,outperforms
translation,29,208,results,outperforms,has,agents
translation,29,208,results,Results,has,agent
translation,29,211,results,full learning,of,both components
translation,29,211,results,both components,has,does not in fact outperform
translation,29,211,results,does not in fact outperform,has,parser learning
translation,29,211,results,Results,observe,full learning
translation,29,246,results,multiple parses,when,updating
translation,29,246,results,state,improves,overall dialog success
translation,29,246,results,updating,has,state
translation,29,246,results,Results,using,multiple parses
translation,30,182,experimental-setup,Experimental setup,implemented using,Chainer
translation,30,212,experimental-setup,mini-batch stochastic gradient decent,with,backpropagation
translation,30,212,experimental-setup,Experimental setup,trained using,mini-batch stochastic gradient decent
translation,30,213,experimental-setup,Adam optimizer,with,gradient clipping
translation,30,213,experimental-setup,Experimental setup,use,Adam optimizer
translation,30,11,experiments,CoNLL 2019,has,Cross -Framework Meaning Representation Parsing ( MRP 2019 )
translation,30,6,model,unified encoder- to - biaffine network,for,frameworks
translation,30,6,model,unified encoder- to - biaffine network,incorporates,shared encoder
translation,30,6,model,shared encoder,to extract,rich input features
translation,30,6,model,decoder networks,to generate,anchorless nodes
translation,30,6,model,anchorless nodes,in,UCCA and AMR
translation,30,6,model,anchorless nodes,in,biaffine networks
translation,30,6,model,biaffine networks,to predict,edges
translation,30,6,model,Model,proposed,unified encoder- to - biaffine network
translation,30,30,model,Hidden states,of,network
translation,30,30,model,Hidden states,fed to,biaffine network
translation,30,30,model,biaffine network,to predict,edges and their labels
translation,30,30,model,Model,has,Hidden states
translation,30,153,model,Model,employ,extended pointer - generator network
translation,30,165,model,edge prediction,apply,Chu-Liu-Edmonds algorithm
translation,30,165,model,Chu-Liu-Edmonds algorithm,to find,maximum spanning tree
translation,30,165,model,Model,For,edge prediction
translation,30,7,results,ranked fifth,with,macro- averaged MRP F1 score
translation,30,7,results,macro- averaged MRP F1 score,of,0.7604
translation,30,7,results,outperformed,has,baseline unified transition - based MRP
translation,30,7,results,Results,has,Our system
translation,30,22,results,multi-task setup,after,formal run
translation,30,22,results,improvement,in,performance
translation,30,22,results,Results,evaluated,multi-task setup
translation,30,185,results,Our system,obtained,macro-averaged MRP F1 score
translation,30,185,results,Our system,ranked,fifth
translation,30,185,results,macro-averaged MRP F1 score,of,0.7604
translation,30,185,results,fifth,amongst,all submissions
translation,30,185,results,Results,has,Our system
translation,30,186,results,outperformed,in,all frameworks
translation,30,186,results,conventional unified architecture,for,MRP
translation,30,186,results,conventional unified architecture,in,all frameworks
translation,30,186,results,outperformed,has,conventional unified architecture
translation,30,194,results,singleframework learning variant ( SFL ),without,BERT ( SFL )
translation,30,194,results,singleframework learning variant ( SFL ),without,BERT ( BERT + SFL ( NT ) )
translation,30,194,results,singleframework learning variant ( SFL ),performed,better
translation,30,194,results,singleframework learning variant ( SFL ),with,BERT ( BERT + SFL ( NT ) )
translation,30,194,results,better,than,SFL
translation,30,194,results,SFL,with,BERT ( BERT + SFL ( NT ) )
translation,30,194,results,Results,has,singleframework learning variant ( SFL )
translation,30,195,results,SFL,in,comparable condition
translation,30,195,results,multi-task learning variant ( MTL ) with fine-tuning ( BERT + MTL + FT ( NT ) ),has,outperformed
translation,30,195,results,outperformed,has,SFL
translation,30,195,results,Results,has,multi-task learning variant ( MTL ) with fine-tuning ( BERT + MTL + FT ( NT ) )
translation,30,200,results,outperformed,has,baseline unified transitionbased MRP
translation,31,201,experiments,competitive,with,SHRG ( Synchronous Hyperedge Replacement Grammar ) method
translation,31,201,experiments,SHRG ( Synchronous Hyperedge Replacement Grammar ) method,to formalize,string - tograph problem
translation,31,144,hyperparameters,all the neural AMR parsing systems,use,256
translation,31,144,hyperparameters,256,for,hidden layer size
translation,31,144,hyperparameters,256,for,word embedding size
translation,31,144,hyperparameters,Hyperparameters,For training,all the neural AMR parsing systems
translation,31,145,hyperparameters,Stochastic gradient descent,to optimize,cross-entropy loss function
translation,31,145,hyperparameters,Stochastic gradient descent,set,drop out rate
translation,31,145,hyperparameters,drop out rate,to be,0.5
translation,31,145,hyperparameters,Hyperparameters,has,Stochastic gradient descent
translation,31,146,hyperparameters,model,for,150 epochs
translation,31,146,hyperparameters,model,with,initial learning rate
translation,31,146,hyperparameters,model,with,learning rate decay factor
translation,31,146,hyperparameters,initial learning rate,of,0.5
translation,31,146,hyperparameters,learning rate decay factor,if,model
translation,31,146,hyperparameters,0.95,if,model
translation,31,146,hyperparameters,does n't improve,for,3 last epochs
translation,31,146,hyperparameters,learning rate decay factor,has,0.95
translation,31,146,hyperparameters,model,has,does n't improve
translation,31,146,hyperparameters,Hyperparameters,train,model
translation,31,6,model,sequence - to-sequence model,for,AMR parsing
translation,31,6,model,Model,describe,sequence - to-sequence model
translation,31,28,model,categorization strategy,first maps,low frequency concepts and entity subgraphs
translation,31,28,model,low frequency concepts and entity subgraphs,to,reduced set of category types
translation,31,29,model,each type,use,heuristic alignments
translation,31,29,model,heuristic alignments,to connect,source side spans and target side concepts or subgraphs
translation,31,29,model,Model,to map,each type
translation,31,29,model,Model,use,heuristic alignments
translation,31,219,model,sequence - to-sequence model,for,AMR parsing
translation,31,219,model,Model,described,sequence - to-sequence model
translation,31,32,results,Our model,provides,competitive benchmark
translation,31,32,results,improves significantly,in comparison with,previously reported sequence - to-sequence results
translation,31,32,results,competitive benchmark,in comparison with,state- ofthe - art results
translation,31,32,results,state- ofthe - art results,without using,dependency parses or other external semantic resources
translation,31,32,results,Our model,has,improves significantly
translation,31,32,results,Results,has,Our model
translation,31,157,results,re-categorization,improves,F-score
translation,31,157,results,F-score,by,13 points
translation,31,157,results,13 points,on,development set
translation,31,179,results,supervised attention model,able to,further improve
translation,31,179,results,Smatch score,by,another 2 points
translation,31,179,results,3 points increase,in,recall
translation,31,179,results,further improve,has,Smatch score
translation,31,179,results,Results,has,supervised attention model
translation,31,197,results,our system,has,fails to outperform
translation,31,197,results,fails to outperform,has,state - of - the - art system
translation,32,120,experimental-setup,ground truth,using,Stanford parser
translation,32,120,experimental-setup,ground truth,train for,10 epochs
translation,32,120,experimental-setup,Stanford parser,has,"Manning et al. , 2014 )"
translation,32,120,experimental-setup,Experimental setup,binarize,ground truth
translation,32,5,model,vector averaging approach,is,locally greedy
translation,32,5,model,vector averaging approach,cannot recover from,errors
translation,32,5,model,errors,when computing,highest scoring parse tree
translation,32,5,model,highest scoring parse tree,in,bottom - up chart parsing
translation,32,6,model,improved variant of DIORA,that encodes,single tree
translation,32,6,model,single tree,rather than,softlyweighted mixture of trees
translation,32,6,model,beam,at,each cell
translation,32,6,model,S-DIORA,has,improved variant of DIORA
translation,32,6,model,Model,introduce,S-DIORA
translation,32,21,model,DIORA,easily recover from,local errors
translation,32,21,model,Model,extend,DIORA
translation,32,22,model,weight assignment,used for,vector averaging
translation,32,22,model,vector averaging,with,sparse operator
translation,32,22,model,sparse operator,equivalent to,one-hot argmax function
translation,32,22,model,each representation,accurately encodes,single tree
translation,32,22,model,Model,replace,weight assignment
translation,32,30,model,extension to DIORA,called,S-DIORA
translation,32,30,model,extension to DIORA,called,S-DIORA
translation,32,30,model,extension to DIORA,allows for,easy recovery
translation,32,30,model,easy recovery,from,local errors
translation,32,30,model,New results,in,unsupervised constituency parsing
translation,32,30,model,New results,improving over,previous state of the art
translation,32,30,model,previous state of the art,by,2.2 6 % F1
translation,32,30,model,Thorough error analysis,of,parse tree output
translation,32,30,model,Thorough error analysis,revealing,useful insights
translation,32,30,model,improves,over,baselines
translation,32,30,model,S-DIORA,has,improves
translation,32,122,results,not competitive,with,Kitaev and Klein ( 2018 ) parser
translation,32,122,results,Results,see that,DIORA
translation,32,137,results,S-DIORA,sees,large improvement
translation,32,137,results,large improvement,in,WSJ - 10
translation,32,137,results,Results,has,S-DIORA
translation,32,165,results,segment recall,see that,C-PCFG
translation,32,165,results,C-PCFG,outperforms,DIORA
translation,32,165,results,DIORA,in,segment recall
translation,32,165,results,DIORA,for,NP and PP
translation,32,165,results,segment recall,for,NP and PP
translation,32,165,results,Results,By,segment recall
translation,32,179,results,XLNet =0,is,worst
translation,32,179,results,worst,of,all models
translation,32,179,results,all models,in,S - F1 and VP segment recall
translation,32,179,results,Results,see that,XLNet =0
translation,32,200,results,NLI,provides,S-DIORA
translation,32,200,results,S-DIORA,with,substantial advantage
translation,32,200,results,substantial advantage,in,segment recall
translation,32,200,results,segment recall,for,VP and PP
translation,32,200,results,Results,Training on,NLI
translation,32,203,results,improves,increase,beam size
translation,32,203,results,S-DIORA P T B,improves over,DIORA
translation,32,203,results,Effects of Beam Size Performance,has,improves
translation,32,203,results,Results,has,Effects of Beam Size Performance
translation,32,205,results,beam,helps,different classes of errors
translation,32,205,results,Results,Increasing,beam
translation,33,161,results,performance,of,baseline UDPipe system
translation,33,211,results,ICS PAS,is,most successful system
translation,33,211,results,most successful system,in the domain of,small treebanks
translation,33,211,results,Results,has,ICS PAS
translation,34,77,ablation-analysis,CFG mode,obtain,51.2 % reduction
translation,34,77,ablation-analysis,51.2 % reduction,in,parsing failure
translation,34,77,ablation-analysis,Ablation analysis,In,CFG mode
translation,34,107,results,reduction,is,51.2 %
translation,34,107,results,59.1 % reduction,in,parse failures
translation,34,107,results,baseline backtracking,has,reduction
translation,34,107,results,Optimistic backtracking,has,59.1 % reduction
translation,34,107,results,Results,Using,baseline backtracking
translation,34,107,results,Results,Using,Optimistic backtracking
translation,35,44,baselines,MaltParser,is,deterministic transition - based dependency parser
translation,35,44,baselines,deterministic transition - based dependency parser,obtains,dependency tree
translation,35,44,baselines,dependency tree,in,linear-time
translation,35,44,baselines,linear-time,in,single pass
translation,35,44,baselines,single pass,over,input
translation,35,44,baselines,input,using,stack of partially analyzed items
translation,35,44,baselines,input,means of,history - based feature models
translation,35,44,baselines,remaining input sequence,means of,history - based feature models
translation,35,44,baselines,Baselines,has,MaltParser
translation,35,46,baselines,MST,represents,"global , exhaustive graphbased parsing"
translation,35,46,baselines,"global , exhaustive graphbased parsing",that finds,highest scoring directed spanning tree
translation,35,46,baselines,highest scoring directed spanning tree,in,graph
translation,35,46,baselines,Baselines,has,MST
translation,35,51,baselines,ZPar,performs,transition - based dependency parsing
translation,35,51,baselines,transition - based dependency parsing,with,stack of partial analysis
translation,35,51,baselines,transition - based dependency parsing,queue of,remaining inputs
translation,35,51,baselines,Baselines,has,ZPar
translation,35,7,results,parser combinations,showing that,semantically enhanced parsers
translation,35,7,results,semantically enhanced parsers,yield,small significant gain
translation,35,7,results,small significant gain,only on,more semantically oriented LTH treebank conversion
translation,35,7,results,Results,explore,parser combinations
translation,35,41,results,results,from,parsing
translation,35,41,results,LTH output,lower than,Penn2 Malt conversions
translation,35,41,results,parsing,has,LTH output
translation,35,41,results,Results,from,parsing
translation,35,41,results,Results,has,results
translation,35,95,results,only significant increase,over,baseline
translation,35,95,results,only significant increase,for,ZPar
translation,35,95,results,ZPar,with,SS
translation,35,95,results,MST,with,clusters
translation,35,95,results,Results,shows,only significant increase
translation,35,98,results,SF,helps,all parsers
translation,35,98,results,Results,see that,SF
translation,35,99,results,Bit clusters,has,improve significantly
translation,35,99,results,improve significantly,has,MST
translation,35,99,results,Results,has,Bit clusters
translation,35,101,results,automatically acquired clusters,are,specially effective
translation,35,101,results,specially effective,with,MST parser
translation,35,101,results,MST parser,in,both treebank conversions
translation,35,101,results,Results,conclude that,automatically acquired clusters
translation,35,116,results,combination of the baselines,has,without any semantic information
translation,35,116,results,combination of the baselines,has,considerably improves
translation,35,116,results,without any semantic information,has,considerably improves
translation,35,116,results,considerably improves,has,best baseline
translation,35,116,results,Results,shows,combination of the baselines
translation,35,117,results,semantics,not give,noticeable increase
translation,35,117,results,noticeable increase,with respect to,combining
translation,35,117,results,combining,has,baselines
translation,35,117,results,Results,Adding,semantics
translation,35,119,results,3 baselines,does not give,improvement
translation,35,119,results,improvement,over,best baseline
translation,35,119,results,ZPar,has,clearly outperforms
translation,35,119,results,clearly outperforms,has,other parsers
translation,35,119,results,Results,Combining,3 baselines
translation,35,120,results,small but significant,for,SF and clusters
translation,35,120,results,Results,adding,semantic parsers
translation,35,132,results,three baseline parsers,did not give,any improvement
translation,35,132,results,any improvement,over,best single parser ( ZPar )
translation,35,132,results,any improvement,hypothesize that,gain
translation,35,132,results,gain,coming from,parser combinations
translation,35,132,results,gain,addition of,semantic information
translation,35,132,results,parser combinations,addition of,semantic information
translation,35,133,results,improvements,coming from,Word - Net 's semantic file ( SF )
translation,35,133,results,unevenly distributed,sentences that contain,POS errors
translation,35,133,results,Results,suggests that,improvements
translation,35,144,results,WordNet semantic file information,give,small significant increment
translation,35,144,results,small significant increment,in,more fine - grained LTH representation
translation,35,144,results,parser combination,has,WordNet semantic file information
translation,35,144,results,Results,For,parser combination
translation,36,176,ablation-analysis,partial lexical ablation,by using,word embeddings
translation,36,176,ablation-analysis,word embeddings,at,leaves
translation,36,176,ablation-analysis,word embeddings,propagating,lexical information
translation,36,176,ablation-analysis,leaves,instead of,bidirectional LSTM
translation,36,177,ablation-analysis,degradation,in,F1
translation,36,177,ablation-analysis,degradation,from,full lexical ablation
translation,36,177,ablation-analysis,F1,about half of,degradation
translation,36,177,ablation-analysis,degradation,from,full lexical ablation
translation,36,177,ablation-analysis,Ablation analysis,has,degradation
translation,36,138,experimental-setup,training,lower,forest size limit
translation,36,138,experimental-setup,forest size limit,to,2000
translation,36,138,experimental-setup,forest size limit,to reduce,training times
translation,36,138,experimental-setup,2000,to reduce,training times
translation,36,138,experimental-setup,Experimental setup,During,training
translation,36,139,experimental-setup,model,trained for,30 epochs
translation,36,139,experimental-setup,30 epochs,using,"ADAM ( Kingma and Ba , 2014 )"
translation,36,139,experimental-setup,early stopping,based on,development F1
translation,36,139,experimental-setup,Experimental setup,trained for,30 epochs
translation,36,139,experimental-setup,Experimental setup,has,model
translation,36,140,experimental-setup,LSTM cells and hidden states,have,64 dimensions
translation,36,140,experimental-setup,Experimental setup,has,LSTM cells and hidden states
translation,36,141,experimental-setup,word representations,with,pre-trained 50 - dimensional embeddings
translation,36,141,experimental-setup,Experimental setup,initialize,word representations
translation,36,142,experimental-setup,Embeddings,for,categories
translation,36,142,experimental-setup,categories,have,16 dimensions
translation,36,142,experimental-setup,Experimental setup,has,Embeddings
translation,36,143,experimental-setup,dropout,probability of,0.4
translation,36,143,experimental-setup,0.4,at,word embedding layer
translation,36,143,experimental-setup,Experimental setup,apply,dropout
translation,36,145,experimental-setup,neural networks,implemented using,CNN library
translation,36,145,experimental-setup,CCG parser,implemented using,EasySRL library
translation,36,145,experimental-setup,Experimental setup,has,neural networks
translation,36,145,experimental-setup,Experimental setup,has,CCG parser
translation,36,4,model,first global recursive neural parsing model,with,optimality guarantees
translation,36,4,model,optimality guarantees,during,decoding
translation,36,4,model,Model,introduce,first global recursive neural parsing model
translation,36,5,model,global features,give up,dynamic programs
translation,36,5,model,global features,search directly in,space
translation,36,5,model,space,of,all possible subtrees
translation,36,5,model,Model,To support,global features
translation,36,7,model,existing parsing models,which have,informative bounds
translation,36,7,model,informative bounds,on,outside score
translation,36,7,model,informative bounds,with,global model
translation,36,7,model,global model,that has,loose bounds
translation,36,7,model,global model,needs to model,non-local phenomena
translation,36,7,model,Model,augment,existing parsing models
translation,36,8,model,global model,trained with,novel objective
translation,36,8,model,novel objective,encourages,parser
translation,36,8,model,parser,has,to search both efficiently and accurately
translation,36,8,model,Model,has,global model
translation,36,14,model,first global recursive neural parsing approach,with,optimality guarantees
translation,36,14,model,first global recursive neural parsing approach,use it to build,state - of - the - art CCG parser
translation,36,14,model,optimality guarantees,for,decoding
translation,36,14,model,Model,introduce,first global recursive neural parsing approach
translation,36,15,model,learning,of,global representations
translation,36,15,model,global representations,modify,parser
translation,36,15,model,parser,to search directly in,space
translation,36,15,model,space,of,all possible parse trees
translation,36,15,model,all possible parse trees,with no,dynamic programming
translation,36,15,model,Model,To enable,learning
translation,36,25,model,loss function,defined over,states
translation,36,25,model,loss function,penalizes,model
translation,36,25,model,model,whenever,top agenda item
translation,36,25,model,top agenda item,not a part of,gold parse
translation,36,25,model,states,has,of the A * search agenda
translation,36,25,model,Model,has,loss function
translation,36,121,model,new all-violations update,minimizes,sum of all observed violations
translation,36,121,model,Model,introduce,new all-violations update
translation,36,122,model,allviolations update,encourages,correct parses
translation,36,122,model,allviolations update,robust to,parses
translation,36,122,model,correct parses,to be,explored early
translation,36,122,model,parses,with,multiple deviations
translation,36,122,model,multiple deviations,from,gold parse
translation,36,122,model,Model,has,allviolations update
translation,36,153,results,global features,improve over,supertagfactored model
translation,36,153,results,supertagfactored model,by,0.6 F1
translation,36,153,results,Results,has,global features
translation,36,185,results,Our novel all-violation updates,has,outperform
translation,36,185,results,outperform,has,alternatives
translation,36,185,results,Results,has,Our novel all-violation updates
translation,37,215,ablation-analysis,beneficial effect,of,unsupported features
translation,37,215,ablation-analysis,unsupported features,for,SVM case
translation,37,216,ablation-analysis,unsupported features,convey,helpful information
translation,37,216,ablation-analysis,helpful information,can be used with,little extra cost
translation,37,216,ablation-analysis,most languages,has,unsupported features
translation,37,216,ablation-analysis,Ablation analysis,For,most languages
translation,37,19,baselines,family of losses,subsuming,CRFs
translation,37,19,baselines,family of losses,subsuming,structured SVMs
translation,37,4,model,Model,unified view of,two state - of- theart non-projective dependency parsers
translation,37,5,model,model assumptions,with,factor graph
translation,37,5,model,model assumptions,shed light on,optimization problems
translation,37,5,model,Model,representing,model assumptions
translation,37,6,model,new aggressive online algorithm,to learn,model parameters
translation,37,6,model,model parameters,makes use of,underlying variational representation
translation,37,6,model,Model,propose,new aggressive online algorithm
translation,37,11,model,inference,in,factor graph
translation,37,11,model,objective functions,over,local approximations
translation,37,11,model,local approximations,of,marginal polytope
translation,37,16,model,general method,for,inference
translation,37,16,model,inference,in,factor graphs
translation,37,16,model,factor graphs,with,hard constraints
translation,37,16,model,Model,present,general method
translation,37,18,model,aggressive online algorithm,generalizes,"MIRA ( Crammer et al. , 2006 )"
translation,37,18,model,"MIRA ( Crammer et al. , 2006 )",to,arbitrary loss functions
translation,37,18,model,Model,propose,aggressive online algorithm
translation,37,20,model,technique,for including,features
translation,37,20,model,features,not attested in,training data
translation,37,20,model,richer models,without,substantial runtime costs
translation,37,212,results,arc-factored case performance,is,similar
translation,37,212,results,second-order models,seems to be,consistent gain
translation,37,212,results,consistent gain,when,SVM loss
translation,37,212,results,Results,in,arc-factored case performance
translation,38,120,baselines,MH 3 parser,is,projective instantiation
translation,38,120,baselines,projective instantiation,of,MH k parser family
translation,38,120,baselines,Baselines,has,MH 3 parser
translation,38,148,baselines,"non-projective parsing models ( MST , MH 4 - hybrid , 1EC )",combine them into,ensemble
translation,38,7,model,MH,compatible with,minimal transition - based feature sets
translation,38,7,model,MH,introduce,transition - based interpretation
translation,38,7,model,transition - based interpretation,of,parser items
translation,38,7,model,transition - based interpretation,in which,parser items
translation,38,7,model,parser items,mapped to,sequences of transitions
translation,38,7,model,Model,To make,MH
translation,38,8,model,global decoding,for,non-projective transition - based parsing
translation,38,8,model,Model,obtain,first implementation
translation,38,17,model,'s ( 2017a ) approach,to,mildly nonprojective parsing
translation,38,27,results,MH 4 parser,remains,effective
translation,38,27,results,effective,in,parsing
translation,38,27,results,parsing,has,projective treebanks
translation,38,27,results,our baseline parser,has,fully non-projective maximum spanning tree algorithm
translation,38,27,results,our baseline parser,has,falls behind
translation,38,27,results,Results,has,MH 4 parser
translation,38,133,results,MST,considers,non-projective structures
translation,38,133,results,MST,enjoys,theoretical advantage
translation,38,133,results,theoretical advantage,over,projective MH 3
translation,38,133,results,projective MH 3,especially for,most non-projective languages
translation,38,133,results,Results,has,MST
translation,38,140,results,MH 4,recovers,more short dependencies
translation,38,140,results,more short dependencies,than,1EC
translation,38,140,results,better,at,longer -distance ones
translation,38,140,results,Results,observe,MH 4
translation,38,141,results,richer feature representation,of,MH 4 - hybrid
translation,38,141,results,MH 4 - hybrid,helps,all our languages
translation,38,141,results,MH 4 - two,has,richer feature representation
translation,38,141,results,Results,In comparison to,MH 4 - two
translation,38,142,results,react differently,to,switching
translation,38,142,results,switching,from,global to greedy models
translation,38,142,results,MH 4 and MH 3,has,react differently
translation,38,142,results,Results,has,MH 4 and MH 3
translation,38,143,results,MH 4,covers,more structures
translation,38,143,results,more structures,than,MH 3
translation,38,143,results,more capable,in,global case
translation,38,143,results,Results,has,MH 4
translation,38,146,results,best ensemble model,from,CoNLL 2017 shared task
translation,38,146,results,Results,Comparison with,CoNLL Shared Task Results
translation,38,147,results,1EC,in,7 out of the 10 languages
translation,38,147,results,MH,has,outperforms
translation,38,147,results,outperforms,has,1EC
translation,38,147,results,Results,has,MH
translation,38,149,results,average result,is,competitive
translation,38,149,results,competitive,with,best CoNLL submis-sions
translation,38,149,results,Results,has,average result
translation,38,151,results,10 most projective languages,that have,single treebank
translation,38,152,results,MH 4,remains,most effective
translation,38,152,results,MH 4,by,much smaller margin
translation,38,152,results,Results,has,MH 4
translation,39,60,baselines,decoder,is,GRU based RNN
translation,39,60,baselines,GRU based RNN,works in,sequential way
translation,39,60,baselines,GRU based RNN,generates,word
translation,39,60,baselines,word,at,each time step
translation,39,60,baselines,Baselines,has,decoder
translation,39,131,baselines,PntNet,is,end-to - end learning pointer network approach
translation,39,131,baselines,Seq2SQL ( no RL ),in which,two separate classifiers
translation,39,131,baselines,Seq2SQL,in which,reinforcement learning
translation,39,131,baselines,reinforcement learning,for,model training
translation,39,131,baselines,Baselines,has,PntNet
translation,39,115,hyperparameters,dimension,of,concatenated word embedding
translation,39,115,hyperparameters,concatenated word embedding,is,400
translation,39,115,hyperparameters,Hyperparameters,has,dimension
translation,39,116,hyperparameters,embedding values,to avoid,over-fitting
translation,39,116,hyperparameters,Hyperparameters,clamp,embedding values
translation,39,117,hyperparameters,dimension,of,encoder and decoder hidden state
translation,39,117,hyperparameters,encoder and decoder hidden state,as,200
translation,39,117,hyperparameters,Hyperparameters,set,dimension
translation,39,118,hyperparameters,training,randomize,model parameters
translation,39,118,hyperparameters,training,update,model
translation,39,118,hyperparameters,model parameters,from,uniform distribution
translation,39,118,hyperparameters,model parameters,set,learning rate
translation,39,118,hyperparameters,model parameters,with,stochastic gradient descent
translation,39,118,hyperparameters,uniform distribution,with,fan- in and fan-out
translation,39,118,hyperparameters,batch size,as,64
translation,39,118,hyperparameters,learning rate,of,SGD
translation,39,118,hyperparameters,SGD,as,0.5
translation,39,118,hyperparameters,model,with,stochastic gradient descent
translation,39,118,hyperparameters,Hyperparameters,During,training
translation,39,118,hyperparameters,Hyperparameters,update,model
translation,39,18,model,generative semantic parser,considers,structure of table
translation,39,18,model,generative semantic parser,considers,syntax of SQL language
translation,39,18,model,Model,present,generative semantic parser
translation,39,20,model,pointer networks,encodes,question
translation,39,20,model,pointer networks,synthesizes,SQL query
translation,39,20,model,question,into,continuous vectors
translation,39,20,model,SQL query,with,three channels
translation,39,20,model,Model,based on,pointer networks
translation,39,20,model,Model,synthesizes,SQL query
translation,39,119,model,Greedy search,used in,inference process
translation,39,119,model,Model,has,Greedy search
translation,39,192,model,Syntax - and Table - Aware seMantic Parser,automatically maps,natural language questions
translation,39,192,model,natural language questions,to,SQL queries
translation,39,192,model,STAMP,has,Syntax - and Table - Aware seMantic Parser
translation,39,192,model,Model,develop,STAMP
translation,39,193,model,switch,to,which channel
translation,39,193,model,which channel,at,each time step
translation,39,193,model,STAMP,has,three channels
translation,39,193,model,Model,has,STAMP
translation,39,194,model,STAMP,considers,cell information
translation,39,194,model,STAMP,considers,relation
translation,39,194,model,relation,between,cell and column name
translation,39,194,model,cell and column name,in,generation process
translation,39,194,model,Model,has,STAMP
translation,39,9,results,state- of- the - art execution accuracy,from,69.0 %
translation,39,9,results,69.0 %,to,74.4 %
translation,39,9,results,Our approach,has,significantly improves
translation,39,9,results,significantly improves,has,state- of- the - art execution accuracy
translation,39,9,results,Results,has,Our approach
translation,39,145,results,STAMP,performs,better
translation,39,145,results,better,than,existing systems
translation,39,145,results,existing systems,on,WikiSQL
translation,39,145,results,Results,see that,STAMP
translation,39,146,results,not significantly improve,has,performance
translation,39,146,results,Results,Incorporating,RL strategy
translation,39,147,results,Our simplified model,achieves,better accuracy
translation,39,147,results,better accuracy,than,Aug.
translation,39,147,results,Our simplified model,has,STAMP ( w/ o cell )
translation,39,147,results,Results,has,Our simplified model
translation,39,154,results,main improvement,of,STAMP
translation,39,154,results,STAMP,comes from,WHERE clause
translation,39,154,results,WHERE clause,is,key challenge
translation,39,154,results,key challenge,of,Wik-iSQL dataset
translation,39,154,results,Results,see that,main improvement
translation,39,156,results,accuracies,of,STAMP
translation,39,156,results,STAMP,on,SELECT column and SELECT aggregator
translation,39,156,results,SELECT column and SELECT aggregator,are,not as high
translation,39,156,results,not as high,as,SQLNet
translation,39,156,results,Results,has,accuracies
translation,39,172,results,test accuracy,of,STAMP
translation,39,172,results,STAMP,is,14.5 %
translation,39,172,results,14.5 %,has,big gap
translation,39,172,results,big gap,between,best systems
translation,39,172,results,best systems,on,WikiTableQuestions
translation,39,172,results,Results,show that,test accuracy
translation,39,174,results,STAMP model,learnt from,combination of WikiSQL and pseudo question - SQL pairs
translation,39,174,results,combination of WikiSQL and pseudo question - SQL pairs,could achieve,21.0 %
translation,39,174,results,21.0 %,on,test set
translation,39,174,results,Results,has,STAMP model
translation,40,82,results,our parser,achieves,67 % F 1 Smatch
translation,40,82,results,67 % F 1 Smatch,on,LDC2015E86 test set
translation,41,260,ablation-analysis,partial derivations,allowed us to learn,370 K more features
translation,41,260,ablation-analysis,partial derivations,allowed us to learn,22 %
translation,41,260,ablation-analysis,22 %,of,observed embeddings
translation,41,260,ablation-analysis,Ablation analysis,Using,partial derivations
translation,41,281,experimental-setup,both parsers,with,16 cores
translation,41,281,experimental-setup,both parsers,with,122GB memory
translation,41,281,experimental-setup,Experimental setup,run,both parsers
translation,41,4,model,Model,present,shift-reduce CCG semantic parser
translation,41,5,model,parser,uses,neural network architecture
translation,41,5,model,neural network architecture,balances,model capacity and computational cost
translation,41,5,model,Model,has,parser
translation,41,6,model,model,from,computationally expensive loglinear CKY parser
translation,41,6,model,Model,train by transferring,model
translation,41,16,model,shift-reduce parsing,to,semantic parsing
translation,41,16,model,Model,apply,shift-reduce parsing
translation,41,17,model,learned Combinatory Categorial Grammar,has,CCG
translation,41,17,model,Model,study,learned Combinatory Categorial Grammar
translation,41,17,model,Model,transferring,learned Combinatory Categorial Grammar
translation,41,9,results,Our parser,performs,comparably
translation,41,9,results,Our parser,doing,significantly fewer operations
translation,41,9,results,comparably,to,CKY parser
translation,41,9,results,Results,has,Our parser
translation,41,39,results,shift-reduce parser,provides,equivalent performance
translation,41,39,results,equivalent performance,to,CKY parser
translation,41,39,results,CKY parser,to generate,training data
translation,41,39,results,Results,has,shift-reduce parser
translation,41,40,results,greedy CCG semantic parsing results,at,relatively modest 9 % decrease
translation,41,40,results,relatively modest 9 % decrease,in,performance
translation,41,40,results,source CKY parser,with,beam of one
translation,41,40,results,beam of one,demonstrates,71 % decrease
translation,41,40,results,Results,use,beam search
translation,41,259,results,com-plete model,gives,best single-model performance
translation,41,259,results,com-plete model,observe,benefits
translation,41,259,results,best single-model performance,of,65.3 F1 SMATCH
translation,41,259,results,benefits,for,semantic embeddings
translation,41,259,results,benefits,learning from,partial derivations
translation,41,259,results,Results,has,com-plete model
translation,41,262,results,overall improvement,in,performance
translation,41,262,results,Results,observe,overall improvement
translation,41,263,results,benefit,of using,semantic embeddings
translation,41,263,results,multiple models,has,benefit
translation,41,263,results,semantic embeddings,has,vanishes
translation,41,263,results,Results,with,multiple models
translation,41,266,results,significant drop,in,performance
translation,41,266,results,significant drop,demonstrates,overall benefit
translation,41,266,results,Results,observe,significant drop
translation,41,267,results,development performance,of,best performing ensemble model
translation,41,267,results,best performing ensemble model,for,different beam sizes
translation,41,276,results,comparable,to,"CKY parser of ( Artzi et al. , 2015 )"
translation,41,276,results,Results,has,performance
translation,42,189,ablation-analysis,sketch encoder,is,beneficial
translation,42,189,ablation-analysis,sketch encoder,there is,8.9 point difference
translation,42,189,ablation-analysis,8.9 point difference,in,accuracy
translation,42,189,ablation-analysis,accuracy,between,COARSE2FINE and the oracle
translation,42,189,ablation-analysis,Ablation analysis,observe,sketch encoder
translation,42,239,ablation-analysis,sketch encoder,harms,perfor - mance
translation,42,239,ablation-analysis,Ablation analysis,removing,sketch encoder
translation,42,177,experimental-setup,Dimensions,of,hidden vectors and word embeddings
translation,42,177,experimental-setup,hidden vectors and word embeddings,selected from,"{ 250 , 300 } and { 150 , 200 , 250 , 300 }"
translation,42,177,experimental-setup,Experimental setup,has,Dimensions
translation,42,178,experimental-setup,dropout rate,selected from,"{ 0.3 , 0.5 }"
translation,42,178,experimental-setup,Experimental setup,has,dropout rate
translation,42,179,experimental-setup,Label smoothing,employed for,GEO
translation,42,179,experimental-setup,Label smoothing,employed for,ATIS
translation,42,179,experimental-setup,Experimental setup,has,Label smoothing
translation,42,180,experimental-setup,smoothing parameter,set to,0.1
translation,42,180,experimental-setup,Experimental setup,has,smoothing parameter
translation,42,5,model,structure - aware neural architecture,decomposes,semantic parsing process
translation,42,5,model,semantic parsing process,into,two stages
translation,42,5,model,Model,propose,structure - aware neural architecture
translation,42,6,model,input utterance,generate,rough sketch
translation,42,6,model,rough sketch,of,meaning
translation,42,6,model,rough sketch,where,low-level information
translation,42,6,model,low-level information,such as,variable names and arguments
translation,42,6,model,Model,Given,input utterance
translation,42,13,model,decoding process,into,two stages
translation,42,13,model,Model,propose to decompose,decoding process
translation,42,17,model,sketch,constrains,generation process
translation,42,17,model,sketch,encoded into,vectors
translation,42,17,model,vectors,to guide,decoding
translation,42,17,model,Model,has,sketch
translation,42,210,model,meaning sketches,abstract away from,low-level information
translation,42,210,model,meaning sketches,predict,missing details
translation,42,210,model,low-level information,such as,arguments and variable names
translation,42,210,model,missing details,to obtain,full meaning representations
translation,42,210,model,Model,generate,meaning sketches
translation,42,190,results,Results,on,WIKISQL
translation,42,191,results,Our model,superior to,ONESTAGE
translation,42,191,results,Our model,superior to,previous best performing systems
translation,42,191,results,Results,has,Our model
translation,42,192,results,COARSE2FINE 's accuracies,on,aggregation agg op and agg col
translation,42,192,results,aggregation agg op and agg col,are,90.2 % and 92.0 %
translation,42,192,results,90.2 % and 92.0 %,comparable to,SQLNET
translation,42,192,results,Results,has,COARSE2FINE 's accuracies
translation,42,193,results,most gain,obtained by,improved decoder
translation,42,193,results,improved decoder,of,WHERE clause
translation,42,193,results,Results,has,most gain
translation,42,194,results,tableaware input encoder,is,critical
translation,42,194,results,critical,for,doing well
translation,42,194,results,doing well,on,this task
translation,42,194,results,Results,find that,tableaware input encoder
translation,42,201,results,Sketches,produced by,COARSE2FINE
translation,42,201,results,COARSE2FINE,are,more accurate
translation,42,201,results,Results,has,Sketches
translation,42,203,results,better sketches,bring,accuracy gains
translation,42,203,results,accuracy gains,on,"GEO , ATIS , and DJANGO"
translation,42,203,results,Results,show that,better sketches
translation,42,204,results,sketches,predicted by,COARSE2FINE
translation,42,204,results,sketches,are,marginally better
translation,42,204,results,marginally better,compared with,ONESTAGE
translation,42,204,results,WIKISQL,has,sketches
translation,42,204,results,Results,On,WIKISQL
translation,42,240,results,our method,performs,competitively
translation,42,240,results,competitively,despite the use of,relatively simple decoders
translation,42,242,results,sketch,boosts,performance
translation,42,242,results,dicting,has,sketch
translation,42,242,results,sketch,has,correctly
translation,42,246,results,best reported result,has,in the literature
translation,43,203,experimental-setup,two bidirectional LSTMs,with,hidden layer
translation,43,203,experimental-setup,two bidirectional LSTMs,resulting in,context sensitive embedding
translation,43,203,experimental-setup,hidden layer,of size,300
translation,43,203,experimental-setup,context sensitive embedding,size,600
translation,43,203,experimental-setup,Experimental setup,stack,two bidirectional LSTMs
translation,43,205,experimental-setup,feed -forward networks,have,unique elu-activated hidden layer
translation,43,205,experimental-setup,unique elu-activated hidden layer,of size,100
translation,43,205,experimental-setup,Experimental setup,has,feed -forward networks
translation,43,206,experimental-setup,parameters,with,dropout ratio
translation,43,206,experimental-setup,dropout ratio,of,0.5
translation,43,206,experimental-setup,0.5,on,LSTM input
translation,43,206,experimental-setup,Experimental setup,regularize,parameters
translation,43,207,experimental-setup,parameters,by maximizing,likelihood
translation,43,207,experimental-setup,likelihood,of,training data
translation,43,207,experimental-setup,likelihood,through,stochastic subgradient descent
translation,43,207,experimental-setup,stochastic subgradient descent,using,"Adam ( Kingma and Ba , 2015 )"
translation,43,207,experimental-setup,Experimental setup,estimate,parameters
translation,43,208,experimental-setup,Dynet library,with,default parameters
translation,43,9,model,alternative approach,based on,variant of spinal TAGs
translation,43,9,model,alternative approach,allows,parses
translation,43,9,model,parses,has,with discontinuity
translation,43,9,model,Model,propose,alternative approach
translation,43,15,model,complexity,resort to,Lagrangian relaxation
translation,43,15,model,efficient resolution,based on,dual decomposition
translation,43,15,model,efficient resolution,combines,local search
translation,43,15,model,simple non-projective dependency parser,on,contracted graph
translation,43,15,model,local search,on,each cluster
translation,43,15,model,each cluster,to find,global consensus
translation,43,15,model,Model,to bypass,complexity
translation,43,15,model,Model,propose,efficient resolution
translation,43,119,model,new decoding method,for,GMSA
translation,43,119,model,GMSA,based on,dual decomposition
translation,43,119,model,Lagrangian relaxation,where,problem
translation,43,119,model,problem,decomposed in,several independent subproblems
translation,43,119,model,Model,propose,new decoding method
translation,43,178,model,probability distributions,with,neural networks
translation,43,178,model,neural architecture,on top of,bidirectional recurrent networks
translation,43,178,model,context sensitive representations,of,words
translation,43,178,model,Model,implement,probability distributions
translation,43,229,results,Tiger corpus,achieve,on par results
translation,43,229,results,Results,On,Tiger corpus
translation,44,103,ablation-analysis,breakdown,by,type of MWE
translation,44,103,ablation-analysis,type of MWE,addition of,MWEspecific features
translation,44,103,ablation-analysis,MWEspecific features,leads to,performance improvement
translation,44,103,ablation-analysis,performance improvement,especially for,functional MWEs
translation,44,103,ablation-analysis,Ablation analysis,has,breakdown
translation,44,76,experiments,dependency parsing,used,"Redshift ( Honnibal et al. , 2013 )"
translation,44,76,experiments,"Redshift ( Honnibal et al. , 2013 )",for,all models
translation,44,76,experiments,beam size,of,16
translation,44,76,experiments,16,for,decoding
translation,44,5,model,corpus,ensures,consistency
translation,44,5,model,consistency,between,dependency structures and MWEs
translation,44,5,model,dependency structures and MWEs,including,named entities
translation,44,5,model,Model,construct,corpus
translation,44,6,model,Model,predict,MWEspans
translation,44,93,results,joint model,with,pipeline model
translation,44,93,results,UAS / LAS,for,all sentences
translation,44,93,results,Results,Comparing,joint model
translation,44,101,results,MWE - specific features,to,joint model
translation,44,101,results,MWE - specific features,observe,at least a 2.52 / 3.00 point improvement
translation,44,101,results,at least a 2.52 / 3.00 point improvement,in terms of,UAS / LAS
translation,44,101,results,2.90,/,2.99 point
translation,44,101,results,improvement,regarding,FUM / FTM
translation,44,101,results,2.99 point,has,improvement
translation,44,101,results,Results,adding,MWE - specific features
translation,44,102,results,1.35,/,1.28 point
translation,44,102,results,improvement,with,joint ( + pred span )
translation,44,102,results,pipeline model,in terms of,FUM / FTM
translation,44,102,results,1.35,has,1.28 point
translation,44,102,results,1.28 point,has,improvement
translation,44,102,results,Results,obtain,1.35
translation,44,105,results,joint ( + pred span ),with,joint ( + dict )
translation,44,105,results,former,is,0.40
translation,44,105,results,former,is,0.82
translation,44,105,results,former,is,0.82 points better
translation,44,105,results,0.40,/,0.55 points better
translation,44,105,results,0.40,/,0.82 points better
translation,44,105,results,0.55 points better,than,latter
translation,44,105,results,latter,in terms of,UAS / LAS
translation,44,105,results,UAS / LAS,regarding,first tokens of MWEs
translation,44,105,results,0.82,/,0.82 points better
translation,44,105,results,0.82 points better,than,latter
translation,44,105,results,latter,regarding,FUM / FTM
translation,44,105,results,joint ( + pred span ),has,former
translation,44,105,results,joint ( + dict ),has,former
translation,45,22,ablation-analysis,vocabulary size,in,unsupervised parsing
translation,45,22,ablation-analysis,unsupervised parsing,has,vocabulary size
translation,45,70,baselines,PRPN and ON - LSTM,are,left-to- right neural language models
translation,45,70,baselines,left-to- right neural language models,where,syntactic distance
translation,45,70,baselines,syntactic distance,between,consecutive words
translation,45,70,baselines,syntactic distance,computed from,model output
translation,45,70,baselines,syntactic distance,used to infer,constituency parse tree
translation,45,70,baselines,"Shen et al. , 2018 b )",between,consecutive words
translation,45,70,baselines,consecutive words,computed from,model output
translation,45,70,baselines,DIORA,has,"Drozdov et al. , 2019 )"
translation,45,70,baselines,Compound PCFG,has,"Kim et al. , 2019a )"
translation,45,70,baselines,syntactic distance,has,"Shen et al. , 2018 b )"
translation,45,64,hyperparameters,word embeddings,of,"Benepar ( Kitaev and Klein , 2018 )"
translation,45,64,hyperparameters,word embeddings,with,word embeddings
translation,45,64,hyperparameters,word embeddings,from,"LSTM - based ( Hochreiter and Schmidhuber , 1997 ) language model"
translation,45,64,hyperparameters,word embeddings,from,"LSTM - based ( Hochreiter and Schmidhuber , 1997 ) language model"
translation,45,64,hyperparameters,"LSTM - based ( Hochreiter and Schmidhuber , 1997 ) language model",trained on,WSJ training set
translation,45,64,hyperparameters,Hyperparameters,initialize,word embeddings
translation,45,71,model,DIORA,learns,text -span representations and span-level scores
translation,45,71,model,DIORA,by optimizing,masked language modeling objective
translation,45,71,model,text -span representations and span-level scores,by optimizing,masked language modeling objective
translation,45,71,model,Model,has,DIORA
translation,45,72,model,compound PCFG,uses,neural parameterization
translation,45,72,model,compound PCFG,uses,per-sentence latent vector
translation,45,72,model,neural parameterization,of,PCFG
translation,45,72,model,per-sentence latent vector,introduces,context sensitivity
translation,45,72,model,Model,has,compound PCFG
translation,45,91,results,performance,of,all models tested
translation,45,91,results,Results,find that,vocabulary size
translation,45,94,results,Self - Training,Improves,all Models
translation,45,94,results,Self - Training,Improves,all models
translation,45,94,results,iterative self-training,to,unsupervised parsing models
translation,45,94,results,Results,apply,iterative self-training
translation,45,94,results,Results,has,Self - Training
translation,45,96,results,5 - step self - training,better than,1 - step self - training
translation,45,96,results,1 - step self - training,for,base models
translation,46,5,model,UDPipe,for,universal dependency parsing
translation,46,5,model,UDPipe,where,transitionbased models
translation,46,5,model,transitionbased models,trained for,different treebanks
translation,46,6,model,raw texts,as,input
translation,46,6,model,several intermediate steps,like,tokenizing and tagging
translation,46,27,results,models,on,different treebanks
translation,46,27,results,score,of,66.53 %
translation,46,27,results,66.53 %,in,macro- averaged LAS F1 - score measurement
translation,46,27,results,Results,trained and tuned,models
translation,47,17,results,our submission,achieved,highest average scores
translation,47,17,results,highest average scores,in,all of the main evaluation metrics
translation,47,17,results,highest average scores,averaged over,9 low-resource languages
translation,47,146,results,our submission,performs,particularly well
translation,47,146,results,particularly well,in,MLAS
translation,47,146,results,Results,has,our submission
translation,47,147,results,Armenian and Kazakh,managed to,win
translation,47,147,results,win,in,MLAS and BLEX
translation,47,147,results,Results,For,Armenian and Kazakh
translation,47,153,results,various specialized cross-lingual techniques,managed to surpass,stronger parsers
translation,47,153,results,stronger parsers,on,low-resource languages
translation,47,153,results,Results,applying,various specialized cross-lingual techniques
translation,48,5,model,transition - based dependency parser,with,online reordering
translation,48,5,model,99.7 %,of,necessary dependencies
translation,48,5,model,99.7 %,while maintaining,linear algorithm complexity
translation,48,5,model,Model,built,transition - based dependency parser
translation,48,6,model,pre-and post-processing,of,dependency graph
translation,48,6,model,rich feature set,with,additional semantic features
translation,48,6,model,Model,To improve,parsing quality
translation,48,73,model,Semantic Features,help to,ex-tract
translation,48,73,model,ex-tract,has,non-local dependencies
translation,48,73,model,Model,has,Semantic Features
translation,48,72,results,Bootstrapping,has,improves
translation,48,72,results,improves,has,recall
translation,48,72,results,harms,has,precision
translation,48,72,results,Results,has,Bootstrapping
translation,48,74,results,PP_BS_SEM system,with,all features included
translation,48,74,results,PP_BS_SEM system,performed,better
translation,48,74,results,better,than,all other
translation,48,74,results,all other,on,""" text "" corpus"
translation,48,74,results,comparable results,on,""" sdpv2 "" corpus"
translation,48,74,results,Results,has,PP_BS_SEM system
translation,48,77,results,Our submitted system,shown,"highest precision , recall and F1 - score values"
translation,48,77,results,Our submitted system,did not perform,well
translation,48,77,results,"highest precision , recall and F1 - score values",for,all dependencies
translation,48,77,results,well,on,nonlocal labeled dependencies
translation,48,77,results,Results,has,Our submitted system
translation,49,184,results,Berkeley parser,exceeds,Stanford and DP - TSG
translation,49,184,results,parsing accuracy,has,Berkeley parser
translation,49,184,results,Results,In terms of,parsing accuracy
translation,49,189,results,gain,is,more substantial 8.2 % F1
translation,49,189,results,French,has,gain
translation,49,189,results,Results,For,French
translation,49,192,results,DP - TSG,is,less accurate
translation,49,192,results,DP - TSG,is,more effective
translation,49,192,results,DP - TSG,is,more effective
translation,49,192,results,less accurate,as,general parsing model
translation,49,192,results,more effective,at identifying,MWEs
translation,49,192,results,Results,has,DP - TSG
translation,49,197,results,outperforms,by,36.4 % F1
translation,49,197,results,outperforms,providing,syntactic subcategory information
translation,49,197,results,mwetoolkit,by,36.4 % F1
translation,49,197,results,mwetoolkit,providing,syntactic subcategory information
translation,49,197,results,TSG - based parsing model,has,outperforms
translation,49,197,results,outperforms,has,mwetoolkit
translation,49,197,results,Results,has,TSG - based parsing model
translation,50,8,model,algorithmic framework,named,Semantic - Enriched SQL generator ( SE - SQL )
translation,50,8,model,algorithmic framework,enables,flexibly access database
translation,50,8,model,flexibly access database,than,rigid API
translation,50,8,model,flexibly access database,keeping,performance quality
translation,50,8,model,rigid API,in,application
translation,50,8,model,performance quality,for,most commonly used cases
translation,50,8,model,Model,introduce,algorithmic framework
translation,50,23,model,service-oriented Text-to - SQL parser,has,translating natural language utterance
translation,50,23,model,Model,develop,service-oriented Text-to - SQL parser
translation,50,97,results,confidence measure,in,from-clause prediction
translation,50,97,results,proposed nonparametric prediction model,achieved,85 % true positive
translation,50,97,results,proposed nonparametric prediction model,achieved,55 % true negative
translation,50,97,results,proposed nonparametric prediction model,achieved,6 % false positive
translation,50,97,results,proposed nonparametric prediction model,achieved,6 % false negative
translation,50,97,results,confidence measure,has,proposed nonparametric prediction model
translation,50,97,results,from-clause prediction,has,proposed nonparametric prediction model
translation,50,97,results,Results,Regarding,confidence measure
translation,51,228,ablation-analysis,our ablation study,makes clear that,no best noise model
translation,51,228,ablation-analysis,no best noise model,for,all datasets and tasks
translation,51,228,ablation-analysis,Ablation analysis,has,our ablation study
translation,51,156,hyperparameters,all models,with,"Adam optimizer ( Kingma and Ba , 2015 )"
translation,51,156,hyperparameters,"Adam optimizer ( Kingma and Ba , 2015 )",for,maximally 30 epochs
translation,51,156,hyperparameters,Hyperparameters,train,all models
translation,51,157,hyperparameters,supervised models,when,L sup
translation,51,157,hyperparameters,early,when,L sup
translation,51,157,hyperparameters,does not decrease,on,val
translation,51,157,hyperparameters,val,for,10 epochs
translation,51,157,hyperparameters,supervised models,has,early
translation,51,157,hyperparameters,L sup,has,does not decrease
translation,51,157,hyperparameters,Hyperparameters,stop,supervised models
translation,51,158,hyperparameters,Unsupervised models,stopped after,5 iterations
translation,51,158,hyperparameters,5 iterations,on,VG
translation,51,158,hyperparameters,Hyperparameters,has,Unsupervised models
translation,51,7,model,first approach,to,unsupervised text generation
translation,51,7,model,unsupervised text generation,from,KGs
translation,51,7,model,Model,present,first approach
translation,51,20,model,unsupervised model,requires,"unlabeled ( i.e. , non-parallel ) texts and graphs"
translation,51,20,model,easily adapts,to,new KG domains
translation,51,20,model,"unlabeled ( i.e. , non-parallel ) texts and graphs",from,target domain
translation,51,20,model,Model,propose,unsupervised model
translation,51,100,model,standard encoder-decoder model,with,attention and copy mechanism
translation,51,100,model,Model,equip,standard encoder-decoder model
translation,51,9,results,strong baselines,for,text ? graph conversion tasks
translation,51,9,results,text ? graph conversion tasks,without,manual adaptation
translation,51,9,results,manual adaptation,from,one dataset
translation,51,9,results,Our system,has,outperforms
translation,51,9,results,outperforms,has,strong baselines
translation,51,9,results,Results,has,Our system
translation,51,153,results,percentage of graph tokens,occurring in,text
translation,51,153,results,VG,than for,WebNLG
translation,51,153,results,Results,has,percentage of graph tokens
translation,51,154,results,VG graphs,contain,more details
translation,51,154,results,more details,than,corresponding texts
translation,51,154,results,characteristic feature,of,domain of image captions
translation,51,154,results,domain of image captions,mainly describe,salient image parts
translation,51,154,results,Results,has,VG graphs
translation,51,173,results,R graph ? text,performs,much better
translation,51,173,results,much better,on,WebNLG
translation,51,173,results,WebNLG,than,VG
translation,51,174,results,consistently outperform,on,all metrics
translation,51,174,results,our unsupervised models,has,consistently outperform
translation,51,174,results,Results,has,our unsupervised models
translation,51,175,results,does not always perform better,than,noise sampling
translation,51,175,results,noise composition,has,does not always perform better
translation,51,175,results,Results,has,noise composition
translation,51,177,results,supervised and unsupervised models,perform,nearly on par
translation,51,177,results,Results,has,supervised and unsupervised models
translation,51,178,results,Real supervision,not seem to give,much better guidance
translation,51,178,results,much better guidance,in,training
translation,51,178,results,much better guidance,than,our unsupervised regime
translation,51,178,results,training,than,our unsupervised regime
translation,51,178,results,Results,has,Real supervision
translation,51,191,results,U,is,more reliable
translation,51,191,results,more reliable,for,text ? graph performance
translation,51,191,results,text generation quality,has,U
translation,51,191,results,Results,shows,U
translation,51,191,results,Results,compared to,text generation quality
translation,51,192,results,sampled noise,correctly identifies,best iteration
translation,51,192,results,sampled noise,for,composed noise
translation,51,192,results,composed noise,chooses,second best
translation,51,192,results,Results,For,sampled noise
translation,51,192,results,Results,for,composed noise
translation,51,201,results,major advantage,over,other presented systems
translation,51,201,results,outperformed,by,our best unsupervised model
translation,51,203,results,fail,to predict,any fact
translation,51,203,results,any fact,from,WebNLG
translation,51,203,results,R text ? graph and SSGP,has,fail
translation,51,203,results,Results,has,R text ? graph and SSGP
translation,51,207,results,our system,uses,R text ? graph
translation,51,207,results,our system,performs,significantly better
translation,51,207,results,R text ? graph,during,unsupervised training
translation,51,207,results,Results,has,our system
translation,51,208,results,Supervision,helps more,WebNLG
translation,51,208,results,WebNLG,than on,VG
translation,51,208,results,Results,has,Supervision
translation,51,225,results,our two rule systems,provide,two important pieces of information
translation,51,225,results,R graph,helps distinguish,data format tokens
translation,51,225,results,R graph,helps find,probable candidate words
translation,51,225,results,data format tokens,from,text tokens
translation,51,225,results,R text,helps find,probable candidate words
translation,51,225,results,graph,helps find,probable candidate words
translation,51,225,results,probable candidate words,in,text
translation,51,225,results,probable candidate words,that form,facts
translation,51,225,results,text,that form,facts
translation,51,225,results,facts,for,data output
translation,51,225,results,R text,has,graph
translation,51,225,results,Results,hypothesize,our two rule systems
translation,51,227,results,our unsupervised models,always improve on,rule- based systems
translation,51,227,results,increases,from,6.2/18.3
translation,51,227,results,increases,from,text ? graph F1
translation,51,227,results,6.2/18.3,to,19.5/37.4
translation,51,227,results,6.2/18.3,to,text ? graph F1
translation,51,227,results,19.5/37.4,on,VG ball / WebNLG
translation,51,227,results,text ? graph F1,from,14.4/0.0
translation,51,227,results,14.4/0.0,to,18.5/31.0
translation,51,227,results,graph ? text BLEU,has,increases
translation,51,227,results,Results,observe,our unsupervised models
translation,52,4,model,approximation algorithm,for,PCFG parsing
translation,52,16,model,algebraic formulation,of,inside-outside algorithm
translation,52,16,model,algebraic formulation,based on,tensor formulation
translation,52,16,model,inside-outside algorithm,for,PCFGs
translation,52,16,model,tensor formulation,developed for,latent - variable PCFGs
translation,52,16,model,Model,rely on,algebraic formulation
translation,52,17,model,known techniques,for,tensor decomposition
translation,52,17,model,tensor decomposition,to approximate,source PCFG
translation,52,17,model,novel algorithm,for,approximate PCFG parsing
translation,52,17,model,Model,develop,novel algorithm
translation,52,170,results,baseline,uses,vanilla CKY algorithm
translation,52,170,results,baseline,get,speed up
translation,52,170,results,speed up,factor of,4.75
translation,52,170,results,speed up,factor of,6.5
translation,52,170,results,4.75,for,Arabic ( r = 140 )
translation,52,170,results,6.5,for,English ( r = 260 )
translation,52,170,results,Results,compared to,baseline
translation,52,171,results,performance,in,several cases
translation,52,171,results,improves,has,performance
translation,52,171,results,Results,using,tensor approximation
translation,53,119,experiments,domain independent information,when,relevant ( gold ) domain-specific training data
translation,53,119,experiments,domain independent information,improves,learning
translation,53,119,experiments,relevant ( gold ) domain-specific training data,is,available
translation,53,4,model,Model,has,Semantic parsing
translation,53,5,model,novel interpretation model,augments,domain dependent model
translation,53,5,model,domain dependent model,with,abstract information
translation,53,5,model,abstract information,shared by,multiple domains
translation,53,5,model,Model,suggest,novel interpretation model
translation,53,15,model,domain independent approach,to,semantic parsing
translation,53,15,model,Model,develop,domain independent approach
translation,53,16,model,layer of representation,applicable to,multiple domains
translation,53,16,model,Model,developing,layer of representation
translation,53,17,model,intermediate layer,capturing,shallow semantic relations
translation,53,17,model,shallow semantic relations,between,input sentence constituents
translation,53,17,model,Model,add,intermediate layer
translation,53,28,model,domain-dependent,has,mapping between text and a closed set of symbols
translation,53,28,model,Model,consists of,domain-dependent
translation,53,28,model,Model,consists of,domain independent ( abstract predicateargument structures ) information
translation,53,128,model,additional hidden layer,to,semantic interpretation process
translation,53,128,model,additional hidden layer,capturing,shallow but domain-independent semantic information
translation,53,108,results,Our system,performs,well
translation,53,108,results,well,on,matching task
translation,53,108,results,matching task,without,domain information
translation,53,108,results,matching task,any,domain information
translation,53,108,results,Results,has,Our system
translation,53,116,results,domain-independent information,is,extremely useful
translation,53,116,results,domain-independent information,make up for,missing domain information
translation,53,116,results,both tasks,has,domain-independent information
translation,53,116,results,Results,in,both tasks
translation,53,117,results,performance,for,matching task
translation,53,117,results,matching task,using,only domain independent information ( PRED - ARGS )
translation,53,117,results,matching task,was,surprisingly good
translation,53,117,results,only domain independent information ( PRED - ARGS ),was,surprisingly good
translation,53,117,results,surprisingly good,with,accuracy
translation,53,117,results,accuracy,of,0.69
translation,53,117,results,Results,has,performance
translation,53,118,results,domain-specific lexical information ( COMBINEDRI +S ),pushes,result
translation,53,118,results,result,over,0.9
translation,53,118,results,Results,Adding,domain-specific lexical information ( COMBINEDRI +S )
translation,53,122,results,domain independent information,is,helpful
translation,53,122,results,learned models,beyond,supervision
translation,53,122,results,supervision,offered by,relevant domain training data
translation,53,122,results,Results,transferring,domain independent information
translation,53,129,results,Our experiments,show that,domain-independent knowledge
translation,53,129,results,transferred,between,domains
translation,54,5,model,parser,trained using,Berkeley Grammar Trainer
translation,54,5,model,parser,trained using,multinomial logistic regression classifier
translation,54,5,model,two components,has,parser
translation,54,22,results,current system,yields,best non-local dependency parsing accuracy
translation,54,22,results,best non-local dependency parsing accuracy,for,newspaper corpus
translation,54,88,results,current system,around,3 percentage lower
translation,54,88,results,3 percentage lower,than,top system
translation,54,88,results,Results,see that,current system
translation,55,33,model,Model,identify,much more efficient solution
translation,56,84,baselines,secondorder parser ( dep2 ),uses,head-modifier and sibling dependency parts
translation,56,84,baselines,secondorder parser ( dep2 ),uses,grandparent dependency part
translation,56,84,baselines,Baselines,has,secondorder parser ( dep2 )
translation,56,73,hyperparameters,n-best size,of,16
translation,56,73,hyperparameters,best iteration time,of,four
translation,56,73,hyperparameters,four,on,development set
translation,56,73,hyperparameters,Hyperparameters,choose,n-best size
translation,56,73,hyperparameters,Hyperparameters,choose,best iteration time
translation,56,5,model,novel joint inference scheme,able to leverage,consensus information
translation,56,5,model,consensus information,between,heterogeneous treebanks
translation,56,5,model,heterogeneous treebanks,in,parsing phase
translation,56,5,model,Model,present,novel joint inference scheme
translation,56,14,model,Model,present,joint inference scheme
translation,56,86,results,dependency parsing performance,improved,consis-tently
translation,56,86,results,consis-tently,for,both treebanks ( CTB4 or CDT )
translation,56,86,results,more joint inference features,has,incrementally
translation,56,86,results,more joint inference features,has,dependency parsing performance
translation,56,86,results,Results,adding,more joint inference features
translation,56,92,results,our approach,has,significantly outperforms
translation,56,92,results,significantly outperforms,has,many systems
translation,56,104,results,CDT,find that,"2,000 sentences"
translation,56,104,results,CDT,out of,"2,000 sentences"
translation,56,104,results,"2,000 sentences",on,development set
translation,56,104,results,341 sentences,benefit from,joint inference
translation,56,104,results,"2,000 sentences",has,341 sentences
translation,56,104,results,Results,For,CDT
translation,56,105,results,dependency parsing results,is,improved
translation,56,105,results,joint inference,worsens,dependency parsing result
translation,56,105,results,dependency parsing result,for,some sentences
translation,57,107,ablation-analysis,Parameter sharing,shown to be,helpful
translation,57,107,ablation-analysis,Ablation analysis,has,Parameter sharing
translation,57,108,ablation-analysis,average performance increase,on,ATIS
translation,57,108,ablation-analysis,ATIS,mainly comes from,Chinese and Indonesian
translation,57,108,ablation-analysis,Ablation analysis,observe,average performance increase
translation,57,5,experiments,one multilingual model,capable of,parsing
translation,57,5,experiments,parsing,into,corresponding formal semantic representations
translation,57,5,experiments,natural language sentences,from,multiple different languages
translation,57,5,experiments,natural language sentences,into,corresponding formal semantic representations
translation,57,5,experiments,parsing,has,natural language sentences
translation,57,10,experiments,single-source setting,where,input
translation,57,10,experiments,single-source setting,where,input
translation,57,10,experiments,input,consists of,single sentence
translation,57,10,experiments,input,consists of,parallel sentences in multiple languages
translation,57,10,experiments,input,consists of,parallel sentences in multiple languages
translation,57,10,experiments,multi-source setting,where,input
translation,57,10,experiments,input,consists of,parallel sentences in multiple languages
translation,57,6,model,existing sequence - to - tree model,to,multi-task learning framework
translation,57,6,model,multi-task learning framework,shares,decoder
translation,57,6,model,decoder,for generating,semantic representations
translation,57,6,model,Model,extend,existing sequence - to - tree model
translation,57,14,model,sentences,in,several languages
translation,57,14,model,Model,propose,parsing architecture
translation,57,15,model,existing sequence - totree model,to,multitask learning framework
translation,57,15,model,Model,extend,existing sequence - totree model
translation,57,16,model,multiple encoders,one for,each language
translation,57,16,model,one decoder,shared across,source languages
translation,57,16,model,source languages,for generating,semantic representations
translation,57,16,model,Model,consists of,multiple encoders
translation,57,16,model,Model,consists of,one decoder
translation,57,19,model,attention mechanism,to integrate,multisource information
translation,57,19,model,Model,modify,attention mechanism
translation,57,105,results,performance,of,monolingual sequence - to- tree model
translation,57,105,results,performance,of,multilingual model
translation,57,105,results,multilingual model,with,separate and shared output parameters
translation,57,105,results,MULTI,with,separate and shared output parameters
translation,57,105,results,separate and shared output parameters,under,single-source setting
translation,57,105,results,Results,compares,performance
translation,57,106,results,monolingual model,by,up to 1.34 % average accuracy
translation,57,106,results,up to 1.34 % average accuracy,on,GEO
translation,57,106,results,multilingual model,has,outperform
translation,57,106,results,outperform,has,monolingual model
translation,57,112,results,system combination,at,model level
translation,57,112,results,system combination,able to give,better performance
translation,57,112,results,better performance,on average,up to 4.29 %
translation,57,112,results,up to 4.29 %,on,GEO
translation,57,112,results,Results,observe,system combination
translation,57,113,results,word level and sentence level,shows,comparable performance
translation,57,113,results,comparable performance,on,both datasets
translation,57,113,results,Results,Combining at,word level and sentence level
translation,57,114,results,benefit,is,more apparent
translation,57,114,results,benefit,include,English
translation,57,114,results,English,in,system combination
translation,57,114,results,Results,seen that,benefit
translation,57,114,results,Results,include,English
translation,59,181,ablation-analysis,"gold ( BGI + i+ b , TurboParser )",=,"( 4.98 % , 5.5 % )"
translation,59,181,ablation-analysis,Ablation analysis,has,"gold ( BGI + i+ b , TurboParser )"
translation,59,128,experimental-setup,our algorithms,within,"Tur-boParser ( Martins et al. , 2013 )"
translation,59,128,experimental-setup,Experimental setup,implemented,our algorithms
translation,59,141,experimental-setup,Run times,computed on,Intel ( R ) Xeon ( R ) CPU E5-2697 v3@2.60 GHz machine
translation,59,141,experimental-setup,Intel ( R ) Xeon ( R ) CPU E5-2697 v3@2.60 GHz machine,with,20 GB RAM memory
translation,59,141,experimental-setup,Experimental setup,has,Run times
translation,59,7,experiments,algorithm,within,second-order TurboParser
translation,59,7,experiments,datasets,CoNLL 2006 and 2007 shared task on,multilingual dependency parsing
translation,59,18,model,greedy search algorithm,for,"highorder , non-projective graph - based dependency parsing"
translation,59,18,model,Model,propose,greedy search algorithm
translation,59,20,model,graph- based objective,into,sum of terms
translation,59,20,model,graph- based objective,into,sum
translation,59,20,model,graph- based objective,show that,our basic greedy algorithm
translation,59,20,model,sum of terms,instead of,globally optimizing
translation,59,20,model,our basic greedy algorithm,relaxes,global objective
translation,59,20,model,global objective,by sequentially optimizing,terms
translation,59,20,model,terms,instead of,globally optimizing
translation,59,20,model,globally optimizing,has,sum
translation,59,20,model,Model,factorize,graph- based objective
translation,59,21,model,training method,specializes in,local decisions
translation,59,22,model,global parameter training,based on,comparison
translation,59,22,model,comparison,between,induced tree and the gold tree
translation,59,22,model,Model,supports,global parameter training
translation,59,29,model,ensemble method,integrates,information
translation,59,29,model,information,from,output tree
translation,59,29,model,output tree,of,original TurboParser
translation,59,29,model,arc weights,learned by,our variant of the parser
translation,59,29,model,our variant of the parser,into,our search algorithm
translation,59,29,model,our search algorithm,to generate,new tree
translation,59,29,model,Model,implement,ensemble method
translation,59,8,results,Our algorithm,improves,run time
translation,59,8,results,run time,of,parser
translation,59,8,results,1 %,in,UAS
translation,59,8,results,1 %,across,languages
translation,59,8,results,UAS,across,languages
translation,59,8,results,Results,has,Our algorithm
translation,59,9,results,ensemble method,exploiting,joint power
translation,59,9,results,ensemble method,achieves,average UAS 0.27 %
translation,59,9,results,joint power,of,parsers
translation,59,9,results,higher,than,TurboParser
translation,59,9,results,average UAS 0.27 %,has,higher
translation,59,28,results,parser,achieves,UAS scores
translation,59,28,results,UAS scores,of,87.78 % and 89.25 %
translation,59,28,results,87.78 % and 89.25 %,for,first and second order parsing
translation,59,28,results,87.98 % and 90.26 %,achieved by,original TurboParser
translation,59,28,results,languages,has,parser
translation,59,28,results,Results,across,languages
translation,59,153,results,UAS,for,second order parsing
translation,59,153,results,UAS,is,very low
translation,59,153,results,second order parsing,with,basic greedy inference
translation,59,153,results,second order parsing,is,very low
translation,59,153,results,Results,has,UAS
translation,59,166,results,consistent improvements,of,ensemble models
translation,59,166,results,consistent improvements,of,ensemble models
translation,59,166,results,ensemble models,over,TurboParser
translation,59,166,results,ensemble models,achieve,UAS
translation,59,166,results,TurboParser,for,second order parsing
translation,59,166,results,ensemble models,achieve,UAS
translation,59,166,results,UAS,of,90.5- 90.53 %
translation,59,166,results,UAS,compared to,90.26 %
translation,59,166,results,90.5- 90.53 %,compared to,90.26 %
translation,59,166,results,90.26 %,of,TurboParser
translation,59,166,results,second order parsing,has,ensemble models
translation,59,168,results,undirected UAS measure,gain does not come from,arc directionality improvements
translation,59,169,results,ensemble methods,share,almost all of their arcs
translation,59,169,results,almost all of their arcs,with,TurboParser
translation,59,169,results,Results,has,ensemble methods
translation,59,178,results,ensemble modeling,improves,UAS
translation,59,178,results,ensemble modeling,improves,UAS
translation,59,178,results,ensemble modeling,leading to,gain
translation,59,178,results,UAS,over,TurboParser
translation,59,178,results,UAS,for,first order parsing
translation,59,178,results,UAS,leading to,gain
translation,59,178,results,TurboParser,for,first order parsing
translation,59,178,results,gain,of,0.3 %
translation,59,178,results,0.3 %,in,UAS
translation,59,178,results,0.3 %,for,BGI + i+ b ensemble
translation,59,178,results,UAS,for,BGI + i+ b ensemble
translation,59,178,results,BGI + i+ b ensemble,has,79.29 % vs. 78.99 %
translation,59,178,results,Results,has,ensemble modeling
translation,60,85,ablation-analysis,maximum rule height,has,increases F1
translation,60,85,ablation-analysis,Ablation analysis,increasing,maximum rule height
translation,60,61,results,outperforms,by,1.53 Bleu
translation,60,61,results,forestto-string system,has,outperforms
translation,60,61,results,outperforms,has,tree-to-string system
translation,60,62,results,trees,selected by,forest- to -string system
translation,60,62,results,trees,score,much lower
translation,60,62,results,much lower,according to,labeled - bracket F1
translation,60,62,results,Results,find that,trees
translation,60,82,results,system,trained to optimize,labeled - bracket F1 ( max - F1 )
translation,60,82,results,labeled - bracket F1 ( max - F1 ),obtains,much lower Bleu score
translation,60,82,results,much lower Bleu score,than,to maximize Bleu ( max - Bleu )
translation,60,82,results,much lower Bleu score,than,unsurprisingly
translation,60,82,results,to maximize Bleu ( max - Bleu ),has,unsurprisingly
translation,60,82,results,Results,has,system
translation,60,83,results,max -F1 system,obtains,higher F1 score
translation,60,83,results,higher F1 score,compared with,max - Bleu system
translation,60,89,results,increase,with,language model size
translation,60,89,results,increase,with,largest language model
translation,60,89,results,increase,with,largest language model
translation,60,89,results,language model size,with,largest language model
translation,60,89,results,largest language model,yielding,net improvement
translation,60,89,results,net improvement,of,0.82
translation,60,89,results,0.82,over,baseline parser
translation,60,89,results,Results,see that,parsing performance
translation,60,95,results,parsing performance,correlates with,parallel data size
translation,61,133,baselines,H&J,is,L2R parsing based joint model
translation,61,133,baselines,Baselines,has,H&J
translation,61,161,baselines,CRF - based labeling model,with,lexical and POStag features
translation,61,161,baselines,lexical and POStag features,as,baselines
translation,61,37,experiments,our method,on,Chinese annotated data
translation,61,173,hyperparameters,beam size,for,POS - tagger and parsing
translation,61,173,hyperparameters,beam size,set to,5
translation,61,173,hyperparameters,POS - tagger and parsing,set to,5
translation,61,173,hyperparameters,Hyperparameters,has,beam size
translation,61,6,model,disfluency detection approach,based on,right - to - left transitionbased parsing
translation,61,6,model,disfluency detection approach,efficiently identify,disfluencies
translation,61,6,model,disfluency detection approach,keep,ASR outputs
translation,61,6,model,ASR outputs,has,grammatical
translation,61,6,model,Model,efficient,disfluency detection approach
translation,61,7,model,global view,to capture,long-range dependencies
translation,61,7,model,long-range dependencies,for,disfluency detection
translation,61,7,model,syntactic and disfluency features,with,linear complexity
translation,61,26,model,detecting disfluencies,using,right - to- left transition - based dependency parsing ( R2L parsing )
translation,61,26,model,right - to- left transition - based dependency parsing ( R2L parsing ),where,words
translation,61,26,model,consumed,from,right to left
translation,61,26,model,consumed,to build,parsing tree
translation,61,26,model,right to left,to build,parsing tree
translation,61,26,model,parsing tree,based on which,current word
translation,61,26,model,current word,predicted to be,disfluent or normal
translation,61,26,model,Model,propose,detecting disfluencies
translation,61,36,results,our method,achieve,85.1 % f-score
translation,61,36,results,85.1 % f-score,with,gain
translation,61,36,results,85.1 % f-score,with,gain
translation,61,36,results,gain,of,0.7 point
translation,61,36,results,gain,of,1 point
translation,61,36,results,gain,of,1 point
translation,61,36,results,0.7 point,over,state - of- the - art M 3 N labeling model
translation,61,36,results,gain,of,1 point
translation,61,36,results,1 point,over,state - of - the - art joint model
translation,61,36,results,Results,show,our method
translation,61,139,results,our BCT model,with,new disfluency features
translation,61,139,results,our BCT model,achieves,best performance
translation,61,139,results,best performance,on,disfluency detection
translation,61,139,results,best performance,on,dependency parsing
translation,61,140,results,performance,of,CRF model
translation,61,140,results,CRF model,is,low
translation,61,140,results,Results,has,performance
translation,61,142,results,BCT model,outperforms,M 3 N ? model
translation,61,142,results,accuracy,of,84.4 %
translation,61,148,results,our models,with,only basic features
translation,61,148,results,our models,score about 3 points below,models
translation,61,148,results,our models,adding,new features
translation,61,148,results,only basic features,score about 3 points below,models
translation,61,148,results,models,adding,new features
translation,61,149,results,parsing accuracy,has,BCT model
translation,61,149,results,BCT model,has,outperforms
translation,61,149,results,outperforms,has,all the other models
translation,61,157,results,all other models,except that,performance
translation,61,157,results,performance,on,determiner
translation,61,157,results,determiner,lower than,M 3 N
translation,61,157,results,M 3 N,shows,our algorithm
translation,61,157,results,our BCT model,has,outperforms
translation,61,157,results,outperforms,has,all other models
translation,61,157,results,our algorithm,has,significantly tackle
translation,61,157,results,significantly tackle,has,common disfluencies
translation,61,157,results,Results,has,our BCT model
translation,61,162,results,CRF model,with,bag of words and POS - tag features
translation,61,162,results,CRF model,by,more than 15 points
translation,61,162,results,more than 15 points,on,f-score
translation,61,162,results,Our models,has,outperform
translation,61,162,results,outperform,has,CRF model
translation,61,162,results,Results,has,Our models
translation,61,163,results,standard transition - based parsing,is,not robust
translation,61,163,results,not robust,in parsing,disfluent text
translation,61,174,results,parsing accuracy,on,SWBD
translation,61,174,results,SWBD,lower than,WSJ
translation,61,174,results,Results,has,parsing accuracy
translation,61,175,results,performances,of,R2L and L2R parsing
translation,61,175,results,comparable,on,SWBD and WSJ test sets
translation,61,175,results,Results,has,performances
translation,62,156,baselines,first baseline ( IR ),simulates,information retrieval
translation,62,156,baselines,first baseline ( IR ),selects,answer y
translation,62,156,baselines,answer y,among,entities
translation,62,156,baselines,answer y,using,log-linear model
translation,62,156,baselines,entities,in,table
translation,62,156,baselines,log-linear model,over,entities ( table cells )
translation,62,156,baselines,log-linear model,rather than,logical forms
translation,62,156,baselines,entities ( table cells ),rather than,logical forms
translation,62,156,baselines,Baselines,has,first baseline ( IR )
translation,62,62,hyperparameters,L 1 regularization,with,? = 3 ? 10 ?5
translation,62,62,hyperparameters,? = 3 ? 10 ?5,obtained from,cross-validation
translation,62,5,model,question - answer pairs,as,supervision
translation,62,162,results,Our system,gets,accuracy
translation,62,162,results,accuracy,of,37.1 %
translation,62,162,results,37.1 %,on,test data
translation,62,162,results,significantly higher,than,both baselines
translation,62,162,results,oracle,is,76.6 %
translation,62,162,results,Results,has,Our system
translation,63,6,experimental-setup,character - based bidirectional long shortterm memory ( LSTM ) networks,for,tokenization
translation,63,6,experimental-setup,character - based bidirectional long shortterm memory ( LSTM ) networks,for,POS tagging
translation,63,6,experimental-setup,Experimental setup,use,character - based bidirectional long shortterm memory ( LSTM ) networks
translation,63,114,experimental-setup,number of clusters,set to,256
translation,63,114,experimental-setup,Experimental setup,has,number of clusters
translation,63,117,experimental-setup,fast align toolkit,used for,word alignment
translation,63,117,experimental-setup,Experimental setup,has,fast align toolkit
translation,63,119,experimental-setup,Dynet toolkit,for,implementation
translation,63,119,experimental-setup,implementation,of,all our neural models
translation,63,119,experimental-setup,Experimental setup,use,Dynet toolkit
translation,63,45,experiments,UDPipe POS tagger,trained using,averaged perceptron
translation,63,45,experiments,averaged perceptron,with,feature engineering
translation,63,115,experiments,cross-lingual transfer parsing,use,parallel data
translation,63,115,experiments,parallel data,from,OPUS
translation,63,115,experiments,parallel data,to derive,cross-lingual word embeddings
translation,63,115,experiments,cross-lingual transfer parsing,has,of lowresource languages
translation,63,142,experiments,outperform,has,rank - 1 system
translation,63,142,experiments,rank - 1 system,has,significantly
translation,63,143,experiments,performance,of,word segmentation
translation,63,143,experiments,crucial,for,pipeline system
translation,63,146,experiments,transfer parsing approaches,for,cross-domain and crosslingual adaption
translation,63,146,experiments,resources,from,multiple treebanks
translation,63,7,model,list- based transitionbased algorithm,for,general non-projective parsing
translation,63,7,model,list- based transitionbased algorithm,present,improved Stack - LSTM - based architecture
translation,63,7,model,improved Stack - LSTM - based architecture,for representing,each transition state
translation,63,7,model,improved Stack - LSTM - based architecture,making,predictions
translation,63,7,model,Model,employ,list- based transitionbased algorithm
translation,63,7,model,Model,present,improved Stack - LSTM - based architecture
translation,63,8,model,low / zero-resource languages and cross-domain data,use,model transfer approach
translation,63,8,model,model transfer approach,effective use of,existing resources
translation,63,8,model,Model,to parse,low / zero-resource languages and cross-domain data
translation,63,48,model,Model,modeling from,characters
translation,63,71,model,Incremental Tree -LSTM,obtains,sub-tree representations
translation,63,71,model,sub-tree representations,has,incrementally
translation,63,71,model,Model,propose,Incremental Tree -LSTM
translation,63,72,model,dependency arc,collect,representations
translation,63,72,model,representations,of,all the found modifiers
translation,63,72,model,representations,of,sub-tree
translation,63,72,model,all the found modifiers,of,head
translation,63,72,model,all the found modifiers,of,head
translation,63,72,model,embedding,of,head
translation,63,72,model,embedding,as,representation
translation,63,72,model,representation,of,sub-tree
translation,63,72,model,Model,collect,representations
translation,63,9,results,substantial gains,against,UDPipe baseline
translation,63,9,results,average improvement,of,3.76 %
translation,63,9,results,3.76 %,in,LAS
translation,63,9,results,LAS,of,all languages
translation,63,9,results,Results,demonstrate,substantial gains
translation,63,133,results,cross-domain and cross-lingual transfer parsing,improve over,supervised systems
translation,63,133,results,supervised systems,has,significantly
translation,63,133,results,Results,see that,cross-domain and cross-lingual transfer parsing
translation,63,133,results,Results,both,cross-domain and cross-lingual transfer parsing
translation,63,135,results,end-to - end universal parsing system,with comparison to,UDPipe baseline models
translation,63,135,results,Results,of,end-to - end universal parsing system
translation,63,136,results,substantial gains,over,UDPipe
translation,63,136,results,UDPipe,on,76 out of 81 treebanks
translation,63,136,results,UDPipe,with,3.76 % improvements
translation,63,136,results,3.76 % improvements,in,average LAS
translation,63,136,results,Results,obtain,substantial gains
translation,63,147,results,significant improvements,against,UDPipe baseline systems
translation,63,147,results,UDPipe baseline systems,on,most of the test sets
translation,63,147,results,Results,obtain,significant improvements
translation,64,57,experimental-setup,embedding tables,from,"pretrained RoBERTa - base ( Liu et al. , 2019 )"
translation,64,57,experimental-setup,embedding tables,from,"BERT - base ( Devlin et al. , 2018 )"
translation,64,57,experimental-setup,embedding tables,from,DistilBERT - base
translation,64,57,experimental-setup,embedding tables,from,ALBERT - large - v2
translation,64,57,experimental-setup,embedding tables,from,"ALBERT - base - v2 ( Lan et al. , 2019 )"
translation,64,57,experimental-setup,embedding tables,from,huggingface transformers library
translation,64,57,experimental-setup,embedding tables,from,huggingface transformers library
translation,64,57,experimental-setup,"ALBERT - base - v2 ( Lan et al. , 2019 )",from,huggingface transformers library
translation,64,57,experimental-setup,Experimental setup,extract,embedding tables
translation,64,69,experimental-setup,transformer model training,base,our implementation
translation,64,69,experimental-setup,our implementation,on,huggingface transformers library v2.6.0
translation,64,69,experimental-setup,Experimental setup,For,transformer model training
translation,64,70,experimental-setup,AdamW optimizer,with,10 % warmup steps
translation,64,70,experimental-setup,AdamW optimizer,with,linear learning rate decay
translation,64,70,experimental-setup,linear learning rate decay,to,0
translation,64,70,experimental-setup,Experimental setup,use,AdamW optimizer
translation,64,72,experimental-setup,code embeddings,with,32 codebooks
translation,64,72,experimental-setup,Experimental setup,learn,code embeddings
translation,64,83,experimental-setup,code embeddings,for,"{ 500 , 700 , 900 , 1100 , 1300 } epochs"
translation,64,83,experimental-setup,Experimental setup,learn,code embeddings
translation,64,84,experimental-setup,transformer models,with,original and code embeddings
translation,64,84,experimental-setup,transformer models,with,sequence length
translation,64,84,experimental-setup,original and code embeddings,for,40 epochs
translation,64,84,experimental-setup,original and code embeddings,with,batch size 16
translation,64,84,experimental-setup,original and code embeddings,with,sequence length
translation,64,84,experimental-setup,sequence length,has,128
translation,64,84,experimental-setup,Experimental setup,train,transformer models
translation,64,86,experimental-setup,peak learning rate,has,"{ 2e - 5 , 3e - 5 , ... , 6e - 5 }"
translation,64,86,experimental-setup,peak learning rate,has,weight decay
translation,64,86,experimental-setup,weight decay,has,"{ 0.01 , 0.05 , 0.1 }"
translation,64,86,experimental-setup,Experimental setup,experiment with,peak learning rate
translation,64,86,experimental-setup,Experimental setup,experiment with,weight decay
translation,64,96,experimental-setup,different pretrained BERT - variants,as,encoder
translation,64,96,experimental-setup,different pretrained BERT - variants,as,transformer decoder layers
translation,64,96,experimental-setup,transformer decoder layers,with,d model = 768
translation,64,96,experimental-setup,pointer generator network,uses,scaled dot-product attention
translation,64,96,experimental-setup,scaled dot-product attention,to score,tokens
translation,64,96,experimental-setup,Experimental setup,use,different pretrained BERT - variants
translation,64,96,experimental-setup,Experimental setup,use,transformer decoder layers
translation,64,97,experimental-setup,crossentropy loss,with,label smoothing
translation,64,97,experimental-setup,label smoothing,of,0.1
translation,64,97,experimental-setup,Experimental setup,trained using,crossentropy loss
translation,64,98,experimental-setup,code embeddings,for,900 epochs
translation,64,98,experimental-setup,900 epochs,has,offline
translation,64,98,experimental-setup,Experimental setup,train,code embeddings
translation,64,99,experimental-setup,0.01,used for,transformer training
translation,64,99,experimental-setup,Learning rate,has,2e - 5
translation,64,99,experimental-setup,weight decay,has,0.01
translation,64,99,experimental-setup,Experimental setup,has,Learning rate
translation,64,99,experimental-setup,Experimental setup,has,weight decay
translation,64,101,experimental-setup,inference,employ,beam decoding
translation,64,101,experimental-setup,beam decoding,with,width 5
translation,64,101,experimental-setup,Experimental setup,During,inference
translation,64,115,experimental-setup,embeddings,from,pretrained ALBERT - base - v2
translation,64,115,experimental-setup,code embeddings,with,M
translation,64,115,experimental-setup,code embeddings,with,K
translation,64,115,experimental-setup,M,in,"{ 8 , 16 , 32 , 64 }"
translation,64,115,experimental-setup,M,in,"{ 16 , 32 , 64 }"
translation,64,115,experimental-setup,K,in,"{ 16 , 32 , 64 }"
translation,64,115,experimental-setup,Experimental setup,use,embeddings
translation,64,32,experiments,outperforms,on,GLUE
translation,64,32,experiments,BERT - large,on,GLUE
translation,64,32,experiments,ALBERT - xxlarge,has,outperforms
translation,64,32,experiments,outperforms,has,BERT - large
translation,64,32,experiments,GLUE,has,"Wang et al. , 2018 )"
translation,64,32,experiments,RACE,has,"et al. , 2017 )"
translation,64,71,experiments,Forr code embedding learning,base,our implementation
translation,64,71,experiments,our implementation,on,Shu and Nakayama ( 2017 )
translation,64,85,experiments,Uncased BERT and DistilBERT,perform,better
translation,64,85,experiments,better,than,cased versions
translation,64,110,experiments,code embeddings,trained for,different numbers of epochs
translation,64,110,experiments,code embeddings,finetune,semantic parsing
translation,64,110,experiments,different numbers of epochs,to,ALBERT - base - v2
translation,64,111,experiments,SNIPS and ATIS,find,best validation setting
translation,64,111,experiments,best validation setting,among,"learning rate { 2,3,4,5,6}e - 5"
translation,64,111,experiments,best validation setting,among,weight decay
translation,64,111,experiments,weight decay,has,"{ 0.01 , 0.05 , 0.01 }"
translation,64,20,model,task - oriented semantic parsing models,produce,near state - of - the - art performances
translation,64,20,model,near state - of - the - art performances,by compressing,existing large models
translation,64,20,model,space - efficient,has,task - oriented semantic parsing models
translation,64,20,model,Model,build,space - efficient
translation,64,21,model,compositional code embeddings,to significantly compress,BERT - base and RoBERTa - base encoders
translation,64,21,model,BERT - base and RoBERTa - base encoders,with,little performance loss
translation,64,21,model,Model,learn,compositional code embeddings
translation,64,58,model,embedding tables,in,transformer models
translation,64,58,model,embedding tables,with,compositional code approximations
translation,64,58,model,embedding tables,evaluate,compressed language models
translation,64,58,model,compressed language models,by finetuning,downstream tasks
translation,64,58,model,Model,replace,embedding tables
translation,64,24,results,20.47 % ? 34.22 % encoder compression rates,with,> 97.5 % semantic parsing performance preservation
translation,64,87,results,different transformer encoders,to establish,strong baselines
translation,64,87,results,strong baselines,which achieve,EM values
translation,64,87,results,EM values,that are,within 1.5 %
translation,64,87,results,within 1.5 %,of,state - of - the- art
translation,64,87,results,Results,use,different transformer encoders
translation,64,88,results,models,based on,compressed ALBERT - large - v2 encoder ( 54MB )
translation,64,88,results,compressed ALBERT - large - v2 encoder ( 54MB ),perserves,> 99.6 % EM
translation,64,88,results,> 99.6 % EM,of,previous state- ofthe - art model
translation,64,88,results,previous state- ofthe - art model,which uses,BERT encoder ( 420 MB )
translation,64,89,results,compressed encoders,perserve,> 97.5 % EM
translation,64,89,results,> 97.5 % EM,of,uncompressed counterparts
translation,64,89,results,uncompressed counterparts,under,same training settings
translation,64,102,results,Our greatly compressed models,present,98?99 % performances
translation,64,102,results,98?99 % performances,of,original models
translation,64,102,results,Results,has,Our greatly compressed models
translation,64,116,results,MSE loss,for,embeddings
translation,64,116,results,embeddings,with,larger M and K
translation,64,116,results,700 epochs,has,MSE loss
translation,64,116,results,Results,after,700 epochs
translation,64,119,results,larger M,yields,better performances
translation,64,119,results,Results,has,larger M
translation,64,123,results,compositional code embeddings,to compress,model embeddings
translation,64,123,results,compositional code embeddings,observe,97.5 % performance preservation
translation,64,123,results,model embeddings,by,95.15 % ? 98.46 %
translation,64,123,results,pretrained encoders,by,20.47 % ? 34.22 %
translation,64,123,results,97.5 % performance preservation,on,"SNIPS , ATIS , and Facebook TOP"
translation,64,123,results,Results,learn,compositional code embeddings
translation,64,124,results,compressed ALBERT - large,is,54MB
translation,64,124,results,compressed ALBERT - large,achieve,99.6 % performances
translation,64,124,results,99.6 % performances,of,previous state - of - the - art models
translation,64,124,results,previous state - of - the - art models,on,SNIPS and ATIS
translation,64,124,results,Results,has,compressed ALBERT - large
translation,65,5,model,bidirectional LSTM ( BiLSTM ) based language model,pre-trained to predict,words
translation,65,5,model,words,in,plain text
translation,65,5,model,multi-layer perceptron ( MLP ) decision model,uses,features
translation,65,5,model,features,from,language model
translation,65,5,model,features,to predict,correct actions
translation,65,5,model,correct actions,for,ArcHybrid transition based parser
translation,65,5,model,Model,consists of,bidirectional LSTM ( BiLSTM ) based language model
translation,65,11,model,features,for,parser
translation,65,11,model,features,from,bidirectional LSTM language model
translation,65,11,model,bidirectional LSTM language model,trained with,pre-tokenized text
translation,65,11,model,pre-tokenized text,to predict,words
translation,65,11,model,words,in,sentence
translation,65,11,model,words,using,left and the right context
translation,65,11,model,Model,derive,features
translation,65,12,model,word embeddings and context embeddings,from,language model
translation,65,12,model,Model,derive,word embeddings and context embeddings
translation,65,4,results,dense vectors,derived from,language model
translation,65,4,results,dense vectors,represent,left / right context
translation,65,4,results,dense vectors,demonstrate,context embeddings
translation,65,4,results,left / right context,of,word instance
translation,65,4,results,context embeddings,has,dense vectors
translation,65,4,results,Results,introduce,context embeddings
translation,66,218,ablation-analysis,drops,by,about four points
translation,66,218,ablation-analysis,tokenization,has,UD parsing performance
translation,66,218,ablation-analysis,gold tokenization,has,UD parsing performance
translation,66,218,ablation-analysis,UD parsing performance,has,drops
translation,66,218,ablation-analysis,Ablation analysis,importance of,tokenization
translation,66,218,ablation-analysis,Ablation analysis,without,gold tokenization
translation,66,20,experimental-setup,annotation,includes,tokenization
translation,66,20,experimental-setup,annotation,includes,part-of-speech ( POS ) tags
translation,66,20,experimental-setup,annotation,includes,( labeled ) Universal Dependencies
translation,66,20,experimental-setup,Experimental setup,has,annotation
translation,66,182,experimental-setup,experiments,run on,Xeon E5-2670 2.6 GHz machine
translation,66,182,experimental-setup,Experimental setup,has,experiments
translation,66,9,model,annotation noise,propose,new method
translation,66,9,model,new method,to distill,ensemble of 20 transition - based parsers
translation,66,9,model,ensemble of 20 transition - based parsers,into,single one
translation,66,9,model,Model,To overcome,annotation noise
translation,66,9,model,Model,propose,new method
translation,66,19,model,new English tweet treebank,of,"55,607 tokens"
translation,66,19,model,new English tweet treebank,follows,UD guidelines
translation,66,19,model,new English tweet treebank,contends with,social media-specific challenges
translation,66,19,model,"55,607 tokens",follows,UD guidelines
translation,66,19,model,Model,introduce,new English tweet treebank
translation,66,24,model,20 parser ensemble,into,single greedy parser
translation,66,24,model,Model,distill,20 parser ensemble
translation,66,32,model,new distillation method,for training,greedy parser
translation,66,32,model,new distillation method,leading to,better performance
translation,66,32,model,new distillation method,leading to,without efficiency sacrifices
translation,66,32,model,better performance,than,existing methods
translation,66,32,model,Model,propose,new distillation method
translation,66,153,model,new character - level bidirectional LSTM ( bi- LSTM ) sequence - labeling model,for,tokenization
translation,66,153,model,Model,introduce,new character - level bidirectional LSTM ( bi- LSTM ) sequence - labeling model
translation,66,203,model,ensemble,into,single transition - based parser
translation,66,25,results,learning,directly from,exploration
translation,66,25,results,learning,is,more beneficial
translation,66,25,results,exploration,of,ensemble parser
translation,66,25,results,exploration,is,more beneficial
translation,66,25,results,ensemble parser,is,more beneficial
translation,66,25,results,more beneficial,learning from,"gold standard "" oracle "" transition sequence"
translation,66,25,results,Results,show,learning
translation,66,165,results,bi-LSTM tokenizer,achieves,best accuracy
translation,66,165,results,best accuracy,among,all these tokenizers
translation,66,165,results,Results,has,bi-LSTM tokenizer
translation,66,217,results,our pipeline,has,outperforms
translation,66,217,results,outperforms,has,state of the art
translation,67,145,ablation-analysis,drops,in comparison with,model
translation,67,145,ablation-analysis,model,trained without using,automatically parsed data
translation,67,145,ablation-analysis,50 K automatically parsed sentences,has,performance
translation,67,145,ablation-analysis,performance,has,drops
translation,67,145,ablation-analysis,Ablation analysis,use,50 K automatically parsed sentences
translation,67,146,ablation-analysis,automatically parsed data,to,100K sentences
translation,67,146,ablation-analysis,parsing performance,improves,1 percent
translation,67,146,ablation-analysis,automatically parsed data,has,parsing performance
translation,67,146,ablation-analysis,100K sentences,has,parsing performance
translation,67,146,ablation-analysis,POS tagging accuracy,has,drops slightly
translation,67,146,ablation-analysis,Ablation analysis,increase,automatically parsed data
translation,67,147,ablation-analysis,automatically parsed data,to,200K sentences
translation,67,147,ablation-analysis,automatically parsed data,both,parsing performance
translation,67,147,ablation-analysis,automatically parsed data,both,POS tagging accuracy
translation,67,147,ablation-analysis,POS tagging accuracy,has,improve
translation,67,147,ablation-analysis,Ablation analysis,increase,automatically parsed data
translation,67,150,ablation-analysis,Pre-Train strategy,is,much more helpful
translation,67,150,ablation-analysis,Chinese,has,Pre-Train strategy
translation,67,150,ablation-analysis,Ablation analysis,for,Chinese
translation,67,85,hyperparameters,minibatched AdaGrad,in which,learning rate
translation,67,85,hyperparameters,adapted differently,for,different parameters
translation,67,85,hyperparameters,Hyperparameters,utilize,minibatched AdaGrad
translation,67,104,hyperparameters,several hyper-parameters,e.g.,word embedding dimension ( wordDim )
translation,67,104,hyperparameters,several hyper-parameters,e.g.,hidden layer node size ( hiddenSize )
translation,67,104,hyperparameters,several hyper-parameters,e.g.,Dropout ratio ( dropRatio )
translation,67,104,hyperparameters,several hyper-parameters,e.g.,beam size
translation,67,104,hyperparameters,beam size,for,inference ( beamSize )
translation,67,109,hyperparameters,wordDim,=,300
translation,67,109,hyperparameters,hiddenSize,=,300
translation,67,109,hyperparameters,beamSize,=,8
translation,67,109,hyperparameters,Hyperparameters,set,wordDim
translation,67,109,hyperparameters,Hyperparameters,set,hiddenSize
translation,67,109,hyperparameters,Hyperparameters,set,beamSize
translation,67,5,model,set of effective features,via,neural networks
translation,67,5,model,Model,automatically learn,set of effective features
translation,67,6,model,feedforward neural network model,takes as input,"few primitive units ( words , POS tags and certain contextual tokens )"
translation,67,6,model,feedforward neural network model,makes,parsing predictions
translation,67,6,model,"few primitive units ( words , POS tags and certain contextual tokens )",from,local context
translation,67,6,model,"few primitive units ( words , POS tags and certain contextual tokens )",induces,feature representation
translation,67,6,model,feature representation,in,hidden layer
translation,67,6,model,parsing predictions,in,output layer
translation,67,6,model,Model,build,feedforward neural network model
translation,67,7,model,feature representation and the prediction model parameters,using,back propagation algorithm
translation,67,26,model,shift-reduce parsing strategy,to build,constituent structure
translation,67,26,model,shift-reduce parsing strategy,train,feedforward neural network model
translation,67,26,model,constituent structure,of,sentence
translation,67,26,model,feedforward neural network model,to jointly learn,feature representations
translation,67,26,model,feedforward neural network model,make,parsing predictions
translation,67,26,model,Model,choose,shift-reduce parsing strategy
translation,67,26,model,Model,train,feedforward neural network model
translation,67,27,model,input layer,of,network
translation,67,27,model,network,takes as input,"few primitive units ( words , POS tags and certain contextual tokens )"
translation,67,27,model,"few primitive units ( words , POS tags and certain contextual tokens )",from,local context
translation,67,27,model,hidden layer,aims to induce,distributed feature representation
translation,67,27,model,distributed feature representation,by combining,all the primitive units
translation,67,27,model,all the primitive units,with,different weights
translation,67,27,model,output layer,to make,parsing predictions
translation,67,27,model,parsing predictions,based on,feature representation
translation,67,27,model,Model,has,input layer
translation,67,28,model,training process,using,backpropagation algorithm
translation,67,28,model,feature representation and prediction model parameters,using,backpropagation algorithm
translation,67,28,model,Model,During,training process
translation,67,28,model,Model,simultaneously learns,feature representation and prediction model parameters
translation,67,8,results,model,large amount of,automatically parsed data
translation,67,8,results,fine-tuning,on,manually annotated Treebank data
translation,67,8,results,our parser,achieves,highest F 1 score
translation,67,8,results,our parser,achieves,competitive F 1 score
translation,67,8,results,highest F 1 score,at,86.6 %
translation,67,8,results,highest F 1 score,at,90.7 %
translation,67,8,results,86.6 %,on,Chinese Treebank 5.1
translation,67,8,results,competitive F 1 score,at,90.7 %
translation,67,8,results,90.7 %,on,English Treebank
translation,67,8,results,Results,pre-training,model
translation,67,33,results,F 1 = 86.6 %,for,Chinese
translation,67,33,results,F 1 = 86.6 %,for,English
translation,67,33,results,outperforms,comparable to,state - of - the - art systems
translation,67,33,results,final performance,is,F 1 = 90.7 %
translation,67,33,results,outperforms,has,all the state - of - theart systems
translation,67,33,results,English,has,final performance
translation,67,33,results,Results,On,standard data sets
translation,67,34,results,outperforms,by,3.4 percentage points
translation,67,34,results,outperforms,by,2.5 percentage points
translation,67,34,results,Berkeley Parser,by,2.5 percentage points
translation,67,34,results,3.4 percentage points,for,Chinese
translation,67,34,results,2.5 percentage points,for,English
translation,67,34,results,cross-domain data sets,has,our model
translation,67,34,results,our model,has,outperforms
translation,67,34,results,outperforms,has,Berkeley Parser
translation,67,34,results,Results,On,cross-domain data sets
translation,67,118,results,wordDim,from,50 to 300
translation,67,118,results,parsing performance,improves,1.5 percentage points
translation,67,118,results,wordDim,has,parsing performance
translation,67,118,results,50 to 300,has,parsing performance
translation,67,118,results,Results,increasing,wordDim
translation,67,148,results,Pre-Train strategy,performance of,all three configurations
translation,67,148,results,all three configurations,improves,performance
translation,67,148,results,performance,against,model
translation,67,148,results,Results,For,Pre-Train strategy
translation,67,149,results,Mix-Train strategy,when,same amount of automatically parsed data
translation,67,149,results,same amount of automatically parsed data,is,used
translation,67,149,results,Pre-Train strategy,has,consistently outperforms
translation,67,149,results,consistently outperforms,has,Mix-Train strategy
translation,67,149,results,Results,has,Pre-Train strategy
translation,67,153,results,consistent degradation,in,performance
translation,67,153,results,performance,against,model
translation,67,156,results,parsing performance,of,Pre-Train setting
translation,67,156,results,consistently improves,as,size of automatically parsed data
translation,67,156,results,Pre-Train setting,has,consistently improves
translation,67,156,results,size of automatically parsed data,has,increases
translation,67,156,results,Results,has,parsing performance
translation,67,157,results,Results,Comparing With,State-of- the- art Systems
translation,67,166,results,performance,of,systems
translation,67,166,results,"our "" Pretrain - Finetune "" system",shows,fairly large gain
translation,67,166,results,fairly large gain,over,""" Supervised "" system"
translation,67,166,results,Results,Comparing,performance
translation,67,171,results,""" Pretrain - Finetune "" system",with,all the stateof - the - art systems
translation,67,171,results,""" Pretrain - Finetune "" system",see,our system
translation,67,171,results,our system,has,surpass
translation,67,171,results,surpass,has,all the other systems
translation,67,175,results,"Our "" Pretrain - Finetune "" system",achieves,much better performance
translation,67,175,results,much better performance,than,""" Supervised "" system"
translation,67,175,results,Results,has,"Our "" Pretrain - Finetune "" system"
translation,67,176,results,outperforms,has,all other neural network based systems
translation,67,176,results,Results,has,"Our "" Pretrain - Finetune "" system"
translation,67,189,results,Berkeley Parser,on average,"our "" Pretrain - Finetune "" model"
translation,67,189,results,"our "" Pretrain - Finetune "" model",is,3.4 percentage points better
translation,67,189,results,"our "" Pretrain - Finetune "" model",is,3.2 percentage points
translation,67,189,results,3.4 percentage points better,in terms of,parsing accuracy
translation,67,189,results,better,in terms of,POS tagging accuracy
translation,67,189,results,3.2 percentage points,has,better
translation,67,189,results,Results,Compared with,Berkeley Parser
translation,68,20,ablation-analysis,impact,of adding,additional BiLSTM layers and highway connections
translation,68,20,ablation-analysis,Ablation analysis,explore,impact
translation,68,134,ablation-analysis,Ablation analysis,Adding,third BiLSTM layer
translation,68,142,ablation-analysis,highway connections,become,crucial
translation,68,142,ablation-analysis,crucial,as,number of layers
translation,68,142,ablation-analysis,number of layers,has,increases
translation,68,142,ablation-analysis,Ablation analysis,has,highway connections
translation,68,25,experiments,parsing,using,same feature representations
translation,68,25,experiments,same feature representations,from,BiLSTMs
translation,68,126,experiments,BiLSTM POS tagger,yielded,97.37 % and 97.53 % tagging accuracy
translation,68,126,experiments,97.37 % and 97.53 % tagging accuracy,on,dev and test sets
translation,68,126,experiments,performance,on par with,state - of- the - art
translation,68,126,experiments,Supertaggers,has,BiLSTM POS tagger
translation,68,41,hyperparameters,each word,represented via,concatenation
translation,68,41,hyperparameters,concatenation,of,100 - dimensional embedding of the word
translation,68,41,hyperparameters,concatenation,of,100 - dimensional embedding
translation,68,41,hyperparameters,100 - dimensional embedding,of,predicted part of speech ( POS ) tag
translation,68,41,hyperparameters,30 dimensional character - level representation,from,CNNs
translation,68,41,hyperparameters,Hyperparameters,input for,each word
translation,68,43,hyperparameters,word embeddings,to be,pre-trained GloVe vectors
translation,68,43,hyperparameters,word embeddings,for,words
translation,68,43,hyperparameters,words,initialize,embedding
translation,68,43,hyperparameters,embedding,to,zero vector
translation,68,43,hyperparameters,words,has,not in GloVe
translation,68,43,hyperparameters,Hyperparameters,initialize,word embeddings
translation,68,43,hyperparameters,Hyperparameters,for,words
translation,68,44,hyperparameters,other embeddings,are,randomly initialized
translation,68,44,hyperparameters,Hyperparameters,has,other embeddings
translation,68,59,hyperparameters,BiLSTMs layers,with,512 units each
translation,68,59,hyperparameters,Hyperparameters,use,BiLSTMs layers
translation,68,60,hyperparameters,"layer - to - layer , and recurrent ( Gal and Ghahramani , 2016 ) dropout rates",are,0.5
translation,68,60,hyperparameters,Input,has,"layer - to - layer , and recurrent ( Gal and Ghahramani , 2016 ) dropout rates"
translation,68,60,hyperparameters,Hyperparameters,has,Input
translation,68,60,hyperparameters,Hyperparameters,has,"layer - to - layer , and recurrent ( Gal and Ghahramani , 2016 ) dropout rates"
translation,68,61,hyperparameters,CNN character - level representation,used,hyperparameters
translation,68,61,hyperparameters,hyperparameters,from,Ma and Hovy ( 2016 )
translation,68,61,hyperparameters,Hyperparameters,For,CNN character - level representation
translation,68,62,hyperparameters,network,including,embeddings
translation,68,62,hyperparameters,network,by optimizing,negative log-likelihood
translation,68,62,hyperparameters,negative log-likelihood,of,observed sequences
translation,68,62,hyperparameters,observed sequences,of,supertags
translation,68,62,hyperparameters,minibatch stochastic fashion,with,Adam optimization algorithm
translation,68,62,hyperparameters,Adam optimization algorithm,with,batch size
translation,68,62,hyperparameters,batch size,has,"100 and = 0.01 ( Kingma and Ba , 2015 )"
translation,68,62,hyperparameters,Hyperparameters,train,network
translation,68,78,hyperparameters,other embeddings,are,randomly initialized
translation,68,78,hyperparameters,Hyperparameters,has,other embeddings
translation,68,101,hyperparameters,BiLSTMs layers,with,400 units each
translation,68,101,hyperparameters,Hyperparameters,use,BiLSTMs layers
translation,68,102,hyperparameters,"layer - to - layer , and recurrent dropout rates",are,0.33
translation,68,102,hyperparameters,Hyperparameters,has,Input
translation,68,102,hyperparameters,Hyperparameters,has,"layer - to - layer , and recurrent dropout rates"
translation,68,103,hyperparameters,depths,of,all MLPs
translation,68,103,hyperparameters,depths,of,MLPs
translation,68,103,hyperparameters,all MLPs,are,all 1
translation,68,103,hyperparameters,MLPs,for,unlabeled attachment
translation,68,103,hyperparameters,MLPs,for,labeling
translation,68,103,hyperparameters,MLPs,for,labeling
translation,68,103,hyperparameters,labeling,contain,500 ( d arc ) and 100 ( d rel ) units
translation,68,103,hyperparameters,Hyperparameters,has,depths
translation,68,103,hyperparameters,Hyperparameters,has,MLPs
translation,68,104,hyperparameters,character - level CNNs,use,hyperparameters
translation,68,104,hyperparameters,hyperparameters,from,Ma and Hovy ( 2016 )
translation,68,104,hyperparameters,Hyperparameters,For,character - level CNNs
translation,68,113,hyperparameters,MLPs,for,POS tagging and supertagging
translation,68,113,hyperparameters,MLPs,contain,500 units
translation,68,113,hyperparameters,POS tagging and supertagging,contain,500 units
translation,68,113,hyperparameters,Hyperparameters,has,MLPs
translation,68,4,model,graph- based Tree Adjoining Grammar ( TAG ) parser,uses,BiL-STMs
translation,68,4,model,graph- based Tree Adjoining Grammar ( TAG ) parser,uses,highway connections
translation,68,4,model,graph- based Tree Adjoining Grammar ( TAG ) parser,uses,characterlevel CNNs
translation,68,4,model,Model,present,graph- based Tree Adjoining Grammar ( TAG ) parser
translation,68,6,model,graph- based parsing architecture,allows for,global inference
translation,68,6,model,rich feature representations,for,TAG parsing
translation,68,6,model,Model,has,graph- based parsing architecture
translation,68,18,model,character - level Convolutional Neural Networks ( CNNs ),for encoding,morphological information
translation,68,18,model,morphological information,instead of,suffix embeddings
translation,68,18,model,Model,use,character - level Convolutional Neural Networks ( CNNs )
translation,68,19,model,concatenation,after,each BiLSTM layer
translation,68,19,model,Model,perform,concatenation
translation,68,42,model,each character,in,word
translation,68,42,model,each character,in,word
translation,68,42,model,each character,by,30 dimensional vector
translation,68,42,model,30 filters,produce,30 dimensional vector
translation,68,42,model,30 dimensional vector,for,word
translation,68,5,results,best end-to - end parser,jointly performs,supertagging
translation,68,5,results,best end-to - end parser,jointly performs,POS tagging
translation,68,5,results,best end-to - end parser,jointly performs,parsing
translation,68,5,results,best end-to - end parser,jointly performs,outperforms
translation,68,5,results,previously reported best results,more than,2.2 LAS and UAS points
translation,68,5,results,best end-to - end parser,has,outperforms
translation,68,5,results,outperforms,has,previously reported best results
translation,68,5,results,Results,has,best end-to - end parser
translation,68,21,results,increase,of,1.3 %
translation,68,21,results,1.3 %,in,accuracy
translation,68,32,results,best results,in,both tasks
translation,68,32,results,end-to - end parser,has,outperforms
translation,68,32,results,outperforms,has,best results
translation,68,32,results,Results,demonstrate,end-to - end parser
translation,68,133,results,predicted POS,to,input
translation,68,133,results,input,somewhat helps,supertagging
translation,68,133,results,Results,Adding,predicted POS
translation,68,135,results,Our supertagging model ( BiLSTM3 - HW - CNN - POS ),performs,best
translation,68,135,results,best,on,dev set
translation,68,135,results,best,achieves,accuracy
translation,68,135,results,accuracy,of,90.81 %
translation,68,135,results,90.81 %,on,test set
translation,68,135,results,previously best result,by,more than 1.3 %
translation,68,135,results,outperforming,has,previously best result
translation,68,135,results,Results,has,Our supertagging model ( BiLSTM3 - HW - CNN - POS )
translation,68,140,results,performance,with,BiLSTM3
translation,68,140,results,predicted POS tags or supertags,has,deteriorates
translation,68,140,results,deteriorates,has,performance
translation,68,140,results,Results,adding,predicted POS tags or supertags
translation,68,143,results,parsing model,with,best dev performance
translation,68,143,results,best dev performance,on,test set
translation,68,143,results,best dev performance,has,CNN )
translation,68,143,results,Results,evaluate,parsing model
translation,68,144,results,improvements,of,1.8 and 1.7 points
translation,68,144,results,1.8 and 1.7 points,from,state - of- the- art
translation,68,148,results,our full joint model,that,our joint graph - based parser
translation,68,148,results,our full joint model,performs,1 2 3 4 5 6
translation,68,148,results,our full joint model,performs,our joint graph - based parser
translation,68,148,results,distance,between,dependency and the root of a parse
translation,68,148,results,Results,observe,our full joint model
translation,68,149,results,outperforms,across,all conditions
translation,68,149,results,graph - based parser,has,outperforms
translation,68,149,results,outperforms,has,shift-reduce parser
translation,68,149,results,Results,has,graph - based parser
translation,68,150,results,graph - based parser,shows,less of an effect
translation,68,150,results,less of an effect,of,dependency length
translation,68,161,results,Parsing performance,falls behind,best non-joint parser
translation,68,161,results,best non-joint parser,by,0.7 LAS points
translation,68,161,results,Results,has,Parsing performance
translation,68,193,results,joint method,yields,78.1 %
translation,68,193,results,joint method,yields,76.4 %
translation,68,193,results,joint method,yields,improvements
translation,68,193,results,78.1 %,in,accuracy
translation,68,193,results,76.4 %,in,F1
translation,68,193,results,improvements,of,2.4 and 2.7 points
translation,68,193,results,2.4 and 2.7 points,over,previously reported best results
translation,68,193,results,Results,has,joint method
translation,68,204,results,other parsers,including,neural network shift-reduce TAG parser
translation,68,204,results,joint parser,has,outperforms
translation,68,204,results,outperforms,has,other parsers
translation,68,204,results,Results,has,joint parser
translation,68,205,results,Our data-driven parsers,yield,relatively low performance
translation,68,205,results,relatively low performance,in,ObQ and RNR constructions
translation,68,205,results,Results,has,Our data-driven parsers
translation,69,29,ablation-analysis,uptraining,with,"2,000 labeled questions"
translation,69,29,ablation-analysis,"2,000 labeled questions",improves,accuracy
translation,69,29,ablation-analysis,accuracy,to,84.14 %
translation,69,29,ablation-analysis,accuracy,fully recovering,drop
translation,69,29,ablation-analysis,drop,between,in - domain and out -of- domain accuracy
translation,69,29,ablation-analysis,Ablation analysis,Combining,uptraining
translation,69,85,ablation-analysis,lexicalized parser,loses,1.5 % F 1
translation,69,85,ablation-analysis,latent variable parser,loses,only 0.7 %
translation,69,85,ablation-analysis,training and test data,has,lexicalized parser
translation,69,85,ablation-analysis,Ablation analysis,When,training and test data
translation,69,79,experiments,constituency parsers,observe,lexicalized ( reranking ) parser
translation,69,79,experiments,constituency parsers,observe,loses more
translation,69,79,experiments,lexicalized ( reranking ) parser,and Johnson ( 2005 ),loses more
translation,69,79,experiments,loses more,than,latent variable approach
translation,69,79,experiments,lexicalized ( reranking ) parser,has,loses more
translation,69,9,results,results,comparable to,2 K labeled questions
translation,69,9,results,2 K labeled questions,for,training
translation,69,9,results,Results,Uptraining with,100K unlabeled questions
translation,69,28,results,accuracy,of,linear time parser
translation,69,28,results,linear time parser,on,question test set
translation,69,28,results,60.06 % ( LAS ),to,76.94 %
translation,69,28,results,76.94 %,after,uptraining
translation,69,28,results,Results,has,accuracy
translation,69,95,results,even a modest amount of labeled data,from,target domain
translation,69,95,results,significantly boost,giving,double - digit improvements
translation,69,95,results,parsing performance,giving,double - digit improvements
translation,69,95,results,double - digit improvements,in,some cases
translation,69,95,results,significantly boost,has,parsing performance
translation,69,127,results,self-training,provides,only modest improvements
translation,69,127,results,only modest improvements,of,less than 2 %
translation,69,127,results,uptraining,gives,double - digit improvements
translation,69,127,results,double - digit improvements,in,some cases
translation,69,131,results,Our uptraining procedure,improves,parse quality
translation,69,131,results,parse quality,on,out -of- domain data
translation,69,131,results,parse quality,level of,in-domain accuracy
translation,69,131,results,Results,has,Our uptraining procedure
translation,69,184,results,large amounts of unlabeled data,gives,similar improvements
translation,69,184,results,similar improvements,having access to,"2,000 labeled sentences"
translation,69,184,results,"2,000 labeled sentences",from,target domain
translation,69,184,results,Results,Uptraining with,large amounts of unlabeled data
translation,70,91,baselines,DCST ++,is,augmentation
translation,70,91,baselines,augmentation,over,DCST
translation,70,91,baselines,augmentation,integrates,encoder outputs
translation,70,91,baselines,encoder outputs,from,neural morphological tagger
translation,70,91,baselines,encoder outputs,"al. , 2020 )",gating mechanism
translation,70,91,baselines,Baselines,has,DCST ++
translation,70,5,model,joint morphological parsing and dependency parsing,has,in Sanskrit
translation,70,6,model,Energy based model framework,proposed for,several structured prediction tasks
translation,70,6,model,several structured prediction tasks,in,Sanskrit
translation,70,6,model,Energy based model framework,has,"et al. , 2020 )"
translation,70,6,model,Model,extend,Energy based model framework
translation,70,7,model,framework 's default input graph generation method,modified to generate,multigraph
translation,70,7,model,multigraph,use of,exact search inference
translation,70,7,model,Model,has,framework 's default input graph generation method
translation,70,8,model,input search space,using,linguistically motivated approach
translation,70,8,model,linguistically motivated approach,rooted in,traditional grammatical analysis of Sanskrit
translation,70,8,model,Model,prune,input search space
translation,70,14,model,joint model,for,morphosyntactic parsing
translation,70,14,model,multigraph - EBM ( MG - EBM ),has,joint model
translation,70,14,model,Model,present,multigraph - EBM ( MG - EBM )
translation,70,59,model,Proposed Model Multigraph -EBM ( MG - EBM ),extends,EBM framework
translation,70,59,model,MG - EBM,has,Proposed Model Multigraph -EBM ( MG - EBM )
translation,70,59,model,Model,has,MG - EBM
translation,70,36,results,MP results,obtained from,joint model
translation,70,36,results,joint model,has,outperforms
translation,70,36,results,outperforms,has,standalone MP models
translation,70,36,results,Results,demonstrate that,MP results
translation,70,102,results,state of the art ( SOTA ) results,in,MP
translation,70,102,results,state of the art ( SOTA ) results,in,DP
translation,70,102,results,state of the art ( SOTA ) results,in,DP
translation,70,102,results,state of the art ( SOTA ) results,both in,standalone
translation,70,102,results,state of the art ( SOTA ) results,both in,joint morphosyntactic parsing setting
translation,70,102,results,standalone,with,gold morphological tags as input
translation,70,104,results,MG - EBM,has,outperforms
translation,70,104,results,outperforms,has,C-EBM
translation,70,104,results,Results,shows,MG - EBM
translation,70,105,results,MG-EBM,achieves,SOTA results
translation,70,105,results,SOTA results,for,DP
translation,70,105,results,SOTA results,followed by,DCST ++.
translation,70,105,results,DP,in,joint setting
translation,70,105,results,Results,has,MG-EBM
translation,70,111,results,5 point and 11 point F- Score increase,for,morphological parsing
translation,70,111,results,morphological parsing,compared to,P-EBM
translation,70,112,results,JP -EBM - Prune,has,outperforms
translation,70,112,results,outperforms,has,C-EBM
translation,70,112,results,Results,has,JP -EBM - Prune
translation,70,118,results,joint models,for,morphological parsing
translation,70,118,results,joint models,for,DP
translation,70,118,results,DP,has,outperform
translation,70,118,results,outperform,has,YAP
translation,70,118,results,Results,has,joint models
translation,70,119,results,joint models,for,morphological parsing and DP
translation,70,119,results,morphological parsing and DP,has,outperform
translation,70,119,results,outperform,has,pipeline EBM model C-EBM + T-EBM
translation,70,119,results,Results,has,joint models
translation,71,15,model,MSTParser,to incorporate,very simple model
translation,71,15,model,very simple model,of,morphological agreement
translation,71,126,results,agr,generated,best performance
translation,71,126,results,best performance,in,all but two cases
translation,71,126,results,outperforming,by,margins
translation,71,126,results,orig,by,margins
translation,71,126,results,margins,ranging from,0.8 % ( Arabic ) to 5.3 % ( Latin ) absolute
translation,71,126,results,Chinese,has,agr
translation,71,126,results,outperforming,has,orig
translation,71,126,results,Results,Excluding,Chinese
translation,71,140,results,Improvement,with,agr
translation,71,140,results,roughly uniform,across,all dataset sizes
translation,71,140,results,Results,has,Improvement
translation,71,146,results,less sensitivity,to,automatic tags
translation,71,146,results,less sensitivity,to,automatic tags
translation,71,146,results,orig features,improve,accuracy
translation,71,146,results,accuracy,when using,automatic tags
translation,71,146,results,Results,find,less sensitivity
translation,71,147,results,automatic data,affects,all feature sets
translation,71,147,results,negatively,by,2.1 % to 2.9 %
translation,71,147,results,all feature sets,has,negatively
translation,71,147,results,Results,Using,automatic data
translation,71,158,results,feature configurations,run on,all treebanks
translation,71,158,results,all treebanks,with,PPL feature
translation,71,158,results,Performance increases,from,orig to agr
translation,71,158,results,generally smaller,with,maximum
translation,71,158,results,maximum,of,4.6 % absolute
translation,71,158,results,Results,has,feature configurations
translation,71,160,results,Pearson 's r,between,morphological features per token and the new error reduction data
translation,71,160,results,Pearson 's r,gives,stronger correlation coefficient
translation,71,160,results,morphological features per token and the new error reduction data,gives,stronger correlation coefficient
translation,71,160,results,stronger correlation coefficient,of,0.748
translation,71,160,results,0.748,for,agr
translation,71,160,results,Results,Calculating,Pearson 's r
translation,72,46,experimental-setup,Experimental setup,implemented with,"DyNet ( Neubig et al. , 2017 )"
translation,72,106,experimental-setup,Cost-augmented decoding,applied during,training
translation,72,106,experimental-setup,Experimental setup,has,Cost-augmented decoding
translation,72,142,experimental-setup,neural network computation,implemented with,"DyNet ( Neubig et al. , 2017 )"
translation,72,142,experimental-setup,Experimental setup,has,neural network computation
translation,72,146,experimental-setup,all languages and all treebanks,trained,models
translation,72,146,experimental-setup,models,with,2 - layer - deep
translation,72,146,experimental-setup,models,with,192 - unit-wide
translation,72,146,experimental-setup,direction ) word- level bi-LSTMs,as,feature extractors
translation,72,146,experimental-setup,Experimental setup,For,all languages and all treebanks
translation,72,147,experimental-setup,Lexicalized character bi-LSTMs,are,2 layers deep
translation,72,147,experimental-setup,Lexicalized character bi-LSTMs,are,128 units wide
translation,72,147,experimental-setup,128 units wide,with,64 - dimensional input character embeddings
translation,72,147,experimental-setup,Experimental setup,has,Lexicalized character bi-LSTMs
translation,72,153,experimental-setup,hidden layers,used,tanh
translation,72,153,experimental-setup,tanh,as,activation functions
translation,72,156,experimental-setup,dropout,at,every stage
translation,72,156,experimental-setup,Experimental setup,applied,dropout
translation,72,157,experimental-setup,dropout rates,of,0.3
translation,72,157,experimental-setup,MLPs,has,dropout rates
translation,72,157,experimental-setup,Experimental setup,has,MLPs
translation,72,158,experimental-setup,dropout rates,of,0.3
translation,72,158,experimental-setup,0.3,for,input and recurrent connections
translation,72,158,experimental-setup,Bi-LSTMs,has,both character - level and word-level
translation,72,158,experimental-setup,Experimental setup,has,Bi-LSTMs
translation,72,159,experimental-setup,input vectors,to,word- level LSTMs
translation,72,159,experimental-setup,input vectors,to encourage,models
translation,72,159,experimental-setup,word- level LSTMs,for,15 %
translation,72,159,experimental-setup,models,gain,more information
translation,72,159,experimental-setup,more information,from,context
translation,72,159,experimental-setup,Experimental setup,zeroed out,input vectors
translation,72,160,experimental-setup,each model,randomly shuffled,training set
translation,72,160,experimental-setup,each model,grouped sentences into,mini-batches
translation,72,160,experimental-setup,training set,before starting,each epoch
translation,72,160,experimental-setup,mini-batches,has,of approximately 100 words
translation,72,160,experimental-setup,Experimental setup,trained,each model
translation,72,160,experimental-setup,Experimental setup,grouped sentences into,mini-batches
translation,72,161,experimental-setup,discriminative loss functions,optimized via,"Adam optimizer ( Kingma and Ba , 2015 )"
translation,72,161,experimental-setup,"Adam optimizer ( Kingma and Ba , 2015 )",with,default hyperparameters
translation,72,161,experimental-setup,default hyperparameters,except,initial learning rate
translation,72,161,experimental-setup,initial learning rate,set to be,0.002
translation,72,161,experimental-setup,Experimental setup,has,discriminative loss functions
translation,72,164,experimental-setup,"stackpropagation ( Zhang and Weiss , 2016 )",where,auxiliary task
translation,72,164,experimental-setup,auxiliary task,of,UPOS prediction
translation,72,164,experimental-setup,auxiliary task,used as,regularizer
translation,72,164,experimental-setup,Experimental setup,technique of,"stackpropagation ( Zhang and Weiss , 2016 )"
translation,72,168,experimental-setup,training,simultaneously trained,"MST , AHDP , AEDP and arc labeling models"
translation,72,168,experimental-setup,"MST , AHDP , AEDP and arc labeling models",with,shared LSTM feature extractors
translation,72,168,experimental-setup,Experimental setup,To speed up,training
translation,72,180,experimental-setup,entire training process,of,all models
translation,72,180,experimental-setup,entire training process,done using,8 CPU cores
translation,72,180,experimental-setup,all models,in,ensemble
translation,72,180,experimental-setup,all models,done using,8 CPU cores
translation,72,180,experimental-setup,ensemble,for,all treebanks
translation,72,180,experimental-setup,i7-4790 @ 3.60 GHz ),in,one week
translation,72,180,experimental-setup,Experimental setup,has,entire training process
translation,72,11,model,ensemble of global parsing models,including,two novel global transition - based models
translation,72,11,model,Model,highlight,neural - network - based feature extractors
translation,72,11,model,Model,highlight,ensemble of global parsing models
translation,72,13,model,representational power,of,bi-LSTMs
translation,72,13,model,compact features,for,graph - based and transition - based parsing frameworks
translation,72,40,model,unlabeled parsing,with,ensemble of three global models
translation,72,40,model,Model,focuses on,unlabeled parsing
translation,72,52,model,features,for,each word
translation,72,52,model,features,consider,one sentence at a time
translation,72,52,model,each word,in,isolation
translation,72,52,model,one sentence at a time,for,context-sensitive representations
translation,72,52,model,Model,consider,one sentence at a time
translation,72,7,results,our system,ranked,second
translation,72,7,results,second,in,official end-toend evaluation
translation,72,7,results,official end-toend evaluation,with,macro-average
translation,72,7,results,macro-average,of,75.00 LAS F1 score
translation,72,7,results,75.00 LAS F1 score,over,81 test treebanks
translation,72,7,results,baseline tokenizers,has,our system
translation,72,7,results,parsing,has,our system
translation,72,7,results,Results,relying on,baseline tokenizers
translation,72,8,results,top average performance,on,four surprise languages
translation,72,8,results,top average performance,on,small treebank subset
translation,72,15,results,two bi-LSTM vectors,as,features
translation,72,15,results,all three global parsing paradigms,have,efficient Opn 3 q implementations
translation,72,15,results,two bi-LSTM vectors,has,all three global parsing paradigms
translation,72,15,results,features,has,all three global parsing paradigms
translation,72,15,results,Results,With,two bi-LSTM vectors
translation,72,27,results,full system,scored,macro-average LAS F1 score
translation,72,27,results,macro-average LAS F1 score,of,75.00
translation,72,27,results,75.00,ranked,second
translation,72,27,results,second,among,all participating systems
translation,72,27,results,Results,has,full system
translation,72,118,results,Our system,achieved,second place
translation,72,118,results,second place,in,overall ranking
translation,72,118,results,Results,has,Our system
translation,72,119,results,average performance,on,small treebanks ( 8 treebanks ) and surprise languages ( 4 treebanks
translation,72,119,results,average performance,scored,first
translation,72,119,results,first,among,all teams
translation,72,119,results,Results,considering,average performance
translation,72,130,results,top third,for,most treebanks
translation,72,130,results,most treebanks,except,ja pud
translation,72,130,results,most treebanks,except,en pud
translation,72,130,results,most treebanks,except,ru pud
translation,72,130,results,Results,ranked within,top third
translation,72,134,results,two languages,observe,improvement
translation,72,134,results,improvement,on,LAS scores
translation,72,134,results,LAS scores,of,7.53 and 14.73
translation,72,134,results,Results,has,two languages
translation,72,137,results,AEDP,gives,higher unlabeled parsing performance
translation,72,137,results,ensemble of three instances of AEDPs,achieves,comparable performance
translation,72,137,results,comparable performance,to,our full system
translation,72,138,results,arc-labeling ensemble,gives,another gain
translation,72,138,results,another gain,in,LAS result
translation,72,138,results,LAS result,of,0.31
translation,72,138,results,Results,has,arc-labeling ensemble
translation,73,4,experiments,Question - Answer driven,has,Semantic Role Labeling ( QA - SRL ) annotations
translation,73,5,experiments,QA - SRL Bank 2.0,consists of,"over 250,000"
translation,73,5,experiments,question - answer pairs,for,"over 64,000 sentences"
translation,73,5,experiments,"over 64,000 sentences",across,3 domains
translation,73,5,experiments,"over 250,000",has,question - answer pairs
translation,73,23,model,streamlined web interface,including,autosuggest mechanism
translation,73,23,model,streamlined web interface,including,automatic quality control
translation,73,23,model,streamlined web interface,to boost,recall
translation,73,23,model,streamlined web interface,use,validation stage
translation,73,23,model,automatic quality control,to boost,recall
translation,73,23,model,validation stage,to en-sure,high precision
translation,73,23,model,Model,introduce,streamlined web interface
translation,73,23,model,Model,use,validation stage
translation,73,40,model,crowdsourcing pipeline,to collect,annotations
translation,73,40,model,annotations,has,"rapidly , cheaply"
translation,73,40,model,Model,introduce,crowdsourcing pipeline
translation,73,85,results,questions,rejected in,validation step
translation,73,85,results,greatly increases,with,a small decrease
translation,73,85,results,recall,with,a small decrease
translation,73,85,results,a small decrease,in,precision
translation,73,85,results,readding,has,questions
translation,73,85,results,validation step,has,greatly increases
translation,73,85,results,greatly increases,has,recall
translation,73,85,results,Results,has,readding
translation,73,133,results,span-based model,significantly improves over,BIO model
translation,73,133,results,BIO model,in,precision and recall
translation,73,133,results,BIO model,both,precision and recall
translation,73,133,results,Results,has,span-based model
translation,73,141,results,question generation,on,development set
translation,73,142,results,sequential model 's exact match accuracy,is,significantly higher
translation,73,142,results,word - level accuracy,is,roughly comparable
translation,73,142,results,Results,has,sequential model 's exact match accuracy
translation,73,179,results,sequence - based question generation models,are,much more accurate
translation,73,179,results,much more accurate,than,local model
translation,73,179,results,Results,has,sequence - based question generation models
translation,73,181,results,threshold value,which gives,similar number of questions per sentence
translation,73,181,results,question and span accuracy,are,82.64 % and 77.61 %
translation,73,181,results,Results,choose,threshold value
translation,74,5,baselines,neural network - based dependency parser,has,greedy transition approach
translation,74,5,baselines,greedy transition approach,to,dependency parsing
translation,74,10,baselines,Baselines,has,CCG
translation,74,76,hyperparameters,embedding size,=,100
translation,74,76,hyperparameters,embedding size,=,200
translation,74,76,hyperparameters,hidden layer size,=,200
translation,74,77,hyperparameters,word embeddings,has,pre-trained 100 dimensional word embeddings
translation,74,77,hyperparameters,Hyperparameters,For,word embeddings
translation,74,78,hyperparameters,POS tags and supertags,fed to,neural network
translation,74,78,hyperparameters,vectors,fed to,neural network
translation,74,78,hyperparameters,neural network,during,training
translation,74,78,hyperparameters,POS tags and supertags,has,vectors
translation,74,78,hyperparameters,Hyperparameters,For,POS tags and supertags
translation,74,8,model,neural network parser,trained together with,dependencies and simplified CCG tags
translation,74,8,model,neural network parser,trained together with,other features
translation,74,8,model,Model,has,neural network parser
translation,74,67,results,recovery accuracy,is,87.36 %
translation,74,67,results,English,has,recovery accuracy
translation,74,67,results,Results,On,English
translation,74,93,results,tagger,predicting,French supertags
translation,74,93,results,tagger,performs,well
translation,74,93,results,French supertags,transferred from,English data
translation,74,97,results,Results,has,Shared Task
translation,74,98,results,drop,in,accuracy
translation,74,98,results,accuracy,compared to,our pre-evaluation phase
translation,74,98,results,Results,see,drop
translation,75,4,baselines,update,to,UDPipe 1.0
translation,75,4,baselines,trainable pipeline,performs,sentence segmentation
translation,75,4,baselines,trainable pipeline,performs,tokenization
translation,75,4,baselines,trainable pipeline,performs,POS tagging
translation,75,4,baselines,trainable pipeline,performs,lemmatization
translation,75,4,baselines,trainable pipeline,performs,dependency parsing
translation,75,4,baselines,update,has,trainable pipeline
translation,75,4,baselines,UDPipe 1.0,has,trainable pipeline
translation,75,16,baselines,sentence segmentation,using,UD version 2 treebanks
translation,75,16,baselines,UD version 2 treebanks,as,training data
translation,75,2,experiments,Parsing UD 2.0,with,UDPipe
translation,75,5,results,models,for,all 50 languages
translation,75,5,results,all 50 languages,of,UD 2.0
translation,75,5,results,easily,using,data
translation,75,5,results,data,in,CoNLL -U format
translation,75,5,results,Results,provide,models
translation,76,68,experimental-setup,parser architecture,with,bidirectional LSTM
translation,76,68,experimental-setup,minimal feature set,namely,three tokens
translation,76,68,experimental-setup,minimal feature set,namely,one token
translation,76,68,experimental-setup,three tokens,in,stack
translation,76,68,experimental-setup,one token,in,buffer
translation,76,68,experimental-setup,Experimental setup,implement,parser architecture
translation,76,69,experimental-setup,each token,compose,characterbased representations
translation,76,69,experimental-setup,each token,concatenate them with,randomly initialized embeddings
translation,76,69,experimental-setup,characterbased representations,with,convolutional neural networks
translation,76,69,experimental-setup,randomly initialized embeddings,of,word form
translation,76,69,experimental-setup,randomly initialized embeddings,of,universal POS tag
translation,76,69,experimental-setup,randomly initialized embeddings,of,morphological features
translation,76,69,experimental-setup,Experimental setup,For,each token
translation,76,4,model,reinforcement learning ( RL ),to approximate,dynamic oracles
translation,76,4,model,dynamic oracles,for,transition systems
translation,76,4,model,dynamic oracles,where,exact dynamic oracles
translation,76,5,model,oracle parsing,as,reinforcement learning problem
translation,76,5,model,reward function,inspired by,classical dynamic oracle
translation,76,5,model,Deep Q-Learning ( DQN ) techniques,to train,oracle
translation,76,5,model,oracle,with,gold trees
translation,76,5,model,gold trees,as,features
translation,76,5,model,Model,treat,oracle parsing
translation,76,5,model,Model,use,Deep Q-Learning ( DQN ) techniques
translation,76,6,model,knowledge and data-driven methods,enables,efficient dynamic oracle
translation,76,6,model,efficient dynamic oracle,improves,parser performance
translation,76,6,model,parser performance,over,static oracles
translation,76,6,model,static oracles,in,several transition systems
translation,76,6,model,Model,combination of,knowledge and data-driven methods
translation,76,91,results,ADO,on average,outperforms
translation,76,91,results,beneficial,in,"41 , 40 , 41 , and 35 treebanks"
translation,76,91,results,"41 , 40 , 41 , and 35 treebanks",for,four systems
translation,76,91,results,static baseline,by,"0.33 % , 0.33 % , 0.51 % , 0.21 %"
translation,76,91,results,55 treebanks,has,ADO
translation,76,91,results,outperforms,has,static baseline
translation,76,91,results,Results,Out of,55 treebanks
translation,76,92,results,treebank characteristics,training with,ADOs
translation,76,92,results,beneficial,in,most cases
translation,76,92,results,beneficial,irrespective of,projectiveness
translation,76,92,results,most cases,irrespective of,projectiveness
translation,76,92,results,projectiveness,of,treebank
translation,76,92,results,Results,considering,treebank characteristics
translation,76,95,results,benefit,of,ADO and EDO
translation,76,95,results,ADO and EDO,is,very close
translation,76,95,results,static baseline,by,0.21 % and 0.27 %
translation,76,95,results,HYBRID,has,benefit
translation,76,95,results,outperform,has,static baseline
translation,76,95,results,Results,In,HYBRID
translation,77,192,ablation-analysis,Error Analysis,on,Phrasal- Anchoring
translation,77,192,ablation-analysis,Phrasal- Anchoring,According to,attributes
translation,77,192,ablation-analysis,our model,performs,7 - 8 points
translation,77,192,ablation-analysis,worse,than,top 1 model
translation,77,192,ablation-analysis,Error Analysis,has,our model
translation,77,192,ablation-analysis,Phrasal- Anchoring,has,our model
translation,77,192,ablation-analysis,attributes,has,our model
translation,77,192,ablation-analysis,7 - 8 points,has,worse
translation,77,192,ablation-analysis,Ablation analysis,on,Phrasal- Anchoring
translation,77,192,ablation-analysis,Ablation analysis,has,Error Analysis
translation,77,168,baselines,latent AMR model,to model,posterior alignment
translation,77,168,baselines,latent AMR model,use,another Bi-LSTM
translation,77,168,baselines,posterior alignment,use,another Bi-LSTM
translation,77,168,baselines,another Bi-LSTM,for,node sequence encoding
translation,77,169,hyperparameters,phrasal - anchoring model setup,use,8 - layers 8headers transformer
translation,77,169,hyperparameters,8 - layers 8headers transformer,with,position encoding
translation,77,169,hyperparameters,position encoding,to encode,input sentence
translation,77,169,hyperparameters,Hyperparameters,For,phrasal - anchoring model setup
translation,77,169,hyperparameters,Hyperparameters,use,8 - layers 8headers transformer
translation,77,170,hyperparameters,all sentence encoders,use,character - level CNN model
translation,77,170,hyperparameters,character - level CNN model,as,character - level embedding
translation,77,170,hyperparameters,character - level embedding,without,pre-trained deep contextualized embedding model
translation,77,170,hyperparameters,Hyperparameters,For,all sentence encoders
translation,77,173,hyperparameters,"Adam ( Kingma and Ba , 2014 )",using,batch size
translation,77,173,hyperparameters,64,for,graph - based model
translation,77,173,hyperparameters,250,for,CKY - based model
translation,77,173,hyperparameters,batch size,has,64
translation,77,173,hyperparameters,Hyperparameters,trained with,"Adam ( Kingma and Ba , 2014 )"
translation,77,175,hyperparameters,early - stopping,to avoid,over-fitting
translation,77,175,hyperparameters,Hyperparameters,exploit,early - stopping
translation,77,180,results,our system,ranked,top 1
translation,77,180,results,top 1,in,AMR subtask
translation,77,180,results,top 5 models,in,large margin
translation,77,180,results,latent alignment based AMR parser,has,our system
translation,77,180,results,outperformed,has,top 5 models
translation,77,180,results,Results,using,latent alignment based AMR parser
translation,77,181,results,Our parser,on,PSD
translation,77,181,results,PSD,ranked,6
translation,77,181,results,worse,then,top 5 model
translation,77,181,results,only 0.02 %,has,worse
translation,77,181,results,Results,has,Our parser
translation,77,188,results,all our 3 models,perform,almost as good
translation,77,188,results,all our 3 models,perform,worse
translation,77,188,results,almost as good,as,top 1 model
translation,77,188,results,top 1 model,of,each subtask
translation,77,188,results,each subtask,on,node label prediction
translation,77,188,results,worse,on,top and edge prediction
translation,77,188,results,Results,see that,all our 3 models
translation,77,190,results,performance gap,between,node labels and node anchors
translation,77,190,results,node labels and node anchors,are,almost consistent
translation,77,190,results,our model,on predicting,NULL nodes
translation,77,190,results,Results,found,performance gap
translation,78,77,baselines,first two baselines,are,Convex - MST
translation,78,77,baselines,first two baselines,are,LC - DMV
translation,78,77,baselines,LC - DMV,are,independently trained
translation,78,77,baselines,Baselines,has,first two baselines
translation,78,99,baselines,D-I,is,third baseline
translation,78,99,baselines,third baseline,in which,LC - DMV training
translation,78,99,baselines,LC - DMV training,initialized by,parses
translation,78,99,baselines,parses,produced from,trained Convex - MST model
translation,78,99,baselines,Baselines,has,D-I
translation,78,6,model,new learning strategy,learns,generative model and a discriminative model
translation,78,6,model,generative model and a discriminative model,based on,dual decomposition method
translation,78,6,model,jointly,based on,dual decomposition method
translation,78,6,model,generative model and a discriminative model,has,jointly
translation,78,6,model,Model,propose,new learning strategy
translation,78,21,model,two state - of - the - art models,of,unsupervised dependency parsing
translation,78,21,model,generative model,called,"LC - DMV ( Noji et al. , 2016 )"
translation,78,21,model,discriminative model,called,Convex - MST
translation,78,21,model,Convex - MST,has,"Grave and Elhadad , 2015 )"
translation,78,21,model,Model,jointly train,two state - of - the - art models
translation,78,22,model,learning algorithm,based on,"dual decomposition ( Dantzig and Wolfe , 1960 ) inference algorithm"
translation,78,22,model,learning algorithm,encourages,two models
translation,78,22,model,two models,to influence,each other
translation,78,22,model,each other,during,training
translation,78,22,model,Model,employ,learning algorithm
translation,78,7,results,method,is,simple and general
translation,78,7,results,method,is,effective
translation,78,7,results,effective,to capture,advantages
translation,78,7,results,advantages,has,of both models
translation,78,7,results,improve,has,learning results
translation,78,7,results,Results,has,method
translation,78,20,results,language - independent rules,between,pairs
translation,78,20,results,language - independent rules,to guide,learning
translation,78,20,results,pairs,of,POS tags
translation,78,20,results,state- ofthe - art performance,on,UD treebank dataset
translation,78,20,results,Results,utilizing,language - independent rules
translation,78,82,results,LC - DMV and Convex - MST,achieve,superior overall performance
translation,78,82,results,superior overall performance,than,separately trained
translation,78,82,results,separately trained,with or without,mutual initialization
translation,78,82,results,joint training and independent decoding,has,LC - DMV and Convex - MST
translation,78,82,results,Results,see that,joint training and independent decoding
translation,78,83,results,Joint decoding,with,our jointly trained models
translation,78,83,results,our jointly trained models,performs,worse
translation,78,83,results,worse,than,independent decoding
translation,78,83,results,Results,has,Joint decoding
translation,79,31,ablation-analysis,parsing model,incorporate,phrase embeddings
translation,79,31,ablation-analysis,phrase embeddings,into,model
translation,79,31,ablation-analysis,significantly improves,has,parsing accuracy
translation,79,31,ablation-analysis,Ablation analysis,To further strengthen,parsing model
translation,79,29,baselines,first model,is,simple first-order model
translation,79,29,baselines,simple first-order model,uses,only atomic features
translation,79,29,baselines,simple first-order model,uses,any combinational features
translation,79,29,baselines,simple first-order model,does not use,any combinational features
translation,79,29,baselines,Baselines,has,first model
translation,79,134,experimental-setup,embedding size d,=,50
translation,79,134,experimental-setup,hidden layer ( Layer 2 ) size,=,200
translation,79,134,experimental-setup,discount parameter,margin loss ? =,0.3
translation,79,134,experimental-setup,initial learning rate,of,AdaGrad alpha
translation,79,134,experimental-setup,AdaGrad alpha,=,0.1
translation,79,137,experimental-setup,MSTParser,for,conventional first-order model
translation,79,137,experimental-setup,MSTParser,for,second-order model
translation,79,137,experimental-setup,second-order model,has,"McDonald and Pereira , 2006 )"
translation,79,137,experimental-setup,Experimental setup,use,MSTParser
translation,79,141,experimental-setup,parsing speeds,measured on,workstation
translation,79,141,experimental-setup,workstation,with,Intel Xeon 3.4 GHz CPU
translation,79,141,experimental-setup,workstation,with,32GB RAM
translation,79,141,experimental-setup,Experimental setup,has,parsing speeds
translation,79,5,model,general and effective Neural Network model,for,graph - based dependency parsing
translation,79,5,model,Model,propose,general and effective Neural Network model
translation,79,6,model,high-order feature combinations,using,atomic features
translation,79,6,model,high-order feature combinations,by exploiting,novel activation function tanhcube
translation,79,6,model,Model,automatically learn,high-order feature combinations
translation,79,7,model,simple yet effective way,to utilize,phrase - level information
translation,79,7,model,phrase - level information,expensive to use in,conventional graph - based parsers
translation,79,7,model,Model,propose,simple yet effective way
translation,79,19,model,general and effective Neural Network model,for,graph - based dependency parsing
translation,79,19,model,Model,propose,general and effective Neural Network model
translation,79,23,model,phrase-level information,through,distributed representation
translation,79,23,model,distributed representation,for,phrases ( phrase embeddings )
translation,79,23,model,Model,exploit,phrase-level information
translation,79,32,results,our first-order model,to,secondorder model
translation,79,32,results,secondorder model,exploits,interactions
translation,79,32,results,interactions,between,two adjacent dependency edges
translation,79,32,results,Results,extend,our first-order model
translation,79,142,results,1 - order-atomic - rand,performs,conventional first-order model
translation,79,142,results,1 - order-atomic - rand,as well as,conventional first-order model
translation,79,142,results,1 - order-atomic - rand,perform,better
translation,79,142,results,2 - order - phrase - rand,perform,better
translation,79,142,results,better,than,conventional models
translation,79,142,results,conventional models,in,MSTParser
translation,79,142,results,random initialization,has,1 - order-atomic - rand
translation,79,142,results,Results,even with,random initialization
translation,79,143,results,Pretraining,improves,performance
translation,79,143,results,performance,of,all three models
translation,79,143,results,Results,has,Pretraining
translation,79,144,results,1 - order- phrase,better than,1 - order-atomic
translation,79,144,results,Results,has,1 - order- phrase
translation,79,145,results,2order - phrase,improves,performance
translation,79,145,results,Results,has,2order - phrase
translation,79,146,results,All three models,perform,significantly better
translation,79,146,results,significantly better,than,counterparts
translation,79,146,results,counterparts,in,MSTParser
translation,79,146,results,MSTParser,where,millions of features
translation,79,146,results,1 - order - phrase,works,surprisingly well
translation,79,146,results,surprisingly well,even beats,conventional second-order model
translation,79,146,results,Results,has,All three models
translation,79,147,results,1 - order-atomic,is,fastest
translation,79,147,results,other two models,have,similar speeds
translation,79,147,results,similar speeds,as,MSTParser
translation,79,147,results,parsing speed,has,1 - order-atomic
translation,79,147,results,Results,to,parsing speed
translation,79,153,results,tanh-cube function,has,outperforms
translation,79,153,results,outperforms,has,cube function
translation,79,153,results,Results,see,tanh-cube function
translation,79,154,results,tanh-cube function and cube function,performs,better
translation,79,154,results,better,than,tanh function
translation,79,154,results,Results,has,tanh-cube function and cube function
translation,80,13,model,constituent trees,annotated with,grammatical and semantic tags
translation,80,13,model,constituent trees,annotated with,special tag
translation,80,13,model,special tag,indicating,gapped conjuncts
translation,80,99,results,previous method,even without,BERT
translation,80,99,results,outperformed,has,previous method
translation,80,99,results,Results,has,outperformed
translation,81,25,ablation-analysis,typological features,are,crucial
translation,81,25,ablation-analysis,typological features,leading to,substantial LAS increase
translation,81,25,ablation-analysis,typological features,leading to,no loss
translation,81,25,ablation-analysis,substantial LAS increase,on,zero-shot languages
translation,81,25,ablation-analysis,no loss,on,high- resource languages
translation,81,25,ablation-analysis,high- resource languages,compared to,language embeddings
translation,81,25,ablation-analysis,our model,has,typological features
translation,81,25,ablation-analysis,language embeddings,has,learned from scratch
translation,81,25,ablation-analysis,Ablation analysis,In,our model
translation,81,25,ablation-analysis,Ablation analysis,has,typological features
translation,81,168,ablation-analysis,most important,in,biaffine attention layer
translation,81,168,ablation-analysis,low-resource languages,has,typological feature based parameter sharing
translation,81,168,ablation-analysis,Ablation analysis,for,low-resource languages
translation,81,106,baselines,"UUparser +BERT ( Kulmizev et al. , 2019 )",using,mBERT embeddings
translation,81,106,baselines,graph- based BLSTM parser,using,mBERT embeddings
translation,81,106,baselines,"UUparser +BERT ( Kulmizev et al. , 2019 )",has,graph- based BLSTM parser
translation,81,106,baselines,Baselines,has,"UUparser +BERT ( Kulmizev et al. , 2019 )"
translation,81,101,experimental-setup,256 and 32,for,adapter size and language embedding size
translation,81,101,experimental-setup,Experimental setup,use,256 and 32
translation,81,6,model,novel multilingual task adaptation approach,based on,contextual parameter generation
translation,81,6,model,novel multilingual task adaptation approach,based on,adapter modules
translation,81,6,model,Model,propose,novel multilingual task adaptation approach
translation,81,7,model,adapters,via,language embeddings
translation,81,7,model,adapters,sharing,model parameters
translation,81,7,model,model parameters,across,languages
translation,81,7,model,Model,learn,adapters
translation,81,20,model,good balance,between,maximum sharing
translation,81,20,model,good balance,between,language -specific capacity
translation,81,20,model,language -specific capacity,in,multilingual dependency parsing
translation,81,20,model,Model,strike,good balance
translation,81,21,model,language -specific parameters,including,adapter modules
translation,81,21,model,adapter modules,as a function of,language embeddings
translation,81,21,model,Model,propose,new multilingual parser
translation,81,22,model,parameters,across,languages
translation,81,22,model,parameters,ensuring,generalization and transfer ability
translation,81,22,model,language -specific parameterization,in,single multilingual model
translation,81,22,model,Model,share,parameters
translation,81,22,model,Model,enables,language -specific parameterization
translation,81,23,model,language embeddings,mix of,linguistically curated and predicted typological features
translation,81,23,model,linguistically curated and predicted typological features,obtained from,URIEL language typology database
translation,81,23,model,URIEL language typology database,supports,3718 languages
translation,81,23,model,3718 languages,all languages represented in,UD
translation,81,23,model,Model,to learn,language embeddings
translation,81,32,model,simple but effective method,for conditioning,language adaptation
translation,81,32,model,language adaptation,on,existing typological language features
translation,81,32,model,Model,provide,simple but effective method
translation,81,80,model,taskspecific as well as language -specific adaptation,via,CPG
translation,81,82,model,amount of sharing,across,languages
translation,81,82,model,amount of sharing,generate,trainable parameters
translation,81,82,model,trainable parameters,of,model
translation,81,82,model,trainable parameters,using,contextual parameter generator ( CPG ) function
translation,81,82,model,model,using,contextual parameter generator ( CPG ) function
translation,81,82,model,Model,control,amount of sharing
translation,81,112,model,UDapter - proxy,trained without,typology features
translation,81,112,model,Adapter-only,has,only task-specific adapter modules
translation,81,112,model,typology features,has,separate language embedding
translation,81,112,model,Model,has,Adapter-only
translation,81,9,results,strong monolingual and multilingual baselines,on,majority
translation,81,9,results,majority,of both,high- resource and lowresource ( zero-shot ) languages
translation,81,9,results,resulting parser,has,UDapter
translation,81,9,results,resulting parser,has,outperforms
translation,81,9,results,UDapter,has,outperforms
translation,81,9,results,outperforms,has,strong monolingual and multilingual baselines
translation,81,9,results,Results,has,resulting parser
translation,81,115,results,monolingual and multilingual baselines,on,high-resource and zero-shot languages
translation,81,115,results,outperforms,has,monolingual and multilingual baselines
translation,81,118,results,UDapter,beats,previous work
translation,81,118,results,consistently outperforms,both,our monolingual and multilingual baselines
translation,81,118,results,our monolingual and multilingual baselines,in,all languages
translation,81,118,results,previous work,setting,new state of the art
translation,81,118,results,new state of the art,in,9 out of 13 languages
translation,81,118,results,UDapter,has,consistently outperforms
translation,81,118,results,consistently outperforms,has,our monolingual and multilingual baselines
translation,81,118,results,Results,has,UDapter
translation,81,119,results,Statistical significance testing,applied between,UDapter and multi / mono-udify
translation,81,119,results,UDapter and multi / mono-udify,confirms,UDapter 's performance
translation,81,119,results,UDapter 's performance,is,significantly better
translation,81,119,results,significantly better,than,baselines
translation,81,119,results,baselines,in,11 out of 13 languages
translation,81,119,results,Results,has,Statistical significance testing
translation,81,120,results,multiudify,gives,worst performance
translation,81,120,results,worst performance,in,typologically diverse high- resource setting
translation,81,120,results,comparable baselines,has,multiudify
translation,81,120,results,Results,Among,comparable baselines
translation,81,121,results,multilingual model,clearly worse than,monolingually trained counterparts
translation,81,121,results,Results,has,multilingual model
translation,81,124,results,adapter-only model,has,considerably outperforms
translation,81,124,results,considerably outperforms,has,multi-udify ( 85.0 vs 83.0 )
translation,81,124,results,Results,has,adapter-only model
translation,81,125,results,UDapter,achieves,overall best results
translation,81,125,results,overall best results,with,consistent gains
translation,81,125,results,consistent gains,over,multi-udify and adapter-only
translation,81,125,results,Results,has,UDapter
translation,81,126,results,Average LAS,on,30 low-resource languages
translation,81,126,results,UDapter,shows,benefits
translation,81,126,results,Low-Resource Languages,has,Average LAS
translation,81,126,results,UDapter,has,slightly outperforms
translation,81,126,results,slightly outperforms,has,multi-udify baseline
translation,81,126,results,multi-udify baseline,has,36.5 vs 36.3 )
translation,81,126,results,Results,has,Low-Resource Languages
translation,81,130,results,UDapter - proxy results,choosing,proxy language embedding
translation,81,130,results,proxy language embedding,from,same language family
translation,81,130,results,underperforms,has,UDapter
translation,81,130,results,Results,has,UDapter - proxy results
translation,81,135,results,higher,for,languages with less training data
translation,81,135,results,Results,has,gains
translation,81,137,results,difference,between,models
translation,81,137,results,models,is,small
translation,81,137,results,small,compared to,high- resource languages ( + 1.2 LAS )
translation,81,137,results,zero-shot languages,has,difference
translation,81,137,results,Results,For,zero-shot languages
translation,81,138,results,UDapter,is,outperforms
translation,81,138,results,multi-udify,in,13 out of 22 ( non - mBERT ) languages
translation,81,138,results,outperforms,has,multi-udify
translation,81,138,results,Results,notice,UDapter
translation,81,144,results,high- resource set,models with and without,typological features
translation,81,144,results,very similar average LAS,has,87.3 and 87.1
translation,81,144,results,Results,on,high- resource set
translation,81,145,results,zero-shot languages,use of,centroid embedding
translation,81,145,results,centroid embedding,performs,very poorly
translation,81,145,results,very poorly,has,9.0 vs 36.5 average LAS score
translation,81,145,results,9.0 vs 36.5 average LAS score,has,over 30 languages
translation,81,145,results,Results,On,zero-shot languages
translation,81,146,results,proxy language embedding,belonging to,same family
translation,81,146,results,underperforms,has,UDapter
translation,81,146,results,Results,using,proxy language embedding
translation,81,160,results,CPG,with,typological features
translation,81,160,results,typological features,led to,best results
translation,81,160,results,best results,over,all languages
translation,81,160,results,Results,addition of,CPG
translation,81,163,results,model size,brings,slight gain
translation,81,163,results,slight gain,to,high- resource languages
translation,81,163,results,small loss,in,zero-shot setup
translation,81,163,results,Results,increase in,model size
translation,81,165,results,UDapter,including,CPG
translation,81,165,results,UDapter,increases,model size
translation,81,165,results,model size,by,language embeddings
translation,81,165,results,UDapter,has,outperforms
translation,81,165,results,outperforms,has,both adapter-only models
translation,81,165,results,Results,has,UDapter
translation,81,167,results,most of the gain,in,high- resource languages
translation,81,167,results,CPG,on,multilingual encoder
translation,81,167,results,Results,show,most of the gain
translation,82,131,experimental-setup,Dynet AdamTrainer,chosen for,gradient updates
translation,82,131,experimental-setup,gradient updates,with,default parameters
translation,82,131,experimental-setup,Experimental setup,has,Dynet AdamTrainer
translation,82,48,model,left-to- right maximum-entropy tagger,relying on,features
translation,82,48,model,features,based on,training corpus
translation,82,48,model,Model,developed,new part- of-speech tagging system
translation,82,60,results,second improvement,over,MElt
translation,82,60,results,MElt,is,alVWTagger
translation,82,60,results,alVWTagger,predicts,"part- of-speech ( here , a UPOS )"
translation,82,60,results,alVWTagger,predicts,set of morphological features
translation,82,60,results,Results,has,second improvement
translation,82,146,results,"real , yet unofficial scores",are,encouraging
translation,82,146,results,Results,has,"real , yet unofficial scores"
translation,82,154,results,great,combined with,features
translation,82,190,results,UD - Pipe baseline,results in,much stronger impact
translation,82,190,results,much stronger impact,of,training corpus size
translation,82,190,results,our own tagger,leads to,more stable results
translation,82,190,results,Results,using,UD - Pipe baseline
translation,82,190,results,Results,using,our own tagger
translation,83,170,ablation-analysis,predicted AMR graph nodes,as,additional input
translation,83,170,ablation-analysis,additional input,for,next state computation
translation,83,170,ablation-analysis,BLEU score,increases from,30.49
translation,83,170,ablation-analysis,30.49,to,30.72
translation,83,170,ablation-analysis,Meteor score,reaches,35.94
translation,83,170,ablation-analysis,predicted AMR graph nodes,has,BLEU score
translation,83,170,ablation-analysis,additional input,has,BLEU score
translation,83,170,ablation-analysis,next state computation,has,BLEU score
translation,83,170,ablation-analysis,Ablation analysis,using,predicted AMR graph nodes
translation,83,172,ablation-analysis,edge prediction,leads to,performance boosts
translation,83,172,ablation-analysis,Ablation analysis,has,edge prediction
translation,83,175,ablation-analysis,node prediction and edge prediction losses ( Both Prediction ),leads to,better model performance
translation,83,175,ablation-analysis,Ablation analysis,combining,node prediction and edge prediction losses ( Both Prediction )
translation,83,184,ablation-analysis,Both types of prediction accuracies,have,strong positive correlations
translation,83,184,ablation-analysis,strong positive correlations,with,final BLEU scores
translation,83,184,ablation-analysis,combination,yields,further boost
translation,83,184,ablation-analysis,further boost,on,correlation coefficient
translation,83,184,ablation-analysis,Ablation analysis,has,Both types of prediction accuracies
translation,83,113,baselines,feature - based model,of,Zhu et al . ( 2019 )
translation,83,113,baselines,feature - based model,of,our baseline
translation,83,113,baselines,feature - based model,as,our baseline
translation,83,113,baselines,our baseline,has,)
translation,83,107,experimental-setup,standard simplifier,to preprocess,AMR graphs
translation,83,107,experimental-setup,AMR graphs,adopting,Stanford tokenizer
translation,83,107,experimental-setup,AMR graphs,adopting,Subword Tool
translation,83,107,experimental-setup,Subword Tool,to segment,text
translation,83,107,experimental-setup,text,into,subword units
translation,83,107,experimental-setup,Experimental setup,take,standard simplifier
translation,83,115,experimental-setup,hidden layers and word embeddings,are,512
translation,83,115,experimental-setup,relation embedding,is,64
translation,83,116,experimental-setup,hidden size,of,biaffine attention module
translation,83,116,experimental-setup,biaffine attention module,is,512
translation,83,116,experimental-setup,Experimental setup,has,hidden size
translation,83,117,experimental-setup,"Adam ( Kingma and Ba , 2015 )",with,learning rate
translation,83,117,experimental-setup,learning rate,of,0.5
translation,83,117,experimental-setup,0.5,for,optimization
translation,83,117,experimental-setup,Experimental setup,use,"Adam ( Kingma and Ba , 2015 )"
translation,83,118,experimental-setup,500K steps,on,single 2080 Ti GPU
translation,83,118,experimental-setup,Experimental setup,trained for,500K steps
translation,83,146,experiments,Our systems,give,significantly better results
translation,83,146,experiments,significantly better results,than,previous systems
translation,83,146,experiments,previous systems,using,different encoders
translation,83,146,experiments,different encoders,including,"LSTM ( Konstas et al. , 2017 )"
translation,83,6,model,decoder,back predicts,projected AMR graphs
translation,83,6,model,projected AMR graphs,on,target sentence
translation,83,6,model,target sentence,during,text generation
translation,83,6,model,Model,propose,decoder
translation,83,22,model,online back parsing,where,AMR graph structure
translation,83,22,model,AMR graph structure,constructed through,autoregressive sentence construction process
translation,83,22,model,Model,perform,online back parsing
translation,83,39,model,graph Transformer encoder,for,AMR encoding
translation,83,39,model,standard Transformer decoder,for,text generation
translation,83,39,model,Model,exploits,graph Transformer encoder
translation,83,78,model,edge prediction sub-task,preserve,node-to-node relations
translation,83,78,model,node-to-node relations,in,AMR graph
translation,83,78,model,AMR graph,during,text generation
translation,83,78,model,Model,has,edge prediction sub-task
translation,83,114,model,both the encoder and decoder,have,6 layers
translation,83,208,model,output,predict,input
translation,83,208,model,input,while,output
translation,83,208,model,constructed,allowing,stronger information sharing
translation,83,208,model,Model,predict,input
translation,83,130,results,node prediction task,observed that,cross entropy loss ( CE )
translation,83,130,results,node prediction task,observed that,mean squared error loss ( MSE )
translation,83,130,results,mean squared error loss ( MSE ),give,significantly better results
translation,83,130,results,significantly better results,than,baseline
translation,83,130,results,significantly better results,with,0.46 and 0.65 improvement
translation,83,130,results,0.46 and 0.65 improvement,in terms of,BLEU
translation,83,130,results,Results,For,node prediction task
translation,83,131,results,CE,gives,better result
translation,83,131,results,better result,than,MSE
translation,83,131,results,Results,has,CE
translation,83,135,results,model,using,independent relation embeddings
translation,83,135,results,model,using,shared relation embeddings
translation,83,135,results,model,with,shared relation embeddings
translation,83,135,results,model,with,shared relation embeddings
translation,83,135,results,model,gives,slightly better results
translation,83,135,results,slightly better results,with,less parameters
translation,83,135,results,model,has,model
translation,83,135,results,independent relation embeddings,has,model
translation,83,135,results,Results,Compared with,model
translation,83,150,results,proposed approach,achieves,significant ( p < 0.01 ) improvements
translation,83,150,results,significant ( p < 0.01 ) improvements,giving,BLEU scores
translation,83,150,results,BLEU scores,of,31.48 and 34.19
translation,83,150,results,31.48 and 34.19,on,LDC2015E86 and LDC2017T10
translation,83,150,results,our baseline ( G- Trans - F- Ours ),has,proposed approach
translation,83,150,results,Results,Compared with,our baseline ( G- Trans - F- Ours )
translation,83,151,results,outputs,of,our model
translation,83,151,results,our model,have,0.8 more words
translation,83,151,results,0.8 more words,than,baseline
translation,83,151,results,Results,has,outputs
translation,83,154,results,our model,gives,higher scores
translation,83,154,results,higher scores,of,concept preservation rate
translation,83,154,results,concept preservation rate,than,baseline
translation,83,154,results,concept preservation rate,with,improvements
translation,83,154,results,baseline,on,both datasets
translation,83,154,results,improvements,of,3.6 and 3.3
translation,83,154,results,Results,has,our model
translation,83,155,results,relation preservation rate,of,our model
translation,83,155,results,relation preservation rate,is,better
translation,83,155,results,better,than,baseline
translation,83,155,results,Results,has,relation preservation rate
translation,83,157,results,our model,gives,better results
translation,83,157,results,better results,than,baseline
translation,83,157,results,generation fluency,has,our model
translation,83,163,results,baseline,performs,well
translation,83,163,results,Results,has,baseline
translation,83,164,results,our approach,achieves,better results
translation,83,164,results,better results,showing,our back - parsing mechanism
translation,83,164,results,Results,has,our approach
translation,83,169,results,baseline,observe,performance improvement
translation,83,169,results,performance improvement,of,0.34 BLEU
translation,83,169,results,0.34 BLEU,by adding,node prediction loss only
translation,83,169,results,Results,Compared with,baseline
translation,83,173,results,predicted relations,for,next state computation
translation,83,173,results,predicted relations,gives,improvement
translation,83,173,results,improvement,of,0.92 BLEU
translation,83,173,results,0.92 BLEU,over,baseline
translation,83,173,results,next state computation,has,Edge Prediction ( Int . )
translation,83,173,results,Results,integrating,predicted relations
translation,83,174,results,Edge prediction,results in,larger improvements
translation,83,174,results,larger improvements,than,node prediction
translation,83,174,results,Results,has,Edge prediction
translation,83,176,results,both node and edge predictions,improves,system
translation,83,176,results,system,to,31.48 BLEU and 36.15 Meteor
translation,83,176,results,Results,Integrating,both node and edge predictions
translation,83,185,results,Results,has,Performances VS AMR Graphs Sizes
translation,84,22,ablation-analysis,largest overall impact,on,UAS
translation,84,22,ablation-analysis,Ablation analysis,show,erroneously attaching NPs
translation,84,23,ablation-analysis,NPs and punctuation,have,most substantial cascading impact
translation,84,23,ablation-analysis,Ablation analysis,has,NPs and punctuation
translation,84,136,ablation-analysis,number of constraints,is,reduced
translation,84,136,ablation-analysis,reduced,has,coverage
translation,84,136,ablation-analysis,coverage,has,drops
translation,84,138,ablation-analysis,number of constraints,permits,more states
translation,84,138,ablation-analysis,more states,which do not yet violate,constraint
translation,84,138,ablation-analysis,more states,yield,undesired states
translation,84,138,ablation-analysis,undesired states,has,later
translation,84,138,ablation-analysis,Ablation analysis,Reducing,number of constraints
translation,84,139,ablation-analysis,largest impact,on,coverage
translation,84,139,ablation-analysis,largest impact,reducing it to,93.2 %
translation,84,139,ablation-analysis,Ablation analysis,has,Punctuation constraints
translation,84,140,ablation-analysis,"NP attachments , clauses , and modifier attachments",incur,substantial coverage reductions
translation,84,140,ablation-analysis,Ablation analysis,has,"NP attachments , clauses , and modifier attachments"
translation,84,145,ablation-analysis,NP attachment constraints,causes,4.4 % drop
translation,84,145,ablation-analysis,4.4 % drop,in,coverage
translation,84,145,ablation-analysis,coverage,for,ZPar
translation,84,145,ablation-analysis,effective con-straint percentage,is,below 5 %
translation,84,145,ablation-analysis,below 5 %,for,both parsers
translation,84,145,ablation-analysis,Ablation analysis,Applying,NP attachment constraints
translation,84,24,results,particularly large impact,on,accuracy
translation,84,24,results,correct punctuation arcs,has,particularly large impact
translation,84,24,results,Results,Enforcing,correct punctuation arcs
translation,84,130,results,ZPar,is,more accurate
translation,84,134,results,ZPar,make,fewer mistakes
translation,84,134,results,fewer mistakes,across,each error category
translation,84,134,results,Results,has,ZPar
translation,84,135,results,ZPar 's coverage,is,98.5 %
translation,84,135,results,98.5 %,when applying,all constraints
translation,84,135,results,Results,has,ZPar 's coverage
translation,84,161,results,NP internal and coordination errors,when using,NP attachment constraints
translation,84,161,results,NP internal and coordination errors,when using,NP internal constraints
translation,84,161,results,MSTparser,has,ZPar
translation,84,161,results,Results,compared to,MSTparser
translation,84,176,results,ZPar,achieves,slightly larger cascaded impact
translation,84,176,results,slightly larger cascaded impact,than,MSTparser ( 0.6 % to 0.5 % )
translation,84,176,results,Results,has,ZPar
translation,84,178,results,Root attachment,make,few root attachment errors
translation,84,178,results,Both parsers,make,few root attachment errors
translation,84,178,results,MSTparser,is,less accurate
translation,84,178,results,less accurate,than,ZPar
translation,84,178,results,Root attachment,has,Both parsers
translation,84,178,results,Results,has,Root attachment
translation,84,180,results,Root errors,are,most displaced
translation,84,180,results,most displaced,of,any error class
translation,84,180,results,most displaced,at,9.3 words
translation,84,180,results,most displaced,at,9.9
translation,84,180,results,9.3 words,for,MSTparser
translation,84,180,results,9.9,for,ZPar
translation,84,180,results,Results,has,Root errors
translation,84,185,results,disproportionate impact,on,remainder of the parse
translation,84,185,results,correct punctuation,has,disproportionate impact
translation,84,185,results,Results,Enforcing,correct punctuation
translation,85,213,baselines,first baseline,is,possible worlds model
translation,85,213,baselines,WORLDS,is,possible worlds model
translation,85,213,baselines,possible worlds model,based on,Malinowski and Fritz ( 2014 )
translation,85,213,baselines,first baseline,has,WORLDS
translation,85,213,baselines,Baselines,has,first baseline
translation,85,219,baselines,first baseline,is,textonly answer selection model
translation,85,219,baselines,LSTM,is,textonly answer selection model
translation,85,219,baselines,first baseline,has,LSTM
translation,85,219,baselines,Baselines,has,first baseline
translation,85,219,baselines,Baselines,has,LSTM
translation,85,220,baselines,VQA,is,neural network
translation,85,220,baselines,neural network,for,visual question answering
translation,85,224,baselines,third baseline,is,neural network
translation,85,224,baselines,DQA,is,neural network
translation,85,224,baselines,neural network,rectifies,limitation
translation,85,224,baselines,third baseline,has,DQA
translation,85,224,baselines,Baselines,has,third baseline
translation,85,110,experiments,beam size,of,100
translation,85,110,experiments,beam size,of,100 executions
translation,85,110,experiments,100,in,semantic parser
translation,85,110,experiments,10 highest - scoring logical forms,with,beam
translation,85,110,experiments,beam,of,100 executions
translation,85,20,model,domain theory,for,task
translation,85,20,model,task,as,probabilistic program
translation,85,20,model,probabilistic program,train,joint loglinear model
translation,85,20,model,joint loglinear model,to semantically parse,questions
translation,85,20,model,questions,to,logical forms
translation,85,20,model,Model,define,domain theory
translation,85,22,model,semantic parsing,to represent,compositionality
translation,85,22,model,compositionality,in,language
translation,85,22,model,probabilistic programming,to specify,background knowledge
translation,85,22,model,linear-time approximate inference,over,environment
translation,85,22,model,Model,leverages,semantic parsing
translation,85,22,model,Model,leverages,probabilistic programming
translation,85,221,model,each image,as,vector
translation,85,221,model,vector,using,final layer
translation,85,221,model,final layer,of,pre-trained VGG19 model
translation,85,221,model,Model,represents,each image
translation,85,267,model,global features,of,logical form and executions
translation,85,267,model,global features,help,model
translation,85,267,model,model,avoid,implausible interpretations
translation,85,267,model,Model,includes,global features
translation,85,267,model,Model,avoid,implausible interpretations
translation,85,229,results,LSTM,performs,well
translation,85,229,results,Results,has,LSTM
translation,85,231,results,other neural network models,have,similar performance
translation,85,231,results,similar performance,to,LSTM
translation,85,231,results,Results,has,other neural network models
translation,85,232,results,P 3,has,outperforms
translation,85,232,results,outperforms,has,WORLDS
translation,85,232,results,Results,find that,P 3
translation,85,236,results,VQA and DQA,perform,similarly
translation,85,236,results,similarly,to,LSTM
translation,85,236,results,Results,has,VQA and DQA
translation,85,237,results,accuracies,of,WORLDS and P 3
translation,85,237,results,Results,find that,accuracies
translation,85,241,results,LSTM answer selection,improves,accuracy
translation,85,241,results,accuracy,by,9 points
translation,85,241,results,Results,find that,LSTM answer selection
translation,85,242,results,Global features,improve,accuracy
translation,85,242,results,accuracy,by,7 points
translation,85,242,results,Results,has,Global features
translation,85,260,results,P 3,trained with,labeled environments
translation,85,260,results,slightly outperforms,in,QA condition
translation,85,260,results,P 3,trained with,labeled environments
translation,85,260,results,outperforms,trained with,additional logical form labels
translation,85,260,results,prior work,trained with,additional logical form labels
translation,85,260,results,P 3,has,slightly outperforms
translation,85,260,results,labeled environments,has,outperforms
translation,85,260,results,outperforms,has,prior work
translation,85,260,results,Results,has,P 3
translation,86,183,ablation-analysis,all but the gating mechanism,make,noticeable differences
translation,86,183,ablation-analysis,noticeable differences,in,macro F 1
translation,86,184,ablation-analysis,both gating and conv,show,mixed contribution
translation,86,184,ablation-analysis,both gating and conv,improved,overall performance
translation,86,184,ablation-analysis,mixed contribution,to,each treebank
translation,86,184,ablation-analysis,overall performance,through,treebank - level component selection
translation,86,184,ablation-analysis,overall performance,has,further
translation,86,184,ablation-analysis,Ablation analysis,find that,both gating and conv
translation,86,185,ablation-analysis,conv,greatly helps identify,MWTs
translation,86,185,ablation-analysis,conv,greatly helps identify,sentence breaks
translation,86,185,ablation-analysis,MWTs,in,Hebrew ( + 34.89 Words F 1 )
translation,86,185,ablation-analysis,sentence breaks,in,Ancient Greek - PROIEL ( + 18.77 Sents F 1 )
translation,86,185,ablation-analysis,Ablation analysis,has,surprising discovery
translation,86,192,ablation-analysis,added parameters,not affect,UPOS performance significantly
translation,86,192,ablation-analysis,added parameters,help improve,XPOS and UFeats prediction
translation,86,195,ablation-analysis,three individual components,in,lemmatizer
translation,86,198,ablation-analysis,slightly decreases,improves,performance
translation,86,198,ablation-analysis,F 1 score,on,small treebanks
translation,86,198,ablation-analysis,performance,on,lowresource languages
translation,86,198,ablation-analysis,overall gain,of,0.11 F 1
translation,86,198,ablation-analysis,edit classifier,has,slightly decreases
translation,86,198,ablation-analysis,slightly decreases,has,F 1 score
translation,86,198,ablation-analysis,lowresource languages,has,substantially
translation,86,198,ablation-analysis,substantially,has,+ 0.91 F 1 )
translation,86,198,ablation-analysis,Ablation analysis,adding,edit classifier
translation,86,206,ablation-analysis,both terms,contribute,significantly
translation,86,206,ablation-analysis,significantly,to,final parser performance
translation,86,206,ablation-analysis,distance term,contributing,slightly more
translation,86,132,hyperparameters,tokenizer / sentence segmenter,uses,BiLSTMs
translation,86,132,hyperparameters,tokenizer / sentence segmenter,takes,32d character embeddings
translation,86,132,hyperparameters,BiLSTMs,with,64d hidden states
translation,86,132,hyperparameters,64d hidden states,in,each direction
translation,86,132,hyperparameters,32d character embeddings,as,input
translation,86,132,hyperparameters,Hyperparameters,has,tokenizer / sentence segmenter
translation,86,133,hyperparameters,training,employ,dropout
translation,86,133,hyperparameters,dropout,to,input embeddings and hidden states
translation,86,133,hyperparameters,input embeddings and hidden states,at,each layer
translation,86,133,hyperparameters,each layer,with,p = .33
translation,86,133,hyperparameters,Hyperparameters,During,training
translation,86,134,hyperparameters,input unit,with,special < UNK > unit
translation,86,134,hyperparameters,special < UNK > unit,with,p = .33
translation,86,134,hyperparameters,Hyperparameters,randomly replace,input unit
translation,86,135,hyperparameters,noise,to,gating mechanism
translation,86,135,hyperparameters,gating mechanism,by randomly setting,gates
translation,86,135,hyperparameters,gates,to,1
translation,86,135,hyperparameters,1,with,p = .02
translation,86,135,hyperparameters,temperature,to,2
translation,86,135,hyperparameters,temperature,to make,model
translation,86,135,hyperparameters,more robust,to,tokenization errors
translation,86,135,hyperparameters,tokenization errors,at,test time
translation,86,135,hyperparameters,model,has,more robust
translation,86,135,hyperparameters,Hyperparameters,add,noise
translation,86,136,hyperparameters,Optimization,performed with,"Adam ( Kingma and Ba , 2015 )"
translation,86,136,hyperparameters,"Adam ( Kingma and Ba , 2015 )",with,initial learning rate
translation,86,136,hyperparameters,initial learning rate,of,.002
translation,86,136,hyperparameters,.002,for,"up to 20,000 steps"
translation,86,136,hyperparameters,Hyperparameters,has,Optimization
translation,86,137,hyperparameters,convolutional component,use,filter sizes
translation,86,137,hyperparameters,filter sizes,of,1 and 9
translation,86,137,hyperparameters,size,use,64 channels
translation,86,137,hyperparameters,Hyperparameters,For,convolutional component
translation,86,139,hyperparameters,MWT expander,use,BiLSTMs
translation,86,139,hyperparameters,MWT expander,use,dropout rate p = .5
translation,86,139,hyperparameters,BiLSTMs,with,256d hidden states
translation,86,139,hyperparameters,BiLSTMs,with,512d LSTM decoder
translation,86,139,hyperparameters,BiLSTMs,with,64d character embeddings
translation,86,139,hyperparameters,BiLSTMs,with,dropout rate p = .5
translation,86,139,hyperparameters,256d hidden states,in,each direction
translation,86,139,hyperparameters,each direction,as,encoder
translation,86,139,hyperparameters,dropout rate p = .5,for,inputs and hidden states
translation,86,139,hyperparameters,Hyperparameters,For,MWT expander
translation,86,140,hyperparameters,100 epochs,with,standard Adam hyperparameters
translation,86,140,hyperparameters,15 th epoch,by,factor
translation,86,140,hyperparameters,factor,of,0.9
translation,86,140,hyperparameters,Hyperparameters,has,Models
translation,86,140,hyperparameters,Hyperparameters,has,learning rate
translation,86,141,hyperparameters,Beam search,of,beam size 8
translation,86,141,hyperparameters,Hyperparameters,has,Beam search
translation,86,142,hyperparameters,lemmatizer,uses,BiLSTMs
translation,86,142,hyperparameters,BiLSTMs,with,100d hidden states
translation,86,142,hyperparameters,BiLSTMs,with,dropout rate p = .5
translation,86,142,hyperparameters,100d hidden states,in,each direction
translation,86,142,hyperparameters,each direction,of,encoder
translation,86,142,hyperparameters,50d character embeddings,as,input
translation,86,142,hyperparameters,dropout rate p = .5,for,inputs and hidden states
translation,86,142,hyperparameters,Hyperparameters,has,lemmatizer
translation,86,143,hyperparameters,decoder,is,LSTM
translation,86,143,hyperparameters,LSTM,with,200d hidden states
translation,86,143,hyperparameters,Hyperparameters,has,decoder
translation,86,144,hyperparameters,training,jointly minimize,cross-entropy loss
translation,86,144,hyperparameters,training,jointly minimize,negative log-likelihood loss
translation,86,144,hyperparameters,cross-entropy loss,of,edit classifier
translation,86,144,hyperparameters,negative log-likelihood loss,of,seq2seq lemmatizer
translation,86,144,hyperparameters,Hyperparameters,During,training
translation,86,145,hyperparameters,Models,trained up to 60 epochs,standard Adam hyperparameters
translation,86,145,hyperparameters,Hyperparameters,has,Models
translation,86,147,hyperparameters,Hyperparameters,use,75d uncased frequent word and lemma embeddings
translation,86,148,hyperparameters,transformed,to be,125d
translation,86,148,hyperparameters,Hyperparameters,has,Pretrained embeddings and character - based word representations
translation,86,149,hyperparameters,all embeddings,randomly replaced with,< drop > symbol
translation,86,149,hyperparameters,< drop > symbol,with,p = .33
translation,86,149,hyperparameters,training,has,all embeddings
translation,86,149,hyperparameters,Hyperparameters,During,training
translation,86,150,hyperparameters,2 - layer 200d BiL -STMs,for,tagger
translation,86,150,hyperparameters,3 - layer 400d BiLSTMs,for,parser
translation,86,150,hyperparameters,Hyperparameters,use,2 - layer 200d BiL -STMs
translation,86,150,hyperparameters,Hyperparameters,use,3 - layer 400d BiLSTMs
translation,86,151,hyperparameters,dropout,in,all feedforward connections
translation,86,151,hyperparameters,dropout,in,"all recurrent connections ( Gal and Ghahramani , 2016 )"
translation,86,151,hyperparameters,all feedforward connections,with,p = .5
translation,86,151,hyperparameters,all feedforward connections,with,p = .25
translation,86,151,hyperparameters,all feedforward connections,with,p = .25
translation,86,151,hyperparameters,"all recurrent connections ( Gal and Ghahramani , 2016 )",with,p = .25
translation,86,151,hyperparameters,Hyperparameters,employ,dropout
translation,86,151,hyperparameters,Hyperparameters,employ,"all recurrent connections ( Gal and Ghahramani , 2016 )"
translation,86,152,hyperparameters,classifiers,use,400d FC layers
translation,86,152,hyperparameters,400d FC layers,with,ReLU nonlinearity
translation,86,152,hyperparameters,Hyperparameters,has,classifiers
translation,86,5,model,neural pipeline system,takes,raw text
translation,86,5,model,neural pipeline system,performs,all tasks
translation,86,5,model,raw text,as,input
translation,86,5,model,all tasks,required by,shared task
translation,86,5,model,all tasks,ranging from,dependency parsing
translation,86,5,model,Model,introduce,neural pipeline system
translation,86,13,model,CoNLL 2018 UD Shared Task,built,raw-textto-CoNLL - U pipeline system
translation,86,13,model,raw-textto-CoNLL - U pipeline system,that performs,all tasks
translation,86,13,model,all tasks,required by,Shared Task
translation,86,14,model,competitive performance,in,stages
translation,86,88,model,concatenated encoder final states,put through,FC layer
translation,86,88,model,concatenated encoder final states,fed into,3 way classifier
translation,86,88,model,FC layer,with,ReLU nonlinearity
translation,86,88,model,Model,has,concatenated encoder final states
translation,86,138,model,convolutional outputs,concatenated in,hidden layer
translation,86,138,model,convolutional outputs,before,affine transform
translation,86,138,model,residual connection,for,BiLSTM
translation,86,138,model,Model,has,convolutional outputs
translation,86,156,results,our system,achieves,competitive performance
translation,86,156,results,competitive performance,on,nearly all of the metrics
translation,86,156,results,competitive performance,when,macro- averaged
translation,86,156,results,macro- averaged,over,all treebanks
translation,86,157,results,top performance,on,several metrics
translation,86,157,results,top performance,evaluated only on,big treebanks
translation,86,157,results,several metrics,evaluated only on,big treebanks
translation,86,157,results,big treebanks,showing,our systems
translation,86,157,results,our systems,effectively leverage,statistical patterns
translation,86,157,results,statistical patterns,in,data
translation,86,158,results,our system,achieved,competitive results
translation,86,158,results,competitive results,on,metrics
translation,86,160,results,our UFeats classifier,is,very accurate
translation,86,160,results,Results,find that,our UFeats classifier
translation,86,161,results,top performance,on,UFeats F 1
translation,86,161,results,parser,achieve,top MLAS
translation,86,161,results,helped,has,parser
translation,86,162,results,contribution,from,consistency modeling
translation,86,162,results,consistency modeling,in,POS tagger / UFeats classifier
translation,86,162,results,"individual metrics ( UPOS , XPOS , and UFeats )",achieve,lower advantage margin
translation,86,162,results,lower advantage margin,over,reference systems
translation,86,162,results,lower advantage margin,compared to,AllTags metric
translation,86,162,results,Results,note,contribution
translation,86,163,results,biggest disparity,between,all-treebanks and big-treebanks results
translation,86,163,results,all-treebanks and big-treebanks results,comes from,sentence segmentation
translation,86,163,results,Results,has,biggest disparity
translation,86,186,results,seq2seq,helps with,word segmentation performance
translation,86,186,results,word segmentation performance,on,all treebanks
translation,86,186,results,Results,case of,seq2seq
translation,86,193,results,biaffine classifier,is,markedly more consistent
translation,86,193,results,markedly more consistent,than,affine one
translation,86,193,results,affine one,with,shared representations
translation,86,193,results,Results,has,biaffine classifier
translation,86,196,results,our lemmatizer,with,all components
translation,86,196,results,our lemmatizer,achieves,best overall performance
translation,86,196,results,Results,find that,our lemmatizer
translation,86,197,results,"neural components ( i.e. , edit & seq2seq )",drastically improves,overall lemmatization performance
translation,86,197,results,overall lemmatization performance,over,simple dictionary - based approach ( + 6.77 F 1 )
translation,86,197,results,Results,adding,"neural components ( i.e. , edit & seq2seq )"
translation,86,209,results,parser,benefits from,improved POS / UFeats tagger performance
translation,87,190,ablation-analysis,GENIA,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,GENIA,parsing,GENIA20
translation,87,190,ablation-analysis,GENIA,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,parsing,with,GENIA10 - trained parser
translation,87,190,ablation-analysis,parsing,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,parsing,parsing,GENIA20
translation,87,190,ablation-analysis,parsing,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,GENIA10,with,GENIA10 - trained parser
translation,87,190,ablation-analysis,GENIA10,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,GENIA10,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,harms,enhances,performance ( 53.23 vs. 50.00 )
translation,87,190,ablation-analysis,GENIA20,with,GENIA20 - trained parser
translation,87,190,ablation-analysis,GENIA20,enhances,performance ( 53.23 vs. 50.00 )
translation,87,190,ablation-analysis,GENIA20 - trained parser,enhances,performance ( 53.23 vs. 50.00 )
translation,87,190,ablation-analysis,parsing,has,GENIA10
translation,87,190,ablation-analysis,GENIA10 - trained parser,has,harms
translation,87,190,ablation-analysis,harms,has,performance ( 45.28 vs. 60.23 )
translation,87,190,ablation-analysis,Ablation analysis,For,GENIA
translation,87,190,ablation-analysis,Ablation analysis,parsing,GENIA20
translation,87,205,ablation-analysis,confidence - based ZL,leads to,performance degradation
translation,87,205,ablation-analysis,performance degradation,on,HT
translation,87,205,ablation-analysis,BROWN,has,confidence - based ZL
translation,87,205,ablation-analysis,confidence - based ZL,has,gener-ally
translation,87,205,ablation-analysis,Ablation analysis,For,BROWN
translation,87,17,results,parser,trained with,entire WSJ corpus
translation,87,17,results,F-score performance,on,"WSJ10 , WSJ20 and the entire WSJ corpora"
translation,87,17,results,"WSJ10 , WSJ20 and the entire WSJ corpora",are,"76 , 64.8 and 56.7"
translation,87,17,results,entire WSJ corpus,has,F-score performance
translation,87,17,results,Results,When,parser
translation,87,165,results,all test corpora and sizes,of,high quality training subset ( N H )
translation,87,165,results,zoomed learning,improves,parser performance
translation,87,165,results,all test corpora and sizes,has,zoomed learning
translation,87,165,results,high quality training subset ( N H ),has,zoomed learning
translation,87,165,results,Results,For,all test corpora and sizes
translation,87,166,results,ZL,improves,parser performance
translation,87,166,results,parser performance,by,1.13 % ( WSJ )
translation,87,166,results,parser performance,by,1.46 %
translation,87,166,results,4.47 %,has,GENIA )
translation,87,166,results,Results,has,ZL
translation,87,167,results,most substantial improvement,pro-vided by,EZL
translation,87,167,results,most substantial improvement,pro-vided by,EZL
translation,87,167,results,best results,for,some N H values
translation,87,167,results,best results,for,others
translation,87,167,results,some N H values,achieved by,BZL
translation,87,167,results,others,by,EZL
translation,87,167,results,WSJ,has,most substantial improvement
translation,87,167,results,BROWN and GENIA,has,best results
translation,87,167,results,Results,For,WSJ
translation,87,167,results,Results,for,BROWN and GENIA
translation,87,168,results,zoomed learning with random selection ( RZL ),improves,parser performance
translation,87,168,results,parser performance,on,entire test corpus
translation,87,168,results,all three corpora,has,zoomed learning with random selection ( RZL )
translation,87,168,results,Results,for,all three corpora
translation,87,188,results,WSJ10,with,WSJ10 - trained parser
translation,87,188,results,F-score results,are,59.29
translation,87,188,results,fully - trained parser,is,76.00
translation,87,188,results,WSJ10,has,F-score results
translation,87,188,results,WSJ10 - trained parser,has,F-score results
translation,87,188,results,Results,When parsing,WSJ10
translation,87,199,results,Oracle selection,superior for,lower quality subset
translation,87,199,results,Oracle selection,inferior for,high quality subset
translation,87,199,results,Results,has,Oracle selection
translation,87,200,results,confidence - based ZL ( BZL and EZL ),provides,substantial improvement
translation,87,200,results,substantial improvement,for,LT
translation,87,200,results,Results,has,confidence - based ZL ( BZL and EZL )
translation,87,201,results,F-score improvement,up to,1.32 % ( WSJ10 )
translation,87,201,results,F-score improvement,up to,3.13 % ( WSJ20 )
translation,87,201,results,F-score improvement,up to,3.35 % ( WSJ40 )
translation,87,201,results,F-score improvement,up to,3.23 % ( the entire WSJ )
translation,87,201,results,WSJ,has,F-score improvement
translation,87,201,results,Results,For,WSJ
translation,87,202,results,improvement,up to,1.42 %
translation,87,202,results,BROWN,has,improvement
translation,87,202,results,Results,For,BROWN
translation,87,203,results,confidence - based ZL,is,less effective
translation,87,203,results,HT,has,confidence - based ZL
translation,87,203,results,Results,For,HT
translation,87,204,results,EZL,leads to,small improvement
translation,87,204,results,small improvement,on,HT
translation,87,204,results,BZL,leads to,performance degradation
translation,87,204,results,WSJ and its subcorpora,has,EZL
translation,87,206,results,EZL and BZL,improve,parser performance
translation,87,206,results,parser performance,on,LT and HT
translation,87,206,results,LT and HT,for,most N H values
translation,87,206,results,GENIA,has,EZL and BZL
translation,87,206,results,Results,For,GENIA
translation,87,214,results,oracle selection,is,dramatically better
translation,87,214,results,dramatically better,than,confidence - based selection
translation,87,214,results,low quality test subset,has,oracle selection
translation,87,214,results,Results,For,low quality test subset
translation,87,217,results,Oracle- based and confidence - based zoomed learning,demonstrate,same trend
translation,87,217,results,improve,over,baseline
translation,87,217,results,baseline,for,LT
translation,87,217,results,LT,much more than for,HT
translation,87,217,results,same trend,has,improve
translation,87,217,results,Results,has,Oracle- based and confidence - based zoomed learning
translation,87,218,results,oracle - based ZL,even,harms
translation,87,218,results,BZL,does not benefit from,averaging effect of EZL
translation,87,218,results,HT,has,oracle - based ZL
translation,87,218,results,harms,has,results
translation,87,218,results,Results,For,HT
translation,87,221,results,Integration,of,experimental results
translation,87,221,results,experimental results,for,zoomed learning
translation,87,221,results,zoomed learning,with,three selection methods
translation,87,221,results,zoomed learning,leads to,important conclusion
translation,87,221,results,Results,for,zoomed learning
translation,87,221,results,Results,has,Integration
translation,87,222,results,confidence score,used by,zoomed learning algorithm
translation,87,222,results,performance improvement,for,low quality test subset
translation,87,222,results,performance improvement,on,high quality subset
translation,87,222,results,more substantial degradation,in,performance
translation,87,222,results,performance,on,high quality subset
translation,87,222,results,more accurate,has,confidence score
translation,87,222,results,more accurate,has,more substantial
translation,87,222,results,confidence score,has,more substantial
translation,87,222,results,zoomed learning algorithm,has,more substantial
translation,87,222,results,Results,has,more accurate
translation,88,132,baselines,high-quality latent - variable parser,retrained on,our Switchboard data
translation,88,111,experimental-setup,encoder and decoder,are,3 - layer deep LSTM - RNNs
translation,88,111,experimental-setup,3 - layer deep LSTM - RNNs,with,256 hidden units
translation,88,111,experimental-setup,256 hidden units,in,each layer
translation,88,111,experimental-setup,Experimental setup,has,Model Training and Inference
translation,88,112,experimental-setup,convolution operation,uses,5 filters
translation,88,112,experimental-setup,location - aware attention,has,convolution operation
translation,88,112,experimental-setup,5 filters,has,of width 40 each
translation,88,112,experimental-setup,Experimental setup,For,location - aware attention
translation,88,115,experimental-setup,number of configurations,explored,acoustic-prosodic features
translation,88,115,experimental-setup,number of configurations,for,acoustic-prosodic features
translation,88,115,experimental-setup,number of configurations,tuning based on,dev set parsing performance
translation,88,115,experimental-setup,Experimental setup,has,number of configurations
translation,88,116,experimental-setup,Pause embeddings,tuned over,"{ 4 , 16 , 32 } dimensions"
translation,88,116,experimental-setup,Experimental setup,has,Pause embeddings
translation,88,117,experimental-setup,CNN,try,different configurations
translation,88,117,experimental-setup,different configurations,of,"filter widths w ? { [ 10 , 25 , 50 ] , [ 5 , 10 , 25 , 50 ]"
translation,88,117,experimental-setup,Experimental setup,For,CNN
translation,88,119,experimental-setup,f0 and energy phenomena,on,various levels
translation,88,119,experimental-setup,"w = 5 , 10",for,sub-word
translation,88,119,experimental-setup,w = 25,for,word
translation,88,119,experimental-setup,w = 50,for,word and extended context
translation,88,119,experimental-setup,Experimental setup,has,filter size
translation,88,120,experimental-setup,best model,uses,32 - dimensional pause embeddings
translation,88,120,experimental-setup,best model,uses,N = 32 filters
translation,88,120,experimental-setup,N = 32 filters,of,"widths w = [ 5 , 10 , 25 , 50 ]"
translation,88,120,experimental-setup,Experimental setup,has,best model
translation,88,121,experimental-setup,optimization,use,"Adam ( Kingma and Ba , 2014 )"
translation,88,121,experimental-setup,"Adam ( Kingma and Ba , 2014 )",with,minibatch size
translation,88,121,experimental-setup,minibatch size,of,64
translation,88,121,experimental-setup,Experimental setup,For,optimization
translation,88,122,experimental-setup,initial learning rate,is,0.001
translation,88,122,experimental-setup,0.001,decayed by,factor
translation,88,122,experimental-setup,factor,of,0.9
translation,88,122,experimental-setup,training loss,has,degrades
translation,88,122,experimental-setup,Experimental setup,has,initial learning rate
translation,88,123,experimental-setup,up to 50 epochs,with,early stopping
translation,88,124,experimental-setup,dropout,with,0.3 probability
translation,88,124,experimental-setup,0.3 probability,applied on,output
translation,88,124,experimental-setup,output,of,all LSTM layers
translation,88,124,experimental-setup,regularization,has,dropout
translation,88,124,experimental-setup,Experimental setup,For,regularization
translation,88,125,experimental-setup,inference,use,greedy decoder
translation,88,125,experimental-setup,greedy decoder,to generate,linearized parse
translation,88,125,experimental-setup,Experimental setup,For,inference
translation,88,128,experimental-setup,"TensorFlow ( Abadi et al. , 2015 )",to implement,all models
translation,88,144,model,CL -attn model,with,three kinds of acoustic-prosodic features
translation,88,144,model,CNN mappings,of,fundamental frequency ( f0 ) and energy ( E ) features ( f0/E - CNN )
translation,88,144,model,Model,extend,CL -attn model
translation,88,134,results,con-tent+location attention,beats,Berkeley parser
translation,88,134,results,Berkeley parser,by about,2.5 %
translation,88,134,results,F1,has,con-tent+location attention
translation,88,134,results,content-only attention,has,by about 4.5 %
translation,88,134,results,Results,In terms of,F1
translation,88,135,results,Flat - F1 scores,for,both encoder-decoder models
translation,88,135,results,both encoder-decoder models,lower than,corresponding F1 scores
translation,88,135,results,Results,has,Flat - F1 scores
translation,88,145,results,performance,over,text-only baseline
translation,88,147,results,text + p + ? + f0/E-CNN model,uses,all three types of features
translation,88,147,results,best performance,with,gain
translation,88,147,results,gain,of,0.7 %
translation,88,147,results,0.7 %,over,already - strong text-only baseline
translation,88,147,results,all three types of features,has,best performance
translation,88,147,results,Results,has,text + p + ? + f0/E-CNN model
translation,88,154,results,acoustic-prosodic features,improves over,text-only baseline
translation,88,154,results,Results,adding,acoustic-prosodic features
translation,88,166,results,Both models,do,worse
translation,88,166,results,worse,on,longer sentences
translation,88,166,results,Results,has,Both models
translation,88,167,results,performance difference,between,our best model and the text-only model
translation,88,167,results,our best model and the text-only model,increases with,sentence length
translation,88,167,results,Results,has,performance difference
translation,88,170,results,parse scores,on,subsets
translation,88,170,results,subsets,of,fluent and disfluent sentences
translation,88,170,results,performance gain,in,disfluent set
translation,88,170,results,Results,presents,parse scores
translation,88,197,results,oracle disfluencies,as,features
translation,88,197,results,oracle disfluencies,improve,CL-attn text-only parsing performance
translation,88,197,results,Results,Experiments with,oracle disfluencies
translation,89,25,results,parsing and machine translation of dependency parsers,trained with,annotated noun phrase structure
translation,90,8,experiments,discourse parser,is,simpler
translation,90,8,experiments,discourse parser,to,state of the art
translation,90,8,experiments,state of the art,for,English
translation,90,8,experiments,harmonization,of,discourse treebanks
translation,90,8,experiments,discourse treebanks,across,languages
translation,90,33,model,significantly better,than,existing parsers
translation,90,33,model,existing parsers,for,English
translation,90,33,model,English,on,2/3 standard metrics
translation,90,33,model,Model,propose,new discourse parser
translation,91,15,model,Model,present,higher - order constituent parsing
translation,91,16,model,multiple adjacent grammar rules,in,each part
translation,91,16,model,multiple adjacent grammar rules,to utilize,more local structural context
translation,91,16,model,each part,of,parse tree
translation,91,16,model,more local structural context,to decide,plausibility
translation,91,16,model,plausibility,of,grammar rule instance
translation,91,16,model,Model,allows,multiple adjacent grammar rules
translation,91,17,results,PTB WSJ and Chinese Treebank,achieves,best F1 scores
translation,91,17,results,best F1 scores,of,91.86 % and 85.58 %
translation,91,17,results,Results,Evaluated on,PTB WSJ and Chinese Treebank
translation,92,145,experimental-setup,networks,in,"TensorFlow ( Abadi et al. , 2015 )"
translation,92,145,experimental-setup,Experimental setup,implement,networks
translation,92,146,experimental-setup,training,shuffle,order
translation,92,146,experimental-setup,order,of,sentences
translation,92,146,experimental-setup,order,to form,mini-batches
translation,92,146,experimental-setup,sentences,in,training set
translation,92,146,experimental-setup,Experimental setup,During,training
translation,92,152,experimental-setup,each direction,of,LSTM computation
translation,92,152,experimental-setup,each direction,of,LSTM
translation,92,152,experimental-setup,LSTM computation,involves,two layers
translation,92,152,experimental-setup,supertagger,has,each direction
translation,92,152,experimental-setup,Experimental setup,For,supertagger
translation,92,153,experimental-setup,"hidden units , layerto-layer , and input units dropout rates",are,"0.5 , 0.5 , and 0.2"
translation,92,153,experimental-setup,Experimental setup,has,"hidden units , layerto-layer , and input units dropout rates"
translation,92,156,experimental-setup,parser,initialize,supertag embedding matrix E
translation,92,156,experimental-setup,parser,initialize,substitution memory embedding matrix M
translation,92,156,experimental-setup,substitution memory embedding matrix M,according to,"Uniform ( ? 1 ? d , 1 ? d )"
translation,92,156,experimental-setup,Experimental setup,For,parser
translation,92,157,experimental-setup,embedding dimensions d,for,supertag and substitution memory embeddings
translation,92,157,experimental-setup,supertag and substitution memory embeddings,are,50
translation,92,157,experimental-setup,number of units,is,200
translation,92,157,experimental-setup,200,on,both of the two hidden layers
translation,92,157,experimental-setup,input dropout rate,is,0.2
translation,92,157,experimental-setup,input dropout rate,is,0.3
translation,92,157,experimental-setup,input dropout rate,is,0.3
translation,92,157,experimental-setup,hidden dropout rate,is,0.3
translation,92,158,experimental-setup,k = 3 or 5,for,stack / buffer scope
translation,92,158,experimental-setup,Experimental setup,choose,k = 3 or 5
translation,92,255,experimental-setup,"Syntaxnet ( Andor et al. , 2016 )",with,global normalization beam size 16
translation,92,255,experimental-setup,global normalization beam size 16,using,TensorFlow toolkit
translation,92,255,experimental-setup,Experimental setup,trained,"Syntaxnet ( Andor et al. , 2016 )"
translation,92,77,hyperparameters,word embeddings,to be,pre-trained GloVe vectors
translation,92,77,hyperparameters,word embeddings,for,words
translation,92,77,hyperparameters,embedding,to,zero vector
translation,92,77,hyperparameters,Hyperparameters,initialize,word embeddings
translation,92,77,hyperparameters,Hyperparameters,for,words
translation,92,78,hyperparameters,other embeddings,are,randomly initialized
translation,92,78,hyperparameters,Hyperparameters,has,other embeddings
translation,92,23,model,robust supertaggingbased parsing of TAG,by using,dense representation
translation,92,23,model,possible,by using,dense representation
translation,92,23,model,dense representation,of,supertags
translation,92,23,model,dense representation,induced using,neural networks
translation,92,23,model,Model,show that,robust supertaggingbased parsing of TAG
translation,92,24,model,neural network supertagger,based on,bi-directional LSTM ( BLSTM ) architecture
translation,92,24,model,Model,present,neural network supertagger
translation,92,27,model,shift-reduce parsing model,based on,feedforward neural network
translation,92,27,model,feedforward neural network,makes use of,dense supertag embeddings
translation,92,27,model,Model,present,shift-reduce parsing model
translation,92,161,results,on Section 00 supertagging accuracy,of,"89.32 % , 90.67 %"
translation,92,161,results,"89.32 % , 90.67 %",disregard,177 sentences
translation,92,161,results,177 sentences,that contain,unseen supertag
translation,92,161,results,Results,achieve,on Section 00 supertagging accuracy
translation,92,184,results,all other models,besides,Word Vector model
translation,92,184,results,supertagger,has,outperforms
translation,92,184,results,outperforms,has,all other models
translation,92,184,results,Results,has,supertagger
translation,92,194,results,neural network parser,achieves,state - of - the - art performance
translation,92,194,results,state - of - the - art performance,compared to,parsers
translation,92,194,results,parsers,make use of,lexical information
translation,92,194,results,parsers,make use of,POS tags
translation,92,194,results,parsers,make use of,hand-engineered features
translation,92,194,results,Results,see that,combination of the BLSTM supertagger
translation,92,195,results,neural network parser,with,beam size 16
translation,92,195,results,slightly better,than,chart parser
translation,92,195,results,gold supertags,has,neural network parser
translation,92,195,results,Results,With,gold supertags
translation,92,196,results,"SyntaxNet ( Andor et al. , 2016 )",with,computationally expensive global normalization
translation,92,196,results,supertag-based parser,has,outperforms
translation,92,196,results,outperforms,has,"SyntaxNet ( Andor et al. , 2016 )"
translation,92,215,results,3 types of analogies,particularly,type A - 3
translation,92,215,results,Results,for,3 types of analogies
translation,93,132,ablation-analysis,daughter features,in conjunction with,grand - daughter
translation,93,132,ablation-analysis,daughter features,causes,big drop
translation,93,132,ablation-analysis,big drop,in,performance
translation,93,132,ablation-analysis,performance,for,vast majority of cases
translation,93,132,ablation-analysis,Ablation analysis,removing,daughter features
translation,93,139,ablation-analysis,small but constant drop,in,performance
translation,93,153,ablation-analysis,no grand- daughter features,diminish,dimension
translation,93,153,ablation-analysis,dimension,of,input vector
translation,93,153,ablation-analysis,input vector,from,1860 dimensions to 1420
translation,93,153,ablation-analysis,reduction,has,of ?23 %
translation,93,155,ablation-analysis,input vector,by,?71 %
translation,93,155,ablation-analysis,speed,increased by,factor of 2.5
translation,93,155,ablation-analysis,Ablation analysis,skip,daughter features
translation,93,79,experimental-setup,stochastic gradient descent ( SGD ),with,exponential decay
translation,93,79,experimental-setup,exponential decay,has,lr = 0.02
translation,93,80,experimental-setup,Dropout,set to,50 %
translation,93,80,experimental-setup,Experimental setup,has,Dropout
translation,93,82,experimental-setup,internal embeddings,initialized according to,"Glorot uniform ( Glorot and Bengio , 2010 )"
translation,93,82,experimental-setup,oracle,during,training phase
translation,93,82,experimental-setup,Experimental setup,used,internal embeddings
translation,93,111,experimental-setup,test set,on,single thread
translation,93,111,experimental-setup,single thread,on,Intel ( R ) Core ( TM ) i7-7700 CPU @ 3.60 GHz
translation,93,111,experimental-setup,Experimental setup,run on,test set
translation,93,124,results,ARC -STANDARD algorithm,for,47 out of 52 treebanks
translation,93,124,results,no significant accuracy loss,with respect to,de facto standard features
translation,93,124,results,Results,In the case of,ARC -STANDARD algorithm
translation,93,131,results,Results,has,Impact of the NO- GD / D features
translation,93,154,results,average thousands of tokens parsed per second,of,de facto standard features
translation,93,154,results,average,obtained without,grand- daughter features
translation,93,154,results,grand- daughter features,was,3.7
translation,93,154,results,Results,has,average thousands of tokens parsed per second
translation,94,233,ablation-analysis,last two data points,using,Double - DOP model
translation,94,233,ablation-analysis,treebank,gets,too refined
translation,94,233,ablation-analysis,Double - DOP model,improves,accuracy
translation,94,233,ablation-analysis,last two data points,has,treebank
translation,94,233,ablation-analysis,Ablation analysis,For,last two data points
translation,94,5,model,syntactic fragments,of,arbitrary size
translation,94,5,model,syntactic fragments,from,treebank
translation,94,5,model,syntactic fragments,to analyze,new sentences
translation,94,5,model,arbitrary size,from,treebank
translation,94,7,model,parsing,define,transform - backtransform approach
translation,94,7,model,parsing,to use,standard PCFG technology
translation,94,7,model,transform - backtransform approach,to use,standard PCFG technology
translation,94,7,model,Model,For,parsing
translation,94,19,model,novel DOP model ( Double - DOP ),extract,restricted yet representative subset of fragments
translation,94,19,model,recurring at least twice,in,treebank
translation,94,19,model,Model,present,novel DOP model ( Double - DOP )
translation,94,20,model,explicit representation,of,fragments
translation,94,20,model,probabilistic models,on top of,symbolic grammar
translation,94,20,model,Model,has,explicit representation
translation,94,26,model,simple transformation,of,extracted fragments
translation,94,26,model,simple transformation,to use,offthe-shelf PCFG parsing and inference
translation,94,26,model,extracted fragments,into,CFG - rules
translation,94,26,model,extracted fragments,to use,offthe-shelf PCFG parsing and inference
translation,94,26,model,Model,present,simple transformation
translation,94,27,model,Double - DOP,with,recent state-splitting approaches
translation,94,27,model,Double - DOP,yielding,even more accurate parser
translation,94,27,model,Model,integrate,Double - DOP
translation,94,240,model,all constructions,has,recurring
translation,94,240,model,recurring,has,at least twice
translation,94,222,results,RFE,is,best estimator ( 87.2 F1 13 )
translation,94,222,results,best estimator ( 87.2 F1 13 ),followed by,EWE ( 86.8 )
translation,94,222,results,best estimator ( 87.2 F1 13 ),followed by,ML ( 86.6 )
translation,94,222,results,Results,find that,RFE
translation,95,104,ablation-analysis,POS - tag information,plays,most important role
translation,95,104,ablation-analysis,most important role,in,our model
translation,95,106,ablation-analysis,s and r vectors,introduction of,stack information
translation,95,106,ablation-analysis,stack information,strongly improve,parsing performance
translation,95,106,ablation-analysis,Ablation analysis,For,s and r vectors
translation,95,65,experimental-setup,vocabulary,to contain,up to 20 k most frequent words
translation,95,65,experimental-setup,remaining words,into,< unk > token
translation,95,65,experimental-setup,Experimental setup,limit,vocabulary
translation,95,65,experimental-setup,Experimental setup,convert,remaining words
translation,95,68,experimental-setup,dimension,of,POS - tag / action embedding
translation,95,68,experimental-setup,dimension,of,hidden units
translation,95,68,experimental-setup,dimension,of,POS - tag / action embedding
translation,95,68,experimental-setup,word embedding,is,300
translation,95,68,experimental-setup,dimension,of,POS - tag / action embedding
translation,95,68,experimental-setup,POS - tag / action embedding,is,32
translation,95,68,experimental-setup,size,of,hidden units
translation,95,68,experimental-setup,hidden units,in,GRU
translation,95,68,experimental-setup,GRU,is,500
translation,95,68,experimental-setup,Experimental setup,has,dimension
translation,95,68,experimental-setup,Experimental setup,has,dimension
translation,95,68,experimental-setup,Experimental setup,has,size
translation,95,70,experimental-setup,300 - dimensional pretrained GloVe vectors,to initialize,word embedding matrix
translation,95,70,experimental-setup,Experimental setup,used,300 - dimensional pretrained GloVe vectors
translation,95,71,experimental-setup,Other model parameters,initialized using,normal distribution
translation,95,71,experimental-setup,normal distribution,with,mean of 0
translation,95,71,experimental-setup,normal distribution,with,variance
translation,95,71,experimental-setup,Dyer15,has,"Dyer et al. , 2015"
translation,95,71,experimental-setup,K&G16,has,"Kiperwasser and Goldberg , 2016 )"
translation,95,71,experimental-setup,DENSE,has,"Zhang et al. , 2017 )"
translation,95,71,experimental-setup,"Zhang et al. , 2017 )",has,6 /
translation,95,71,experimental-setup,6 /,has,d row + d col )
translation,95,71,experimental-setup,Experimental setup,has,Other model parameters
translation,95,72,experimental-setup,vanilla SGD algorithm,with,mini-batch size
translation,95,72,experimental-setup,vanilla SGD algorithm,with,32
translation,95,72,experimental-setup,64,for,English dataset
translation,95,72,experimental-setup,32,for,Chinese dataset
translation,95,72,experimental-setup,mini-batch size,has,64
translation,95,72,experimental-setup,Experimental setup,trained on,Tesla K40 m GPU
translation,95,73,experimental-setup,initial learning rate,set to,2
translation,95,73,experimental-setup,halved,when,unlabeled attachment scores ( UAS )
translation,95,73,experimental-setup,halved,not,increase
translation,95,73,experimental-setup,unlabeled attachment scores ( UAS ),on,development set
translation,95,73,experimental-setup,unlabeled attachment scores ( UAS ),not,increase
translation,95,73,experimental-setup,development set,do,increase
translation,95,73,experimental-setup,development set,not,increase
translation,95,73,experimental-setup,increase,for,900 batches
translation,95,73,experimental-setup,Experimental setup,has,initial learning rate
translation,95,74,experimental-setup,gradient exploding problem,rescale,gradient
translation,95,74,experimental-setup,gradient,when,norm
translation,95,74,experimental-setup,norm,exceeds,1
translation,95,74,experimental-setup,Dropout,applied to,our model
translation,95,74,experimental-setup,our model,with,strategy
translation,95,74,experimental-setup,our model,with,dropout rate
translation,95,74,experimental-setup,dropout rate,is,0.2
translation,95,74,experimental-setup,Experimental setup,To alleviate,gradient exploding problem
translation,95,75,experimental-setup,beam search,to find,best action sequence
translation,95,75,experimental-setup,best action sequence,with,beam size 8
translation,95,75,experimental-setup,testing,has,beam search
translation,95,75,experimental-setup,Experimental setup,For,testing
translation,95,5,model,stack - based multi-layer attention model,for,seq2seq learning
translation,95,5,model,stack - based multi-layer attention model,to better leverage,structural linguistics information
translation,95,5,model,Model,propose,stack - based multi-layer attention model
translation,95,6,model,two binary vectors,to track,decoding stack
translation,95,6,model,decoding stack,in,transition - based parsing
translation,95,6,model,multi-layer attention,to capture,multiple word dependencies
translation,95,6,model,multiple word dependencies,in,partial trees
translation,95,6,model,Model,has,two binary vectors
translation,95,18,model,Model,propose,stack - based multilayer attention mechanism
translation,95,19,model,stack,introduce,two binary vectors
translation,95,19,model,word,pushed into,stack
translation,95,19,model,Model,simulate,stack
translation,95,20,model,complex structural information,propose,multi-layer attention
translation,95,20,model,multi-layer attention,based on,stack information
translation,95,20,model,multi-layer attention,based on,previous action
translation,95,20,model,multi-layer attention,based on,input sentence
translation,95,20,model,Model,To model,complex structural information
translation,95,21,model,multi-layer attention,aims to capture,multiple word dependencies
translation,95,21,model,multiple word dependencies,in,partial trees
translation,95,21,model,partial trees,for,action prediction
translation,95,21,model,Model,has,multi-layer attention
translation,95,67,model,3 - layers GRU,used for,encoder and decoder
translation,95,69,model,3 - layers attention structure,adopted in,our model
translation,95,69,model,Model,has,3 - layers attention structure
translation,95,81,results,basic seq2seq model,with,1.87 UAS ( English ) and 1.61 UAS ( Chinese ) improvements
translation,95,81,results,basic seq2seq model,on,test set
translation,95,81,results,1.87 UAS ( English ) and 1.61 UAS ( Chinese ) improvements,on,test set
translation,95,81,results,our proposed model,has,significantly outperform
translation,95,81,results,significantly outperform,has,basic seq2seq model
translation,95,81,results,Results,see that,our proposed model
translation,95,83,results,our model,achieves,better UAS accuracy
translation,95,83,results,our model,achieves,slightly lower
translation,95,83,results,better UAS accuracy,than,Z&N11
translation,95,83,results,better UAS accuracy,than,C&M14
translation,95,83,results,better UAS accuracy,than,ConBSO
translation,95,83,results,better UAS accuracy,than,Dyer15
translation,95,83,results,better UAS accuracy,on,development and test set
translation,95,83,results,Results,has,our model
translation,96,32,hyperparameters,joint sentence segmentation,add,extra boundary tags
translation,96,32,hyperparameters,Hyperparameters,To enable,joint sentence segmentation
translation,96,57,hyperparameters,both the word and character models,use,two layers of BiLSTMs
translation,96,57,hyperparameters,two layers of BiLSTMs,with,300 LSTM cells per layer
translation,96,57,hyperparameters,Hyperparameters,For,both the word and character models
translation,96,58,hyperparameters,batches,with,8000 words and 20000 characters
translation,96,58,hyperparameters,Hyperparameters,employ,batches
translation,96,98,hyperparameters,all models,for,30 epochs
translation,96,98,hyperparameters,all models,with,hyper-parameter settings
translation,96,98,hyperparameters,30 epochs,with,hyper-parameter settings
translation,96,98,hyperparameters,Hyperparameters,train,all models
translation,96,30,model,input character sequence,model,prediction
translation,96,30,model,prediction,of,word boundary tags
translation,96,30,model,prediction,as,sequence labelling problem
translation,96,30,model,sequence labelling problem,using,BiRNN - CRF framework
translation,96,30,model,Model,Given,input character sequence
translation,96,81,model,parser,extended with,SWAP transition
translation,96,81,model,parser,construction of,nonprojective dependency trees
translation,96,81,model,SWAP transition,construction of,nonprojective dependency trees
translation,96,81,model,Model,has,parser
translation,96,7,results,7th of 27 teams,for,LAS and MLAS metrics
translation,96,8,results,Our system,obtained,best scores
translation,96,8,results,best scores,for,word segmentation
translation,96,8,results,best scores,for,universal POS tagging
translation,96,8,results,best scores,for,morphological features
translation,96,8,results,Results,has,Our system
translation,96,171,results,LAS scores,see that,our results
translation,96,171,results,our results,are,significantly above
translation,96,171,results,significantly above,with,especially strong result
translation,96,171,results,mean,for,"all aggregate sets of treebanks ( ALL , BIG , PUD , SMALL , LOW - RESOURCE )"
translation,96,171,results,especially strong result,for,low-resource group
translation,96,171,results,significantly above,has,mean
translation,96,171,results,Results,Looking first at,LAS scores
translation,96,171,results,Results,are,significantly above
translation,96,173,results,significantly worse,than,mean
translation,96,173,results,mean,only for,Afrikaans AfriBooms
translation,96,173,results,mean,only for,Old French SRCMF
translation,96,173,results,mean,only for,Galician CTG
translation,96,173,results,mean,only for,Latin PROIEL
translation,96,173,results,mean,only for,Portuguese Bosque
translation,96,173,results,Results,has,results
translation,96,177,results,parsing accuracy,in,remarkably consistent fashion
translation,96,177,results,in fact improve,has,parsing accuracy
translation,96,177,results,Results,merging,treebanks and languages
translation,96,178,results,parsed,with,multi-treebank model
translation,96,178,results,( marginally ) higher score,with,mono-treebank baseline model
translation,96,178,results,multi-treebank model,has,four
translation,96,178,results,four,has,( marginally ) higher score
translation,96,178,results,Results,For,64 test sets
translation,96,179,results,aggregate sets,see that,pooling of resources
translation,96,179,results,helps most,for,LOW - RESOURCE ( 25.33 vs. 17.72 )
translation,96,179,results,aggregate sets,has,pooling of resources
translation,96,179,results,pooling of resources,has,helps most
translation,96,179,results,Results,Looking at,aggregate sets
translation,96,180,results,our treebank embedding method,is,reli-able method
translation,96,180,results,Results,find,very encouraging
translation,96,180,results,Results,indicate,our treebank embedding method
translation,96,186,results,sentence segmentation,achieved,second best scores
translation,96,186,results,Uppsala system,achieved,second best scores
translation,96,186,results,mean,for,all aggregates
translation,96,186,results,all aggregates,except,SMALL
translation,96,186,results,sentence segmentation,has,Uppsala system
translation,96,186,results,significantly above,has,mean
translation,96,186,results,Results,For,sentence segmentation
translation,96,188,results,word segmentation,obtained,best results
translation,96,188,results,mean,for,all groups
translation,96,188,results,all groups,except,SMALL
translation,96,188,results,best results,has,strongly outperforming
translation,96,188,results,strongly outperforming,has,mean
translation,96,188,results,Results,For,word segmentation
translation,96,193,results,Uppsala system,ranks,first overall
translation,96,193,results,first overall,scores,more than one std dev
translation,96,193,results,more than one std dev,above,mean
translation,96,193,results,mean,for,all aggregates
translation,96,193,results,Results,has,Uppsala system
translation,96,196,results,results,for,morphological features
translation,96,196,results,less substantial improvements,over,mean
translation,96,196,results,Results,for,morphological features
translation,96,196,results,Results,has,results
translation,96,203,results,Uppsala system,ranked,second
translation,96,203,results,second,for,event extraction
translation,96,203,results,second,first for,opinion analysis
translation,96,203,results,16th ( out of 16 systems ),for,negation resolution
translation,96,203,results,extrinsic evaluation,has,Uppsala system
translation,96,204,results,Our results,for,first two tasks
translation,96,204,results,first two tasks,are,better than expected
translation,96,204,results,Results,for,first two tasks
translation,96,204,results,Results,has,Our results
translation,96,205,results,performance,is,very low
translation,96,205,results,very low,on,negation resolution task
translation,96,205,results,Results,has,performance
translation,97,153,ablation-analysis,pretrained feature extractors ( BERT ),tend to,improve
translation,97,153,ablation-analysis,improve,with respect to,two labels
translation,97,153,ablation-analysis,most,with respect to,two labels
translation,97,153,ablation-analysis,improve,has,most
translation,97,153,ablation-analysis,Ablation analysis,observe,pretrained feature extractors ( BERT )
translation,97,154,ablation-analysis,crucial,to predict,syntactic trees
translation,97,154,ablation-analysis,35 % or 29 %,of,errors
translation,97,154,ablation-analysis,errors,with or without,pretrained feature extractors
translation,97,154,ablation-analysis,syntactic trees,has,correctly
translation,97,155,ablation-analysis,Accuracy,of,( D ) - type SRL relations
translation,97,155,ablation-analysis,even larger impact,on,overall performance
translation,97,155,ablation-analysis,( D ) - type SRL relations,has,even larger impact
translation,97,155,ablation-analysis,Ablation analysis,has,Accuracy
translation,97,140,baselines,BIO,-,tagging
translation,97,140,baselines,BIO,has,tagging
translation,97,140,baselines,semi-Markov CRF,has,"Swayamdipta et al. , 2018 )"
translation,97,140,baselines,structured tuning,has,"Li et al. , 2020 )"
translation,97,141,baselines,strong BIOtagging model,trained with,CRF loss
translation,97,141,baselines,CRF loss,as,our baseline model ( BIO - CRF )
translation,97,6,model,conversion scheme,packs,SRL annotations
translation,97,6,model,SRL annotations,into,dependency tree representations
translation,97,6,model,dependency tree representations,through,joint labels
translation,97,6,model,joint labels,that permit,highly accurate recovery
translation,97,6,model,highly accurate recovery,back to,original format
translation,97,6,model,Model,present,conversion scheme
translation,97,193,model,back - and - forth conversion,to and from,joint label space
translation,97,142,results,our models,are,competitive
translation,97,142,results,competitive,with,state- ofthe - art models
translation,97,142,results,Results,show,our models
translation,97,143,results,slightly underperform,-,CRF baseline models
translation,97,143,results,BIO,-,CRF baseline models
translation,97,143,results,CRF baseline models,on,English
translation,97,143,results,larger,on,Chinese
translation,97,143,results,Our models,has,slightly underperform
translation,97,143,results,slightly underperform,has,BIO
translation,97,143,results,slightly underperform,has,CRF baseline models
translation,97,143,results,BIO,has,CRF baseline models
translation,97,143,results,Results,has,Our models
translation,97,149,results,per-label F1 scores,comparing,baseline model
translation,97,149,results,Results,presents,per-label F1 scores
translation,97,150,results,Our method,exhibits,similar overall performance
translation,97,150,results,similar overall performance,to,baseline BIO - CRF model
translation,97,150,results,Results,has,Our method
translation,97,157,results,benefits,of,pretrained feature extractors
translation,97,157,results,pretrained feature extractors,mostly stem from,improved accuracies
translation,97,157,results,improved accuracies,of,syntactic component
translation,97,157,results,Results,observe that,benefits
translation,98,171,baselines,discriminative reranker,to,our lexical models ( + rerank )
translation,98,190,baselines,GeoQuery case,compare against,two classic grammar-based models
translation,98,190,baselines,GeoQuery case,compare against,"feature rich , neural hybrid tree model ( nHT )"
translation,98,192,baselines,grammar,with,complex wordorder constraints
translation,98,167,experimental-setup,Foma finite-state toolkit of Hulden ( 2009 ),to construct,all graphs
translation,98,167,experimental-setup,all graphs,in,our experiments
translation,98,167,experimental-setup,Experimental setup,use,Foma finite-state toolkit of Hulden ( 2009 )
translation,98,168,experimental-setup,Experimental setup,Cython version of,"Dynet ( Neubig et al. , 2017 )"
translation,98,6,experiments,translating text to code signature representations,using,software component datasets
translation,98,5,model,semantic parsing models,trained on,multiple datasets and natural languages
translation,98,5,model,Model,learning,semantic parsing models
translation,98,34,model,shortest path problems,in,directed graphs
translation,98,174,results,Results,for,Stdlib and Py27
translation,98,176,results,our experiments,show that,training
translation,98,176,results,polyglot models,on,multiple datasets
translation,98,176,results,polyglot models,lead to,large improvements
translation,98,176,results,large improvements,over,training individual models
translation,98,176,results,training individual models,especially on,Py27 datasets
translation,98,176,results,polyglot model,resulted in,nearly 9 % average increase
translation,98,176,results,nearly 9 % average increase,in,accuracy
translation,98,176,results,training,has,polyglot models
translation,98,180,results,neural models,strongly outperformed by,all other models
translation,98,180,results,all other models,both in,monolingual and polyglot case
translation,98,180,results,Results,has,neural models
translation,98,188,results,Results,has,Semantic Parsing Results
translation,98,189,results,neural models,especially,strongly outperform
translation,98,189,results,neural models,especially,competitive
translation,98,189,results,neural models,are,competitive
translation,98,189,results,competitive,with,related work
translation,98,189,results,neural models,has,strongly outperform
translation,98,189,results,strongly outperform,has,all other models
translation,98,191,results,polyglot Geo,achieves,best performance
translation,98,191,results,Results,see that,polyglot Geo
translation,98,195,results,neural model,to be,more robust
translation,98,195,results,more robust,than,lexical models
translation,98,195,results,lexical models,has,with reranking
translation,98,195,results,Results,find,neural model
translation,98,196,results,lexical models,perform,poorly
translation,98,196,results,Results,has,lexical models
translation,99,6,results,system,ranked,11th
translation,99,6,results,system,being,10th
translation,99,6,results,system,being,26th
translation,99,6,results,11th,among,33 participants
translation,99,6,results,11th,being,8th
translation,99,6,results,11th,being,10th
translation,99,6,results,11th,being,12th
translation,99,6,results,11th,being,26th
translation,99,6,results,8th,on,small treebanks
translation,99,6,results,10th,on,large treebanks
translation,99,6,results,12th,on,parallel test sets
translation,99,6,results,26th,on,surprise languages
translation,99,6,results,Results,has,system
translation,99,100,results,Overall rank,of,our system
translation,99,100,results,our system,in,official evaluation
translation,99,100,results,macro-averaged labeled attachment score ( LAS ),of,68.59 %
translation,99,100,results,Results,has,Overall rank
translation,99,102,results,macro- LAS score,across,all treebanks
translation,99,102,results,macro- LAS score,clearly behind,"winning system ( Stanford , 76.30 % )"
translation,99,102,results,all treebanks,clearly behind,"winning system ( Stanford , 76.30 % )"
translation,99,102,results,our pre-trained word embeddings,able to,improve
translation,99,102,results,improve,over,baseline UDPipe
translation,99,102,results,baseline UDPipe,by,0.24 % points
translation,99,102,results,Results,On,macro- LAS score
translation,99,103,results,treebanks,with,very little of training data
translation,99,103,results,2.38 %,over,baseline system
translation,99,103,results,Results,looking only at,treebanks
translation,99,108,results,sixth,on,universal part- of-speech tagging
translation,99,108,results,second,on,morphological features
translation,99,109,results,modest improvement,of,+ 0.22 % ( upos )
translation,99,109,results,modest improvement,of,+ 0.27 % ( feats )
translation,99,109,results,+ 0.27 % ( feats ),over,baseline models
translation,99,109,results,Results,see,modest improvement
translation,100,238,ablation-analysis,Our results,suggest,all intermediate layers
translation,100,238,ablation-analysis,all intermediate layers,of,network
translation,100,238,ablation-analysis,all intermediate layers,are,discriminative
translation,100,238,ablation-analysis,all intermediate layers,indeed,discriminative
translation,100,238,ablation-analysis,network,are,discriminative
translation,100,238,ablation-analysis,Ablation analysis,suggest,all intermediate layers
translation,100,238,ablation-analysis,Ablation analysis,has,Our results
translation,100,166,experimental-setup,reimplementation,of,"ZPar ( Zhang and Nivre , 2011 )"
translation,100,166,experimental-setup,transition - based parser,with,beam search
translation,100,166,experimental-setup,"Berkeley - Parser ( Petrov et al. , 2006 )",has,latent variable constituency parser
translation,100,166,experimental-setup,"ZPar ( Zhang and Nivre , 2011 )",has,transition - based parser
translation,100,166,experimental-setup,Experimental setup,process it,"Berkeley - Parser ( Petrov et al. , 2006 )"
translation,100,166,experimental-setup,Experimental setup,process it,reimplementation
translation,100,180,experimental-setup,D word = 64 and D tag = D label = 32,for,embedding dimensions
translation,100,180,experimental-setup,Experimental setup,set,D word = 64 and D tag = D label = 32
translation,100,190,experiments,outperforms,by,substantial margin
translation,100,190,experiments,comparison,by,substantial margin
translation,100,190,experiments,WSJ and Web tasks,has,our parser
translation,100,190,experiments,our parser,has,outperforms
translation,100,190,experiments,outperforms,has,all dependency parsers
translation,100,5,model,neural network representation,using,gold corpus
translation,100,5,model,gold corpus,augmented by,large number of automatically parsed sentences
translation,100,5,model,Model,learn,neural network representation
translation,100,102,model,semi-supervised structured learning scheme,yields,substantial improvements
translation,100,102,model,substantial improvements,in,accuracy
translation,100,102,model,accuracy,over,baseline neural network model
translation,100,102,model,Model,investigate,semi-supervised structured learning scheme
translation,100,108,model,gold data,with,large corpus
translation,100,108,model,large corpus,of,high quality automatic parses
translation,100,108,model,Model,supplement,gold data
translation,100,236,model,all hidden layers,crucial for,structured perceptron
translation,100,236,model,Model,Using,all hidden layers
translation,100,25,results,unlabeled data,into,training
translation,100,25,results,accuracy,of,our model
translation,100,25,results,our model,to,94.26 % UAS
translation,100,25,results,our model,to,93.46 % UAS / 91.49 % LAS
translation,100,25,results,94.26 % UAS,/,92.41 % LAS
translation,100,25,results,93.46 % UAS / 91.49 % LAS,for,greedy model
translation,100,25,results,92.41 % LAS,has,93.46 % UAS / 91.49 % LAS
translation,100,25,results,Results,by incorporating,unlabeled data
translation,100,31,results,parsing accuracy,by,0.8 %
translation,100,31,results,Penn Treebank,has,structured learning approach
translation,100,31,results,structured learning approach,has,significantly improves
translation,100,31,results,significantly improves,has,parsing accuracy
translation,100,31,results,Results,On,Penn Treebank
translation,100,36,results,10 million automatically parsed tokens,to,training data
translation,100,36,results,10 million automatically parsed tokens,improve,accuracy
translation,100,36,results,accuracy,of,our parsers
translation,100,36,results,our parsers,by,almost ?1.0 %
translation,100,36,results,almost ?1.0 %,on,web domain data
translation,100,36,results,Results,adding,10 million automatically parsed tokens
translation,100,50,results,10 million automatically parsed tokens,to,training data
translation,100,50,results,10 million automatically parsed tokens,improves,accuracy
translation,100,50,results,accuracy,of,our parsers
translation,100,50,results,accuracy,by,0.7 %
translation,100,50,results,our parsers,by,0.7 %
translation,100,50,results,Results,Adding,10 million automatically parsed tokens
translation,100,191,results,Question ( QTB ) dataset,is,more sensitive
translation,100,191,results,more sensitive,to,smaller beam size
translation,100,191,results,more sensitive,increase to,B = 32
translation,100,191,results,smaller beam size,to train,models
translation,100,191,results,models,in,reasonable time
translation,100,191,results,B = 32,at,inference time
translation,100,191,results,perceptron performance,goes up to,92.29 % LAS
translation,100,191,results,inference time,has,perceptron performance
translation,100,191,results,Results,has,Question ( QTB ) dataset
translation,100,193,results,help,on,dev set
translation,100,193,results,baseline,on,dev set
translation,100,193,results,test set performance,not,improve
translation,100,193,results,help,has,baseline
translation,100,193,results,improve,has,significantly
translation,100,193,results,Results,has,tritraining
translation,100,194,results,quite exciting,to see that,tri-training
translation,100,194,results,quite exciting,after,tri-training
translation,100,194,results,our greedy parser,is,more accurate
translation,100,194,results,our greedy parser,is,competitive
translation,100,194,results,more accurate,than,any of the baseline dependency parsers
translation,100,194,results,more accurate,than,competitive
translation,100,194,results,competitive,with,Berkeley - Parser
translation,100,194,results,Berkeley - Parser,used to generate,tri-training data
translation,100,194,results,tri-training,has,our greedy parser
translation,100,194,results,Results,after,tri-training
translation,100,195,results,tri-training,helps,most dramatically
translation,100,195,results,most dramatically,to increase,accuracy
translation,100,195,results,accuracy,on,Treebank Union setup
translation,100,195,results,Treebank Union setup,with,diverse domains
translation,100,195,results,0.4- 1.0 % absolute LAS improvement gains,for,our most accurate model
translation,100,195,results,Results,has,tri-training
translation,100,219,results,second hidden layer,results in,large gain
translation,100,219,results,large gain,on,tune set
translation,100,219,results,no gain,on,dev set
translation,100,219,results,dev set,if,pre-trained embeddings
translation,100,219,results,Results,adding,second hidden layer
translation,100,222,results,hidden units,yields,large gains
translation,100,222,results,Results,Increasing,hidden units
translation,100,235,results,marginally improve,for,softmax model
translation,100,235,results,accuracy,for,softmax model
translation,100,235,results,marginally improve,has,accuracy
translation,100,239,results,activations,proved to be,most effective representation
translation,100,239,results,most effective representation,for,structured perceptron
translation,100,245,results,our neural network model,training on,output
translation,100,245,results,our neural network model,training on,data
translation,100,245,results,output,of,BerkeleyParser
translation,100,245,results,output,yields,only modest gains
translation,100,245,results,data,where,two parsers agree
translation,100,245,results,Results,For,our neural network model
translation,100,245,results,Results,training on,data
translation,100,246,results,greedy models,after,tri-training
translation,100,246,results,greedy neural network model,surpasses,BerkeleyParser
translation,100,246,results,BerkeleyParser,in,accuracy
translation,100,246,results,greedy models,has,greedy neural network model
translation,100,246,results,tri-training,has,greedy neural network model
translation,100,247,results,improved,far more than,tri-training
translation,100,247,results,results,far more than,tri-training
translation,100,247,results,tri-training,for,baseline
translation,100,247,results,up-training,has,improved
translation,100,247,results,improved,has,results
translation,100,247,results,Results,interesting to note,up-training
translation,100,249,results,Error Analysis,using,structured perceptron
translation,100,249,results,structured perceptron,improved,error rates
translation,100,249,results,error rates,on,some of the common and difficult labels
translation,100,249,results,error rates,improved by,> 1 %
translation,100,249,results,some of the common and difficult labels,improved by,> 1 %
translation,100,249,results,nsubj,improved by,> 1 %
translation,100,249,results,Results,has,Error Analysis
translation,101,181,ablation-analysis,Ablation analysis,observed,notable phenomenon
translation,101,33,baselines,CoNLL,combines,frameworks
translation,101,33,baselines,frameworks,for,graph - based meaning representation
translation,101,33,baselines,Baselines,has,CoNLL
translation,101,25,hyperparameters,pre-trained language model BERT,as,encoder
translation,101,25,hyperparameters,Hyperparameters,use,pre-trained language model BERT
translation,101,174,hyperparameters,hidden dropout ( dropout rate = 0.1 ),to,outputs
translation,101,174,hyperparameters,outputs,of,each module
translation,101,174,hyperparameters,each module,in,our model
translation,101,174,hyperparameters,Hyperparameters,apply,hidden dropout ( dropout rate = 0.1 )
translation,101,5,model,graph- based approach,to model,variety of semantic graph parsing tasks
translation,101,26,model,training phase,to prevent,nodes
translation,101,26,model,training phase,use,random sampling method
translation,101,26,model,random sampling method,on,golden graph nodes
translation,101,26,model,random sampling method,to push,as many correct nodes as possible
translation,101,26,model,as many correct nodes as possible,to join,edge training
translation,101,26,model,Model,In,training phase
translation,101,186,model,our model,be,end-toend
translation,101,186,model,Model,make,our model
translation,101,186,model,Model,be,end-toend
translation,101,187,model,previous graph pruning algorithm,to,variety of semantic graphs
translation,101,187,model,previous graph pruning algorithm,adopt,multi-task learning
translation,101,187,model,multi-task learning,for,multiple objectives
translation,101,187,model,Model,introduce,previous graph pruning algorithm
translation,101,187,model,Model,adopt,multi-task learning
translation,101,188,model,semantic graph task,as,multi-objective learning task
translation,101,188,model,multi-objective learning task,of,"nodes , edges , node attributes , and edge attributes"
translation,101,188,model,Model,model,semantic graph task
translation,101,189,model,nodes candidates,scored and then pruned within,model
translation,101,189,model,nodes candidates,forming,end-to - end style parsing system
translation,101,189,model,model,controlling,overall graph search space
translation,101,189,model,Model,has,nodes candidates
translation,101,27,results,our system,ranked,second place
translation,101,27,results,second place,in,overall F 1 metric
translation,101,27,results,overall F 1 metric,among,16 participating systems
translation,101,28,results,DM framework,achieved,best results
translation,101,28,results,our system,achieved,best results
translation,101,28,results,DM framework,has,our system
translation,101,28,results,Results,On,DM framework
translation,101,29,results,Our system,on,"other 4 frameworks ( PSD , EDS , UCCA"
translation,101,29,results,"other 4 frameworks ( PSD , EDS , UCCA",ranked,third place
translation,101,29,results,Results,has,Our system
translation,101,178,results,state - of- the - art MRP F 1 score,on,top nodes component
translation,101,178,results,Results,obtained,state - of- the - art MRP F 1 score
translation,101,180,results,Our model,achieved,best results
translation,101,180,results,best results,on,DM framework
translation,101,180,results,Results,has,Our model
translation,101,182,results,our results,on,EDS framework
translation,101,182,results,EDS framework,have,biggest gap
translation,101,182,results,biggest gap,with,other priority teams
translation,101,182,results,Results,on,EDS framework
translation,102,156,ablation-analysis,neural structure,pushes up,scores
translation,102,156,ablation-analysis,scores,has,exceptionally
translation,102,156,ablation-analysis,Ablation analysis,introduction of,neural structure
translation,102,178,ablation-analysis,parsing models,push up,performance
translation,102,178,ablation-analysis,both without and with ECs together,push up,performance
translation,102,178,ablation-analysis,parsing models,has,both without and with ECs together
translation,102,178,ablation-analysis,performance,has,further
translation,102,178,ablation-analysis,Ablation analysis,when jointing,parsing models
translation,102,161,baselines,Pre3,uses,larger window length
translation,102,161,baselines,larger window length,to incorporate,richer contextual tokens
translation,102,161,baselines,searching space,for,decoding
translation,102,161,baselines,searching space,grows,larger
translation,102,161,baselines,decoding,grows,larger
translation,102,161,baselines,richer contextual tokens,has,searching space
translation,102,161,baselines,Baselines,has,Pre3
translation,102,4,model,neural models,for,parsing detection of empty categories
translation,102,4,model,neural models,pre-and in -,parsing detection of empty categories
translation,102,4,model,Model,study,neural models
translation,102,30,results,pre-parsing model,has,without any syntactic information
translation,102,30,results,without any syntactic information,has,outperforms
translation,102,30,results,outperforms,has,best existing linear in- parsing and post-parsing ECD models
translation,102,34,results,in - parsing neural models,obtain,better predictions
translation,102,34,results,better predictions,than,pre-parsing model
translation,102,34,results,Results,Our,in - parsing neural models
translation,102,34,results,Results,has,in - parsing neural models
translation,102,155,results,overall performances,of,two sequential models
translation,102,155,results,two sequential models,on,development data
translation,102,159,results,difference,among,four kinds of representations
translation,102,159,results,four kinds of representations,is,not so obvious
translation,102,159,results,Results,find that,difference
translation,102,163,results,POS tags,show,higher scores
translation,102,163,results,higher scores,as,more syntactic clues
translation,102,163,results,more syntactic clues,are,incorporated
translation,102,165,results,simple neural pre-parsing model,has,outperforms
translation,102,165,results,outperforms,has,state- ofthe - art linear in - parsing systems
translation,102,165,results,Results,see that,simple neural pre-parsing model
translation,102,173,results,EC types,find that,OP and T
translation,102,173,results,OP and T,benefit most from,parsing information
translation,102,173,results,F 1 score,increasing by,about ten points
translation,102,173,results,more markedly,than,other types
translation,102,173,results,Results,regarding,EC types
translation,102,175,results,impact,of,automatic detection
translation,102,175,results,automatic detection,of,empty categories
translation,102,175,results,empty categories,on,parsing
translation,102,175,results,parsing,has,overt words
translation,102,177,results,empty elements,into,dependency parsing
translation,102,177,results,empty elements,improve,neural parsing accuracy
translation,102,177,results,neural parsing accuracy,of,overt words
translation,102,177,results,Results,integrating,empty elements
translation,103,245,experimental-setup,"Mate parser ( Bohnet , 2010 )",to generate,pseudo trees
translation,103,245,experimental-setup,Experimental setup,utilize,"Mate parser ( Bohnet , 2010 )"
translation,103,4,model,new Maximum Subgraph algorithm,for,first-order parsing
translation,103,4,model,first-order parsing,to,"1endpoint - crossing , pagenumber - 2 graphs"
translation,103,4,model,Model,propose,new Maximum Subgraph algorithm
translation,103,5,model,algorithm,separates,construction
translation,103,5,model,algorithm,in,single construction step
translation,103,5,model,two characteristics,separates,construction
translation,103,5,model,two characteristics,in,single construction step
translation,103,5,model,construction,for,noncrossing edges
translation,103,5,model,construction,for,crossing edges
translation,103,5,model,arc,is,deterministic
translation,103,5,model,algorithm,has,two characteristics
translation,103,5,model,Model,separates,construction
translation,103,5,model,Model,in,single construction step
translation,103,5,model,Model,has,algorithm
translation,103,7,model,new algorithm,for,quasi-second -order parsing
translation,103,24,model,alternative Maximum Subgraph algorithm,for,first-order parsing to 1EC / P2 graphs
translation,103,24,model,Model,introduce,alternative Maximum Subgraph algorithm
translation,103,25,model,our new algorithm,separates,construction
translation,103,25,model,construction,for,noncrossing edges and possible crossing edges
translation,103,25,model,GCHSW,has,our new algorithm
translation,103,25,model,our new algorithm,has,two characteristics
translation,103,26,model,new algorithm,to perform,secondorder parsing
translation,103,26,model,Model,introduce,new algorithm
translation,103,94,model,Model,propose,alternative first-order dynamic programming algorithm
translation,103,29,results,all data sets,find that,our second-order parsing models
translation,103,29,results,our second-order parsing models,are,more ac-curate
translation,103,29,results,more ac-curate,than,first-order baseline
translation,103,29,results,Results,On,all data sets
translation,103,30,results,features,derived from,syntactic trees
translation,103,30,results,features,get,absolute unlabeled F-score improvement
translation,103,30,results,absolute unlabeled F-score improvement,of,1.3
translation,103,30,results,Results,not use,features
translation,103,250,results,firstorder parser,obtains,considerably good accuracy
translation,103,250,results,considerably good accuracy,with,rich syntactic features
translation,103,250,results,Results,see that,firstorder parser
translation,103,251,results,higher -order features,improves,parsing substantially
translation,103,251,results,parsing substantially,for,all data sets
translation,103,251,results,Results,introduction of,higher -order features
translation,103,252,results,syntactic trees,utilized,. improvement
translation,103,252,results,. improvement,is,smaller but still significant
translation,103,252,results,smaller but still significant,on,three SemEval data sets
translation,103,252,results,syntactic trees,has,. improvement
translation,103,259,results,purely Maximum Subgraph parser,obtains,better results
translation,103,259,results,better results,on,DeepBank and CCGBank
translation,103,259,results,book embedding parser,better on,other two data sets
translation,104,51,hyperparameters,gaussian,prior to,2
translation,104,51,hyperparameters,gaussian,train,model
translation,104,51,hyperparameters,model,in,1000 iterations
translation,104,51,hyperparameters,Hyperparameters,set,gaussian
translation,104,51,hyperparameters,Hyperparameters,train,model
translation,104,23,results,our system,achieves,80.45 %
translation,104,23,results,our system,achieves,62.8 %
translation,104,23,results,80.45 %,in term of,unlabeled attachment score ( UAS )
translation,104,23,results,62.8 %,in term of,LAS
translation,104,60,results,simple method,for,combination
translation,104,60,results,combination,over,three single parsers
translation,104,60,results,system combination technique,achieves,1.1 points improvement
translation,104,60,results,1.1 points improvement,over,highest single system
translation,104,60,results,simple method,has,system combination technique
translation,104,60,results,combination,has,system combination technique
translation,104,60,results,three single parsers,has,system combination technique
translation,104,60,results,Results,using,simple method
translation,104,66,results,performance,of,relation labeling
translation,104,66,results,performance,still far from,perfect
translation,104,66,results,relation labeling,still far from,perfect
translation,104,66,results,golden parse tree,has,performance
translation,104,66,results,Results,given,golden parse tree
translation,105,65,experimental-setup,jPTDP,implemented using,DYNET v2.0
translation,105,65,experimental-setup,Experimental setup,has,jPTDP
translation,105,67,experimental-setup,objective function,using,"Adam ( Kingma and Ba , 2014 )"
translation,105,67,experimental-setup,"Adam ( Kingma and Ba , 2014 )",with,default DYNET parameter settings
translation,105,67,experimental-setup,Experimental setup,optimize,objective function
translation,105,68,experimental-setup,fixed random seed,utilize,pre-trained embeddings
translation,105,68,experimental-setup,Experimental setup,use,fixed random seed
translation,105,68,experimental-setup,Experimental setup,utilize,pre-trained embeddings
translation,105,69,experimental-setup,word dropout rate,of,0.25
translation,105,69,experimental-setup,Gaussian noise,with,? = 0.2
translation,105,69,experimental-setup,Experimental setup,apply,word dropout rate
translation,105,69,experimental-setup,Experimental setup,apply,Gaussian noise
translation,105,70,experimental-setup,Experimental setup,run for,30 epochs
translation,105,81,experiments,our model jPTDP,outperforms,Stack - propagation
translation,105,81,experiments,dependency parsing,has,our model jPTDP
translation,105,5,model,bidirectional LSTMs,to learn,feature representations
translation,105,5,model,feature representations,shared for,POS tagging and dependency parsing tasks
translation,105,5,model,Model,uses,bidirectional LSTMs
translation,105,18,model,novel neural architecture,for,joint POS tagging and graph - based dependency parsing
translation,105,18,model,Model,propose,novel neural architecture
translation,105,19,model,latent feature representations,shared for,POS tagging and dependency parsing tasks
translation,105,19,model,latent feature representations,by using,BiLSTMthe bidirectional LSTM
translation,105,19,model,POS tagging and dependency parsing tasks,by using,BiLSTMthe bidirectional LSTM
translation,105,19,model,Model,learns,latent feature representations
translation,105,6,results,state - of - the - art neural networkbased Stack - propagation model,for,joint POS tagging and transition - based dependency parsing
translation,105,6,results,our model,has,outperforms
translation,105,6,results,outperforms,has,state - of - the - art neural networkbased Stack - propagation model
translation,105,77,results,our joint model jPTDP,obtains,similar POS tagging accuracies
translation,105,77,results,similar POS tagging accuracies,to,BiLSTM - aux model
translation,105,77,results,POS tagging,has,our joint model jPTDP
translation,105,77,results,Results,Regarding,POS tagging
translation,105,78,results,Our model,achieves,higher averaged POS tagging accuracy
translation,105,78,results,higher averaged POS tagging accuracy,than,joint model Stackpropagation
translation,105,78,results,Results,has,Our model
translation,105,82,results,about 7 % absolute lower LAS score,than,Stack - propagation
translation,105,82,results,Stack - propagation,on,Dutch ( nl )
translation,105,82,results,Results,produces,about 7 % absolute lower LAS score
translation,105,85,results,nl,into,account
translation,105,85,results,averaged LAS score,over,all remaining languages
translation,105,85,results,all remaining languages,is,1.1 % absolute higher
translation,105,85,results,1.1 % absolute higher,than,Stack - propagation 's
translation,105,85,results,nl,has,averaged LAS score
translation,105,85,results,Results,Without taking,nl
translation,105,87,results,absolute LAS improvement,of,4.4 %
translation,105,87,results,4.4 %,on,average
translation,105,87,results,morphologically rich languages,get,averaged improvement
translation,105,87,results,averaged improvement,of,9.3 %
translation,105,87,results,Results,shows,absolute LAS improvement
translation,106,115,ablation-analysis,out-of- domain BROWN tests,find that,adding ? deps
translation,106,115,ablation-analysis,in a statistically significant way,to,LAS and UAS
translation,106,115,ablation-analysis,Ablation analysis,on,out-of- domain BROWN tests
translation,106,7,results,"strong , statistically significant gains",on,dependency recovery
translation,106,7,results,dependency recovery,on,out-of- domain tests
translation,106,7,results,Results,find,"strong , statistically significant gains"
translation,106,22,results,higher - order dependency features,show,consistent and unambiguous contribution
translation,106,22,results,consistent and unambiguous contribution,to,dependency accuracy
translation,106,22,results,consistent and unambiguous contribution,on,outof-domain tests
translation,106,22,results,labelled and unlabelled,of,our phrase -structure parsers
translation,106,22,results,our phrase -structure parsers,on,outof-domain tests
translation,106,112,results,statistically significant improvement,in,accuracy
translation,106,112,results,accuracy,on,most metrics
translation,106,112,results,higher - order dependency feature set,has,? deps
translation,106,112,results,Results,adding in,higher - order dependency feature set
translation,106,113,results,in- domain WSJ test set,find that,? phrase + deps
translation,106,113,results,? phrase + deps,is,significantly better
translation,106,113,results,significantly better,than,component parts
translation,106,113,results,significantly better,either of,component parts
translation,106,113,results,component parts,on,all metrics
translation,106,113,results,Results,On,in- domain WSJ test set
translation,106,114,results,phrase + deps +gen,significantly better than,phrase + gen
translation,106,114,results,phrase + deps +gen,on,F 1
translation,106,114,results,phrase + gen,on,F 1
translation,106,114,results,phrase + gen,not on,UAS
translation,106,114,results,phrase + gen,not on,LAS
translation,106,114,results,Results,has,phrase + deps +gen
translation,106,124,results,discriminative parsing models,are,very strong
translation,106,124,results,discriminative parsing models,adding,? gen
translation,106,124,results,? gen,yields,considerable gains
translation,106,124,results,? gen feature,has,discriminative parsing models
translation,106,124,results,Results,even without,? gen feature
translation,106,126,results,LAS scores,of,our models
translation,106,126,results,our models,are,relatively weak
translation,106,126,results,without ? gen,are,relatively weak
translation,106,126,results,our models,has,without ? gen
translation,106,126,results,Results,Note,LAS scores
translation,106,130,results,our ? phrase + deps model,as accurate as,Huang's
translation,106,130,results,Results,has,our ? phrase + deps model
translation,107,83,hyperparameters,Hyperparameters,use,10 training iterations
translation,107,5,model,second-order MST model,by adding,two third - order features
translation,107,85,results,features,of,third - order
translation,107,85,results,best result,reaches,labeled attachment score
translation,107,85,results,labeled attachment score,of,62.48 %
translation,107,85,results,62.48 %,on,developing dataset
translation,107,85,results,62.48 %,ignores,punctuation
translation,107,85,results,features,has,best result
translation,107,85,results,third - order,has,best result
translation,107,85,results,Results,After adjusting,features
translation,107,86,results,currently best result,to,SemEval - 2012
translation,107,86,results,currently best result,is,61.58 %
translation,107,86,results,SemEval - 2012,is,61.58 %
translation,107,86,results,61.58 %,on,test dataset
translation,107,86,results,Results,submitted,currently best result
translation,107,87,results,third - order features,to,second-order model
translation,107,87,results,dependency parsing accuracies,by,1.21 %
translation,107,87,results,dependency parsing accuracies,by,0.15 %
translation,107,87,results,1.21 %,comparing to,first-order model
translation,107,87,results,0.15 %,comparing to,second-order model
translation,107,87,results,Results,adding,third - order features
translation,108,63,experiments,/sivareddyg,/,graph-parser
translation,108,63,experiments,github.com,has,/sivareddyg
translation,108,63,experiments,/sivareddyg,has,graph-parser
translation,108,80,results,almost twice as well,as,semi-supervised parsers
translation,108,80,results,semi-supervised parsers,on,CCGbank LF1
translation,108,80,results,semantic evaluation,see,comparatively small gain
translation,108,80,results,comparatively small gain,in,performance
translation,108,80,results,performance,has,43 vs 46 )
translation,108,88,results,number of entities,in,sentence
translation,108,88,results,number of entities,in,sentences
translation,108,88,results,model,performing,worse
translation,108,88,results,worse,than,unsupervised parser
translation,108,88,results,unsupervised parser,on,sentences
translation,108,88,results,sentences,with,four entities
translation,108,88,results,number of entities,has,increase
translation,108,88,results,number of entities,has,model
translation,108,88,results,sentence,has,increase
translation,108,88,results,increase,has,model
translation,108,88,results,model,has,weakens
translation,108,91,results,Ensemble systems,incorporate,syntax and a Bag-of - Words baseline
translation,108,91,results,Ensemble systems,may yield,even better performance
translation,108,91,results,syntax and a Bag-of - Words baseline,may yield,even better performance
translation,108,91,results,Results,has,Ensemble systems
translation,110,109,ablation-analysis,ErrorAct,resulted in,3.6 % and 1.2 % improvement
translation,110,109,ablation-analysis,3.6 % and 1.2 % improvement,in,UAS and disfluency detection F1
translation,110,110,ablation-analysis,Backoff,contributes to,further improved UAS
translation,110,110,ablation-analysis,WCN,increase in,disfluency detection accuracy
translation,110,110,ablation-analysis,Ablation analysis,has,Backoff
translation,110,7,model,parsing method,handles,disfluency and ASR errors
translation,110,7,model,disfluency and ASR errors,using,incremental shift-reduce algorithm
translation,110,7,model,incremental shift-reduce algorithm,with,several novel features
translation,110,7,model,several novel features,suited to,ASR output texts
translation,110,7,model,Model,propose,parsing method
translation,110,8,model,alignment - based method,for transferring,gold dependency annotation
translation,110,8,model,gold dependency annotation,to,ASR output texts
translation,110,8,model,ASR output texts,to construct,training data
translation,110,8,model,training data,for,our parser
translation,110,8,model,Model,introduce,alignment - based method
translation,110,21,model,method,for,joint dependency parsing and disfluency detection
translation,110,21,model,robustly parse,has,ASR output texts
translation,110,21,model,Model,propose,method
translation,110,22,results,disfluencies and ASR errors,using,incremental shift-reduce algorithm
translation,110,22,results,incremental shift-reduce algorithm,with,novel features
translation,110,22,results,novel features,that consider,recognition errors
translation,110,22,results,recognition errors,of,ASR system
translation,110,117,results,parser,trained on,ASR texts
translation,110,117,results,parser,trained on,transcription texts
translation,110,117,results,parser,trained on,transcription texts
translation,110,117,results,ASR texts,outperforms,parser
translation,110,117,results,parser,trained on,transcription texts
translation,110,117,results,transcription texts,with,improvement
translation,110,117,results,improvement,of,1.5 % and 0.7 %
translation,110,117,results,1.5 % and 0.7 %,for,UAS and disfluency detection
translation,110,117,results,newly proposed features,has,parser
translation,110,117,results,Results,with,newly proposed features
translation,110,118,results,"Train , Test )",observe,significant decreases
translation,110,118,results,"rans , T rans )",observe,significant decreases
translation,110,118,results,significant decreases,in,performance
translation,110,118,results,performance,in,both of the tasks
translation,110,118,results,both of the tasks,conducted on,ASR texts
translation,111,7,ablation-analysis,two fundamental factors,for,low-resource domain adaptation
translation,111,7,ablation-analysis,Ablation analysis,identify,two fundamental factors
translation,111,24,ablation-analysis,two key factors,for,successfully adapting
translation,111,24,ablation-analysis,task - oriented semantic parsers,to,new domains
translation,111,24,ablation-analysis,successfully adapting,has,task - oriented semantic parsers
translation,111,24,ablation-analysis,Ablation analysis,identify,two key factors
translation,111,130,ablation-analysis,pre-trained representations,is,crucial
translation,111,130,ablation-analysis,crucial,in,low-resource setting
translation,111,130,ablation-analysis,low-resource setting,with,very small amount of training data
translation,111,130,ablation-analysis,Ablation analysis,has,pre-trained representations
translation,111,136,ablation-analysis,source training,improve,performance
translation,111,136,ablation-analysis,performance,from,33.8 % to 55
translation,111,136,ablation-analysis,nonpretrained LSTM model,has,source training
translation,111,136,ablation-analysis,Ablation analysis,With,nonpretrained LSTM model
translation,111,137,ablation-analysis,Ablation analysis,has,4 %
translation,111,138,ablation-analysis,BART,learns,sufficiently good representation
translation,111,138,ablation-analysis,hypothesis,might be that,source training
translation,111,138,ablation-analysis,source training,is,no longer important
translation,111,138,ablation-analysis,BART,learns,sufficiently good representation
translation,111,138,ablation-analysis,sufficiently good representation,to provide,model generalization
translation,111,138,ablation-analysis,BART,has,hypothesis
translation,111,138,ablation-analysis,Ablation analysis,With,BART
translation,111,139,ablation-analysis,BART model 's performance,by,8.4 %
translation,111,139,ablation-analysis,source training,has,improves
translation,111,139,ablation-analysis,improves,has,BART model 's performance
translation,111,157,ablation-analysis,meta-learning,proves to be,beneficial
translation,111,157,ablation-analysis,meta-learning,improves,performance
translation,111,157,ablation-analysis,beneficial,for,extremely low resource setting
translation,111,157,ablation-analysis,performance,of,BART
translation,111,157,ablation-analysis,by about 2.5 %,at,10 and 25 SPIS
translation,111,157,ablation-analysis,Ablation analysis,observe,meta-learning
translation,111,158,ablation-analysis,gradually reduced,as,amount of training data
translation,111,158,ablation-analysis,amount of training data,has,increases
translation,111,158,ablation-analysis,Ablation analysis,has,performance gap
translation,111,159,ablation-analysis,steeper performance drop,for,both models
translation,111,159,ablation-analysis,both models,when,go below 25 SPIS
translation,111,159,ablation-analysis,Ablation analysis,notice,steeper performance drop
translation,111,122,baselines,BART -COPYPTR,is,proposed model
translation,111,122,baselines,proposed model,leverages,BART
translation,111,122,baselines,BART,to initialize,both the encoder and decoder
translation,111,122,baselines,Baselines,has,BART -COPYPTR
translation,111,163,experimental-setup,Experimental setup,has,Dropout ( p = 0.4 )
translation,111,164,experimental-setup,"Adam ( Kingma and Ba , 2015 )",used for,optimization
translation,111,164,experimental-setup,"Adam ( Kingma and Ba , 2015 )",with,learning rate
translation,111,164,experimental-setup,optimization,with,learning rate
translation,111,164,experimental-setup,learning rate,of,10 ?3
translation,111,164,experimental-setup,learning rate,of,5 ? 10 ?4
translation,111,164,experimental-setup,10 ?3,for,source training
translation,111,164,experimental-setup,5 ? 10 ?4,for,low-resource fine-tuning
translation,111,164,experimental-setup,Experimental setup,has,"Adam ( Kingma and Ba , 2015 )"
translation,111,165,experimental-setup,encoder,is,12 - layer transformer
translation,111,165,experimental-setup,encoder,is,smaller transformer model
translation,111,165,experimental-setup,12 - layer transformer,with,embedding size
translation,111,165,experimental-setup,12 - layer transformer,with,embedding size
translation,111,165,experimental-setup,embedding size,of,768
translation,111,165,experimental-setup,embedding size,of,256
translation,111,165,experimental-setup,embedding size,of,256
translation,111,165,experimental-setup,decoder,is,smaller transformer model
translation,111,165,experimental-setup,decoder,is,embedding size
translation,111,165,experimental-setup,smaller transformer model,with,3 layers
translation,111,165,experimental-setup,embedding size,of,256
translation,111,165,experimental-setup,RoBERTa models,has,encoder
translation,111,165,experimental-setup,Experimental setup,For,RoBERTa models
translation,111,166,experimental-setup,BART models,follow,embedding size
translation,111,166,experimental-setup,both the encoder and decoder,follow,size
translation,111,166,experimental-setup,both the encoder and decoder,follow,embedding size
translation,111,166,experimental-setup,size,of,pre-trained model
translation,111,166,experimental-setup,pre-trained model,with,12 layers
translation,111,166,experimental-setup,embedding size,of,1024
translation,111,166,experimental-setup,BART models,has,both the encoder and decoder
translation,111,166,experimental-setup,Experimental setup,For,BART models
translation,111,167,experimental-setup,all transformer - based models,has,Dropout ( p = 0.3 )
translation,111,167,experimental-setup,Experimental setup,For,all transformer - based models
translation,111,168,experimental-setup,Adam,used for,optimization
translation,111,168,experimental-setup,Adam,with,learning rate
translation,111,168,experimental-setup,learning rate,of,10 ?4
translation,111,168,experimental-setup,learning rate,of,5 ? 10 ?5
translation,111,168,experimental-setup,10 ?4,for,source training
translation,111,168,experimental-setup,5 ? 10 ?5,for,finetuning
translation,111,168,experimental-setup,Experimental setup,has,Adam
translation,111,169,experimental-setup,inverse square- root learning rate schedule,employed with,warmup period
translation,111,169,experimental-setup,warmup period,of,4000 updates
translation,111,169,experimental-setup,warmup period,of,2000
translation,111,169,experimental-setup,4000 updates,for,source training
translation,111,169,experimental-setup,2000,for,fine-tuning
translation,111,169,experimental-setup,Experimental setup,has,inverse square- root learning rate schedule
translation,111,170,experimental-setup,meta-learning,select,k = 5
translation,111,170,experimental-setup,meta-learning,select,batch size
translation,111,170,experimental-setup,batch size,of,32
translation,111,170,experimental-setup,32,for,Reptile
translation,111,170,experimental-setup,Reptile,with,inner ( ? ) and outer ( ? ) learning rates
translation,111,170,experimental-setup,inner ( ? ) and outer ( ? ) learning rates,being,5 ? 10 ?5
translation,111,170,experimental-setup,Experimental setup,For,meta-learning
translation,111,171,experimental-setup,100 epochs,on,source domains
translation,111,171,experimental-setup,100 epochs,with,batch size
translation,111,171,experimental-setup,100 epochs,using,early stopping
translation,111,171,experimental-setup,batch size,of,128
translation,111,171,experimental-setup,early stopping,if,validation accuracy
translation,111,171,experimental-setup,does not improve,in,last 10 epochs
translation,111,171,experimental-setup,validation accuracy,has,does not improve
translation,111,172,experimental-setup,Finetuning,done for,2000 epochs
translation,111,172,experimental-setup,Finetuning,with,batch size
translation,111,172,experimental-setup,batch size,of,64 ( LSTM and RoBERTa )
translation,111,172,experimental-setup,batch size,of,32 ( BART and meta-learning )
translation,111,172,experimental-setup,Experimental setup,has,Finetuning
translation,111,173,experimental-setup,Model validation,performed once every,10 epochs
translation,111,173,experimental-setup,10 epochs,during,fine-tuning
translation,111,173,experimental-setup,stops early,after,20 consecutive validations
translation,111,173,experimental-setup,20 consecutive validations,with,no improvements
translation,111,173,experimental-setup,Experimental setup,has,Model validation
translation,111,174,experimental-setup,Nvidia Telsa P100 GPU,with,16GB memory
translation,111,174,experimental-setup,Experimental setup,implemented with,fairseq framework
translation,111,174,experimental-setup,Experimental setup,trained on,Nvidia Telsa P100 GPU
translation,111,8,experiments,representation learning,uses,"BART ( Lewis et al. , 2020 )"
translation,111,8,experiments,"BART ( Lewis et al. , 2020 )",to initialize,our model
translation,111,8,experiments,outperforms,has,encoder-only pre-trained representations
translation,111,151,experiments,Meta-learning,show,metalearning
translation,111,151,experiments,model training,for transferring to,low-resource domains
translation,111,151,experiments,improve,has,model training
translation,111,9,hyperparameters,optimization - based meta-learning,to improve,generalization
translation,111,9,hyperparameters,generalization,to,lowresource domains
translation,111,9,hyperparameters,optimization - based meta-learning,has,"al. , 2017 )"
translation,111,9,hyperparameters,Hyperparameters,train with,optimization - based meta-learning
translation,111,6,model,supervised neural model,at,10 - fold data reduction
translation,111,6,model,outperforms,has,supervised neural model
translation,111,6,model,Model,adapting,taskoriented semantic parsers to low-resource domains
translation,111,6,model,Model,propose,novel method
translation,111,23,model,compositional task - oriented semantic parsers,for,low-resource domains
translation,111,23,model,competitive,against,supervised models
translation,111,23,model,supervised models,trained with,10x more data
translation,111,23,model,Model,build,compositional task - oriented semantic parsers
translation,111,29,model,optimization - based meta-learning,to improve,generalization
translation,111,29,model,generalization,of,BART model
translation,111,29,model,BART model,trained on,source domains
translation,111,29,model,Model,employ,optimization - based meta-learning
translation,111,72,results,source-training,has,significantly improves
translation,111,72,results,significantly improves,has,final performance
translation,111,131,results,outperforms,by,17 %
translation,111,131,results,outperforms,by,30 %
translation,111,131,results,LSTM,by,17 %
translation,111,131,results,LSTM,by,30 %
translation,111,131,results,17 %,in,ST + FT setting
translation,111,131,results,30 %,in,FT only setting
translation,111,131,results,BART,has,outperforms
translation,111,131,results,outperforms,has,LSTM
translation,111,132,results,BART,is,superior choice
translation,111,132,results,superior choice,over,RoBERTa
translation,111,132,results,RoBERTa,to initialize,our SEQ2SEQ - COPYPTR model
translation,111,132,results,Results,demonstrate,BART
translation,111,133,results,performance gap,of,4.3 %
translation,111,133,results,4.3 %,between,BART and RoBERTa
translation,111,133,results,BART and RoBERTa,on,reminder domain
translation,111,134,results,outperforms,by,larger margin
translation,111,134,results,RoBERTa,by,larger margin
translation,111,134,results,larger margin,on,compositional queries
translation,111,134,results,compositional queries,than,flat ones
translation,111,134,results,8.7 % relative improvement,on,compositional queries
translation,111,134,results,6.3 %,on,flat
translation,111,134,results,BART,has,outperforms
translation,111,134,results,outperforms,has,RoBERTa
translation,111,152,results,standard source training,replaced with,Reptile
translation,111,152,results,accuracy,of,BART model
translation,111,152,results,substantially improved,on,both target domains
translation,111,152,results,best performance,achieved across,all low-resource models ( + 2.1 % )
translation,111,152,results,standard source training,has,accuracy
translation,111,152,results,Reptile,has,accuracy
translation,111,152,results,Results,When,standard source training
translation,111,153,results,supervised mod- els,trained with,500 SPIS
translation,111,153,results,Reptile + BART model,3.6 % away from,stateof - the- art RoBERTa - based one
translation,111,153,results,supervised mod- els,has,10 - fold data increase
translation,111,153,results,supervised mod- els,has,Reptile + BART model
translation,111,153,results,500 SPIS,has,10 - fold data increase
translation,111,153,results,Reptile + BART model,has,outperforms
translation,111,153,results,outperforms,has,LSTMbased model
translation,111,153,results,Results,compared to,supervised mod- els
translation,111,154,results,performance,on,reminder domain
translation,111,154,results,reminder domain,of,two best models
translation,111,154,results,Results,has,Accuracy vs. SPIS
translation,112,195,ablation-analysis,UAS,is,24.52 %
translation,112,195,ablation-analysis,UAS,shows that,implicitly - learned latent graphs
translation,112,195,ablation-analysis,implicitly - learned latent graphs,partially consistent with,human-defined syntactic structures
translation,112,195,ablation-analysis,Ablation analysis,has,UAS
translation,112,118,baselines,Model Configurations LGP -NMT,is,model
translation,112,118,baselines,model,learns,Latent Graph Parsing
translation,112,118,baselines,Latent Graph Parsing,for,NMT
translation,112,118,baselines,Baselines,has,Model Configurations LGP -NMT
translation,112,119,baselines,LGP - NMT +,constructed by,pre-training
translation,112,119,baselines,latent parser,in,LGP - NMT
translation,112,119,baselines,pre-training,has,latent parser
translation,112,119,baselines,Baselines,has,LGP - NMT +
translation,112,131,experiments,LGP - NMT,performs,worse
translation,112,131,experiments,worse,than,SEQ and UNI
translation,112,97,hyperparameters,weight matrices,in,latent graph parser
translation,112,97,hyperparameters,weight matrices,initialized with,uniform random values
translation,112,97,hyperparameters,uniform random values,in,"[? ? 6 ? row+col , + ? 6 ? row+col ]"
translation,112,97,hyperparameters,Hyperparameters,has,weight matrices
translation,112,98,hyperparameters,bias vectors and the weight matrices,in,softmax layers
translation,112,98,hyperparameters,bias vectors and the weight matrices,initialized with,zeros
translation,112,98,hyperparameters,bias vectors and the weight matrices,of,forget gates
translation,112,98,hyperparameters,bias vectors,of,forget gates
translation,112,98,hyperparameters,bias vectors,initialized by,ones
translation,112,98,hyperparameters,forget gates,in,LSTMs
translation,112,98,hyperparameters,forget gates,initialized by,ones
translation,112,98,hyperparameters,Hyperparameters,has,bias vectors and the weight matrices
translation,112,98,hyperparameters,Hyperparameters,has,bias vectors
translation,112,99,hyperparameters,negative samples,set to,"2,000"
translation,112,99,hyperparameters,negative samples,set to,"2,500"
translation,112,99,hyperparameters,"2,000",for,small and medium training datasets
translation,112,99,hyperparameters,"2,000",for,large training dataset
translation,112,99,hyperparameters,"2,500",for,large training dataset
translation,112,99,hyperparameters,Hyperparameters,number of,negative samples
translation,112,100,hyperparameters,mini-batch size,set to,128
translation,112,100,hyperparameters,momentum rate,set to,0.75
translation,112,100,hyperparameters,momentum rate,set to,0.70
translation,112,100,hyperparameters,0.75,for,small and medium training datasets
translation,112,100,hyperparameters,0.70,for,large training dataset
translation,112,100,hyperparameters,Hyperparameters,has,mini-batch size
translation,112,100,hyperparameters,Hyperparameters,has,momentum rate
translation,112,101,hyperparameters,gradient clipping technique,with,clipping value
translation,112,101,hyperparameters,clipping value,of,1.0
translation,112,101,hyperparameters,Hyperparameters,has,gradient clipping technique
translation,112,102,hyperparameters,initial learning rate,set to,1.0
translation,112,102,hyperparameters,halved,when,translation accuracy
translation,112,102,hyperparameters,translation accuracy,has,decreased
translation,112,102,hyperparameters,Hyperparameters,has,initial learning rate
translation,112,102,hyperparameters,Hyperparameters,has,learning rate
translation,112,105,hyperparameters,regularization,used,L2
translation,112,105,hyperparameters,L2,-,norm regularization
translation,112,105,hyperparameters,norm regularization,with,coefficient
translation,112,105,hyperparameters,"dropout ( Hinton et al. , 2012 )",with,dropout rate
translation,112,105,hyperparameters,dropout rate,of,0.2
translation,112,105,hyperparameters,L2,has,norm regularization
translation,112,105,hyperparameters,coefficient,has,of 10 ?6
translation,112,105,hyperparameters,Hyperparameters,For,regularization
translation,112,105,hyperparameters,Hyperparameters,applied,"dropout ( Hinton et al. , 2012 )"
translation,112,106,hyperparameters,beam size,for,beam search algorithm
translation,112,106,hyperparameters,beam size,was,50
translation,112,106,hyperparameters,beam search algorithm,was,12
translation,112,106,hyperparameters,beam search algorithm,was,50
translation,112,106,hyperparameters,12,for,small and medium training datasets
translation,112,106,hyperparameters,50,for,large training dataset
translation,112,106,hyperparameters,Hyperparameters,has,beam size
translation,112,4,model,neural machine translation,jointly learns,translation
translation,112,4,model,neural machine translation,jointly learns,source-side latent graph representations of sentences
translation,112,5,model,end-to - end model,learns,latent graph parser
translation,112,5,model,latent graph parser,as part of,encoder
translation,112,5,model,encoder,of,attention - based neural machine translation model
translation,112,5,model,parser,optimized according to,translation objective
translation,112,15,model,novel NMT model,can learn,task - specific latent graph structure
translation,112,15,model,task - specific latent graph structure,for,each source -side sentence
translation,112,15,model,Model,present,novel NMT model
translation,112,19,model,latent parser,independently pre-trained with,human-annotated treebanks
translation,112,19,model,latent parser,adapted to,translation task
translation,112,19,model,Model,has,latent parser
translation,112,21,results,previous best results,by,large margin
translation,112,21,results,large margin,on,WAT English - to - Japanese dataset
translation,112,21,results,Our final ensemble model,has,outperforms
translation,112,21,results,outperforms,has,previous best results
translation,112,21,results,Results,has,Our final ensemble model
translation,112,134,results,DEP,performs,worst
translation,112,134,results,Results,see that,DEP
translation,112,138,results,small amount,of,training samples
translation,112,138,results,small amount,performs,better
translation,112,138,results,better,than using,all the training samples
translation,112,138,results,Results,using,small amount
translation,112,146,results,LGP - NMT,slightly better than,SEQ
translation,112,147,results,LGP - NMT,shows that,our adaptive learning
translation,112,147,results,our adaptive learning,is,more effective
translation,112,147,results,more effective,than using,uniform graph weights
translation,112,147,results,LGP - NMT,has,significantly outperforms
translation,112,147,results,significantly outperforms,has,UNI
translation,112,147,results,Results,has,LGP - NMT
translation,112,148,results,significantly outperforms,in terms of,BLEU score
translation,112,148,results,SEQ,in terms of,BLEU score
translation,112,148,results,pre-training,has,our model
translation,112,148,results,pre-training,has,LGP - NMT +
translation,112,148,results,our model,has,LGP - NMT +
translation,112,148,results,LGP - NMT +,has,significantly outperforms
translation,112,148,results,significantly outperforms,has,SEQ
translation,112,149,results,DEP,performs,worst
translation,112,149,results,worst,among,all the models
translation,112,149,results,Results,has,DEP
translation,112,157,results,translation scores,of,our model
translation,112,157,results,translation scores,can be further improved,pre-training
translation,112,157,results,pre-training,has,model
translation,112,157,results,Results,see that,translation scores
translation,112,159,results,Our proposed models,has,outperform
translation,112,159,results,LGP - NMT and LGP - NMT +,has,outperform
translation,112,159,results,outperform,has,SEQ
translation,112,159,results,outperform,has,all of the previous best results
translation,112,159,results,Results,has,Our proposed models
translation,112,160,results,our implementation,of,sequential model ( SEQ )
translation,112,160,results,our implementation,provides,very strong baseline
translation,112,160,results,sequential model ( SEQ ),provides,very strong baseline
translation,112,160,results,comparable,to,previous state of the art
translation,112,160,results,comparable,without using,ensemble techniques
translation,112,208,results,UAS,of,pre-trained dependency trees
translation,112,208,results,UAS,of,adapted latent graphs
translation,112,208,results,pre-trained dependency trees,is,92.52 %
translation,112,208,results,adapted latent graphs,is,18.94 %
translation,112,208,results,Results,has,UAS
translation,112,209,results,lower,than,UAS
translation,112,209,results,without pretraining,has,24.52 % )
translation,112,210,results,our model,with,pre-training
translation,112,210,results,our model,better than,without pre-training
translation,112,210,results,translation accuracy,has,our model
translation,112,210,results,Results,in terms of,translation accuracy
translation,113,5,model,Model,present,incremental predictive dependency parser
translation,113,6,model,connected analyses,for,sentence prefixes
translation,113,6,model,state - of - the - art dependency parser,has,connected analyses
translation,113,6,model,Model,extending,state - of - the - art dependency parser
translation,113,128,results,labeled whole -sentence accuracy,of,93.02 %
translation,113,128,results,93.02 %,for,German
translation,113,129,results,whole -sentence accuracy,for,parsing
translation,113,129,results,parsing,with,VNs
translation,113,129,results,VNs,is,93.33 %
translation,113,129,results,Results,has,whole -sentence accuracy
translation,114,177,ablation-analysis,languages without external word embeddings,pro-cessed by,our system
translation,114,177,ablation-analysis,our system,has,significantly below
translation,114,177,ablation-analysis,significantly below,has,overall performance
translation,114,177,ablation-analysis,Ablation analysis,has,languages without external word embeddings
translation,114,196,ablation-analysis,1.2 p.p.,for,LAS
translation,114,196,ablation-analysis,2.9 p.p.,for,MLAS
translation,114,196,ablation-analysis,1.7 p.p.,for,BLEX
translation,114,111,experimental-setup,filters,have,kernel
translation,114,111,experimental-setup,kernel,has,of size 3
translation,114,112,experimental-setup,size,of,64
translation,114,112,experimental-setup,input character embedding,has,size
translation,114,112,experimental-setup,Experimental setup,has,input character embedding
translation,114,119,experimental-setup,tagger,uses,fully connected network
translation,114,119,experimental-setup,fully connected network,with,hidden layer
translation,114,119,experimental-setup,hidden layer,of,size 64
translation,114,119,experimental-setup,Experimental setup,has,tagger
translation,114,120,experimental-setup,morphological features,uses,hidden layer
translation,114,120,experimental-setup,hidden layer,of,128 neurons
translation,114,123,experimental-setup,filters,have,kernel
translation,114,123,experimental-setup,kernel,has,of size 3
translation,114,125,experimental-setup,input characters,represented as,embeddings
translation,114,125,experimental-setup,input characters,concatenated with,features
translation,114,125,experimental-setup,input characters,reduced to,32 dimensions
translation,114,125,experimental-setup,input characters,with,single fully connected layer
translation,114,125,experimental-setup,embeddings,with,256 dimensions
translation,114,125,experimental-setup,features,extracted with,biLSTM encoder
translation,114,125,experimental-setup,32 dimensions,with,single fully connected layer
translation,114,125,experimental-setup,Experimental setup,has,input characters
translation,114,127,experimental-setup,arc model,uses,heads ' and dependents ' vector representations
translation,114,127,experimental-setup,heads ' and dependents ' vector representations,with,512 dimensions
translation,114,127,experimental-setup,Experimental setup,has,arc model
translation,114,128,experimental-setup,labelling model,uses,128 - dimensional vectors
translation,114,128,experimental-setup,Experimental setup,has,labelling model
translation,114,129,experimental-setup,All fully connected layers,use,tanh activation function
translation,114,129,experimental-setup,All fully connected layers,use,"rectified linear unit ( ReLU , Nair and Hinton , 2010 )"
translation,114,129,experimental-setup,all convolutional layers,use,"rectified linear unit ( ReLU , Nair and Hinton , 2010 )"
translation,114,129,experimental-setup,Experimental setup,has,All fully connected layers
translation,114,131,experimental-setup,Gaussian Dropout,with,dropout rate
translation,114,131,experimental-setup,dropout rate,of,0.25
translation,114,131,experimental-setup,Gaussian Noise,with,standard deviation on 0.2
translation,114,131,experimental-setup,Gaussian Noise,to,final word embedding
translation,114,131,experimental-setup,Gaussian Noise,after processing,each biLSTM layer
translation,114,131,experimental-setup,Experimental setup,apply,Gaussian Dropout
translation,114,131,experimental-setup,Experimental setup,apply,Gaussian Noise
translation,114,132,experimental-setup,All fully connected layers,use,"standard dropout ( Srivastava et al. , 2014 )"
translation,114,132,experimental-setup,"standard dropout ( Srivastava et al. , 2014 )",with,dropout rate
translation,114,132,experimental-setup,dropout rate,of,0.25
translation,114,132,experimental-setup,Experimental setup,has,All fully connected layers
translation,114,133,experimental-setup,biLSTM layers,use,standard and recurrent dropout
translation,114,133,experimental-setup,standard and recurrent dropout,rate of,0.25
translation,114,133,experimental-setup,Experimental setup,has,biLSTM layers
translation,114,134,experimental-setup,biLSTM and convolutional layers,use,L2 regularization
translation,114,134,experimental-setup,biLSTM and convolutional layers,use,L2 regularization
translation,114,134,experimental-setup,biLSTM and convolutional layers,use,L2 regularization
translation,114,134,experimental-setup,L2 regularization,with,rate of 1 ? 10 ?6
translation,114,134,experimental-setup,L2 regularization,with,rate of 1 ? 10 ?5
translation,114,134,experimental-setup,L2 regularization,with,rate of 1 ? 10 ?5
translation,114,134,experimental-setup,trainable embeddings,use,L2 regularization
translation,114,134,experimental-setup,L2 regularization,with,rate of 1 ? 10 ?5
translation,114,134,experimental-setup,Experimental setup,has,biLSTM and convolutional layers
translation,114,136,experimental-setup,cross-entropy loss,all parts of,system
translation,114,136,experimental-setup,Experimental setup,use,cross-entropy loss
translation,114,138,experimental-setup,final loss,is,weighted sum of losses
translation,114,138,experimental-setup,0.05,for,part- of-speech tagging
translation,114,138,experimental-setup,0.05,for,lemmatization
translation,114,138,experimental-setup,? 0.2,for,morphological features prediction
translation,114,138,experimental-setup,? 0.2,for,arc prediction
translation,114,138,experimental-setup,? 0.2,for,arc prediction
translation,114,138,experimental-setup,? 0.2,for,arc prediction
translation,114,138,experimental-setup,0.05,for,lemmatization
translation,114,138,experimental-setup,0.2,for,arc prediction
translation,114,138,experimental-setup,0.8,for,label prediction
translation,114,138,experimental-setup,Experimental setup,has,final loss
translation,114,139,experimental-setup,"ADAM ( Kingma and Ba , 2014 )",with,learning rate
translation,114,139,experimental-setup,learning rate,equal to,0.002 and ? 1 = ? 2 = 0.9
translation,114,143,experimental-setup,model,trained for,maximum
translation,114,143,experimental-setup,maximum,of,400 epochs
translation,114,143,experimental-setup,learning rate,reduced twice by,factor of two
translation,114,143,experimental-setup,validation score,reaches,plateau
translation,114,143,experimental-setup,Experimental setup,has,model
translation,114,143,experimental-setup,Experimental setup,has,learning rate
translation,114,171,experiments,our system,performs,very well
translation,114,171,experiments,very well,on,Czech ' cs fictree ' treebank and English 'en gum ' treebank
translation,114,171,experiments,very well,on,Latin ' la ittb ' treebank
translation,114,171,experiments,1st place,in,BLEX
translation,114,171,experiments,1st place,in,LAS
translation,114,171,experiments,1st place,in,MLAS and BLEX
translation,114,171,experiments,1st place,in,MLAS and BLEX
translation,114,171,experiments,2nd place,in,LAS
translation,114,171,experiments,1st place,in,MLAS and BLEX
translation,114,171,experiments,big treebanks,has,our system
translation,114,171,experiments,Latin ' la ittb ' treebank,has,1st place
translation,114,5,model,dependency parser,based on,features
translation,114,5,model,features,extracted by,biL - STM network
translation,114,7,model,novelty,use of,additional loss function
translation,114,7,model,novelty,use of,self-training
translation,114,7,model,additional loss function,reduces,number of cycles
translation,114,7,model,number of cycles,in,predicted dependency graphs
translation,114,7,model,self-training,to increase,system performance
translation,114,7,model,Model,use of,additional loss function
translation,114,7,model,Model,has,novelty
translation,114,110,model,character level embedding,calculated with,three convolutional layers
translation,114,110,model,three convolutional layers,with,"512 , 128 and 64 filters"
translation,114,110,model,"512 , 128 and 64 filters",with,dilation rates
translation,114,110,model,dilation rates,equal to,"1 , 2 and 4"
translation,114,110,model,Model,has,character level embedding
translation,114,122,model,lemmatizer,uses,three convolutional layers
translation,114,122,model,three convolutional layers,with,256 filters
translation,114,122,model,three convolutional layers,with,dilation rates
translation,114,122,model,dilation rates,equal to,"1 , 2 and 4"
translation,114,122,model,Model,has,lemmatizer
translation,114,124,model,final convolutional layer,with,kernel size
translation,114,124,model,kernel size,equal to,1
translation,114,124,model,Model,has,final convolutional layer
translation,114,142,model,sentence length,forces,model
translation,114,142,model,model,to focus on,longer ( and usually more difficult ) sentences
translation,114,142,model,Model,has,Each observation ( i.e. sentence )
translation,114,144,model,general model,trained on,all sentences
translation,114,144,model,all sentences,from,treebanks
translation,114,144,model,model,fine-tuned for,each treebank
translation,114,144,model,languages with multiple treebanks,has,model
translation,114,144,model,Model,For,languages with multiple treebanks
translation,114,8,results,proposed system,i.e.,ICS PAS ( Warszawa )
translation,114,8,results,proposed system,ranked,3th / 4th
translation,114,8,results,3th / 4th,in,official evaluation
translation,114,8,results,73.02,has,LAS )
translation,114,8,results,64.44,has,BLEX )
translation,114,8,results,Results,has,proposed system
translation,114,166,results,system,ranks,3th / 4th
translation,114,166,results,3th / 4th,for,all three main metrics
translation,114,166,results,3th / 4th,ex aequo with LATTICE and UDPipe Future for,LAS
translation,114,170,results,LAS score,ranks,3rd
translation,114,170,results,our system,ranks,3rd
translation,114,170,results,LAS score,has,our system
translation,114,170,results,Results,With respect to,LAS score
translation,114,173,results,other fusional languages,e.g.,Galician
translation,114,173,results,other fusional languages,e.g.,Ancient Greek
translation,114,173,results,other fusional languages,e.g.,Polish
translation,114,173,results,other fusional languages,e.g.,Ukrainian
translation,114,173,results,other fusional languages,e.g.,Dutch
translation,114,173,results,other fusional languages,e.g.,Swedish
translation,114,173,results,other fusional languages,e.g.,French
translation,114,173,results,other fusional languages,e.g.,Italian
translation,114,173,results,other fusional languages,e.g.,Spanish
translation,114,173,results,other fusional languages,e.g.,Basque
translation,114,173,results,other fusional languages,e.g.,our system
translation,114,173,results,other fusional languages,provides,quite satisfying results
translation,114,173,results,our system,provides,quite satisfying results
translation,114,173,results,Results,for,other fusional languages
translation,114,194,results,results,of,models
translation,114,194,results,models,estimated on,training data
translation,114,194,results,performance,of,system
translation,114,194,results,self-training,has,significantly increases
translation,114,194,results,significantly increases,has,performance
translation,114,194,results,Results,Comparing,results
translation,114,194,results,Results,Comparing,models
translation,114,194,results,Results,of,models
translation,114,194,results,Results,notice,self-training
translation,114,195,results,increase,for,all metrics
translation,114,195,results,all metrics,for,all treebanks
translation,114,195,results,all treebanks,except for,gsd
translation,114,195,results,gsd,has,( Chinese )
translation,115,84,ablation-analysis,model parameters,necessary for,training
translation,115,84,ablation-analysis,model parameters,reduced by,factor of 7
translation,115,84,ablation-analysis,model,reduced by,factor of 7
translation,115,84,ablation-analysis,training,has,model
translation,115,84,ablation-analysis,Ablation analysis,number of,model parameters
translation,115,100,ablation-analysis,domain 's training set size,relative improvement in,performance
translation,115,100,ablation-analysis,DOMAINENCODING accuracy,compared to,INDEP
translation,115,100,ablation-analysis,performance,has,DOMAINENCODING accuracy
translation,115,100,ablation-analysis,Ablation analysis,observed,domain 's training set size
translation,115,71,hyperparameters,model,for,30 epochs
translation,115,71,hyperparameters,model,with,initial learning rate
translation,115,71,hyperparameters,model,halved,learning rate
translation,115,71,hyperparameters,initial learning rate,of,0.1
translation,115,71,hyperparameters,learning rate,every,5 epochs
translation,115,71,hyperparameters,learning rate,starting from,epoch 15
translation,115,71,hyperparameters,Hyperparameters,trained,model
translation,115,71,hyperparameters,Hyperparameters,halved,learning rate
translation,115,72,hyperparameters,word vectors,for,words
translation,115,72,hyperparameters,words,occur only once in,training set
translation,115,72,hyperparameters,Hyperparameters,replaced,word vectors
translation,115,5,model,structural regularities,in,language
translation,115,5,model,structural regularities,train,semantic parsers
translation,115,5,model,semantic parsers,over,multiple knowledge - bases ( KBs )
translation,115,5,model,semantic parsers,while sharing,information
translation,115,5,model,information,across,datasets
translation,115,5,model,language,has,in different domains
translation,115,5,model,Model,exploit,structural regularities
translation,115,5,model,Model,train,semantic parsers
translation,115,11,model,orthogonal solution,to pool,examples
translation,115,11,model,examples,from,multiple datasets
translation,115,11,model,examples,train,model
translation,115,11,model,multiple datasets,in,different domains
translation,115,11,model,model,over,all examples
translation,115,11,model,Model,suggest,orthogonal solution
translation,115,17,model,models,share,representations
translation,115,17,model,representations,across,domains
translation,115,17,model,representations,during,decoding of logical form
translation,115,17,model,domains,during,encoding of language
translation,115,17,model,Model,examine,models
translation,115,17,model,Model,share,representations
translation,115,6,results,parsing accuracy,by training,single sequence - tosequence model
translation,115,6,results,single sequence - tosequence model,over,multiple KBs
translation,115,6,results,single sequence - tosequence model,when providing,encoding
translation,115,6,results,encoding,of,domain
translation,115,6,results,encoding,at,decoding time
translation,115,6,results,domain,at,decoding time
translation,115,6,results,substantially improve,has,parsing accuracy
translation,115,81,results,multiple KBs,improves,average accuracy
translation,115,81,results,average accuracy,over,all domains
translation,115,81,results,all domains,for,all our proposed models
translation,115,81,results,improves,as,more parameters
translation,115,81,results,performance,has,improves
translation,115,81,results,more parameters,has,are shared
translation,115,81,results,Results,training on,multiple KBs
translation,115,82,results,strongest results,come when,parameter sharing
translation,115,82,results,parameter sharing,is,maximal
translation,115,82,results,one-hot domain representation,at,decoding time ( DOMAINENCODING )
translation,115,82,results,maximal,has,"i.e. , single encoder and single decoder"
translation,115,82,results,Results,has,strongest results
translation,115,95,results,DOMAINEN -CODING,obtain,accuracy
translation,115,95,results,accuracy,of,79.1 %
translation,115,95,results,79.1 %,comparing to,79.6 %
translation,115,95,results,Results,train,DOMAINEN -CODING
translation,116,195,ablation-analysis,subcat features,is,not beneficial
translation,116,195,ablation-analysis,not beneficial,as,attachment scores
translation,116,195,ablation-analysis,precision,has,decrease
translation,116,195,ablation-analysis,Ablation analysis,use of,subcat features
translation,116,205,ablation-analysis,subcat features,help,increasing precision
translation,116,205,ablation-analysis,systematically outperforming,has,baselines
translation,116,205,ablation-analysis,Ablation analysis,has,subcat features
translation,116,216,ablation-analysis,positive impact,on,prediction
translation,116,216,ablation-analysis,prediction,of,MORPH dependency
translation,116,216,ablation-analysis,negative effect,on,attachment scores
translation,116,216,ablation-analysis,subcat features,has,positive impact
translation,116,216,ablation-analysis,Ablation analysis,introduction of,subcat features
translation,116,221,ablation-analysis,important impact,on,quality of the prediction
translation,116,221,ablation-analysis,subcat features,has,important impact
translation,116,221,ablation-analysis,Ablation analysis,introduction of,subcat features
translation,116,6,model,joint dependency parsing and multiword expressions identification,in which,complex function words
translation,116,6,model,complex function words,represented as,individual tokens
translation,116,6,model,individual tokens,linked with,morphological dependencies
translation,116,6,model,Model,for,joint dependency parsing and multiword expressions identification
translation,116,7,model,parser,includes,standard secondorder features
translation,116,7,model,parser,includes,verbal subcategorization features
translation,116,7,model,verbal subcategorization features,derived from,syntactic lexicon
translation,116,20,model,segmentation decisions,into,linking decisions
translation,116,20,model,Model,jointly parse and tokenize,MWEs
translation,116,203,results,our parser,without,subcat features
translation,116,203,results,two cases,has,per-construction majority baseline ( indiv. )
translation,116,203,results,per-construction majority baseline ( indiv. ),has,outperforms
translation,116,203,results,outperforms,has,our parser
translation,116,203,results,Results,in,two cases
translation,116,206,results,beneficial but limited impact,on,results
translation,116,206,results,beneficial but limited impact,increasing,precision
translation,116,206,results,subcat features,has,beneficial but limited impact
translation,116,206,results,lowering,has,bit recall
translation,116,206,results,Results,introduction of,subcat features
translation,116,207,results,more precise,than,Stanford parser
translation,116,207,results,more precise,at predicting,MORPH links
translation,116,207,results,MORPH links,specially for,bien que and en-Baseline prec.
translation,116,207,results,Results,has,our models
translation,116,215,results,frequency,of,de +DET constructions
translation,116,215,results,de +DET constructions,higher than,ADV + que constructions
translation,116,215,results,Results,shows that,frequency
translation,116,218,results,prediction,of,correct structure
translation,116,218,results,correct structure,of,de +DET constructions
translation,116,218,results,correct structure,of,ADV + que constructions
translation,116,218,results,more difficult,than,ADV + que constructions
translation,116,218,results,ADV + que constructions,for,parser
translation,116,218,results,Results,reveals,prediction
translation,116,222,results,subcat features,slightly improves,identification of de les
translation,116,222,results,Results,use of,subcat features
translation,117,178,ablation-analysis,all auxiliary tasks,contributed,less
translation,117,178,ablation-analysis,less,than,DM and UD ++
translation,117,178,ablation-analysis,best scores,in,in- domain UCCA parsing
translation,117,178,ablation-analysis,best scores,with,74.9 % F 1
translation,117,178,ablation-analysis,74.9 % F 1,on,primary edges
translation,117,178,ablation-analysis,Ablation analysis,Using,all auxiliary tasks
translation,117,185,ablation-analysis,3.7 % error reduction,in,French
translation,117,185,ablation-analysis,French and German in - domain parsing,has,3.7 % error reduction
translation,117,185,ablation-analysis,French and German in - domain parsing,has,1 %
translation,117,160,hyperparameters,embeddings,has,randomly
translation,117,160,hyperparameters,Hyperparameters,initialize,embeddings
translation,117,161,hyperparameters,"dropout ( Srivastava et al. , 2014 )",between,MLP layers
translation,117,161,hyperparameters,"dropout ( Srivastava et al. , 2014 )",between,BiLSTM layers
translation,117,161,hyperparameters,"recurrent dropout ( Gal and Ghahramani , 2016 )",between,BiLSTM layers
translation,117,161,hyperparameters,Hyperparameters,use,"dropout ( Srivastava et al. , 2014 )"
translation,117,161,hyperparameters,Hyperparameters,use,"recurrent dropout ( Gal and Ghahramani , 2016 )"
translation,117,163,hyperparameters,node dropout,with,probability of 0.1
translation,117,163,hyperparameters,probability of 0.1,at,each step
translation,117,163,hyperparameters,all features,associated with,single node
translation,117,163,hyperparameters,all features,replaced with,zero vectors
translation,117,163,hyperparameters,single node,in,parser state
translation,117,163,hyperparameters,novel form of dropout,has,node dropout
translation,117,164,hyperparameters,optimization,use,minibatch size
translation,117,164,hyperparameters,optimization,train with,stochastic gradient descent
translation,117,164,hyperparameters,minibatch size,of,100
translation,117,164,hyperparameters,weights,by,10 ?5
translation,117,164,hyperparameters,10 ?5,at,each update
translation,117,164,hyperparameters,stochastic gradient descent,for,N epochs
translation,117,164,hyperparameters,stochastic gradient descent,for,N epochs
translation,117,164,hyperparameters,stochastic gradient descent,with,learning rate
translation,117,164,hyperparameters,stochastic gradient descent,followed by,"AMS - Grad ( Sashank J. Reddi , 2018 )"
translation,117,164,hyperparameters,stochastic gradient descent,for,N epochs
translation,117,164,hyperparameters,N epochs,with,learning rate
translation,117,164,hyperparameters,learning rate,of,0.1
translation,117,164,hyperparameters,"AMS - Grad ( Sashank J. Reddi , 2018 )",for,N epochs
translation,117,164,hyperparameters,N epochs,with,"? = 0.001 , ? 1 = 0.9 and ? 2 = 0.999"
translation,117,164,hyperparameters,Hyperparameters,For,optimization
translation,117,164,hyperparameters,Hyperparameters,train with,stochastic gradient descent
translation,117,165,hyperparameters,N = 50,for,English and German
translation,117,165,hyperparameters,N = 400,for,French
translation,117,165,hyperparameters,Hyperparameters,use,N = 50
translation,117,165,hyperparameters,Hyperparameters,use,N = 400
translation,117,15,model,general transition - based DAG parser,able to parse,UCCA
translation,117,15,model,general transition - based DAG parser,able to parse,AMR
translation,117,15,model,general transition - based DAG parser,able to parse,SDP and UD
translation,117,15,model,Model,propose,general transition - based DAG parser
translation,117,7,results,UCCA parsing,in both,in - domain and out -of- domain settings
translation,117,7,results,multitask learning,has,significantly improves
translation,117,7,results,significantly improves,has,UCCA parsing
translation,117,7,results,Results,show that,multitask learning
translation,117,16,results,parser,using,MTL
translation,117,16,results,significant improvements,on,UCCA parsing
translation,117,16,results,significant improvements,over,single - task training
translation,117,16,results,outof-domain settings,in,English
translation,117,16,results,in- domain setting,in,German
translation,117,16,results,in- domain setting,in,French
translation,117,16,results,Results,train,parser
translation,117,175,results,Results,on,English indomain Wiki test set
translation,117,176,results,MTL,with,all auxiliary tasks and their combinations
translation,117,176,results,all auxiliary tasks and their combinations,improves,primary F 1 score
translation,117,176,results,primary F 1 score,over,single task baseline
translation,117,176,results,Results,has,MTL
translation,117,177,results,improvement,is,statistically significant
translation,117,177,results,most settings,has,improvement
translation,117,177,results,Results,In,most settings
translation,117,179,results,Remote F 1,improved,some settings
translation,117,179,results,Results,has,Remote F 1
translation,117,180,results,baseline single - task model ( Single ),is,slightly better
translation,117,180,results,slightly better,than,current state - of- the - art
translation,117,180,results,Results,Note,baseline single - task model ( Single )
translation,117,181,results,20K corpora,in,three languages
translation,117,181,results,Results,on,20K corpora
translation,117,182,results,improvements,from using,MTL
translation,117,182,results,MTL,are,even more marked
translation,117,182,results,English out - of- domain,has,improvements
translation,117,182,results,Results,For,English out - of- domain
translation,117,183,results,improvement,is,largely additive
translation,117,183,results,best model,using,all three auxiliary tasks ( All )
translation,117,183,results,best model,yields,error reduction
translation,117,183,results,error reduction,of,2.9 %
translation,117,183,results,largely additive,has,best model
translation,117,183,results,Results,has,improvement
translation,117,184,results,single - task baseline,slightly better than,HAR17
translation,117,184,results,Results,has,single - task baseline
translation,117,198,results,average improvements,adding,each of the tasks
translation,117,198,results,average improvements,find,AMR
translation,117,198,results,average improvements,find,DM
translation,117,198,results,most beneficial,in,in - domain and out - of- domain settings
translation,117,198,results,AMR,has,least beneficial
translation,117,198,results,DM,has,most beneficial
translation,117,198,results,Results,Comparing,average improvements
translation,117,202,results,full MTL model,on,unlabeled auxiliary tasks
translation,117,202,results,full MTL model,yielded,64.7 % unlabeled Smatch F 1
translation,117,202,results,64.7 % unlabeled Smatch F 1,on,AMR development set
translation,117,202,results,64.7 % unlabeled Smatch F 1,when using,oracle concept identification
translation,117,202,results,27.2 % unlabeled F 1,on,DM development set
translation,117,202,results,4.9 % UAS,on,UD development set
translation,117,202,results,Results,Evaluating,full MTL model
translation,118,166,experiments,state - of - the - art performance,on,public Dolphin question set
translation,118,166,experiments,absolute recall improvement,of,nearly 5 %
translation,118,166,experiments,nearly 5 %,with,small loss
translation,118,166,experiments,small loss,in,precision
translation,118,166,experiments,EUCLID,has,state - of - the - art performance
translation,118,5,model,tree transducer cascade,as,basic architecture
translation,118,5,model,our system,called,EU - CLID
translation,118,5,model,our system,propagates,uncertainty
translation,118,5,model,uncertainty,from,multiple sources
translation,118,5,model,uncertainty,until,confidently resolved
translation,118,5,model,uncertainty,can be,confidently resolved
translation,118,5,model,tree transducer cascade,has,our system
translation,118,5,model,basic architecture,has,our system
translation,118,5,model,Model,By using,tree transducer cascade
translation,118,45,model,complex discourse semantics and anaphoric phenomena,found in,Math SAT questions
translation,118,45,model,Model,handle,complex discourse semantics and anaphoric phenomena
translation,118,7,results,EUCLID,to,public Dolphin algebra question set
translation,118,7,results,state - of - the - art F 1 - score,from,73.9 %
translation,118,7,results,73.9 %,to,77.0 %
translation,118,7,results,Results,apply,EUCLID
translation,118,21,results,end-to- end system,called,EUCLID
translation,118,21,results,end-to- end system,achieves,43 % recall
translation,118,21,results,end-to- end system,achieves,91 % precision
translation,118,21,results,91 % precision,on,SAT closed - vocabulary algebra questions
translation,118,21,results,Results,produce,end-to- end system
translation,118,139,results,Our basic system,obtains,66.3 % recall
translation,118,139,results,66.3 % recall,on,development questions
translation,118,139,results,Results,has,Our basic system
translation,118,155,results,trained parse ranker module,improves,performance
translation,118,155,results,performance,on,Dolphin development set
translation,118,155,results,Dolphin development set,to,75.7 % recall
translation,118,155,results,Dolphin development set,to,85.5 % precision
translation,118,155,results,97.3 % precision,from,66.3 % recall
translation,118,155,results,97.3 % precision,from,85.5 % precision
translation,118,155,results,Results,Adding,trained parse ranker module
translation,118,157,results,closed - vocabulary algebra subsets,of,math SAT question sets
translation,118,157,results,Results,for,closed - vocabulary algebra subsets
translation,118,157,results,Results,for,Dolphin question sets
translation,118,158,results,EUCLID,generalizes,reasonably well
translation,118,158,results,EUCLID,achieving,approximately 60 %
translation,118,158,results,reasonably well,to,blind SAT questions
translation,118,158,results,approximately 60 %,of,system 's recall
translation,118,158,results,approximately 60 %,at,precision
translation,118,158,results,system 's recall,on,training questions
translation,118,158,results,precision,of,approximately 91 %
translation,118,158,results,Results,has,EUCLID
translation,119,102,experimental-setup,word,have,50 dimensions
translation,119,102,experimental-setup,Experimental setup,In,lemma-suffix model
translation,119,103,experimental-setup,forward and backward word vectors,of,suffix of a word
translation,119,103,experimental-setup,forward and backward word vectors,have,50 dimensions
translation,119,103,experimental-setup,suffix of a word,have,50 dimensions
translation,119,103,experimental-setup,Experimental setup,has,forward and backward word vectors
translation,119,104,experimental-setup,morphological features model,each of,forward and backward word vectors
translation,119,104,experimental-setup,forward and backward word vectors,of,word
translation,119,104,experimental-setup,word,have,50 dimensions
translation,119,104,experimental-setup,Experimental setup,In,morphological features model
translation,119,105,experimental-setup,four morphological feature vectors,have,25 dimensions
translation,119,105,experimental-setup,Experimental setup,Each of,four morphological feature vectors
translation,119,6,experiments,LSTM - based dependency parser,with,character - based word embeddings
translation,119,139,experiments,lemma-suffix embedding model,applied to,"Danish , Hungarian , Kazakh , Turkish , and Uyghur languages"
translation,119,4,model,two word representation models,for,agglutinative languages
translation,119,4,model,Model,propose,two word representation models
translation,119,11,model,morphologically enhanced character - based word embeddings,to improve,parsing performance
translation,119,11,model,parsing performance,especially for,agglutinative languages
translation,119,11,model,Model,propose,morphologically enhanced character - based word embeddings
translation,119,12,model,our approach,to,transitionbased dependency parser
translation,119,12,model,transitionbased dependency parser,uses,stack Long Short Term Memory structures ( LSTMs )
translation,119,12,model,stack Long Short Term Memory structures ( LSTMs ),to predict,parser state
translation,119,12,model,Model,apply,our approach
translation,119,101,model,original character - based embedding model,with,our embedding models
translation,119,101,model,Model,replaced,original character - based embedding model
translation,119,117,results,"LAS , MLAS , and BLEX results",in,CoNLL-18 Shared Task
translation,119,117,results,Results,has,Shared Task
translation,119,130,results,our morphological features model,outperforms,baseline model
translation,119,130,results,baseline model,in terms of,parsing scores
translation,119,140,results,best performance,reached in,Hungarian language
translation,119,140,results,Hungarian language,with,more than 4 % increase
translation,119,140,results,more than 4 % increase,in,LAS score
translation,119,140,results,Results,has,best performance
translation,119,141,results,baseline,in,Turkish
translation,119,141,results,Our model,has,outperforms
translation,119,141,results,outperforms,has,baseline
translation,119,141,results,Results,has,Our model
translation,119,151,results,pre-trained word vectors,increases,parsing performance
translation,119,151,results,parsing performance,by,great extent
translation,119,151,results,parsing performance,for,Turkish
translation,119,151,results,great extent,for,Turkish
translation,119,151,results,Results,usage of,pre-trained word vectors
translation,119,152,results,Facebook word vectors,has,outperform
translation,119,152,results,outperform,has,CoNLL -17 UD word vectors
translation,119,152,results,Results,observe,Facebook word vectors
translation,120,163,baselines,UD - Pipe,performing,parsing
translation,120,163,baselines,trainable pipeline,for processing,CoNLL -U files
translation,120,163,baselines,CoNLL -U files,performing,tokenization
translation,120,163,baselines,CoNLL -U files,performing,morphological analysis
translation,120,163,baselines,CoNLL -U files,performing,POS tagging
translation,120,163,baselines,CoNLL -U files,performing,parsing
translation,120,163,baselines,UD - Pipe,has,trainable pipeline
translation,120,163,baselines,Baselines,has,UD - Pipe
translation,120,18,hyperparameters,word and context embeddings,from,initial representer
translation,120,18,hyperparameters,word and context embeddings,as,initial representer
translation,120,18,hyperparameters,Hyperparameters,employ,word and context embeddings
translation,120,4,model,tree-stack LSTM,to model,state of a transition based parser
translation,120,4,model,state of a transition based parser,with,recurrent neural networks
translation,120,4,model,Model,introduce,tree-stack LSTM
translation,120,9,model,Tree - RNN,updates,embeddings
translation,120,9,model,embeddings,based on,transitions
translation,120,9,model,Model,has,Tree - RNN
translation,120,10,results,our model,improves,performance
translation,120,10,results,performance,with,low resource languages
translation,120,10,results,low resource languages,compared with,predecessors
translation,120,10,results,Results,show,our model
translation,121,6,model,small set of actions,that derive,AMR subgraphs
translation,121,6,model,AMR subgraphs,by,transformations
translation,121,6,model,transformations,on,spans of text
translation,121,6,model,Model,propose,small set of actions
translation,121,226,results,end-to - end performance,compared to,rule- based approach
translation,121,226,results,rule- based approach,improving,F 1
translation,121,226,results,F 1,by,roughly 1 point ( 64/59/61 P/R/ F 1 to 65/59/62 P/R/ F 1 )
translation,121,226,results,our informativeness - based alignment objective,has,slightly improves
translation,121,226,results,slightly improves,has,end-to - end performance
translation,121,226,results,Results,find,our informativeness - based alignment objective
translation,121,227,results,automatic alignments,over,LDC2014T12 corpus
translation,121,227,results,action classifier,achieved,test accuracy
translation,121,227,results,test accuracy,of,0.841
translation,121,227,results,automatic alignments,has,action classifier
translation,121,227,results,LDC2014T12 corpus,has,action classifier
translation,121,227,results,Results,On,automatic alignments
translation,121,230,results,DICT action lookup table,achieved,accuracy
translation,121,230,results,accuracy,of,0.67
translation,121,230,results,Results,has,DICT action lookup table
translation,122,73,experimental-setup,AdaGrad,to optimize,w
translation,122,81,experiments,performances,of,CRF autoencoder
translation,122,81,experiments,CRF autoencoder,using,objective function
translation,122,81,experiments,negative log likelihood,vs. using,our Viterbi version
translation,122,6,model,unsupervised dependency parsing model,based on,CRF autoencoder
translation,122,6,model,Model,develop,unsupervised dependency parsing model
translation,122,7,model,encoder part,of,our model
translation,122,7,model,encoder part,is,discriminative and globally normalized
translation,122,7,model,encoder part,to use,rich features
translation,122,7,model,our model,is,discriminative and globally normalized
translation,122,7,model,our model,to use,rich features
translation,122,7,model,discriminative and globally normalized,to use,rich features
translation,122,7,model,rich features,as well as,universal linguistic priors
translation,122,7,model,Model,has,encoder part
translation,122,24,model,unsupervised discriminative dependency parser,based on,CRF autoencoder framework
translation,122,24,model,Model,propose,unsupervised discriminative dependency parser
translation,122,80,results,our method,achieves,comparable performance
translation,122,80,results,comparable performance,with,state - of - the - art systems
translation,122,82,results,Viterbi version,leads to,much better performance
translation,122,82,results,much better performance,has,55.7 vs. 41.8 in parsing accuracy
translation,122,82,results,Results,find,Viterbi version
translation,122,89,results,our method,achieves,best results
translation,122,89,results,Results,can be seen that,our method
translation,122,90,results,Convex - MST,both with and without,linguistic prior
translation,122,90,results,our method,has,outperforms
translation,122,90,results,outperforms,has,Convex - MST
translation,122,90,results,Results,has,our method
translation,122,91,results,universal linguistic prior,greatly improves,performance
translation,122,91,results,performance,of,Convex - MST and our model
translation,122,91,results,Results,see that,universal linguistic prior
translation,122,91,results,Results,utilizing,universal linguistic prior
translation,123,108,hyperparameters,hidden size,of,1200
translation,123,108,hyperparameters,1200,for,each direction
translation,123,108,hyperparameters,each direction,on,all LSTMs
translation,123,108,hyperparameters,0.3 dropout,in,all the feedforward connections
translation,123,108,hyperparameters,0.3 dropout,in,recurrent connection dropout
translation,123,108,hyperparameters,0.2,has,recurrent connection dropout
translation,123,109,hyperparameters,convolutional filter size,is,2
translation,123,109,hyperparameters,Hyperparameters,has,convolutional filter size
translation,123,110,hyperparameters,number of convolutional channels,is,1200
translation,123,110,hyperparameters,Hyperparameters,has,number of convolutional channels
translation,123,111,hyperparameters,embedding layer,maps,word indexes to word embeddings
translation,123,111,hyperparameters,word indexes to word embeddings,is,randomly initialized
translation,123,112,hyperparameters,word embeddings,sized,400
translation,123,112,hyperparameters,Hyperparameters,has,word embeddings
translation,123,113,hyperparameters,input word embedding,during,training
translation,123,113,hyperparameters,training,with,zero vector
translation,123,113,hyperparameters,zero vector,probability of,0.1
translation,123,113,hyperparameters,Hyperparameters,randomly swap,input word embedding
translation,123,115,hyperparameters,Training,conducted with,Adam algorithm
translation,123,115,hyperparameters,Adam algorithm,with,l2 regularization decay 1 ? 10 ?6
translation,123,115,hyperparameters,Hyperparameters,has,Training
translation,123,127,hyperparameters,hidden size,for,LSTM networks
translation,123,127,hyperparameters,hidden size,set to,1200
translation,123,127,hyperparameters,Hyperparameters,has,hidden size
translation,123,128,hyperparameters,dropout rate,of,0.4
translation,123,128,hyperparameters,dropout rate,of,0.1
translation,123,128,hyperparameters,0.4,on,feed -forward connections
translation,123,128,hyperparameters,0.1,has,recurrent connection dropout
translation,123,129,hyperparameters,1200 channels,with,filter size
translation,123,129,hyperparameters,filter size,of,2
translation,123,129,hyperparameters,convolutional layer,has,1200 channels
translation,123,129,hyperparameters,Hyperparameters,has,convolutional layer
translation,123,130,hyperparameters,400 dimensional,has,word embeddings
translation,123,130,hyperparameters,Hyperparameters,use,400 dimensional
translation,123,131,hyperparameters,randomly swapped,with,zero vector
translation,123,131,hyperparameters,zero vector,with,probability of 0.1
translation,123,131,hyperparameters,training,has,input word embeddings
translation,123,131,hyperparameters,Hyperparameters,During,training
translation,123,132,hyperparameters,l2 regularization,weighted by,1?10 ?6
translation,123,132,hyperparameters,1?10 ?6,parameters of,network
translation,123,132,hyperparameters,Hyperparameters,apply,l2 regularization
translation,123,4,model,Model,propose,novel constituency parsing scheme
translation,123,5,model,vector of real-valued scalars,named,syntactic distances
translation,123,5,model,vector of real-valued scalars,for,each split position
translation,123,5,model,each split position,in,input sentence
translation,123,5,model,Model,predicts,vector of real-valued scalars
translation,123,19,model,"novel , fully - parallel model",for,constituency parsing
translation,123,19,model,Model,propose,"novel , fully - parallel model"
translation,123,20,model,parse tree,from,sentence
translation,123,20,model,parse tree,proceed,top-down manner
translation,123,20,model,top-down manner,recursively splitting,larger constituents
translation,123,20,model,larger constituents,into,smaller constituents
translation,123,20,model,Model,To construct,parse tree
translation,123,27,model,syntactic distances,efficiently done in,O( n log n )
translation,123,27,model,O( n log n ),makes,decoding
translation,123,27,model,syntactic distances,has,to a tree
translation,123,8,results,Our model,achieves,competitive performance
translation,123,8,results,Our model,achieves,outperforms
translation,123,8,results,competitive performance,amongst,"single model , discriminative parsers"
translation,123,8,results,"single model , discriminative parsers",in,PTB dataset
translation,123,8,results,previous models,in,CTB dataset
translation,123,8,results,outperforms,has,previous models
translation,123,8,results,Results,has,Our model
translation,123,117,results,Our model,performs,good performance
translation,123,117,results,Our model,achieves,good performance
translation,123,117,results,good performance,for,single-model constituency parsing
translation,123,117,results,single-model constituency parsing,trained without,external data
translation,123,117,results,Results,has,Our model
translation,123,140,results,model,trained with,MSE loss
translation,123,140,results,model,trained with,rank loss
translation,123,140,results,model,trained with,rank loss
translation,123,140,results,model,trained with,rank loss
translation,123,140,results,MSE loss,has,underperforms
translation,123,140,results,underperforms,has,considerably
translation,123,140,results,underperforms,has,model
translation,123,140,results,considerably,has,model
translation,123,140,results,Results,has,model
translation,124,167,baselines,Berkeley - Tags,improved version of,Berkeley parser
translation,124,167,baselines,Berkeley parser,designed for,shared task
translation,124,167,baselines,Baselines,has,Berkeley - Tags
translation,124,95,hyperparameters,Hyperparameters,set,momentum term
translation,124,96,hyperparameters,minibatch size,of,200 trees
translation,124,5,model,CRF,factors over,anchored rule productions
translation,124,5,model,CRF,instead of,linear potential functions
translation,124,5,model,linear potential functions,based on,sparse features
translation,124,5,model,linear potential functions,use,nonlinear potentials
translation,124,5,model,nonlinear potentials,computed via,feedforward neural network
translation,124,5,model,Model,is,CRF
translation,124,5,model,Model,structurally,CRF
translation,124,5,model,Model,instead of,linear potential functions
translation,124,5,model,Model,use,nonlinear potentials
translation,124,16,model,CRF constituency parser,where,individual anchored rule productions
translation,124,16,model,scored,based on,nonlinear features
translation,124,16,model,nonlinear features,computed with,feedforward neural network
translation,124,16,model,Model,present,CRF constituency parser
translation,124,17,model,"separate , identicallyparameterized replicate",exists,each possible span and split point
translation,124,17,model,"separate , identicallyparameterized replicate",for,each possible span and split point
translation,124,17,model,"separate , identicallyparameterized replicate",has,of the network
translation,124,17,model,Model,has,"separate , identicallyparameterized replicate"
translation,124,8,results,neural CRF,already exceeds,strong baseline CRF model
translation,124,8,results,dense features,has,neural CRF
translation,124,8,results,Results,Using,dense features
translation,124,26,results,neural CRF model,obtains,high performance
translation,124,26,results,dense learned features alone,has,neural CRF model
translation,124,26,results,outperforming,has,CRF parser
translation,124,26,results,Results,Using,dense learned features alone
translation,124,27,results,resulting model,gets,91.1 F 1
translation,124,27,results,resulting model,gets,outperforming
translation,124,27,results,91.1 F 1,on,section 23
translation,124,27,results,section 23,of,Penn Treebank
translation,124,27,results,sparse indicators,has,resulting model
translation,124,27,results,outperforming,has,parser
translation,124,142,results,rectified linear units,perform,best
translation,124,142,results,best,followed by,tanh units
translation,124,142,results,best,followed by,cubic units
translation,124,142,results,tanh units,followed by,cubic units
translation,124,142,results,Results,see that,rectified linear units
translation,124,147,results,purely linear architecture ( 0 HL ),performs,surprisingly well
translation,124,147,results,purely linear architecture ( 0 HL ),is,less effective
translation,124,147,results,purely linear architecture ( 0 HL ),still,less effective
translation,124,147,results,less effective,than,network
translation,124,147,results,network,with,one hidden layer
translation,124,147,results,Results,shows,purely linear architecture ( 0 HL )
translation,124,168,results,Best,is,reranked ensemble
translation,124,168,results,Best,constitutes,best published numbers
translation,124,168,results,reranked ensemble,of,modified Berkeley parsers
translation,124,168,results,Results,has,Best
translation,124,170,results,strong baselines,such as,"Berkeley Parser ( Petrov and Klein , 2007 )"
translation,124,170,results,strong baselines,such as,"CVG Stanford parser ( Socher et al. , 2013 )"
translation,124,170,results,outperform,has,strong baselines
translation,124,170,results,Results,has,outperform
translation,124,174,results,Our method,achieves,performance
translation,124,174,results,close,to,parser
translation,124,174,results,performance,has,close
translation,124,174,results,Results,has,Our method
translation,124,182,results,our system,improves upon,performance
translation,124,182,results,performance,of,parser
translation,124,182,results,parser,from,Hall et al . ( 2014 )
translation,124,182,results,top single parser,from,"shared task ( Crabb? and Seddah , 2014 )"
translation,124,182,results,robust improvements,on,all languages
translation,125,163,ablation-analysis,10 - topic model,profits from,soft clustering
translation,125,163,ablation-analysis,soft clustering,indicates that,soft clustering
translation,125,163,ablation-analysis,soft clustering,alleviate,data sparseness problem
translation,125,163,ablation-analysis,data sparseness problem,of,POS tagging experts
translation,125,163,ablation-analysis,POS tagging experts,for,larger numbers of topics
translation,125,163,ablation-analysis,Ablation analysis,has,10 - topic model
translation,125,5,model,Model,create,POS tagging and dependency parsing experts for heterogeneous data
translation,125,23,results,improvement,of,0.3 percent points
translation,125,23,results,0.3 percent points,over,full training set
translation,125,23,results,Results,For,POS tagging
translation,125,150,results,experts,perform,better
translation,125,150,results,better,than,both baselines
translation,125,150,results,model,trained on,full training set
translation,125,150,results,model,with,"randomly chosen "" topics"
translation,125,150,results,model,with,"randomly chosen "" topics"
translation,125,150,results,2 - topic setting,has,experts
translation,125,150,results,Results,assume,2 - topic setting
translation,125,151,results,2 - topic expert model,reaches,accuracy
translation,125,151,results,accuracy,of,96.95 %
translation,125,151,results,slightly higher,than,full training set accuracy
translation,125,151,results,full training set accuracy,of,96.69 %
translation,125,151,results,Results,has,2 - topic expert model
translation,125,154,results,increase,of,0.54 percent points
translation,125,154,results,0.54 percent points,over,accuracy
translation,125,154,results,accuracy,of,2 random split setting
translation,125,155,results,random split,of,same size
translation,125,155,results,random split,by,0.9 percent points
translation,125,155,results,same size,by,0.9 percent points
translation,125,155,results,10 - topic setting,has,topic expert model
translation,125,155,results,topic expert model,has,outperforms
translation,125,155,results,outperforms,has,random split
translation,125,155,results,Results,For,10 - topic setting
translation,125,157,results,topic expert model,does not reach,accuracy
translation,125,157,results,accuracy,of,baseline
translation,125,157,results,baseline,using,full training set
translation,125,157,results,Results,has,topic expert model
translation,125,171,results,hard clustering results,indicate,2 - topic expert model
translation,125,171,results,2 - topic expert model,reaches,improvement
translation,125,171,results,improvement,over,baseline
translation,125,171,results,baseline,using,full training set
translation,125,171,results,full training set,for,labeled attachment score ( LAS )
translation,125,171,results,full training set,for,unlabeled attachment score ( UAS )
translation,125,171,results,Results,has,hard clustering results
translation,125,172,results,increase,of,around 2 %
translation,125,172,results,increase,of,0.43 %
translation,125,172,results,around 2 %,over,baseline
translation,125,172,results,baseline,for,LAS
translation,125,172,results,increase,of,0.43 %
translation,125,172,results,0.43 %,for,UAS
translation,125,172,results,Results,find,increase
translation,125,172,results,Results,find,increase
translation,125,174,results,LAS,for,UAS
translation,125,174,results,difference,is,0.29 percent points
translation,125,174,results,difference,is,1.41 percent points
translation,125,174,results,difference,for,UAS
translation,125,174,results,difference,is,1.41 percent points
translation,125,174,results,difference,is,1.41 percent points
translation,125,174,results,LAS,has,difference
translation,125,174,results,LAS,has,difference
translation,125,174,results,UAS,has,difference
translation,125,174,results,Results,For,LAS
translation,125,174,results,Results,for,UAS
translation,125,176,results,outperform,with,gain
translation,125,176,results,gain,of,more than 3 percent points
translation,125,176,results,2 - topic and the 10topic experts,has,outperform
translation,125,176,results,outperform,has,random split baseline
translation,125,176,results,Results,Both,2 - topic and the 10topic experts
translation,125,176,results,Results,has,2 - topic and the 10topic experts
translation,125,177,results,soft clustering results,same trends as in,POS tagging experiments
translation,125,177,results,Results,has,soft clustering results
translation,125,178,results,outperforms,by,1.19 percent points
translation,125,178,results,full baseline,by,1.19 percent points
translation,125,178,results,2 - topic setting,has,soft clustering
translation,125,178,results,soft clustering,has,outperforms
translation,125,178,results,outperforms,has,full baseline
translation,125,178,results,Results,For,2 - topic setting
translation,125,180,results,soft clustering,as,hard clustering setting
translation,125,180,results,10 - topic setting,has,soft clustering
translation,125,180,results,soft clustering,has,outperforms
translation,125,180,results,outperforms,has,full baseline
translation,125,180,results,Results,In,10 - topic setting
translation,125,191,results,2 - topic setting,using,topic modeling experts
translation,125,191,results,topic modeling experts,on,POS level
translation,125,191,results,topic modeling experts,on,parsing level
translation,125,191,results,topic modeling experts,as well as on,parsing level
translation,125,191,results,topic modeling experts,reaches,highest results
translation,125,191,results,improvement,of,around 2 %
translation,125,191,results,around 2 %,in,LAS
translation,125,191,results,around 2 %,in,full baseline parser
translation,125,191,results,around 2 %,in comparison to,full baseline parser
translation,125,191,results,around 2 %,from,86.70 %
translation,125,191,results,86.70 %,has,to 88.35 %
translation,125,191,results,Results,in,2 - topic setting
translation,125,193,results,topic modeling expert,reaches,90.55 %
translation,125,193,results,Results,has,topic modeling expert
translation,125,194,results,topic modeling setting,for,10topic setting
translation,125,194,results,outperforms,does not reach,full baseline
translation,125,194,results,10topic setting,has,outperforms
translation,125,194,results,outperforms,has,random baseline
translation,125,195,results,full POS tagging baseline,along with,topic model parsing experts
translation,125,195,results,full POS tagging baseline,see that,latter model
translation,125,195,results,full topic model,see that,latter model
translation,125,195,results,latter model,reaches,only very minimal gains
translation,125,195,results,only very minimal gains,by using,topic modeling POS tagger
translation,125,195,results,topic modeling POS tagger,when,2 topics
translation,125,195,results,topic modeling POS tagger,when,negative trend
translation,125,195,results,topic modeling POS tagger,use,2 topics
translation,125,195,results,topic modeling POS tagger,use,10 topics
translation,125,195,results,topic modeling POS tagger,have,negative trend
translation,125,200,results,outperform,by,almost 2 percent points
translation,125,200,results,general parser,trained on,full training set
translation,125,200,results,2 - topic parsing experts,has,outperform
translation,125,200,results,outperform,has,general parser
translation,125,200,results,Results,show that,2 - topic parsing experts
translation,126,95,baselines,Our reimplementation,of,graphbased parser
translation,126,95,baselines,Biaffine,has,Our reimplementation
translation,126,95,baselines,Baselines,has,Biaffine
translation,126,100,baselines,"Ours ( domain , lang )",trained with,all six treebanks
translation,126,100,baselines,all six treebanks,of,two languages
translation,126,100,baselines,Baselines,has,"Ours ( domain , lang )"
translation,126,120,baselines,best model,for,fr
translation,126,120,baselines,fr,is,biaffine model
translation,126,120,baselines,biaffine model,without,joint training
translation,126,120,baselines,Baselines,has,best model
translation,126,83,experimental-setup,400-dimentional LSTMs,for,each direction
translation,126,83,experimental-setup,500 - dimentional MLP,for,arc prediction
translation,126,83,experimental-setup,500 - dimentional MLP,for,label prediction
translation,126,83,experimental-setup,500 - dimentional MLP,for,label prediction
translation,126,83,experimental-setup,100 - dimentional MLP,for,label prediction
translation,126,84,experimental-setup,100 - dimensional pre-trained word embeddings,trained by,"word2vec ( Mikolov et al. , 2013 )"
translation,126,84,experimental-setup,100 - dimensional pre-trained word embeddings,trained by,100 - dimensional randomly initialized POS tag embeddings
translation,126,84,experimental-setup,Experimental setup,use,100 - dimensional pre-trained word embeddings
translation,126,84,experimental-setup,Experimental setup,use,100 - dimensional randomly initialized POS tag embeddings
translation,126,85,experimental-setup,adversarial training,fix,?
translation,126,85,experimental-setup,adversarial training,fix,0.5
translation,126,85,experimental-setup,?,to,0.5
translation,126,87,experimental-setup,"dropout ( Srivastava et al. , 2014 )",with,0.33 rate
translation,126,87,experimental-setup,0.33 rate,at,input and output layers
translation,126,87,experimental-setup,Experimental setup,apply,"dropout ( Srivastava et al. , 2014 )"
translation,126,88,experimental-setup,optimization,use,"Adam ( Kingma and Ba , 2014 )"
translation,126,88,experimental-setup,"Adam ( Kingma and Ba , 2014 )",with,batch size
translation,126,88,experimental-setup,"Adam ( Kingma and Ba , 2014 )",with,gradient clipping
translation,126,88,experimental-setup,batch size,of,128
translation,126,88,experimental-setup,gradient clipping,of,5
translation,126,88,experimental-setup,Experimental setup,For,optimization
translation,126,89,experimental-setup,early stopping,based on,performance
translation,126,89,experimental-setup,performance,on,development set
translation,126,96,experimental-setup,"Chainer ( Tokui et al. , 2015 )",for,our implementation
translation,126,96,experimental-setup,Experimental setup,use,"Chainer ( Tokui et al. , 2015 )"
translation,126,4,experiments,CoNLL 2017 shared task,exploits,shared common knowledge
translation,126,4,experiments,shared common knowledge,of,language
translation,126,4,experiments,shared common knowledge,via,domain adaptation technique
translation,126,4,experiments,language,across,different domains
translation,126,146,experiments,UDPipe ( 68.35 ),is,13th rank
translation,126,13,model,approach,as a kind of,domain adaptation
translation,126,13,model,domain adaptation,treat,dominant treebank
translation,126,13,model,domain adaptation,treat,others
translation,126,13,model,dominant treebank,as,source domain
translation,126,13,model,others,as,target domains
translation,126,13,model,Model,formulate,approach
translation,126,13,model,Model,treat,dominant treebank
translation,126,14,model,domain adaptation,call,SharedGateAdvNet
translation,126,14,model,architecture,for,domain adaptation
translation,126,14,model,adversarial training,learns,domain-invariant feature representations
translation,126,14,model,domain-invariant feature representations,through,adversarial domain classifier
translation,126,14,model,Model,call,SharedGateAdvNet
translation,126,15,model,additional neural layer,for,each domain
translation,126,15,model,additional neural layer,captures,domainspecific feature representations
translation,126,17,model,architecture,to obtain,representation
translation,126,17,model,architecture,feed it into,graph - based dependency parsing model
translation,126,17,model,representation,of,each token
translation,126,17,model,each token,of,sentence
translation,126,17,model,graph - based dependency parsing model,where,each dependency arc score
translation,126,17,model,each dependency arc score,calculated using,bilinear attention
translation,126,17,model,Model,utilize,architecture
translation,126,6,results,already outperforms,by,large margin
translation,126,6,results,official baseline model ( UDPipe ),by,large margin
translation,126,6,results,baseline graphbased parser,has,already outperforms
translation,126,6,results,already outperforms,has,official baseline model ( UDPipe )
translation,126,6,results,Results,find,baseline graphbased parser
translation,126,7,results,our technique,to,treebanks
translation,126,7,results,our technique,observe,additional gain
translation,126,7,results,treebanks,of,same language
translation,126,7,results,same language,with,different domains
translation,126,7,results,additional gain,in,performance
translation,126,7,results,Results,applying,our technique
translation,126,21,results,domain adaptation technique,to,set of treebanks
translation,126,21,results,domain adaptation technique,observe,clear improvement
translation,126,21,results,set of treebanks,of,same language
translation,126,21,results,clear improvement,of,scores
translation,126,21,results,scores,especially for,treebanks
translation,126,21,results,treebanks,with,lesser training data
translation,126,21,results,Results,apply,domain adaptation technique
translation,126,23,results,some score improvements,in,low-resource languages
translation,126,23,results,Results,observe,some score improvements
translation,126,101,results,Joint training,of,two languages
translation,126,101,results,two languages,brings,small improvement
translation,126,101,results,small improvement,on,smaller French treebanks ( fr partut and fr sequoia )
translation,126,101,results,Results,has,Joint training
translation,126,108,results,training treebanks,observe,score improvement
translation,126,108,results,score improvement,for,domains
translation,126,108,results,domains,with,less data
translation,126,108,results,Results,mixing,training treebanks
translation,126,109,results,additional small gain,with,adversarial training ( Biaffine - MIX Adv )
translation,126,109,results,Results,observe,additional small gain
translation,126,110,results,our proposed architectures ( SharedGateNet and SharedGateAdvNet ),perform,better
translation,126,112,results,Our final architecture SharedGateAdvNet,has,slightly outperforms
translation,126,112,results,slightly outperforms,has,SharedGateNet
translation,126,112,results,Results,has,Our final architecture SharedGateAdvNet
translation,126,119,results,fr partut and fr sequoria,observe,small score improvement
translation,126,119,results,small score improvement,by jointly learning,two languages
translation,126,119,results,Results,especially for,fr partut and fr sequoria
translation,126,119,results,Results,observe,small score improvement
translation,126,132,results,UDPipe,in,most treebanks
translation,126,132,results,baseline biaffine parser,has,outperforms
translation,126,132,results,outperforms,has,UDPipe
translation,126,132,results,Results,has,baseline biaffine parser
translation,126,141,results,UDPipe,in,many test treebanks
translation,126,141,results,UDPipe,in,69 out of 81 treebanks
translation,126,141,results,Our system,has,outperforms
translation,126,141,results,outperforms,has,UDPipe
translation,126,141,results,Results,has,Our system
translation,126,145,results,Our system ( NAIST - SATO ),achieves,overall average LAS
translation,126,145,results,overall average LAS,of,70.13
translation,126,145,results,6th rank,among,33 participants
translation,126,145,results,33 participants,in,shared task
translation,126,145,results,Results,has,Our system ( NAIST - SATO )
translation,127,97,ablation-analysis,rule LCP ? NP,reduces,number of V-shaped categories
translation,127,97,ablation-analysis,rule LCP ? NP,not substantially affecting,quantity of other category shapes
translation,127,97,ablation-analysis,number of V-shaped categories,by,10 %
translation,127,97,ablation-analysis,Ablation analysis,Introducing,rule LCP ? NP
translation,127,4,model,Combinatory Categorial Grammar,to,wide -coverage parsing
translation,127,4,model,wide -coverage parsing,in,Chinese
translation,127,4,model,wide -coverage parsing,with,new Chinese CCGbank
translation,127,4,model,Chinese,with,new Chinese CCGbank
translation,127,4,model,Model,apply,Combinatory Categorial Grammar
translation,127,10,results,F - scores,of,72.73 ( P&K )
translation,127,10,results,F - scores,of,67.09 ( C&C )
translation,127,10,results,67.09 ( C&C ),on,labelled dependencies
translation,127,10,results,labelled dependencies,computed over,? 6 test set
translation,127,10,results,first Chinese ? parsing results,has,F - scores
translation,127,10,results,Results,obtain,first Chinese ? parsing results
translation,127,83,results,C&C,receives,small increases
translation,127,83,results,small increases,in,supertagger accuracy and coverage
translation,127,83,results,parsing performance,remains,largely unchanged
translation,127,83,results,P&K performance,degrades,slightly
translation,127,84,results,C,yields,best results
translation,127,84,results,best results,out of,three corpora
translation,127,84,results,best results,with,LF gains
translation,127,84,results,LF gains,of,"1.07 ( P&K ) , 1.28 ( ? ) and 0.63 ( ? )"
translation,127,84,results,"1.07 ( P&K ) , 1.28 ( ? ) and 0.63 ( ? )",over,base Chinese CCGbank
translation,127,88,results,C&C,gains,more ( 1.15 % )
translation,127,88,results,more ( 1.15 % ),than,P&K
translation,127,88,results,P&K,on,common sentences
translation,127,88,results,sentences,has,parsed
translation,128,98,ablation-analysis,decline,compared to,normal output
translation,128,115,ablation-analysis,last character,coarsely used as,lemma
translation,128,107,baselines,primary system,gets,third
translation,128,107,baselines,Baselines,has,primary system
translation,128,30,experiments,problem of multi-level labels,conducted by,parsing
translation,128,30,experiments,parsing,has,different levels separately
translation,128,30,experiments,outputs,has,together
translation,128,100,model,split method and lemma method,combined as,primary system
translation,128,100,model,Model,has,split method and lemma method
translation,128,114,model,Last character of word,is,useful feature
translation,128,114,model,Model,has,Last character of word
translation,128,6,results,our system,can improve,LAS
translation,128,6,results,our system,get,second prize
translation,128,6,results,LAS,about,one percent
translation,128,6,results,second prize,out of,nine participating systems
translation,128,6,results,combination of the two proposed methods,has,our system
translation,128,61,results,improvement,of,0.85
translation,128,61,results,improvement,of,0.93
translation,128,61,results,0.85,could be obtained,0.93
translation,128,61,results,0.85,while,0.93
translation,128,61,results,0.93,for,test data
translation,128,61,results,develop data,has,improvement
translation,128,61,results,Results,For,develop data
translation,128,99,results,Combined experiment,on,split and lemma Improvements
translation,128,99,results,split and lemma Improvements,first two methods in,experiment
translation,128,99,results,Results,has,Combined experiment
translation,128,106,results,contrast system,got,second prize
translation,128,106,results,Results,got,second prize
translation,128,106,results,Results,has,contrast system
translation,128,108,results,improvement,of,about one percent
translation,128,108,results,about one percent,for,primary and contrast system
translation,128,110,results,Parsing,is,more effective and accurate
translation,128,110,results,more effective and accurate,on,short sentences
translation,128,110,results,Results,has,Parsing
translation,128,129,results,Both of the systems,get,improvements
translation,128,129,results,improvements,for,about one percent
translation,128,129,results,about one percent,on,LAS
translation,128,129,results,Results,has,Both of the systems
translation,129,183,ablation-analysis,syntactic information,in,constituency tree
translation,129,183,ablation-analysis,syntactic information,helpful for describing,word relationships
translation,129,183,ablation-analysis,word relationships,critical to,our overall performance
translation,129,183,ablation-analysis,Ablation analysis,observe,syntactic information
translation,129,184,ablation-analysis,bidirectional GraphSAGE,encoding,from both forward and backward nodes
translation,129,184,ablation-analysis,bidirectional GraphSAGE,enhance,final performance
translation,129,184,ablation-analysis,from both forward and backward nodes,according to,edge direction
translation,129,184,ablation-analysis,Ablation analysis,found that,bidirectional GraphSAGE
translation,129,185,ablation-analysis,parent feeding and sibling feeding mechanism,enrich,paternal and fraternal information
translation,129,185,ablation-analysis,paternal and fraternal information,in,decoding
translation,129,185,ablation-analysis,paternal and fraternal information,play,important roles
translation,129,185,ablation-analysis,important roles,in,whole model
translation,129,185,ablation-analysis,Ablation analysis,has,parent feeding and sibling feeding mechanism
translation,129,213,ablation-analysis,constituency structure and other components,in,our model
translation,129,213,ablation-analysis,constituency structure and other components,play,significant roles
translation,129,213,ablation-analysis,significant roles,for,Graph2tree
translation,129,213,ablation-analysis,significant roles,to achieve,high performance
translation,129,213,ablation-analysis,Graph2tree,to achieve,high performance
translation,129,213,ablation-analysis,high performance,in,MWP solving
translation,129,213,ablation-analysis,Attention mechanism,has,constituency structure and other components
translation,129,213,ablation-analysis,Ablation analysis,has,Attention mechanism
translation,129,214,ablation-analysis,Graph2 Tree,on,math word problem ( MAWAPS )
translation,129,214,ablation-analysis,Ablation analysis,of,Graph2 Tree
translation,129,159,baselines,Seq2Seq model,with,Copy mechanism
translation,129,196,baselines,Group - Att,has,"Li et al. , 2019 )"
translation,129,196,baselines,Baselines,On,MAWPS
translation,129,197,baselines,TP - N2F,has,"Chen et al. , 2019a )"
translation,129,197,baselines,Baselines,On,MATHQA
translation,129,149,experimental-setup,graph construction,use,dependency parser and constituency parser
translation,129,149,experimental-setup,dependency parser and constituency parser,from,CoreNLP
translation,129,149,experimental-setup,Experimental setup,For,graph construction
translation,129,151,experimental-setup,Adam optimizer,with,batch size
translation,129,151,experimental-setup,batch size,of,20
translation,129,151,experimental-setup,Experimental setup,use,Adam optimizer
translation,129,154,experimental-setup,learning rate,set to,0.001
translation,129,154,experimental-setup,Experimental setup,has,learning rate
translation,129,155,experimental-setup,BiRNN,use,one- layer BiLSTM
translation,129,155,experimental-setup,BiRNN,is,one- layer BiLSTM
translation,129,155,experimental-setup,one- layer BiLSTM,with,hidden size
translation,129,155,experimental-setup,hidden size,of,150
translation,129,155,experimental-setup,hop size,in,GNN
translation,129,155,experimental-setup,GNN,chosen from,"{ 2,3,4 , 5 ,6 }"
translation,129,155,experimental-setup,graph encoder,has,BiRNN
translation,129,155,experimental-setup,Experimental setup,In,graph encoder
translation,129,156,experimental-setup,decoder,employ,one- layer LSTM
translation,129,156,experimental-setup,decoder,is,one- layer LSTM
translation,129,156,experimental-setup,one- layer LSTM,with,hidden size
translation,129,156,experimental-setup,hidden size,of,300
translation,129,156,experimental-setup,Experimental setup,has,decoder
translation,129,157,experimental-setup,dropout rate,chosen from,"{ 0.1,0.3,0.5 }."
translation,129,157,experimental-setup,Experimental setup,has,dropout rate
translation,129,6,model,Graph -to - Tree Neural Networks,namely,Graph2 Tree
translation,129,6,model,Graph -to - Tree Neural Networks,encodes,augmented graph-structured input
translation,129,6,model,Graph -to - Tree Neural Networks,decodes,tree-structured output
translation,129,6,model,Graph2 Tree,consisting of,graph encoder
translation,129,6,model,Graph2 Tree,consisting of,hierarchical tree decoder
translation,129,6,model,hierarchical tree decoder,encodes,augmented graph-structured input
translation,129,6,model,hierarchical tree decoder,decodes,tree-structured output
translation,129,6,model,Model,present,Graph -to - Tree Neural Networks
translation,129,26,model,Graph - to - Tree neural networks,namely,Graph2 Tree
translation,129,26,model,Graph - to - Tree neural networks,consisting of,hierarchical tree decoder
translation,129,26,model,Graph - to - Tree neural networks,leverages,structural information
translation,129,26,model,Graph2 Tree,consisting of,graph encoder
translation,129,26,model,Graph2 Tree,consisting of,hierarchical tree decoder
translation,129,26,model,structural information,of,source graphs and target trees
translation,129,26,model,Model,propose,Graph - to - Tree neural networks
translation,129,27,model,Graph2 Tree model,learns,mapping
translation,129,27,model,mapping,from,structured object
translation,129,27,model,structured object,such as,graph
translation,129,27,model,graph,to,another structured object
translation,129,27,model,another structured object,such as,tree
translation,129,27,model,Model,has,Graph2 Tree model
translation,129,29,model,graph encoder,first learns from,input graph
translation,129,29,model,input graph,constructed from,various inputs
translation,129,29,model,various inputs,combining,word sequence
translation,129,29,model,various inputs,combining,corresponding dependency or constituency tree
translation,129,29,model,tree decoder,generates,tree object
translation,129,29,model,tree object,from,learned graph vector representations
translation,129,29,model,learned graph vector representations,to explicitly capture,compositional structure
translation,129,29,model,compositional structure,of,tree
translation,129,30,model,novel Graph2tree model,with,separated attention mechanism
translation,129,30,model,separated attention mechanism,to jointly learn,final hidden vector
translation,129,30,model,final hidden vector,of,corresponding graph nodes
translation,129,30,model,final hidden vector,to align,generation process
translation,129,30,model,generation process,between,heterogeneous graph input
translation,129,30,model,generation process,between,hierarchical tree output
translation,129,30,model,Model,present,novel Graph2tree model
translation,129,95,model,proposed BiGraphSAGE,extends,"widely used GraphSAGE ( Hamilton et al. , 2017 )"
translation,129,95,model,forward and backward node embeddings,of,graph G
translation,129,95,model,forward and backward node embeddings,in,interleaved fashion
translation,129,95,model,Model,has,proposed BiGraphSAGE
translation,129,186,model,different types of nodes,in,input graph
translation,129,186,model,separate attention mechanism,proved useful,our model
translation,129,186,model,different types of nodes,has,separate attention mechanism
translation,129,186,model,input graph,has,separate attention mechanism
translation,129,186,model,Model,designed for,different types of nodes
translation,129,169,results,proposed Graph2 Tree,achieves,comparable exact-match accuracy
translation,129,169,results,comparable exact-match accuracy,compared to,other state- ofthe - art baselines
translation,129,169,results,proposed Graph2 Tree,has,outperforms
translation,129,169,results,Results,shows,proposed Graph2 Tree
translation,130,259,ablation-analysis,node-local information,harder to,predict
translation,130,259,ablation-analysis,predict,than,labeled edges
translation,130,259,ablation-analysis,binary top property,has,node-local information
translation,130,259,ablation-analysis,node-local information,has,fine - grained labels and properties
translation,130,259,ablation-analysis,Ablation analysis,Except for,binary top property
translation,130,223,experimental-setup,MRP scoring,implemented in,open-source mtool software
translation,130,283,results,stark differences,in,overall parser accuracy
translation,130,283,results,overall parser accuracy,across,frameworks
translation,130,283,results,low - 70s to mid-90s F 1 ranges,with,mostly decreasing performance
translation,130,283,results,mostly decreasing performance,when moving from,bi-lexical Flavor ( 0 ) graphs
translation,130,361,results,MTL version,of,TUPA system
translation,130,361,results,MTL version,performs,much worse
translation,130,361,results,much worse,than,single - task version
translation,130,361,results,Results,has,MTL version
translation,130,362,results,Hitachi systems,show,MTL results
translation,130,362,results,MTL results,that are,slightly better
translation,130,362,results,slightly better,than,single framework results
translation,130,362,results,Results,has,Hitachi systems
translation,131,139,ablation-analysis,single encoder,adding,characters
translation,131,139,ablation-analysis,single encoder,beneficial for,6 out of 7 linguistic sources
translation,131,139,ablation-analysis,characters,beneficial for,6 out of 7 linguistic sources
translation,131,139,ablation-analysis,Ablation analysis,In,single encoder
translation,131,150,experiments,BERT model,is,bert-multilingual -uncased
translation,131,54,hyperparameters,one encoder,use,"char-CNN ( Kim et al. , 2016 )"
translation,131,54,hyperparameters,"char-CNN ( Kim et al. , 2016 )",runs,"Convolutional Neural Network ( LeCun et al. , 1990 )"
translation,131,54,hyperparameters,"Convolutional Neural Network ( LeCun et al. , 1990 )",over,characters
translation,131,54,hyperparameters,characters,for,each token
translation,131,54,hyperparameters,Hyperparameters,For,one encoder
translation,131,81,hyperparameters,char-CNN model,use,100 filters
translation,131,81,hyperparameters,char-CNN model,use,n-gram filter sizes
translation,131,81,hyperparameters,char-CNN model,of,n-gram filter sizes
translation,131,81,hyperparameters,char-CNN model,for,English
translation,131,81,hyperparameters,char-CNN model,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,100 filters,for,English
translation,131,81,hyperparameters,100 filters,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,embedding size,of,75
translation,131,81,hyperparameters,embedding size,for,English
translation,131,81,hyperparameters,embedding size,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,75,for,English
translation,131,81,hyperparameters,75,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,n-gram filter sizes,of,"[ 1 , 2 , 3 ]"
translation,131,81,hyperparameters,n-gram filter sizes,of,English
translation,131,81,hyperparameters,n-gram filter sizes,of,"[ 1 , 2 , 3 , 4 , 5 ]"
translation,131,81,hyperparameters,n-gram filter sizes,of,"German , Italian and Dutch"
translation,131,81,hyperparameters,n-gram filter sizes,for,English
translation,131,81,hyperparameters,n-gram filter sizes,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,n-gram filter sizes,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,"[ 1 , 2 , 3 ]",for,English
translation,131,81,hyperparameters,"[ 1 , 2 , 3 ]",for,"German , Italian and Dutch"
translation,131,81,hyperparameters,"[ 1 , 2 , 3 ]",for,"German , Italian and Dutch"
translation,131,81,hyperparameters,English,for,"German , Italian and Dutch"
translation,131,81,hyperparameters,"[ 1 , 2 , 3 , 4 , 5 ]",for,"German , Italian and Dutch"
translation,131,81,hyperparameters,100 filters,has,embedding size
translation,131,81,hyperparameters,Hyperparameters,For,char-CNN model
translation,131,4,model,character - level and contextual language model representations,to improve,performance
translation,131,4,model,performance,on,Discourse Representation Structure parsing
translation,131,4,model,Model,combine,character - level and contextual language model representations
translation,131,137,model,Model,has,Combining characters and linguistic features
translation,131,126,results,LMs vs char-level models,has,DRS parsing
translation,131,126,results,pretrained language models,has,outperform
translation,131,126,results,outperform,has,char-only model
translation,131,127,results,worse performance,for,all representations
translation,131,127,results,Transformer model,has,worse performance
translation,131,127,results,Results,has,Transformer model
translation,131,128,results,BERT - BASE,is,best model
translation,131,128,results,Results,find that,BERT - BASE
translation,131,132,results,both methods,results in,clear and significant improvement
translation,131,132,results,clear and significant improvement,over,BERT - only baseline
translation,131,132,results,87.6,versus,88.1
translation,131,136,results,improve,has,performance
translation,131,140,results,scores,are,not higher
translation,131,140,results,scores,adding,characters on their own
translation,131,140,results,not higher,adding,characters on their own
translation,131,140,results,Results,has,scores
translation,131,146,results,characters,results in,improvement
translation,131,146,results,characters,results in,improvement
translation,131,146,results,characters,results in,improvement
translation,131,146,results,improvement,for,all the LMs under consideration
translation,131,146,results,Results,see that,characters
translation,131,146,results,Results,adding,characters
translation,131,151,results,Results,for,both PMB releases
translation,131,152,results,all languages,adding,characters
translation,131,152,results,characters,leads to,clear improvement
translation,131,152,results,clear improvement,for,one and two encoders
translation,131,152,results,clear improvement,both,one and two encoders
translation,131,152,results,Results,For,all languages
translation,131,153,results,two -encoder setup,seems to be,preferable
translation,131,153,results,preferable,for,"smaller , non-English data sets"
translation,131,153,results,Results,has,two -encoder setup
translation,131,154,results,2.2.0,outperform,system
translation,131,154,results,2.2.0,obtain,competitive scores
translation,131,154,results,system,for,German and Italian
translation,131,154,results,competitive scores,for,Dutch
translation,131,154,results,Results,For,2.2.0
translation,131,175,results,best performance,on,six of the seven phenomena selected
translation,131,175,results,Our best model ( + ch+sem ),has,best performance
translation,131,175,results,Results,has,Our best model ( + ch+sem )
translation,131,176,results,character - level representations,seem to,help
translation,131,176,results,+ char models,improve on,baseline ( BERT )
translation,131,176,results,baseline ( BERT ),in,almost all instances
translation,131,176,results,Results,has,character - level representations
translation,131,189,results,similar trend,for,all models
translation,131,189,results,decrease,in,performance
translation,131,189,results,performance,for,longer sentences
translation,131,189,results,similar trend,has,decrease
translation,131,189,results,all models,has,decrease
translation,131,189,results,Results,see,similar trend
translation,131,196,results,decreases,for,longer sentences
translation,131,196,results,performance,has,decreases
translation,131,196,results,Results,see that,performance
translation,131,197,results,Transformer model,not seem to catch up,bi-LSTM models
translation,131,197,results,bi-LSTM models,even for,longer documents
translation,131,197,results,Results,has,Transformer model
translation,131,198,results,characters,beneficial for,longer documents
translation,131,198,results,Results,addition of,characters
translation,131,200,results,improved,has,performance
translation,131,200,results,Results,adding,character - level representations
translation,131,205,results,BERT models,has,outperformed
translation,131,205,results,outperformed,has,larger ROBERTA models
translation,131,205,results,Results,has,unexpected finding
translation,132,131,ablation-analysis,joint span decoder,benefit,constituent and dependency syntactic parsing
translation,132,131,ablation-analysis,Ablation analysis,shows,joint span decoder
translation,132,51,baselines,multi-task learning ( MTL ) approach,sharing,parameters
translation,132,51,baselines,parameters,of,token representation and self-attention encoder
translation,132,129,baselines,joint span syntactic parsing decoder,with,separate learning constituent syntactic parsing model
translation,132,129,baselines,separate learning constituent syntactic parsing model,takes,same token representation
translation,132,129,baselines,separate learning constituent syntactic parsing model,takes,self-attention encoder
translation,132,129,baselines,separate learning constituent syntactic parsing model,takes,joint learning setting
translation,132,129,baselines,joint learning setting,of,semantic parsing
translation,132,129,baselines,semantic parsing,on,PTB dev set
translation,132,113,experimental-setup,100D GloVe,has,pre-trained embeddings
translation,132,113,experimental-setup,Experimental setup,use,100D GloVe
translation,132,114,experimental-setup,self-attention encoder,set,12 self-attention layers
translation,132,114,experimental-setup,Experimental setup,For,self-attention encoder
translation,132,116,experimental-setup,candidates pruning,set,? p = 0.4 and ? a = 0.6
translation,132,116,experimental-setup,candidates pruning,set,m p = 30 and m a = 300
translation,132,116,experimental-setup,? p = 0.4 and ? a = 0.6,pruning,predicates and arguments
translation,132,116,experimental-setup,m p = 30 and m a = 300,for,max numbers of predicates and arguments
translation,132,116,experimental-setup,Experimental setup,For,candidates pruning
translation,132,120,experimental-setup,0.33 dropout,for,biaffine attention and MLP layers
translation,132,120,experimental-setup,Experimental setup,has,Training Details
translation,132,121,experimental-setup,up to 150 epochs,with,batch size 150
translation,132,121,experimental-setup,up to 150 epochs,on,single NVIDIA GeForce GTX 1080 Ti GPU
translation,132,121,experimental-setup,batch size 150,on,single NVIDIA GeForce GTX 1080 Ti GPU
translation,132,121,experimental-setup,single NVIDIA GeForce GTX 1080 Ti GPU,with,Intel i7- 7800X CPU
translation,132,121,experimental-setup,Experimental setup,trained for,up to 150 epochs
translation,132,109,experiments,predicate disambiguation task,use,off-the-shelf disambiguator
translation,132,115,experiments,semantic role scorer,use,512 - dimensional MLP layers
translation,132,115,experiments,semantic role scorer,use,256 - dimensional feed -forward networks
translation,132,117,experiments,constituent span scorer,apply,hidden size
translation,132,117,experiments,hidden size,of,250 - dimensional feed -forward networks
translation,132,118,experiments,dependency head scorer,employ,two 1024 dimensional MLP layers
translation,132,118,experiments,two 1024 dimensional MLP layers,with,ReLU
translation,132,118,experiments,ReLU,as,activation function
translation,132,118,experiments,activation function,for learning,specific representation
translation,132,118,experiments,1024 - dimensional parameter matrix,for,biaffine attention
translation,132,147,experiments,Our single model,in,XLNet setting
translation,132,147,experiments,Our single model,achieving,96.18 F1 score
translation,132,147,experiments,Our single model,achieving,97.23 % UAS
translation,132,147,experiments,Our single model,achieving,95.65 % LAS
translation,132,147,experiments,96.18 F1 score,of,constituent syntactic parsing
translation,132,147,experiments,95.65 % LAS,of,dependency syntactic parsing
translation,132,7,model,syntactic and semantic parsing,on,span and dependency representations
translation,132,7,model,syntactic information,in,encoder of neural network
translation,132,7,model,effectively,in,encoder of neural network
translation,132,7,model,syntactic information,has,effectively
translation,132,7,model,Model,novel joint model of,syntactic and semantic parsing
translation,132,18,model,span and dependency representation,of,semantic role labeling ( SRL )
translation,132,18,model,span and dependency representation,of,syntax
translation,132,18,model,joint model,with,multi-task learning
translation,132,18,model,multi-task learning,in,balanced mode
translation,132,18,model,balanced mode,improves,semantic and syntactic parsing
translation,132,18,model,Model,propose,joint model
translation,132,19,model,semantics,learned,end-to - end way
translation,132,19,model,end-to - end way,with,uniform representation
translation,132,19,model,syntactic parsing,represented as,joint span structure
translation,132,19,model,joint span structure,relating to,head - driven phrase structure grammar ( HPSG )
translation,132,19,model,joint span structure,incorporate,head and phrase information
translation,132,19,model,head - driven phrase structure grammar ( HPSG ),incorporate,head and phrase information
translation,132,19,model,head and phrase information,of,dependency and constituent syntactic parsing
translation,132,50,model,encoder-decoder backbone,apply,self-attention encoder ( Vaswani et al. )
translation,132,50,model,self-attention encoder ( Vaswani et al. ),modified by,position partition
translation,132,50,model,Model,Using,encoder-decoder backbone
translation,132,119,model,our model,with,BERT and XLNet
translation,132,119,model,our model,with,BERT and XLNet
translation,132,119,model,our model,set,2 layers of self-attention
translation,132,119,model,2 layers of self-attention,for,BERT and XLNet
translation,132,119,model,Model,augmenting,our model
translation,132,119,model,Model,set,2 layers of self-attention
translation,132,132,results,directly predicted dependencies,from,our model
translation,132,132,results,directly predicted dependencies,are,better
translation,132,132,results,predicted constituent parse trees,in,UAS term
translation,132,135,results,end-to - end mode,find that,constituent syntactic parsing
translation,132,135,results,end-to - end mode,find that,dependency syntactic parsing
translation,132,135,results,constituent syntactic parsing,boost,both styles of semantics
translation,132,135,results,Results,In,end-to - end mode
translation,132,138,results,joint learning,of,our uniform SRL
translation,132,138,results,performs better,than,separate learning
translation,132,138,results,separate learning,of,dependency or span SRL
translation,132,138,results,dependency or span SRL,in,both modes
translation,132,138,results,our uniform SRL,has,performs better
translation,132,138,results,Results,has,joint learning
translation,132,139,results,joint semantic and constituent syntactic parsing,achieve,relatively better SRL results
translation,132,139,results,relatively better SRL results,than,other settings
translation,132,139,results,Results,has,joint semantic and constituent syntactic parsing
translation,132,145,results,our joint model setting,boosts,syntactic parsing
translation,132,145,results,our joint model setting,boosts,SRL
translation,133,185,experiments,"OpenNMT - py ( Klein et al. , 2017 )",with,copy mechanism
translation,133,185,experiments,sequence - tosequence model,with,copy mechanism
translation,133,185,experiments,"OpenNMT - py ( Klein et al. , 2017 )",has,sequence - tosequence model
translation,133,177,results,considerable improvements,for,both corpora
translation,133,177,results,considerable improvements,especially for,reentrancy prediction
translation,133,177,results,considerable improvements,also for,Smatch
translation,133,177,results,increase,by,10.4 and 10.3 points
translation,133,177,results,increase,by,1.7 points
translation,133,177,results,increase,by,1.7 points
translation,133,177,results,increase,by,1.7 points
translation,133,177,results,1.7 points,for,both corpora
translation,133,177,results,reentrancy prediction,has,increase
translation,133,177,results,Smatch,has,increase
translation,133,178,results,ADD corrections,provide,more than half
translation,133,178,results,ADD corrections,provide,slightly less than half
translation,133,178,results,more than half,of,reentrancy score improvement
translation,133,178,results,reentrancy score improvement,provided by,ALL corrections
translation,133,178,results,slightly less than half,of,Smatch improvement
translation,133,178,results,Results,has,ADD corrections
translation,134,4,model,Model,formulate,AMR parsing
translation,134,5,model,novel componentwise beam search algorithm,for,relation identification
translation,134,5,model,novel componentwise beam search algorithm,incorporate,decoder
translation,134,5,model,relation identification,in,incremental fashion
translation,134,5,model,decoder,into,unified framework
translation,134,5,model,unified framework,based on,multiple - beam search
translation,134,5,model,bi-directional information flow,between,two subtasks
translation,134,5,model,two subtasks,in,single incremental model
translation,134,5,model,Model,develop,novel componentwise beam search algorithm
translation,134,5,model,Model,incorporate,decoder
translation,134,31,model,novel Component - Wise Beam Search ( CWBS ) algorithm,for,incremental relation identification
translation,134,31,model,incremental relation identification,examine,accuracy loss
translation,134,31,model,accuracy loss,in,fully incremental fashion
translation,134,31,model,accuracy loss,compared to,global fashion
translation,134,31,model,Model,develop,novel Component - Wise Beam Search ( CWBS ) algorithm
translation,134,32,model,segment - based decoder,for,concept identification
translation,134,32,model,segment - based decoder,incorporate,CWBS algorithm
translation,134,32,model,CWBS algorithm,for,relation identification
translation,134,32,model,CWBS algorithm,combining,two subtasks
translation,134,32,model,relation identification,into,framework
translation,134,32,model,two subtasks,in,single incremental model
translation,134,32,model,Model,adopt,segment - based decoder
translation,134,33,model,parameter estimation,has,"violation - fixing "" perceptron"
translation,134,33,model,Model,For,parameter estimation
translation,134,170,model,system 2,using,more lexical features
translation,134,170,model,more lexical features,to capture,association
translation,134,170,model,association,between,concept and the context
translation,134,170,model,Model,implemented,system 2
translation,134,163,results,incremental algorithm CWBS,achieves,almost the same performance
translation,134,163,results,almost the same performance,as,non-incremental algorithm MSCG
translation,134,163,results,non-incremental algorithm MSCG,in,JAMR
translation,134,163,results,non-incremental algorithm MSCG,using,same features
translation,134,164,results,CWBS,is,competitive alternative
translation,134,164,results,competitive alternative,to,MSCG
translation,134,164,results,Results,indicate,CWBS
translation,134,165,results,Results,has,Joint Model vs. Pipelined Model
translation,134,169,results,F-measure,gain,6 % absolute improvement
translation,134,169,results,F-measure,gain,5 % absolute improvement
translation,134,169,results,5 % absolute improvement,over,results of JAMR
translation,134,169,results,Results,In terms of,F-measure
translation,134,172,results,3 % improvement,over,two different datasets
translation,134,172,results,3 % improvement,by adding,only
translation,134,172,results,3 % improvement,by adding,some additional lexical features
translation,134,172,results,only,has,some additional lexical features
translation,134,172,results,Results,gain,3 % improvement
translation,134,173,results,Results,has,Comparison with State- of- the-art
translation,134,177,results,our parser,achieves,better performance
translation,134,177,results,better performance,than,other approaches
translation,134,177,results,Results,see that,our parser
translation,134,181,results,our approach,obtains,comparable performance
translation,134,181,results,comparable performance,with,CAMR
translation,134,181,results,our approach,has,outperforms
translation,134,181,results,outperforms,has,CAMR
translation,134,181,results,Results,show,our approach
translation,134,182,results,our approach,achieves,slightly lower performance
translation,134,182,results,slightly lower performance,compared to,SMBT - based parser
translation,134,182,results,SMBT - based parser,adds,data and features
translation,134,182,results,data and features,drawn from,various external semantic resources
translation,134,182,results,Results,has,our approach
translation,135,54,ablation-analysis,hyperbolic tangent function,in,GRU
translation,135,54,ablation-analysis,hyperbolic tangent function,with,faster to compute
translation,135,54,ablation-analysis,GRU,with,LReL function
translation,135,54,ablation-analysis,Ablation analysis,replace,hyperbolic tangent function
translation,135,174,ablation-analysis,query components,using,single query component
translation,135,174,ablation-analysis,single query component,degrades,UAS
translation,135,174,ablation-analysis,UAS,by,0.7-0.9 %
translation,135,174,ablation-analysis,LAS,by,around 1.0 %
translation,135,174,ablation-analysis,Ablation analysis,In terms of,query components
translation,135,175,ablation-analysis,soft headword embedding,used for,arc label predictions
translation,135,175,ablation-analysis,soft headword embedding,not fed into,next hidden state
translation,135,175,ablation-analysis,Model 6,has,soft headword embedding
translation,135,175,ablation-analysis,Ablation analysis,For,Model 6
translation,135,139,experiments,English,compare with,stateof - the - art graph - based dependency parsers
translation,135,157,experiments,highly competitive parsing accuracy,as,stateof - the - art parsers
translation,135,157,experiments,BiAtt - DP,has,highly competitive parsing accuracy
translation,135,106,hyperparameters,learning rate,halved at,each iteration
translation,135,106,hyperparameters,loglikelihood,of,dev set
translation,135,106,hyperparameters,Hyperparameters,has,learning rate
translation,135,108,hyperparameters,learning parameters,except,bias terms
translation,135,108,hyperparameters,initialized randomly,according to,"Gaussian distribution N ( 0 , 10 ?2 )"
translation,135,108,hyperparameters,Hyperparameters,has,learning parameters
translation,135,109,hyperparameters,initial learning rate,with,step size
translation,135,109,hyperparameters,step size,of,0.0002
translation,135,109,hyperparameters,best one,based on,log-likelihood
translation,135,109,hyperparameters,best one,dev set at,first epoch
translation,135,109,hyperparameters,Hyperparameters,tune,initial learning rate
translation,135,109,hyperparameters,Hyperparameters,choose,best one
translation,135,110,hyperparameters,selected initial learning rates,fall in,range
translation,135,110,hyperparameters,selected initial learning rates,for,"hidden layer size [ 128 , 320 ]"
translation,135,110,hyperparameters,selected initial learning rates,for,hidden layer size
translation,135,110,hyperparameters,selected initial learning rates,for,hidden layer size
translation,135,110,hyperparameters,range,of,"[ 0.0004 , 0.0010 ]"
translation,135,110,hyperparameters,range,of,"hidden layer size [ 128 , 320 ]"
translation,135,110,hyperparameters,range,of,larger
translation,135,110,hyperparameters,range,of,smaller hidden layer size
translation,135,110,hyperparameters,range,of,"[ 0.0016 , 0.0034 ]"
translation,135,110,hyperparameters,range,of,hidden layer size
translation,135,110,hyperparameters,range,of,around 80
translation,135,110,hyperparameters,range,for,"hidden layer size [ 128 , 320 ]"
translation,135,110,hyperparameters,range,for,hidden layer size
translation,135,110,hyperparameters,range,for,hidden layer size
translation,135,110,hyperparameters,"[ 0.0004 , 0.0010 ]",for,"hidden layer size [ 128 , 320 ]"
translation,135,110,hyperparameters,"[ 0.0004 , 0.0010 ]",for,hidden layer size
translation,135,110,hyperparameters,"[ 0.0004 , 0.0010 ]",for,hidden layer size
translation,135,110,hyperparameters,"hidden layer size [ 128 , 320 ]",for,hidden layer size
translation,135,110,hyperparameters,larger,when using,smaller hidden layer size
translation,135,110,hyperparameters,larger,for,hidden layer size
translation,135,110,hyperparameters,smaller hidden layer size,for,hidden layer size
translation,135,110,hyperparameters,"[ 0.0016 , 0.0034 ]",for,hidden layer size
translation,135,110,hyperparameters,hidden layer size,has,around 80
translation,135,110,hyperparameters,Hyperparameters,has,selected initial learning rates
translation,135,111,hyperparameters,training data,are,randomly shuffled
translation,135,111,hyperparameters,randomly shuffled,at,every epoch
translation,135,111,hyperparameters,Hyperparameters,has,training data
translation,135,125,hyperparameters,E form,by,pretrained word embeddings
translation,135,125,hyperparameters,pretrained word embeddings,For,12 other languages
translation,135,125,hyperparameters,pretrained word embeddings,randomly hold out,5 %
translation,135,125,hyperparameters,5 %,of,training data
translation,135,125,hyperparameters,training data,as,dev set
translation,135,125,hyperparameters,Hyperparameters,initialize,E form
translation,135,134,hyperparameters,12 other languages,use,square matrices
translation,135,134,hyperparameters,square matrices,for,embedding parameters E's
translation,135,134,hyperparameters,Hyperparameters,For,12 other languages
translation,135,4,model,novel bi-directional attention model,for,dependency parsing
translation,135,4,model,novel bi-directional attention model,learns to,agree
translation,135,4,model,agree,on,headword predictions
translation,135,4,model,headword predictions,from,forward and backward parsing directions
translation,135,4,model,Model,develop,novel bi-directional attention model
translation,135,5,model,parsing procedure,for,each direction
translation,135,5,model,each direction,formulated as,sequentially querying
translation,135,5,model,memory component,stores,continuous headword embeddings
translation,135,5,model,sequentially querying,has,memory component
translation,135,5,model,Model,has,parsing procedure
translation,135,6,model,parser,makes use of,soft headword embeddings
translation,135,6,model,Model,to implicitly capture,high-order parsing history
translation,135,16,model,training objective function,to enforce,attention agreement
translation,135,16,model,attention agreement,between,both directions
translation,135,16,model,Model,design,training objective function
translation,135,17,model,dependency parser ( BiAtt - DP ),using,bi-directional attention model
translation,135,17,model,bi-directional attention model,based on,memory network
translation,135,17,model,Model,develop,dependency parser ( BiAtt - DP )
translation,135,18,model,simple and interpretable approximation,for,agreement objective
translation,135,18,model,Model,derive,simple and interpretable approximation
translation,135,28,results,our model,achieves,highest unlabeled attachment score ( UAS )
translation,135,28,results,highest unlabeled attachment score ( UAS ),on,"Chinese , Czech , Dutch , German , Spanish and Turkish"
translation,135,28,results,Results,has,our model
translation,135,115,results,headword predictions,made through,MST search
translation,135,115,results,slightly improves,has,UAS and LAS
translation,135,115,results,slightly improves,has,less than 0.3 % absolutely
translation,135,115,results,UAS and LAS,has,less than 0.3 % absolutely
translation,135,115,results,Results,has,headword predictions
translation,135,116,results,proposed BiAtt - DP,achieves,competitive parsing accuracy
translation,135,116,results,proposed BiAtt - DP,obtains,better UAS
translation,135,116,results,competitive parsing accuracy,on,all languages
translation,135,116,results,all languages,as,state - of - the - art parsers
translation,135,116,results,better UAS,in,6 languages
translation,135,116,results,Results,has,proposed BiAtt - DP
translation,135,140,results,better accuracy,than,Chen and Manning ( 2014 )
translation,135,140,results,better accuracy,than,feed-forward neural network
translation,135,140,results,better accuracy,uses,feed-forward neural network
translation,135,141,results,BiAtt - DP,outperforms,Bohnet and Nivre ( 2012 )
translation,135,141,results,integrated parsing and tagging models,has,BiAtt - DP
translation,135,142,results,CTB,achieves,best UAS
translation,135,142,results,CTB,achieves,similar LAS
translation,135,142,results,Results,On,CTB
translation,135,156,results,RBGParser,in,most languages
translation,135,156,results,most languages,except,"Japanese , Slovene , and Swedish"
translation,135,156,results,proposed BiAtt -DP,has,outperforms
translation,135,156,results,outperforms,has,RBGParser
translation,135,156,results,Results,show that,proposed BiAtt -DP
translation,135,158,results,best UAS,for,5 out of 12 languages
translation,135,159,results,UAS gaps,between,BiAtt-DP and state - of - the - art parsers
translation,135,159,results,BiAtt-DP and state - of - the - art parsers,within,1.0 %
translation,135,159,results,1.0 %,except,Swedish
translation,135,159,results,remaining seven languages,has,UAS gaps
translation,135,159,results,Results,For,remaining seven languages
translation,135,162,results,consistently outperforms,by,up to 5 % absolute UAS score
translation,135,162,results,both parser,by,up to 5 % absolute UAS score
translation,135,162,results,BiAtt - DP,has,consistently outperforms
translation,135,162,results,consistently outperforms,has,both parser
translation,135,162,results,Results,has,BiAtt - DP
translation,135,167,results,BiAtt -DP,achieves,better UAS
translation,135,168,results,improvement,on,recall
translation,135,168,results,much more significant,than,uncrossed arcs
translation,135,168,results,uncrossed arcs,around,1 - 3 % absolutely
translation,135,168,results,Results,observe that,improvement
translation,135,170,results,impact,of using,"pre-trained word embeddings , POS tags"
translation,135,170,results,bidirectional query components,on,our model
translation,135,170,results,Results,study,impact
translation,136,25,baselines,UDPipe,employs,GRU network
translation,136,25,baselines,GRU network,during,inference of segmentation and tokenization
translation,136,25,baselines,Baselines,has,UDPipe
translation,136,65,experimental-setup,30,",",000 steps
translation,136,65,experimental-setup,000 steps,for,each model
translation,136,65,experimental-setup,fine- tune ( onot necessary ),for,100 steps
translation,136,65,experimental-setup,100 steps,for,given language
translation,136,65,experimental-setup,30,has,000 steps
translation,136,65,experimental-setup,Experimental setup,train,30
translation,136,65,experimental-setup,Experimental setup,train,000 steps
translation,136,66,experimental-setup,dimension,is,100
translation,136,66,experimental-setup,all the input features,has,dimension
translation,136,66,experimental-setup,Experimental setup,For,all the input features
translation,136,67,experimental-setup,LSTM,use,hidden size
translation,136,67,experimental-setup,LSTM,use,number of layers
translation,136,67,experimental-setup,hidden size,equals to,400
translation,136,67,experimental-setup,number of layers,is,3
translation,136,67,experimental-setup,3,.,0.33 % dropout rate
translation,136,67,experimental-setup,0.33 % dropout rate,applied to,input and LSTM hidden layer
translation,136,67,experimental-setup,LSTM,has,number of layers
translation,136,67,experimental-setup,Experimental setup,For,LSTM
translation,136,68,experimental-setup,"Bayesian dropout ( Gal and Ghahramani , 2016 )",in,LSTM layers
translation,136,68,experimental-setup,Experimental setup,use,"Bayesian dropout ( Gal and Ghahramani , 2016 )"
translation,136,69,experimental-setup,word dropout,in,input layer
translation,136,69,experimental-setup,Experimental setup,use,word dropout
translation,136,45,model,embedding vectors,for,characters and morphological features
translation,136,45,model,embedding vectors,for,each word
translation,136,45,model,each word,apply,Convolutional Network ( CNN )
translation,136,45,model,Convolutional Network ( CNN ),to encode,variable length embeddings
translation,136,45,model,variable length embeddings,into,one fixed length feature
translation,136,45,model,Model,assign,embedding vectors
translation,136,6,results,baseline method,by,4.4 % and 2.1 %
translation,136,6,results,baseline method,on,development and test set
translation,136,6,results,4.4 % and 2.1 %,on,development and test set
translation,136,6,results,development and test set,of,CoNLL 2018 UD Shared Task
translation,136,6,results,Our system,has,outperforms
translation,136,6,results,outperforms,has,baseline method
translation,136,6,results,Results,has,Our system
translation,136,79,results,our system,achieves,higher improvements
translation,136,79,results,higher improvements,on,datasets
translation,136,79,results,datasets,with,large size of training data
translation,136,79,results,Results,find that,our system
translation,137,169,ablation-analysis,removal,from,training set
translation,137,169,ablation-analysis,training set,of,most distant language
translation,137,169,ablation-analysis,multilingual ( - ZH ) model,achieves,around 2 F1 points
translation,137,169,ablation-analysis,around 2 F1 points,compared to,multilingual version
translation,137,169,ablation-analysis,multilingual version,including,Chinese
translation,137,169,ablation-analysis,Ablation analysis,confirmed by,removal
translation,137,11,experimental-setup,XL - AMR,at,github.com
translation,137,11,experimental-setup,XL - AMR,at,SapienzaNLP
translation,137,11,experimental-setup,XL - AMR,/,SapienzaNLP
translation,137,11,experimental-setup,SapienzaNLP,has,/ xlamr
translation,137,11,experimental-setup,Experimental setup,release,XL - AMR
translation,137,28,experiments,XL - AMR,study,different transfer learning techniques
translation,137,28,experiments,different transfer learning techniques,enable,training
translation,137,28,experiments,model transfer,relies on,language - independent features
translation,137,28,experiments,annotation projection,relying on,parallel corpora
translation,137,28,experiments,annotation projection,relying on,available English AMR parsers
translation,137,28,experiments,automatic translation,of,training corpora
translation,137,28,experiments,XL - AMR,has,cross-lingual AMR parser
translation,137,159,experiments,language-specific XL - AMR par,trained on,less instances
translation,137,159,experiments,?- shot models,by,large margin
translation,137,159,experiments,language-specific XL - AMR par,has,outperforms
translation,137,159,experiments,outperforms,has,?- shot models
translation,137,7,model,cross-lingual AMR parsing,explore,different transfer learning techniques
translation,137,7,model,different transfer learning techniques,for producing,automatic AMR annotations
translation,137,7,model,different transfer learning techniques,develop,crosslingual AMR parser
translation,137,7,model,automatic AMR annotations,across,languages
translation,137,7,model,Model,to enable,cross-lingual AMR parsing
translation,137,7,model,Model,explore,different transfer learning techniques
translation,137,7,model,Model,develop,crosslingual AMR parser
translation,137,13,model,sentences,as,"rooted , directed and acyclic graphs"
translation,137,13,model,"rooted , directed and acyclic graphs",in which,nodes
translation,137,13,model,nodes,are,concepts
translation,137,13,model,edges,are,semantic relations
translation,137,13,model,Model,represents,sentences
translation,137,157,results,XL - AMR par+ ?,noticeably improves over,XL - AMR amr ?
translation,137,157,results,Results,has,XL - AMR par+ ?
translation,137,160,results,AMREAGER,trained on,same sentences
translation,137,160,results,same sentences,from,Europarl
translation,137,160,results,Results,surpasses,AMREAGER
translation,137,161,results,results,are,further improved
translation,137,161,results,further improved,when,jointly training
translation,137,161,results,jointly training,in,multiple languages
translation,137,161,results,Results,are,further improved
translation,137,161,results,Results,when,jointly training
translation,137,161,results,Results,has,results
translation,137,164,results,XL - AMR par + models,significantly improve over,XL - AMR par bilingual and multilingual models
translation,137,164,results,Results,has,XL - AMR par + models
translation,137,166,results,XL - AMR trans models,perform,best
translation,137,166,results,performances,of,language -specific variants
translation,137,166,results,language -specific variants,has,outperform
translation,137,166,results,Results,has,XL - AMR trans models
translation,137,168,results,translated sentences,in,other languages
translation,137,168,results,slightly harms,has,performances
translation,137,168,results,Results,inclusion of,translated sentences
translation,137,171,results,English gold AMR 2.0,i.e.,XL - AMR trans +
translation,137,171,results,model,benefits from,better quality
translation,137,171,results,better quality,of,dataset
translation,137,171,results,English gold AMR 2.0,has,model
translation,137,171,results,XL - AMR trans +,has,model
translation,137,171,results,Results,when adding,English gold AMR 2.0
translation,137,172,results,XL - AMR trans +,is,best performing
translation,137,172,results,best performing,across the board,"German , Spanish and Italian"
translation,137,172,results,best performing,surpassing,AMREAGER
translation,137,172,results,AMREAGER,by,at least 14 F1 points
translation,137,172,results,AMREAGER,by,at least 5 F1 points
translation,137,172,results,AMREAGER,by,at least 5 F1 points
translation,137,172,results,XL - AMR par and XL - AMR par +,by,at least 5 F1 points
translation,137,172,results,at least 5 F1 points,in,each language
translation,137,172,results,Results,bilingual version of,XL - AMR trans +
translation,137,173,results,best results,in,Chinese
translation,137,173,results,best results,achieved by,languagespecific XL - AMR trans
translation,137,173,results,Chinese,achieved by,languagespecific XL - AMR trans
translation,137,173,results,Chinese,achieved by,?- shot models
translation,137,173,results,languagespecific XL - AMR trans,surpassing,AMREAGER
translation,137,173,results,languagespecific XL - AMR trans,surpassing,?- shot models
translation,137,173,results,AMREAGER,by,8 F1 points
translation,137,173,results,?- shot models,by,more than 17 F1 points
translation,137,173,results,Results,has,best results
translation,137,178,results,AMREAGER,in,all subtasks
translation,137,178,results,all subtasks,except for,Negations
translation,137,178,results,all subtasks,except for,Named Entities in Chinese
translation,137,178,results,Negations,in,German
translation,137,178,results,best model,has,outperforms
translation,137,178,results,outperforms,has,AMREAGER
translation,137,178,results,Results,has,best model
translation,137,179,results,XL - AMR trans +,achieves,significantly higher performance
translation,137,179,results,significantly higher performance,in,Reentrancies
translation,137,179,results,significantly higher performance,compared to,AMREAGER
translation,137,179,results,Reentrancies,in,all the tested languages
translation,137,179,results,Results,has,XL - AMR trans +
translation,137,180,results,gold standard training data,i.e.,GOLDAMR -SILVERTRNS
translation,137,180,results,gold standard training data,leads,XL - AMR
translation,137,180,results,XL - AMR,to achieve,higher performances
translation,137,180,results,parallel sentences,associated with,silver AMR graphs
translation,137,180,results,Results,translating,gold standard training data
translation,137,212,results,XL - AMR,overcomes,most of the foregoing structural divergences
translation,137,212,results,most of the foregoing structural divergences,exception of,two cases
translation,137,212,results,conflational divergence,in,German
translation,137,212,results,Results,has,XL - AMR
translation,138,218,baselines,Top - Down,uses,beam search - based top-down method
translation,138,218,baselines,Top - Down,uses,Passive - Aggressive algorithm
translation,138,218,baselines,Top - Down,uses,Lader
translation,138,218,baselines,beam search - based top-down method,for,parsing
translation,138,218,baselines,Passive - Aggressive algorithm,for,parameter estimation
translation,138,218,baselines,Lader,uses,CYK algorithm
translation,138,218,baselines,Lader,uses,on-line SVM algorithm
translation,138,218,baselines,CYK algorithm,with,cube pruning
translation,138,218,baselines,CYK algorithm,with,on-line SVM algorithm
translation,138,218,baselines,Baselines,has,Top - Down
translation,138,5,experiments,BTG - based preordering framework,applied to,any language
translation,138,5,experiments,any language,using,parallel text
translation,138,167,hyperparameters,distortion limit,set to,5 words
translation,138,167,hyperparameters,Hyperparameters,has,distortion limit
translation,138,168,hyperparameters,Word alignments,learned using,3 iterations
translation,138,168,hyperparameters,Word alignments,learned using,3 iterations
translation,138,168,hyperparameters,3 iterations,of,IBM Model - 1
translation,138,168,hyperparameters,3 iterations,of,HMM alignment model
translation,138,168,hyperparameters,3 iterations,of,HMM alignment model
translation,138,168,hyperparameters,3 iterations,of,HMM alignment model
translation,138,168,hyperparameters,Hyperparameters,has,Word alignments
translation,138,169,hyperparameters,Lattice - based minimum error rate training ( MERT ),to optimize,feature weights
translation,138,169,hyperparameters,Hyperparameters,has,Lattice - based minimum error rate training ( MERT )
translation,138,20,model,efficient incremental top-down BTG parsing method,applied to,preordering
translation,138,20,model,Model,propose,efficient incremental top-down BTG parsing method
translation,138,166,model,decoder,adopts,regular distance distortion model
translation,138,166,model,decoder,incorporates,maximum entropy based lexicalized phrase reordering model
translation,138,166,model,Model,has,decoder
translation,138,226,results,higher BLEU scores,than,Lader
translation,138,226,results,all the language pairs,has,Top -Down
translation,138,226,results,Results,For,all the language pairs
translation,138,227,results,ja-en and ur-en,using,Forced - 100 k
translation,138,227,results,Forced - 100 k,instead of,EM - 100k
translation,138,227,results,Forced - 100 k,for,Top - Down
translation,138,227,results,Forced - 100 k,improved,BLEU scores
translation,138,227,results,BLEU scores,more than,0.6
translation,138,227,results,Results,For,ja-en and ur-en
translation,138,228,results,Manual - Rules,performed,best
translation,138,228,results,best,for,en-ja
translation,138,228,results,Results,has,Manual - Rules
translation,139,50,baselines,PARSE,generates,k-best parse trees
translation,139,83,experiments,Berkeley parsing model,trained with,5 split-merge iterations
translation,139,4,model,annotation adaptation,for,constituency treebanks
translation,139,4,model,iterative optimization procedure,to build,much larger treebank
translation,139,73,model,each iteration,of,training
translation,139,73,model,treebank transformation,of,source - to- target and target - to - source
translation,139,73,model,transformed treebank,provides,more appropriate annotation
translation,139,73,model,more appropriate annotation,for,subsequent iteration
translation,139,73,model,training,has,treebank transformation
translation,139,73,model,Model,At,each iteration
translation,139,33,results,improved parser,achieves,F-measure
translation,139,33,results,F-measure,of,0.95 % absolute improvement
translation,139,33,results,0.95 % absolute improvement,over,baseline parser
translation,139,33,results,baseline parser,trained on,CTB only
translation,139,33,results,addidional transformed treebank,has,improved parser
translation,139,33,results,Results,With,addidional transformed treebank
translation,139,92,results,our approach,has,outperforms
translation,139,92,results,outperforms,has,two strong baseline systems
translation,139,92,results,Results,shows,our approach
translation,139,93,results,0.69 % absolute improvement,on,CTB test data
translation,139,93,results,0.69 % absolute improvement,when,whole CTB training data
translation,139,93,results,CTB test data,over,direct parsing baseline
translation,139,93,results,whole CTB training data,used for,training
translation,139,94,results,our approach,further extends,advantage
translation,139,94,results,advantage,over,two baseline systems
translation,139,94,results,CTB training data,has,decreases
translation,139,94,results,Results,find that,our approach
translation,139,95,results,our approach,effective for improving,parser performance
translation,139,103,results,iterative training strategy,leads to,better parser
translation,139,103,results,better parser,with,higher accuracy
translation,139,103,results,basic annotation transformation,has,iterative training strategy
translation,139,103,results,Results,Compared to,basic annotation transformation
translation,139,104,results,final optimized parsing results,on,CTB test set
translation,139,104,results,final optimized parsing results,contributes,0.95 % absolute improvement
translation,139,104,results,0.95 % absolute improvement,over,directly parsing baseline
translation,139,104,results,Results,reports,final optimized parsing results
translation,141,219,baselines,first two,use,random word embedding
translation,141,219,baselines,random word embedding,for,initialization
translation,141,219,baselines,first one,not use,label refinement
translation,141,219,baselines,Baselines,has,first two
translation,141,148,hyperparameters,word case and prefix / suffix,use,random initialization
translation,141,148,hyperparameters,random initialization,for,embeddings
translation,141,148,hyperparameters,Hyperparameters,For,word case and prefix / suffix
translation,141,163,hyperparameters,head -modifier embeddings,for,each token
translation,141,163,hyperparameters,Hyperparameters,train,head -modifier embeddings
translation,141,8,model,algorithms,to derive,query 's syntactic structure
translation,141,8,model,query 's syntactic structure,dependency trees of,clicked sentences
translation,141,28,model,end-to - end solution,from,treebank construction
translation,141,28,model,treebank construction,to,syntactic parsing
translation,141,28,model,syntactic parsing,for,web queries
translation,141,28,model,Model,propose,end-to - end solution
translation,141,162,model,context-free signals,as,prior knowledge
translation,141,162,model,Model,treat,context-free signals
translation,141,29,results,Our model,achieves,UAS
translation,141,29,results,Our model,achieves,LAS
translation,141,29,results,UAS,of,0.830
translation,141,29,results,UAS,is,dramatic improvement
translation,141,29,results,LAS,of,0.747
translation,141,29,results,0.747,on,web queries
translation,141,29,results,dramatic improvement,over,state - of - the - art parsers
translation,141,29,results,state - of - the - art parsers,trained from,standard treebanks
translation,141,29,results,Results,has,Our model
translation,141,157,results,Our approach,has,outperforms
translation,141,157,results,outperforms,has,context-free approach
translation,141,157,results,Results,has,Our approach
translation,141,174,results,about 3 % advantage,in,UAS
translation,141,174,results,UAS,over,randomized embeddings
translation,141,174,results,head-modifier embeddings,has,about 3 % advantage
translation,141,174,results,Results,has,head-modifier embeddings
translation,141,175,results,pretrained word2vec embeddings,achieve,3 % advantage
translation,141,175,results,Results,using,pretrained word2vec embeddings
translation,141,206,results,overall success rate,is,high
translation,141,206,results,Results,has,overall success rate
translation,141,220,results,competitors,on,query parsing task
translation,141,220,results,QueryParser,has,consistently outperformed
translation,141,220,results,consistently outperformed,has,competitors
translation,141,220,results,Results,concluded that,QueryParser
translation,141,221,results,Pretrained word2vec embeddings,improve,performance
translation,141,221,results,Pretrained word2vec embeddings,improve,performance
translation,141,221,results,Pretrained word2vec embeddings,postprocess of,label refinement
translation,141,221,results,performance,by,3 - 5 percent
translation,141,221,results,performance,by,1 - 2 percent
translation,141,221,results,performance,by,1 - 2 percent
translation,141,221,results,performance,by,1 - 2 percent
translation,141,221,results,Results,postprocess of,label refinement
translation,141,221,results,Results,has,Pretrained word2vec embeddings
translation,141,222,results,conventional depencency parsers,trained on,sentence dataset
translation,141,222,results,syntactic signals,in,input
translation,141,222,results,Results,shows,conventional depencency parsers
translation,142,41,model,supervised semantic parsing,by learning from,weaker form of supervision
translation,142,41,model,Model,extend,supervised semantic parsing
translation,142,168,results,LNL models,perform,significantly better
translation,142,168,results,significantly better,than,Keyword filtering ( p < 0.05 )
translation,142,168,results,Results,observe,LNL models
translation,142,173,results,LNL,requires,fewer examples
translation,142,173,results,fewer examples,to reach,near optimal performance
translation,142,173,results,fewer examples,before,plateaus
translation,142,173,results,near optimal performance,before,plateaus
translation,142,173,results,our approach,has,consistently outperforms
translation,142,173,results,consistently outperforms,has,bag-of-words model ( BoW )
translation,142,173,results,consistently outperforms,has,LNL
translation,142,173,results,Results,observe,our approach
translation,143,145,experimental-setup,Word embeddings,pretrained with,Gensim 2 implementation of word2vec
translation,143,145,experimental-setup,Gensim 2 implementation of word2vec,on,English GigaWord corpus
translation,143,145,experimental-setup,Experimental setup,has,Word embeddings
translation,143,146,experimental-setup,dimensionality,of,word embeddings
translation,143,146,experimental-setup,word embeddings,set to,50
translation,143,146,experimental-setup,Experimental setup,has,dimensionality
translation,143,151,experimental-setup,initialized randomly,with,"Xavier 's initialization ( Glorot and Bengio , 2010 )"
translation,143,151,experimental-setup,Experimental setup,has,neural network parameters
translation,143,129,hyperparameters,Stochastic gradient descent,with,momentum
translation,143,129,hyperparameters,momentum,to update,parameters
translation,143,129,hyperparameters,parameters,of,network
translation,143,129,hyperparameters,Hyperparameters,has,Stochastic gradient descent
translation,143,130,hyperparameters,momentum,set to,0.9
translation,143,130,hyperparameters,learning rate,is,0.001
translation,143,5,model,linear-time parser,novel way of representing,discourse constituents
translation,143,5,model,discourse constituents,based on,neural networks
translation,143,5,model,neural networks,takes into account,global contextual information
translation,143,5,model,neural networks,able to capture,long-distance dependencies
translation,143,5,model,Model,propose,linear-time parser
translation,143,23,model,Model,propose,simple and efficient linear-time discourse parser
translation,143,24,model,linear-time complexity,use,two -stage approach
translation,143,24,model,two -stage approach,parse,each sentence
translation,143,24,model,each sentence,in,document
translation,143,24,model,each sentence,in,document
translation,143,24,model,each sentence,into,tree
translation,143,24,model,document,into,tree
translation,143,24,model,document,into,tree
translation,143,24,model,document,into,tree
translation,143,24,model,tree,whose,leaves
translation,143,24,model,tree,whose,leaves
translation,143,24,model,tree,whose,leaves
translation,143,24,model,leaves,correspond to,EDUs
translation,143,24,model,document,into,tree
translation,143,24,model,tree,whose,leaves
translation,143,24,model,leaves,correspond to,already preprocessed sentences
translation,143,24,model,Model,To guarantee,linear-time complexity
translation,143,30,model,documents,as,trees
translation,143,30,model,trees,using,recursive neural networks
translation,143,30,model,Model,represent,documents
translation,143,45,model,HILDA,parses,document
translation,143,45,model,document,pre-segmented into,EDUs
translation,143,45,model,document,working,iteratively
translation,143,45,model,EDUs,with,two support vector machine classifiers
translation,143,45,model,two support vector machine classifiers,working,iteratively
translation,143,45,model,iteratively,in,pipeline
translation,143,45,model,Model,has,HILDA
translation,143,54,model,Discourse constituents,at,all levels
translation,143,54,model,Discourse constituents,modeled with,recurrent neural networks
translation,143,54,model,recurrent neural networks,adopting,"relatively simple , yet efficient , architecture"
translation,143,54,model,"relatively simple , yet efficient , architecture",compared to,previously proposed neural systems
translation,143,54,model,Model,has,Discourse constituents
translation,143,63,model,multi-sentential parser,creates,discourse tree
translation,143,63,model,discourse tree,for,entire document
translation,143,63,model,Model,has,multi-sentential parser
translation,143,64,model,two conditional random field ( CRF ) models,assigning,relations
translation,143,64,model,one,for creating,discourse structure
translation,143,64,model,two conditional random field ( CRF ) models,has,one
translation,143,64,model,Model,To guarantee,linear-time complexity
translation,143,198,model,simple and efficient discourse parser,adopts,twostage parsing strategy
translation,143,198,model,CIDER,has,simple and efficient discourse parser
translation,143,198,model,Model,described,CIDER
translation,143,199,model,novel way,to learn,contextually informed representations
translation,143,199,model,contextually informed representations,of,constituents
translation,143,199,model,constituents,with,LSTM minus method
translation,143,199,model,LSTM minus method,at,sentence and document level
translation,143,199,model,Model,proposed,novel way
translation,143,165,results,span ( S ) metric,on,nuclearity ( N ) metric
translation,143,165,results,CIDER,with,flat span representations
translation,143,165,results,CIDER,with,tree span representations
translation,143,165,results,CIDER,with,tree span representations
translation,143,165,results,CIDER,on,nuclearity ( N ) metric
translation,143,165,results,CIDER,on,relation ( R ) metric
translation,143,165,results,flat span representations,much better than,CIDER
translation,143,165,results,CIDER,with,tree span representations
translation,143,165,results,CIDER,with,tree span representations
translation,143,165,results,tree representations,inferior to,flat representations
translation,143,165,results,performance gap,is,narrower
translation,143,165,results,tree span representations,are,superior
translation,143,165,results,span ( S ) metric,has,CIDER
translation,143,165,results,span ( S ) metric,has,tree span representations
translation,143,165,results,nuclearity ( N ) metric,has,tree representations
translation,143,165,results,nuclearity ( N ) metric,has,tree span representations
translation,143,165,results,relation ( R ) metric,has,tree span representations
translation,143,165,results,Results,on,span ( S ) metric
translation,143,167,results,Span features,based on,LSTM minus method
translation,143,167,results,bring improvements,over,vanilla LSTM representations
translation,143,167,results,vanilla LSTM representations,on,span and nuclearity metrics
translation,143,167,results,LSTM minus method,has,bring improvements
translation,143,167,results,Results,has,Span features
translation,143,168,results,span and tree representations,achieves,best results
translation,143,168,results,Results,combination of,span and tree representations
translation,143,174,results,our parser,achieves,comparable results
translation,143,174,results,our parser,achieves,best performance
translation,143,174,results,comparable results,on,span and relation metric
translation,143,174,results,best performance,on,nuclearity metric
translation,143,174,results,linear-time systems,has,our parser
translation,143,174,results,Results,Amongst,linear-time systems
translation,143,183,results,CIDER,across,all metrics
translation,143,183,results,outperforms,across,all metrics
translation,143,183,results,Li et al . ( 2014 ),across,all metrics
translation,143,183,results,CIDER,has,outperforms
translation,143,183,results,outperforms,has,Li et al . ( 2014 )
translation,143,190,results,CIDER,significantly better ( p < 0.05 ) than,Heilman and Sagae ( 2015 )
translation,143,190,results,CIDER,better than,Ji and Eisenstein ( 2014 )
translation,143,190,results,significantly better,than,Feng and Hirst 's 2014 system
translation,143,190,results,significantly better,on,relation metric ( p < 0.05 )
translation,143,190,results,Feng and Hirst 's 2014 system,on,relation metric ( p < 0.05 )
translation,143,190,results,Ji and Eisenstein ( 2014 ),on,span metric
translation,143,190,results,Results,has,CIDER
translation,144,96,hyperparameters,data,is,head - binarized
translation,144,96,hyperparameters,resampled,under,appropriate vague gamma and beta priors
translation,144,96,hyperparameters,Hyperparameters,has,data
translation,144,96,hyperparameters,Hyperparameters,has,hyperparameters
translation,144,26,model,blocked sampling strategy,for,TAG
translation,144,26,model,blocked sampling strategy,that is,effective and efficient
translation,144,26,model,blocked sampling strategy,prove,superiority
translation,144,26,model,superiority,against,local Gibbs sampling approach
translation,144,26,model,Model,formulate,blocked sampling strategy
translation,144,100,model,TAG Gibbs,locally adding / removing,potential adjunctions
translation,144,100,model,Model,has,TAG Gibbs
translation,144,27,results,TAG,contains,TSG
translation,144,27,results,TAG,contains,TSG
translation,144,27,results,better model,for,treebank data
translation,144,27,results,better model,leads to,improved parsing performance
translation,144,27,results,treebank data,than,TSG
translation,144,27,results,Results,show,nonparametric inference
translation,144,27,results,Results,via,nonparametric inference
translation,145,148,ablation-analysis,performance,by,0.28 LAS - F1 points
translation,145,148,ablation-analysis,performance,by,0.63
translation,145,148,ablation-analysis,0.28 LAS - F1 points,on,test treebank
translation,145,148,ablation-analysis,0.63,on,development treebank
translation,145,148,ablation-analysis,hurts,has,performance
translation,145,148,ablation-analysis,Ablation analysis,removing,NER features ( ? NER )
translation,145,150,ablation-analysis,performance,to,larger degree
translation,145,150,ablation-analysis,larger degree,by,2.87 LAS - F1 points
translation,145,150,ablation-analysis,2.87 LAS - F1 points,on,test set
translation,145,150,ablation-analysis,hurt,has,performance
translation,145,150,ablation-analysis,pre-trained word embeddings,has,slightly improves
translation,145,150,ablation-analysis,slightly improves,has,performance
translation,145,150,ablation-analysis,Ablation analysis,Removing,POS features
translation,145,151,ablation-analysis,remote edges and transitions,from,TUPA
translation,145,151,ablation-analysis,remote edges and transitions,causes,very small decrease
translation,145,151,ablation-analysis,very small decrease,in,LAS - F1
translation,145,151,ablation-analysis,Ablation analysis,Removing,remote edges and transitions
translation,145,17,experimental-setup,Universal Dependencies,using,TUPA
translation,145,17,experimental-setup,Universal Dependencies,employ,bidirectional conversion protocol
translation,145,17,experimental-setup,bidirectional conversion protocol,to represent,UD trees and graphs
translation,145,17,experimental-setup,UD trees and graphs,in,UCCA - like unified DAG format ( ?2 )
translation,145,17,experimental-setup,Experimental setup,To parse,Universal Dependencies
translation,145,17,experimental-setup,Experimental setup,employ,bidirectional conversion protocol
translation,145,135,experiments,trained TUPA models,on,all available development treebanks
translation,145,135,experiments,all available development treebanks,after fixing,bugs
translation,145,111,hyperparameters,"dropout ( Srivastava et al. , 2014 )",between,MLP layers
translation,145,111,hyperparameters,"dropout ( Srivastava et al. , 2014 )",between,BiLSTM layers
translation,145,111,hyperparameters,"recurrent dropout ( Gal and Ghahramani , 2016 )",between,BiLSTM layers
translation,145,111,hyperparameters,Hyperparameters,use,"dropout ( Srivastava et al. , 2014 )"
translation,145,111,hyperparameters,Hyperparameters,use,"recurrent dropout ( Gal and Ghahramani , 2016 )"
translation,145,112,hyperparameters,"word , lemma , coarseand fine - grained POS tag dropout",with,? = 0.2
translation,145,112,hyperparameters,embedding,replaced with,zero vector
translation,145,112,hyperparameters,Hyperparameters,use,"word , lemma , coarseand fine - grained POS tag dropout"
translation,145,113,hyperparameters,node dropout,with,probability
translation,145,113,hyperparameters,probability,of,0.1
translation,145,113,hyperparameters,0.1,at,each step
translation,145,113,hyperparameters,all features,associated with,single node
translation,145,113,hyperparameters,all features,replaced with,zero vectors
translation,145,113,hyperparameters,single node,in,parser state
translation,145,113,hyperparameters,node dropout,has,all features
translation,145,113,hyperparameters,each step,has,all features
translation,145,113,hyperparameters,Hyperparameters,use,node dropout
translation,145,114,hyperparameters,optimization,use,minibatch size
translation,145,114,hyperparameters,optimization,train with,stochastic gradient descent
translation,145,114,hyperparameters,minibatch size,of,100
translation,145,114,hyperparameters,weights,by,10 ?5
translation,145,114,hyperparameters,10 ?5,at,each update
translation,145,114,hyperparameters,stochastic gradient descent,for,50 epochs
translation,145,114,hyperparameters,stochastic gradient descent,followed by,"AMSGrad ( Sashank J. Reddi , 2018 )"
translation,145,114,hyperparameters,50 epochs,with,learning rate
translation,145,114,hyperparameters,learning rate,of,0.1
translation,145,114,hyperparameters,"AMSGrad ( Sashank J. Reddi , 2018 )",for,250 epochs
translation,145,114,hyperparameters,250 epochs,with,"? = 0.001 , ? 1 = 0.9 and ? 2 = 0.999"
translation,145,114,hyperparameters,Hyperparameters,For,optimization
translation,145,114,hyperparameters,Hyperparameters,train with,stochastic gradient descent
translation,145,119,hyperparameters,corpora,with,less than 100 training sentences
translation,145,119,hyperparameters,corpora,use,750 epochs
translation,145,119,hyperparameters,750 epochs,of,AMSGrad
translation,145,119,hyperparameters,AMSGrad,instead of,250
translation,145,119,hyperparameters,Hyperparameters,For,corpora
translation,145,19,model,enhanced dependencies,as part of,dependency graph
translation,145,26,model,TUPA,convert,UD trees and graphs
translation,145,26,model,UD trees and graphs,into,unified DAG format
translation,145,128,results,Our system,ranked,24th
translation,145,128,results,Our system,ranked,23rd
translation,145,128,results,Our system,ranked,21st
translation,145,128,results,24th,in,LAS - F1 ranking
translation,145,128,results,23rd,by,MLAS
translation,145,128,results,MLAS,average of,44.6
translation,145,128,results,21st,by,BLEX
translation,145,128,results,Results,has,Our system
translation,146,172,ablation-analysis,G2GTr,achieves,significant improvement
translation,146,172,ablation-analysis,significant improvement,has,4.62 % LAS RER
translation,146,172,ablation-analysis,Ablation analysis,adding,G2GTr
translation,146,185,ablation-analysis,graph output mechanism,results in,drop
translation,146,185,ablation-analysis,drop,in,performance
translation,146,185,ablation-analysis,performance,particularly in,long dependencies
translation,146,185,ablation-analysis,Ablation analysis,Excluding,graph output mechanism
translation,146,27,baselines,two novel Transformer models,of,transition - based dependency parsing
translation,146,27,baselines,two novel Transformer models,called,Sentence Transformer
translation,146,27,baselines,two novel Transformer models,called,State Transformer
translation,146,167,experimental-setup,AdamW optimiser,to fine-tune,model parameters
translation,146,167,experimental-setup,Wolf et al . ( 2019 ),to fine-tune,model parameters
translation,146,167,experimental-setup,Experimental setup,use,AdamW optimiser
translation,146,151,experiments,models,on,Universal Dependency Treebanks ( UD v2.3 )
translation,146,4,model,Graph2 Graph Transformer architecture,for conditioning on and predicting,arbitrary graphs
translation,146,4,model,Model,propose,Graph2 Graph Transformer architecture
translation,146,15,model,attention - based mechanism,for conditioning on,graphs
translation,146,15,model,attention - based mechanism,for conditioning on,graphs
translation,146,15,model,attention - like mechanism,for predicting,graphs
translation,146,15,model,Model,version of,Transformer architecture
translation,146,16,model,Model,call,architecture Graph2 Graph Transformer
translation,146,26,model,next dependency relation,using,only the vectors
translation,146,26,model,only the vectors,tokens involved in,relation
translation,146,26,model,Model,predicts,next dependency relation
translation,146,28,model,Sentence Transformer,computes,contextualised embeddings
translation,146,28,model,Sentence Transformer,computes,contextualised embeddings
translation,146,28,model,Sentence Transformer,uses,current parser state
translation,146,28,model,Sentence Transformer,uses,contextualised embeddings
translation,146,28,model,contextualised embeddings,for,each token
translation,146,28,model,each token,of,input sentence
translation,146,28,model,current parser state,to identify,which tokens
translation,146,28,model,current parser state,uses,contextualised embeddings
translation,146,28,model,which tokens,could be involved in,next valid parse transition
translation,146,28,model,contextualised embeddings,to choose,best transition
translation,146,28,model,Model,has,Sentence Transformer
translation,146,29,model,State Transformer,directly use,current parser state
translation,146,29,model,State Transformer,choose,best transition
translation,146,29,model,current parser state,input to,model
translation,146,29,model,model,along with,encoding
translation,146,29,model,encoding,of,partially constructed parse graph
translation,146,29,model,best transition,using,embeddings
translation,146,29,model,Model,For,State Transformer
translation,146,31,model,Graph2 Graph Transformer architecture,can be effectively initialised with,standard pre-trained Transformer models
translation,146,42,model,predicting graphs,call,Graph2 Graph Transformer ( G2 GTr )
translation,146,42,model,Model,call,Graph2 Graph Transformer ( G2 GTr )
translation,146,30,results,Both baseline models,achieve,competitive or better results
translation,146,30,results,competitive or better results,than,previous state - of - the - art traditional transition - based models
translation,146,30,results,substantial improvement,by integrating,Graph2 Graph Transformer
translation,146,30,results,Results,has,Both baseline models
translation,146,32,results,Graph2 Graph Transformer parser,with,"pretrained BERT ( Devlin et al. , 2018 ) parameters"
translation,146,32,results,"pretrained BERT ( Devlin et al. , 2018 ) parameters",leads to,substantial improvements
translation,146,32,results,Results,Initialising,Graph2 Graph Transformer parser
translation,146,43,results,G2 GTr,supports,arbitrary input graphs
translation,146,43,results,G2 GTr,supports,arbitrary edges
translation,146,43,results,arbitrary edges,in,output graph
translation,146,43,results,Results,has,G2 GTr
translation,146,117,results,Predicting transitions,embedding of,first sub-word
translation,146,117,results,first sub-word,for,each word
translation,146,117,results,better performance,than using,last one
translation,146,117,results,Results,has,Predicting transitions
translation,146,173,results,BERT SentTr+G2 GTr model,with,7 self-attention layers
translation,146,173,results,7 self-attention layers,instead of,6
translation,146,173,results,7 self-attention layers,resulting in,2.19 % LAS RER
translation,146,173,results,Results,evaluate,BERT SentTr+G2 GTr model
translation,146,174,results,LAS scores,on,13 UD Treebanks
translation,146,174,results,LAS scores,of,transition - based model
translation,146,174,results,scores,of,transition - based model
translation,146,174,results,transition - based model,uses,deep contextualized word representations
translation,146,174,results,deep contextualized word representations,of,BERT and ELMo
translation,146,174,results,Results,show,LAS scores
translation,146,174,results,Results,use,scores
translation,146,175,results,baseline,on,9 languages
translation,146,175,results,BERT StateTr+G2GTr model,has,outperforms
translation,146,175,results,outperforms,has,baseline
translation,146,175,results,Results,has,BERT StateTr+G2GTr model
translation,146,178,results,BERT SentTr+G2GTr model,performs,substantially better
translation,146,178,results,substantially better,than,baseline
translation,146,178,results,baseline,on,all languages
translation,146,178,results,Results,has,BERT SentTr+G2GTr model
translation,146,184,results,other models,on,longer ( more difficult ) dependencies
translation,146,184,results,integrated G2GTr models,has,outperform
translation,146,184,results,outperform,has,other models
translation,146,184,results,Results,has,integrated G2GTr models
translation,146,186,results,composition component,in,StateTr+G2 GTr model
translation,146,186,results,composition component,does n't,improve
translation,146,186,results,performance,at,any length
translation,146,186,results,improve,has,performance
translation,146,186,results,Results,Keeping,composition component
translation,147,10,results,human- in- the - loop parser,improves on,state of the art
translation,147,10,results,state of the art,with,less than 2 questions per sentence
translation,147,10,results,state of the art,with,gain
translation,147,10,results,gain,of,1.7 F1
translation,147,10,results,1.7 F1,on,10 %
translation,147,10,results,Results,has,human- in- the - loop parser
translation,147,107,results,much larger improvement ( 1.7 F1 ),on,subset of sentences
translation,147,107,results,changed,after,reparsing
translation,147,109,results,improvements,on,CCGbank
translation,147,109,results,CCGbank,are,modest
translation,147,109,results,Results,has,improvements
translation,147,140,results,in - domain and out-domain,have,modest gain
translation,147,140,results,modest gain,over,entire corpus
translation,147,140,results,Results,For,in - domain and out-domain
translation,147,140,results,Results,both,in - domain and out-domain
translation,148,26,experiments,our model,using,Knet deep learning framework
translation,148,26,experiments,Knet deep learning framework,in,Julia language
translation,148,20,hyperparameters,word vectors,coming from,language model
translation,148,20,hyperparameters,language model,pretrained on,very large language corpora
translation,148,20,hyperparameters,Hyperparameters,use,word vectors
translation,148,37,hyperparameters,parsing model,uses,pretrained word embeddings
translation,148,37,hyperparameters,pretrained word embeddings,from,K?rnap et al . ( 2017 )
translation,148,37,hyperparameters,Hyperparameters,has,parsing model
translation,148,60,hyperparameters,three - layer bidirectional LSTM,to represent,sentence
translation,148,60,hyperparameters,Hyperparameters,used,three - layer bidirectional LSTM
translation,148,61,hyperparameters,hidden size,of,200
translation,148,61,hyperparameters,200,for,both forward and backward LSTMs
translation,148,61,hyperparameters,Hyperparameters,used,hidden size
translation,148,62,hyperparameters,"Dropout ( Srivastava et al. , 2014 )",performed at,input
translation,148,62,hyperparameters,input,of,each LSTM layer
translation,148,62,hyperparameters,Hyperparameters,has,"Dropout ( Srivastava et al. , 2014 )"
translation,148,92,hyperparameters,350 - dimensional word embeddings,for,word model
translation,148,92,hyperparameters,Hyperparameters,used,150 - dimensional tag and feature embeddings
translation,148,92,hyperparameters,Hyperparameters,used,350 - dimensional word embeddings
translation,148,93,hyperparameters,hidden size,of,200
translation,148,93,hyperparameters,200,for,forward and backward RNNs
translation,148,93,hyperparameters,forward and backward RNNs,producing,400 - dimensional feature context vectors
translation,148,93,hyperparameters,Bi-RNN sentence model,has,hidden size
translation,148,93,hyperparameters,Hyperparameters,has,Bi-RNN sentence model
translation,148,94,hyperparameters,hidden size,of,400
translation,148,94,hyperparameters,hidden size,of,100
translation,148,94,hyperparameters,400,for,arc MLPs
translation,148,94,hyperparameters,100,for,relation MLPs
translation,148,94,hyperparameters,Hyperparameters,used,hidden size
translation,148,94,hyperparameters,Hyperparameters,used,100
translation,148,96,hyperparameters,"Adam optimizer ( Kingma and Ba , 2014 )",with,standard parameters
translation,148,96,hyperparameters,Hyperparameters,used,"Adam optimizer ( Kingma and Ba , 2014 )"
translation,148,97,hyperparameters,model,for,25 to 100 epochs
translation,148,97,hyperparameters,model,based on,validation labeled attachment accuracy
translation,148,97,hyperparameters,model,based on,validation labeled attachment accuracy
translation,148,5,model,state- of - the - art biaffine parser,with,structural meta-learning module
translation,148,5,model,structural meta-learning module,combines,local and global label predictions
translation,148,5,model,Model,extends,state- of - the - art biaffine parser
translation,148,21,model,word embeddings,with,learnable embeddings
translation,148,21,model,word embeddings,with,FEATs
translation,148,21,model,learnable embeddings,for,UPOS tags
translation,148,21,model,learnable embeddings,for,XPOS tags
translation,148,21,model,Model,extend,word embeddings
translation,148,23,model,meta-learning module,allows,structured and unstructured predictions
translation,148,23,model,structured and unstructured predictions,to be combined as,weighted sum
translation,148,23,model,Model,propose,meta-learning module
translation,148,24,model,additional computational complexity,paid off,our simple word -level model
translation,148,24,model,our simple word -level model,in,encoder part
translation,148,24,model,Model,has,additional computational complexity
translation,148,35,model,dependency parsing model,based on,graph - based parser
translation,148,35,model,Model,propose,dependency parsing model
translation,148,123,model,decoding mechanism,called,SMeta
translation,148,123,model,decoding mechanism,for,graph - based neural dependency parsing
translation,148,123,model,Model,proposed,decoding mechanism
translation,148,124,model,structured and unstructured prediction methods,using,meta-learning
translation,148,124,model,Model,combine,structured and unstructured prediction methods
translation,148,113,results,MLAS score,of,our model
translation,148,113,results,MLAS score,of,our model
translation,148,113,results,our model,is,46.40 %
translation,148,113,results,our model,ranked,22nd
translation,148,113,results,22nd,out of,submissions
translation,148,113,results,submissions,of,27 models
translation,148,113,results,Results,has,MLAS score
translation,148,114,results,BLEX score,of,our model
translation,148,114,results,BLEX score,of,our model
translation,148,114,results,our model,is,49.17 %
translation,148,114,results,our model,ranked 21st out of,best BLEX results
translation,148,114,results,best BLEX results,of,all 27 models
translation,148,114,results,all 27 models,including,unofficial runs
translation,148,114,results,Results,has,BLEX score
translation,148,118,results,very small or no training data,such as,Japanese Modern
translation,148,118,results,very small or no training data,such as,Russian Taiga
translation,148,118,results,very small or no training data,such as,Irish IDT
translation,148,118,results,very small or no training data,get,lower scores
translation,149,17,model,"off-the-shelf , trainable , dependency parser",is enough to build,highly - competitive constituent parser
translation,149,17,model,Model,show that,"off-the-shelf , trainable , dependency parser"
translation,149,26,results,discontinuous parsing,surpass,current state of the art
translation,149,26,results,discontinuous parsing,achieving,fast parsing speeds
translation,149,26,results,current state of the art,by,wide margin
translation,149,26,results,wide margin,on,two German datasets ( TIGER and NEGRA )
translation,149,26,results,Results,For,discontinuous parsing
translation,149,197,results,Results,on,English PTB
translation,149,198,results,our simple reduction - based c-parser,surpasses,three Stanford parsers
translation,149,198,results,on par,with,Berkeley parser
translation,149,198,results,Results,see that,our simple reduction - based c-parser
translation,149,202,results,Results,on,SPMRL Datasets
translation,149,211,results,outperforms,with or without,prescribed POS tags
translation,149,211,results,"Berkeley parser ( Petrov and Klein , 2007 )",with or without,prescribed POS tags
translation,149,211,results,all languages ex-cept French,has,our system
translation,149,211,results,our system,has,outperforms
translation,149,211,results,outperforms,has,"Berkeley parser ( Petrov and Klein , 2007 )"
translation,149,211,results,Results,For,all languages ex-cept French
translation,149,212,results,average F 1 - scores,superior to,best non-reranking system
translation,149,212,results,best non-reranking system,participating in,"shared task ( Crabb? and Seddah , 2014 )"
translation,149,212,results,best results,for,4 out of 8 languages
translation,149,212,results,Results,has,average F 1 - scores
translation,149,213,results,Discontinuous Treebanks,experimented on,two widely - used discontinuous German treebanks
translation,149,213,results,Results,on,Discontinuous Treebanks
translation,149,213,results,Results,experimented on,two widely - used discontinuous German treebanks
translation,149,222,results,our approach,achieving,state - of - the - art accuracies
translation,149,222,results,outperforms,achieving,state - of - the - art accuracies
translation,149,222,results,state - of - the - art accuracies,for,both datasets
translation,149,222,results,our approach,has,outperforms
translation,149,222,results,outperforms,has,all the competitors
translation,149,222,results,Results,observe,our approach
translation,150,37,ablation-analysis,Ablation analysis,has,Latent syntactic features
translation,150,105,ablation-analysis,NPMI,for,schema linking
translation,150,105,ablation-analysis,6 + % absolute decrease,in,exact matching accuracies
translation,150,105,ablation-analysis,exact matching accuracies,of,IRNet
translation,150,105,ablation-analysis,exact matching accuracies,both,development and test sets
translation,150,105,ablation-analysis,Ablation analysis,Without using,NPMI
translation,150,73,baselines,PhoBERT,is,monolingual variant
translation,150,73,baselines,monolingual variant,of,RoBERTa
translation,150,73,baselines,monolingual variant,pre-trained on,20 GB
translation,150,73,baselines,RoBERTa,for,Vietnamese
translation,150,73,baselines,20 GB,of,word- level Vietnamese texts
translation,150,73,baselines,Baselines,has,PhoBERT
translation,150,103,baselines,Baselines,has,NPMI - based schema linking
translation,150,31,experiments,first public largescale Text-to - SQL dataset,for,Vietnamese semantic parsing task
translation,150,51,experiments,word-level version,apply,RDRSegmenter
translation,150,51,experiments,RDRSegmenter,from,VnCoreNLP
translation,150,51,experiments,RDRSegmenter,to perform,automatic Vietnamese word segmentation
translation,150,98,experiments,EditSQL and IRNet,on,English Spider dataset
translation,150,98,experiments,EditSQL and IRNet,use of,strong pretrained language model RoBERTa
translation,150,98,experiments,strong pretrained language model RoBERTa,instead of,BERT
translation,150,34,results,Our human- translated dataset,is,far more reliable
translation,150,34,results,far more reliable,than,dataset
translation,150,34,results,dataset,consisting of,machine - translated questions
translation,150,34,results,overall result,obtained for,Vietnamese
translation,150,34,results,Vietnamese,comparable to,English
translation,150,35,results,Automatic Vietnamese word segmentation,improves,performances
translation,150,35,results,performances,of,baselines
translation,150,35,results,Results,has,Automatic Vietnamese word segmentation
translation,150,36,results,NPMI score,linking,cell value
translation,150,36,results,useful,linking,cell value
translation,150,36,results,cell value,mentioned in,question
translation,150,36,results,question,to,column
translation,150,36,results,column,in,database schema
translation,150,36,results,Results,has,NPMI score
translation,150,38,results,Highest improvements,accounted for the use of,pre-trained language models
translation,150,38,results,pre-trained language models,where,"PhoBERT ( Nguyen and Nguyen , 2020 )"
translation,150,38,results,"PhoBERT ( Nguyen and Nguyen , 2020 )",helps produce,higher results
translation,150,38,results,higher results,than,"XLM -R ( Conneau et al. , 2020 )"
translation,150,38,results,Results,has,Highest improvements
translation,150,53,results,Original ( Easy question,involving,one table
translation,150,53,results,Results,has,Original ( Easy question
translation,150,77,results,EditSQL and IRNet,require,input pre-trained embeddings
translation,150,77,results,input pre-trained embeddings,for,syllables [ MT ]
translation,150,92,results,human-translated dataset,is,far more reliable
translation,150,92,results,far more reliable,than,dataset
translation,150,92,results,dataset,consisting of,machinetranslated questions
translation,150,92,results,Results,find that,human-translated dataset
translation,150,93,results,word level,compared to,machine - translated dataset
translation,150,93,results,our dataset,obtains,about 30.2- 17.4 ? 13 % and 43.6- 21.6 = 22 % absolute improvements
translation,150,93,results,about 30.2- 17.4 ? 13 % and 43.6- 21.6 = 22 % absolute improvements,in,accuracies
translation,150,93,results,accuracies,of,EditSQL and IRNet
translation,150,93,results,word level,has,our dataset
translation,150,93,results,machine - translated dataset,has,our dataset
translation,150,93,results,Results,at,word level
translation,150,94,results,word- based Text- to - SQL parsing,obtains,about 5 + % absolute higher accuracies
translation,150,94,results,about 5 + % absolute higher accuracies,than,syllable - based Text-to - SQL parsing
translation,150,94,results,Results,has,word- based Text- to - SQL parsing
translation,150,95,results,latent syntactic features,dumped from,pre-trained dependency parser jPTDP
translation,150,95,results,latent syntactic features,help improve,performances
translation,150,95,results,pre-trained dependency parser jPTDP,for,Vietnamese
translation,150,95,results,performances,of,baselines
translation,150,95,results,IRNet,has,43.6%?47.1 %
translation,150,95,results,Results,has,latent syntactic features
translation,150,96,results,biggest improvements,use of,pre-trained language models
translation,150,96,results,Results,has,biggest improvements
translation,150,97,results,PhoBERT,helps produce,higher results
translation,150,97,results,higher results,than,XLM -R
translation,150,97,results,IRNet,53.2 % vs.,52.8 %
translation,150,97,results,EditSQL,has,52.6 % vs. 51.3 %
translation,150,97,results,Results,has,PhoBERT
translation,150,99,results,overall results,for,Vietnamese
translation,150,99,results,Vietnamese,are,smaller
translation,150,99,results,Vietnamese,are,comparable
translation,150,99,results,comparable,to,English results
translation,150,99,results,Results,find that,overall results
translation,150,102,results,pre-trained language models PhoBERT and XLM -R,help produce,substantially higher results
translation,150,102,results,substantially higher results,than,latent syntactic features
translation,150,102,results,substantially higher results,especially for,WHERE component
translation,150,102,results,Results,in most cases,pre-trained language models PhoBERT and XLM -R
translation,151,216,ablation-analysis,prior,helps identify,unnatural samples
translation,151,216,ablation-analysis,Ablation analysis,showing,prior
translation,151,217,ablation-analysis,drops,when,? > 0.1
translation,151,217,ablation-analysis,model,has,drops
translation,151,23,experiments,STRUCTVAE,to,semantic parsing
translation,151,23,experiments,STRUCTVAE,to,Python code generation
translation,151,23,experiments,STRUCTVAE,on,Python code generation
translation,151,23,experiments,semantic parsing,on,ATIS domain
translation,151,199,experiments,classical tri-gram Kneser - Ney language model,as,prior
translation,151,138,hyperparameters,sample size,is,5
translation,151,138,hyperparameters,| S ( x ) |,is,5
translation,151,138,hyperparameters,sample size,has,| S ( x ) |
translation,151,138,hyperparameters,Hyperparameters,has,sample size
translation,151,6,model,Model,introduce,STRUCTVAE
translation,151,7,model,STRUCTVAE,models,latent MRs
translation,151,7,model,latent MRs,not observed in,unlabeled data
translation,151,7,model,latent MRs,as,treestructured latent variables
translation,151,7,model,Model,has,STRUCTVAE
translation,151,17,model,Model,focus on,semi-supervised learning
translation,151,18,model,principled deep generative approach,for,semi-supervised learning
translation,151,18,model,semi-supervised learning,with,tree-structured latent variables
translation,151,18,model,STRUCTVAE,has,principled deep generative approach
translation,151,18,model,Model,propose,STRUCTVAE
translation,151,18,model,Model,propose,principled deep generative approach
translation,151,24,model,transition - based semantic parser,uses,"Abstract Syntax Trees ( ASTs , ? 3.2 )"
translation,151,24,model,Model,implement,transition - based semantic parser
translation,151,198,model,inference model,trained to construct,tree-structured logical form
translation,151,198,model,tree-structured logical form,using,transition actions
translation,151,198,model,Model,has,inference model
translation,151,156,results,our supervised parser,trained on,full data
translation,151,156,results,our supervised parser,is,competitive
translation,151,156,results,our supervised parser,on par with,Abstract Syntax Network ( ASN )
translation,151,156,results,competitive,with,existing neural network based models
translation,151,156,results,competitive,surpassing,SEQ2TREE model
translation,151,156,results,Abstract Syntax Network ( ASN ),without using,extra supervision
translation,151,156,results,ATIS,has,our supervised parser
translation,151,156,results,Results,On,ATIS
translation,151,157,results,DJANGO,has,our model
translation,151,157,results,our model,has,significantly outperforms
translation,151,157,results,significantly outperforms,has,YN17 system
translation,151,157,results,Results,On,DJANGO
translation,151,160,results,proposed STRUCTVAE,with,supervised parser
translation,151,160,results,supervised parser,when,extra unlabeled data
translation,151,160,results,semi-supervised learning,with,STRUCTVAE
translation,151,160,results,semi-supervised learning,consistently achieves,better performance
translation,151,160,results,STRUCTVAE,consistently achieves,better performance
translation,151,160,results,Results,comparing,proposed STRUCTVAE
translation,151,161,results,our model,registers results,as competitive
translation,151,161,results,as competitive,as,previous state - of - the - art method ( YN17 )
translation,151,161,results,previous state - of - the - art method ( YN17 ),using,only half the training data
translation,151,161,results,DJANGO,has,our model
translation,151,161,results,Results,on,DJANGO
translation,151,163,results,STRUCTVAE,with,self-training
translation,151,163,results,STRUCTVAE,find,STRUCTVAE
translation,151,163,results,STRUCTVAE,find,outperforms
translation,151,163,results,STRUCTVAE,find,SELFTRAIN
translation,151,163,results,SELFTRAIN,in,eight out of ten settings
translation,151,163,results,SELFTRAIN,under-performs,supervised parser
translation,151,163,results,supervised parser,in,four out of ten settings
translation,151,163,results,STRUCTVAE,has,outperforms
translation,151,163,results,outperforms,has,SELFTRAIN
translation,151,163,results,Results,comparing,STRUCTVAE
translation,151,202,results,significant gains,demonstrating,compatibility
translation,151,204,results,STRUCT -VAE - SEQ,achieves,improvements
translation,151,204,results,improvements,of,8- 10 points
translation,151,204,results,8- 10 points,when,| L | < 1000
translation,151,204,results,Results,found that,gains
translation,151,214,results,STRUCTVAE,degenerates to,vanilla auto-encoder
translation,151,214,results,vanilla auto-encoder,without,"prior distribution ( i.e. , ? = 0 )"
translation,151,214,results,vanilla auto-encoder,under-performs,supervised baseline
translation,151,214,results,Results,When,STRUCTVAE
translation,151,218,results,Results,has,Impact of Unlabeled Data Size
translation,152,64,experimental-setup,jPTDP v2.0,implemented using,DYNET v2.0
translation,152,64,experimental-setup,DYNET v2.0,with,fixed random seed
translation,152,64,experimental-setup,Experimental setup,has,jPTDP v2.0
translation,152,65,experimental-setup,Word embeddings,by,pre-trained word vectors
translation,152,65,experimental-setup,Word embeddings,are,randomly initialized
translation,152,65,experimental-setup,character and POS tag embeddings,are,randomly initialized
translation,152,65,experimental-setup,Experimental setup,has,Word embeddings
translation,152,66,experimental-setup,character - level word embeddings,use,one- layer BiLSTM seq
translation,152,66,experimental-setup,character - level word embeddings,set,size
translation,152,66,experimental-setup,size,of,LSTM hidden states
translation,152,66,experimental-setup,LSTM hidden states,equal to,vector size
translation,152,66,experimental-setup,vector size,of,character embeddings
translation,152,66,experimental-setup,Experimental setup,For learning,character - level word embeddings
translation,152,67,experimental-setup,"dropout ( Srivastava et al. , 2014 )",with,67 % keep probability
translation,152,67,experimental-setup,67 % keep probability,to,inputs
translation,152,67,experimental-setup,inputs,of,BiLSTMs and MLPs
translation,152,67,experimental-setup,Experimental setup,apply,"dropout ( Srivastava et al. , 2014 )"
translation,152,70,experimental-setup,objective loss,using,"Adam ( Kingma and Ba , 2014 )"
translation,152,70,experimental-setup,"Adam ( Kingma and Ba , 2014 )",with,initial learning rate
translation,152,70,experimental-setup,initial learning rate,at,0.001
translation,152,70,experimental-setup,Experimental setup,optimize,objective loss
translation,152,71,experimental-setup,training,run for,30 epochs
translation,152,71,experimental-setup,training,restart,Adam optimizer
translation,152,71,experimental-setup,training,anneal,initial learning rate
translation,152,71,experimental-setup,initial learning rate,at,proportion
translation,152,71,experimental-setup,proportion,of,0.5
translation,152,71,experimental-setup,0.5,every,10 epochs
translation,152,71,experimental-setup,Experimental setup,For,training
translation,152,75,experimental-setup,number of hidden nodes,in,MLPs
translation,152,75,experimental-setup,MLPs,at,100
translation,152,75,experimental-setup,Experimental setup,fix,number of hidden nodes
translation,152,76,experimental-setup,minimal grid search,of,hyper-parameters
translation,152,76,experimental-setup,hyper-parameters,to select,number of BiLSTM pos and BiLSTM dep layers
translation,152,76,experimental-setup,hyper-parameters,to select,size
translation,152,76,experimental-setup,number of BiLSTM pos and BiLSTM dep layers,from,"{ 1 , 2 }"
translation,152,76,experimental-setup,size,of,LSTM hidden states
translation,152,76,experimental-setup,LSTM hidden states,in,each layer
translation,152,76,experimental-setup,each layer,from,"{ 128 , 256 }"
translation,152,77,experimental-setup,number of BiLSTM layers,at,2
translation,152,77,experimental-setup,size of hidden states,at,128
translation,152,77,experimental-setup,Experimental setup,fix,size of hidden states
translation,152,81,experimental-setup,Word embeddings,initialized by,100 dimensional GloVe word vectors
translation,152,81,experimental-setup,100 dimensional GloVe word vectors,pre-trained on,Wikipedia and Gigaword
translation,152,81,experimental-setup,Experimental setup,has,Word embeddings
translation,152,78,experiments,Experiments,on,English Penn treebank
translation,152,5,model,well - known BIST graph - based dependency parser,by incorporating,BiLSTM - based tagging component
translation,152,5,model,BiLSTM - based tagging component,to produce,automatically predicted POS tags
translation,152,5,model,automatically predicted POS tags,for,parser
translation,152,5,model,Model,extends,well - known BIST graph - based dependency parser
translation,152,19,model,novel neural network - based model,for,jointly learning
translation,152,19,model,jointly learning,has,POS tagging and dependency paring
translation,152,19,model,Model,present,novel neural network - based model
translation,152,20,model,joint model,extends,well - known BIST graph - based dependency parser
translation,152,20,model,well - known BIST graph - based dependency parser,with,additional lower - level BiLSTM - based tagging component
translation,152,20,model,Model,has,joint model
translation,152,6,results,our model,obtains,strong UAS and LAS scores
translation,152,6,results,our model,obtains,state - of - the - art POS tagging accuracy
translation,152,6,results,our model,obtaining,state - of - the - art POS tagging accuracy
translation,152,6,results,strong UAS and LAS scores,at,94.51 % and 92.87 %
translation,152,6,results,strong UAS and LAS scores,producing,1.5 + % absolute improvements
translation,152,6,results,1.5 + % absolute improvements,to,BIST graph - based parser
translation,152,6,results,state - of - the - art POS tagging accuracy,at,97.97 %
translation,152,6,results,benchmark English Penn treebank,has,our model
translation,152,6,results,Results,On,benchmark English Penn treebank
translation,152,22,results,our model,duces,1.5 + % absolute improvement
translation,152,22,results,our model,obtaining,state - of- the - art POS tagging accuracy
translation,152,22,results,1.5 + % absolute improvement,over,BIST graph - based parser
translation,152,22,results,1.5 + % absolute improvement,with,strong UAS score
translation,152,22,results,1.5 + % absolute improvement,with,LAS score
translation,152,22,results,strong UAS score,of,94.51 %
translation,152,22,results,LAS score,of,92.87 %
translation,152,22,results,state - of- the - art POS tagging accuracy,of,97.97 %
translation,152,22,results,benchmark English Penn treebank test,has,our model
translation,152,22,results,Results,Evaluated on,benchmark English Penn treebank test
translation,152,85,results,our model,produces,very competitive parsing results
translation,152,85,results,Results,has,our model
translation,152,86,results,UAS score,at,94.51 %
translation,152,86,results,LAS score,at,92.87 %
translation,152,86,results,% and 1.9 % absolute higher,than,UAS and LAS scores
translation,152,88,results,0.9 % lower parsing scores,than,state - of - the - art dependency parser
translation,152,88,results,Results,achieve,0.9 % lower parsing scores
translation,152,93,results,state - of - the - art POS tagging accuracy,at,97.97 %
translation,152,93,results,97.97 %,on,test Section
translation,152,93,results,Results,obtain,state - of - the - art POS tagging accuracy
translation,152,107,results,baseline UDPipe 1.2,with,0.6 % absolute higher
translation,152,107,results,baseline UDPipe 1.2,with,2.5 + % higher
translation,152,107,results,outperform,has,baseline UDPipe 1.2
translation,152,107,results,0.6 % absolute higher,has,average UPOS F1 score
translation,152,107,results,2.5 + % higher,has,average UAS and LAS F1 scores
translation,152,108,results,""" big "" category",consisting of,61 treebank test sets
translation,152,108,results,""" big "" category",obtain,0.8 % higher UPOS
translation,152,108,results,""" big "" category",obtain,3.6 % higher
translation,152,108,results,3.6 % higher,than,UDPipe 1.2
translation,152,108,results,LAS,than,UDPipe 1.2
translation,152,108,results,3.1 % higher,has,UAS
translation,152,108,results,3.6 % higher,has,LAS
translation,152,108,results,Results,for,""" big "" category"
translation,152,133,results,highest F1 scores,for,biomedical event extraction
translation,152,133,results,highest F1 scores,for,opinion analysis
translation,152,133,results,Results,achieved,highest F1 scores
translation,153,4,model,dependency parsing scheme,jointly parses,sentence
translation,153,4,model,dependency parsing scheme,repairs,grammatical errors
translation,153,4,model,grammatical errors,by extending,non-directional transitionbased formalism
translation,153,4,model,non-directional transitionbased formalism,with,three additional actions
translation,153,4,model,Model,propose,dependency parsing scheme
translation,153,5,model,simple constraints,that ensure,parser termination
translation,153,5,model,Model,introduce,simple constraints
translation,153,20,model,Model,introduce,simple constraints
translation,153,79,results,accuracy,is,lower
translation,153,79,results,lower,when,parser
translation,153,79,results,parser,trained on,noisier data
translation,153,79,results,error-free test set ( 0 % ),has,baseline ( EF pipeline )
translation,153,79,results,baseline ( EF pipeline ),has,outperforms
translation,153,79,results,outperforms,has,other EREF models
translation,153,86,results,grammaticality improvement ( 1 - 4 scale ),on,TLE corpus
translation,153,86,results,successful and failure corrections,by,EREF
translation,153,86,results,Results,demonstrates,grammaticality improvement ( 1 - 4 scale )
translation,153,86,results,Results,result of,grammaticality improvement ( 1 - 4 scale )
translation,153,87,results,Minimally trained models ( E05 and E10 ),show,little improvement
translation,153,87,results,little improvement,in,grammaticality
translation,153,87,results,Results,has,Minimally trained models ( E05 and E10 )
translation,153,88,results,models,with,higher error-injection rates ( E15 and E20 )
translation,153,88,results,higher error-injection rates ( E15 and E20 ),achieve,0.1 to 0.3 improvements
translation,153,88,results,Results,has,models
translation,154,150,ablation-analysis,weakest points,of,our system
translation,154,150,ablation-analysis,weakest points,are,top nodes prediction
translation,154,150,ablation-analysis,weakest points,are,edges prediction
translation,154,150,ablation-analysis,our system,are,edges prediction
translation,154,150,ablation-analysis,Ablation analysis,has,weakest points
translation,154,159,ablation-analysis,decrease,without,BERT embeddings
translation,154,159,ablation-analysis,decrease,showing,contextual embeddings
translation,154,159,ablation-analysis,all metrics,has,decrease
translation,154,159,ablation-analysis,Ablation analysis,has,all metrics
translation,154,123,experimental-setup,network,using,lazy variant
translation,154,123,experimental-setup,lazy variant,of,Adam optimizer
translation,154,123,experimental-setup,lazy variant,with,? 2 = 0.98
translation,154,123,experimental-setup,? 2 = 0.98,for,10 epochs
translation,154,123,experimental-setup,10 epochs,with,learning rate
translation,154,123,experimental-setup,learning rate,of,10 ?3
translation,154,123,experimental-setup,learning rate,of,10 ?4
translation,154,123,experimental-setup,5 additional epochs,with,learning rate
translation,154,123,experimental-setup,learning rate,has,10 ?4
translation,154,123,experimental-setup,Experimental setup,trained,network
translation,154,124,experimental-setup,batch size,of,64 graphs
translation,154,124,experimental-setup,Experimental setup,utilized,batch size
translation,154,126,experimental-setup,training time,on,single GPU
translation,154,126,experimental-setup,single GPU,was,1 - 4 hours
translation,154,126,experimental-setup,single GPU,was,10 hours
translation,154,126,experimental-setup,1 - 4 hours,for,DM
translation,154,126,experimental-setup,1 - 4 hours,for,PSD
translation,154,126,experimental-setup,1 - 4 hours,for,EDS
translation,154,126,experimental-setup,1 - 4 hours,for,UCCA
translation,154,126,experimental-setup,10 hours,for,AMR
translation,154,126,experimental-setup,Experimental setup,has,training time
translation,154,130,experimental-setup,token embeddings,using,two layers of bidirectional LSTMs
translation,154,130,experimental-setup,token embeddings,using,dimension
translation,154,130,experimental-setup,two layers of bidirectional LSTMs,with,residual connections
translation,154,130,experimental-setup,dimension,of,768
translation,154,130,experimental-setup,Experimental setup,processed,token embeddings
translation,154,132,experimental-setup,dropout,with,rate 0.3
translation,154,132,experimental-setup,dropout,with,rate
translation,154,132,experimental-setup,dropout,on,all node representations
translation,154,132,experimental-setup,dropout,with,rate
translation,154,132,experimental-setup,rate 0.3,before and after,every LSTM layer
translation,154,132,experimental-setup,word dropout,with,rate
translation,154,132,experimental-setup,rate,of,0.2
translation,154,132,experimental-setup,Experimental setup,employed,dropout
translation,154,132,experimental-setup,Experimental setup,utilized,word dropout
translation,154,133,experimental-setup,dimensionality,of,1024
translation,154,133,experimental-setup,AddEdges operation,has,all attention layers
translation,154,133,experimental-setup,Experimental setup,In,AddEdges operation
translation,154,10,experiments,five formally and linguistically different approaches,to,meaning representation
translation,154,10,experiments,five formally and linguistically different approaches,with,varying degree of linguistic and structural complexity
translation,154,10,experiments,five formally and linguistically different approaches,EDS,Elementary Dependency Structures
translation,154,10,experiments,DM,EDS,Elementary Dependency Structures
translation,154,10,experiments,PSD,EDS,Elementary Dependency Structures
translation,154,6,model,Model,For,MRP 2019
translation,154,11,model,uniform meaning representation parsing,propose,"uniform , language and structure agnostic graph- to - graph neural network architecture"
translation,154,11,model,"uniform , language and structure agnostic graph- to - graph neural network architecture",models,semantic representation
translation,154,11,model,semantic representation,from,input sequences
translation,154,11,model,Model,propose,"uniform , language and structure agnostic graph- to - graph neural network architecture"
translation,154,166,model,uniform graph- to - graph architecture,for,parsing into semantic graphs
translation,154,166,model,Model,introduced,uniform graph- to - graph architecture
translation,154,148,results,our system,reaches,high accuracy
translation,154,148,results,high accuracy,in,node labels and properties prediction
translation,154,148,results,Results,has,our system
translation,154,152,results,our system,would achieve,"ranks 5 , 4 , 4 , 4 and 4"
translation,154,152,results,"ranks 5 , 4 , 4 , 4 and 4",on,"DM , PSD , EDS , UCCA and AMR"
translation,154,152,results,"ranks 5 , 4 , 4 , 4 and 4",showing,relatively balanced performance
translation,154,152,results,Results,has,our system
translation,154,153,results,largest absolute performance gap,of,our system
translation,154,153,results,our system,occurs on,UCCA
translation,154,153,results,UCCA,reach,8 percent points lower score
translation,154,153,results,8 percent points lower score,than,best system
translation,154,153,results,Results,has,largest absolute performance gap
translation,154,158,results,macro- averaged all performance,without,BERT embeddings
translation,154,158,results,macro- averaged all performance,is,substantially lower
translation,154,158,results,79 %,compared to,84 %
translation,154,158,results,Results,has,macro- averaged all performance
translation,154,162,results,system performance,increased by,more than 1 percent point
translation,154,162,results,Results,has,system performance
translation,154,164,results,weakest point,of,our solution
translation,154,164,results,our solution,are,edge predictions
translation,154,164,results,edge predictions,rank,"8 , 7 , 6 , 4 and 3"
translation,154,164,results,"8 , 7 , 6 , 4 and 3",on,"DM , PSD , EDS , UCCA and AMR"
translation,155,21,baselines,Machine Translation ( MT ),to translate,input sentences
translation,155,21,baselines,input sentences,has,into English
translation,155,18,experiments,AMR parsers,for,"Italian , Spanish , German and Chinese"
translation,155,18,experiments,AMR parsers,using,annotation projection
translation,155,18,experiments,"Italian , Spanish , German and Chinese",using,annotation projection
translation,155,18,experiments,annotation projection,where,existing annotations
translation,155,18,experiments,existing annotations,are projected from,source language ( English )
translation,155,18,experiments,source language ( English ),through,parallel corpus
translation,155,19,results,parsers,able to recover,AMR structures
translation,155,19,results,AMR structures,when,structural differences
translation,155,19,results,structural differences,between,languages
translation,155,100,results,Google Translate system,has,outperforms
translation,155,100,results,outperforms,has,all other systems
translation,155,100,results,Results,has,Google Translate system
translation,155,101,results,MTbased systems,give,better parsing results
translation,155,101,results,MTbased systems,in general,better parsing results
translation,155,181,results,results,of,projection - based AMR parsers
translation,155,181,results,projection - based AMR parsers,vast room for,improvements
translation,155,181,results,Results,of,projection - based AMR parsers
translation,155,181,results,Results,has,results
translation,156,5,model,word vector representations,that exploit,"structural ( i.e. , dependency - based ) contexts"
translation,156,5,model,"structural ( i.e. , dependency - based ) contexts",considering,morpho-syntactic information
translation,156,5,model,morpho-syntactic information,associated with,each word and its contexts
translation,156,6,model,delexicalized word embeddings,trained on,any set of languages
translation,156,6,model,delexicalized word embeddings,capture,features
translation,156,6,model,delexicalized word embeddings,used in combination with,standard language -specific features
translation,156,6,model,features,shared across,languages
translation,156,6,model,features,used in combination with,standard language -specific features
translation,156,6,model,standard language -specific features,to train,lexicalized parser
translation,156,6,model,lexicalized parser,in,target language
translation,156,6,model,Model,has,delexicalized word embeddings
translation,156,16,model,vectorial representations,in which,lemma information
translation,156,16,model,lemma information,abstracted away from both,words and contexts
translation,156,16,model,Model,use of,vectorial representations
translation,156,174,results,French and Romanian,not show,real improvement
translation,156,174,results,six other languages,show,substantial performance increases
translation,156,174,results,substantial performance increases,with,embeddings
translation,156,174,results,French and Romanian,has,six other languages
translation,156,174,results,real improvement,has,six other languages
translation,156,174,results,Results,Except for,French and Romanian
translation,156,175,results,improvements,are,statistically significant
translation,156,175,results,statistically significant,for,all languages
translation,156,175,results,all languages,except for,Basque and Hebrew
translation,156,175,results,Results,has,improvements
translation,156,178,results,results,for,English
translation,156,178,results,English,significant and close to,each other
translation,156,178,results,each other,for,all types of embeddings
translation,156,178,results,Results,for,English
translation,156,178,results,Results,has,results
translation,156,180,results,Basque and Gothic,display,largest improvements
translation,156,180,results,largest improvements,with,structural morpho-syntactic embeddings
translation,156,185,results,improve,for,"four languages ( en , eu , hu , ro )"
translation,156,185,results,improve,compared to,best monolingual setting
translation,156,185,results,"four languages ( en , eu , hu , ro )",in,cross-lingual setting
translation,156,185,results,"four languages ( en , eu , hu , ro )",compared to,best monolingual setting
translation,156,185,results,Parsing accuracy,has,improve
translation,156,185,results,Results,has,Parsing accuracy
translation,156,186,results,multilingual embeddings,deliver,parsing performance
translation,156,186,results,monolingual ones,for,other four languages
translation,156,186,results,baseline MST parser,for,all languages ( but Gothic )
translation,156,186,results,multilingual embeddings,has,do not outperform
translation,156,186,results,do not outperform,has,monolingual ones
translation,156,188,results,largest gains,achieved with,structural ( or mixed ) embeddings
translation,156,188,results,Results,notice,largest gains
translation,156,198,results,best parsing performance,for,English
translation,156,198,results,best parsing performance,achieved when using,additional data from Gothic
translation,156,198,results,English,achieved when using,additional data from Gothic
translation,156,198,results,Results,has,best parsing performance
translation,157,231,ablation-analysis,system,dropped,0.66 points
translation,157,231,ablation-analysis,system,dropped,0.64 points
translation,157,231,ablation-analysis,0.66 points,for,first-order
translation,157,231,ablation-analysis,0.64 points,for,second-order
translation,157,231,ablation-analysis,second-order,compared with,system PAG
translation,157,231,ablation-analysis,original bilingual features ( PAG o ),has,system
translation,157,231,ablation-analysis,Ablation analysis,used,original bilingual features ( PAG o )
translation,157,261,baselines,higher performance,added,"source subtree features ( Chen et al. , 2009 )"
translation,157,261,baselines,"source subtree features ( Chen et al. , 2009 )",to,our system
translation,157,202,experimental-setup,SRILM,to train,5 - gram language model
translation,157,202,experimental-setup,Experimental setup,used,SRILM
translation,157,254,experiments,test data,into,English
translation,157,254,experiments,English,using,Moses system
translation,157,254,experiments,parsers,on,new test data
translation,157,25,model,bitext parsing approach,produce,bilingual constraints
translation,157,25,model,bilingual constraints,on,existing monolingual treebanks
translation,157,25,model,existing monolingual treebanks,with the help of,SMT systems
translation,157,25,model,Model,propose,bitext parsing approach
translation,157,41,results,significantly outperforms,on,standard test data
translation,157,41,results,state- ofthe - art baseline systems,on,standard test data
translation,157,41,results,standard test data,of,CTB
translation,157,41,results,CTB,containing,"about 3,000 sentences"
translation,157,41,results,parser,has,significantly outperforms
translation,157,41,results,significantly outperforms,has,state- ofthe - art baseline systems
translation,157,41,results,Results,has,parser
translation,157,212,results,unlabeled attachment score ( UAS ),of,second-order Parser t
translation,157,212,results,second-order Parser t,was,91.92
translation,157,212,results,Results,has,unlabeled attachment score ( UAS )
translation,157,229,results,absolute improvement,of,1.02 points
translation,157,229,results,absolute improvement,of,1.29 points
translation,157,229,results,1.02 points,for,first-order model
translation,157,229,results,1.29 points,for,second-order model
translation,157,229,results,second-order model,by adding,verified bilingual features
translation,157,229,results,Results,obtained,absolute improvement
translation,157,230,results,improvements,of,final systems ( PAG )
translation,157,230,results,improvements,over,Baselines
translation,157,230,results,final systems ( PAG ),over,Baselines
translation,157,230,results,significant,in,McNemar 's Test
translation,157,230,results,Results,has,improvements
translation,157,249,results,our system,has,outperformed
translation,157,249,results,outperformed,has,Baseline
translation,157,256,results,outperformed,for both,first - and second-order models
translation,157,256,results,PAG,has,outperformed
translation,157,256,results,outperformed,has,baseline systems
translation,157,256,results,Results,showed,PAG
translation,157,260,results,our new parser,achieved,better accuracy
translation,157,260,results,better accuracy,than,Huang2009
translation,157,260,results,better accuracy,comparable to,Chen2010 BI
translation,157,260,results,Results,showed,our new parser
translation,157,263,results,all of the training data,of,CTB7
translation,157,263,results,all of the training data,obtained,more powerful baseline
translation,157,263,results,more powerful baseline,performed,much better
translation,157,263,results,much better,than,previous reported results
translation,157,263,results,Results,using,all of the training data
translation,157,264,results,Our parser,achieved,91.66
translation,157,264,results,Results,has,Our parser
translation,158,123,ablation-analysis,external lexicon,helps,tagger
translation,158,123,ablation-analysis,tagger,for,most of the languages
translation,158,123,ablation-analysis,tagger,for,languages
translation,158,123,ablation-analysis,tagger,specifically for,languages
translation,158,123,ablation-analysis,languages,such as,French
translation,158,123,ablation-analysis,Ablation analysis,Leveraging,external lexicon
translation,158,138,ablation-analysis,lexicon,helps,UPoS tagging performance
translation,158,138,ablation-analysis,all the treebanks ( except Turkish ),has,lexicon
translation,158,138,ablation-analysis,Ablation analysis,For,all the treebanks ( except Turkish )
translation,158,31,baselines,Backbone parser ELMoLex,uses,BiAF parser
translation,158,31,baselines,state - of - the - art graph - based parser,as,backbone
translation,158,31,baselines,BiAF parser,has,state - of - the - art graph - based parser
translation,158,31,baselines,Baselines,has,Backbone parser ELMoLex
translation,158,8,experiments,EL - MoLex 1,ranked,11 th
translation,158,8,experiments,EL - MoLex 1,ranked,9 th
translation,158,8,experiments,EL - MoLex 1,ranked,9 th
translation,158,8,experiments,11 th,by,Labeled Attachment Score metric ( 70.64 % )
translation,158,8,experiments,11 th,by,Morphology - aware LAS metric
translation,158,8,experiments,11 th,by,Bilexical dependency metric ( 60.70 % )
translation,158,8,experiments,11 th,by,Bilexical dependency metric ( 60.70 % )
translation,158,8,experiments,9 th,by,Bilexical dependency metric ( 60.70 % )
translation,158,8,experiments,Morphology - aware LAS metric,has,55.74 % )
translation,158,47,experiments,"deep , contextualized word representation",yields,promising result
translation,158,47,experiments,"deep , contextualized word representation",has,ELMo
translation,158,47,experiments,ELMo,has,)
translation,158,166,experiments,ELMoLex,ranked,7 th
translation,158,166,experiments,ELMoLex,ranked,11 th
translation,158,166,experiments,7 th,for,"Event Extraction , Negation Resolution tasks"
translation,158,166,experiments,11 th,for,Opinion Analysis task
translation,158,166,experiments,Opinion Analysis task,by,F1 score
translation,158,5,model,deep Biaffine ( BiAF ) parser,with,novel features
translation,158,5,model,novel features,to perform,competitively
translation,158,5,model,indomain version,of,ELMo features
translation,158,5,model,indomain version,which provide,contextdependent word representations
translation,158,5,model,"disambiguated , embedded , morphosyntactic features",from,"lexicons ( Sagot , 2018 )"
translation,158,5,model,Model,augment,deep Biaffine ( BiAF ) parser
translation,158,18,model,Embeddings from Language Model ( ELMo ) features,which are,context dependent
translation,158,18,model,Embeddings from Language Model ( ELMo ) features,obtained from,linear combination
translation,158,18,model,linear combination,of,several layers
translation,158,18,model,several layers,of,pre-trained BiLSTM -LM
translation,158,18,model,Model,propose to use,Embeddings from Language Model ( ELMo ) features
translation,158,86,model,Character - level embedding layer,of,ELMoLex
translation,158,86,model,representation,based on,known characters
translation,158,86,model,known characters,extracted from,OOV word naturally
translation,158,86,model,v GP ( char ),has,Character - level embedding layer
translation,158,86,model,Model,has,v GP ( char )
translation,158,86,model,Model,has,Character - level embedding layer
translation,158,141,model,architecture,for which,lexical information
translation,158,141,model,lexical information,feeded using,nhot encoded vector
translation,158,141,model,nhot encoded vector,concatenated with,other word representation vectors
translation,158,9,results,ELMoLex,ranked,7 th
translation,158,9,results,ELMoLex,ranked,11 th
translation,158,9,results,7 th,for,"Event Extraction , Negation Resolution tasks"
translation,158,9,results,11 th,for,Opinion Analysis task
translation,158,9,results,Opinion Analysis task,by,F1 score
translation,158,9,results,extrinsic evaluation setup,has,ELMoLex
translation,158,23,results,ELMoLex,ranked,11 th
translation,158,23,results,ELMoLex,ranked,Morphology - aware LAS ( MLAS ) metric
translation,158,23,results,ELMoLex,ranked,9 th
translation,158,23,results,ELMoLex,ranked,9 th
translation,158,23,results,11 th,by,Labeled Attachment Score ( LAS ) metric
translation,158,23,results,11 th,by,BiLEXical dependency ( BLEX ) metric
translation,158,23,results,9 th,by,BiLEXical dependency ( BLEX ) metric
translation,158,23,results,Labeled Attachment Score ( LAS ) metric,has,70.64 %
translation,158,23,results,Morphology - aware LAS ( MLAS ) metric,has,55.74 %
translation,158,23,results,BiLEXical dependency ( BLEX ) metric,has,60.70 %
translation,158,23,results,Results,has,ELMoLex
translation,158,122,results,recurrent cell,better suited at,encoding word morphology
translation,158,122,results,recurrent cell,for,languages
translation,158,122,results,1 - D convolution layer,embedding,lexical information
translation,158,122,results,lexical information,into,continuous space
translation,158,122,results,continuous space,useful for,improving
translation,158,122,results,external lexicon,useful for,better UPoS tagging
translation,158,122,results,works better,than,recurrent cell
translation,158,122,results,recurrent cell,for,languages
translation,158,122,results,languages,such as,Vietnamese and Chinese
translation,158,122,results,layer,has,works better
translation,158,122,results,Results,Is,recurrent cell
translation,158,122,results,Results,has,recurrent cell
translation,158,129,results,continuous embedding layer,for,lexicon features
translation,158,129,results,continuous embedding layer,leads to,better performance
translation,158,129,results,better performance,compare to,straight n-hot encoding
translation,158,129,results,Results,Using,continuous embedding layer
translation,158,132,results,convolution layer,as,morphological embedding technique
translation,158,132,results,convolution layer,provides,poorer results
translation,158,132,results,poorer results,compared to,recurrent cell
translation,158,132,results,recurrent cell,except for,two cases
translation,158,132,results,Results,using,convolution layer
translation,158,143,results,embedding layers,provides,better results
translation,158,143,results,better results,than,simple n-hot encoded representation
translation,158,143,results,languages,has,embedding layers
translation,158,143,results,Results,in,languages
translation,158,144,results,most of the treebanks,performed,significantly above
translation,158,144,results,UDPipe baseline,for,UPoS tagging
translation,158,144,results,significantly above,has,UDPipe baseline
translation,158,144,results,Results,For,most of the treebanks
translation,158,153,results,ELMo or Lexicon,entails,high train time
translation,158,153,results,BiAF model,by,large margin
translation,158,153,results,External lexicons,brings in,valuable information
translation,158,153,results,valuable information,about,OOV words
translation,158,153,results,valuable information,words with,irregular morphology
translation,158,153,results,outperforming,by,large margin
translation,158,153,results,BiAF,by,large margin
translation,158,153,results,ELMo,entails,high train time
translation,158,153,results,high train time,due to,additional LSTM operation
translation,158,153,results,outperform,has,BiAF model
translation,158,153,results,outperforming,has,BiAF
translation,158,153,results,Results,Utilizing,ELMo or Lexicon
translation,158,153,results,Results,Utilizing,External lexicons
translation,158,153,results,Results,Utilizing,ELMo
translation,158,158,results,superior performance,in,final LAS score
translation,158,158,results,superior performance,compared to,our last year submission
translation,158,158,results,Results,reached,superior performance
translation,159,187,hyperparameters,beam size B,of,beam search
translation,159,187,hyperparameters,beam search,chosen to be,B = 100
translation,159,187,hyperparameters,Hyperparameters,has,beam size B
translation,159,188,hyperparameters,K-nearest neighbor parameter,chosen as,K = 40
translation,159,188,hyperparameters,Hyperparameters,has,K-nearest neighbor parameter
translation,159,5,model,new online learning algorithm,searches,faster
translation,159,5,model,faster,as,training
translation,159,5,model,training,has,progresses
translation,159,5,model,Model,propose,new online learning algorithm
translation,159,7,results,WIKITABLEQUESTIONS dataset,expand,search space
translation,159,7,results,search space,of,existing model
translation,159,7,results,existing model,to improve,state - of- theart accuracy
translation,159,7,results,state - of- theart accuracy,from,38.7 % to 42.7 %
translation,159,7,results,state - of- theart accuracy,use,macro grammars
translation,159,7,results,state - of- theart accuracy,use,holistic triggering
translation,159,7,results,holistic triggering,to achieve,11x speedup
translation,159,7,results,holistic triggering,to achieve,accuracy
translation,159,7,results,accuracy,of,43.7 %
translation,159,7,results,Results,On,WIKITABLEQUESTIONS dataset
translation,159,33,results,11x speedup,training with,only the base grammar
translation,159,33,results,Results,training with,macro grammars
translation,159,177,results,Our algorithm,achieves,order- of-magnitude speedup
translation,159,177,results,order- of-magnitude speedup,over,parser
translation,159,177,results,parser,trained with,base grammar
translation,159,177,results,parser,trained with,parser in PL15
translation,159,177,results,outperforms,has,others
translation,159,177,results,Results,has,Our algorithm
translation,159,211,results,macro grammar,slightly improves,accuracy
translation,159,211,results,accuracy,to,43.7 %
translation,159,211,results,43.7 %,on,test set
translation,159,211,results,Results,Learning,macro grammar
translation,159,212,results,averaged accuracy,achieved by,base grammar and the macro grammar
translation,159,212,results,base grammar and the macro grammar,are,close ( 40.6 % vs 40.4 % )
translation,159,212,results,three train- dev splits,has,averaged accuracy
translation,159,212,results,Results,On,three train- dev splits
translation,159,220,results,both variants,result in,decreased efficiency
translation,159,220,results,Results,shows,both variants
translation,160,71,hyperparameters,word embeddings,use,"pre-trained GloVe embeddings ( Pennington et al. , 2014 )"
translation,160,71,hyperparameters,word embeddings,use,"pre-trained BERT ( Devlin et al. , 2019 )"
translation,160,71,hyperparameters,Hyperparameters,For,word embeddings
translation,160,122,hyperparameters,optimizer,is,"Adam ( Kingma and Ba , 2015 )"
translation,160,122,hyperparameters,Hyperparameters,has,optimizer
translation,160,75,model,handmade features,to,classifier
translation,160,78,model,vector embeddings,to represent,node labels and transition actions
translation,160,78,model,node labels and transition actions,embedded to,vectors
translation,160,131,results,our model,better than,TUPA baseline
translation,160,131,results,some of the frameworks,has,our model
translation,160,131,results,Results,For,some of the frameworks
translation,160,144,results,MRP results and SMATCH results,are,better
translation,160,144,results,AMR,has,MRP results and SMATCH results
translation,160,144,results,Results,for,AMR
translation,160,145,results,"precision , recall , and F 1 results",for,MRP metric
translation,160,145,results,much higher,than,TUPA
translation,160,145,results,Results,compare,"precision , recall , and F 1 results"
translation,160,148,results,Fewer actions,make,prediction
translation,160,148,results,prediction,has,more accurate
translation,160,148,results,Results,has,Fewer actions
translation,161,118,ablation-analysis,our data selection method,effective for improving,model performance
translation,161,118,ablation-analysis,model performance,to,some extent
translation,161,118,ablation-analysis,Ablation analysis,proves,our data selection method
translation,161,5,experimental-setup,"tokenizer , tagger and parser",for,each treebank
translation,161,5,experimental-setup,each treebank,based on,open source pipeline tool UDPipe
translation,161,5,experimental-setup,Experimental setup,train,"tokenizer , tagger and parser"
translation,161,40,experimental-setup,pretrained embeddings,for,word forms
translation,161,40,experimental-setup,pretrained embeddings,by,"word2vec ( Mikolov et al. , 2013 )"
translation,161,40,experimental-setup,word forms,with,provided training data
translation,161,40,experimental-setup,provided training data,by,"word2vec ( Mikolov et al. , 2013 )"
translation,161,40,experimental-setup,Experimental setup,adopt,pretrained embeddings
translation,161,41,experimental-setup,parameter settings,of,word2vec
translation,161,41,experimental-setup,Experimental setup,has,parameter settings
translation,161,42,experimental-setup,skip-gram model,to train,word vectors
translation,161,42,experimental-setup,word vectors,with,dimension
translation,161,42,experimental-setup,dimension,of,50
translation,161,42,experimental-setup,Experimental setup,use,skip-gram model
translation,161,43,experimental-setup,context window,set to,10 words
translation,161,43,experimental-setup,dropped,if,frequency
translation,161,43,experimental-setup,frequency,is,twice
translation,161,43,experimental-setup,frequency,less than,twice
translation,161,43,experimental-setup,Experimental setup,has,context window
translation,161,59,experimental-setup,GRU dimension,tuned on,development set
translation,161,59,experimental-setup,learning rate,tuned on,development set
translation,161,59,experimental-setup,Experimental setup,has,GRU dimension
translation,161,59,experimental-setup,Experimental setup,has,dropout probability
translation,161,60,experimental-setup,tokenizers,trained for,100 epochs
translation,161,60,experimental-setup,Experimental setup,has,tokenizers
translation,161,83,experimental-setup,parser,employs,FORM embeddings
translation,161,83,experimental-setup,parser,employs,UPOS
translation,161,83,experimental-setup,parser,employs,DEPREL embeddings
translation,161,83,experimental-setup,DEPREL embeddings,of,dimension 20
translation,161,83,experimental-setup,FORM embeddings,has,of dimension 50
translation,161,83,experimental-setup,Experimental setup,has,parser
translation,161,84,experimental-setup,FORM embeddings,pretrained with,word2vec
translation,161,84,experimental-setup,word2vec,using,training data
translation,161,84,experimental-setup,Experimental setup,has,FORM embeddings
translation,161,84,experimental-setup,Experimental setup,has,other embeddings
translation,161,85,experimental-setup,embeddings,updated for,each iteration
translation,161,85,experimental-setup,each iteration,during,training
translation,161,85,experimental-setup,Experimental setup,has,embeddings
translation,161,86,experimental-setup,hidden layer size,set to,200
translation,161,86,experimental-setup,batch size,limited to,30
translation,161,86,experimental-setup,Experimental setup,has,hidden layer size
translation,161,86,experimental-setup,Experimental setup,has,batch size
translation,161,87,experimental-setup,parsing models,trained for,10 iterations
translation,161,87,experimental-setup,Experimental setup,has,parsing models
translation,161,10,experiments,multilingual,has,parsers
translation,161,58,model,segmenter and tokenizer network,employs,character embeddings
translation,161,58,model,segmenter and tokenizer network,trained using,dropout
translation,161,58,model,dropout,both before and after,recurrent units
translation,161,58,model,Model,has,segmenter and tokenizer network
translation,161,102,results,Our system,ranks,19
translation,161,102,results,Our system,ranks,17
translation,161,102,results,Our system,ranks,13
translation,161,102,results,19,in,LAS
translation,161,102,results,17,in,MLAS
translation,161,102,results,13,in,BLEX
translation,161,102,results,BLEX,on,main metric ranking board
translation,161,102,results,Results,has,Our system
translation,161,103,results,main evaluation scores,of,our system
translation,161,103,results,our system,on,all treebanks
translation,161,103,results,our system,on,big treebanks
translation,161,103,results,our system,on,PUD treebanks
translation,161,103,results,our system,on,small treebanks
translation,161,103,results,our system,on,low-resource languages
translation,161,103,results,our system,gives,similar performance
translation,161,103,results,similar performance,to,BASELINE UDPipe 1.2 system
translation,161,103,results,Results,has,main evaluation scores
translation,161,106,results,Our system,shows,consistent improvement
translation,161,106,results,consistent improvement,over,baseline model
translation,161,106,results,baseline model,for,all the three metrics
translation,161,106,results,all the three metrics,on,PUD treebanks
translation,161,106,results,Results,has,Our system
translation,161,113,results,model,trained by,mixed English and Swedish treebanks
translation,161,113,results,mixed English and Swedish treebanks,all data of,same languages
translation,161,113,results,mixed English and Swedish treebanks,shows,better performance
translation,161,113,results,better performance,than,single largest treebank
translation,161,113,results,better performance,of,single largest treebank
translation,161,113,results,Results,has,model
translation,161,114,results,models,trained by,largest Czech and Finnish treebanks
translation,161,114,results,largest Czech and Finnish treebanks,get,higher score
translation,161,114,results,Results,has,models
translation,161,117,results,good performance,on,pcm nsc and th pud treebanks
translation,161,117,results,pcm nsc and th pud treebanks,when,most teams
translation,161,117,results,most teams,get,unsatisfying result
translation,161,117,results,our system,has,good performance
translation,161,117,results,Results,Note,our system
translation,161,120,results,our system,ranks,first
translation,161,120,results,first,in,Sentence Segmentation F1 score
translation,161,120,results,Sentence Segmentation F1 score,of,PUD treebanks
translation,161,120,results,Results,has,our system
translation,162,7,model,text,to,projective equations
translation,162,7,model,parse,has,text
translation,162,44,model,pipeline,of,structured predictors
translation,162,44,model,structured predictors,that identify,irrelevant quantities
translation,162,44,model,structured predictors,recognize,coreferent variables
translation,162,44,model,structured predictors,generate,equations
translation,162,44,model,irrelevant quantities,recognize,coreferent variables
translation,163,77,baselines,COPYGEN,is,copy - generate model
translation,163,77,baselines,copy - generate model,modified for,relation prediction task
translation,163,77,baselines,Baselines,has,COPYGEN
translation,163,82,baselines,GLOBALMODEL,without,instance learning component
translation,163,82,baselines,parametric model approach,without,instance learning component
translation,163,82,baselines,"P g ( r| m s , m d )",=,Softmax ( FFN g ( e q ) )
translation,163,82,baselines,GLOBALMODEL,has,parametric model approach
translation,163,82,baselines,Baselines,has,GLOBALMODEL
translation,163,84,baselines,LOCALMODEL,without,global model
translation,163,84,baselines,Instance based local approach,without,global model
translation,163,84,baselines,LOCALMODEL,has,Instance based local approach
translation,163,84,baselines,Baselines,has,LOCALMODEL
translation,163,98,baselines,alternative datadependent method,for combining,parametric and instance based approach ( III vs VI )
translation,163,98,baselines,Baselines,compare to,alternative datadependent method
translation,163,7,experiments,Candidate similar sentences,retrieved using,SciBERT embeddings
translation,163,6,model,instance - based approach,to,relation prediction sub-task
translation,163,6,model,relation prediction sub-task,within,shallow semantic parsing
translation,163,6,model,semantic labels,from,structurally similar sentences
translation,163,6,model,structurally similar sentences,in,training set
translation,163,6,model,Model,explores,instance - based approach
translation,163,19,model,instance - based edge-factored approach,for,relation prediction sub-problem
translation,163,19,model,relation prediction sub-problem,of,shallow semantic parsing
translation,163,19,model,Model,propose,instance - based edge-factored approach
translation,163,76,model,edge factored parametric approach,using,"lexical , dependency and entity - type features"
translation,163,76,model,Model,is,edge factored parametric approach
translation,163,26,results,proposed local and global approach,to outperform,baseline methods
translation,163,26,results,baseline methods,based on,parametric approaches
translation,163,26,results,0.75 F1 absolute,in,WLP
translation,163,26,results,1 F1 absolute,in,MSPT
translation,163,26,results,1 F1 absolute,in,prior work
translation,163,26,results,prior work,by,2.69 F1 absolute
translation,163,26,results,2.69 F1 absolute,on,WLP corpus
translation,163,26,results,2.69 F1 absolute,has,12.7 % error reduction
translation,163,88,results,outperforms,on,WLP
translation,163,88,results,prior work,on,WLP
translation,163,88,results,proposed approach,has,outperforms
translation,163,88,results,outperforms,has,prior work
translation,163,88,results,prior work,has,vs VI )
translation,163,88,results,WLP,has,vs VI )
translation,163,89,results,parametric and the instance based approach,trade off,precision and recall
translation,163,89,results,Results,note,parametric and the instance based approach
translation,163,99,results,Our approach,with,stronger inductive bias
translation,163,99,results,stronger inductive bias,to,copy relations
translation,163,99,results,copy relations,has,outperforms
translation,163,99,results,Results,has,Our approach
translation,163,100,results,similarly,to,GLOB - ALMODEL
translation,163,100,results,GLOB - ALMODEL,has,)
translation,165,6,baselines,minimal features,present,Opn 3 q global training methods
translation,165,142,experimental-setup,2 - layer bi-directional LSTMs,with,256 hidden cell units
translation,165,142,experimental-setup,Experimental setup,use,2 - layer bi-directional LSTMs
translation,165,143,experimental-setup,Inputs,are,100 - dimensional word vectors
translation,165,143,experimental-setup,Inputs,are,pre-trained skipgram-model vectors
translation,165,143,experimental-setup,Inputs,concatenations of,28 - dimensional randomly - initialized partof-speech embeddings
translation,165,143,experimental-setup,Inputs,concatenations of,100 - dimensional word vectors
translation,165,143,experimental-setup,Inputs,concatenations of,pre-trained skipgram-model vectors
translation,165,143,experimental-setup,100 - dimensional word vectors,initialized from,GloVe vectors
translation,165,143,experimental-setup,Experimental setup,has,Inputs
translation,165,144,experimental-setup,concatenation,of,bi-LSTM feature vectors
translation,165,144,experimental-setup,bi-LSTM feature vectors,passed through,multi-layer perceptron ( MLP )
translation,165,144,experimental-setup,multi-layer perceptron ( MLP ),with,1 hidden layer
translation,165,144,experimental-setup,Experimental setup,has,concatenation
translation,165,145,experimental-setup,dropout rate,for,"bi-LSTM ( Gal and Ghahramani , 2016 )"
translation,165,145,experimental-setup,Experimental setup,set,dropout rate
translation,165,145,experimental-setup,Experimental setup,set,"MLP ( Srivastava et al. , 2014 )"
translation,165,147,experimental-setup,parameters,except,word embed - dings
translation,165,147,experimental-setup,Experimental setup,has,parameters
translation,165,148,experimental-setup,"Approximately 1,000 tokens",form,mini-batch
translation,165,148,experimental-setup,mini-batch,for,sub-gradient computation
translation,165,148,experimental-setup,Experimental setup,has,"Approximately 1,000 tokens"
translation,165,149,experimental-setup,each model,for,20 epochs
translation,165,149,experimental-setup,model selection,based on,development UAS
translation,165,149,experimental-setup,Experimental setup,train,each model
translation,165,150,experimental-setup,structured loss function,optimized via,"Adam ( Kingma and Ba , 2015 )"
translation,165,150,experimental-setup,Experimental setup,proposed,structured loss function
translation,165,151,experimental-setup,neural network computation,based on,python interface
translation,165,151,experimental-setup,python interface,to,DyNet
translation,165,151,experimental-setup,Experimental setup,has,neural network computation
translation,165,4,model,minimal feature set,for,transition - based dependency parsing
translation,165,4,model,Model,present,minimal feature set
translation,165,7,results,ensembles,including,our new parsers
translation,165,7,results,ensembles,achieve,best unlabeled attachment score reported ( to our knowledge
translation,165,7,results,ensembles,achieve,""" second- best-in- class "" result"
translation,165,7,results,best unlabeled attachment score reported ( to our knowledge,on,Chinese Treebank
translation,165,7,results,""" second- best-in- class "" result",on,English Penn Treebank
translation,165,7,results,Results,using,ensembles
translation,165,152,results,minimal feature sets,perform,larger ones
translation,165,152,results,minimal feature sets,as well as,larger ones
translation,165,152,results,larger ones,in,locally - trained models
translation,165,152,results,test data,has,minimal feature sets
translation,165,153,results,clear trend,of,global models
translation,165,153,results,outperforming,on,both datasets
translation,165,153,results,local models,for,two different transition systems
translation,165,153,results,local models,on,both datasets
translation,165,153,results,global models,has,outperforming
translation,165,153,results,outperforming,has,local models
translation,165,155,results,arceager,has,edge
translation,165,155,results,three types of global models,has,arceager
translation,165,155,results,Results,Of,three types of global models
translation,165,158,results,Our models,are,competitive
translation,165,158,results,ensemble of 15 globallytrained models,achieves,95.33 and 90.22
translation,165,158,results,ensemble of 15 globallytrained models,achieves,second highest
translation,165,158,results,95.33 and 90.22,on,PTB and CTB
translation,165,158,results,Results,has,Our models
translation,165,170,results,global training,enabled,our minimal feature set
translation,165,170,results,our minimal feature set,with,ensemble of parsers
translation,165,170,results,ensemble of parsers,achieves,90.22 UAS
translation,165,170,results,ensemble of parsers,achieves,95.33 UAS
translation,165,170,results,90.22 UAS,on,Chinese Treebank
translation,165,170,results,95.33 UAS,on,Penn Treebank
translation,166,80,ablation-analysis,CPOS tag,improves,LAS measure
translation,166,80,ablation-analysis,LAS measure,between,0.5 % and 0.72 %
translation,166,80,ablation-analysis,Ablation analysis,show,CPOS tag
translation,166,73,experimental-setup,our system,based on,MacBook Air
translation,166,73,experimental-setup,MacBook Air,with,Intel Core i5 1.6 GHz CPU
translation,166,73,experimental-setup,MacBook Air,with,4G memory
translation,166,73,experimental-setup,Experimental setup,trained,our system
translation,166,17,experiments,multilingual dependency parsing system Mengest,for,CoNLL 2017 UD Shared Task
translation,166,18,model,BiLSTM feature extractor,for,feature representation
translation,166,18,model,MLP classifier,for,transition system
translation,166,98,model,fast and lightweight multilingual dependency parsing,for,CoNLL 2017 UD Shared Task
translation,166,98,model,fast and lightweight multilingual dependency parsing,composed of,MLP classifier
translation,166,98,model,CoNLL 2017 UD Shared Task,composed of,MLP classifier
translation,166,98,model,Model,present,fast and lightweight multilingual dependency parsing
translation,166,81,results,BiLSTM Feature Representation Performances,of,simple feature representation and extended feature representation
translation,166,81,results,Results,has,BiLSTM Feature Representation Performances
translation,166,82,results,performance,of,our system
translation,166,82,results,extended feature representation,has,slightly increases
translation,166,82,results,slightly increases,has,performance
translation,166,82,results,Results,show,extended feature representation
translation,166,88,results,macro- average LAS,of,8 small treebanks
translation,166,88,results,8 small treebanks,is,33.88 %
translation,166,88,results,Results,has,macro- average LAS
translation,166,89,results,macroaverage LAS,of,14 PUD treebanks
translation,166,89,results,14 PUD treebanks,is,63.68 %
translation,166,89,results,Results,has,macroaverage LAS
translation,166,90,results,macro- average LAS,of,4 surprise language treebanks
translation,166,90,results,4 surprise language treebanks,is,11.31 %
translation,166,90,results,Results,has,macro- average LAS
translation,166,91,results,macroaveraged LAS F1 score,of,our system
translation,166,91,results,our system,on,all treebanks
translation,166,91,results,all treebanks,is,61.33 %
translation,166,91,results,Results,has,macroaveraged LAS F1 score
translation,167,80,experimental-setup,our system,based on,Nvidia GeForce GTX Titan X
translation,167,80,experimental-setup,Experimental setup,trained,our system
translation,167,82,experimental-setup,Dynet neural network library,to build,our system
translation,167,82,experimental-setup,Experimental setup,used,Dynet neural network library
translation,167,102,model,graph- based dependency parsing system,for,CoNLL 2018 UD Shared Task
translation,167,102,model,graph- based dependency parsing system,composed of,bi-affine pointer networks
translation,167,102,model,Model,present,graph- based dependency parsing system
translation,167,6,results,our system,gets,70.90 LAS F1 score ( rank 9/26 )
translation,167,6,results,our system,gets,55.92 MLAS ( 10/26 )
translation,167,6,results,our system,gets,60.91
translation,167,6,results,60.91,has,BLEX ( 8/26 )
translation,167,14,results,our system,gets,70.90 LAS F1 score ( rank 9/26 )
translation,167,14,results,our system,gets,55.92 MLAS ( 10/26 )
translation,167,14,results,our system,gets,60.91
translation,167,14,results,60.91,has,BLEX ( 8/26 )
translation,167,16,results,our system,obtains,sixth place
translation,167,16,results,sixth place,with,MLAS score
translation,167,16,results,MLAS score,of,63.73
translation,167,16,results,categories of small treebanks,has,our system
translation,167,16,results,Results,in,categories of small treebanks
translation,167,87,results,baseline,obtained with,UDPipe1.2
translation,167,87,results,our system,gained,5.10 LAS improvement
translation,167,87,results,baseline,has,our system
translation,167,87,results,UDPipe1.2,has,our system
translation,167,88,results,Our system,shows,better results
translation,167,88,results,better results,on,7 small treebanks
translation,167,88,results,Results,has,Our system
translation,168,26,baselines,first method,adapts,existing procedure
translation,168,26,baselines,existing procedure,for,parsing sentences
translation,168,26,baselines,existing procedure,uses,composite labels
translation,168,26,baselines,parsing sentences,with,elided function words
translation,168,26,baselines,composite labels,deterministically turned into,dependency graphs
translation,168,26,baselines,Baselines,has,first method
translation,168,6,model,two methods,for,parsing to a Universal Dependencies graph representation
translation,168,6,model,elided material,with,additional nodes and edges
translation,168,27,model,second method,is,novel procedure
translation,168,27,model,second method,employs,unsupervised method
translation,168,27,model,novel procedure,relies on,parser
translation,168,27,model,parser,to identify,gap
translation,168,27,model,unsupervised method,to reconstruct,elided predicates
translation,168,27,model,unsupervised method,reattach,arguments
translation,168,27,model,arguments,to,reconstructed predicate
translation,168,27,model,Model,has,second method
translation,168,7,results,both methods,reconstruct,elided material
translation,168,7,results,elided material,from,dependency trees
translation,168,7,results,high accuracy,when,parser
translation,168,7,results,parser,correctly predicts,existence
translation,168,7,results,existence,of,gap
translation,168,28,results,methods,reconstruct,elided predicates
translation,168,28,results,elided predicates,with,very high accuracy
translation,168,28,results,very high accuracy,from,gold standard dependency trees
translation,168,28,results,Results,find,methods
translation,168,29,results,our methods,achieve,sentence - level accuracy
translation,168,29,results,sentence - level accuracy,of,32 % and 34 %
translation,168,29,results,output,has,our methods
translation,168,29,results,significantly outperforming,has,recently proposed constituent parser
translation,168,29,results,Results,applied to,output
translation,168,122,results,no significant difference,between,parser
translation,168,122,results,no significant difference,between,parser
translation,168,122,results,parser,trained on,UD representation ( ORPHAN )
translation,168,122,results,parser,trained on,composite representation ( COMPOSITE )
translation,168,122,results,parser,trained on,composite representation ( COMPOSITE )
translation,168,122,results,composite representation ( COMPOSITE ),tested on,EWT data sets
translation,168,123,results,OR - PHAN parser,performs,significantly better ( p < 0.01 )
translation,168,123,results,significantly better ( p < 0.01 ),in terms of,labeled attachment score
translation,168,123,results,GAPPING datasets,has,OR - PHAN parser
translation,168,125,results,labeled attachment score,of,remnants
translation,168,125,results,remnants,is,significantly higher
translation,168,125,results,significantly higher,for,ORPHAN parser
translation,168,125,results,ORPHAN parser,than for,COMPOSITE parser
translation,168,125,results,Results,has,labeled attachment score
translation,168,126,results,unlabeled attachment score,on,test set
translation,168,126,results,unlabeled attachment score,is,higher
translation,168,126,results,test set,is,higher
translation,168,126,results,higher,for,ORPHAN parser
translation,168,126,results,Results,has,unlabeled attachment score
translation,168,139,results,the canonical clause structure,from,gold dependency trees
translation,168,139,results,gold dependency trees,with,high accuracy
translation,168,141,results,two methods,work,equally well
translation,168,141,results,equally well,in terms of,all metrics
translation,168,141,results,all metrics,except for,sentence - level accuracy
translation,168,141,results,significantly higher,for,COMPOSITE procedure
translation,168,141,results,Results,has,two methods
translation,168,149,results,tends to be a bit higher,for,ORPHAN procedure
translation,168,149,results,recall,in terms of,sentence - level accuracy
translation,168,149,results,tends to be a bit higher,for,COMPOSITE method
translation,168,149,results,methods,seem to perform,equally well
translation,168,149,results,precision,has,tends to be a bit higher
translation,168,149,results,recall,has,tends to be a bit higher
translation,168,149,results,Results,has,precision
translation,168,178,results,correct dependency label,for,108/110 relations
translation,168,178,results,correct dependency label,leading to,labeled precision
translation,168,178,results,correct dependency label,leading to,labeled recall
translation,168,178,results,labeled recall,of,98.18 %
translation,168,178,results,Results,predicts,correct dependency label
translation,169,29,results,evaluation scores,are,significantly higher
translation,169,29,results,significantly higher,than,previous cross-lingual studies
translation,169,29,results,Results,has,evaluation scores
translation,169,93,results,best source language,from,same language group
translation,169,93,results,Germanic and Romance target languages,has,best source language
translation,169,93,results,Results,for,Germanic and Romance target languages
translation,169,95,results,Swedish,is,best source language
translation,169,95,results,best source language,for both,German and English
translation,169,96,results,Romance languages,has,crosslingual parser
translation,169,96,results,Results,For,Romance languages
translation,170,140,ablation-analysis,65.24 % LAS F 1 score,for,submitted model
translation,170,140,ablation-analysis,Ablation analysis,tain,65.24 % LAS F 1 score
translation,170,4,baselines,Baselines,describes,UALing
translation,170,7,experimental-setup,training and parsing,done with,baseline UDPipe system
translation,170,7,experimental-setup,Experimental setup,has,training and parsing
translation,170,51,experiments,UDPipe 1.1,as,baseline system
translation,170,51,experiments,UDPipe 1.1,as,UD version 2.0 datasets
translation,170,8,results,our approach,reduces,size of training data
translation,170,8,results,our approach,retains,performance
translation,170,8,results,performance,within,0.5 %
translation,170,8,results,0.5 %,of,baseline system
translation,170,8,results,size of training data,has,significantly
translation,170,9,results,our system,performs,faster
translation,170,9,results,faster,than,"na?ve , complete corpus method"
translation,170,9,results,training data size,has,our system
translation,170,9,results,Results,reduction in,training data size
translation,170,52,results,size of training data,down to,76.25 %
translation,170,52,results,76.25 %,of,original
translation,170,52,results,76.25 %,using,proposed method
translation,170,73,results,similarity measures,to select,subset
translation,170,73,results,proposed method,slightly outperforms,results
translation,170,73,results,results,obtained by,original training data set
translation,170,73,results,similarity measures,has,proposed method
translation,170,73,results,subset,has,of the original training data
translation,170,73,results,subset,has,proposed method
translation,170,73,results,of the original training data,has,proposed method
translation,170,73,results,Results,Using,similarity measures
translation,170,74,results,parsing result,by,0.01 % and 0.15 %
translation,170,74,results,0.01 % and 0.15 %,using,94 % and 77 %
translation,170,74,results,94 % and 77 %,of,original training datasets
translation,170,74,results,94 % and 77 %,for,German and Dutch
translation,170,74,results,original training datasets,for,German and Dutch
translation,170,74,results,Results,improves,parsing result
translation,170,101,results,corpus compression levels,compare to,similarity - based approaches
translation,170,101,results,parsing results,has,drop significantly
translation,170,101,results,Results,has,corpus compression levels
translation,171,119,ablation-analysis,prepSRL features,have,slightly negative influence
translation,171,119,ablation-analysis,slightly negative influence,on,parsing accuracy
translation,171,119,ablation-analysis,parsing accuracy,of,CAMR
translation,171,119,ablation-analysis,Ablation analysis,reveal,prepSRL features
translation,171,6,experiments,Prepositional semantics,included as,features
translation,171,6,experiments,features,into,transition - based AMR parsing system CAMR
translation,171,120,results,Smatch F-score,remains,same
translation,171,120,results,same,over,all trained models
translation,171,120,results,recall,reduced by,1 %
translation,171,120,results,1 %,when adding,prepSRL features
translation,171,120,results,Results,has,Smatch F-score
translation,171,121,results,model,with,prepSRL
translation,171,121,results,prepSRL,achieves,Smatch score
translation,171,121,results,Smatch score,of,0.60
translation,171,121,results,0.60,on,SemEval - 2016
translation,171,121,results,Results,has,model
translation,172,76,ablation-analysis,CAMR parser wrapper,is,largest contributor
translation,172,76,ablation-analysis,largest contributor,to,our results
translation,172,76,ablation-analysis,Ablation analysis,has,CAMR parser wrapper
translation,172,68,experimental-setup,TensorFlow seq2seq model hyperparameters,within,constraints
translation,172,68,experimental-setup,TensorFlow seq2seq model hyperparameters,trained for,30 epochs
translation,172,68,experimental-setup,constraints,of,available GPU memory
translation,172,68,experimental-setup,single bucket,of size,480
translation,172,68,experimental-setup,vocabulary size,has,120
translation,172,68,experimental-setup,vocabulary size,has,number of distinct characters )
translation,172,68,experimental-setup,120,has,number of distinct characters )
translation,172,68,experimental-setup,batch size,has,4
translation,172,68,experimental-setup,Experimental setup,optimized,TensorFlow seq2seq model hyperparameters
translation,172,7,model,per-sentence smatch,with,ensemble method
translation,172,7,model,ensemble method,for selecting,best AMR graph
translation,172,7,model,best AMR graph,among,set of AMR graphs
translation,172,7,model,set of AMR graphs,for,same sentence
translation,172,9,results,character - level neural translation,attains,surprising 7 % gain
translation,172,9,results,surprising 7 % gain,over,carefully optimized word - level neural translation
translation,172,9,results,AMR parsing,has,character - level neural translation
translation,172,9,results,Results,For,AMR parsing
translation,172,51,results,overall gain,from,our wrapper
translation,172,51,results,overall gain,about,4 %
translation,172,51,results,our wrapper,is,4 %
translation,172,51,results,our wrapper,about,4 %
translation,172,51,results,Results,has,overall gain
translation,172,77,results,3 runs,of,CAMR + wrapper
translation,172,77,results,additional boost,to,results
translation,172,78,results,neural AMR parser,in,ensemble
translation,172,78,results,neural AMR parser,doubled,gain
translation,172,78,results,ensemble,doubled,gain
translation,172,78,results,Results,Including,neural AMR parser
translation,173,137,ablation-analysis,POS tags embeddings,attribute to,better performance
translation,173,137,ablation-analysis,Ablation analysis,sharing,POS tags embeddings
translation,173,119,baselines,three versions,based on,standard EM
translation,173,119,baselines,three versions,based on,Viterbi EM
translation,173,119,baselines,approach,based on,standard EM
translation,173,119,baselines,approach,based on,"softmax EM ( Tu and Honavar , 2012 )"
translation,173,119,baselines,approach,based on,Viterbi EM
translation,173,119,baselines,Baselines,tested,three versions
translation,173,101,hyperparameters,Hyperparameters,used,informed initialization method
translation,173,102,hyperparameters,length,of,embeddings
translation,173,102,hyperparameters,embeddings,set to,10
translation,173,102,hyperparameters,10,for,POS tags and valence
translation,173,102,hyperparameters,Hyperparameters,has,length
translation,173,103,hyperparameters,neural networks,with,batch size 10
translation,173,103,hyperparameters,neural networks,used,change of the validation set loss function
translation,173,103,hyperparameters,change of the validation set loss function,as,stop criteria
translation,173,103,hyperparameters,Hyperparameters,trained,neural networks
translation,173,8,model,novel approach,to,unsupervised dependency parsing
translation,173,8,model,novel approach,uses,neural model
translation,173,8,model,neural model,to predict,grammar rule probabilities
translation,173,8,model,grammar rule probabilities,based on,distributed representation of POS tags
translation,173,8,model,Model,propose,novel approach
translation,173,9,model,distributed representation,automatically learned from,data
translation,173,9,model,distributed representation,captures,correlations
translation,173,9,model,correlations,between,POS tags
translation,173,9,model,Model,has,distributed representation
translation,173,20,model,neural model,into,DMV model
translation,173,20,model,DMV model,to predict,grammar rule probabilities
translation,173,20,model,grammar rule probabilities,based on,distributed representation
translation,173,20,model,distributed representation,of,POS tags
translation,173,20,model,Model,incorporate,neural model
translation,173,21,model,neural network parameters,as well as,distributed representations
translation,173,21,model,distributed representations,from,data
translation,173,21,model,Model,learn,neural network parameters
translation,173,22,model,correlations,between,POS tags
translation,173,22,model,correlations,contribute to,improvement
translation,173,22,model,improvement,of,parsing accuracy
translation,173,22,model,Model,has,correlations
translation,173,25,results,our approach,able to achieve,better performance
translation,173,25,results,better performance,than,baseline methods
translation,173,25,results,better performance,without,any parameter tuning
translation,173,25,results,eight additional languages,has,our approach
translation,173,25,results,Results,datasets of,eight additional languages
translation,173,108,results,our approach with Viterbi EM,has,significantly outperforms
translation,173,108,results,significantly outperforms,has,EM and viterbi EM baselines
translation,173,108,results,outperforms,has,two previous approaches
translation,173,108,results,Results,be seen that,our approach with Viterbi EM
translation,173,109,results,Results,on,extended DMV model
translation,173,111,results,comparable accuracy,with,recent state - of - the - art systems
translation,173,111,results,Results,achieve,comparable accuracy
translation,173,122,results,Our neural based methods,achieve,better results
translation,173,122,results,better results,than,corresponding baselines
translation,173,122,results,corresponding baselines,in,75.0 %
translation,173,122,results,75.0 %,of,cases
translation,173,122,results,test sentences,no longer than,10 and 77.5 %
translation,173,122,results,10 and 77.5 %,for,all test sentences
translation,173,122,results,Results,has,Our neural based methods
translation,173,129,results,Nonlinear activation functions,seen to,significantly outperform
translation,173,129,results,significantly outperform,has,linear activation functions
translation,174,165,ablation-analysis,head and dependent regressors,have,significant effects
translation,174,165,ablation-analysis,significant effects,on,number of regressions ( eye regressions )
translation,174,165,ablation-analysis,Ablation analysis,has,head and dependent regressors
translation,174,166,ablation-analysis,word frequency ( freq ) and surprisal ( surp ),have,significant negative effect
translation,174,166,ablation-analysis,Ablation analysis,both,word frequency ( freq ) and surprisal ( surp )
translation,174,15,experiments,dependency structures,as,valid approximation
translation,174,15,experiments,dependency structures,reflected in,eye movement regressions
translation,174,15,experiments,valid approximation,of,syntactic properties
translation,174,15,experiments,syntactic properties,of,sentences
translation,174,15,experiments,eye movement regressions,during,naturalistic text reading
translation,174,16,model,regressions,from,each word
translation,174,16,model,regressions,from,words
translation,174,16,model,regressions,relate,dependency relations
translation,174,16,model,each word,in,text
translation,174,16,model,dependency relations,that link,pairs
translation,174,16,model,pairs,of,words
translation,174,16,model,words,in,sentence
translation,174,16,model,Model,consider,regressions
translation,174,17,model,syntactic properties,of,sentences
translation,174,17,model,syntactic properties,as,shallow structural information
translation,174,17,model,shallow structural information,at,word level
translation,174,17,model,Model,represent,syntactic properties
translation,174,172,results,difference,in,model fit
translation,175,185,ablation-analysis,block,notice that,our DEP
translation,175,185,ablation-analysis,our DEP,boosts,performance
translation,175,185,ablation-analysis,performance,in,many languages
translation,175,185,ablation-analysis,settings,with,constraint
translation,175,185,ablation-analysis,constraint,on,root tags
translation,175,185,ablation-analysis,No root constraint,has,block
translation,175,185,ablation-analysis,Ablation analysis,see,No root constraint
translation,175,27,experiments,grammar induction,from,part- of-speech tagged text
translation,175,156,hyperparameters,regularization parameter,to,10
translation,175,156,hyperparameters,Hyperparameters,set,regularization parameter
translation,175,157,hyperparameters,100 iterations,of,EM
translation,175,157,hyperparameters,Hyperparameters,run,100 iterations
translation,175,5,model,grammar induction tasks,by restricting,search space
translation,175,5,model,search space,of,model
translation,175,5,model,model,to,trees
translation,175,5,model,trees,with,limited centerembedding
translation,175,6,model,tabulation of left-corner parsing,captures,degree of center-embedding
translation,175,6,model,degree of center-embedding,of,parse
translation,175,6,model,degree of center-embedding,via,stack depth
translation,175,15,model,universal syntactic bias,not yet been exploited in,grammar induction
translation,175,32,model,left-corner ( LC ) parsing,as,push-down automaton ( PDA )
translation,175,32,model,Model,describe,left-corner ( LC ) parsing
translation,175,182,results,depth 2,degrades,performance
translation,175,182,results,Results,see that,depth 2
translation,175,182,results,Results,allowing,depth 2
translation,175,209,results,combination ( DEP + LEN ),performs,best
translation,175,209,results,combination ( DEP + LEN ),performs,worst
translation,175,209,results,best,in,UAS
translation,175,209,results,worst,in,bracket F1
translation,175,209,results,Results,has,combination ( DEP + LEN )
translation,176,6,experiments,13 multilingual models,for,shared task
translation,176,6,experiments,69,has,monolingual language models
translation,176,115,experiments,extremely resource - poor languages ( surprise languages ),ranked,12 th
translation,176,115,experiments,12 th,with,36.93 LAS score
translation,176,4,model,multilingual dependency parser,developed for,CoNLL 2017 UD
translation,176,4,model,monolingual BIST - parser,as,multi-source multilingual trainable parser
translation,176,17,model,system,using,monolingual
translation,176,17,model,system,using,multilingual strategy
translation,176,7,results,Our multilingual approach,making use of,different resources
translation,176,7,results,Our multilingual approach,yield,better results
translation,176,7,results,different resources,yield,better results
translation,176,7,results,better results,than,monolingual approach
translation,176,7,results,monolingual approach,for,11 languages
translation,176,7,results,Results,has,Our multilingual approach
translation,176,8,results,Our system,ranked,5 th
translation,176,8,results,Our system,achieved,70.93 overall LAS score
translation,176,8,results,70.93 overall LAS score,over,81 test corpora
translation,176,8,results,Results,has,Our system
translation,176,71,results,F1 - measure,of,our system
translation,176,71,results,F1 - measure,of,our monolingual system
translation,176,71,results,our monolingual system,for,resource - poor languages
translation,176,71,results,our monolingual system,for,most of the resource - rich languages
translation,176,71,results,Results,has,F1 - measure
translation,176,113,results,multilingual models,on,small treebank dataset
translation,176,113,results,Results,when using,multilingual models
translation,176,114,results,4 th,with,54.78 LAS score
translation,176,114,results,Results,ranked,4 th
translation,176,117,results,monolingual models,of,surprise languages
translation,176,117,results,monolingual models,see,improvement
translation,176,117,results,improvement,between,2.5 and 9.31 percent
translation,176,117,results,Results,compare,monolingual models
translation,176,118,results,same kind of improvement,observed,ParTUT group
translation,176,118,results,same kind of improvement,for,ParTUT group
translation,176,118,results,Results,has,same kind of improvement
translation,176,119,results,multilingual approach,improves,performance
translation,176,119,results,performance,by,almost 3 points
translation,176,119,results,Results,has,multilingual approach
translation,177,73,ablation-analysis,RD ? RG,to,augmented candidate setting
translation,177,73,ablation-analysis,RD ? RG,decreases,performance
translation,177,73,ablation-analysis,performance,from,93.45 F1
translation,177,73,ablation-analysis,performance,on,development set
translation,177,73,ablation-analysis,93.45 F1,to,92.78 F1
translation,177,73,ablation-analysis,Ablation analysis,Going from,RD ? RG
translation,177,77,ablation-analysis,model combination,contributes to,success
translation,177,77,ablation-analysis,model combination,contributes to,larger extent
translation,177,77,ablation-analysis,model combination,to,larger extent
translation,177,77,ablation-analysis,success,of,both models
translation,177,77,ablation-analysis,larger extent,for,RG
translation,177,77,ablation-analysis,Ablation analysis,suggest,model combination
translation,177,127,ablation-analysis,+ S data setting,all,significant
translation,177,127,ablation-analysis,+ S data setting,are,significant
translation,177,127,ablation-analysis,Ablation analysis,In,+ S data setting
translation,177,59,hyperparameters,LSTM generative model ( LM ),use,pre-trained model
translation,177,59,hyperparameters,Hyperparameters,For,LSTM generative model ( LM )
translation,177,60,hyperparameters,RNNG discriminative ( RD ) and generative ( RG ) models,using,pretrained word embeddings
translation,177,60,hyperparameters,pretrained word embeddings,for,discriminative model
translation,177,60,hyperparameters,Hyperparameters,train,RNNG discriminative ( RD ) and generative ( RG ) models
translation,177,63,hyperparameters,actionsynchronous beam search,with,beam size K = 100
translation,177,63,hyperparameters,actionsynchronous beam search,with,word-synchronous beam
translation,177,63,hyperparameters,beam size K = 100,for,RD
translation,177,63,hyperparameters,word-synchronous beam,with,K w = 100 and K a = 1000
translation,177,63,hyperparameters,K w = 100 and K a = 1000,for,generative models RG and LM
translation,177,63,hyperparameters,Hyperparameters,use,actionsynchronous beam search
translation,177,63,hyperparameters,Hyperparameters,use,word-synchronous beam
translation,177,6,model,algorithm,for,direct search
translation,177,22,model,beam-based search procedure,with,augmented state space
translation,177,22,model,augmented state space,search directly in,generative models
translation,177,55,results,comparatively larger gains,in,performance
translation,177,55,results,performance,between,larger beam sizes
translation,177,55,results,RG,has,comparatively larger gains
translation,177,55,results,underperforming,has,LM
translation,177,55,results,Results,has,RG
translation,177,67,results,higher performance,for,LM model
translation,177,67,results,LM model,when using,candidate list
translation,177,67,results,candidate list,from,RD parser
translation,177,67,results,92.79 F1,on,development data
translation,177,67,results,RD parser,has,93.66 F1
translation,177,67,results,Results,found,higher performance
translation,177,75,results,"smaller , but still significant , effect",case of,LM
translation,177,75,results,93.47,for,RD ? LM ? LM
translation,177,75,results,LM,has,RD ? LM
translation,177,83,results,scores,of,both models
translation,177,83,results,both models,improves on,score of either model alone
translation,177,85,results,Score combination,more than compensates for,decrease
translation,177,85,results,decrease,in,performance
translation,177,85,results,candidates,from,generative model
translation,177,85,results,RD,improves upon,RD ? RG
translation,177,85,results,RD,improves upon,RD ? RG ? RG
translation,177,85,results,RD + RG,improves upon,RD ? RG
translation,177,85,results,RD + RG,improves upon,RD ? RG ? RG
translation,177,85,results,generative model,has,RD + RG
translation,177,85,results,RD,has,RD + RG
translation,177,85,results,Results,has,Score combination
translation,177,90,results,candidates and scores,obtain,93.94 F1
translation,177,90,results,Results,Combining,candidates and scores
translation,177,100,results,Performance,when using,only the ensembled RD models
translation,177,100,results,only the ensembled RD models,is,lower
translation,177,100,results,lower,than,rescoring
translation,177,100,results,single RD model,with,score combinations
translation,177,100,results,score combinations,of,single models
translation,177,100,results,rescoring,has,single RD model
translation,177,100,results,Results,has,Performance
translation,177,101,results,PTB setting,ensembling with,score combination
translation,177,101,results,best overall result,of,94.25
translation,177,101,results,Results,In,PTB setting
translation,177,102,results,not better,than,combination of single models
translation,177,102,results,combination of single models,candidates from,generative models
translation,177,102,results,Results,In,silver training data setting
translation,177,104,results,generative models,yields,results
translation,177,104,results,results,that are,partly surprising
translation,177,104,results,lower performance,than,candidates
translation,177,104,results,Results,Searching directly in,generative models
translation,177,105,results,explicitly combining scores,allows,reranking setup
translation,177,105,results,reranking setup,to achieve,better performance
translation,177,105,results,better performance,than,implicit combination
translation,177,106,results,hypothesis,that,generative models
translation,177,106,results,generative models,can achieve,good results
translation,177,106,results,good results,on their own,LSTM generative model
translation,177,106,results,good results,with,LSTM generative model
translation,177,106,results,LSTM generative model,showing,particularly strong and selfcontained performance
translation,177,106,results,Results,see support,hypothesis
translation,178,71,baselines,ensemble of five MTPs,trained on,11 million trees
translation,178,71,baselines,11 million trees,of,highconfidence corpus 4 ( HC )
translation,178,71,baselines,ensemble of six one - to - many sequence models,trained on,HC and 4.5 millions of English - German translation sentence pairs
translation,178,71,baselines,Baselines,compare,LSTM -LM ( GS )
translation,178,50,experimental-setup,github.com,has,/ cdg720/emnlp2016
translation,178,64,experimental-setup,activations,with,probability 0.45
translation,178,64,experimental-setup,probability 0.45,smaller than,0.7
translation,178,64,experimental-setup,Experimental setup,drop,activations
translation,178,38,hyperparameters,starting states,with,previous minibatch 's last hidden states
translation,178,38,hyperparameters,Hyperparameters,initialize,starting states
translation,178,39,hyperparameters,initialized,to be,"one ( Jozefowicz et al. , 2015 )"
translation,178,39,hyperparameters,rest of model parameters,sampled from,"U (?0.05 , 0.05 )"
translation,178,39,hyperparameters,Hyperparameters,has,forget gate bias
translation,178,40,hyperparameters,Dropout,applied to,non-recurrent connections
translation,178,40,hyperparameters,gradients,clipped when,norm
translation,178,40,hyperparameters,Hyperparameters,has,Dropout
translation,178,5,results,trees,converted to,Stanford dependencies
translation,178,5,results,UAS and LAS,are,95.9 % and 94.1 %
translation,178,5,results,trees,has,UAS and LAS
translation,178,5,results,Stanford dependencies,has,UAS and LAS
translation,178,5,results,Results,When,trees
translation,178,69,results,92.6 F 1,outperforms,"ensemble of five MTPs ( Vinyals et al. , 2015 )"
translation,178,69,results,92.6 F 1,has,LSTM -LM ( G )
translation,178,69,results,RNNG,has,"Dyer et al. , 2016 )"
translation,178,78,results,single LSTM -LM ( GS ),together with,Charniak ( GS )
translation,178,78,results,single LSTM -LM ( GS ),together with,Charniak ( GS )
translation,178,78,results,single LSTM -LM ( GS ),ensemble of eight LSTM - LMs ( GS ) with,Charniak ( GS )
translation,178,78,results,Charniak ( GS ),reaches,93.6
translation,178,78,results,Results,has,single LSTM -LM ( GS )
translation,178,79,results,trees,converted to,Stanford dependencies
translation,178,79,results,5 UAS and LAS,are,95.9 % and 94.1 %
translation,178,79,results,trees,has,5 UAS and LAS
translation,178,79,results,Stanford dependencies,has,5 UAS and LAS
translation,178,79,results,Results,When,trees
translation,179,73,baselines,tsVB,uses,variational Bayesian inference
translation,179,73,baselines,variational Bayesian inference,to learn,weights
translation,179,73,baselines,weights,for,tree transducer
translation,179,73,baselines,UBL,learns,CCG lexicon
translation,179,73,baselines,CCG lexicon,with,semantic annotations
translation,179,73,baselines,hybridtree,learns,synchronous generative model
translation,179,73,baselines,synchronous generative model,over,variable - free MRs and NL strings
translation,179,61,experimental-setup,IBM Model 4 implementation,from,++ toolkit
translation,179,61,experimental-setup,IBM Model 4 implementation,for,alignment
translation,179,61,experimental-setup,phrase - based and hierarchical models,implemented in,"Moses toolkit ( Koehn et al. , 2007 )"
translation,179,61,experimental-setup,phrase - based and hierarchical models,for,rule extraction
translation,179,61,experimental-setup,"Moses toolkit ( Koehn et al. , 2007 )",for,rule extraction
translation,179,61,experimental-setup,Experimental setup,use,IBM Model 4 implementation
translation,179,66,results,hierarchical model,performs,better
translation,179,66,results,better,in,all languages
translation,179,66,results,all languages,apart from,Greek
translation,179,66,results,Results,find that,hierarchical model
translation,179,74,results,hierarchical translation model,achieves,scores competitive
translation,179,74,results,scores competitive,with,state of the art
translation,179,74,results,English GeoQuery data,has,hierarchical translation model
translation,179,74,results,Results,on,English GeoQuery data
translation,180,175,ablation-analysis,difference,between,UDP N oP R and BL
translation,180,175,ablation-analysis,UDP N oP R and BL,see that,UDP N oP R
translation,180,175,ablation-analysis,UDP N oP R,contributes with,4 UAS points
translation,180,175,ablation-analysis,4 UAS points,over,baseline
translation,180,175,ablation-analysis,Ablation analysis,measure,difference
translation,180,186,ablation-analysis,performance,drops,2 %
translation,180,186,ablation-analysis,personalization,has,performance
translation,180,186,ablation-analysis,Ablation analysis,Without,personalization
translation,180,4,model,UDP,for,Universal Dependencies ( UD )
translation,180,4,model,first training -free parser,for,Universal Dependencies ( UD )
translation,180,4,model,UDP,has,first training -free parser
translation,180,4,model,Model,propose,UDP
translation,180,5,model,algorithm,based on,PageRank
translation,180,5,model,algorithm,based on,small set of head attachment rules
translation,180,5,model,Model,has,algorithm
translation,180,6,model,two -step decoding,to guarantee,function words
translation,180,6,model,function words,attached as,leaf nodes
translation,180,6,model,Model,features,two -step decoding
translation,180,43,model,parser,requires,no training data
translation,180,43,model,Model,propose,parser
translation,180,8,results,UDP,offers,linguistically sound unsupervised alternative
translation,180,8,results,linguistically sound unsupervised alternative,to,cross-lingual parsing
translation,180,8,results,cross-lingual parsing,for,UD
translation,180,8,results,Results,has,UDP
translation,180,157,results,UDP,is,competitive system
translation,180,157,results,average difference,of,6.4 %
translation,180,157,results,Results,shows,UDP
translation,180,158,results,MSD,on,one language ( Hindi )
translation,180,158,results,UDP,has,outperforms
translation,180,158,results,outperforms,has,MSD
translation,180,158,results,Results,has,UDP
translation,180,159,results,evaluation scenario,with,predicted POS
translation,180,159,results,predicted POS,observe that,our system
translation,180,159,results,only marginally ( 2.2 % ),compared to,MSD
translation,180,159,results,our system,has,drops
translation,180,159,results,drops,has,only marginally ( 2.2 % )
translation,180,159,results,MSD,has,2.7 % )
translation,180,159,results,Results,on,evaluation scenario
translation,180,165,results,outperforms,by,?4 %
translation,180,165,results,adjacency baselines ( BL ),by,?4 %
translation,180,165,results,adjacency baselines ( BL ),on average,two type-based naive POS tag scenario
translation,180,165,results,adjacency baselines ( BL ),on,two type-based naive POS tag scenario
translation,180,165,results,?4 %,on average,two type-based naive POS tag scenario
translation,180,165,results,?4 %,on,two type-based naive POS tag scenario
translation,180,165,results,UDP,has,outperforms
translation,180,165,results,outperforms,has,adjacency baselines ( BL )
translation,180,165,results,Results,has,UDP
translation,180,189,results,bigram heuristic,to determine,adposition direction
translation,180,189,results,predominant pre-or postposition preference,for,all languages
translation,180,189,results,average ADP UAS,of,75 %
translation,180,189,results,Results,has,bigram heuristic
translation,181,15,model,span,within,sentence
translation,181,23,model,initial model,by fine-tuning,BERT
translation,181,23,model,BERT,on,unlabeled data
translation,181,23,model,real sentences and distractors,produced by,random corruptions
translation,181,23,model,Model,learn,initial model
translation,181,95,results,real / fake RoBERTa model,to,supervised version
translation,181,95,results,real / fake RoBERTa model,find that,former
translation,181,95,results,real / fake RoBERTa model,find that,latter
translation,181,95,results,former,achieves,0.21 MCC
translation,181,95,results,former,achieves,0.73 MCC
translation,181,95,results,former,achieves,0.73 MCC
translation,181,95,results,0.21 MCC,on,CoLA development set
translation,181,95,results,Matthews Correlation Coefficient ),on,CoLA development set
translation,181,95,results,latter,achieves,0.73 MCC
translation,181,95,results,0.21 MCC,has,Matthews Correlation Coefficient )
translation,181,95,results,Results,Comparing,real / fake RoBERTa model
translation,181,98,results,parsing,via,constituency tests
translation,181,98,results,supervised model,by,6 F1
translation,181,98,results,supervised model,about,6 F1
translation,181,98,results,parsing,has,real / fake RoBERTa model
translation,181,98,results,constituency tests,has,real / fake RoBERTa model
translation,181,98,results,real / fake RoBERTa model,has,outperforms
translation,181,98,results,outperforms,has,supervised model
translation,181,98,results,Results,used for,parsing
translation,181,139,results,URNNG,achieve,71.3 F1
translation,181,139,results,performance,of,supervised binary RNNG + URNNG
translation,181,139,results,supervised binary RNNG + URNNG,with,gap
translation,181,139,results,gap,of,1.5 points
translation,181,139,results,Results,After,URNNG
translation,181,161,results,all of the tests,have,better F1
translation,181,161,results,refinement,has,all of the tests
translation,181,161,results,Results,After,refinement
translation,181,161,results,Results,has,all of the tests
translation,182,154,hyperparameters,word embeddings,initialized by,50 dimensional pre-trained word vectors
translation,182,154,hyperparameters,50 dimensional pre-trained word vectors,from,GloVe
translation,182,154,hyperparameters,50 dimensional pre-trained word vectors,updated in,training process
translation,182,154,hyperparameters,50 dimensional,has,word embeddings
translation,182,154,hyperparameters,Hyperparameters,use,50 dimensional
translation,182,156,hyperparameters,size,of,LSTM hidden layer
translation,182,156,hyperparameters,LSTM hidden layer,set at,50
translation,182,156,hyperparameters,Hyperparameters,has,size
translation,182,157,hyperparameters,"RMSProp ( Tieleman and Hinton , 2012 )",with,learning rate
translation,182,157,hyperparameters,"RMSProp ( Tieleman and Hinton , 2012 )",mini-,batch size
translation,182,157,hyperparameters,learning rate,of,0.005
translation,182,157,hyperparameters,batch size,of,32
translation,182,157,hyperparameters,32,for,optimization
translation,182,158,hyperparameters,dropout layer,with,probability 0.5
translation,182,158,hyperparameters,probability 0.5,for,regularization
translation,182,158,hyperparameters,Hyperparameters,use,dropout layer
translation,182,17,model,answer type prediction model,can improve,ranking
translation,182,17,model,ranking,of,candidate logical forms
translation,182,17,model,candidate logical forms,generated by,semantic parsing
translation,182,17,model,Model,propose,answer type prediction model
translation,182,198,model,context,via,question abstraction
translation,182,198,model,question abstraction,able to recover,useful information
translation,182,198,model,useful information,when,entity linking
translation,182,198,model,entity linking,is,uncertain
translation,182,198,model,Model,Utilizing,context
translation,182,216,model,neural answer type inference method,incorporated in,existing grounded semantic parsers
translation,182,216,model,ranking,of,candidate logical forms
translation,182,216,model,Model,propose,neural answer type inference method
translation,182,7,results,question,into,statement form
translation,182,7,results,our LSTM model,achieves,better accuracy
translation,182,7,results,question,has,our LSTM model
translation,182,7,results,Results,convert,question
translation,182,8,results,predicted type information,to rerank,logical forms
translation,182,8,results,logical forms,returned by,AgendaIL
translation,182,8,results,F1 - score,from,49.7 %
translation,182,8,results,49.7 %,to,52.6 %
translation,182,8,results,52.6 %,on,WE - BQUESTIONS data
translation,182,8,results,improve,has,F1 - score
translation,182,8,results,Results,Using,predicted type information
translation,182,173,results,Our system,adds,2.9 % absolute improvement
translation,182,173,results,Our system,achieves,52.6 %
translation,182,173,results,2.9 % absolute improvement,over,AgendaIL
translation,182,173,results,52.6 %,in,F 1 measure
translation,182,173,results,Results,has,Our system
translation,182,177,results,our method,tested without using,SimpleQuestions data
translation,182,177,results,SimpleQuestions data,for,pretraining question abstraction module
translation,182,177,results,F1 score,of,51.6 %
translation,183,112,ablation-analysis,three - level relation classification and tree features,bring,improvement
translation,183,112,ablation-analysis,improvement,of,about 1 percent
translation,183,112,ablation-analysis,about 1 percent,on,relation labeling
translation,183,112,ablation-analysis,Ablation analysis,see that,three - level relation classification and tree features
translation,183,109,baselines,Simp - 2,adopts,two -stage strategy
translation,183,109,baselines,Simp - 2,uses,only one classifier
translation,183,109,baselines,only one classifier,to classify,all the relations
translation,183,109,baselines,Baselines,has,Simp - 2
translation,183,115,experiments,Attribution and Same - Unit relations,are,top 2 relations
translation,183,67,hyperparameters,SVM classifiers,for,four classification tasks
translation,183,67,hyperparameters,SVM classifiers,for,three relation classifiers
translation,183,67,hyperparameters,Hyperparameters,use,SVM classifiers
translation,183,68,hyperparameters,linear kernel,for,fast training
translation,183,68,hyperparameters,linear kernel,use,squared hinge loss
translation,183,68,hyperparameters,squared hinge loss,with,L 1 penalty
translation,183,68,hyperparameters,L 1 penalty,on,error term
translation,183,68,hyperparameters,Hyperparameters,take,linear kernel
translation,183,68,hyperparameters,Hyperparameters,use,squared hinge loss
translation,183,69,hyperparameters,penalty coefficient C,set to,1
translation,183,69,hyperparameters,Hyperparameters,has,penalty coefficient C
translation,183,7,model,pipelined two -stage parsing method,for generating,RST tree
translation,183,7,model,RST tree,from,text
translation,183,7,model,Model,design,pipelined two -stage parsing method
translation,183,100,results,our method,exceeds,most systems
translation,183,100,results,all the others,with respect to,span and nuclearity
translation,183,100,results,our method,has,outperforms
translation,183,100,results,outperforms,has,all the others
translation,183,100,results,Results,see that,our method
translation,183,101,results,significantly outperforms,on building,naked tree structure ( span and nuclearity )
translation,183,101,results,other transition - based models,on building,naked tree structure ( span and nuclearity )
translation,183,101,results,significantly outperforms,has,other transition - based models
translation,183,101,results,other transition - based models,has,"Ji and Eisenstein , 2014"
translation,183,110,results,pipelined two stages,bring,significant improvement
translation,183,110,results,significant improvement,with respect to,all the aspects
translation,183,110,results,significant improvement,compared to,Simp - 1
translation,183,110,results,Results,observe that,pipelined two stages
translation,183,114,results,three - level relation labeling,does not achieve,prominent improvement
translation,183,114,results,prominent improvement,get,some interesting results
translation,183,114,results,Results,get,some interesting results
translation,183,117,results,Textual - Organization and Topic- Comment relaitons,gain,increase
translation,183,117,results,increase,by,20 % and 8 %
translation,184,152,ablation-analysis,Ablation experiments,without,pre-trained word embeddings or word lemma embeddings
translation,184,152,ablation-analysis,model,performs,worse
translation,184,152,ablation-analysis,Ablation experiments,has,model
translation,184,152,ablation-analysis,pre-trained word embeddings or word lemma embeddings,has,model
translation,184,153,ablation-analysis,pretrained word embeddings,contribute,more
translation,184,153,ablation-analysis,lemma embeddings,has,pretrained word embeddings
translation,184,153,ablation-analysis,Ablation analysis,Compared to,lemma embeddings
translation,184,158,ablation-analysis,additionally omitted,observe,further performance gains
translation,184,216,experimental-setup,single GPU,without,batches
translation,184,216,experimental-setup,Experimental setup,trained on,single GPU
translation,184,19,experiments,DRT parsing,as,structure prediction problem
translation,184,126,experiments,GMB,following,tree conversion process
translation,184,130,hyperparameters,dimensions,of,word and lemma embeddings
translation,184,130,hyperparameters,word and lemma embeddings,were,64 and 32
translation,184,130,hyperparameters,Hyperparameters,has,dimensions
translation,184,131,hyperparameters,dimensions,of,hidden vectors
translation,184,131,hyperparameters,hidden vectors,were,256
translation,184,131,hyperparameters,hidden vectors,were,128
translation,184,131,hyperparameters,256,for,encoder
translation,184,131,hyperparameters,128,for,decoder
translation,184,131,hyperparameters,Hyperparameters,has,dimensions
translation,184,133,hyperparameters,dropout rate,was,0.1
translation,184,133,hyperparameters,Hyperparameters,has,dropout rate
translation,184,134,hyperparameters,Pre-trained word embeddings ( 100 dimensions ),generated with,Word2 Vec
translation,184,134,hyperparameters,Word2 Vec,trained on,AFP portion of the English Gigaword corpus
translation,184,134,hyperparameters,Hyperparameters,has,Pre-trained word embeddings ( 100 dimensions )
translation,184,4,model,open-domain neural semantic parser,generates,formal meaning representations
translation,184,4,model,formal meaning representations,in the style of,Discourse Representation Theory ( DRT
translation,184,4,model,Model,introduce,open-domain neural semantic parser
translation,184,5,model,method,transforms,Discourse Representation Structures ( DRSs )
translation,184,5,model,method,develop,structure - aware model
translation,184,5,model,Discourse Representation Structures ( DRSs ),to,trees
translation,184,5,model,structure - aware model,decomposes,decoding process
translation,184,5,model,decoding process,into,three stages
translation,184,30,model,piecemeal mode of generation,yields,more accurate predictions
translation,184,30,model,predictions,taking,more global context
translation,184,30,model,more global context,into,account
translation,184,30,model,refine,has,predictions
translation,184,30,model,Model,has,piecemeal mode of generation
translation,184,32,model,open-domain semantic parser,yields,discourse representation structures
translation,184,32,model,novel end-toend neural model,equipped with,structured decoder
translation,184,32,model,novel end-toend neural model,decomposes,parsing process
translation,184,32,model,parsing process,into,three stages
translation,184,32,model,DRS - to- tree conversion method,transforms,DRSs
translation,184,32,model,DRSs,to,tree - based representations
translation,184,132,model,encoder,used,two hidden layers
translation,184,132,model,Model,has,encoder
translation,184,151,results,shallow structured decoder,performs,better
translation,184,151,results,better,than,baseline decoder
translation,184,151,results,proposed deep structure decoder,has,outperforms
translation,184,154,results,Results,on,test set
translation,184,156,results,shallow structure model,improves,precision
translation,184,156,results,precision,over,baseline
translation,184,156,results,slight loss,in,recall
translation,184,156,results,deep structure model,performs,best
translation,184,156,results,best,by,large margin
translation,184,156,results,Results,observe,shallow structure model
translation,184,157,results,referents,not,taken into account
translation,184,157,results,referents,has,performance
translation,184,157,results,taken into account,has,performance
translation,184,157,results,performance,has,improves
translation,184,157,results,Results,When,referents
translation,184,161,results,parser,is,better
translation,184,161,results,better,on,sentences
translation,184,161,results,sentences,do not represent,SDRSs
translation,184,161,results,SDRSs,has,vs 68.36 F 1 )
translation,184,161,results,Results,has,parser
translation,184,162,results,rhetorical relations ( linking segments ),predicted,fairly accurately
translation,184,162,results,Results,found that,rhetorical relations ( linking segments )
translation,184,163,results,F 1 performance,on,sentences
translation,184,164,results,similar trend,for,all models
translation,184,164,results,similar trend,as,sentence length
translation,184,164,results,similar trend,has,model performance
translation,184,164,results,all models,has,model performance
translation,184,164,results,sentence length,has,increases
translation,184,164,results,model performance,has,decreases
translation,184,164,results,Results,observe,similar trend
translation,184,165,results,baseline and shallow models,do not perform,well
translation,184,165,results,well,on,short sentences
translation,184,165,results,Results,has,baseline and shallow models
translation,185,5,experiments,each question,into,entity mention and a relation pattern
translation,185,104,hyperparameters,Five hundred neurons,used in,convolutional layer
translation,185,104,hyperparameters,Five hundred neurons,used in,max-pooling layer
translation,185,104,hyperparameters,Five hundred neurons,used in,final semantic layer
translation,185,104,hyperparameters,Hyperparameters,has,Five hundred neurons
translation,185,105,hyperparameters,learning rate,of,0.002
translation,185,105,hyperparameters,converged,after,150 iterations
translation,185,105,hyperparameters,training,has,converged
translation,185,4,model,semantic parsing framework,based on,semantic similarity
translation,185,4,model,semantic similarity,for,open domain question answering ( QA )
translation,185,4,model,Model,develop,semantic parsing framework
translation,185,6,model,convolutional neural network models,measure,similarity
translation,185,6,model,convolutional neural network models,measure,similarity
translation,185,6,model,similarity,of,entity mentions with entities
translation,185,6,model,similarity,of,relation patterns and relations
translation,185,6,model,similarity,of,relation patterns and relations
translation,185,6,model,entity mentions with entities,in,knowledge base ( KB )
translation,185,6,model,similarity,of,relation patterns and relations
translation,185,6,model,relation patterns and relations,in,KB
translation,185,6,model,Model,Using,convolutional neural network models
translation,185,16,model,semantic parsing framework,tailored to,single-relation questions
translation,185,16,model,Model,propose,semantic parsing framework
translation,185,17,model,novel semantic similarity model,using,convolutional neural networks
translation,185,57,model,Convolutional Neural Network based Semantic Model,develop,new convolutional neural network ( CNN ) based semantic model ( CNNSM )
translation,185,57,model,new convolutional neural network ( CNN ) based semantic model ( CNNSM ),for,semantic parsing
translation,185,57,model,Model,develop,new convolutional neural network ( CNN ) based semantic model ( CNNSM )
translation,185,57,model,Model,has,Convolutional Neural Network based Semantic Model
translation,185,59,model,CNNSM,uses,max pooling layer
translation,185,59,model,max pooling layer,to extract,most salient local features
translation,185,59,model,most salient local features,to form,fixed - length global feature vector
translation,185,127,model,semantic parsing framework,for,single-relation questions
translation,185,127,model,Model,propose,semantic parsing framework
translation,185,128,model,relation patterns and entity mentions,using,semantic similarity function
translation,185,128,model,semantic similarity function,rather than,lexical rules
translation,185,129,model,similarity model,trained using,convolutional neural networks
translation,185,129,model,convolutional neural networks,with,letter-trigrams vectors
translation,185,129,model,Model,has,similarity model
translation,185,20,results,general semantic similarity model,to match,patterns and relations
translation,185,20,results,higher precision,at,all the recall points
translation,185,20,results,questions,in,same test set
translation,185,20,results,patterns and relations,has,as well as mentions and entities
translation,185,20,results,outperforms,has,existing rule learning system
translation,185,20,results,Results,By using,general semantic similarity model
translation,185,121,results,precision,of,our CNNSM pm system
translation,185,121,results,our CNNSM pm system,is,consistently higher
translation,185,121,results,consistently higher,than,PARALEX
translation,185,121,results,consistently higher,across,all recall regions
translation,185,122,results,CNNSM m system,performs,similarly
translation,185,122,results,CNNSM m system,performs,inferior
translation,185,122,results,similarly,to,CNNSM pm
translation,185,122,results,CNNSM pm,in,high precision regime
translation,185,122,results,inferior,when,recall
translation,185,122,results,Results,has,CNNSM m system
translation,185,131,results,Our method,achieves,higher precision
translation,185,131,results,higher precision,on,QA task
translation,185,131,results,QA task,than,"previous work , PARALEX"
translation,185,131,results,Results,has,Our method
translation,186,178,baselines,RBG parser,has,"Lei et al. , 2014 b )"
translation,186,208,baselines,+E,post-process,non-projective output
translation,186,208,baselines,non-projective output,with,Eisner algorithm
translation,186,208,baselines,Baselines,has,+E
translation,186,138,experimental-setup,our models,on,Nvidia GPU card
translation,186,138,experimental-setup,Experimental setup,trained,our models
translation,186,139,experimental-setup,Model parameters,uniformly initialized to,"[?0.1 , 0.1 ]"
translation,186,139,experimental-setup,Experimental setup,has,Model parameters
translation,186,140,experimental-setup,"Adam ( Kingma and Ba , 2014 )",to optimize,our models
translation,186,140,experimental-setup,our models,with,hyper-parameters
translation,186,140,experimental-setup,hyper-parameters,i.e.,learning rate
translation,186,140,experimental-setup,hyper-parameters,i.e.,first momentum coefficient
translation,186,140,experimental-setup,hyper-parameters,i.e.,second momentum coefficient
translation,186,140,experimental-setup,learning rate,has,0.001
translation,186,140,experimental-setup,first momentum coefficient,has,0.9
translation,186,140,experimental-setup,second momentum coefficient,has,0.999
translation,186,141,experimental-setup,gradient exploding problem,rescaled,gradient
translation,186,141,experimental-setup,gradient,when,norm
translation,186,141,experimental-setup,norm,exceeded,5
translation,186,141,experimental-setup,Dropout,applied to,our model
translation,186,141,experimental-setup,Experimental setup,To alleviate,gradient exploding problem
translation,186,141,experimental-setup,Experimental setup,rescaled,gradient
translation,186,142,experimental-setup,d,is,hidden unit size
translation,186,143,experimental-setup,pre-trained word vectors,to initialize,word embedding matrix
translation,186,143,experimental-setup,Experimental setup,used,pre-trained word vectors
translation,186,144,experimental-setup,PTB experiments,used,300 dimensional
translation,186,144,experimental-setup,300 dimensional,pre-trained GloVe,vectors
translation,186,144,experimental-setup,Experimental setup,For,PTB experiments
translation,186,147,experimental-setup,POS tag embedding size,set to,q = 30
translation,186,147,experimental-setup,POS tag embedding size,set to,q = 50
translation,186,147,experimental-setup,POS tag embedding size,set to,q = 40
translation,186,147,experimental-setup,q = 30,in,English experiments
translation,186,147,experimental-setup,q = 50,in,Chinese experiments
translation,186,147,experimental-setup,q = 40,in,Czech and German experiments
translation,186,147,experimental-setup,Experimental setup,has,POS tag embedding size
translation,186,101,hyperparameters,two -layer rectifier network,for,classification task
translation,186,101,hyperparameters,Hyperparameters,employ,two -layer rectifier network
translation,186,5,model,Model,formalize,dependency parsing
translation,186,6,model,DENSE,produces,distribution
translation,186,6,model,distribution,over,possible heads
translation,186,6,model,possible heads,for,each word
translation,186,6,model,each word,using,features
translation,186,6,model,features,obtained from,bidirectional recurrent neural network
translation,186,6,model,Model,call,DENSE
translation,186,7,model,structural constraints,during,training
translation,186,7,model,DENSE,generates ( at inference time ),trees
translation,186,7,model,trees,for,overwhelming majority of sentences
translation,186,7,model,non-tree outputs,adjusted with,maximum spanning tree algorithm
translation,186,7,model,structural constraints,has,DENSE
translation,186,7,model,training,has,DENSE
translation,186,7,model,Model,Without enforcing,structural constraints
translation,186,27,model,simple neural network - based model,learns to select,head
translation,186,27,model,head,for,each word
translation,186,27,model,each word,in,sentence
translation,186,27,model,Model,propose,simple neural network - based model
translation,186,28,model,bidirectional recurrent neural networks,to learn,feature representations
translation,186,28,model,feature representations,for,words
translation,186,28,model,words,in,sentence
translation,186,28,model,Model,call,DENSE
translation,186,28,model,Model,employs,bidirectional recurrent neural networks
translation,186,193,model,DENSE,train without,graph - based algorithm
translation,186,193,model,neural dependency parser,train without,transition system
translation,186,193,model,neural dependency parser,train without,graph - based algorithm
translation,186,193,model,DENSE,has,neural dependency parser
translation,186,193,model,Model,presented,DENSE
translation,186,193,model,Model,presented,neural dependency parser
translation,186,169,results,output,of,parser
translation,186,169,results,output,improves,performance
translation,186,169,results,parser,with,Eisner algorithm
translation,186,169,results,performance,by,0.21 %
translation,186,170,results,1 - order-atomic features,are,inferior
translation,186,170,results,inferior,compared to,LSTM
translation,186,170,results,Results,observe that,1 - order-atomic features
translation,186,181,results,all other first ( and second ) order parsers,on,German and Czech
translation,186,181,results,DENSE,has,outperforms
translation,186,181,results,outperforms,has,all other first ( and second ) order parsers
translation,186,181,results,Results,has,DENSE
translation,186,182,results,slight a improvement,on both,UAS and LAS
translation,186,182,results,slight a improvement,when using,MST algorithm
translation,186,183,results,German,on,Czech
translation,186,183,results,DENSE,comparable with,best third - order parser ( Turbo -3rd )
translation,186,183,results,DENSE,on,Czech
translation,186,183,results,Czech,lags behind,Turbo - 3rd and RBG - 3rd
translation,186,183,results,German,has,DENSE
translation,186,183,results,Results,On,German
translation,186,183,results,Results,on,Czech
translation,186,186,results,DENSE,is,consistently worse
translation,186,187,results,Our experimental results,using,MST algorithm
translation,186,187,results,MST algorithm,during,inference
translation,186,187,results,inference,has,slightly improve
translation,186,187,results,slightly improve,has,model 's performance
translation,186,187,results,Results,using,MST algorithm
translation,186,187,results,Results,has,Our experimental results
translation,186,213,results,all previous neural models,on,UAS and LAS
translation,186,213,results,outperforms,has,all previous neural models
translation,186,214,results,DENSE,performs,competitively
translation,186,214,results,competitively,with,Z&M14
translation,186,214,results,non-neural model,with,com - a
translation,186,214,results,Z&M14,has,non-neural model
translation,186,214,results,Results,has,DENSE
translation,187,4,model,temporal phrases,has,given only a corpus of utterances
translation,187,214,results,Our system,performs,well
translation,187,214,results,Our system,performs,competitive
translation,187,214,results,well,above,GUTime baseline
translation,187,214,results,competitive,with,both of the more recent systems
translation,187,214,results,Results,has,Our system
translation,188,147,baselines,Transducers without pretraining,try,2 - stacked BiLSTM
translation,188,147,baselines,2 - stacked BiLSTM,where,generation of h i
translation,188,147,baselines,generation of h i,conditioned on,left and right context
translation,188,147,baselines,Baselines,has,Transducers without pretraining
translation,188,160,experiments,English,use,discontinuous Penn Treebank ( DPTB )
translation,188,160,experiments,discontinuous Penn Treebank ( DPTB ),by,Evang and Kallmeyer ( 2011 )
translation,188,4,model,Model,reduces,discontinuous parsing
translation,188,159,model,each word,with,first sub-word
translation,188,159,model,embedding,as,only input
translation,188,159,model,only input,for,models
translation,188,174,results,Results,on,dev sets
translation,188,176,results,pointer - based encoding,with,simplified PoS tags
translation,188,176,results,pointer - based encoding,lead,clear improvements
translation,188,176,results,pointer - based encoding,to,clear improvements
translation,188,176,results,Results,has,pointer - based encoding
translation,188,181,results,latter,performs,better
translation,188,181,results,inverse permutation encodings,has,latter
translation,189,181,ablation-analysis,"Glove , lemma and character embeddings",are,fine-tuning
translation,189,181,ablation-analysis,"Glove , lemma and character embeddings",helpful for,DM
translation,189,181,ablation-analysis,fine-tuning,on,training set
translation,189,181,ablation-analysis,training set,slightly improves,performance
translation,189,181,ablation-analysis,Ablation analysis,found that,"Glove , lemma and character embeddings"
translation,189,185,baselines,subtoken pooling,compared,performance
translation,189,185,baselines,performance,of using,first subtoken pooling
translation,189,185,baselines,performance,of using,average pooling
translation,189,185,baselines,average pooling,as,token embedding
translation,189,155,experiments,our system,ranks,"6 th , 5 th and 7 th"
translation,189,155,experiments,"6 th , 5 th and 7 th",among,13 teams
translation,189,155,experiments,"PSD , EDS and AMR graph",has,our system
translation,189,144,hyperparameters,our model,used,"Adam ( Kingma and Ba , 2015 )"
translation,189,144,hyperparameters,"Adam ( Kingma and Ba , 2015 )",to optimize,our system
translation,189,144,hyperparameters,"Adam ( Kingma and Ba , 2015 )",annealing,learning rate
translation,189,144,hyperparameters,learning rate,by,0.5
translation,189,144,hyperparameters,0.5,for,"10,000 steps"
translation,189,144,hyperparameters,Hyperparameters,trained,our model
translation,189,145,hyperparameters,model,for,"100,000 iterations"
translation,189,145,hyperparameters,model,terminated with,"10,000 iterations"
translation,189,145,hyperparameters,"100,000 iterations",with,batch size
translation,189,145,hyperparameters,batch size,of,"6,000 tokens"
translation,189,145,hyperparameters,Hyperparameters,trained,model
translation,189,145,hyperparameters,Hyperparameters,terminated with,"10,000 iterations"
translation,189,5,model,graph- based parser,combines,extended pointergenerator network
translation,189,5,model,extended pointergenerator network,that generates,nodes
translation,189,5,model,second-order mean field variational inference module,predicts,edges
translation,189,6,results,Our system,achieved,1 st and 2 nd place
translation,189,6,results,Our system,achieved,3 rd place
translation,189,6,results,Our system,achieved,3 rd place
translation,189,6,results,1 st and 2 nd place,for,DM and PSD frameworks
translation,189,6,results,1 st and 2 nd place,for,DM framework
translation,189,6,results,1 st and 2 nd place,for,DM framework
translation,189,6,results,3 rd place,for,DM framework
translation,189,6,results,3 rd place,on,cross-framework ranks
translation,189,6,results,DM framework,on,cross-framework ranks
translation,189,6,results,Results,has,Our system
translation,189,23,results,our system,gets,94.88 F1 score
translation,189,23,results,94.88 F1 score,in,cross-framework metric
translation,189,23,results,cross-framework metric,for,DM
translation,189,24,results,our system,gets,92.98 and 81.61 labeled F1 score
translation,189,24,results,92.98 and 81.61 labeled F1 score,for,DM and PSD
translation,189,24,results,in - framework metrics,has,our system
translation,189,24,results,Results,For,in - framework metrics
translation,189,148,results,results,still,competitive
translation,189,148,results,competitive,to,other teams
translation,189,148,results,Results,still,competitive
translation,189,148,results,Results,has,results
translation,189,150,results,Our system,performs,well
translation,189,150,results,well,on,DM framework
translation,189,150,results,well,with,F1 score
translation,189,150,results,DM framework,with,F1 score
translation,189,150,results,only 0.4 percent F1,below,best score on DM
translation,189,150,results,F1 score,has,only 0.4 percent F1
translation,189,150,results,Results,has,Our system
translation,189,152,results,gold lemmas,from,original SDP dataset
translation,189,152,results,gold lemmas,from,companion data
translation,189,152,results,lemmas,from,companion data
translation,189,152,results,lemmas,have,only 71.4 % accuracy
translation,189,152,results,gold lemmas,has,lemmas
translation,189,152,results,original SDP dataset,has,lemmas
translation,189,158,results,competitive,on,all the components
translation,189,158,results,all the components,except,labels
translation,189,158,results,PSD,has,our system
translation,189,158,results,Results,For,PSD
translation,189,161,results,outperforms,by,0.5 and 0.8 F1 scores
translation,189,161,results,best of the other systems,by,0.5 and 0.8 F1 scores
translation,189,161,results,best of the other systems,on,all and lpps test sets
translation,189,161,results,0.5 and 0.8 F1 scores,on,all and lpps test sets
translation,189,161,results,DM,has,our system
translation,189,161,results,our system,has,outperforms
translation,189,161,results,outperforms,has,best of the other systems
translation,189,161,results,Results,For,DM
translation,189,162,results,outperforms,by,0.4 F1 score
translation,189,162,results,outperforms,by,only 0.05 F1 score
translation,189,162,results,best of the other systems,by,0.4 F1 score
translation,189,162,results,0.4 F1 score,for,lpps
translation,189,162,results,only 0.05 F1 score,below,best score
translation,189,162,results,PSD,has,our system
translation,189,162,results,our system,has,outperforms
translation,189,162,results,outperforms,has,best of the other systems
translation,189,162,results,Results,For,PSD
translation,189,167,results,our second-order edge prediction,is,useful
translation,189,167,results,useful,not only,SDP frameworks
translation,189,167,results,useful,on,SDP frameworks
translation,189,167,results,useful,on,AMR framework
translation,189,167,results,useful,on,AMR framework
translation,189,167,results,Results,shows that,our second-order edge prediction
translation,189,168,results,huge gap,between,test and development results
translation,189,168,results,test and development results,on both,MRP and the Smatch scores
translation,189,170,results,EDS,For,EDS
translation,189,170,results,EDS,For,our parser
translation,189,170,results,our parser,ranks,5 th
translation,189,170,results,EDS,has,our parser
translation,189,170,results,EDS,has,our parser
translation,189,170,results,Results,For,EDS
translation,189,170,results,Results,has,EDS
translation,189,182,results,ELMo embedding,is,helpful
translation,189,182,results,cannot outperform,has,BERT embedding
translation,189,182,results,Results,has,ELMo embedding
translation,189,186,results,average pooling,slightly better than,first pooling
translation,189,186,results,Results,found that,average pooling
translation,189,188,results,syntactic information,as,embeddings
translation,189,188,results,syntactic information,is,not very helpful
translation,189,188,results,not very helpful,for,task
translation,189,188,results,Results,shows that,syntactic information
translation,189,197,results,further improvement,on,DM and PSD
translation,189,197,results,DM and PSD,shows,named entity tags
translation,189,197,results,named entity tags,helpful for,semantic dependency parsing
translation,189,197,results,Results,With,lemma and named entity embeddings
translation,190,110,ablation-analysis,accuracy,of,dependencies
translation,190,110,ablation-analysis,dependencies,involved in,gapping
translation,190,110,ablation-analysis,gapping,improved,web crawl enrichment
translation,190,110,ablation-analysis,Russian and Slovak,has,accuracy
translation,190,110,ablation-analysis,Ablation analysis,For,Russian and Slovak
translation,190,69,hyperparameters,sampling,on,two features
translation,190,69,hyperparameters,two features,of,every tree
translation,190,69,hyperparameters,number of unique dependency relation types,divided by,number of tokens
translation,190,69,hyperparameters,Hyperparameters,base,sampling
translation,190,87,results,Parsing results,for,different sampling strategies
translation,190,87,results,results,follow,intuitively expectable pattern
translation,190,87,results,sample,with,least tokens
translation,190,87,results,least tokens,results in,worst score
translation,190,87,results,treebank distribution,receives,better score
translation,190,87,results,intuitively expectable pattern,has,sample
translation,190,87,results,Results,follow,intuitively expectable pattern
translation,190,87,results,Results,has,Parsing results
translation,190,88,results,sampling strategy,mimics,treebank distribution
translation,190,88,results,sampling strategy,score,almost 3 pp lower
translation,190,88,results,Slovak,has,sampling strategy
translation,190,88,results,Results,for,Slovak
translation,190,97,results,all languages,except,Czech
translation,190,97,results,all languages,improve,overall parsing accuracy
translation,190,97,results,overall parsing accuracy,for,Slovak
translation,190,97,results,Slovak,as much as,2.7 pp
translation,190,97,results,Results,for,all languages
translation,190,98,results,smaller,has,treebank
translation,190,98,results,smaller,has,larger
translation,190,98,results,treebank,has,larger
translation,190,98,results,larger,has,benefit
translation,190,115,results,Slovak,see,significant improvement
translation,190,115,results,significant improvement,in,overall parsing accuracy
translation,190,115,results,Results,For,Slovak
translation,190,124,results,artificial data,results in,lower performance
translation,190,124,results,lower performance,on,orphans
translation,190,124,results,orphans,for,"Czech , Slovak and Russian"
translation,190,124,results,higher,for,Finnish
translation,190,124,results,web crawl,has,artificial data
translation,190,124,results,Results,Compared to,web crawl
translation,190,126,results,very substantial improvement,achieved on,English
translation,190,126,results,recall,of,9.62 %
translation,190,126,results,Results,has,very substantial improvement
translation,191,203,ablation-analysis,ablation results,indicate,model uncertainty
translation,191,203,ablation-analysis,model uncertainty,plays,most important role
translation,191,203,ablation-analysis,most important role,among,confidence metrics
translation,191,203,ablation-analysis,Ablation analysis,indicate,model uncertainty
translation,191,203,ablation-analysis,Ablation analysis,has,ablation results
translation,191,204,ablation-analysis,metrics,of,data uncertainty
translation,191,204,ablation-analysis,data uncertainty,affects,performance
translation,191,204,ablation-analysis,performance,has,less
translation,191,204,ablation-analysis,Ablation analysis,removing,metrics
translation,191,176,experimental-setup,smoothing constant,of,"RMSProp ( Tieleman and Hinton , 2012 )"
translation,191,176,experimental-setup,smoothing constant,were,0.002 and 0.95
translation,191,176,experimental-setup,"RMSProp ( Tieleman and Hinton , 2012 )",were,0.002 and 0.95
translation,191,176,experimental-setup,Experimental setup,has,learning rate
translation,191,176,experimental-setup,Experimental setup,has,smoothing constant
translation,191,177,experimental-setup,dropout rate,was,0.25
translation,191,177,experimental-setup,Experimental setup,has,dropout rate
translation,191,178,experimental-setup,two - layer LSTM,used for,IFTTT
translation,191,178,experimental-setup,one- layer LSTM,employed for,DJANGO
translation,191,178,experimental-setup,Experimental setup,has,two - layer LSTM
translation,191,179,experimental-setup,Dimensions,for,word embedding and hidden vector
translation,191,179,experimental-setup,Dimensions,selected from,"{ 150 , 250 }"
translation,191,179,experimental-setup,word embedding and hidden vector,selected from,"{ 150 , 250 }"
translation,191,179,experimental-setup,Experimental setup,has,Dimensions
translation,191,180,experimental-setup,beam size,during,decoding
translation,191,180,experimental-setup,decoding,was,5
translation,191,180,experimental-setup,Experimental setup,has,beam size
translation,191,187,experimental-setup,model uncertainty,set,dropout rate
translation,191,187,experimental-setup,model uncertainty,performed,30 inference passes
translation,191,187,experimental-setup,dropout rate,to,0.1
translation,191,187,experimental-setup,Experimental setup,To estimate,model uncertainty
translation,191,187,experimental-setup,Experimental setup,performed,30 inference passes
translation,191,189,experimental-setup,language model,estimated using,"KenLM ( Heafield et al. , 2013 )"
translation,191,189,experimental-setup,Experimental setup,has,language model
translation,191,191,experimental-setup,confidence metrics,implemented in,batch mode
translation,191,191,experimental-setup,confidence metrics,to take full advantage of,GPUs
translation,191,191,experimental-setup,Experimental setup,has,confidence metrics
translation,191,193,experimental-setup,number of boosted trees,selected from,"{ 20 , 50 }"
translation,191,193,experimental-setup,Experimental setup,has,number of boosted trees
translation,191,194,experimental-setup,maximum tree depth,selected from,"{ 3 , 4 , 5 }"
translation,191,194,experimental-setup,Experimental setup,has,maximum tree depth
translation,191,195,experimental-setup,subsample ratio,to,0.8
translation,191,195,experimental-setup,Experimental setup,set,subsample ratio
translation,191,4,model,confidence modeling,for,neural semantic parsers
translation,191,4,model,confidence modeling,built upon,sequence - to-sequence models
translation,191,4,model,Model,focus on,confidence modeling
translation,191,14,model,model 's confidence,in,predictions
translation,191,14,model,immediate and meaningful feedback,regarding,uncertain outputs
translation,191,29,model,method,based on,backpropagation
translation,191,29,model,model behavior,by identifying,which parts
translation,191,29,model,which parts,of,input
translation,191,29,model,which parts,contribute to,uncertain predictions
translation,191,29,model,interpret,has,model behavior
translation,191,241,model,uncertainty interpretation method,for,neural semantic parsing
translation,191,241,model,Model,presented,confidence estimation model
translation,191,31,results,thresholding confidence scores,achieves,good trade - off
translation,191,31,results,good trade - off,between,coverage and accuracy
translation,191,31,results,Results,demonstrate,thresholding confidence scores
translation,191,186,results,accuracy,of,our parser
translation,191,186,results,our parser,is,53.7 %
translation,191,186,results,53.7 %,better than,result ( 45.1 % )
translation,191,186,results,result ( 45.1 % ),of,sequence - to-sequence model
translation,191,186,results,Results,has,accuracy
translation,191,202,results,POSTERIOR,by,large margin
translation,191,202,results,our method,has,CONF
translation,191,202,results,CONF,has,outperforms
translation,191,202,results,outperforms,has,POSTERIOR
translation,191,214,results,F1 score,improves,monotonically
translation,191,214,results,monotonically,for,POSTERIOR and our method
translation,191,214,results,better performance,when,coverage
translation,191,214,results,coverage,is,same
translation,191,214,results,Results,has,F1 score
translation,191,234,results,BACKPROP,achieves,better interpretation quality
translation,191,234,results,better interpretation quality,than,attention mechanism
translation,191,234,results,Results,has,BACKPROP
translation,192,33,baselines,sequence-toaction method,generates,semantic graphs
translation,192,33,baselines,semantic graphs,using,RNN model
translation,192,33,baselines,RNN model,learned,end-to - end
translation,192,33,baselines,end-to - end,from,training data
translation,192,131,experimental-setup,100 dimensional word vectors,for,sentence encoding
translation,192,131,experimental-setup,Experimental setup,use,200 hidden units
translation,192,131,experimental-setup,Experimental setup,use,100 dimensional word vectors
translation,192,132,experimental-setup,dimensions,of,action embedding
translation,192,132,experimental-setup,dimensions,tuned on,validation datasets
translation,192,132,experimental-setup,validation datasets,for,each corpus
translation,192,132,experimental-setup,Experimental setup,has,dimensions
translation,192,133,experimental-setup,all parameters,by,uniformly sampling
translation,192,133,experimental-setup,uniformly sampling,within,"interval [ - 0.1 , 0.1 ]"
translation,192,133,experimental-setup,Experimental setup,initialize,all parameters
translation,192,134,experimental-setup,model,for,total of 30 epochs
translation,192,134,experimental-setup,model,halve,learning rate
translation,192,134,experimental-setup,total of 30 epochs,with,initial learning rate
translation,192,134,experimental-setup,initial learning rate,of,0.1
translation,192,134,experimental-setup,learning rate,every,5 epochs
translation,192,134,experimental-setup,learning rate,after,epoch 15
translation,192,134,experimental-setup,5 epochs,after,epoch 15
translation,192,134,experimental-setup,Experimental setup,train,model
translation,192,136,experimental-setup,beam size,set as,5
translation,192,136,experimental-setup,Experimental setup,has,beam size
translation,192,217,experiments,all three datasets,has,Soc. Blo
translation,192,4,model,neural semantic parsing,models,semantic parsing
translation,192,5,model,advantages,from,two recent promising directions
translation,192,5,model,two recent promising directions,of,semantic parsing
translation,192,5,model,Model,leverages,advantages
translation,192,6,model,semantic graph,to represent,meaning of a sentence
translation,192,6,model,tight - coupling,with,knowledge bases
translation,192,6,model,Model,uses,semantic graph
translation,192,7,model,RNN model,effectively map,sentences to action sequences
translation,192,7,model,sentences to action sequences,for,semantic graph generation
translation,192,7,model,Model,propose,RNN model
translation,192,27,model,new neural semantic parsing framework,has,Sequence - to - Action
translation,192,27,model,Model,propose,new neural semantic parsing framework
translation,192,28,model,semantic parsing,as,end-to - end semantic graph generation process
translation,192,28,model,Model,model,semantic parsing
translation,192,32,model,parsing,by incorporating,structure and semantic constraints
translation,192,32,model,structure and semantic constraints,during,decoding
translation,192,32,model,Model,enhance,parsing
translation,192,43,model,sequence - to-action model,including,Seq2Seq RNN model
translation,192,43,model,sequence - to-action model,action set encoding for,semantic graph generation
translation,192,43,model,Seq2Seq RNN model,for,action sequence prediction
translation,192,43,model,Model,design,sequence - to-action model
translation,192,135,model,word vectors,for,words
translation,192,135,model,words,occurring only once with,universal word vector
translation,192,135,model,Model,replace,word vectors
translation,192,154,model,Structure constraints,enhance,semantic parsing
translation,192,154,model,graph,using,generated action sequence
translation,192,154,model,Model,has,Structure constraints
translation,192,157,model,knowledge base schemas,during,decoding
translation,192,157,model,semantic constraints,effective for,semantic parsing
translation,192,157,model,knowledge base schemas,has,semantic constraints
translation,192,157,model,decoding,has,semantic constraints
translation,192,157,model,Model,By leveraging,knowledge base schemas
translation,192,206,model,Sequence-to - Action,models,semantic parsing
translation,192,206,model,semantic parsing,has,as an end-to - end semantic graph generation process
translation,192,206,model,Model,proposes,Sequence-to - Action
translation,192,36,results,action sequence encoding,better capture,structure and semantic information
translation,192,36,results,action sequence encoding,is,more compact
translation,192,36,results,Results,find that,action sequence encoding
translation,192,148,results,advantages,of,semantic graph representation
translation,192,148,results,prediction ability,of,Seq2Seq model
translation,192,148,results,our method,achieves,stateof - the - art performance
translation,192,148,results,our method,gets,competitive performance
translation,192,148,results,stateof - the - art performance,on,OVERNIGHT dataset
translation,192,148,results,competitive performance,on,GEO and ATIS dataset
translation,192,148,results,advantages,has,our method
translation,192,148,results,semantic graph representation,has,our method
translation,192,148,results,Results,By synthetizing,advantages
translation,192,149,results,full model ( Seq2Act + C1 + C2 ),gets,best test accuracy
translation,192,149,results,best test accuracy,of,88.9
translation,192,149,results,Seq2 Act model,gets,better results
translation,192,149,results,better results,than,all Seq2Seq baselines
translation,192,150,results,Seq2 Act model,achieve,test accuracy
translation,192,150,results,test accuracy,of,87.5
translation,192,150,results,best accuracy 87.1,of,Seq2Seq baseline
translation,192,150,results,GEO,has,Seq2 Act model
translation,192,150,results,Results,On,GEO
translation,192,151,results,Seq2 Act model,obtains,test accuracy
translation,192,151,results,test accuracy,of,84.6
translation,192,151,results,ATIS,has,Seq2 Act model
translation,192,151,results,Results,On,ATIS
translation,192,152,results,Seq2 Act model,gets,test accuracy
translation,192,152,results,Seq2 Act model,better than,best Seq2Seq baseline
translation,192,152,results,test accuracy,of,78.0
translation,192,152,results,OVERNGIHT,has,Seq2 Act model
translation,192,152,results,Results,On,OVERNGIHT
translation,192,155,results,all three datasets,has,Seq2Act ( + C1 )
translation,192,155,results,Seq2Act ( + C1 ),has,outperforms
translation,192,155,results,outperforms,has,basic Seq2 Act model
translation,192,155,results,Results,In,all three datasets
translation,192,158,results,Seq2Act ( + C1 + C2 ),gets,best performance
translation,192,158,results,best performance,on,all three datasets
translation,192,158,results,Seq2Act and Seq2Act ( + C1 ),has,Seq2Act ( + C1 + C2 )
translation,192,158,results,Results,Compared to,Seq2Act and Seq2Act ( + C1 )
translation,192,165,results,action sequence encoding,is,more compact
translation,192,165,results,more compact,than,linearized logical form encoding
translation,192,165,results,action sequence,is,shorter
translation,192,165,results,shorter,on,all three datasets
translation,192,165,results,shorter,"35.5 % , 9.2 % and 28.5 % reduction in",length
translation,192,165,results,linearized logical form encoding,has,action sequence
translation,192,165,results,Results,see,action sequence encoding
translation,192,165,results,Results,has,action sequence encoding
translation,192,216,results,our full model,gets,second best test accuracy
translation,192,216,results,second best test accuracy,of,85.5
translation,192,216,results,ATIS,has,our full model
translation,192,223,results,our full model,gets,state - of- the- art accuracy
translation,192,223,results,state - of- the- art accuracy,of,79.0
translation,192,223,results,outperforms,with,extra augmented training data
translation,192,223,results,Jia and Liang,with,extra augmented training data
translation,192,223,results,OVERNIGHT,has,our full model
translation,192,223,results,outperforms,has,Jia and Liang
translation,192,223,results,Results,On,OVERNIGHT
translation,193,113,baselines,baselines,has,delexicalized parser
translation,193,107,experimental-setup,alignment,use,sentence - level hunalign algorithm
translation,193,107,experimental-setup,Experimental setup,For,alignment
translation,193,115,experimental-setup,500 - dimensional word embeddings,trained on,translations of the Bible
translation,193,116,experimental-setup,word embeddings,trained using,skipgram
translation,193,116,experimental-setup,with negative sampling,on,word-by-sentence PMI matrix
translation,193,116,experimental-setup,word-by-sentence PMI matrix,induced from,Edinburgh Bible Corpus
translation,193,116,experimental-setup,skipgram,has,with negative sampling
translation,193,116,experimental-setup,Experimental setup,has,word embeddings
translation,193,122,experimental-setup,weights,initialized using,normalized values
translation,193,122,experimental-setup,normalized values,add,1
translation,193,122,experimental-setup,1,to,initial forget gate bias
translation,193,122,experimental-setup,Experimental setup,add,1
translation,193,122,experimental-setup,Experimental setup,has,weights
translation,193,123,experimental-setup,network,using,"RMSprop ( Tieleman and Hinton , 2012 )"
translation,193,123,experimental-setup,"RMSprop ( Tieleman and Hinton , 2012 )",with,hyperparameters
translation,193,123,experimental-setup,minibatches,of,64 sentences
translation,193,123,experimental-setup,hyperparameters,has,? = 0.1 and ? = 0.9
translation,193,123,experimental-setup,Experimental setup,trained,network
translation,193,125,experimental-setup,dropouts,after,each LSTMlayer
translation,193,125,experimental-setup,dropouts,between,input layer and the first LSTM - layer
translation,193,125,experimental-setup,dropouts,with,dropout probability
translation,193,125,experimental-setup,each LSTMlayer,with,dropout probability p = 0.5
translation,193,125,experimental-setup,each LSTMlayer,with,dropout probability
translation,193,125,experimental-setup,input layer and the first LSTM - layer,with,dropout probability
translation,193,125,experimental-setup,Experimental setup,applied,dropouts
translation,193,125,experimental-setup,Experimental setup,between,input layer and the first LSTM - layer
translation,193,126,experimental-setup,gradient clipping factor,of,15
translation,193,126,experimental-setup,Experimental setup,employed,gradient clipping factor
translation,193,128,experimental-setup,"10 , 50 , 100 , and 200 hidden units per layer",with,up to 6 layers
translation,193,128,experimental-setup,Experimental setup,experimented with,"10 , 50 , 100 , and 200 hidden units per layer"
translation,193,129,experimental-setup,greedy search,on,monolingual parsing
translation,193,129,experimental-setup,greedy search,determined,optimal network shape
translation,193,129,experimental-setup,optimal network shape,to contain,100 units per direction
translation,193,129,experimental-setup,optimal network shape,total of,4 layers
translation,193,129,experimental-setup,100 units per direction,per,hidden layer
translation,193,129,experimental-setup,Experimental setup,Using,greedy search
translation,193,114,experiments,cross-lingual evaluation,of,Tensor -LSTM
translation,193,114,experiments,Tensor -LSTM,using,cross entropy loss
translation,193,114,experiments,Tensor -LSTM,using,mean squared loss
translation,193,114,experiments,cross entropy loss,with,early decoding
translation,193,114,experiments,mean squared loss,with,late decoding
translation,193,127,experiments,monolingual setting,used,early stopping
translation,193,127,experiments,early stopping,on,development set
translation,193,142,experiments,monolingual setting,compare,parser
translation,193,142,experiments,"fast , capable graph - based parser",used as,component
translation,193,142,experiments,component,in,many larger systems
translation,193,142,experiments,"TurboParser ( Martins et al. , 2010 )",has,"fast , capable graph - based parser"
translation,193,5,model,end-to- end graph - based neural network dependency parser,trained to reproduce,matrices of edge scores
translation,193,5,model,Model,present,end-to- end graph - based neural network dependency parser
translation,193,10,model,Model,develop and evaluate,graph - based parser
translation,193,25,model,end-to - end neural graph - based dependency parser,apply it in,cross-lingual setting
translation,193,25,model,Model,present,end-to - end neural graph - based dependency parser
translation,193,27,model,input,is,dense weighted graph
translation,193,27,model,our parser,superior to,previous best approaches
translation,193,27,model,previous best approaches,to,cross-lingual parsing
translation,193,6,results,our approach,to,cross-lingual dependency parsing
translation,193,6,results,our approach,achieves,absolute improvement
translation,193,6,results,cross-lingual dependency parsing,is,simpler
translation,193,6,results,absolute improvement,of,2.25 %
translation,193,6,results,2.25 %,averaged across,10 languages
translation,193,6,results,2.25 %,compared to,previous state of the art
translation,193,6,results,Results,show,our approach
translation,193,26,results,parser,accepts,any weighted or non-weighted graph
translation,193,26,results,more flexible,than,similar parsers
translation,193,26,results,any weighted or non-weighted graph,over,token sequence
translation,193,26,results,token sequence,as,input
translation,193,26,results,Results,has,parser
translation,193,140,results,best performance,achieved at,10000 sentences
translation,193,140,results,best performance,with,6 and 5 epochs
translation,193,140,results,6 and 5 epochs,for,cross entropy and mean squared loss
translation,193,140,results,Results,see that,best performance
translation,193,155,results,performance,across,different languages
translation,193,155,results,different languages,is,large
translation,193,155,results,large,for,all systems
translation,193,155,results,Results,variation in,performance
translation,193,157,results,Tensor-LSTM,with,mean squared loss
translation,193,157,results,mean squared loss,has,outperforms
translation,193,157,results,outperforms,has,all other systems
translation,193,163,results,French and Spanish,not follow,same trend
translation,193,163,results,same trend,with,cross entropy loss
translation,193,163,results,outperforming,despite,high number of missing labels
translation,193,163,results,mean squared loss,despite,high number of missing labels
translation,193,163,results,cross entropy loss,has,outperforming
translation,193,163,results,outperforming,has,mean squared loss
translation,193,163,results,Results,has,French and Spanish
translation,193,164,results,performance,on,French and Spanish
translation,193,164,results,performance,seen to be,very high
translation,193,164,results,French and Spanish,for,both systems
translation,193,164,results,French and Spanish,seen to be,very high
translation,193,170,results,faster,with,percentage of deleted labels
translation,193,170,results,percentage of deleted labels,for,cross entropy model
translation,193,170,results,performance,has,drops
translation,193,170,results,drops,has,faster
translation,194,8,model,topdown parsing algorithm,for,RGL
translation,194,8,model,topdown parsing algorithm,runs in,time
translation,194,8,model,time,linear in the size of,input graph
translation,194,8,model,Model,provide,topdown parsing algorithm
translation,194,27,model,Regular Graph Grammars,has,RGG
translation,194,27,model,Model,propose,Regular Graph Grammars
translation,195,101,baselines,Post-ordering,based on,phrase - based SMT ( PO - PBMT )
translation,195,101,baselines,Postordering,based on,hierarchical phrase - based SMT ( PO - HPBMT )
translation,195,92,experimental-setup,Mecab 3 v0.98,used for,Japanese morphological analysis
translation,195,92,experimental-setup,Experimental setup,has,Mecab 3 v0.98
translation,195,94,experiments,gram language models,using,SRILM
translation,195,94,experiments,5 -,has,gram language models
translation,195,99,experiments,10 - best parsing results,of,Berkeley parser
translation,195,110,results,proposed method,achieved,best scores
translation,195,110,results,best scores,for,RIBES and BLEU
translation,195,110,results,best scores,both,RIBES and BLEU
translation,195,110,results,RIBES and BLEU,for,NTCIR - 9 and NTCIR - 8 test data
translation,195,115,results,our post-ordering method,is,more effective
translation,195,115,results,more effective,than,PO - PBMT
translation,195,115,results,more effective,than,PO- HPBMT
translation,195,115,results,Results,show,our post-ordering method
translation,195,116,results,RIBES,based on,rank order correlation coefficient
translation,195,116,results,proposed method,correctly recovered,word order
translation,195,116,results,word order,of,English sentences
translation,195,116,results,Results,show that,proposed method
translation,195,116,results,Results,has,RIBES
translation,196,69,ablation-analysis,hurt,has,parser performance
translation,196,138,baselines,6 treebanks,trained,20 baseline Stack - LSTM models
translation,196,138,baselines,20 baseline Stack - LSTM models,for,parsing
translation,196,138,baselines,20 baseline Stack - LSTM models,per,treebank
translation,196,138,baselines,Baselines,For,6 treebanks
translation,196,105,experiments,10 - 20 parsing models,per,languagetreebank
translation,196,142,experiments,models,trained with,character embeddings
translation,196,143,experiments,diverse set of word embeddings,for,Stack - LSTM
translation,196,143,experiments,diverse set of word embeddings,with dimension 100,hu_szeged
translation,196,143,experiments,ja_gsd,has,in- house cross-lingual embeddings
translation,196,143,experiments,zh_gsd,has,Facebook embeddings
translation,196,166,experiments,exceptions,are,low resource languages
translation,196,166,experiments,low resource languages,like,kmr_mg
translation,196,166,experiments,kmr_mg,in which,our system
translation,196,175,experiments,punctuation,indicative of,sentence breaks and other character patterns
translation,196,175,experiments,it_postwita,has,our system
translation,196,175,experiments,sentence breaks and other character patterns,has,our system
translation,196,175,experiments,our system,has,outperformed
translation,196,175,experiments,outperformed,has,UDPipe future
translation,196,140,hyperparameters,Independent LSTM models,trained on,each treebank
translation,196,140,hyperparameters,each treebank,for,labeling
translation,196,140,hyperparameters,Hyperparameters,has,Independent LSTM models
translation,196,141,hyperparameters,models,for,6 treebanks
translation,196,141,hyperparameters,Hyperparameters,has,models
translation,196,145,hyperparameters,input and hidden- layer dimension,to,100
translation,196,145,hyperparameters,action vector dimension,to,20
translation,196,145,hyperparameters,Hyperparameters,set,input and hidden- layer dimension
translation,196,145,hyperparameters,Hyperparameters,set,action vector dimension
translation,196,146,hyperparameters,CoNLL 2017 pretrained embeddings,has,dimension 100 )
translation,196,146,hyperparameters,Hyperparameters,has,CoNLL 2017 pretrained embeddings
translation,196,152,hyperparameters,Cross Lingual Cross-lingual models,trained with,input and hidden layers
translation,196,152,hyperparameters,Cross Lingual Cross-lingual models,trained with,action vectors
translation,196,152,hyperparameters,input and hidden layers,of dimension,100 each
translation,196,152,hyperparameters,action vectors,dimension,20
translation,196,152,hyperparameters,Hyperparameters,has,Cross Lingual Cross-lingual models
translation,196,153,hyperparameters,Pretrained multilingual word embeddings,dimension,300
translation,196,153,hyperparameters,pretrained language embeddings,dimension,192
translation,196,153,hyperparameters,Hyperparameters,has,Pretrained multilingual word embeddings
translation,196,153,hyperparameters,Hyperparameters,has,pretrained language embeddings
translation,196,159,hyperparameters,Segmentation Sentence segmentation models,have,hidden layer dimension
translation,196,159,hyperparameters,hidden layer dimension,equal to,100
translation,196,159,hyperparameters,Hyperparameters,has,Segmentation Sentence segmentation models
translation,196,161,hyperparameters,sliding window width,of,100 words
translation,196,161,hyperparameters,overlap,between,adjacent windows
translation,196,161,hyperparameters,adjacent windows,is,30 words
translation,196,161,hyperparameters,Hyperparameters,has,sliding window width
translation,196,161,hyperparameters,Hyperparameters,has,overlap
translation,196,5,model,new joint transition - based parser,based on,Stack - LSTM framework
translation,196,5,model,new joint transition - based parser,based on,Arc-Standard algorithm
translation,196,5,model,Arc-Standard algorithm,handles,tokenization
translation,196,5,model,Arc-Standard algorithm,handles,part- of-speech tagging
translation,196,5,model,Arc-Standard algorithm,handles,morphological tagging
translation,196,5,model,Arc-Standard algorithm,handles,dependency parsing
translation,196,5,model,tokenization,in,one single model
translation,196,5,model,dependency parsing,in,one single model
translation,196,26,results,Our system,ranked,13th
translation,196,26,results,Our system,ranked,7th
translation,196,26,results,Our system,ranked,4th
translation,196,26,results,7th,for,low resource languages
translation,196,26,results,4th,in,sentence segmentation
translation,196,26,results,Results,has,Our system
translation,196,112,results,77.53 LAS,in average,single model output
translation,196,112,results,77.53 LAS,on,single model output
translation,196,112,results,single model output,of,58 treebanks dev set
translation,196,112,results,10 - model ensemble,improves,LAS
translation,196,112,results,10 - model ensemble,improves,LAS
translation,196,112,results,10 - model ensemble,improves,LAS
translation,196,112,results,LAS,to,78.75
translation,196,112,results,LAS,to,79.79
translation,196,112,results,LAS,to,79.79
translation,196,112,results,20 - model ensemble,improves,LAS
translation,196,112,results,LAS,to,79.79
translation,196,112,results,77.53 LAS,has,10 - model ensemble
translation,196,112,results,Results,From,77.53 LAS
translation,196,165,results,Our system,far from,best system
translation,196,165,results,substantially surpassed,far from,best system
translation,196,165,results,Our system,has,substantially surpassed
translation,196,165,results,substantially surpassed,has,baseline
translation,196,165,results,Results,has,Our system
translation,196,167,results,ko_gsd and ko_kaist,scores,17.61 and 13.56
translation,196,167,results,higher,than,baseline UDPipe 1.2
translation,196,167,results,17.61 and 13.56,has,higher
translation,196,167,results,Results,In,ko_gsd and ko_kaist
translation,196,169,results,overall tokenization score,is,97.30
translation,196,169,results,tokenization score,on,big treebanks
translation,196,169,results,big treebanks,is,99.24
translation,196,169,results,Results,has,overall tokenization score
translation,196,171,results,0.5,above,baseline
translation,196,171,results,0.36,below,top-ranking system
translation,196,171,results,4th,has,0.5
translation,196,171,results,Results,ranked,4th
translation,196,174,results,our system,outperformed,UDPipe
translation,196,174,results,our system,outperformed,UDPipe 1.2
translation,196,174,results,UDPipe,by,3.79
translation,196,174,results,UDPipe,by,3.99
translation,196,174,results,UDPipe 1.2,by,3.99
translation,196,174,results,la_projel,has,our system
translation,196,174,results,Results,for,la_projel
translation,197,204,experimental-setup,State size,of,both the encoder and the decoder
translation,197,204,experimental-setup,State size,of,word embedding size
translation,197,204,experimental-setup,State size,set to,100
translation,197,204,experimental-setup,both the encoder and the decoder,set to,100
translation,197,204,experimental-setup,word embedding size,set to,300
translation,197,204,experimental-setup,Experimental setup,has,State size
translation,197,204,experimental-setup,Experimental setup,has,word embedding size
translation,197,205,experimental-setup,Input and output dropout rate,of,GRU cells
translation,197,205,experimental-setup,GRU cells,are,0.7 and 0.5
translation,197,205,experimental-setup,mini-batch size,is,512
translation,197,205,experimental-setup,Experimental setup,has,Input and output dropout rate
translation,197,205,experimental-setup,Experimental setup,has,mini-batch size
translation,197,206,experimental-setup,Adam,with,default parameters
translation,197,206,experimental-setup,default parameters,for,optimization
translation,197,206,experimental-setup,Experimental setup,use,Adam
translation,197,207,experimental-setup,gradient clipping,with,cap
translation,197,207,experimental-setup,cap,for,global norm
translation,197,207,experimental-setup,global norm,at,5
translation,197,207,experimental-setup,Experimental setup,use,gradient clipping
translation,197,5,model,cross-domain semantic parsing,as,domain adaptation problem
translation,197,5,model,domain adaptation problem,train,semantic parser
translation,197,5,model,semantic parser,on,some source domains
translation,197,5,model,Model,formulate,cross-domain semantic parsing
translation,197,23,model,natural language,as,intermediate representation
translation,197,23,model,intermediate representation,for,crossdomain semantic parsing
translation,197,23,model,Model,use,natural language
translation,197,68,model,paraphrase model,based on,sequence - to-sequence ( Seq2Seq ) model
translation,197,68,model,paraphrase model,trained,end to end
translation,197,68,model,Model,propose,paraphrase model
translation,197,33,results,proposed standardization technique,lead to,about 10 % absolute improvement
translation,197,33,results,about 10 % absolute improvement,in,accuracy
translation,197,33,results,pretrained word embeddings,has,proposed standardization technique
translation,197,69,results,outperforms,by,large margin
translation,197,69,results,previous loglinear models,by,large margin
translation,197,69,results,large margin,in,in-domain setting
translation,197,69,results,outperforms,has,previous loglinear models
translation,197,213,results,Our base model ( Random + I ),achieves,accuracy
translation,197,213,results,accuracy,comparable to,previous best in- domain model
translation,197,213,results,Results,has,Our base model ( Random + I )
translation,197,214,results,our full model,able to outperform,previous best model
translation,197,214,results,our full model,achieve,best accuracy
translation,197,214,results,best accuracy,on,6 out of the 8 domains
translation,197,214,results,our main novelties,has,cross-domain training
translation,197,214,results,our main novelties,has,our full model
translation,197,214,results,cross-domain training,has,our full model
translation,197,214,results,word embedding standardization,has,our full model
translation,197,214,results,Results,With,our main novelties
translation,197,218,results,raw WORD2VEC vectors,with,per-example normalization
translation,197,218,results,significantly worse,than,random initialization
translation,197,218,results,raw WORD2VEC vectors,has,performance
translation,197,218,results,per-example normalization,has,performance
translation,197,218,results,random initialization,has,6.2 % and 7.3 %
translation,197,218,results,Results,Directly using,raw WORD2VEC vectors
translation,197,229,results,gain,of,cross-domain training
translation,197,229,results,cross-domain training,is,most significant
translation,197,229,results,most significant,when,indomain training data
translation,197,229,results,indomain training data,is,scarce
translation,197,229,results,Results,has,gain
translation,197,230,results,gain,becomes,smaller
translation,197,230,results,more in- domain training data,has,gain
translation,198,180,baselines,Local,is,na?ve argument identification strategy
translation,198,180,baselines,na?ve argument identification strategy,selects,best span
translation,198,180,baselines,best span,for,each role r
translation,198,180,baselines,best span,according to,score function
translation,198,180,baselines,Baselines,has,Local
translation,198,182,baselines,SEMAFOR,employs,greedy beam search
translation,198,182,baselines,greedy beam search,to eliminate,overlaps
translation,198,182,baselines,overlaps,between,predicted arguments
translation,198,182,baselines,Baselines,has,SEMAFOR
translation,198,185,baselines,"CPLEX , LP",uses,CPLEX
translation,198,185,baselines,"CPLEX , LP",to solve,relaxed LP
translation,198,185,baselines,CPLEX,to solve,relaxed LP
translation,198,185,baselines,Baselines,has,"CPLEX , LP"
translation,198,187,baselines,Baselines,has,"CPLEX , exact"
translation,198,188,baselines,"AD 3 , LP",LP version of,CPLEX
translation,198,188,baselines,"AD 3 , LP",where,relaxed problem
translation,198,188,baselines,relaxed problem,solved using,AD 3
translation,198,188,baselines,Baselines,has,"AD 3 , LP"
translation,198,6,model,dual decomposition algorithm,adapt for,exact decoding
translation,198,6,model,exact decoding,via,branch
translation,198,6,model,exact decoding,via,bound technique
translation,198,6,model,Model,employ,dual decomposition algorithm
translation,198,16,model,algorithm,finds,full collection of arguments
translation,198,16,model,full collection of arguments,given,semantic frame
translation,198,16,model,full collection of arguments,has,of a predicate
translation,198,20,model,optimization techniques,called,dual decomposition
translation,198,21,model,"modular , extensible , parallelizable approach",in which,semantic constraints
translation,198,21,model,semantic constraints,map not just to,declarative components
translation,198,21,model,procedural ones,in the form of,workers
translation,198,21,model,Model,derive,"modular , extensible , parallelizable approach"
translation,198,22,model,wrapping,in,branch - andbound search procedure
translation,198,22,model,algorithm,in,branch - andbound search procedure
translation,198,22,model,wrapping,has,algorithm
translation,198,7,results,baseline,makes,local predictions
translation,198,7,results,baseline,achieve,better argument identification scores
translation,198,7,results,local predictions,achieve,better argument identification scores
translation,198,7,results,local predictions,avoid,all structural violations
translation,199,116,ablation-analysis,Dependency trees,help,bit more
translation,199,116,ablation-analysis,bit more,achieving,0.6920
translation,199,116,ablation-analysis,Ablation analysis,has,Dependency trees
translation,199,5,baselines,Stack - LSTMs,to represent,our parser state
translation,199,5,baselines,Stack - LSTMs,make,decisions
translation,199,5,baselines,decisions,has,greedily
translation,199,80,experimental-setup,variant of the skip n-gram model,with,LDC English Gigaword corpus
translation,199,80,experimental-setup,Experimental setup,use,variant of the skip n-gram model
translation,199,4,model,Model,present,transition - based AMR parser
translation,199,20,model,parsing algorithm,makes use of,STACK
translation,199,20,model,STACK,that stores,AMR nodes and / or words
translation,199,20,model,BUFFER,contains,words
translation,199,20,model,words,has,that have yet to be processed
translation,199,20,model,Model,has,parsing algorithm
translation,199,118,model,- based algorithm,for,AMR parsing
translation,199,118,model,Model,implement it using,Stack - LSTMS
translation,199,16,results,results,incorporating,POS tags and dependency trees
translation,199,16,results,POS tags and dependency trees,into,our model
translation,199,16,results,Results,improve,results
translation,199,16,results,Results,incorporating,POS tags and dependency trees
translation,199,110,results,results,of,parser
translation,199,110,results,parser,without,character - based embeddings
translation,199,110,results,parser,with,pretrained word embeddings
translation,199,110,results,better,in,Newswire dataset
translation,199,110,results,1 point,in,full test set
translation,199,110,results,more,in,full test set
translation,199,110,results,2 points,has,better
translation,199,110,results,1 point,has,more
translation,199,110,results,Results,looking at,results
translation,199,110,results,Results,of,parser
translation,199,111,results,parser,with,character - based embeddings
translation,199,111,results,parser,without,pretrained word embeddings
translation,199,111,results,parser,achieves,0.61
translation,199,111,results,parser,achieves,0.61
translation,199,111,results,0.61,in,full test set
translation,199,111,results,parser,has,parser
translation,199,111,results,character - based embeddings,has,parser
translation,199,111,results,pretrained word embeddings,has,parser
translation,199,111,results,parser,has,more difficulty to learn
translation,199,111,results,Results,has,parser
translation,199,112,results,model,does not use,character - based embeddings
translation,199,112,results,model,does not use,pretrained word embeddings
translation,199,112,results,pretrained word embeddings,is,worst
translation,199,112,results,worst,achieving,only 0.59
translation,199,112,results,only 0.59,in,full test set
translation,199,112,results,Results,has,model
translation,199,114,results,parser,starts from,plain text sentences
translation,199,114,results,parser,incorporate,more information
translation,199,114,results,plain text sentences,incorporate,more information
translation,199,114,results,more information,into,our model
translation,199,115,results,POS tags,provide,small improvements
translation,199,115,results,0.6822,for,model
translation,199,115,results,model,runs with,POS tags
translation,199,115,results,small improvements,has,0.6801
translation,199,115,results,small improvements,has,without POS tags
translation,199,115,results,0.6801,has,without POS tags
translation,199,115,results,Results,has,POS tags
translation,200,95,baselines,?23,in,Penn Treebank
translation,200,82,hyperparameters,EM,for,concave models
translation,200,82,hyperparameters,concave models,for,100 iterations
translation,200,82,hyperparameters,Hyperparameters,run,EM
translation,200,87,hyperparameters,each DMV,for,200 iterations
translation,200,87,hyperparameters,each DMV,use,minimum Bayes risk decoding
translation,200,87,hyperparameters,minimum Bayes risk decoding,with,final model
translation,200,87,hyperparameters,final model,on,test data
translation,200,87,hyperparameters,Hyperparameters,train,each DMV
translation,200,88,hyperparameters,several initializers,for training,DMV
translation,200,88,hyperparameters,DMV,including,uniform initializer ( UNIF )
translation,200,88,hyperparameters,DMV,including,K&M
translation,200,88,hyperparameters,DMV,including,our trained concave models CCV1 and CCV2
translation,200,88,hyperparameters,Hyperparameters,use,several initializers
translation,200,18,model,simple concave models,for,dependency grammar induction
translation,200,18,model,Model,present,simple concave models
translation,200,98,results,DMV,initialized with,CCV2
translation,200,98,results,CCV2,achieves,substantial improvement
translation,200,98,results,substantial improvement,over,all others
translation,200,98,results,Results,has,DMV
translation,200,99,results,sentences,has,of length ? 20 words
translation,200,99,results,sentences,has,performance
translation,200,99,results,of length ? 20 words,has,performance
translation,200,99,results,),has,performance
translation,200,99,results,performance,has,rivals
translation,200,102,results,CCV2,performs,best
translation,200,102,results,CCV2,does,at least as well
translation,200,102,results,CCV1,does,at least as well
translation,200,102,results,at least as well,as,K&M
translation,200,102,results,Results,has,CCV2
translation,200,107,results,CCV2,leads to,substantiallyhigher likelihoods
translation,200,107,results,substantiallyhigher likelihoods,than,other initializers
translation,200,107,results,Results,has,CCV2
translation,201,182,ablation-analysis,Ablation analysis,empirically study,lexicalist / constructivist hypothesis
translation,201,146,experimental-setup,DyNet,to implement,neural models
translation,201,146,experimental-setup,automatic batch technique,in,DyNet
translation,201,146,experimental-setup,DyNet,to perform,mini-batch gradient descent training
translation,201,176,experimental-setup,different labels,for,syntactic trees
translation,201,176,experimental-setup,Experimental setup,try,different labels
translation,201,19,model,"high-quality , linguistically - informed SHRG",from,compositional semantic annotations
translation,201,19,model,"high-quality , linguistically - informed SHRG",dubbed,English Resource Semantics 1 ( ERS )
translation,201,19,model,compositional semantic annotations,licensed by,English Resource Grammar ( ERG
translation,201,19,model,inducing,has,"high-quality , linguistically - informed SHRG"
translation,201,20,model,robust SHRG parser,able to produce,semantic analysis
translation,201,20,model,semantic analysis,for,all sentences
translation,201,207,model,Model,introduce,neural SHRG - based semantic parser
translation,201,208,model,deep linguistic knowledge,partially in,symbolic way
translation,201,208,model,deep linguistic knowledge,partially in,statistical way
translation,201,208,model,Model,encode,deep linguistic knowledge
translation,201,4,results,SHRG - based parser,can produce,semantic graphs
translation,201,4,results,semantic graphs,by relating,synchronous production rules
translation,201,4,results,synchronous production rules,to,syntacto-semantic composition process
translation,201,4,results,semantic graphs,has,more accurately
translation,201,4,results,Results,demonstrate,SHRG - based parser
translation,201,5,results,Our parser,achieves,accuracy
translation,201,5,results,accuracy,of,90.35
translation,201,5,results,90.35,for,EDS ( 89.51 for DMRS )
translation,201,5,results,90.35,in terms of,ELEMENTARY DEPENDENCY MATCH
translation,201,5,results,EDS ( 89.51 for DMRS ),in terms of,ELEMENTARY DEPENDENCY MATCH
translation,201,5,results,ELEMENTARY DEPENDENCY MATCH,is,4.87 ( 5.45 ) point improvement
translation,201,5,results,4.87 ( 5.45 ) point improvement,over,best existing data-driven model
translation,201,5,results,Results,has,Our parser
translation,201,17,results,performance,achieved by,SHRG - based parser
translation,201,17,results,Results,show that,performance
translation,201,21,results,Our parser,achieves,accuracy
translation,201,21,results,accuracy,of,90.35
translation,201,21,results,accuracy,of,89.51
translation,201,21,results,90.35,for,EDS
translation,201,21,results,89.51,for,DMRS
translation,201,21,results,DMRS,in terms of,EL-EMENTARY DEPENDENCY MATCH ( EDM )
translation,201,21,results,best existing grammar-free model,", 2017 )",significant margin
translation,201,21,results,best existing grammar-free model,by,significant margin
translation,201,21,results,outperforms,has,best existing grammar-free model
translation,201,21,results,Results,has,Our parser
translation,201,177,results,Models,based on,coarsegrained labels
translation,201,177,results,Models,achieve,optimal performance
translation,201,178,results,test set,of,EDS data
translation,201,178,results,Results,on,test set
translation,201,179,results,state - of - the - art performance,with,remarkable improvement
translation,201,179,results,remarkable improvement,over,Buys and Blunsom ( 2017 ) 's neural parser
translation,201,179,results,Results,achieve,state - of - the - art performance
translation,202,16,model,reranking parser,takes advantage of,features
translation,202,16,model,features,based on,context
translation,202,16,model,context,surrounding,sentence
translation,202,16,model,Model,implementing,reranking parser
translation,202,17,results,similar model,does not make use of,context
translation,202,17,results,outperforms,has,generative baseline parser
translation,202,17,results,rivals,has,similar model
translation,202,142,results,generative baseline model,in terms of,F1
translation,202,142,results,reranked model,that uses,extra-sentential context
translation,202,142,results,version,does not use,extra-sentential context
translation,202,142,results,extra-sentential context,in,development set
translation,202,142,results,extra-sentential context,in,test set
translation,202,142,results,reranked models,has,outperform
translation,202,142,results,outperform,has,generative baseline model
translation,202,142,results,extra-sentential context,has,outperforms
translation,202,142,results,outperforms,has,version
translation,202,142,results,Results,see that,reranked models
translation,202,143,results,Bikel 's randomized parsing evaluation comparator,find that,both reranking models
translation,202,143,results,outperform,to,statistical significance
translation,202,143,results,statistical significance,for,recall and precision
translation,202,143,results,both reranking models,has,outperform
translation,202,143,results,outperform,has,baseline generative model
translation,202,143,results,Results,Using,Bikel 's randomized parsing evaluation comparator
translation,202,144,results,context-ignorant reranker,not,precision ( p = 0.42 )
translation,202,144,results,context - aware reranker,on,recall ( p < 0.01 )
translation,202,144,results,context - aware reranker,on,precision ( p = 0.42 )
translation,202,144,results,context-ignorant reranker,has,outperforms
translation,202,144,results,outperforms,has,context - aware reranker
translation,202,144,results,Results,has,context-ignorant reranker
translation,202,160,results,generative baseline,provides,higher level of performance
translation,202,164,results,"perfect context "" model",achieves,F1s
translation,202,164,results,F1s,of,90.42 % and 90.00 %
translation,202,164,results,Results,has,"perfect context "" model"
translation,203,106,ablation-analysis,five feature templates,resulted in,largest drop
translation,203,106,ablation-analysis,largest drop,to,performance
translation,203,106,ablation-analysis,largest drop,on,development set
translation,203,106,ablation-analysis,performance,on,development set
translation,203,106,ablation-analysis,Ablation analysis,present,five feature templates
translation,203,91,experiments,WEBQA,obtained,32.6 F 1
translation,203,91,experiments,32.6 F 1,compared to,40.9 F 1
translation,203,91,experiments,40.9 F 1,of,COMPQ
translation,203,92,experiments,candidate extraction step,finds,correct answer
translation,203,92,experiments,correct answer,in,top-K candidates
translation,203,92,experiments,top-K candidates,in,65.9 %
translation,203,92,experiments,top-K candidates,in,62.7 %
translation,203,92,experiments,65.9 %,of,development examples
translation,203,92,experiments,62.7 %,of,test examples
translation,203,93,experiments,test F 1,on,examples
translation,203,93,experiments,examples,for which,candidate extraction succeeded ( WEBQA - SUBSET )
translation,203,93,experiments,candidate extraction succeeded ( WEBQA - SUBSET ),is,"51.9 ( 53.4 p@1 , 67.5 MRR )"
translation,203,95,experiments,COMPQ,obtained,42.2 F 1
translation,203,95,experiments,42.2 F 1,on,test set
translation,203,95,experiments,42.2 F 1,compared to,40.9 F 1
translation,203,95,experiments,40.9 F 1,when training on,COM - PLEXQUESTIONS only
translation,203,17,model,simple log-linear model,in the spirit of,traditional web- based QA systems
translation,203,17,model,simple log-linear model,extracting,answer
translation,203,17,model,questions,by,querying
translation,203,17,model,answer,from,returned web snippets
translation,203,17,model,answers,has,questions
translation,203,17,model,querying,has,web
translation,203,17,model,Model,develop,simple log-linear model
translation,203,20,results,simple QA model,performs,reasonably well
translation,203,20,results,35 F 1,compared to,state - of- the- art
translation,203,20,results,Results,find that,simple QA model
translation,203,21,results,subset of questions,for which,right answer
translation,203,21,results,right answer,can be found in,one of the web snippets
translation,203,21,results,one of the web snippets,outperform,semantic parser
translation,203,21,results,Results,for,subset of questions
translation,203,22,results,superlatives and relation composition constructions,are,challenging
translation,203,22,results,challenging,for,webbased QA system
translation,203,22,results,conjunctions and events,are,easier
translation,203,22,results,Results,find that,superlatives and relation composition constructions
translation,203,96,results,predictions,to,subset
translation,203,96,results,subset,for which,candidate extraction
translation,203,96,results,F 1,of,COMPQ - SUBSET
translation,203,96,results,COMPQ - SUBSET,is,48.5
translation,203,96,results,48.5,is,3.4 F 1 points lower
translation,203,96,results,3.4 F 1 points lower,than,WEBQA - SUBSET
translation,203,96,results,predictions,has,F 1
translation,203,96,results,subset,has,F 1
translation,203,96,results,candidate extraction,has,succeeded
translation,203,96,results,Results,Restricting,predictions
translation,204,85,experimental-setup,word segmentation system,with,100 iterations
translation,204,85,experimental-setup,100 iterations,of,Maximum Entropy model
translation,204,85,experimental-setup,Maximum Entropy model,using,OpenNLP toolkit
translation,204,85,experimental-setup,Experimental setup,train,word segmentation system
translation,204,98,experimental-setup,lattice - based framework,set,maximum iteration
translation,204,98,experimental-setup,maximum iteration,as,K = 20
translation,204,98,experimental-setup,Experimental setup,For,lattice - based framework
translation,204,4,experiments,pipeline approach,suffers from,error propagation
translation,204,4,experiments,joint learning approach,suffers from,inefficient decoding
translation,204,4,experiments,inefficient decoding,due to,large combined search space
translation,204,4,experiments,Chinese word segmentation,has,pipeline approach
translation,204,4,experiments,POS tagging and parsing,has,pipeline approach
translation,204,5,model,novel lattice - based framework,in which,Chinese sentence
translation,204,5,model,Chinese sentence,segmented into,word lattice
translation,204,5,model,a lattice - based parser,used to process,lattice
translation,204,5,model,Model,present,novel lattice - based framework
translation,204,18,model,novel lattice - based framework,for,Chinese
translation,204,18,model,Model,present,novel lattice - based framework
translation,204,19,model,input Chinese sentence,segmented into,word lattice
translation,204,19,model,word lattice,is,compact representation
translation,204,19,model,compact representation,small set of,high-quality word segmentations
translation,204,19,model,Model,has,input Chinese sentence
translation,204,20,model,lattice - based POS tagger and a lattice - based parser,to process,word lattice
translation,204,20,model,word lattice,from,two different viewpoints
translation,204,87,results,our word segmentation system,comparable with,state - of - the - art systems
translation,204,87,results,upper bound F1 score,of,word lattice
translation,204,87,results,upper bound F1 score,exceeds,99.6 %
translation,204,87,results,Results,shows,our word segmentation system
translation,204,93,results,joint word segmentation and POS tagging performance,shows,our lattice - based POS tagger
translation,204,93,results,our lattice - based POS tagger,obtains,results
translation,204,93,results,comparable,with,state - of - the - art systems
translation,204,96,results,lattice - based parser,gets,better performance
translation,204,96,results,better performance,than,pipeline system
translation,204,96,results,better performance,among,all three subtasks
translation,204,96,results,Results,find,lattice - based parser
translation,204,101,results,lattice - based framework,achieves,improvement
translation,204,101,results,improvement,over,lattice - based parser
translation,204,101,results,0.16 points,for,word segmentation
translation,204,101,results,1.19 points,for,POS tagging
translation,204,101,results,1.65 points,for,parsing
translation,204,101,results,Results,shows,lattice - based framework
translation,204,102,results,lattice - based POS tagger,by,0.65 points
translation,204,102,results,0.65 points,on,POS tagging accuracy
translation,204,102,results,outperforms,has,lattice - based POS tagger
translation,204,102,results,Results,has,outperforms
translation,204,103,results,Our lattice - based framework,improves over,best joint inference parsing system
translation,204,103,results,best joint inference parsing system,by,0.57 points
translation,204,103,results,Results,has,Our lattice - based framework
translation,205,37,ablation-analysis,alignments,within,joint model
translation,205,37,ablation-analysis,alignments,is,beneficial
translation,205,37,ablation-analysis,Ablation analysis,inducing,alignments
translation,205,38,ablation-analysis,alignments,follow,standard approach
translation,205,38,ablation-analysis,alignments,produce them on,preprocessing
translation,205,38,ablation-analysis,drops,by,0.9 % Smatch
translation,205,38,ablation-analysis,alignments,has,performance
translation,205,38,ablation-analysis,standard approach,has,performance
translation,205,38,ablation-analysis,performance,has,drops
translation,205,38,ablation-analysis,Ablation analysis,instead of,alignments
translation,205,193,ablation-analysis,substantial drop,in,performance
translation,205,193,ablation-analysis,performance,indicates that,prior knowledge
translation,205,193,ablation-analysis,prior knowledge,about,nature of alignments
translation,205,193,ablation-analysis,prior knowledge,appears,beneficial
translation,205,193,ablation-analysis,nature of alignments,appears,beneficial
translation,205,194,ablation-analysis,additional regularizer,for,Gumbel - Sinkhorn approximation
translation,205,194,ablation-analysis,Ablation analysis,remove,additional regularizer
translation,205,195,ablation-analysis,performance drop,in,Smatch score ( ' No Sinkhorn reg ' )
translation,205,195,ablation-analysis,Smatch score ( ' No Sinkhorn reg ' ),is,moderate
translation,205,195,ablation-analysis,Ablation analysis,has,performance drop
translation,205,196,ablation-analysis,simple hierarchical relaxation,rather than,our softer version
translation,205,196,ablation-analysis,substantial drop,in,performance
translation,205,196,ablation-analysis,our softer version,has,of the loss
translation,205,196,ablation-analysis,Ablation analysis,using,simple hierarchical relaxation
translation,205,229,ablation-analysis,vanilla one- pass model,results in,1.4 % drop
translation,205,229,ablation-analysis,1.4 % drop,in,Smatch score
translation,205,229,ablation-analysis,Ablation analysis,Using,vanilla one- pass model
translation,205,173,experimental-setup,"Adam ( Kingma and Ba , 2014 )",to optimize,loss
translation,205,173,experimental-setup,"Adam ( Kingma and Ba , 2014 )",to train,root classifier
translation,205,174,experimental-setup,best model,trained,fully jointly
translation,205,174,experimental-setup,best model,do,early stopping
translation,205,174,experimental-setup,early stopping,on,development set scores
translation,205,174,experimental-setup,Experimental setup,has,best model
translation,205,175,experimental-setup,Training,takes,approximately 6 hours
translation,205,175,experimental-setup,approximately 6 hours,on,single GeForce GTX 1080 Ti
translation,205,175,experimental-setup,single GeForce GTX 1080 Ti,with,Intel Xeon CPU E5-2620 v4
translation,205,175,experimental-setup,Experimental setup,has,Training
translation,205,7,model,neural parser,treats,alignments
translation,205,7,model,alignments,as,latent variables
translation,205,7,model,latent variables,within,"joint probabilistic model of concepts , relations and alignments"
translation,205,7,model,Model,introduce,neural parser
translation,205,8,model,exact inference,use,continuous relaxation
translation,205,8,model,continuous relaxation,of,discrete alignments
translation,205,8,model,Model,use,variational autoencoding framework
translation,205,8,model,Model,has,exact inference
translation,205,26,model,continuous relaxation,of,alignment problem
translation,205,26,model,Model,use,continuous relaxation
translation,205,9,results,joint modeling,preferable to using,pipeline
translation,205,9,results,pipeline,of,align and parse
translation,205,9,results,Results,show,joint modeling
translation,205,178,results,substantially outperforms,on,both datasets
translation,205,178,results,all the previous models,on,both datasets
translation,205,178,results,Our model,has,substantially outperforms
translation,205,178,results,substantially outperforms,has,all the previous models
translation,205,178,results,Results,has,Our model
translation,206,6,baselines,basic tasks,including,tokenization
translation,206,6,baselines,basic tasks,including,lemmatization
translation,206,6,baselines,basic tasks,including,morphology prediction
translation,206,6,baselines,basic tasks,employ,official baseline model ( UDPipe )
translation,206,6,baselines,morphology prediction,employ,official baseline model ( UDPipe )
translation,206,79,experimental-setup,word vectors,with,50 - dimensional pretrained word embeddings
translation,206,79,experimental-setup,word vectors,with,100 - dimensional tag embeddings
translation,206,79,experimental-setup,word vectors,with,512 - dimensional recurrent states
translation,206,79,experimental-setup,Experimental setup,initialize,word vectors
translation,206,81,experimental-setup,"Adam ( Kingma and Ba , 2015 )",setting,learning rate
translation,206,81,experimental-setup,Experimental setup,optimize with,"Adam ( Kingma and Ba , 2015 )"
translation,206,82,experimental-setup,models,for,up to 100 epochs
translation,206,82,experimental-setup,models,with,batch size 32
translation,206,82,experimental-setup,models,occupying,2 to 3 GB graphic memory
translation,206,82,experimental-setup,up to 100 epochs,with,batch size 32
translation,206,82,experimental-setup,batch size 32,on,3 NVIDIA GeForce GTX 1080 Ti GPUs
translation,206,82,experimental-setup,3 NVIDIA GeForce GTX 1080 Ti GPUs,with,200 to 500 sentences per second
translation,206,82,experimental-setup,Experimental setup,train,models
translation,206,7,hyperparameters,low-resource languages,adopt,sampling method
translation,206,7,hyperparameters,sampling method,based on,other richresource languages
translation,206,7,hyperparameters,Hyperparameters,To train,low-resource languages
translation,206,5,model,Our system,predicts,part- of-speech tag and dependency tree
translation,206,5,model,part- of-speech tag and dependency tree,has,jointly
translation,206,5,model,Model,has,Our system
translation,206,80,model,Our system,drops,embeddings and hidden states independently
translation,206,80,model,embeddings and hidden states independently,with,33 % probability
translation,206,80,model,Model,has,Our system
translation,206,8,results,Our system,achieves,macro-average
translation,206,8,results,macro-average,of,68.31 %
translation,206,8,results,macro-average,with,improvement
translation,206,8,results,improvement,of,2.51 %
translation,206,8,results,2.51 %,compared with,UDPipe
translation,206,8,results,68.31 %,has,LAS F1 score
translation,206,8,results,Results,has,Our system
translation,206,22,results,our system,achieves,macro-average
translation,206,22,results,macro-average,of,68.31 %
translation,206,22,results,improvement,of,2.51 %
translation,206,22,results,UDPipe baseline,has,our system
translation,206,22,results,68.31 %,has,LAS F1 score
translation,206,84,results,absolute gains ( 1.28- 3.08 % ),on average,LAS
translation,206,84,results,absolute gains ( 1.28- 3.08 % ),on average,UAS
translation,206,84,results,absolute gains ( 1.28- 3.08 % ),on average,MLAS
translation,206,84,results,absolute gains ( 1.28- 3.08 % ),on average,CLAS
translation,206,84,results,Results,with,absolute gains ( 1.28- 3.08 % )
translation,206,85,results,our joint model,could improve,performance
translation,206,85,results,performance,of,universal dependency parsing
translation,206,85,results,Results,show,our joint model
translation,206,86,results,our joint model,obtains,lower averaged accuracy
translation,206,86,results,lower averaged accuracy,in,UPOS and XPOS
translation,206,86,results,POS tagging,has,our joint model
translation,206,86,results,Results,in the case of,POS tagging
translation,207,195,ablation-analysis,random forest mechanism,by,single greedy optimal decision tree
translation,207,195,ablation-analysis,single greedy optimal decision tree,for,probability computation
translation,207,195,ablation-analysis,reduced,to,86.3 % f-measure
translation,207,195,ablation-analysis,probability computation,has,performance
translation,207,195,ablation-analysis,Ablation analysis,replace,random forest mechanism
translation,207,4,model,syntactic parser,works,left-to - right and top down
translation,207,4,model,left-to - right and top down,maintaining,fully - connected parse tree
translation,207,4,model,fully - connected parse tree,for,few alternative parse hypotheses
translation,207,4,model,Model,present,syntactic parser
translation,207,12,model,syntactic parser,works,top-down and left-to - right
translation,207,12,model,fullyconnected parse tree,for,few alternative parse hypotheses
translation,207,12,model,Model,present,syntactic parser
translation,207,184,results,very significant improvement,over,original Roark parser
translation,207,184,results,very significant improvement,has,89.4 % vs.86.8 % )
translation,207,184,results,Results,has,middle group
translation,207,187,results,two models,perform,much better
translation,207,187,results,combined,has,two models
translation,208,142,ablation-analysis,performance,shows,effectiveness
translation,208,142,ablation-analysis,each component,to,our basic model
translation,208,142,ablation-analysis,effectiveness,of,attention mechanism
translation,208,142,ablation-analysis,effectiveness,of,tensor-based transformation function
translation,208,142,ablation-analysis,Ablation analysis,has,performance
translation,208,147,ablation-analysis,OOV word embeddings,has,performance
translation,208,147,ablation-analysis,performance,has,decreases slightly
translation,208,147,ablation-analysis,Ablation analysis,Without mapping,OOV word embeddings
translation,208,139,baselines,performance,of,basic hierarchical bidirectional LSTM network
translation,208,139,baselines,performance,of,tensor-based transformation ( TE )
translation,208,139,baselines,performance,of,handcrafted features ( HF )
translation,208,139,baselines,basic hierarchical bidirectional LSTM network,without,attention mechanism ( ATT )
translation,208,139,baselines,basic hierarchical bidirectional LSTM network,without,tensor-based transformation ( TE )
translation,208,139,baselines,basic hierarchical bidirectional LSTM network,without,handcrafted features ( HF )
translation,208,128,experimental-setup,dimension,of,word embeddings
translation,208,128,experimental-setup,dimension,of,POS embeddings
translation,208,128,experimental-setup,word embeddings,set to,50
translation,208,128,experimental-setup,POS embeddings,set to,10
translation,208,128,experimental-setup,Experimental setup,has,dimension
translation,208,129,experimental-setup,word embeddings,with,"GloVe ( Pennington et al. , 2014 )"
translation,208,129,experimental-setup,"GloVe ( Pennington et al. , 2014 )",on,English Gigaword
translation,208,129,experimental-setup,Experimental setup,pre-trained,word embeddings
translation,208,134,experimental-setup,Stanford CoreNLP toolkit,to preprocess,text
translation,208,134,experimental-setup,text,including,lemmatization
translation,208,134,experimental-setup,text,including,POS tagging
translation,208,134,experimental-setup,Experimental setup,use,Stanford CoreNLP toolkit
translation,208,135,experimental-setup,Theano library,to implement,our parsing model
translation,208,135,experimental-setup,Experimental setup,use,Theano library
translation,208,136,experimental-setup,all parameters,within,"( - 0.012 , 0.012 )"
translation,208,136,experimental-setup,"( - 0.012 , 0.012 )",except,word embeddings
translation,208,136,experimental-setup,Experimental setup,randomly initialize,all parameters
translation,208,137,experimental-setup,dropout strategy,to avoid,overfitting
translation,208,137,experimental-setup,dropout strategy,set,dropout rate
translation,208,137,experimental-setup,dropout rate,to be,0.3
translation,208,137,experimental-setup,Experimental setup,adopt,dropout strategy
translation,208,137,experimental-setup,Experimental setup,set,dropout rate
translation,208,119,hyperparameters,"Adadelta ( Zeiler , 2012 )",with,mini-batch
translation,208,119,hyperparameters,"Adadelta ( Zeiler , 2012 )",set,initial learning rate
translation,208,119,hyperparameters,mini-batch,to minimize,objective function
translation,208,119,hyperparameters,initial learning rate,to be,0.012
translation,208,119,hyperparameters,Hyperparameters,adopt,"Adadelta ( Zeiler , 2012 )"
translation,208,5,model,attention - based hierarchical neural network model,for,discourse parsing
translation,208,5,model,Model,propose,attention - based hierarchical neural network model
translation,208,6,model,tensor-based transformation function,to model,complicated feature interactions
translation,208,6,model,Model,incorporate,tensor-based transformation function
translation,208,20,model,hierarchical bidirectional Long Short - Term Memory ( bi- LSTM ) network,to learn,representations of text spans
translation,208,20,model,Model,propose to use,hierarchical bidirectional Long Short - Term Memory ( bi- LSTM ) network
translation,208,23,model,attention mechanism,to attend,over all EDUs
translation,208,23,model,over all EDUs,to pick up,prominent semantic information
translation,208,23,model,prominent semantic information,of,text span
translation,208,23,model,Model,apply,attention mechanism
translation,208,143,results,performance,is,competitive
translation,208,143,results,performance,still,competitive
translation,208,143,results,handcrafted features,has,performance
translation,208,143,results,Results,Even without,handcrafted features
translation,209,127,baselines,stack LSTM parsing model,without,POS tags ( ? POS )
translation,209,127,baselines,stack LSTM parsing model,without,pretrained language model embeddings ( ? pretraining )
translation,209,127,baselines,stack,instead of,composed representations ( ? composition )
translation,209,100,experimental-setup,computations,for,single parsing model
translation,209,100,experimental-setup,computations,run on,single thread
translation,209,100,experimental-setup,single thread,on,CPU
translation,209,100,experimental-setup,Experimental setup,has,computations
translation,209,104,experimental-setup,2 norm,of,gradient
translation,209,104,experimental-setup,5,before applying,weight update rule
translation,209,104,experimental-setup,Experimental setup,mitigate,effects
translation,209,105,experimental-setup,2 penalty,of,1 ? 10 ?6
translation,209,105,experimental-setup,2 penalty,applied to,all weights
translation,209,105,experimental-setup,1 ? 10 ?6,applied to,all weights
translation,209,105,experimental-setup,Experimental setup,has,2 penalty
translation,209,109,experimental-setup,LSTM hidden states,of size,100
translation,209,109,experimental-setup,LSTM hidden states,two layers of,LSTMs
translation,209,109,experimental-setup,LSTMs,for,each stack
translation,209,109,experimental-setup,Experimental setup,has,LSTM hidden states
translation,209,110,experimental-setup,Embeddings,of,parser actions
translation,209,110,experimental-setup,parser actions,used in,composition functions
translation,209,110,experimental-setup,composition functions,have,16 dimensions
translation,209,110,experimental-setup,output embedding size,is,20 dimensions
translation,209,110,experimental-setup,Experimental setup,has,Embeddings
translation,209,111,experimental-setup,Pretained word embeddings,have,100 dimensions ( English )
translation,209,111,experimental-setup,Pretained word embeddings,have,80 dimensions ( Chinese )
translation,209,111,experimental-setup,learned word embeddings,have,32 dimensions
translation,209,111,experimental-setup,Experimental setup,has,Pretained word embeddings
translation,209,111,experimental-setup,Experimental setup,has,learned word embeddings
translation,209,112,experimental-setup,Part of speech embeddings,have,12 dimensions
translation,209,112,experimental-setup,Experimental setup,has,Part of speech embeddings
translation,209,4,model,technique,has,for learning representations of parser states in transitionbased dependency parsers
translation,209,5,model,primary innovation,is,new control structure
translation,209,5,model,new control structure,sequence - to-sequence neural networksthe,stack LSTM
translation,209,5,model,Model,has,primary innovation
translation,209,6,model,pushed to or popped,in,constant time
translation,209,6,model,LSTM,maintains,continuous space embedding
translation,209,6,model,continuous space embedding,of,stack contents
translation,209,13,model,representations,of,parser state
translation,209,13,model,representations,sensitive to,complete contents of the parser 's state
translation,209,13,model,representations,sensitive to,complete contents of the stack of partially constructed syntactic structures
translation,209,13,model,learning,has,representations
translation,209,8,results,Standard backpropagation techniques,used for,training
translation,209,8,results,Standard backpropagation techniques,yield,state - of - the - art parsing performance
translation,209,8,results,Results,has,Standard backpropagation techniques
translation,209,130,results,parser,has,substantially outperforms
translation,209,130,results,substantially outperforms,has,baseline neural network parser
translation,209,130,results,Results,has,parser
translation,209,133,results,composed representations,of,dependency tree fragments
translation,209,133,results,Results,using,composed representations
translation,209,134,results,outperform,that use,only classical RNNs
translation,209,134,results,outperform,quite capable of learning,good representations
translation,209,134,results,baselines,that use,only classical RNNs
translation,209,134,results,LSTMs,has,outperform
translation,209,134,results,outperform,has,baselines
translation,210,17,baselines,lexical actions,has,compete
translation,210,6,model,conventional action- level beam search,used for,discriminative neural models
translation,210,6,model,discriminative neural models,decode directly in,generative models
translation,210,6,model,Model,alternative to,conventional action- level beam search
translation,210,19,model,enhanced candidate selection strategy,yields,significant improvements
translation,210,19,model,significant improvements,for,all beam sizes
translation,210,19,model,Model,propose,enhanced candidate selection strategy
translation,210,21,results,final search procedure,surpass,prior state- ofthe - art results
translation,210,21,results,prior state- ofthe - art results,among,single-model parsers
translation,210,21,results,prior state- ofthe - art results,obtaining,F1 score
translation,210,21,results,single-model parsers,on,Penn Treebank
translation,210,21,results,F1 score,of,92.56
translation,210,57,results,Word - level search,with,k w = k/10
translation,210,57,results,search without a bottleneck,at,all beam sizes
translation,210,57,results,Word - level search,has,consistently outperforms
translation,210,57,results,k w = k/10,has,consistently outperforms
translation,210,57,results,consistently outperforms,has,search without a bottleneck
translation,210,57,results,Results,has,Word - level search
translation,210,67,results,fast-tracked candidates,offers,significant gains
translation,210,67,results,significant gains,under,all settings
translation,210,67,results,Results,Note,fast-tracked candidates
translation,210,67,results,Results,use of,fast-tracked candidates
translation,210,68,results,top result,improves from,92.93
translation,210,68,results,top result,use of,fasttracked candidates
translation,210,68,results,92.93,use of,fasttracked candidates
translation,210,68,results,fasttracked candidates,surpassing,prior single -model systems
translation,210,68,results,prior single -model systems,on,development set
translation,210,68,results,92.93,has,to 93.18
translation,210,68,results,Results,has,top result
translation,210,96,results,performance,on par with,unpruned setup
translation,210,96,results,Results,observe,performance
translation,210,122,results,F1 scores,of,92.56
translation,210,122,results,F1 scores,of,92.53
translation,210,122,results,92.56,on,test set
translation,210,122,results,92.56,without,pruning
translation,210,122,results,92.53,when,1 ? 8/26 ? 69.2 % of OPEN successors are pruned
translation,210,122,results,92.53,obtaining,performance
translation,210,122,results,performance,well above,previous state - of - the - art scores
translation,210,122,results,previous state - of - the - art scores,for,single -model parsers
translation,210,122,results,Results,achieve,F1 scores
translation,211,33,ablation-analysis,joint learning,of,several pre-training tasks
translation,211,33,ablation-analysis,Ablation analysis,explore,joint learning
translation,211,139,ablation-analysis,drop,of,Smatch F1 score
translation,211,139,ablation-analysis,Smatch F1 score,from,71.5 to 70.0
translation,211,37,baselines,AMR Parsing,as,Seq2Seq Learning Seq2Seq Modeling
translation,211,50,baselines,PTM - MT,is,seq2seq neural machine translation ( NMT ) model
translation,211,50,baselines,seq2seq neural machine translation ( NMT ) model,trained on,publicly available bilingual dataset
translation,211,50,baselines,Baselines,has,PTM - MT
translation,211,97,experimental-setup,number of layers,in,encoder and decoder
translation,211,97,experimental-setup,number of layers,is,6
translation,211,97,experimental-setup,encoder and decoder,is,6
translation,211,97,experimental-setup,number of heads,is,8
translation,211,97,experimental-setup,Experimental setup,has,number of layers
translation,211,97,experimental-setup,Experimental setup,has,number of heads
translation,211,98,experimental-setup,embedding size and the hidden size,are,512
translation,211,98,experimental-setup,size of feedforward network,is,2048
translation,211,98,experimental-setup,Experimental setup,has,embedding size and the hidden size
translation,211,98,experimental-setup,Experimental setup,has,size of feedforward network
translation,211,99,experimental-setup,Adam optimizer,? 1 of,0.9 and ? 2 of 0.998
translation,211,99,experimental-setup,Experimental setup,use,Adam optimizer
translation,211,100,experimental-setup,label smoothing epsilon,are,"16000 , 2.0 , 0.1 and 0.1"
translation,211,100,experimental-setup,Warm up step,has,learning rate
translation,211,100,experimental-setup,Warm up step,has,dropout rate
translation,211,100,experimental-setup,Experimental setup,has,Warm up step
translation,211,101,experimental-setup,batch token -size,to,"8,192"
translation,211,101,experimental-setup,Experimental setup,set,batch token -size
translation,211,102,experimental-setup,models,for,300K steps
translation,211,102,experimental-setup,best performance,on,WMT2014 Englishto - German development set
translation,211,102,experimental-setup,Experimental setup,train,models
translation,211,7,experiments,seq2seq pre-training approach,to build,pre-trained models
translation,211,7,experiments,pre-trained models,in,single and joint way
translation,211,7,experiments,single and joint way,on,three relevant tasks
translation,211,114,experiments,pre-trained model,of,NMT
translation,211,114,experiments,NMT,achieves,best performance
translation,211,114,experiments,best performance,followed by,pre-trained models
translation,211,114,experiments,pre-trained models,on,AMR parsing
translation,211,114,experiments,pre-trained models,on,syntactic parsing
translation,211,8,model,vanilla fine-tuning method,to,multi-task learning fine-tuning method
translation,211,8,model,multi-task learning fine-tuning method,optimizes for,performance
translation,211,8,model,performance,of,AMR parsing
translation,211,8,model,response,of,pre-trained models
translation,211,8,model,Model,extend,vanilla fine-tuning method
translation,211,34,model,vanilla fine-tuning method,to optimize for,performance
translation,211,34,model,vanilla fine-tuning method,to optimize for,response preservation
translation,211,34,model,performance,of,AMR parsing
translation,211,34,model,response preservation,of,pre-trained models
translation,211,34,model,Model,extend,vanilla fine-tuning method
translation,211,38,model,encoder,consists of,stack of multiple identical layers
translation,211,38,model,one,implements,multi-head selfattention mechanism
translation,211,38,model,one,other is,positionwise fully connected feed -forward network
translation,211,38,model,Model,has,encoder
translation,211,39,model,decoder,composed of,stack of multiple identical layers
translation,211,39,model,Model,has,decoder
translation,211,40,model,encoder layers,plus,additional sub-layer
translation,211,40,model,additional sub-layer,performs,multihead attention
translation,211,40,model,multihead attention,output of,encoder stack
translation,211,35,results,experimentation,on,two widely used English benchmarks
translation,211,35,results,two widely used English benchmarks,shows,our approach
translation,211,35,results,our approach,has,substantially improves
translation,211,35,results,substantially improves,has,performance
translation,211,113,results,performance,of,AMR parsing
translation,211,113,results,significantly improve,has,performance
translation,211,116,results,Joint pre-trained models,on,two or more pre-training tasks
translation,211,116,results,Joint pre-trained models,improve,performance
translation,211,116,results,two or more pre-training tasks,improve,performance
translation,211,116,results,performance,of,AMR parsing
translation,211,116,results,Results,has,Joint pre-trained models
translation,211,126,results,best performance,for,Reentrancies
translation,211,126,results,best performance,for,NER
translation,211,126,results,best performance,for,SRL
translation,211,126,results,Results,obtain,best performance
translation,211,127,results,lower performance,for,Wiki and Negations
translation,211,140,results,four different methods,of incorporating,BERT
translation,211,140,results,four different methods,result in,very different performance
translation,211,140,results,very different performance,ranging from,71.5 to 75.2
translation,211,140,results,71.5 to 75.2,in,Smatch F1 score
translation,211,141,results,BERT,as,embedding or extra feature
translation,211,141,results,BERT,achieves,similar performance
translation,211,141,results,similar performance,much higher than,performance
translation,211,141,results,Results,incorporating,BERT
translation,211,143,results,outperform,using,BERT
translation,211,143,results,pre-trained seq2seq models,has,outperform
translation,211,143,results,Results,has,pre-trained seq2seq models
translation,211,154,results,performance AMR parsing,from,77.9 )
translation,211,154,results,performance AMR parsing,71.5 to,75.3
translation,211,154,results,performance AMR parsing,in,Smatch F1 score
translation,211,154,results,PTM - SynPar ( or PTM - SemPar ),has,significantly improves
translation,211,154,results,significantly improves,has,performance AMR parsing
translation,211,154,results,Results,pre-trained model of,PTM - SynPar ( or PTM - SemPar )
translation,211,155,results,joint pre-training,with either,PTM - SynPar
translation,211,155,results,joint pre-training,with either,PTM - SemPar
translation,211,155,results,joint pre-training,gives another,up to 1.0 improvement
translation,211,155,results,in the presence of PTM - MT,has,joint pre-training
translation,211,166,results,pre-training,on,EN - FR dataset
translation,211,166,results,pre-training,achieves,even slight higher performance
translation,211,166,results,EN - FR dataset,achieves,even slight higher performance
translation,211,166,results,even slight higher performance,than,EN - DE dataset
translation,211,166,results,Results,observe that,pre-training
translation,212,131,ablation-analysis,diversity pruning,gets most of its wins,difficult attachment decisions
translation,212,131,ablation-analysis,Ablation analysis,has,diversity pruning
translation,212,97,experiments,Penn-S-3.3.0,with,gold POS tags
translation,212,97,experiments,Penn-S-3.3.0,with,gold tags
translation,212,97,experiments,Penn-S-3.3.0,evaluated with,non-gold ( Stanford tagger )
translation,212,97,experiments,Penn-S-3.3.0,evaluated with,gold tags
translation,212,107,experiments,Penn-S-3.3.0 results,see that,our model
translation,212,107,experiments,competitive,with,Berkeley constituency parser
translation,212,107,experiments,our model,has,outperforms
translation,212,107,experiments,outperforms,has,Tur-boPaser
translation,212,4,model,inference,to maintain,label and structural ambiguity
translation,212,4,model,Model,extend,cube-pruned dependency parsing framework
translation,212,24,model,primary ( entire ) beam,into,disjoint groups
translation,212,24,model,disjoint groups,according to,identity of unlabeled structure
translation,212,24,model,Model,partition,primary ( entire ) beam
translation,212,25,model,size,of,secondary beam
translation,212,25,model,size,restrict,label ambiguity
translation,212,25,model,structural diversity,within,primary beam
translation,212,25,model,Model,limiting,size
translation,212,105,results,slight improvement,for,Penn - YM
translation,212,108,results,gold tags,assumed,cube-pruning
translation,212,108,results,gold tags,has,cube-pruning
translation,212,108,results,cube-pruning,has,significantly outperforms
translation,212,108,results,significantly outperforms,has,Berkeley
translation,212,117,results,accuracy improvements,are,consistent
translation,212,117,results,consistent,across,development set and the test set
translation,212,117,results,development set and the test set,for,all three data sets
translation,212,122,results,enlarging,does not recover,wins
translation,212,122,results,wins,of,structural diversity
translation,212,122,results,structural diversity,on,Penn-S-2.0.5 and CTB - 5
translation,212,122,results,enlarging,has,beam
translation,212,122,results,Results,indicates,enlarging
translation,213,195,ablation-analysis,average training time per epoch,for,PP
translation,213,195,ablation-analysis,average training time per epoch,for,PP
translation,213,195,ablation-analysis,PP,is,"1,794 seconds"
translation,213,195,ablation-analysis,"1,794 seconds",compared to,"2,021"
translation,213,195,ablation-analysis,"2,021",for,vanilla
translation,213,195,ablation-analysis,"2,021",leads to,10 % reduction
translation,213,195,ablation-analysis,10 % reduction,in,decoding time
translation,213,195,ablation-analysis,Ablation analysis,has,average training time per epoch
translation,213,182,baselines,two state- ofthe - art slot-filling DST models,based on,encoderdecoders
translation,213,182,baselines,encoderdecoders,include,"COMER ( Ren et al. , 2019 )"
translation,213,182,baselines,encoderdecoders,include,previous user dialog state
translation,213,182,baselines,"COMER ( Ren et al. , 2019 )",encodes,previous system response transcription
translation,213,182,baselines,"COMER ( Ren et al. , 2019 )",encodes,previous user dialog state
translation,213,182,baselines,"COMER ( Ren et al. , 2019 )",decodes,slot values
translation,213,182,baselines,"TRADE ( Wu et al. , 2019 )",encodes,all utterances
translation,213,182,baselines,Baselines,introduce,two state- ofthe - art slot-filling DST models
translation,213,6,experiments,27 k conversations,annotated with,tree-structured dialog states and system acts
translation,213,5,model,DST,as,semantic parsing task
translation,213,5,model,DST,incorporate,semantic compositionality
translation,213,5,model,DST,incorporate,crossdomain knowledge sharing
translation,213,5,model,DST,incorporate,co-reference
translation,213,5,model,semantic parsing task,over,hierarchical representations
translation,213,5,model,Model,formulating,DST
translation,213,42,model,conversational semantic parser,tackles,DST
translation,213,43,model,current user utterance and representations,of,dialog history
translation,213,43,model,decoder,generates,updated dialog state
translation,213,43,model,updated dialog state,mixture of,generation and copy mechanism
translation,213,44,model,practical conversations,with,intent switching and resumption
translation,213,44,model,practical conversations,adopt,"stack ( Rudnicky and Xu , 1999 )"
translation,213,44,model,"stack ( Rudnicky and Xu , 1999 )",to represent,multiple tasks
translation,213,44,model,multiple tasks,in,dialog history
translation,213,44,model,parent-pointer decoder,to speed up,decoding
translation,213,44,model,Model,to track,practical conversations
translation,213,44,model,Model,adopt,"stack ( Rudnicky and Xu , 1999 )"
translation,213,8,results,encoder-decoder framework,DST with,hierarchical representations
translation,213,8,results,encoder-decoder framework,leads to,20 % improvement
translation,213,8,results,20 % improvement,over,state - of - the - art DST approaches
translation,213,8,results,20 % improvement,operate on,flat meaning space
translation,213,8,results,state - of - the - art DST approaches,operate on,flat meaning space
translation,213,8,results,flat meaning space,of,slot-value pairs
translation,213,8,results,Results,describe,encoder-decoder framework
translation,213,46,results,our approach,leads to,20 % improvement
translation,213,46,results,20 % improvement,over,stateof - the- art DST approaches
translation,213,46,results,stateof - the- art DST approaches,operate on,flat meaning space
translation,213,46,results,Results,has,our approach
translation,213,191,results,TED - FLAT,has,slightly outperforms
translation,213,191,results,slightly outperforms,has,COMER and TRADE
translation,213,191,results,Results,has,TED - FLAT
translation,213,194,results,vanilla and PP decoders,achieve,same exact match accuracy
translation,213,194,results,Results,has,vanilla and PP decoders
translation,214,126,ablation-analysis,ELMo,in,tagger
translation,214,126,ablation-analysis,ELMo,leads to,macro-averaged improvement
translation,214,126,ablation-analysis,ELMo,leads to,macro- averaged error reduction
translation,214,126,ablation-analysis,macro-averaged improvement,of,0.56 %
translation,214,126,ablation-analysis,0.56 %,in,UPOS
translation,214,126,ablation-analysis,macro- averaged error reduction,is,17.83 %
translation,214,126,ablation-analysis,Ablation analysis,Using,ELMo
translation,214,135,ablation-analysis,Parser ensemble,leads to,averaged improvement
translation,214,135,ablation-analysis,Parser ensemble,leads to,averaged error reduction
translation,214,135,ablation-analysis,averaged improvement,of,0.55 %
translation,214,135,ablation-analysis,0.55 %,in,LAS
translation,214,135,ablation-analysis,averaged error reduction,is,4.0 %
translation,214,135,ablation-analysis,Ablation analysis,has,Parser ensemble
translation,214,153,ablation-analysis,concatenation,contributes to,improvements
translation,214,153,ablation-analysis,some treebanks,has,concatenation
translation,214,153,ablation-analysis,Ablation analysis,For,some treebanks
translation,214,104,experimental-setup,100 - dimensional pretrained word embeddings,released by,shared task
translation,214,104,experimental-setup,shared task,for,large languages
translation,214,104,experimental-setup,Experimental setup,use,100 - dimensional pretrained word embeddings
translation,214,108,experimental-setup,same hyperparameter settings,for,BiLSTM ( LM )
translation,214,108,experimental-setup,same hyperparameter settings,for,character CNN
translation,214,108,experimental-setup,Experimental setup,use,same hyperparameter settings
translation,214,115,experimental-setup,ELMo,use,dropout
translation,214,115,experimental-setup,dropout,of,33 %
translation,214,115,experimental-setup,33 %,on,projected vectors
translation,214,115,experimental-setup,Experimental setup,trained with,ELMo
translation,214,112,experiments,ELMo,on,one language
translation,214,112,experiments,ELMo,takes,roughly 3 days
translation,214,112,experiments,roughly 3 days,on,NVIDIA P100 GPU
translation,214,91,model,character ELMo,into,our tokenizer
translation,214,91,model,Model,incorporate,character ELMo
translation,214,8,results,our system,ranked,outperformed
translation,214,8,results,outperformed,has,other systems
translation,214,127,results,ELMo,in,parser
translation,214,127,results,ELMo,leads to,macro-averaged improvement
translation,214,127,results,ELMo,leads to,macro- averaged error reduction
translation,214,127,results,macro-averaged improvement,of,0.84 %
translation,214,127,results,0.84 %,in,LAS
translation,214,127,results,macro- averaged error reduction,is,7.88 %
translation,214,127,results,Results,Using,ELMo
translation,214,128,results,ELMo,improves,tagging performance
translation,214,128,results,tagging performance,almost on,every treebank
translation,214,128,results,tagging performance,except for,gl ctg
translation,214,128,results,every treebank,except for,gl ctg
translation,214,128,results,Results,has,ELMo
translation,214,136,results,ensemble,is,effective way
translation,214,136,results,effective way,to improve,parsing performance
translation,214,136,results,Results,indicate,ensemble
translation,214,137,results,relationship,between,gains
translation,214,137,results,relationship,between,treebank size
translation,214,137,results,gains,using,ensemble
translation,214,137,results,gains,using,ensemble
translation,214,137,results,small treebank,benefit more from,ensemble
translation,214,137,results,Results,has,relationship
translation,214,143,results,general trend,is,treebank
translation,214,143,results,general trend,is,cross-domain concatenation
translation,214,143,results,general trend,for,treebank
translation,214,143,results,treebank,with,small training set
translation,214,143,results,cross-domain concatenation,achieves,better performance
translation,214,143,results,general trend,has,cross-domain concatenation
translation,214,143,results,treebank,has,cross-domain concatenation
translation,214,143,results,small training set,has,cross-domain concatenation
translation,214,143,results,Results,has,general trend
translation,214,144,results,concatenation,does not improve,performance
translation,214,144,results,those with large training set,has,concatenation
translation,214,144,results,worsen,has,results
translation,214,144,results,Results,for,those with large training set
translation,214,154,results,"Parsing Japanese , Vietnamese , and Chinese",benefits from,better word segmentation
translation,214,154,results,Results,has,"Parsing Japanese , Vietnamese , and Chinese"
translation,214,157,results,Our system without ensemble,achieves,macroaveraged LAS
translation,214,157,results,macroaveraged LAS,of,75.26
translation,214,157,results,first,according to,LAS
translation,214,157,results,LAS,in,shared task
translation,214,157,results,Results,has,Our system without ensemble
translation,215,61,baselines,Baselines,has,Experiment # 2 : Lexicalization Baselines
translation,215,4,results,categories,induced by,unsupervised word clustering
translation,215,4,results,gold part- of-speech tags,in,dependency grammar induction
translation,215,4,results,Results,show that,categories
translation,215,113,results,our new system 's performance,degrades,only slightly
translation,215,113,results,only slightly,by,0.2 %
translation,215,113,results,accuracy,has,"from switching to unsupervised tags ( e.g. , 2.6 % )"
translation,215,171,results,gold parts - of-speech,in,supervised modes
translation,215,171,results,outperform,has,gold parts - of-speech
translation,215,188,results,Clark 's ( 2000 ) flat clustering,using,contextual cues
translation,215,188,results,Results,relaxing,Clark 's ( 2000 ) flat clustering
translation,215,219,results,our context-sensitive system without gold tags,scores,66.8 %
translation,215,219,results,our context-sensitive system without gold tags,has,)
translation,216,205,experimental-setup,training,use,"Adam ( Kingma and Ba , 2015 )"
translation,216,205,experimental-setup,Experimental setup,For,training
translation,216,5,model,Efficient Training,by realizing,stack LSTM parallel training
translation,216,5,model,Effective Encoding,via adopting,"deep contextualized word embeddings BERT ( Devlin et al. , 2019 )"
translation,216,5,model,Model,extended,basic transition - based parser
translation,216,226,model,unified pipeline,for,meaning representation parsing
translation,216,226,model,unified pipeline,suitable for,main stream graphbanks
translation,216,226,model,Model,proposed,unified pipeline
translation,216,7,results,our system,ranked,first
translation,216,7,results,first,according to,ALL - F1 ( 86.2 % )
translation,216,216,results,Our parser,benefits a lot from,BERT
translation,216,216,results,BERT,compared with,GloVe
translation,216,216,results,Results,has,Our parser
translation,217,6,model,reinforcement learning,to,greedy dependency parsing
translation,217,6,model,Model,apply,reinforcement learning
translation,217,7,results,Reinforcement learning,improves,accuracy
translation,217,7,results,accuracy,of,labeled and unlabeled dependencies
translation,217,7,results,accuracy,of,high performance greedy parser
translation,217,7,results,accuracy,both,labeled and unlabeled dependencies
translation,217,7,results,accuracy,both,high performance greedy parser
translation,217,7,results,accuracy,of,high performance greedy parser
translation,217,7,results,labeled and unlabeled dependencies,of,Stanford Neural Dependency Parser
translation,217,7,results,Results,has,Reinforcement learning
translation,217,23,results,Reinforcement learning,increased,both unlabeled and labeled accuracy
translation,217,23,results,both unlabeled and labeled accuracy,on,Penn TreeBank and German part of SPMRL
translation,217,23,results,Results,has,Reinforcement learning
translation,218,16,model,error classes,augment,system
translation,218,16,model,system,to recognize,Chinese-specific parse errors
translation,218,16,model,Model,To accommodate,error classes
translation,218,16,model,Model,augment,system
translation,219,151,ablation-analysis,additional features,significantly improve,performance
translation,219,151,ablation-analysis,additional features,significantly improve,performance
translation,219,151,ablation-analysis,additional features,improve,performance
translation,219,151,ablation-analysis,performance,by,11.66 %
translation,219,151,ablation-analysis,performance,by,11.07 %
translation,219,151,ablation-analysis,performance,in,accuracy
translation,219,151,ablation-analysis,performance,by,11.07 %
translation,219,151,ablation-analysis,11.66 %,in,accuracy
translation,219,151,ablation-analysis,11.66 %,in,accuracy
translation,219,151,ablation-analysis,11.66 %,in,accuracy
translation,219,151,ablation-analysis,accuracy,from,74.93 % to 86.59 %
translation,219,151,ablation-analysis,74.93 % to 86.59 %,for,negation scope identification
translation,219,151,ablation-analysis,74.93 % to 86.59 %,for,speculation scope identification
translation,219,151,ablation-analysis,performance,by,11.07 %
translation,219,151,ablation-analysis,11.07 %,in,accuracy
translation,219,151,ablation-analysis,11.07 %,from,77.29 % to 88.36 %
translation,219,151,ablation-analysis,77.29 % to 88.36 %,for,speculation scope identification
translation,219,151,ablation-analysis,Ablation analysis,shows,additional features
translation,219,152,ablation-analysis,"CFACCP5 , AC2W , CFCP1 )",related to,neighboring words
translation,219,152,ablation-analysis,neighboring words,of,cue
translation,219,152,ablation-analysis,neighboring words,play,critical role
translation,219,152,ablation-analysis,critical role,for,negation and speculation scope identification
translation,219,174,ablation-analysis,automatic syntactic parsing,lowers,performance
translation,219,174,ablation-analysis,performance,on,abstracts subcorpus
translation,219,174,ablation-analysis,negation and speculaiton scope identification,has,automatic syntactic parsing
translation,219,174,ablation-analysis,Ablation analysis,For,negation and speculaiton scope identification
translation,219,199,ablation-analysis,recall,of,speculation cue recognition
translation,219,199,ablation-analysis,speculation cue recognition,on,clinical reports subcorpus
translation,219,199,ablation-analysis,speculation cue recognition,is,very low
translation,219,144,hyperparameters,SVMLight,selected as,our classifier
translation,219,144,hyperparameters,Hyperparameters,has,SVMLight
translation,219,4,model,scope learning problem,via,simplified shallow semantic parsing
translation,219,4,model,Model,approaches,scope learning problem
translation,219,62,results,10 - fold cross-validation,on,GTB1.0
translation,219,62,results,GTB1.0,shows,parser
translation,219,62,results,parser,achieves,performance
translation,219,62,results,performance,of,86.57
translation,219,62,results,Results,has,10 - fold cross-validation
translation,219,158,results,speculation scope identification,achieves,higher performance
translation,219,158,results,higher performance,than,negation scope identification
translation,219,158,results,golden parse trees and golden cues,has,speculation scope identification
translation,219,158,results,Results,given,golden parse trees and golden cues
translation,219,166,results,abstracts subcorpus,using,10 - fold cross-validation
translation,219,166,results,10 - fold cross-validation,shows that,simple postprocessing rule
translation,219,166,results,simple postprocessing rule,gets,performance
translation,219,166,results,performance,of,80.59 and 86.08
translation,219,166,results,80.59 and 86.08,in,accuracy
translation,219,166,results,accuracy,for,negation and speculation scope identification
translation,219,177,results,speculation scope identification,consistently achieves,higher performance
translation,219,177,results,higher performance,than,negaiton scope identification
translation,219,177,results,negaiton scope identification,when,golden parse trees
translation,219,177,results,golden parse trees,are,availabe
translation,219,177,results,speculation scope identification,achieves,comparable performance
translation,219,177,results,comparable performance,with,negation scope identification
translation,219,177,results,comparable performance,with,negation scope identification
translation,219,177,results,negation scope identification,on,abstracts subcorpus and the full papers subcorpus
translation,219,177,results,negation scope identification,on,clinical report subcorpus
translation,219,177,results,negation scope identification,on,clinical report subcorpus
translation,219,177,results,speculation scope identification,performs,~ 20 % lower
translation,219,177,results,negation scope identification,on,clinical report subcorpus
translation,219,179,results,performance gap,between,performance
translation,219,179,results,performance gap,between,oracle performance
translation,219,179,results,performance,of,our scope finder
translation,219,179,results,performance,of,oracle performance
translation,219,179,results,Results,Given,performance gap
translation,219,183,results,our baseline system,with,four basic features
translation,219,183,results,our baseline system,achieves,comparable performance
translation,219,183,results,comparable performance,with,Morante et al . ( 2008 ) and Daelemans ( 2009a & 2009 b )
translation,219,183,results,Results,shows,our baseline system
translation,219,185,results,state - of - the - art ones,using,chunking approach
translation,219,185,results,chunking approach,especially on,abstracts and full papers subcorpora
translation,219,185,results,our final system,has,significantly outperforms
translation,219,185,results,significantly outperforms,has,state - of - the - art ones
translation,219,185,results,Results,shows,our final system
translation,219,186,results,improvement,on,clinical reports subcorpora
translation,219,186,results,clinical reports subcorpora,for,negation scope identification
translation,219,186,results,clinical reports subcorpora,is,much less apparent
translation,219,186,results,negation scope identification,is,much less apparent
translation,219,186,results,Results,has,improvement
translation,219,187,results,our parsing approach,to,speculation scope identification
translation,219,187,results,speculation scope identification,has,outperforms
translation,219,187,results,outperforms,has,rule- based method
translation,219,187,results,Results,shows,our parsing approach
translation,219,196,results,negation cue recognition,higher than,speculation cue recognition
translation,219,196,results,speculation cue recognition,on,all the three subcorpora
translation,220,212,baselines,Baselines,compares,UDEPLAMBDA
translation,220,7,model,semantic interface,for,UD
translation,220,7,model,natural language,to,logical forms
translation,220,7,model,natural language,can process,dependency graphs
translation,220,7,model,logical forms,in,almost language - independent fashion
translation,220,7,model,UDEPLAMBDA,has,semantic interface
translation,220,7,model,Model,introduce,UDEPLAMBDA
translation,220,15,model,semantic interface,for,UD
translation,220,15,model,semantic interface,maps,natural language to logical forms
translation,220,15,model,natural language to logical forms,representing,underlying predicate - argument structures
translation,220,15,model,natural language to logical forms,in,almost language - independent manner
translation,220,15,model,underlying predicate - argument structures,in,almost language - independent manner
translation,220,15,model,UDEP - LAMBDA,has,semantic interface
translation,220,15,model,Model,introduce,UDEP - LAMBDA
translation,220,10,results,English,achieves,4.9 F 1 point improvement
translation,220,10,results,4.9 F 1 point improvement,over,state - of- the - art
translation,220,10,results,state - of- the - art,on,Graph - Questions
translation,220,10,results,Results,For,English
translation,220,25,results,strong baselines,across,languages and datasets
translation,220,25,results,U-DEPLAMBDA,has,outperforms
translation,220,25,results,outperforms,has,strong baselines
translation,220,25,results,Results,experimentally show,U-DEPLAMBDA
translation,220,204,results,SINGLEEVENT and DEP - TREE representations,in,all languages
translation,220,204,results,UDEPLAMBDA,has,consistently outperforms
translation,220,204,results,consistently outperforms,has,SINGLEEVENT and DEP - TREE representations
translation,220,205,results,performance,on par with,CCG - GRAPH
translation,220,205,results,English,has,performance
translation,220,205,results,Results,For,English
translation,220,206,results,results,are,lower
translation,220,206,results,lower,for,German
translation,220,206,results,German,compared to,Spanish
translation,220,215,results,GraphQuestions,achieve,new state - of - the - art result
translation,220,215,results,new state - of - the - art result,with,gain
translation,220,215,results,gain,of,4.8 F 1 points
translation,220,215,results,4.8 F 1 points,over,previously reported best result
translation,220,215,results,Results,On,GraphQuestions
translation,220,216,results,WebQuestions,2.1 points below,best model
translation,220,216,results,WebQuestions,2.1 points below,3.8 points
translation,220,216,results,best model,using,comparable resources
translation,220,216,results,3.8 points,below,state of the art
translation,220,216,results,Results,On,WebQuestions
translation,221,78,hyperparameters,weight matrices,initialized using,normalized initialization technique
translation,221,78,hyperparameters,normalized initialization technique,of,Glorot and Bengio ( 2010 )
translation,221,78,hyperparameters,Hyperparameters,has,weight matrices
translation,221,79,hyperparameters,Hidden layer sizes,fixed as follows,"h E 0 = 2 , 240"
translation,221,79,hyperparameters,Hidden layer sizes,fixed as follows,h I 0 = 32
translation,221,79,hyperparameters,Hidden layer sizes,fixed as follows,h E 1 = 100
translation,221,79,hyperparameters,Hidden layer sizes,fixed as follows,"h I 1 = 16 , h 2 = 116 , h 3 = 64"
translation,221,79,hyperparameters,Hyperparameters,has,Hidden layer sizes
translation,221,80,hyperparameters,biases,set to,0.02
translation,221,80,hyperparameters,Hyperparameters,has,biases
translation,221,81,hyperparameters,network,using,mini-batches
translation,221,81,hyperparameters,mini-batches,of size,200
translation,221,81,hyperparameters,mini-batches,under,early stopping settings
translation,221,81,hyperparameters,early stopping settings,with,RM - Sprop
translation,221,81,hyperparameters,RM - Sprop,as,optimization algorithm
translation,221,81,hyperparameters,Hyperparameters,train,network
translation,221,8,model,AMR graph nodes,represent,concepts
translation,221,8,model,Model,has,AMR graph nodes
translation,221,94,results,SRL features,resulted in,better performance
translation,221,94,results,better performance,compared to,baseline model
translation,221,94,results,Results,using,SRL features
translation,221,95,results,beneficial,widening,context
translation,221,95,results,context,include,more configuration elements
translation,221,95,results,Results,Conditioning on,wider context
translation,221,96,results,NN classifier,did not improve,parser performance
translation,222,179,ablation-analysis,synthetic data,decreased,model 's ability
translation,222,179,ablation-analysis,model 's ability,to parse,variants
translation,222,179,ablation-analysis,Ablation analysis,adding,synthetic data
translation,222,154,baselines,Baselines,has,UDPipe vs. Deep Biaffine
translation,222,129,experimental-setup,Word Embeddings,use,200 dimensional
translation,222,129,experimental-setup,word embeddings,trained with,word2vec
translation,222,129,experimental-setup,word2vec,on,TwitterAAE corpus
translation,222,129,experimental-setup,TwitterAAE corpus,contains,60.8 million tweets
translation,222,129,experimental-setup,200 dimensional,has,word embeddings
translation,222,129,experimental-setup,Experimental setup,use,200 dimensional
translation,222,129,experimental-setup,Experimental setup,has,Word Embeddings
translation,222,5,experiments,English dependency parsing,to handle,social media English
translation,222,5,experiments,English dependency parsing,developing and annotating,new dataset
translation,222,5,experiments,social media English,particularly,social media African -American English ( AAE )
translation,222,5,experiments,new dataset,of,500 tweets
translation,222,16,model,standards,to handle,Twitter-specific and AAE - specific features
translation,222,16,model,Twitter-specific and AAE - specific features,within,Universal Dependencies 2.0 ( ?3 )
translation,222,16,model,Twitter-specific and AAE - specific features,by selecting and annotating,new dataset
translation,222,16,model,new dataset,of,500 tweets
translation,222,16,model,Model,develop,standards
translation,222,18,model,cross-domain training methods,to improve,Twitter AAE dependency parsing
translation,222,18,model,Twitter AAE dependency parsing,with,"no , or very little , in - domain labeled data"
translation,222,18,model,Model,investigate,cross-domain training methods
translation,222,147,results,Morpho- Tagger,across,all settings
translation,222,147,results,Morpho- Tagger,ranging from,2.8 % LAS improvement
translation,222,147,results,2.8 % LAS improvement,when trained only on,UD Treebank
translation,222,147,results,4.7 % and 5.4 % improvements,when trained with,Twitter embeddings and both Twitter embeddings and synthetic data
translation,222,147,results,UDPipe's ARK Tagger setting,has,outperformed
translation,222,147,results,outperformed,has,Morpho- Tagger
translation,222,147,results,Results,has,UDPipe's ARK Tagger setting
translation,222,153,results,synthetic data and Twitter-trained embeddings,were,independently helpful
translation,222,153,results,embeddings,provided,1.4- 5.3 % boost
translation,222,153,results,1.4- 5.3 % boost,across,UDPipe and Deep Biaffine models
translation,222,153,results,synthetic data,provided,1.3- 5.7 % additional boost
translation,222,153,results,Results,observed,synthetic data and Twitter-trained embeddings
translation,222,153,results,Results,observed,embeddings
translation,222,155,results,baseline models,for,UDPipe and Deep Biaffine
translation,222,155,results,UDPipe and Deep Biaffine,are,not directly comparable
translation,222,155,results,not directly comparable,in,Twitter embeddings setting
translation,222,155,results,UDPipe,by,5.1 %
translation,222,155,results,Twitter embeddings setting,has,Deep Biaffine
translation,222,155,results,Deep Biaffine,has,outperformed
translation,222,155,results,outperformed,has,UDPipe
translation,222,156,results,UDPipe 's performance,approached,Deep Biaffine
translation,222,156,results,synthetic data,has,UDPipe 's performance
translation,222,156,results,Twitter embeddings,has,UDPipe 's performance
translation,222,167,results,Twitter-trained embeddings and synthetic data,contains,AAE - specific features
translation,222,167,results,Twitter-trained embeddings and synthetic data,increases,performance gap
translation,222,167,results,performance gap,across,both UDPipe settings
translation,222,167,results,Results,adding,Twitter-trained embeddings and synthetic data
translation,222,177,results,lexical variants,challenged,all four models
translation,222,177,results,switching,from,Morpho- Tagger set - ting
translation,222,177,results,switching,produced,significant accuracy increases
translation,222,177,results,Morpho- Tagger set - ting,to,ARK Tagger settings
translation,222,177,results,ARK Tagger settings,produced,significant accuracy increases
translation,222,177,results,all four models,has,switching
translation,222,177,results,Results,has,lexical variants
translation,222,178,results,greatest improvement,from,ARK Tagger setting
translation,222,178,results,greatest improvement,using,ARK Tagger setting
translation,222,178,results,ARK Tagger setting,with,Twitter-trained embeddings
translation,222,178,results,Results,observed,greatest improvement
translation,223,173,hyperparameters,Hyperparameters,trained for,10 epochs
translation,223,174,hyperparameters,regularization,employ,training
translation,223,174,hyperparameters,regularization,during,training
translation,223,174,hyperparameters,"dropout ( Hinton et al. , 2012 )",applied with,probability 0.5
translation,223,174,hyperparameters,probability 0.5,to,recurrent outputs
translation,223,176,hyperparameters,ADADELTA method,to schedule,learning rates
translation,223,176,hyperparameters,learning rates,for,all weights
translation,223,176,hyperparameters,Hyperparameters,use,ADADELTA method
translation,223,6,model,new shiftreduce system,whose,stack
translation,223,6,model,stack,contains,sentence spans
translation,223,6,model,sentence spans,represented by,bare minimum of LSTM features
translation,223,6,model,Model,introduce,new shiftreduce system
translation,223,7,model,first provably optimal dynamic oracle,for,constituency parsing
translation,223,7,model,first provably optimal dynamic oracle,runs in,amortized O( 1 ) time
translation,223,7,model,amortized O( 1 ) time,compared to,O(n 3 ) oracles
translation,223,7,model,O(n 3 ) oracles,for,standard dependency parsing
translation,223,7,model,Model,design,first provably optimal dynamic oracle
translation,223,14,model,new parsing framework,more suitable for,constituency parsing
translation,223,14,model,new parsing framework,accurately modeled by,neural networks
translation,223,14,model,Model,design,new parsing framework
translation,223,15,model,adaptation,has,of the shift-reduce system
translation,223,215,model,new transition - based constituency parser,built around,sentence spans
translation,223,215,model,Model,developed,new transition - based constituency parser
translation,223,217,model,fast dynamic oracle,can determine,optimal set of actions
translation,223,217,model,optimal set of actions,with respect to,gold training tree
translation,223,217,model,gold training tree,in,arbitrary state
translation,223,200,results,greedy parser,trained using,dynamic oracles
translation,223,200,results,greedy parser,achieves,best F 1 score
translation,223,200,results,dynamic oracles,with,exploration
translation,223,200,results,best F 1 score,of,any closed - set single -model parser
translation,223,218,results,LSTM model,achieve,state - of - the - art accuracy
translation,223,218,results,few sentence spans,as,features
translation,223,218,results,state - of - the - art accuracy,on,Penn Treebank
translation,223,218,results,Penn Treebank,for,all parsers without reranking
translation,223,218,results,Results,Using,LSTM model
translation,224,59,baselines,first baseline ( WORD ),uses,wordlookup model
translation,224,59,baselines,wordlookup model,with,randomly initialized word embeddings
translation,224,59,baselines,Baselines,has,first baseline ( WORD )
translation,224,60,baselines,second baseline ( W2V ),uses,pre-trained word embeddings
translation,224,60,baselines,pre-trained word embeddings,by,"word2vec ( Mikolov et al. , 2013 )"
translation,224,60,baselines,pre-trained word embeddings,with,CBOW model
translation,224,60,baselines,pre-trained word embeddings,with,default parameters
translation,224,60,baselines,default parameters,on,unlabeled texts
translation,224,60,baselines,unlabeled texts,from,shared task organizers
translation,224,60,baselines,Baselines,has,second baseline ( W2V )
translation,224,61,baselines,third baseline ( LSTM ),uses,bidirectional LSTM
translation,224,61,baselines,bidirectional LSTM,as,character composition model
translation,224,61,baselines,Baselines,has,third baseline ( LSTM )
translation,224,64,baselines,Baselines,experiment with,four combined models
translation,224,109,experiments,three agglutinative languages,suffer,most
translation,224,109,experiments,Korean,suffer,most
translation,224,109,experiments,most,with,masked words
translation,224,109,experiments,three agglutinative languages,has,Basque
translation,224,109,experiments,three agglutinative languages,has,Korean
translation,224,4,model,transition - based dependency parser,uses,convolutional neural network
translation,224,4,model,convolutional neural network,to compose,word representations
translation,224,4,model,word representations,from,characters
translation,224,4,model,Model,present,transition - based dependency parser
translation,224,5,results,character composition model,shows,great improvement
translation,224,5,results,great improvement,over,word-lookup model
translation,224,5,results,great improvement,especially for,parsing agglutinative languages
translation,224,5,results,Results,has,character composition model
translation,224,7,results,outperforms,by,margin
translation,224,7,results,previous best greedy parser,by,margin
translation,224,7,results,margin,of,3 %
translation,224,7,results,SPMRL data sets,has,our system
translation,224,7,results,our system,has,outperforms
translation,224,7,results,outperforms,has,previous best greedy parser
translation,224,7,results,Results,On,SPMRL data sets
translation,224,23,results,CNN model,performs,at least as good
translation,224,23,results,at least as good,as,word-lookup model
translation,224,23,results,other morphologically rich languages,has,CNN model
translation,224,23,results,Results,On,other morphologically rich languages
translation,224,24,results,outperforms,both,original and our re-implementation
translation,224,24,results,original and our re-implementation,of,bidirectional LSTM model
translation,224,24,results,bidirectional LSTM model,by,Ballesteros et al . ( 2015 )
translation,224,24,results,bidirectional LSTM model,by,large margin
translation,224,24,results,bidirectional LSTM model,by,large margin
translation,224,24,results,Ballesteros et al . ( 2015 ),by,large margin
translation,224,24,results,our CNN model,has,outperforms
translation,224,24,results,Results,has,our CNN model
translation,224,68,results,macro average,of,all languages
translation,224,68,results,CNN model,performs,2.17 % higher
translation,224,68,results,CNN model,performs,1.24 % higher
translation,224,68,results,2.17 % higher,than,WORD model
translation,224,68,results,1.24 % higher,than,W2V model
translation,224,68,results,macro average,has,CNN model
translation,224,68,results,all languages,has,CNN model
translation,224,68,results,Results,In,macro average
translation,224,69,results,LSTM model,performs,only 0.9 %
translation,224,69,results,LSTM model,performs,1.27 % lower
translation,224,69,results,higher,than,WORD model
translation,224,69,results,1.27 % lower,than,CNN model
translation,224,69,results,only 0.9 %,has,higher
translation,224,69,results,Results,has,LSTM model
translation,224,70,results,CNN model,shows,large improvement
translation,224,70,results,large improvement,in,four languages
translation,224,70,results,Results,has,CNN model
translation,224,73,results,additional word-lookup model,does not significantly improve,CNN moodel
translation,224,73,results,CNN moodel,from,82.75 %
translation,224,73,results,CNN moodel,from,82.90 %
translation,224,73,results,82.75 %,in,CNN
translation,224,73,results,82.75 %,to,82.90 %
translation,224,73,results,82.90 %,in,CNN + WORD
translation,224,73,results,LSTM model,improved by,much larger margin
translation,224,75,results,combined CNN + WORD model,still better than,LSTM + WORD model
translation,224,75,results,Results,has,combined CNN + WORD model
translation,224,80,results,B15 - LSTM model,improves,own baseline
translation,224,80,results,own baseline,by,1.1 %
translation,224,81,results,CNN model,improved from,strong baseline
translation,224,81,results,our WORD model,performs,already 2.22 %
translation,224,81,results,higher,than,B15 - WORD model
translation,224,81,results,strong baseline,has,our WORD model
translation,224,81,results,already 2.22 %,has,higher
translation,224,81,results,Results,has,CNN model
translation,224,82,results,WORD model,except for,Hebrew
translation,224,82,results,CNN model,has,almost always outperforms
translation,224,82,results,almost always outperforms,has,WORD model
translation,224,83,results,LSTM and B15 - LSTM,lower than,baseline
translation,224,83,results,baseline,on,"three agglutinative languages ( Basque , Hungarian , and Korean )"
translation,224,83,results,baseline,on,other six
translation,224,83,results,baseline,on,other six
translation,224,83,results,baseline,on,other six
translation,224,83,results,Results,both,LSTM and B15 - LSTM
translation,224,83,results,Results,has,LSTM and B15 - LSTM
translation,224,102,results,general trend,in,results
translation,224,102,results,results,is that,improvements
translation,224,102,results,improvements,of,both models
translation,224,102,results,both models,in,OOV case
translation,224,102,results,both models,larger than in,IV case
translation,224,102,results,Results,is that,improvements
translation,224,102,results,Results,has,general trend
translation,224,103,results,CNN,improves on,seven languages
translation,224,103,results,CNN,improves on,eight languages
translation,224,103,results,CNN,performs,consistently better
translation,224,103,results,seven languages,in,IV case
translation,224,103,results,eight languages,in,OOV case
translation,224,103,results,consistently better,than,LSTM
translation,224,103,results,LSTM,in,both cases
translation,224,103,results,Results,has,CNN
translation,225,200,ablation-analysis,mod-erate drop,in,accuracy ( 1.4 % LAS points )
translation,225,200,ablation-analysis,mod-erate drop,caused due to,normalization errors
translation,225,200,ablation-analysis,accuracy ( 1.4 % LAS points ),caused due to,normalization errors
translation,225,65,experimental-setup,language models,trained on,monolingual data
translation,225,65,experimental-setup,monolingual data,of,Hindi and English
translation,225,65,experimental-setup,monolingual data,using,KenLM toolkit
translation,225,65,experimental-setup,Experimental setup,has,language models
translation,225,107,experimental-setup,pseudo-projective transformations,of,Nivre and Nilsson ( 2005 )
translation,225,107,experimental-setup,pseudo-projective transformations,to handle,higher percentage of non-projective arcs
translation,225,107,experimental-setup,higher percentage of non-projective arcs,in,CS data ( ? 2 % )
translation,225,107,experimental-setup,Experimental setup,use,pseudo-projective transformations
translation,225,155,experimental-setup,"language identification , POS tagging and parsing models",include,lexical features
translation,225,155,experimental-setup,"language identification , POS tagging and parsing models",of,range
translation,225,155,experimental-setup,"language identification , POS tagging and parsing models",use,randomly initialized embeddings
translation,225,155,experimental-setup,"language identification , POS tagging and parsing models",for,non-lexical units
translation,225,155,experimental-setup,lexical features,in,input layer
translation,225,155,experimental-setup,lexical features,for,non-lexical units
translation,225,155,experimental-setup,input layer,of,our neural networks
translation,225,155,experimental-setup,our neural networks,using,64 - dimension pre-trained word embeddings
translation,225,155,experimental-setup,our neural networks,for,non-lexical units
translation,225,155,experimental-setup,64 - dimension pre-trained word embeddings,for,non-lexical units
translation,225,155,experimental-setup,randomly initialized embeddings,within,range
translation,225,155,experimental-setup,randomly initialized embeddings,for,non-lexical units
translation,225,155,experimental-setup,range,of,"[?0.1 , +0.1 ]"
translation,225,155,experimental-setup,range,of,non-lexical units
translation,225,155,experimental-setup,range,of,POS tags and dictionary flags
translation,225,155,experimental-setup,range,for,non-lexical units
translation,225,155,experimental-setup,"[?0.1 , +0.1 ]",for,non-lexical units
translation,225,155,experimental-setup,non-lexical units,such as,POS tags and dictionary flags
translation,225,155,experimental-setup,Experimental setup,For,"language identification , POS tagging and parsing models"
translation,225,156,experimental-setup,32 - dimensional character embeddings,for,all the three models
translation,225,156,experimental-setup,32 - dimensional POS tag embeddings,for,pipelined parsing models
translation,225,156,experimental-setup,Experimental setup,use,32 - dimensional character embeddings
translation,225,156,experimental-setup,Experimental setup,use,32 - dimensional POS tag embeddings
translation,225,159,experimental-setup,word representations,learned using,Skip-gram model
translation,225,159,experimental-setup,Skip-gram model,with,negative sampling
translation,225,159,experimental-setup,Experimental setup,has,word representations
translation,225,160,experimental-setup,projection algorithm,to transform,Hindi and English monolingual embeddings
translation,225,160,experimental-setup,Hindi and English monolingual embeddings,into,same semantic space
translation,225,160,experimental-setup,same semantic space,using,"bilingual lexicon ( ? 63,000 entries )"
translation,225,160,experimental-setup,Experimental setup,use,projection algorithm
translation,225,162,experimental-setup,normalization models,use,32 - dimensional character embeddings
translation,225,162,experimental-setup,32 - dimensional character embeddings,uniformly initialized within,range
translation,225,162,experimental-setup,range,of,"[?0.1 , +0.1 ]"
translation,225,162,experimental-setup,Experimental setup,For,normalization models
translation,225,164,experimental-setup,POS tagger specific Bi-LSTMs,have,128 cells
translation,225,164,experimental-setup,parser specific Bi-LSTMs,have,256 cells
translation,225,164,experimental-setup,Experimental setup,has,POS tagger specific Bi-LSTMs
translation,225,166,experimental-setup,character Bi-LSTMs,have,32 cells
translation,225,166,experimental-setup,32 cells,for,all three models
translation,225,166,experimental-setup,Experimental setup,has,character Bi-LSTMs
translation,225,167,experimental-setup,hidden layer,of,MLP
translation,225,167,experimental-setup,64 nodes,for,language identification network
translation,225,167,experimental-setup,128 nodes,for,POS tagger
translation,225,167,experimental-setup,256 nodes,for,parser
translation,225,167,experimental-setup,MLP,has,64 nodes
translation,225,167,experimental-setup,Experimental setup,has,hidden layer
translation,225,168,experimental-setup,hyperbolic tangent,as,activation function
translation,225,168,experimental-setup,activation function,in,all tasks
translation,225,168,experimental-setup,Experimental setup,use,hyperbolic tangent
translation,225,169,experimental-setup,normalization models,use,single layered Bi-LSTMs
translation,225,169,experimental-setup,single layered Bi-LSTMs,with,512 cells
translation,225,169,experimental-setup,512 cells,for,encoding and decoding
translation,225,169,experimental-setup,encoding and decoding,of,character sequences
translation,225,169,experimental-setup,Experimental setup,In,normalization models
translation,225,172,experimental-setup,LSTM weights,initialized with,random orthonormal matrices
translation,225,172,experimental-setup,Experimental setup,has,LSTM weights
translation,225,173,experimental-setup,dropout rate,to,30 %
translation,225,173,experimental-setup,dropout rate,to,50 %
translation,225,173,experimental-setup,dropout rate,to,50 %
translation,225,173,experimental-setup,30 %,for,POS tagger and parser Bi-LSTM and MLP hidden states
translation,225,173,experimental-setup,language identification network,set,dropout
translation,225,173,experimental-setup,dropout,to,50 %
translation,225,173,experimental-setup,Experimental setup,set,dropout rate
translation,225,173,experimental-setup,Experimental setup,for,language identification network
translation,225,174,experimental-setup,up to 100 epochs,with,early stopping
translation,225,174,experimental-setup,early stopping,based on,development set
translation,225,174,experimental-setup,Experimental setup,trained for,up to 100 epochs
translation,225,175,experimental-setup,normalization,train,encoderdecoder models
translation,225,175,experimental-setup,encoderdecoder models,for,25 epochs
translation,225,175,experimental-setup,encoderdecoder models,using,vanilla SGD
translation,225,175,experimental-setup,25 epochs,using,vanilla SGD
translation,225,175,experimental-setup,Experimental setup,In case of,normalization
translation,225,176,experimental-setup,learning rate,of,1.0
translation,225,176,experimental-setup,learning rate,after,8 epochs
translation,225,176,experimental-setup,8 epochs,reduce,half
translation,225,176,experimental-setup,half,for,every epoch
translation,225,176,experimental-setup,Experimental setup,start with,learning rate
translation,225,176,experimental-setup,Experimental setup,after,8 epochs
translation,225,177,experimental-setup,mini-batch size,of,128
translation,225,177,experimental-setup,norm,exceeds,5
translation,225,177,experimental-setup,Experimental setup,use,mini-batch size
translation,225,178,experimental-setup,dropout rate,of,30 %
translation,225,178,experimental-setup,30 %,for,Bi-LSTM
translation,225,179,experimental-setup,POS tagging and parsing code,implemented in,DyNet
translation,225,179,experimental-setup,Open - NMT toolkit,for,neural machine translation
translation,225,179,experimental-setup,Language identification,has,POS tagging and parsing code
translation,225,179,experimental-setup,Experimental setup,has,Language identification
translation,225,8,experiments,dependency parsing,of,code-switching data
translation,225,8,experiments,code-switching data,of,Hindi and English multilingual speakers from Twitter
translation,225,51,experiments,normalization and back -transliteration problems,as,general sequence to sequence learning problem
translation,225,171,experiments,"language identification , POS tagging and parsing networks",use,momentum SGD
translation,225,171,experiments,momentum SGD,for,learning
translation,225,171,experiments,learning,with,minibatch size
translation,225,171,experiments,minibatch size,of,1
translation,225,10,model,normalization and back - transliteration models,with,decoding process
translation,225,10,model,decoding process,tailored for,code-switching data
translation,225,31,model,first codeswitching treebank,provides,syntactic annotations
translation,225,31,model,syntactic annotations,required for,parsing mixed - grammar syntactic structures
translation,225,31,model,Model,present,first codeswitching treebank
translation,225,32,model,parsing pipeline,designed explicitly for,Hindi-English CS data
translation,225,32,model,Model,present,parsing pipeline
translation,225,113,model,parameters,of,tagger network
translation,225,113,model,parameters,act as,regularization
translation,225,113,model,tagger network,are,shared
translation,225,113,model,regularization,on,parsing model
translation,225,113,model,Model,has,parameters
translation,225,127,model,non-linear feed -forward network,to predict,labeled transitions
translation,225,127,model,labeled transitions,for,parser configurations
translation,225,127,model,Model,use,non-linear feed -forward network
translation,225,204,model,Model,has,Cross-lingual embeddings
translation,225,183,results,neural stacking,for learning,POS tagging and parsing
translation,225,183,results,neural stacking,for,knowledge transfer
translation,225,183,results,knowledge transfer,from,Bilingual model
translation,225,183,results,Results,uses,neural stacking
translation,225,184,results,POS tagging and syntactic knowledge,using,neural stacking
translation,225,184,results,neural stacking,gives,1.5 % LAS 7 improvement
translation,225,184,results,1.5 % LAS 7 improvement,over,naive approach
translation,225,184,results,naive approach,of,data augmentation
translation,225,184,results,Results,Transferring,POS tagging and syntactic knowledge
translation,225,185,results,Bilingual model,trained on,union of Hindi and English data sets
translation,225,185,results,Bilingual model,is,least accurate
translation,225,185,results,least accurate,of,all our parsing models
translation,225,185,results,Results,has,Bilingual model
translation,225,187,results,Results,for,English
translation,225,188,results,CS model,trained only on,CS training data
translation,225,188,results,CS model,is,slightly more accurate
translation,225,188,results,slightly more accurate,than,Bilingual model
translation,225,188,results,Results,has,CS model
translation,225,197,results,tagging and parsing models,that use,normalization
translation,225,197,results,normalization,without,decoding
translation,225,197,results,normalization,achieve,average of 1 % improvement
translation,225,197,results,average of 1 % improvement,over,models
translation,225,197,results,Results,has,tagging and parsing models
translation,225,198,results,our 3 - step decoding,leads to,higher gains
translation,225,198,results,higher gains,in,tagging
translation,225,198,results,tagging,has,as well as parsing accuracies
translation,225,198,results,Results,has,our 3 - step decoding
translation,225,199,results,around 2.8 % improvements,in,tagging
translation,225,199,results,around 4.6 %,in,parsing
translation,225,199,results,first - best word forms,from,normalization models
translation,225,199,results,Results,achieved,around 2.8 % improvements
translation,225,199,results,Results,achieved,around 4.6 %
translation,225,203,results,Cross-lingual embeddings,brought around,?0.5 % improvements
translation,225,203,results,?0.5 % improvements,in,tagging and parsing
