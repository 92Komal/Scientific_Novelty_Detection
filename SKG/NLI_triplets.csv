topic,paper_ID,sentence_ID,info-unit,sub,pred,obj
translation,0,141,baselines,Classifier - EDA,is,classifier
translation,0,141,baselines,classifier,trained with,data augmentation techniques
translation,0,141,baselines,data augmentation techniques,in,Wei and Zou ( 2019 )
translation,0,141,baselines,Baselines,has,Classifier - EDA
translation,0,146,baselines,Emb - kNN,is,kNN method
translation,0,146,baselines,kNN method,with,S ( Ro ) BERT
translation,0,146,baselines,Emb-kNN - vanilla,without using,intent training examples
translation,0,146,baselines,intent training examples,for,finetuning
translation,0,146,baselines,S ( Ro ) BERT,has,a )
translation,0,146,baselines,Baselines,has,kNN baselines
translation,0,147,baselines,TF - IDF - kNN,is,kNN baseline
translation,0,147,baselines,kNN baseline,using,TF - IDF vectors
translation,0,147,baselines,Baselines,has,TF - IDF - kNN
translation,0,128,experimental-setup,label smoothing,to,all the cross-entropy loss functions
translation,0,128,experimental-setup,Experimental setup,apply,label smoothing
translation,0,129,experimental-setup,Experiments,conducted on,single NVIDIA Tesla V100 GPU
translation,0,129,experimental-setup,single NVIDIA Tesla V100 GPU,with,16GB memory
translation,0,129,experimental-setup,Experimental setup,conducted on,single NVIDIA Tesla V100 GPU
translation,0,129,experimental-setup,Experimental setup,has,Experiments
translation,0,127,experiments,three NLI datasets,from,GLUE benchmark
translation,0,127,experiments,"MNLI WNLI ( Levesque et al. , 2011 )",from,GLUE benchmark
translation,0,127,experiments,GLUE benchmark,to pre-train,proposed model
translation,0,161,experiments,entire CLINC150 dataset,with,150 intents
translation,0,175,experiments,latency,measured on,single NVIDIA Tesla V100 GPU
translation,0,175,experiments,single NVIDIA Tesla V100 GPU,where,batch size
translation,0,175,experiments,batch size,is,1
translation,0,175,experiments,1,to simulate,online use case
translation,0,6,model,discriminative nearest neighbor classification,with,deep self-attention
translation,0,6,model,simple yet effective approach,has,discriminative nearest neighbor classification
translation,0,7,model,BERTstyle pairwise encoding,to train,binary classifier
translation,0,7,model,binary classifier,that estimates,best matched training example
translation,0,7,model,best matched training example,for,user input
translation,0,8,model,discriminative ability,by transferring,natural language inference ( NLI ) model
translation,0,8,model,Model,boost,discriminative ability
translation,0,18,model,Model,propose,discriminative nearest neighbor classification ( DNNC ) model
translation,0,19,model,limited training examples,both in,training and inference time
translation,0,19,model,training and inference time,as,nearest neighbor classification schema
translation,0,20,model,BERT - style paired text encoding,with,deep self-attention
translation,0,20,model,deep self-attention,to directly model,relations
translation,0,20,model,relations,between,pairs
translation,0,20,model,pairs,of,user utterances
translation,0,20,model,Model,leverage,BERT - style paired text encoding
translation,0,155,results,Model Performance,has,CLINC150 Dataset Single domains
translation,0,155,results,Results,has,Model Performance
translation,0,156,results,test set results,of,5 - shot and 10 - shot
translation,0,156,results,test set results,of,OOS detection
translation,0,156,results,5 - shot and 10 - shot,has,in- domain classification
translation,0,157,results,proposed DNNC method,consistently attains,best results
translation,0,157,results,best results,across,all the four domains
translation,0,157,results,5 - shot setting,has,proposed DNNC method
translation,0,157,results,Results,In,5 - shot setting
translation,0,158,results,comparison,between,DNNC - scratch and DNNC
translation,0,158,results,DNNC - scratch and DNNC,shows,our NLI task transfer
translation,0,158,results,our NLI task transfer,is,effective
translation,0,158,results,Results,has,comparison
translation,0,159,results,all the approaches,experience,accuracy improvement
translation,0,159,results,accuracy improvement,due to,additional training data
translation,0,159,results,dominant performance,of,DNNC
translation,0,159,results,10 - shot setting,has,all the approaches
translation,0,159,results,DNNC,has,weakens
translation,0,159,results,Results,In,10 - shot setting
translation,0,160,results,our DNNC,comparable with or even surpasses,some of the 50 - shot classifier 's scores
translation,0,160,results,Results,see that,our DNNC
translation,0,162,results,USE + ConveRT,performs,worse
translation,0,162,results,worse,than,RoBERTabased classifier
translation,0,162,results,RoBERTabased classifier,on,OOD detection task
translation,0,162,results,Results,has,USE + ConveRT
translation,0,163,results,advantage,of,DNNC
translation,0,163,results,DNNC,for,in- domain intent detection
translation,0,163,results,DNNC,is,clear
translation,0,163,results,DNNC,with,10 - shot in - domain accuracy
translation,0,163,results,in- domain intent detection,is,clear
translation,0,163,results,clear,with,10 - shot in - domain accuracy
translation,0,163,results,10 - shot in - domain accuracy,close to,upper-bound accuracy
translation,0,163,results,upper-bound accuracy,for,classifier baseline
translation,0,163,results,Results,has,advantage
translation,0,164,results,our DNNC method,more confident about,prediction
translation,0,164,results,OOS recall,becomes,lower
translation,0,164,results,lower,in,10 - shot setting
translation,0,166,results,USE + ConveRT baseline,evaluated along with,OOS detection task
translation,0,166,results,not as good,as,other RoBERTabased models
translation,0,166,results,USE + ConveRT baseline,has,overall accuracy
translation,0,166,results,OOS detection task,has,overall accuracy
translation,0,171,results,DNNC,is,most robust
translation,0,171,results,most robust,to,threshold selection
translation,0,171,results,5 - shot setting,has,DNNC
translation,0,171,results,Results,observe,5 - shot setting
translation,0,171,results,Results,in,5 - shot setting
translation,0,184,results,DNNC - joint model,shows,competitiveness
translation,0,184,results,competitiveness,in,both inference latency and accuracy
translation,0,184,results,Results,has,DNNC - joint model
translation,1,2,experiments,OCNLI,has,Original Chinese Natural Language Inference
translation,1,5,experiments,Chinese,called,Original Chinese Natural Language Inference dataset ( OCNLI )
translation,1,94,model,new hypothesis elicitation method,called,multi-hypothesis elicitation
translation,1,140,results,lower,for,MULTICONSTRAINT
translation,1,140,results,Results,has,Agreement
translation,1,174,results,All of the non-transformer models,perform,poorly
translation,1,174,results,BERT and RoBERTa,reach,?20 percentagepoint advantage
translation,1,174,results,?20 percentagepoint advantage,over,strongest of these models ( ESIM )
translation,1,174,results,Results,has,All of the non-transformer models
translation,1,176,results,best model,based on,RoBERTa
translation,1,176,results,best model,still about 12 points below,human performance
translation,1,176,results,human performance,on,our test data
translation,1,176,results,transformers,has,strongly outperform
translation,1,176,results,transformers,has,best model
translation,1,176,results,strongly outperform,has,other baseline models
translation,1,176,results,Results,find,transformers
translation,1,197,results,OCNLI and XNLI,combined for,fine-tuning
translation,1,197,results,OCNLI and XNLI,reaches,lower accuracies
translation,1,197,results,improves performance,over,results
translation,1,197,results,improves performance,reaches,lower accuracies
translation,1,197,results,results,using,XNLI
translation,1,197,results,lower accuracies,than,fine-tuning
translation,1,197,results,lower accuracies,on,considerably smaller OCNLI
translation,1,197,results,Results,when,OCNLI and XNLI
translation,1,197,results,Results,using,XNLI
translation,2,58,baselines,7 different architectures,with,hierarchical convolutional networks
translation,2,58,baselines,standard recurrent encoders,with,Long Short - Term Memory ( LSTM )
translation,2,58,baselines,standard recurrent encoders,with,mean or max pooling
translation,2,58,baselines,standard recurrent encoders,with,hierarchical convolutional networks
translation,2,58,baselines,standard recurrent encoders,with,hierarchical convolutional networks
translation,2,58,baselines,concatenation,of,last hidden states
translation,2,58,baselines,last hidden states,of,forward and backward GRU
translation,2,58,baselines,last hidden states,of,hierarchical convolutional networks
translation,2,58,baselines,Bi-directional LSTMs ( BiLSTM ),with,mean or max pooling
translation,2,58,baselines,Bi-directional LSTMs ( BiLSTM ),with,self-attentive network
translation,2,58,baselines,Bi-directional LSTMs ( BiLSTM ),with,hierarchical convolutional networks
translation,2,58,baselines,Baselines,compare,7 different architectures
translation,2,60,baselines,recurrent neural networks,using,LSTM
translation,2,137,baselines,best published supervised methods ( no transfer ),consider,AdaSent
translation,2,137,baselines,best published supervised methods ( no transfer ),consider,TF -KLD
translation,2,137,baselines,best published supervised methods ( no transfer ),consider,Tree-LSTM
translation,2,137,baselines,best published supervised methods ( no transfer ),consider,Illinois -LH system
translation,2,137,baselines,TF -KLD,has,"Ji and Eisenstein , 2013 )"
translation,2,137,baselines,Tree-LSTM,has,"Tai et al. , 2015 )"
translation,2,137,baselines,Illinois -LH system,has,"Lai and Hockenmaier , 2014 )"
translation,2,137,baselines,Baselines,For,best published supervised methods ( no transfer )
translation,2,7,experiments,universal sentence representations,trained using,supervised data
translation,2,7,experiments,supervised data,of,Stanford Natural Language Inference datasets
translation,2,7,experiments,unsupervised methods,like,SkipThought vectors
translation,2,7,experiments,unsupervised methods,on,wide range of transfer tasks
translation,2,7,experiments,consistently outperform,has,unsupervised methods
translation,2,101,experiments,SICK dataset,for,entailment ( SICK -E )
translation,2,101,experiments,SICK dataset,for,semantic relatedness ( SICK - R )
translation,2,171,experiments,competitive,with,PPDB based paragramphrase embeddings
translation,2,171,experiments,competitive,with,pearson score
translation,2,171,experiments,competitive,with,pearson score
translation,2,171,experiments,PPDB based paragramphrase embeddings,with,pearson score
translation,2,171,experiments,pearson score,of,0.70
translation,2,85,hyperparameters,each epoch,divide,learning rate
translation,2,85,hyperparameters,learning rate,by,5
translation,2,85,hyperparameters,learning rate,if,dev accuracy
translation,2,85,hyperparameters,dev accuracy,has,decreases
translation,2,85,hyperparameters,Hyperparameters,At,each epoch
translation,2,86,hyperparameters,minibatches,of size,64
translation,2,86,hyperparameters,training,is,stopped
translation,2,86,hyperparameters,stopped,when,learning rate
translation,2,86,hyperparameters,learning rate,goes under,threshold of 10 ?5
translation,2,86,hyperparameters,Hyperparameters,use,minibatches
translation,2,87,hyperparameters,classifier,use,multi-layer perceptron
translation,2,87,hyperparameters,multi-layer perceptron,with,1 hidden-layer
translation,2,87,hyperparameters,1 hidden-layer,of,512 hidden units
translation,2,87,hyperparameters,Hyperparameters,For,classifier
translation,2,88,hyperparameters,opensource GloVe vectors,trained on,Common Crawl 840B
translation,2,88,hyperparameters,Common Crawl 840B,with,300 dimensions
translation,2,88,hyperparameters,Common Crawl 840B,as,fixed word embeddings
translation,2,88,hyperparameters,Hyperparameters,use,opensource GloVe vectors
translation,2,99,hyperparameters,L2 penalty,of,logistic regression
translation,2,99,hyperparameters,L2 penalty,with,grid-search
translation,2,99,hyperparameters,logistic regression,with,grid-search
translation,2,99,hyperparameters,grid-search,on,validation set
translation,2,99,hyperparameters,Hyperparameters,tune,L2 penalty
translation,2,17,results,sentence embeddings,trained on,various supervised tasks
translation,2,17,results,sentence embeddings,generated from,models
translation,2,17,results,sentence embeddings,generated from,models
translation,2,17,results,sentence embeddings,reach,best results
translation,2,17,results,models,trained on,natural language inference ( NLI ) task
translation,2,17,results,Results,compare,sentence embeddings
translation,2,20,results,impact,of,sentence encoding architecture
translation,2,20,results,sentence encoding architecture,on,representational transferability
translation,2,20,results,sentence encoding architecture,compare,"convolutional , recurrent and even simpler word composition schemes"
translation,2,20,results,Results,investigate,impact
translation,2,126,results,BiLSTM -4096,with,max-pooling operation
translation,2,126,results,BiLSTM -4096,performs,best
translation,2,126,results,best,on,SNLI and transfer tasks
translation,2,126,results,Results,has,BiLSTM -4096
translation,2,127,results,micro and macro averages,performs,significantly better
translation,2,127,results,significantly better,than,other models
translation,2,133,results,transfer quality,sensitive to,optimization algorithm
translation,2,133,results,transfer quality,when training with,Adam
translation,2,133,results,Adam,instead of,SGD
translation,2,133,results,Adam,observed that,BiLSTM - max
translation,2,133,results,BiLSTM - max,converged,faster
translation,2,133,results,BiLSTM - max,obtained,worse results
translation,2,133,results,faster,on,SNLI
translation,2,133,results,worse results,on,transfer tasks
translation,2,138,results,Our model,trained on,SST
translation,2,138,results,Our model,trained on,SST
translation,2,138,results,SST,obtained,83.4
translation,2,138,results,SST,obtained,86.0
translation,2,138,results,83.4,for,MR
translation,2,138,results,86.0,for,SST
translation,2,138,results,Results,has,Our model
translation,2,140,results,increased embedding sizes,lead to,increased performance
translation,2,140,results,increased performance,for,almost all models
translation,2,156,results,previous transfer learning approaches,on,SICK -E
translation,2,156,results,previous transfer learning approaches,used,parameters
translation,2,156,results,parameters,of,LSTM model
translation,2,156,results,parameters,trained on,SNLI
translation,2,156,results,parameters,to fine-tune,SICK
translation,2,156,results,parameters,on,SICK
translation,2,156,results,significantly outperformed,has,previous transfer learning approaches
translation,2,161,results,SkipThought vectors,perform,significantly better
translation,2,161,results,SkipThought vectors,going from,33.8 to 37.9
translation,2,161,results,SkipThought vectors,from,25.9 to 30.6
translation,2,161,results,significantly better,than,original setting
translation,2,161,results,33.8 to 37.9,for,caption retrieval R@1
translation,2,161,results,25.9 to 30.6,on,image retrieval R@1
translation,2,161,results,ResNet features and 30 k more training data,has,SkipThought vectors
translation,2,162,results,Our approach,pushes,results
translation,2,162,results,results,from,37.9
translation,2,162,results,results,from,30.6
translation,2,162,results,37.9,to,42.4
translation,2,162,results,42.4,on,caption retrieval
translation,2,162,results,30.6,to,33.2
translation,2,162,results,33.2,on,image retrieval
translation,2,162,results,results,has,even further
translation,2,162,results,Results,from,37.9
translation,2,162,results,Results,from,30.6
translation,2,162,results,Results,has,Our approach
translation,2,169,results,significant boost,in,performance
translation,2,169,results,significant boost,compared to,model
translation,2,169,results,model,trained only on,SLNI
translation,2,169,results,Results,observe,significant boost
translation,2,170,results,Our model,reaches,AdaSent performance
translation,2,170,results,AdaSent performance,on,CR
translation,2,170,results,Results,has,Our model
translation,3,227,ablation-analysis,SNLI - filter,accuracy of,MACD
translation,3,227,ablation-analysis,MACD,increases by,0.97
translation,3,227,ablation-analysis,0.97,compared to,best competitor ( i.e. BERT )
translation,3,227,ablation-analysis,Ablation analysis,for,SNLI - filter
translation,3,239,ablation-analysis,decreases,when,proposed optimizations
translation,3,239,ablation-analysis,proposed optimizations,are,removed
translation,3,239,ablation-analysis,effectiveness,has,decreases
translation,3,239,ablation-analysis,Ablation analysis,show that,effectiveness
translation,3,201,baselines,MACD,including,BiLST -M+ELMO
translation,3,201,baselines,classical NLP models,including,BiLSTM
translation,3,201,baselines,classical NLP models,including,BiLST -M+ELMO
translation,3,201,baselines,Baselines,compare,MACD
translation,3,164,experimental-setup,lifelong learning regularization,to achieve,modality anchoring
translation,3,164,experimental-setup,Experimental setup,add,lifelong learning regularization
translation,3,172,experimental-setup,experiments,run over,computer
translation,3,172,experimental-setup,computer,with,4 Nvdia Tesla V100 GPUs
translation,3,172,experimental-setup,Experimental setup,has,experiments
translation,3,174,experiments,COCO,as,text2image dataset D t2i
translation,3,174,experiments,text2image dataset D t2i,for,self-supervised learning
translation,3,7,model,Model,propose,Multimodal Aligned Contrastive Decoupled learning ( MACD ) network
translation,3,8,model,MACD,forces,decoupled text encoder
translation,3,8,model,decoupled text encoder,to represent,visual information
translation,3,8,model,visual information,via,contrastive learning
translation,3,8,model,Model,has,MACD
translation,3,165,model,text encoder,keep,original textual representation
translation,3,165,model,original textual representation,by,masked language model ( MLM )
translation,3,165,model,Model,For,text encoder
translation,3,11,results,BiLSTM +ELMO,on,STS -B
translation,3,11,results,unsupervised MACD,has,outperforms
translation,3,11,results,outperforms,has,fully - supervised BiLST -M
translation,3,11,results,Results,has,unsupervised MACD
translation,3,207,results,Results,on,STS -B.
translation,3,208,results,MACD,achieves,significantly higher effectiveness
translation,3,208,results,MACD,achieves,multimodal pretrained model LXMERT and VilBert
translation,3,208,results,significantly higher effectiveness,than,single-modal pre-trained model BERT
translation,3,208,results,significantly higher effectiveness,than,multimodal pretrained model LXMERT and VilBert
translation,3,208,results,Results,has,MACD
translation,3,215,results,MACD,on,SNLI
translation,3,215,results,MACD,under,unsupervised setting
translation,3,215,results,SNLI,under,unsupervised setting
translation,3,215,results,outperforms,by,large margin
translation,3,215,results,competitors,by,large margin
translation,3,215,results,outperforms,has,competitors
translation,3,234,results,MACD,performs,better
translation,3,234,results,better,than,original text encoder ( i.e. BERT )
translation,4,140,ablation-analysis,drops significantly,to,87.24 %
translation,4,140,ablation-analysis,performance,has,drops significantly
translation,4,141,ablation-analysis,POS and NER features,has,performance
translation,4,141,ablation-analysis,performance,has,drops a lot
translation,4,143,ablation-analysis,exact match feature,demonstrates,effectiveness
translation,4,143,ablation-analysis,effectiveness,in,ablation result
translation,4,143,ablation-analysis,Ablation analysis,has,exact match feature
translation,4,144,ablation-analysis,Ablation analysis,ablate,reinforcement learning part
translation,4,145,ablation-analysis,drops,about,0.5 %
translation,4,145,ablation-analysis,result,has,drops
translation,4,145,ablation-analysis,Ablation analysis,has,result
translation,4,112,experimental-setup,Stanford CoreNLP toolkit,to tokenize,words
translation,4,112,experimental-setup,Stanford CoreNLP toolkit,generate,POS and NER tags
translation,4,112,experimental-setup,Experimental setup,use,Stanford CoreNLP toolkit
translation,4,113,experimental-setup,word embeddings,initialized by,300d Glove
translation,4,113,experimental-setup,dimensions,of,POS and NER embeddings
translation,4,113,experimental-setup,POS and NER embeddings,are,30 and 10
translation,4,113,experimental-setup,Experimental setup,has,word embeddings
translation,4,113,experimental-setup,Experimental setup,has,dimensions
translation,4,115,experimental-setup,Tensorflow r 1.3,as,our neural network framework
translation,4,115,experimental-setup,Experimental setup,apply,Tensorflow r 1.3
translation,4,116,experimental-setup,hidden size,as,300
translation,4,116,experimental-setup,300,for,all the LSTM layers
translation,4,116,experimental-setup,"dropout ( Srivastava et al. , 2014 )",between,layers
translation,4,116,experimental-setup,"dropout ( Srivastava et al. , 2014 )",with,initial ratio
translation,4,116,experimental-setup,initial ratio,of,0.9
translation,4,116,experimental-setup,decay rate,as,0.97
translation,4,116,experimental-setup,0.97,for,every 5000 step
translation,4,116,experimental-setup,Experimental setup,set,hidden size
translation,4,116,experimental-setup,Experimental setup,apply,"dropout ( Srivastava et al. , 2014 )"
translation,4,116,experimental-setup,Experimental setup,apply,decay rate
translation,4,117,experimental-setup,AdaDelta,for,optimization
translation,4,117,experimental-setup,AdaDelta,with,? as 0.95 and as 1e - 8
translation,4,117,experimental-setup,Experimental setup,use,AdaDelta
translation,4,118,experimental-setup,our batch size,as,36
translation,4,118,experimental-setup,initial learning rate,as,0.6
translation,4,118,experimental-setup,Experimental setup,set,our batch size
translation,4,118,experimental-setup,Experimental setup,set,initial learning rate
translation,4,119,experimental-setup,parameter,in,objective function
translation,4,119,experimental-setup,parameter,set to be,0.2
translation,4,119,experimental-setup,objective function,set to be,0.2
translation,4,119,experimental-setup,Experimental setup,has,parameter
translation,4,120,experimental-setup,DMP task,use,stochastic gradient descent
translation,4,120,experimental-setup,stochastic gradient descent,with,initial learning rate
translation,4,120,experimental-setup,stochastic gradient descent,with,anneal
translation,4,120,experimental-setup,initial learning rate,as,0.1
translation,4,120,experimental-setup,anneal,by,half
translation,4,120,experimental-setup,validation accuracy,is,lower
translation,4,120,experimental-setup,lower,than,previous epoch
translation,4,120,experimental-setup,half,has,each time
translation,4,120,experimental-setup,each time,has,validation accuracy
translation,4,120,experimental-setup,Experimental setup,For,DMP task
translation,4,121,experimental-setup,number of epochs,set to,10
translation,4,121,experimental-setup,feedforward dropout rate,is,0.2
translation,4,121,experimental-setup,Experimental setup,has,number of epochs
translation,4,121,experimental-setup,Experimental setup,has,feedforward dropout rate
translation,4,9,model,reinforcement learning,to optimize,new objective function
translation,4,9,model,new objective function,with,reward
translation,4,9,model,reward,defined by,property of the NLI datasets
translation,4,36,model,Discourse Marker Augmented Network,for,natural language inference
translation,4,36,model,Discourse Marker Augmented Network,transfer,knowledge
translation,4,36,model,knowledge,from,existing supervised task
translation,4,36,model,Model,propose,Discourse Marker Augmented Network
translation,4,36,model,Model,transfer,knowledge
translation,4,37,model,sentence encoder model,that learns,representations
translation,4,37,model,representations,of,sentences
translation,4,37,model,representations,inject,encoder
translation,4,37,model,sentences,from,DMP task
translation,4,37,model,encoder,to,NLI network
translation,4,37,model,Model,propose,sentence encoder model
translation,4,125,results,method Discourse Marker Augmented Network ( DMAN ),achieves,state - of - the - art results
translation,4,125,results,state - of - the - art results,on,both datasets
translation,4,125,results,method Discourse Marker Augmented Network ( DMAN ),has,clearly outperforms
translation,4,125,results,clearly outperforms,has,all the baselines
translation,4,130,results,performance,of,most of the integrated methods
translation,4,130,results,most of the integrated methods,are,better
translation,4,130,results,better,than,sentence encoding based models
translation,4,130,results,Results,has,performance
translation,4,134,results,performance,of,our model
translation,4,134,results,our model,achieves,89.6 %
translation,4,134,results,our model,achieves,80.3 %
translation,4,134,results,our model,achieves,79.4 %
translation,4,134,results,our model,achieves,all state - of - the - art results
translation,4,134,results,our model,are,all state - of - the - art results
translation,4,134,results,89.6 %,on,SNLI
translation,4,134,results,80.3 %,on,matched MultiNLI
translation,4,134,results,79.4 %,on,mismatched MultiNLI
translation,4,134,results,Results,has,performance
translation,4,138,results,result,is,not satisfactory
translation,4,138,results,result,only using,sentence embedding
translation,4,138,results,sentence embedding,from,discourse markers
translation,4,138,results,sentence embedding,to predict,answer
translation,4,138,results,sentence embedding,is,not ideal
translation,4,138,results,not ideal,in,large-scale datasets
translation,4,138,results,Results,is,not satisfactory
translation,4,138,results,Results,has,result
translation,4,147,results,performance,on,three relation labels
translation,4,147,results,three relation labels,when,model
translation,4,147,results,model,pre-trained on,different discourse markers sets
translation,4,147,results,Results,show,performance
translation,5,73,baselines,Iterative Baseline,training,NLU and NLG iteratively
translation,5,73,baselines,output,from,one model
translation,5,73,baselines,Dual Supervised Learning,has,"Su et al. , 2019 )"
translation,5,73,baselines,Joint Baseline,has,output
translation,5,69,experimental-setup,package,for,preprocessing
translation,5,69,experimental-setup,preprocessing,with,byte-pair-encoding ( BPE )
translation,5,69,experimental-setup,open-sourced Tokenizers,has,package
translation,5,69,experimental-setup,Experimental setup,use,open-sourced Tokenizers
translation,5,22,model,dual inference framework,takes the advantage of,existing models
translation,5,22,model,existing models,from,two dual tasks
translation,5,22,model,existing models,to perform,inference
translation,5,22,model,inference,for,each individual task
translation,5,22,model,each individual task,regarding,duality
translation,5,22,model,duality,between,NLU and NLG
translation,5,22,model,Model,introduces,dual inference framework
translation,5,83,results,all NLU models,achieve,best performance
translation,5,83,results,best performance,by selecting,parameters
translation,5,83,results,parameters,for,intent prediction and slot filling
translation,5,83,results,ATIS,has,all NLU models
translation,5,83,results,Results,For,ATIS
translation,5,84,results,models,with,"( ?=0.5 , ?=0.5 )"
translation,5,84,results,models,with,outperform
translation,5,84,results,NLG,has,models
translation,5,84,results,"( ?=0.5 , ?=0.5 )",has,outperform
translation,5,84,results,outperform,has,baselines
translation,5,84,results,Results,For,NLG
translation,5,85,results,SNIPS,for,models
translation,5,85,results,models,mainly trained by,standard supervised learning
translation,5,85,results,models,mainly trained by,dual supervised learning
translation,5,85,results,proposed method,with,"( ?=0.5 , ?=0.5 )"
translation,5,85,results,proposed method,with,outperform
translation,5,85,results,others,in,NLU and NLG
translation,5,85,results,SNIPS,has,proposed method
translation,5,85,results,models,has,proposed method
translation,5,85,results,standard supervised learning,has,proposed method
translation,5,85,results,"( ?=0.5 , ?=0.5 )",has,outperform
translation,5,85,results,outperform,has,others
translation,5,85,results,Results,of,SNIPS
translation,5,88,results,performance,of,NLU and NLG models
translation,5,88,results,performance,showing,generalization
translation,5,88,results,NLU and NLG models,trained by,different learning algorithms
translation,5,88,results,generalization,to,multiple datasets / domains
translation,5,88,results,consistently improve,has,performance
translation,5,88,results,Results,has,proposed dual inference technique
translation,7,145,ablation-analysis,only the premise ( but not hypothesis ),from,input
translation,7,145,ablation-analysis,slightly decreases,has,overall accuracy
translation,7,145,ablation-analysis,Ablation analysis,removing,only the premise ( but not hypothesis )
translation,7,212,ablation-analysis,generative task,removing,premise
translation,7,212,ablation-analysis,generative task,makes,substantial difference
translation,7,212,ablation-analysis,only a slight difference,in,performance
translation,7,212,ablation-analysis,performance,for,?- ATOMIC ( ?1.64 % )
translation,7,212,ablation-analysis,substantial difference,for,?- SNLI ( ?10.93 % )
translation,7,212,ablation-analysis,Ablation analysis,In,generative task
translation,7,155,experimental-setup,each model,for,single epoch
translation,7,155,experimental-setup,single epoch,with,batch size
translation,7,155,experimental-setup,batch size,of,64
translation,7,155,experimental-setup,Experimental setup,use,Transformers package
translation,7,155,experimental-setup,Experimental setup,train,each model
translation,7,138,results,models,which have access to,"full input ( P , H , U )"
translation,7,138,results,accuracy,very close to,human performance
translation,7,138,results,human performance,on,each dataset
translation,7,138,results,models,has,accuracy
translation,7,138,results,"full input ( P , H , U )",has,accuracy
translation,7,138,results,Results,For,models
translation,7,144,results,15 to 20 points,above,uninformed majority baselines
translation,7,144,results,13 to 15 points below,has,fully - informed models
translation,7,167,results,GPT2 - XL models,perform,best
translation,7,167,results,GPT2 - XL models,slightly worse than,best model ( Bart-large )
translation,7,167,results,best,for,?- ATOMIC and the social norms dataset
translation,7,167,results,best model ( Bart-large ),on,?- SNLI
translation,7,167,results,Results,has,GPT2 - XL models
translation,7,168,results,model size,not have,major impact
translation,7,168,results,major impact,on,performance
translation,7,168,results,GPT2 - S,performing,moderately worse
translation,7,168,results,moderately worse,than,GPT2 - XL
translation,7,168,results,Results,has,model size
translation,7,169,results,lowest performance,across,tasks
translation,7,169,results,tasks,in terms of,BLEU and ROUGE
translation,7,169,results,T5 model,has,lowest performance
translation,7,169,results,Results,has,T5 model
translation,8,23,ablation-analysis,WordNet and Wikidata,has,Wikipedia category graph
translation,8,23,ablation-analysis,Wikipedia category graph,has,more fine- grained connections
translation,8,23,ablation-analysis,Ablation analysis,Compared to,WordNet and Wikidata
translation,8,169,ablation-analysis,pruning levels,in,Wikipedia category graphs
translation,8,169,ablation-analysis,Ablation analysis,explore,pruning levels
translation,8,169,ablation-analysis,Ablation analysis,incorporating,sentential context
translation,8,5,experiments,model performance,on,NLI and LE tasks
translation,8,24,experiments,NLI models,constructed automatically from,Wikipedia category graph
translation,8,24,experiments,Wikipedia category graph,by,automatic filtering
translation,8,24,experiments,automatic filtering,from,Wikipedia category graph
translation,8,140,experiments,Wikidata,is,consistently better
translation,8,140,experiments,consistently better,than,WIKINLI
translation,8,140,experiments,PPDB,has,Wikidata
translation,8,29,results,WIKINLI,brings,consistent improvements
translation,8,29,results,WIKINLI,introduces,additional knowledge
translation,8,29,results,consistent improvements,in,low resource NLI setting
translation,8,29,results,low resource NLI setting,where,limited amounts of training data
translation,8,29,results,plateau,as,number of training instances
translation,8,29,results,additional knowledge,related to,lexical relations
translation,8,29,results,additional knowledge,benefiting,finer - grained LE and NLI tasks
translation,8,29,results,improvements,has,plateau
translation,8,29,results,number of training instances,has,increases
translation,8,29,results,Results,find,WIKINLI
translation,8,137,results,"WIKINLI , Wikidata , or WordNet",improves,performances
translation,8,137,results,"WIKINLI , Wikidata , or WordNet",pretraining on,WIKINLI
translation,8,137,results,performances,on,downstream tasks
translation,8,137,results,WIKINLI,achieves,best per- formance
translation,8,137,results,Results,pretraining on,"WIKINLI , Wikidata , or WordNet"
translation,8,137,results,Results,pretraining on,WIKINLI
translation,8,139,results,BERT - large + WIKINLI,not better than,baseline BERT - large
translation,8,139,results,baseline BERT - large,on,RTE
translation,8,139,results,RoBERTa + WIKINLI,shows,much better performance
translation,8,141,results,BERT - large + WIKINLI,shows,sizeable improvement
translation,8,141,results,sizeable improvement,over,BERT - large baseline
translation,8,141,results,Results,note that,BERT - large + WIKINLI
translation,8,142,results,improvements,to,BERT and RoBERTa
translation,8,142,results,improvements,show that,benefit
translation,8,142,results,BERT and RoBERTa,brought by,WIKINLI
translation,8,142,results,BERT and RoBERTa,show that,benefit
translation,8,142,results,benefit,of,WIKINLI dataset
translation,8,142,results,benefit,generalize to,different models
translation,8,142,results,WIKINLI dataset,generalize to,different models
translation,8,142,results,Results,has,improvements
translation,8,143,results,little benefit,for,SciTail
translation,8,174,results,PPDB,adding,more data
translation,8,174,results,more data,improves,formance
translation,8,174,results,Results,except for,PPDB
translation,8,175,results,Break,observe,significant improvements
translation,8,175,results,significant improvements,when using,fourway WIKINLI
translation,8,175,results,fourway WIKINLI,for,pretraining
translation,8,175,results,threeway WIKINLI,seems to,hurt
translation,8,175,results,hurt,has,performance
translation,8,175,results,Results,For,Break
translation,8,180,results,WIKINLI,works,best
translation,8,180,results,best,when,pretrained alone
translation,8,180,results,Results,has,WIKINLI
translation,8,199,results,more significant improvement,with,less training data
translation,8,199,results,gap,between,BERT - large and WIKINLI
translation,8,199,results,narrows,as,training data size
translation,8,199,results,WIKINLI,has,more significant improvement
translation,8,199,results,gap,has,narrows
translation,8,199,results,BERT - large and WIKINLI,has,narrows
translation,8,199,results,training data size,has,increases
translation,8,199,results,Results,show,WIKINLI
translation,8,199,results,Results,show,gap
translation,8,215,results,pretraining,on,WIKINLI
translation,8,215,results,pretraining,gives,best results
translation,8,215,results,WIKINLI,gives,best results
translation,8,215,results,Results,has,pretraining
translation,9,117,ablation-analysis,hypothesis-only models,provide,interesting limitation
translation,9,117,ablation-analysis,interesting limitation,of,NBOW and InferSent
translation,9,117,ablation-analysis,Ablation analysis,has,hypothesis-only models
translation,9,102,baselines,InferSent,has,"Conneau et al. , 2017 )"
translation,9,103,model,NBOW model,represents,contexts and hypotheses
translation,9,103,model,contexts and hypotheses,average of,GloVe embeddings
translation,9,103,model,Model,has,NBOW model
translation,9,105,model,InferSent model,encodes,contexts and hypotheses independently
translation,9,105,model,contexts and hypotheses independently,with,BiLSTM
translation,9,105,model,sentence representations,extracted using,max-pooling
translation,9,105,model,Model,has,InferSent model
translation,9,99,results,reliable positive effect,label being,entailed
translation,9,99,results,Results,find,reliable positive effect
translation,9,115,results,models,trained on,MNLI
translation,9,115,results,models,perform,poorly
translation,9,115,results,poorly,on,our recast datasets
translation,9,115,results,worse,than,MAJ baseline
translation,9,115,results,Results,see that,models
translation,9,118,results,NBOW and InferSent hypothesis-only models,are,even better
translation,9,118,results,NBOW and InferSent hypothesis-only models,as good as,even better
translation,9,118,results,even better,than,normal models
translation,9,118,results,normal models,across,all datasets
translation,9,118,results,Results,Both,NBOW and InferSent hypothesis-only models
translation,9,118,results,Results,has,NBOW and InferSent hypothesis-only models
translation,9,119,results,improves,when given,context
translation,9,119,results,context,across,all datasets
translation,9,119,results,all datasets,with,TempEval3
translation,9,119,results,RoBERTa,has,improves
translation,9,119,results,Results,has,RoBERTa
translation,9,123,results,All three hypothesis-only models,achieve,high accuracy
translation,9,123,results,high accuracy,on,NLI dataset
translation,9,123,results,NLI dataset,based on,UDS - Duration
translation,9,123,results,Results,has,All three hypothesis-only models
translation,9,128,results,neural models,see,10 % gain
translation,9,128,results,10 % gain,in,accuracy
translation,9,128,results,accuracy,over,template -sensitive majority
translation,9,128,results,Results,has,neural models
translation,10,34,ablation-analysis,simple preprocessing step,performed on,SNLI dataset
translation,10,34,ablation-analysis,Ablation analysis,importance of,simple preprocessing step
translation,10,170,ablation-analysis,all modifications,lead to,new model
translation,10,170,ablation-analysis,Ablation analysis,see that,all modifications
translation,10,171,ablation-analysis,any part,from,our model
translation,10,171,ablation-analysis,our model,has,hurts
translation,10,171,ablation-analysis,hurts,has,development set accuracy
translation,10,171,ablation-analysis,Ablation analysis,removing,any part
translation,10,172,ablation-analysis,three of them,have,noticeable influences
translation,10,172,ablation-analysis,all components,has,three of them
translation,10,172,ablation-analysis,Ablation analysis,Among,all components
translation,10,174,ablation-analysis,importance,of,our proposed dependent reading strategy
translation,10,174,ablation-analysis,our proposed dependent reading strategy,leads to,significant improvement
translation,10,174,ablation-analysis,significant improvement,in,encoding stage
translation,10,174,ablation-analysis,Ablation analysis,illustrate,importance
translation,10,110,hyperparameters,pre-trained 300 - D Glove 840B vectors,to initialize,word embedding vectors
translation,10,111,hyperparameters,All hidden states,of,BiLSTMs
translation,10,111,hyperparameters,BiLSTMs,during,input encoding and inference
translation,10,111,hyperparameters,input encoding and inference,have,450 dimensions
translation,10,111,hyperparameters,Hyperparameters,has,All hidden states
translation,10,112,hyperparameters,weights,learned by minimizing,log-loss
translation,10,112,hyperparameters,log-loss,on,training data
translation,10,112,hyperparameters,Hyperparameters,has,weights
translation,10,113,hyperparameters,initial learning rate,is,0.0004
translation,10,113,hyperparameters,Hyperparameters,has,initial learning rate
translation,10,114,hyperparameters,overfitting,use,"dropout ( Srivastava et al. , 2014 )"
translation,10,114,hyperparameters,"dropout ( Srivastava et al. , 2014 )",with,rate
translation,10,114,hyperparameters,"dropout ( Srivastava et al. , 2014 )",for,regularization
translation,10,114,hyperparameters,"dropout ( Srivastava et al. , 2014 )",applied to,all feedforward connections
translation,10,114,hyperparameters,rate,of,0.4
translation,10,114,hyperparameters,regularization,applied to,all feedforward connections
translation,10,114,hyperparameters,Hyperparameters,To avoid,overfitting
translation,10,116,hyperparameters,fairly small batch size,of,32
translation,10,116,hyperparameters,Hyperparameters,use,fairly small batch size
translation,10,6,model,novel dependent reading bidirectional LSTM network ( DR - BiLSTM ),to efficiently model,relationship
translation,10,6,model,relationship,between,premise
translation,10,6,model,relationship,between,hypothesis
translation,10,6,model,relationship,during,encoding and inference
translation,10,6,model,Model,propose,novel dependent reading bidirectional LSTM network ( DR - BiLSTM )
translation,10,29,model,Model,propose,dependent reading bidirectional LSTM ( DR - BiLSTM )
translation,10,52,model,Model,proposing,novel deep learning model ( DR - BiLSTM )
translation,10,115,model,effective representations,for,NLI task
translation,10,115,model,training,has,word embeddings
translation,10,115,model,Model,During,training
translation,10,33,results,SNLI dataset,show,DR - BiLSTM
translation,10,33,results,DR - BiLSTM,achieves,best single model and ensemble model performance
translation,10,33,results,best single model and ensemble model performance,obtaining,improvements
translation,10,33,results,improvements,of,considerable margin
translation,10,33,results,improvements,over,previous state - of - the - art single and ensemble models
translation,10,33,results,considerable margin,of,0.4 % and 0.3 %
translation,10,146,results,inter-sentence attention - based models,perform better than,sentence encoding based models
translation,10,146,results,Results,see that,inter-sentence attention - based models
translation,10,152,results,trivial preprocessing step,yields to,further improvements
translation,10,152,results,further improvements,of,0.4 % and 0.3 %
translation,10,152,results,further improvements,for,single and ensemble DR - BiLSTM models
translation,10,152,results,Results,utilizing,trivial preprocessing step
translation,10,157,results,proposed single model,achieves,competitive results
translation,10,157,results,competitive results,compared to,reported ensemble models
translation,10,157,results,Results,shows,proposed single model
translation,10,158,results,current state - of - the - art,by obtaining,89.3 % accuracy
translation,10,158,results,Our ensemble model,has,considerably outperforms
translation,10,158,results,considerably outperforms,has,current state - of - the - art
translation,10,158,results,Results,has,Our ensemble model
translation,10,161,results,preprocessing mechanism,leads to,further improvements
translation,10,161,results,further improvements,of,0.4 % and 0.3 %
translation,10,161,results,0.4 % and 0.3 %,on,SNLI test set
translation,10,161,results,0.4 % and 0.3 %,for,our single and ensemble models
translation,10,161,results,Results,see that,preprocessing mechanism
translation,10,162,results,our single model,obtains,state - of - the - art performance
translation,10,162,results,state - of - the - art performance,over,reported single and ensemble models
translation,10,162,results,reported single and ensemble models,by performing,simple preprocessing step
translation,10,162,results,Results,has,our single model
translation,10,163,results,DR - BiLSTM ( Ensem . ) + Process,has,outperforms
translation,10,163,results,outperforms,has,existing state - of - the - art
translation,10,163,results,existing state - of - the - art,has,remarkably
translation,10,163,results,Results,has,DR - BiLSTM ( Ensem . ) + Process
translation,10,178,results,best performance,with,450 - dimensional BiLSTMs
translation,10,178,results,Results,achieve,best performance
translation,10,198,results,DR - BiLSTM ( Ensemble ),performs,best
translation,10,198,results,best,in,all categories
translation,10,198,results,Results,see that,DR - BiLSTM ( Ensemble )
translation,11,145,baselines,BART,is,denoising autoencoder
translation,11,145,baselines,denoising autoencoder,for,pretraining seq-to-seq models
translation,11,145,baselines,Baselines,has,BART
translation,11,144,experiments,DistilBERT,compress,BERT
translation,11,144,experiments,BERT,with,knowledge distillation
translation,11,176,results,all the models,yield,similar JSD scores
translation,11,176,results,similar JSD scores,to,chance baseline
translation,11,176,results,ChaosNLI -M,has,all the models
translation,11,176,results,Results,On,ChaosNLI -M
translation,11,177,results,BERT - base,performs,worse
translation,11,177,results,worse,than,chance baseline
translation,11,177,results,worse,than,chance baseline
translation,11,177,results,chance baseline,on,JSD
translation,11,177,results,scores,of,KL
translation,11,177,results,KL,by,all the models
translation,11,177,results,all the models,way higher than,chance baseline
translation,11,177,results,ChaosNLI -?,has,BERT - base
translation,11,177,results,Results,On,ChaosNLI -?
translation,11,180,results,all the large models,give,higher JSD scores
translation,11,180,results,higher JSD scores,than,base models
translation,11,180,results,ChaosNLI -M,has,all the large models
translation,11,180,results,Results,has,ChaosNLI -M
translation,11,180,results,Results,has,all the large models
translation,11,181,results,all the large models,achieve,higher accuracy
translation,11,181,results,higher accuracy,than,base model counterparts
translation,11,181,results,Results,has,all the large models
translation,11,197,results,accuracy,of,most models
translation,11,197,results,most models,on,bin
translation,11,197,results,bin,with,lowest level of human agreements
translation,11,197,results,lowest level of human agreements,does not surpass,60 %
translation,11,197,results,both ?NLI and NLI,has,accuracy
translation,11,197,results,Results,For,both ?NLI and NLI
translation,12,156,ablation-analysis,external knowledge,add,TransE relation embedding
translation,12,156,ablation-analysis,further improvement,observed,development and test sets
translation,12,156,ablation-analysis,further improvement,on,development and test sets
translation,12,156,ablation-analysis,development and test sets,when,TransE relation embedding
translation,12,156,ablation-analysis,TransE relation embedding,used ( concatenated ) with,semantic relation vectors
translation,12,156,ablation-analysis,Ablation analysis,To further investigate,external knowledge
translation,12,160,ablation-analysis,ESIM,with,external knowledge
translation,12,160,ablation-analysis,ESIM,achieve,significant gains
translation,12,160,ablation-analysis,significant gains,to,77.2 % and 76.4 %
translation,12,160,ablation-analysis,Ablation analysis,extend,ESIM
translation,12,169,ablation-analysis,external knowledge,in collecting,"local inference information ( "" I "" )"
translation,12,169,ablation-analysis,significant gain,to,70.3 % ( + absolute 7.9 % )
translation,12,169,ablation-analysis,external knowledge,has,accuracy
translation,12,169,ablation-analysis,"local inference information ( "" I "" )",has,accuracy
translation,12,169,ablation-analysis,accuracy,has,significant gain
translation,12,169,ablation-analysis,Ablation analysis,utilize,external knowledge
translation,12,170,ablation-analysis,external knowledge,in,"inference composition ( "" C "" )"
translation,12,170,ablation-analysis,accuracy,gets,smaller gain
translation,12,170,ablation-analysis,smaller gain,to,63.4 % ( + absolute 1.0 % )
translation,12,170,ablation-analysis,external knowledge,has,accuracy
translation,12,170,ablation-analysis,"inference composition ( "" C "" )",has,accuracy
translation,12,170,ablation-analysis,Ablation analysis,add,external knowledge
translation,12,211,ablation-analysis,external knowledge,in calculating,"co-attention ( "" A "" )"
translation,12,211,ablation-analysis,accuracy,increases to,66.6 % ( + absolute 4.2 %
translation,12,211,ablation-analysis,external knowledge,has,accuracy
translation,12,211,ablation-analysis,"co-attention ( "" A "" )",has,accuracy
translation,12,211,ablation-analysis,Ablation analysis,add,external knowledge
translation,12,100,baselines,Baselines,has,Synonymy
translation,12,150,baselines,ESIM,is,previous state - of - the - art systems
translation,12,150,baselines,ESIM,one of,previous state - of - the - art systems
translation,12,150,baselines,previous state - of - the - art systems,with,88.0 % test-set accuracy
translation,12,150,baselines,Baselines,has,ESIM
translation,12,138,hyperparameters,dimension,of,hidden states
translation,12,138,hyperparameters,hidden states,of,LSTMs
translation,12,138,hyperparameters,hidden states,of,word embeddings
translation,12,138,hyperparameters,word embeddings,are,300
translation,12,139,hyperparameters,word embeddings,initialized by,300D GloVe 840B
translation,12,139,hyperparameters,out - of- vocabulary words,initialized,randomly
translation,12,139,hyperparameters,Hyperparameters,has,word embeddings
translation,12,139,hyperparameters,Hyperparameters,has,out - of- vocabulary words
translation,12,140,hyperparameters,word embeddings,updated during,training
translation,12,140,hyperparameters,Hyperparameters,has,word embeddings
translation,12,141,hyperparameters,"Adam ( Kingma and Ba , 2014 )",used for,optimization
translation,12,141,hyperparameters,"Adam ( Kingma and Ba , 2014 )",with,initial learning rate
translation,12,141,hyperparameters,optimization,with,initial learning rate
translation,12,141,hyperparameters,initial learning rate,of,0.0004
translation,12,141,hyperparameters,Hyperparameters,has,"Adam ( Kingma and Ba , 2014 )"
translation,12,142,hyperparameters,mini- batch size,set to,32
translation,12,142,hyperparameters,Hyperparameters,has,mini- batch size
translation,12,146,hyperparameters,relations,represented with,vectors
translation,12,146,hyperparameters,vectors,of,20 dimension
translation,12,146,hyperparameters,WordNet,has,relations
translation,12,146,hyperparameters,Hyperparameters,When training TransE,WordNet
translation,12,35,model,neural - network - based NLI models,with,external knowledge
translation,12,35,model,Model,enrich,neural - network - based NLI models
translation,12,37,results,proposed model,improves,state - of - the - art neural NLI models
translation,12,37,results,state - of - the - art neural NLI models,to achieve,better performances
translation,12,37,results,better performances,on,SNLI and MultiNLI datasets
translation,12,37,results,Results,show,proposed model
translation,12,151,results,proposed model,namely,Knowledge - based Inference Model ( KIM )
translation,12,151,results,proposed model,enriches,ESIM
translation,12,151,results,proposed model,obtains,accuracy
translation,12,151,results,Knowledge - based Inference Model ( KIM ),enriches,ESIM
translation,12,151,results,Knowledge - based Inference Model ( KIM ),obtains,accuracy
translation,12,151,results,ESIM,with,external knowledge
translation,12,151,results,accuracy,of,88.6 %
translation,12,151,results,best single -model performance,reported on,SNLI dataset
translation,12,151,results,Results,namely,Knowledge - based Inference Model ( KIM )
translation,12,151,results,Results,has,proposed model
translation,12,159,results,baseline ESIM,achieves,76.8 % and 75.8 %
translation,12,159,results,76.8 % and 75.8 %,on,in- domain and cross-domain test set
translation,12,159,results,Results,has,baseline ESIM
translation,12,175,results,proposed KIM,achieves,83.5 %
translation,12,175,results,Results,has,proposed KIM
translation,12,176,results,accuracy,of,ESIM and KIM
translation,12,176,results,ESIM and KIM,in,each replacement - word category
translation,12,176,results,each replacement - word category,of,"( Glockner et al. , 2018 ) test set"
translation,12,176,results,Results,displays,accuracy
translation,12,177,results,KIM,performs,worse
translation,12,177,results,ESIM,in,13 out of 14 categories
translation,12,177,results,worse,on,synonyms
translation,12,177,results,KIM,has,outperforms
translation,12,177,results,outperforms,has,ESIM
translation,12,177,results,Results,has,KIM
translation,12,177,results,Results,has,outperforms
translation,12,186,results,Our neural - network - based model,for,natural language inference
translation,12,186,results,Our neural - network - based model,achieves,state - of - the - art accuracies
translation,12,186,results,natural language inference,with,external knowledge
translation,12,186,results,external knowledge,namely,KIM
translation,12,186,results,Results,has,Our neural - network - based model
translation,12,192,results,more training data,i.e.,"4 % , 20 % , 100 %"
translation,12,192,results,"4 % , 20 % , 100 %",of,training set
translation,12,192,results,"only "" I """,achieves,significant gain
translation,12,192,results,""" A "" or "" C",bring,any significant improvement
translation,12,192,results,more training data,has,"only "" I """
translation,12,192,results,more training data,has,""" A "" or "" C"
translation,12,192,results,Results,use,more training data
translation,12,198,results,Better accuracies,achieved when using,more external knowledge
translation,12,198,results,Results,has,Better accuracies
translation,12,210,results,restricted training data,i.e.,0.8 % train - ing set
translation,12,210,results,poor accuracy,of,62.4 %
translation,12,210,results,restricted training data,has,baseline ESIM
translation,12,210,results,baseline ESIM,has,poor accuracy
translation,12,221,results,ESIM,in,10 out of 13 categories
translation,12,221,results,10 out of 13 categories,on,cross-domain setting
translation,12,221,results,only 7 out of 13 categories,on,in-domain setting
translation,12,221,results,KIM,has,outperforms or equals
translation,12,221,results,outperforms or equals,has,ESIM
translation,12,221,results,Results,has,KIM
translation,12,221,results,Results,has,outperforms or equals
translation,12,223,results,antonym category,in,cross-domain set
translation,12,223,results,antonym category,has,KIM
translation,12,223,results,antonym category,has,outperform
translation,12,223,results,cross-domain set,has,KIM
translation,12,223,results,cross-domain set,has,outperform
translation,12,223,results,KIM,has,outperform
translation,12,223,results,outperform,has,ESIM
translation,12,223,results,outperform,has,significantly ( + absolute 5.0 % )
translation,12,223,results,ESIM,has,significantly ( + absolute 5.0 % )
translation,12,223,results,Results,for,antonym category
translation,13,89,experimental-setup,300 dimensional GloVe embeddings,to represent,words
translation,13,89,experimental-setup,Experimental setup,use,300 dimensional GloVe embeddings
translation,13,90,experimental-setup,normalized,to have,2 norm of 1
translation,13,90,experimental-setup,projected down,to,200 dimensions
translation,13,90,experimental-setup,Experimental setup,has,Each embedding vector
translation,13,91,experimental-setup,Out- of-vocabulary ( OOV ) words,hashed to,one of 100 random embeddings
translation,13,91,experimental-setup,one of 100 random embeddings,initialized to,mean 0
translation,13,91,experimental-setup,one of 100 random embeddings,initialized to,standard deviation 1
translation,13,91,experimental-setup,Experimental setup,has,Out- of-vocabulary ( OOV ) words
translation,13,93,experimental-setup,other parameter weights ( hidden layers etc. ),initialized from,random Gaussians
translation,13,93,experimental-setup,random Gaussians,with,mean 0
translation,13,93,experimental-setup,random Gaussians,with,standard deviation 0.01
translation,13,93,experimental-setup,Experimental setup,has,other parameter weights ( hidden layers etc. )
translation,13,94,experimental-setup,hyperparameter setting,run on,single machine
translation,13,94,experimental-setup,hyperparameter setting,using,"Adagrad ( Duchi et al. , 2011 )"
translation,13,94,experimental-setup,single machine,with,10 asynchronous gradient -update threads
translation,13,94,experimental-setup,"Adagrad ( Duchi et al. , 2011 )",for,optimization
translation,13,94,experimental-setup,"Adagrad ( Duchi et al. , 2011 )",with,default initial accumulator value
translation,13,94,experimental-setup,default initial accumulator value,of,0.1
translation,13,94,experimental-setup,Experimental setup,has,hyperparameter setting
translation,13,95,experimental-setup,Dropout regularization,used for,all ReLU layers
translation,13,95,experimental-setup,Dropout regularization,not for,final linear layer
translation,13,95,experimental-setup,all ReLU layers,not for,final linear layer
translation,13,95,experimental-setup,Experimental setup,has,Dropout regularization
translation,13,5,model,attention,to decompose,problem
translation,13,5,model,problem,into,subproblems
translation,13,5,model,subproblems,solved,separately
translation,13,5,model,subproblems,making,trivially parallelizable
translation,13,21,model,fully computationally decomposable,with respect to,input text
translation,13,21,model,Model,relies on,alignment
translation,13,25,model,intra-sentence attention,to endow,model
translation,13,25,model,model,with,richer encoding
translation,13,25,model,richer encoding,of,substructures
translation,13,25,model,richer encoding,prior to,alignment step
translation,13,25,model,Model,apply,intra-sentence attention
translation,13,25,model,Model,with,richer encoding
translation,13,6,results,Stanford Natural Language Inference ( SNLI ) dataset,obtain,state - of - the - art results
translation,13,6,results,state - of - the - art results,with,almost an order of magnitude fewer parameters
translation,13,6,results,Results,On,Stanford Natural Language Inference ( SNLI ) dataset
translation,13,98,results,Our vanilla approach,achieves,state - of- theart results
translation,13,98,results,state - of- theart results,with,almost an order of magnitude fewer parameters
translation,13,98,results,almost an order of magnitude fewer parameters,than,LSTMN
translation,13,98,results,Results,has,Our vanilla approach
translation,13,99,results,intra-sentence attention,gives,considerable improvement
translation,13,99,results,considerable improvement,of,0.5 percentage points
translation,13,99,results,0.5 percentage points,over,existing state of the art
translation,13,99,results,Results,Adding,intra-sentence attention
translation,14,204,ablation-analysis,morphological parsing,is,beneficial
translation,14,204,ablation-analysis,beneficial,where,training set
translation,14,204,ablation-analysis,training set,is,small
translation,14,204,ablation-analysis,largely disappears,for,large training sets
translation,14,204,ablation-analysis,importance,has,largely disappears
translation,14,204,ablation-analysis,Ablation analysis,suggests,morphological parsing
translation,14,164,baselines,English-only corpus,for,training
translation,14,196,experimental-setup,model,on,single Tesla V100 GPU
translation,14,196,experimental-setup,single Tesla V100 GPU,of,NVIDIA DGX - 1 system
translation,14,196,experimental-setup,single Tesla V100 GPU,allocating,128GB memory
translation,14,196,experimental-setup,128GB memory,for,1 day
translation,14,196,experimental-setup,Experimental setup,trained,model
translation,14,197,experimental-setup,dataset,into,30 equal shards
translation,14,197,experimental-setup,30 equal shards,for,parallel processing
translation,14,197,experimental-setup,30 equal shards,shuffled,shards
translation,14,197,experimental-setup,shards,prior to,training
translation,14,197,experimental-setup,training,to reduce,adverse effects of variance
translation,14,197,experimental-setup,adverse effects of variance,across,sentence styles
translation,14,197,experimental-setup,sentence styles,in,different shards
translation,14,197,experimental-setup,Experimental setup,split,dataset
translation,14,198,experimental-setup,effective batch size,of,128
translation,14,198,experimental-setup,128,with,gradient accumulation
translation,14,198,experimental-setup,gradient accumulation,to address,memory limitations
translation,14,198,experimental-setup,Experimental setup,used,effective batch size
translation,14,167,hyperparameters,each model,on,train folds
translation,14,167,hyperparameters,each model,fixed,maximum sequence length
translation,14,167,hyperparameters,train folds,of,NLI - TR
translation,14,167,hyperparameters,maximum sequence length,to,128
translation,14,167,hyperparameters,Hyperparameters,fine- tuned,each model
translation,14,168,hyperparameters,common learning rate,of,2 ? 10 ?5
translation,14,168,hyperparameters,batch size,of,8
translation,14,168,hyperparameters,batch size,of,with no gradient accumulation
translation,14,168,hyperparameters,8,has,with no gradient accumulation
translation,14,169,hyperparameters,each model,for,3 epochs
translation,14,169,hyperparameters,each model,using,Hugging - Face 's Transformers Library
translation,14,169,hyperparameters,3 epochs,using,Hugging - Face 's Transformers Library
translation,14,169,hyperparameters,Hyperparameters,fine- tuned,each model
translation,14,134,results,annotation - level label consistency,is,over 90 %
translation,14,134,results,majority - level label consistency,is,over 95 %
translation,14,134,results,Results,has,annotation - level label consistency
translation,14,173,results,NLI -TR,to train,high quality Turkish NLI models
translation,14,173,results,Results,demonstrates,NLI -TR
translation,14,174,results,every model,performed,better
translation,14,174,results,better,on,dev and test folds
translation,14,174,results,better,on,dev folds
translation,14,174,results,dev and test folds,of,SNLI - TR
translation,14,174,results,dev and test folds,of,MultiNLI - TR
translation,14,174,results,dev folds,of,MultiNLI - TR
translation,14,174,results,Results,observe,every model
translation,14,176,results,BERTurk,trained on,Turkish corpus
translation,14,176,results,BERTurk,achieved,highest accuracy
translation,14,176,results,BERTurk,ranked,second
translation,14,176,results,BERT - Multi,used,smaller Turkish corpus
translation,14,176,results,BERT - Multi,ranked,second
translation,14,211,results,all three,converge to,similar performance
translation,14,211,results,similar performance,end of,training
translation,14,211,results,training,on,both datasets
translation,14,211,results,Results,has,all three
translation,14,226,results,All three models,consistently achieve,higher accuracy
translation,14,226,results,higher accuracy,on,XNLI - Dev and XNLI - Test
translation,14,226,results,higher accuracy,when fine-tuned with,MultiNLI - TR
translation,14,226,results,Results,has,All three models
translation,14,227,results,BERTurk,backed by,Turkish-only training corpus
translation,14,227,results,other two models,on,all eight evaluations
translation,14,227,results,BERTurk,has,outperforms
translation,14,227,results,outperforms,has,other two models
translation,14,227,results,Results,illustrates,BERTurk
translation,15,42,experimental-setup,each recast dataset,using,NLTK tokenizer
translation,15,42,experimental-setup,resulting tokens,to,300 - dimensional GloVe vectors
translation,15,42,experimental-setup,300 - dimensional GloVe vectors,trained on,840 billion tokens
translation,15,42,experimental-setup,840 billion tokens,from,Common Crawl
translation,15,42,experimental-setup,840 billion tokens,using,GloVe OOV vector
translation,15,42,experimental-setup,GloVe OOV vector,for,unknown words
translation,15,42,experimental-setup,Experimental setup,preprocess,each recast dataset
translation,15,42,experimental-setup,Experimental setup,map,resulting tokens
translation,15,43,experimental-setup,SGD,with,initial learning rate
translation,15,43,experimental-setup,SGD,with,decay rate
translation,15,43,experimental-setup,initial learning rate,of,0.1
translation,15,43,experimental-setup,decay rate,of,0.99
translation,15,43,experimental-setup,Experimental setup,optimize via,SGD
translation,15,44,experimental-setup,at most 20 epochs,of,training
translation,15,44,experimental-setup,at most 20 epochs,of,training
translation,15,44,experimental-setup,at most 20 epochs,with,optional early stopping
translation,15,44,experimental-setup,accuracy,on,development set
translation,15,44,experimental-setup,learning rate,by,5
translation,15,44,experimental-setup,learning rate,is,< 10 ?5
translation,15,44,experimental-setup,training,when,learning rate
translation,15,44,experimental-setup,learning rate,is,< 10 ?5
translation,15,44,experimental-setup,Experimental setup,allow,at most 20 epochs
translation,15,44,experimental-setup,Experimental setup,divide,learning rate
translation,15,44,experimental-setup,Experimental setup,stop,training
translation,15,103,results,Majority Baseline,Across,six of the ten datasets
translation,15,103,results,best reported results,on,one dataset
translation,15,103,results,Majority Baseline,has,our hypothesis-only model
translation,15,103,results,six of the ten datasets,has,our hypothesis-only model
translation,15,103,results,our hypothesis-only model,has,significantly outperforms
translation,15,103,results,significantly outperforms,has,majority - baseline
translation,15,103,results,outperforming,has,best reported results
translation,15,106,results,largest relative gains,on,humanelicited models
translation,15,106,results,humanelicited models,where,hypothesis-only model
translation,15,106,results,hypothesis-only model,has,more than doubles
translation,15,106,results,more than doubles,has,majority baseline
translation,15,106,results,Results,has,largest relative gains
translation,15,108,results,judged and recast datasets,observe,lower performance margins
translation,15,108,results,lower performance margins,between,majority and hypothesis-only models
translation,15,108,results,majority and hypothesis-only models,compared to,elicited data sets
translation,15,108,results,Results,Among,judged and recast datasets
translation,15,108,results,Results,observe,lower performance margins
translation,15,155,results,hypothesis-only model,performs,similarly
translation,15,155,results,similarly,to,majority baseline
translation,15,155,results,majority baseline,for,entailed examples
translation,15,155,results,improving,by,over 34 %
translation,15,155,results,over 34 %,has,those which are not entailed
translation,15,155,results,Results,has,hypothesis-only model
translation,16,26,ablation-analysis,more of the bias,improves how well,model
translation,16,26,ablation-analysis,model,has,generalises
translation,16,128,ablation-analysis,number of adversarial classifiers,increased,1 to 20
translation,16,128,ablation-analysis,accuracy,of,hypothesis-only bias classifiers
translation,16,128,ablation-analysis,hypothesis-only bias classifiers,reduces from,62 % to 53 %
translation,16,128,ablation-analysis,number of adversarial classifiers,has,accuracy
translation,16,128,ablation-analysis,Ablation analysis,When,number of adversarial classifiers
translation,16,157,experiments,SNLIhard,where,models
translation,16,157,experiments,models,no longer influenced by,hypothesis-only
translation,16,78,hyperparameters,"{ 1 , 5 , 10 , 20 } bias classifiers",for,"{ 256 , 512 , 1024 , 2048 } dimensional sentence representations"
translation,16,78,hyperparameters,Hyperparameters,train with,"{ 1 , 5 , 10 , 20 } bias classifiers"
translation,16,19,model,Model,show,NLI
translation,16,70,model,adversarial training approach,for reducing,hypothesis-only bias
translation,16,70,model,hypothesis-only bias,contained within,sentence representations
translation,16,70,model,Model,follow,adversarial training approach
translation,16,71,model,adversarial training framework,to make use of,multiple adversaries
translation,16,71,model,jointly trained,for predicting,relationship
translation,16,71,model,relationship,between,premise and hypothesis
translation,16,71,model,Model,generalise,adversarial training framework
translation,16,20,results,more bias,removed from,model representations
translation,16,20,results,model representations,has,better
translation,16,20,results,Results,show that,more bias
translation,16,27,results,Our method,improves,model accuracy
translation,16,27,results,model accuracy,across,12 NLI datasets
translation,16,27,results,outperforms,has,previous research
translation,16,27,results,Results,has,Our method
translation,16,121,results,one adversarial classifier,for,"2,048 dimensional sentence representation"
translation,16,121,results,one adversarial classifier,after,representation
translation,16,121,results,representation,was,frozen
translation,16,121,results,chance,to re-learn,hypothesis-only bias
translation,16,121,results,accuracy,of,bias classifiers
translation,16,121,results,accuracy,increased to,62 %
translation,16,121,results,bias classifiers,increased to,62 %
translation,16,121,results,one adversarial classifier,has,accuracy
translation,16,121,results,bias classifiers,has,chance
translation,16,121,results,hypothesis-only bias,has,accuracy
translation,16,121,results,Results,When using,one adversarial classifier
translation,16,136,results,overall model accuracy,does not start to,decrease
translation,16,136,results,20 adversaries,has,overall model accuracy
translation,16,136,results,Results,when using,20 adversaries
translation,16,147,results,non-linear model,used to re-learn,bias
translation,16,147,results,bias,from,frozen sentence representation
translation,16,147,results,less bias,can be recovered,non-linear model
translation,16,147,results,non-linear model,used as,adversarial classifier
translation,16,147,results,adversarial classifier,during,training
translation,16,147,results,training,instead of,linear adversarial classifier
translation,16,147,results,non-linear model,has,less bias
translation,16,147,results,Results,When,non-linear model
translation,16,150,results,linear model,to re-learn,bias
translation,16,150,results,linear model,as,adversary
translation,16,150,results,bias,using,linear model
translation,16,150,results,linear model,as,adversary
translation,16,150,results,adversary,instead of,multi-layer perceptron
translation,16,150,results,amount of bias,can be,recovered
translation,16,150,results,Results,if,linear model
translation,16,158,results,Models,trained using,ensemble of adversaries
translation,16,158,results,ensemble of adversaries,performed,better
translation,16,158,results,better,across,datasets
translation,16,158,results,better,where,statistically significant improvement
translation,16,158,results,statistically significant improvement,compared to,baseline model
translation,16,158,results,baseline model,has,with no adversarial training
translation,16,158,results,Results,has,Models
translation,16,159,results,Models,trained with,one adversary
translation,16,159,results,not significantly better,than,baseline model
translation,16,159,results,SNLI - hard,has,1.1 % improvement
translation,16,159,results,Results,has,Models
translation,16,160,results,statistically significant improvement,when using,ensemble of 20 adversarial classifiers
translation,16,160,results,ensemble of 20 adversarial classifiers,achieving,1.6 % improvement
translation,16,162,results,models,trained with,ensemble of 20 adversarial classifiers
translation,16,162,results,ensemble of 20 adversarial classifiers,performed,better
translation,16,162,results,better,than,only one adversarial classifier
translation,16,162,results,better,when using,only one adversarial classifier
translation,16,162,results,8 of the 13 datasets analysed,has,models
translation,16,162,results,Results,Across,8 of the 13 datasets analysed
translation,16,174,results,models,trained with,ensemble of adversaries
translation,16,174,results,models,performed,better
translation,16,174,results,better,tested on,SNLI - hard
translation,16,174,results,SNLI - hard,compared to using,only one adversarial classifier
translation,16,174,results,Results,has,models
translation,17,10,ablation-analysis,factor of 16,to store,later use
translation,17,10,ablation-analysis,Ablation analysis,through,binary quantization
translation,17,160,ablation-analysis,1 - bit quantization,reducing,storage costs
translation,17,160,ablation-analysis,accuracy,by,0.4 %
translation,17,160,ablation-analysis,storage costs,by,factor
translation,17,160,ablation-analysis,factor,of,16
translation,17,160,ablation-analysis,reduces,has,accuracy
translation,17,160,ablation-analysis,outperforming,has,distillation - based methods
translation,17,160,ablation-analysis,distillation - based methods,has,88.0 % vs. 87.1 %
translation,17,160,ablation-analysis,Ablation analysis,has,1 - bit quantization
translation,17,172,ablation-analysis,Average accuracy,decreases from,88.4 %
translation,17,172,ablation-analysis,88.4 %,to,86.6 %
translation,17,172,ablation-analysis,Ablation analysis,has,Average accuracy
translation,17,100,baselines,Baselines,has,Leave-one- task-out finetuning
translation,17,115,baselines,Baselines,has,Layer -wise pooling approaches
translation,17,118,baselines,LEARNED - COMB,learn,task-specific weighted combination
translation,17,118,baselines,task-specific weighted combination,over,all layers
translation,17,118,baselines,Baselines,has,LEARNED - COMB
translation,17,123,baselines,pool features,with,task-specific Multi-Head Attention ( MHA ) layer
translation,17,123,baselines,MHA,has,pool features
translation,17,123,baselines,Baselines,has,MHA
translation,17,134,hyperparameters,quantization prior,to,leave- one - task - out finetuning
translation,17,134,hyperparameters,quantization prior,to simulate,real-world setting
translation,17,134,hyperparameters,Hyperparameters,apply,quantization prior
translation,17,6,model,computational cost,during,inference
translation,17,6,model,computational cost,can be,amortized
translation,17,6,model,inference,can be,amortized
translation,17,6,model,amortized,over,different predictions ( tasks )
translation,17,6,model,different predictions ( tasks ),using,shared text encoder
translation,17,6,model,Model,has,computational cost
translation,17,25,model,new ways,to make,inference
translation,17,25,model,different models ( models for different tasks ),run over,same piece of text
translation,17,25,model,inference,has,computationally efficient
translation,17,25,model,Model,look at,new ways
translation,17,26,model,new methods,to run,multiple task -specific models
translation,17,26,model,multiple task -specific models,amortizes,computation
translation,17,26,model,computation,over,different tasks
translation,17,26,model,Model,propose,new methods
translation,17,27,model,activations,for,full model
translation,17,27,model,activations,use,smaller task - specific models
translation,17,27,model,Model,compute,activations
translation,17,36,model,multi-task encoder,shared across,tasks
translation,17,148,model,different layer - wise pooling strategies,using,MHA position - wise pooling
translation,17,148,model,Model,consider,different layer - wise pooling strategies
translation,17,57,results,DistilRoBERTa,reaches,95 %
translation,17,57,results,95 %,of,RoBERTa - base 's performance
translation,17,57,results,Results,has,DistilRoBERTa
translation,17,139,results,fine - tuned endto-end,on,single task
translation,17,139,results,Results,has,Baselines
translation,17,141,results,DistilRoBERTa,achieves,competitive accuracy
translation,17,141,results,competitive accuracy,across,many tasks
translation,17,141,results,Results,observe,DistilRoBERTa
translation,17,142,results,Multi-task pretraining,prior to,single - task finetuning
translation,17,142,results,single - task finetuning,improves,results
translation,17,142,results,results,with,average gain
translation,17,142,results,average gain,of,+ 0.2 %
translation,17,142,results,Results,has,Multi-task pretraining
translation,17,144,results,extracting features,from,last layer 's CLS token
translation,17,144,results,poorly,with,15 % drop
translation,17,144,results,15 % drop,in,accuracy
translation,17,144,results,accuracy,compared to,end-to - end finetuned version
translation,17,144,results,freezing,has,pre-trained RoBERTa model
translation,17,144,results,freezing,has,extracting features
translation,17,144,results,end-to - end finetuned version,has,90.5 % ? 75.5 %
translation,17,144,results,Results,observe,freezing
translation,17,147,results,features,across,positions
translation,17,147,results,features,see,slightly higher accuracy
translation,17,147,results,positions,in,last layer
translation,17,147,results,slightly higher accuracy,compared to,CLS token alone
translation,17,147,results,our multi-head attention ( MHA ) pooling,improves,accuracy
translation,17,147,results,accuracy,to,83.3 %
translation,17,147,results,Results,average,features
translation,17,149,results,simple average,over,top 16 layers
translation,17,149,results,top 16 layers,improves,accuracy
translation,17,149,results,accuracy,by,+ 2.2 %
translation,17,149,results,+ 2.2 %,compared to using,last layer
translation,17,149,results,Results,Taking,simple average
translation,17,154,results,multi-head attention ( MHA ) position - wise pooling strategy,performs,best
translation,17,154,results,CLS approach,by,+ 0.9 %
translation,17,154,results,POSITION - AVG strategy,by,+ 0.8 %
translation,17,154,results,outperforming,has,CLS approach
translation,17,154,results,Results,has,multi-head attention ( MHA ) position - wise pooling strategy
translation,17,180,results,Approaches,using,knowledge distillation
translation,17,180,results,Approaches,using,frozen encoders
translation,17,180,results,frozen encoders,reduce,FLOPs
translation,17,180,results,FLOPs,by,an order of magnitude
translation,17,180,results,Results,has,Approaches
translation,18,154,ablation-analysis,proportion,of,label noise
translation,18,154,ablation-analysis,performance,of,InferSent
translation,18,154,ablation-analysis,more rapidly,than,GenNLI
translation,18,154,ablation-analysis,proportion,has,performance
translation,18,154,ablation-analysis,label noise,has,performance
translation,18,154,ablation-analysis,InferSent,has,decreases
translation,18,154,ablation-analysis,decreases,has,more rapidly
translation,18,154,ablation-analysis,Ablation analysis,increase,proportion
translation,18,179,ablation-analysis,copy mechanism,is,essential
translation,18,179,ablation-analysis,Ablation analysis,has,copy mechanism
translation,18,181,ablation-analysis,generative training and fine-tuning objectives,to be,helpful
translation,18,181,ablation-analysis,Ablation analysis,find,generative training and fine-tuning objectives
translation,18,6,baselines,generative classifier,for,NLI tasks
translation,18,6,baselines,GenNLI,has,generative classifier
translation,18,110,baselines,"ESIM ( Chen et al. , 2017a )",as,discriminative baselines
translation,18,113,baselines,pretrained models,are,"BERT ( Devlin et al. , 2019 )"
translation,18,113,baselines,pretrained models,are,"RoBERTa ( Liu et al. , 2019 )"
translation,18,113,baselines,pretrained models,are,"XLNet ( Yang et al. , 2019 )"
translation,18,113,baselines,Baselines,has,pretrained models
translation,18,129,baselines,GenNLI and discriminative baselines,use,"Adam ( Kingma and Ba , 2015 ) optimizer"
translation,18,129,baselines,GenNLI and discriminative baselines,use,SGD
translation,18,129,baselines,"Adam ( Kingma and Ba , 2015 ) optimizer",with,learning rates
translation,18,129,baselines,"Adam ( Kingma and Ba , 2015 ) optimizer",with,SGD
translation,18,129,baselines,"Adam ( Kingma and Ba , 2015 ) optimizer",with,learning rates
translation,18,129,baselines,learning rates,of,0.001 and 0.1
translation,18,129,baselines,SGD,with,learning rates
translation,18,129,baselines,model,with,best performance
translation,18,129,baselines,best performance,on,dev set
translation,18,129,baselines,learning rates,has,"0.1 , 0.5 , 1 , and 2"
translation,18,129,baselines,Baselines,For,GenNLI and discriminative baselines
translation,18,31,experiments,GenNLI,to generate,hypotheses
translation,18,31,experiments,hypotheses,for,given premises and labels
translation,18,111,experiments,InferSent,uses,BiLSTM network
translation,18,111,experiments,BiLSTM network,with,max pooling
translation,18,111,experiments,BiLSTM network,to learn,generic sentence embeddings
translation,18,111,experiments,generic sentence embeddings,perform,well
translation,18,111,experiments,well,on,several NLI tasks
translation,18,117,hyperparameters,generative and discriminative models,initialized with,GloVe pretrained word embeddings
translation,18,117,hyperparameters,Hyperparameters,has,generative and discriminative models
translation,18,118,hyperparameters,word embedding dimension,set to,300
translation,18,118,hyperparameters,the LSTM hidden state dimension,set to,300
translation,18,118,hyperparameters,Hyperparameters,has,word embedding dimension
translation,18,118,hyperparameters,Hyperparameters,has,the LSTM hidden state dimension
translation,18,119,hyperparameters,parameters,including,word embeddings
translation,18,119,hyperparameters,parameters,updated,training
translation,18,119,hyperparameters,Hyperparameters,has,parameters
translation,18,120,hyperparameters,label embedding dimensionality,for,GenNLI
translation,18,120,hyperparameters,label embedding dimensionality,set to,100
translation,18,120,hyperparameters,Hyperparameters,has,label embedding dimensionality
translation,18,123,hyperparameters,training,includes,two steps
translation,18,123,hyperparameters,model,first trained with,generative objective only
translation,18,123,hyperparameters,generative objective only,for,20 epochs
translation,18,123,hyperparameters,generative objective only,followed by,discriminative fine-tuning objective
translation,18,123,hyperparameters,discriminative fine-tuning objective,for,15 epochs
translation,18,123,hyperparameters,two steps,has,model
translation,18,123,hyperparameters,Hyperparameters,has,training
translation,18,131,hyperparameters,"Hugging Face PyTorch implementation ( Wolf et al. , 2019 )",of,"pretrained transformer ( Vaswani et al. , 2017 ) models"
translation,18,131,hyperparameters,Hyperparameters,use,"Hugging Face PyTorch implementation ( Wolf et al. , 2019 )"
translation,18,134,hyperparameters,pretrained models,on,our training sets
translation,18,134,hyperparameters,pretrained models,for,10 epochs
translation,18,134,hyperparameters,our training sets,for,10 epochs
translation,18,134,hyperparameters,Hyperparameters,fine - tune,pretrained models
translation,18,20,model,conditional probability,of,hypothesis
translation,18,20,model,conditional probability,parameterizing,distribution
translation,18,20,model,hypothesis,given,premise and the label
translation,18,20,model,distribution,using,sequence - to-sequence model
translation,18,20,model,sequence - to-sequence model,with,attention
translation,18,20,model,Model,call,GenNLI
translation,18,21,model,training objectives,for,discriminative fine-tuning
translation,18,21,model,training objectives,comparing,classical discriminative criteria
translation,18,21,model,discriminative fine-tuning,of,our generative classifiers
translation,18,21,model,discriminative fine-tuning,comparing,classical discriminative criteria
translation,18,21,model,Model,explore,training objectives
translation,18,42,model,generative models,for,natural language inference
translation,18,42,model,Model,develop,generative models
translation,18,23,results,strong results,with,simple unbounded modification
translation,18,23,results,simple unbounded modification,to,log loss
translation,18,23,results,simple unbounded modification,call,infinilog loss
translation,18,23,results,Results,find,strong results
translation,18,26,results,better performance,than,discriminative classifiers
translation,18,26,results,discriminative classifiers,under,small data setting
translation,18,26,results,GenNLI,has,better performance
translation,18,26,results,Results,has,GenNLI
translation,18,27,results,all BERT - style pretrained models,on,four of the five datasets
translation,18,27,results,100 instances per class,has,GenNLI
translation,18,27,results,GenNLI,has,consistently outperforms
translation,18,27,results,consistently outperforms,has,all BERT - style pretrained models
translation,18,27,results,Results,when,GenNLI
translation,18,27,results,Results,limited to,100 instances per class
translation,18,30,results,discriminative classifiers,when,training data
translation,18,30,results,discriminative classifiers,training labels are,randomly corrupted
translation,18,30,results,training data,shows,severe label imbalance
translation,18,30,results,GenNLI,has,outperforms
translation,18,30,results,outperforms,has,discriminative classifiers
translation,18,30,results,Results,has,GenNLI
translation,18,139,results,training sets,with,100 or fewer instances per class
translation,18,139,results,pretrained baselines,on,all datasets
translation,18,139,results,all datasets,except for,MRPC
translation,18,139,results,training sets,has,GenNLI
translation,18,139,results,100 or fewer instances per class,has,GenNLI
translation,18,139,results,GenNLI,has,outperforms
translation,18,139,results,outperforms,has,pretrained baselines
translation,18,139,results,Results,When using,training sets
translation,18,141,results,better performance,than,other discriminative baselines
translation,18,141,results,small training sets,has,GenNLI
translation,18,141,results,Results,With,small training sets
translation,18,143,results,full training set,has,discriminative baselines
translation,18,143,results,discriminative baselines,has,outperform
translation,18,143,results,outperform,has,GenNLI
translation,18,143,results,Results,see that,full training set
translation,18,143,results,Results,on,full training set
translation,18,153,results,all of the models,robust to,slight noise
translation,18,153,results,Results,find,all of the models
translation,18,166,results,RoBERTa,performs,best
translation,18,169,results,GenNLI,shows,smaller or comparable decrease
translation,18,169,results,smaller or comparable decrease,in,performance
translation,18,169,results,Results,has,GenNLI
translation,18,170,results,GenNLI,always outperforms,In - ferSent
translation,18,170,results,In - ferSent,when keeping,only 10 % of the instances
translation,18,170,results,only 10 % of the instances,for,selected class
translation,18,170,results,generative and discriminative classifiers,has,GenNLI
translation,18,170,results,Results,comparing,generative and discriminative classifiers
translation,18,171,results,instances,in,selected class
translation,18,171,results,InferSent,begins to perform,better
translation,18,171,results,better,than,GenNLI
translation,18,171,results,increases,has,InferSent
translation,18,230,results,generations,for,entailment label
translation,18,230,results,entailment label,look better than,contradiction
translation,18,230,results,Results,note,generations
translation,18,241,results,infinilog,performs,best
translation,18,241,results,best,when using,full training set
translation,18,241,results,full training set,on,four out of five datasets
translation,18,241,results,Results,worth noting,infinilog
translation,19,93,ablation-analysis,syntax,by using,GCN
translation,19,93,ablation-analysis,beneficial,for,generalization of BERT
translation,19,93,ablation-analysis,beneficial,especially identifying,different syntactic structures
translation,19,93,ablation-analysis,generalization of BERT,especially identifying,different syntactic structures
translation,19,93,ablation-analysis,different syntactic structures,in,sentence pair
translation,19,93,ablation-analysis,Ablation analysis,incorporating,syntax
translation,19,79,baselines,three variants,of,proposed model
translation,19,79,baselines,three variants,linking,words
translation,19,79,baselines,proposed model,based on,BERT
translation,19,79,baselines,words,between,premise
translation,19,79,baselines,words,between,hypothesis
translation,19,79,baselines,hypothesis,at,GCN layer
translation,19,79,baselines,co-attention links,linking,same lemmarized words
translation,19,79,baselines,Baselines,consider,three variants
translation,19,81,baselines,BERT - CLS,adds,classifier
translation,19,81,baselines,classifier,to,vector representation
translation,19,81,baselines,vector representation,of [ CLS ] token,BERT model
translation,19,81,baselines,vector representation,of [ CLS ] token,BERT - Attn
translation,19,81,baselines,vector representation,of [ CLS ] token,word output
translation,19,81,baselines,vector representation,of [ CLS ] token,sequentially
translation,19,81,baselines,vector representation,of [ CLS ] token,co-attention layer and classifier
translation,19,81,baselines,vector representation,of [ CLS ] token,BERT + LF
translation,19,81,baselines,vector representation,of [ CLS ] token,SPINN
translation,19,81,baselines,vector representation,of,co-attention layer and classifier
translation,19,81,baselines,vector representation,of,BERT + LF
translation,19,81,baselines,vector representation,input of,classifier layer
translation,19,81,baselines,BERT - Attn,feeds,word output
translation,19,81,baselines,word output,of,BERT
translation,19,81,baselines,sequentially,to,co-attention layer and classifier
translation,19,81,baselines,BERT + LF,adds,syntactic features
translation,19,81,baselines,syntactic features,input of,classifier layer
translation,19,81,baselines,SPINN,encodes,sentences
translation,19,81,baselines,sentences,with,parse tree
translation,19,81,baselines,BERT,has,sequentially
translation,19,86,baselines,BERTrelated baselines,optimised using,Adam optimizer
translation,19,86,baselines,Baselines,has,BERTrelated baselines
translation,19,84,hyperparameters,GCN layers,set at,3
translation,19,85,hyperparameters,BERT components,in,BERT - related models
translation,19,85,hyperparameters,BERT components,initialized with,same pre-trained weights
translation,19,85,hyperparameters,Hyperparameters,has,BERT components
translation,19,87,hyperparameters,two different Adam optimizers,for,BERT and the other components
translation,19,87,hyperparameters,Hyperparameters,adopt,two different Adam optimizers
translation,19,6,model,graph convolutional network,to represent,syntax - based matching graph
translation,19,6,model,syntax - based matching graph,with,heterogeneous matching patterns
translation,19,6,model,Model,investigate,dependency trees
translation,19,23,model,graph convolutional network ( GCN ),to represent,whole matching graph structure
translation,19,23,model,Model,has,graph convolutional network ( GCN )
translation,19,24,results,performance,of,proposed model
translation,19,24,results,performance,is,much better
translation,19,24,results,proposed model,is,much better
translation,19,24,results,much better,than,BERT
translation,19,24,results,much better,than,other syntax - based baselines
translation,19,24,results,premise and non-entailment hypothesis,have,high lexical overlap
translation,19,24,results,premise and non-entailment hypothesis,have,different syntactic structures
translation,19,80,results,outputs,of,BERT and GCN
translation,19,80,results,outputs,results in,little performance improvement
translation,19,80,results,Results,combining,outputs
translation,19,92,results,baselines,including,BERT
translation,19,92,results,our models,has,outperform
translation,19,92,results,outperform,has,baselines
translation,19,92,results,Results,observed that,our models
translation,19,94,results,results,of,GCN - based methods
translation,19,94,results,results,seen that,linking words
translation,19,94,results,GCN - based methods,seen that,linking words
translation,19,94,results,linking words,between,sentence pair
translation,19,94,results,linking words,lead to,better performance
translation,19,94,results,sentence pair,by,co-attention
translation,19,94,results,Results,comparing,results
translation,19,94,results,Results,of,GCN - based methods
translation,19,97,results,syntax,by,GCN
translation,19,97,results,averaged precision,on,six categories of HANS
translation,19,97,results,performance,on,in-domain dataset MNLI
translation,19,97,results,slightly improves,has,performance
translation,19,97,results,Results,incorporating,syntax
translation,19,99,results,proposed GCN - based methods,do,not bring as much improvement
translation,19,99,results,not bring as much improvement,compared to,baselines
translation,19,99,results,baselines,on,Passive subcategory
translation,20,9,model,iterative adversarial fine-tuning method,uses,synthetically created training data
translation,20,9,model,synthetically created training data,based on,boolean and non-boolean heuristics
translation,20,9,model,Model,present,iterative adversarial fine-tuning method
