topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,145,baselines,kv memory,pre-trained on,"persona - chat ( 10,907 dialogues )","kv memory pre-trained on persona - chat ( 10,907 dialogues )",0.7821681499481201
translation,0,145,baselines,kv memory,evaluated on,persona - chat validation ( 1000 dialogues ) and test ( 968 dialogues ) sets,kv memory evaluated on persona - chat validation ( 1000 dialogues ) and test ( 968 dialogues ) sets,0.7109123468399048
translation,0,145,baselines,baselines,has,kv memory,baselines has kv memory,0.5693128108978271
translation,0,146,baselines,poly-encoder,pre-trained on,convai2 dataset,poly-encoder pre-trained on convai2 dataset,0.7850342988967896
translation,0,146,baselines,poly-encoder,evaluated on,"validation ( 1,009 conversations ) and test sets ( 980 conversations )","poly-encoder evaluated on validation ( 1,009 conversations ) and test sets ( 980 conversations )",0.6908272504806519
translation,0,146,baselines,"validation ( 1,009 conversations ) and test sets ( 980 conversations )",from,blendedskilltalk dataset ( bst ),"validation ( 1,009 conversations ) and test sets ( 980 conversations ) from blendedskilltalk dataset ( bst )",0.5428765416145325
translation,0,146,baselines,baselines,has,poly-encoder,baselines has poly-encoder,0.5557693839073181
translation,0,159,hyperparameters,semantic network,used,pre-trained common crawl glove word embeddings ( 300 dimensional ),semantic network used pre-trained common crawl glove word embeddings ( 300 dimensional ),0.5553312301635742
translation,0,159,hyperparameters,pre-trained common crawl glove word embeddings ( 300 dimensional ),with,threshold ? 0.6,pre-trained common crawl glove word embeddings ( 300 dimensional ) with threshold ? 0.6,0.6109426021575928
translation,0,159,hyperparameters,threshold ? 0.6,for adding,edge,threshold ? 0.6 for adding edge,0.7395163774490356
translation,0,159,hyperparameters,edge,between,nodes,edge between nodes,0.6814387440681458
translation,0,159,hyperparameters,hyperparameters,For building,semantic network,hyperparameters For building semantic network,0.6779149770736694
translation,0,8,model,novel probabilistic approach,using,markov random fields ( mrf ),novel probabilistic approach using markov random fields ( mrf ),0.7090462446212769
translation,0,8,model,existing deep-learning methods,for,improved next utterance prediction,existing deep-learning methods for improved next utterance prediction,0.5857141017913818
translation,0,8,model,model,propose,novel probabilistic approach,model propose novel probabilistic approach,0.7217725515365601
translation,0,30,model,novel probabilistic approach,using,markov random fields ( mrf ),novel probabilistic approach using markov random fields ( mrf ),0.7090462446212769
translation,0,30,model,markov random fields ( mrf ),to model,mutual knowledge,markov random fields ( mrf ) to model mutual knowledge,0.7079631090164185
translation,0,30,model,markov random fields ( mrf ),to model,contextual relevance,markov random fields ( mrf ) to model contextual relevance,0.6921594738960266
translation,0,30,model,model,propose,novel probabilistic approach,model propose novel probabilistic approach,0.7217725515365601
translation,0,31,model,existing deep-learning methods,with,our model,existing deep-learning methods with our model,0.5736956000328064
translation,0,31,model,our model,for,improved next utterance prediction,our model for improved next utterance prediction,0.6247735619544983
translation,0,31,model,model,augment,existing deep-learning methods,model augment existing deep-learning methods,0.6386530995368958
translation,0,33,model,algorithm ( mrf - chat ),to augment,existing statistical deep-learning methods,algorithm ( mrf - chat ) to augment existing statistical deep-learning methods,0.7180009484291077
translation,0,33,model,existing statistical deep-learning methods,to improve,performance,existing statistical deep-learning methods to improve performance,0.6589317917823792
translation,0,33,model,performance,of,conversational agents,performance of conversational agents,0.5960277915000916
translation,0,34,results,mrf - chat,is,model agnostic,mrf - chat is model agnostic,0.5883081555366516
translation,0,34,results,mrf - chat,independent of,base model,mrf - chat independent of base model,0.7300466299057007
translation,0,34,results,results,has,mrf - chat,results has mrf - chat,0.5239387154579163
translation,0,192,results,augmented poly- encoder,performs better,polyencoder,augmented poly- encoder performs better polyencoder,0.6720934510231018
translation,0,192,results,conversations,has,with length of 4 utterances,conversations has with length of 4 utterances,0.6069508194923401
translation,0,192,results,conversations,has,augmented poly- encoder,conversations has augmented poly- encoder,0.5849112868309021
translation,0,192,results,with length of 4 utterances,has,augmented poly- encoder,with length of 4 utterances has augmented poly- encoder,0.615363359451294
translation,0,192,results,results,For,conversations,results For conversations,0.5373092889785767
translation,0,206,results,outperform,on,hits@1 and mrr metrics,outperform on hits@1 and mrr metrics,0.5754607915878296
translation,0,206,results,kv - memory and poly- encoder alone,on,hits@1 and mrr metrics,kv - memory and poly- encoder alone on hits@1 and mrr metrics,0.5491452813148499
translation,0,206,results,poly-encoder + mrf - chat,has,outperform,poly-encoder + mrf - chat has outperform,0.6542309522628784
translation,0,206,results,outperform,has,kv - memory and poly- encoder alone,outperform has kv - memory and poly- encoder alone,0.6101057529449463
translation,0,206,results,results,has,automatic evaluation,results has automatic evaluation,0.5600289106369019
translation,0,207,results,mean improvement,on,hits@1,mean improvement on hits@1,0.5447463393211365
translation,0,207,results,hits@1,for,kv - memory,hits@1 for kv - memory,0.6447118520736694
translation,0,207,results,hits@1,for,poly-encoders,hits@1 for poly-encoders,0.658850908279419
translation,0,207,results,hits@1,for,poly-encoders,hits@1 for poly-encoders,0.658850908279419
translation,0,207,results,kv - memory,is,0.159,kv - memory is 0.159,0.5743706822395325
translation,0,207,results,poly-encoders,is,0.186,poly-encoders is 0.186,0.5685795545578003
translation,0,207,results,results,has,mean improvement,results has mean improvement,0.5219807028770447
translation,0,208,results,mean ?,for,mrr,mean ? for mrr,0.6219973564147949
translation,0,208,results,mrr,for,kv - memory,mrr for kv - memory,0.6610974073410034
translation,0,208,results,mrr,for,poly-encoders,mrr for poly-encoders,0.6489666700363159
translation,0,208,results,mrr,is,0.13,mrr is 0.13,0.5788066387176514
translation,0,208,results,mrr,for,poly-encoders,mrr for poly-encoders,0.6489666700363159
translation,0,208,results,kv - memory,is,0.13,kv - memory is 0.13,0.5986575484275818
translation,0,208,results,poly-encoders,is,0.109,poly-encoders is 0.109,0.5797998905181885
translation,0,208,results,results,has,mean ?,results has mean ?,0.5121198892593384
translation,0,209,results,our algorithm 's performance,is,robust,our algorithm 's performance is robust,0.6041085720062256
translation,0,209,results,robust,to,choice,robust to choice,0.6192736625671387
translation,0,209,results,robust,to,conversation length,robust to conversation length,0.5401455760002136
translation,0,209,results,conversation length,in,automatic metrics,conversation length in automatic metrics,0.4748845398426056
translation,0,209,results,results,see that,our algorithm 's performance,results see that our algorithm 's performance,0.6384646892547607
translation,0,216,results,modelling mutual knowledge and contextual relevance,improves,performance,modelling mutual knowledge and contextual relevance improves performance,0.5872045159339905
translation,0,216,results,performance,of,state - of - the - art models,performance of state - of - the - art models,0.5605788826942444
translation,0,216,results,state - of - the - art models,by selecting,utterances,state - of - the - art models by selecting utterances,0.7231793999671936
translation,0,216,results,utterances,with,most relevant concepts,utterances with most relevant concepts,0.6133745908737183
translation,0,218,results,mrf - chat,produced,more,mrf - chat produced more,0.6956930756568909
translation,0,218,results,mrf - chat,produced,on -topic responses,mrf - chat produced on -topic responses,0.6897523403167725
translation,0,218,results,on -topic responses,than,base models,on -topic responses than base models,0.5581561326980591
translation,0,218,results,human evaluations,has,mrf - chat,human evaluations has mrf - chat,0.5885769724845886
translation,0,218,results,more,has,on -topic responses,more has on -topic responses,0.5713048577308655
translation,1,151,ablation-analysis,spm,makes,slight contributions,spm makes slight contributions,0.5432291626930237
translation,1,151,ablation-analysis,slight contributions,to,nct model,slight contributions to nct model,0.5489016771316528
translation,1,151,ablation-analysis,nct model,in terms of,bleu,nct model in terms of bleu,0.6274317502975464
translation,1,151,ablation-analysis,less significant,than,dcm,less significant than dcm,0.6086525321006775
translation,1,151,ablation-analysis,ablation analysis,has,spm,ablation analysis has spm,0.5418246984481812
translation,1,129,baselines,original model,is,rnn - based,original model is rnn - based,0.5766943097114563
translation,1,129,baselines,additional encoder,to incorporate,mixed - language dialogue history,additional encoder to incorporate mixed - language dialogue history,0.7032874822616577
translation,1,129,baselines,context - aware nmt systems,has,original model,context - aware nmt systems has original model,0.5628893375396729
translation,1,110,experimental-setup,transformer models,contain,l = 6 encoder layers,transformer models contain l = 6 encoder layers,0.5930576324462891
translation,1,110,experimental-setup,transformer models,contain,l = 6 decoder layers,transformer models contain l = 6 decoder layers,0.5789194107055664
translation,1,110,experimental-setup,experimental setup,has,transformer models,experimental setup has transformer models,0.5094819664955139
translation,1,111,experimental-setup,training step,for,first pre-training stage,training step for first pre-training stage,0.6458976864814758
translation,1,111,experimental-setup,training step,for,second fine-tuning stage,training step for second fine-tuning stage,0.6231564283370972
translation,1,111,experimental-setup,first pre-training stage,set to,"t 1 = 200,000","first pre-training stage set to t 1 = 200,000",0.7158580422401428
translation,1,111,experimental-setup,second fine-tuning stage,set to,"t 2 = 5,000","second fine-tuning stage set to t 2 = 5,000",0.7172166109085083
translation,1,111,experimental-setup,experimental setup,has,training step,experimental setup has training step,0.5325546264648438
translation,1,112,experimental-setup,batch size,for,each gpu,batch size for each gpu,0.6036682724952698
translation,1,112,experimental-setup,each gpu,set to,4096 tokens,each gpu set to 4096 tokens,0.6593772172927856
translation,1,112,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,1,113,experimental-setup,experiments,in,first stage,experiments in first stage,0.5838447213172913
translation,1,113,experimental-setup,first stage,conducted utilizing,8 nvidia tesla v100 gpus,first stage conducted utilizing 8 nvidia tesla v100 gpus,0.6434941291809082
translation,1,113,experimental-setup,4 gpus,for,second stage,4 gpus for second stage,0.6077414155006409
translation,1,115,experimental-setup,"adam ( kingma and ba , 2014 )",with,? 1 = 0.9 and ? 2 = 0.998,"adam ( kingma and ba , 2014 ) with ? 1 = 0.9 and ? 2 = 0.998",0.6532140970230103
translation,1,115,experimental-setup,learning rate,set to,1.0,learning rate set to 1.0,0.7091217041015625
translation,1,115,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,1,116,experimental-setup,label smoothing,set to,0.1,label smoothing set to 0.1,0.6640933752059937
translation,1,116,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,1,117,experimental-setup,dropout,of,0.1/0.3,dropout of 0.1/0.3,0.6290892958641052
translation,1,117,experimental-setup,0.1/0.3,for,base and big setting,0.1/0.3 for base and big setting,0.6533445715904236
translation,1,118,experimental-setup,| t |,set to,10,| t | set to 10,0.7035831809043884
translation,1,118,experimental-setup,experimental setup,has,| t |,experimental setup has | t |,0.5305099487304688
translation,1,121,experimental-setup,beam size,set to,4,beam size set to 4,0.7776850461959839
translation,1,121,experimental-setup,length penalty,is,0.6,length penalty is 0.6,0.5568951964378357
translation,1,121,experimental-setup,inference,has,beam size,inference has beam size,0.5730389356613159
translation,1,121,experimental-setup,inference,has,length penalty,inference has length penalty,0.5228193998336792
translation,1,121,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,1,121,experimental-setup,experimental setup,has,length penalty,experimental setup has length penalty,0.4614953398704529
translation,1,7,experiments,four auxiliary tasks,including,monolingual response generation,four auxiliary tasks including monolingual response generation,0.6104792356491089
translation,1,7,experiments,four auxiliary tasks,including,cross-lingual response generation,four auxiliary tasks including cross-lingual response generation,0.6146933436393738
translation,1,7,experiments,four auxiliary tasks,including,next utterance discrimination,four auxiliary tasks including next utterance discrimination,0.6671242713928223
translation,1,7,experiments,four auxiliary tasks,including,speaker identification,four auxiliary tasks including speaker identification,0.684770941734314
translation,1,8,experiments,nct model,through,training objectives,nct model through training objectives,0.6827154755592346
translation,1,30,experiments,csa - nct framework,datasets of,different language pairs,csa - nct framework datasets of different language pairs,0.7291982769966125
translation,1,114,experiments,8*4096 and 4*4096 tokens,per,update,8*4096 and 4*4096 tokens per update,0.661396324634552
translation,1,114,experiments,update,for,all experiments,update for all experiments,0.5762354731559753
translation,1,114,experiments,all experiments,in,first-stage and second-stage,all experiments in first-stage and second-stage,0.5438341498374939
translation,1,6,model,chat translation,by introducing,modeling of dialogue characteristics,chat translation by introducing modeling of dialogue characteristics,0.6830337047576904
translation,1,6,model,modeling of dialogue characteristics,into,nct model,modeling of dialogue characteristics into nct model,0.6047919988632202
translation,1,6,model,model,promote,chat translation,model promote chat translation,0.6666784286499023
translation,1,21,model,coherence - speaker - aware nct ( csa - nct ) training framework,to improve,nct model,coherence - speaker - aware nct ( csa - nct ) training framework to improve nct model,0.6303644776344299
translation,1,21,model,nct model,by making use of,dialogue characteristics,nct model by making use of dialogue characteristics,0.7000667452812195
translation,1,21,model,dialogue characteristics,in,conversations,dialogue characteristics in conversations,0.5127557516098022
translation,1,21,model,model,propose,coherence - speaker - aware nct ( csa - nct ) training framework,model propose coherence - speaker - aware nct ( csa - nct ) training framework,0.6320511102676392
translation,1,139,results,substantially outperforms,by,large margin,substantially outperforms by large margin,0.6206852197647095
translation,1,139,results,sentencelevel / context - aware baselines,by,large margin,sentencelevel / context - aware baselines by large margin,0.5454240441322327
translation,1,139,results,1.02,on,en?de,1.02 on en?de,0.6750983595848083
translation,1,139,results,base setting,has,our model,base setting has our model,0.5759850144386292
translation,1,139,results,our model,has,substantially outperforms,our model has substantially outperforms,0.6102578043937683
translation,1,139,results,substantially outperforms,has,sentencelevel / context - aware baselines,substantially outperforms has sentencelevel / context - aware baselines,0.5883991718292236
translation,1,140,results,csa - nct,performs,better,csa - nct performs better,0.6825180649757385
translation,1,140,results,better,on,two directions,better on two directions,0.5721273422241211
translation,1,140,results,lower,than,gate - transformer + ft,lower than gate - transformer + ft,0.617149293422699
translation,1,140,results,ter,has,csa - nct,ter has csa - nct,0.7132515907287598
translation,1,140,results,0.9 ? and 0.7 ?,has,lower,0.9 ? and 0.7 ? has lower,0.5784598588943481
translation,1,140,results,results,In term of,ter,results In term of ter,0.6863987445831299
translation,1,141,results,big setting,on,en?de and de?en,big setting on en?de and de?en,0.6401310563087463
translation,1,141,results,big setting,on,our model,big setting on our model,0.5685731172561646
translation,1,141,results,big setting,on,other existing systems,big setting on other existing systems,0.5768741965293884
translation,1,141,results,our model,consistently surpasses,baselines,our model consistently surpasses baselines,0.7652765512466431
translation,1,141,results,our model,consistently surpasses,other existing systems,our model consistently surpasses other existing systems,0.7454800605773926
translation,1,141,results,big setting,has,our model,big setting has our model,0.5874620079994202
translation,1,141,results,en?de and de?en,has,our model,en?de and de?en has our model,0.6817398071289062
translation,1,141,results,results,Under,big setting,results Under big setting,0.5601499676704407
translation,1,144,results,our model,presents,notable improvements,our model presents notable improvements,0.6498327851295471
translation,1,144,results,notable improvements,over,all comparison models,notable improvements over all comparison models,0.6919194459915161
translation,1,144,results,all comparison models,by,at least 2.43 ? and 0.77 ? bleu gains,all comparison models by at least 2.43 ? and 0.77 ? bleu gains,0.5737617611885071
translation,1,144,results,all comparison models,by,1.73 ? and 1.43 ? bleu gains,all comparison models by 1.73 ? and 1.43 ? bleu gains,0.5387669205665588
translation,1,144,results,at least 2.43 ? and 0.77 ? bleu gains,under,base setting,at least 2.43 ? and 0.77 ? bleu gains under base setting,0.6393013000488281
translation,1,144,results,1.73 ? and 1.43 ? bleu gains,under,big setting,1.73 ? and 1.43 ? bleu gains under big setting,0.6252944469451904
translation,1,144,results,en?zh and zh?en,has,our model,en?zh and zh?en has our model,0.6426789164543152
translation,1,144,results,results,on,en?zh and zh?en,results on en?zh and zh?en,0.5691802501678467
translation,1,149,results,results,under,big setting,results under big setting,0.5601499676704407
translation,1,150,results,dcm,substantially improves,nct model,dcm substantially improves nct model,0.7042081952095032
translation,1,150,results,nct model,in terms of,bleu and ter metrics,nct model in terms of bleu and ter metrics,0.6644065976142883
translation,1,167,results,our model,generates,"more coherent , speaker - relevant , and fluent translations","our model generates more coherent , speaker - relevant , and fluent translations",0.5901129245758057
translation,1,167,results,"more coherent , speaker - relevant , and fluent translations",compared with,other models,"more coherent , speaker - relevant , and fluent translations compared with other models",0.61504065990448
translation,1,167,results,results,show,our model,results show our model,0.6888449192047119
translation,2,90,ablation-analysis,performance,by,0.4 and 0.12,performance by 0.4 and 0.12,0.6126594543457031
translation,2,90,ablation-analysis,performance,in terms of,f1,performance in terms of f1,0.7362748980522156
translation,2,90,ablation-analysis,0.4 and 0.12,in terms of,f1,0.4 and 0.12 in terms of f1,0.7210088968276978
translation,2,90,ablation-analysis,f1,all on,duconv and newsdialog,f1 all on duconv and newsdialog,0.6945160031318665
translation,2,90,ablation-analysis,slightly improve,has,performance,slightly improve has performance,0.5619211196899414
translation,2,90,ablation-analysis,ablation analysis,Pretraining with,mlm objective,ablation analysis Pretraining with mlm objective,0.7365589141845703
translation,2,91,ablation-analysis,overall performance,especially,f1 cross,overall performance especially f1 cross,0.6521875858306885
translation,2,91,ablation-analysis,f1 cross,could be further improved by,at least 0.75 and 2.6,f1 cross could be further improved by at least 0.75 and 2.6,0.6981611251831055
translation,2,91,ablation-analysis,span boundary objective,has,overall performance,span boundary objective has overall performance,0.5281418561935425
translation,2,91,ablation-analysis,ablation analysis,considering,span boundary objective,ablation analysis considering span boundary objective,0.6914316415786743
translation,2,96,ablation-analysis,domain- adaptive pretraining,on,crosswoz,domain- adaptive pretraining on crosswoz,0.588158130645752
translation,2,96,ablation-analysis,domain- adaptive pretraining,improve,performance,domain- adaptive pretraining improve performance,0.6974730491638184
translation,2,96,ablation-analysis,ablation analysis,has,domain- adaptive pretraining,ablation analysis has domain- adaptive pretraining,0.5677886009216309
translation,2,97,ablation-analysis,f1 scores,on,intent and slot,f1 scores on intent and slot,0.5747759342193604
translation,2,97,ablation-analysis,intent and slot,could be,further improved,intent and slot could be further improved,0.7291629314422607
translation,2,97,ablation-analysis,adding either sbo or pmo,has,f1 scores,adding either sbo or pmo has f1 scores,0.6054096817970276
translation,2,97,ablation-analysis,ablation analysis,has,adding either sbo or pmo,ablation analysis has adding either sbo or pmo,0.5731686949729919
translation,2,80,hyperparameters,network parameters,of,our model,network parameters of our model,0.5515972971916199
translation,2,80,hyperparameters,initialized,using,pretrained language model,initialized using pretrained language model,0.5876410007476807
translation,2,80,hyperparameters,hyperparameters,has,network parameters,hyperparameters has network parameters,0.47996386885643005
translation,2,81,hyperparameters,batch size,set to,128,batch size set to 128,0.7535710334777832
translation,2,81,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,2,82,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,2,82,hyperparameters,learning rate,to update,parameters,learning rate to update parameters,0.724581241607666
translation,2,82,hyperparameters,5e - 5,to update,parameters,5e - 5 to update parameters,0.7601245045661926
translation,2,82,hyperparameters,learning rate,has,5e - 5,learning rate has 5e - 5,0.5997864007949829
translation,2,82,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,2,5,model,effectiveness,of,domainadaptive pretraining objectives,effectiveness of domainadaptive pretraining objectives,0.5458935499191284
translation,2,5,model,domainadaptive pretraining objectives,on,downstream tasks,domainadaptive pretraining objectives on downstream tasks,0.5277445912361145
translation,2,5,model,model,probe,effectiveness,model probe effectiveness,0.7505983710289001
translation,2,15,model,"bert model ( devlin et al. , 2019 )",with,three different kinds of unsupervised pretraining objectives,"bert model ( devlin et al. , 2019 ) with three different kinds of unsupervised pretraining objectives",0.5776011943817139
translation,2,15,model,three different kinds of unsupervised pretraining objectives,on,domain-specific training set,three different kinds of unsupervised pretraining objectives on domain-specific training set,0.5180366635322571
translation,2,15,model,domain-specific training set,of,each target task,domain-specific training set of each target task,0.5592260360717773
translation,2,15,model,model,continuously pretrain,"bert model ( devlin et al. , 2019 )","model continuously pretrain bert model ( devlin et al. , 2019 )",0.7272519469261169
translation,2,17,model,correlation,between,arguments and predicates,correlation between arguments and predicates,0.6772812008857727
translation,2,88,results,existing models,benefit from,domain- adaptive pretraining,existing models benefit from domain- adaptive pretraining,0.640097439289093
translation,2,88,results,domain- adaptive pretraining,achieving,new state - of,domain- adaptive pretraining achieving new state - of,0.6725390553474426
translation,2,88,results,two tasks,has,existing models,two tasks has existing models,0.5870025157928467
translation,2,88,results,results,see that,two tasks,results see that two tasks,0.6167418956756592
translation,2,88,results,results,on,two tasks,results on two tasks,0.49677833914756775
translation,2,93,results,our proposed perturbation masking objective,boosts,performance,our proposed perturbation masking objective boosts performance,0.680615246295929
translation,2,93,results,performance,by,larger margin,performance by larger margin,0.6212372183799744
translation,2,93,results,larger margin,than,sbo,larger margin than sbo,0.6106096506118774
translation,2,93,results,results,see that,our proposed perturbation masking objective,results see that our proposed perturbation masking objective,0.6061171889305115
translation,2,94,results,csrl model,could achieve,best performance,csrl model could achieve best performance,0.6933460831642151
translation,2,94,results,baseline,by,1.05 and 3.2 f1 all score,baseline by 1.05 and 3.2 f1 all score,0.577672004699707
translation,2,94,results,without domain- adaptive pretraining,by,1.05 and 3.2 f1 all score,without domain- adaptive pretraining by 1.05 and 3.2 f1 all score,0.5467722415924072
translation,2,94,results,three objectives,has,csrl model,three objectives has csrl model,0.57602858543396
translation,2,94,results,significantly improving,has,baseline,significantly improving has baseline,0.5969834327697754
translation,2,98,results,best performance,achieved when,all three objectives are considered,best performance achieved when all three objectives are considered,0.6761404275894165
translation,2,98,results,results,has,best performance,results has best performance,0.5759831070899963
translation,2,99,results,similar substantial gains,on,slu task,similar substantial gains on slu task,0.5809865593910217
translation,2,99,results,similar substantial gains,on,csrl task,similar substantial gains on csrl task,0.562130331993103
translation,2,99,results,results,observe,similar substantial gains,results observe similar substantial gains,0.6209008693695068
translation,3,23,ablation-analysis,overlap measures,of,dialogue generation task,overlap measures of dialogue generation task,0.5146085619926453
translation,3,23,ablation-analysis,overlap measures,are,significantly lower,overlap measures are significantly lower,0.587058424949646
translation,3,23,ablation-analysis,dialogue generation task,are,significantly lower,dialogue generation task are significantly lower,0.5738229751586914
translation,3,23,ablation-analysis,significantly lower,than,nmt task,significantly lower than nmt task,0.53596431016922
translation,3,23,ablation-analysis,ablation analysis,see that,overlap measures,ablation analysis see that overlap measures,0.6544723510742188
translation,3,122,ablation-analysis,switch mechanism,plays,important role,switch mechanism plays important role,0.697047770023346
translation,3,122,ablation-analysis,important role,in,dialogue generation task,important role in dialogue generation task,0.48005905747413635
translation,3,122,ablation-analysis,ablation analysis,show,switch mechanism,ablation analysis show switch mechanism,0.5899667739868164
translation,3,130,ablation-analysis,reddit,in,"transformer , rs - word and rs - sentence models","reddit in transformer , rs - word and rs - sentence models",0.5346948504447937
translation,3,130,ablation-analysis,"transformer , rs - word and rs - sentence models",by,"2.65 , 0.37 and 0.48 distinct - 2 points","transformer , rs - word and rs - sentence models by 2.65 , 0.37 and 0.48 distinct - 2 points",0.6064942479133606
translation,3,130,ablation-analysis,ablation analysis,results of,reddit,ablation analysis results of reddit,0.6891293525695801
translation,3,98,baselines,random sampling,with,word ( rs - word ),random sampling with word ( rs - word ),0.6685316562652588
translation,3,98,baselines,random sampling,with,sentence ( rs - sentence ),random sampling with sentence ( rs - sentence ),0.662804365158081
translation,3,98,baselines,sentence ( rs - sentence ),has,"zhang et al. , 2019 )","sentence ( rs - sentence ) has zhang et al. , 2019 )",0.5550051927566528
translation,3,98,baselines,baselines,including,transformer - based model,baselines including transformer - based model,0.7203754186630249
translation,3,98,baselines,baselines,has,three baselines,baselines has three baselines,0.6189775466918945
translation,3,123,baselines,rs - word and rs - sentence,replace,ground truth tokens,rs - word and rs - sentence replace ground truth tokens,0.55196613073349
translation,3,123,baselines,ground truth tokens,by,generated tokens,ground truth tokens by generated tokens,0.517392098903656
translation,3,123,baselines,generated tokens,with,random scheduled sampling,generated tokens with random scheduled sampling,0.6461679935455322
translation,3,123,baselines,baselines,has,rs - word and rs - sentence,baselines has rs - word and rs - sentence,0.5882827043533325
translation,3,99,experimental-setup,stc,utilize,chinese word,stc utilize chinese word,0.5326372385025024
translation,3,99,experimental-setup,stc,set,vocabulary size,stc set vocabulary size,0.6613351106643677
translation,3,99,experimental-setup,chinese word,as,input,chinese word as input,0.5536580681800842
translation,3,99,experimental-setup,vocabulary size,as,"10,599","vocabulary size as 10,599",0.5509579181671143
translation,3,99,experimental-setup,experimental setup,For,stc,experimental setup For stc,0.6186135411262512
translation,3,100,experimental-setup,context- response pairs,encoded using,byte-pair encoding ( bpe ),context- response pairs encoded using byte-pair encoding ( bpe ),0.7707095742225647
translation,3,100,experimental-setup,byte-pair encoding ( bpe ),with,vocabularies,byte-pair encoding ( bpe ) with vocabularies,0.6445049047470093
translation,3,100,experimental-setup,vocabularies,of,"11,527 tokens","vocabularies of 11,527 tokens",0.5695579648017883
translation,3,100,experimental-setup,reddit,has,context- response pairs,reddit has context- response pairs,0.6091163754463196
translation,3,100,experimental-setup,experimental setup,For,reddit,experimental setup For reddit,0.6243021488189697
translation,3,101,experimental-setup,dimension,of,all word embedding,dimension of all word embedding,0.5669311881065369
translation,3,101,experimental-setup,all word embedding,is,512,all word embedding is 512,0.5674765110015869
translation,3,101,experimental-setup,beam size,in,testing,beam size in testing,0.5856440663337708
translation,3,101,experimental-setup,testing,is,5,testing is 5,0.6778932213783264
translation,3,101,experimental-setup,fair comparison,has,dimension,fair comparison has dimension,0.5787694454193115
translation,3,101,experimental-setup,fair comparison,has,beam size,fair comparison has beam size,0.5611191987991333
translation,3,101,experimental-setup,models,has,dimension,models has dimension,0.6065985560417175
translation,3,103,experimental-setup,parameters,initialized by,uniform distribution,parameters initialized by uniform distribution,0.7578092813491821
translation,3,103,experimental-setup,uniform distribution,over,"[ ?0.1 , 0.1 ]","uniform distribution over [ ?0.1 , 0.1 ]",0.6437795162200928
translation,3,103,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,3,104,experimental-setup,"adam ( kingma and ba , 2015 )",with,"?1 = 0.9 , ? 2 = 0.98","adam ( kingma and ba , 2015 ) with ?1 = 0.9 , ? 2 = 0.98",0.6447789072990417
translation,3,104,experimental-setup,"adam ( kingma and ba , 2015 )",with,weight decay,"adam ( kingma and ba , 2015 ) with weight decay",0.621263325214386
translation,3,104,experimental-setup,"adam ( kingma and ba , 2015 )",with,weight decay,"adam ( kingma and ba , 2015 ) with weight decay",0.621263325214386
translation,3,104,experimental-setup,weight decay,has,of = 10 ?8,weight decay has of = 10 ?8,0.5988569259643555
translation,3,104,experimental-setup,experimental setup,optimizer,"adam ( kingma and ba , 2015 )","experimental setup optimizer adam ( kingma and ba , 2015 )",0.7249329686164856
translation,3,105,experimental-setup,learning rate,as,0.0007,learning rate as 0.0007,0.5629752278327942
translation,3,105,experimental-setup,maximum tokens,of,batch,maximum tokens of batch,0.5981049537658691
translation,3,105,experimental-setup,batch,as,8192,batch as 8192,0.6534364223480225
translation,3,105,experimental-setup,8192,with,update frequency 2,8192 with update frequency 2,0.6838878989219666
translation,3,105,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,3,105,experimental-setup,experimental setup,set,maximum tokens,experimental setup set maximum tokens,0.6627659797668457
translation,3,106,experimental-setup,models,on,4 tesla p40 gpu cards,models on 4 tesla p40 gpu cards,0.49782422184944153
translation,3,106,experimental-setup,4 tesla p40 gpu cards,with,pytorch,4 tesla p40 gpu cards with pytorch,0.5754206776618958
translation,3,106,experimental-setup,experimental setup,run,models,experimental setup run models,0.6445184350013733
translation,3,8,model,novel adaptive switching mechanism,learns to automatically transit,generated learning,novel adaptive switching mechanism learns to automatically transit generated learning,0.7037965655326843
translation,3,8,model,generated learning,regarding,word - level,generated learning regarding word - level,0.6089386940002441
translation,3,8,model,model,propose,novel adaptive switching mechanism,model propose novel adaptive switching mechanism,0.682672381401062
translation,3,29,model,novel adaptive switch mechanism,as,bridge ( adapbridge ),novel adaptive switch mechanism as bridge ( adapbridge ),0.5654386878013611
translation,3,29,model,novel adaptive switch mechanism,introduces,generator distribution,novel adaptive switch mechanism introduces generator distribution,0.619978129863739
translation,3,29,model,novel adaptive switch mechanism,learns to,automatically transit,novel adaptive switch mechanism learns to automatically transit,0.6995388269424438
translation,3,29,model,generator distribution,to,training phase,generator distribution to training phase,0.554929256439209
translation,3,29,model,automatically transit,between,ground - truth learning,automatically transit between ground - truth learning,0.6357592344284058
translation,3,29,model,automatically transit,between,generated learning,automatically transit between generated learning,0.6664295196533203
translation,3,29,model,generated learning,with respect to,word - level matching scores,generated learning with respect to word - level matching scores,0.6480453014373779
translation,3,29,model,word - level matching scores,such as,cosine similarity,word - level matching scores such as cosine similarity,0.6001563668251038
translation,3,29,model,model,propose,novel adaptive switch mechanism,model propose novel adaptive switch mechanism,0.6897393465042114
translation,3,30,model,each training step,calculate,cosine similarity,each training step calculate cosine similarity,0.6198613047599792
translation,3,30,model,cosine similarity,for,each generated word,cosine similarity for each generated word,0.6043989658355713
translation,3,30,model,each generated word,with respect to,all its ground -truths,each generated word with respect to all its ground -truths,0.637022078037262
translation,3,30,model,model,at,each training step,model at each training step,0.5511857867240906
translation,3,102,model,6 layers,in,both encoder and decoder,6 layers in both encoder and decoder,0.5432308912277222
translation,3,102,model,8 heads,in,multi-head attention,8 heads in multi-head attention,0.5525535345077515
translation,3,102,model,transformer model,has,6 layers,transformer model has 6 layers,0.5797926187515259
translation,3,102,model,transformer model,has,8 heads,transformer model has 8 heads,0.5706098675727844
translation,3,102,model,model,has,transformer model,model has transformer model,0.5662795305252075
translation,3,22,results,nmt wmt '14 dataset,on,dialogue reddit dataset,nmt wmt '14 dataset on dialogue reddit dataset,0.5223844051361084
translation,3,22,results,bleu and similarity,are,27.38 and 0.96,bleu and similarity are 27.38 and 0.96,0.5468333959579468
translation,3,22,results,bleu and similarity,are,2.17 and 0.81,bleu and similarity are 2.17 and 0.81,0.553947389125824
translation,3,22,results,bleu and similarity,are,2.17 and 0.81,bleu and similarity are 2.17 and 0.81,0.553947389125824
translation,3,22,results,bleu and similarity,are,2.17 and 0.81,bleu and similarity are 2.17 and 0.81,0.553947389125824
translation,3,22,results,nmt wmt '14 dataset,has,bleu and similarity,nmt wmt '14 dataset has bleu and similarity,0.5531274080276489
translation,3,22,results,nmt wmt '14 dataset,has,bleu and similarity,nmt wmt '14 dataset has bleu and similarity,0.5531274080276489
translation,3,22,results,dialogue reddit dataset,has,bleu and similarity,dialogue reddit dataset has bleu and similarity,0.5479028224945068
translation,3,22,results,results,show,nmt wmt '14 dataset,results show nmt wmt '14 dataset,0.589120090007782
translation,3,22,results,results,on,nmt wmt '14 dataset,results on nmt wmt '14 dataset,0.5227594375610352
translation,3,22,results,results,on,dialogue reddit dataset,results on dialogue reddit dataset,0.5345375537872314
translation,3,121,results,switch mechanism,such as,rs - word,switch mechanism such as rs - word,0.6562161445617676
translation,3,121,results,switch mechanism,such as,rs - sentence,switch mechanism such as rs - sentence,0.6481245160102844
translation,3,121,results,switch mechanism,such as,adapbridge,switch mechanism such as adapbridge,0.6414037346839905
translation,3,121,results,switch mechanism,such as,outperform,switch mechanism such as outperform,0.6513480544090271
translation,3,121,results,traditional transformer - based model,in terms of,bleu,traditional transformer - based model in terms of bleu,0.7090891003608704
translation,3,121,results,traditional transformer - based model,in terms of,distinct - 2 evaluations,traditional transformer - based model in terms of distinct - 2 evaluations,0.7200767993927002
translation,3,121,results,traditional transformer - based model,Distinct - 1 and,distinct - 2 evaluations,traditional transformer - based model Distinct - 1 and distinct - 2 evaluations,0.6967340707778931
translation,3,121,results,outperform,has,traditional transformer - based model,outperform has traditional transformer - based model,0.5660797357559204
translation,3,125,results,bleu score,on,stc dataset,bleu score on stc dataset,0.48737022280693054
translation,3,125,results,bleu - 4 score,of,adap-bridge,bleu - 4 score of adap-bridge,0.5942825675010681
translation,3,125,results,adap-bridge,is,2.17,adap-bridge is 2.17,0.5969916582107544
translation,3,125,results,2.17,better than,rs - word,2.17 better than rs - word,0.7265080809593201
translation,3,125,results,2.17,better than,rs - sentence,2.17 better than rs - sentence,0.6970649361610413
translation,3,125,results,2.17,better than,2.05 and 2.12,2.17 better than 2.05 and 2.12,0.6757278442382812
translation,3,125,results,bleu score,has,bleu - 4 score,bleu score has bleu - 4 score,0.5256307721138
translation,3,126,results,our model,achieves,best ah - bleu - 2 score,our model achieves best ah - bleu - 2 score,0.6434043645858765
translation,3,126,results,results,has,our model,results has our model,0.5871725678443909
translation,3,129,results,adap-bridge,achieves,significant performance gains,adap-bridge achieves significant performance gains,0.7402546405792236
translation,3,131,results,highest distinct score,on,stc and reddit datasets,highest distinct score on stc and reddit datasets,0.5190551280975342
translation,3,131,results,our model,has,highest distinct score,our model has highest distinct score,0.5849382281303406
translation,3,131,results,results,note,our model,results note our model,0.6268720030784607
translation,3,132,results,proposed adapbridge model,ability to generate,high quality and diverse responses,proposed adapbridge model ability to generate high quality and diverse responses,0.7623181939125061
translation,3,132,results,high quality and diverse responses,compared with,baselines,high quality and diverse responses compared with baselines,0.6598525047302246
translation,3,132,results,results,has,proposed adapbridge model,results has proposed adapbridge model,0.5967204570770264
translation,3,133,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,3,133,results,improvements,are,significant,improvements are significant,0.6254523992538452
translation,3,133,results,significant,on,both two datatests,significant on both two datatests,0.6307836174964905
translation,3,138,results,stc,compared with,transformer,stc compared with transformer,0.6767551898956299
translation,3,138,results,stc,compared with,rs - word oracle,stc compared with rs - word oracle,0.6987418532371521
translation,3,138,results,stc,compared with,rs - sentence oracle,stc compared with rs - sentence oracle,0.6630458831787109
translation,3,138,results,adap- bridge,achieves,performance gains,adap- bridge achieves performance gains,0.723318874835968
translation,3,138,results,"22.38 % , 5.73 % , 7.65 %",on,relevant score,"22.38 % , 5.73 % , 7.65 % on relevant score",0.50972580909729
translation,3,138,results,stc,has,adap- bridge,stc has adap- bridge,0.6054466366767883
translation,3,138,results,transformer,has,adap- bridge,transformer has adap- bridge,0.5763137936592102
translation,3,138,results,rs - sentence oracle,has,adap- bridge,rs - sentence oracle has adap- bridge,0.6316536068916321
translation,3,138,results,performance gains,has,"22.38 % , 5.73 % , 7.65 %","performance gains has 22.38 % , 5.73 % , 7.65 %",0.5330790281295776
translation,3,139,results,mean score,observe that,our adapbridge,mean score observe that our adapbridge,0.5819399356842041
translation,3,139,results,our adapbridge,generates,most relevant responses,our adapbridge generates most relevant responses,0.6637006998062134
translation,3,139,results,our adapbridge,generates,less no-relevant responses,our adapbridge generates less no-relevant responses,0.6842232346534729
translation,3,139,results,results,For,mean score,results For mean score,0.5691258907318115
translation,3,140,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,3,140,results,improvements,are,significant,improvements are significant,0.6254523992538452
translation,3,140,results,significant,on,both two datatests,significant on both two datatests,0.6307836174964905
translation,3,154,results,our method,achieve,significant performance gains,our method achieve significant performance gains,0.6148023009300232
translation,3,154,results,our method,prove,transformer - based model,our method prove transformer - based model,0.6503990292549133
translation,3,154,results,im,prove,transformer - based model,im prove transformer - based model,0.6222564578056335
translation,3,154,results,transformer - based model,by,0.95 bleu - 4 points,transformer - based model by 0.95 bleu - 4 points,0.5820805430412292
translation,3,154,results,results,see that,our method,results see that our method,0.631170928478241
translation,3,155,results,our model,slightly lower than,rs - sentences model,our model slightly lower than rs - sentences model,0.6269177794456482
translation,3,155,results,bleu - 2 score,has,our model,bleu - 2 score has our model,0.5268375873565674
translation,3,155,results,results,For,bleu - 2 score,results For bleu - 2 score,0.5793952941894531
translation,4,50,baselines,transfertransfo,based on,transformer architecture,transfertransfo based on transformer architecture,0.6859341263771057
translation,4,50,baselines,transfertransfo,fine-tunes,generative pretrained model ( gpt ),transfertransfo fine-tunes generative pretrained model ( gpt ),0.6905909180641174
translation,4,50,baselines,generative pretrained model ( gpt ),with,two objective functions,generative pretrained model ( gpt ) with two objective functions,0.6033807396888733
translation,4,50,baselines,baselines,has,transfertransfo,baselines has transfertransfo,0.619397759437561
translation,4,73,experiments,open-domain dialogue generation,using,dailydialog dataset,open-domain dialogue generation using dailydialog dataset,0.6177816987037659
translation,4,73,experiments,goaloriented anti-scam dialogue generation,using,set of fraudulent emails,goaloriented anti-scam dialogue generation using set of fraudulent emails,0.6761610507965088
translation,4,73,experiments,goaloriented anti-scam dialogue generation,using,small set of intent-specific anti-scam response exemplars,goaloriented anti-scam dialogue generation using small set of intent-specific anti-scam response exemplars,0.654319167137146
translation,4,73,experiments,set of fraudulent emails,as,prompts,set of fraudulent emails as prompts,0.5457503199577332
translation,4,73,experiments,small set of intent-specific anti-scam response exemplars,to inform,responses,small set of intent-specific anti-scam response exemplars to inform responses,0.6748659014701843
translation,4,91,hyperparameters,gpt - 2,used as,decoder,gpt - 2 used as decoder,0.6397296190261841
translation,4,91,hyperparameters,hyperparameters,has,gpt - 2,hyperparameters has gpt - 2,0.5590061545372009
translation,4,98,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,4,98,hyperparameters,adam optimizer,with,l2 weight decay,adam optimizer with l2 weight decay,0.5716142058372498
translation,4,98,hyperparameters,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,4,98,hyperparameters,learning rate,of,6.25e - 5,learning rate of 6.25e - 5,0.5975595712661743
translation,4,98,hyperparameters,l2 weight decay,of,0.01,l2 weight decay of 0.01,0.5991005897521973
translation,4,98,hyperparameters,batch size,of,2,batch size of 2,0.6764397025108337
translation,4,98,hyperparameters,hyperparameters,used,adam optimizer,hyperparameters used adam optimizer,0.5959904789924622
translation,4,99,hyperparameters,number of candidates,to,2,number of candidates to 2,0.590396523475647
translation,4,99,hyperparameters,number of candidates,for,next-utterance classification objective,number of candidates for next-utterance classification objective,0.6038762927055359
translation,4,99,hyperparameters,2,for,next-utterance classification objective,2 for next-utterance classification objective,0.4552076458930969
translation,4,99,hyperparameters,hyperparameters,set,number of candidates,hyperparameters set number of candidates,0.6735818982124329
translation,4,100,hyperparameters,model,trained until,maximum,model trained until maximum,0.774394690990448
translation,4,100,hyperparameters,maximum,of,10 epochs,maximum of 10 epochs,0.5964555144309998
translation,4,100,hyperparameters,maximum,with,early stopping criteria,maximum with early stopping criteria,0.617974042892456
translation,4,100,hyperparameters,10 epochs,with,early stopping criteria,10 epochs with early stopping criteria,0.6311696767807007
translation,4,100,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,4,101,hyperparameters,maximum decoding length,to,50 tokens,maximum decoding length to 50 tokens,0.5653599500656128
translation,4,101,hyperparameters,"nucleus sampling ( holtzman et al. , 2020 )",with,threshold,"nucleus sampling ( holtzman et al. , 2020 ) with threshold",0.6363435983657837
translation,4,101,hyperparameters,threshold,of,0.9,threshold of 0.9,0.6172952055931091
translation,4,101,hyperparameters,hyperparameters,set,maximum decoding length,hyperparameters set maximum decoding length,0.6244494915008545
translation,4,101,hyperparameters,hyperparameters,set,minimum,hyperparameters set minimum,0.6960026621818542
translation,4,101,hyperparameters,hyperparameters,use,"nucleus sampling ( holtzman et al. , 2020 )","hyperparameters use nucleus sampling ( holtzman et al. , 2020 )",0.5890206098556519
translation,4,6,model,exemplar-based dialogue generation model,uses,semantic frames,exemplar-based dialogue generation model uses semantic frames,0.5367197394371033
translation,4,6,model,semantic frames,present in,exemplar responses,semantic frames present in exemplar responses,0.7089895009994507
translation,4,6,model,exemplar responses,to guide,response generation,exemplar responses to guide response generation,0.6671700477600098
translation,4,6,model,exemplar-based dialogue generation model,has,edge,exemplar-based dialogue generation model has edge,0.5222394466400146
translation,4,6,model,model,present,exemplar-based dialogue generation model,model present exemplar-based dialogue generation model,0.6124802827835083
translation,4,27,model,locally coherent responses,adhere to,high- level dialogue constraints,locally coherent responses adhere to high- level dialogue constraints,0.631757378578186
translation,4,27,model,high- level dialogue constraints,present,edge,high- level dialogue constraints present edge,0.6445643901824951
translation,4,27,model,semantic structure,of,exemplar response,semantic structure of exemplar response,0.5749191045761108
translation,4,27,model,semantic structure,of,exemplar response,semantic structure of exemplar response,0.5749191045761108
translation,4,27,model,semantic structure,to guide,generation,semantic structure to guide generation,0.6357166767120361
translation,4,27,model,tokens,of,exemplar response,tokens of exemplar response,0.5885741710662842
translation,4,27,model,model,To generate,locally coherent responses,model To generate locally coherent responses,0.7025558352470398
translation,4,27,model,model,uses,semantic structure,model uses semantic structure,0.5825501680374146
translation,4,32,model,zero-shot anti-scam application,show,edge,zero-shot anti-scam application show edge,0.6507282257080078
translation,4,32,model,edge,generates,exemplarconditioned responses,edge generates exemplarconditioned responses,0.6170721054077148
translation,4,32,model,exemplarconditioned responses,that are,"coherent , contextspecific , and adherent","exemplarconditioned responses that are coherent , contextspecific , and adherent",0.6137375235557556
translation,4,32,model,"coherent , contextspecific , and adherent",to,underlying exemplar intents and their high - level goals,"coherent , contextspecific , and adherent to underlying exemplar intents and their high - level goals",0.5060208439826965
translation,4,32,model,model,In,zero-shot anti-scam application,model In zero-shot anti-scam application,0.5357641577720642
translation,4,49,model,"dialogue generation model transfertransfo ( wolf et al. , 2019 )",to control,generation,"dialogue generation model transfertransfo ( wolf et al. , 2019 ) to control generation",0.6813929080963135
translation,4,49,model,generation,by including,semantic frames,generation by including semantic frames,0.6614713072776794
translation,4,49,model,semantic frames,from,exemplar response,semantic frames from exemplar response,0.541137158870697
translation,4,177,model,edge,in,anti-scam domain,edge in anti-scam domain,0.5648810863494873
translation,4,177,model,edge,generate,variety of coherent responses,edge generate variety of coherent responses,0.6251550316810608
translation,4,177,model,variety of coherent responses,to,novel dialogue contexts,variety of coherent responses to novel dialogue contexts,0.5612462759017944
translation,4,177,model,novel dialogue contexts,that capture,high- level intents,novel dialogue contexts that capture high- level intents,0.6911259293556213
translation,4,177,model,high- level intents,of,exemplar responses,high- level intents of exemplar responses,0.5611638426780701
translation,4,177,model,models,on,domain-specific data,models on domain-specific data,0.5179495215415955
translation,4,177,model,model,application of,edge,model application of edge,0.6809368133544922
translation,4,30,results,semantic frames,from,exemplars,semantic frames from exemplars,0.5298910737037659
translation,4,30,results,set of generative and retrievalbased baselines,in,quantitative evaluation,set of generative and retrievalbased baselines in quantitative evaluation,0.4867887794971466
translation,4,30,results,quantitative evaluation,of,response quality,quantitative evaluation of response quality,0.5521978139877319
translation,4,30,results,tokenbased approaches,in capturing,semantic structure,tokenbased approaches in capturing semantic structure,0.6805773973464966
translation,4,30,results,semantic structure,of,exemplar responses,semantic structure of exemplar responses,0.5707359313964844
translation,4,30,results,outperforms,has,set of generative and retrievalbased baselines,outperforms has set of generative and retrievalbased baselines,0.5847978591918945
translation,4,30,results,response quality,has,coherence,response quality has coherence,0.5568335652351379
translation,4,30,results,outperforms,has,tokenbased approaches,outperforms has tokenbased approaches,0.6086979508399963
translation,4,123,results,"edge , gpt2 - tokens , and gpt2 - gen",achieve,higher ratings,"edge , gpt2 - tokens , and gpt2 - gen achieve higher ratings",0.5963780879974365
translation,4,123,results,higher ratings,for,quality metrics,higher ratings for quality metrics,0.5631833076477051
translation,4,123,results,quality metrics,of,"coherence , fluency , consistency , and interestingness","quality metrics of coherence , fluency , consistency , and interestingness",0.5476312041282654
translation,4,123,results,"coherence , fluency , consistency , and interestingness",compared to,lstm based models ( lstm - tokens and lstm - frames ),"coherence , fluency , consistency , and interestingness compared to lstm based models ( lstm - tokens and lstm - frames )",0.6051620841026306
translation,4,123,results,results,has,"edge , gpt2 - tokens , and gpt2 - gen","results has edge , gpt2 - tokens , and gpt2 - gen",0.5173345804214478
translation,4,124,results,models,that use,semantic frames,models that use semantic frames,0.6134806275367737
translation,4,124,results,semantic frames,from,retrieved responses ( edge and lstm - frames ),semantic frames from retrieved responses ( edge and lstm - frames ),0.5109396576881409
translation,4,124,results,semantic frames,from,retrieved response,semantic frames from retrieved response,0.5495273470878601
translation,4,124,results,semantic frames,achieve,higher ratings,semantic frames achieve higher ratings,0.6031877398490906
translation,4,124,results,semantic frames,from,retrieved response,semantic frames from retrieved response,0.5495273470878601
translation,4,124,results,higher ratings,than,models,higher ratings than models,0.6227232217788696
translation,4,124,results,directly used tokens,from,retrieved response,directly used tokens from retrieved response,0.5283229351043701
translation,4,124,results,results,has,models,results has models,0.5335168838500977
translation,4,125,results,our gpt - 2 based approach,that uses,semantic frames,our gpt - 2 based approach that uses semantic frames,0.6635878086090088
translation,4,125,results,semantic frames,from,response exemplars,semantic frames from response exemplars,0.5061860680580139
translation,4,125,results,semantic frames,from,reference responses,semantic frames from reference responses,0.5667445063591003
translation,4,125,results,all other models,on,overall quality metrics,all other models on overall quality metrics,0.4567910432815552
translation,4,125,results,token - based approaches,in preserving,semantics,token - based approaches in preserving semantics,0.7134787440299988
translation,4,125,results,semantics,from,reference responses,semantics from reference responses,0.5781413912773132
translation,4,125,results,edge,has,our gpt - 2 based approach,edge has our gpt - 2 based approach,0.6151230335235596
translation,4,125,results,edge,has,outperforms,edge has outperforms,0.6391387581825256
translation,4,125,results,our gpt - 2 based approach,has,outperforms,our gpt - 2 based approach has outperforms,0.6195192337036133
translation,4,125,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,4,125,results,outperforms,has,token - based approaches,outperforms has token - based approaches,0.6120131015777588
translation,4,125,results,results,has,edge,results has edge,0.44935518503189087
translation,4,125,results,results,has,our gpt - 2 based approach,results has our gpt - 2 based approach,0.5678296685218811
translation,4,126,results,lstm - frames and edge,achieve,high uses semantics rating,lstm - frames and edge achieve high uses semantics rating,0.6497465968132019
translation,4,126,results,results,has,lstm - frames and edge,results has lstm - frames and edge,0.5230823159217834
translation,4,127,results,edge and gpt2 - tokens,achieve,highest maude scores,edge and gpt2 - tokens achieve highest maude scores,0.6372344493865967
translation,4,127,results,edge and gpt2 - tokens,achieve,highest dist -n scores,edge and gpt2 - tokens achieve highest dist -n scores,0.6266230344772339
translation,4,127,results,highest maude scores,as,highest dist -n scores,highest maude scores as highest dist -n scores,0.5169079899787903
translation,4,127,results,results,has,edge and gpt2 - tokens,results has edge and gpt2 - tokens,0.5490078330039978
translation,4,128,results,edge,generates,more coherent and interesting responses,edge generates more coherent and interesting responses,0.6336479783058167
translation,4,128,results,more coherent and interesting responses,compared to,baselines,more coherent and interesting responses compared to baselines,0.6571726202964783
translation,4,128,results,results,has,edge,results has edge,0.44935518503189087
translation,4,135,results,edge,achieves,higher diversity,edge achieves higher diversity,0.6410033106803894
translation,4,135,results,edge,achieves,gpt2 - gen,edge achieves gpt2 - gen,0.6958392858505249
translation,4,135,results,higher diversity,than,lstm - tokens,higher diversity than lstm - tokens,0.6136637330055237
translation,4,135,results,gpt2 - gen,for,all response set sizes,gpt2 - gen for all response set sizes,0.6026505827903748
translation,4,135,results,results,has,edge,results has edge,0.44935518503189087
translation,4,136,results,edge,generated,responses,edge generated responses,0.6347678303718567
translation,4,136,results,responses,with,semantic frames,responses with semantic frames,0.6417174339294434
translation,4,136,results,responses,with,semantic frames,responses with semantic frames,0.6417174339294434
translation,4,136,results,semantic frames,covered,higher percentage,semantic frames covered higher percentage,0.636817455291748
translation,4,136,results,semantic frames,present in,retrieved responses,semantic frames present in retrieved responses,0.7181698679924011
translation,4,136,results,higher percentage,of,semantic frames,higher percentage of semantic frames,0.595579981803894
translation,4,136,results,semantic frames,present in,retrieved responses,semantic frames present in retrieved responses,0.7181698679924011
translation,4,136,results,semcov,is,36 %,semcov is 36 %,0.6551485061645508
translation,4,136,results,semcov,is,63 %,semcov is 63 %,0.6427510976791382
translation,4,136,results,36 %,for,lstm - tokens,36 % for lstm - tokens,0.5993971228599548
translation,4,136,results,63 %,for,edge,63 % for edge,0.699838399887085
translation,4,136,results,lstm - tokens,has,edge,lstm - tokens has edge,0.5763476490974426
translation,4,136,results,retrieved responses,has,semcov,retrieved responses has semcov,0.5627111196517944
translation,4,136,results,results,Compared to,lstm - tokens,results Compared to lstm - tokens,0.6077432036399841
translation,4,138,results,exact tokens,as,edge generated responses,exact tokens as edge generated responses,0.5297807455062866
translation,4,138,results,edge generated responses,contained,lower level of token similarity,edge generated responses contained lower level of token similarity,0.5898239016532898
translation,4,138,results,lower level of token similarity,to,retrieved responses,lower level of token similarity to retrieved responses,0.5600970983505249
translation,4,138,results,bleu - 2,of,0.21,bleu - 2 of 0.21,0.5992866158485413
translation,4,138,results,bleu - 2,of,0.16,bleu - 2 of 0.16,0.622589647769928
translation,4,138,results,bleu - 2,of,0.16,bleu - 2 of 0.16,0.622589647769928
translation,4,138,results,0.21,for,lstm - tokens,0.21 for lstm - tokens,0.5820923447608948
translation,4,138,results,bleu - 2,of,0.16,bleu - 2 of 0.16,0.622589647769928
translation,4,138,results,0.16,for,edge,0.16 for edge,0.6632958650588989
translation,4,138,results,exact tokens,has,less often,exact tokens has less often,0.5856534242630005
translation,4,138,results,retrieved responses,has,bleu - 2,retrieved responses has bleu - 2,0.577363908290863
translation,4,138,results,results,copied,exact tokens,results copied exact tokens,0.6925345063209534
translation,4,142,results,edge,controls,length and semantic structure,edge controls length and semantic structure,0.6306881308555603
translation,4,142,results,edge,produces,longer and more specific responses,edge produces longer and more specific responses,0.6329491138458252
translation,4,142,results,length and semantic structure,of,responses,length and semantic structure of responses,0.5801301002502441
translation,4,142,results,responses,based on,retrieved humanwritten exemplars,responses based on retrieved humanwritten exemplars,0.6507560610771179
translation,4,142,results,longer and more specific responses,compared to,purely generative model,longer and more specific responses compared to purely generative model,0.6301866769790649
translation,4,142,results,results,has,edge,results has edge,0.44935518503189087
translation,4,143,results,edge,benefits from,exemplar- based control,edge benefits from exemplar- based control,0.6976872682571411
translation,4,143,results,results,has,edge,results has edge,0.44935518503189087
translation,5,148,ablation-analysis,maintains context,is,only criterion,maintains context is only criterion,0.573910117149353
translation,5,148,ablation-analysis,only criterion,where,usr,only criterion where usr,0.6396744847297668
translation,5,148,ablation-analysis,usr,has,outperforms,usr has outperforms,0.6396613121032715
translation,5,148,ablation-analysis,outperforms,has,proxy indicators,outperforms has proxy indicators,0.6029856204986572
translation,5,148,ablation-analysis,ablation analysis,has,maintains context,ablation analysis has maintains context,0.5740900635719299
translation,5,179,ablation-analysis,insignificant influence,on,prediction,insignificant influence on prediction,0.5454065799713135
translation,5,179,ablation-analysis,prediction,of,overall quality score,prediction of overall quality score,0.5713154077529907
translation,5,179,ablation-analysis,both datasets,has,single sentence tasks,both datasets has single sentence tasks,0.5471416115760803
translation,5,179,ablation-analysis,ablation analysis,in,both datasets,ablation analysis in both datasets,0.5095816254615784
translation,5,180,ablation-analysis,semantic overlap,between,utterances,semantic overlap between utterances,0.6354286670684814
translation,5,180,ablation-analysis,utterances,via,sts -b and mrpc,utterances via sts -b and mrpc,0.7436313033103943
translation,5,180,ablation-analysis,sts -b and mrpc,plays,significant role,sts -b and mrpc plays significant role,0.7216967940330505
translation,5,180,ablation-analysis,sts -b and mrpc,both cases,significant role,sts -b and mrpc both cases significant role,0.7085673213005066
translation,5,180,ablation-analysis,ablation analysis,has,semantic overlap,ablation analysis has semantic overlap,0.5679850578308105
translation,5,103,experiments,pre-training,done using,two unsupervised tasks,pre-training done using two unsupervised tasks,0.6188912391662598
translation,5,153,experiments,attention,to,interesting quality measure,attention to interesting quality measure,0.5114621520042419
translation,5,153,experiments,interesting quality measure,where,usr,interesting quality measure where usr,0.599283754825592
translation,5,102,hyperparameters,text sequence,use,bert,text sequence use bert,0.6769323945045471
translation,5,102,hyperparameters,bert,has,pre-trained bidirectional transformer encoder language model,bert has pre-trained bidirectional transformer encoder language model,0.5572016835212708
translation,5,102,hyperparameters,hyperparameters,encoding,text sequence,hyperparameters encoding text sequence,0.7655043005943298
translation,5,143,results,combined proxy indicators,via,linear regression,combined proxy indicators via linear regression,0.6364051103591919
translation,5,143,results,all of the criteria,has,combined proxy indicators,all of the criteria has combined proxy indicators,0.575208306312561
translation,5,143,results,linear regression,has,outperform,linear regression has outperform,0.6038109064102173
translation,5,143,results,outperform,has,combined usr metric,outperform has combined usr metric,0.5887925624847412
translation,5,143,results,results,In,all of the criteria,results In all of the criteria,0.48110705614089966
translation,5,145,results,understandable and natural criteria,see that,cola,understandable and natural criteria see that cola,0.6820796132087708
translation,5,145,results,cola,as,single proxy indicator,cola as single proxy indicator,0.49621888995170593
translation,5,145,results,two measures,on,topi-cal chat dataset,two measures on topi-cal chat dataset,0.5206543207168579
translation,5,145,results,weakly infer,has,two measures,weakly infer has two measures,0.5789503455162048
translation,5,145,results,results,Looking at,understandable and natural criteria,results Looking at understandable and natural criteria,0.5740200281143188
translation,5,146,results,stsb and mrpc,in,personachat,stsb and mrpc in personachat,0.6464488506317139
translation,5,149,results,semantic textual similarity benchmark ( stsb ),is,best performer,semantic textual similarity benchmark ( stsb ) is best performer,0.5416288375854492
translation,5,149,results,proxy indicators,has,semantic textual similarity benchmark ( stsb ),proxy indicators has semantic textual similarity benchmark ( stsb ),0.5819547176361084
translation,5,149,results,results,Among,proxy indicators,results Among proxy indicators,0.5352215766906738
translation,5,154,results,linear regression,of,proxy indicators,linear regression of proxy indicators,0.5561096668243408
translation,5,154,results,outperforms,by,considerable margin,outperforms by considerable margin,0.653662919998169
translation,5,154,results,rest,by,considerable margin,rest by considerable margin,0.6248392462730408
translation,5,154,results,proxy indicators,has,outperforms,proxy indicators has outperforms,0.6331529021263123
translation,5,154,results,outperforms,has,rest,outperforms has rest,0.6641153693199158
translation,5,154,results,results,has,linear regression,results has linear regression,0.519572377204895
translation,5,159,results,metric,is,best performer,metric is best performer,0.5605169534683228
translation,5,159,results,best performer,for,latter criterion,best performer for latter criterion,0.6073449850082397
translation,5,160,results,fact - based stsb,compared against,uses knowledge,fact - based stsb compared against uses knowledge,0.7051205039024353
translation,5,160,results,fact - based stsb,delivers,highest correlation score,fact - based stsb delivers highest correlation score,0.653418242931366
translation,5,160,results,highest correlation score,among,all metrics,highest correlation score among all metrics,0.5511409640312195
translation,5,160,results,results,has,fact - based stsb,results has fact - based stsb,0.5308915972709656
translation,5,169,results,pair-wise sentence proxy indicators,applied to,dialogue context,pair-wise sentence proxy indicators applied to dialogue context,0.703751266002655
translation,5,169,results,target response,demonstrate,best ability,target response demonstrate best ability,0.6316075921058655
translation,5,169,results,single sentence,is,worst,single sentence is worst,0.6422768831253052
translation,5,169,results,results,has,pair-wise sentence proxy indicators,results has pair-wise sentence proxy indicators,0.487413227558136
translation,5,171,results,pair-wise tasks,has,outperform,pair-wise tasks has outperform,0.6180470585823059
translation,5,171,results,outperform,has,single-sentence ones,outperform has single-sentence ones,0.5831206440925598
translation,5,171,results,results,evident that,pair-wise tasks,results evident that pair-wise tasks,0.663284182548523
translation,5,172,results,fact - based pair-wise proxy indicators,demonstrate,strong ability,fact - based pair-wise proxy indicators demonstrate strong ability,0.6242642402648926
translation,5,172,results,strong ability,to model,uses knowledge criterion,strong ability to model uses knowledge criterion,0.7246005535125732
translation,5,172,results,results,has,fact - based pair-wise proxy indicators,results has fact - based pair-wise proxy indicators,0.4932042360305786
translation,5,173,results,others,has,underperform,others has underperform,0.5592779517173767
translation,5,175,results,all of the subset combinations,perform,worse,all of the subset combinations perform worse,0.6400576829910278
translation,5,175,results,worse,than,linear regression,worse than linear regression,0.5998520851135254
translation,5,175,results,results,has,all of the subset combinations,results has all of the subset combinations,0.5558775663375854
translation,6,106,ablation-analysis,in modelling cross-lingual embedding spaces,for,conversational setting,in modelling cross-lingual embedding spaces for conversational setting,0.613142192363739
translation,6,106,ablation-analysis,conversational setting,by concatenating,parallel dialogues,conversational setting by concatenating parallel dialogues,0.6507888436317444
translation,6,106,ablation-analysis,parallel dialogues,with,k utterances,parallel dialogues with k utterances,0.637747585773468
translation,6,106,ablation-analysis,parallel dialogues,masking,words,parallel dialogues masking words,0.7638586759567261
translation,6,106,ablation-analysis,randomly,on,concatenated text,randomly on concatenated text,0.5781418085098267
translation,6,106,ablation-analysis,longer context,has,in modelling cross-lingual embedding spaces,longer context has in modelling cross-lingual embedding spaces,0.5246519446372986
translation,6,106,ablation-analysis,words,has,randomly,words has randomly,0.6186580061912537
translation,6,106,ablation-analysis,ablation analysis,importance of,longer context,ablation analysis importance of longer context,0.6595777869224548
translation,6,187,ablation-analysis,errors,on,later dialogue states,errors on later dialogue states,0.5476329922676086
translation,6,79,baselines,slotutterance matching belief tracker ( sumbt ),was,state- ofthe - art,slotutterance matching belief tracker ( sumbt ) was state- ofthe - art,0.5540574193000793
translation,6,79,baselines,state- ofthe - art,for,english multiwoz 2.1 dataset,state- ofthe - art for english multiwoz 2.1 dataset,0.46556612849235535
translation,6,176,baselines,clcsa method,uses,dynamic code-mixed data,clcsa method uses dynamic code-mixed data,0.6467519402503967
translation,6,176,baselines,dynamic code-mixed data,for training,state tracker,dynamic code-mixed data for training state tracker,0.732333242893219
translation,6,176,baselines,baselines,has,clcsa method,baselines has clcsa method,0.5479618906974792
translation,6,9,experiments,parallel and conversational movie subtitles datasets,to design,cross-lingual intermediate tasks,parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks,0.5751861929893494
translation,6,9,experiments,cross-lingual intermediate tasks,suitable for,downstream dialogue tasks,cross-lingual intermediate tasks suitable for downstream dialogue tasks,0.7161580324172974
translation,6,11,experiments,cross-lingual dialogue state tracking task,for,"parallel mul-tiwoz ( english ? chinese , chinese? english ) and multilingual woz ( english ? german , english? italian ) datasets","cross-lingual dialogue state tracking task for parallel mul-tiwoz ( english ? chinese , chinese? english ) and multilingual woz ( english ? german , english? italian ) datasets",0.5682621598243713
translation,6,38,experiments,"200k parallel movie subtitles ( lison and tiedemann , 2016 )",for,intermediate training,"200k parallel movie subtitles ( lison and tiedemann , 2016 ) for intermediate training",0.5923298001289368
translation,6,93,experiments,extensions,to,masked language modelling,extensions to masked language modelling,0.5397672653198242
translation,6,93,experiments,masked language modelling,more suitable for,dialogue task,masked language modelling more suitable for dialogue task,0.6617358922958374
translation,6,168,experiments,machine translation models,are,large transformer models,machine translation models are large transformer models,0.5100350975990295
translation,6,168,experiments,large transformer models,trained on,"paracrawl data ( ba? n et al. , 2020 )","large transformer models trained on paracrawl data ( ba? n et al. , 2020 )",0.7392876148223877
translation,6,168,experiments,"paracrawl data ( ba? n et al. , 2020 )",for,en ? zh and zh ? en,"paracrawl data ( ba? n et al. , 2020 ) for en ? zh and zh ? en",0.6313415765762329
translation,6,8,model,transfer learning process,by,intermediate fine-tuning,transfer learning process by intermediate fine-tuning,0.5694791078567505
translation,6,8,model,intermediate fine-tuning,of,pretrained multilingual models,intermediate fine-tuning of pretrained multilingual models,0.5024222731590271
translation,6,8,model,pretrained multilingual models,where,multilingual models,pretrained multilingual models where multilingual models,0.5259398818016052
translation,6,8,model,multilingual models,fine-tuned with,different but related data and / or tasks,multilingual models fine-tuned with different but related data and / or tasks,0.7605695724487305
translation,6,8,model,model,enhance,transfer learning process,model enhance transfer learning process,0.6366041302680969
translation,6,24,model,effectiveness,of using,cross-lingual intermediate fine-tuning,effectiveness of using cross-lingual intermediate fine-tuning,0.7193818688392639
translation,6,24,model,cross-lingual intermediate fine-tuning,of,multilingual pretrained models,cross-lingual intermediate fine-tuning of multilingual pretrained models,0.5323795080184937
translation,6,24,model,model,demonstrate,effectiveness,model demonstrate effectiveness,0.5955999493598938
translation,6,12,results,impressive improvements,on,parallel multiwoz dataset and the multilingual woz dataset,impressive improvements on parallel multiwoz dataset and the multilingual woz dataset,0.5191778540611267
translation,6,12,results,> 20 %,on,joint goal accuracy,> 20 % on joint goal accuracy,0.49550575017929077
translation,6,12,results,parallel multiwoz dataset and the multilingual woz dataset,over,vanilla baseline,parallel multiwoz dataset and the multilingual woz dataset over vanilla baseline,0.6782398819923401
translation,6,12,results,impressive improvements,has,> 20 %,impressive improvements has > 20 %,0.5698590278625488
translation,6,12,results,results,achieve,impressive improvements,results achieve impressive improvements,0.615774929523468
translation,6,41,results,our proposed intermediate fine-tuning techniques,produce,data-efficient,our proposed intermediate fine-tuning techniques produce data-efficient,0.6592949628829956
translation,6,41,results,data-efficient,has,target language dialogue state trackers,data-efficient has target language dialogue state trackers,0.5456746220588684
translation,6,41,results,results,has,our proposed intermediate fine-tuning techniques,results has our proposed intermediate fine-tuning techniques,0.5690046548843384
translation,6,42,results,state - of- the - art results,for,zero-shot multilingual woz dataset,state - of- the - art results for zero-shot multilingual woz dataset,0.5538211464881897
translation,6,42,results,state - of- the - art results,obtain,> 20 % improvement,state - of- the - art results obtain > 20 % improvement,0.547339677810669
translation,6,42,results,> 20 % improvement,on,joint goal accuracy,> 20 % improvement on joint goal accuracy,0.5029147267341614
translation,6,42,results,joint goal accuracy,with,limited labelled data,joint goal accuracy with limited labelled data,0.5681954622268677
translation,6,42,results,limited labelled data,in,target language,limited labelled data in target language,0.483967661857605
translation,6,42,results,limited labelled data,for,multiwoz dataset,limited labelled data for multiwoz dataset,0.5840985178947449
translation,6,42,results,multiwoz dataset,over,baseline,multiwoz dataset over baseline,0.6949565410614014
translation,6,42,results,results,achieve,state - of- the - art results,results achieve state - of- the - art results,0.5771380066871643
translation,6,42,results,results,obtain,> 20 % improvement,results obtain > 20 % improvement,0.5633775591850281
translation,6,158,results,intermediate finetuning,of,language model,intermediate finetuning of language model,0.5400693416595459
translation,6,158,results,intermediate finetuning,helpful for,dialogue state tracking,intermediate finetuning helpful for dialogue state tracking,0.6086714863777161
translation,6,158,results,results,show that,intermediate finetuning,results show that intermediate finetuning,0.5305674076080322
translation,6,158,results,results,use of,intermediate finetuning,results use of intermediate finetuning,0.6896641850471497
translation,6,159,results,superior,to,task adaptive pretraining ( tapt ),superior to task adaptive pretraining ( tapt ),0.5806772708892822
translation,6,159,results,competitive,to,monolingual objective ( mon - odm ),competitive to monolingual objective ( mon - odm ),0.5432849526405334
translation,6,159,results,competitive,with,tlm,competitive with tlm,0.7300346493721008
translation,6,159,results,tlm,consistently performing,better,tlm consistently performing better,0.808007001876831
translation,6,159,results,better,than,all the cross-lingual objective functions,better than all the cross-lingual objective functions,0.5500435829162598
translation,6,159,results,all the cross-lingual objective functions,in,target language state tracking,all the cross-lingual objective functions in target language state tracking,0.4768315553665161
translation,6,159,results,results,use of,"crosslingual objectives ( xdm , rm , tlm )","results use of crosslingual objectives ( xdm , rm , tlm )",0.6299172043800354
translation,6,161,results,even the weakest intermediate fine- tuning setup,has,15.3 % and 16.2 %,even the weakest intermediate fine- tuning setup has 15.3 % and 16.2 %,0.5743024945259094
translation,6,161,results,even the weakest intermediate fine- tuning setup,has,15.3 % and 16.2 %,even the weakest intermediate fine- tuning setup has 15.3 % and 16.2 %,0.5743024945259094
translation,6,161,results,even the weakest intermediate fine- tuning setup,has,comparison,even the weakest intermediate fine- tuning setup has comparison,0.5708327889442444
translation,6,161,results,15.3 % and 16.2 %,has,comparison,15.3 % and 16.2 % has comparison,0.5747178792953491
translation,6,161,results,results,find that,even the weakest intermediate fine- tuning setup,results find that even the weakest intermediate fine- tuning setup,0.661616861820221
translation,6,177,results,tlm,with,clcsa model,tlm with clcsa model,0.6601114273071289
translation,6,177,results,additive effect,providing,improvement,additive effect providing improvement,0.7067252397537231
translation,6,177,results,improvement,over,model,improvement over model,0.6786442399024963
translation,6,177,results,model,not use,model,model not use model,0.6901511549949646
translation,6,177,results,model,as,intermediate finetuning task,model as intermediate finetuning task,0.5471709966659546
translation,6,177,results,clcsa model,has,additive effect,clcsa model has additive effect,0.5261384844779968
translation,6,177,results,results,observe,tlm,results observe tlm,0.6037039160728455
translation,6,177,results,results,using,tlm,results using tlm,0.6502867937088013
translation,6,185,results,models,trained with,intermediate tasks,models trained with intermediate tasks,0.7260454297065735
translation,6,185,results,intermediate tasks,improve over,vanilla baselines,intermediate tasks improve over vanilla baselines,0.6932714581489563
translation,6,185,results,vanilla baselines,in detecting,cuisine names,vanilla baselines in detecting cuisine names,0.6966841816902161
translation,6,185,results,vanilla baselines,in detecting,names of restaurants,vanilla baselines in detecting names of restaurants,0.6936792731285095
translation,6,185,results,vanilla baselines,in detecting,time periods for booking ( taxi / restaurant ),vanilla baselines in detecting time periods for booking ( taxi / restaurant ),0.6700578927993774
translation,6,185,results,results,found that,models,results found that models,0.6821842789649963
translation,6,188,results,struggled,to identify,less frequent cuisines,struggled to identify less frequent cuisines,0.657924234867096
translation,6,188,results,multilingual woz dataset,has,baseline models,multilingual woz dataset has baseline models,0.5359517931938171
translation,6,188,results,baseline models,has,struggled,baseline models has struggled,0.6223520636558533
translation,6,188,results,results,For,multilingual woz dataset,results For multilingual woz dataset,0.5457001328468323
translation,6,200,results,performance,for,target language,performance for target language,0.5594901442527771
translation,6,200,results,improves,while degrading,source language performance,improves while degrading source language performance,0.6706328988075256
translation,6,200,results,target training data,has,performance,target training data has performance,0.5571731925010681
translation,6,200,results,results,increase in,target training data,results increase in target training data,0.6520750522613525
translation,6,201,results,target language states,during,evaluation,target language states during evaluation,0.6345999836921692
translation,6,201,results,lower performance,than,source language dialogue states,lower performance than source language dialogue states,0.5577027797698975
translation,6,201,results,evaluation,has,lower performance,evaluation has lower performance,0.585205614566803
translation,6,201,results,results,using,target language states,results using target language states,0.6165381073951721
translation,6,210,results,results,for,multiwoz dataset,results for multiwoz dataset,0.6015470623970032
translation,6,211,results,slight advantage,over using,parallel news text,slight advantage over using parallel news text,0.7053444385528564
translation,6,211,results,dialogue data,has,slight advantage,dialogue data has slight advantage,0.5908647179603577
translation,6,211,results,results,find that using,dialogue data,results find that using dialogue data,0.5641657114028931
translation,7,84,ablation-analysis,osr,with and without using,r,osr with and without using r,0.6391086578369141
translation,7,84,ablation-analysis,osr,with and without using,r,osr with and without using r,0.6391086578369141
translation,7,84,ablation-analysis,osr,using,r,osr using r,0.6577194333076477
translation,7,84,ablation-analysis,r,reduces,proportion of generating slots,r reduces proportion of generating slots,0.6794591546058655
translation,7,84,ablation-analysis,r,reduces,proportion of generating slots,r reduces proportion of generating slots,0.6794591546058655
translation,7,84,ablation-analysis,proportion of generating slots,that do not align to,predicted domain,proportion of generating slots that do not align to predicted domain,0.6827365159988403
translation,7,84,ablation-analysis,proportion of generating slots,improves,model performance,proportion of generating slots improves model performance,0.7075693011283875
translation,7,84,ablation-analysis,ablation analysis,comparing,osr,ablation analysis comparing osr,0.6196889281272888
translation,7,4,model,fast and scalable architecture,called,explicit modular decomposition ( emd ),fast and scalable architecture called explicit modular decomposition ( emd ),0.6652092933654785
translation,7,4,model,fast and scalable architecture,incorporate,classification - based and extraction - based methods,fast and scalable architecture incorporate classification - based and extraction - based methods,0.6806922554969788
translation,7,4,model,fast and scalable architecture,design,four modules,fast and scalable architecture design four modules,0.5372118353843689
translation,7,4,model,four modules,for,classification and sequence labelling,four modules for classification and sequence labelling,0.6241973638534546
translation,7,4,model,four modules,to jointly extract,dialogue states,four modules to jointly extract dialogue states,0.6862474679946899
translation,7,4,model,model,present,fast and scalable architecture,model present fast and scalable architecture,0.6445931196212769
translation,7,21,model,fast and scalable method,called,emd,fast and scalable method called emd,0.6888217329978943
translation,7,21,model,fast and scalable method,decompose,dst,fast and scalable method decompose dst,0.7588241696357727
translation,7,21,model,dst,into,three classification modules,dst into three classification modules,0.605347216129303
translation,7,21,model,dst,into,one sequence labeling module,dst into one sequence labeling module,0.5818962454795837
translation,7,21,model,one sequence labeling module,to jointly extract,dialogue states,one sequence labeling module to jointly extract dialogue states,0.7088189721107483
translation,7,21,model,model,propose,fast and scalable method,model propose fast and scalable method,0.7064947485923767
translation,7,74,results,our model,achieves,best performance,our model achieves best performance,0.684498131275177
translation,7,74,results,best performance,of,50.18 %,best performance of 50.18 %,0.5352572798728943
translation,7,74,results,50.18 %,in,multi-domain testset,50.18 % in multi-domain testset,0.5246566534042358
translation,7,74,results,accuracy,achieved in,single- domain,accuracy achieved in single- domain,0.6809759736061096
translation,7,74,results,accuracy,on par with,state - of - theart results,accuracy on par with state - of - theart results,0.6834439635276794
translation,7,74,results,results,has,our model,results has our model,0.5871725678443909
translation,7,83,results,d l,improves,domain accuracy,d l improves domain accuracy,0.726050615310669
translation,7,83,results,results,adding,d l,results adding d l,0.6746222376823425
translation,8,127,ablation-analysis,more sharply,when,discarding,more sharply when discarding,0.7398390173912048
translation,8,127,ablation-analysis,network pruning,than,discarding,network pruning than discarding,0.6024534106254578
translation,8,127,ablation-analysis,tpem,has,drops,tpem has drops,0.6744216680526733
translation,8,127,ablation-analysis,drops,has,more sharply,drops has more sharply,0.6209616661071777
translation,8,127,ablation-analysis,discarding,has,network pruning,discarding has network pruning,0.6034194231033325
translation,8,127,ablation-analysis,discarding,has,other two components,discarding has other two components,0.5800033211708069
translation,8,111,baselines,glmp,has,"wu et al. , 2019 )","glmp has wu et al. , 2019 )",0.5979115962982178
translation,8,111,baselines,baselines,compare,tpem,baselines compare tpem,0.7083485126495361
translation,8,112,baselines,tpem,with,"ucl ( ahn et al. , 2019 )","tpem with ucl ( ahn et al. , 2019 )",0.6563706398010254
translation,8,112,baselines,tpem,is,popular continual learning method,tpem is popular continual learning method,0.544021725654602
translation,8,112,baselines,baselines,compare,tpem,baselines compare tpem,0.7083485126495361
translation,8,102,experimental-setup,word embeddings,randomly initialized from,"normal distribution n ( 0 , 0.1 )","word embeddings randomly initialized from normal distribution n ( 0 , 0.1 )",0.6886900067329407
translation,8,102,experimental-setup,"normal distribution n ( 0 , 0.1 )",with,size,"normal distribution n ( 0 , 0.1 ) with size",0.6234795451164246
translation,8,102,experimental-setup,size,of,128,size of 128,0.6380613446235657
translation,8,103,experimental-setup,size of encoder and decoder,as,128,size of encoder and decoder as 128,0.562328040599823
translation,8,103,experimental-setup,experimental setup,set,size of encoder and decoder,experimental setup set size of encoder and decoder,0.6282476186752319
translation,8,104,experimental-setup,one- shot pruning,with,ratio p = 0.5,one- shot pruning with ratio p = 0.5,0.6477668285369873
translation,8,104,experimental-setup,experimental setup,conduct,one- shot pruning,experimental setup conduct one- shot pruning,0.6564427614212036
translation,8,106,experimental-setup,adam optimizer,to train,model,adam optimizer to train model,0.7131099700927734
translation,8,106,experimental-setup,model,with,initial learning rate,model with initial learning rate,0.5982702374458313
translation,8,106,experimental-setup,initial learning rate,of,1e ?3,initial learning rate of 1e ?3,0.6243574619293213
translation,8,106,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,8,107,experimental-setup,batch size,set to,32,batch size set to 32,0.733751654624939
translation,8,107,experimental-setup,number of memory hop k,set to,3,number of memory hop k set to 3,0.7197291851043701
translation,8,107,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,8,107,experimental-setup,experimental setup,has,number of memory hop k,experimental setup has number of memory hop k,0.5474097728729248
translation,8,108,experimental-setup,maximum re-training epochs,to,5,maximum re-training epochs to 5,0.6153574585914612
translation,8,108,experimental-setup,experimental setup,set,maximum re-training epochs,experimental setup set maximum re-training epochs,0.6228016018867493
translation,8,123,experiments,typical tdss,i.e.,ptr-unk,typical tdss i.e. ptr-unk,0.6938977837562561
translation,8,123,experiments,typical tdss,i.e.,mem2seq,typical tdss i.e. mem2seq,0.684695303440094
translation,8,123,experiments,typical tdss,i.e.,glmp,typical tdss i.e. glmp,0.6921376585960388
translation,8,123,experiments,typical tdss,perform,much worse,typical tdss perform much worse,0.5866163372993469
translation,8,123,experiments,much worse,than,continual learning methods ( ucl and tpem ),much worse than continual learning methods ( ucl and tpem ),0.5582793951034546
translation,8,6,model,tpem,leverages,network pruning,tpem leverages network pruning,0.754963219165802
translation,8,6,model,tpem,adopts,network expanding,tpem adopts network expanding,0.6802376508712769
translation,8,6,model,tpem,introduces,task -specific network masking,tpem introduces task -specific network masking,0.6091734766960144
translation,8,6,model,network pruning,to keep,knowledge,network pruning to keep knowledge,0.5961710214614868
translation,8,6,model,knowledge,for,old tasks,knowledge for old tasks,0.595513105392456
translation,8,6,model,network expanding,to create,free weights,network expanding to create free weights,0.708608865737915
translation,8,6,model,free weights,for,new tasks,free weights for new tasks,0.6295373439788818
translation,8,6,model,task -specific network masking,to alleviate,negative impact,task -specific network masking to alleviate negative impact,0.6318523287773132
translation,8,6,model,negative impact,of,fixed weights,negative impact of fixed weights,0.6015830039978027
translation,8,6,model,fixed weights,of,old tasks,fixed weights of old tasks,0.5851938724517822
translation,8,6,model,old tasks,on,new tasks,old tasks on new tasks,0.5568308234214783
translation,8,6,model,model,has,tpem,model has tpem,0.6331953406333923
translation,8,23,model,continual learning method,for,task - oriented dialogue system,continual learning method for task - oriented dialogue system,0.5987035036087036
translation,8,23,model,continual learning method,with,"iterative network pruning , expanding and masking ( tpem )","continual learning method with iterative network pruning , expanding and masking ( tpem )",0.6468073129653931
translation,8,23,model,continual learning method,preserves,performance,continual learning method preserves performance,0.6941595673561096
translation,8,23,model,"iterative network pruning , expanding and masking ( tpem )",preserves,performance,"iterative network pruning , expanding and masking ( tpem ) preserves performance",0.6883687376976013
translation,8,23,model,performance,on,previously encountered tasks,performance on previously encountered tasks,0.49277740716934204
translation,8,23,model,model,propose,continual learning method,model propose continual learning method,0.6670007109642029
translation,8,26,model,network expanding strategy,devised to gradually create,free weights,network expanding strategy devised to gradually create free weights,0.7258814573287964
translation,8,26,model,free weights,for,new tasks,free weights for new tasks,0.6295373439788818
translation,8,26,model,model,has,network expanding strategy,model has network expanding strategy,0.5230671763420105
translation,8,39,model,continual learning method,for,tds,continual learning method for tds,0.5959858298301697
translation,8,39,model,continual learning method,with,iterative pruning,continual learning method with iterative pruning,0.633249044418335
translation,8,39,model,continual learning method,with,masking,continual learning method with masking,0.6651212573051453
translation,8,39,model,model,propose,continual learning method,model propose continual learning method,0.6670007109642029
translation,8,40,model,pruning,to keep,knowledge,pruning to keep knowledge,0.6412253975868225
translation,8,40,model,knowledge,for,old tasks,knowledge for old tasks,0.595513105392456
translation,8,40,model,model,leverage,pruning,model leverage pruning,0.7873186469078064
translation,8,41,model,network expanding,to create,free weights,network expanding to create free weights,0.708608865737915
translation,8,41,model,free weights,for,new tasks,free weights for new tasks,0.6295373439788818
translation,8,41,model,model,adopt,network expanding,model adopt network expanding,0.7298932075500488
translation,8,42,model,model,has,task -specific binary mask,model has task -specific binary mask,0.5421028733253479
translation,8,50,model,external knowledge,into,seq2seq model,external knowledge into seq2seq model,0.5527219772338867
translation,8,50,model,end-to - end memory networks,to encode,word-level information,end-to - end memory networks to encode word-level information,0.7526580095291138
translation,8,50,model,word-level information,for,dialogue history ( dialogue memory ),word-level information for dialogue history ( dialogue memory ),0.5980923771858215
translation,8,50,model,word-level information,for,structural knowledge base ( kb memory ),word-level information for structural knowledge base ( kb memory ),0.5979725122451782
translation,8,50,model,model,To integrate,external knowledge,model To integrate external knowledge,0.7181389331817627
translation,8,51,model,bag- of - word representations,utilized as,memory embeddings,bag- of - word representations utilized as memory embeddings,0.54185551404953
translation,8,51,model,memory embeddings,for,two memory modules,memory embeddings for two memory modules,0.6209198832511902
translation,8,51,model,model,has,bag- of - word representations,model has bag- of - word representations,0.5136428475379944
translation,8,54,model,each input token,of,dialogue history,each input token of dialogue history,0.5554091930389404
translation,8,54,model,fixed - size vector,via,embedding layer,fixed - size vector via embedding layer,0.6653740406036377
translation,8,54,model,model,convert,each input token,model convert each input token,0.7059659957885742
translation,8,115,model,learning,by utilizing,parameters,learning by utilizing parameters,0.6657397747039795
translation,8,115,model,tds,by utilizing,parameters,tds by utilizing parameters,0.674565851688385
translation,8,115,model,parameters,learned from,past tasks,parameters learned from past tasks,0.6866381764411926
translation,8,115,model,parameters,save,separate model,parameters save separate model,0.6695939898490906
translation,8,115,model,initialization,for,new task,initialization for new task,0.6272878050804138
translation,8,115,model,re-init and re-initexpand,save,separate model,re-init and re-initexpand save separate model,0.6914939880371094
translation,8,115,model,separate model,for,each task,separate model for each task,0.6437675356864929
translation,8,115,model,each task,in,inference,each task in inference,0.5347723364830017
translation,8,115,model,each task,without considering,continual learning scenario,each task without considering continual learning scenario,0.7490900754928589
translation,8,115,model,inference,without considering,continual learning scenario,inference without considering continual learning scenario,0.7965753078460693
translation,8,115,model,learning,has,tds,learning has tds,0.6416093111038208
translation,8,7,results,tpem,leads to,significantly improved results,tpem leads to significantly improved results,0.7018194198608398
translation,8,7,results,significantly improved results,over,strong competitors,significantly improved results over strong competitors,0.6892737746238708
translation,8,125,results,tpem,achieves,significantly better results,tpem achieves significantly better results,0.6746883988380432
translation,8,125,results,significantly better results,than,baseline methods,significantly better results than baseline methods,0.568551778793335
translation,8,125,results,results,has,tpem,results has tpem,0.5459192991256714
translation,8,126,results,improvement,mainly comes from,iterative network pruning,improvement mainly comes from iterative network pruning,0.6864374279975891
translation,8,126,results,improvement,mainly comes from,expanding,improvement mainly comes from expanding,0.7110297083854675
translation,8,126,results,iterative network pruning,has,expanding,iterative network pruning has expanding,0.5853057503700256
translation,8,126,results,results,has,improvement,results has improvement,0.6248279809951782
translation,8,129,results,all the components,achieves,best results,all the components achieves best results,0.6180518269538879
translation,8,129,results,results,combining,all the components,results combining all the components,0.6918228268623352
translation,8,130,results,results,of,re-init and re-init-expand,results of re-init and re-init-expand,0.5899097323417664
translation,8,130,results,re-init and re-init-expand,observe that,only using network expanding,re-init and re-init-expand observe that only using network expanding,0.6285921335220337
translation,8,130,results,only using network expanding,cannot improve,performance,only using network expanding cannot improve performance,0.7842290997505188
translation,8,130,results,performance,of,re-init,performance of re-init,0.6648739576339722
translation,8,130,results,results,comparing,results,results comparing results,0.5670907497406006
translation,8,130,results,results,of,re-init and re-init-expand,results of re-init and re-init-expand,0.5899097323417664
translation,9,93,ablation-analysis,no significant effect,for,instruction followers,no significant effect for instruction followers,0.6059083342552185
translation,9,93,ablation-analysis,overall positive effect,driven by,instruction givers,overall positive effect driven by instruction givers,0.663381814956665
translation,9,93,ablation-analysis,ablation analysis,Analysing,instruction giver and follower informationtransmission,ablation analysis Analysing instruction giver and follower informationtransmission,0.6979607939720154
translation,9,20,hyperparameters,information density,use,pre-trained transformer - based language model,information density use pre-trained transformer - based language model,0.5929251909255981
translation,9,20,hyperparameters,pre-trained transformer - based language model,provides,more robust measurements,pre-trained transformer - based language model provides more robust measurements,0.6214508414268494
translation,9,20,hyperparameters,more robust measurements,than,n-gram models,more robust measurements than n-gram models,0.559935986995697
translation,9,20,hyperparameters,hyperparameters,To estimate,information density,hyperparameters To estimate information density,0.7339636087417603
translation,9,74,results,linear mixed - effect models,show,significant positive effect,linear mixed - effect models show significant positive effect,0.6094200611114502
translation,9,74,results,significant positive effect,of,sentence position,significant positive effect of sentence position,0.5343894958496094
translation,9,74,results,sentence position,on,information content,sentence position on information content,0.5220794677734375
translation,9,74,results,information content,within,"articles ( ? = 1.65e?2 , p < 0.001 )","information content within articles ( ? = 1.65e?2 , p < 0.001 )",0.575840950012207
translation,9,74,results,information content,within,"paragraphs ( ? = 1.53e?2 , p < 0.01 )","information content within paragraphs ( ? = 1.53e?2 , p < 0.01 )",0.6042830348014832
translation,9,74,results,results,has,linear mixed - effect models,results has linear mixed - effect models,0.4219818115234375
translation,9,89,results,effect,in,transactions,effect in transactions,0.594251275062561
translation,9,89,results,transactions,with,backchannels,transactions with backchannels,0.673984706401825
translation,9,89,results,linear mixed - effect models,show,positive effect,linear mixed - effect models show positive effect,0.6062637567520142
translation,9,89,results,positive effect,of,turn position,positive effect of turn position,0.6065090894699097
translation,9,89,results,turn position,has,within transactions without backchannels,turn position has within transactions without backchannels,0.6234459280967712
translation,9,89,results,results,fail to,linear mixed - effect models,results fail to linear mixed - effect models,0.5252410769462585
translation,9,89,results,results,find,effect,results find effect,0.5598922371864319
translation,10,169,ablation-analysis,larger ? mix,for,delorean - ft,larger ? mix for delorean - ft,0.7090256810188293
translation,10,169,ablation-analysis,larger ? mix,lead to,lower bleu,larger ? mix lead to lower bleu,0.67750084400177
translation,10,169,ablation-analysis,delorean - ft,lead to,lower bleu,delorean - ft lead to lower bleu,0.7168235182762146
translation,10,169,ablation-analysis,delorean - ft,lead to,higher p-score,delorean - ft lead to higher p-score,0.7006661891937256
translation,10,169,ablation-analysis,ablation analysis,has,larger ? mix,ablation analysis has larger ? mix,0.5855969786643982
translation,10,174,ablation-analysis,ablation analysis,observe,dialogue history,ablation analysis observe dialogue history,0.6417196393013
translation,10,175,ablation-analysis,sentence deletion,contributes largely to,performance,sentence deletion contributes largely to performance,0.6099199056625366
translation,10,175,ablation-analysis,performance,especially for,persona consistency,performance especially for persona consistency,0.6443437933921814
translation,10,175,ablation-analysis,ablation analysis,has,sentence deletion,ablation analysis has sentence deletion,0.545623242855072
translation,10,140,baselines,state - of - the - art models,for,unsupervised text style transfer,state - of - the - art models for unsupervised text style transfer,0.512076199054718
translation,10,140,baselines,state - of - the - art models,for,counterfactual story generation,state - of - the - art models for counterfactual story generation,0.5713167786598206
translation,10,272,baselines,unmt and cyclegan,Transformers with,distilgpt - 2 encoder and decoder,unmt and cyclegan Transformers with distilgpt - 2 encoder and decoder,0.7508232593536377
translation,10,272,baselines,distilgpt - 2 encoder and decoder,initialized with,distilgpt2,distilgpt - 2 encoder and decoder initialized with distilgpt2,0.7564764618873596
translation,10,272,baselines,baselines,has,unmt and cyclegan,baselines has unmt and cyclegan,0.6202474236488342
translation,10,105,hyperparameters,greedy decoding,at,inference,greedy decoding at inference,0.5769501328468323
translation,10,105,hyperparameters,hyperparameters,apply,label smoothing ( = 0.1 ),hyperparameters apply label smoothing ( = 0.1 ),0.5823986530303955
translation,10,105,hyperparameters,hyperparameters,use,greedy decoding,hyperparameters use greedy decoding,0.6568648219108582
translation,10,148,hyperparameters,gradient estimator,for,optimization,gradient estimator for optimization,0.6118277907371521
translation,10,148,hyperparameters,hyperparameters,Gumbel-softmax straight through,gradient estimator,hyperparameters Gumbel-softmax straight through gradient estimator,0.7627513408660889
translation,10,266,hyperparameters,batch size,is,32,batch size is 32,0.6284153461456299
translation,10,266,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,10,267,hyperparameters,"adam ( kingma and ba , 2015 )",with,initial learning rate 5 ? 10 ?5,"adam ( kingma and ba , 2015 ) with initial learning rate 5 ? 10 ?5",0.6566119194030762
translation,10,267,hyperparameters,"adam ( kingma and ba , 2015 )",with,gradient clip 1.0,"adam ( kingma and ba , 2015 ) with gradient clip 1.0",0.5658566355705261
translation,10,267,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,10,268,hyperparameters,learning rate,decays by,half,learning rate decays by half,0.7433252930641174
translation,10,268,hyperparameters,half,when,average metric,half when average metric,0.6697220206260681
translation,10,268,hyperparameters,average metric,does,improve,average metric does improve,0.3398436903953552
translation,10,268,hyperparameters,average metric,not,improve,average metric not improve,0.6513034105300903
translation,10,268,hyperparameters,improve,for,two validations,improve for two validations,0.6217016577720642
translation,10,268,hyperparameters,terminates,after,three decays,terminates after three decays,0.6715730428695679
translation,10,268,hyperparameters,training,has,terminates,training has terminates,0.5664328336715698
translation,10,268,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,10,268,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,10,6,model,grounded minimal editing framework,minimally edits,existing responses,grounded minimal editing framework minimally edits existing responses,0.7860110402107239
translation,10,6,model,existing responses,grounded on,given concept,existing responses grounded on given concept,0.7749307751655579
translation,10,6,model,model,propose,grounded minimal editing framework,model propose grounded minimal editing framework,0.6399912238121033
translation,10,7,model,personas,propose,grounded minimal editor ( gme ),personas propose grounded minimal editor ( gme ),0.6356884837150574
translation,10,7,model,grounded minimal editor ( gme ),learns to,edit,grounded minimal editor ( gme ) learns to edit,0.7031950354576111
translation,10,7,model,edit,by,disentangling,edit by disentangling,0.6199891567230225
translation,10,7,model,persona-related and persona-agnostic parts,of,response,persona-related and persona-agnostic parts of response,0.6045547723770142
translation,10,7,model,model,Focusing on,personas,model Focusing on personas,0.6861298680305481
translation,10,7,model,model,propose,grounded minimal editor ( gme ),model propose grounded minimal editor ( gme ),0.6567813158035278
translation,10,30,model,grounded minimal editor ( gme ),trained on,persona-grounded dialogue data,grounded minimal editor ( gme ) trained on persona-grounded dialogue data,0.7364429831504822
translation,10,30,model,model,propose,grounded minimal editor ( gme ),model propose grounded minimal editor ( gme ),0.6567813158035278
translation,10,31,model,response templates,sampled by,corrupting persona-related spans and sentences,response templates sampled by corrupting persona-related spans and sentences,0.6716027855873108
translation,10,31,model,corrupting persona-related spans and sentences,based on,gradient - based attribution,corrupting persona-related spans and sentences based on gradient - based attribution,0.6387002468109131
translation,10,31,model,corrupting persona-related spans and sentences,based on,word overlap,corrupting persona-related spans and sentences based on word overlap,0.6216263771057129
translation,10,31,model,model,has,response templates,model has response templates,0.5507633090019226
translation,10,32,model,gme,recombines,persona-related and persona-agnostic expressions,gme recombines persona-related and persona-agnostic expressions,0.6776657700538635
translation,10,32,model,templates,has,gme,templates has gme,0.6019089818000793
translation,10,32,model,model,denoising,templates,model denoising templates,0.764458179473877
translation,10,39,results,gme,improves,persona consistency,gme improves persona consistency,0.7289547920227051
translation,10,39,results,persona consistency,of,responses,persona consistency of responses,0.5581967830657959
translation,10,39,results,responses,generated by,"pretrained blender - 90 m models ( roller et al. , 2020 )","responses generated by pretrained blender - 90 m models ( roller et al. , 2020 )",0.6377485394477844
translation,10,39,results,results,observe,gme,results observe gme,0.607934296131134
translation,10,40,results,gme - edited responses,has,largely outperforms,gme - edited responses has largely outperforms,0.5996169447898865
translation,10,40,results,largely outperforms,has,transfertransfo,largely outperforms has transfertransfo,0.6012764573097229
translation,10,40,results,results,show that,gme - edited responses,results show that gme - edited responses,0.46107861399650574
translation,10,171,results,gme,achieves,31.9 % relative improvement,gme achieves 31.9 % relative improvement,0.665066123008728
translation,10,171,results,31.9 % relative improvement,on,average score,31.9 % relative improvement on average score,0.5252909660339355
translation,10,171,results,31.9 % relative improvement,over,best performing baseline,31.9 % relative improvement over best performing baseline,0.6432362198829651
translation,10,171,results,average score,over,best performing baseline,average score over best performing baseline,0.651330292224884
translation,10,171,results,results,has,gme,results has gme,0.5603944659233093
translation,10,173,results,removing dialogue histories,from,data,removing dialogue histories from data,0.611522376537323
translation,10,173,results,removing sentence deletion,from,gme,removing sentence deletion from gme,0.5873883962631226
translation,10,181,results,human annotators,prefer,gme,human annotators prefer gme,0.7171634435653687
translation,10,181,results,gme,to,baselines,gme to baselines,0.5738308429718018
translation,10,181,results,results,shows,human annotators,results shows human annotators,0.5674270391464233
translation,10,210,results,grounded minimal editing,addresses,transferability issue,grounded minimal editing addresses transferability issue,0.6227607131004333
translation,10,210,results,transferability issue,faced by,transfertransfo,transferability issue faced by transfertransfo,0.7250612378120422
translation,10,210,results,results,show,grounded minimal editing,results show grounded minimal editing,0.6370564699172974
translation,10,215,results,free-marginal ?,for,"knowledge , empathy , persona , and grammaticality","free-marginal ? for knowledge , empathy , persona , and grammaticality",0.5619357228279114
translation,10,215,results,free-marginal ?,for,"almost perfect , substantial , almost perfect , and substantial agreement","free-marginal ? for almost perfect , substantial , almost perfect , and substantial agreement",0.6101428270339966
translation,10,215,results,"knowledge , empathy , persona , and grammaticality",is,"0.92 , 0.70 , 0.85 , and 0.78","knowledge , empathy , persona , and grammaticality is 0.92 , 0.70 , 0.85 , and 0.78",0.5044100284576416
translation,10,216,results,largely improved,after,gme editing,largely improved after gme editing,0.6591351628303528
translation,10,216,results,results,show that,persona consistency,results show that persona consistency,0.4413635730743408
translation,10,217,results,much lower knowledge and empathy,than,responses,much lower knowledge and empathy than responses,0.5558369755744934
translation,10,217,results,responses,edited by,gme,responses edited by gme,0.6124314069747925
translation,10,217,results,trans-fertransfo,has,highest persona consistency,trans-fertransfo has highest persona consistency,0.6151272058486938
translation,10,217,results,results,has,trans-fertransfo,results has trans-fertransfo,0.5863699316978455
translation,10,307,results,largely improved,after,gme editing,largely improved after gme editing,0.6591351628303528
translation,10,307,results,largely improved,from,9.2 to 33.0,largely improved from 9.2 to 33.0,0.5452022552490234
translation,10,307,results,largely improved,from,0.8 to 29.4,largely improved from 0.8 to 29.4,0.5340162515640259
translation,10,307,results,gme editing,from,9.2 to 33.0,gme editing from 9.2 to 33.0,0.5373712778091431
translation,10,307,results,gme editing,from,0.8 to 29.4,gme editing from 0.8 to 29.4,0.535746693611145
translation,10,307,results,9.2 to 33.0,from,0.8 to 29.4,9.2 to 33.0 from 0.8 to 29.4,0.4620265066623688
translation,10,307,results,results,shows that,p-scores,results shows that p-scores,0.5949548482894897
translation,11,50,experiments,2.7b parameter model,performed,best,2.7b parameter model performed best,0.28287002444267273
translation,11,50,experiments,best,in,human evaluations of engagingness,best in human evaluations of engagingness,0.5046512484550476
translation,11,46,model,large transformer - based architectures,trained on,dialogue tasks,large transformer - based architectures trained on dialogue tasks,0.7230018973350525
translation,11,46,model,model,consider,large transformer - based architectures,model consider large transformer - based architectures,0.710425078868866
translation,11,165,results,  bakedin   method,triggers,classifier,  bakedin   method triggers classifier,0.7432113289833069
translation,11,165,results,0.2 % vs. 6.8 %,of,time,0.2 % vs. 6.8 % of time,0.5737589597702026
translation,11,165,results,time,for,preprocessing,time for preprocessing,0.638874888420105
translation,11,165,results,io reddit,has,  bakedin   method,io reddit has   bakedin   method,0.6184419393539429
translation,11,165,results,classifier,has,0.2 % vs. 6.8 %,classifier has 0.2 % vs. 6.8 %,0.5814977288246155
translation,11,165,results,results,has,io reddit,results has io reddit,0.6121582388877869
translation,11,165,results,results,has,  bakedin   method,results has   bakedin   method,0.5248218774795532
translation,11,180,results,standard models,indicate,bst 2.7b,standard models indicate bst 2.7b,0.6004078984260559
translation,11,180,results,bst 2.7b,is,significantly more engaging,bst 2.7b is significantly more engaging,0.554573655128479
translation,11,180,results,significantly more engaging,than,gpt2,significantly more engaging than gpt2,0.5757219195365906
translation,11,180,results,significantly more engaging,than,dialogpt,significantly more engaging than dialogpt,0.5872637629508972
translation,11,180,results,significantly more engaging,than,pushift,significantly more engaging than pushift,0.587489128112793
translation,11,180,results,results,on,standard models,results on standard models,0.48313263058662415
translation,11,183,results,engagingness,found to be,not significantly distinguishable,engagingness found to be not significantly distinguishable,0.6287635564804077
translation,11,183,results,not significantly distinguishable,from,our base bst 2.7b model,not significantly distinguishable from our base bst 2.7b model,0.5890706181526184
translation,11,183,results,results,has,engagingness,results has engagingness,0.545030951499939
translation,11,184,results,baked - in model,performs,similarly,baked - in model performs similarly,0.6633242964744568
translation,11,184,results,similarly,to,base bst 2.7b model,similarly to base bst 2.7b model,0.5833820104598999
translation,11,184,results,base bst 2.7b model,with respect to,engagingness,base bst 2.7b model with respect to engagingness,0.6672770380973816
translation,11,184,results,results,has,baked - in model,results has baked - in model,0.5476374626159668
translation,11,191,results,two -stage bad classifier approach,improves over,our other safety classifiers,two -stage bad classifier approach improves over our other safety classifiers,0.7760790586471558
translation,11,191,results,our other safety classifiers,used in,twostage systems,our other safety classifiers used in twostage systems,0.747003436088562
translation,11,191,results,our other safety classifiers,yielding,94.4 % ok rate,our other safety classifiers yielding 94.4 % ok rate,0.66922527551651
translation,11,191,results,94.4 % ok rate,on,adversarial data,94.4 % ok rate on adversarial data,0.515364944934845
translation,11,191,results,results,has,two -stage bad classifier approach,results has two -stage bad classifier approach,0.5586184859275818
translation,11,193,results,  baked - in   model,see,clear gains,  baked - in   model see clear gains,0.6462103724479675
translation,11,193,results,clear gains,relative to,standard models,clear gains relative to standard models,0.6840540766716003
translation,11,193,results,results,For,  baked - in   model,results For   baked - in   model,0.6390739679336548
translation,11,199,results,dialogpt,produces,safe responses,dialogpt produces safe responses,0.7063271999359131
translation,11,199,results,best case,has,dialogpt,best case has dialogpt,0.6310468316078186
translation,11,199,results,results,has,dialogpt,results has dialogpt,0.5819152593612671
translation,11,200,results,gpt2,performs,worst,gpt2 performs worst,0.7115646004676819
translation,11,200,results,worst,providing,safe responses,worst providing safe responses,0.6962669491767883
translation,11,200,results,safe responses,has,54.4 % of the time,safe responses has 54.4 % of the time,0.5415822267532349
translation,11,200,results,results,has,gpt2,results has gpt2,0.5226525664329529
translation,11,201,results,our two -stage models,get,near perfect scores,our two -stage models get near perfect scores,0.5697536468505859
translation,11,201,results,range,from,97.8 to 98.3,range from 97.8 to 98.3,0.5231968760490417
translation,11,201,results,near perfect scores,has,scores,near perfect scores has scores,0.5834057927131653
translation,11,201,results,scores,has,range,scores has range,0.4971744120121002
translation,11,201,results,results,has,our two -stage models,results has our two -stage models,0.4682919979095459
translation,12,27,ablation-analysis,further pre-training,with,masked language model,further pre-training with masked language model,0.6476837396621704
translation,12,27,ablation-analysis,masked language model,does not achieve,improvements,masked language model does not achieve improvements,0.671333372592926
translation,12,27,ablation-analysis,masked language model,design,special further pre-training tasks,masked language model design special further pre-training tasks,0.5590981841087341
translation,12,27,ablation-analysis,improvements,for,all downstream tasks,improvements for all downstream tasks,0.6156207323074341
translation,12,27,ablation-analysis,special further pre-training tasks,according to,characteristics of dialogue data,special further pre-training tasks according to characteristics of dialogue data,0.6830899119377136
translation,12,27,ablation-analysis,ablation analysis,design,special further pre-training tasks,ablation analysis design special further pre-training tasks,0.5689275860786438
translation,12,27,ablation-analysis,ablation analysis,has,further pre-training,ablation analysis has further pre-training,0.5542293190956116
translation,12,29,ablation-analysis,key factors,influencing,effectiveness,key factors influencing effectiveness,0.7330551743507385
translation,12,29,ablation-analysis,effectiveness,of,further pre-training,effectiveness of further pre-training,0.5729779005050659
translation,12,29,ablation-analysis,further pre-training,on,certain downstream task,further pre-training on certain downstream task,0.5538227558135986
translation,12,29,ablation-analysis,ablation analysis,has,model 's ability and structure,ablation analysis has model 's ability and structure,0.4747025668621063
translation,12,197,ablation-analysis,mlm,leads to,performance drop,mlm leads to performance drop,0.6732101440429688
translation,12,197,ablation-analysis,performance drop,across,almost all downstream tasks,performance drop across almost all downstream tasks,0.7063781023025513
translation,12,197,ablation-analysis,ablation analysis,Removing,mlm,ablation analysis Removing mlm,0.7513031363487244
translation,12,200,ablation-analysis,masked language model,not enough for,further pre-training,masked language model not enough for further pre-training,0.7122219204902649
translation,12,200,ablation-analysis,important role,for,enhancing fine-tuning,important role for enhancing fine-tuning,0.6199392676353455
translation,12,200,ablation-analysis,task - oriented dialogue,has,masked language model,task - oriented dialogue has masked language model,0.5160807371139526
translation,12,200,ablation-analysis,ablation analysis,area of,task - oriented dialogue,ablation analysis area of task - oriented dialogue,0.6644807457923889
translation,12,203,ablation-analysis,further pre-training task,are,key factors,further pre-training task are key factors,0.5516602396965027
translation,12,203,ablation-analysis,influencing the performance,of,fine-tuning,influencing the performance of fine-tuning,0.5591106414794922
translation,12,203,ablation-analysis,fine-tuning,on,downstream task,fine-tuning on downstream task,0.5645554065704346
translation,12,203,ablation-analysis,key factors,has,influencing the performance,key factors has influencing the performance,0.5727415680885315
translation,12,203,ablation-analysis,ablation analysis,Ability and structure of,further pre-training task,ablation analysis Ability and structure of further pre-training task,0.70018070936203
translation,12,147,experimental-setup,pre-training,set,learning rate,pre-training set learning rate,0.6584051847457886
translation,12,147,experimental-setup,pre-training,set,batch size,pre-training set batch size,0.6837484240531921
translation,12,147,experimental-setup,pre-training,set,maximum sequence length,pre-training set maximum sequence length,0.6917949914932251
translation,12,147,experimental-setup,learning rate,equal to,5e - 5,learning rate equal to 5e - 5,0.7101339101791382
translation,12,147,experimental-setup,maximum sequence length,to,512,maximum sequence length to 512,0.5570472478866577
translation,12,147,experimental-setup,experimental setup,For,pre-training,experimental setup For pre-training,0.5703844428062439
translation,12,148,experimental-setup,fine- tuning,set,learning rate,fine- tuning set learning rate,0.6312320232391357
translation,12,148,experimental-setup,learning rate,to,5e - 5,learning rate to 5e - 5,0.6061331629753113
translation,12,148,experimental-setup,experimental setup,For,fine- tuning,experimental setup For fine- tuning,0.6026358008384705
translation,12,149,experimental-setup,batch size,maximizes,gpu usage,batch size maximizes gpu usage,0.7124296426773071
translation,12,149,experimental-setup,experimental setup,use,batch size,experimental setup use batch size,0.6168987154960632
translation,12,150,experimental-setup,our models,using,adam optimizer,our models using adam optimizer,0.6424208879470825
translation,12,150,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,12,151,experimental-setup,early -stopped,using,loss of a validation set,early -stopped using loss of a validation set,0.6539108753204346
translation,12,151,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,12,152,experimental-setup,three times,with,different seeds,three times with different seeds,0.6340556740760803
translation,12,152,experimental-setup,each downstream task,has,three times,each downstream task has three times,0.6031551361083984
translation,12,152,experimental-setup,experimental setup,train,each downstream task,experimental setup train each downstream task,0.6768935322761536
translation,12,153,experimental-setup,4 nvidia v100 gpus,for,further pretraining,4 nvidia v100 gpus for further pretraining,0.6032807230949402
translation,12,153,experimental-setup,one,for,fine-tuning,one for fine-tuning,0.6660579442977905
translation,12,24,model,specially designed pre-training tasks,to further pre-training,bert,specially designed pre-training tasks to further pre-training bert,0.8186143040657043
translation,12,24,model,bert,on,domainspecific corpus,bert on domainspecific corpus,0.5309606194496155
translation,12,24,model,bert,obtaining,multiple new pre-trained models,bert obtaining multiple new pre-trained models,0.655900239944458
translation,12,24,model,model,use,specially designed pre-training tasks,model use specially designed pre-training tasks,0.6587002277374268
translation,12,28,results,have different effects,on,different downstream tasks,have different effects on different downstream tasks,0.530925452709198
translation,12,28,results,results,has,different pre-training tasks,results has different pre-training tasks,0.5222079753875732
translation,12,30,results,two further pre-training tasks,in,multi-task paradigm,two further pre-training tasks in multi-task paradigm,0.480879008769989
translation,12,30,results,two further pre-training tasks,does not lead to,incremental performance improvements,two further pre-training tasks does not lead to incremental performance improvements,0.660219132900238
translation,12,30,results,incremental performance improvements,on,downstream tasks,incremental performance improvements on downstream tasks,0.5482544302940369
translation,12,30,results,training,has,two further pre-training tasks,training has two further pre-training tasks,0.5523919463157654
translation,12,30,results,results,has,training,results has training,0.5440090298652649
translation,12,156,results,m lm 3,does not surpass,bert 2,m lm 3 does not surpass bert 2,0.6588206887245178
translation,12,156,results,bert 2,in,all metrics and datasets,bert 2 in all metrics and datasets,0.4653896987438202
translation,12,156,results,results,has,m lm 3,results has m lm 3,0.574832558631897
translation,12,165,results,bert 2,on,response selection and dialog state tracking task,bert 2 on response selection and dialog state tracking task,0.49472343921661377
translation,12,169,results,"dsp , crm , and dcv",better than,m lm 3,"dsp , crm , and dcv better than m lm 3",0.7047304511070251
translation,12,169,results,m lm 3,on,most of the metrics,m lm 3 on most of the metrics,0.5327240824699402
translation,12,170,results,different pre-training tasks,more beneficial to,different downstream tasks,different pre-training tasks more beneficial to different downstream tasks,0.7297475934028625
translation,12,170,results,dsp,more beneficial to,downstream intent recognition task,dsp more beneficial to downstream intent recognition task,0.6731244921684265
translation,12,170,results,downstream intent recognition task,than,others,downstream intent recognition task than others,0.5766901969909668
translation,12,170,results,dcv,beneficial to,dialogue state tracking,dcv beneficial to dialogue state tracking,0.6712335348129272
translation,12,170,results,results,observe,different pre-training tasks,results observe different pre-training tasks,0.5574641227722168
translation,12,184,results,drp,on,intent recognition,drp on intent recognition,0.5546925067901611
translation,12,184,results,drp,on,dialogue state tracking tasks,drp on dialogue state tracking tasks,0.5425810813903809
translation,12,184,results,dialogue state tracking tasks,across,all metrics,dialogue state tracking tasks across all metrics,0.6447575688362122
translation,12,184,results,enp,has,outperforms,enp has outperforms,0.663175106048584
translation,12,184,results,outperforms,has,drp,outperforms has drp,0.6177823543548584
translation,12,188,results,crm,surpasses,dur,crm surpasses dur,0.6911069750785828
translation,12,188,results,dur,on,response selection task,dur on response selection task,0.5254878401756287
translation,12,188,results,results,show,crm,results show crm,0.4532999098300934
translation,12,204,results,two further pre-training tasks,in,multi-task paradigm,two further pre-training tasks in multi-task paradigm,0.480879008769989
translation,12,204,results,two further pre-training tasks,does not lead to,incremental performance improvement,two further pre-training tasks does not lead to incremental performance improvement,0.6706939339637756
translation,12,204,results,training,has,two further pre-training tasks,training has two further pre-training tasks,0.5523919463157654
translation,12,204,results,results,has,training,results has training,0.5440090298652649
translation,13,160,ablation-analysis,ablation analysis,has,recurrent memory,ablation analysis has recurrent memory,0.5446957349777222
translation,13,4,model,successfully collaborates,with,people,successfully collaborates with people,0.6530786752700806
translation,13,4,model,people,in,partially - observable reference game,people in partially - observable reference game,0.5116389989852905
translation,13,4,model,model,present,grounded neural dialogue model,model present grounded neural dialogue model,0.6766834259033203
translation,13,9,results,our agent,obtaining,20 % relative improvement,our agent obtaining 20 % relative improvement,0.6430302858352661
translation,13,9,results,our agent,obtaining,50 % relative improvement,our agent obtaining 50 % relative improvement,0.6515668034553528
translation,13,9,results,previous state of the art,for,task,previous state of the art for task,0.5639442205429077
translation,13,9,results,20 % relative improvement,in,successful task completion,20 % relative improvement in successful task completion,0.5310987234115601
translation,13,9,results,successful task completion,in,self - play evaluations,successful task completion in self - play evaluations,0.4829636216163635
translation,13,9,results,50 % relative improvement,in,success,50 % relative improvement in success,0.5197863578796387
translation,13,9,results,success,in,human evaluations,success in human evaluations,0.4936010241508484
translation,13,9,results,our agent,has,substantially outperforms,our agent has substantially outperforms,0.6151049137115479
translation,13,9,results,substantially outperforms,has,previous state of the art,substantially outperforms has previous state of the art,0.5582051277160645
translation,13,9,results,results,has,our agent,results has our agent,0.5930655002593994
translation,13,159,results,structured reference resolver,able to learn,features,structured reference resolver able to learn features,0.6051364541053772
translation,13,159,results,structured reference resolver,improves,exact match,structured reference resolver improves exact match,0.6894309520721436
translation,13,159,results,features,in,potentials,features in potentials,0.5769458413124084
translation,13,159,results,exact match,from,44 % to 76 %,exact match from 44 % to 76 %,0.5254272818565369
translation,13,159,results,exact match,compared to,ablated version,exact match compared to ablated version,0.7263467907905579
translation,13,159,results,ablated version,of,our system,ablated version of our system,0.6167852878570557
translation,13,159,results,results,has,structured reference resolver,results has structured reference resolver,0.5677515864372253
translation,13,167,results,substantial improvements,to,our system,substantial improvements to our system,0.5769689083099365
translation,13,167,results,substantial improvements,from,structured referent prediction and the recurrent reference memory,substantial improvements from structured referent prediction and the recurrent reference memory,0.539757490158081
translation,13,167,results,results,see,substantial improvements,results see substantial improvements,0.6006451845169067
translation,13,168,results,our full system,without,pragmatic generation,our full system without pragmatic generation,0.7354815006256104
translation,13,168,results,our full system,improves over,further improvement,our full system improves over further improvement,0.792614758014679
translation,13,168,results,51 % to 58 %,in,hardest setting,51 % to 58 % in hardest setting,0.5361695289611816
translation,13,168,results,further improvement,to,62 %,further improvement to 62 %,0.5601245760917664
translation,13,168,results,62 %,when adding,pragmatic generation procedure,62 % when adding pragmatic generation procedure,0.7255591750144958
translation,13,168,results,results,has,our full system,results has our full system,0.5785884857177734
translation,13,176,results,models of u&a ( 2020 ) and our full + prag,perform,worse,models of u&a ( 2020 ) and our full + prag perform worse,0.6000463366508484
translation,13,176,results,worse,against,humans,worse against humans,0.6733747720718384
translation,13,176,results,humans,than against,agent partners,humans than against agent partners,0.664724588394165
translation,13,176,results,results,has,models of u&a ( 2020 ) and our full + prag,results has models of u&a ( 2020 ) and our full + prag,0.5519337058067322
translation,13,198,results,improvements,from,entity -centric approach,improvements from entity -centric approach,0.580370306968689
translation,13,198,results,entity -centric approach,with,structured memory,entity -centric approach with structured memory,0.6445992588996887
translation,13,198,results,results,find,improvements,results find improvements,0.6222331523895264
translation,14,141,ablation-analysis,models,injecting,additional training signal,models injecting additional training signal,0.6796351075172424
translation,14,141,ablation-analysis,additional training signal,does not help,in-domain performance,additional training signal does not help in-domain performance,0.603394627571106
translation,14,58,experiments,sequence tagging,on,dialogue rewriting,sequence tagging on dialogue rewriting,0.5406657457351685
translation,14,58,experiments,sequence tagging,showing,much better performances,sequence tagging showing much better performances,0.694858968257904
translation,14,58,experiments,much better performances,than,bert - based strong baselines,much better performances than bert - based strong baselines,0.59396892786026
translation,14,124,hyperparameters,model,on top of,"bert - base model ( devlin et al. , 2019 )","model on top of bert - base model ( devlin et al. , 2019 )",0.6871073842048645
translation,14,124,hyperparameters,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,14,124,hyperparameters,"adam ( kingma and ba , 2015 )",setting,learning rate,"adam ( kingma and ba , 2015 ) setting learning rate",0.4149436056613922
translation,14,124,hyperparameters,learning rate,to,3e ?5,learning rate to 3e ?5,0.5943372845649719
translation,14,124,hyperparameters,hyperparameters,implement,baseline,hyperparameters implement baseline,0.6775903105735779
translation,14,124,hyperparameters,hyperparameters,implement,model,hyperparameters implement model,0.6560510396957397
translation,14,124,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,14,6,model,novel sequence - taggingbased model,so that,search space,novel sequence - taggingbased model so that search space,0.5690839886665344
translation,14,6,model,search space,is,significantly reduced,search space is significantly reduced,0.5908280611038208
translation,14,6,model,model,proposing,novel sequence - taggingbased model,model proposing novel sequence - taggingbased model,0.6856078505516052
translation,14,28,model,novel solution,treats,utterance rewriting,novel solution treats utterance rewriting,0.6517734527587891
translation,14,28,model,utterance rewriting,as,multi-task sequence tagging,utterance rewriting as multi-task sequence tagging,0.49811697006225586
translation,14,28,model,model,propose,novel solution,model propose novel solution,0.7334994673728943
translation,14,29,model,each input word,decide,delete,each input word decide delete,0.6901805400848389
translation,14,29,model,each input word,choose,span,each input word choose span,0.7014387249946594
translation,14,29,model,span,from,dialogue context,span from dialogue context,0.5760195255279541
translation,14,29,model,model,for,each input word,model for each input word,0.5972878336906433
translation,14,138,results,our tagging - based models,are,much better,our tagging - based models are much better,0.5556175708770752
translation,14,138,results,our tagging - based models,get,comparable performances,our tagging - based models get comparable performances,0.5400459170341492
translation,14,138,results,much better,than,trans - pg + bert baseline,much better than trans - pg + bert baseline,0.6137669682502747
translation,14,138,results,rewrite test set,has,our tagging - based models,rewrite test set has our tagging - based models,0.5598075985908508
translation,14,138,results,results,On,rewrite test set,results On rewrite test set,0.5913194417953491
translation,14,155,results,sentence - level bleu,is,better,sentence - level bleu is better,0.5317421555519104
translation,14,155,results,better,on,in- domain test set,better on in- domain test set,0.5323776006698608
translation,14,155,results,better,on,helping,better on helping,0.523306131362915
translation,14,155,results,robustness,on,other datasets,robustness on other datasets,0.49638715386390686
translation,14,155,results,two types of extra signals,has,sentence - level bleu,two types of extra signals has sentence - level bleu,0.5701825618743896
translation,14,155,results,helping,has,robustness,helping has robustness,0.5859768390655518
translation,14,155,results,results,Comparing,two types of extra signals,results Comparing two types of extra signals,0.6184571981430054
translation,14,173,results,performances,on,both transfer ( robustness examination ) scenarios,performances on both transfer ( robustness examination ) scenarios,0.5459591746330261
translation,14,173,results,our model,reports,consistently higher numbers,our model reports consistently higher numbers,0.7101125717163086
translation,14,173,results,consistently higher numbers,than,all other systems,consistently higher numbers than all other systems,0.6031138896942139
translation,15,224,ablation-analysis,re-ranking,is,not every effective,re-ranking is not every effective,0.6011925935745239
translation,15,71,baselines,open retrieval question answering ( qa ),develop,baseline models,open retrieval question answering ( qa ) develop baseline models,0.5796704888343811
translation,15,71,baselines,baseline models,based on,retriever - reader architecture,baseline models based on retriever - reader architecture,0.6256023049354553
translation,15,71,baselines,baselines,develop,baseline models,baselines develop baseline models,0.6377736926078796
translation,15,141,baselines,"retrieval -augmented generation ( rag ) ( lewis et al. , 2020 )",as,base model,"retrieval -augmented generation ( rag ) ( lewis et al. , 2020 ) as base model",0.5045471787452698
translation,15,193,baselines,document index,based on,token / structure - segmented passages,document index based on token / structure - segmented passages,0.6243751049041748
translation,15,193,baselines,baselines,has,document index,baselines has document index,0.5598586201667786
translation,15,159,experimental-setup,1 v100 gpus,with,half precision ( fp16 ) training,1 v100 gpus with half precision ( fp16 ) training,0.6519667506217957
translation,15,165,experimental-setup,dpr biencoder,using,train and validation set,dpr biencoder using train and validation set,0.6469021439552307
translation,15,165,experimental-setup,experimental setup,fine- tune,dpr biencoder,experimental setup fine- tune dpr biencoder,0.6773408651351929
translation,15,166,experimental-setup,generator,BART - large pre-trained on,cnn dataset,generator BART - large pre-trained on cnn dataset,0.7285734415054321
translation,15,166,experimental-setup,experimental setup,For,generator,experimental setup For generator,0.5701256990432739
translation,15,174,experimental-setup,gradient checkpointing,to support,large batch size,gradient checkpointing to support large batch size,0.6029006242752075
translation,15,174,experimental-setup,large batch size,set as,128,large batch size set as 128,0.6567292213439941
translation,15,174,experimental-setup,experimental setup,use,gradient checkpointing,experimental setup use gradient checkpointing,0.5787009596824646
translation,15,87,experiments,4796 conversations,with,average of 14 turns,4796 conversations with average of 14 turns,0.6748024821281433
translation,15,87,experiments,average of 14 turns,grounded in,488 documents,average of 14 turns grounded in 488 documents,0.664964497089386
translation,15,87,experiments,488 documents,from,four domains,488 documents from four domains,0.643394947052002
translation,15,98,model,dialogue flows,correspond to,multiple documents,dialogue flows correspond to multiple documents,0.6681031584739685
translation,15,98,model,dialogue flows,re-collect,utterances,dialogue flows re-collect utterances,0.692070484161377
translation,15,98,model,utterances,for,certain turns,utterances for certain turns,0.6696136593818665
translation,15,98,model,certain turns,based on,dialogue scenes,certain turns based on dialogue scenes,0.6528666615486145
translation,15,98,model,dialogue scenes,in,given flow,dialogue scenes in given flow,0.5610759854316711
translation,15,98,model,given flow,via,crowdsourcing,given flow via crowdsourcing,0.725894570350647
translation,15,98,model,model,create,dialogue flows,model create dialogue flows,0.6872274279594421
translation,15,210,results,bm25,better than,dpr - nq,bm25 better than dpr - nq,0.7219417095184326
translation,15,210,results,bm25,worse than,dpr - ft,bm25 worse than dpr - ft,0.7064056396484375
translation,15,210,results,results,has,bm25,results has bm25,0.5764147043228149
translation,15,212,results,dprft,shows,significant improvement,dprft shows significant improvement,0.7141129374504089
translation,15,212,results,significant improvement,over,dpr -nq,significant improvement over dpr -nq,0.6844524145126343
translation,15,212,results,results,has,dprft,results has dprft,0.5214787721633911
translation,15,213,results,both dpr -nq and dpr - ft,benefit from,f1,both dpr -nq and dpr - ft benefit from f1,0.6591951847076416
translation,15,213,results,results,has,both dpr -nq and dpr - ft,results has both dpr -nq and dpr - ft,0.5132487416267395
translation,15,218,results,much worse,than,dpr - nq,much worse than dpr - nq,0.6226545572280884
translation,15,218,results,outperforms,has,dpr,outperforms has dpr,0.6095166802406311
translation,15,220,results,retrieval performance gap,between,d token and d struct,retrieval performance gap between d token and d struct,0.6570062637329102
translation,15,220,results,reduced,after training,fine-tuned question encoder,reduced after training fine-tuned question encoder,0.6956362724304199
translation,15,220,results,fine-tuned question encoder,in,rag,fine-tuned question encoder in rag,0.4980587065219879
translation,15,220,results,results,see,retrieval performance gap,results see retrieval performance gap,0.5861795544624329
translation,15,221,results,retrieval performances,for,two tasks,retrieval performances for two tasks,0.5561559200286865
translation,15,221,results,two tasks,seem,comparable,two tasks seem comparable,0.6649342775344849
translation,15,221,results,generation metric scores,for,task ii,generation metric scores for task ii,0.60256028175354
translation,15,221,results,task ii,are,much lower task i,task ii are much lower task i,0.5756629705429077
translation,15,221,results,much lower task i,as,agent responses,much lower task i as agent responses,0.5241552591323853
translation,15,221,results,agent responses,are,free-formed natural language,agent responses are free-formed natural language,0.5349161028862
translation,15,221,results,results,has,retrieval performances,results has retrieval performances,0.5459342002868652
translation,15,225,results,difference,between,reranking,difference between reranking,0.6777982115745544
translation,15,225,results,reranking,with,two different kinds of encodings,reranking with two different kinds of encodings,0.5965930819511414
translation,15,225,results,two different kinds of encodings,is,insignificant,two different kinds of encodings is insignificant,0.587359607219696
translation,15,225,results,results,has,difference,results has difference,0.5636705756187439
translation,16,10,ablation-analysis,responses,from,humans and dialogpt,responses from humans and dialogpt,0.6412407755851746
translation,16,10,ablation-analysis,humans and dialogpt,contain,more ad hominems,humans and dialogpt contain more ad hominems,0.5952906012535095
translation,16,10,ablation-analysis,more ad hominems,for,discussions,more ad hominems for discussions,0.6783657670021057
translation,16,10,ablation-analysis,discussions,around,marginalized communities,discussions around marginalized communities,0.6603078842163086
translation,16,10,ablation-analysis,different quantities of ad hominems,in,training data,different quantities of ad hominems in training data,0.5173290967941284
translation,16,10,ablation-analysis,constrained decoding techniques,to reduce,ad hominems,constrained decoding techniques to reduce ad hominems,0.6611629128456116
translation,16,10,ablation-analysis,ad hominems,in,generated dialogue responses,ad hominems in generated dialogue responses,0.5578503012657166
translation,16,10,ablation-analysis,ablation analysis,indicate,responses,ablation analysis indicate responses,0.6164200305938721
translation,16,10,ablation-analysis,ablation analysis,use,constrained decoding techniques,ablation analysis use constrained decoding techniques,0.6748286485671997
translation,16,198,ablation-analysis,saliensimtop -k,manages to maintain,decent balance,saliensimtop -k manages to maintain decent balance,0.7370110750198364
translation,16,198,ablation-analysis,decent balance,of generating,coherent and relevant responses,decent balance of generating coherent and relevant responses,0.6896097660064697
translation,16,198,ablation-analysis,ablation analysis,has,saliensimtop -k,ablation analysis has saliensimtop -k,0.6003305315971375
translation,16,44,model,constrained decoding technique,uses,salient n-gram similarity,constrained decoding technique uses salient n-gram similarity,0.5733294486999512
translation,16,44,model,salient n-gram similarity,to steer,top -k sampling,salient n-gram similarity to steer top -k sampling,0.6871975660324097
translation,16,44,model,top -k sampling,away from,ad hominem responses,top -k sampling away from ad hominem responses,0.7487736344337463
translation,16,44,model,model,devise,constrained decoding technique,model devise constrained decoding technique,0.7643408179283142
translation,16,174,results,average wawa scores,include,precision,average wawa scores include precision,0.6083996891975403
translation,16,174,results,average wawa scores,include,recall,average wawa scores include recall,0.6368418335914612
translation,16,174,results,average wawa scores,include,f 1,average wawa scores include f 1,0.5857475996017456
translation,16,174,results,precision,of,0.82,precision of 0.82,0.5481820702552795
translation,16,174,results,recall,of,0.92,recall of 0.92,0.5800190567970276
translation,16,174,results,f 1,of,0.87,f 1 of 0.87,0.5826308131217957
translation,16,174,results,f 1,indicating,moderately high majority agreement,f 1 indicating moderately high majority agreement,0.7195467352867126
translation,16,174,results,0.87,indicating,moderately high majority agreement,0.87 indicating moderately high majority agreement,0.5992918610572815
translation,16,174,results,all rounds of annotations,has,average wawa scores,all rounds of annotations has average wawa scores,0.6059940457344055
translation,16,174,results,results,Across,all rounds of annotations,results Across all rounds of annotations,0.6967959403991699
translation,16,177,results,fine-tuning,on,datasets,fine-tuning on datasets,0.5196076035499573
translation,16,177,results,datasets,that contain,more ad hominem responses,datasets that contain more ad hominem responses,0.6688570380210876
translation,16,177,results,more ad hominem responses,leads to,more generation,more ad hominem responses leads to more generation,0.6705344915390015
translation,16,177,results,more generation,of,ad hominem responses,more generation of ad hominem responses,0.6424168348312378
translation,16,177,results,ad hominem responses,across,topics,ad hominem responses across topics,0.7061551213264465
translation,16,177,results,results,shows,fine-tuning,results shows fine-tuning,0.6444640159606934
translation,16,186,results,pplm,generates,responses,pplm generates responses,0.6630252003669739
translation,16,186,results,relatively lower,in both,coherence and relevance,relatively lower in both coherence and relevance,0.5950580835342407
translation,16,186,results,results,has,pplm,results has pplm,0.5944159030914307
translation,16,199,results,saliensimtop -k,with,finetuning,saliensimtop -k with finetuning,0.6874146461486816
translation,16,199,results,finetuning,on,wfh data,finetuning on wfh data,0.5430881977081299
translation,16,199,results,finetuning,results in,responses,finetuning results in responses,0.6843599677085876
translation,16,199,results,responses,slightly less coherent and mixed in,relevance,responses slightly less coherent and mixed in relevance,0.6496232151985168
translation,16,199,results,relevance,for,different topics,relevance for different topics,0.5730265378952026
translation,16,199,results,results,Combining,saliensimtop -k,results Combining saliensimtop -k,0.6830265522003174
translation,17,19,baselines,tslf,divide,parameters,tslf divide parameters,0.6589801907539368
translation,17,19,baselines,parameters,of,model,parameters of model,0.6297455430030823
translation,17,19,baselines,parameters,into,dialogue -related and knowledge integration -related,parameters into dialogue -related and knowledge integration -related,0.5767005681991577
translation,17,19,baselines,baselines,has,tslf,baselines has tslf,0.5874018669128418
translation,17,87,baselines,transformer - based architecture,incrementally represents,multi-turn dialogues and knowledge,transformer - based architecture incrementally represents multi-turn dialogues and knowledge,0.7555712461471558
translation,17,87,baselines,response decoding,in,two passes,response decoding in two passes,0.5613691210746765
translation,17,87,baselines,simple bart - based model,take,concatenation,simple bart - based model take concatenation,0.6465779542922974
translation,17,87,baselines,concatenation,of,dialogue context and all knowledge,concatenation of dialogue context and all knowledge,0.5793907046318054
translation,17,87,baselines,dialogue context and all knowledge,input of,bart,dialogue context and all knowledge input of bart,0.7126199007034302
translation,17,87,baselines,bart,for,response generation,bart for response generation,0.6544506549835205
translation,17,87,baselines,itdd,has,transformer - based architecture,itdd has transformer - based architecture,0.5935176610946655
translation,17,87,baselines,bart cat,has,simple bart - based model,bart cat has simple bart - based model,0.6068323254585266
translation,17,88,baselines,bart,sets,constraint,bart sets constraint,0.752877950668335
translation,17,88,baselines,bart,directly truncate,text,bart directly truncate text,0.6801422238349915
translation,17,88,baselines,constraint,on,maximum number of tokens,constraint on maximum number of tokens,0.5188974142074585
translation,17,88,baselines,text,that exceeds,length limit,text that exceeds length limit,0.639915943145752
translation,17,88,baselines,bart skt,is,variational model,bart skt is variational model,0.5797508955001831
translation,17,88,baselines,skt,is,variational model,skt is variational model,0.545740008354187
translation,17,88,baselines,bart skt,has,skt,bart skt has skt,0.6563581228256226
translation,17,88,baselines,baselines,has,bart,baselines has bart,0.5961952805519104
translation,17,92,baselines,disentangled response decoder,with,copy mechanism,disentangled response decoder with copy mechanism,0.6233092546463013
translation,17,93,baselines,zrkgc,based on,unilm,zrkgc based on unilm,0.7148816585540771
translation,17,93,baselines,zrkgc,has,double latent variable model,zrkgc has double latent variable model,0.5620567798614502
translation,17,108,experiments,kat - tslf,has,outperforms,kat - tslf has outperforms,0.6548933386802673
translation,17,108,experiments,outperforms,has,drd,outperforms has drd,0.623480498790741
translation,17,98,hyperparameters,base version,of,bart,base version of bart,0.6732295155525208
translation,17,98,hyperparameters,bart,with,139m parameters,bart with 139m parameters,0.6996211409568787
translation,17,98,hyperparameters,number of parameters,of,kat,number of parameters of kat,0.6406198143959045
translation,17,98,hyperparameters,kat,is,196m,kat is 196m,0.6937519907951355
translation,17,98,hyperparameters,hyperparameters,use,base version,hyperparameters use base version,0.6598714590072632
translation,17,99,hyperparameters,batch size,in,"stage i , ii and iii","batch size in stage i , ii and iii",0.5290917158126831
translation,17,99,hyperparameters,batch size,is,"2048 , 128 and 16","batch size is 2048 , 128 and 16",0.5755533576011658
translation,17,99,hyperparameters,"stage i , ii and iii",is,"2048 , 128 and 16","stage i , ii and iii is 2048 , 128 and 16",0.5623889565467834
translation,17,99,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,17,100,hyperparameters,max sequence length,in,source and target,max sequence length in source and target,0.5369626879692078
translation,17,100,hyperparameters,source and target,is,256 and 64,source and target is 256 and 64,0.608244776725769
translation,17,100,hyperparameters,hyperparameters,has,max sequence length,hyperparameters has max sequence length,0.4825800061225891
translation,17,101,hyperparameters,"adamw ( loshchilov and hutter , 2017 )",with,learning rate,"adamw ( loshchilov and hutter , 2017 ) with learning rate",0.5797719955444336
translation,17,101,hyperparameters,learning rate,in,3 epochs,learning rate in 3 epochs,0.523636519908905
translation,17,101,hyperparameters,5e ? 5,in,3 epochs,5e ? 5 in 3 epochs,0.6080002784729004
translation,17,101,hyperparameters,learning rate,has,5e ? 5,learning rate has 5e ? 5,0.6281245350837708
translation,17,102,hyperparameters,beam search,in,response decoding,beam search in response decoding,0.5229020118713379
translation,17,102,hyperparameters,hyperparameters,employ,beam search,hyperparameters employ beam search,0.5714670419692993
translation,17,7,model,novel threestage learning framework,based on,weakly supervised learning,novel threestage learning framework based on weakly supervised learning,0.5933464765548706
translation,17,7,model,weakly supervised learning,benefits from,large scale ungrounded dialogues,weakly supervised learning benefits from large scale ungrounded dialogues,0.5820501446723938
translation,17,7,model,weakly supervised learning,benefits from,unstructured knowledge base,weakly supervised learning benefits from unstructured knowledge base,0.6339618563652039
translation,17,7,model,model,propose,novel threestage learning framework,model propose novel threestage learning framework,0.6741106510162354
translation,17,8,model,variant of transformer,with,decoupled decoder,variant of transformer with decoupled decoder,0.6837935447692871
translation,17,8,model,decoupled decoder,facilitates,disentangled learning,decoupled decoder facilitates disentangled learning,0.6184787750244141
translation,17,8,model,disentangled learning,of,response generation and knowledge incorporation,disentangled learning of response generation and knowledge incorporation,0.5490812063217163
translation,17,8,model,model,devise,variant of transformer,model devise variant of transformer,0.7546404004096985
translation,17,24,model,"vari - ant of vanilla transformer ( vaswani et al. , 2017 )",whose,parameters,"vari - ant of vanilla transformer ( vaswani et al. , 2017 ) whose parameters",0.6439461708068848
translation,17,24,model,parameters,are,decoupled,parameters are decoupled,0.6265212297439575
translation,17,24,model,decoupled,facilitates,separate learning,decoupled facilitates separate learning,0.6755526065826416
translation,17,24,model,separate learning,of,dialogue generation,separate learning of dialogue generation,0.5662189722061157
translation,17,24,model,separate learning,of,knowledge incorporation,separate learning of knowledge incorporation,0.585863471031189
translation,17,24,model,knowledge - aware transformer ( kat ),has,"vari - ant of vanilla transformer ( vaswani et al. , 2017 )","knowledge - aware transformer ( kat ) has vari - ant of vanilla transformer ( vaswani et al. , 2017 )",0.5724400281906128
translation,17,24,model,model,devise,knowledge - aware transformer ( kat ),model devise knowledge - aware transformer ( kat ),0.7365936040878296
translation,17,104,results,kat,achieves,state - of - the - art performance,kat achieves state - of - the - art performance,0.6841275691986084
translation,17,104,results,state - of - the - art performance,without using,additional corpora,state - of - the - art performance without using additional corpora,0.7056179046630859
translation,17,104,results,full - data scenario,has,kat,full - data scenario has kat,0.5770390033721924
translation,17,104,results,results,In,full - data scenario,results In full - data scenario,0.5454381704330444
translation,17,105,results,additional resources,are,unnecessary,additional resources are unnecessary,0.5675100088119507
translation,17,105,results,additional resources,when,enriched training datas,additional resources when enriched training datas,0.6782299280166626
translation,17,105,results,unnecessary,when,enriched training datas,unnecessary when enriched training datas,0.726072371006012
translation,17,105,results,kat - tslf,achieves,comparable performance,kat - tslf achieves comparable performance,0.6944165229797363
translation,17,105,results,comparable performance,with,bart cat / skt,comparable performance with bart cat / skt,0.6731171011924744
translation,17,105,results,tslf,has,little effect,tslf has little effect,0.6227323412895203
translation,17,105,results,results,has,additional resources,results has additional resources,0.4998587667942047
translation,17,109,results,performance,of,kat - tslf,performance of kat - tslf,0.6080430746078491
translation,17,109,results,kat - tslf,surpasses,zrkgc,kat - tslf surpasses zrkgc,0.6881228685379028
translation,17,109,results,zrkgc,in,most evaluation metrics,zrkgc in most evaluation metrics,0.48599398136138916
translation,17,109,results,responses,generated by,kat,responses generated by kat,0.6750409007072449
translation,17,109,results,kat,have,higher dist -n,kat have higher dist -n,0.645709216594696
translation,17,109,results,zero resources,has,performance,zero resources has performance,0.6160901188850403
translation,17,111,results,responses,from,our kat - tslf,responses from our kat - tslf,0.6264432072639465
translation,17,111,results,responses,are,more fluent and more contextually coherent,responses are more fluent and more contextually coherent,0.5565361976623535
translation,17,111,results,our kat - tslf,are,more fluent and more contextually coherent,our kat - tslf are more fluent and more contextually coherent,0.586925745010376
translation,17,111,results,more fluent and more contextually coherent,than,bart skt and zrkgc,more fluent and more contextually coherent than bart skt and zrkgc,0.615582287311554
translation,17,111,results,results,observe,responses,results observe responses,0.5295823812484741
translation,17,112,results,stronger knowledge relevance,in the case of,full data,stronger knowledge relevance in the case of full data,0.6321997046470642
translation,17,112,results,stronger knowledge relevance,thanks to,well - designed knowledge selection module,stronger knowledge relevance thanks to well - designed knowledge selection module,0.4769189655780792
translation,17,112,results,our low-resource model,has,skt,our low-resource model has skt,0.6363392472267151
translation,17,112,results,skt,has,stronger knowledge relevance,skt has stronger knowledge relevance,0.56163090467453
translation,17,112,results,results,Compared with,our low-resource model,results Compared with our low-resource model,0.6593718528747559
translation,18,157,ablation-analysis,colv,introduces,competitive boost,colv introduces competitive boost,0.6764291524887085
translation,18,157,ablation-analysis,competitive boost,in,response quality,competitive boost in response quality,0.5222164988517761
translation,18,157,ablation-analysis,competitive boost,in line with,automatic evaluation,competitive boost in line with automatic evaluation,0.6230119466781616
translation,18,157,ablation-analysis,competitive boost,confirming,superior performance,competitive boost confirming superior performance,0.679828405380249
translation,18,157,ablation-analysis,collaborative latent variables,has,colv,collaborative latent variables has colv,0.6068967580795288
translation,18,157,ablation-analysis,ablation analysis,Augmented with,collaborative latent variables,ablation analysis Augmented with collaborative latent variables,0.725287675857544
translation,18,164,ablation-analysis,ablation analysis,on,wow and holl -e,ablation analysis on wow and holl -e,0.6062102317810059
translation,18,165,ablation-analysis,performance,of,knowledge selection,performance of knowledge selection,0.5821611285209656
translation,18,165,ablation-analysis,drops largely,with respect to,accuracy metric,drops largely with respect to accuracy metric,0.6684975028038025
translation,18,165,ablation-analysis,knowledge latent and heuristic matching,has,performance,knowledge latent and heuristic matching has performance,0.5888440608978271
translation,18,165,ablation-analysis,knowledge selection,has,drops largely,knowledge selection has drops largely,0.6120608448982239
translation,18,165,ablation-analysis,ablation analysis,observe,knowledge latent and heuristic matching,ablation analysis observe knowledge latent and heuristic matching,0.5980180501937866
translation,18,165,ablation-analysis,ablation analysis,without either,knowledge latent and heuristic matching,ablation analysis without either knowledge latent and heuristic matching,0.7141563892364502
translation,18,167,ablation-analysis,values,of,generative metrics,values of generative metrics,0.5324254035949707
translation,18,167,ablation-analysis,generative metrics,e.g.,ppl,generative metrics e.g. ppl,0.669396162033081
translation,18,167,ablation-analysis,generative metrics,e.g.,"bleu -4 , rouge - 1/2 and dist - 2","generative metrics e.g. bleu -4 , rouge - 1/2 and dist - 2",0.6342321038246155
translation,18,167,ablation-analysis,significantly,remove,response latent variables,significantly remove response latent variables,0.6058985590934753
translation,18,167,ablation-analysis,generative metrics,has,drop,generative metrics has drop,0.5630893707275391
translation,18,167,ablation-analysis,drop,has,significantly,drop has significantly,0.6707428097724915
translation,18,167,ablation-analysis,ablation analysis,has,values,ablation analysis has values,0.569972813129425
translation,18,168,ablation-analysis,collaborative latent variables,helpful to refine,coherence,collaborative latent variables helpful to refine coherence,0.7021174430847168
translation,18,168,ablation-analysis,collaborative latent variables,helpful to refine,knowledge engagement,collaborative latent variables helpful to refine knowledge engagement,0.6776989698410034
translation,18,168,ablation-analysis,collaborative latent variables,helpful to refine,diversity,collaborative latent variables helpful to refine diversity,0.698927640914917
translation,18,168,ablation-analysis,diversity,of,generated responses,diversity of generated responses,0.5864410996437073
translation,18,168,ablation-analysis,ablation analysis,affirms,collaborative latent variables,ablation analysis affirms collaborative latent variables,0.6989753842353821
translation,18,169,ablation-analysis,similar performance,of,our model,similar performance of our model,0.5909931063652039
translation,18,169,ablation-analysis,our model,with,baseline model memnet,our model with baseline model memnet,0.5841012597084045
translation,18,169,ablation-analysis,vanilla transformer model,with,knowledge memory network,vanilla transformer model with knowledge memory network,0.6181842684745789
translation,18,169,ablation-analysis,baseline model memnet,has,vanilla transformer model,baseline model memnet has vanilla transformer model,0.522022545337677
translation,18,124,baselines,bidirectional lstm - based encoderdecoder framework,with,attention mechanism,bidirectional lstm - based encoderdecoder framework with attention mechanism,0.5927135348320007
translation,18,124,baselines,s2sa,has,bidirectional lstm - based encoderdecoder framework,s2sa has bidirectional lstm - based encoderdecoder framework,0.5171331167221069
translation,18,126,baselines,encoder-decoder architecture,relying solely on,multi-head self-attention mechanisms,encoder-decoder architecture relying solely on multi-head self-attention mechanisms,0.7081736326217651
translation,18,126,baselines,transformer,has,encoder-decoder architecture,transformer has encoder-decoder architecture,0.5801298022270203
translation,18,129,baselines,lstm - based model,with,posterior knowledge selection mechanism,lstm - based model with posterior knowledge selection mechanism,0.5992632508277893
translation,18,129,baselines,posterior knowledge selection mechanism,uses,posterior knowledge distribution,posterior knowledge selection mechanism uses posterior knowledge distribution,0.5750572085380554
translation,18,129,baselines,posterior knowledge distribution,as,pseudo-label,posterior knowledge distribution as pseudo-label,0.48856690526008606
translation,18,129,baselines,pseudo-label,for,knowledge selection,pseudo-label for knowledge selection,0.5938781499862671
translation,18,129,baselines,postks,has,lstm - based model,postks has lstm - based model,0.5919008255004883
translation,18,129,baselines,baselines,has,postks,baselines has postks,0.5903317928314209
translation,18,130,baselines,sequential latent knowledge selection model,keeps track of,prior and posterior distribution,sequential latent knowledge selection model keeps track of prior and posterior distribution,0.7224188446998596
translation,18,130,baselines,prior and posterior distribution,over,knowledge,prior and posterior distribution over knowledge,0.6850160360336304
translation,18,130,baselines,knowledge,in,sequential process,knowledge in sequential process,0.548603355884552
translation,18,130,baselines,slks,has,sequential latent knowledge selection model,slks has sequential latent knowledge selection model,0.5001675486564636
translation,18,130,baselines,baselines,has,slks,baselines has slks,0.6005513668060303
translation,18,131,baselines,dukenet,modeling,knowledge shift and tracking processes,dukenet modeling knowledge shift and tracking processes,0.7588290572166443
translation,18,131,baselines,knowledge shift and tracking processes,with,dual learning paradigm,knowledge shift and tracking processes with dual learning paradigm,0.6082438826560974
translation,18,131,baselines,dukenet,has,dual knowledge interaction network,dukenet has dual knowledge interaction network,0.5605764389038086
translation,18,131,baselines,baselines,has,dukenet,baselines has dukenet,0.5913091897964478
translation,18,132,baselines,pipm,with,knowledge distillation training strategy,pipm with knowledge distillation training strategy,0.6150761842727661
translation,18,132,baselines,slks model,with,posterior information prediction module,slks model with posterior information prediction module,0.6239412426948547
translation,18,132,baselines,slks model,with,knowledge distillation training strategy,slks model with knowledge distillation training strategy,0.5982911586761475
translation,18,132,baselines,pipm,has,slks model,pipm has slks model,0.5804862380027771
translation,18,132,baselines,baselines,has,pipm,baselines has pipm,0.5851266980171204
translation,18,143,experimental-setup,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,18,143,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,18,144,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,18,144,experimental-setup,maximum gradient norm,of,0.4,maximum gradient norm of 0.4,0.5995920896530151
translation,18,145,experimental-setup,all models,on,tesla p40 gpu,all models on tesla p40 gpu,0.4914952218532562
translation,18,145,experimental-setup,best models,based on,performance,best models based on performance,0.6404714584350586
translation,18,145,experimental-setup,performance,on,validation set,performance on validation set,0.5804513692855835
translation,18,145,experimental-setup,experimental setup,run,all models,experimental setup run all models,0.7095304131507874
translation,18,6,model,diversity,of both,knowledge selection,diversity of both knowledge selection,0.6666228175163269
translation,18,6,model,diversity,of both,knowledge - aware response generation,diversity of both knowledge - aware response generation,0.650979220867157
translation,18,6,model,diversity,propose,collaborative latent variable ( colv ) model,diversity propose collaborative latent variable ( colv ) model,0.632439911365509
translation,18,6,model,collaborative latent variable ( colv ) model,two aspects simultaneously in,separate yet collaborative latent spaces,collaborative latent variable ( colv ) model two aspects simultaneously in separate yet collaborative latent spaces,0.6868312954902649
translation,18,6,model,model,to improve,diversity,model to improve diversity,0.7466177940368652
translation,18,6,model,model,propose,collaborative latent variable ( colv ) model,model propose collaborative latent variable ( colv ) model,0.6351398229598999
translation,18,7,model,proposed model,firstly draws,knowledge candidate,proposed model firstly draws knowledge candidate,0.7126175761222839
translation,18,7,model,proposed model,samples,response,proposed model samples response,0.7024848461151123
translation,18,7,model,knowledge candidate,from,latent space,knowledge candidate from latent space,0.5244057774543762
translation,18,7,model,response,from,another collaborative latent space,response from another collaborative latent space,0.6258493065834045
translation,18,7,model,response,conditioned on,context,response conditioned on context,0.7702096700668335
translation,18,7,model,response,conditioned on,selected knowledge,response conditioned on selected knowledge,0.747080385684967
translation,18,7,model,generation,has,proposed model,generation has proposed model,0.5782413482666016
translation,18,7,model,model,During,generation,model During generation,0.6883525848388672
translation,18,42,model,diversity,of both,knowledge selection,diversity of both knowledge selection,0.6666228175163269
translation,18,42,model,diversity,of both,knowledge - aware response generation,diversity of both knowledge - aware response generation,0.650979220867157
translation,18,42,model,diversity,propose,collaborative latent variable ( colv ) model,diversity propose collaborative latent variable ( colv ) model,0.632439911365509
translation,18,42,model,collaborative latent variable ( colv ) model,to integrate,both aspects,collaborative latent variable ( colv ) model to integrate both aspects,0.7740064263343811
translation,18,42,model,both aspects,in,separate yet collaborative latent spaces,both aspects in separate yet collaborative latent spaces,0.5240315198898315
translation,18,42,model,model,to simultaneously improve,diversity,model to simultaneously improve diversity,0.7156494855880737
translation,18,42,model,model,propose,collaborative latent variable ( colv ) model,model propose collaborative latent variable ( colv ) model,0.6351398229598999
translation,18,43,model,proposed model,firstly draws,knowledge candidate,proposed model firstly draws knowledge candidate,0.7126175761222839
translation,18,43,model,proposed model,samples,response,proposed model samples response,0.7024848461151123
translation,18,43,model,knowledge candidate,from,latent space,knowledge candidate from latent space,0.5244057774543762
translation,18,43,model,response,from,another collaborative latent space,response from another collaborative latent space,0.6258493065834045
translation,18,43,model,response,conditioned on,context,response conditioned on context,0.7702096700668335
translation,18,43,model,response,conditioned on,selected knowledge,response conditioned on selected knowledge,0.747080385684967
translation,18,43,model,generation,has,proposed model,generation has proposed model,0.5782413482666016
translation,18,43,model,model,During,generation,model During generation,0.6883525848388672
translation,18,128,model,transformer memory network,for,knowledge selection,transformer memory network for knowledge selection,0.5983952879905701
translation,18,128,model,transformer memory network,for,transformer decoder,transformer memory network for transformer decoder,0.6344080567359924
translation,18,128,model,transformer memory network,for,utterance prediction,transformer memory network for utterance prediction,0.613986074924469
translation,18,128,model,transformer decoder,for,utterance prediction,transformer decoder for utterance prediction,0.6229646801948547
translation,18,128,model,model,has,transformer memory network,model has transformer memory network,0.5515838265419006
translation,18,148,results,significant improvement,over,baseline models,significant improvement over baseline models,0.6524149775505066
translation,18,148,results,generation performance,has,colv,generation performance has colv,0.5845420360565186
translation,18,148,results,colv,has,significant improvement,colv has significant improvement,0.6008396744728088
translation,18,153,results,colv,has,consistently outperforms,colv has consistently outperforms,0.6207004189491272
translation,18,153,results,consistently outperforms,has,all the compared models,consistently outperforms has all the compared models,0.5963823199272156
translation,18,153,results,results,has,colv,results has colv,0.5554550290107727
translation,18,154,results,colv,exhibits,significant improvements,colv exhibits significant improvements,0.7157189846038818
translation,18,154,results,significant improvements,comparing with,vanilla s2sa and transformer,significant improvements comparing with vanilla s2sa and transformer,0.7687662839889526
translation,18,154,results,results,notice that,colv,results notice that colv,0.6307430863380432
translation,18,155,results,colv,substantially reaches,better performances,colv substantially reaches better performances,0.7012476325035095
translation,18,155,results,better performances,than,strong baselines,better performances than strong baselines,0.5310966372489929
translation,18,155,results,better performances,e.g.,skls,better performances e.g. skls,0.669820249080658
translation,18,155,results,better performances,e.g.,pipm,better performances e.g. pipm,0.6744162440299988
translation,18,155,results,results,has,colv,results has colv,0.5554550290107727
translation,19,40,baselines,dependency based approach ( dba ),create,rule-based generator,dependency based approach ( dba ) create rule-based generator,0.6576781868934631
translation,19,40,baselines,rule-based generator,by handcrafting,dependency templates,rule-based generator by handcrafting dependency templates,0.8034707307815552
translation,19,40,baselines,baselines,has,dependency based approach ( dba ),baselines has dependency based approach ( dba ),0.5626248717308044
translation,19,60,experimental-setup,idkd,into,train and validation split,idkd into train and validation split,0.6203781366348267
translation,19,60,experimental-setup,train and validation split,of,80:20,train and validation split of 80:20,0.610605001449585
translation,19,60,experimental-setup,experimental setup,divide,idkd,experimental setup divide idkd,0.7233496308326721
translation,19,61,experimental-setup,transformers code,from,"hug-gingface ( wolf et al. , 2020 )","transformers code from hug-gingface ( wolf et al. , 2020 )",0.5645511150360107
translation,19,61,experimental-setup,transformers code,to fine- tune,t5 - base model,transformers code to fine- tune t5 - base model,0.699665367603302
translation,19,61,experimental-setup,"hug-gingface ( wolf et al. , 2020 )",to fine- tune,t5 - base model,"hug-gingface ( wolf et al. , 2020 ) to fine- tune t5 - base model",0.7136402726173401
translation,19,61,experimental-setup,t5 - base model,over,idkd,t5 - base model over idkd,0.693127453327179
translation,19,61,experimental-setup,idkd,for,2 epochs,idkd for 2 epochs,0.6624544262886047
translation,19,61,experimental-setup,experimental setup,use,transformers code,experimental setup use transformers code,0.6471864581108093
translation,19,69,results,t5 responses,tend to be,more grammatical,t5 responses tend to be more grammatical,0.715831458568573
translation,19,69,results,more grammatical,than,dependency counterparts,more grammatical than dependency counterparts,0.5994307994842529
translation,19,69,results,more grammatical,by,large margin,more grammatical by large margin,0.5748404860496521
translation,19,69,results,large margin,of,6 %,large margin of 6 %,0.5771484375
translation,19,69,results,results,has,t5 responses,results has t5 responses,0.5249393582344055
translation,19,74,results,sentence length,mostly remains,unaffected,sentence length mostly remains unaffected,0.7007235288619995
translation,19,74,results,unaffected,across,two models,unaffected across two models,0.7094728946685791
translation,19,74,results,results,has,sentence length,results has sentence length,0.5148271322250366
translation,19,75,results,reply,to,54.5 %,reply to 54.5 %,0.5780165195465088
translation,19,75,results,54.5 %,of,random qqp queries,54.5 % of random qqp queries,0.5587459802627563
translation,19,75,results,results,has,rule- based model,results has rule- based model,0.5454078316688538
translation,20,152,experiments,inference speed,conduct,all the experiments,inference speed conduct all the experiments,0.658690869808197
translation,20,152,experiments,all the experiments,with,maximum affordable batch size,all the experiments with maximum affordable batch size,0.6107466220855713
translation,20,152,experiments,maximum affordable batch size,to fully exploit,2 v100 gpus,maximum affordable batch size to fully exploit 2 v100 gpus,0.6075937151908875
translation,20,152,experiments,2 v100 gpus,with,16gb gpu ram each,2 v100 gpus with 16gb gpu ram each,0.6181742548942566
translation,20,175,experiments,snli,helps,intent task,snli helps intent task,0.6330553889274597
translation,20,175,experiments,squad2,mainly helps on,noncat task,squad2 mainly helps on noncat task,0.6285085082054138
translation,20,147,results,cross - encoder,performs,best,cross - encoder performs best,0.6425666213035583
translation,20,147,results,best,over,all subtasks,best over all subtasks,0.6813076734542847
translation,20,147,results,results,has,cross - encoder,results has cross - encoder,0.5404387712478638
translation,20,148,results,our fusion- encoder,with,partial attention,our fusion- encoder with partial attention,0.6416120529174805
translation,20,148,results,partial attention,outperforms,dual- encoder,partial attention outperforms dual- encoder,0.7465270757675171
translation,20,148,results,dual- encoder,by,large margin,dual- encoder by large margin,0.560501754283905
translation,20,148,results,dual- encoder,on,categorical and noncategorical slots predictions,dual- encoder on categorical and noncategorical slots predictions,0.5028231739997864
translation,20,148,results,results,has,our fusion- encoder,results has our fusion- encoder,0.5737553238868713
translation,20,149,results,seen services,found that,dual-encoder and fusion - encoder,seen services found that dual-encoder and fusion - encoder,0.6813699007034302
translation,20,149,results,cross-encoder,on,intent and req tasks,cross-encoder on intent and req tasks,0.5574500560760498
translation,20,149,results,results,on,seen services,results on seen services,0.561490535736084
translation,20,155,results,dual - encoder,achieves,highest inference speed,dual - encoder achieves highest inference speed,0.671750009059906
translation,20,155,results,highest inference speed,of,603.35 examples per gpu second,highest inference speed of 603.35 examples per gpu second,0.5279591083526611
translation,20,155,results,results,has,dual - encoder,results has dual - encoder,0.54952472448349
translation,20,180,results,impact of description styles,has,q3 ),impact of description styles has q3 ),0.582844614982605
translation,20,180,results,results,has,impact of description styles,results has impact of description styles,0.5363060832023621
translation,20,223,results,results,Does,question format help ?,results Does question format help ?,0.34571573138237
translation,20,224,results,extra question format,improve,performance,extra question format improve performance,0.6863282918930054
translation,20,224,results,performance,on,cat and noncat task,performance on cat and noncat task,0.5128839015960693
translation,20,224,results,performance,on,sg - dst and multiwoz 2.2 datasets,performance on sg - dst and multiwoz 2.2 datasets,0.4390402138233185
translation,20,224,results,cat and noncat task,on,sg - dst and multiwoz 2.2 datasets,cat and noncat task on sg - dst and multiwoz 2.2 datasets,0.4341273605823517
translation,20,224,results,cat and noncat task,not for,intent and req tasks,cat and noncat task not for intent and req tasks,0.6532713174819946
translation,20,226,results,simple question pattern,to,nameonly,simple question pattern to nameonly,0.5529720187187195
translation,20,226,results,results,adding,simple question pattern,results adding simple question pattern,0.7043554186820984
translation,20,230,results,q-name,from,pretrained model,q-name from pretrained model,0.5107947587966919
translation,20,230,results,q- orig,more similar to,natural questions,q- orig more similar to natural questions,0.6879987716674805
translation,20,230,results,q- orig,gains more than,q-name,q- orig gains more than q-name,0.7033177614212036
translation,20,230,results,q- orig,from,pretrained model,q- orig from pretrained model,0.5775174498558044
translation,20,230,results,natural questions,in,squad2,natural questions in squad2,0.5623388290405273
translation,20,230,results,natural questions,in,squad2,natural questions in squad2,0.5623388290405273
translation,20,230,results,q-orig,gains more than,q-name,q-orig gains more than q-name,0.7033177614212036
translation,20,230,results,q-name,from,pretrained model,q-name from pretrained model,0.5107947587966919
translation,20,230,results,pretrained model,on,squad2,pretrained model on squad2,0.532464861869812
translation,20,230,results,q-name,has,q- orig,q-name has q- orig,0.6203116178512573
translation,20,230,results,results,comparing to,q-name,results comparing to q-name,0.6790627241134644
translation,20,239,results,orig description,performs,more robust,orig description performs more robust,0.6097407937049866
translation,20,239,results,more robust,especially on,noncat task,more robust especially on noncat task,0.6533584594726562
translation,20,239,results,other tasks,has,orig description,other tasks has orig description,0.5521588921546936
translation,20,239,results,results,on,other tasks,results on other tasks,0.5096456408500671
translation,20,243,results,orig,performs,more robust,orig performs more robust,0.6532672643661499
translation,20,243,results,more robust,than,nameonly,more robust than nameonly,0.6443731784820557
translation,20,243,results,more robust,when,schema descriptions,more robust when schema descriptions,0.6318143606185913
translation,20,243,results,nameonly,when,schema descriptions,nameonly when schema descriptions,0.6360305547714233
translation,20,243,results,schema descriptions,paraphrased on,unseen services,schema descriptions paraphrased on unseen services,0.6175014972686768
translation,20,243,results,results,has,orig,results has orig,0.4230024218559265
translation,21,6,model,unlabeled conversations,combine,coda,unlabeled conversations combine coda,0.734089195728302
translation,21,6,model,unlabeled conversations,with,pseudo summaries,unlabeled conversations with pseudo summaries,0.6110904216766357
translation,21,6,model,coda,with,two -stage noisy selftraining,coda with two -stage noisy selftraining,0.6594259142875671
translation,21,6,model,summarization model,on,unlabeled conversations,summarization model on unlabeled conversations,0.481389582157135
translation,21,6,model,summarization model,on,labeled conversations,summarization model on labeled conversations,0.49946901202201843
translation,21,6,model,unlabeled conversations,with,pseudo summaries,unlabeled conversations with pseudo summaries,0.6110904216766357
translation,21,6,model,model,To further utilize,unlabeled conversations,model To further utilize unlabeled conversations,0.6639145016670227
translation,21,15,model,conversational data augmentation ( coda ),for,conversation summarization,conversational data augmentation ( coda ) for conversation summarization,0.5866430401802063
translation,21,15,model,conversation summarization,guided by,conversation structures and context,conversation summarization guided by conversation structures and context,0.6229646801948547
translation,21,15,model,randomly swap or delete utterances,in,conversations,randomly swap or delete utterances in conversations,0.5671581625938416
translation,21,15,model,conversations,to perturb,discourse relations,conversations to perturb discourse relations,0.6510878801345825
translation,21,15,model,randomly insert utterances,based on,dialogue acts,randomly insert utterances based on dialogue acts,0.6397028565406799
translation,21,15,model,dialogue acts,like,self-talk,dialogue acts like self-talk,0.636215090751648
translation,21,15,model,conditional - generation - based substitution,randomly substitute,utterances,conditional - generation - based substitution randomly substitute utterances,0.6689193248748779
translation,21,15,model,utterances,in,conversations,utterances in conversations,0.5681173205375671
translation,21,15,model,random swapping / deletion,has,randomly swap or delete utterances,random swapping / deletion has randomly swap or delete utterances,0.6303767561912537
translation,21,15,model,dialogue - acts - guided insertion,has,randomly insert utterances,dialogue - acts - guided insertion has randomly insert utterances,0.6158682703971863
translation,21,17,model,performance,when,labeled summaries,performance when labeled summaries,0.6733083724975586
translation,21,17,model,performance,extend,coda,performance extend coda,0.7164583802223206
translation,21,17,model,labeled summaries,extend,coda,labeled summaries extend coda,0.758137047290802
translation,21,17,model,coda,to,semisupervised settings,coda to semisupervised settings,0.5317879915237427
translation,21,17,model,coda,combine,coda,coda combine coda,0.7372279167175293
translation,21,17,model,coda,with,two -stage noisy self-training,coda with two -stage noisy self-training,0.6565250754356384
translation,21,17,model,semi-coda,combine,coda,semi-coda combine coda,0.7598025798797607
translation,21,17,model,coda,with,two -stage noisy self-training,coda with two -stage noisy self-training,0.6565250754356384
translation,21,17,model,two -stage noisy self-training,to utilize,conversations,two -stage noisy self-training to utilize conversations,0.6516345143318176
translation,21,17,model,conversations,has,without annotated summaries,conversations has without annotated summaries,0.5874492526054382
translation,21,17,model,model,To further enhance,performance,model To further enhance performance,0.7176869511604309
translation,21,17,model,model,extend,coda,model extend coda,0.7550089359283447
translation,21,17,model,model,combine,coda,model combine coda,0.7485147714614868
translation,22,6,model,controllable and coherent dialogue,devise,conceptguided non-autoregressive model ( cg - nar ),controllable and coherent dialogue devise conceptguided non-autoregressive model ( cg - nar ),0.7285869121551514
translation,22,6,model,conceptguided non-autoregressive model ( cg - nar ),for,open-domain dialogue generation,conceptguided non-autoregressive model ( cg - nar ) for open-domain dialogue generation,0.5842926502227783
translation,22,6,model,model,To facilitate,controllable and coherent dialogue,model To facilitate controllable and coherent dialogue,0.701939046382904
translation,22,6,model,model,devise,conceptguided non-autoregressive model ( cg - nar ),model devise conceptguided non-autoregressive model ( cg - nar ),0.7738667130470276
translation,22,7,model,multi-concept planning module,learns to identify,multiple associated concepts,multi-concept planning module learns to identify multiple associated concepts,0.765857994556427
translation,22,7,model,multiple associated concepts,from,concept graph,multiple associated concepts from concept graph,0.5522586703300476
translation,22,7,model,multiple associated concepts,from,customized insertion transformer,multiple associated concepts from customized insertion transformer,0.6044440865516663
translation,22,7,model,customized insertion transformer,performs,concept-guided non-autoregressive generation,customized insertion transformer performs concept-guided non-autoregressive generation,0.57997727394104
translation,22,7,model,concept-guided non-autoregressive generation,to complete,response,concept-guided non-autoregressive generation to complete response,0.6810928583145142
translation,22,7,model,model,comprises,multi-concept planning module,model comprises multi-concept planning module,0.6454984545707703
translation,22,21,model,concept-guided nonautoregressive model ( cg - nar ),to facilitate,dialogue coherence,concept-guided nonautoregressive model ( cg - nar ) to facilitate dialogue coherence,0.7261393666267395
translation,22,21,model,dialogue coherence,by explicitly introducing,multiple concepts,dialogue coherence by explicitly introducing multiple concepts,0.6368311047554016
translation,22,21,model,multiple concepts,into,dialogue responses,multiple concepts into dialogue responses,0.6084170937538147
translation,22,21,model,model,devise,concept-guided nonautoregressive model ( cg - nar ),model devise concept-guided nonautoregressive model ( cg - nar ),0.7517741322517395
translation,22,23,model,novel multiconcept planning module,learns to manage,concept transitions,novel multiconcept planning module learns to manage concept transitions,0.7522820234298706
translation,22,23,model,concept transitions,in,dialogue flow,concept transitions in dialogue flow,0.5541634559631348
translation,22,26,model,remaining words,of,response,remaining words of response,0.5812162160873413
translation,22,26,model,response,generated in,parallel,response generated in parallel,0.7125717997550964
translation,22,26,model,model,has,remaining words,model has remaining words,0.5636871457099915
translation,23,84,ablation-analysis,span prediction objectives,are,more effective,span prediction objectives are more effective,0.588837742805481
translation,23,84,ablation-analysis,more effective,for,dst,more effective for dst,0.6370610594749451
translation,23,85,ablation-analysis,pre-training,has,model quality,pre-training has model quality,0.5359020829200745
translation,23,85,ablation-analysis,model quality,has,drops,model quality has drops,0.5741984248161316
translation,23,85,ablation-analysis,drops,has,miserably,drops has miserably,0.6057277321815491
translation,23,85,ablation-analysis,ablation analysis,Without,pre-training,ablation analysis Without pre-training,0.7325960993766785
translation,23,19,baselines,seq2seq performance,compare,two versions,seq2seq performance compare two versions,0.6679506301879883
translation,23,19,baselines,full conversation history,as,context,full conversation history as context,0.5357139110565186
translation,23,19,baselines,previously predicted states,as,summary of context,previously predicted states as summary of context,0.49914538860321045
translation,23,19,baselines,recurrently,as,summary of context,recurrently as summary of context,0.567609429359436
translation,23,19,baselines,previously predicted states,has,recurrently,previously predicted states has recurrently,0.6054426431655884
translation,23,36,baselines,baselines,has,recurrent-state model,baselines has recurrent-state model,0.5433100461959839
translation,23,74,baselines,trippy,has,"heck et al. , 2020 )","trippy has heck et al. , 2020 )",0.5851168632507324
translation,23,60,experimental-setup,"open-source framework lingvo ( shen et al. , 2019 )",has,each encoder and decoder,"open-source framework lingvo ( shen et al. , 2019 ) has each encoder and decoder",0.5272168517112732
translation,23,60,experimental-setup,each encoder and decoder,has,12 transformer layers,each encoder and decoder has 12 transformer layers,0.5583216547966003
translation,23,60,experimental-setup,embedding dimension,has,768,embedding dimension has 768,0.622395396232605
translation,23,60,experimental-setup,experimental setup,built with,"open-source framework lingvo ( shen et al. , 2019 )","experimental setup built with open-source framework lingvo ( shen et al. , 2019 )",0.6511850357055664
translation,23,61,experimental-setup,experimental setup,trained with,16 tpuv3 chips,experimental setup trained with 16 tpuv3 chips,0.6901889443397522
translation,23,62,experimental-setup,"memory -efficient adafactor ( shazeer and stern , 2018 )",as,optimizer,"memory -efficient adafactor ( shazeer and stern , 2018 ) as optimizer",0.48472750186920166
translation,23,62,experimental-setup,"memory -efficient adafactor ( shazeer and stern , 2018 )",with,learning rate,"memory -efficient adafactor ( shazeer and stern , 2018 ) with learning rate",0.5873807668685913
translation,23,62,experimental-setup,"memory -efficient adafactor ( shazeer and stern , 2018 )",with,inverse squared root decay schedule,"memory -efficient adafactor ( shazeer and stern , 2018 ) with inverse squared root decay schedule",0.6027544140815735
translation,23,62,experimental-setup,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,23,62,experimental-setup,optimizer,with,inverse squared root decay schedule,optimizer with inverse squared root decay schedule,0.6236938238143921
translation,23,62,experimental-setup,learning rate,has,0.01,learning rate has 0.01,0.5422797799110413
translation,23,62,experimental-setup,experimental setup,use,"memory -efficient adafactor ( shazeer and stern , 2018 )","experimental setup use memory -efficient adafactor ( shazeer and stern , 2018 )",0.6079676747322083
translation,23,63,experimental-setup,default sentencepiece model,provided by,t5,default sentencepiece model provided by t5,0.6473006010055542
translation,23,63,experimental-setup,t5,with,vocabulary size,t5 with vocabulary size,0.6784740090370178
translation,23,63,experimental-setup,vocabulary size,has,32 k,vocabulary size has 32 k,0.6021468639373779
translation,23,63,experimental-setup,experimental setup,use,default sentencepiece model,experimental setup use default sentencepiece model,0.5683272480964661
translation,23,65,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,23,65,experimental-setup,beam search,with,size 5,beam search with size 5,0.7335683107376099
translation,23,65,experimental-setup,experimental setup,For,decoding,experimental setup For decoding,0.5741471648216248
translation,23,66,experimental-setup,label smoothing,with,uncertainty 0.1,label smoothing with uncertainty 0.1,0.5892043709754944
translation,23,66,experimental-setup,uncertainty 0.1,during,training,uncertainty 0.1 during training,0.7023636698722839
translation,23,66,experimental-setup,experimental setup,enabled,label smoothing,experimental setup enabled label smoothing,0.6176489591598511
translation,23,18,experiments,"pegasus ( zhang et al. , 2020 b )",task of,dst,"pegasus ( zhang et al. , 2020 b ) task of dst",0.6789854168891907
translation,23,18,experiments,pre-training procedure,designed for,text summarization,pre-training procedure designed for text summarization,0.6704152226448059
translation,23,18,experiments,pre-training procedure,task of,dst,pre-training procedure task of dst,0.7209135293960571
translation,23,18,experiments,"pegasus ( zhang et al. , 2020 b )",has,pre-training procedure,"pegasus ( zhang et al. , 2020 b ) has pre-training procedure",0.5353542566299438
translation,23,55,model,self-supervised objective,named,gap sentence generation ( gsg ),self-supervised objective named gap sentence generation ( gsg ),0.6436899304389954
translation,23,55,model,self-supervised objective,identifies,potentially important sentences,self-supervised objective identifies potentially important sentences,0.6432551741600037
translation,23,55,model,potentially important sentences,in,paragraph,potentially important sentences in paragraph,0.4974161386489868
translation,23,55,model,potentially important sentences,according to,heuristics,potentially important sentences according to heuristics,0.6897619366645813
translation,23,55,model,decoder,to predict,pivoting sentences,decoder to predict pivoting sentences,0.718265175819397
translation,23,6,results,pre-training objective,makes,significant difference,pre-training objective makes significant difference,0.5822828412055969
translation,23,6,results,significant difference,to,state tracking quality,significant difference to state tracking quality,0.5791285634040833
translation,23,6,results,results,choice of,pre-training objective,results choice of pre-training objective,0.6187490820884705
translation,23,24,results,pre-training procedures,involving,masked span prediction,pre-training procedures involving masked span prediction,0.6127794981002808
translation,23,24,results,masked span prediction,work,consistently better,masked span prediction work consistently better,0.5919143557548523
translation,23,24,results,consistently better,than,auto-regressive language modeling objectives,consistently better than auto-regressive language modeling objectives,0.5233238339424133
translation,23,24,results,results,has,pre-training procedures,results has pre-training procedures,0.4970493018627167
translation,23,25,results,text summarization,works,surprisingly well,text summarization works surprisingly well,0.5665493011474609
translation,23,25,results,surprisingly well,for,dst,surprisingly well for dst,0.6465885043144226
translation,23,80,results,pre-training procedures,with,masked span prediction,pre-training procedures with masked span prediction,0.5973803997039795
translation,23,80,results,masked span prediction,performed,better,masked span prediction performed better,0.26491081714630127
translation,23,80,results,better,than using,  arlm   alone,better than using   arlm   alone,0.7164336442947388
translation,23,80,results,results,has,pre-training procedures,results has pre-training procedures,0.4970493018627167
translation,23,82,results,pegasus pre-training,works,almost equally well or better,pegasus pre-training works almost equally well or better,0.5969569683074951
translation,23,82,results,almost equally well or better,than,t5 pretraining,almost equally well or better than t5 pretraining,0.6196721196174622
translation,23,82,results,results,has,pegasus pre-training,results has pegasus pre-training,0.5124392509460449
translation,23,90,results,recurrent-state models,achieved,reasonably good jga,recurrent-state models achieved reasonably good jga,0.7020974159240723
translation,23,90,results,reasonably good jga,on,all data sets,reasonably good jga on all data sets,0.5234941840171814
translation,23,90,results,worse,than,full-history model,worse than full-history model,0.5628754496574402
translation,23,90,results,results,show,recurrent-state models,results show recurrent-state models,0.5878068804740906
translation,24,84,ablation-analysis,set of questions,lowers,performance,set of questions lowers performance,0.7286931872367859
translation,24,84,ablation-analysis,randomly re-ranking,has,set of questions,randomly re-ranking has set of questions,0.5844209790229797
translation,24,6,model,beam search re-ranking algorithm,guides,effective goal-oriented strategy,beam search re-ranking algorithm guides effective goal-oriented strategy,0.6819508671760559
translation,24,6,model,effective goal-oriented strategy,by asking,questions,effective goal-oriented strategy by asking questions,0.708204448223114
translation,24,6,model,questions,confirm,model 's conjecture,questions confirm model 's conjecture,0.6016451716423035
translation,24,6,model,model 's conjecture,about,referent,model 's conjecture about referent,0.6734480857849121
translation,24,118,model,multimodal conversational model,based on,decoding strategy,multimodal conversational model based on decoding strategy,0.6461039781570435
translation,24,118,model,decoding strategy,inspired by,cognitive studies,decoding strategy inspired by cognitive studies,0.5993815064430237
translation,24,118,model,confirm - it,has,multimodal conversational model,confirm - it has multimodal conversational model,0.6009068489074707
translation,24,118,model,cognitive studies,has,of human behavior,cognitive studies has of human behavior,0.5108455419540405
translation,24,118,model,model,propose,confirm - it,model propose confirm - it,0.7493707537651062
translation,24,36,results,task accuracy,of both,conversational agent and human subjects,task accuracy of both conversational agent and human subjects,0.6454737782478333
translation,24,36,results,increases,when receiving,dialogues,increases when receiving dialogues,0.7590257525444031
translation,24,36,results,dialogues,generated by,confirm - it re-ranking algorithm,dialogues generated by confirm - it re-ranking algorithm,0.7205641865730286
translation,24,36,results,results,show that,task accuracy,results show that task accuracy,0.4507634937763214
translation,24,83,results,model,undergoes,re-ranking phase,model undergoes re-ranking phase,0.6670079231262207
translation,24,83,results,increase,of,+ 4.35 %,increase of + 4.35 %,0.5819785594940186
translation,24,83,results,increase,of,+ 4.8 %,increase of + 4.8 %,0.5878477692604065
translation,24,83,results,increase,of,+ 4.8 %,increase of + 4.8 %,0.5878477692604065
translation,24,83,results,+ 4.35 %,outputs,question,+ 4.35 % outputs question,0.7406861186027527
translation,24,83,results,question,selected by,plain beam search,question selected by plain beam search,0.7325002551078796
translation,24,83,results,increase,of,+ 4.8 %,increase of + 4.8 %,0.5878477692604065
translation,24,83,results,+ 4.8 %,against,greedy search,+ 4.8 % against greedy search,0.6471999883651733
translation,24,83,results,model,has,confirm - it accuracy,model has confirm - it accuracy,0.571499764919281
translation,24,83,results,re-ranking phase,has,confirm - it accuracy,re-ranking phase has confirm - it accuracy,0.5800278186798096
translation,24,83,results,confirm - it accuracy,has,increase,confirm - it accuracy has increase,0.5966927409172058
translation,24,85,results,confirmation - driven strategies,help generate,more effective dialogues,confirmation - driven strategies help generate more effective dialogues,0.712590217590332
translation,24,85,results,results,shows that,confirmation - driven strategies,results shows that confirmation - driven strategies,0.6486278772354126
translation,24,119,results,our model,generates,dialogues,our model generates dialogues,0.7208939790725708
translation,24,119,results,dialogues,are,more effective,dialogues are more effective,0.60662841796875
translation,24,119,results,dialogues,are,more natural,dialogues are more natural,0.6411439776420593
translation,24,119,results,more effective,based on,task - accuracy,more effective based on task - accuracy,0.6144933104515076
translation,24,119,results,proposed beam search rereanking algorithm,has,our model,proposed beam search rereanking algorithm has our model,0.5691489577293396
translation,24,119,results,results,through,proposed beam search rereanking algorithm,results through proposed beam search rereanking algorithm,0.6699872016906738
translation,25,161,ablation-analysis,inaccurate advantage estimation,early stage of,training process,inaccurate advantage estimation early stage of training process,0.6966783404350281
translation,25,161,ablation-analysis,inaccurate advantage estimation,cannot reduce,probabilities of useless actions,inaccurate advantage estimation cannot reduce probabilities of useless actions,0.6868554353713989
translation,25,161,ablation-analysis,clipping,has,inaccurate advantage estimation,clipping has inaccurate advantage estimation,0.5392204523086548
translation,25,161,ablation-analysis,probabilities of useless actions,has,efficiently,probabilities of useless actions has efficiently,0.5855806469917297
translation,25,161,ablation-analysis,ablation analysis,Without,clipping,ablation analysis Without clipping,0.7398266792297363
translation,25,163,ablation-analysis,is slightly worse,than,others,is slightly worse than others,0.6483349800109863
translation,25,163,ablation-analysis,gae,has,final performance,gae has final performance,0.5416673421859741
translation,25,163,ablation-analysis,final performance,has,is slightly worse,final performance has is slightly worse,0.5939499735832214
translation,25,163,ablation-analysis,ablation analysis,clip,gae,ablation analysis clip gae,0.8235440254211426
translation,25,31,baselines,lcpo,is,model-free and parameter - free algorithm,lcpo is model-free and parameter - free algorithm,0.5729795694351196
translation,25,31,baselines,baselines,has,lcpo,baselines has lcpo,0.5811079740524292
translation,25,123,experiments,goal-driven simulated user,on,semantic level,goal-driven simulated user on semantic level,0.5528088808059692
translation,25,210,experiments,optimiser,is,"adam ( kingma and ba , 2014 )","optimiser is adam ( kingma and ba , 2014 )",0.5362327694892883
translation,25,210,experiments,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,25,210,experiments,learning rate,has,0.001,learning rate has 0.001,0.5318801999092102
translation,25,69,hyperparameters,proximal policy optimisation ( ppo ),for,policy optimisation,proximal policy optimisation ( ppo ) for policy optimisation,0.5805880427360535
translation,25,69,hyperparameters,hyperparameters,adopt,proximal policy optimisation ( ppo ),hyperparameters adopt proximal policy optimisation ( ppo ),0.6328557133674622
translation,25,206,hyperparameters,state and action di- mension,of,policy and value networks,state and action di- mension of policy and value networks,0.5949221849441528
translation,25,206,hyperparameters,policy and value networks,are,268 and 16,policy and value networks are 268 and 16,0.6038487553596497
translation,25,206,hyperparameters,hyperparameters,has,state and action di- mension,hyperparameters has state and action di- mension,0.5428580045700073
translation,25,207,hyperparameters,dimensions,of,two hidden layers,dimensions of two hidden layers,0.565187394618988
translation,25,207,hyperparameters,two hidden layers,are,130 and 50,two hidden layers are 130 and 50,0.5839582085609436
translation,25,207,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,25,208,hyperparameters,n = 100 transitions,to update,policy,n = 100 transitions to update policy,0.7821452617645264
translation,25,208,hyperparameters,policy,with,k = 10 epochs,policy with k = 10 epochs,0.6393759250640869
translation,25,208,hyperparameters,policy,mini-,batch size b = 16,policy mini- batch size b = 16,0.6426448822021484
translation,25,211,hyperparameters,entropy coefficient,is,0.01,entropy coefficient is 0.01,0.576224148273468
translation,25,211,hyperparameters,hyperparameters,has,entropy coefficient,hyperparameters has entropy coefficient,0.5165164470672607
translation,25,211,hyperparameters,hyperparameters,has,standardised advantages,hyperparameters has standardised advantages,0.5311326384544373
translation,25,5,model,loop -clipping policy optimisation ( lcpo ),to eliminate,useless responses,loop -clipping policy optimisation ( lcpo ) to eliminate useless responses,0.672473669052124
translation,25,5,model,model,propose,loop -clipping policy optimisation ( lcpo ),model propose loop -clipping policy optimisation ( lcpo ),0.6699560880661011
translation,25,6,model,lcpo,consists of,two stages,lcpo consists of two stages,0.7168689966201782
translation,25,6,model,model,has,lcpo,model has lcpo,0.6106889843940735
translation,25,9,model,advantage clipping,estimate and clip,advantages,advantage clipping estimate and clip advantages,0.6632605195045471
translation,25,9,model,advantage clipping,of,useless responses and normal ones,advantage clipping of useless responses and normal ones,0.5868069529533386
translation,25,9,model,advantages,of,useless responses and normal ones,advantages of useless responses and normal ones,0.5550443530082703
translation,25,9,model,model,in,advantage clipping,model in advantage clipping,0.5613369941711426
translation,25,30,model,loop -clipping policy optimisation ( lcpo ),clips off,useless actions,loop -clipping policy optimisation ( lcpo ) clips off useless actions,0.779382050037384
translation,25,30,model,loop -clipping policy optimisation ( lcpo ),computes,advantages,loop -clipping policy optimisation ( lcpo ) computes advantages,0.7558257579803467
translation,25,30,model,loop -clipping policy optimisation ( lcpo ),optimises,policy,loop -clipping policy optimisation ( lcpo ) optimises policy,0.7342928051948547
translation,25,30,model,useless actions,in,trajectories,useless actions in trajectories,0.5157543420791626
translation,25,30,model,advantages,of,actions,advantages of actions,0.5726017355918884
translation,25,30,model,actions,of,loop,actions of loop,0.6028302907943726
translation,25,30,model,in / out,of,loop,in / out of loop,0.6419161558151245
translation,25,30,model,policy,based on,proximal policy optimisation ( ppo ),policy based on proximal policy optimisation ( ppo ),0.7008918523788452
translation,25,30,model,actions,has,in / out,actions has in / out,0.6382260322570801
translation,25,30,model,model,propose,loop -clipping policy optimisation ( lcpo ),model propose loop -clipping policy optimisation ( lcpo ),0.6699560880661011
translation,25,57,model,tam,learns,user model,tam learns user model,0.6956278681755066
translation,25,57,model,user model,during,dialogue interaction,user model during dialogue interaction,0.6281191110610962
translation,25,57,model,model,has,tam,model has tam,0.6109487414360046
translation,25,67,model,loop -clipping policy optimisation ( lcpo ),to improve,sample efficiency,loop -clipping policy optimisation ( lcpo ) to improve sample efficiency,0.6208605170249939
translation,25,67,model,model,propose,loop -clipping policy optimisation ( lcpo ),model propose loop -clipping policy optimisation ( lcpo ),0.6699560880661011
translation,25,68,model,lcpo,consists of,three components,lcpo consists of three components,0.7375560402870178
translation,25,68,model,model,has,lcpo,model has lcpo,0.6106889843940735
translation,25,36,results,lcpo,trained with,100 dialogue,lcpo trained with 100 dialogue,0.7702018618583679
translation,25,36,results,100 dialogue,receives,3.7/5 scores,100 dialogue receives 3.7/5 scores,0.6947132349014282
translation,25,36,results,high remarks,of,conciseness and fluency,high remarks of conciseness and fluency,0.5539237260818481
translation,25,36,results,humanin-the - loop experiment,has,lcpo,humanin-the - loop experiment has lcpo,0.6154378056526184
translation,25,36,results,results,In,humanin-the - loop experiment,results In humanin-the - loop experiment,0.5407370328903198
translation,25,136,results,lcpo,has,best final performance,lcpo has best final performance,0.57706618309021
translation,25,142,results,low resource scenario,where,dialogue policy,low resource scenario where dialogue policy,0.6281213760375977
translation,25,142,results,dialogue policy,trained by,200 dialogues,dialogue policy trained by 200 dialogues,0.6821421384811401
translation,25,142,results,lcpo,outperforms,other baselines,lcpo outperforms other baselines,0.7173171639442444
translation,25,142,results,other baselines,with,small variance,other baselines with small variance,0.5950325131416321
translation,25,142,results,low resource scenario,has,lcpo,low resource scenario has lcpo,0.5910685658454895
translation,25,142,results,dialogue policy,has,lcpo,dialogue policy has lcpo,0.6287989020347595
translation,25,142,results,results,In,low resource scenario,results In low resource scenario,0.5839752554893494
translation,25,159,results,low-resource scenario ( less than 200 dialogues ),has,clipping both gae and lae,low-resource scenario ( less than 200 dialogues ) has clipping both gae and lae,0.5824093222618103
translation,25,159,results,clipping both gae and lae,has,outperforms,clipping both gae and lae has outperforms,0.6124902367591858
translation,25,159,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,25,159,results,other methods,has,considerably,other methods has considerably,0.5549749135971069
translation,25,159,results,results,In,low-resource scenario ( less than 200 dialogues ),results In low-resource scenario ( less than 200 dialogues ),0.5060300230979919
translation,25,160,results,lcpo,with no advantage clipping,worst,lcpo with no advantage clipping worst,0.7762572169303894
translation,25,160,results,results,has,lcpo,results has lcpo,0.5508912801742554
translation,25,162,results,final performance,after training,agents,final performance after training agents,0.7601088881492615
translation,25,162,results,agents,with,2000 dialogues,agents with 2000 dialogues,0.6680463552474976
translation,25,162,results,all of the methods,perform,similarly,all of the methods perform similarly,0.6454455852508545
translation,25,162,results,final performance,has,all of the methods,final performance has all of the methods,0.5405124425888062
translation,25,162,results,agents,has,all of the methods,agents has all of the methods,0.5936331748962402
translation,25,162,results,2000 dialogues,has,all of the methods,2000 dialogues has all of the methods,0.5986807346343994
translation,25,162,results,results,Regarding,final performance,results Regarding final performance,0.6272870302200317
translation,26,7,model,multi-source pretraining paradigm,to better leverage,external summary data,multi-source pretraining paradigm to better leverage external summary data,0.694794774055481
translation,26,7,model,model,propose,multi-source pretraining paradigm,model propose multi-source pretraining paradigm,0.6297709345817566
translation,26,8,model,large-scale in- domain nonsummary data,to separately pretrain,dialogue encoder and the summary decoder,large-scale in- domain nonsummary data to separately pretrain dialogue encoder and the summary decoder,0.681509792804718
translation,26,8,model,model,exploit,large-scale in- domain nonsummary data,model exploit large-scale in- domain nonsummary data,0.6897851824760437
translation,26,9,model,combined encoder-decoder model,pretrained on,out-of- domain summary data,combined encoder-decoder model pretrained on out-of- domain summary data,0.7687072157859802
translation,26,9,model,out-of- domain summary data,using,adversarial critics,out-of- domain summary data using adversarial critics,0.641096830368042
translation,26,9,model,adversarial critics,to facilitate,domain-agnostic summarization,adversarial critics to facilitate domain-agnostic summarization,0.6299073100090027
translation,26,9,model,model,has,combined encoder-decoder model,model has combined encoder-decoder model,0.5629696249961853
translation,26,25,model,novel pretraining paradigm,called,domain-agnostic multi-source pretraining ( dams ),novel pretraining paradigm called domain-agnostic multi-source pretraining ( dams ),0.6572556495666504
translation,26,25,model,novel pretraining paradigm,to summarize,dialogues,novel pretraining paradigm to summarize dialogues,0.7222744226455688
translation,26,25,model,dialogues,in,low-resource setting,dialogues in low-resource setting,0.5524715185165405
translation,26,25,model,model,introduce,novel pretraining paradigm,model introduce novel pretraining paradigm,0.6431609392166138
translation,26,26,model,pretraining,of,dialogue summarization,pretraining of dialogue summarization,0.5317040681838989
translation,26,28,model,summary decoder,pretrained on,large-scale summarylike short texts,summary decoder pretrained on large-scale summarylike short texts,0.7385729551315308
translation,26,28,model,large-scale summarylike short texts,to learn,language model,large-scale summarylike short texts to learn language model,0.5649698376655579
translation,26,28,model,language model,in the style of,dialogue summaries,language model in the style of dialogue summaries,0.6739495992660522
translation,26,28,model,model,has,summary decoder,model has summary decoder,0.5919745564460754
translation,26,29,model,encoder and decoder,combined and pretrained on,external summary data,encoder and decoder combined and pretrained on external summary data,0.7580891251564026
translation,26,29,model,external summary data,to go through,integral process,external summary data to go through integral process,0.6726195216178894
translation,26,29,model,integral process,of,summarization,integral process of summarization,0.6030057668685913
translation,26,29,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,26,32,model,adversarial critics,to capture,features,adversarial critics to capture features,0.6817454099655151
translation,26,32,model,features,shared between,dialogues and general documents,features shared between dialogues and general documents,0.7206833362579346
translation,26,32,model,model,has,adversarial critics,model has adversarial critics,0.5435144305229187
translation,27,5,baselines,legoeval,features,flexible task design,legoeval features flexible task design,0.6595738530158997
translation,27,5,baselines,flexible task design,by providing,python api,flexible task design by providing python api,0.6032081842422485
translation,27,5,baselines,python api,maps to,commonly used react,python api maps to commonly used react,0.6812954545021057
translation,27,4,model,legoeval,has,open-source toolkit,legoeval has open-source toolkit,0.5726871490478516
translation,27,22,model,human evaluation tasks,on,amt,human evaluation tasks on amt,0.5344615578651428
translation,27,22,model,human evaluation tasks,in,one click,human evaluation tasks in one click,0.5402312278747559
translation,27,22,model,amt,in,one click,amt in one click,0.5712230801582336
translation,27,22,model,legoeval,has,open-source toolkit,legoeval has open-source toolkit,0.5726871490478516
translation,27,22,model,model,present,legoeval,model present legoeval,0.7421934604644775
translation,27,148,results,results,BlenderBot ( 2.7B ) vs.,meena,results BlenderBot ( 2.7B ) vs. meena,0.643469512462616
translation,27,154,results,results,0.72 vs.,0.28,results 0.72 vs. 0.28,0.5920462012290955
translation,27,154,results,0.28,for,engagingness,0.28 for engagingness,0.5809676647186279
translation,27,154,results,0.28,for,engagingness,0.28 for engagingness,0.5809676647186279
translation,27,154,results,0.32,for,humanness,0.32 for humanness,0.5541106462478638
translation,27,154,results,0.32,for,humanness,0.32 for humanness,0.5541106462478638
translation,27,154,results,0.25,for,engagingness,0.25 for engagingness,0.6248226761817932
translation,27,154,results,0.35,for,humanness,0.35 for humanness,0.5683490633964539
translation,27,154,results,results,0.72 vs.,0.28,results 0.72 vs. 0.28,0.5920462012290955
translation,27,154,results,results,0.68 vs.,0.32,results 0.68 vs. 0.32,0.6034678220748901
translation,27,154,results,results,0.75 vs.,0.25,results 0.75 vs. 0.25,0.582608163356781
translation,27,154,results,results,0.75 vs.,0.65,results 0.75 vs. 0.65,0.6076235771179199
translation,27,154,results,results,has,results,results has results,0.48582205176353455
translation,28,124,ablation-analysis,dstqa,in,4 out of 5 domains,dstqa in 4 out of 5 domains,0.5557855367660522
translation,28,124,ablation-analysis,ablation analysis,has,dstqa,ablation analysis has dstqa,0.5280937552452087
translation,28,131,ablation-analysis,ct strategy,during,qa training,ct strategy during qa training,0.719351053237915
translation,28,131,ablation-analysis,ct strategy,affects,performance,ct strategy affects performance,0.7727504372596741
translation,28,131,ablation-analysis,performance,in,train domain,performance in train domain,0.5146176218986511
translation,28,131,ablation-analysis,removing both nqs and ct,decreases,jga,removing both nqs and ct decreases jga,0.7278153896331787
translation,28,131,ablation-analysis,jga,in,all the domains,jga in all the domains,0.5693520903587341
translation,28,131,ablation-analysis,dramatically,in,all the domains,dramatically in all the domains,0.5655127167701721
translation,28,131,ablation-analysis,jga,has,dramatically,jga has dramatically,0.7051570415496826
translation,28,131,ablation-analysis,ablation analysis,removing,ct strategy,ablation analysis removing ct strategy,0.7266876697540283
translation,28,133,ablation-analysis,nqs and ct,observed,large jga improvement,nqs and ct observed large jga improvement,0.7253597974777222
translation,28,133,ablation-analysis,nqs and ct,observed,relatively small jga improvement,nqs and ct observed relatively small jga improvement,0.7255195379257202
translation,28,133,ablation-analysis,large jga improvement,in,taxi domain,large jga improvement in taxi domain,0.5607004761695862
translation,28,133,ablation-analysis,around 30 % ),in,taxi domain,around 30 % ) in taxi domain,0.522243320941925
translation,28,133,ablation-analysis,relatively small jga improvement,in,attraction and train domains,relatively small jga improvement in attraction and train domains,0.5052577257156372
translation,28,133,ablation-analysis,around 10 % ),in,attraction and train domains,around 10 % ) in attraction and train domains,0.5368198752403259
translation,28,133,ablation-analysis,ratios of unanswerable slots,are,47.70 % and 49.58 %,ratios of unanswerable slots are 47.70 % and 49.58 %,0.5748674869537354
translation,28,133,ablation-analysis,large jga improvement,has,around 30 % ),large jga improvement has around 30 % ),0.5862553119659424
translation,28,133,ablation-analysis,relatively small jga improvement,has,around 10 % ),relatively small jga improvement has around 10 % ),0.5751885771751404
translation,28,133,ablation-analysis,ablation analysis,adding,nqs and ct,ablation analysis adding nqs and ct,0.7443715929985046
translation,28,99,baselines,transferable dialogue state generator,utilizes,copy mechanism,transferable dialogue state generator utilizes copy mechanism,0.6199156045913696
translation,28,99,baselines,copy mechanism,to facilitate,domain knowledge transfer,copy mechanism to facilitate domain knowledge transfer,0.6831347346305847
translation,28,99,baselines,baselines,has,transferable dialogue state generator,baselines has transferable dialogue state generator,0.5402464270591736
translation,28,111,baselines,sumbt,evaluated in,cross - domain setting,sumbt evaluated in cross - domain setting,0.7269818186759949
translation,28,111,baselines,cross - domain setting,where,models,cross - domain setting where models,0.6243030428886414
translation,28,111,baselines,models,trained on,four domains,models trained on four domains,0.7367066144943237
translation,28,111,baselines,four domains,in,multiwoz,four domains in multiwoz,0.6324669718742371
translation,28,111,baselines,four domains,in,zero-shot,four domains in zero-shot,0.5701883435249329
translation,28,111,baselines,zero-shot,on,held - out domain,zero-shot on held - out domain,0.5780252814292908
translation,28,51,experimental-setup,750 gb,of,clean and natural english text,750 gb of clean and natural english text,0.5765604972839355
translation,28,51,experimental-setup,clean and natural english text,with,masking language modeling objective,clean and natural english text with masking language modeling objective,0.5747963786125183
translation,28,51,experimental-setup,masking language modeling objective,predicting,missing spans,masking language modeling objective predicting missing spans,0.6233448386192322
translation,28,51,experimental-setup,missing spans,using,decoder,missing spans using decoder,0.7082245945930481
translation,28,51,experimental-setup,experimental setup,pre-trained on,750 gb,experimental setup pre-trained on 750 gb,0.7305026650428772
translation,28,89,experimental-setup,qa training stage,set,ratio,qa training stage set ratio,0.6997911930084229
translation,28,89,experimental-setup,qa training stage,set,ratio,qa training stage set ratio,0.6997911930084229
translation,28,89,experimental-setup,qa training stage,train,models,qa training stage train models,0.7488919496536255
translation,28,89,experimental-setup,ratio,of,generating,ratio of generating,0.6691072583198547
translation,28,89,experimental-setup,ratio,of,negative sampled questions,ratio of negative sampled questions,0.6234405636787415
translation,28,89,experimental-setup,ratio,of,negative sampled questions,ratio of negative sampled questions,0.6234405636787415
translation,28,89,experimental-setup,ratio,of,negative sampled questions,ratio of negative sampled questions,0.6234405636787415
translation,28,89,experimental-setup,ratio,of,truncated context,ratio of truncated context,0.6130029559135437
translation,28,89,experimental-setup,truncated context,is,0.95 : 0.05,truncated context is 0.95 : 0.05,0.5901052951812744
translation,28,89,experimental-setup,models,with,batch size,models with batch size,0.6254024505615234
translation,28,89,experimental-setup,1024,for,5 epochs,1024 for 5 epochs,0.6598606109619141
translation,28,89,experimental-setup,generating,has,unanswerable question,generating has unanswerable question,0.6156501770019531
translation,28,89,experimental-setup,batch size,has,1024,batch size has 1024,0.621269166469574
translation,28,89,experimental-setup,experimental setup,In,qa training stage,experimental setup In qa training stage,0.5522584319114685
translation,28,89,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,28,90,experimental-setup,dst zero-shot testing,treat,all the slots,dst zero-shot testing treat all the slots,0.597072958946228
translation,28,90,experimental-setup,dst zero-shot testing,generate,all the slot values,dst zero-shot testing generate all the slot values,0.6728653311729431
translation,28,90,experimental-setup,all the slots,as,non-categorical,all the slots as non-categorical,0.5342373847961426
translation,28,90,experimental-setup,experimental setup,In,dst zero-shot testing,experimental setup In dst zero-shot testing,0.5363380312919617
translation,28,95,experimental-setup,fine-tuned,with,"1 % , 5 % and 10 %","fine-tuned with 1 % , 5 % and 10 %",0.6736442446708679
translation,28,95,experimental-setup,"1 % , 5 % and 10 %",of,target domain data,"1 % , 5 % and 10 % of target domain data",0.6351169943809509
translation,28,95,experimental-setup,"1 % , 5 % and 10 %",for,20 epochs,"1 % , 5 % and 10 % for 20 epochs",0.6246299147605896
translation,28,95,experimental-setup,few-shot experiments,has,qa pretrained models,few-shot experiments has qa pretrained models,0.5516248941421509
translation,28,95,experimental-setup,experimental setup,For,few-shot experiments,experimental setup For few-shot experiments,0.6106818318367004
translation,28,88,experiments,transferqa,based on,"t5 - large ( raffel et al. , 2020 )","transferqa based on t5 - large ( raffel et al. , 2020 )",0.6770193576812744
translation,28,88,experiments,"adamw ( loshchilov and hutter , 2018 ) optimizer",with,initial learning rate,"adamw ( loshchilov and hutter , 2018 ) optimizer with initial learning rate",0.590076208114624
translation,28,88,experiments,initial learning rate,of,0.00005,initial learning rate of 0.00005,0.5699113607406616
translation,28,5,model,crosstask knowledge,from,general question answering ( qa ) corpora,crosstask knowledge from general question answering ( qa ) corpora,0.5134308338165283
translation,28,5,model,general question answering ( qa ) corpora,for,zero-shot dst task,general question answering ( qa ) corpora for zero-shot dst task,0.5668092966079712
translation,28,5,model,model,transfer,crosstask knowledge,model transfer crosstask knowledge,0.7128548622131348
translation,28,6,model,transferable generative qa model,seamlessly combines,extractive qa,transferable generative qa model seamlessly combines extractive qa,0.6806497573852539
translation,28,6,model,transferable generative qa model,seamlessly combines,multichoice qa,transferable generative qa model seamlessly combines multichoice qa,0.6874074935913086
translation,28,6,model,transferable generative qa model,tracks,categorical slots and non-categorical slots,transferable generative qa model tracks categorical slots and non-categorical slots,0.7006857395172119
translation,28,6,model,multichoice qa,via,text - to - text transformer framework,multichoice qa via text - to - text transformer framework,0.6345468163490295
translation,28,6,model,categorical slots and non-categorical slots,in,dst,categorical slots and non-categorical slots in dst,0.5877529978752136
translation,28,6,model,transferqa,has,transferable generative qa model,transferqa has transferable generative qa model,0.5478253960609436
translation,28,6,model,model,propose,transferqa,model propose transferqa,0.7106987237930298
translation,28,7,model,two effective ways,to construct,unanswerable questions,two effective ways to construct unanswerable questions,0.7164140939712524
translation,28,7,model,unanswerable questions,namely,negative question sampling,unanswerable questions namely negative question sampling,0.6757297515869141
translation,28,7,model,unanswerable questions,namely,context truncation,unanswerable questions namely context truncation,0.6885266304016113
translation,28,7,model,our model,to handle,  none   value slots,our model to handle   none   value slots,0.7726535797119141
translation,28,7,model,  none   value slots,in,zero-shot dst setting,  none   value slots in zero-shot dst setting,0.5271955728530884
translation,28,7,model,model,introduce,two effective ways,model introduce two effective ways,0.6113002300262451
translation,28,7,model,model,to handle,  none   value slots,model to handle   none   value slots,0.7116782665252686
translation,28,29,model,unified generative qa model,seamlessly combines,extractive qa,unified generative qa model seamlessly combines extractive qa,0.6794119477272034
translation,28,29,model,unified generative qa model,seamlessly combines,multi-choice qa,unified generative qa model seamlessly combines multi-choice qa,0.6914451122283936
translation,28,29,model,multi-choice qa,via,text - to - text transformer framework,multi-choice qa via text - to - text transformer framework,0.6379233002662659
translation,28,29,model,transferqa,has,unified generative qa model,transferqa has unified generative qa model,0.5322810411453247
translation,28,29,model,model,propose,transferqa,model propose transferqa,0.7106987237930298
translation,28,30,model,simple unified text - totext interface,for tracking,categorical slots and non-categorical slots,simple unified text - totext interface for tracking categorical slots and non-categorical slots,0.7406820058822632
translation,28,30,model,model,to leverage,extractive and multi-choice qa datasets,model to leverage extractive and multi-choice qa datasets,0.6788272857666016
translation,28,30,model,model,provides,simple unified text - totext interface,model provides simple unified text - totext interface,0.6406149864196777
translation,28,31,model,two effective ways,to construct,unanswerable questions,two effective ways to construct unanswerable questions,0.7164140939712524
translation,28,31,model,unanswerable questions,namely,negative question sampling,unanswerable questions namely negative question sampling,0.6757297515869141
translation,28,31,model,unanswerable questions,namely,context truncation,unanswerable questions namely context truncation,0.6885266304016113
translation,28,31,model,unanswerable questions,simulate,out-of- domain slots and in-domain unmentioned slots,unanswerable questions simulate out-of- domain slots and in-domain unmentioned slots,0.6021679639816284
translation,28,31,model,out-of- domain slots and in-domain unmentioned slots,in,multi-domain dst,out-of- domain slots and in-domain unmentioned slots in multi-domain dst,0.5619585514068604
translation,28,101,model,slot-utterance matching belief tracker,based on,"language model bert ( devlin et al. , 2018 )","slot-utterance matching belief tracker based on language model bert ( devlin et al. , 2018 )",0.5944356918334961
translation,28,101,model,model,has,slot-utterance matching belief tracker,model has slot-utterance matching belief tracker,0.5491427779197693
translation,28,112,results,our transferqa,achieves,significantly higher jga,our transferqa achieves significantly higher jga,0.7412733435630798
translation,28,112,results,significantly higher jga,compared to,previous zero-shot results,significantly higher jga compared to previous zero-shot results,0.668748676776886
translation,28,112,results,our transferqa,has,without any dst training data,our transferqa has without any dst training data,0.5792502760887146
translation,28,112,results,significantly higher jga,has,7.59 % on average,significantly higher jga has 7.59 % on average,0.5758057236671448
translation,28,112,results,results,has,our transferqa,results has our transferqa,0.5836531519889832
translation,28,113,results,sgd dataset,where,sgd - baseline,sgd dataset where sgd - baseline,0.6119852066040039
translation,28,113,results,sgd - baseline,trained with,whole sgd training set,sgd - baseline trained with whole sgd training set,0.7403638362884521
translation,28,113,results,results,on,sgd dataset,results on sgd dataset,0.5405164361000061
translation,28,114,results,consistently higher,in terms of,jga and aga,consistently higher in terms of jga and aga,0.7643119692802429
translation,28,114,results,jga and aga,in,unseen domains,jga and aga in unseen domains,0.5572004914283752
translation,28,114,results,competitive,in,seen domains,competitive in seen domains,0.5890363454818726
translation,28,114,results,results,has,transferqa zero-shot performance,results has transferqa zero-shot performance,0.5755557417869568
translation,28,115,results,both datasets,shows,effectiveness,both datasets shows effectiveness,0.7214621305465698
translation,28,115,results,effectiveness,of,cross- task zero-shot transferring,effectiveness of cross- task zero-shot transferring,0.554840624332428
translation,28,115,results,results,on,both datasets,results on both datasets,0.49870386719703674
translation,28,123,results,both cross- task transferring approaches,has,outperform,both cross- task transferring approaches has outperform,0.5958407521247864
translation,28,123,results,outperform,has,crossdomain transferring approaches,outperform has crossdomain transferring approaches,0.5857895612716675
translation,28,123,results,results,show,both cross- task transferring approaches,results show both cross- task transferring approaches,0.5615825653076172
translation,28,125,results,transferqa,achieves,around 1 % lower jga,transferqa achieves around 1 % lower jga,0.7095948457717896
translation,28,125,results,transferqa,achieves,consistently higher jga,transferqa achieves consistently higher jga,0.7254953384399414
translation,28,125,results,around 1 % lower jga,in,hotel domain,around 1 % lower jga in hotel domain,0.5326572060585022
translation,28,125,results,consistently higher jga,on,other domains,consistently higher jga on other domains,0.47982439398765564
translation,28,125,results,consistently higher jga,under,different data ratio settings,consistently higher jga under different data ratio settings,0.6901841759681702
translation,28,125,results,starc,has,transferqa,starc has transferqa,0.6486302018165588
translation,28,125,results,results,Compared to,starc,results Compared to starc,0.6700639128684998
translation,28,126,results,1 %,of,in- domain data,1 % of in- domain data,0.6123145222663879
translation,28,126,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,28,126,results,starc,in,most domains ( except hotel ),starc in most domains ( except hotel ),0.49825388193130493
translation,28,126,results,starc,by,large margin,starc by large margin,0.5545369982719421
translation,28,126,results,4.49 %,in,train domain,4.49 % in train domain,0.526470422744751
translation,28,126,results,in- domain data,has,our model,in- domain data has our model,0.5756474733352661
translation,28,126,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,28,126,results,outperforms,has,starc,outperforms has starc,0.6117899417877197
translation,28,130,results,ct and nqs,gives,best result,ct and nqs gives best result,0.6858289837837219
translation,28,130,results,best result,in terms of,average jga,best result in terms of average jga,0.6928163766860962
translation,28,130,results,average jga,for,transferqa -large,average jga for transferqa -large,0.6402026414871216
translation,28,130,results,average jga,for,transferqa - base,average jga for transferqa - base,0.6143121719360352
translation,28,130,results,results,Applying,ct and nqs,results Applying ct and nqs,0.6415095329284668
translation,29,69,baselines,dgpt -a gpt - 2 architecture,trained on,147m reddit comment threads,dgpt -a gpt - 2 architecture trained on 147m reddit comment threads,0.742339551448822
translation,29,82,experiments,blender bot,uses,beam search,blender bot uses beam search,0.6334890127182007
translation,29,82,experiments,beam search,with,beam size = 10 and min,beam search with beam size = 10 and min,0.6799449920654297
translation,29,154,results,dgpt model,with,full thread context,dgpt model with full thread context,0.613395094871521
translation,29,154,results,bert and nbow models,which lack,global context,bert and nbow models which lack global context,0.7894757986068726
translation,29,154,results,full thread context,has,outperforms,full thread context has outperforms,0.6241511702537537
translation,29,154,results,outperforms,has,bert and nbow models,outperforms has bert and nbow models,0.6143180131912231
translation,29,154,results,results,has,dgpt model,results has dgpt model,0.5128039717674255
translation,29,155,results,dgpt classifier,achieves,higher accuracy,dgpt classifier achieves higher accuracy,0.7088009715080261
translation,29,155,results,higher accuracy,for detecting,offensiveness,higher accuracy for detecting offensiveness,0.6963741183280945
translation,29,155,results,offensiveness,in,first utterance ( first u f 1 ),offensiveness in first utterance ( first u f 1 ),0.48313358426094055
translation,29,155,results,first utterance ( first u f 1 ),compared to,bert,first utterance ( first u f 1 ) compared to bert,0.6931188106536865
translation,29,155,results,offensive task,has,dgpt classifier,offensive task has dgpt classifier,0.5493398308753967
translation,29,155,results,results,For,offensive task,results For offensive task,0.5582028031349182
translation,29,157,results,training set,with,sbic data,training set with sbic data,0.6591915488243103
translation,29,157,results,sbic data,shows,further improvement,sbic data shows further improvement,0.7229117751121521
translation,29,157,results,further improvement,in,all the metrics,further improvement in all the metrics,0.5128822922706604
translation,29,157,results,results,Augmenting,training set,results Augmenting training set,0.6491796970367432
translation,29,158,results,best model,achieves,0.714 f 1,best model achieves 0.714 f 1,0.580747663974762
translation,29,158,results,0.714 f 1,on,all utterances,0.714 f 1 on all utterances,0.5437033176422119
translation,29,159,results,classification models,perform,worse,classification models perform worse,0.6426911354064941
translation,29,159,results,worse,on,dialogue model responses,worse on dialogue model responses,0.593512237071991
translation,29,159,results,dialogue model responses,within,our dataset,dialogue model responses within our dataset,0.6560022830963135
translation,29,159,results,results,has,classification models,results has classification models,0.5261950492858887
translation,29,196,results,pretrained dialogue models dgpt and gpt - 3,generate,? 30 % and ? 41 % offensive responses,pretrained dialogue models dgpt and gpt - 3 generate ? 30 % and ? 41 % offensive responses,0.6371105313301086
translation,29,196,results,? 30 % and ? 41 % offensive responses,tested in,offensive contexts,? 30 % and ? 41 % offensive responses tested in offensive contexts,0.742817223072052
translation,29,196,results,results,has,pretrained dialogue models dgpt and gpt - 3,results has pretrained dialogue models dgpt and gpt - 3,0.5363502502441406
translation,29,212,results,dapt model,achieves,lowest ' agree ' responses,dapt model achieves lowest ' agree ' responses,0.6676362156867981
translation,29,212,results,dapt model,achieves,highest ' neutral ' responses,dapt model achieves highest ' neutral ' responses,0.6899225115776062
translation,29,212,results,dapt model,achieves,slightly more offensive,dapt model achieves slightly more offensive,0.6576793789863586
translation,29,212,results,slightly more offensive,than,facebook 's blender chatbot,slightly more offensive than facebook 's blender chatbot,0.6201883554458618
translation,29,212,results,human evals,has,dapt model,human evals has dapt model,0.5854436755180359
translation,29,212,results,results,According to,human evals,results According to human evals,0.6706870198249817
translation,29,213,results,blender,is,least offensive,blender is least offensive,0.5918977856636047
translation,29,213,results,blender,is,most agreeing,blender is most agreeing,0.5587902069091797
translation,29,213,results,most agreeing,among,all evaluated models,most agreeing among all evaluated models,0.5846118927001953
translation,29,213,results,results,has,blender,results has blender,0.5758205056190491
translation,30,291,ablation-analysis,more than 80 %,of,redundant qa pairs,more than 80 % of redundant qa pairs,0.6473221182823181
translation,30,291,ablation-analysis,redundant qa pairs,exist in,generated summary,redundant qa pairs exist in generated summary,0.6873939633369446
translation,30,291,ablation-analysis,ablation analysis,has,more than 80 %,ablation analysis has more than 80 %,0.5303409695625305
translation,30,201,baselines,baselines,has,"lexpagerank ( erkan and radev , 2004 )","baselines has lexpagerank ( erkan and radev , 2004 )",0.5089179277420044
translation,30,203,baselines,supervised extractive summarization,by scoring,each utterance,supervised extractive summarization by scoring each utterance,0.7238767743110657
translation,30,203,baselines,each utterance,using,rnn,each utterance using rnn,0.6494377851486206
translation,30,203,baselines,summarunner,has,),summarunner has ),0.6226474046707153
translation,30,203,baselines,summarunner,has,supervised extractive summarization,summarunner has supervised extractive summarization,0.5111163258552551
translation,30,203,baselines,),has,supervised extractive summarization,) has supervised extractive summarization,0.5758985280990601
translation,30,203,baselines,baselines,has,summarunner,baselines has summarunner,0.5826749801635742
translation,30,209,baselines,rnn - based seq2seq model,using,source word copy mechanism,rnn - based seq2seq model using source word copy mechanism,0.6405209302902222
translation,30,209,baselines,rnn - based seq2seq model,using,attention coverage mechanism,rnn - based seq2seq model using attention coverage mechanism,0.6365137696266174
translation,30,210,baselines,baselines,has,"fast - rl ( chen and bansal , 2018 )","baselines has fast - rl ( chen and bansal , 2018 )",0.5172646045684814
translation,30,213,baselines,baselines,has,"bertabs ( liu and lapata , 2019 )","baselines has bertabs ( liu and lapata , 2019 )",0.5716451406478882
translation,30,215,baselines,baselines,has,tds +satm,baselines has tds +satm,0.557595431804657
translation,30,229,baselines,baselines,has,"bertscore ( zhang et al. , 2020 )","baselines has bertscore ( zhang et al. , 2020 )",0.5344494581222534
translation,30,231,baselines,baselines,has,"moverscore ( zhao et al. , 2019 )","baselines has moverscore ( zhao et al. , 2019 )",0.5157960057258606
translation,30,202,model,dialogue utterances,by,pagerank algorithm,dialogue utterances by pagerank algorithm,0.5801615118980408
translation,30,202,model,dialogue utterances,in,order,dialogue utterances in order,0.5286978483200073
translation,30,202,model,utterances,in,order,utterances in order,0.5267197489738464
translation,30,202,model,utterances,length of,summary,utterances length of summary,0.6345527768135071
translation,30,202,model,summary,reaches,limit,summary reaches limit,0.7034710049629211
translation,30,214,model,pretrained bert,as,encoder,pretrained bert as encoder,0.5577908158302307
translation,30,214,model,transformerbased network,as,decoder,transformerbased network as decoder,0.5841867327690125
translation,30,214,model,model,uses,pretrained bert,model uses pretrained bert,0.6296400427818298
translation,30,214,model,model,uses,transformerbased network,model uses transformerbased network,0.5639694929122925
translation,30,8,results,csds,improves,abstractive summaries,csds improves abstractive summaries,0.6438702344894409
translation,30,8,results,results,has,csds,results has csds,0.5241585969924927
translation,30,248,results,abstractive methods,perform better than,extractive methods,abstractive methods perform better than extractive methods,0.6342930197715759
translation,30,248,results,extractive methods,with,large margin,extractive methods with large margin,0.6347478032112122
translation,30,248,results,results,observe that,abstractive methods,results observe that abstractive methods,0.45102548599243164
translation,30,249,results,summarunner,achieves,best results,summarunner achieves best results,0.6733815670013428
translation,30,249,results,extractive methods,has,summarunner,extractive methods has summarunner,0.5867274403572083
translation,30,249,results,results,Among,extractive methods,results Among extractive methods,0.5566853284835815
translation,30,250,results,fast-rl and fast - rl *,perform,best,fast-rl and fast - rl * perform best,0.6060338020324707
translation,30,250,results,best,on,almost all metrics,best on almost all metrics,0.4893215596675873
translation,30,250,results,almost all metrics,except for,rouge -l,almost all metrics except for rouge -l,0.6106302738189697
translation,30,250,results,abstractive methods,has,fast-rl and fast - rl *,abstractive methods has fast-rl and fast - rl *,0.5500026345252991
translation,30,252,results,"enhanced methods ( fast - rl * , tds + satm * )",usually better than,original version,"enhanced methods ( fast - rl * , tds + satm * ) usually better than original version",0.7655959129333496
translation,30,252,results,original version,on,overall summary,original version on overall summary,0.5122971534729004
translation,30,252,results,original version,on,user summary,original version on user summary,0.4988277852535248
translation,30,254,results,agent summary scores,are,much lower,agent summary scores are much lower,0.5953827500343323
translation,30,254,results,much lower,than,overall summary and user summary,much lower than overall summary and user summary,0.5723256468772888
translation,30,254,results,overall summary and user summary,in,most metrics,overall summary and user summary in most metrics,0.4494045078754425
translation,30,260,results,inter-annotator agreement study,on,three volunteers ' scores,inter-annotator agreement study on three volunteers ' scores,0.5153231024742126
translation,30,260,results,kappa score,is,0.52,kappa score is 0.52,0.5134148001670837
translation,30,260,results,results,run,inter-annotator agreement study,results run inter-annotator agreement study,0.6278761029243469
translation,30,262,results,all methods,perform,poorly,all methods perform poorly,0.6648768186569214
translation,30,262,results,poorly,on,nonredundancy,poorly on nonredundancy,0.5976601243019104
translation,30,262,results,results,has,all methods,results has all methods,0.48065561056137085
translation,30,265,results,nearly 30 percent,of,overall summaries,nearly 30 percent of overall summaries,0.6306896805763245
translation,30,265,results,overall summaries,have,unmatched questions and answers,overall summaries have unmatched questions and answers,0.547059953212738
translation,30,265,results,unmatched questions and answers,through,matching rate,unmatched questions and answers through matching rate,0.6397169232368469
translation,30,265,results,results,find that,nearly 30 percent,results find that nearly 30 percent,0.6413671374320984
translation,31,139,ablation-analysis,learning,benefits from,more negative passage examples,learning benefits from more negative passage examples,0.604159414768219
translation,31,139,ablation-analysis,number used,constrained by,memory consumption,number used constrained by memory consumption,0.6522950530052185
translation,31,139,ablation-analysis,memory consumption,up to,10,memory consumption up to 10,0.5953038930892944
translation,31,139,ablation-analysis,10,for,models,10 for models,0.69578617811203
translation,31,139,ablation-analysis,models,with,posterior regularization,models with posterior regularization,0.5756165981292725
translation,31,139,ablation-analysis,models,with,up to 20,models with up to 20,0.6885442137718201
translation,31,139,ablation-analysis,ablation analysis,find that,learning,ablation analysis find that learning,0.6417298316955566
translation,31,160,ablation-analysis,bertlarge,helps further improve,overall results,bertlarge helps further improve overall results,0.7328373193740845
translation,31,160,ablation-analysis,ablation analysis,has,bertlarge,ablation analysis has bertlarge,0.6053886413574219
translation,31,177,ablation-analysis,know-ctx,helps enhance,performance,know-ctx helps enhance performance,0.7787293195724487
translation,31,177,ablation-analysis,performance,on,doc2 dial,performance on doc2 dial,0.6330128908157349
translation,31,177,ablation-analysis,ablation analysis,Adding,know-ctx,ablation analysis Adding know-ctx,0.746984601020813
translation,31,146,baselines,bertqa - span,predicts,start and end knowledge spans,bertqa - span predicts start and end knowledge spans,0.721167266368866
translation,31,146,baselines,start and end knowledge spans,instead of,tokens,start and end knowledge spans instead of tokens,0.6512889266014099
translation,31,146,baselines,baselines,has,bertqa - span,baselines has bertqa - span,0.5907809138298035
translation,31,152,baselines,state- of - the - art model,on,wow,state- of - the - art model on wow,0.5645541548728943
translation,31,152,baselines,wow,encodes,all knowledge sentences and dialogue turns,wow encodes all knowledge sentences and dialogue turns,0.7090661525726318
translation,31,152,baselines,all knowledge sentences and dialogue turns,with,bert ( or rnn ),all knowledge sentences and dialogue turns with bert ( or rnn ),0.6276610493659973
translation,31,152,baselines,slks,has,state- of - the - art model,slks has state- of - the - art model,0.5522505044937134
translation,31,152,baselines,baselines,has,slks,baselines has slks,0.6005513668060303
translation,31,26,experiments,dialki,uses,multi-task objective,dialki uses multi-task objective,0.5883010029792786
translation,31,26,experiments,multi-task objective,to identify,knowledge,multi-task objective to identify knowledge,0.6763634085655212
translation,31,26,experiments,multi-task objective,to identify,used knowledge,multi-task objective to identify used knowledge,0.6575620174407959
translation,31,26,experiments,knowledge,for,next turn,knowledge for next turn,0.6512306332588196
translation,31,26,experiments,knowledge,for,previous turns,knowledge for previous turns,0.5936601758003235
translation,31,26,experiments,knowledge,for,previous turns,knowledge for previous turns,0.5936601758003235
translation,31,26,experiments,knowledge,helps improve,learning,knowledge helps improve learning,0.723223090171814
translation,31,26,experiments,used knowledge,for,previous turns,used knowledge for previous turns,0.5735809803009033
translation,31,26,experiments,used knowledge,helps improve,learning,used knowledge helps improve learning,0.7268974781036377
translation,31,26,experiments,learning,of,dialogue and document representations,learning of dialogue and document representations,0.5026537179946899
translation,31,26,experiments,learning,by capturing,interdependencies,learning by capturing interdependencies,0.7474677562713623
translation,31,26,experiments,dialogue and document representations,by capturing,interdependencies,dialogue and document representations by capturing interdependencies,0.7053927183151245
translation,31,26,experiments,interdependencies,between,next agent utterance,interdependencies between next agent utterance,0.6594216227531433
translation,31,26,experiments,interdependencies,between,previous utterances,interdependencies between previous utterances,0.650518536567688
translation,31,26,experiments,interdependencies,between,grounding document,interdependencies between grounding document,0.6484740972518921
translation,31,140,experiments,up to 20 passages,from,target document,up to 20 passages from target document,0.5769280195236206
translation,31,140,experiments,inference,has,up to 20 passages,inference has up to 20 passages,0.6154413819313049
translation,31,132,hyperparameters,3e ?5,as,learning rates,3e ?5 as learning rates,0.5816817283630371
translation,31,132,hyperparameters,3e ?5,as,1000,3e ?5 as 1000,0.6138164401054382
translation,31,132,hyperparameters,1000,as,warmup steps,1000 as warmup steps,0.5511519908905029
translation,31,132,hyperparameters,hyperparameters,use,uncased base bert,hyperparameters use uncased base bert,0.620629072189331
translation,31,136,hyperparameters,maximum length,of,dialogue context,maximum length of dialogue context,0.570330798625946
translation,31,136,hyperparameters,dialogue context,is,128,dialogue context is 128,0.557444155216217
translation,31,136,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,31,138,hyperparameters,multiple passages,from,grounding document,multiple passages from grounding document,0.5488661527633667
translation,31,5,model,knowledge identification model,leverages,document structure,knowledge identification model leverages document structure,0.6610444784164429
translation,31,5,model,document structure,to provide,dialogue - contextualized passage encodings,document structure to provide dialogue - contextualized passage encodings,0.6688797473907471
translation,31,5,model,model,introduce,knowledge identification model,model introduce knowledge identification model,0.5891817212104797
translation,31,22,model,multi-passage reader models,in,open question answering,multi-passage reader models in open question answering,0.4970625936985016
translation,31,22,model,multi-passage reader models,to obtain,dense encodings,multi-passage reader models to obtain dense encodings,0.5994000434875488
translation,31,22,model,dense encodings,of,different spans,dense encodings of different spans,0.6005731821060181
translation,31,22,model,different spans,in,multiple passages,different spans in multiple passages,0.5335931777954102
translation,31,22,model,multiple passages,in,grounding document,multiple passages in grounding document,0.4957936108112335
translation,31,23,model,di - alki,extracts,knowledge,di - alki extracts knowledge,0.6917891502380371
translation,31,23,model,knowledge,given,long document,knowledge given long document,0.6914157271385193
translation,31,23,model,long document,by dividing it into,paragraphs or sections,long document by dividing it into paragraphs or sections,0.5911630988121033
translation,31,23,model,model,has,di - alki,model has di - alki,0.6486729383468628
translation,31,32,model,multi-task learning framework,models,dialogue -document interactions,multi-task learning framework models dialogue -document interactions,0.670836329460144
translation,31,32,model,dialogue -document interactions,via,auxiliary task,dialogue -document interactions via auxiliary task,0.6395876407623291
translation,31,32,model,auxiliary task,of,history knowledge prediction,auxiliary task of history knowledge prediction,0.5501221418380737
translation,31,32,model,auxiliary task,of,knowledge contextualization mechanism,auxiliary task of knowledge contextualization mechanism,0.5373446345329285
translation,31,32,model,model,introduce,multi-task learning framework,model introduce multi-task learning framework,0.5986069440841675
translation,31,153,model,two grus,to update,states,two grus to update states,0.7751200199127197
translation,31,153,model,states,of,dialog history,states of dialog history,0.5792955756187439
translation,31,153,model,states,of,previously selected sentences,states of previously selected sentences,0.5428820848464966
translation,31,153,model,model,uses,two grus,model uses two grus,0.6803038120269775
translation,31,27,results,significantly improves,over,baselines,significantly improves over baselines,0.7216042280197144
translation,31,27,results,baselines,on,two conversational datasets,baselines on two conversational datasets,0.4809538722038269
translation,31,27,results,baselines,particularly on,previously unseen documents or topics,baselines particularly on previously unseen documents or topics,0.5709298253059387
translation,31,27,results,our model,has,significantly improves,our model has significantly improves,0.5935119986534119
translation,31,27,results,results,has,our model,results has our model,0.5871725678443909
translation,31,156,results,systems,in,blind held - out test set,systems in blind held - out test set,0.538364052772522
translation,31,156,results,blind held - out test set,with,unseen covid -19 domain,blind held - out test set with unseen covid -19 domain,0.6172502636909485
translation,31,158,results,full model,of,dialki,full model of dialki,0.6424619555473328
translation,31,158,results,results,has,full model,results has full model,0.523858904838562
translation,31,159,results,significant improvement,from,dialki ( l next only ),significant improvement from dialki ( l next only ),0.5499972701072693
translation,31,159,results,significant improvement,shows,benefit,significant improvement shows benefit,0.6476544737815857
translation,31,159,results,dialki ( l next only ),over,bertqa - token,dialki ( l next only ) over bertqa - token,0.6591833233833313
translation,31,159,results,bertqa - token,takes,full document,bertqa - token takes full document,0.6758881211280823
translation,31,159,results,full document,as,single string,full document as single string,0.5615732669830322
translation,31,159,results,benefit,of,our multi-passage framework,benefit of our multi-passage framework,0.5511478781700134
translation,31,159,results,results,has,significant improvement,results has significant improvement,0.5643457770347595
translation,31,161,results,conversations,on,seen and unseen topics,conversations on seen and unseen topics,0.5644999742507935
translation,31,161,results,results,has,wow,results has wow,0.5789391994476318
translation,31,162,results,significantly outperforms,encode,knowledge sentences,significantly outperforms encode knowledge sentences,0.734827995300293
translation,31,162,results,dialki,has,significantly outperforms,dialki has significantly outperforms,0.6263027191162109
translation,31,162,results,significantly outperforms,has,all other systems,significantly outperforms has all other systems,0.5828918814659119
translation,31,162,results,knowledge sentences,has,disjointly,knowledge sentences has disjointly,0.6488953828811646
translation,31,162,results,results,has,dialki,results has dialki,0.5242040157318115
translation,31,174,results,dialki,consistently beat,baseline models,dialki consistently beat baseline models,0.7790049910545349
translation,31,174,results,baseline models,process,full document,baseline models process full document,0.7448248267173767
translation,31,174,results,baseline models,process,isolated sentences,baseline models process isolated sentences,0.6704854965209961
translation,31,174,results,full document,as,single string,full document as single string,0.5615732669830322
translation,31,174,results,results,observe,dialki,results observe dialki,0.582304835319519
translation,31,175,results,our framework,leads a,smaller performance gap,our framework leads a smaller performance gap,0.6575069427490234
translation,31,175,results,smaller performance gap,between,seen and unseen examples,smaller performance gap between seen and unseen examples,0.63014155626297
translation,31,175,results,results,has,our framework,results has our framework,0.6097875237464905
translation,31,176,results,auxiliary history knowledge prediction loss ( l hist ),leads to,further improvements,auxiliary history knowledge prediction loss ( l hist ) leads to further improvements,0.6497273445129395
translation,31,176,results,further improvements,on,both datasets,further improvements on both datasets,0.49930092692375183
translation,31,176,results,results,Adding,auxiliary history knowledge prediction loss ( l hist ),results Adding auxiliary history knowledge prediction loss ( l hist ),0.6481159925460815
translation,31,178,results,posterior regularization ( l adv ),is,effective,posterior regularization ( l adv ) is effective,0.5372392535209656
translation,31,178,results,effective,on,both datasets,effective on both datasets,0.5469654202461243
translation,31,178,results,doc2 dial,gets,more advantage,doc2 dial gets more advantage,0.6714684963226318
translation,31,178,results,more advantage,especially on,unseen subset,more advantage especially on unseen subset,0.6579699516296387
translation,31,178,results,results,Adding,posterior regularization ( l adv ),results Adding posterior regularization ( l adv ),0.6626512408256531
translation,31,179,results,all model components,yields,best results,all model components yields best results,0.7030032277107239
translation,31,179,results,results,Combining,all model components,results Combining all model components,0.6557393074035645
translation,31,184,results,knowledge,predicted by,dialki,knowledge predicted by dialki,0.7761645317077637
translation,31,184,results,knowledge,leads to,almost 3 points,knowledge leads to almost 3 points,0.6751868724822998
translation,31,184,results,almost 3 points,in,sacrebleu score,almost 3 points in sacrebleu score,0.5231131911277771
translation,31,188,results,outperforms,in locating,passage,outperforms in locating passage,0.7410498261451721
translation,31,188,results,baseline models,in locating,passage,baseline models in locating passage,0.7370065450668335
translation,31,188,results,passage,containing,knowledge string,passage containing knowledge string,0.6329675912857056
translation,31,188,results,dialki,has,outperforms,dialki has outperforms,0.6569294929504395
translation,31,188,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,31,188,results,results,shows,dialki,results shows dialki,0.6315726041793823
translation,32,46,baselines,original bart large checkpoint model,on,samsum,original bart large checkpoint model on samsum,0.5632471442222595
translation,32,46,baselines,fine-tuning,has,original bart large checkpoint model,fine-tuning has original bart large checkpoint model,0.5691624283790588
translation,32,47,baselines,baselines,has,multi-view seq2seq,baselines has multi-view seq2seq,0.5426007509231567
translation,32,42,experimental-setup,experimental setup,use,"bart large architecture ( lewis et al. , 2020 )","experimental setup use bart large architecture ( lewis et al. , 2020 )",0.5900759696960449
translation,32,44,experiments,experiments,run using,"fairseq ( ott et al. , 2019 )","experiments run using fairseq ( ott et al. , 2019 )",0.7502274513244629
translation,32,58,results,rouge improvements,on,both validation and test sets,rouge improvements on both validation and test sets,0.544126033782959
translation,32,58,results,both validation and test sets,of,samsum,both validation and test sets of samsum,0.6360777020454407
translation,32,58,results,results,observe,rouge improvements,results observe rouge improvements,0.5646216869354248
translation,32,61,results,"more participants ( 7 , 8 , 12 )",exhibit,higher rouge boost,"more participants ( 7 , 8 , 12 ) exhibit higher rouge boost",0.6252287030220032
translation,32,91,results,mtl,gives,performance boost,mtl gives performance boost,0.640429675579071
translation,32,91,results,performance boost,in,almost all cases,performance boost in almost all cases,0.5477023720741272
translation,32,91,results,vanilla bart and the multi-view ss baseline,on,development and test sets,vanilla bart and the multi-view ss baseline on development and test sets,0.5590388178825378
translation,32,91,results,vanilla bart and the multi-view ss baseline,both,development and test sets,vanilla bart and the multi-view ss baseline both development and test sets,0.6615393757820129
translation,32,91,results,outperforming,has,vanilla bart and the multi-view ss baseline,outperforming has vanilla bart and the multi-view ss baseline,0.5737215280532837
translation,32,91,results,results,clear that,mtl,results clear that mtl,0.7224762439727783
translation,32,104,results,vanilla bart,on,all metrics,vanilla bart on all metrics,0.5664314031600952
translation,32,104,results,multiview ss baseline,on,test set rouge - 2 and rouge -l,multiview ss baseline on test set rouge - 2 and rouge -l,0.5302022099494934
translation,32,104,results,"best model ( personachat , word masking )",has,outperforms,"best model ( personachat , word masking ) has outperforms",0.5760400295257568
translation,32,104,results,outperforms,has,vanilla bart,outperforms has vanilla bart,0.6809602379798889
translation,32,104,results,results,has,"best model ( personachat , word masking )","results has best model ( personachat , word masking )",0.5185699462890625
translation,32,105,results,personachat,better than,pretraining,personachat better than pretraining,0.7232937216758728
translation,32,105,results,pretraining,on,personachat and reddit,pretraining on personachat and reddit,0.5990781188011169
translation,32,105,results,results,BART pretrained on,personachat,results BART pretrained on personachat,0.7074735164642334
translation,32,107,results,whole word masking,performs,slightly better,whole word masking performs slightly better,0.6157891154289246
translation,32,107,results,slightly better,than,span masking,slightly better than span masking,0.5900478363037109
translation,32,107,results,results,see that,whole word masking,results see that whole word masking,0.5364817380905151
translation,32,109,results,pretraining,using,dialoguespecific objectives,pretraining using dialoguespecific objectives,0.609320878982544
translation,32,109,results,dialoguespecific objectives,performing,well,dialoguespecific objectives performing well,0.6618483066558838
translation,32,109,results,random span masking,on,validation set,random span masking on validation set,0.5594385266304016
translation,32,109,results,even outperforming,has,random span masking,even outperforming has random span masking,0.6128261089324951
translation,32,109,results,results,see that,pretraining,results see that pretraining,0.650836706161499
translation,32,113,results,fine-tuning,on,samsum and roc,fine-tuning on samsum and roc,0.5412383675575256
translation,32,113,results,samsum and roc,gives,best performance,samsum and roc gives best performance,0.6257723569869995
translation,32,113,results,best performance,over,validation set,best performance over validation set,0.692641019821167
translation,32,113,results,outperforming,has,all other settings,outperforming has all other settings,0.5863034129142761
translation,32,113,results,results,pretraining on,personachat,results pretraining on personachat,0.7538394331932068
translation,32,114,results,test set,performing,very well,test set performing very well,0.5923434495925903
translation,32,114,results,slightly outperformed,by,multi-tasking,slightly outperformed by multi-tasking,0.5964035391807556
translation,32,114,results,multi-tasking,with,roc,multi-tasking with roc,0.6406440138816833
translation,32,114,results,roc,in,rouge - 2 and rouge -l.,roc in rouge - 2 and rouge -l.,0.5979043245315552
translation,32,114,results,results,On,test set,results On test set,0.582119882106781
translation,32,116,results,name substitution,does not give,performance boost,name substitution does not give performance boost,0.6860013604164124
translation,32,116,results,performance boost,when used in combination with,pretraining and mtl,performance boost when used in combination with pretraining and mtl,0.6193525195121765
translation,32,116,results,results,observe,name substitution,results observe name substitution,0.5996958613395691
translation,33,35,model,end,to distill,ensemble of neural belief trackers,end to distill ensemble of neural belief trackers,0.6473368406295776
translation,33,35,model,end,incorporate,additional uncertainty measures,end incorporate additional uncertainty measures,0.7304021716117859
translation,33,35,model,ensemble of neural belief trackers,into,single model,ensemble of neural belief trackers into single model,0.5556899905204773
translation,33,35,model,additional uncertainty measures,namely,confidence scores,additional uncertainty measures namely confidence scores,0.6658011674880981
translation,33,35,model,additional uncertainty measures,namely,total uncertainty ( entropy ),additional uncertainty measures namely total uncertainty ( entropy ),0.6815721392631531
translation,33,35,model,additional uncertainty measures,namely,knowledge uncertainty ( mutual information ),additional uncertainty measures namely knowledge uncertainty ( mutual information ),0.6797667145729065
translation,33,35,model,additional uncertainty measures,namely,belief state,additional uncertainty measures namely belief state,0.6618061661720276
translation,33,35,model,additional uncertainty measures,into,belief state,additional uncertainty measures into belief state,0.5377014875411987
translation,33,35,model,belief state,of,neural dialogue system,belief state of neural dialogue system,0.5873854756355286
translation,33,35,model,model,use,end,model use end,0.7003007531166077
translation,33,36,model,uncertainty - aware neural belief tracker,allows,downstream dialogue policy models,uncertainty - aware neural belief tracker allows downstream dialogue policy models,0.6317131519317627
translation,33,36,model,downstream dialogue policy models,to use,information,downstream dialogue policy models to use information,0.6868258714675903
translation,33,36,model,information,to resolve,confusion,information to resolve confusion,0.7095836997032166
translation,33,36,model,model,yields,uncertainty - aware neural belief tracker,model yields uncertainty - aware neural belief tracker,0.6829836964607239
translation,33,38,model,set similarity,for,accurate state predictions,set similarity for accurate state predictions,0.6117659211158752
translation,33,38,model,setsumbt,has,modified sumbt belief tracking model,setsumbt has modified sumbt belief tracking model,0.5611000061035156
translation,33,38,model,model,present,setsumbt,model present setsumbt,0.7194509506225586
translation,33,39,model,ensemble distribution distillation,to obtain,"well - calibrated , rich estimates of uncertainty","ensemble distribution distillation to obtain well - calibrated , rich estimates of uncertainty",0.5512208938598633
translation,33,39,model,"well - calibrated , rich estimates of uncertainty",in,dialogue belief tracker,"well - calibrated , rich estimates of uncertainty in dialogue belief tracker",0.49791836738586426
translation,33,39,model,model,deploy,ensemble distribution distillation,model deploy ensemble distribution distillation,0.7126340270042419
translation,34,35,baselines,ntrd,is,neural approach,ntrd is neural approach,0.6355089545249939
translation,34,35,baselines,ntrd,firstly generates,response,ntrd firstly generates response,0.6551153063774109
translation,34,35,baselines,neural approach,firstly generates,response,neural approach firstly generates response,0.6897892951965332
translation,34,35,baselines,template,with,slot locations,template with slot locations,0.6340293288230896
translation,34,35,baselines,slot locations,explicitly tied to,recommended items,slot locations explicitly tied to recommended items,0.7372382879257202
translation,34,35,baselines,baselines,has,ntrd,baselines has ntrd,0.5786079168319702
translation,34,155,baselines,dialogue generation model,based on,"hred ( serban et al. , 2017 )","dialogue generation model based on hred ( serban et al. , 2017 )",0.6420136094093323
translation,34,156,baselines,baselines,has,"kbrd ( chen et al. , 2019 )","baselines has kbrd ( chen et al. , 2019 )",0.5323027968406677
translation,34,159,baselines,baselines,has,"kgsf ( zhou et al. , 2020a )","baselines has kgsf ( zhou et al. , 2020a )",0.5172192454338074
translation,34,145,experimental-setup,experimental setup,implemented in,pytorch,experimental setup implemented in pytorch,0.7101436853408813
translation,34,145,experimental-setup,experimental setup,trained on,one nvidia tesla v100 32g card,experimental setup trained on one nvidia tesla v100 32g card,0.7146136164665222
translation,34,147,experimental-setup,embedding size d h,of,item,embedding size d h of item,0.5917390584945679
translation,34,147,experimental-setup,embedding size d h,set to,128,embedding size d h set to 128,0.6750209331512451
translation,34,147,experimental-setup,item,in,recommender module,item in recommender module,0.5459825396537781
translation,34,147,experimental-setup,embedding size d e,in,dialogue module,embedding size d e in dialogue module,0.5085371136665344
translation,34,147,experimental-setup,dialogue module,set to,300,dialogue module set to 300,0.6913914680480957
translation,34,147,experimental-setup,experimental setup,has,embedding size d h,experimental setup has embedding size d h,0.5491801500320435
translation,34,147,experimental-setup,experimental setup,has,embedding size d e,experimental setup has embedding size d e,0.5619317293167114
translation,34,148,experimental-setup,procedure,in,kgsf,procedure in kgsf,0.5235747694969177
translation,34,148,experimental-setup,kgsf,to pre-train,knowledge graph,kgsf to pre-train knowledge graph,0.6375799179077148
translation,34,148,experimental-setup,knowledge graph,in,recommender module,knowledge graph in recommender module,0.524808406829834
translation,34,148,experimental-setup,knowledge graph,using,mutual information maximization ( mim ) loss,knowledge graph using mutual information maximization ( mim ) loss,0.670892596244812
translation,34,148,experimental-setup,mutual information maximization ( mim ) loss,for,3 epochs,mutual information maximization ( mim ) loss for 3 epochs,0.586146354675293
translation,34,148,experimental-setup,experimental setup,follow,procedure,experimental setup follow procedure,0.5794583559036255
translation,34,150,experimental-setup,training,of,response template generator,training of response template generator,0.5713884830474854
translation,34,150,experimental-setup,special token,add it to,vocabulary,special token add it to vocabulary,0.663781464099884
translation,34,150,experimental-setup,special token,has,item ],special token has item ],0.5640770792961121
translation,34,150,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,34,152,experimental-setup,batch size,set to,32,batch size set to 32,0.733751654624939
translation,34,152,experimental-setup,gradient clipping,restricts,"[ 0 , 0.1 ]","gradient clipping restricts [ 0 , 0.1 ]",0.623054027557373
translation,34,152,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,34,153,experimental-setup,generation loss and the item selection loss,trained simultaneously with,weight ? = 5,generation loss and the item selection loss trained simultaneously with weight ? = 5,0.7255089282989502
translation,34,153,experimental-setup,experimental setup,has,generation loss and the item selection loss,experimental setup has generation loss and the item selection loss,0.5416980385780334
translation,34,151,experiments,adam optimizer,with,1e ? 3 learning rate,adam optimizer with 1e ? 3 learning rate,0.6103087067604065
translation,34,7,model,novel framework,called,ntrd,novel framework called ntrd,0.6845389008522034
translation,34,7,model,ntrd,for,recommender dialogue system,ntrd for recommender dialogue system,0.6204903721809387
translation,34,7,model,dialogue generation,from,item recommendation,dialogue generation from item recommendation,0.5502219200134277
translation,34,7,model,model,introduce,novel framework,model introduce novel framework,0.6889747381210327
translation,34,8,model,two key components,i.e.,response template generator,two key components i.e. response template generator,0.6520670652389526
translation,34,8,model,two key components,i.e.,item selector,two key components i.e. item selector,0.6492749452590942
translation,34,8,model,ntrd,has,two key components,ntrd has two key components,0.5831893682479858
translation,34,8,model,model,has,ntrd,model has ntrd,0.6266400218009949
translation,34,9,model,former,adopts,encoder-decoder model,former adopts encoder-decoder model,0.6683493852615356
translation,34,9,model,encoder-decoder model,to generate,response template,encoder-decoder model to generate response template,0.7085217237472534
translation,34,9,model,response template,with,slot locations,response template with slot locations,0.6223158240318298
translation,34,9,model,slot locations,tied to,target items,slot locations tied to target items,0.6754050254821777
translation,34,9,model,slot locations,with,proper items,slot locations with proper items,0.6064360737800598
translation,34,9,model,latter,fills in,slot locations,latter fills in slot locations,0.7275356650352478
translation,34,9,model,slot locations,with,proper items,slot locations with proper items,0.6064360737800598
translation,34,9,model,slot locations,using,sufficient attention mechanism,slot locations using sufficient attention mechanism,0.6916778087615967
translation,34,9,model,proper items,using,sufficient attention mechanism,proper items using sufficient attention mechanism,0.671879768371582
translation,34,9,model,model,has,former,model has former,0.6011738181114197
translation,34,10,model,strengths,of,classical slot filling approaches,strengths of classical slot filling approaches,0.5685863494873047
translation,34,10,model,strengths,of,modern neural nlg approaches,strengths of modern neural nlg approaches,0.5594821572303772
translation,34,34,model,neural templates,for,recommender dialogue system,neural templates for recommender dialogue system,0.610671877861023
translation,34,34,model,neural templates,i.e.,ntrd,neural templates i.e. ntrd,0.6625888347625732
translation,34,34,model,model,propose to learn,neural templates,model propose to learn neural templates,0.7548954486846924
translation,34,37,model,entire architecture ( response template generator and item selector ),trained in,endto-end manner,entire architecture ( response template generator and item selector ) trained in endto-end manner,0.7191524505615234
translation,34,37,model,model,has,entire architecture ( response template generator and item selector ),model has entire architecture ( response template generator and item selector ),0.548528254032135
translation,34,38,model,advantages,of,classical slot filling approaches,advantages of classical slot filling approaches,0.5314247012138367
translation,34,38,model,advantages,of,modern neural nlg approaches,advantages of modern neural nlg approaches,0.5152577757835388
translation,34,38,model,modern neural nlg approaches,brings,naturally sounded responses,modern neural nlg approaches brings naturally sounded responses,0.6292725205421448
translation,34,38,model,modern neural nlg approaches,brings,more flexible item recommendation,modern neural nlg approaches brings more flexible item recommendation,0.586855947971344
translation,34,87,model,response template,adopt,transformer - based network,response template adopt transformer - based network,0.6429609060287476
translation,34,87,model,transformer - based network,to model,process,transformer - based network to model process,0.69656902551651
translation,34,87,model,model,To generate,response template,model To generate response template,0.7200734615325928
translation,34,173,model,ntrd,decouples,response generation and item injection,ntrd decouples response generation and item injection,0.772527277469635
translation,34,173,model,ntrd,filling,slots,ntrd filling slots,0.7445192933082581
translation,34,173,model,response generation and item injection,by first learning,response templates,response generation and item injection by first learning response templates,0.7061230540275574
translation,34,173,model,response generation and item injection,filling,slots,response generation and item injection filling slots,0.7660362124443054
translation,34,173,model,slots,with,proper items,slots with proper items,0.651591956615448
translation,34,173,model,model,has,ntrd,model has ntrd,0.6266400218009949
translation,34,220,model,dialogue generation,from,item recommendation,dialogue generation from item recommendation,0.5502219200134277
translation,34,220,model,dialogue generation,via,two -stage strategy,dialogue generation via two -stage strategy,0.64957195520401
translation,34,220,model,ntrd,has,novel recommender dialogue framework,ntrd has novel recommender dialogue framework,0.5660871267318726
translation,34,220,model,model,introduce,ntrd,model introduce ntrd,0.6775980591773987
translation,34,221,model,classical slot filling approaches,with,modern neural nlg approaches,classical slot filling approaches with modern neural nlg approaches,0.6119421720504761
translation,34,221,model,recommender dialogue system,has,more flexible and controllable,recommender dialogue system has more flexible and controllable,0.5163630247116089
translation,34,166,results,our ntrd,obviously,better,our ntrd obviously better,0.7606092095375061
translation,34,166,results,better,on,all automatic metrics,better on all automatic metrics,0.5281844735145569
translation,34,166,results,better,compared to,baseline models,better compared to baseline models,0.6628897190093994
translation,34,166,results,all automatic metrics,compared to,baseline models,all automatic metrics compared to baseline models,0.5875211358070374
translation,34,166,results,results,see,our ntrd,results see our ntrd,0.5934497714042664
translation,34,166,results,results,has,our ntrd,results has our ntrd,0.5972240567207336
translation,34,167,results,ntrd,achieves,best performance,ntrd achieves best performance,0.7087803483009338
translation,34,167,results,best performance,on,ppl,best performance on ppl,0.5812777280807495
translation,34,167,results,results,has,ntrd,results has ntrd,0.5616890788078308
translation,34,168,results,baselines,with,large margin,baselines with large margin,0.6904941201210022
translation,34,168,results,large margin,on,dist - 2/3/4,large margin on dist - 2/3/4,0.5962989330291748
translation,34,168,results,diversity,has,ntrd,diversity has ntrd,0.5868069529533386
translation,34,168,results,ntrd,has,consistently outperforms,ntrd has consistently outperforms,0.6217522621154785
translation,34,168,results,consistently outperforms,has,baselines,consistently outperforms has baselines,0.6030921339988708
translation,34,168,results,results,In terms of,diversity,results In terms of diversity,0.6581099033355713
translation,34,168,results,results,has,ntrd,results has ntrd,0.5616890788078308
translation,34,171,results,all fleiss 's kappa values,exceed,0.6,all fleiss 's kappa values exceed 0.6,0.5922877788543701
translation,34,172,results,ntrd,performs,better,ntrd performs better,0.6942599415779114
translation,34,172,results,better,in terms of,fluency and informativeness,better in terms of fluency and informativeness,0.6520392298698425
translation,34,172,results,kgsf,has,ntrd,kgsf has ntrd,0.6553320288658142
translation,34,172,results,results,Compared to,kgsf,results Compared to kgsf,0.6830822825431824
translation,34,179,results,recommendation,based on,final produced responses,recommendation based on final produced responses,0.6870721578598022
translation,34,179,results,state - of - the - art method kgsf,performs,poorly,state - of - the - art method kgsf performs poorly,0.6475957632064819
translation,34,179,results,poorly,with,only 0.889 % rer@1,poorly with only 0.889 % rer@1,0.6521340012550354
translation,34,179,results,recommendation,has,state - of - the - art method kgsf,recommendation has state - of - the - art method kgsf,0.5528163313865662
translation,34,179,results,final produced responses,has,state - of - the - art method kgsf,final produced responses has state - of - the - art method kgsf,0.5989362597465515
translation,34,182,results,our ntrd framework,performs,significantly better,our ntrd framework performs significantly better,0.6247543096542358
translation,34,182,results,obvious advantage,of incorporating,precise items,obvious advantage of incorporating precise items,0.6175314784049988
translation,34,182,results,results,has,our ntrd framework,results has our ntrd framework,0.5893794894218445
translation,34,183,results,ntrd,achieves,highest item ratio,ntrd achieves highest item ratio,0.6588571667671204
translation,34,183,results,ntrd,achieves,item diversity,ntrd achieves item diversity,0.6547720432281494
translation,34,183,results,results,has,ntrd,results has ntrd,0.5616890788078308
translation,34,218,results,ntrd,able to provide,informative and interesting item recommendation,ntrd able to provide informative and interesting item recommendation,0.6132063865661621
translation,34,218,results,ntrd,behaves,more naturally and interactively,ntrd behaves more naturally and interactively,0.6536165475845337
translation,34,218,results,informative and interesting item recommendation,at,utterance level,informative and interesting item recommendation at utterance level,0.5272161960601807
translation,34,218,results,more naturally and interactively,at,dialogue level,more naturally and interactively at dialogue level,0.5517828464508057
translation,35,8,model,model,propose,new fine- grained post-training method,model propose new fine- grained post-training method,0.6698806881904602
translation,35,9,model,utterance level interactions,by training,every short context- response pair,utterance level interactions by training every short context- response pair,0.8035339117050171
translation,35,9,model,every short context- response pair,in,dialogue session,every short context- response pair in dialogue session,0.5440754890441895
translation,35,9,model,model,learns,utterance level interactions,model learns utterance level interactions,0.7083502411842346
translation,35,10,model,semantic relevance and coherence,between,dialogue utterances,semantic relevance and coherence between dialogue utterances,0.601614773273468
translation,35,10,model,new training objective,has,utterance relevance classification,new training objective has utterance relevance classification,0.5493484735488892
translation,35,10,model,model,by using,new training objective,model by using new training objective,0.6609089374542236
translation,35,29,model,model,by dividing,entire dialogue,model by dividing entire dialogue,0.6016262769699097
translation,35,29,model,entire dialogue,into,multiple short context- response pairs,entire dialogue into multiple short context- response pairs,0.5762953162193298
translation,35,29,model,model,train,model,model train model,0.6881781220436096
translation,35,30,model,objective,called,utterance relevance classification,objective called utterance relevance classification,0.6144457459449768
translation,35,30,model,relation,between,given utterances and the target utterance,relation between given utterances and the target utterance,0.6128799319267273
translation,35,30,model,relation,into,more fine - grained labels,relation into more fine - grained labels,0.6159979104995728
translation,37,211,ablation-analysis,performance,of,tucore - gcn robert a,performance of tucore - gcn robert a,0.6434707045555115
translation,37,211,ablation-analysis,tucore - gcn robert a,drops by,0.4 f 1 score,tucore - gcn robert a drops by 0.4 f 1 score,0.750892698764801
translation,37,211,ablation-analysis,tucore - gcn robert a,drops by,0.1 f 1 c score,tucore - gcn robert a drops by 0.1 f 1 c score,0.7311784029006958
translation,37,211,ablation-analysis,tucore - gcn robert a,drops by,0.72 and 1.65 f 1 scores,tucore - gcn robert a drops by 0.72 and 1.65 f 1 scores,0.7154983282089233
translation,37,211,ablation-analysis,0.1 f 1 c score,on,di-alogre test set,0.1 f 1 c score on di-alogre test set,0.5346404910087585
translation,37,211,ablation-analysis,0.72 and 1.65 f 1 scores,on,meld and emorynlp test set,0.72 and 1.65 f 1 scores on meld and emorynlp test set,0.5125185251235962
translation,37,211,ablation-analysis,speaker embedding,has,performance,speaker embedding has performance,0.5462239980697632
translation,37,211,ablation-analysis,ablation analysis,Without,speaker embedding,ablation analysis Without speaker embedding,0.6406152248382568
translation,37,215,ablation-analysis,performance,of,tucore - gcn,performance of tucore - gcn,0.5836418271064758
translation,37,215,ablation-analysis,tucore - gcn,RoBERT a,sharply drops,tucore - gcn RoBERT a sharply drops,0.6258209347724915
translation,37,215,ablation-analysis,sharply drops,by,1.1 f 1 score,sharply drops by 1.1 f 1 score,0.6072537899017334
translation,37,215,ablation-analysis,sharply drops,by,0.6 f 1 c score,sharply drops by 0.6 f 1 c score,0.6114234924316406
translation,37,215,ablation-analysis,sharply drops,by,0.77 and 2.17 f 1 scores,sharply drops by 0.77 and 2.17 f 1 scores,0.6235708594322205
translation,37,215,ablation-analysis,0.6 f 1 c score,on,dialogre test set,0.6 f 1 c score on dialogre test set,0.5243024826049805
translation,37,215,ablation-analysis,0.77 and 2.17 f 1 scores,on,meld and emorynlp test set,0.77 and 2.17 f 1 scores on meld and emorynlp test set,0.5158639550209045
translation,37,215,ablation-analysis,without turn attention,has,performance,without turn attention has performance,0.5706049203872681
translation,37,215,ablation-analysis,tucore - gcn,has,sharply drops,tucore - gcn has sharply drops,0.6127517223358154
translation,37,215,ablation-analysis,ablation analysis,has,without turn attention,ablation analysis has without turn attention,0.5810985565185547
translation,37,217,ablation-analysis,turn- level bilstm,for,turn nodes,turn- level bilstm for turn nodes,0.5861139893531799
translation,37,217,ablation-analysis,turn nodes,in,dialogue graph,turn nodes in dialogue graph,0.5374950170516968
translation,37,217,ablation-analysis,dialogue graph,with,sequential nodes module,dialogue graph with sequential nodes module,0.6095597147941589
translation,37,217,ablation-analysis,ablation analysis,removed,turn- level bilstm,ablation analysis removed turn- level bilstm,0.6731996536254883
translation,37,219,ablation-analysis,performance,of,tucore - gcn robert a,performance of tucore - gcn robert a,0.6434707045555115
translation,37,219,ablation-analysis,tucore - gcn robert a,drops by,0.6 f 1 score,tucore - gcn robert a drops by 0.6 f 1 score,0.751409113407135
translation,37,219,ablation-analysis,tucore - gcn robert a,drops by,0.2 f 1 c score,tucore - gcn robert a drops by 0.2 f 1 c score,0.7285311222076416
translation,37,219,ablation-analysis,tucore - gcn robert a,drops by,0.34 and 0.89 f 1 scores,tucore - gcn robert a drops by 0.34 and 0.89 f 1 scores,0.7306195497512817
translation,37,219,ablation-analysis,0.2 f 1 c score,on,dialogre test set,0.2 f 1 c score on dialogre test set,0.5189909934997559
translation,37,219,ablation-analysis,0.34 and 0.89 f 1 scores,on,meld and emorynlp test set,0.34 and 0.89 f 1 scores on meld and emorynlp test set,0.5206285119056702
translation,37,219,ablation-analysis,turn- level bilstm,has,performance,turn- level bilstm has performance,0.49462875723838806
translation,37,219,ablation-analysis,ablation analysis,Without,turn- level bilstm,ablation analysis Without turn- level bilstm,0.6979284286499023
translation,37,230,ablation-analysis,roberta s 's f 1 score difference,between,asymmetric relation group and the symmetric relation group,roberta s 's f 1 score difference between asymmetric relation group and the symmetric relation group,0.6221721172332764
translation,37,230,ablation-analysis,tucore - gcn robert a 's f 1 score difference,reduced by,2.9,tucore - gcn robert a 's f 1 score difference reduced by 2.9,0.7016986012458801
translation,37,230,ablation-analysis,roberta s 's f 1 score difference,has,tucore - gcn robert a 's f 1 score difference,roberta s 's f 1 score difference has tucore - gcn robert a 's f 1 score difference,0.5934136509895325
translation,37,230,ablation-analysis,ablation analysis,compared to,roberta s 's f 1 score difference,ablation analysis compared to roberta s 's f 1 score difference,0.6833921670913696
translation,37,146,baselines,state- of- the - art model,for,dialogre,state- of- the - art model for dialogre,0.6209101676940918
translation,37,146,baselines,"gdpnet ( xue et al. , 2021 )",has,state- of- the - art model,"gdpnet ( xue et al. , 2021 ) has state- of- the - art model",0.5298920273780823
translation,37,146,baselines,baselines,has,"gdpnet ( xue et al. , 2021 )","baselines has gdpnet ( xue et al. , 2021 )",0.5209707617759705
translation,37,147,baselines,gdpnet,finds,indicative words,gdpnet finds indicative words,0.6322678923606873
translation,37,147,baselines,gdpnet,refining,graph,gdpnet refining graph,0.7230684161186218
translation,37,147,baselines,indicative words,from,long sequences,indicative words from long sequences,0.53169846534729
translation,37,147,baselines,indicative words,by constructing,latent multi-view graph,indicative words by constructing latent multi-view graph,0.6382697820663452
translation,37,147,baselines,indicative words,refining,graph,indicative words refining graph,0.7412171959877014
translation,37,147,baselines,baselines,has,gdpnet,baselines has gdpnet,0.5793147683143616
translation,37,187,baselines,roberta r,initialized with,pretrained parameters,roberta r initialized with pretrained parameters,0.8063594698905945
translation,37,187,baselines,roberta baseline,for,erc,roberta baseline for erc,0.6436348557472229
translation,37,187,baselines,pretrained parameters,of,roberta - large,pretrained parameters of roberta - large,0.5705615282058716
translation,37,187,baselines,roberta r,has,roberta baseline,roberta r has roberta baseline,0.5817660093307495
translation,37,187,baselines,baselines,has,roberta r,baselines has roberta r,0.5991511940956116
translation,37,190,baselines,state- of- the - art model,for,dailydialog,state- of- the - art model for dailydialog,0.6269773244857788
translation,37,190,baselines,state- of- the - art model,uses,conditional ran-field layer,state- of- the - art model uses conditional ran-field layer,0.5637527704238892
translation,37,190,baselines,dailydialog,uses,conditional ran-field layer,dailydialog uses conditional ran-field layer,0.6318092346191406
translation,37,190,baselines,"cesta ( wang et al. , 2020 )",has,state- of- the - art model,"cesta ( wang et al. , 2020 ) has state- of- the - art model",0.5452576875686646
translation,37,190,baselines,baselines,has,"cesta ( wang et al. , 2020 )","baselines has cesta ( wang et al. , 2020 )",0.5475256443023682
translation,37,132,experiments,two versions,of,tucore - gcn,two versions of tucore - gcn,0.6110728979110718
translation,37,132,experiments,two versions,of,tucore -gcn bert,two versions of tucore -gcn bert,0.6286072134971619
translation,37,132,experiments,two versions,of,tucore -gcn robert a,two versions of tucore -gcn robert a,0.6284800171852112
translation,37,132,experiments,two versions,based on,uncased base model,two versions based on uncased base model,0.7032526731491089
translation,37,132,experiments,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,37,132,experiments,optimizer,with,weight decay 0.01,optimizer with weight decay 0.01,0.615176796913147
translation,37,157,experiments,roberta - large,achieves,state - of - the - art performance,roberta - large achieves state - of - the - art performance,0.680931806564331
translation,37,157,experiments,state - of - the - art performance,on,dialogre dataset,state - of - the - art performance on dialogre dataset,0.525959849357605
translation,37,157,experiments,dialogre dataset,with,f 1 score 73.1,dialogre dataset with f 1 score 73.1,0.5981097221374512
translation,37,157,experiments,dialogre dataset,with,f 1 c score 65.9,dialogre dataset with f 1 c score 65.9,0.6216388940811157
translation,37,200,experiments,tucore - gcn robert a,shows,slightly lower performance,tucore - gcn robert a shows slightly lower performance,0.6731773018836975
translation,37,200,experiments,slightly lower performance,than,cesta,slightly lower performance than cesta,0.6003689765930176
translation,37,200,experiments,cesta,on,dailydialog dataset,cesta on dailydialog dataset,0.5750316977500916
translation,37,6,model,turn context aware graph convolutional network ( tucore - gcn ),paying attention to,way people understand dialogues,turn context aware graph convolutional network ( tucore - gcn ) paying attention to way people understand dialogues,0.6593660116195679
translation,37,6,model,model,propose,turn context aware graph convolutional network ( tucore - gcn ),model propose turn context aware graph convolutional network ( tucore - gcn ),0.6360303163528442
translation,37,7,model,emotion recognition in conversations ( erc ),as,dialoguebased re,emotion recognition in conversations ( erc ) as dialoguebased re,0.5523388385772705
translation,37,34,model,turn context aware graph convolutional network ( tucore - gcn ),for,dialogue - based re,turn context aware graph convolutional network ( tucore - gcn ) for dialogue - based re,0.666053295135498
translation,37,34,model,model,propose,turn context aware graph convolutional network ( tucore - gcn ),model propose turn context aware graph convolutional network ( tucore - gcn ),0.6360303163528442
translation,37,36,model,tucore - gcn,encodes,input sequence,tucore - gcn encodes input sequence,0.6841663718223572
translation,37,36,model,input sequence,to reflect,speaker information,input sequence to reflect speaker information,0.6219238042831421
translation,37,36,model,speaker information,in,dialogue,speaker information in dialogue,0.5386844873428345
translation,37,36,model,dialogue,by applying,"bert s ( yu et al. , 2020 )","dialogue by applying bert s ( yu et al. , 2020 )",0.703345775604248
translation,37,36,model,speaker embedding,of,"sa - bert ( gu et al. , 2020 )","speaker embedding of sa - bert ( gu et al. , 2020 )",0.5561380982398987
translation,37,36,model,model,has,tucore - gcn,model has tucore - gcn,0.5975062251091003
translation,37,38,model,tucore - gcn,constructs,heterogeneous dialogue graph,tucore - gcn constructs heterogeneous dialogue graph,0.626210629940033
translation,37,38,model,heterogeneous dialogue graph,to capture,relational information,heterogeneous dialogue graph to capture relational information,0.6458320617675781
translation,37,38,model,relational information,between,arguments,relational information between arguments,0.6675395369529724
translation,37,38,model,arguments,in,dialogue,arguments in dialogue,0.597180962562561
translation,37,38,model,model,has,tucore - gcn,model has tucore - gcn,0.5975062251091003
translation,37,186,model,some information,reflecting,relation types,some information reflecting relation types,0.697289228439331
translation,37,186,model,relation types,through,relational position encodings,relation types through relational position encodings,0.6627990007400513
translation,37,186,model,relational position encodings,can capture,speaker dependency and sequential information,relational position encodings can capture speaker dependency and sequential information,0.6657783389091492
translation,37,186,model,model,provided with,some information,model provided with some information,0.5932394862174988
translation,37,222,model,tucore - gcn,for,dialogue - based re,tucore - gcn for dialogue - based re,0.6848369240760803
translation,37,222,model,model,propose,tucore - gcn,model propose tucore - gcn,0.6903391480445862
translation,37,9,results,state - of - theart models,on,most of the benchmark datasets,state - of - theart models on most of the benchmark datasets,0.4716336727142334
translation,37,9,results,tucore - gcn,has,outperforms,tucore - gcn has outperforms,0.6404736638069153
translation,37,9,results,outperforms,has,state - of - theart models,outperforms has state - of - theart models,0.5775024890899658
translation,37,151,results,performance,of,tucore - gcn,performance of tucore - gcn,0.5836418271064758
translation,37,151,results,tucore - gcn,on,dialogre dataset,tucore - gcn on dialogre dataset,0.5399332642555237
translation,37,151,results,results,show,performance,results show performance,0.6846795082092285
translation,37,152,results,models,using,bert,models using bert,0.7283592224121094
translation,37,152,results,outperforms,by,2.9 ? 7.1 f 1 c scores,outperforms by 2.9 ? 7.1 f 1 c scores,0.6103948950767517
translation,37,152,results,outperforms,on,test set,outperforms on test set,0.5777032375335693
translation,37,152,results,all baselines,by,5.3 ? 7.6 f 1 scores,all baselines by 5.3 ? 7.6 f 1 scores,0.5695277452468872
translation,37,152,results,all baselines,by,2.9 ? 7.1 f 1 c scores,all baselines by 2.9 ? 7.1 f 1 c scores,0.5539340376853943
translation,37,152,results,all baselines,on,test set,all baselines on test set,0.5255734324455261
translation,37,152,results,2.9 ? 7.1 f 1 c scores,on,test set,2.9 ? 7.1 f 1 c scores on test set,0.5214686989784241
translation,37,152,results,models,has,tucore - gcn bert,models has tucore - gcn bert,0.6288071870803833
translation,37,152,results,bert,has,tucore - gcn bert,bert has tucore - gcn bert,0.660765528678894
translation,37,152,results,tucore - gcn bert,has,outperforms,tucore - gcn bert has outperforms,0.677166223526001
translation,37,152,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,37,152,results,results,Among,models,results Among models,0.6251620650291443
translation,37,153,results,gdpnet,achieved,highperformance improvement,gdpnet achieved highperformance improvement,0.7234505414962769
translation,37,153,results,state- of - the - art model,achieved,highperformance improvement,state- of - the - art model achieved highperformance improvement,0.6520320773124695
translation,37,153,results,highperformance improvement,at,f 1 c,highperformance improvement at f 1 c,0.5836540460586548
translation,37,153,results,highperformance improvement,at,both f 1 and f 1 c,highperformance improvement at both f 1 and f 1 c,0.5773236155509949
translation,37,153,results,highperformance improvement,at,both f 1 and f 1 c,highperformance improvement at both f 1 and f 1 c,0.5773236155509949
translation,37,153,results,tucore - gcn,showed,high- performance improvement,tucore - gcn showed high- performance improvement,0.6663218140602112
translation,37,153,results,high- performance improvement,at,both f 1 and f 1 c,high- performance improvement at both f 1 and f 1 c,0.5732517242431641
translation,37,153,results,gdpnet,has,state- of - the - art model,gdpnet has state- of - the - art model,0.5570521950721741
translation,37,153,results,results,has,gdpnet,results has gdpnet,0.5488812327384949
translation,37,154,results,models,using,roberta,models using roberta,0.7007377743721008
translation,37,154,results,tucore - gcn robert a,yields,great improvement,tucore - gcn robert a yields great improvement,0.7201094031333923
translation,37,154,results,great improvement,of,f 1 / f 1 c,great improvement of f 1 / f 1 c,0.6362856030464172
translation,37,154,results,great improvement,on,test set,great improvement on test set,0.5429145097732544
translation,37,154,results,great improvement,by,1.8/2.2,great improvement by 1.8/2.2,0.5534077882766724
translation,37,154,results,great improvement,in comparison with,strong baseline roberta s,great improvement in comparison with strong baseline roberta s,0.6766104102134705
translation,37,154,results,test set,by,1.8/2.2,test set by 1.8/2.2,0.5758267641067505
translation,37,154,results,models,has,tucore - gcn robert a,models has tucore - gcn robert a,0.6284923553466797
translation,37,154,results,roberta,has,tucore - gcn robert a,roberta has tucore - gcn robert a,0.5806926488876343
translation,37,154,results,results,Among,models,results Among models,0.6251620650291443
translation,37,156,results,tucore - gcn,show,outstanding performance,tucore - gcn show outstanding performance,0.6511474251747131
translation,37,156,results,bert - base,used as,encoder,bert - base used as encoder,0.669009268283844
translation,37,156,results,results,has,tucore - gcn,results has tucore - gcn,0.5515173673629761
translation,37,168,results,improved performance,in,all groups,improved performance in all groups,0.4941823184490204
translation,37,168,results,all groups,compared to,bert and bert s,all groups compared to bert and bert s,0.747295081615448
translation,37,168,results,bert and bert s,especially for,asymmetric inverse relations,bert and bert s especially for asymmetric inverse relations,0.6277503967285156
translation,37,168,results,tucore -gcn bert,has,improved performance,tucore -gcn bert has improved performance,0.614689826965332
translation,37,168,results,results,has,tucore -gcn bert,results has tucore -gcn bert,0.5685886144638062
translation,37,193,results,performance,of,tucore - gcn,performance of tucore - gcn,0.5836418271064758
translation,37,193,results,tucore - gcn,on,erc datasets,tucore - gcn on erc datasets,0.5518826842308044
translation,37,193,results,tucore - gcn,in comparison with,other baselines,tucore - gcn in comparison with other baselines,0.6614136695861816
translation,37,193,results,results,show,performance,results show performance,0.6846795082092285
translation,37,194,results,performance,of,tucore - gcn bert ( f 1 ( ? ) ),performance of tucore - gcn bert ( f 1 ( ? ) ),0.6221861243247986
translation,37,194,results,performance,of,tucore - gcn robert a,performance of tucore - gcn robert a,0.6434707045555115
translation,37,194,results,performance,of,tucore - gcn robert a,performance of tucore - gcn robert a,0.6434707045555115
translation,37,194,results,tucore - gcn bert ( f 1 ( ? ) ),on,development sets,tucore - gcn bert ( f 1 ( ? ) ) on development sets,0.6034018993377686
translation,37,194,results,development sets,of,"meld , emorynlp , and dailydialog","development sets of meld , emorynlp , and dailydialog",0.591002345085144
translation,37,194,results,"meld , emorynlp , and dailydialog",is,"59.75 ( 0.5 ) , 37.95 ( 0.8 ) , 60.25 ( 0.4 )","meld , emorynlp , and dailydialog is 59.75 ( 0.5 ) , 37.95 ( 0.8 ) , 60.25 ( 0.4 )",0.4917759895324707
translation,37,194,results,performance,of,tucore - gcn robert a,performance of tucore - gcn robert a,0.6434707045555115
translation,37,194,results,tucore - gcn robert a,is,"65.94 ( 0.5 ) , 40.17 ( 0.6 ) , 62.83 ( 0.5 )","tucore - gcn robert a is 65.94 ( 0.5 ) , 40.17 ( 0.6 ) , 62.83 ( 0.5 )",0.5634307265281677
translation,37,198,results,previous state - of- the - art model,for,meld and emorynlp,previous state - of- the - art model for meld and emorynlp,0.5652037262916565
translation,37,198,results,previous state - of- the - art model,by,"0.15 , 1.13 , and 3.43","previous state - of- the - art model by 0.15 , 1.13 , and 3.43",0.5393223166465759
translation,37,198,results,meld and emorynlp,by,"0.15 , 1.13 , and 3.43","meld and emorynlp by 0.15 , 1.13 , and 3.43",0.5847887396812439
translation,37,198,results,"0.15 , 1.13 , and 3.43",on,test sets,"0.15 , 1.13 , and 3.43 on test sets",0.5482542514801025
translation,37,198,results,test sets,of,"meld , emorynlp , and dailydialog","test sets of meld , emorynlp , and dailydialog",0.580848753452301
translation,37,198,results,tucore -gcn robert a,has,outperforms,tucore -gcn robert a has outperforms,0.6348037719726562
translation,37,198,results,outperforms,has,cosmic,outperforms has cosmic,0.6336317658424377
translation,37,198,results,outperforms,has,previous state - of- the - art model,outperforms has previous state - of- the - art model,0.5479778051376343
translation,37,198,results,cosmic,has,previous state - of- the - art model,cosmic has previous state - of- the - art model,0.5744377374649048
translation,37,198,results,results,has,tucore -gcn robert a,results has tucore -gcn robert a,0.5870671272277832
translation,38,190,ablation-analysis,ablation analysis,Engaging in,small talk,ablation analysis Engaging in small talk,0.5614609122276306
translation,38,193,ablation-analysis,proself strategies,show,general pattern,proself strategies show general pattern,0.5664811134338379
translation,38,193,ablation-analysis,general pattern,to predict,lower,general pattern to predict lower,0.7870414853096008
translation,38,193,ablation-analysis,satisfaction and likeness ratings,for,self and the partner,satisfaction and likeness ratings for self and the partner,0.6198971271514893
translation,38,193,ablation-analysis,lower,has,satisfaction and likeness ratings,lower has satisfaction and likeness ratings,0.5571764707565308
translation,38,193,ablation-analysis,ablation analysis,has,proself strategies,ablation analysis has proself strategies,0.5692776441574097
translation,38,197,ablation-analysis,dialogue behavior,of,negotiator,dialogue behavior of negotiator,0.5545444488525391
translation,38,197,ablation-analysis,dialogue behavior,significantly relates to,behavior of their opponent,dialogue behavior significantly relates to behavior of their opponent,0.6936875581741333
translation,38,197,ablation-analysis,negotiator,significantly relates to,behavior of their opponent,negotiator significantly relates to behavior of their opponent,0.6567878723144531
translation,38,197,ablation-analysis,ablation analysis,observe,dialogue behavior,ablation analysis observe dialogue behavior,0.6167082786560059
translation,38,198,ablation-analysis,our findings,show,prosocial strategies,our findings show prosocial strategies,0.5435957908630371
translation,38,198,ablation-analysis,prosocial strategies,associated with,prosocial behavior,prosocial strategies associated with prosocial behavior,0.6250940561294556
translation,38,198,ablation-analysis,prosocial strategies,achieve,more favorable outcomes,prosocial strategies achieve more favorable outcomes,0.5866524577140808
translation,38,198,ablation-analysis,more likely,associated with,prosocial behavior,more likely associated with prosocial behavior,0.639965295791626
translation,38,198,ablation-analysis,prosocial behavior,in,opponents,prosocial behavior in opponents,0.43857085704803467
translation,38,198,ablation-analysis,more favorable outcomes,in,our negotiation scenario,more favorable outcomes in our negotiation scenario,0.5247849822044373
translation,38,198,ablation-analysis,more favorable outcomes,to,proself,more favorable outcomes to proself,0.551861584186554
translation,38,198,ablation-analysis,ablation analysis,has,our findings,ablation analysis has our findings,0.5671295523643494
translation,38,231,experimental-setup,idpt,fine- tune,bert,idpt fine- tune bert,0.7042059302330017
translation,38,231,experimental-setup,bert,for,20 epochs,bert for 20 epochs,0.6504294872283936
translation,38,231,experimental-setup,bert,using,masking probability,bert using masking probability,0.7380716800689697
translation,38,231,experimental-setup,masking probability,of,0.3,masking probability of 0.3,0.6194120645523071
translation,38,231,experimental-setup,experimental setup,For,idpt,experimental setup For idpt,0.6010282635688782
translation,38,235,experimental-setup,embedding dimension,is,768,embedding dimension is 768,0.6228898763656616
translation,38,235,experimental-setup,768,for,encoder and the task -specific self-attention layers,768 for encoder and the task -specific self-attention layers,0.5956786870956421
translation,38,235,experimental-setup,experimental setup,has,embedding dimension,experimental setup has embedding dimension,0.5115625262260437
translation,38,236,experimental-setup,turn position embeddings,of,32 dimensions,turn position embeddings of 32 dimensions,0.5962557792663574
translation,38,236,experimental-setup,experimental setup,use,turn position embeddings,experimental setup use turn position embeddings,0.613927960395813
translation,38,237,experimental-setup,models,with,adam optimizer,models with adam optimizer,0.5802606344223022
translation,38,237,experimental-setup,models,with,learning rate,models with learning rate,0.593966007232666
translation,38,237,experimental-setup,models,with,weight decay,models with weight decay,0.6393474340438843
translation,38,237,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,38,237,experimental-setup,adam optimizer,with,weight decay,adam optimizer with weight decay,0.6082910299301147
translation,38,237,experimental-setup,learning rate,of,5e ?05,learning rate of 5e ?05,0.6102628707885742
translation,38,237,experimental-setup,weight decay,of,0.01,weight decay of 0.01,0.603249192237854
translation,38,237,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,38,238,experimental-setup,relu activation,for,feed -forward layers,relu activation for feed -forward layers,0.5762012600898743
translation,38,238,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,38,238,experimental-setup,0.1,to prevent,overfitting,0.1 to prevent overfitting,0.5516940355300903
translation,38,238,experimental-setup,experimental setup,use,relu activation,experimental setup use relu activation,0.6283796429634094
translation,38,238,experimental-setup,experimental setup,use,dropout,experimental setup use dropout,0.5759405493736267
translation,38,239,experimental-setup,maximum of 720 iterations,with,batch size,maximum of 720 iterations with batch size,0.6078979969024658
translation,38,239,experimental-setup,batch size,of,64 ( ? 13 epochs ),batch size of 64 ( ? 13 epochs ),0.5719982385635376
translation,38,239,experimental-setup,experimental setup,trained for,maximum of 720 iterations,experimental setup trained for maximum of 720 iterations,0.7143127918243408
translation,38,369,experimental-setup,experiments,performed on,single nvidia tesla v100 gpu,experiments performed on single nvidia tesla v100 gpu,0.6151320934295654
translation,38,369,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,38,370,experimental-setup,training,takes,two hours,training takes two hours,0.644727885723114
translation,38,370,experimental-setup,two hours,to complete,single model,two hours to complete single model,0.6736909747123718
translation,38,370,experimental-setup,single model,on,all the crossvalidation folds,single model on all the crossvalidation folds,0.5320817828178406
translation,38,370,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,38,376,experimental-setup,learning rate,in,"{ 3e ?5 , 4e ?5 , 5e ?5 }","learning rate in { 3e ?5 , 4e ?5 , 5e ?5 }",0.5224112868309021
translation,38,376,experimental-setup,weight decay,in,"{ 0.0 , 0.01 , 0.001 }","weight decay in { 0.0 , 0.01 , 0.001 }",0.5351832509040833
translation,38,376,experimental-setup,dropout,in,"{ 0.0 , 0.1 , 0.2 , 0.3 }","dropout in { 0.0 , 0.1 , 0.2 , 0.3 }",0.5855093598365784
translation,38,376,experimental-setup,experimental setup,vary,learning rate,experimental setup vary learning rate,0.666426420211792
translation,38,376,experimental-setup,experimental setup,vary,weight decay,experimental setup vary weight decay,0.6651346683502197
translation,38,377,experimental-setup,rest of the hyperparameters,fixed based on,available computational and space resources,rest of the hyperparameters fixed based on available computational and space resources,0.7095160484313965
translation,38,377,experimental-setup,experimental setup,has,rest of the hyperparameters,experimental setup has rest of the hyperparameters,0.5013777613639832
translation,38,31,model,tractable closed - domain abstraction,from,negotiation literature,tractable closed - domain abstraction from negotiation literature,0.528176486492157
translation,38,31,model,tractable closed - domain abstraction,infused with,real-world camping scenario,tractable closed - domain abstraction infused with real-world camping scenario,0.6253970265388489
translation,38,31,model,real-world camping scenario,resulting in,rich dialogues,real-world camping scenario resulting in rich dialogues,0.6502314805984497
translation,38,31,model,model,based on,tractable closed - domain abstraction,model based on tractable closed - domain abstraction,0.5825462937355042
translation,38,217,model,each unannotated dialogue,as,separate sequence,each unannotated dialogue as separate sequence,0.5517643094062805
translation,38,217,model,each unannotated dialogue,fine- tune,bert - base architecture,each unannotated dialogue fine- tune bert - base architecture,0.7052993178367615
translation,38,217,model,bert - base architecture,on,masked language modelling ( mlm ) objective,bert - base architecture on masked language modelling ( mlm ) objective,0.5246081948280334
translation,38,217,model,model,consider,each unannotated dialogue,model consider each unannotated dialogue,0.7462537884712219
translation,38,217,model,model,fine- tune,bert - base architecture,model fine- tune bert - base architecture,0.6917954683303833
translation,38,227,model,logistic regression model,for,each label,logistic regression model for each label,0.6046907901763916
translation,38,227,model,each label,based on,bag-of-words feature representation,each label based on bag-of-words feature representation,0.5909600257873535
translation,38,227,model,bag-of-words feature representation,of,input utterance,bag-of-words feature representation of input utterance,0.5308130383491516
translation,38,227,model,model,implement,logistic regression model,model implement logistic regression model,0.5954656600952148
translation,38,10,results,performance,for,all strategy labels,performance for all strategy labels,0.6171457767486572
translation,38,10,results,multi-task learning,has,substantially improves,multi-task learning has substantially improves,0.5603711605072021
translation,38,10,results,substantially improves,has,performance,substantially improves has performance,0.5890060663223267
translation,38,10,results,results,find that,multi-task learning,results find that multi-task learning,0.583889901638031
translation,38,189,results,prosocial strategies,shows,general pattern,prosocial strategies shows general pattern,0.5712311267852783
translation,38,189,results,general pattern,to predict,higher ratings,general pattern to predict higher ratings,0.7246792316436768
translation,38,189,results,higher ratings,for,subjective measures of satisfaction and likeness,higher ratings for subjective measures of satisfaction and likeness,0.5691742300987244
translation,38,189,results,self,has,as well as the partner,self has as well as the partner,0.5896780490875244
translation,38,189,results,results,greater use of,prosocial strategies,results greater use of prosocial strategies,0.5975532531738281
translation,38,243,results,majority baseline,fails to recognize,any of the strategies,majority baseline fails to recognize any of the strategies,0.729462206363678
translation,38,243,results,majority baseline,data being skewed towards,negative class,majority baseline data being skewed towards negative class,0.7375050187110901
translation,38,243,results,results,has,majority baseline,results has majority baseline,0.5583981275558472
translation,39,153,ablation-analysis,in - text pcr,adding,topic information,in - text pcr adding topic information,0.7477176189422607
translation,39,153,ablation-analysis,topic information,slightly influences,precision,topic information slightly influences precision,0.7865498661994934
translation,39,153,ablation-analysis,significantly improving,has,recall,significantly improving has recall,0.59623122215271
translation,39,153,ablation-analysis,ablation analysis,for,in - text pcr,ablation analysis for in - text pcr,0.6348894238471985
translation,39,174,ablation-analysis,all components,contribute to,ultimate success,all components contribute to ultimate success,0.6859272122383118
translation,39,174,ablation-analysis,ablation analysis,see that,all components,ablation analysis see that all components,0.6192435622215271
translation,39,175,ablation-analysis,drops,when removing,topic prediction loss,drops when removing topic prediction loss,0.6257801055908203
translation,39,175,ablation-analysis,topic prediction loss,as,regularization,topic prediction loss as regularization,0.4388866126537323
translation,39,175,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,39,175,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,39,178,ablation-analysis,joint training,of,both the in - text and out - of- text pcr,joint training of both the in - text and out - of- text pcr,0.548764705657959
translation,39,178,ablation-analysis,performance drop,on,both tasks,performance drop on both tasks,0.5160791277885437
translation,39,126,baselines,topic prediction module,basis of,end-to - end coreference resolution models,topic prediction module basis of end-to - end coreference resolution models,0.6429758071899414
translation,39,126,baselines,end-to - end coreference resolution models,contains,local similarity score module,end-to - end coreference resolution models contains local similarity score module,0.585015594959259
translation,39,126,baselines,end-to - end model,with,lstm,end-to - end model with lstm,0.6080536842346191
translation,39,126,baselines,lstm,based on,elmo embedding,lstm based on elmo embedding,0.6058299541473389
translation,39,144,experimental-setup,n tp,set to,40,n tp set to 40,0.753340482711792
translation,39,144,experimental-setup,40,for,lda,40 for lda,0.5861578583717346
translation,39,144,experimental-setup,number of topics,has,n tp,number of topics has n tp,0.59245765209198
translation,39,144,experimental-setup,experimental setup,has,number of topics,experimental setup has number of topics,0.5466199517250061
translation,39,8,model,local context and global topics,of,dialogues,local context and global topics of dialogues,0.5816940069198608
translation,39,8,model,dialogues,to solve,out- of- text pcr problem,dialogues to solve out- of- text pcr problem,0.674110472202301
translation,39,8,model,model,jointly leverage,local context and global topics,model jointly leverage local context and global topics,0.7713116407394409
translation,39,34,model,larger scores,to,objects,larger scores to objects,0.598526656627655
translation,39,34,model,objects,which are,more relevant,objects which are more relevant,0.6371340155601501
translation,39,34,model,more relevant,to,topics,more relevant to topics,0.5406811833381653
translation,39,34,model,model,first identifies,overall dialogue topics,model first identifies overall dialogue topics,0.7344526648521423
translation,39,34,model,model,assign,larger scores,model assign larger scores,0.7106062769889832
translation,39,50,model,exophora,in,vis - pro,exophora in vis - pro,0.5846544504165649
translation,39,50,model,exophora,as,only input,exophora as only input,0.4913525879383087
translation,39,50,model,vis - pro,with,texts,vis - pro with texts,0.7317656874656677
translation,39,50,model,model,resolve,exophora,model resolve exophora,0.6411092281341553
translation,39,145,model,topic prediction module,in,model,topic prediction module in model,0.49080318212509155
translation,39,145,model,topic prediction module,contains,one hidden layer,topic prediction module contains one hidden layer,0.6057410836219788
translation,39,145,model,one hidden layer,size,"1,000","one hidden layer size 1,000",0.7524797916412354
translation,39,145,model,model,has,topic prediction module,model has topic prediction module,0.5549010634422302
translation,39,151,results,bert and spanbert based models,has,outperform,bert and spanbert based models has outperform,0.6366050243377686
translation,39,151,results,outperform,has,elmo - lstm based models,outperform has elmo - lstm based models,0.5941086411476135
translation,39,152,results,global topics,improves,recall,global topics improves recall,0.7644420266151428
translation,39,152,results,recall,for,exophoric and endophoric pronouns,recall for exophoric and endophoric pronouns,0.6135042905807495
translation,39,152,results,results,incorporating,global topics,results incorporating global topics,0.6805542707443237
translation,39,169,results,proposed model,incorporates,overall topics,proposed model incorporates overall topics,0.7327693104743958
translation,39,169,results,proposed model,boosts,performance,proposed model boosts performance,0.7230547666549683
translation,39,169,results,performance,by,large margin,performance by large margin,0.6227608919143677
translation,39,169,results,large margin,especially on,infrequent pronouns,large margin especially on infrequent pronouns,0.6536589860916138
translation,39,169,results,local information,has,proposed model,local information has proposed model,0.5692682266235352
translation,39,180,results,tion,achieving,comparable scores,tion achieving comparable scores,0.6863366961479187
translation,39,180,results,consistently improves,achieving,comparable scores,consistently improves achieving comparable scores,0.6558341979980469
translation,39,180,results,performance,on,outof-text pcr,performance on outof-text pcr,0.5883980393409729
translation,39,180,results,outof-text pcr,for,all models,outof-text pcr for all models,0.6159032583236694
translation,39,180,results,comparable scores,on,in - text one,comparable scores on in - text one,0.5840106010437012
translation,39,180,results,tion,has,consistently improves,tion has consistently improves,0.6275318264961243
translation,39,180,results,consistently improves,has,performance,consistently improves has performance,0.5987896919250488
translation,39,180,results,results,has,tion,results has tion,0.573542058467865
translation,39,180,results,results,has,consistently improves,results has consistently improves,0.6220737099647522
translation,39,181,results,results,compared to,bert - base,results compared to bert - base,0.6747634410858154
translation,40,7,model,model,propose,zero-shot dialogue disentanglement solution,model propose zero-shot dialogue disentanglement solution,0.6845553517341614
translation,40,8,model,model,on,multi-participant response selection dataset,model on multi-participant response selection dataset,0.5262426137924194
translation,40,8,model,model,apply,trained model,model apply trained model,0.6306142807006836
translation,40,8,model,multi-participant response selection dataset,harvested from,web,multi-participant response selection dataset harvested from web,0.7248067259788513
translation,40,8,model,trained model,to perform,zero-shot dialogue disentanglement,trained model to perform zero-shot dialogue disentanglement,0.6548879742622375
translation,40,8,model,model,train,model,model train model,0.6881781220436096
translation,40,9,results,our model,achieve,cluster f1 score,our model achieve cluster f1 score,0.6194888353347778
translation,40,9,results,cluster f1 score,of,25,cluster f1 score of 25,0.6665952801704407
translation,40,9,results,labeled data,has,our model,labeled data has our model,0.5599310994148254
translation,40,9,results,results,Without,labeled data,results Without labeled data,0.6935657858848572
translation,40,60,results,performance,of,our approach,performance of our approach,0.5712711215019226
translation,40,60,results,performance,comparable to,previous work,performance comparable to previous work,0.6660481691360474
translation,40,60,results,our approach,comparable to,previous work,our approach comparable to previous work,0.6553454995155334
translation,40,60,results,results,has,performance,results has performance,0.5972660779953003
translation,40,103,results,10 %,of,data,10 % of data,0.6686494946479797
translation,40,103,results,10 %,achieve,92 %,10 % achieve 92 %,0.7124189734458923
translation,40,103,results,92 %,of,performance,92 % of performance,0.5949466824531555
translation,40,103,results,performance,trained using,full data,performance trained using full data,0.6974627375602722
translation,40,103,results,results,With,10 %,results With 10 %,0.6599565744400024
translation,41,107,baselines,saclog,to,classical rnn - based generative dst model,saclog to classical rnn - based generative dst model,0.5338472723960876
translation,41,63,experiments,language modelling ( lm ) loss,as,auxiliary loss l aux,language modelling ( lm ) loss as auxiliary loss l aux,0.49611029028892517
translation,41,63,experiments,auxiliary loss l aux,to learn,contextual representations,auxiliary loss l aux to learn contextual representations,0.5973904728889465
translation,41,63,experiments,contextual representations,of,natural language,contextual representations of natural language,0.4606482684612274
translation,41,62,hyperparameters,pre-trained corpus,constructed from,multiwoz2.1 dialogs,pre-trained corpus constructed from multiwoz2.1 dialogs,0.6378217339515686
translation,41,62,hyperparameters,pre-trained corpus,constructed from,"off- the-shelf synthesized dialogs ( campagna et al. , 2020 )","pre-trained corpus constructed from off- the-shelf synthesized dialogs ( campagna et al. , 2020 )",0.642087996006012
translation,41,62,hyperparameters,"off- the-shelf synthesized dialogs ( campagna et al. , 2020 )",contains,"337,346 dialog data","off- the-shelf synthesized dialogs ( campagna et al. , 2020 ) contains 337,346 dialog data",0.6033734679222107
translation,41,62,hyperparameters,hyperparameters,has,pre-trained corpus,hyperparameters has pre-trained corpus,0.5068570971488953
translation,41,90,hyperparameters,bert base,as,encoder,bert base as encoder,0.6009477972984314
translation,41,90,hyperparameters,bert base,as,encoder,bert base as encoder,0.6009477972984314
translation,41,90,hyperparameters,bert base,as,encoder,bert base as encoder,0.6009477972984314
translation,41,90,hyperparameters,[ cls ] embedding,as,slot embedding,[ cls ] embedding as slot embedding,0.5581044554710388
translation,41,90,hyperparameters,slot embedding,in,trippy,slot embedding in trippy,0.5781533122062683
translation,41,90,hyperparameters,slot embedding,for,trade,slot embedding for trade,0.6582711338996887
translation,41,90,hyperparameters,bi-gru,as,encoder,bi-gru as encoder,0.5968528389930725
translation,41,90,hyperparameters,concatenation,of,first and last hidden state,concatenation of first and last hidden state,0.6074246168136597
translation,41,90,hyperparameters,first and last hidden state,as,slot embedding,first and last hidden state as slot embedding,0.5436073541641235
translation,41,90,hyperparameters,slot embedding,for,trade,slot embedding for trade,0.6582711338996887
translation,41,90,hyperparameters,hyperparameters,use,bert base,hyperparameters use bert base,0.6331146955490112
translation,41,90,hyperparameters,hyperparameters,use,bi-gru,hyperparameters use bi-gru,0.6452304124832153
translation,41,91,hyperparameters,trippy,to add,2 new slot operations ( i.e. refer/ dontcare ),trippy to add 2 new slot operations ( i.e. refer/ dontcare ),0.6946759819984436
translation,41,91,hyperparameters,2 new slot operations ( i.e. refer/ dontcare ),into,classification types,2 new slot operations ( i.e. refer/ dontcare ) into classification types,0.5339252352714539
translation,41,91,hyperparameters,classification types,of,l cls,classification types of l cls,0.6337263584136963
translation,41,91,hyperparameters,hyperparameters,follow,trippy,hyperparameters follow trippy,0.6030921339988708
translation,41,92,hyperparameters,preview module,use,"adam ( kingma and ba , 2015 )","preview module use adam ( kingma and ba , 2015 )",0.6326752305030823
translation,41,92,hyperparameters,"adam ( kingma and ba , 2015 )",with,fixed learning rate 3e - 5,"adam ( kingma and ba , 2015 ) with fixed learning rate 3e - 5",0.6283904314041138
translation,41,92,hyperparameters,fixed learning rate 3e - 5,for,3 epochs,fixed learning rate 3e - 5 for 3 epochs,0.6048329472541809
translation,41,92,hyperparameters,3 epochs,in,pretraining,3 epochs in pretraining,0.5091337561607361
translation,41,92,hyperparameters,hyperparameters,For,preview module,hyperparameters For preview module,0.5730506181716919
translation,41,93,hyperparameters,batch size,for,l aux,batch size for l aux,0.679897665977478
translation,41,93,hyperparameters,batch size,for,l seq and l cls,batch size for l seq and l cls,0.6648525595664978
translation,41,93,hyperparameters,batch size,for,l seq and l cls,batch size for l seq and l cls,0.6648525595664978
translation,41,93,hyperparameters,l aux,is,14,l aux is 14,0.7305629253387451
translation,41,93,hyperparameters,batch size,for,l seq and l cls,batch size for l seq and l cls,0.6648525595664978
translation,41,93,hyperparameters,l seq and l cls,is,64,l seq and l cls is 64,0.645404040813446
translation,41,93,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,41,93,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,41,94,hyperparameters,curriculum module,perform,warm - up strategy,curriculum module perform warm - up strategy,0.6222343444824219
translation,41,94,hyperparameters,warm - up strategy,for,adam optimizer,warm - up strategy for adam optimizer,0.6111139059066772
translation,41,94,hyperparameters,warm - up strategy,with,maximum learning rate 1e - 4,warm - up strategy with maximum learning rate 1e - 4,0.6615945100784302
translation,41,94,hyperparameters,adam optimizer,with,maximum learning rate 1e - 4,adam optimizer with maximum learning rate 1e - 4,0.611945629119873
translation,41,94,hyperparameters,hyperparameters,For,curriculum module,hyperparameters For curriculum module,0.5870254039764404
translation,41,95,hyperparameters,models,on,full dataset,models on full dataset,0.5265276432037354
translation,41,95,hyperparameters,models,for,2 epochs,models for 2 epochs,0.6465221643447876
translation,41,95,hyperparameters,full dataset,for,2 epochs,full dataset for 2 epochs,0.6163589358329773
translation,41,96,hyperparameters,subsets,are,accumulated,subsets are accumulated,0.6783931851387024
translation,41,96,hyperparameters,subsets,train for,10 extra epochs,subsets train for 10 extra epochs,0.7221553921699524
translation,41,96,hyperparameters,accumulated,train for,10 extra epochs,accumulated train for 10 extra epochs,0.6962518692016602
translation,41,96,hyperparameters,10 extra epochs,with,minimum learning rate 1e - 6,10 extra epochs with minimum learning rate 1e - 6,0.6415230631828308
translation,41,97,hyperparameters,hyperparameters,set,bucket number n = 10,hyperparameters set bucket number n = 10,0.6446423530578613
translation,41,97,hyperparameters,hyperparameters,set,crossed fold k = 5,hyperparameters set crossed fold k = 5,0.6465217471122742
translation,41,98,hyperparameters,batch size,is,36,batch size is 36,0.6311392784118652
translation,41,98,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,41,5,model,curriculum learning ( cl ),to better leverage,structure and schema structure,curriculum learning ( cl ) to better leverage structure and schema structure,0.6915450096130371
translation,41,5,model,structure and schema structure,for,task - oriented dialogs,structure and schema structure for task - oriented dialogs,0.6257014274597168
translation,41,5,model,model,propose to use,curriculum learning ( cl ),model propose to use curriculum learning ( cl ),0.7562386393547058
translation,41,6,model,model-agnostic framework,called,schema- aware curriculum learning,model-agnostic framework called schema- aware curriculum learning,0.6593359708786011
translation,41,6,model,model-agnostic framework,consists of,review module,model-agnostic framework consists of review module,0.675458550453186
translation,41,6,model,schema- aware curriculum learning,for,dialog state tracking ( saclog ),schema- aware curriculum learning for dialog state tracking ( saclog ),0.5864272713661194
translation,41,6,model,dialog state tracking ( saclog ),consists of,review module,dialog state tracking ( saclog ) consists of review module,0.6608540415763855
translation,41,6,model,preview module,pre-trains,dst model,preview module pre-trains dst model,0.7442689538002014
translation,41,6,model,dst model,with,schema information,dst model with schema information,0.5717512965202332
translation,41,6,model,curriculum module,optimizes,model,curriculum module optimizes model,0.7255297303199768
translation,41,6,model,model,with,cl,model with cl,0.7324655055999756
translation,41,6,model,model,with,cl training,model with cl training,0.7021299004554749
translation,41,6,model,review module,augments,mispredicted data,review module augments mispredicted data,0.7141633033752441
translation,41,6,model,mispredicted data,to reinforce,cl training,mispredicted data to reinforce cl training,0.7114480137825012
translation,41,6,model,model,propose,model-agnostic framework,model propose model-agnostic framework,0.6826637983322144
translation,41,21,model,cl training,expand,examples,cl training expand examples,0.6563517451286316
translation,41,21,model,examples,with,frequent mispredictions,examples with frequent mispredictions,0.5942608714103699
translation,41,21,model,frequent mispredictions,during,cl,frequent mispredictions during cl,0.7131103277206421
translation,41,21,model,cl,based upon,schema,cl based upon schema,0.7204199433326721
translation,41,22,model,novel framework,named as,schema- aware curriculum learning,novel framework named as schema- aware curriculum learning,0.6293407678604126
translation,41,22,model,schema- aware curriculum learning,for,dialog state tracking ( saclog ),schema- aware curriculum learning for dialog state tracking ( saclog ),0.5864272713661194
translation,41,22,model,dialog state tracking ( saclog ),consists of,three components,dialog state tracking ( saclog ) consists of three components,0.6656389236450195
translation,41,22,model,preview module,pre-trains,base part,preview module pre-trains base part,0.7256629467010498
translation,41,22,model,base part,of,"dst model ( e.g. , bert and rnn )","base part of dst model ( e.g. , bert and rnn )",0.5566006898880005
translation,41,22,model,"dst model ( e.g. , bert and rnn )",with,objectives,"dst model ( e.g. , bert and rnn ) with objectives",0.5985696911811829
translation,41,22,model,curriculum module,organizes,training data,curriculum module organizes training data,0.6361672282218933
translation,41,22,model,training data,from,easy to hard,training data from easy to hard,0.5675427913665771
translation,41,22,model,training data,optimizes,model,training data optimizes model,0.7771373987197876
translation,41,22,model,model,with,cl,model with cl,0.7324655055999756
translation,41,22,model,review module,leverages,schema-based data augmentation,review module leverages schema-based data augmentation,0.7691658735275269
translation,41,22,model,schema-based data augmentation,to extend,mispredicted data,schema-based data augmentation to extend mispredicted data,0.6816968321800232
translation,41,22,model,three components,has,preview module,three components has preview module,0.5691335797309875
translation,41,22,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,41,22,model,model,propose,cl training process,model propose cl training process,0.6755545139312744
translation,41,23,model,approach,is,model- agnostic,approach is model- agnostic,0.5887487530708313
translation,41,7,results,our proposed approach,improves,dst performance,our proposed approach improves dst performance,0.7237749099731445
translation,41,7,results,dst performance,over,transformerbased and rnn - based dst model ( trippy and trade ),dst performance over transformerbased and rnn - based dst model ( trippy and trade ),0.6751714944839478
translation,41,7,results,results,show,our proposed approach,results show our proposed approach,0.6588364839553833
translation,41,103,results,trippy,obtain,state - of - the - art performance,trippy obtain state - of - the - art performance,0.5640628933906555
translation,41,103,results,state - of - the - art performance,on,both datasets,state - of - the - art performance on both datasets,0.48667988181114197
translation,41,103,results,both datasets,with,saclog,both datasets with saclog,0.6873693466186523
translation,41,108,results,saclog,improves,trade,saclog improves trade,0.766326367855072
translation,41,108,results,trade,by,3? 4 % jga,trade by 3? 4 % jga,0.6689382195472717
translation,41,108,results,trade,around,3? 4 % jga,trade around 3? 4 % jga,0.7137885093688965
translation,41,108,results,results,has,saclog,results has saclog,0.5771453380584717
translation,42,219,ablation-analysis,our ablation study,confirms,tf - idf masking,our ablation study confirms tf - idf masking,0.696549117565155
translation,42,219,ablation-analysis,brings improvement,in,almost all automatic metrics,brings improvement in almost all automatic metrics,0.4999924302101135
translation,42,219,ablation-analysis,tf - idf masking,has,brings improvement,tf - idf masking has brings improvement,0.6121394634246826
translation,42,219,ablation-analysis,ablation analysis,confirms,tf - idf masking,ablation analysis confirms tf - idf masking,0.6584566235542297
translation,42,219,ablation-analysis,ablation analysis,has,our ablation study,ablation analysis has our ablation study,0.5378058552742004
translation,42,128,baselines,two strong baselines,designed for,personalized response generation,two strong baselines designed for personalized response generation,0.653317391872406
translation,42,128,baselines,two strong baselines,two others for,topic-aware generation,two strong baselines two others for topic-aware generation,0.6735681891441345
translation,42,128,baselines,baselines,choose,two strong baselines,baselines choose two strong baselines,0.694702684879303
translation,42,130,baselines,seq2seq model,using,four lstm layers,seq2seq model using four lstm layers,0.6071863174438477
translation,42,130,baselines,speaker model,has,seq2seq model,speaker model has seq2seq model,0.5363943576812744
translation,42,130,baselines,baselines,has,speaker model,baselines has speaker model,0.5464064478874207
translation,42,132,baselines,mt - speaker,jointly trains,speaker model and a conditioned auto-encoder,mt - speaker jointly trains speaker model and a conditioned auto-encoder,0.7394131422042847
translation,42,132,baselines,speaker model and a conditioned auto-encoder,with,shared decoder parameters,speaker model and a conditioned auto-encoder with shared decoder parameters,0.6214218735694885
translation,42,132,baselines,baselines,has,mt - speaker,baselines has mt - speaker,0.5832574367523193
translation,42,140,baselines,decoder-only transformer,initialized with,gpt - 2 parameters,decoder-only transformer initialized with gpt - 2 parameters,0.7598955035209656
translation,42,140,baselines,c-trans - dec,has,decoder-only transformer,c-trans - dec has decoder-only transformer,0.591662585735321
translation,42,140,baselines,baselines,has,c-trans - dec,baselines has c-trans - dec,0.6098670959472656
translation,42,154,experimental-setup,warm - up proportion,set to,0.1.,warm - up proportion set to 0.1.,0.7331509590148926
translation,42,154,experimental-setup,tokens,of,target side,tokens of target side,0.6018759608268738
translation,42,154,experimental-setup,target side,are,randomly masked,target side are randomly masked,0.5882564187049866
translation,42,154,experimental-setup,0.1.,has,25,0.1. has 25,0.5448682904243469
translation,42,154,experimental-setup,25,has,tokens,25 has tokens,0.6557570099830627
translation,42,154,experimental-setup,experimental setup,has,warm - up proportion,experimental setup has warm - up proportion,0.5389435291290283
translation,42,155,experimental-setup,decoding,prevent,duplicated bigrams,decoding prevent duplicated bigrams,0.5504384636878967
translation,42,155,experimental-setup,beam size,is,10,beam size is 10,0.6545984148979187
translation,42,155,experimental-setup,decoding,has,beam size,decoding has beam size,0.5500069856643677
translation,42,155,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,42,156,experimental-setup,all the parameters,for,four epochs,all the parameters for four epochs,0.6190140247344971
translation,42,156,experimental-setup,end-to - end,for,four epochs,end-to - end for four epochs,0.6268337965011597
translation,42,156,experimental-setup,four epochs,on,two p100 gpus,four epochs on two p100 gpus,0.5624943375587463
translation,42,156,experimental-setup,all the parameters,has,end-to - end,all the parameters has end-to - end,0.5812370181083679
translation,42,156,experimental-setup,experimental setup,fine- tune,all the parameters,experimental setup fine- tune all the parameters,0.6748524904251099
translation,42,21,experiments,3 tasks,to jointly optimize,same pre-trained transformer - conditioned dialogue generation task,3 tasks to jointly optimize same pre-trained transformer - conditioned dialogue generation task,0.6631535887718201
translation,42,21,experiments,3 tasks,to jointly optimize,conditioned language encoding task,3 tasks to jointly optimize conditioned language encoding task,0.649703323841095
translation,42,21,experiments,same pre-trained transformer - conditioned dialogue generation task,on,labeled dialogue data,same pre-trained transformer - conditioned dialogue generation task on labeled dialogue data,0.5026862025260925
translation,42,21,experiments,same pre-trained transformer - conditioned dialogue generation task,on,labeled text data,same pre-trained transformer - conditioned dialogue generation task on labeled text data,0.5004016160964966
translation,42,21,experiments,same pre-trained transformer - conditioned dialogue generation task,on,labeled text data,same pre-trained transformer - conditioned dialogue generation task on labeled text data,0.5004016160964966
translation,42,21,experiments,conditioned language generation task,on,labeled text data,conditioned language generation task on labeled text data,0.48030006885528564
translation,42,147,experiments,speaker model and mt - speaker model,based on,opennmt,speaker model and mt - speaker model based on opennmt,0.6577396988868713
translation,42,6,model,multi-task learning approach,to leverage,labeled dialogue and text data,multi-task learning approach to leverage labeled dialogue and text data,0.6475729942321777
translation,42,6,model,model,propose,multi-task learning approach,model propose multi-task learning approach,0.6610496044158936
translation,42,20,model,multi-task learning approach,to leverage,labeled dialogue and text data,multi-task learning approach to leverage labeled dialogue and text data,0.6475729942321777
translation,42,20,model,model,propose,multi-task learning approach,model propose multi-task learning approach,0.6610496044158936
translation,42,25,model,approach,incorporates,all types of data,approach incorporates all types of data,0.7081347107887268
translation,42,25,model,all types of data,within,same framework,all types of data within same framework,0.7127861380577087
translation,42,25,model,labeled data,has,approach,labeled data has approach,0.5853177905082703
translation,42,25,model,model,leverage,labeled data,model leverage labeled data,0.7268154621124268
translation,42,26,model,tf - idf based masking,selects,more conditionrelated tokens,tf - idf based masking selects more conditionrelated tokens,0.6851652264595032
translation,42,26,model,model,propose,tf - idf based masking,model propose tf - idf based masking,0.7013051509857178
translation,42,27,model,conditioned generation,propose,non-parametric attention - based gating mechanism,conditioned generation propose non-parametric attention - based gating mechanism,0.5946618318557739
translation,42,27,model,non-parametric attention - based gating mechanism,chooses between,generating,non-parametric attention - based gating mechanism chooses between generating,0.6271995902061462
translation,42,27,model,condition - related word,at,each position,condition - related word at each position,0.50553959608078
translation,42,27,model,generating,has,general word,generating has general word,0.6061724424362183
translation,42,27,model,generating,has,condition - related word,generating has condition - related word,0.5827263593673706
translation,42,27,model,model,for,conditioned generation,model for conditioned generation,0.6494430899620056
translation,42,135,model,ta - seq2seq,leverages,topic information,ta - seq2seq leverages topic information,0.7724327445030212
translation,42,135,model,topic information,by,joint attention mechanism,topic information by joint attention mechanism,0.46733367443084717
translation,42,135,model,topic information,by,biased generation probability,topic information by biased generation probability,0.55333012342453
translation,42,135,model,model,has,ta - seq2seq,model has ta - seq2seq,0.6101915240287781
translation,42,136,model,thred,built based on,hred,thred built based on hred,0.7278335690498352
translation,42,136,model,thred,incorporates,topic words,thred incorporates topic words,0.6935263872146606
translation,42,136,model,topic words,via,hierarchical joint attention mechanism,topic words via hierarchical joint attention mechanism,0.5896965265274048
translation,42,136,model,model,has,thred,model has thred,0.6585326194763184
translation,42,190,results,performance,of,w/o ctext,performance of w/o ctext,0.6205623745918274
translation,42,190,results,w/o ctext,similar to,c-trans - dec's,w/o ctext similar to c-trans - dec's,0.6933860778808594
translation,42,190,results,slight advantage,in,condition consistency,slight advantage in condition consistency,0.5187290906906128
translation,42,190,results,small disadvantage,in,response appropriateness,small disadvantage in response appropriateness,0.508970320224762
translation,42,190,results,results,has,performance,results has performance,0.5972660779953003
translation,42,194,results,bert,in,almost all automatic metrics,bert in almost all automatic metrics,0.5655934810638428
translation,42,194,results,outperforms,has,bert,outperforms has bert,0.7186265587806702
translation,42,194,results,results,When,large persona dialogue,results When large persona dialogue,0.572637140750885
translation,42,195,results,only small-scale labeled dialogue data,are,available,only small-scale labeled dialogue data are available,0.5204591155052185
translation,42,195,results,only small-scale labeled dialogue data,are,all three conditioned models,only small-scale labeled dialogue data are all three conditioned models,0.5125299692153931
translation,42,195,results,all three conditioned models,perform,worse,all three conditioned models perform worse,0.6296676993370056
translation,42,195,results,worse,than,bert,worse than bert,0.7024371027946472
translation,42,195,results,only small-scale labeled dialogue data,has,all three conditioned models,only small-scale labeled dialogue data has all three conditioned models,0.5175178050994873
translation,42,195,results,results,when,only small-scale labeled dialogue data,results when only small-scale labeled dialogue data,0.5559961199760437
translation,42,204,results,our approach,is,best,our approach is best,0.6001749038696289
translation,42,204,results,best,on,smallscale topic dialogue,best on smallscale topic dialogue,0.5330550670623779
translation,42,207,results,two - step ft,on,labeled dialogue data,two - step ft on labeled dialogue data,0.5071303248405457
translation,42,207,results,two - step ft,then on,labeled dialogue data,two - step ft then on labeled dialogue data,0.5866889953613281
translation,42,207,results,first fine-tuning,on,labeled texts,first fine-tuning on labeled texts,0.48245418071746826
translation,42,207,results,labeled dialogue data,does not always produce,good performance,labeled dialogue data does not always produce good performance,0.7185720801353455
translation,42,207,results,results,has,two - step ft,results has two - step ft,0.5452346801757812
translation,42,208,results,comparable performance,to,our approach,comparable performance to our approach,0.5454919934272766
translation,42,208,results,our approach,on,large-scale datasets,our approach on large-scale datasets,0.4897525906562805
translation,42,208,results,small - scale datasets,perform,worse,small - scale datasets perform worse,0.6040646433830261
translation,42,208,results,worse,than,w/o ctext,worse than w/o ctext,0.530789852142334
translation,42,208,results,results,on,small - scale datasets,results on small - scale datasets,0.5180219411849976
translation,42,225,results,only small-scale labeled data,are,available,only small-scale labeled data are available,0.5499135851860046
translation,42,225,results,model,with,attention gating,model with attention gating,0.6308559775352478
translation,42,225,results,model,generates,responses,model generates responses,0.624602198600769
translation,42,225,results,attention gating,generates,responses,attention gating generates responses,0.6156777739524841
translation,42,225,results,significantly more similar,to,ground -truth,significantly more similar to ground -truth,0.5250926613807678
translation,42,225,results,only small-scale labeled data,has,model,only small-scale labeled data has model,0.4891678988933563
translation,42,225,results,results,When,only small-scale labeled data,results When only small-scale labeled data,0.5907098650932312
translation,43,5,model,modulated,to decide,what types of information,modulated to decide what types of information,0.7279693484306335
translation,43,5,model,modulated,to decide,perspective,modulated to decide perspective,0.7223649024963379
translation,43,5,model,summaries,to tackle,under-constrained problem,summaries to tackle under-constrained problem,0.6870839595794678
translation,43,5,model,under-constrained problem,in,summarization tasks,under-constrained problem in summarization tasks,0.4867398142814636
translation,43,5,model,model,has,conditional sequences,model has conditional sequences,0.6057878136634827
translation,43,7,model,training,exploit,occurrence planning,training exploit occurrence planning,0.7363252639770508
translation,43,7,model,occurrence planning,of,personal named entities and coreference information,occurrence planning of personal named entities and coreference information,0.5229179263114929
translation,43,7,model,personal named entities and coreference information,to improve,temporal coherence,personal named entities and coreference information to improve temporal coherence,0.6219443082809448
translation,43,7,model,hallucination,in,neural generation,hallucination in neural generation,0.5215574502944946
translation,43,7,model,model,During,training,model During training,0.714866042137146
translation,43,33,model,model,introduce,controllable dialogue summarization framework,model introduce controllable dialogue summarization framework,0.5906299352645874
translation,43,38,model,coreference resolution information,into,contextual representation,coreference resolution information into contextual representation,0.4833811819553375
translation,43,38,model,contextual representation,by,graph - based neural component,contextual representation by graph - based neural component,0.5190343856811523
translation,43,38,model,graph - based neural component,to further reduce,incorrect reasoning,graph - based neural component to further reduce incorrect reasoning,0.6449204683303833
translation,44,114,ablation-analysis,augmentation,of,descriptions,augmentation of descriptions,0.6352370977401733
translation,44,114,ablation-analysis,augmentation,improve,overall jga,augmentation improve overall jga,0.7501562833786011
translation,44,114,ablation-analysis,descriptions,improve,overall jga,descriptions improve overall jga,0.6827509999275208
translation,44,114,ablation-analysis,overall jga,by,over 1 %,overall jga by over 1 %,0.5803368091583252
translation,44,114,ablation-analysis,over 1 %,in,t5 - small and t5 - base,over 1 % in t5 - small and t5 - base,0.6047536730766296
translation,44,114,ablation-analysis,ablation analysis,With,augmentation,ablation analysis With augmentation,0.6428064107894897
translation,44,142,ablation-analysis,0.4 % point drop,in,jga,0.4 % point drop in jga,0.5819023251533508
translation,44,142,ablation-analysis,0.4 % point drop,in,jga,0.4 % point drop in jga,0.5819023251533508
translation,44,142,ablation-analysis,0.4 % point drop,in,jga,0.4 % point drop in jga,0.5819023251533508
translation,44,142,ablation-analysis,0.8 % point drop,in,jga,0.8 % point drop in jga,0.5839632749557495
translation,44,143,ablation-analysis,0.1 % point drop,in,jga,0.1 % point drop in jga,0.5827621221542358
translation,44,144,ablation-analysis,slot descriptions,are,most important part,slot descriptions are most important part,0.5632830858230591
translation,44,144,ablation-analysis,most important part,of,schema prompts,most important part of schema prompts,0.6111591458320618
translation,44,144,ablation-analysis,domain descriptions,are,relatively less effective,domain descriptions are relatively less effective,0.5323274731636047
translation,44,144,ablation-analysis,ablation analysis,shows,slot descriptions,ablation analysis shows slot descriptions,0.6357131600379944
translation,44,144,ablation-analysis,ablation analysis,shows,domain descriptions,ablation analysis shows domain descriptions,0.6217215061187744
translation,44,22,baselines,generation - based dst approach,using,pre-trained sequence - to- sequence model,generation - based dst approach using pre-trained sequence - to- sequence model,0.6571224927902222
translation,44,22,baselines,new strategy,of adding,taskspecific prompts,new strategy of adding taskspecific prompts,0.6204660534858704
translation,44,22,baselines,taskspecific prompts,input for,sequence - to-sequence dst models,taskspecific prompts input for sequence - to-sequence dst models,0.7790305614471436
translation,44,128,experiments,m2m,has,borrowed natural language augmented,m2m has borrowed natural language augmented,0.5776117444038391
translation,44,7,model,new variation,of,language modeling approach,new variation of language modeling approach,0.5226074457168579
translation,44,7,model,new variation,uses,schema-driven prompting,new variation uses schema-driven prompting,0.6226900815963745
translation,44,7,model,schema-driven prompting,to provide,task - aware history encoding,schema-driven prompting to provide task - aware history encoding,0.645960807800293
translation,44,7,model,task - aware history encoding,used for,categorical and non-categorical slots,task - aware history encoding used for categorical and non-categorical slots,0.6014484763145447
translation,44,7,model,model,introduce,new variation,model introduce new variation,0.6800877451896667
translation,44,23,model,dialogue context,with,domain and slot prompts,dialogue context with domain and slot prompts,0.6345222592353821
translation,44,23,model,domain and slot prompts,as input to,encoder,domain and slot prompts as input to encoder,0.7480160593986511
translation,44,23,model,encoder,where,prompts,encoder where prompts,0.6862510442733765
translation,44,23,model,model,concatenate,dialogue context,model concatenate dialogue context,0.6743629574775696
translation,44,31,model,candidate schema labels,jointly encoded with,dialogue context,candidate schema labels jointly encoded with dialogue context,0.729872465133667
translation,44,31,model,task - aware contextualization,for initializing,decoder,task - aware contextualization for initializing decoder,0.7236332297325134
translation,44,31,model,model,has,candidate schema labels,model has candidate schema labels,0.5361018180847168
translation,44,32,model,natural language descriptions,of,schema categories,natural language descriptions of schema categories,0.5029357671737671
translation,44,32,model,natural language descriptions,incorporated in,encoding,natural language descriptions incorporated in encoding,0.6428170800209045
translation,44,32,model,schema categories,associated with,database documentation,schema categories associated with database documentation,0.6019312739372253
translation,44,32,model,schema categories,incorporated in,encoding,schema categories incorporated in encoding,0.703721284866333
translation,44,32,model,encoding,as,prompts,encoding as prompts,0.5921671986579895
translation,44,32,model,prompts,to,language model,prompts to language model,0.5585063695907593
translation,44,32,model,uniform handling,of,categorical and non-categorical slots,uniform handling of categorical and non-categorical slots,0.6115521192550659
translation,44,32,model,model,has,natural language descriptions,model has natural language descriptions,0.48930028080940247
translation,44,8,results,performance,augmenting,prompting,performance augmenting prompting,0.7407962083816528
translation,44,8,results,prompting,with,schema descriptions,prompting with schema descriptions,0.623115062713623
translation,44,8,results,results,improve,performance,results improve performance,0.6927347779273987
translation,44,9,results,our purely generative system,achieves,state - of- the - art performance,our purely generative system achieves state - of- the - art performance,0.601105272769928
translation,44,9,results,our purely generative system,achieves,competitive performance,our purely generative system achieves competitive performance,0.6582893133163452
translation,44,9,results,our purely generative system,achieves,competitive performance,our purely generative system achieves competitive performance,0.6582893133163452
translation,44,9,results,state - of- the - art performance,on,multiwoz 2.2,state - of- the - art performance on multiwoz 2.2,0.55189049243927
translation,44,9,results,competitive performance,on,two other benchmarks,competitive performance on two other benchmarks,0.5123942494392395
translation,44,9,results,results,has,our purely generative system,results has our purely generative system,0.5559024214744568
translation,44,33,results,simple approach,achieves,state - of- theart ( sota ) results,simple approach achieves state - of- theart ( sota ) results,0.6510374546051025
translation,44,33,results,state - of- theart ( sota ) results,on,multiwoz 2.2,state - of- theart ( sota ) results on multiwoz 2.2,0.5518642067909241
translation,44,33,results,performance,on par with,sota,performance on par with sota,0.6532710194587708
translation,44,33,results,sota,on,multiwoz 2.1 and m2m,sota on multiwoz 2.1 and m2m,0.5864245295524597
translation,44,33,results,strong pretrained text - to - text model,has,simple approach,strong pretrained text - to - text model has simple approach,0.5582537651062012
translation,44,111,results,sequential decoding strategy,worse than,independent decoding strategy,sequential decoding strategy worse than independent decoding strategy,0.6606043577194214
translation,44,111,results,independent decoding strategy,by,over 5 %,independent decoding strategy by over 5 %,0.5789304971694946
translation,44,111,results,over 5 %,with,t5 - small and t5 - base,over 5 % with t5 - small and t5 - base,0.6916016340255737
translation,44,111,results,results,has,sequential decoding strategy,results has sequential decoding strategy,0.5273641347885132
translation,44,112,results,our system,achieves,sota performance,our system achieves sota performance,0.723965585231781
translation,44,112,results,sota performance,using,independent decoding,sota performance using independent decoding,0.6738817095756531
translation,44,113,results,t5 - base systems,has,outperform,t5 - base systems has outperform,0.6428540349006653
translation,44,113,results,outperform,has,t5 - small systems,outperform has t5 - small systems,0.6489853262901306
translation,44,113,results,results,has,t5 - base systems,results has t5 - base systems,0.5890311598777771
translation,44,118,results,t5 - base models,perform,consistently better,t5 - base models perform consistently better,0.6275640726089478
translation,44,118,results,consistently better,than,t5 - small models,consistently better than t5 - small models,0.6097988486289978
translation,44,118,results,results,observe,t5 - base models,results observe t5 - base models,0.5614489316940308
translation,44,119,results,descriptions,consistently improves,performance,descriptions consistently improves performance,0.7900252938270569
translation,44,119,results,performance,of,both models,performance of both models,0.5794581770896912
translation,44,119,results,results,using,descriptions,results using descriptions,0.5978832840919495
translation,44,120,results,outperform,that do not use,extra dialogue data,outperform that do not use extra dialogue data,0.7360461354255676
translation,44,120,results,baselines,that do not use,extra dialogue data,baselines that do not use extra dialogue data,0.7188364863395691
translation,44,120,results,models,has,outperform,models has outperform,0.6452599167823792
translation,44,120,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,44,120,results,results,has,models,results has models,0.5335168838500977
translation,44,121,results,our model,is,better,our model is better,0.602020263671875
translation,44,121,results,better,by,over 4 %,better by over 4 %,0.586521565914154
translation,44,121,results,over 4 %,even without,descriptions,over 4 % even without descriptions,0.7946222424507141
translation,44,121,results,mintl ( t5 - small ),has,our model,mintl ( t5 - small ) has our model,0.612049400806427
translation,44,121,results,results,comparing with,mintl ( t5 - small ),results comparing with mintl ( t5 - small ),0.6575949788093567
translation,44,127,results,performance gain,brought by,augmenting natural language descriptions,performance gain brought by augmenting natural language descriptions,0.6666550040245056
translation,44,127,results,augmenting natural language descriptions,is,less pronounced,augmenting natural language descriptions is less pronounced,0.5484215617179871
translation,44,127,results,multiwoz 2.2,has,performance gain,multiwoz 2.2 has performance gain,0.5479792952537537
translation,44,127,results,results,compared with,multiwoz 2.2,results compared with multiwoz 2.2,0.6404048204421997
translation,44,131,results,improvements,of,descriptions,improvements of descriptions,0.6293895244598389
translation,44,131,results,descriptions,evident,restaurant domain,descriptions evident restaurant domain,0.6663520336151123
translation,44,131,results,results,has,improvements,results has improvements,0.615561842918396
translation,44,136,results,t5 - small and t5 - base models,models with,sequential decoding,t5 - small and t5 - base models models with sequential decoding,0.7799402475357056
translation,44,136,results,sequential decoding,perform,worse,sequential decoding perform worse,0.6538618803024292
translation,44,136,results,worse,than,corresponding models,worse than corresponding models,0.6324472427368164
translation,44,136,results,corresponding models,with,independent decoding,corresponding models with independent decoding,0.6392507553100586
translation,44,136,results,independent decoding,for,categorical and non-categorical slots,independent decoding for categorical and non-categorical slots,0.6596027612686157
translation,44,136,results,results,For,t5 - small and t5 - base models,results For t5 - small and t5 - base models,0.5999875068664551
translation,44,137,results,independent decoding models,achieve,more pronounced improvement,independent decoding models achieve more pronounced improvement,0.6087583303451538
translation,44,137,results,more pronounced improvement,in,categorical slots,more pronounced improvement in categorical slots,0.6097719669342041
translation,44,137,results,results,has,independent decoding models,results has independent decoding models,0.4885082542896271
translation,44,138,results,performance gains,for,both types of slots,performance gains for both types of slots,0.6285241842269897
translation,44,138,results,both types of slots,for,t-base,both types of slots for t-base,0.6527125835418701
translation,44,138,results,only non-categorical slots,for,t5 - small,only non-categorical slots for t5 - small,0.6451796889305115
translation,45,74,ablation-analysis,top-k sampling,results in,highest hallucination percentage ( 40.33 % ),top-k sampling results in highest hallucination percentage ( 40.33 % ),0.6288803219795227
translation,45,74,ablation-analysis,ablation analysis,has,top-k sampling,ablation analysis has top-k sampling,0.5453864336013794
translation,45,76,ablation-analysis,diversity,in,response generation,diversity in response generation,0.5278832912445068
translation,45,76,ablation-analysis,hallucination,has,e.g. nucleus =0.9,hallucination has e.g. nucleus =0.9,0.567333996295929
translation,45,76,ablation-analysis,ablation analysis,Increased,diversity,ablation analysis Increased diversity,0.7291942834854126
translation,45,182,ablation-analysis,nph,decreases,critic score,nph decreases critic score,0.7025046348571777
translation,45,182,ablation-analysis,nph,increases,faithfulness,nph increases faithfulness,0.6909278631210327
translation,45,182,ablation-analysis,critic score,by,8.17 points,critic score by 8.17 points,0.5633430480957031
translation,45,182,ablation-analysis,faithfulness,by,6.67 points,faithfulness by 6.67 points,0.5361250042915344
translation,45,182,ablation-analysis,6.67 points,on,feqa,6.67 points on feqa,0.5967424511909485
translation,45,182,ablation-analysis,adapterbot,has,nph,adapterbot has nph,0.6312811374664307
translation,45,182,ablation-analysis,ablation analysis,in,adapterbot,ablation analysis in adapterbot,0.5436604022979736
translation,45,190,ablation-analysis,sans negatives,lead to,lower perplexity model neg.,sans negatives lead to lower perplexity model neg.,0.646825909614563
translation,45,190,ablation-analysis,sans negatives,lead to,better retrieval performance,sans negatives lead to better retrieval performance,0.643828272819519
translation,45,190,ablation-analysis,lower perplexity model neg.,has,candidates ppl hits@1 hits@3 hits@10 mr mrr,lower perplexity model neg. has candidates ppl hits@1 hits@3 hits@10 mr mrr,0.5680896639823914
translation,45,190,ablation-analysis,ablation analysis,observe,sans negatives,ablation analysis observe sans negatives,0.6126569509506226
translation,45,215,ablation-analysis,generation methods,when paired with,nph,generation methods when paired with nph,0.6913998126983643
translation,45,215,ablation-analysis,nph,reduce,hallucinations,nph reduce hallucinations,0.6976072788238525
translation,45,215,ablation-analysis,hallucinations,by,large margin,hallucinations by large margin,0.5671184659004211
translation,45,215,ablation-analysis,hallucinations,with,marginal drop,hallucinations with marginal drop,0.6544978618621826
translation,45,215,ablation-analysis,42.05 %,for,gpt2 - kb responses,42.05 % for gpt2 - kb responses,0.6294220685958862
translation,45,215,ablation-analysis,gpt2 - kb responses,with,marginal drop,gpt2 - kb responses with marginal drop,0.6468464136123657
translation,45,215,ablation-analysis,marginal drop,in,fluency ( 4.32 % ),marginal drop in fluency ( 4.32 % ),0.5554004311561584
translation,45,215,ablation-analysis,large margin,has,42.05 %,large margin has 42.05 %,0.5438416004180908
translation,45,215,ablation-analysis,ablation analysis,has,generation methods,ablation analysis has generation methods,0.5054192543029785
translation,45,25,baselines,nph,follows,generate - then - refine approach,nph follows generate - then - refine approach,0.7187667489051819
translation,45,25,baselines,generate - then - refine approach,by augmenting,conventional dialogue generation,generate - then - refine approach by augmenting conventional dialogue generation,0.7128521800041199
translation,45,25,baselines,conventional dialogue generation,with,additional refinement stage,conventional dialogue generation with additional refinement stage,0.6287860870361328
translation,45,25,baselines,additional refinement stage,enabling,dialogue system,additional refinement stage enabling dialogue system,0.7808565497398376
translation,45,25,baselines,dialogue system,to correct,potential hallucinations,dialogue system to correct potential hallucinations,0.6880519986152649
translation,45,25,baselines,potential hallucinations,by querying,kg,potential hallucinations by querying kg,0.7791809439659119
translation,45,25,baselines,baselines,has,nph,baselines has nph,0.5550090670585632
translation,45,126,baselines,"compgcn ( vashishth et al. , 2020 )",is,graph convolutional network,"compgcn ( vashishth et al. , 2020 ) is graph convolutional network",0.5405635833740234
translation,45,126,baselines,graph convolutional network,purposely built for,multi-relational data,graph convolutional network purposely built for multi-relational data,0.6734691858291626
translation,45,127,experimental-setup,compgcn network,with,gpt2 embeddings,compgcn network with gpt2 embeddings,0.658325731754303
translation,45,127,experimental-setup,offline,with,gpt2 embeddings,offline with gpt2 embeddings,0.6438010334968567
translation,45,127,experimental-setup,gpt2 embeddings,for,all entities and relations,gpt2 embeddings for all entities and relations,0.5704036951065063
translation,45,127,experimental-setup,gpt2 embeddings,before running,few rounds,gpt2 embeddings before running few rounds,0.7602296471595764
translation,45,127,experimental-setup,all entities and relations,in,full graph g,all entities and relations in full graph g,0.48971250653266907
translation,45,127,experimental-setup,few rounds,of,message passing,few rounds of message passing,0.5710312724113464
translation,45,127,experimental-setup,message passing,optimizing for,standard relation prediction objective,message passing optimizing for standard relation prediction objective,0.7004069685935974
translation,45,127,experimental-setup,compgcn network,has,offline,compgcn network has offline,0.6129933595657349
translation,45,127,experimental-setup,experimental setup,initialize,compgcn network,experimental setup initialize compgcn network,0.7493669390678406
translation,45,7,model,neu -ral path hunter,follows,generatethen - refine strategy,neu -ral path hunter follows generatethen - refine strategy,0.710034191608429
translation,45,7,model,generatethen - refine strategy,whereby,generated response,generatethen - refine strategy whereby generated response,0.6990821361541748
translation,45,7,model,amended,using,kg,amended using kg,0.5953246355056763
translation,45,7,model,model,propose,neu -ral path hunter,model propose neu -ral path hunter,0.6669482588768005
translation,45,8,model,neural path hunter,leverages,separate tokenlevel fact critic,neural path hunter leverages separate tokenlevel fact critic,0.7212398052215576
translation,45,8,model,separate tokenlevel fact critic,to identify,plausible sources of hallucination,separate tokenlevel fact critic to identify plausible sources of hallucination,0.6843163371086121
translation,45,8,model,plausible sources of hallucination,followed by,refinement stage,plausible sources of hallucination followed by refinement stage,0.6971745491027832
translation,45,8,model,refinement stage,retrieves,correct entities,refinement stage retrieves correct entities,0.735540509223938
translation,45,8,model,correct entities,by crafting,query signal,correct entities by crafting query signal,0.6800863742828369
translation,45,8,model,query signal,propagated over,k-hop subgraph,query signal propagated over k-hop subgraph,0.7455729842185974
translation,45,8,model,model,has,neural path hunter,model has neural path hunter,0.5623617172241211
translation,45,24,model,faithfulness,introduce,neural path hunter ( nph ),faithfulness introduce neural path hunter ( nph ),0.6495464444160461
translation,45,24,model,model,To enforce,faithfulness,model To enforce faithfulness,0.7799322009086609
translation,45,24,model,model,introduce,neural path hunter ( nph ),model introduce neural path hunter ( nph ),0.6776496171951294
translation,45,85,model,any generated response,without retraining,model,any generated response without retraining model,0.805280327796936
translation,45,85,model,neural path hunter ( nph ),has,refinement strategy,neural path hunter ( nph ) has refinement strategy,0.5759143829345703
translation,45,85,model,model,introduce,neural path hunter ( nph ),model introduce neural path hunter ( nph ),0.6776496171951294
translation,45,86,model,nph,composed of,two modules,nph composed of two modules,0.7497604489326477
translation,45,86,model,model,has,nph,model has nph,0.6251773238182068
translation,45,125,model,final hidden layer,of,pre-trained gpt2,final hidden layer of pre-trained gpt2,0.5533056855201721
translation,45,125,model,pre-trained gpt2,to obtain,initial embeddings,pre-trained gpt2 to obtain initial embeddings,0.5721175074577332
translation,45,125,model,initial embeddings,for,each node,initial embeddings for each node,0.5668659210205078
translation,45,125,model,each node,in,g k c 2,each node in g k c 2,0.5772867202758789
translation,45,125,model,model,first,final hidden layer,model first final hidden layer,0.7522336840629578
translation,45,154,model,sans,selects,hard negatives,sans selects hard negatives,0.7289190888404846
translation,45,154,model,sans,selecting,negative samples,sans selecting negative samples,0.7062929272651672
translation,45,154,model,hard negatives,by leveraging,graph structure,hard negatives by leveraging graph structure,0.6992015838623047
translation,45,154,model,negative samples,from,context entity 's k-hop subgraph ( e.g. g 1 c ),negative samples from context entity 's k-hop subgraph ( e.g. g 1 c ),0.5218296051025391
translation,45,154,model,model,has,sans,model has sans,0.6364842057228088
translation,45,165,model,adapterbot,uses,fixed backbone conversational model,adapterbot uses fixed backbone conversational model,0.6008189916610718
translation,45,165,model,adapterbot,encodes,multiple dialogue skills,adapterbot encodes multiple dialogue skills,0.7187494039535522
translation,45,165,model,adapterbot,process,inputs,adapterbot process inputs,0.768534243106842
translation,45,165,model,fixed backbone conversational model,such as,dialgpt,fixed backbone conversational model such as dialgpt,0.613033652305603
translation,45,165,model,multiple dialogue skills,via,different adapters,multiple dialogue skills via different adapters,0.6729326844215393
translation,45,165,model,inputs,by concatenating,"d , k n","inputs by concatenating d , k n",0.64619380235672
translation,45,165,model,inputs,by concatenating,generated response,inputs by concatenating generated response,0.6310645341873169
translation,45,165,model,model,has,adapterbot,model has adapterbot,0.6182665824890137
translation,45,203,model,node embeddings,learned by,compgcn network,node embeddings learned by compgcn network,0.6870344877243042
translation,45,203,model,node embeddings,trained on,standard relation prediction task,node embeddings trained on standard relation prediction task,0.6634598970413208
translation,45,203,model,standard relation prediction task,over,entire graph g,standard relation prediction task over entire graph g,0.61790531873703
translation,45,203,model,model,Utilizing,node embeddings,model Utilizing node embeddings,0.6276065111160278
translation,45,173,results,roberta-intrin-extrin,achieves,highest f1 ( 70.35 % ),roberta-intrin-extrin achieves highest f1 ( 70.35 % ),0.6394829154014587
translation,45,173,results,highest f1 ( 70.35 % ),compared to,classifiers,highest f1 ( 70.35 % ) compared to classifiers,0.6657729744911194
translation,45,173,results,classifiers,trained on,first two synthetic datasets,classifiers trained on first two synthetic datasets,0.6828861236572266
translation,45,173,results,results,observe,roberta-intrin-extrin,results observe roberta-intrin-extrin,0.5799382925033569
translation,45,180,results,consistently performs favourably,in reducing,hallucination,consistently performs favourably in reducing hallucination,0.674683690071106
translation,45,180,results,hallucination,across,feqa and the hallucination critic,hallucination across feqa and the hallucination critic,0.6250633001327515
translation,45,180,results,nph,has,consistently performs favourably,nph has consistently performs favourably,0.6103255152702332
translation,45,180,results,results,find,nph,results find nph,0.6007335782051086
translation,45,181,results,strongest iteration,of,each baseline model,strongest iteration of each baseline model,0.5582894086837769
translation,45,181,results,each baseline model,is,original model,each baseline model is original model,0.5285736322402954
translation,45,181,results,original model,paired with,full nph module,original model paired with full nph module,0.6738544702529907
translation,45,181,results,results,observe,strongest iteration,results observe strongest iteration,0.6491358280181885
translation,45,188,results,key metrics,such as,hits@3 and hits@10,key metrics such as hits@3 and hits@10,0.6268495321273804
translation,45,188,results,hits@3 and hits@10,are,nearly saturated,hits@3 and hits@10 are nearly saturated,0.6629326343536377
translation,45,188,results,nearly saturated,when using,complete nph module,nearly saturated when using complete nph module,0.6783027648925781
translation,45,188,results,complete nph module,with,gpt2 embeddings,complete nph module with gpt2 embeddings,0.6358880996704102
translation,45,188,results,gpt2 embeddings,for,kg - entity memory,gpt2 embeddings for kg - entity memory,0.576424241065979
translation,45,188,results,results,find that,key metrics,results find that key metrics,0.5636084675788879
translation,45,195,results,nph-w/o mlm,performs,worse,nph-w/o mlm performs worse,0.6575039029121399
translation,45,195,results,worse,than,nph,worse than nph,0.6183855533599854
translation,45,195,results,nph,across,all models,nph across all models,0.7375272512435913
translation,45,196,results,degrades substantially ( e.g. ? 26 hits@1 ),when using,pre-trained gpt2 embeddings,degrades substantially ( e.g. ? 26 hits@1 ) when using pre-trained gpt2 embeddings,0.7153393030166626
translation,45,196,results,pre-trained gpt2 embeddings,as,entity memory,pre-trained gpt2 embeddings as entity memory,0.4768208861351013
translation,45,196,results,performance,has,without mlm,performance has without mlm,0.5874338746070862
translation,45,196,results,performance,has,degrades substantially ( e.g. ? 26 hits@1 ),performance has degrades substantially ( e.g. ? 26 hits@1 ),0.597283124923706
translation,45,196,results,without mlm,has,degrades substantially ( e.g. ? 26 hits@1 ),without mlm has degrades substantially ( e.g. ? 26 hits@1 ),0.6014779806137085
translation,45,196,results,results,observe that,performance,results observe that performance,0.6149371862411499
translation,45,199,results,nph-w/o critic,performs,worst,nph-w/o critic performs worst,0.692115068435669
translation,45,199,results,worst,in,every metric,worst in every metric,0.5690885782241821
translation,45,199,results,worst,compared to,all baselines,worst compared to all baselines,0.6742016673088074
translation,45,199,results,every metric,compared to,all baselines,every metric compared to all baselines,0.6450023055076599
translation,45,199,results,results,find that,nph-w/o critic,results find that nph-w/o critic,0.6421399116516113
translation,45,205,results,dramatic difference,in,both perplexity and retrieval performance,dramatic difference in both perplexity and retrieval performance,0.5330135226249695
translation,45,205,results,results,notice,dramatic difference,results notice dramatic difference,0.7565371990203857
translation,45,214,results,hallucination critic,achieves,precision,hallucination critic achieves precision,0.6433790326118469
translation,45,214,results,precision,of,97.5 %,precision of 97.5 %,0.5368227958679199
translation,45,214,results,precision,of,95.5 %,precision of 95.5 %,0.5330206751823425
translation,45,214,results,precision,of,97.0 %,precision of 97.0 %,0.5436971187591553
translation,45,214,results,97.5 %,for,gpt2 - kb responses,97.5 % for gpt2 - kb responses,0.6150940656661987
translation,45,214,results,95.5 %,for,adapterbot,95.5 % for adapterbot,0.6168613433837891
translation,45,214,results,97.0 %,for,gpt2 -ke,97.0 % for gpt2 -ke,0.6517173051834106
translation,45,214,results,results,see that,hallucination critic,results see that hallucination critic,0.7096356749534607
translation,46,159,ablation-analysis,our model,drops in,all consistency types,our model drops in all consistency types,0.6534096598625183
translation,46,159,ablation-analysis,ablation analysis,observe,our model,ablation analysis observe our model,0.6410791873931885
translation,46,130,experimental-setup,batch size,use in,our framework,batch size use in our framework,0.5695534348487854
translation,46,130,experimental-setup,batch size,selected from,"{ 4 , 8 }","batch size selected from { 4 , 8 }",0.5879337787628174
translation,46,130,experimental-setup,our framework,selected from,"{ 4 , 8 }","our framework selected from { 4 , 8 }",0.6146047115325928
translation,46,130,experimental-setup,learning rate,selected from,{ 5e ?6 } to { 2e ?5 },learning rate selected from { 5e ?6 } to { 2e ?5 },0.6128279566764832
translation,46,130,experimental-setup,learning rate,with,step,learning rate with step,0.647045373916626
translation,46,130,experimental-setup,{ 5e ?6 } to { 2e ?5 },with,step,{ 5e ?6 } to { 2e ?5 } with step,0.6261897683143616
translation,46,130,experimental-setup,step,of,{ 1e ?6 }.,step of { 1e ?6 }.,0.5326180458068848
translation,46,130,experimental-setup,pre-trained models,has,batch size,pre-trained models has batch size,0.5433884263038635
translation,46,130,experimental-setup,experimental setup,For,pre-trained models,experimental setup For pre-trained models,0.5756514072418213
translation,46,131,experimental-setup,max length,to,512 tokens,max length to 512 tokens,0.544956624507904
translation,46,131,experimental-setup,512 tokens,for,models,512 tokens for models,0.6224441528320312
translation,46,131,experimental-setup,models,except,longformer,models except longformer,0.7098813652992249
translation,46,131,experimental-setup,experimental setup,set,max length,experimental setup set max length,0.6685183644294739
translation,46,133,experimental-setup,experiments,conducted at,titan xp and tesla v100 gpus,experiments conducted at titan xp and tesla v100 gpus,0.5584874153137207
translation,46,133,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,46,8,model,novel dataset,for,consistency identification,novel dataset for consistency identification,0.579655110836029
translation,46,8,model,consistency identification,in,taskoriented dialog system,consistency identification in taskoriented dialog system,0.5087951421737671
translation,46,8,model,ci - tod,has,novel dataset,ci - tod has novel dataset,0.6123195290565491
translation,46,8,model,model,introduce,ci - tod,model introduce ci - tod,0.6736096739768982
translation,46,144,results,human performance,is,93.2 %,human performance is 93.2 %,0.5498852133750916
translation,46,145,results,non pre-trained and pretrained models,perform,significantly worse,non pre-trained and pretrained models perform significantly worse,0.5846119523048401
translation,46,145,results,significantly worse,than,humans,significantly worse than humans,0.6474841833114624
translation,46,145,results,results,all of,non pre-trained and pretrained models,results all of non pre-trained and pretrained models,0.5434979796409607
translation,46,151,results,good,at,deciding,good at deciding,0.5766879320144653
translation,46,151,results,deciding,has,all consistency types,deciding has all consistency types,0.5504451990127563
translation,46,151,results,results,observe that,humans,results observe that humans,0.565315842628479
translation,46,152,results,best pre-trained model ( bart ),obtains,worst results,best pre-trained model ( bart ) obtains worst results,0.566098690032959
translation,46,152,results,worst results,on,hi type,worst results on hi type,0.5493747591972351
translation,46,152,results,results,find that,best pre-trained model ( bart ),results find that best pre-trained model ( bart ),0.6347001194953918
translation,46,158,results,bart,without,contextual information,bart without contextual information,0.7637618780136108
translation,46,164,results,multitask training,outperforms,separate task training paradigm,multitask training outperforms separate task training paradigm,0.6711064577102661
translation,46,164,results,separate task training paradigm,in,all metrics,separate task training paradigm in all metrics,0.4650975465774536
translation,46,170,results,scores,positively correlated with,human judgments,scores positively correlated with human judgments,0.6940082311630249
translation,46,170,results,pearson correlation coefficient,of,0.9,pearson correlation coefficient of 0.9,0.5495741367340088
translation,46,170,results,results,see that,scores,results see that scores,0.6290275454521179
translation,47,107,ablation-analysis,intent detection performance,improved by,slot context knowledge,intent detection performance improved by slot context knowledge,0.696170449256897
translation,47,107,ablation-analysis,slot filling,minimizing,intent detection objective function,slot filling minimizing intent detection objective function,0.7622163891792297
translation,47,166,ablation-analysis,slot filling,improved by,almost 1.0 %,slot filling improved by almost 1.0 %,0.7309341430664062
translation,47,166,ablation-analysis,sentence accuracy,by,1.4 %,sentence accuracy by 1.4 %,0.5595094561576843
translation,47,166,ablation-analysis,ablation analysis,has,slot filling,ablation analysis has slot filling,0.5658401250839233
translation,47,178,ablation-analysis,dramatically,compared with,our proposed model,dramatically compared with our proposed model,0.706153392791748
translation,47,178,ablation-analysis,key,has,-value memory and gating architecture,key has -value memory and gating architecture,0.5664380788803101
translation,47,178,ablation-analysis,key,has,performance,key has performance,0.5850120782852173
translation,47,178,ablation-analysis,-value memory and gating architecture,has,performance,-value memory and gating architecture has performance,0.5806261897087097
translation,47,178,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,47,178,ablation-analysis,drops,has,dramatically,drops has dramatically,0.6438937783241272
translation,47,178,ablation-analysis,ablation analysis,remove,key,ablation analysis remove key,0.722849428653717
translation,47,186,ablation-analysis,gating mechanism,has,performance,gating mechanism has performance,0.5664674639701843
translation,47,186,ablation-analysis,performance,has,improves further,performance has improves further,0.6148099303245544
translation,47,186,ablation-analysis,ablation analysis,add,gating mechanism,ablation analysis add gating mechanism,0.559248149394989
translation,47,151,hyperparameters,l2 reularization,used in,our model,l2 reularization used in our model,0.668938159942627
translation,47,151,hyperparameters,l2 reularization,is,1 ? 10 ?6,l2 reularization is 1 ? 10 ?6,0.5800275206565857
translation,47,151,hyperparameters,our model,is,1 ? 10 ?6,our model is 1 ? 10 ?6,0.6127349734306335
translation,47,151,hyperparameters,dropout ratio,set to,0.4,dropout ratio set to 0.4,0.6950680613517761
translation,47,151,hyperparameters,0.4,for reducing,overfit,0.4 for reducing overfit,0.635857343673706
translation,47,151,hyperparameters,hyperparameters,has,l2 reularization,hyperparameters has l2 reularization,0.4965498745441437
translation,47,151,hyperparameters,hyperparameters,has,dropout ratio,hyperparameters has dropout ratio,0.5014679431915283
translation,47,152,hyperparameters,memory columns,set to,20,memory columns set to 20,0.7496505379676819
translation,47,152,hyperparameters,dimensions,of,memory column vectors,dimensions of memory column vectors,0.5921913385391235
translation,47,152,hyperparameters,memory column vectors,set to,64,memory column vectors set to 64,0.6745231747627258
translation,47,152,hyperparameters,memory column vectors,set to,200,memory column vectors set to 200,0.7203054428100586
translation,47,152,hyperparameters,64,for,atis,64 for atis,0.7426815629005432
translation,47,152,hyperparameters,200,for,snips,200 for snips,0.6646126508712769
translation,47,152,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,47,153,hyperparameters,optimizer,is,"adam ( kingma and ba , 2014 )","optimizer is adam ( kingma and ba , 2014 )",0.5048646926879883
translation,47,153,hyperparameters,hyperparameters,has,optimizer,hyperparameters has optimizer,0.5107399225234985
translation,47,9,model,novel approach,to model,long-term slot context,novel approach to model long-term slot context,0.7274552583694458
translation,47,9,model,novel approach,to fully utilize,semantic correlation,novel approach to fully utilize semantic correlation,0.6746625900268555
translation,47,9,model,semantic correlation,between,slots and intents,semantic correlation between slots and intents,0.638359546661377
translation,47,9,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,47,9,model,model,to fully utilize,semantic correlation,model to fully utilize semantic correlation,0.6837630867958069
translation,47,10,model,key-value memory network,to model,slot context dynamically,key-value memory network to model slot context dynamically,0.6771919131278992
translation,47,10,model,key-value memory network,to track,more important slot tags,key-value memory network to track more important slot tags,0.7201180458068848
translation,47,10,model,more important slot tags,fed into,our decoder,more important slot tags fed into our decoder,0.7093884944915771
translation,47,10,model,our decoder,for,slot tagging,our decoder for slot tagging,0.670031726360321
translation,47,10,model,model,adopt,key-value memory network,model adopt key-value memory network,0.6418228149414062
translation,47,11,model,gated memory information,to perform,intent detection,gated memory information to perform intent detection,0.6301238536834717
translation,47,11,model,model,has,gated memory information,model has gated memory information,0.5833425521850586
translation,47,33,model,new framework,to jointly model,intent detection and slot filling,new framework to jointly model intent detection and slot filling,0.7141131162643433
translation,47,33,model,intent detection and slot filling,deeper level of,semantic modeling,intent detection and slot filling deeper level of semantic modeling,0.6571329236030579
translation,47,33,model,model,propose,new framework,model propose new framework,0.7318839430809021
translation,47,62,model,memory networks,to model,long-term slot context knowledge,memory networks to model long-term slot context knowledge,0.6785704493522644
translation,47,62,model,memory networks,to model,interaction,memory networks to model interaction,0.6984056830406189
translation,47,62,model,model,demonstrate,memory networks,model demonstrate memory networks,0.5672133564949036
translation,47,69,model,memory network,encodes,longterm slot context information,memory network encodes longterm slot context information,0.7191110253334045
translation,47,69,model,longterm slot context information,by incorporating,historical slot tags,longterm slot context information by incorporating historical slot tags,0.6577233076095581
translation,47,69,model,historical slot tags,through,memory attention and write operations,historical slot tags through memory attention and write operations,0.7001311182975769
translation,47,69,model,memory attention and write operations,of,memory network,memory attention and write operations of memory network,0.5607194900512695
translation,47,69,model,model,has,memory network,model has memory network,0.5397774577140808
translation,47,82,model,key-value memory network,memorizes,information,key-value memory network memorizes information,0.7165886163711548
translation,47,82,model,information,large array of,external memory slots,information large array of external memory slots,0.7033867835998535
translation,47,82,model,model,adopt,key-value memory network,model adopt key-value memory network,0.6418228149414062
translation,47,106,model,interaction,between,intent detection,interaction between intent detection,0.6225576996803284
translation,47,106,model,interaction,between,slot filling,interaction between slot filling,0.6748419404029846
translation,47,106,model,interaction,can be,effectively modeled and executed,interaction can be effectively modeled and executed,0.6463503241539001
translation,47,106,model,slot filling,can be,effectively modeled and executed,slot filling can be effectively modeled and executed,0.700954258441925
translation,47,106,model,shared key-value memory,has,interaction,shared key-value memory has interaction,0.5703216791152954
translation,47,106,model,model,Through,shared key-value memory,model Through shared key-value memory,0.6561375260353088
translation,47,104,results,gated intent detection,Sharing,slot context information,gated intent detection Sharing slot context information,0.6813387870788574
translation,47,104,results,slot context information,with,intent detection,slot context information with intent detection,0.6040320992469788
translation,47,104,results,slot context information,improves,intent detection performance,slot context information improves intent detection performance,0.6668798327445984
translation,47,104,results,results,performing,gated intent detection,results performing gated intent detection,0.6338024139404297
translation,47,162,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,47,162,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,47,165,results,our model,achieves,good results,our model achieves good results,0.6495798826217651
translation,47,165,results,good results,in,slot filling,good results in slot filling,0.5700047612190247
translation,47,165,results,good results,in,overall sentence,good results in overall sentence,0.5033976435661316
translation,47,165,results,snips dataset,has,our model,snips dataset has our model,0.6078373193740845
translation,47,165,results,results,in,snips dataset,results in snips dataset,0.53764408826828
translation,47,181,results,key -value memory,does improve,performance,key -value memory does improve performance,0.7367351651191711
translation,47,181,results,performance,in,large scale,performance in large scale,0.576792299747467
translation,47,181,results,results,see that,key -value memory,results see that key -value memory,0.6608364582061768
translation,47,185,results,slot context information,with,intent detection,slot context information with intent detection,0.6040320992469788
translation,47,185,results,slot context information,improves,intent accuracy,slot context information improves intent accuracy,0.6768751740455627
translation,47,185,results,slot filling,through,joint optimization,slot filling through joint optimization,0.6451850533485413
translation,47,185,results,betters,has,slot filling,betters has slot filling,0.6190754771232605
translation,47,185,results,results,Sharing,slot context information,results Sharing slot context information,0.7275393009185791
translation,48,235,ablation-analysis,intent information,to,bert model,intent information to bert model,0.4978703260421753
translation,48,235,ablation-analysis,intent information,causes,noticeable boost,intent information causes noticeable boost,0.6068755984306335
translation,48,235,ablation-analysis,noticeable boost,in,dialog success,noticeable boost in dialog success,0.5391257405281067
translation,48,235,ablation-analysis,noticeable boost,bringing,score,noticeable boost bringing score,0.6779971718788147
translation,48,235,ablation-analysis,score,to,32.3 %,score to 32.3 %,0.546707272529602
translation,48,235,ablation-analysis,ablation analysis,supplying,intent information,ablation analysis supplying intent information,0.7695462703704834
translation,48,236,ablation-analysis,model,with,knowledge of the guidelines,model with knowledge of the guidelines,0.642160177230835
translation,48,236,ablation-analysis,model,dropped,performance,model dropped performance,0.7157953977584839
translation,48,236,ablation-analysis,performance,down to,30.6 %,performance down to 30.6 %,0.6951557993888855
translation,48,236,ablation-analysis,ablation analysis,augmenting,model,ablation analysis augmenting model,0.658530056476593
translation,48,240,ablation-analysis,peak observed performance,of,32.7 %,peak observed performance of 32.7 %,0.5568144917488098
translation,48,241,ablation-analysis,action information,away from,actionbased conversations,action information away from actionbased conversations,0.6565457582473755
translation,48,241,ablation-analysis,action information,causes,major performance drop,action information causes major performance drop,0.7049384713172913
translation,48,241,ablation-analysis,actionbased conversations,has,unsurprisingly,actionbased conversations has unsurprisingly,0.5898828506469727
translation,48,176,baselines,backbone,of,baseline systems,backbone of baseline systems,0.6062275171279907
translation,48,176,baselines,backbone,is,pre-trained transformer - based model,backbone is pre-trained transformer - based model,0.6099031567573547
translation,48,176,baselines,baseline systems,is,pre-trained transformer - based model,baseline systems is pre-trained transformer - based model,0.54212486743927
translation,48,176,baselines,pre-trained transformer - based model,acting as,context encoder,pre-trained transformer - based model acting as context encoder,0.6138313412666321
translation,48,176,baselines,baselines,has,backbone,baselines has backbone,0.5940814018249512
translation,48,211,baselines,two types of frameworks,has,pipeline version,two types of frameworks has pipeline version,0.5720623731613159
translation,48,211,baselines,baselines,experimented with,two types of frameworks,baselines experimented with two types of frameworks,0.7244109511375427
translation,48,6,experiments,customer service dialogue,introduce,action - based conversations dataset ( abcd ),customer service dialogue introduce action - based conversations dataset ( abcd ),0.5998989343643188
translation,48,6,experiments,fully - labeled dataset,with,over 10 k,fully - labeled dataset with over 10 k,0.6198233366012573
translation,48,6,experiments,over 10 k,containing,55 distinct user intents,over 10 k containing 55 distinct user intents,0.6446720957756042
translation,48,6,experiments,human-to-human dialogues,containing,55 distinct user intents,human-to-human dialogues containing 55 distinct user intents,0.6241167187690735
translation,48,6,experiments,55 distinct user intents,requiring,unique sequences of actions,55 distinct user intents requiring unique sequences of actions,0.6596654057502747
translation,48,6,experiments,unique sequences of actions,constrained by,policies,unique sequences of actions constrained by policies,0.7564366459846497
translation,48,6,experiments,policies,to achieve,task success,policies to achieve task success,0.6712995171546936
translation,48,6,experiments,action - based conversations dataset ( abcd ),has,fully - labeled dataset,action - based conversations dataset ( abcd ) has fully - labeled dataset,0.5409214496612549
translation,48,6,experiments,over 10 k,has,human-to-human dialogues,over 10 k has human-to-human dialogues,0.6084703803062439
translation,48,177,model,utterances,together with,[ sep ] token,utterances together with [ sep ] token,0.6529965996742249
translation,48,177,model,utterances,together with,entire input,utterances together with entire input,0.6418194770812988
translation,48,177,model,utterances,together with,word-piece,utterances together with word-piece,0.6782847046852112
translation,48,177,model,utterances,tokenize,entire input,utterances tokenize entire input,0.7110608220100403
translation,48,177,model,entire input,using,word-piece,entire input using word-piece,0.6746445298194885
translation,48,212,model,pipeline version,trains,each subtask separately,pipeline version trains each subtask separately,0.7927865982055664
translation,48,212,model,pipeline version,optimizes,all tasks jointly,pipeline version optimizes all tasks jointly,0.7291898727416992
translation,48,212,model,end-to - end,optimizes,all tasks jointly,end-to - end optimizes all tasks jointly,0.7553566694259644
translation,48,212,model,model,has,pipeline version,model has pipeline version,0.5884581804275513
translation,48,213,model,pipeline model,uses,bert model,pipeline model uses bert model,0.5927296876907349
translation,48,213,model,bert model,trained with,radam optimizer,bert model trained with radam optimizer,0.7670016288757324
translation,48,213,model,model,has,pipeline model,model has pipeline model,0.5828603506088257
translation,48,31,results,agent guidelines,boosts,performance,agent guidelines boosts performance,0.7595413327217102
translation,48,31,results,performance,with,top models,performance with top models,0.6658898591995239
translation,48,31,results,top models,relying on,both aspects,top models relying on both aspects,0.7187279462814331
translation,48,31,results,both aspects,to reach,31.9 % accuracy,both aspects to reach 31.9 % accuracy,0.6404853463172913
translation,48,218,results,both tasks,moving from,pipeline architecture,both tasks moving from pipeline architecture,0.6179582476615906
translation,48,218,results,pipeline architecture,to,jointly trained method,pipeline architecture to jointly trained method,0.5376364588737488
translation,48,218,results,jointly trained method,displayed,noticeable improvement,jointly trained method displayed noticeable improvement,0.6778267025947571
translation,48,218,results,noticeable improvement,in,accuracy,noticeable improvement in accuracy,0.5189640522003174
translation,48,218,results,results,For,both tasks,results For both tasks,0.5271528959274292
translation,48,220,results,ast task,found,steady improvements,ast task found steady improvements,0.6503168344497681
translation,48,220,results,steady improvements,move from,older,steady improvements move from older,0.7232733964920044
translation,48,220,results,steady improvements,move from,newer models,steady improvements move from newer models,0.6851791739463806
translation,48,220,results,newer models,with,vanilla bert,newer models with vanilla bert,0.6620171070098877
translation,48,220,results,vanilla bert,at,59.5 % accuracy,vanilla bert at 59.5 % accuracy,0.5453165769577026
translation,48,220,results,roberta,doing,best,roberta doing best,0.5863946080207825
translation,48,220,results,best,at,65.8 %,best at 65.8 %,0.5448032021522522
translation,48,220,results,results,In,ast task,results In ast task,0.4967501759529114
translation,48,221,results,roberta - large,has,outperforms,roberta - large has outperforms,0.6510949730873108
translation,48,221,results,outperforms,has,bert,outperforms has bert,0.7186265587806702
translation,48,221,results,results,For,cds task,results For cds task,0.5451756715774536
translation,49,7,model,novel complementary policy learning ( cpl ) framework,exploits,complementary advantages,novel complementary policy learning ( cpl ) framework exploits complementary advantages,0.7514690160751343
translation,49,7,model,novel complementary policy learning ( cpl ) framework,exploits,deep q-network ( dqn ) policy,novel complementary policy learning ( cpl ) framework exploits deep q-network ( dqn ) policy,0.6937824487686157
translation,49,7,model,complementary advantages,of,episodic memory ( em ) policy,complementary advantages of episodic memory ( em ) policy,0.5381085276603699
translation,49,7,model,complementary advantages,of,deep q-network ( dqn ) policy,complementary advantages of deep q-network ( dqn ) policy,0.5451586246490479
translation,49,7,model,deep q-network ( dqn ) policy,to achieve,fast and effective dialogue policy learning,deep q-network ( dqn ) policy to achieve fast and effective dialogue policy learning,0.6268178224563599
translation,49,7,model,model,proposes,novel complementary policy learning ( cpl ) framework,model proposes novel complementary policy learning ( cpl ) framework,0.667987048625946
translation,49,8,model,confidence controller,to control,complementary time,confidence controller to control complementary time,0.7385236620903015
translation,49,8,model,complementary time,according to,relative efficacy,complementary time according to relative efficacy,0.6825053691864014
translation,49,8,model,relative efficacy,at,different stages,relative efficacy at different stages,0.5552424788475037
translation,50,149,ablation-analysis,any key component,of,comet,any key component of comet,0.6493011713027954
translation,50,149,ablation-analysis,any key component,has,both the bleu and entity f1 metrics,any key component has both the bleu and entity f1 metrics,0.5970274806022644
translation,50,149,ablation-analysis,comet,has,both the bleu and entity f1 metrics,comet has both the bleu and entity f1 metrics,0.6109572052955627
translation,50,149,ablation-analysis,both the bleu and entity f1 metrics,has,degrade,both the bleu and entity f1 metrics has degrade,0.5750299692153931
translation,50,150,ablation-analysis,memory mask,is,removed,memory mask is removed,0.5950101017951965
translation,50,150,ablation-analysis,entity f1 score,drops to,49.6,entity f1 score drops to 49.6,0.6530864238739014
translation,50,150,ablation-analysis,memory mask,has,entity f1 score,memory mask has entity f1 score,0.5666901469230652
translation,50,150,ablation-analysis,removed,has,entity f1 score,removed has entity f1 score,0.535184383392334
translation,50,150,ablation-analysis,ablation analysis,If,memory mask,ablation analysis If memory mask,0.6221275925636292
translation,50,152,ablation-analysis,variant,has,without the sum. rep,variant has without the sum. rep,0.6096456050872803
translation,50,152,ablation-analysis,ablation analysis,For,variant,ablation analysis For variant,0.6561044454574585
translation,50,154,ablation-analysis,information,from,dialogue history ( h enc n ),information from dialogue history ( h enc n ),0.5742950439453125
translation,50,154,ablation-analysis,information,from,memory ( e k ),information from memory ( e k ),0.538280725479126
translation,50,154,ablation-analysis,ablation analysis,remove,gate,ablation analysis remove gate,0.746170163154602
translation,50,156,ablation-analysis,gate mechanism,to fuse,both information sources,gate mechanism to fuse both information sources,0.7227263450622559
translation,50,156,ablation-analysis,both information sources,helpful for,entity linking,both information sources helpful for entity linking,0.6233322620391846
translation,50,156,ablation-analysis,ablation analysis,using,gate mechanism,ablation analysis using gate mechanism,0.6763554811477661
translation,50,157,ablation-analysis,performance,drops to,62.3,performance drops to 62.3,0.6831132769584656
translation,50,157,ablation-analysis,"l 2,1norm",has,performance,"l 2,1norm has performance",0.5975567698478699
translation,50,157,ablation-analysis,ablation analysis,When removing,"l 2,1norm","ablation analysis When removing l 2,1norm",0.7752171754837036
translation,50,158,ablation-analysis,our context - aware memory,with,other ways,our context - aware memory with other ways,0.6491985321044922
translation,50,158,ablation-analysis,other ways,of representing,kb,other ways of representing kb,0.7227404713630676
translation,50,158,ablation-analysis,ablation analysis,replace,our context - aware memory,ablation analysis replace our context - aware memory,0.5795317888259888
translation,50,168,ablation-analysis,more full attention,added,more performance,more full attention added more performance,0.6724876165390015
translation,50,168,ablation-analysis,more performance,of,comet,more performance of comet,0.6068552732467651
translation,50,168,ablation-analysis,drops,in,all of the metrics,drops in all of the metrics,0.5274286270141602
translation,50,168,ablation-analysis,more full attention,has,more performance,more full attention has more performance,0.5810967683792114
translation,50,168,ablation-analysis,ablation analysis,has,more full attention,ablation analysis has more full attention,0.5321798920631409
translation,50,124,baselines,mem2seq,incorporates,multi-hop attention mechanism,mem2seq incorporates multi-hop attention mechanism,0.7052817940711975
translation,50,124,baselines,mem2seq,incorporates,multi-hop attention mechanism,mem2seq incorporates multi-hop attention mechanism,0.7052817940711975
translation,50,124,baselines,multi-hop attention mechanism,in,memory networks,multi-hop attention mechanism in memory networks,0.5276318788528442
translation,50,124,baselines,multi-hop attention mechanism,into,pointer networks,multi-hop attention mechanism into pointer networks,0.5681375861167908
translation,50,126,baselines,glmp ( triplet,uses,global memory encoder,glmp ( triplet uses global memory encoder,0.5820603966712952
translation,50,126,baselines,glmp ( triplet,uses,local memory decoder,glmp ( triplet uses local memory decoder,0.5874788165092468
translation,50,126,baselines,local memory decoder,to incorporate,external knowledge,local memory decoder to incorporate external knowledge,0.6507510542869568
translation,50,126,baselines,external knowledge,into,learning framework,external knowledge into learning framework,0.5728738307952881
translation,50,126,baselines,baselines,has,glmp ( triplet,baselines has glmp ( triplet,0.5978425145149231
translation,50,127,baselines,df - net,applies,dynamic fusion mechanism,df - net applies dynamic fusion mechanism,0.5850232839584351
translation,50,127,baselines,dynamic fusion mechanism,to transfer,knowledge,dynamic fusion mechanism to transfer knowledge,0.652101993560791
translation,50,127,baselines,knowledge,in,different domains,knowledge in different domains,0.5264222621917725
translation,50,127,baselines,baselines,has,df - net,baselines has df - net,0.5995398759841919
translation,50,128,baselines,graphdialog,exploits,graph structural information,graphdialog exploits graph structural information,0.704570472240448
translation,50,128,baselines,graph structural information,in,kb,graph structural information in kb,0.5278247594833374
translation,50,128,baselines,graph structural information,in,dependency parsing tree,graph structural information in dependency parsing tree,0.47768884897232056
translation,50,128,baselines,graph structural information,in,dependency parsing tree,graph structural information in dependency parsing tree,0.47768884897232056
translation,50,128,baselines,dependency parsing tree,of,dialogue,dependency parsing tree of dialogue,0.5283721685409546
translation,50,128,baselines,baselines,has,graphdialog,baselines has graphdialog,0.602821946144104
translation,50,189,baselines,dialogue systems,with,end-to - end memory networks,dialogue systems with end-to - end memory networks,0.6178399920463562
translation,50,189,baselines,dialogue state,as,fixed - size distributed representation,dialogue state as fixed - size distributed representation,0.5161198973655701
translation,50,125,experiments,kb - retriever,improves,entity - consistency,kb - retriever improves entity - consistency,0.6989350318908691
translation,50,125,experiments,entity - consistency,by first selecting,target row,entity - consistency by first selecting target row,0.7008798718452454
translation,50,125,experiments,entity - consistency,picking,relevant column,entity - consistency picking relevant column,0.7065188884735107
translation,50,125,experiments,relevant column,in,this row,relevant column in this row,0.5217530727386475
translation,50,116,hyperparameters,dimension,of,embeddings and hidden vectors,dimension of embeddings and hidden vectors,0.5848599672317505
translation,50,116,hyperparameters,dimension,set to,512,dimension set to 512,0.7443968057632446
translation,50,116,hyperparameters,embeddings and hidden vectors,set to,512,embeddings and hidden vectors set to 512,0.6693189740180969
translation,50,116,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,50,117,hyperparameters,number of layers ( n ),in,dialogue history encoder and response generation decoder,number of layers ( n ) in dialogue history encoder and response generation decoder,0.484732061624527
translation,50,117,hyperparameters,dialogue history encoder and response generation decoder,set to,6,dialogue history encoder and response generation decoder set to 6,0.6890340447425842
translation,50,117,hyperparameters,hyperparameters,has,number of layers ( n ),hyperparameters has number of layers ( n ),0.5112301111221313
translation,50,118,hyperparameters,number of layers,for,context - aware memory generation,number of layers for context - aware memory generation,0.5703897476196289
translation,50,118,hyperparameters,context - aware memory generation,set to,3,context - aware memory generation set to 3,0.618463397026062
translation,50,118,hyperparameters,k ),set to,3,k ) set to 3,0.7173712253570557
translation,50,118,hyperparameters,context - aware memory generation,has,k ),context - aware memory generation has k ),0.6087154150009155
translation,50,118,hyperparameters,hyperparameters,has,number of layers,hyperparameters has number of layers,0.5200415253639221
translation,50,119,hyperparameters,number of heads,in,each part,number of heads in each part,0.5272340178489685
translation,50,119,hyperparameters,each part,of,comet,each part of comet,0.6422222256660461
translation,50,119,hyperparameters,comet,set to,8,comet set to 8,0.7703431844711304
translation,50,119,hyperparameters,hyperparameters,has,number of heads,hyperparameters has number of heads,0.5380078554153442
translation,50,121,hyperparameters,adam optimizer,to train,our model from scratch,adam optimizer to train our model from scratch,0.6635767221450806
translation,50,121,hyperparameters,our model from scratch,with,learning rate,our model from scratch with learning rate,0.6011796593666077
translation,50,121,hyperparameters,learning rate,of,1e ?4,learning rate of 1e ?4,0.6287522315979004
translation,50,121,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,50,6,model,entity representation,by dynamically perceiving,all the relevant entities,entity representation by dynamically perceiving all the relevant entities,0.662185549736023
translation,50,6,model,entity representation,by dynamically perceiving,dialogue history,entity representation by dynamically perceiving dialogue history,0.7000526189804077
translation,50,6,model,model,fully contextualize,entity representation,model fully contextualize entity representation,0.6837443709373474
translation,50,7,model,contextaware memory enhanced transformer framework ( comet ),treats,kb,contextaware memory enhanced transformer framework ( comet ) treats kb,0.6847833395004272
translation,50,7,model,contextaware memory enhanced transformer framework ( comet ),leverages,novel memory mask,contextaware memory enhanced transformer framework ( comet ) leverages novel memory mask,0.7168859839439392
translation,50,7,model,kb,as,sequence,kb as sequence,0.5193776488304138
translation,50,7,model,novel memory mask,to enforce,entity,novel memory mask to enforce entity,0.7202920913696289
translation,50,7,model,novel memory mask,avoiding,distraction,novel memory mask avoiding distraction,0.6759898066520691
translation,50,7,model,entity,to only focus on,relevant entities,entity to only focus on relevant entities,0.6344221830368042
translation,50,7,model,entity,to only focus on,dialogue history,entity to only focus on dialogue history,0.6834559440612793
translation,50,7,model,model,propose,contextaware memory enhanced transformer framework ( comet ),model propose contextaware memory enhanced transformer framework ( comet ),0.6896865367889404
translation,50,30,model,context - aware memory enhanced transformer ( comet ),provides,unified solution,context - aware memory enhanced transformer ( comet ) provides unified solution,0.6703332662582397
translation,50,30,model,unified solution,to fully contextualize,entity,unified solution to fully contextualize entity,0.693307101726532
translation,50,30,model,entity,with,awareness,entity with awareness,0.6599805951118469
translation,50,30,model,awareness,of,kb and dialogue contexts,awareness of kb and dialogue contexts,0.5672513246536255
translation,50,30,model,awareness,both,kb and dialogue contexts,awareness both kb and dialogue contexts,0.6733796000480652
translation,50,30,model,model,propose,context - aware memory enhanced transformer ( comet ),model propose context - aware memory enhanced transformer ( comet ),0.6968954801559448
translation,50,31,model,memory - masked en-coder,to encode,entity sequence,memory - masked en-coder to encode entity sequence,0.7087703347206116
translation,50,31,model,entity sequence,of,kb,entity sequence of kb,0.5369883179664612
translation,50,31,model,entity sequence,along with,information,entity sequence along with information,0.5933040976524353
translation,50,31,model,information,of,dialogue history,information of dialogue history,0.5463536977767944
translation,50,120,model,greedy strategy,used without,beam-search,greedy strategy used without beam-search,0.708855926990509
translation,50,120,model,beam-search,during,decoding,beam-search during decoding,0.6525691151618958
translation,50,120,model,model,has,greedy strategy,model has greedy strategy,0.5817820429801941
translation,50,190,model,belief spans,to track,dialogue believes,belief spans to track dialogue believes,0.7084531784057617
translation,50,190,model,dialogue believes,allowing,task - oriented dialogue systems,dialogue believes allowing task - oriented dialogue systems,0.6550394296646118
translation,50,190,model,task - oriented dialogue systems,to be modeled in,sequence - tosequence way,task - oriented dialogue systems to be modeled in sequence - tosequence way,0.671510636806488
translation,50,190,model,model,designs,belief spans,model designs belief spans,0.6734299659729004
translation,50,132,results,comet,achieves,best performance,comet achieves best performance,0.7210606932640076
translation,50,132,results,best performance,over,both datasets,best performance over both datasets,0.6362408399581909
translation,50,133,results,previous methods,by,2.9 %,previous methods by 2.9 %,0.519507884979248
translation,50,133,results,previous methods,by,2.1 %,previous methods by 2.1 %,0.5325247645378113
translation,50,133,results,2.9 %,on,smd dataset,2.9 % on smd dataset,0.5512402057647705
translation,50,133,results,2.1 %,on,multi-woz 2.1 dataset,2.1 % on multi-woz 2.1 dataset,0.4705991744995117
translation,50,133,results,outperforms,has,previous methods,outperforms has previous methods,0.5876122713088989
translation,50,133,results,results,for,bleu score,results for bleu score,0.577290415763855
translation,50,134,results,comet,achieves,highest entity f1 score,comet achieves highest entity f1 score,0.6429087519645691
translation,50,134,results,highest entity f1 score,on,both datasets,highest entity f1 score on both datasets,0.48547273874282837
translation,50,134,results,results,has,comet,results has comet,0.6075225472450256
translation,50,135,results,improvements,of,0.9 % and 7.3 %,improvements of 0.9 % and 7.3 %,0.6118123531341553
translation,50,135,results,0.9 % and 7.3 %,attained on,smd and multi-woz 2.1 datasets,0.9 % and 7.3 % attained on smd and multi-woz 2.1 datasets,0.5485938787460327
translation,50,135,results,results,has,improvements,results has improvements,0.615561842918396
translation,50,136,results,each domain,has,improvement,each domain has improvement,0.561004638671875
translation,50,136,results,two datasets,has,improvement,two datasets has improvement,0.5851908922195435
translation,50,136,results,results,In,each domain,results In each domain,0.5307285189628601
translation,50,139,results,bleu score,of,comet,bleu score of comet,0.5501707792282104
translation,50,139,results,comet,higher than,kb - transformer,comet higher than kb - transformer,0.7370670437812805
translation,50,139,results,kb - transformer,by,3.4 %,kb - transformer by 3.4 %,0.586463987827301
translation,50,139,results,smd dataset,has,bleu score,smd dataset has bleu score,0.499098539352417
translation,50,139,results,results,On,smd dataset,results On smd dataset,0.5586501359939575
translation,50,140,results,improvement,introduced by,comet,improvement introduced by comet,0.7568864822387695
translation,50,140,results,comet,on,entity f1 score,comet on entity f1 score,0.5167292952537537
translation,50,140,results,entity f1 score,as significant as,26.5 %,entity f1 score as significant as 26.5 %,0.643875002861023
translation,50,140,results,results,has,improvement,results has improvement,0.6248279809951782
translation,50,163,results,other kb representations,by,1.6 %,other kb representations by 1.6 %,0.5256643891334534
translation,50,163,results,1.6 %,of,entity f1,1.6 % of entity f1,0.5684154629707336
translation,50,163,results,kb context,has,our method,kb context has our method,0.5947301983833313
translation,50,163,results,our method,has,still outperform,our method has still outperform,0.6065051555633545
translation,50,163,results,still outperform,has,other kb representations,still outperform has other kb representations,0.5887323021888733
translation,50,163,results,results,only considering,kb context,results only considering kb context,0.693211555480957
translation,51,126,baselines,simpletod baseline,as,off- the-shelf task - oriented dialogue model,simpletod baseline as off- the-shelf task - oriented dialogue model,0.5274267196655273
translation,51,126,baselines,off- the-shelf task - oriented dialogue model,for,arranger and rewriter,off- the-shelf task - oriented dialogue model for arranger and rewriter,0.5971665382385254
translation,51,124,experimental-setup,causal language models,use,12 - layer gpt - 2 ( 117 m parameters ),causal language models use 12 - layer gpt - 2 ( 117 m parameters ),0.5941952466964722
translation,51,124,experimental-setup,12 - layer gpt - 2 ( 117 m parameters ),as,pre-trained language model,12 - layer gpt - 2 ( 117 m parameters ) as pre-trained language model,0.4910431206226349
translation,51,124,experimental-setup,12 - layer gpt - 2 ( 117 m parameters ),fine-tune,ten epochs,12 - layer gpt - 2 ( 117 m parameters ) fine-tune ten epochs,0.6775758862495422
translation,51,124,experimental-setup,pre-trained language model,has,"radford et al. , 2019 )","pre-trained language model has radford et al. , 2019 )",0.5352529287338257
translation,51,124,experimental-setup,experimental setup,for,causal language models,experimental setup for causal language models,0.5466713905334473
translation,51,125,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,51,125,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,51,127,experimental-setup,90m parameter model,on,good chit-chat candidates,90m parameter model on good chit-chat candidates,0.538754940032959
translation,51,127,experimental-setup,90m parameter model,each of,good chit-chat candidates,90m parameter model each of good chit-chat candidates,0.606523871421814
translation,51,127,experimental-setup,good chit-chat candidates,with,associated dialogue history,good chit-chat candidates with associated dialogue history,0.6445472836494446
translation,51,127,experimental-setup,associated dialogue history,as,context,associated dialogue history as context,0.5428376197814941
translation,51,127,experimental-setup,context,from,training set,context from training set,0.5483828186988831
translation,51,127,experimental-setup,training set,of,accentor - sgd,training set of accentor - sgd,0.5715030431747437
translation,51,127,experimental-setup,off-the-shelf chit- chat model,in,arranger and rewriter,off-the-shelf chit- chat model in arranger and rewriter,0.5367695689201355
translation,51,127,experimental-setup,experimental setup,fine- tune,90m parameter model,experimental setup fine- tune 90m parameter model,0.648063063621521
translation,51,128,experimental-setup,"roberta base ( liu et al. , 2019 )",as,pre-trained language model,"roberta base ( liu et al. , 2019 ) as pre-trained language model",0.46306347846984863
translation,51,128,experimental-setup,pre-trained language model,for,arranger,pre-trained language model for arranger,0.6386246681213379
translation,51,128,experimental-setup,three epochs,with,learning rate,three epochs with learning rate,0.6689472794532776
translation,51,128,experimental-setup,learning rate,of,2 ? 10 ?5,learning rate of 2 ? 10 ?5,0.6528677344322205
translation,51,128,experimental-setup,batch size,of,24,batch size of 24,0.6606289148330688
translation,51,128,experimental-setup,experimental setup,use,"roberta base ( liu et al. , 2019 )","experimental setup use roberta base ( liu et al. , 2019 )",0.5792344212532043
translation,51,5,model,chit - chat,to ENhance,task - oriented dialogues ( accentor ),chit - chat to ENhance task - oriented dialogues ( accentor ),0.6927746534347534
translation,51,6,model,human ? ai collaborative data collection approach,for generating,diverse chitchat responses,human ? ai collaborative data collection approach for generating diverse chitchat responses,0.7207500338554382
translation,51,6,model,diverse chitchat responses,to augment,task - oriented dialogues,diverse chitchat responses to augment task - oriented dialogues,0.6965770125389099
translation,51,6,model,task - oriented dialogues,with,minimal annotation effort,task - oriented dialogues with minimal annotation effort,0.6035155653953552
translation,51,6,model,model,propose,human ? ai collaborative data collection approach,model propose human ? ai collaborative data collection approach,0.6762983798980713
translation,51,16,model,chit - chat,to ENhance,task - oriented dialogues,chit - chat to ENhance task - oriented dialogues,0.6687051057815552
translation,51,16,model,task - oriented dialogues,has,accentor ),task - oriented dialogues has accentor ),0.6359236836433411
translation,51,18,model,human ? ai collaborative data construction approach,effectively add,suitable chit-chat,human ? ai collaborative data construction approach effectively add suitable chit-chat,0.7465968132019043
translation,51,18,model,suitable chit-chat,to,beginning or end,suitable chit-chat to beginning or end,0.6026417016983032
translation,51,18,model,beginning or end,of,system responses,beginning or end of system responses,0.6090120673179626
translation,51,18,model,system responses,in,existing task - oriented dialogue datasets,system responses in existing task - oriented dialogue datasets,0.4504584074020386
translation,51,18,model,model,propose,human ? ai collaborative data construction approach,model propose human ? ai collaborative data construction approach,0.6738790273666382
translation,51,28,model,data augmentation approach,for generating,diverse chit- chat supervisory data,data augmentation approach for generating diverse chit- chat supervisory data,0.7205705642700195
translation,51,28,model,data augmentation approach,leveraging,custom filter model,data augmentation approach leveraging custom filter model,0.7296812534332275
translation,51,28,model,diverse chit- chat supervisory data,for,task - oriented dialogues,diverse chit- chat supervisory data for task - oriented dialogues,0.6390101313591003
translation,51,28,model,diverse chit- chat supervisory data,leveraging,pre-trained generative models,diverse chit- chat supervisory data leveraging pre-trained generative models,0.6950944662094116
translation,51,28,model,custom filter model,to minimize,human annotation effort,custom filter model to minimize human annotation effort,0.6337356567382812
translation,51,28,model,new versions,of,popular task -oriented datasets,new versions of popular task -oriented datasets,0.5314014554023743
translation,51,28,model,annotations,to,23.8 k dialogues,annotations to 23.8 k dialogues,0.567336916923523
translation,51,28,model,substantially outperforming,has,state - of - the - art approach,substantially outperforming has state - of - the - art approach,0.573478639125824
translation,51,28,model,model,propose,data augmentation approach,model propose data augmentation approach,0.6756566762924194
translation,51,28,model,model,propose,human evaluation of engagingness,model propose human evaluation of engagingness,0.6674681901931763
translation,51,28,model,model,propose,interestingness,model propose interestingness,0.6549332141876221
translation,51,28,model,model,propose,humanness,model propose humanness,0.700015664100647
translation,51,28,model,model,in terms of,human evaluation of engagingness,model in terms of human evaluation of engagingness,0.685415506362915
translation,51,28,model,model,in terms of,interestingness,model in terms of interestingness,0.6757474541664124
translation,51,28,model,model,in terms of,humanness,model in terms of humanness,0.6376338005065918
translation,51,136,results,chit-chat augmented dialogues,from,accentor - sgd,chit-chat augmented dialogues from accentor - sgd,0.6102372407913208
translation,51,136,results,human judges,than,originals,human judges than originals,0.5775282382965088
translation,51,137,results,different injection frequency ranges,is,best,different injection frequency ranges is best,0.571022629737854
translation,51,137,results,"( 0.2 , 0.3 ]",is,best,"( 0.2 , 0.3 ] is best",0.5387201905250549
translation,51,137,results,different injection frequency ranges,has,"( 0.2 , 0.3 ]","different injection frequency ranges has ( 0.2 , 0.3 ]",0.5473327040672302
translation,51,137,results,results,Among,different injection frequency ranges,results Among different injection frequency ranges,0.5969588160514832
translation,51,158,results,our proposed models,achieve,similar task performance level,our proposed models achieve similar task performance level,0.6810991764068604
translation,51,158,results,similar task performance level,compared with,simpletod baseline,similar task performance level compared with simpletod baseline,0.6873432397842407
translation,51,158,results,results,has,our proposed models,results has our proposed models,0.5751316547393799
translation,51,159,results,proposed models,achieve,lower bleu - 4 orig,proposed models achieve lower bleu - 4 orig,0.6759455800056458
translation,51,159,results,proposed models,achieve,higher bleu - 4 aug,proposed models achieve higher bleu - 4 aug,0.6746742129325867
translation,51,167,results,outperform,over,four acute - eval metrics,outperform over four acute - eval metrics,0.6798778176307678
translation,51,167,results,simpletod baseline,over,four acute - eval metrics,simpletod baseline over four acute - eval metrics,0.6775096654891968
translation,51,167,results,all of the chit-chat augmented models,has,outperform,all of the chit-chat augmented models has outperform,0.6016278862953186
translation,51,167,results,outperform,has,simpletod baseline,outperform has simpletod baseline,0.6039119362831116
translation,51,167,results,results,has,all of the chit-chat augmented models,results has all of the chit-chat augmented models,0.5429968237876892
translation,51,168,results,no one,shows,clear win,no one shows clear win,0.6786431074142456
translation,51,168,results,clear win,over,other two,clear win over other two,0.6579949855804443
translation,51,168,results,other two,on,quantitative level,other two on quantitative level,0.53546541929245
translation,51,168,results,chit-chat augmented models,has,no one,chit-chat augmented models has no one,0.6711503267288208
translation,51,168,results,results,Among,chit-chat augmented models,results Among chit-chat augmented models,0.5732058882713318
translation,51,171,results,modified arranger,achieves,higher win percentage,modified arranger achieves higher win percentage,0.6705812215805054
translation,51,171,results,higher win percentage,over,simpletod,higher win percentage over simpletod,0.6920512318611145
translation,51,171,results,original arranger,has,modified arranger,original arranger has modified arranger,0.5961115956306458
translation,51,171,results,results,Compared with,original arranger,results Compared with original arranger,0.7044768333435059
translation,52,25,hyperparameters,new pseudo-labeled examples,start with,small seed set,new pseudo-labeled examples start with small seed set,0.6334152817726135
translation,52,25,hyperparameters,small seed set,of,known oos examples,small seed set of known oos examples,0.581680417060852
translation,52,25,hyperparameters,hyperparameters,To create,new pseudo-labeled examples,hyperparameters To create new pseudo-labeled examples,0.6229062080383301
translation,52,7,model,gold,as,orthogonal technique,gold as orthogonal technique,0.5868891477584839
translation,52,7,model,orthogonal technique,augments,existing data,orthogonal technique augments existing data,0.7640251517295837
translation,52,7,model,existing data,to train,better oos detectors,existing data to train better oos detectors,0.7097538709640503
translation,52,7,model,better oos detectors,operating in,low-data regimes,better oos detectors operating in low-data regimes,0.784551203250885
translation,52,7,model,model,introduce,gold,model introduce gold,0.6685554385185242
translation,52,8,model,gold,generates,pseudo-labeled candidates,gold generates pseudo-labeled candidates,0.6636984348297119
translation,52,8,model,gold,keeps,only the most beneficial candidates,gold keeps only the most beneficial candidates,0.7078701853752136
translation,52,8,model,pseudo-labeled candidates,using,samples,pseudo-labeled candidates using samples,0.641574501991272
translation,52,8,model,samples,from,auxiliary dataset,samples from auxiliary dataset,0.5910555124282837
translation,52,8,model,only the most beneficial candidates,for,training,only the most beneficial candidates for training,0.6502707600593567
translation,52,8,model,only the most beneficial candidates,through,novel filtering mechanism,only the most beneficial candidates through novel filtering mechanism,0.6650874018669128
translation,52,8,model,model,has,gold,model has gold,0.6353580951690674
translation,52,24,model,data augmentation ( gold ),to improve,oos detection in dialogue,data augmentation ( gold ) to improve oos detection in dialogue,0.6665610074996948
translation,52,24,model,method,has,of generating out-of-scope labels,method has of generating out-of-scope labels,0.5446017384529114
translation,52,24,model,model,propose,method,model propose method,0.6280754208564758
translation,52,9,results,top gold model,achieving,relative gains,top gold model achieving relative gains,0.6867316961288452
translation,52,9,results,outperforms,on,all key metrics,outperforms on all key metrics,0.5003544092178345
translation,52,9,results,all existing methods,on,all key metrics,all existing methods on all key metrics,0.4569498300552368
translation,52,9,results,relative gains,of,"52.4 % , 48.9 % and 50.3 %","relative gains of 52.4 % , 48.9 % and 50.3 %",0.553138256072998
translation,52,9,results,relative gains,against,median baseline performance,relative gains against median baseline performance,0.6240179538726807
translation,52,9,results,three target benchmarks,has,top gold model,three target benchmarks has top gold model,0.519745945930481
translation,52,9,results,top gold model,has,outperforms,top gold model has outperforms,0.6208486557006836
translation,52,9,results,outperforms,has,all existing methods,outperforms has all existing methods,0.5677774548530579
translation,53,42,ablation-analysis,mske - dialog,has,better performance,mske - dialog has better performance,0.6186884641647339
translation,53,42,ablation-analysis,our model,achieve,promising results,our model achieve promising results,0.605862557888031
translation,53,42,ablation-analysis,mske - dialog,has,better performance,mske - dialog has better performance,0.6186884641647339
translation,53,125,ablation-analysis,text knowledge,bring,more contributions,text knowledge bring more contributions,0.6334545612335205
translation,53,125,ablation-analysis,three knowledge sources,has,commonsense knowledge,three knowledge sources has commonsense knowledge,0.5328015685081482
translation,53,125,ablation-analysis,ablation analysis,among,three knowledge sources,ablation analysis among three knowledge sources,0.5596230030059814
translation,53,109,baselines,baselines,include,conkadi,baselines include conkadi,0.5827071666717529
translation,53,109,baselines,baselines,include,transinfo,baselines include transinfo,0.5993944406509399
translation,53,109,baselines,baselines,include,refnet,baselines include refnet,0.5608128309249878
translation,53,109,baselines,baselines,include,gpt base,baselines include gpt base,0.5843537449836731
translation,53,109,baselines,baselines,include,naive seq2seq,baselines include naive seq2seq,0.5472973585128784
translation,53,109,baselines,baselines,include,conkadi,baselines include conkadi,0.5827071666717529
translation,53,109,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,53,110,experiments,3 welleducated native speakers,to annotate,sampled 200 test cases,3 welleducated native speakers to annotate sampled 200 test cases,0.736107349395752
translation,53,6,model,mske - dialog,simultaneously leverage,multiple heterogeneous knowledge sources,mske - dialog simultaneously leverage multiple heterogeneous knowledge sources,0.6560192704200745
translation,53,6,model,multiple heterogeneous knowledge sources,to improve,knowledge coverage,multiple heterogeneous knowledge sources to improve knowledge coverage,0.6577600836753845
translation,53,6,model,multiple heterogeneous knowledge sources,propose,multi-reference generation,multiple heterogeneous knowledge sources propose multi-reference generation,0.586442232131958
translation,53,6,model,multiple heterogeneous knowledge sources,propose,multi-reference generation,multiple heterogeneous knowledge sources propose multi-reference generation,0.586442232131958
translation,53,6,model,multi-reference selection,to better select,context,multi-reference selection to better select context,0.7196826934814453
translation,53,6,model,multi-reference generation,to generate,informative responses,multi-reference generation to generate informative responses,0.7087168097496033
translation,53,6,model,informative responses,by referring to,multiple generation references,informative responses by referring to multiple generation references,0.644640326499939
translation,53,6,model,model,proposes,dialogue generation model,model proposes dialogue generation model,0.6926978230476379
translation,53,6,model,model,propose,multi-reference selection,model propose multi-reference selection,0.6652337908744812
translation,53,6,model,model,propose,multi-reference generation,model propose multi-reference generation,0.6642096042633057
translation,53,32,model,multi-source heterogeneous knowledge - enhanced dialogue generation model,has,mske - dialog,multi-source heterogeneous knowledge - enhanced dialogue generation model has mske - dialog,0.5704701542854309
translation,53,32,model,model,proposes,multi-source heterogeneous knowledge - enhanced dialogue generation model,model proposes multi-source heterogeneous knowledge - enhanced dialogue generation model,0.6343480944633484
translation,53,36,model,impact of topic conflict,propose,multi-reference selection mechanism,impact of topic conflict propose multi-reference selection mechanism,0.5943800806999207
translation,53,36,model,model,To alleviate,impact of topic conflict,model To alleviate impact of topic conflict,0.7097743153572083
translation,53,37,model,dynamic selection gate,to select,relevant knowledge,dynamic selection gate to select relevant knowledge,0.742358386516571
translation,53,37,model,relevant knowledge,from,different sources,relevant knowledge from different sources,0.5519241690635681
translation,53,37,model,model,uses,global relevance gate,model uses global relevance gate,0.60383141040802
translation,53,37,model,model,uses,dynamic selection gate,model uses dynamic selection gate,0.623985230922699
translation,53,38,model,multi-reference generation mechanism,construct,unified dynamic vocab,multi-reference generation mechanism construct unified dynamic vocab,0.6331983804702759
translation,53,38,model,unified dynamic vocab,refer to,all inputs,unified dynamic vocab refer to all inputs,0.6039672493934631
translation,53,38,model,all inputs,during,decoding,all inputs during decoding,0.7203482389450073
translation,53,38,model,model,propose,multi-reference generation mechanism,model propose multi-reference generation mechanism,0.650447428226471
translation,53,33,results,mske - dialog,improve,knowledge coverage,mske - dialog improve knowledge coverage,0.648335337638855
translation,53,33,results,knowledge coverage,by integrating,more knowledge sources,knowledge coverage by integrating more knowledge sources,0.5655813813209534
translation,53,33,results,results,has,mske - dialog,results has mske - dialog,0.5588350892066956
translation,53,41,results,outperform,surpass,fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ),outperform surpass fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ),0.7090076804161072
translation,53,41,results,various stateof - the- art knowledge - enhanced methods,by,notable margins,various stateof - the- art knowledge - enhanced methods by notable margins,0.5984584093093872
translation,53,41,results,various stateof - the- art knowledge - enhanced methods,surpass,fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ),various stateof - the- art knowledge - enhanced methods surpass fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ),0.6783874034881592
translation,53,41,results,fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ),with,fewer parameters,fine-tuned pretraining system cdial - gpt ( gpt & gpt2 ) with fewer parameters,0.6212414503097534
translation,53,41,results,automatic and human evaluations,has,mske - dialog,automatic and human evaluations has mske - dialog,0.5958001017570496
translation,53,41,results,mske - dialog,has,outperform,mske - dialog has outperform,0.6963285803794861
translation,53,41,results,outperform,has,various stateof - the- art knowledge - enhanced methods,outperform has various stateof - the- art knowledge - enhanced methods,0.5469737648963928
translation,53,41,results,results,In,automatic and human evaluations,results In automatic and human evaluations,0.4823596477508545
translation,53,104,results,mske - dialog,undoubtedly beats,other baselines,mske - dialog undoubtedly beats other baselines,0.5895246863365173
translation,53,104,results,other baselines,in,overall score,other baselines in overall score,0.477977991104126
translation,53,104,results,other baselines,with,cooperation,other baselines with cooperation,0.6817272901535034
translation,53,104,results,cooperation,of,three heterogeneous knowledge sources,cooperation of three heterogeneous knowledge sources,0.5565488338470459
translation,53,104,results,aspect of knowledge,has,mske - dialog,aspect of knowledge has mske - dialog,0.6467764377593994
translation,53,104,results,results,Moving to,aspect of knowledge,results Moving to aspect of knowledge,0.6446752548217773
translation,53,105,results,mske - dialog,loses to,refnet / ccm,mske - dialog loses to refnet / ccm,0.7493678331375122
translation,53,105,results,refnet / ccm,in terms of,text / commonsense entity score,refnet / ccm in terms of text / commonsense entity score,0.685440182685852
translation,53,105,results,results,has,mske - dialog,results has mske - dialog,0.5588350892066956
translation,53,112,results,outperform,in,human evaluation,outperform in human evaluation,0.5437728762626648
translation,53,112,results,baselines,in,human evaluation,baselines in human evaluation,0.4990224242210388
translation,53,112,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,53,113,results,better performance,in,human evaluation,better performance in human evaluation,0.49327534437179565
translation,53,113,results,automatic results,has,seq2seq,automatic results has seq2seq,0.5816501379013062
translation,53,113,results,seq2seq,has,better performance,seq2seq has better performance,0.5938483476638794
translation,53,113,results,results,Compared with,automatic results,results Compared with automatic results,0.6478643417358398
translation,53,122,results,all three single -source variants,have,improvements,all three single -source variants have improvements,0.542495846748352
translation,53,122,results,improvements,in,overall performance,improvements in overall performance,0.5283680558204651
translation,53,122,results,improvements,in,the perplexity,improvements in the perplexity,0.5516400337219238
translation,53,122,results,improvements,both,overall performance,improvements both overall performance,0.6647248864173889
translation,53,122,results,improvements,both,the perplexity,improvements both the perplexity,0.6624993085861206
translation,53,122,results,base,has,all three single -source variants,base has all three single -source variants,0.5859410762786865
translation,53,122,results,results,to,base,results to base,0.49034446477890015
translation,53,126,results,perplexity,of,context / base +ibt,perplexity of context / base +ibt,0.5570927858352661
translation,53,126,results,context / base +ibt,have,notable improvements,context / base +ibt have notable improvements,0.5390758514404297
translation,53,126,results,improvement,of,overall score,improvement of overall score,0.5118563771247864
translation,53,126,results,results,has,perplexity,results has perplexity,0.5649449229240417
translation,53,128,results,our approach,beat,best knowledge - enhanced baselines,our approach beat best knowledge - enhanced baselines,0.7033551931381226
translation,53,128,results,best knowledge - enhanced baselines,without using,more source knowledge sources,best knowledge - enhanced baselines without using more source knowledge sources,0.7309765815734863
translation,53,128,results,results,worth noting,our approach,results worth noting our approach,0.6443368196487427
translation,53,130,results,full,not achieve,best,full not achieve best,0.6841061115264893
translation,53,130,results,best,in,each aspect,best in each aspect,0.5157327651977539
translation,53,130,results,results,has,full,results has full,0.47586148977279663
translation,53,140,results,mske - dialog,with,only 1 2 ? 1 4 conversational data,mske - dialog with only 1 2 ? 1 4 conversational data,0.6453784108161926
translation,53,140,results,only 1 2 ? 1 4 conversational data,archive,comparable performance,only 1 2 ? 1 4 conversational data archive comparable performance,0.7030732035636902
translation,53,140,results,comparable performance,with,nonknowledge - enhanced variant,comparable performance with nonknowledge - enhanced variant,0.6700361371040344
translation,53,140,results,multi-source knowledge,has,mske - dialog,multi-source knowledge has mske - dialog,0.6200063228607178
translation,53,140,results,results,incorporation of,multi-source knowledge,results incorporation of multi-source knowledge,0.5704226493835449
translation,54,36,ablation-analysis,final-turn annotations,in,multiwoz 2.0,final-turn annotations in multiwoz 2.0,0.5204044580459595
translation,54,36,ablation-analysis,multiwoz 2.0,reduces,training set,multiwoz 2.0 reduces training set,0.6439695358276367
translation,54,36,ablation-analysis,training set,to,14.3 %,training set to 14.3 %,0.5537711381912231
translation,54,36,ablation-analysis,14.3 %,of,original size,14.3 % of original size,0.5727428793907166
translation,54,36,ablation-analysis,original size,in,annotated turns ),original size in annotated turns ),0.5330392122268677
translation,54,36,ablation-analysis,ablation analysis,Using,final-turn annotations,ablation analysis Using final-turn annotations,0.649864137172699
translation,54,139,ablation-analysis,performance,of,baseline gpt - 2 model,performance of baseline gpt - 2 model,0.561856746673584
translation,54,139,ablation-analysis,drops,to,48.07 % joint accuracy,drops to 48.07 % joint accuracy,0.563784122467041
translation,54,139,ablation-analysis,sparsely - supervised scenario,has,performance,sparsely - supervised scenario has performance,0.5393131971359253
translation,54,139,ablation-analysis,baseline gpt - 2 model,has,drops,baseline gpt - 2 model has drops,0.6066336035728455
translation,54,139,ablation-analysis,ablation analysis,In,sparsely - supervised scenario,ablation analysis In sparsely - supervised scenario,0.5327386260032654
translation,54,187,ablation-analysis,0.15 % boost,when,jaccard,0.15 % boost when jaccard,0.603787899017334
translation,54,187,ablation-analysis,0.15 % boost,further improves,performance,0.15 % boost further improves performance,0.7596478462219238
translation,54,187,ablation-analysis,jaccard,is around,0.2,jaccard is around 0.2,0.6273742318153381
translation,54,187,ablation-analysis,performance,to,0.6 %,performance to 0.6 %,0.5339478850364685
translation,54,187,ablation-analysis,0.6 %,at,jaccard value,0.6 % at jaccard value,0.5015969276428223
translation,54,187,ablation-analysis,jaccard value,of,0.88,jaccard value of 0.88,0.48233866691589355
translation,54,187,ablation-analysis,l4p4k2-dsvgraph -lastturn,has,0.15 % boost,l4p4k2-dsvgraph -lastturn has 0.15 % boost,0.5978963375091553
translation,54,187,ablation-analysis,ablation analysis,has,l4p4k2-dsvgraph -lastturn,ablation analysis has l4p4k2-dsvgraph -lastturn,0.5631552934646606
translation,54,121,baselines,bi-lstm - based dst model,utilizing,graph attention network,bi-lstm - based dst model utilizing graph attention network,0.5947892665863037
translation,54,121,baselines,graph attention network,to capture,inter-slot relationships,graph attention network to capture inter-slot relationships,0.6753617525100708
translation,54,122,baselines,"simpletod * ( hosseini - asl et al. , 2020 )",has,gpt - 2 - based dialogue state tracker,"simpletod * ( hosseini - asl et al. , 2020 ) has gpt - 2 - based dialogue state tracker",0.5595610737800598
translation,54,122,baselines,baselines,has,"simpletod * ( hosseini - asl et al. , 2020 )","baselines has simpletod * ( hosseini - asl et al. , 2020 )",0.5406360626220703
translation,54,6,model,novel hybrid architecture,augments,gpt - 2,novel hybrid architecture augments gpt - 2,0.7164236307144165
translation,54,6,model,gpt - 2,with,representations,gpt - 2 with representations,0.6892198920249939
translation,54,6,model,representations,derived from,graph attention networks,representations derived from graph attention networks,0.5829488039016724
translation,54,6,model,representations,to allow,"causal , sequential prediction","representations to allow causal , sequential prediction",0.6905929446220398
translation,54,6,model,"causal , sequential prediction",of,slot values,"causal , sequential prediction of slot values",0.5933836698532104
translation,54,6,model,model,present,novel hybrid architecture,model present novel hybrid architecture,0.6820623874664307
translation,54,7,model,model architecture,captures,inter-slot relationships and dependencies,model architecture captures inter-slot relationships and dependencies,0.7170381546020508
translation,54,7,model,inter-slot relationships and dependencies,across,domains,inter-slot relationships and dependencies across domains,0.6857848763465881
translation,54,7,model,model,has,model architecture,model has model architecture,0.5430979132652283
translation,54,12,model,novel hybrid architecture,augments,"gpt - 2 ( radford et al. , 2019 )","novel hybrid architecture augments gpt - 2 ( radford et al. , 2019 )",0.7143067717552185
translation,54,12,model,"gpt - 2 ( radford et al. , 2019 )",with,dialogue act representations,"gpt - 2 ( radford et al. , 2019 ) with dialogue act representations",0.6116007566452026
translation,54,12,model,"gpt - 2 ( radford et al. , 2019 )",allows,"causal , sequential prediction","gpt - 2 ( radford et al. , 2019 ) allows causal , sequential prediction",0.6410279870033264
translation,54,12,model,dialogue act representations,derived from,graph attention networks ( gats ),dialogue act representations derived from graph attention networks ( gats ),0.6440958380699158
translation,54,12,model,dialogue act representations,allows,"causal , sequential prediction","dialogue act representations allows causal , sequential prediction",0.6175320148468018
translation,54,12,model,"causal , sequential prediction",of,slot values,"causal , sequential prediction of slot values",0.5933836698532104
translation,54,12,model,"causal , sequential prediction",of,slots and values,"causal , sequential prediction of slots and values",0.585186243057251
translation,54,12,model,relationships,between,slots and values,relationships between slots and values,0.6481884121894836
translation,54,12,model,slots and values,across,domains,slots and values across domains,0.7153401374816895
translation,54,12,model,model,present,novel hybrid architecture,model present novel hybrid architecture,0.6820623874664307
translation,54,13,model,gats,to improve,predictions,gats to improve predictions,0.7273420095443726
translation,54,13,model,predictions,of,values,predictions of values,0.6104308366775513
translation,54,13,model,values,shared across,domain-slots,values shared across domain-slots,0.7068354487419128
translation,54,39,model,our architecture,mit-igates,limitation of dsts,our architecture mit-igates limitation of dsts,0.7262150049209595
translation,54,39,model,limitation of dsts,based on,gpt - 2 alone,limitation of dsts based on gpt - 2 alone,0.7028310894966125
translation,54,39,model,domain-slot values,in,left-to - right manner,domain-slot values in left-to - right manner,0.519597589969635
translation,54,39,model,model,demonstrate,our architecture,model demonstrate our architecture,0.6459529995918274
translation,54,40,model,knowledge - aware models,capture,relationships,knowledge - aware models capture relationships,0.7540515661239624
translation,54,40,model,relationships,between,domain-slots,relationships between domain-slots,0.6830804944038391
translation,54,40,model,model,investigate,knowledge - aware models,model investigate knowledge - aware models,0.6232374906539917
translation,54,188,model,graph modules,enable,models,graph modules enable models,0.6877356171607971
translation,54,188,model,models,to exploit,inter-slot dependencies,models to exploit inter-slot dependencies,0.6950057148933411
translation,54,188,model,model,conclude that,graph modules,model conclude that graph modules,0.6281402111053467
translation,54,136,results,l0p0k0 - nograph,achieves,higher joint accuracy,l0p0k0 - nograph achieves higher joint accuracy,0.6851920485496521
translation,54,136,results,higher joint accuracy,than,most of the baseline models,higher joint accuracy than most of the baseline models,0.5324518084526062
translation,54,136,results,higher joint accuracy,setting,strong baseline,higher joint accuracy setting strong baseline,0.40352174639701843
translation,54,136,results,most of the baseline models,including,graph - based models,most of the baseline models including graph - based models,0.6469630599021912
translation,54,136,results,graph - based models,such as,gcdst and som - dst + sg,graph - based models such as gcdst and som - dst + sg,0.6153647899627686
translation,54,136,results,strong baseline,for,further improvement,strong baseline for further improvement,0.6324383616447449
translation,54,136,results,further improvement,to,gpt - 2 - based generation,further improvement to gpt - 2 - based generation,0.5777495503425598
translation,54,137,results,l4p4k2 -* models,with,multiple gat layers,l4p4k2 -* models with multiple gat layers,0.6512753367424011
translation,54,137,results,multiple gat layers,to encourage,inter-slot information exchange,multiple gat layers to encourage inter-slot information exchange,0.6328404545783997
translation,54,137,results,results,has,l4p4k2 -* models,results has l4p4k2 -* models,0.5423543453216553
translation,54,138,results,l4p4k2 -dsgraph,achieves,54.86 %,l4p4k2 -dsgraph achieves 54.86 %,0.677693784236908
translation,54,138,results,54.86 %,in,joint accuracy,54.86 % in joint accuracy,0.5102838277816772
translation,54,138,results,results,has,l4p4k2 -dsgraph,results has l4p4k2 -dsgraph,0.5475122928619385
translation,54,140,results,gat,in,system,gat in system,0.6278471350669861
translation,54,140,results,gat,achieves,50.43 %,gat achieves 50.43 %,0.683803141117096
translation,54,140,results,50.43 %,in,joint accuracy,50.43 % in joint accuracy,0.5106474161148071
translation,54,140,results,50.43 %,leading to,3 % degradation,50.43 % leading to 3 % degradation,0.7457232475280762
translation,54,140,results,3 % degradation,relative to,l0p0k0 - nograph,3 % degradation relative to l0p0k0 - nograph,0.7146957516670227
translation,54,140,results,fine -tuned,with,full set of annotated dialogue turns,fine -tuned with full set of annotated dialogue turns,0.5850796699523926
translation,54,140,results,l0p0k0 - nograph,has,fine -tuned,l0p0k0 - nograph has fine -tuned,0.6083463430404663
translation,54,140,results,results,Incorporating,gat,results Incorporating gat,0.5258368849754333
translation,54,141,results,dstqa * - lastturn,utilizes,bidirectional lstm modules,dstqa * - lastturn utilizes bidirectional lstm modules,0.6402639746665955
translation,54,141,results,dstqa * - lastturn,exhibits,sharp performance decrease,dstqa * - lastturn exhibits sharp performance decrease,0.6912572383880615
translation,54,141,results,sharp performance decrease,to,22.88 % joint accuracy,sharp performance decrease to 22.88 % joint accuracy,0.5333825349807739
translation,54,141,results,results,has,dstqa * - lastturn,results has dstqa * - lastturn,0.5529201030731201
translation,54,143,results,l1p1k2-dsvgraph -lastturn,improves,accuracy,l1p1k2-dsvgraph -lastturn improves accuracy,0.7188122868537903
translation,54,143,results,accuracy,by incorporating,gat representations,accuracy by incorporating gat representations,0.6883159279823303
translation,54,143,results,gat representations,in which,slot nodes,gat representations in which slot nodes,0.6372281908988953
translation,54,143,results,slot nodes,depend on,only value nodes,slot nodes depend on only value nodes,0.7240524291992188
translation,54,143,results,base system,has,l1p1k2-dsvgraph -lastturn,base system has l1p1k2-dsvgraph -lastturn,0.6118337512016296
translation,54,143,results,results,Relative to,base system,results Relative to base system,0.7538384795188904
translation,54,147,results,dsvgraph,able to capture,dependencies,dsvgraph able to capture dependencies,0.7138110995292664
translation,54,147,results,dependencies,between,slots,dependencies between slots,0.698536217212677
translation,54,147,results,dependencies,resulting in,slight improvement,dependencies resulting in slight improvement,0.7162334322929382
translation,54,147,results,slots,share,values,slots share values,0.6821349263191223
translation,54,147,results,slight improvement,over,dsgraph,slight improvement over dsgraph,0.6866050958633423
translation,54,147,results,slight improvement,when,number of layers / hops,slight improvement when number of layers / hops,0.6520500779151917
translation,54,147,results,dsgraph,when,number of layers / hops,dsgraph when number of layers / hops,0.6260694861412048
translation,54,147,results,number of layers / hops,are,limited,number of layers / hops are limited,0.5719887614250183
translation,54,147,results,values nodes,has,dsvgraph,values nodes has dsvgraph,0.5733169317245483
translation,54,147,results,results,Through modelling,values nodes,results Through modelling values nodes,0.7241494059562683
translation,54,148,results,sufficient layers,of,gats,sufficient layers of gats,0.6778081059455872
translation,54,148,results,dsgraph,compensates for,lack of explicit value nodes and matches,dsgraph compensates for lack of explicit value nodes and matches,0.7451851963996887
translation,54,148,results,performance,of,dsvgraph,performance of dsvgraph,0.5938689708709717
translation,54,148,results,sufficient layers,has,dsgraph,sufficient layers has dsgraph,0.6308988332748413
translation,54,148,results,gats,has,dsgraph,gats has dsgraph,0.6446861028671265
translation,54,148,results,outperforms,has,performance,outperforms has performance,0.6510229110717773
translation,54,148,results,results,With,sufficient layers,results With sufficient layers,0.6294520497322083
translation,54,160,results,gats,improve,predictions,gats improve predictions,0.6886264085769653
translation,54,160,results,predictions,at,intermediate dialogue turns,predictions at intermediate dialogue turns,0.5562330484390259
translation,54,160,results,results,has,gats,results has gats,0.5632804036140442
translation,54,164,results,baseline model,trained with,all training samples ( l4p4k2 - dsgraph ),baseline model trained with all training samples ( l4p4k2 - dsgraph ),0.7271593809127808
translation,54,164,results,all training samples ( l4p4k2 - dsgraph ),shows,downward trend,all training samples ( l4p4k2 - dsgraph ) shows downward trend,0.689134955406189
translation,54,164,results,downward trend,in,prediction accuracy,downward trend in prediction accuracy,0.5088938474655151
translation,54,164,results,downward trend,as,dialogue,downward trend as dialogue,0.6197351813316345
translation,54,164,results,prediction accuracy,as,dialogue,prediction accuracy as dialogue,0.5978821516036987
translation,54,164,results,dialogue,has,progresses,dialogue has progresses,0.6379854083061218
translation,54,166,results,performance,throughout,dialogue sessions,performance throughout dialogue sessions,0.5721105933189392
translation,54,166,results,performance,lags by,around 5 %,performance lags by around 5 %,0.7533272504806519
translation,54,166,results,around 5 %,in,joint accuracy,around 5 % in joint accuracy,0.5717850923538208
translation,54,166,results,l4p4k2 -dsgraph,has,) and l0p0k0 - nograph - lastturn,l4p4k2 -dsgraph has ) and l0p0k0 - nograph - lastturn,0.6086894273757935
translation,54,166,results,l4p4k2 -dsgraph,has,performance,l4p4k2 -dsgraph has performance,0.5798747539520264
translation,54,166,results,) and l0p0k0 - nograph - lastturn,has,performance,) and l0p0k0 - nograph - lastturn has performance,0.6204062104225159
translation,54,166,results,),has,performance,) has performance,0.5912755727767944
translation,54,166,results,results,Comparing,l4p4k2 -dsgraph,results Comparing l4p4k2 -dsgraph,0.6191274523735046
translation,54,186,results,graph- enhanced models ( yellow and green ),perform,better,graph- enhanced models ( yellow and green ) perform better,0.6360840201377869
translation,54,186,results,better,when predicting,values,better when predicting values,0.7167726755142212
translation,54,186,results,values,that have,high jaccard scores,values that have high jaccard scores,0.6722649335861206
translation,54,186,results,baseline without gats,has,graph- enhanced models ( yellow and green ),baseline without gats has graph- enhanced models ( yellow and green ),0.5544437170028687
translation,55,107,ablation-analysis,time difference encoding,from,contreph,time difference encoding from contreph,0.6054729223251343
translation,55,107,ablation-analysis,time difference encoding,leads to,drop,time difference encoding leads to drop,0.6891276240348816
translation,55,107,ablation-analysis,drop,of,1.55 % and 1.37 %,drop of 1.55 % and 1.37 %,0.6396807432174683
translation,55,107,ablation-analysis,1.55 % and 1.37 %,in,em score,1.55 % and 1.37 % in em score,0.5761986970901489
translation,55,107,ablation-analysis,1.55 % and 1.37 %,on,machine and human-annotation test sets,1.55 % and 1.37 % on machine and human-annotation test sets,0.5165313482284546
translation,55,107,ablation-analysis,ablation analysis,removing,time difference encoding,ablation analysis removing time difference encoding,0.7571213841438293
translation,55,28,baselines,contreph,detects,any of the user queries,contreph detects any of the user queries,0.7299860715866089
translation,55,28,baselines,contreph,extracts,most probable rephrase span,contreph extracts most probable rephrase span,0.6747065186500549
translation,55,28,baselines,any of the user queries,in,dialogue session,any of the user queries in dialogue session,0.5307589769363403
translation,55,28,baselines,dialogue session,is,rephrased,dialogue session is rephrased,0.578773558139801
translation,55,28,baselines,most probable rephrase span,led to,satisfactory response,most probable rephrase span led to satisfactory response,0.6967278718948364
translation,55,28,baselines,satisfactory response,from,dialogue agent,satisfactory response from dialogue agent,0.5794587731361389
translation,55,28,baselines,baselines,has,contreph,baselines has contreph,0.6054129004478455
translation,55,97,baselines,"bert -nsp ( devlin et al. , 2019 )",fine- tune,bert,"bert -nsp ( devlin et al. , 2019 ) fine- tune bert",0.7034517526626587
translation,55,97,baselines,"bert -nsp ( devlin et al. , 2019 )",predict if,input pairs,"bert -nsp ( devlin et al. , 2019 ) predict if input pairs",0.692966878414154
translation,55,97,baselines,bert,with,same training split,bert with same training split,0.6950027346611023
translation,55,97,baselines,input pairs,has,are rephrases,input pairs has are rephrases,0.6345916390419006
translation,55,97,baselines,baselines,has,"bert -nsp ( devlin et al. , 2019 )","baselines has bert -nsp ( devlin et al. , 2019 )",0.5555265545845032
translation,55,98,baselines,"dpr ( karpukhin et al. , 2020 )",follow,recent retrieval model dpr,"dpr ( karpukhin et al. , 2020 ) follow recent retrieval model dpr",0.548120379447937
translation,55,98,baselines,recent retrieval model dpr,to train,dual bert model,recent retrieval model dpr to train dual bert model,0.6169788241386414
translation,55,98,baselines,dual bert model,with,positive rephrase pairs,dual bert model with positive rephrase pairs,0.6573539972305298
translation,55,98,baselines,dual bert model,with,in - batch negatives,dual bert model with in - batch negatives,0.6472047567367554
translation,55,98,baselines,baselines,has,"dpr ( karpukhin et al. , 2020 )","baselines has dpr ( karpukhin et al. , 2020 )",0.5246394872665405
translation,55,99,baselines,baselines,has,"simcse ( gao et al. , 2021 )","baselines has simcse ( gao et al. , 2021 )",0.5220866203308105
translation,55,111,baselines,contreph,utilizes,self-attention mechanism,contreph utilizes self-attention mechanism,0.6367254257202148
translation,55,111,baselines,self-attention mechanism,across,all turns,self-attention mechanism across all turns,0.7256167531013489
translation,55,111,baselines,all turns,of,dialogue,all turns of dialogue,0.6473247408866882
translation,55,109,experiments,human test set,is,more challenging,human test set is more challenging,0.5504782795906067
translation,55,109,experiments,more challenging,due to,different domain distribution,more challenging due to different domain distribution,0.6698742508888245
translation,55,109,experiments,em scores,are,much lower,em scores are much lower,0.5892612338066101
translation,55,109,experiments,much lower,compared to,machineannotated set,much lower compared to machineannotated set,0.679309070110321
translation,55,8,model,contextual rephrase detection model contreph,to automatically identify,rephrases,contextual rephrase detection model contreph to automatically identify rephrases,0.7408670783042908
translation,55,8,model,rephrases,from,multiturn dialogues,rephrases from multiturn dialogues,0.6371960639953613
translation,55,8,model,model,propose,contextual rephrase detection model contreph,model propose contextual rephrase detection model contreph,0.6833468675613403
translation,55,27,model,automatic user rephrase detection approach contreph,leverages,implicit user feedback and dialogue context,automatic user rephrase detection approach contreph leverages implicit user feedback and dialogue context,0.661792516708374
translation,55,27,model,implicit user feedback and dialogue context,in,multi-turn dialogue setting,implicit user feedback and dialogue context in multi-turn dialogue setting,0.5226319432258606
translation,55,27,model,model,propose,automatic user rephrase detection approach contreph,model propose automatic user rephrase detection approach contreph,0.6931604146957397
translation,55,104,results,contreph,consistently achieves,better performance,contreph consistently achieves better performance,0.7740752100944519
translation,55,104,results,better performance,on,machine and human-annotation test sets,better performance on machine and human-annotation test sets,0.5186256170272827
translation,55,104,results,results,has,contreph,results has contreph,0.596258819103241
translation,55,105,results,state - of- the - art pairwise bert - nsp method,on,human test set,state - of- the - art pairwise bert - nsp method on human test set,0.4957539141178131
translation,55,105,results,state - of- the - art pairwise bert - nsp method,improves,overall em score,state - of- the - art pairwise bert - nsp method improves overall em score,0.6310321092605591
translation,55,105,results,human test set,by,27.89 %,human test set by 27.89 %,0.5649285316467285
translation,55,105,results,27.89 %,on,em score,27.89 % on em score,0.546539843082428
translation,55,105,results,overall em score,by,almost 6 %,overall em score by almost 6 %,0.5747400522232056
translation,55,105,results,almost 6 %,on,machineannotated set,almost 6 % on machineannotated set,0.5910740494728088
translation,55,110,results,bert - nsp,achieves,best results,bert - nsp achieves best results,0.6882439851760864
translation,55,110,results,bert - nsp,encodes,two queries,bert - nsp encodes two queries,0.7273881435394287
translation,55,110,results,best results,amongst,baselines,best results amongst baselines,0.6123931407928467
translation,55,110,results,best results,highlights,benefits,best results highlights benefits,0.6212839484214783
translation,55,110,results,mechanism,across,queries,mechanism across queries,0.6937588453292847
translation,55,110,results,two queries,as,single sequence,two queries as single sequence,0.5618417859077454
translation,55,110,results,single sequence,with,separator,single sequence with separator,0.690538227558136
translation,55,110,results,other baselines,apply,similarity function,other baselines apply similarity function,0.6153719425201416
translation,55,110,results,results,has,bert - nsp,results has bert - nsp,0.5344567894935608
translation,56,4,experiments,largescale media interview dataset,consisting of,463.6 k transcripts,largescale media interview dataset consisting of 463.6 k transcripts,0.6811637282371521
translation,56,4,experiments,463.6 k transcripts,with,abstractive summaries,463.6 k transcripts with abstractive summaries,0.602008044719696
translation,56,4,experiments,mediasum,has,largescale media interview dataset,mediasum has largescale media interview dataset,0.5842763781547546
translation,56,5,experiments,interview transcripts,from,npr and cnn,interview transcripts from npr and cnn,0.5591152310371399
translation,56,5,experiments,overview and topic descriptions,as,summaries,overview and topic descriptions as summaries,0.5727939009666443
translation,56,102,results,lead - 3 baseline,has,relatively weak performance,lead - 3 baseline has relatively weak performance,0.5833470225334167
translation,56,104,results,pre-trained models,such as,bart and unilm,pre-trained models such as bart and unilm,0.6173880696296692
translation,56,104,results,bart and unilm,has,outperform,bart and unilm has outperform,0.6577351689338684
translation,56,104,results,outperform,has,non-pretrained ptgen model,outperform has non-pretrained ptgen model,0.5831829309463501
translation,56,104,results,results,has,pre-trained models,results has pre-trained models,0.5230302810668945
translation,56,108,results,all three datasets,training on,mediasum,all three datasets training on mediasum,0.7461757063865662
translation,56,108,results,mediasum,leads to,improvement,mediasum leads to improvement,0.6925893425941467
translation,56,108,results,improvement,on,target dataset,improvement on target dataset,0.5444322228431702
translation,56,108,results,results,on,all three datasets,results on all three datasets,0.48488834500312805
translation,57,145,ablation-analysis,f1 bleu1 and f1 bleu4,increased by,2.0 % and 1.5 %,f1 bleu1 and f1 bleu4 increased by 2.0 % and 1.5 %,0.7196899652481079
translation,57,145,ablation-analysis,2.0 % and 1.5 %,on,test set,2.0 % and 1.5 % on test set,0.5656593441963196
translation,57,145,ablation-analysis,ablation analysis,has,f1 bleu1 and f1 bleu4,ablation analysis has f1 bleu1 and f1 bleu4,0.5541270971298218
translation,57,163,ablation-analysis,encoding process,contribute to,overall performance,encoding process contribute to overall performance,0.6698026657104492
translation,57,163,ablation-analysis,overall performance,removing,any one of them,overall performance removing any one of them,0.6358680129051208
translation,57,163,ablation-analysis,any one of them,causes,performance drop,any one of them causes performance drop,0.6854321360588074
translation,57,163,ablation-analysis,performance drop,on,f1 bleu 1 and f1 bleu 4,performance drop on f1 bleu 1 and f1 bleu 4,0.5635685324668884
translation,57,163,ablation-analysis,ablation analysis,verify,sequential states and graph states,ablation analysis verify sequential states and graph states,0.7500239014625549
translation,57,164,ablation-analysis,two matrices,drops by,great margin,two matrices drops by great margin,0.7934502363204956
translation,57,164,ablation-analysis,ablation analysis,when removing,gs / ss,ablation analysis when removing gs / ss,0.720237672328949
translation,57,165,ablation-analysis,gap,between,decision making,gap between decision making,0.7005082964897156
translation,57,165,ablation-analysis,gap,between,question generation,gap between question generation,0.647183358669281
translation,57,165,ablation-analysis,ablation analysis,indicate,gap,ablation analysis indicate gap,0.6596407890319824
translation,57,165,ablation-analysis,ablation analysis,bridging,gap,ablation analysis bridging gap,0.8090997338294983
translation,57,73,baselines,tf - idf,stands for,term frequency - inverse document frequency,tf - idf stands for term frequency - inverse document frequency,0.6687595844268799
translation,57,73,baselines,term frequency - inverse document frequency,to reflect,relevant,term frequency - inverse document frequency to reflect relevant,0.6385358572006226
translation,57,73,baselines,baselines,has,tf - idf,baselines has tf - idf,0.5253828167915344
translation,57,134,hyperparameters,dimension,of,hidden states,dimension of hidden states,0.5819122791290283
translation,57,134,hyperparameters,hidden states,is,768,hidden states is 768,0.6309125423431396
translation,57,134,hyperparameters,768,for,both the encoder and decoder,768 for both the encoder and decoder,0.6411107778549194
translation,57,134,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,57,135,hyperparameters,training process,uses,"adam ( kingma and ba , 2015 )","training process uses adam ( kingma and ba , 2015 )",0.5795588493347168
translation,57,135,hyperparameters,"adam ( kingma and ba , 2015 )",for,5 epochs,"adam ( kingma and ba , 2015 ) for 5 epochs",0.5852137804031372
translation,57,135,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,57,135,hyperparameters,5 epochs,with,learning rate,5 epochs with learning rate,0.6657905578613281
translation,57,135,hyperparameters,learning rate,set to,5e - 5,learning rate set to 5e - 5,0.7400732040405273
translation,57,135,hyperparameters,hyperparameters,has,training process,hyperparameters has training process,0.5041748881340027
translation,57,136,hyperparameters,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,57,136,hyperparameters,gradient clipping,with,total batch size,gradient clipping with total batch size,0.6380109190940857
translation,57,136,hyperparameters,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
translation,57,136,hyperparameters,total batch size,of,16,total batch size of 16,0.6770024299621582
translation,57,137,hyperparameters,parameter,in,decision making objective,parameter in decision making objective,0.47748616337776184
translation,57,137,hyperparameters,parameter,set to,3.0,parameter set to 3.0,0.7290019392967224
translation,57,137,hyperparameters,decision making objective,set to,3.0,decision making objective set to 3.0,0.677586019039154
translation,57,137,hyperparameters,hyperparameters,has,parameter,hyperparameters has parameter,0.4950053095817566
translation,57,138,hyperparameters,bart - based decoder,for,question generation,bart - based decoder for question generation,0.5786451697349548
translation,57,138,hyperparameters,beam size,set to,10,beam size set to 10,0.77127605676651
translation,57,138,hyperparameters,10,for,inference,10 for inference,0.6652396321296692
translation,57,138,hyperparameters,bart - based decoder,has,beam size,bart - based decoder has beam size,0.546232283115387
translation,57,138,hyperparameters,question generation,has,beam size,question generation has beam size,0.5607765913009644
translation,57,138,hyperparameters,hyperparameters,For,bart - based decoder,hyperparameters For bart - based decoder,0.5844084024429321
translation,57,36,model,end-to - end system,by,open-retrieval,end-to - end system by open-retrieval,0.5844831466674805
translation,57,36,model,end-to - end system,bridging,decision making and question generation,end-to - end system bridging decision making and question generation,0.7796770930290222
translation,57,36,model,end-to - end system,to bridge,information transition,end-to - end system to bridge information transition,0.6112115979194641
translation,57,36,model,open-retrieval,of,supporting evidence,open-retrieval of supporting evidence,0.5816945433616638
translation,57,36,model,decision making and question generation,has,oscar ),decision making and question generation has oscar ),0.6255084872245789
translation,57,36,model,model,design,end-to - end system,model design end-to - end system,0.6581339836120605
translation,57,39,model,end-to - end framework,where,dialogue states,end-to - end framework where dialogue states,0.6158818602561951
translation,57,39,model,dialogue states,for,decision making,dialogue states for decision making,0.649935245513916
translation,57,39,model,model,design,end-to - end framework,model design end-to - end framework,0.6412641406059265
translation,57,58,model,first attempt,to bridge,gap,first attempt to bridge gap,0.669040322303772
translation,57,58,model,gap,between,decision making,gap between decision making,0.7005082964897156
translation,57,58,model,gap,between,question generation,gap between question generation,0.647183358669281
translation,57,58,model,two dialogue states,in,only one decoder,two dialogue states in only one decoder,0.4932193458080292
translation,57,142,results,baselines,in,all of the metrics,baselines in all of the metrics,0.4629885256290436
translation,57,142,results,oscar,has,outperforms,oscar has outperforms,0.6598230004310608
translation,57,142,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,57,142,results,results,indicate,oscar,results indicate oscar,0.6218354105949402
translation,57,143,results,mudern,by,1.3 %,mudern by 1.3 %,0.5979453325271606
translation,57,143,results,mudern,for,decision making stage,mudern for decision making stage,0.6549578905105591
translation,57,143,results,mudern,on,test set,mudern on test set,0.5964273810386658
translation,57,143,results,1.3 %,in,micro acc.,1.3 % in micro acc.,0.5432178974151611
translation,57,143,results,1.3 %,in,macro acc,1.3 % in macro acc,0.5252958536148071
translation,57,143,results,1.3 %,in,macro acc,1.3 % in macro acc,0.5252958536148071
translation,57,143,results,1.1 %,in,macro acc,1.1 % in macro acc,0.5368375778198242
translation,57,143,results,1.1 %,for,decision making stage,1.1 % for decision making stage,0.6205565929412842
translation,57,143,results,1.1 %,on,test set,1.1 % on test set,0.5439011454582214
translation,57,143,results,decision making stage,on,test set,decision making stage on test set,0.5962845087051392
translation,57,143,results,outperforms,has,public state - of- the - art model,outperforms has public state - of- the - art model,0.5483105778694153
translation,57,143,results,public state - of- the - art model,has,mudern,public state - of- the - art model has mudern,0.616856038570404
translation,57,143,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,57,144,results,greatly boosted,via,our approaches,greatly boosted via our approaches,0.6577498912811279
translation,57,144,results,results,has,question generation quality,results has question generation quality,0.485413134098053
translation,57,147,results,results,show,consistent gains,results show consistent gains,0.634734570980072
translation,57,147,results,consistent gains,for,seen and unseen splits,consistent gains for seen and unseen splits,0.6686463356018066
translation,57,147,results,consistent gains,both of,seen and unseen splits,consistent gains both of seen and unseen splits,0.6300259232521057
translation,57,147,results,results,show,consistent gains,results show consistent gains,0.634734570980072
translation,57,147,results,results,has,results,results has results,0.48582205176353455
translation,57,153,results,tf - idf,performs,better,tf - idf performs better,0.6299367547035217
translation,57,153,results,tf - idf,combining,tf -idf and dpr ( dpr + + ),tf - idf combining tf -idf and dpr ( dpr + + ),0.701705813407898
translation,57,153,results,better,than,dpr,better than dpr,0.6101847290992737
translation,57,153,results,tf -idf and dpr ( dpr + + ),yields,substantial improvements,tf -idf and dpr ( dpr + + ) yields substantial improvements,0.7396758198738098
translation,57,153,results,results,see that,tf - idf,results see that tf - idf,0.6500274538993835
translation,57,153,results,results,combining,tf -idf and dpr ( dpr + + ),results combining tf -idf and dpr ( dpr + + ),0.6838964819908142
translation,57,160,results,our method,achieve,better performance,our method achieve better performance,0.5949803590774536
translation,57,160,results,better performance,than,discern,better performance than discern,0.5749058723449707
translation,57,160,results,results,observe that,our method,results observe that our method,0.5894128680229187
translation,57,167,results,different strategies,when fusing,contextual states,different strategies when fusing contextual states,0.759270191192627
translation,57,167,results,contextual states,into,bart decoder,contextual states into bart decoder,0.6357685327529907
translation,57,167,results,gating mechanism,yields,best performance,gating mechanism yields best performance,0.7055995464324951
translation,57,167,results,results,explore,performance,results explore performance,0.6754196882247925
translation,57,172,results,t5,as,our backbone,t5 as our backbone,0.6094756126403809
translation,57,172,results,t5,achieving,better performance,t5 achieving better performance,0.6835936903953552
translation,57,172,results,better performance,for,test ( f1bleu1/f1bleu4 ),better performance for test ( f1bleu1/f1bleu4 ),0.6279695630073547
translation,57,172,results,53.7/45.0,for,dev,53.7/45.0 for dev,0.6422716379165649
translation,57,172,results,better performance,has,53.7/45.0,better performance has 53.7/45.0,0.5365778803825378
translation,57,172,results,results,employ,t5,results employ t5,0.5752673745155334
translation,57,179,results,obvious advantage,on,openretrieval task,obvious advantage on openretrieval task,0.4935494661331177
translation,57,179,results,obvious advantage,indicating,strong ability,obvious advantage indicating strong ability,0.6168745160102844
translation,57,179,results,strong ability,to extract,key information,strong ability to extract key information,0.7115748524665833
translation,57,179,results,key information,from,noisy documents,key information from noisy documents,0.5449643731117249
translation,57,179,results,results,show,obvious advantage,results show obvious advantage,0.6679548025131226
translation,58,6,baselines,simple yet effective architectural method,based on,residual adapters,simple yet effective architectural method based on residual adapters,0.6356958150863647
translation,58,6,baselines,baselines,implement and compare,multiple existing continual learning baselines,baselines implement and compare multiple existing continual learning baselines,0.679518461227417
translation,58,4,model,model,has,continual learning in task - oriented dialogue systems,model has continual learning in task - oriented dialogue systems,0.532289981842041
translation,59,107,ablation-analysis,hurts,leading to,lower joint goal accuracy,hurts leading to lower joint goal accuracy,0.7182554602622986
translation,59,107,ablation-analysis,lower joint goal accuracy,in,restaurant domain,lower joint goal accuracy in restaurant domain,0.5171342492103577
translation,59,107,ablation-analysis,slot values,has,hurts,slot values has hurts,0.628688633441925
translation,59,107,ablation-analysis,hurts,has,learning,hurts has learning,0.5625209212303162
translation,59,107,ablation-analysis,ablation analysis,incorporating,slot values,ablation analysis incorporating slot values,0.7439471483230591
translation,59,108,ablation-analysis,value candidates,improve,categorical slots,value candidates improve categorical slots,0.6727595925331116
translation,59,108,ablation-analysis,ablation analysis,adding,value candidates,ablation analysis adding value candidates,0.7737953066825867
translation,59,81,baselines,slot-utterance matching belief tracker,based on,"language model bert ( devlin et al. , 2018 )","slot-utterance matching belief tracker based on language model bert ( devlin et al. , 2018 )",0.5944356918334961
translation,59,81,baselines,baselines,has,slot-utterance matching belief tracker,baselines has slot-utterance matching belief tracker,0.5450891852378845
translation,59,68,experimental-setup,t5dst,based on,t5small ( 60 m parameters ) model,t5dst based on t5small ( 60 m parameters ) model,0.6743712425231934
translation,59,68,experimental-setup,t5small ( 60 m parameters ) model,which has,6 encoder- decoder layers,t5small ( 60 m parameters ) model which has 6 encoder- decoder layers,0.6239820718765259
translation,59,68,experimental-setup,experimental setup,implement,t5dst,experimental setup implement t5dst,0.66776442527771
translation,59,69,experimental-setup,"adamw ( loshchilov and hutter , 2018 ) optimizer",with,initial learning rate,"adamw ( loshchilov and hutter , 2018 ) optimizer with initial learning rate",0.590076208114624
translation,59,69,experimental-setup,initial learning rate,of,0.0001,initial learning rate of 0.0001,0.5696305632591248
translation,59,69,experimental-setup,experimental setup,trained using,"adamw ( loshchilov and hutter , 2018 ) optimizer","experimental setup trained using adamw ( loshchilov and hutter , 2018 ) optimizer",0.7220340371131897
translation,59,70,experimental-setup,all crossdomain zero-shot experiments,train,models,all crossdomain zero-shot experiments train models,0.6831235289573669
translation,59,70,experimental-setup,models,with,batch size,models with batch size,0.6254024505615234
translation,59,70,experimental-setup,128,for,5 epochs,128 for 5 epochs,0.6660571694374084
translation,59,70,experimental-setup,batch size,has,128,batch size has 128,0.6398258209228516
translation,59,70,experimental-setup,experimental setup,In,all crossdomain zero-shot experiments,experimental setup In all crossdomain zero-shot experiments,0.5127171277999878
translation,59,73,experimental-setup,full shot training,train,our model,full shot training train our model,0.6601430773735046
translation,59,73,experimental-setup,our model,for,at most 10 epochs,our model for at most 10 epochs,0.5880039930343628
translation,59,73,experimental-setup,our model,with,early stop,our model with early stop,0.6789460778236389
translation,59,73,experimental-setup,at most 10 epochs,with,batch size 64,at most 10 epochs with batch size 64,0.5780400037765503
translation,59,73,experimental-setup,early stop,according to,loss,early stop according to loss,0.6875298023223877
translation,59,73,experimental-setup,loss,in,validation set,loss in validation set,0.5002934336662292
translation,59,73,experimental-setup,experimental setup,For,full shot training,experimental setup For full shot training,0.5263009667396545
translation,59,76,experimental-setup,greedy decoding,in,test time,greedy decoding in test time,0.5125577449798584
translation,59,76,experimental-setup,experimental setup,use,greedy decoding,experimental setup use greedy decoding,0.6241755485534668
translation,59,5,model,slot description enhanced generative approach,for,zero-shot cross-domain dst,slot description enhanced generative approach for zero-shot cross-domain dst,0.6067564487457275
translation,59,5,model,model,propose,slot description enhanced generative approach,model propose slot description enhanced generative approach,0.6912499070167542
translation,59,6,model,dialogue context and slots,with,pre-trained self-attentive encoder,dialogue context and slots with pre-trained self-attentive encoder,0.6377302408218384
translation,59,6,model,slot values,in,auto-regressive manner,slot values in auto-regressive manner,0.5544225573539734
translation,59,6,model,model,first encodes,dialogue context and slots,model first encodes dialogue context and slots,0.6600472927093506
translation,59,7,model,slot type informed descriptions,that capture,shared information,slot type informed descriptions that capture shared information,0.7060160040855408
translation,59,7,model,shared information,across,slots,shared information across slots,0.6831014752388
translation,59,7,model,shared information,to facilitate,cross-domain knowledge transfer,shared information to facilitate cross-domain knowledge transfer,0.6886681914329529
translation,59,7,model,model,incorporate,slot type informed descriptions,model incorporate slot type informed descriptions,0.7117243409156799
translation,59,23,model,large scale pre-trained sequence - to-sequence ( seq2seq ) models,effective encoding of,slot descriptions,large scale pre-trained sequence - to-sequence ( seq2seq ) models effective encoding of slot descriptions,0.7271466255187988
translation,59,24,model,generative dst model,called,t5dst,generative dst model called t5dst,0.6995141506195068
translation,59,24,model,generative dst model,models,relation,generative dst model models relation,0.7604504823684692
translation,59,24,model,generative dst model,models,dialogue context,generative dst model models dialogue context,0.7051692605018616
translation,59,24,model,generative dst model,generates,slot value,generative dst model generates slot value,0.659894585609436
translation,59,24,model,relation,of,slot,relation of slot,0.6207640171051025
translation,59,24,model,relation,of,dialogue context,relation of dialogue context,0.535801351070404
translation,59,24,model,dialogue context,with,self-attentive encoder,dialogue context with self-attentive encoder,0.6282849311828613
translation,59,24,model,slot value,with,decoder,slot value with decoder,0.6855265498161316
translation,59,24,model,decoder,in,autoregressive manner,decoder in autoregressive manner,0.5851545333862305
translation,59,24,model,model,introduce,generative dst model,model introduce generative dst model,0.6169271469116211
translation,59,26,model,cross-domain transferability,propose,slot type informed descriptions,cross-domain transferability propose slot type informed descriptions,0.6441422700881958
translation,59,26,model,slot type informed descriptions,that capture,shared information,slot type informed descriptions that capture shared information,0.7060160040855408
translation,59,26,model,shared information,of,different slots,shared information of different slots,0.6071040034294128
translation,59,26,model,model,To further enhance,cross-domain transferability,model To further enhance cross-domain transferability,0.6844291687011719
translation,59,99,results,t5dst,achieves,significantly higher performance,t5dst achieves significantly higher performance,0.7125971913337708
translation,59,99,results,significantly higher performance,in terms of,averaged joint goal accuracy,significantly higher performance in terms of averaged joint goal accuracy,0.6902081370353699
translation,59,99,results,significantly higher performance,compared to,three baseline models trade,significantly higher performance compared to three baseline models trade,0.7387086749076843
translation,59,100,results,our model,effectively capture,slot-context relation,our model effectively capture slot-context relation,0.6843411326408386
translation,59,100,results,our model,generalize,better,our model generalize better,0.7520231604576111
translation,59,100,results,better,in,unseen domains,better in unseen domains,0.5761732459068298
translation,59,100,results,results,demonstrate,our model,results demonstrate our model,0.6473821401596069
translation,59,101,results,slot-names,with,human annotated slot descriptions,slot-names with human annotated slot descriptions,0.6165656447410583
translation,59,101,results,does not bring improvement,to,zero-shot performance,does not bring improvement to zero-shot performance,0.5755795240402222
translation,59,101,results,human annotated slot descriptions,has,does not bring improvement,human annotated slot descriptions has does not bring improvement,0.6069384217262268
translation,59,101,results,results,Replacing,slot-names,results Replacing slot-names,0.6209133267402649
translation,59,105,results,model,using,naive slot descriptions,model using naive slot descriptions,0.6839496493339539
translation,59,105,results,naive slot descriptions,gives,similar performance,naive slot descriptions gives similar performance,0.6187194585800171
translation,59,105,results,results,has,model,results has model,0.5339115858078003
translation,59,110,results,models,trained with,question style descriptions,models trained with question style descriptions,0.6913248300552368
translation,59,110,results,question style descriptions,improves,performance,question style descriptions improves performance,0.6965370178222656
translation,59,110,results,performance,in,some domains,performance in some domains,0.5447567105293274
translation,59,110,results,fails,in,others,fails in others,0.5928916335105896
translation,59,110,results,results,has,models,results has models,0.5335168838500977
translation,59,111,results,our proposed slot type informed descriptions,consistently improves,zero-shot performance,our proposed slot type informed descriptions consistently improves zero-shot performance,0.7944988012313843
translation,59,111,results,zero-shot performance,of,t5dst,zero-shot performance of t5dst,0.6251863837242126
translation,59,111,results,t5dst,in,all the domains,t5dst in all the domains,0.5441440939903259
translation,59,111,results,results,has,our proposed slot type informed descriptions,results has our proposed slot type informed descriptions,0.5571365356445312
translation,59,112,results,average of 2 %,has,joint goal accuracy improvement,average of 2 % has joint goal accuracy improvement,0.5556885004043579
translation,59,112,results,results,produced,average of 2 %,results produced average of 2 %,0.7312402725219727
translation,59,114,results,slot accuracy,of,models,slot accuracy of models,0.6226983070373535
translation,59,114,results,models,using,naive and slot type description,models using naive and slot type description,0.666732907295227
translation,59,114,results,results,show,slot accuracy,results show slot accuracy,0.6593942046165466
translation,59,115,results,obverse significant gain,of,time slots,obverse significant gain of time slots,0.6242061257362366
translation,59,115,results,obverse significant gain,of,"location slots ( e.g. , departure and destination )","obverse significant gain of location slots ( e.g. , departure and destination )",0.5896627902984619
translation,59,115,results,obverse significant gain,of,"number slots ( e.g. , book stay and book people )","obverse significant gain of number slots ( e.g. , book stay and book people )",0.5892040729522705
translation,59,115,results,"number slots ( e.g. , book stay and book people )",by adding,slot type information,"number slots ( e.g. , book stay and book people ) by adding slot type information",0.6881483197212219
translation,59,115,results,results,Compared to,naive description,results Compared to naive description,0.672050952911377
translation,59,119,results,dstqa model,in,4 out of 5 domains,dstqa model in 4 out of 5 domains,0.5518020987510681
translation,59,119,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,59,119,results,outperforms,has,dstqa model,outperforms has dstqa model,0.5919824242591858
translation,59,119,results,results,has,our model,results has our model,0.5871725678443909
translation,60,159,ablation-analysis,contribution,of,nli component,contribution of nli component,0.5838293433189392
translation,60,159,ablation-analysis,nli component,resulted in,even higher gains,nli component resulted in even higher gains,0.6817415356636047
translation,60,159,ablation-analysis,even higher gains,in terms of,correlation,even higher gains in terms of correlation,0.7883266806602478
translation,60,159,ablation-analysis,even higher gains,showing,benefit,even higher gains showing benefit,0.7275915741920471
translation,60,159,ablation-analysis,correlation,in comparison to,wow experiments,correlation in comparison to wow experiments,0.7151641845703125
translation,60,159,ablation-analysis,benefit,of using,intricate span comparison method,benefit of using intricate span comparison method,0.7171440124511719
translation,60,159,ablation-analysis,ablation analysis,has,contribution,ablation analysis has contribution,0.5164179801940918
translation,60,22,baselines,question answering ( qa ),for,dialogue generation evaluation,question answering ( qa ) for dialogue generation evaluation,0.5808135271072388
translation,60,25,baselines,novel comparison method,using,natural language inference models ( nli ;,novel comparison method using natural language inference models ( nli ;,0.6847227215766907
translation,60,25,baselines,novel comparison method,more robust to,lexical variability,novel comparison method more robust to lexical variability,0.7168852686882019
translation,60,105,baselines,dodecadialogue,is,multi-task model,dodecadialogue is multi-task model,0.5733237862586975
translation,60,105,baselines,multi-task model,fine-tuned on,wow,multi-task model fine-tuned on wow,0.7665631175041199
translation,60,105,baselines,wow,in,dodecadialogue benchmark,wow in dodecadialogue benchmark,0.540967583656311
translation,60,105,baselines,baselines,has,dodecadialogue,baselines has dodecadialogue,0.5755103230476379
translation,60,4,results,results,has,neural knowledge- grounded generative,results has neural knowledge- grounded generative,0.5485879182815552
translation,60,29,results,q 2,reaches,significantly higher correlations,q 2 reaches significantly higher correlations,0.7267098426818848
translation,60,29,results,significantly higher correlations,with,human judgments,significantly higher correlations with human judgments,0.6207250356674194
translation,60,29,results,human judgments,on,all datasets,human judgments on all datasets,0.4510139226913452
translation,60,29,results,results,has,q 2,results has q 2,0.5348592400550842
translation,60,133,results,inconsistent data,for,all baselines,inconsistent data for all baselines,0.6211280226707458
translation,60,169,results,q 2,performs,better,q 2 performs better,0.7394158244132996
translation,60,169,results,better,than,end-to- end nli baselines,better than end-to- end nli baselines,0.5865022540092468
translation,60,169,results,results,has,q 2,results has q 2,0.5348592400550842
translation,60,171,results,three datasets,demonstrate,"q 2 's zero-shot , reference - response-free capability","three datasets demonstrate q 2 's zero-shot , reference - response-free capability",0.6337693333625793
translation,60,171,results,"q 2 's zero-shot , reference - response-free capability",to generalize to,various dialogue tasks,"q 2 's zero-shot , reference - response-free capability to generalize to various dialogue tasks",0.7021327018737793
translation,60,171,results,results,on,three datasets,results on three datasets,0.49755188822746277
translation,60,178,results,correlations,with,human judgments,correlations with human judgments,0.6459334492683411
translation,60,178,results,barely influenced,showing,robustness,barely influenced showing robustness,0.7207245230674744
translation,60,181,results,smaller qg model,results in,lower q 2 scores,smaller qg model results in lower q 2 scores,0.6594626307487488
translation,60,181,results,lower q 2 scores,for,all data splits,lower q 2 scores for all data splits,0.6340651512145996
translation,60,182,results,higher q 2 scores,in,all cases,higher q 2 scores in all cases,0.5525476932525635
translation,60,182,results,smaller qa model,has,opposite outcome,smaller qa model has opposite outcome,0.5973864197731018
translation,60,182,results,opposite outcome,has,higher q 2 scores,opposite outcome has higher q 2 scores,0.5658314824104309
translation,60,182,results,results,using,smaller qa model,results using smaller qa model,0.7063326239585876
translation,60,231,results,qg - qa based methods,showed,higher correlations,qg - qa based methods showed higher correlations,0.7131431698799133
translation,60,231,results,higher correlations,with,human judgments,higher correlations with human judgments,0.6053839325904846
translation,60,231,results,human judgments,of,factual consistency,human judgments of factual consistency,0.532204806804657
translation,60,231,results,abstractive summaries,has,qg - qa based methods,abstractive summaries has qg - qa based methods,0.5432571172714233
translation,61,16,experiments,collaborate,to achieve,joint goals,collaborate to achieve joint goals,0.6884754300117493
translation,62,135,experimental-setup,implementation,uses,nvida titan rtx gpu,implementation uses nvida titan rtx gpu,0.5771809220314026
translation,62,135,experimental-setup,implementation,uses,training,implementation uses training,0.6428120732307434
translation,62,135,experimental-setup,nvida titan rtx gpu,for,training,nvida titan rtx gpu for training,0.6097574234008789
translation,62,135,experimental-setup,nvida titan rtx gpu,for,training,nvida titan rtx gpu for training,0.6097574234008789
translation,62,135,experimental-setup,experimental setup,has,implementation,experimental setup has implementation,0.5296319723129272
translation,62,6,model,45 k multimodal dialogue dataset,created with,minimal human intervention,45 k multimodal dialogue dataset created with minimal human intervention,0.6543537974357605
translation,62,6,model,model,proposes,45 k multimodal dialogue dataset,model proposes 45 k multimodal dialogue dataset,0.647872269153595
translation,62,73,model,simple retrieval model,composed of,three modules,simple retrieval model composed of three modules,0.6972825527191162
translation,62,73,model,simple retrieval model,composed of,fusion module,simple retrieval model composed of fusion module,0.6793239116668701
translation,62,73,model,"resnext - 101 ( xie et al. , 2017 )",for,image encoder,"resnext - 101 ( xie et al. , 2017 ) for image encoder",0.5953713655471802
translation,62,73,model,"bert ( devlin et al. , 2019 )",for,text encoder,"bert ( devlin et al. , 2019 ) for text encoder",0.6153209209442139
translation,62,73,model,model,use,simple retrieval model,model use simple retrieval model,0.6547335982322693
translation,62,67,results,average scores,evaluated by,three annotators,average scores evaluated by three annotators,0.6669957041740417
translation,62,67,results,three annotators,shown to be,"2.56 , 2.17 , and 3.13","three annotators shown to be 2.56 , 2.17 , and 3.13",0.6183474063873291
translation,62,67,results,"q1 , q2 , and q3",has,average scores,"q1 , q2 , and q3 has average scores",0.5567630529403687
translation,62,67,results,results,For,"q1 , q2 , and q3","results For q1 , q2 , and q3",0.595840573310852
translation,62,81,results,r@1 performance,of,retrieval model,r@1 performance of retrieval model,0.5949780941009521
translation,62,81,results,retrieval model,obtained,50.35 and 14.38,retrieval model obtained 50.35 and 14.38,0.622155487537384
translation,62,81,results,50.35 and 14.38,on,current and next sentence prediction task,50.35 and 14.38 on current and next sentence prediction task,0.50328129529953
translation,62,81,results,baseline,on,both tasks,baseline on both tasks,0.5174500346183777
translation,62,81,results,outperforming,has,baseline,outperforming has baseline,0.6440464854240417
translation,62,88,results,recall measure,for,ground - truth answers,recall measure for ground - truth answers,0.5441272258758545
translation,62,88,results,ground - truth answers,in,model,ground - truth answers in model,0.491466224193573
translation,62,88,results,ground - truth answers,in,model,ground - truth answers in model,0.491466224193573
translation,62,88,results,model,considers,images,model considers images,0.7150904536247253
translation,62,88,results,model,both,context and image,model both context and image,0.6705045104026794
translation,62,88,results,model,considering,images,model considering images,0.7409290671348572
translation,62,88,results,context and image,is,higher,context and image is higher,0.6330778002738953
translation,62,88,results,higher,than,model,higher than model,0.6345841288566589
translation,62,88,results,model,considering,images,model considering images,0.7409290671348572
translation,62,88,results,model,only,images,model only images,0.7429303526878357
translation,62,88,results,results,show that,recall measure,results show that recall measure,0.5524351000785828
translation,62,134,results,our retrieval model,has,significantly outperforms,our retrieval model has significantly outperforms,0.612432599067688
translation,62,134,results,significantly outperforms,has,information retrieval baseline,significantly outperforms has information retrieval baseline,0.5838168263435364
translation,63,141,ablation-analysis,ablation knowledge,has,increases,ablation knowledge has increases,0.5962934494018555
translation,63,141,ablation-analysis,two methods,has,significantly decreases,two methods has significantly decreases,0.5732128620147705
translation,63,141,ablation-analysis,ablation analysis,As,ablation knowledge,ablation analysis As ablation knowledge,0.5153361558914185
translation,63,145,ablation-analysis,each loss,contributes,a lot,each loss contributes a lot,0.6196203231811523
translation,63,145,ablation-analysis,a lot,in,automatic evaluation,a lot in automatic evaluation,0.521335780620575
translation,63,145,ablation-analysis,a lot,with,f1,a lot with f1,0.7142963409423828
translation,63,145,ablation-analysis,increasing largely,by adding,each objective,increasing largely by adding each objective,0.7168207764625549
translation,63,145,ablation-analysis,f1,has,increasing largely,f1 has increasing largely,0.6297160387039185
translation,63,145,ablation-analysis,ablation analysis,see that,each loss,ablation analysis see that each loss,0.6648178100585938
translation,63,41,baselines,tnf,accelerates,pre-training,tnf accelerates pre-training,0.6779582500457764
translation,63,41,baselines,pre-training,by taking notes for,rare words,pre-training by taking notes for rare words,0.6747720241546631
translation,63,41,baselines,baselines,has,tnf,baselines has tnf,0.5985491871833801
translation,63,94,experimental-setup,ke - blender,with,transformers,ke - blender with transformers,0.7407450675964355
translation,63,94,experimental-setup,blenderbot - 90 m,as,pre-trained language model,blenderbot - 90 m as pre-trained language model,0.498138427734375
translation,63,94,experimental-setup,experimental setup,implement,ke - blender,experimental setup implement ke - blender,0.6398611068725586
translation,63,95,experimental-setup,adamw,with,batch size,adamw with batch size,0.6275458335876465
translation,63,95,experimental-setup,adamw,to optimize,parameters,adamw to optimize parameters,0.7094350457191467
translation,63,95,experimental-setup,batch size,of,128,batch size of 128,0.6836684346199036
translation,63,95,experimental-setup,128,to optimize,parameters,128 to optimize parameters,0.7025567889213562
translation,63,95,experimental-setup,experimental setup,has,adamw,experimental setup has adamw,0.5612581372261047
translation,63,96,experimental-setup,initial learning rate,set as,1e - 5,initial learning rate set as 1e - 5,0.6392835378646851
translation,63,96,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,63,97,experimental-setup,maximum input tokens,as,512,maximum input tokens as 512,0.551478922367096
translation,63,97,experimental-setup,experimental setup,set,maximum input tokens,experimental setup set maximum input tokens,0.6495475172996521
translation,63,156,experiments,perplexity,of,ground - truth wikipedia knowledge,perplexity of ground - truth wikipedia knowledge,0.5241336226463318
translation,63,156,experiments,ground - truth wikipedia knowledge,on,test unseen,ground - truth wikipedia knowledge on test unseen,0.4787176847457886
translation,63,156,experiments,test unseen,is,6.81,test unseen is 6.81,0.5452311038970947
translation,63,156,experiments,test unseen,only,6.81,test unseen only 6.81,0.6251845359802246
translation,63,26,model,"blender ( roller et al. , 2020 )",as,backbone model,"blender ( roller et al. , 2020 ) as backbone model",0.494843453168869
translation,63,26,model,two auxiliary training objectives,in,finetuning,two auxiliary training objectives in finetuning,0.5251460671424866
translation,63,26,model,model,take,"blender ( roller et al. , 2020 )","model take blender ( roller et al. , 2020 )",0.6306227445602417
translation,63,26,model,model,propose,two auxiliary training objectives,model propose two auxiliary training objectives,0.6856528520584106
translation,63,27,model,first objective,Interpret,masked word,first objective Interpret masked word,0.6959765553474426
translation,63,27,model,masked word,predicts,word 's definition,masked word predicts word 's definition,0.7346861362457275
translation,63,27,model,word 's definition,based on,context,word 's definition based on context,0.6366132497787476
translation,63,27,model,model,has,first objective,model has first objective,0.537074625492096
translation,63,135,model,model,Inference with,retrieved knowledge,model Inference with retrieved knowledge,0.7193086743354797
translation,63,119,results,ke - blender,performs,competitively,ke - blender performs competitively,0.663968026638031
translation,63,119,results,competitively,in,knowledgeavailable setting,competitively in knowledgeavailable setting,0.5550658702850342
translation,63,119,results,strong baselines,has,ke - blender,strong baselines has ke - blender,0.5971726179122925
translation,63,119,results,results,Compared with,strong baselines,results Compared with strong baselines,0.7150382995605469
translation,63,122,results,knowledge grounded methods,perform,well,knowledge grounded methods perform well,0.6087181568145752
translation,63,122,results,well,when,knowledge,well when knowledge,0.6481377482414246
translation,63,122,results,knowledge,provided during,inference,knowledge provided during inference,0.7266526818275452
translation,63,122,results,our method,is,robust,our method is robust,0.6188791394233704
translation,63,122,results,does not degrade,in,w/ knowledge setting,does not degrade in w/ knowledge setting,0.5270668268203735
translation,63,122,results,results,has,knowledge grounded methods,results has knowledge grounded methods,0.4532232880592346
translation,63,126,results,our method,shows,large advantages,our method shows large advantages,0.6230193972587585
translation,63,126,results,large advantages,in,all metrics,large advantages in all metrics,0.5095175504684448
translation,63,126,results,large advantages,achieving,16.7 f1 score,large advantages achieving 16.7 f1 score,0.6428481936454773
translation,63,131,results,our method,gives,best performance,our method gives best performance,0.5973793864250183
translation,63,131,results,best performance,demonstrating,strong performance,best performance demonstrating strong performance,0.7135971188545227
translation,63,131,results,strong performance,when,knowledge,strong performance when knowledge,0.6136168837547302
translation,63,131,results,results,has,our method,results has our method,0.5589964985847473
translation,63,132,results,outperforms,when,knowledge,outperforms when knowledge,0.6777231693267822
translation,63,132,results,simple knowledge grounded methods,when,knowledge,simple knowledge grounded methods when knowledge,0.589381217956543
translation,63,132,results,knowledge,is,available,knowledge is available,0.5555298924446106
translation,63,132,results,outperforms,has,simple knowledge grounded methods,outperforms has simple knowledge grounded methods,0.5943895578384399
translation,63,136,results,knowledge,is,unavailable,knowledge is unavailable,0.5718426704406738
translation,63,136,results,ke - blender,giving,highly competitive results,ke - blender giving highly competitive results,0.7114894390106201
translation,63,136,results,blender and kg - blender ( p< 0.01 ),measured in,knowledgeable and coherent,blender and kg - blender ( p< 0.01 ) measured in knowledgeable and coherent,0.6445813179016113
translation,63,136,results,blender and kg - blender ( p< 0.01 ),giving,highly competitive results,blender and kg - blender ( p< 0.01 ) giving highly competitive results,0.632523775100708
translation,63,136,results,knowledge,has,ke - blender,knowledge has ke - blender,0.6693315505981445
translation,63,136,results,unavailable,has,ke - blender,unavailable has ke - blender,0.6917638182640076
translation,63,136,results,ke - blender,has,significantly outperforms,ke - blender has significantly outperforms,0.6152206659317017
translation,63,136,results,significantly outperforms,has,blender and kg - blender ( p< 0.01 ),significantly outperforms has blender and kg - blender ( p< 0.01 ),0.5670196413993835
translation,63,136,results,results,When,knowledge,results When knowledge,0.6308422684669495
translation,63,151,results,ke - blender,shows,superior performance,ke - blender shows superior performance,0.6625074744224548
translation,63,151,results,superior performance,by producing,informative and knowledgeable responses,superior performance by producing informative and knowledgeable responses,0.6630050539970398
translation,63,151,results,results,has,ke - blender,results has ke - blender,0.5750617980957031
translation,64,9,model,model,formalize,knowledge - driven slot constraints,model formalize knowledge - driven slot constraints,0.7155117988586426
translation,64,13,model,nlu,construct,semantic frame,nlu construct semantic frame,0.7217603921890259
translation,64,34,model,model,represent,slot constraints,model represent slot constraints,0.6080135703086853
translation,64,35,model,three approaches,based on,pipeline approach,three approaches based on pipeline approach,0.7152365446090698
translation,64,35,model,three approaches,based on,end-to- end approach,three approaches based on end-to- end approach,0.6955485343933105
translation,64,35,model,two domains,of,"multidogo dataset ( peskov et al. , 2019 )","two domains of multidogo dataset ( peskov et al. , 2019 )",0.5179466605186462
translation,64,35,model,two domains,augmented with,constraint violation labels,two domains augmented with constraint violation labels,0.6983746290206909
translation,64,35,model,model,propose,three approaches,model propose three approaches,0.7077018022537231
translation,64,166,results,performance,of,jointbert,performance of jointbert,0.659844696521759
translation,64,166,results,performance,of,slot labelling,performance of slot labelling,0.6269195079803467
translation,64,166,results,jointbert,for,intent classification,jointbert for intent classification,0.6024606823921204
translation,64,166,results,jointbert,for,slot labelling,jointbert for slot labelling,0.6208069920539856
translation,64,167,results,jointbert,performed,better,jointbert performed better,0.26725733280181885
translation,64,167,results,better,on,insurance domain,better on insurance domain,0.5177342295646667
translation,64,167,results,better,for,ic and sl,better for ic and sl,0.6701563596725464
translation,64,167,results,results,seen that,jointbert,results seen that jointbert,0.6619483828544617
translation,64,180,results,simplest method,yielded,acceptable results,simplest method yielded acceptable results,0.5541631579399109
translation,64,180,results,simplest method,yielded,surprisingly good results,simplest method yielded surprisingly good results,0.585594654083252
translation,64,180,results,exact match,yielded,surprisingly good results,exact match yielded surprisingly good results,0.6587461829185486
translation,64,180,results,acceptable results,for,fast food domain,acceptable results for fast food domain,0.6272234320640564
translation,64,180,results,surprisingly good results,for,insurance domain,surprisingly good results for insurance domain,0.6314970850944519
translation,64,180,results,simplest method,has,exact match,simplest method has exact match,0.5102361440658569
translation,64,180,results,results,has,simplest method,results has simplest method,0.5392941236495972
translation,64,187,results,reasonable none thresholds,to,nli and average,reasonable none thresholds to nli and average,0.6112923622131348
translation,64,187,results,boosted,up,results,boosted up results,0.6271374821662903
translation,64,187,results,results,for,all the metrics,results for all the metrics,0.5809031128883362
translation,64,187,results,nli and average,has,boosted,nli and average has boosted,0.6494281888008118
translation,64,187,results,results,applying,reasonable none thresholds,results applying reasonable none thresholds,0.7349017858505249
translation,64,187,results,results,for,all the metrics,results for all the metrics,0.5809031128883362
translation,64,188,results,average method,with,threshold,average method with threshold,0.6446095705032349
translation,64,188,results,threshold,of,0.5,threshold of 0.5,0.6345103979110718
translation,64,188,results,threshold,achieved,best link accuracy and f1,threshold achieved best link accuracy and f1,0.7477987408638
translation,64,188,results,0.5,achieved,best link accuracy and f1,0.5 achieved best link accuracy and f1,0.6616869568824768
translation,64,188,results,best link accuracy and f1,for,insurance domain and the fast food domain,best link accuracy and f1 for insurance domain and the fast food domain,0.6132581233978271
translation,64,188,results,results,has,average method,results has average method,0.5619539618492126
translation,64,189,results,combination of methods,results in,better entity linking performance,combination of methods results in better entity linking performance,0.6204804182052612
translation,64,189,results,results,using,combination of methods,results using combination of methods,0.6156701445579529
translation,64,210,results,deterministic pipeline ( dp ) and the probabilistic pipeline ( pp ) approaches,see that,dp,deterministic pipeline ( dp ) and the probabilistic pipeline ( pp ) approaches see that dp,0.6773124933242798
translation,64,210,results,outperformed,in,most settings,outperformed in most settings,0.568120002746582
translation,64,210,results,outperformed,especially in,insurance domain,outperformed especially in insurance domain,0.6249573826789856
translation,64,210,results,pp,in,most settings,pp in most settings,0.6339598894119263
translation,64,210,results,most settings,especially in,insurance domain,most settings especially in insurance domain,0.5884847640991211
translation,64,210,results,dp,has,outperformed,dp has outperformed,0.6632216572761536
translation,64,210,results,outperformed,has,pp,outperformed has pp,0.6498726010322571
translation,64,210,results,results,Comparing,deterministic pipeline ( dp ) and the probabilistic pipeline ( pp ) approaches,results Comparing deterministic pipeline ( dp ) and the probabilistic pipeline ( pp ) approaches,0.6997816562652588
translation,64,213,results,dp and pp,in,insurance domain,dp and pp in insurance domain,0.5506297945976257
translation,64,213,results,dp and pp,in,fast food domain,dp and pp in fast food domain,0.5393655300140381
translation,64,213,results,dp and pp,in,fast food domain,dp and pp in fast food domain,0.5393655300140381
translation,64,213,results,competitive,to,dp and pp,competitive to dp and pp,0.613960862159729
translation,64,213,results,dp and pp,in,fast food domain,dp and pp in fast food domain,0.5393655300140381
translation,64,213,results,end-to- end approach ( ee ),has,outperformed,end-to- end approach ( ee ) has outperformed,0.5368916988372803
translation,64,213,results,outperformed,has,dp and pp,outperformed has dp and pp,0.6163343191146851
translation,65,172,ablation-analysis,each step,of,domain-lifelong learning process,each step of domain-lifelong learning process,0.5682923197746277
translation,65,172,ablation-analysis,performance gap,between,emar and our method kpn,performance gap between emar and our method kpn,0.6318588256835938
translation,65,172,ablation-analysis,ablation analysis,At,each step,ablation analysis At each step,0.549068808555603
translation,65,186,ablation-analysis,- mpr,remove,multi-prototype enhanced retrospection,- mpr remove multi-prototype enhanced retrospection,0.7176257371902466
translation,65,186,ablation-analysis,- mpr,randomly select,samples,- mpr randomly select samples,0.6731054782867432
translation,65,186,ablation-analysis,ablation analysis,For,- mpr,ablation analysis For - mpr,0.6411301493644714
translation,65,197,ablation-analysis,any knowledge distillation strategy,has,encoder feature distillation ( efd ),any knowledge distillation strategy has encoder feature distillation ( efd ),0.5826273560523987
translation,65,197,ablation-analysis,ablation analysis,Removing,any knowledge distillation strategy,ablation analysis Removing any knowledge distillation strategy,0.7756566405296326
translation,65,198,ablation-analysis,all knowledge distillation strategies ( mskd ),has,performance further declines,all knowledge distillation strategies ( mskd ) has performance further declines,0.5966492891311646
translation,65,198,ablation-analysis,ablation analysis,remove,all knowledge distillation strategies ( mskd ),ablation analysis remove all knowledge distillation strategies ( mskd ),0.6908365488052368
translation,65,200,ablation-analysis,multi-prototype enhanced retrospection,has,performance,multi-prototype enhanced retrospection has performance,0.5577406883239746
translation,65,200,ablation-analysis,multi-strategy knowledge distillation,has,performance,multi-strategy knowledge distillation has performance,0.5401707291603088
translation,65,200,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,65,200,ablation-analysis,drops,has,significantly,drops has significantly,0.6735619306564331
translation,65,200,ablation-analysis,ablation analysis,remove,multi-prototype enhanced retrospection,ablation analysis remove multi-prototype enhanced retrospection,0.674553632736206
translation,65,201,ablation-analysis,two components,is,very effective,two components is very effective,0.5602961778640747
translation,65,201,ablation-analysis,ablation analysis,simultaneously exploiting,two components,ablation analysis simultaneously exploiting two components,0.7553014159202576
translation,65,210,ablation-analysis,proposed multiprototype enhanced retrospection,effectively selects,most representative samples,proposed multiprototype enhanced retrospection effectively selects most representative samples,0.781780481338501
translation,65,210,ablation-analysis,ablation analysis,has,proposed multiprototype enhanced retrospection,ablation analysis has proposed multiprototype enhanced retrospection,0.5734930038452148
translation,65,157,baselines,update,of,important parameters,update of important parameters,0.5544055104255676
translation,65,157,baselines,update,by adding,l 2 regularization,update by adding l 2 regularization,0.6458519697189331
translation,65,157,baselines,l 2 regularization,of,parameter changes,l 2 regularization of parameter changes,0.5702307820320129
translation,65,157,baselines,baselines,adopt,other model- agnostic lifelong learning methods,baselines adopt other model- agnostic lifelong learning methods,0.5743910074234009
translation,65,158,baselines,current network,with,original network,current network with original network,0.632748007774353
translation,65,158,baselines,original network,by,knowledge distillation,original network by knowledge distillation,0.5825067758560181
translation,65,158,baselines,lwf,has,"and hoiem , 2017 )","lwf has and hoiem , 2017 )",0.5860306620597839
translation,65,158,baselines,baselines,has,lwf,baselines has lwf,0.570166826248169
translation,65,159,baselines,emr,alleviates,forgetting,emr alleviates forgetting,0.7953776121139526
translation,65,159,baselines,forgetting,by randomly storing,some old samples,forgetting by randomly storing some old samples,0.7415943145751953
translation,65,159,baselines,baselines,has,emr,baselines has emr,0.566661536693573
translation,65,203,baselines,baselines,compare,method kpn,baselines compare method kpn,0.6789782047271729
translation,65,16,experiments,domain-lifelong learning,for,dialogue state tracking ( dll - dst ),domain-lifelong learning for dialogue state tracking ( dll - dst ),0.631165623664856
translation,65,187,experiments,outperforms,by,1.51 % and 3.6 %,outperforms by 1.51 % and 3.6 %,0.6198310256004333
translation,65,187,experiments,1.51 % and 3.6 %,in terms of,whole jga,1.51 % and 3.6 % in terms of whole jga,0.7548835873603821
translation,65,187,experiments,method kpn,has,outperforms,method kpn has outperforms,0.6532742381095886
translation,65,149,hyperparameters,learning rate,set to,5e ? 5,learning rate set to 5e ? 5,0.7431448698043823
translation,65,149,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,65,150,hyperparameters,batch size,is,4,batch size is 4,0.6537848711013794
translation,65,150,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,65,151,hyperparameters,hyper-parameters,are,0.2 and 0.1,hyper-parameters are 0.2 and 0.1,0.5736122131347656
translation,65,151,hyperparameters,hyperparameters,has,hyper-parameters,hyperparameters has hyper-parameters,0.526509702205658
translation,65,153,hyperparameters,memory,is,50,memory is 50,0.6251314878463745
translation,65,154,hyperparameters,hyper- parameters,obtained by,grid search,hyper- parameters obtained by grid search,0.6833842992782593
translation,65,154,hyperparameters,grid search,on,validation set,grid search on validation set,0.5906150937080383
translation,65,154,hyperparameters,hyperparameters,has,hyper- parameters,hyperparameters has hyper- parameters,0.526509702205658
translation,65,8,model,novel domainlifelong learning method,called,knowledge preservation networks ( kpn ),novel domainlifelong learning method called knowledge preservation networks ( kpn ),0.6453564763069153
translation,65,8,model,novel domainlifelong learning method,consists of,multi-strategy knowledge distillation,novel domainlifelong learning method consists of multi-strategy knowledge distillation,0.6108843088150024
translation,65,8,model,novel domainlifelong learning method,to solve,problems,novel domainlifelong learning method to solve problems,0.6755165457725525
translation,65,8,model,knowledge preservation networks ( kpn ),consists of,multi-strategy knowledge distillation,knowledge preservation networks ( kpn ) consists of multi-strategy knowledge distillation,0.6392707824707031
translation,65,8,model,model,propose,novel domainlifelong learning method,model propose novel domainlifelong learning method,0.6681336760520935
translation,65,37,model,knowledge preservation networks ( kpn ),to handle,expression diversity,knowledge preservation networks ( kpn ) to handle expression diversity,0.7167472839355469
translation,65,37,model,expression diversity,propose,multi-prototype enhanced retrospection,expression diversity propose multi-prototype enhanced retrospection,0.6008176207542419
translation,65,37,model,expression diversity,computes,multiple slot prototypes,expression diversity computes multiple slot prototypes,0.7109983563423157
translation,65,37,model,multi-prototype enhanced retrospection,computes,multiple slot prototypes,multi-prototype enhanced retrospection computes multiple slot prototypes,0.7538742423057556
translation,65,37,model,multi-prototype enhanced retrospection,selects,most representative old samples,multi-prototype enhanced retrospection selects most representative old samples,0.625636100769043
translation,65,37,model,multiple slot prototypes,for,each domain,multiple slot prototypes for each domain,0.6329181790351868
translation,65,37,model,most representative old samples,based on,slot prototypes,most representative old samples based on slot prototypes,0.6437672972679138
translation,65,37,model,combinatorial explosion problem,propose,multi-strategy knowledge distillation,combinatorial explosion problem propose multi-strategy knowledge distillation,0.598454475402832
translation,65,37,model,multi-strategy knowledge distillation,enables,model,multi-strategy knowledge distillation enables model,0.5815830230712891
translation,65,37,model,knowledge,of,model,knowledge of model,0.5640254020690918
translation,65,37,model,model,trained in,last step,model trained in last step,0.7965952157974243
translation,65,37,model,last step,from,multiple aspects,last step from multiple aspects,0.5967041254043579
translation,65,37,model,model,propose,knowledge preservation networks ( kpn ),model propose knowledge preservation networks ( kpn ),0.6565905809402466
translation,65,37,model,model,to cope with,combinatorial explosion problem,model to cope with combinatorial explosion problem,0.6186123490333557
translation,65,37,model,model,propose,multi-strategy knowledge distillation,model propose multi-strategy knowledge distillation,0.6434160470962524
translation,65,54,model,knowledge preservation networks ( kpn ),to handle,dll - dst task,knowledge preservation networks ( kpn ) to handle dll - dst task,0.7124198079109192
translation,65,54,model,model,propose,knowledge preservation networks ( kpn ),model propose knowledge preservation networks ( kpn ),0.6565905809402466
translation,65,156,model,model- agnostic lifelong learning method,to handle,dll - dst task,model- agnostic lifelong learning method to handle dll - dst task,0.6847162246704102
translation,65,156,model,model,propose,model- agnostic lifelong learning method,model propose model- agnostic lifelong learning method,0.6550092101097107
translation,65,169,results,our proposed method kpn,achieves,state - of - theart performance,our proposed method kpn achieves state - of - theart performance,0.6408493518829346
translation,65,169,results,state - of - theart performance,in both,multiwoz and sgd benchmarks,state - of - theart performance in both multiwoz and sgd benchmarks,0.6143093705177307
translation,65,169,results,our proposed method kpn,has,significantly outperforms,our proposed method kpn has significantly outperforms,0.626405656337738
translation,65,169,results,significantly outperforms,has,other baselines,significantly outperforms has other baselines,0.5847466588020325
translation,65,169,results,results,observe,our proposed method kpn,results observe our proposed method kpn,0.6048627495765686
translation,65,170,results,our method,achieves,4.25 % and 8.27 % improvements,our method achieves 4.25 % and 8.27 % improvements,0.6449825763702393
translation,65,170,results,4.25 % and 8.27 % improvements,of,whole joint goal accuracy,4.25 % and 8.27 % improvements of whole joint goal accuracy,0.540273129940033
translation,65,170,results,whole joint goal accuracy,on,multi-woz benchmark and the sgd benchmark,whole joint goal accuracy on multi-woz benchmark and the sgd benchmark,0.5214207172393799
translation,65,170,results,emar,has,our method,emar has our method,0.6268696784973145
translation,65,170,results,results,compared to,emar,results compared to emar,0.6729850769042969
translation,65,177,results,finetune,achieves,worst results,finetune achieves worst results,0.6749447584152222
translation,65,177,results,worst results,on,both benchmarks,worst results on both benchmarks,0.4676153361797333
translation,65,177,results,results,has,finetune,results has finetune,0.5962889790534973
translation,65,188,results,effective,in selecting,most representative samples,effective in selecting most representative samples,0.7617841958999634
translation,65,188,results,most representative samples,from,diverse dialogues,most representative samples from diverse dialogues,0.5837371945381165
translation,65,188,results,results,show,multi-prototype enhanced retrospection,results show multi-prototype enhanced retrospection,0.5693870782852173
translation,65,192,results,+ icarl,is,even worse,+ icarl is even worse,0.6109271049499512
translation,65,192,results,even worse,than,random selection   - mpr,even worse than random selection   - mpr,0.615420937538147
translation,65,192,results,kpn,has,significantly outperforms,kpn has significantly outperforms,0.6253151893615723
translation,65,192,results,significantly outperforms,has,+ i carl,significantly outperforms has + i carl,0.6199343204498291
translation,65,192,results,results,has,kpn,results has kpn,0.5846155881881714
translation,65,199,results,performance,on,old domains,performance on old domains,0.5160799026489258
translation,65,199,results,original model,from,multiple perspectives,original model from multiple perspectives,0.5501646995544434
translation,65,199,results,retain,has,performance,retain has performance,0.6133943796157837
translation,65,199,results,results,shows,knowledge distillation strategies,results shows knowledge distillation strategies,0.6058592796325684
translation,65,206,results,emar and our method kpn,achieve,performance improvements,emar and our method kpn achieve performance improvements,0.6540385484695435
translation,65,206,results,performance improvements,as,number of reserved samples,performance improvements as number of reserved samples,0.5044888854026794
translation,65,206,results,number of reserved samples,has,increases,number of reserved samples has increases,0.6115884780883789
translation,65,206,results,results,has,emar and our method kpn,results has emar and our method kpn,0.607964813709259
translation,65,207,results,our method,has,significantly outperforms,our method has significantly outperforms,0.6120707988739014
translation,65,207,results,significantly outperforms,has,emar,significantly outperforms has emar,0.6024028658866882
translation,65,208,results,our method,using,only 30 samples,our method using only 30 samples,0.7059169411659241
translation,65,208,results,only 30 samples,achieves,comparable performance,only 30 samples achieves comparable performance,0.6502204537391663
translation,65,208,results,comparable performance,to,emar,comparable performance to emar,0.5839827060699463
translation,65,208,results,emar,using,50 samples,emar using 50 samples,0.7469838857650757
translation,65,208,results,results,has,our method,results has our method,0.5589964985847473
translation,66,10,ablation-analysis,performance,of,query rewrite,performance of query rewrite,0.5724535584449768
translation,66,10,ablation-analysis,query rewrite,can be,substantially boosted ( + 2.3 % f1 ),query rewrite can be substantially boosted ( + 2.3 % f1 ),0.644104540348053
translation,66,10,ablation-analysis,substantially boosted ( + 2.3 % f1 ),with the aid of,coreference modeling,substantially boosted ( + 2.3 % f1 ) with the aid of coreference modeling,0.6982041001319885
translation,66,10,ablation-analysis,ablation analysis,show that,performance,ablation analysis show that performance,0.47405195236206055
translation,66,161,ablation-analysis,reference match,drops from,82.0 to 78.7,reference match drops from 82.0 to 78.7,0.7052274942398071
translation,66,161,ablation-analysis,ablation analysis,see,coreference resolution,ablation analysis see coreference resolution,0.5742336511611938
translation,66,161,ablation-analysis,ablation analysis,without,coreference resolution,ablation analysis without coreference resolution,0.7268621325492859
translation,66,183,ablation-analysis,different components,in,our joint model,different components in our joint model,0.5271949172019958
translation,66,183,ablation-analysis,different components,contribute to,performance,different components contribute to performance,0.7042018175125122
translation,66,183,ablation-analysis,performance,of,query rewrite,performance of query rewrite,0.5724535584449768
translation,66,183,ablation-analysis,ablation analysis,investigate,different components,ablation analysis investigate different components,0.6183867454528809
translation,66,185,ablation-analysis,degrades,with,drop,degrades with drop,0.7242103219032288
translation,66,185,ablation-analysis,drop,of,2.9 % f1,drop of 2.9 % f1,0.5896645188331604
translation,66,185,ablation-analysis,drop,of,1.4 % rm rate,drop of 1.4 % rm rate,0.6090332269668579
translation,66,185,ablation-analysis,designed coref2qr attention layer,has,performance,designed coref2qr attention layer has performance,0.5256252884864807
translation,66,185,ablation-analysis,performance,has,degrades,performance has degrades,0.5837839841842651
translation,66,185,ablation-analysis,ablation analysis,without,designed coref2qr attention layer,ablation analysis without designed coref2qr attention layer,0.7296643257141113
translation,66,190,ablation-analysis,performance drop,can be,up to 5.9 % f1 ( 60.2 -> 54.3 ),performance drop can be up to 5.9 % f1 ( 60.2 -> 54.3 ),0.6163911819458008
translation,66,190,ablation-analysis,binary head,has,performance drop,binary head has performance drop,0.5870541930198669
translation,66,190,ablation-analysis,ablation analysis,Without,binary head,ablation analysis Without binary head,0.7355805039405823
translation,66,151,baselines,standard seq-to-seq model,with,attention ( seq2seq ),standard seq-to-seq model with attention ( seq2seq ),0.6430513262748718
translation,66,151,baselines,baselines,has,standard seq-to-seq model,baselines has standard seq-to-seq model,0.5285471081733704
translation,66,195,baselines,seq2seq + pg model,is,baseline seq2seq model,seq2seq + pg model is baseline seq2seq model,0.5537399053573608
translation,66,195,baselines,baseline seq2seq model,with,pointer - generator,baseline seq2seq model with pointer - generator,0.5960221290588379
translation,66,195,baselines,qr - only model,is,our model variant,qr - only model is our model variant,0.5480057597160339
translation,66,195,baselines,qr - only model,trained without,coreference modeling,qr - only model trained without coreference modeling,0.7734708786010742
translation,66,195,baselines,baselines,has,seq2seq + pg model,baselines has seq2seq + pg model,0.5352162718772888
translation,66,141,experimental-setup,gpt - 2 decoder layers and word classification layer,in,our model,gpt - 2 decoder layers and word classification layer in our model,0.49997666478157043
translation,66,141,experimental-setup,gpt - 2 decoder layers and word classification layer,initialized with,pre-trained weights,gpt - 2 decoder layers and word classification layer initialized with pre-trained weights,0.7425071001052856
translation,66,141,experimental-setup,pre-trained weights,from,gpt - 2 small model,pre-trained weights from gpt - 2 small model,0.5644859671592712
translation,66,141,experimental-setup,experimental setup,has,gpt - 2 decoder layers and word classification layer,experimental setup has gpt - 2 decoder layers and word classification layer,0.5299673676490784
translation,66,142,experimental-setup,model,using,"adam ( kingma and ba , 2014 ) optimizer","model using adam ( kingma and ba , 2014 ) optimizer",0.6570383310317993
translation,66,142,experimental-setup,"adam ( kingma and ba , 2014 ) optimizer",with,learning rate,"adam ( kingma and ba , 2014 ) optimizer with learning rate",0.6116575002670288
translation,66,142,experimental-setup,"adam ( kingma and ba , 2014 ) optimizer",with,batch size,"adam ( kingma and ba , 2014 ) optimizer with batch size",0.6164420247077942
translation,66,142,experimental-setup,learning rate,has,5e - 05,learning rate has 5e - 05,0.5807519555091858
translation,66,142,experimental-setup,batch size,has,15,batch size has 15,0.6369971632957458
translation,66,142,experimental-setup,experimental setup,fine - tune,model,experimental setup fine - tune model,0.6560331583023071
translation,66,153,experimental-setup,size,of,hidden states,size of hidden states,0.6102981567382812
translation,66,153,experimental-setup,hidden states,is,300,hidden states is 300,0.6092528104782104
translation,66,153,experimental-setup,word vectors,initialized with,glove embeddings,word vectors initialized with glove embeddings,0.7033036947250366
translation,66,153,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,66,153,experimental-setup,experimental setup,has,word vectors,experimental setup has word vectors,0.5082172751426697
translation,66,7,model,novel joint learning framework,of modeling,coreference resolution and query rewriting,novel joint learning framework of modeling coreference resolution and query rewriting,0.6170257329940796
translation,66,7,model,coreference resolution and query rewriting,for,"complex , multi-turn dialogue understanding","coreference resolution and query rewriting for complex , multi-turn dialogue understanding",0.5582180619239807
translation,66,7,model,model,propose,novel joint learning framework,model propose novel joint learning framework,0.6240191459655762
translation,66,8,model,joint learning model,first predicts,coreference links,joint learning model first predicts coreference links,0.6480512022972107
translation,66,8,model,joint learning model,generates,selfcontained rewritten user query,joint learning model generates selfcontained rewritten user query,0.5953945517539978
translation,66,8,model,coreference links,between,query and the dialogue context,coreference links between query and the dialogue context,0.6272396445274353
translation,66,31,model,novel joint learning framework,incorporates,benefits,novel joint learning framework incorporates benefits,0.6863330006599426
translation,66,31,model,benefits,of,reference resolution,benefits of reference resolution,0.5711773037910461
translation,66,31,model,reference resolution,into,query rewrite task,reference resolution into query rewrite task,0.5511267185211182
translation,66,31,model,model,propose,novel joint learning framework,model propose novel joint learning framework,0.6240191459655762
translation,66,35,model,joint learning model,adopting,"gpt - 2 ( radford et al. , 2019 ) architecture","joint learning model adopting gpt - 2 ( radford et al. , 2019 ) architecture",0.648383378982544
translation,66,35,model,"gpt - 2 ( radford et al. , 2019 ) architecture",learns,query rewrite,"gpt - 2 ( radford et al. , 2019 ) architecture learns query rewrite",0.6731631755828857
translation,66,35,model,"gpt - 2 ( radford et al. , 2019 ) architecture",learns,coreference resolution,"gpt - 2 ( radford et al. , 2019 ) architecture learns coreference resolution",0.6705372929573059
translation,66,35,model,model,design,joint learning model,model design joint learning model,0.5364589095115662
translation,66,36,model,ongoing dialogue,first predicts,coreference links,ongoing dialogue first predicts coreference links,0.658266544342041
translation,66,36,model,coreference links,between,latest user query and the dialogue context,coreference links between latest user query and the dialogue context,0.6149976849555969
translation,66,36,model,model,Given,ongoing dialogue,model Given ongoing dialogue,0.6766722202301025
translation,66,36,model,model,first predicts,coreference links,model first predicts coreference links,0.6897128224372864
translation,66,63,model,model,present,novel joint learning approach,model present novel joint learning approach,0.6264494061470032
translation,66,152,model,concatenation,of,dialogue context and the query,concatenation of dialogue context and the query,0.5727787613868713
translation,66,152,model,concatenation,fed as,input,concatenation fed as input,0.6513335704803467
translation,66,152,model,dialogue context and the query,fed as,input,dialogue context and the query fed as input,0.6531518697738647
translation,66,152,model,model,has,concatenation,model has concatenation,0.5566678643226624
translation,66,188,model,binary head,plays,essential role,binary head plays essential role,0.6925312876701355
translation,66,188,model,essential role,in,our model,essential role in our model,0.5336748957633972
translation,66,188,model,model,has,binary head,model has binary head,0.5867990255355835
translation,66,11,results,joint model,has,outperforms,joint model has outperforms,0.6411589980125427
translation,66,11,results,outperforms,has,state - of - the - art coreference resolution model ( + 2 % f1 ),outperforms has state - of - the - art coreference resolution model ( + 2 % f1 ),0.5727494359016418
translation,66,11,results,results,has,joint model,results has joint model,0.5378851294517517
translation,66,158,results,all lstm - based seq-to-seq models,on,all metrics,all lstm - based seq-to-seq models on all metrics,0.5002593994140625
translation,66,158,results,joint model,has,substantially outperforms,joint model has substantially outperforms,0.6126189827919006
translation,66,158,results,substantially outperforms,has,all lstm - based seq-to-seq models,substantially outperforms has all lstm - based seq-to-seq models,0.5747302770614624
translation,66,158,results,results,find that,joint model,results find that joint model,0.6351833343505859
translation,66,174,results,spanbert,obtains,better results,spanbert obtains better results,0.6708808541297913
translation,66,174,results,better results,than,bert,better results than bert,0.6100012063980103
translation,66,174,results,results,has,spanbert,results has spanbert,0.6318476796150208
translation,66,176,results,our joint learning model,achieves,competitive and even slightly better results,our joint learning model achieves competitive and even slightly better results,0.6409845352172852
translation,66,176,results,results,has,our joint learning model,results has our joint learning model,0.5602497458457947
translation,66,181,results,results,of,corefonly model,results of corefonly model,0.5943487286567688
translation,66,181,results,corefonly model,are,very close,corefonly model are very close,0.6005898714065552
translation,66,181,results,very close,to,joint model,very close to joint model,0.5939525365829468
translation,66,181,results,very close,of,joint model,very close of joint model,0.6168005466461182
translation,66,181,results,addition of coreference resolution,in,joint learning,addition of coreference resolution in joint learning,0.5289562940597534
translation,66,181,results,addition of coreference resolution,beneficial to,query,addition of coreference resolution beneficial to query,0.7197253108024597
translation,66,181,results,results,of,corefonly model,results of corefonly model,0.5943487286567688
translation,66,186,results,supervision,of,coreference modeling,supervision of coreference modeling,0.523137629032135
translation,66,186,results,coreference modeling,from,our joint learning model,coreference modeling from our joint learning model,0.5078532099723816
translation,66,186,results,model,produces,worse results,model produces worse results,0.6783768534660339
translation,66,186,results,worse results,compared to,complete model,worse results compared to complete model,0.6937931776046753
translation,66,186,results,supervision,has,model,supervision has model,0.5633701086044312
translation,66,186,results,coreference modeling,has,model,coreference modeling has model,0.5143081545829773
translation,66,186,results,our joint learning model,has,model,our joint learning model has model,0.5439168810844421
translation,66,186,results,results,removing,supervision,results removing supervision,0.642310380935669
translation,66,187,results,model 's ability,of generating,rewritten query,model 's ability of generating rewritten query,0.6804398894309998
translation,66,187,results,improves,including,ability,improves including ability,0.7464015483856201
translation,66,187,results,ability,to rewrite,anaphora,ability to rewrite anaphora,0.6334916949272156
translation,66,187,results,anaphora,with,antecedent,anaphora with antecedent,0.6220042109489441
translation,66,187,results,anaphora,by lever-aging,information,anaphora by lever-aging information,0.7075060606002808
translation,66,187,results,information,from,coreference resolution modeling,information from coreference resolution modeling,0.5131485462188721
translation,66,187,results,joint learning,has,model 's ability,joint learning has model 's ability,0.4991213381290436
translation,66,187,results,results,indicate,joint learning,results indicate joint learning,0.5239988565444946
translation,66,187,results,results,through,joint learning,results through joint learning,0.5713822841644287
translation,66,189,results,accuracy,of,binary classifier,accuracy of binary classifier,0.6193614602088928
translation,66,189,results,binary classifier,is,93.9 %,binary classifier is 93.9 %,0.5441507697105408
translation,66,189,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,66,196,results,dialogue,contains,coreferences,dialogue contains coreferences,0.6417478322982788
translation,66,196,results,dialogue,contains,coreferences,dialogue contains coreferences,0.6417478322982788
translation,66,196,results,coreferences,when,not present,coreferences when not present,0.6574810743331909
translation,66,196,results,coreferences,are,not present,coreferences are not present,0.6180477142333984
translation,66,196,results,joint learning model,more capable of rewriting,query,joint learning model more capable of rewriting query,0.6792739033699036
translation,66,196,results,joint learning model,more capable of rewriting,query,joint learning model more capable of rewriting query,0.6792739033699036
translation,66,196,results,joint learning model,when,coreferences,joint learning model when coreferences,0.6706095933914185
translation,66,196,results,joint learning model,perform,competitively,joint learning model perform competitively,0.5476946234703064
translation,66,196,results,query,by leveraging,coreference predictions,query by leveraging coreference predictions,0.7067343592643738
translation,66,196,results,coreferences,are,not present,coreferences are not present,0.6180477142333984
translation,66,196,results,coreferences,are,query,coreferences are query,0.6231579780578613
translation,66,196,results,rewriting,on account of,information omission,rewriting on account of information omission,0.7053664922714233
translation,66,196,results,joint model,perform,competitively,joint model perform competitively,0.6487202644348145
translation,66,196,results,competitively,with,qr -only model,competitively with qr -only model,0.6826616525650024
translation,66,196,results,dialogue,has,joint learning model,dialogue has joint learning model,0.5499729514122009
translation,66,196,results,coreferences,has,joint learning model,coreferences has joint learning model,0.551909327507019
translation,66,196,results,information omission,has,joint model,information omission has joint model,0.5633208751678467
translation,66,196,results,results,when,dialogue,results when dialogue,0.5504100322723389
translation,67,166,ablation-analysis,positive styles,reduce,level of toxicity,positive styles reduce level of toxicity,0.73044353723526
translation,67,166,ablation-analysis,level of toxicity,by,large margin,level of toxicity by large margin,0.5302570462226868
translation,67,166,ablation-analysis,large margin,for,both metrics ( classifier and blocklist ),large margin for both metrics ( classifier and blocklist ),0.5695528388023376
translation,67,166,ablation-analysis,ablation analysis,indicate,positive styles,ablation analysis indicate positive styles,0.6194413900375366
translation,67,212,experimental-setup,learning rate,between,5e - 6 and 3e - 5,learning rate between 5e - 6 and 3e - 5,0.6717797517776489
translation,67,212,experimental-setup,learning rate,using,100 warmup steps,learning rate using 100 warmup steps,0.6222474575042725
translation,67,212,experimental-setup,experimental setup,optimized using,"adam ( kingma and ba , 2014 )","experimental setup optimized using adam ( kingma and ba , 2014 )",0.6987733244895935
translation,67,214,experimental-setup,models,on,8 gpus,models on 8 gpus,0.4775305390357971
translation,67,214,experimental-setup,8 gpus,for,around 10 k train updates,8 gpus for around 10 k train updates,0.5613079071044922
translation,67,214,experimental-setup,around 10 k train updates,using,similar optimization setup,around 10 k train updates using similar optimization setup,0.6127628087997437
translation,67,214,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,67,221,experiments,io reddit,with,faster r-cnn features,io reddit with faster r-cnn features,0.6416196823120117
translation,67,221,experiments,io reddit,with,resnext wsl features,io reddit with resnext wsl features,0.6774765849113464
translation,67,221,experiments,io reddit,with,resnext wsl features,io reddit with resnext wsl features,0.6774765849113464
translation,67,221,experiments,average ppl,of,9.13,average ppl of 9.13,0.5945255160331726
translation,67,221,experiments,9.13,over,two datasets,9.13 over two datasets,0.6041886210441589
translation,67,7,model,state - of - the - art open- domain dialogue agents,with those from,state - of - the - art vision models,state - of - the - art open- domain dialogue agents with those from state - of - the - art vision models,0.5960291028022766
translation,67,7,model,model,investigate,combining components,model investigate combining components,0.6318601369857788
translation,67,17,model,extension of largescale conversational agents,to,image - based dia-logue,extension of largescale conversational agents to image - based dia-logue,0.5616551637649536
translation,67,17,model,model,explore,extension of largescale conversational agents,model explore extension of largescale conversational agents,0.6424725651741028
translation,67,21,results,image feature embeddings,to,input,image feature embeddings to input,0.5086997747421265
translation,67,21,results,image feature embeddings,leads to,better performance,image feature embeddings leads to better performance,0.6438473463058472
translation,67,21,results,input,of,our model 's encoder,input of our model 's encoder,0.6000712513923645
translation,67,21,results,better performance,concatenating,embeddings,better performance concatenating embeddings,0.6689097285270691
translation,67,21,results,embeddings,to,encoder 's output,embeddings to encoder 's output,0.5483206510543823
translation,67,21,results,spatially based image embeddings,performs,better,spatially based image embeddings performs better,0.637562096118927
translation,67,21,results,better,than,single -vector embeddings,better than single -vector embeddings,0.5620173215866089
translation,67,102,results,late fusion scheme,holding,all other variables,late fusion scheme holding all other variables,0.6854444146156311
translation,67,102,results,early fusion scheme,has,outperforms,early fusion scheme has outperforms,0.6170421242713928
translation,67,102,results,outperforms,has,late fusion scheme,outperforms has late fusion scheme,0.6031740307807922
translation,67,102,results,all other variables,has,constant,all other variables has constant,0.4436992406845093
translation,67,102,results,results,has,early fusion scheme,results has early fusion scheme,0.5774197578430176
translation,67,108,results,architecture choices,find that,our early fusion architecture,architecture choices find that our early fusion architecture,0.6159887313842773
translation,67,108,results,improves performance,on,image - chat,improves performance on image - chat,0.5456545948982239
translation,67,108,results,improves performance,with,faster r-cnn features,improves performance with faster r-cnn features,0.5843612551689148
translation,67,108,results,image - chat,across,all ablation regimes,image - chat across all ablation regimes,0.7498962879180908
translation,67,108,results,faster r-cnn features,yielding,best performance,faster r-cnn features yielding best performance,0.6518039703369141
translation,67,108,results,our early fusion architecture,has,improves performance,our early fusion architecture has improves performance,0.606695294380188
translation,67,108,results,results,In terms of,architecture choices,results In terms of architecture choices,0.6988382935523987
translation,67,108,results,results,find that,our early fusion architecture,results find that our early fusion architecture,0.6013340950012207
translation,67,114,results,dodeca model,performs,well,dodeca model performs well,0.6814936399459839
translation,67,114,results,well,across,board,well across board,0.7420985698699951
translation,67,114,results,"highest rouge -l , bleu -4 , and f1 scores",for,three text-only datasets,"highest rouge -l , bleu -4 , and f1 scores for three text-only datasets",0.5619790554046631
translation,67,114,results,results,note,dodeca model,results note dodeca model,0.6096106767654419
translation,67,118,results,mmb,performs,nearly the same,mmb performs nearly the same,0.6161330938339233
translation,67,118,results,nearly the same,on,all four text-only datasets,nearly the same on all four text-only datasets,0.5087816715240479
translation,67,118,results,predecessor,has,text-only blenderbot,predecessor has text-only blenderbot,0.5498566627502441
translation,67,118,results,predecessor,has,mmb,predecessor has mmb,0.6054362654685974
translation,67,118,results,text-only blenderbot,has,mmb,text-only blenderbot has mmb,0.5974948406219482
translation,67,119,results,performance,on,image - chat,performance on image - chat,0.5737122893333435
translation,67,119,results,image - chat,to,models,image - chat to models,0.5974732637405396
translation,67,119,results,models,trained on,multi-modal data,models trained on multi-modal data,0.756118655204773
translation,67,119,results,outperforms,in terms of,f1 score,outperforms in terms of f1 score,0.657401978969574
translation,67,119,results,dodeca,in terms of,f1 score,dodeca in terms of f1 score,0.7229104042053223
translation,67,119,results,performance,has,mmb,performance has mmb,0.591628909111023
translation,67,119,results,models,has,mmb,models has mmb,0.5880308747291565
translation,67,119,results,mmb,has,outperforms,mmb has outperforms,0.6539745330810547
translation,67,119,results,outperforms,has,dodeca,outperforms has dodeca,0.6612223982810974
translation,67,119,results,f1 score,has,13.1 vs. 12.9 ),f1 score has 13.1 vs. 12.9 ),0.5605732798576355
translation,67,119,results,outperforms,has,2 ammc,outperforms has 2 ammc,0.6382083892822266
translation,67,119,results,results,comparing,performance,results comparing performance,0.7179481983184814
translation,67,167,results,well,with,our previous experiments,well with our previous experiments,0.6827627420425415
translation,67,167,results,well,on,degendering,well on degendering,0.608841061592102
translation,67,167,results,well,on,degendering process,well on degendering process,0.5966809391975403
translation,67,167,results,our previous experiments,on,degendering,our previous experiments on degendering,0.6352491974830627
translation,67,167,results,reduced,across,all styles,reduced across all styles,0.7450677752494812
translation,67,167,results,all styles,after applying,degendering process,all styles after applying degendering process,0.7879434823989868
translation,67,167,results,results,align,well,results align well,0.6326678395271301
translation,67,219,results,spatially - based image features ( resnext wsl spatial,yields,better performance,spatially - based image features ( resnext wsl spatial yields better performance,0.7175241112709045
translation,67,219,results,better performance,than,single vector image representation ( resnext wsl ),better performance than single vector image representation ( resnext wsl ),0.5690778493881226
translation,67,222,results,faster r-cnn features,has,additionally outperforms,faster r-cnn features has additionally outperforms,0.612282395362854
translation,67,222,results,results,using,faster r-cnn features,results using faster r-cnn features,0.6349745392799377
translation,67,229,results,image fusion,using,our early fusion technique,image fusion using our early fusion technique,0.6795778870582581
translation,67,229,results,performance,on,image - chat,performance on image - chat,0.5737122893333435
translation,67,229,results,image - chat,across,all ablation regimes,image - chat across all ablation regimes,0.7498962879180908
translation,67,229,results,our early fusion technique,has,improves,our early fusion technique has improves,0.6029163599014282
translation,67,229,results,improves,has,performance,improves has performance,0.5770372748374939
translation,67,229,results,results,using,our early fusion technique,results using our early fusion technique,0.6312990784645081
translation,67,229,results,results,has,image fusion,results has image fusion,0.5664584040641785
translation,67,281,results,faster r-cnn image features,results in,best average performance,faster r-cnn image features results in best average performance,0.5814346075057983
translation,67,281,results,faster r-cnn image features,results in,best performance,faster r-cnn image features results in best performance,0.580238938331604
translation,67,281,results,best performance,on,image - chat,best performance on image - chat,0.564231812953949
translation,67,281,results,results,note,faster r-cnn image features,results note faster r-cnn image features,0.5317855477333069
translation,67,281,results,results,using,faster r-cnn image features,results using faster r-cnn image features,0.6387156844139099
translation,67,288,results,mmb style,has,significantly outperforms,mmb style has significantly outperforms,0.6165902018547058
translation,67,288,results,significantly outperforms,has,dodeca and often 2 ammc,significantly outperforms has dodeca and often 2 ammc,0.6155558824539185
translation,67,288,results,results,has,mmb style,results has mmb style,0.5287641882896423
translation,68,31,ablation-analysis,semantic matching relationships,between,context and the response,semantic matching relationships between context and the response,0.6734616756439209
translation,68,31,ablation-analysis,semantic matching relationships,implicitly modeled through,contrastive learning,semantic matching relationships implicitly modeled through contrastive learning,0.6908569931983948
translation,68,175,experimental-setup,"tensorflow ( abadi et al. , 2016 )",with,cuda 10.0 support,"tensorflow ( abadi et al. , 2016 ) with cuda 10.0 support",0.5447182655334473
translation,68,175,experimental-setup,experimental setup,implemented in,"tensorflow ( abadi et al. , 2016 )","experimental setup implemented in tensorflow ( abadi et al. , 2016 )",0.6347739696502686
translation,68,176,experimental-setup,all datasets,continue,pre-training bert,all datasets continue pre-training bert,0.6600298285484314
translation,68,176,experimental-setup,pre-training bert,for,approximately 0.5 epochs,pre-training bert for approximately 0.5 epochs,0.6084035038948059
translation,68,176,experimental-setup,approximately 0.5 epochs,to improve,domain adaption ability,approximately 0.5 epochs to improve domain adaption ability,0.6822485327720642
translation,68,176,experimental-setup,domain adaption ability,keeping,general domain information,domain adaption ability keeping general domain information,0.6551043391227722
translation,68,176,experimental-setup,general domain information,has,as much as possible,general domain information has as much as possible,0.5712283253669739
translation,68,176,experimental-setup,experimental setup,For,all datasets,experimental setup For all datasets,0.5118299126625061
translation,68,177,experimental-setup,continue pre-training stage,use,masking probability,continue pre-training stage use masking probability,0.7245937585830688
translation,68,177,experimental-setup,continue pre-training stage,use,learning rate,continue pre-training stage use learning rate,0.6861756443977356
translation,68,177,experimental-setup,continue pre-training stage,use,batch size,continue pre-training stage use batch size,0.6830622553825378
translation,68,177,experimental-setup,continue pre-training stage,use,maximum of 10 masked lm predictions per sequence,continue pre-training stage use maximum of 10 masked lm predictions per sequence,0.6289995908737183
translation,68,177,experimental-setup,masking probability,of,0.15,masking probability of 0.15,0.6072866320610046
translation,68,177,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,68,177,experimental-setup,batch size,of,50,batch size of 50,0.6921922564506531
translation,68,177,experimental-setup,experimental setup,During,continue pre-training stage,experimental setup During continue pre-training stage,0.7191964983940125
translation,68,178,experimental-setup,contrastive learning stage,freeze,bottom 6 layers,contrastive learning stage freeze bottom 6 layers,0.6536973118782043
translation,68,178,experimental-setup,bottom 6 layers,of,bert,bottom 6 layers of bert,0.6062372326850891
translation,68,178,experimental-setup,bottom 6 layers,to prevent,catastrophic forgetting,bottom 6 layers to prevent catastrophic forgetting,0.662575364112854
translation,68,178,experimental-setup,model,to be trained with,larger batch size,model to be trained with larger batch size,0.7082903385162354
translation,68,178,experimental-setup,experimental setup,During,contrastive learning stage,experimental setup During contrastive learning stage,0.6618497967720032
translation,68,180,experimental-setup,number of context turns,set to,"20 , 5e - 5 , and 3","number of context turns set to 20 , 5e - 5 , and 3",0.7218177914619446
translation,68,180,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,68,180,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,68,181,experimental-setup,maximum sequence length,set to,"100 , 50 , 50","maximum sequence length set to 100 , 50 , 50",0.7448933124542236
translation,68,181,experimental-setup,"100 , 50 , 50",for,jddc,"100 , 50 , 50 for jddc",0.6907979249954224
translation,68,181,experimental-setup,"100 , 50 , 50",for,mdc,"100 , 50 , 50 for mdc",0.6872356534004211
translation,68,181,experimental-setup,experimental setup,has,maximum sequence length,experimental setup has maximum sequence length,0.5124795436859131
translation,68,7,model,dialoguecse,has,dialogue - based contrastive learning approach,dialoguecse has dialogue - based contrastive learning approach,0.5959903001785278
translation,68,7,model,model,propose,dialoguecse,model propose dialoguecse,0.7155735492706299
translation,68,8,model,dialoguecse,introduces,novel matching - guided embedding ( mge ) mechanism,dialoguecse introduces novel matching - guided embedding ( mge ) mechanism,0.6378713250160217
translation,68,8,model,novel matching - guided embedding ( mge ) mechanism,generates,contextaware embedding,novel matching - guided embedding ( mge ) mechanism generates contextaware embedding,0.5892989039421082
translation,68,8,model,contextaware embedding,for,each candidate response embedding,contextaware embedding for each candidate response embedding,0.6061986684799194
translation,68,8,model,guidance,of,multi-turn context- response matching matrices,guidance of multi-turn context- response matching matrices,0.6092151999473572
translation,68,8,model,each candidate response embedding,has,i.e. the context- free embedding,each candidate response embedding has i.e. the context- free embedding,0.5732564330101013
translation,68,8,model,model,has,dialoguecse,model has dialoguecse,0.6617928147315979
translation,68,9,model,each context - aware embedding,with,corresponding context - free embedding,each context - aware embedding with corresponding context - free embedding,0.6073281168937683
translation,68,9,model,contrastive loss,across,all pairs,contrastive loss across all pairs,0.7221569418907166
translation,68,9,model,model,pairs,each context - aware embedding,model pairs each context - aware embedding,0.7046743631362915
translation,68,30,model,dialoguecse,has,dialogue - based contrastive learning,dialoguecse has dialogue - based contrastive learning,0.5850348472595215
translation,68,30,model,model,propose,dialoguecse,model propose dialoguecse,0.7155735492706299
translation,68,32,model,model,introduce,novel matching - guided embedding ( mge ) mechanism,model introduce novel matching - guided embedding ( mge ) mechanism,0.6507496237754822
translation,68,33,model,mge,performs,token - level dot-product operation,mge performs token - level dot-product operation,0.6051347255706787
translation,68,33,model,each utterance,in,context,each utterance in context,0.503588080406189
translation,68,33,model,token - level dot-product operation,across,all the utterance - response pairs,token - level dot-product operation across all the utterance - response pairs,0.6628936529159546
translation,68,33,model,token - level dot-product operation,to obtain,multi-turn matching matrices,token - level dot-product operation to obtain multi-turn matching matrices,0.6222779154777527
translation,68,33,model,pairs,has,each utterance,pairs has each utterance,0.6198593378067017
translation,68,33,model,model,has,mge,model has mge,0.6410251259803772
translation,68,173,model,mlp layer,on top of,sentence encoders,mlp layer on top of sentence encoders,0.6732886433601379
translation,68,188,model,dialoguecse,models,semantic relationships,dialoguecse models semantic relationships,0.7285322546958923
translation,68,188,model,dialoguecse,achieves,better performance,dialoguecse achieves better performance,0.6862093210220337
translation,68,188,model,semantic relationships,in,each utterance - response pair,semantic relationships in each utterance - response pair,0.5141566395759583
translation,68,188,model,semantic relationships,distills,important information,semantic relationships distills important information,0.6874422430992126
translation,68,188,model,important information,at,turn-level,important information at turn-level,0.5354433059692383
translation,68,188,model,turn-level,from,multi-turn dialogue context,turn-level from multi-turn dialogue context,0.5679857134819031
translation,68,188,model,model,has,dialoguecse,model has dialoguecse,0.6617928147315979
translation,68,44,results,significantly outperforms,in terms of,map and spearman 's correlation metrics,significantly outperforms in terms of map and spearman 's correlation metrics,0.7566222548484802
translation,68,44,results,baselines,on,three datasets,baselines on three datasets,0.5036138892173767
translation,68,44,results,dialoguecse,has,significantly outperforms,dialoguecse has significantly outperforms,0.6162236928939819
translation,68,44,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,68,185,results,even larger improvements,between,di-aloguecse and the domain-adaptive baselines,even larger improvements between di-aloguecse and the domain-adaptive baselines,0.6379479765892029
translation,68,185,results,di-aloguecse and the domain-adaptive baselines,including,bert ( adapt ) and its variants,di-aloguecse and the domain-adaptive baselines including bert ( adapt ) and its variants,0.62184739112854
translation,68,187,results,dialoguecse,eliminates,gap,dialoguecse eliminates gap,0.7124117016792297
translation,68,187,results,gap,between,training and evaluating,gap between training and evaluating,0.6740210056304932
translation,68,187,results,gap,gaining,significant improvements,gap gaining significant improvements,0.7360004186630249
translation,68,187,results,significant improvements,on,sr and d-sts tasks,significant improvements on sr and d-sts tasks,0.508557140827179
translation,68,187,results,contrastive learning,has,dialoguecse,contrastive learning has dialoguecse,0.6302605271339417
translation,68,187,results,results,by introducing,contrastive learning,results by introducing contrastive learning,0.6175974011421204
translation,68,189,results,performances,of,dialoguecse i 1 and dialoguecse i 2,performances of dialoguecse i 1 and dialoguecse i 2,0.6381416320800781
translation,68,189,results,dialoguecse i 1 and dialoguecse i 2,find that,weighted sum aggregation strategy,dialoguecse i 1 and dialoguecse i 2 find that weighted sum aggregation strategy,0.68707275390625
translation,68,189,results,weighted sum aggregation strategy,brings,significant deterioration,weighted sum aggregation strategy brings significant deterioration,0.5916403532028198
translation,68,189,results,significant deterioration,on,all metrics,significant deterioration on all metrics,0.5329799056053162
translation,68,189,results,results,comparing,performances,results comparing performances,0.6760835647583008
translation,68,193,results,bert ( adapt ),achieves,significantly better performance,bert ( adapt ) achieves significantly better performance,0.6983862519264221
translation,68,193,results,significantly better performance,than,original bert,significantly better performance than original bert,0.6311911940574646
translation,68,193,results,significantly better performance,especially on,jddc and ecd,significantly better performance especially on jddc and ecd,0.6620747447013855
translation,68,193,results,results,notice,bert ( adapt ),results notice bert ( adapt ),0.7277233600616455
translation,68,204,results,our model,benefited from,multi-turn dialogue context,our model benefited from multi-turn dialogue context,0.5523092150688171
translation,68,204,results,our model,exhibits,consistently better performance,our model exhibits consistently better performance,0.6969320178031921
translation,68,204,results,consistently better performance,than,baseline,consistently better performance than baseline,0.5692775249481201
translation,68,204,results,results,observe that,our model,results observe that our model,0.6364359259605408
translation,68,214,results,our model,achieve,even superior performance,our model achieve even superior performance,0.6559560298919678
translation,68,214,results,even superior performance,over,siamesebert,even superior performance over siamesebert,0.7197999358177185
translation,68,214,results,siamesebert,trained on,larger datasets,siamesebert trained on larger datasets,0.7041486501693726
translation,68,214,results,few dialogues,has,our model,few dialogues has our model,0.6080281138420105
translation,68,214,results,results,when using,few dialogues,results when using few dialogues,0.7008874416351318
translation,69,75,ablation-analysis,ablation analysis,design,three ablation models,ablation analysis design three ablation models,0.5777648091316223
translation,69,75,ablation-analysis,ablation analysis,design,mha,ablation analysis design mha,0.6260650157928467
translation,69,79,ablation-analysis,most of the token,can be directly extracted from,context,most of the token can be directly extracted from context,0.7695279717445374
translation,69,79,ablation-analysis,82.0 %,in,multiwoz2.0,82.0 % in multiwoz2.0,0.5688164234161377
translation,69,79,ablation-analysis,82.0 %,in,multiwoz2.1,82.0 % in multiwoz2.1,0.569788932800293
translation,69,79,ablation-analysis,84.2 %,in,multiwoz2.1,84.2 % in multiwoz2.1,0.5585145950317383
translation,69,79,ablation-analysis,83.7 %,in,crosswoz,83.7 % in crosswoz,0.5782241821289062
translation,69,79,ablation-analysis,ablation analysis,find that,most of the token,ablation analysis find that most of the token,0.656569242477417
translation,69,86,ablation-analysis,degradation,of,"- mha , - ext and - gen","degradation of - mha , - ext and - gen",0.6543793082237244
translation,69,97,ablation-analysis,performance,of,som,performance of som,0.6500336527824402
translation,69,97,ablation-analysis,performance,decreases,more sharply,performance decreases more sharply,0.7905755043029785
translation,69,97,ablation-analysis,som,decreases,more sharply,som decreases more sharply,0.8302679061889648
translation,69,97,ablation-analysis,more sharply,as,more instances,more sharply as more instances,0.5733950734138489
translation,69,97,ablation-analysis,more instances,set to be,oov,more instances set to be oov,0.7311884164810181
translation,69,74,baselines,geex,with,trippy,geex with trippy,0.6752046346664429
translation,69,74,baselines,d-streader,has,"gao et al. , 2019 b","d-streader has gao et al. , 2019 b",0.6066393256187439
translation,69,74,baselines,trippy,has,"heck et al. , 2020 )","trippy has heck et al. , 2020 )",0.5851168632507324
translation,69,71,hyperparameters,hidden size,set to,300,hidden size set to 300,0.7481361627578735
translation,69,71,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,69,72,hyperparameters,learning rate,initialized to,10 ?3,learning rate initialized to 10 ?3,0.673264741897583
translation,69,72,hyperparameters,decay rate,of,0.5,decay rate of 0.5,0.6363998651504517
translation,69,72,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,69,24,model,generation and extraction combined method,with,hierarchical ontology integration,generation and extraction combined method with hierarchical ontology integration,0.5971576571464539
translation,69,24,model,hierarchical ontology integration,named,geex,hierarchical ontology integration named geex,0.6757490038871765
translation,69,24,model,hierarchical ontology integration,for,dialogue state tracking,hierarchical ontology integration for dialogue state tracking,0.5743005871772766
translation,69,24,model,model,propose,generation and extraction combined method,model propose generation and extraction combined method,0.6319795846939087
translation,69,25,model,hierarchical semantics,of,ontology,hierarchical semantics of ontology,0.5315904021263123
translation,69,25,model,hierarchical semantics,to enhance,representation,hierarchical semantics to enhance representation,0.6831669807434082
translation,69,25,model,representation,of,slots,representation of slots,0.6544833779335022
translation,69,25,model,slots,in,multiple domains,slots in multiple domains,0.5908097624778748
translation,69,25,model,model,explore,hierarchical semantics,model explore hierarchical semantics,0.6148319840431213
translation,69,26,model,directed acyclic graph,to represent,ontology,directed acyclic graph to represent ontology,0.6892247200012207
translation,69,26,model,directed acyclic graph,enhance,slots interaction,directed acyclic graph enhance slots interaction,0.6662832498550415
translation,69,26,model,slots interaction,between,domains,slots interaction between domains,0.6648349761962891
translation,69,26,model,slots interaction,with,masked hierarchical attention,slots interaction with masked hierarchical attention,0.6469638347625732
translation,69,26,model,model,adopt,directed acyclic graph,model adopt directed acyclic graph,0.6293625831604004
translation,69,30,model,oov problem,leverage,generation and extraction,oov problem leverage generation and extraction,0.6917949914932251
translation,69,30,model,generation and extraction,by combining,two methods,generation and extraction by combining two methods,0.7339945435523987
translation,69,78,results,geex,achieves,higher score,geex achieves higher score,0.6910601854324341
translation,69,78,results,higher score,than,"generation decoding models ( i.e. , trade , som )","higher score than generation decoding models ( i.e. , trade , som )",0.5400466322898865
translation,69,78,results,results,has,geex,results has geex,0.5214683413505554
translation,69,87,results,- ext,outperforms,som,- ext outperforms som,0.7581189870834351
translation,69,87,results,- ext,outperforms,gen,- ext outperforms gen,0.7361473441123962
translation,69,87,results,som,in,generation decoding method,som in generation decoding method,0.47440725564956665
translation,69,87,results,gen,outperforms,dstreader,gen outperforms dstreader,0.760844886302948
translation,69,87,results,dstreader,in,extraction decoding method,dstreader in extraction decoding method,0.5074777603149414
translation,69,87,results,results,has,- ext,results has - ext,0.5689112544059753
translation,69,95,results,geex,performs,well,geex performs well,0.6927690505981445
translation,69,95,results,well,in,all oov rates,well in all oov rates,0.566155195236206
translation,69,95,results,sumbt,has,dstreader,sumbt has dstreader,0.6451376676559448
translation,69,95,results,sumbt,has,geex,sumbt has geex,0.6969369649887085
translation,69,95,results,som,has,geex,som has geex,0.65531325340271
translation,69,95,results,results,compared with,sumbt,results compared with sumbt,0.710235059261322
translation,70,134,experimental-setup,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,70,134,experimental-setup,"adam ( kingma and ba , 2015 )",with,linear decay,"adam ( kingma and ba , 2015 ) with linear decay",0.6092814803123474
translation,70,134,experimental-setup,optimizer,with,learning rate 1 ? 10 ?4,optimizer with learning rate 1 ? 10 ?4,0.6505854725837708
translation,70,134,experimental-setup,linear decay,as in,devlin et al . ( 2019 ),linear decay as in devlin et al . ( 2019 ),0.6273398995399475
translation,70,134,experimental-setup,experimental setup,use,"adam ( kingma and ba , 2015 )","experimental setup use adam ( kingma and ba , 2015 )",0.579606831073761
translation,70,136,experimental-setup,"cvdn ( thomason et al. , 2019 )",part of,r2r 's augmented data,"cvdn ( thomason et al. , 2019 ) part of r2r 's augmented data",0.6829467415809631
translation,70,136,experimental-setup,"cvdn ( thomason et al. , 2019 )",has,r2r,"cvdn ( thomason et al. , 2019 ) has r2r",0.5794322490692139
translation,70,136,experimental-setup,experimental setup,use,"cvdn ( thomason et al. , 2019 )","experimental setup use cvdn ( thomason et al. , 2019 )",0.5703935623168945
translation,70,136,experimental-setup,experimental setup,as,training data,experimental setup as training data,0.4799615144729614
translation,70,139,experimental-setup,experimental setup,use,resnet -152 feature,experimental setup use resnet -152 feature,0.594820499420166
translation,70,139,experimental-setup,experimental setup,use,resnet50 based clip feature,experimental setup use resnet50 based clip feature,0.5653361082077026
translation,70,140,experimental-setup,experiments,run using,nvidia titan xp / geforce gtx 1080 ti / geforce rtx 2080 ti gpus,experiments run using nvidia titan xp / geforce gtx 1080 ti / geforce rtx 2080 ti gpus,0.6930975914001465
translation,70,140,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,70,141,experimental-setup,py- torch,to build,all models,py- torch to build all models,0.7448705434799194
translation,70,142,experimental-setup,manual tuning,layers of,transformer model=,manual tuning layers of transformer model=,0.6563274264335632
translation,70,142,experimental-setup,manual tuning,for selecting,hyper-parameters,manual tuning for selecting hyper-parameters,0.740673303604126
translation,70,142,experimental-setup,transformer model=,has,"5 ( cross-modal ) / 3 ( language ) , 9/5 }","transformer model= has 5 ( cross-modal ) / 3 ( language ) , 9/5 }",0.585204541683197
translation,70,142,experimental-setup,experimental setup,use,manual tuning,experimental setup use manual tuning,0.6208105087280273
translation,70,6,experiments,navigation from dialogue history ( ndh ) task,based on,cooperative vision - and - dialogue navigation ( cvdn ) dataset,navigation from dialogue history ( ndh ) task based on cooperative vision - and - dialogue navigation ( cvdn ) dataset,0.6175862550735474
translation,70,6,experiments,navigation from dialogue history ( ndh ) task,present,stateof - the - art model,navigation from dialogue history ( ndh ) task present stateof - the - art model,0.6360006332397461
translation,70,6,experiments,stateof - the - art model,built upon,vision - language transformers,stateof - the - art model built upon vision - language transformers,0.6443095207214355
translation,70,135,experiments,l2 loss,for,visual view prediction,l2 loss for visual view prediction,0.5780776739120483
translation,70,135,experiments,cross-entropy loss,for,masked language model,cross-entropy loss for masked language model,0.5799517035484314
translation,70,135,experiments,cross-entropy loss,for,next view selection,cross-entropy loss for next view selection,0.562384843826294
translation,70,137,experiments,ndh task model,use,"adamw ( loshchilov and hutter , 2018 )","ndh task model use adamw ( loshchilov and hutter , 2018 )",0.588263988494873
translation,70,137,experiments,"adamw ( loshchilov and hutter , 2018 )",as,optimizer,"adamw ( loshchilov and hutter , 2018 ) as optimizer",0.47599339485168457
translation,70,137,experiments,optimizer,with,learning rate 1 ? 10 ?5,optimizer with learning rate 1 ? 10 ?5,0.6575257778167725
translation,70,146,experiments,eaml,has,environment -agnostic multitask learning,eaml has environment -agnostic multitask learning,0.5256584882736206
translation,70,97,results,ndh - full,gives,agent,ndh - full gives agent,0.6708328127861023
translation,70,97,results,ndh - full,encourages,agent,ndh - full encourages agent,0.7299860715866089
translation,70,97,results,agent,to understand,long instructions,agent to understand long instructions,0.6626471281051636
translation,70,97,results,full supervision,on,how to reach,full supervision on how to reach,0.5269644260406494
translation,70,97,results,agent,to understand,long instructions,agent to understand long instructions,0.6626471281051636
translation,70,97,results,agent,to understand,navigate,agent to understand navigate,0.7133442163467407
translation,70,97,results,navigate,based on,instructions,navigate based on instructions,0.6286006569862366
translation,70,97,results,ndh,has,ndh - full,ndh has ndh - full,0.6355194449424744
translation,70,97,results,agent,has,full supervision,agent has full supervision,0.5756112337112427
translation,70,97,results,how to reach,has,target,how to reach has target,0.5756513476371765
translation,70,97,results,results,compared with,ndh,results compared with ndh,0.6925593614578247
translation,70,97,results,results,compared with,ndh - full,results compared with ndh - full,0.6623393893241882
translation,70,145,results,all the stateof - the - art models,in,validation unseen environment,all the stateof - the - art models in validation unseen environment,0.4902770221233368
translation,70,145,results,ranks 1st,on,ndh task leaderboard ( ' sagent ' team ),ranks 1st on ndh task leaderboard ( ' sagent ' team ),0.581321120262146
translation,70,145,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,70,145,results,outperforms,has,all the stateof - the - art models,outperforms has all the stateof - the - art models,0.5632014274597168
translation,70,145,results,results,has,our model,results has our model,0.5871725678443909
translation,70,151,results,all the state - of - the - art models,on,primary evaluation metric - goal progress,all the state - of - the - art models on primary evaluation metric - goal progress,0.4810783267021179
translation,70,151,results,all the state - of - the - art models,ranks,1st,all the state - of - the - art models ranks 1st,0.7012438178062439
translation,70,151,results,primary evaluation metric - goal progress,by,large margin,primary evaluation metric - goal progress by large margin,0.5190756320953369
translation,70,151,results,1st,on,leaderboard ( ' sagent ' team ),1st on leaderboard ( ' sagent ' team ),0.5792157053947449
translation,70,151,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,70,151,results,outperforms,has,all the state - of - the - art models,outperforms has all the state - of - the - art models,0.5579462647438049
translation,70,151,results,results,has,our model,results has our model,0.5871725678443909
translation,70,160,results,gp score,of,global target model,gp score of global target model,0.5899631977081299
translation,70,160,results,global target model,much higher than,local target model,global target model much higher than local target model,0.6271837949752808
translation,70,160,results,local target model,has,5.51 vs. 3.82 ),local target model has 5.51 vs. 3.82 ),0.5406109094619751
translation,70,160,results,results,has,gp score,results has gp score,0.534735381603241
translation,70,170,results,results,has,ndh -full task,results has ndh -full task,0.5171016454696655
translation,70,173,results,full supervision,towards,target goal region ( full - dialogue ),full supervision towards target goal region ( full - dialogue ),0.6404725313186646
translation,70,173,results,other baselines,in,all metrics,other baselines in all metrics,0.4424518048763275
translation,70,173,results,full supervision,has,agent,full supervision has agent,0.5689579844474792
translation,70,173,results,target goal region ( full - dialogue ),has,agent,target goal region ( full - dialogue ) has agent,0.5672253966331482
translation,70,173,results,agent,has,outperforms,agent has outperforms,0.6695883870124817
translation,70,173,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,71,107,baselines,mem2seq,takes,dialog history and kb entities,mem2seq takes dialog history and kb entities,0.6294543147087097
translation,71,107,baselines,mem2seq,utilizes,pointer gate,mem2seq utilizes pointer gate,0.6309749484062195
translation,71,107,baselines,dialog history and kb entities,as,input,dialog history and kb entities as input,0.5292066335678101
translation,71,107,baselines,pointer gate,to control,generating,pointer gate to control generating,0.7047242522239685
translation,71,107,baselines,pointer gate,to control,copying an entity word,pointer gate to control copying an entity word,0.6528474688529968
translation,71,107,baselines,generating,has,vocabulary word,generating has vocabulary word,0.5921920537948608
translation,71,107,baselines,baselines,has,mem2seq,baselines has mem2seq,0.5307025909423828
translation,71,108,baselines,retriever module,to extract,most relevant knowledge items,retriever module to extract most relevant knowledge items,0.6881728172302246
translation,71,108,baselines,retriever module,filter,irrelevant information,retriever module filter irrelevant information,0.7650234699249268
translation,71,108,baselines,irrelevant information,for,response generation,irrelevant information for response generation,0.5980862975120544
translation,71,111,baselines,"net ( qin et al. , 2020 )",uses,dynamic fusion network,"net ( qin et al. , 2020 ) uses dynamic fusion network",0.5953037142753601
translation,71,111,baselines,dynamic fusion network,to dynamically exploit,correlation,dynamic fusion network to dynamically exploit correlation,0.7145524024963379
translation,71,111,baselines,correlation,for,fine- grained knowledge transfer,correlation for fine- grained knowledge transfer,0.5993492603302002
translation,71,115,hyperparameters,our model,using,adam optimizer,our model using adam optimizer,0.6453019976615906
translation,71,115,hyperparameters,our model,choose,learning rate,our model choose learning rate,0.6597100496292114
translation,71,115,hyperparameters,adam optimizer,choose,learning rate,adam optimizer choose learning rate,0.6583065390586853
translation,71,115,hyperparameters,learning rate,between,"[ 1e ?3 , 1e ?4 ]","learning rate between [ 1e ?3 , 1e ?4 ]",0.5833355188369751
translation,71,115,hyperparameters,our model,has,end-to - end,our model has end-to - end,0.5682708621025085
translation,71,115,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,71,117,hyperparameters,dropout ratio,selected from,"{ 0.1 , 0.15 , 0.2 , 0.25 , 0.3 }","dropout ratio selected from { 0.1 , 0.15 , 0.2 , 0.25 , 0.3 }",0.5906366109848022
translation,71,117,hyperparameters,batch size,from,"{ 8 , 16 , 32 }","batch size from { 8 , 16 , 32 }",0.5669276714324951
translation,71,117,hyperparameters,hyperparameters,has,dropout ratio,hyperparameters has dropout ratio,0.5014679431915283
translation,71,118,hyperparameters,hyper-parameters,such as,hidden size,hyper-parameters such as hidden size,0.6372836828231812
translation,71,118,hyperparameters,hyper-parameters,such as,dropout,hyper-parameters such as dropout,0.6316230893135071
translation,71,118,hyperparameters,hyper-parameters,such as,batch size,hyper-parameters such as batch size,0.6412779688835144
translation,71,118,hyperparameters,hyper-parameters,such as,embedding dimensionality,hyper-parameters such as embedding dimensionality,0.6198343634605408
translation,71,118,hyperparameters,hyper-parameters,tuned with,grid-search,hyper-parameters tuned with grid-search,0.8122509717941284
translation,71,118,hyperparameters,embedding dimensionality,tuned with,grid-search,embedding dimensionality tuned with grid-search,0.7573611736297607
translation,71,118,hyperparameters,grid-search,over,development set,grid-search over development set,0.6749589443206787
translation,71,118,hyperparameters,hyperparameters,has,hyper-parameters,hyperparameters has hyper-parameters,0.526509702205658
translation,71,6,model,novel intention mechanism,better model,deterministic entity knowledge,novel intention mechanism better model deterministic entity knowledge,0.7765839099884033
translation,71,6,model,model,propose,novel intention mechanism,model propose novel intention mechanism,0.6448947787284851
translation,71,20,model,novel intention reasoning network ( ir - net ),is,memory - augmented seq2seq model,novel intention reasoning network ( ir - net ) is memory - augmented seq2seq model,0.5608681440353394
translation,71,20,model,memory - augmented seq2seq model,equipped with,intention reasoning module,memory - augmented seq2seq model equipped with intention reasoning module,0.7052778005599976
translation,71,20,model,intention reasoning module,obtaining,intentionaware representation,intention reasoning module obtaining intentionaware representation,0.6388669610023499
translation,71,20,model,model,propose,novel intention reasoning network ( ir - net ),model propose novel intention reasoning network ( ir - net ),0.6460062861442566
translation,71,21,model,novel intention mechanism,directly incorporates,tail - token,novel intention mechanism directly incorporates tail - token,0.7565388679504395
translation,71,21,model,tail - token,of,knowledge triple,tail - token of knowledge triple,0.6016799211502075
translation,71,21,model,tail - token,by comparing,similarity,tail - token by comparing similarity,0.768415093421936
translation,71,21,model,similarity,between,query vector,similarity between query vector,0.5972585082054138
translation,71,21,model,similarity,between,triple 's head -token,similarity between triple 's head -token,0.6327307224273682
translation,71,21,model,triple 's head -token,to model,deterministic knowledge,triple 's head -token to model deterministic knowledge,0.6682903170585632
translation,71,21,model,model,propose,novel intention mechanism,model propose novel intention mechanism,0.6448947787284851
translation,71,22,model,intention reasoning module,consists of,token - level joint reasoning,intention reasoning module consists of token - level joint reasoning,0.6047309041023254
translation,71,22,model,intention reasoning module,consists of,multi-hop reasoning,intention reasoning module consists of multi-hop reasoning,0.6035423874855042
translation,71,22,model,intention reasoning module,responsible for capturing,specific target information,intention reasoning module responsible for capturing specific target information,0.6620739698410034
translation,71,22,model,specific target information,from,breadth and depth,specific target information from breadth and depth,0.5341098308563232
translation,71,22,model,specific target information,to generate,intention - aware representations,specific target information to generate intention - aware representations,0.6530803442001343
translation,71,22,model,specific target information,to improve,integrality and accuracy,specific target information to improve integrality and accuracy,0.6926703453063965
translation,71,22,model,intention - aware representations,to improve,integrality and accuracy,intention - aware representations to improve integrality and accuracy,0.6432220339775085
translation,71,22,model,model,proposing,intention reasoning module,model proposing intention reasoning module,0.7506402730941772
translation,71,128,model,integration,of,cross-domain knowledge,integration of cross-domain knowledge,0.5722945928573608
translation,71,128,model,cross-domain knowledge,in,multihop reasoning,cross-domain knowledge in multihop reasoning,0.5382779240608215
translation,71,128,model,cross-domain knowledge,makes,generated responses,cross-domain knowledge makes generated responses,0.6198334693908691
translation,71,128,model,hierarchical lstm decoder,in,ir - net,hierarchical lstm decoder in ir - net,0.5224937796592712
translation,71,128,model,hierarchical lstm decoder,can learn,more forms of expressions,hierarchical lstm decoder can learn more forms of expressions,0.6599705219268799
translation,71,128,model,generated responses,has,more diverse,generated responses has more diverse,0.5802673101425171
translation,71,24,results,current stateof - the - art models,in,automatic and human evaluation,current stateof - the - art models in automatic and human evaluation,0.4804048538208008
translation,71,24,results,consistently outperforms,has,current stateof - the - art models,consistently outperforms has current stateof - the - art models,0.5585004091262817
translation,71,123,results,our model ir - net,achieves,state - of - the - art performance,our model ir - net achieves state - of - the - art performance,0.6299118399620056
translation,71,123,results,state - of - the - art performance,on,two multi-domain datasets smd and multi-woz 2.1,state - of - the - art performance on two multi-domain datasets smd and multi-woz 2.1,0.46953490376472473
translation,71,123,results,results,observe that,our model ir - net,results observe that our model ir - net,0.5602338314056396
translation,71,124,results,ir - net,exhibits,highest bleu,ir - net exhibits highest bleu,0.617793619632721
translation,71,124,results,highest bleu,compared with,other baselines,highest bleu compared with other baselines,0.6382896900177002
translation,71,124,results,smd dataset,has,ir - net,smd dataset has ir - net,0.5748693943023682
translation,71,124,results,results,On,smd dataset,results On smd dataset,0.5586501359939575
translation,71,125,results,model,outperforms,df - net,model outperforms df - net,0.7573458552360535
translation,71,125,results,2.6 % and 0.5 %,on,macro - f1 and micro - f1,2.6 % and 0.5 % on macro - f1 and micro - f1,0.5852228403091431
translation,71,125,results,results,has,model,results has model,0.5339115858078003
translation,71,126,results,trend,for,similar performance improvement,trend for similar performance improvement,0.6367504000663757
translation,71,126,results,multi-woz 2.1,has,trend,multi-woz 2.1 has trend,0.5124974846839905
translation,71,126,results,results,On,multi-woz 2.1,results On multi-woz 2.1,0.543877363204956
translation,71,127,results,ir - net,achieves,consistently lower bleu and rouge scores,ir - net achieves consistently lower bleu and rouge scores,0.6643573641777039
translation,71,127,results,smd multi-woz 2.1 model,has,bleu,smd multi-woz 2.1 model has bleu,0.5515512824058533
translation,71,127,results,bleu,has,macro-f1 micro-f1,bleu has macro-f1 micro-f1,0.6069720983505249
translation,71,127,results,other baselines,has,ir - net,other baselines has ir - net,0.5662624835968018
translation,71,127,results,results,has,smd multi-woz 2.1 model,results has smd multi-woz 2.1 model,0.5066500902175903
translation,71,134,results,other three models,on,all metrics,other three models on all metrics,0.505668044090271
translation,71,134,results,outperforms,has,other three models,outperforms has other three models,0.569718062877655
