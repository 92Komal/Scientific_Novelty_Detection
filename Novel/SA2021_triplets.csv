topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,26,ablation-analysis,large dataset,by extending,"ms - marco ( nguyen et al. , 2016 ) and natural question ( kwiatkowski et al. , 2019 ) datasets","large dataset by extending ms - marco ( nguyen et al. , 2016 ) and natural question ( kwiatkowski et al. , 2019 ) datasets",0.7025793790817261
translation,0,26,ablation-analysis,"ms - marco ( nguyen et al. , 2016 ) and natural question ( kwiatkowski et al. , 2019 ) datasets",to account for,multimodal outputs,"ms - marco ( nguyen et al. , 2016 ) and natural question ( kwiatkowski et al. , 2019 ) datasets to account for multimodal outputs",0.6357976198196411
translation,0,26,ablation-analysis,ablation analysis,curate,large dataset,ablation analysis curate large dataset,0.7459626197814941
translation,0,117,baselines,e&m baseline,pretrain,text extraction,e&m baseline pretrain text extraction,0.7289901375770569
translation,0,117,baselines,text extraction,with,squad dataset,text extraction with squad dataset,0.5918264985084534
translation,0,122,experiments,mexbert,use,adam optimizer,mexbert use adam optimizer,0.6190097332000732
translation,0,122,experiments,mexbert,train it till,validation loss,mexbert train it till validation loss,0.6525813937187195
translation,0,122,experiments,adam optimizer,initialized with,learning rate,adam optimizer initialized with learning rate,0.7295401692390442
translation,0,122,experiments,adam optimizer,train it till,validation loss,adam optimizer train it till validation loss,0.635117769241333
translation,0,122,experiments,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,0,122,experiments,validation loss,has,saturates,validation loss has saturates,0.592502236366272
translation,0,121,hyperparameters,bert pretrained embeddings,for,textual stream,bert pretrained embeddings for textual stream,0.5805532336235046
translation,0,121,hyperparameters,textual stream,of,mexbert,textual stream of mexbert,0.5891968607902527
translation,0,121,hyperparameters,hyperparameters,use,bert pretrained embeddings,hyperparameters use bert pretrained embeddings,0.5638443827629089
translation,0,121,hyperparameters,hyperparameters,use,n ta = n t b = n v = 6,hyperparameters use n ta = n t b = n v = 6,0.626608669757843
translation,0,123,hyperparameters,model,trained over,4 v100 machines,model trained over 4 v100 machines,0.7730672359466553
translation,0,123,hyperparameters,4 v100 machines,using,batch size,4 v100 machines using batch size,0.6305992603302002
translation,0,123,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,0,123,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,0,123,hyperparameters,64,for,pretraining,64 for pretraining,0.6373363137245178
translation,0,123,hyperparameters,hyperparameters,trained over,4 v100 machines,hyperparameters trained over 4 v100 machines,0.7474038004875183
translation,0,123,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,0,124,hyperparameters,pretraining,use,adam optimizer,pretraining use adam optimizer,0.6046673059463501
translation,0,124,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,0,124,hyperparameters,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,0,124,hyperparameters,0.0001,for,2 epochs,0.0001 for 2 epochs,0.6230432391166687
translation,0,124,hyperparameters,2 epochs,over,3.2 million image - text pairs,2 epochs over 3.2 million image - text pairs,0.5656431913375854
translation,0,124,hyperparameters,ablations,during,pretraining stage,ablations during pretraining stage,0.7426111102104187
translation,0,124,hyperparameters,hyperparameters,For,pretraining,hyperparameters For pretraining,0.518390417098999
translation,0,125,hyperparameters,textual embeddings,with,vocabulary size,textual embeddings with vocabulary size,0.5549163818359375
translation,0,125,hyperparameters,textual embeddings,with,522,textual embeddings with 522,0.6252310276031494
translation,0,125,hyperparameters,textual embeddings,with,intermediate hidden embedding size,textual embeddings with intermediate hidden embedding size,0.5926222801208496
translation,0,125,hyperparameters,vocabulary size,of,30,vocabulary size of 30,0.6708650588989258
translation,0,125,hyperparameters,vocabulary size,of,522,vocabulary size of 522,0.6480286717414856
translation,0,125,hyperparameters,30,",",522,"30 , 522",0.6935141682624817
translation,0,125,hyperparameters,3072,for,textual and visual features,3072 for textual and visual features,0.6024065017700195
translation,0,125,hyperparameters,768 dimensional,has,textual embeddings,768 dimensional has textual embeddings,0.5640314817428589
translation,0,125,hyperparameters,intermediate hidden embedding size,has,3072,intermediate hidden embedding size has 3072,0.6249880790710449
translation,0,125,hyperparameters,hyperparameters,use,768 dimensional,hyperparameters use 768 dimensional,0.6473396420478821
translation,0,125,hyperparameters,hyperparameters,use,intermediate hidden embedding size,hyperparameters use intermediate hidden embedding size,0.6022611856460571
translation,0,126,hyperparameters,4096 dimensional vgg - 19 image features,into,2048 dimensions,4096 dimensional vgg - 19 image features into 2048 dimensions,0.543445348739624
translation,0,126,hyperparameters,4096 dimensional vgg - 19 image features,use it as,input,4096 dimensional vgg - 19 image features use it as input,0.5326526165008545
translation,0,126,hyperparameters,input,to,visual stream,input to visual stream,0.5820993781089783
translation,0,126,hyperparameters,hyperparameters,project,4096 dimensional vgg - 19 image features,hyperparameters project 4096 dimensional vgg - 19 image features,0.6807615756988525
translation,0,6,model,model,propose,task - mimoqa - multimodal input multimodal output question answering,model propose task - mimoqa - multimodal input multimodal output question answering,0.6652691960334778
translation,0,8,model,joint textual and visual attention,towards producing,multimodal output,joint textual and visual attention towards producing multimodal output,0.6602863073348999
translation,0,8,model,model,propose,multimodal question - answering framework,model propose multimodal question - answering framework,0.6477821469306946
translation,0,24,model,novel multimodal framework,for extracting,multimodal answers,novel multimodal framework for extracting multimodal answers,0.6745684742927551
translation,0,24,model,novel multimodal framework,compare it against,relevant strong baselines,novel multimodal framework compare it against relevant strong baselines,0.6454901695251465
translation,0,24,model,multimodal answers,to,given question,multimodal answers to given question,0.585201621055603
translation,0,24,model,mexbert,has,novel multimodal framework,mexbert has novel multimodal framework,0.5755336284637451
translation,0,24,model,model,propose,mexbert,model propose mexbert,0.6846976280212402
translation,0,25,model,method,includes,novel pretraining methodology,method includes novel pretraining methodology,0.6029712557792664
translation,0,25,model,method,uses,proxy supervision technique,method uses proxy supervision technique,0.6297076940536499
translation,0,25,model,proxy supervision technique,for,image selection,proxy supervision technique for image selection,0.6066771745681763
translation,0,25,model,model,uses,proxy supervision technique,model uses proxy supervision technique,0.6120868921279907
translation,0,51,model,textual stream,employs,two types of layers,textual stream employs two types of layers,0.5648068189620972
translation,0,51,model,textual stream,employs,additional cross-attention layers,textual stream employs additional cross-attention layers,0.5433626770973206
translation,0,51,model,two types of layers,has,regular self-attention layers,two types of layers has regular self-attention layers,0.5459692478179932
translation,0,141,results,image metrics,found,precision@1,image metrics found precision@1,0.5850197076797485
translation,0,141,results,precision@1,most strongly correlated with,human judgement,precision@1 most strongly correlated with human judgement,0.6545090675354004
translation,0,141,results,human judgement,has,0.5421 ),human judgement has 0.5421 ),0.5497012734413147
translation,0,141,results,results,For,image metrics,results For image metrics,0.555777370929718
translation,0,174,results,proxy supervision mechanism,while training,model,proxy supervision mechanism while training model,0.6462666988372803
translation,0,174,results,model,find,very significant improvement,model find very significant improvement,0.5782914757728577
translation,0,174,results,very significant improvement,specially in,precision @ 1 scores,very significant improvement specially in precision @ 1 scores,0.6190690398216248
translation,0,174,results,results,Applying,proxy supervision mechanism,results Applying proxy supervision mechanism,0.6732624769210815
translation,0,178,results,visual pretraining,yields,larger improvements,visual pretraining yields larger improvements,0.7002803087234497
translation,0,178,results,larger improvements,on,precision@1 metric,larger improvements on precision@1 metric,0.4852542281150818
translation,0,178,results,language pretraining,provides,marginal improvements,language pretraining provides marginal improvements,0.6113986968994141
translation,0,178,results,results,has,visual pretraining,results has visual pretraining,0.5294809937477112
translation,1,134,experiments,preprocessing,uses,spanbert - based coreference system,preprocessing uses spanbert - based coreference system,0.5661961436271667
translation,1,134,experiments,preprocessing,uses,"bert - based srl system ( shi and lin , 2019 )","preprocessing uses bert - based srl system ( shi and lin , 2019 )",0.5962516665458679
translation,1,132,hyperparameters,beam size b = 20,for,constrained alignment,beam size b = 20 for constrained alignment,0.5941236615180969
translation,1,132,hyperparameters,hyperparameters,set,beam size b = 20,hyperparameters set beam size b = 20,0.639312207698822
translation,1,133,hyperparameters,model,using,"adam ( kingma and ba , 2014 )","model using adam ( kingma and ba , 2014 )",0.6956616044044495
translation,1,133,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,1,133,hyperparameters,learning rate,set to,2e - 5,learning rate set to 2e - 5,0.7298941016197205
translation,1,133,hyperparameters,hyperparameters,use,bert - base-uncased,hyperparameters use bert - base-uncased,0.6372579336166382
translation,1,133,hyperparameters,hyperparameters,fine - tune,model,hyperparameters fine - tune model,0.7390584349632263
translation,1,136,hyperparameters,global model,initialize,weights,global model initialize weights,0.7288594841957092
translation,1,136,hyperparameters,global model,fine - tune,ssvm loss,global model fine - tune ssvm loss,0.6763852834701538
translation,1,136,hyperparameters,weights,using,locally trained model,weights using locally trained model,0.665924072265625
translation,1,136,hyperparameters,hyperparameters,For,global model,hyperparameters For global model,0.5727614760398865
translation,1,5,model,question answering,as,alignment problem,question answering as alignment problem,0.5077497959136963
translation,1,6,model,question and context,into,smaller units,question and context into smaller units,0.6174953579902649
translation,1,6,model,smaller units,based on,"off- the-shelf semantic representations ( here , semantic roles )","smaller units based on off- the-shelf semantic representations ( here , semantic roles )",0.5909615159034729
translation,1,6,model,question,to,subgraph,question to subgraph,0.6338566541671753
translation,1,6,model,subgraph,of,context,subgraph of context,0.5824438333511353
translation,1,6,model,context,to find,answer,context to find answer,0.7042497396469116
translation,1,6,model,model,decompose,question and context,model decompose question and context,0.7170715928077698
translation,1,7,model,our model,as,structured svm,our model as structured svm,0.5099431276321411
translation,1,7,model,structured svm,with,alignment scores,structured svm with alignment scores,0.6502763032913208
translation,1,7,model,alignment scores,computed via,bert,alignment scores computed via bert,0.724682092666626
translation,1,7,model,end-to - end,using,beam search,end-to - end using beam search,0.6884840130805969
translation,1,7,model,beam search,for,approximate inference,beam search for approximate inference,0.5720199346542358
translation,1,7,model,model,formulate,our model,model formulate our model,0.6870517134666443
translation,1,7,model,model,as,structured svm,model as structured svm,0.5584609508514404
translation,1,253,model,our model,around,explicit alignment scoring,our model around explicit alignment scoring,0.7043860554695129
translation,1,253,model,our model,show that,our approach,our model show that our approach,0.5228199362754822
translation,1,253,model,our approach,generalize,better,our approach generalize better,0.7277902364730835
translation,1,253,model,better,to,other domains,better to other domains,0.5413065552711487
translation,1,253,model,model,structuring,our model,model structuring our model,0.7255035638809204
translation,1,141,results,not as good,as,bert qa,not as good as bert qa,0.6540934443473816
translation,1,141,results,bert qa,on,normal squad,bert qa on normal squad,0.6194449067115784
translation,1,141,results,outperforms it,in,challenging settings,outperforms it in challenging settings,0.5899881720542908
translation,1,141,results,results,has,our model,results has our model,0.5871725678443909
translation,1,145,results,performance,in,adversarial settings,performance in adversarial settings,0.543838381767273
translation,1,145,results,global training and inference,has,improve,global training and inference has improve,0.5893315076828003
translation,1,145,results,improve,has,performance,improve has performance,0.5578044652938843
translation,1,145,results,results,has,global training and inference,results has global training and inference,0.5114513635635376
translation,1,153,results,strict wh answer extraction,gives,strong performance,strict wh answer extraction gives strong performance,0.6127561330795288
translation,1,153,results,strong performance,From,ablation,strong performance From ablation,0.6051855087280273
translation,1,153,results,strong performance,observe,our   strictest   system,strong performance observe our   strictest   system,0.5852829217910767
translation,1,153,results,our   strictest   system,that extracts,answer,our   strictest   system that extracts answer,0.8026028871536255
translation,1,153,results,answer,using,whaligned node,answer using whaligned node,0.6981950402259827
translation,1,153,results,whaligned node,is,worse,whaligned node is worse,0.6464276313781738
translation,1,153,results,whaligned node,only,worse,whaligned node only worse,0.7473528981208801
translation,1,153,results,worse,by,3 - 4 points of f1,worse by 3 - 4 points of f1,0.632164716720581
translation,1,153,results,3 - 4 points of f1,on,most datasets,3 - 4 points of f1 on most datasets,0.5215761661529541
translation,1,153,results,answer,has,only,answer has only,0.5855820178985596
translation,1,153,results,results,Using,strict wh answer extraction,results Using strict wh answer extraction,0.6293092370033264
translation,1,153,results,results,observe,our   strictest   system,results observe our   strictest   system,0.6145346164703369
translation,1,157,results,results,subsets of,universal triggers dataset,results subsets of universal triggers dataset,0.6739934086799622
translation,1,158,results,every trigger,results in,bigger performance drop,every trigger results in bigger performance drop,0.6039908528327942
translation,1,158,results,bigger performance drop,on,bert qa,bigger performance drop on bert qa,0.6229203939437866
translation,1,158,results,bert qa,than,our model,bert qa than our model,0.6237041354179382
translation,1,158,results,results,see that,every trigger,results see that every trigger,0.680896520614624
translation,1,162,results,adversarial attacks,on,span-based question answering models,adversarial attacks on span-based question answering models,0.5353458523750305
translation,1,162,results,span-based question answering models,may not fool,our model,span-based question answering models may not fool our model,0.723638653755188
translation,1,164,results,performance,of,our model,performance of our model,0.5847885608673096
translation,1,164,results,performance,on,squad - 1.1 data,performance on squad - 1.1 data,0.5373183488845825
translation,1,164,results,our model,on,squad - 1.1 data,our model on squad - 1.1 data,0.5547560453414917
translation,1,164,results,our model,achieve,best overall performance,our model achieve best overall performance,0.6275098323822021
translation,1,164,results,squad - 1.1 data,is,relatively lower,squad - 1.1 data is relatively lower,0.5886185765266418
translation,1,164,results,relatively lower,compared to,those methods,relatively lower compared to those methods,0.714041531085968
translation,1,164,results,results,note that,performance,results note that performance,0.6311826109886169
translation,1,207,results,coverage,same as,entity constraint,coverage same as entity constraint,0.5457351207733154
translation,1,207,results,performance,under,alignment score constraint,performance under alignment score constraint,0.6381195187568665
translation,1,207,results,performance,is,even better,performance is even better,0.6041994094848633
translation,1,207,results,coverage,has,performance,coverage has performance,0.600222110748291
translation,1,207,results,entity constraint,has,performance,entity constraint has performance,0.5694220066070557
translation,1,207,results,results,when,coverage,results when coverage,0.6433955430984497
translation,1,265,results,results,on,squad addonesent,results on squad addonesent,0.5526737570762634
translation,1,266,results,bert qa,on,squad addonesent,bert qa on squad addonesent,0.6285651326179504
translation,1,266,results,results,compared to,bert qa,results compared to bert qa,0.6899746656417847
translation,1,302,results,hard entity constraint,achieve,71.4 f1 score,hard entity constraint achieve 71.4 f1 score,0.5790452361106873
translation,1,302,results,71.4 f1 score,is,8.6 improvement,71.4 f1 score is 8.6 improvement,0.5711986422538757
translation,1,302,results,8.6 improvement,over,un-,8.6 improvement over un-,0.6775999069213867
translation,1,302,results,constrained model,at,cost,constrained model at cost,0.5553392171859741
translation,2,216,ablation-analysis,outperforms,has,all other baselines,outperforms has all other baselines,0.5747320652008057
translation,2,216,ablation-analysis,ablation analysis,ablation of,rp,ablation analysis ablation of rp,0.8089476227760315
translation,2,234,ablation-analysis,ablation of acsa,brings,performance degradation,ablation of acsa brings performance degradation,0.5886204838752747
translation,2,234,ablation-analysis,performance degradation,of,rp,performance degradation of rp,0.6500968933105469
translation,2,234,ablation-analysis,rp,on,both metrics,rp on both metrics,0.5805268883705139
translation,2,234,ablation-analysis,ablation analysis,has,ablation of acsa,ablation analysis has ablation of acsa,0.5147429704666138
translation,2,198,baselines,non-bert based models,include,"textcnn ( kim , 2014 )","non-bert based models include textcnn ( kim , 2014 )",0.5646549463272095
translation,2,198,baselines,non-bert based models,include,"bilstm + attn ( zhou et al. , 2016 )","non-bert based models include bilstm + attn ( zhou et al. , 2016 )",0.49882829189300537
translation,2,198,baselines,non-bert based models,include,atae - lstm,non-bert based models include atae - lstm,0.5481953024864197
translation,2,198,baselines,non-bert based models,include,"cap-snet ( sabour et al. , 2017 )","non-bert based models include cap-snet ( sabour et al. , 2017 )",0.5632950067520142
translation,2,198,baselines,bert - based models,include,"vanilla bert ( devlin et al. , 2018 )","bert - based models include vanilla bert ( devlin et al. , 2018 )",0.558539628982544
translation,2,198,baselines,bert - based models,include,"qa-bert ( sun et al. , 2019 )","bert - based models include qa-bert ( sun et al. , 2019 )",0.5764646530151367
translation,2,198,baselines,bert - based models,include,"capsnet-bert ( jiang et al. , 2019 )","bert - based models include capsnet-bert ( jiang et al. , 2019 )",0.5487188100814819
translation,2,198,baselines,baselines,has,non-bert based models,baselines has non-bert based models,0.5750448703765869
translation,2,199,experimental-setup,non-bert - based models,initialize,inputs,non-bert - based models initialize inputs,0.7529618740081787
translation,2,199,experimental-setup,inputs,with,pre-trained embeddings,inputs with pre-trained embeddings,0.601285994052887
translation,2,199,experimental-setup,experimental setup,In terms of,non-bert - based models,experimental setup In terms of non-bert - based models,0.6965632438659668
translation,2,202,experimental-setup,bert - based models,adopt,12 - layer google bert base,bert - based models adopt 12 - layer google bert base,0.6915709972381592
translation,2,202,experimental-setup,12 - layer google bert base,to encode,inputs,12 - layer google bert base to encode inputs,0.7357175350189209
translation,2,202,experimental-setup,experimental setup,In terms of,bert - based models,experimental setup In terms of bert - based models,0.7106228470802307
translation,2,203,experimental-setup,batch sizes,set as,32 and 16,batch sizes set as 32 and 16,0.6837053894996643
translation,2,203,experimental-setup,32 and 16,for,non-bert - based models,32 and 16 for non-bert - based models,0.648293673992157
translation,2,203,experimental-setup,32 and 16,for,bert - based models,32 and 16 for bert - based models,0.6629627346992493
translation,2,203,experimental-setup,experimental setup,has,batch sizes,experimental setup has batch sizes,0.5236980319023132
translation,2,205,experimental-setup,maximum sequence length,set as,512,maximum sequence length set as 512,0.6801012754440308
translation,2,205,experimental-setup,experimental setup,has,maximum sequence length,experimental setup has maximum sequence length,0.5124795436859131
translation,2,206,experimental-setup,number of epochs,set as,3,number of epochs set as 3,0.6542441248893738
translation,2,206,experimental-setup,experimental setup,has,number of epochs,experimental setup has number of epochs,0.5322787761688232
translation,2,207,experimental-setup,learning rates,set as,0.001 and 0.00005,learning rates set as 0.001 and 0.00005,0.6366111040115356
translation,2,207,experimental-setup,0.001 and 0.00005,for,non-bert - based models,0.001 and 0.00005 for non-bert - based models,0.6214225888252258
translation,2,207,experimental-setup,0.001 and 0.00005,for,bert - based models,0.001 and 0.00005 for bert - based models,0.635610818862915
translation,2,207,experimental-setup,experimental setup,has,learning rates,experimental setup has learning rates,0.5043967366218567
translation,2,208,experimental-setup,experimental setup,trained on,single nvidia tesla 32g v100 volta gpu,experimental setup trained on single nvidia tesla 32g v100 volta gpu,0.7237432599067688
translation,2,200,experiments,chinese asap,utilize,jieba,chinese asap utilize jieba,0.6105450987815857
translation,2,200,experiments,chinese asap,adopt,tencent chinese word embeddings,chinese asap adopt tencent chinese word embeddings,0.5971593856811523
translation,2,200,experiments,jieba,to segment,chinese texts,jieba to segment chinese texts,0.6291021704673767
translation,2,200,experiments,jieba,composed of,8,jieba composed of 8,0.6803070306777954
translation,2,200,experiments,tencent chinese word embeddings,composed of,8,tencent chinese word embeddings composed of 8,0.5934950709342957
translation,2,200,experiments,tencent chinese word embeddings,composed of,"000 , 000 words","tencent chinese word embeddings composed of 000 , 000 words",0.6243324875831604
translation,2,200,experiments,),composed of,8,) composed of 8,0.6727218627929688
translation,2,200,experiments,),composed of,"000 , 000 words",") composed of 000 , 000 words",0.7669529318809509
translation,2,200,experiments,8,",","000 , 000 words","8 , 000 , 000 words",0.6687828898429871
translation,2,200,experiments,tencent chinese word embeddings,has,),tencent chinese word embeddings has ),0.5683133602142334
translation,2,201,experiments,english restaurant,adopt,300 - dimensional word embeddings,english restaurant adopt 300 - dimensional word embeddings,0.6011168956756592
translation,2,201,experiments,300 - dimensional word embeddings,pre-trained by,glove,300 - dimensional word embeddings pre-trained by glove,0.7620936632156372
translation,2,204,experiments,"adam optimizer ( kingma and ba , 2014 )",employed with,? 1 = 0.9 and ? 2 = 0.999,"adam optimizer ( kingma and ba , 2014 ) employed with ? 1 = 0.9 and ? 2 = 0.999",0.652041494846344
translation,2,40,model,model,presents,large-scale chinese restaurant review dataset,model presents large-scale chinese restaurant review dataset,0.5018475651741028
translation,2,47,model,joint model,employs,fine-to- coarse semantic capability,joint model employs fine-to- coarse semantic capability,0.5775598287582397
translation,2,50,model,large-scale chinese review dataset,towards,aspect category sentiment analysis,large-scale chinese review dataset towards aspect category sentiment analysis,0.5904213786125183
translation,2,50,model,large-scale chinese review dataset,towards,rating prediction,large-scale chinese review dataset towards rating prediction,0.5187992453575134
translation,2,50,model,rating prediction,named as,asap,rating prediction named as asap,0.6601923108100891
translation,2,50,model,asap,as many as,"46 , 730 real-world restaurant reviews","asap as many as 46 , 730 real-world restaurant reviews",0.6123000383377075
translation,2,50,model,"46 , 730 real-world restaurant reviews",annotated from,18 pre-defined aspect categories,"46 , 730 real-world restaurant reviews annotated from 18 pre-defined aspect categories",0.6959911584854126
translation,2,50,model,model,present,large-scale chinese review dataset,model present large-scale chinese review dataset,0.5166491270065308
translation,2,166,model,joint learning model,to address,acsa and rp,joint learning model to address acsa and rp,0.6310635805130005
translation,2,166,model,acsa and rp,in,multitask learning manner,acsa and rp in multitask learning manner,0.5474001169204712
translation,2,166,model,model,propose,joint learning model,model propose joint learning model,0.6386464834213257
translation,2,213,model,user review,treat,pre-defined aspect categories,user review treat pre-defined aspect categories,0.5872492790222168
translation,2,213,model,capsnet - bert,treat,pre-defined aspect categories,capsnet - bert treat pre-defined aspect categories,0.5973053574562073
translation,2,213,model,our joint model,combines them together with,multi-task learning framework,our joint model combines them together with multi-task learning framework,0.7099190950393677
translation,2,213,model,user review,has,vanilla - bert,user review has vanilla - bert,0.6376578211784363
translation,2,213,model,pre-defined aspect categories,has,independently,pre-defined aspect categories has independently,0.59365314245224
translation,2,213,model,model,Given,user review,model Given user review,0.7020931243896484
translation,2,51,results,https,:,//github.com,https : //github.com,0.6116153597831726
translation,2,51,results,https,:,/meituan-dianping,https : /meituan-dianping,0.6474690437316895
translation,2,51,results,https,explore,performance,https explore performance,0.7131853699684143
translation,2,51,results,/meituan-dianping,/,asap,/meituan-dianping / asap,0.759727954864502
translation,2,51,results,/meituan-dianping,/,asap,/meituan-dianping / asap,0.759727954864502
translation,2,51,results,performance,of,widely used models,performance of widely used models,0.5619772672653198
translation,2,51,results,widely used models,for,acsa and rp,widely used models for acsa and rp,0.5665002465248108
translation,2,51,results,acsa and rp,on,asap,acsa and rp on asap,0.6296358704566956
translation,2,51,results,//github.com,has,/meituan-dianping,//github.com has /meituan-dianping,0.504658043384552
translation,2,51,results,results,explore,performance,results explore performance,0.6754196882247925
translation,2,212,results,two variants,of,joint model,two variants of joint model,0.6022828221321106
translation,2,212,results,joint model,perform,better,joint model perform better,0.6372862458229065
translation,2,212,results,better,than,vanilla - bert,better than vanilla - bert,0.7007288932800293
translation,2,212,results,better,than,qa - bert,better than qa - bert,0.6761513352394104
translation,2,212,results,better,than,capsnet - bert,better than capsnet - bert,0.6675556302070618
translation,2,212,results,results,has,two variants,results has two variants,0.5471349358558655
translation,2,215,results,joint model,is,more efficient,joint model is more efficient,0.5753860473632812
translation,2,215,results,more efficient,than,other competitors,more efficient than other competitors,0.622593104839325
translation,2,215,results,more efficient,especially when,number of aspect categories,more efficient especially when number of aspect categories,0.5968297719955444
translation,2,215,results,other competitors,especially when,number of aspect categories,other competitors especially when number of aspect categories,0.5863860845565796
translation,2,215,results,number of aspect categories,is,large,number of aspect categories is large,0.560001015663147
translation,2,215,results,results,has,joint model,results has joint model,0.5378851294517517
translation,2,217,results,rp,to,acsa,rp to acsa,0.6883108019828796
translation,2,217,results,rp,brings,marginal improvement,rp brings marginal improvement,0.6214178800582886
translation,2,217,results,results,introduction of,rp,results introduction of rp,0.635371208190918
translation,2,232,results,our joint model,combines,acsa and rp,our joint model combines acsa and rp,0.747307300567627
translation,2,232,results,acsa and rp,has,outperforms,acsa and rp has outperforms,0.6383302211761475
translation,2,232,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,2,232,results,other models,has,considerably,other models has considerably,0.5606749057769775
translation,2,232,results,results,has,our joint model,results has our joint model,0.5684905052185059
translation,2,233,results,joint model,built upon,bert,joint model built upon bert,0.7043010592460632
translation,2,233,results,results,has,performance improvement,results has performance improvement,0.5912312269210815
translation,2,235,results,fine- grained aspect category sentiment prediction,of,review,fine- grained aspect category sentiment prediction of review,0.5102981925010681
translation,2,235,results,fine- grained aspect category sentiment prediction,helps,model,fine- grained aspect category sentiment prediction helps model,0.518795907497406
translation,2,235,results,model,predict,overall rating,model predict overall rating,0.7143746018409729
translation,2,235,results,overall rating,has,more accurately,overall rating has more accurately,0.5306667685508728
translation,2,235,results,results,conclude that,fine- grained aspect category sentiment prediction,results conclude that fine- grained aspect category sentiment prediction,0.594230055809021
translation,3,208,ablation-analysis,gmt,with,our mt,gmt with our mt,0.7488729357719421
translation,3,208,ablation-analysis,gmt,results in,large performance drop,gmt results in large performance drop,0.5313822627067566
translation,3,208,ablation-analysis,large performance drop,in,bengali,large performance drop in bengali,0.5534752011299133
translation,3,208,ablation-analysis,large performance drop,in,telugu,large performance drop in telugu,0.5961171388626099
translation,3,208,ablation-analysis,bengali,has,6.6 vs. 19.0 f1 points,bengali has 6.6 vs. 19.0 f1 points,0.5804768204689026
translation,3,208,ablation-analysis,telugu,has,1.7 vs. 13.6 ),telugu has 1.7 vs. 13.6 ),0.5808572769165039
translation,3,208,ablation-analysis,ablation analysis,Replacing,gmt,ablation analysis Replacing gmt,0.6634983420372009
translation,3,209,ablation-analysis,gs,with,bm25 retrieval,gs with bm25 retrieval,0.715161919593811
translation,3,209,ablation-analysis,gs,causes,large performance drop,gs causes large performance drop,0.6508272886276245
translation,3,209,ablation-analysis,bm25 retrieval,in,target languages,bm25 retrieval in target languages,0.4965894818305969
translation,3,209,ablation-analysis,large performance drop,in,all languages,large performance drop in all languages,0.5119159817695618
translation,3,209,ablation-analysis,ablation analysis,replacing,gs,ablation analysis replacing gs,0.6828680634498596
translation,3,220,ablation-analysis,single language wikipedia ablations,in,xor - full,single language wikipedia ablations in xor - full,0.5314888954162598
translation,3,220,ablation-analysis,ablation analysis,has,single language wikipedia ablations,ablation analysis has single language wikipedia ablations,0.5559208393096924
translation,3,152,baselines,f1,over,annotated answer 's token set,f1 over annotated answer 's token set,0.6600648760795593
translation,3,174,baselines,bm25 retrieval baseline,use,elasticsearch,bm25 retrieval baseline use elasticsearch,0.5294970870018005
translation,3,174,baselines,elasticsearch,to store and search,documents,elasticsearch to store and search documents,0.7776958346366882
translation,3,174,baselines,documents,using,bm25 similarities,documents using bm25 similarities,0.6471664309501648
translation,3,7,experiments,large-scale dataset,built on,40 k information - seeking questions,large-scale dataset built on 40 k information - seeking questions,0.635479748249054
translation,3,7,experiments,40 k information - seeking questions,across,7 diverse non-english languages,40 k information - seeking questions across 7 diverse non-english languages,0.6439688801765442
translation,3,7,experiments,7 diverse non-english languages,TYDI QA could not find,same - language answers,7 diverse non-english languages TYDI QA could not find same - language answers,0.7003247141838074
translation,3,31,experiments,dataset,of,40 k annotated questions and answers,dataset of 40 k annotated questions and answers,0.5501439571380615
translation,3,31,experiments,xor - tydi qa ),of,40 k annotated questions and answers,xor - tydi qa ) of 40 k annotated questions and answers,0.5662245750427246
translation,3,31,experiments,40 k annotated questions and answers,across,7 typologically diverse languages,40 k annotated questions and answers across 7 typologically diverse languages,0.6427035927772522
translation,3,132,experiments,xor - retrieve,has,cross-lingual paragraph retrieval task,xor - retrieve has cross-lingual paragraph retrieval task,0.5340436100959778
translation,3,141,experiments,query translation,train,transformer machine translation ( mt ) models,query translation train transformer machine translation ( mt ) models,0.6661931276321411
translation,3,141,experiments,transformer machine translation ( mt ) models,on,publicly available corpora,transformer machine translation ( mt ) models on publicly available corpora,0.47477084398269653
translation,3,141,experiments,publicly available corpora,for,easy replication,publicly available corpora for easy replication,0.5725511312484741
translation,3,156,experiments,xor - full,has,round trip task,xor - full has round trip task,0.6144675612449646
translation,3,171,experiments,three xor qa tasks,present,challenges,three xor qa tasks present challenges,0.6412525773048401
translation,3,153,model,machine reading model,to find,minimal span,machine reading model to find minimal span,0.5756939649581909
translation,3,153,model,given paragraphs,selected from,previous xor - retrieve step,given paragraphs selected from previous xor - retrieve step,0.6049924492835999
translation,3,153,model,answers,has,question,answers has question,0.640134871006012
translation,3,153,model,answers,has,given paragraphs,answers has given paragraphs,0.599530041217804
translation,3,44,results,best baseline,average of,18.7 f1 points,best baseline average of 18.7 f1 points,0.695664644241333
translation,3,44,results,18.7 f1 points,on,xor - full,18.7 f1 points on xor - full,0.5372368693351746
translation,3,44,results,results,has,best baseline,results has best baseline,0.5728188753128052
translation,3,184,results,best r@5kt macro-,averaged over,7 languages,best r@5kt macro- averaged over 7 languages,0.7631471753120422
translation,3,184,results,7 languages,comes from,running dpr,7 languages comes from running dpr,0.592346727848053
translation,3,184,results,running dpr,on,human translations,running dpr on human translations,0.5200145840644836
translation,3,184,results,human translations,has,72.1,human translations has 72.1,0.5627578496932983
translation,3,184,results,results,has,best r@5kt macro-,results has best r@5kt macro-,0.6256871819496155
translation,3,188,results,translate baselines,has,outperform,translate baselines has outperform,0.6309138536453247
translation,3,188,results,outperform,has,multilingual approach,outperform has multilingual approach,0.6094804406166077
translation,3,188,results,results,has,translate baselines,results has translate baselines,0.5764281153678894
translation,3,189,results,bm25,has,substantially underperforms,bm25 has substantially underperforms,0.6415526270866394
translation,3,189,results,substantially underperforms,has,other two models,substantially underperforms has other two models,0.549176037311554
translation,3,189,results,results,has,bm25,results has bm25,0.5764147043228149
translation,3,194,results,average macro f1 score,queries translated by,human translators,average macro f1 score queries translated by human translators,0.7954944968223572
translation,3,194,results,substantially higher,than,mt - based models,substantially higher than mt - based models,0.6084629893302917
translation,3,194,results,32.9 and 20.5 f1 points,for,gmt and our mt,32.9 and 20.5 f1 points for gmt and our mt,0.6333621740341187
translation,3,194,results,results,has,average macro f1 score,results has average macro f1 score,0.5676881074905396
translation,3,196,results,multilingual approach,has,consistently underperforms,multilingual approach has consistently underperforms,0.6086735725402832
translation,3,196,results,consistently underperforms,has,translation - based methods,consistently underperforms has translation - based methods,0.5983108878135681
translation,3,196,results,results,has,multilingual approach,results has multilingual approach,0.5611485838890076
translation,3,198,results,translation - based approach,with,our mt system,translation - based approach with our mt system,0.6294516324996948
translation,3,198,results,multilingual baseline,has,significantly outperforms,multilingual baseline has significantly outperforms,0.541965126991272
translation,3,198,results,significantly outperforms,has,translation - based approach,significantly outperforms has translation - based approach,0.6017888784408569
translation,3,198,results,our mt system,has,14.4 vs. 3.6 f1 points,our mt system has 14.4 vs. 3.6 f1 points,0.5662646889686584
translation,3,198,results,results,has,multilingual baseline,results has multilingual baseline,0.583256185054779
translation,3,202,results,best baseline,achieves,39.5,best baseline achieves 39.5,0.6420812606811523
translation,3,202,results,39.5,in,arabic,39.5 in arabic,0.5827392339706421
translation,3,202,results,39.5,compared to,23.5 f1 points,39.5 compared to 23.5 f1 points,0.6666178107261658
translation,3,202,results,23.5 f1 points,in,japanese,23.5 f1 points in japanese,0.5638972520828247
translation,3,202,results,results,has,best baseline,results has best baseline,0.5728188753128052
translation,3,204,results,results,on,xor - full task,results on xor - full task,0.5533570051193237
translation,3,205,results,first pipeline,uses,gmt,first pipeline uses gmt,0.6169643402099609
translation,3,205,results,first pipeline,uses,google search ( gs ),first pipeline uses google search ( gs ),0.6032732725143433
translation,3,205,results,first pipeline,uses,dpr,first pipeline uses dpr,0.5947534441947937
translation,3,205,results,dpr,yields,best average performance,dpr yields best average performance,0.7129696011543274
translation,3,205,results,results,has,first pipeline,results has first pipeline,0.563590943813324
translation,3,210,results,underperforms,has,translation - based counterpart,underperforms has translation - based counterpart,0.5768493413925171
translation,3,210,results,translation - based counterpart,has,15.7 vs. 18.7 f1 points,translation - based counterpart has 15.7 vs. 18.7 f1 points,0.5643424987792969
translation,3,215,results,gmt,has,significantly outperforms,gmt has significantly outperforms,0.6171446442604065
translation,3,215,results,significantly outperforms,has,other two baselines,significantly outperforms has other two baselines,0.57750004529953
translation,3,215,results,results,has,gmt,results has gmt,0.5507026314735413
translation,3,216,results,high bleu scores,lead to,better qa performance,high bleu scores lead to better qa performance,0.635521650314331
translation,3,222,results,w eng only,in,all languages,w eng only in all languages,0.5829769372940063
translation,3,222,results,best system,applies,gmt and dpr,best system applies gmt and dpr,0.6028389930725098
translation,3,222,results,best pipeline,that uses,"both w i , eng","best pipeline that uses both w i , eng",0.7026563286781311
translation,3,222,results,"both w i , eng",in,all languages,"both w i , eng in all languages",0.606873095035553
translation,3,222,results,all languages,except for,finnish and japanese,all languages except for finnish and japanese,0.6433210968971252
translation,3,222,results,w eng only,has,best system,w eng only has best system,0.6141223907470703
translation,3,222,results,w eng only,has,underperforms,w eng only has underperforms,0.6602233648300171
translation,3,222,results,best system,has,underperforms,best system has underperforms,0.5954533219337463
translation,3,222,results,underperforms,has,best pipeline,underperforms has best pipeline,0.5694183707237244
translation,3,222,results,results,In,w eng only,results In w eng only,0.6055901646614075
translation,3,223,results,underperforms,has,"best w i , eng pipeline","underperforms has best w i , eng pipeline",0.6254803538322449
translation,4,195,ablation-analysis,ablation analysis,nuance of,language,ablation analysis nuance of language,0.6833549737930298
translation,4,213,ablation-analysis,performance,on,two datasets,performance on two datasets,0.5100376009941101
translation,4,213,ablation-analysis,decreases significantly,with,model ablation,decreases significantly with model ablation,0.6706048846244812
translation,4,213,ablation-analysis,model ablation,of,sentiment knowledge sharing,model ablation of sentiment knowledge sharing,0.5569376945495605
translation,4,213,ablation-analysis,model ablation,of,category embedding,model ablation of category embedding,0.5800955295562744
translation,4,191,baselines,sks,is,proposed model,sks is proposed model,0.6161704063415527
translation,4,191,baselines,sks,detects,hate speech,sks detects hate speech,0.7517507672309875
translation,4,191,baselines,hate speech,based on,sentiment knowledge sharing,hate speech based on sentiment knowledge sharing,0.6502949595451355
translation,4,191,baselines,baselines,has,sks,baselines has sks,0.6035316586494446
translation,4,233,experiments,effectiveness of multitask learning,in,hate speech detection tasks,effectiveness of multitask learning in hate speech detection tasks,0.5256631970405579
translation,4,165,hyperparameters,all word vectors,initialized by,glove common crawl embeddings ( 840b token ),all word vectors initialized by glove common crawl embeddings ( 840b token ),0.7180039286613464
translation,4,165,hyperparameters,dimension,is,300,dimension is 300,0.659263551235199
translation,4,165,hyperparameters,input layer,has,all word vectors,input layer has all word vectors,0.533136785030365
translation,4,165,hyperparameters,hyperparameters,for,input layer,hyperparameters for input layer,0.5201629996299744
translation,4,166,hyperparameters,category embeddings,are,initialized randomly,category embeddings are initialized randomly,0.5220717191696167
translation,4,166,hyperparameters,dimension,is,100,dimension is 100,0.6710469722747803
translation,4,166,hyperparameters,hyperparameters,has,category embeddings,hyperparameters has category embeddings,0.49540990591049194
translation,4,166,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,4,168,hyperparameters,one layer,with,400 neurons,one layer with 400 neurons,0.5976401567459106
translation,4,168,hyperparameters,one layer,with,200 neurons,one layer with 200 neurons,0.6081809997558594
translation,4,168,hyperparameters,one layer,with,200 neurons,one layer with 200 neurons,0.6081809997558594
translation,4,168,hyperparameters,two layers,with,200 neurons,two layers with 200 neurons,0.5916296243667603
translation,4,168,hyperparameters,first feed- forward network,has,one layer,first feed- forward network has one layer,0.5774263143539429
translation,4,168,hyperparameters,second,has,two layers,second has two layers,0.602923572063446
translation,4,168,hyperparameters,hyperparameters,has,first feed- forward network,hyperparameters has first feed- forward network,0.5634109377861023
translation,4,169,hyperparameters,dropout,used after,each layer,dropout used after each layer,0.6776195168495178
translation,4,169,hyperparameters,rate,is,0.1,rate is 0.1,0.5869660973548889
translation,4,169,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,4,169,hyperparameters,hyperparameters,has,rate,hyperparameters has rate,0.5241252779960632
translation,4,170,hyperparameters,optimizer,is,rmsprop,optimizer is rmsprop,0.5385867357254028
translation,4,170,hyperparameters,learning rate,is,0.001,learning rate is 0.001,0.5655806064605713
translation,4,170,hyperparameters,hyperparameters,has,optimizer,hyperparameters has optimizer,0.5107399225234985
translation,4,170,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,4,171,hyperparameters,mini-batch,of,512 instances,mini-batch of 512 instances,0.5758602023124695
translation,4,171,hyperparameters,hyperparameters,trained by,mini-batch,hyperparameters trained by mini-batch,0.6985909938812256
translation,4,172,hyperparameters,overfitting,use,learning rate decay,overfitting use learning rate decay,0.608546257019043
translation,4,172,hyperparameters,overfitting,use,early stop,overfitting use early stop,0.6291104555130005
translation,4,172,hyperparameters,early stop,in,training process,early stop in training process,0.5649832487106323
translation,4,172,hyperparameters,hyperparameters,To prevent,overfitting,hyperparameters To prevent overfitting,0.5773724317550659
translation,4,8,model,hate speech detection framework,based on,sentiment knowledge sharing,hate speech detection framework based on sentiment knowledge sharing,0.6078788638114929
translation,4,8,model,model,propose,hate speech detection framework,model propose hate speech detection framework,0.6502180099487305
translation,4,9,model,sentiment features,from,external resources,sentiment features from external resources,0.4842859208583832
translation,4,9,model,features,from,different feature extraction units,features from different feature extraction units,0.5610299110412598
translation,4,9,model,different feature extraction units,to detect,hate speech,different feature extraction units to detect hate speech,0.6960586905479431
translation,4,9,model,model,While extracting,affective features,model While extracting affective features,0.7267194986343384
translation,4,31,model,hate speech detection framework,based on,sentiment knowledge sharing ( sks ),hate speech detection framework based on sentiment knowledge sharing ( sks ),0.6701952815055847
translation,4,31,model,model,propose,hate speech detection framework,model propose hate speech detection framework,0.6502180099487305
translation,4,35,model,gated attention mechanism,to fuse,features,gated attention mechanism to fuse features,0.724432647228241
translation,4,35,model,model,use,multiple feature extraction units,model use multiple feature extraction units,0.6475134491920471
translation,4,167,model,sentiment knowledge sharing layer,has,multi-head attention,sentiment knowledge sharing layer has multi-head attention,0.4995589554309845
translation,4,167,model,multi-head attention,has,4 heads,multi-head attention has 4 heads,0.5950478315353394
translation,4,167,model,model,For,sentiment knowledge sharing layer,model For sentiment knowledge sharing layer,0.5625248551368713
translation,4,183,model,model,has,two - layer bigru,model has two - layer bigru,0.5290061235427856
translation,4,234,model,multiple feature extraction units,to share,multi-task parameters,multiple feature extraction units to share multi-task parameters,0.6174383759498596
translation,4,234,model,gated attention,to fuse,features,gated attention to fuse features,0.7046794295310974
translation,4,234,model,features,for,hate speech detection,features for hate speech detection,0.6108865141868591
translation,4,234,model,model,to,multiple feature extraction units,model to multiple feature extraction units,0.5323301553726196
translation,4,234,model,model,use,multiple feature extraction units,model use multiple feature extraction units,0.6475134491920471
translation,4,192,results,overall performance comparison,of,sks,overall performance comparison of sks,0.6069039106369019
translation,4,192,results,performance,of,model,performance of model,0.6080846190452576
translation,4,192,results,performance,is,quite different,performance is quite different,0.5799128413200378
translation,4,192,results,quite different,on,two datasets,quite different on two datasets,0.5196197628974915
translation,4,192,results,results,has,overall performance comparison,results has overall performance comparison,0.5560598373413086
translation,4,193,results,dv dataset,for,se dataset,dv dataset for se dataset,0.614374041557312
translation,4,193,results,f1 value,is,90 %,f1 value is 90 %,0.5826061367988586
translation,4,193,results,f1 value,about,90 %,f1 value about 90 %,0.6356253027915955
translation,4,193,results,dv dataset,has,f1 value,dv dataset has f1 value,0.5569947361946106
translation,4,193,results,dv dataset,has,f1 value,dv dataset has f1 value,0.5569947361946106
translation,4,193,results,se dataset,has,f1 value,se dataset has f1 value,0.5182346105575562
translation,4,193,results,results,For,dv dataset,results For dv dataset,0.6183969378471375
translation,4,193,results,results,for,se dataset,results for se dataset,0.6462938189506531
translation,4,196,results,performance,of,svm,performance of svm,0.6313489675521851
translation,4,196,results,performance,is,much worse,performance is much worse,0.5778314471244812
translation,4,196,results,svm,based on,features,svm based on features,0.669562041759491
translation,4,196,results,svm,is,much worse,svm is much worse,0.580880880355835
translation,4,196,results,much worse,than,neural network,much worse than neural network,0.6360896229743958
translation,4,196,results,results,has,performance,results has performance,0.5972660779953003
translation,4,197,results,performance,is,unacceptable,performance is unacceptable,0.640865683555603
translation,4,197,results,se dataset,has,performance,se dataset has performance,0.581134557723999
translation,4,197,results,results,on,se dataset,results on se dataset,0.596757709980011
translation,4,199,results,performance,of,hybrid neural network,performance of hybrid neural network,0.6014615297317505
translation,4,199,results,better,than,simple recurrent neural network ( rnn ),better than simple recurrent neural network ( rnn ),0.5989169478416443
translation,4,199,results,results,has,performance,results has performance,0.5972660779953003
translation,4,200,results,traditional rnns,such as,lstm,traditional rnns such as lstm,0.5893682241439819
translation,4,200,results,traditional rnns,whether,cnn,traditional rnns whether cnn,0.6526474356651306
translation,4,200,results,traditional rnns,whether,bigru,traditional rnns whether bigru,0.7151052355766296
translation,4,200,results,traditional rnns,has,performance,traditional rnns has performance,0.5568603277206421
translation,4,200,results,performance,has,small improvement,performance has small improvement,0.6047921180725098
translation,4,200,results,results,Compared with,traditional rnns,results Compared with traditional rnns,0.6246380805969238
translation,4,202,results,traditional rnns,such as,lstm and gru,traditional rnns such as lstm and gru,0.6224291920661926
translation,4,202,results,traditional rnns,have,almost the same performance,traditional rnns have almost the same performance,0.5411317944526672
translation,4,202,results,lstm and gru,have,almost the same performance,lstm and gru have almost the same performance,0.5585094094276428
translation,4,202,results,results,has,traditional rnns,results has traditional rnns,0.5282254219055176
translation,4,203,results,bert,achieves,better performance,bert achieves better performance,0.710902988910675
translation,4,203,results,better performance,on,dv dataset,better performance on dv dataset,0.5350620150566101
translation,4,203,results,results,has,bert,results has bert,0.43097156286239624
translation,4,204,results,bert and gpt,achieve,worse performance,bert and gpt achieve worse performance,0.6675046682357788
translation,4,204,results,worse performance,on,se dataset,worse performance on se dataset,0.5449500679969788
translation,4,204,results,results,has,bert and gpt,results has bert and gpt,0.538033664226532
translation,4,214,results,performance,of,model,performance of model,0.6080846190452576
translation,4,214,results,performance,is,better,performance is better,0.6231186985969543
translation,4,214,results,model,is,better,model is better,0.634016752243042
translation,4,214,results,better,than,existing hybrid neural networks,better than existing hybrid neural networks,0.6295562982559204
translation,4,214,results,results,has,performance,results has performance,0.5972660779953003
translation,4,219,results,outperforms,proves,effectiveness,outperforms proves effectiveness,0.7968528270721436
translation,4,219,results,other models,proves,effectiveness,other models proves effectiveness,0.6386727690696716
translation,4,219,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,4,221,results,performance,of,model,performance of model,0.6080846190452576
translation,4,221,results,model,is,further improved,model is further improved,0.5937908291816711
translation,4,221,results,further improved,on,both datasets,further improved on both datasets,0.5549041628837585
translation,4,221,results,further improved,when,gated attention,further improved when gated attention,0.6684145927429199
translation,4,221,results,both datasets,when,gated attention,both datasets when gated attention,0.5969426035881042
translation,4,228,results,performance,of,model,performance of model,0.6080846190452576
translation,4,228,results,performance,is,poor,performance is poor,0.6001739501953125
translation,4,228,results,model,is,poor,model is poor,0.6221761703491211
translation,4,228,results,poor,when,ratio of the two types of data,poor when ratio of the two types of data,0.6838377118110657
translation,4,228,results,ratio of the two types of data,is,1:2,ratio of the two types of data is 1:2,0.6024134755134583
translation,4,229,results,ratio,of,sentiment data,ratio of sentiment data,0.6250576972961426
translation,4,229,results,performance,of,model,performance of model,0.6080846190452576
translation,4,229,results,performance,is,improved,performance is improved,0.6214577555656433
translation,4,229,results,model,is,improved,model is improved,0.6447973847389221
translation,4,229,results,sentiment data,has,increases,sentiment data has increases,0.6265279650688171
translation,4,229,results,increases,has,performance,increases has performance,0.5952932834625244
translation,5,186,ablation-analysis,evidence prediction,as,multi-task scaffolding objective,evidence prediction as multi-task scaffolding objective,0.49450090527534485
translation,5,186,ablation-analysis,multi-task scaffolding objective,improving,results,multi-task scaffolding objective improving results,0.6518255472183228
translation,5,186,ablation-analysis,results,by,? = + 0.8 points,results by ? = + 0.8 points,0.6040971875190735
translation,5,186,ablation-analysis,ablation analysis,using,evidence prediction,ablation analysis using evidence prediction,0.6911725997924805
translation,5,175,experimental-setup,models,using,adam optimizer,models using adam optimizer,0.6263576149940491
translation,5,175,experimental-setup,models,using,triangular learning rate scheduler,models using triangular learning rate scheduler,0.6878322958946228
translation,5,175,experimental-setup,triangular learning rate scheduler,with,10 % warmup,triangular learning rate scheduler with 10 % warmup,0.6937991976737976
translation,5,175,experimental-setup,triangular learning rate scheduler,has,"howard and ruder , 2018 )","triangular learning rate scheduler has howard and ruder , 2018 )",0.5614042282104492
translation,5,175,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,5,178,experimental-setup,final configuration,was,10 epochs,final configuration was 10 epochs,0.5913902521133423
translation,5,178,experimental-setup,final configuration,was,peak learning rate,final configuration was peak learning rate,0.5703520774841309
translation,5,178,experimental-setup,final configuration,was,batch size,final configuration was batch size,0.6364415287971497
translation,5,178,experimental-setup,peak learning rate,of,5e ?5,peak learning rate of 5e ?5,0.6604199409484863
translation,5,178,experimental-setup,batch size,of,2,batch size of 2,0.6764397025108337
translation,5,178,experimental-setup,experimental setup,has,final configuration,experimental setup has final configuration,0.533889651298523
translation,5,179,experimental-setup,full text,use,gradient checkpointing,full text use gradient checkpointing,0.630591869354248
translation,5,179,experimental-setup,gradient checkpointing,to reduce,memory consumption,gradient checkpointing to reduce memory consumption,0.637428879737854
translation,5,179,experimental-setup,experimental setup,When handling,full text,experimental setup When handling full text,0.6230906248092651
translation,5,180,experimental-setup,our experiments,on,single rtx 8000 gpu,our experiments on single rtx 8000 gpu,0.5269976854324341
translation,5,180,experimental-setup,experimental setup,run,our experiments,experimental setup run our experiments,0.7337764501571655
translation,5,177,experiments,"{ 1 , 3 , 5 } epochs",with,learning rates,"{ 1 , 3 , 5 } epochs with learning rates",0.616419792175293
translation,5,177,experiments,smaller batch sizes,work better than,larger ones,smaller batch sizes work better than larger ones,0.7516373991966248
translation,5,177,experiments,learning rates,has,"{ 1e ?5 , 3e ?5 , 5e ?5 , 9e ?5 }","learning rates has { 1e ?5 , 3e ?5 , 5e ?5 , 9e ?5 }",0.5571175217628479
translation,5,163,hyperparameters,led,using,both loss functions,led using both loss functions,0.6647923588752747
translation,5,163,hyperparameters,both loss functions,in,multi-task training setup,both loss functions in multi-task training setup,0.460664838552475
translation,5,163,hyperparameters,both loss functions,has,teacher - forced text generation,both loss functions has teacher - forced text generation,0.5763869881629944
translation,5,163,hyperparameters,hyperparameters,train,led,hyperparameters train led,0.684116005897522
translation,5,162,model,longformer 's global attention,over,tokens,longformer 's global attention over tokens,0.7045546174049377
translation,5,162,model,longformer 's global attention,to facilitate,direct information flow,longformer 's global attention to facilitate direct information flow,0.6623421311378479
translation,5,162,model,direct information flow,across,paragraphs,direct information flow across paragraphs,0.6802852749824524
translation,5,162,model,model,add,longformer 's global attention,model add longformer 's global attention,0.592632532119751
translation,5,46,results,humans,by,27 f 1 points,humans by 27 f 1 points,0.592602014541626
translation,5,46,results,humans,by,32 f 1 points,humans by 32 f 1 points,0.5887001156806946
translation,5,46,results,32 f 1 points,selecting,paragraphs,32 f 1 points selecting paragraphs,0.6816309690475464
translation,5,112,results,annotators,agreed on,type of the evidence ( text vs. figure ),annotators agreed on type of the evidence ( text vs. figure ),0.7101610898971558
translation,5,112,results,type of the evidence ( text vs. figure ),in,84.0 %,type of the evidence ( text vs. figure ) in 84.0 %,0.4565252363681793
translation,5,112,results,84.0 %,of,cases,84.0 % of cases,0.5668161511421204
translation,5,112,results,results,find that,annotators,results find that annotators,0.5359574556350708
translation,5,151,results,led 's support,for,input sequence length,led 's support for input sequence length,0.6066712737083435
translation,5,151,results,led 's support,encode,99 %,led 's support encode 99 %,0.714387059211731
translation,5,151,results,input sequence length,of,16 k tokens,input sequence length of 16 k tokens,0.5785008072853088
translation,5,151,results,99 %,of,paper full texts,99 % of paper full texts,0.5421769618988037
translation,5,151,results,99 %,without,truncation,99 % without truncation,0.7586565613746643
translation,5,151,results,paper full texts,in,qasper dataset,paper full texts in qasper dataset,0.46335679292678833
translation,5,151,results,results,With,led 's support,results With led 's support,0.5971161127090454
translation,5,183,results,more context,has,performance,more context has performance,0.5834740996360779
translation,5,183,results,performance,has,improves,performance has improves,0.6178221106529236
translation,5,184,results,significant overall performance improvement ( ? = + 9.5 ),over,best heuristic,significant overall performance improvement ( ? = + 9.5 ) over best heuristic,0.6349616646766663
translation,5,184,results,results,encoding,entire context,results encoding entire context,0.8248587250709534
translation,5,188,results,evidence selection performance,of,led -large and led - base models,evidence selection performance of led -large and led - base models,0.584601879119873
translation,5,188,results,led -large and led - base models,compared with,simpler baselines,led -large and led - base models compared with simpler baselines,0.6706991195678711
translation,5,188,results,results,illustrates,evidence selection performance,results illustrates evidence selection performance,0.6323285102844238
translation,6,124,ablation-analysis,mined negatives ( dtr +hn ),yields,additional improvement,mined negatives ( dtr +hn ) yields additional improvement,0.7210497260093689
translation,6,124,ablation-analysis,additional improvement,of,more than 5 points,additional improvement of more than 5 points,0.5181341767311096
translation,6,124,ablation-analysis,ablation analysis,addition of,mined negatives ( dtr +hn ),ablation analysis addition of mined negatives ( dtr +hn ),0.6980575919151306
translation,6,116,experimental-setup,"bm25 ( robertson and zaragoza , 2009 ) implementation",of,gensim,"bm25 ( robertson and zaragoza , 2009 ) implementation of gensim",0.5752117037773132
translation,6,116,experimental-setup,gensim,has,"?eh?ek and sojka , 2010 )","gensim has ?eh?ek and sojka , 2010 )",0.6116188764572144
translation,6,116,experimental-setup,experimental setup,use,"bm25 ( robertson and zaragoza , 2009 ) implementation","experimental setup use bm25 ( robertson and zaragoza , 2009 ) implementation",0.5827990174293518
translation,6,26,model,two-step approach,of,retriever model,two-step approach of retriever model,0.589388370513916
translation,6,26,model,retriever model,that retrieves,small set,retriever model that retrieves small set,0.7262235879898071
translation,6,26,model,small set,of,candidate tables,small set of candidate tables,0.6122359037399292
translation,6,26,model,small set,followed by,qa model,small set followed by qa model,0.7042709589004517
translation,6,26,model,candidate tables,from,corpus,candidate tables from corpus,0.5917072892189026
translation,6,26,model,model,follow,two-step approach,model follow two-step approach,0.6496819257736206
translation,6,122,results,all dense models,pretrained out - peform,bm25 baseline,all dense models pretrained out - peform bm25 baseline,0.8033411502838135
translation,6,122,results,results,find that,all dense models,results find that all dense models,0.6331145167350769
translation,6,123,results,model,uses,tapas table embeddings ( dtr ),model uses tapas table embeddings ( dtr ),0.5839592814445496
translation,6,123,results,tapas table embeddings ( dtr ),out -performs,dense baselines,tapas table embeddings ( dtr ) out -performs dense baselines,0.7254898548126221
translation,6,123,results,dense baselines,by,more than 1 point,dense baselines by more than 1 point,0.5778080821037292
translation,6,123,results,more than 1 point,in,r@10,more than 1 point in r@10,0.6101627349853516
translation,6,123,results,results,has,model,results has model,0.5339115858078003
translation,6,125,results,mining negatives,from,dtr,mining negatives from dtr,0.6157564520835876
translation,6,125,results,mining negatives,from,bm25,mining negatives from bm25,0.6270092725753784
translation,6,125,results,mining negatives,works better,mining negatives,mining negatives works better mining negatives,0.6854056119918823
translation,6,125,results,mining negatives,from,bm25,mining negatives from bm25,0.6270092725753784
translation,6,125,results,results,has,mining negatives,results has mining negatives,0.6164311766624451
translation,6,129,results,all dense models,out -perform,bm25 baseline,all dense models out -perform bm25 baseline,0.6823720335960388
translation,6,129,results,results,find that,all dense models,results find that all dense models,0.6331145167350769
translation,6,130,results,bert reader,by,more than 3 points in em,bert reader by more than 3 points in em,0.6501671075820923
translation,6,130,results,tapas - based reader,has,outperforms,tapas - based reader has outperforms,0.6078812479972839
translation,6,130,results,outperforms,has,bert reader,outperforms has bert reader,0.6808372735977173
translation,6,130,results,results,has,tapas - based reader,results has tapas - based reader,0.5407297015190125
translation,6,131,results,simple dtr model,out -performs,baselines,simple dtr model out -performs baselines,0.7461192607879639
translation,6,131,results,baselines,by,more than 1 point,baselines by more than 1 point,0.6137261390686035
translation,6,131,results,more than 1 point,in,em,more than 1 point in em,0.619696855545044
translation,6,131,results,results,has,simple dtr model,results has simple dtr model,0.5478670597076416
translation,6,132,results,hard negatives,from,bm25 ( + hnbm25 ),hard negatives from bm25 ( + hnbm25 ),0.5514355301856995
translation,6,132,results,hard negatives,from,dtr ( +hn ),hard negatives from dtr ( +hn ),0.5477820634841919
translation,6,132,results,hard negatives,from,dtr ( +hn ),hard negatives from dtr ( +hn ),0.5477820634841919
translation,6,132,results,bm25 ( + hnbm25 ),improve,dtr 's performance,bm25 ( + hnbm25 ) improve dtr 's performance,0.6806535720825195
translation,6,132,results,bm25 ( + hnbm25 ),improve,performance,bm25 ( + hnbm25 ) improve performance,0.7017089128494263
translation,6,132,results,dtr 's performance,by,1 point,dtr 's performance by 1 point,0.589493989944458
translation,6,132,results,dtr 's performance,by,2 points,dtr 's performance by 2 points,0.5877887010574341
translation,6,132,results,dtr 's performance,by,2 points,dtr 's performance by 2 points,0.5877887010574341
translation,6,132,results,hard negatives,from,dtr ( +hn ),hard negatives from dtr ( +hn ),0.5477820634841919
translation,6,132,results,hard negatives,improve,performance,hard negatives improve performance,0.7079929113388062
translation,6,132,results,dtr ( +hn ),improve,performance,dtr ( +hn ) improve performance,0.7029415965080261
translation,6,132,results,performance,by,2 points,performance by 2 points,0.6287950873374939
translation,6,132,results,results,has,hard negatives,results has hard negatives,0.5736886858940125
translation,6,133,results,mcnemar 's significance test,for,proposed model,mcnemar 's significance test for proposed model,0.6244835257530212
translation,6,133,results,significantly better ( p< 0.05 ),than,all baselines,significantly better ( p< 0.05 ) than all baselines,0.5613695979118347
translation,6,135,results,best model,on,dev set,best model on dev set,0.5760170817375183
translation,6,135,results,best model,find that,29 %,best model find that 29 %,0.6213961839675903
translation,6,135,results,best model,for,34 %,best model for 34 %,0.594539225101471
translation,6,135,results,29 %,of,questions,29 % of questions,0.6175565719604492
translation,6,135,results,questions,are,answered correctly,questions are answered correctly,0.5484997034072876
translation,6,135,results,14 %,require,list answer,14 % require list answer,0.6820701956748962
translation,6,135,results,12 %,do not have,table candidate,12 % do not have table candidate,0.6397033333778381
translation,6,135,results,table candidate,for,11 %,table candidate for 11 %,0.6615318655967712
translation,6,135,results,model,for,34 %,model for 34 %,0.6417138576507568
translation,6,135,results,reader,fails to extract,correct span,reader fails to extract correct span,0.7239185571670532
translation,6,135,results,11 %,has,model,11 % has model,0.6239039897918701
translation,6,135,results,34 %,has,reader,34 % has reader,0.6675546169281006
translation,6,135,results,results,Analyzing,best model,results Analyzing best model,0.6357983946800232
translation,7,7,model,every subtask target,as,sequence,every subtask target as sequence,0.5563583374023438
translation,7,7,model,sequence,mixed by,pointer indexes and sentiment class indexes,sequence mixed by pointer indexes and sentiment class indexes,0.7585940957069397
translation,7,7,model,all absa subtasks,into,unified generative formulation,all absa subtasks into unified generative formulation,0.5764474272727966
translation,7,7,model,model,redefine,every subtask target,model redefine every subtask target,0.7455434203147888
translation,7,8,model,pre-training sequence - to-sequence model bart,to solve,all absa subtasks,pre-training sequence - to-sequence model bart to solve all absa subtasks,0.624698281288147
translation,7,8,model,all absa subtasks,in,endto-end framework,all absa subtasks in endto-end framework,0.5353735089302063
translation,7,8,model,model,exploit,pre-training sequence - to-sequence model bart,model exploit pre-training sequence - to-sequence model bart,0.7393837571144104
translation,7,42,model,extraction and classification tasks,as,pointer indexes and class indexes generation,extraction and classification tasks as pointer indexes and class indexes generation,0.4604059159755707
translation,7,42,model,model,model,extraction and classification tasks,model model extraction and classification tasks,0.7145747542381287
translation,7,43,model,"sequence - to-sequence pre-trained model bart ( lewis et al. , 2020 )",as,backbone,"sequence - to-sequence pre-trained model bart ( lewis et al. , 2020 ) as backbone",0.5168836116790771
translation,7,43,model,backbone,to generate,target sequence,backbone to generate target sequence,0.7083858251571655
translation,7,43,model,target sequence,in,end-to - end process,target sequence in end-to - end process,0.5445238947868347
translation,8,167,ablation-analysis,dialogue -specific mixed training,helps reduce,gap,dialogue -specific mixed training helps reduce gap,0.7196975350379944
translation,8,167,ablation-analysis,gap,for,thai,gap for thai,0.7797099947929382
translation,8,167,ablation-analysis,thai,on,intent accuracy,thai on intent accuracy,0.5160990357398987
translation,8,167,ablation-analysis,ablation analysis,adds,dialogue -specific mixed training,ablation analysis adds dialogue -specific mixed training,0.5857774019241333
translation,8,167,ablation-analysis,ablation analysis,more,dialogue -specific mixed training,ablation analysis more dialogue -specific mixed training,0.6351732015609741
translation,8,169,ablation-analysis,english data,to,fine-tuning ( ft w/en ),english data to fine-tuning ( ft w/en ),0.5784669518470764
translation,8,169,ablation-analysis,english data,is,slightly harmful,english data is slightly harmful,0.5728899240493774
translation,8,169,ablation-analysis,fine-tuning ( ft w/en ),is,slightly harmful,fine-tuning ( ft w/en ) is slightly harmful,0.5972573757171631
translation,8,169,ablation-analysis,ablation analysis,Adding,english data,ablation analysis Adding english data,0.6287497878074646
translation,8,7,baselines,cross-lingual meta-transfer learning adaptation approach,for,nlu,cross-lingual meta-transfer learning adaptation approach for nlu,0.6038824915885925
translation,8,7,baselines,x- metra - ada,has,cross-lingual meta-transfer learning adaptation approach,x- metra - ada has cross-lingual meta-transfer learning adaptation approach,0.5642004013061523
translation,8,7,baselines,baselines,propose,x- metra - ada,baselines propose x- metra - ada,0.6520649790763855
translation,8,8,baselines,maml,has,optimization - based meta-learning approach,maml has optimization - based meta-learning approach,0.5563421845436096
translation,8,114,baselines,initial model,fine-tuned on,dev split,initial model fine-tuned on dev split,0.7557370066642761
translation,8,114,baselines,dev split,of,target language,dev split of target language,0.606968104839325
translation,8,114,baselines,mono,has,initial model,mono has initial model,0.6181835532188416
translation,8,114,baselines,baselines,has,mono,baselines has mono,0.6368564367294312
translation,8,116,baselines,pre model,on,dev split,pre model on dev split,0.6250427961349487
translation,8,116,baselines,dev split,of,target language,dev split of target language,0.606968104839325
translation,8,120,baselines,metra,use,pre model,metra use pre model,0.6674591302871704
translation,8,120,baselines,metra,use,pre model,metra use pre model,0.6674591302871704
translation,8,120,baselines,metra,use,pre model,metra use pre model,0.6674591302871704
translation,8,120,baselines,pre model,as,meta-train,pre model as meta-train,0.4906931221485138
translation,8,120,baselines,pre model,as,meta-train,pre model as meta-train,0.4906931221485138
translation,8,120,baselines,pre model,for,meta-train,pre model for meta-train,0.6107965111732483
translation,8,120,baselines,pre model,for,meta-train,pre model for meta-train,0.6107965111732483
translation,8,120,baselines,english,to form,support sets,english to form support sets,0.6580648422241211
translation,8,120,baselines,english,to form,support sets,english to form support sets,0.6580648422241211
translation,8,120,baselines,english,to form,support sets,english to form support sets,0.6580648422241211
translation,8,120,baselines,support sets,in,d meta?train,support sets in d meta?train,0.5280344486236572
translation,8,120,baselines,support sets,in,d meta-train,support sets in d meta-train,0.5032572150230408
translation,8,120,baselines,support sets,in,d meta-train,support sets in d meta-train,0.5032572150230408
translation,8,120,baselines,support sets,in,d meta-train,support sets in d meta-train,0.5032572150230408
translation,8,120,baselines,target language,to form,query sets,target language to form query sets,0.612823486328125
translation,8,120,baselines,query sets,in,d meta?train,query sets in d meta?train,0.5375848412513733
translation,8,120,baselines,query sets,in,d meta-train,query sets in d meta-train,0.533595085144043
translation,8,120,baselines,ada,use,pre model,ada use pre model,0.670068621635437
translation,8,120,baselines,pre model,for,meta-train,pre model for meta-train,0.6107965111732483
translation,8,120,baselines,english,to form,support sets,english to form support sets,0.6580648422241211
translation,8,120,baselines,support sets,in,d meta-train,support sets in d meta-train,0.5032572150230408
translation,8,134,baselines,baselines,For,mtod,baselines For mtod,0.6123278737068176
translation,8,136,baselines,xlm,trained on,translation language modeling ( tlm ),xlm trained on translation language modeling ( tlm ),0.7331554293632507
translation,8,136,baselines,masked language modeling ( mlm ),For,tydiqa - goldp,masked language modeling ( mlm ) For tydiqa - goldp,0.6143166422843933
translation,8,136,baselines,baselines,include,xlm,baselines include xlm,0.6724268198013306
translation,8,121,experimental-setup,mtod,use,75 %,mtod use 75 %,0.6596350073814392
translation,8,121,experimental-setup,75 %,of,dev split,75 % of dev split,0.6450307965278625
translation,8,121,experimental-setup,dev split,of,target language,dev split of target language,0.606968104839325
translation,8,121,experimental-setup,target language,to form,query sets,target language to form query sets,0.612823486328125
translation,8,121,experimental-setup,query sets,in,d meta-train,query sets in d meta-train,0.533595085144043
translation,8,121,experimental-setup,experimental setup,For,mtod,experimental setup For mtod,0.6114218235015869
translation,8,122,experimental-setup,remaining 25 %,of,dev split,remaining 25 % of dev split,0.6255766153335571
translation,8,122,experimental-setup,dev split,of,target language,dev split of target language,0.606968104839325
translation,8,122,experimental-setup,target language,for,support and query sets,target language for support and query sets,0.6559023261070251
translation,8,122,experimental-setup,support and query sets,of,d meta-adapt,support and query sets of d meta-adapt,0.6093697547912598
translation,8,122,experimental-setup,experimental setup,use,remaining 25 %,experimental setup use remaining 25 %,0.6598131060600281
translation,8,123,experimental-setup,qa,use,ratios,qa use ratios,0.6965357065200806
translation,8,123,experimental-setup,ratios,of,60 %,ratios of 60 %,0.6924086213111877
translation,8,123,experimental-setup,ratios,of,40 %,ratios of 40 %,0.6871641874313354
translation,8,123,experimental-setup,60 %,for,d meta-train,60 % for d meta-train,0.6500052809715271
translation,8,123,experimental-setup,40 %,for,d meta-adapt,40 % for d meta-adapt,0.6260133981704712
translation,8,123,experimental-setup,experimental setup,For,qa,experimental setup For qa,0.5931345224380493
translation,8,140,experimental-setup,xlm-r-distilroberta- base - paraphrase - v1 6 model,for computing,similarities,xlm-r-distilroberta- base - paraphrase - v1 6 model for computing similarities,0.7254616022109985
translation,8,140,experimental-setup,similarities,when constructing,qa meta-dataset,similarities when constructing qa meta-dataset,0.6916182637214661
translation,8,140,experimental-setup,experimental setup,use,xlm-r-distilroberta- base - paraphrase - v1 6 model,experimental setup use xlm-r-distilroberta- base - paraphrase - v1 6 model,0.6114177107810974
translation,8,141,experimental-setup,implementation,of,x-metra - ada from scratch,implementation of x-metra - ada from scratch,0.5670537948608398
translation,8,141,experimental-setup,implementation,uses,"learn2learn ( arnold et al. , 2020 )","implementation uses learn2learn ( arnold et al. , 2020 )",0.5251032114028931
translation,8,141,experimental-setup,x-metra - ada from scratch,uses,"learn2learn ( arnold et al. , 2020 )","x-metra - ada from scratch uses learn2learn ( arnold et al. , 2020 )",0.5629093647003174
translation,8,141,experimental-setup,"learn2learn ( arnold et al. , 2020 )",for,differentiation,"learn2learn ( arnold et al. , 2020 ) for differentiation",0.5692039132118225
translation,8,141,experimental-setup,"learn2learn ( arnold et al. , 2020 )",for,update rules,"learn2learn ( arnold et al. , 2020 ) for update rules",0.5641345381736755
translation,8,141,experimental-setup,update rules,in,inner loop,update rules in inner loop,0.517534613609314
translation,8,141,experimental-setup,experimental setup,has,implementation,experimental setup has implementation,0.5296319723129272
translation,8,143,experimental-setup,first-order approximation option,in,learn2learn,first-order approximation option in learn2learn,0.512902557849884
translation,8,143,experimental-setup,learn2learn,for updating,outer loop,learn2learn for updating outer loop,0.7245853543281555
translation,8,143,experimental-setup,experimental setup,use,first-order approximation option,experimental setup use first-order approximation option,0.6372247934341431
translation,8,135,experiments,multilingual version,of,contextualized word vectors,multilingual version of contextualized word vectors,0.5604180693626404
translation,8,135,experiments,contextualized word vectors,with,autoencoder objective,contextualized word vectors with autoencoder objective,0.5851284861564636
translation,8,135,experiments,mcove,has,multilingual version,mcove has multilingual version,0.6133967638015747
translation,8,144,experiments,average and standard deviation,of,best model,average and standard deviation of best model,0.6138493418693542
translation,8,144,experiments,best model,for,few-shot language,best model for few-shot language,0.5269866585731506
translation,8,179,experiments,x - metra - ada,learns,more stable and successful adaptation,x - metra - ada learns more stable and successful adaptation,0.6312347054481506
translation,8,179,experiments,more stable and successful adaptation,to,that language,more stable and successful adaptation to that language,0.5958210229873657
translation,8,179,experiments,more stable and successful adaptation,on top of,model,more stable and successful adaptation on top of model,0.706452488899231
translation,8,179,experiments,model,pre-trained on,english,model pre-trained on english,0.7445151209831238
translation,8,179,experiments,english,with,less over-fitting,english with less over-fitting,0.6361895203590393
translation,8,49,model,multilingual pre-trained model,use it to initialize,word-piece embeddings layer,multilingual pre-trained model use it to initialize word-piece embeddings layer,0.6461942791938782
translation,8,49,model,model,given,multilingual pre-trained model,model given multilingual pre-trained model,0.6522624492645264
translation,8,49,model,model,use it to initialize,word-piece embeddings layer,model use it to initialize word-piece embeddings layer,0.6277305483818054
translation,8,159,results,pre model,performs,worse,pre model performs worse,0.6766723394393921
translation,8,159,results,worse,than,other baselines,worse than other baselines,0.5912690162658691
translation,8,159,results,results,has,pre model,results has pre model,0.5968173146247864
translation,8,160,results,less,than,simplest baseline,less than simplest baseline,0.5784645676612854
translation,8,160,results,mcove,when transferring to,thai,mcove when transferring to thai,0.7775021195411682
translation,8,160,results,thai,with,decrease,thai with decrease,0.7088825702667236
translation,8,160,results,thai,with,average cross-lingual relative loss,thai with average cross-lingual relative loss,0.6451565623283386
translation,8,160,results,decrease,of,25.3 % and 23.1 %,decrease of 25.3 % and 23.1 %,0.6014282703399658
translation,8,160,results,average cross-lingual relative loss,of,4.5 % and 2.1 %,average cross-lingual relative loss of 4.5 % and 2.1 %,0.5785138010978699
translation,8,160,results,4.5 % and 2.1 %,for,intent classification and slot filling,4.5 % and 2.1 % for intent classification and slot filling,0.6303744912147522
translation,8,168,results,positive effects,of,crosslingual fine-tuning,positive effects of crosslingual fine-tuning,0.5495070219039917
translation,8,168,results,fine-tuning,with,inlanguage data,fine-tuning with inlanguage data,0.6213057637214661
translation,8,168,results,inlanguage data,on top of,pre ( i.e. ft ),inlanguage data on top of pre ( i.e. ft ),0.7509809732437134
translation,8,168,results,positive effects,has,fine-tuning,positive effects has fine-tuning,0.5719038844108582
translation,8,168,results,results,confirm,positive effects,results confirm positive effects,0.5606284141540527
translation,8,171,results,pairwise two -sample t-test,find,results,pairwise two -sample t-test find results,0.5722952485084534
translation,8,171,results,results,of,x-metra - ada,results of x-metra - ada,0.5649294853210449
translation,8,171,results,x-metra - ada,compared to,ft,x-metra - ada compared to ft,0.7139537930488586
translation,8,171,results,x-metra - ada,to be,statistically significant,x-metra - ada to be statistically significant,0.6060500144958496
translation,8,171,results,ft,on,intent classification,ft on intent classification,0.5590963363647461
translation,8,171,results,ft,to be,statistically significant,ft to be statistically significant,0.579777181148529
translation,8,171,results,2.4 %,for,spanish and thai,2.4 % for spanish and thai,0.5672134757041931
translation,8,171,results,results,perform,pairwise two -sample t-test,results perform pairwise two -sample t-test,0.5956621170043945
translation,8,171,results,results,of,x-metra - ada,results of x-metra - ada,0.5649294853210449
translation,8,172,results,all previous external baselines and fine-tuning models,for,spanish and thai,all previous external baselines and fine-tuning models for spanish and thai,0.5826641917228699
translation,8,172,results,outperforms,has,all previous external baselines and fine-tuning models,outperforms has all previous external baselines and fine-tuning models,0.545398473739624
translation,8,173,results,best overall performance,with,average cross-lingual cross - task increase,best overall performance with average cross-lingual cross - task increase,0.663347065448761
translation,8,173,results,average cross-lingual cross - task increase,of,3.2 %,average cross-lingual cross - task increase of 3.2 %,0.5454140901565552
translation,8,173,results,average cross-lingual cross - task increase,of,6.9 %,average cross-lingual cross - task increase of 6.9 %,0.5452818274497986
translation,8,173,results,average cross-lingual cross - task increase,of,12.6 %,average cross-lingual cross - task increase of 12.6 %,0.5445936322212219
translation,8,173,results,3.2 %,over,ft baseline,3.2 % over ft baseline,0.6530067324638367
translation,8,173,results,6.9 %,over,ft w/en,6.9 % over ft w/en,0.5520437955856323
translation,8,173,results,12.6 %,over,mono,12.6 % over mono,0.6528503894805908
translation,8,173,results,results,achieve,best overall performance,results achieve best overall performance,0.6541040539741516
translation,8,174,results,least stability,suggested by,higher average standard deviation,least stability suggested by higher average standard deviation,0.6455969214439392
translation,8,174,results,all models,has,mono,all models has mono,0.6286420822143555
translation,8,174,results,mono,has,least stability,mono has least stability,0.6294949650764465
translation,8,174,results,results,Among,all models,results Among all models,0.5917154550552368
translation,8,175,results,thai,compared to,spanish,thai compared to spanish,0.6724755167961121
translation,8,175,results,results,tendency for,x-metra - ada,results tendency for x-metra - ada,0.6744995713233948
translation,8,177,results,fine -tuning,on,small amounts,fine -tuning on small amounts,0.5267003774642944
translation,8,177,results,small amounts,of,dev data,small amounts of dev data,0.6008108854293823
translation,8,177,results,generalize,to,new languages,generalize to new languages,0.6066703200340271
translation,8,177,results,model,has,generalize,model has generalize,0.6254764795303345
translation,8,177,results,results,has,fine -tuning,results has fine -tuning,0.5543118715286255
translation,8,178,results,mono baselines,exhibit,less stability,mono baselines exhibit less stability,0.6782404780387878
translation,8,178,results,less stability,than,x- metra - ada,less stability than x- metra - ada,0.6345818042755127
translation,8,178,results,results,has,mono baselines,results has mono baselines,0.5703135132789612
translation,8,180,results,methods,for,tydiqa - goldp,methods for tydiqa - goldp,0.5842881798744202
translation,8,182,results,benefits,of,fine-tuning,benefits of fine-tuning,0.5733046531677246
translation,8,182,results,benefits,of,improvements,benefits of improvements,0.6096367239952087
translation,8,182,results,improvements,from,x-metra - ada,improvements from x-metra - ada,0.6078686714172363
translation,8,182,results,results,has,benefits,results has benefits,0.5237578749656677
translation,8,184,results,x - metra,increases by,10.8 % and 1.5 %,x - metra increases by 10.8 % and 1.5 %,0.7059206366539001
translation,8,184,results,10.8 % and 1.5 %,over,best external and fine-tuning baseline,10.8 % and 1.5 % over best external and fine-tuning baseline,0.6625069975852966
translation,8,184,results,results,has,x - metra,results has x - metra,0.5594708323478699
translation,8,185,results,x-metra - ada,has,outperforms,x-metra - ada has outperforms,0.6488386392593384
translation,8,185,results,outperforms,has,x- metra,outperforms has x- metra,0.6638725996017456
translation,8,185,results,outperforms,has,on average,outperforms has on average,0.6283096075057983
translation,8,185,results,x- metra,has,on average,x- metra has on average,0.6126301884651184
translation,8,185,results,results,has,x-metra - ada,results has x-metra - ada,0.550746738910675
translation,8,187,results,meta-learning,has,significantly and consistently outperforms,meta-learning has significantly and consistently outperforms,0.5847517848014832
translation,8,187,results,significantly and consistently outperforms,has,fine-tuning,significantly and consistently outperforms has fine-tuning,0.6038287281990051
translation,8,187,results,results,has,meta-learning,results has meta-learning,0.5214784145355225
translation,8,198,results,spanish,continuing to use,english,spanish continuing to use english,0.6869434118270874
translation,8,198,results,english,in,naive fine-tuning,english in naive fine-tuning,0.514096736907959
translation,8,198,results,english,reaches,better performance,english reaches better performance,0.7368330359458923
translation,8,198,results,naive fine-tuning,to,spanish,naive fine-tuning to spanish,0.5745266079902649
translation,8,198,results,better performance,than,both variants of meta-learning,better performance than both variants of meta-learning,0.5773571133613586
translation,8,198,results,both variants of meta-learning,for,slot filling,both variants of meta-learning for slot filling,0.6261647343635559
translation,8,198,results,slot filling,on,spanish,slot filling on spanish,0.598910927772522
translation,8,198,results,results,For,spanish,results For spanish,0.5729057192802429
translation,8,211,results,percentage,of,query data,percentage of query data,0.6182652711868286
translation,8,211,results,gap,between,x-metra - ada and ft,gap between x-metra - ada and ft,0.7228928804397583
translation,8,211,results,slots,is,steadier,slots is steadier,0.6405308246612549
translation,8,211,results,query data,has,increases,query data has increases,0.6290614604949951
translation,8,211,results,x-metra - ada and ft,has,increases slightly,x-metra - ada and ft has increases slightly,0.6162249445915222
translation,8,211,results,results,as,percentage,results as percentage,0.6006213426589966
translation,9,22,ablation-analysis,tl,improvements of,mtl,tl improvements of mtl,0.6834688186645508
translation,9,22,ablation-analysis,ablation analysis,has,tl,ablation analysis has tl,0.5647212862968445
translation,9,113,ablation-analysis,results,suggest,transfer learning,results suggest transfer learning,0.5534752607345581
translation,9,113,ablation-analysis,ablation analysis,suggest,transfer learning,ablation analysis suggest transfer learning,0.620030403137207
translation,9,113,ablation-analysis,ablation analysis,has,results,ablation analysis has results,0.4875600337982178
translation,9,6,model,multi-task learning method,to incorporate,information,multi-task learning method to incorporate information,0.6237377524375916
translation,9,6,model,information,from,syntactic and semantic auxiliary tasks,information from syntactic and semantic auxiliary tasks,0.5104738473892212
translation,9,6,model,model,propose,multi-task learning method,model propose multi-task learning method,0.6251544952392578
translation,9,21,results,performance,negatively affected by,negation and speculation,performance negatively affected by negation and speculation,0.6504676342010498
translation,9,21,results,mtl and transfer learning ( tl ) models,are,more robust,mtl and transfer learning ( tl ) models are more robust,0.5588759779930115
translation,9,21,results,more robust,than,single task learning ( stl ),more robust than single task learning ( stl ),0.5588297843933105
translation,9,21,results,results,find that,performance,results find that performance,0.6555676460266113
translation,9,114,results,mtl,not,hurt,mtl not hurt,0.74611896276474
translation,9,114,results,significantly better,than,all of the mtl models,significantly better than all of the mtl models,0.5529176592826843
translation,9,114,results,significantly better,across,datasets and embeddings,significantly better across datasets and embeddings,0.6909186244010925
translation,9,114,results,all of the mtl models,across,datasets and embeddings,all of the mtl models across datasets and embeddings,0.7065606117248535
translation,9,114,results,datasets and embeddings,for,f,datasets and embeddings for f,0.5986635684967041
translation,9,114,results,hurt,has,stl models,hurt has stl models,0.6047647595405579
translation,9,114,results,results,suggest,mtl,results suggest mtl,0.6061134338378906
translation,9,122,results,laptop n eg and restaurant n eg,incorporating,negation auxiliary tasks,laptop n eg and restaurant n eg incorporating negation auxiliary tasks,0.7037323713302612
translation,9,122,results,laptop n eg and restaurant n eg,gives,average improvement,laptop n eg and restaurant n eg gives average improvement,0.6160801649093628
translation,9,122,results,average improvement,of,3.8 pp,average improvement of 3.8 pp,0.579349160194397
translation,9,122,results,3.8 pp,on,f 1 -i metric,3.8 pp on f 1 -i metric,0.620198130607605
translation,9,122,results,3.8 pp,when using,glove embeddings,3.8 pp when using glove embeddings,0.7009806632995605
translation,9,122,results,f 1 -i metric,when using,glove embeddings,f 1 -i metric when using glove embeddings,0.7071125507354736
translation,9,122,results,results,On,laptop n eg and restaurant n eg,results On laptop n eg and restaurant n eg,0.5391751527786255
translation,9,123,results,negation,improves,sentiment classification scores,negation improves sentiment classification scores,0.6949690580368042
translation,9,123,results,results,MTL with,negation,results MTL with negation,0.6055795550346375
translation,9,125,results,results,jointly learning,dependency relations ( dr ),results jointly learning dependency relations ( dr ),0.6499572396278381
translation,9,126,results,tl,instead of,glove embeddings,tl instead of glove embeddings,0.6432694792747498
translation,9,126,results,best mtl model ( neg sf u ),does marginally beat,stl tl equivalent,best mtl model ( neg sf u ) does marginally beat stl tl equivalent,0.7374154925346375
translation,9,126,results,tl,has,best mtl model ( neg sf u ),tl has best mtl model ( neg sf u ),0.5978434681892395
translation,9,126,results,glove embeddings,has,best mtl model ( neg sf u ),glove embeddings has best mtl model ( neg sf u ),0.5776839852333069
translation,9,126,results,results,when using,tl,results when using tl,0.5913710594177246
translation,9,127,results,laptop spec and restaurant spec mtl models,improve,results,laptop spec and restaurant spec mtl models improve results,0.6514990925788879
translation,9,127,results,results,when using,glove embeddings,results when using glove embeddings,0.633013129234314
translation,9,127,results,glove embeddings,with,additional speculation ( spec ) and dependency relation ( dr ) data,glove embeddings with additional speculation ( spec ) and dependency relation ( dr ) data,0.6297321319580078
translation,9,127,results,additional speculation ( spec ) and dependency relation ( dr ) data,improving,f 1 -i metric,additional speculation ( spec ) and dependency relation ( dr ) data improving f 1 -i metric,0.6843998432159424
translation,9,127,results,f 1 -i metric,by,0.5 pp and 0.49 pp,f 1 -i metric by 0.5 pp and 0.49 pp,0.6174498796463013
translation,9,127,results,results,On,laptop spec and restaurant spec mtl models,results On laptop spec and restaurant spec mtl models,0.4987366199493408
translation,9,128,results,mtl,leads to,benefits,mtl leads to benefits,0.6757272481918335
translation,9,128,results,benefits,on,restaurant dataset,benefits on restaurant dataset,0.5148009061813354
translation,9,128,results,tl,has,mtl,tl has mtl,0.7042742371559143
translation,9,128,results,results,with,tl,results with tl,0.5424041748046875
translation,9,128,results,results,with,mtl,results with mtl,0.6288378834724426
translation,9,129,results,syntactic auxiliary tasks,like,dr,syntactic auxiliary tasks like dr,0.6105102300643921
translation,9,129,results,syntactic auxiliary tasks,than,semantic tasks,syntactic auxiliary tasks than semantic tasks,0.545774519443512
translation,9,129,results,dr,than,semantic tasks,dr than semantic tasks,0.5706674456596375
translation,9,129,results,semantic tasks,like,neg cd,semantic tasks like neg cd,0.6029451489448547
translation,9,133,results,stl model,using,glove and tl,stl model using glove and tl,0.7063345313072205
translation,9,133,results,improves,by,9.55 pp,improves by 9.55 pp,0.6254011392593384
translation,9,133,results,9.55 pp,on,negation dataset,9.55 pp on negation dataset,0.5688300728797913
translation,9,133,results,9.55 pp,compared to,3.65 pp,9.55 pp compared to 3.65 pp,0.6688814759254456
translation,9,133,results,3.65 pp,for,speculation,3.65 pp for speculation,0.6126760840415955
translation,9,133,results,model,has,improves,model has improves,0.6412354707717896
translation,9,133,results,results,comparing,stl model,results comparing stl model,0.6289038062095642
translation,10,33,experiments,two datasets,for assessing,automatic evaluation metrics,two datasets for assessing automatic evaluation metrics,0.6274510025978088
translation,10,33,experiments,automatic evaluation metrics,with regard to,correctness,automatic evaluation metrics with regard to correctness,0.5513543486595154
translation,10,33,experiments,correctness,in,genqa domain,correctness in genqa domain,0.5096223950386047
translation,10,115,hyperparameters,model parameters,choose,bert- baseuncased variants,model parameters choose bert- baseuncased variants,0.6942079663276672
translation,10,115,hyperparameters,model parameters,use,one fully - connected layer,model parameters use one fully - connected layer,0.59691321849823
translation,10,115,hyperparameters,bert- baseuncased variants,for,bert model,bert- baseuncased variants for bert model,0.6487858295440674
translation,10,115,hyperparameters,one fully - connected layer,with,softmax layer,one fully - connected layer with softmax layer,0.5851373672485352
translation,10,115,hyperparameters,hyperparameters,For,model parameters,hyperparameters For model parameters,0.5195245742797852
translation,10,115,hyperparameters,hyperparameters,use,one fully - connected layer,hyperparameters use one fully - connected layer,0.5694217085838318
translation,10,116,hyperparameters,5 epochs,choose,model,5 epochs choose model,0.7371450066566467
translation,10,116,hyperparameters,model,shows,minimum evaluation loss,model shows minimum evaluation loss,0.6379119753837585
translation,10,116,hyperparameters,hyperparameters,train,5 epochs,hyperparameters train 5 epochs,0.6590479016304016
translation,10,6,model,new metric,for evaluating,correctness of genqa,new metric for evaluating correctness of genqa,0.7106055021286011
translation,10,6,model,kpqametric,has,new metric,kpqametric has new metric,0.6289508938789368
translation,10,6,model,model,propose,kpqametric,model propose kpqametric,0.6884665489196777
translation,10,7,model,new metric,assigns,different weights,new metric assigns different weights,0.6992000937461853
translation,10,7,model,different weights,to,each token,different weights to each token,0.5414512157440186
translation,10,7,model,each token,via,keyphrase prediction,each token via keyphrase prediction,0.6992661952972412
translation,10,7,model,keyphrase prediction,judging,generated answer sentence,keyphrase prediction judging generated answer sentence,0.6281816959381104
translation,10,7,model,model,has,new metric,model has new metric,0.5850754380226135
translation,10,29,model,keyphrase predictor,for,question answering ( kpqa ),keyphrase predictor for question answering ( kpqa ),0.6457785964012146
translation,10,126,results,significantly higher correlation score,for,our proposed kpqa - metric,significantly higher correlation score for our proposed kpqa - metric,0.6082972884178162
translation,10,126,results,our proposed kpqa - metric,compared to,existing metrics,our proposed kpqa - metric compared to existing metrics,0.6039146184921265
translation,10,126,results,existing metrics,especially for,ms - marco and avsd,existing metrics especially for ms - marco and avsd,0.6367220878601074
translation,10,126,results,ms - marco and avsd,where,answers,ms - marco and avsd where answers,0.6723085641860962
translation,10,126,results,answers,are,full-sentences,answers are full-sentences,0.6029333472251892
translation,10,126,results,full-sentences,rather than,short phrases,full-sentences rather than short phrases,0.6348483562469482
translation,10,126,results,results,observe,significantly higher correlation score,results observe significantly higher correlation score,0.6277368068695068
translation,10,127,results,gap,in,performance,gap in performance,0.5397233963012695
translation,10,127,results,performance,between,kpqa -metric and existing metrics,performance between kpqa -metric and existing metrics,0.6007333397865295
translation,10,127,results,performance,is,low,performance is low,0.6135802865028381
translation,10,127,results,narrativeqa,has,gap,narrativeqa has gap,0.5945816040039062
translation,10,127,results,results,For,narrativeqa,results For narrativeqa,0.5308181643486023
translation,10,144,results,  - kpqa   metric,better than,  - kp   metric,  - kpqa   metric better than   - kp   metric,0.7067844867706299
translation,10,144,results,  - kp   metric,for,all of the three variants,  - kp   metric for all of the three variants,0.6102082133293152
translation,10,144,results,results,observe,  - kpqa   metric,results observe   - kpqa   metric,0.6111018657684326
translation,10,148,results,outperform,in,all of the question types,outperform in all of the question types,0.5303829312324524
translation,10,148,results,kpqa -metric variants,has,outperform,kpqa -metric variants has outperform,0.6210949420928955
translation,10,148,results,outperform,has,original version,outperform has original version,0.6124585270881653
translation,10,191,results,kpqa - metric,shows,more matches,kpqa - metric shows more matches,0.6519231796264648
translation,10,191,results,more matches,than,previous metrics,more matches than previous metrics,0.5802620053291321
translation,10,191,results,more matches,in,datasets,more matches in datasets,0.5048075318336487
translation,10,191,results,previous metrics,in,datasets,previous metrics in datasets,0.4768712818622589
translation,10,191,results,results,has,kpqa - metric,results has kpqa - metric,0.5769659876823425
translation,11,154,ablation-analysis,question filtration,train,better qa model,question filtration train better qa model,0.750918984413147
translation,11,154,ablation-analysis,better qa model,leading to,+ 2.3 f1,better qa model leading to + 2.3 f1,0.7075582146644592
translation,11,154,ablation-analysis,better qa model,leading to,+ 1.1 f1,better qa model leading to + 1.1 f1,0.7134493589401245
translation,11,154,ablation-analysis,+ 2.3 f1,for,hybridqa,+ 2.3 f1 for hybridqa,0.6825484037399292
translation,11,154,ablation-analysis,+ 1.1 f1,for,hot-potqa,+ 1.1 f1 for hot-potqa,0.6725171804428101
translation,11,154,ablation-analysis,ablation analysis,has,question filtration,ablation analysis has question filtration,0.5865183472633362
translation,11,7,baselines,mqa - qg,generates,questions,mqa - qg generates questions,0.6935086846351624
translation,11,7,baselines,questions,by first selecting / generating,relevant information,questions by first selecting / generating relevant information,0.593281626701355
translation,11,7,baselines,questions,integrating,multiple information,questions integrating multiple information,0.749993622303009
translation,11,7,baselines,relevant information,from,each data source,relevant information from each data source,0.5207428932189941
translation,11,7,baselines,multiple information,to form,multi-hop question,multiple information to form multi-hop question,0.6510393619537354
translation,11,7,baselines,baselines,has,mqa - qg,baselines has mqa - qg,0.5797743201255798
translation,11,5,model,well - performed multi-hop qa model,without referencing,human-labeled multihop question - answer pairs,well - performed multi-hop qa model without referencing human-labeled multihop question - answer pairs,0.737797737121582
translation,11,5,model,human-labeled multihop question - answer pairs,i.e.,unsupervised multi-hop qa,human-labeled multihop question - answer pairs i.e. unsupervised multi-hop qa,0.6322598457336426
translation,11,5,model,model,train,well - performed multi-hop qa model,model train well - performed multi-hop qa model,0.6730151772499084
translation,11,6,model,unsupervised framework,can generate,human-like multi-hop training data,unsupervised framework can generate human-like multi-hop training data,0.7354812026023865
translation,11,6,model,human-like multi-hop training data,from,homogeneous and heterogeneous data sources,human-like multi-hop training data from homogeneous and heterogeneous data sources,0.5655827522277832
translation,11,6,model,mqa - qg,has,unsupervised framework,mqa - qg has unsupervised framework,0.5671074390411377
translation,11,6,model,model,propose,mqa - qg,model propose mqa - qg,0.7116631269454956
translation,11,21,model,generation,of,multi-hop question,generation of multi-hop question,0.5863320827484131
translation,11,21,model,multi-hop question,into,two steps,multi-hop question into two steps,0.624737560749054
translation,11,21,model,relevant information,from,each data source,relevant information from each data source,0.5207428932189941
translation,11,21,model,multiple information,to form,question,multiple information to form question,0.6572433710098267
translation,11,21,model,model,propose,multi-hop question generator ( mqa - qg ),model propose multi-hop question generator ( mqa - qg ),0.6734597086906433
translation,11,22,model,set of basic operators,to retrieve / generate,relevant information,set of basic operators to retrieve / generate relevant information,0.7165514826774597
translation,11,22,model,set of basic operators,to aggregate,different information,set of basic operators to aggregate different information,0.7452479600906372
translation,11,22,model,relevant information,from,each input source,relevant information from each input source,0.5420456528663635
translation,11,22,model,relevant information,to aggregate,different information,relevant information to aggregate different information,0.7307510375976562
translation,11,22,model,model,defines,set of basic operators,model defines set of basic operators,0.7172858715057373
translation,11,45,model,first framework,for,unsupervised multi-hop qa,first framework for unsupervised multi-hop qa,0.6394277215003967
translation,11,45,model,model,propose,first framework,model propose first framework,0.6857028007507324
translation,11,106,model,question paraphrasing model,based on,bart model,question paraphrasing model based on bart model,0.6364277005195618
translation,11,106,model,bart model,to paraphrase,each generated question,bart model to paraphrase each generated question,0.7925649881362915
translation,11,106,model,model,train,question paraphrasing model,model train question paraphrasing model,0.6852760910987854
translation,11,189,model,unsupervised multi-hop qa,propose,novel framework mqa - qg,unsupervised multi-hop qa propose novel framework mqa - qg,0.6674520373344421
translation,11,189,model,novel framework mqa - qg,to generate,multi-hop questions,novel framework mqa - qg to generate multi-hop questions,0.6948524117469788
translation,11,189,model,multi-hop questions,via,composing reasoning graphs,multi-hop questions via composing reasoning graphs,0.5911391973495483
translation,11,189,model,composing reasoning graphs,built upon,basic operators,composing reasoning graphs built upon basic operators,0.6188254356384277
translation,11,189,model,model,study,unsupervised multi-hop qa,model study unsupervised multi-hop qa,0.5690547823905945
translation,11,189,model,model,propose,novel framework mqa - qg,model propose novel framework mqa - qg,0.686897337436676
translation,11,8,results,generated training data,train,competent multi-hop qa,generated training data train competent multi-hop qa,0.6845682263374329
translation,11,8,results,competent multi-hop qa,achieves,61 % and 83 %,competent multi-hop qa achieves 61 % and 83 %,0.6991090178489685
translation,11,8,results,61 % and 83 %,of,supervised learning performance,61 % and 83 % of supervised learning performance,0.5959995985031128
translation,11,8,results,supervised learning performance,for,hybridqa and the hotpotqa dataset,supervised learning performance for hybridqa and the hotpotqa dataset,0.5956768989562988
translation,11,8,results,results,Using,generated training data,results Using generated training data,0.6831866502761841
translation,11,31,results,mqa - qg,generate,highquality multi-hop questions,mqa - qg generate highquality multi-hop questions,0.6888304948806763
translation,11,31,results,highquality multi-hop questions,for,both datasets,highquality multi-hop questions for both datasets,0.5406370759010315
translation,11,32,results,generated questions,used to train,surprisingly well qa model,generated questions used to train surprisingly well qa model,0.738323986530304
translation,11,32,results,surprisingly well qa model,reaching,61 % and 83 %,surprisingly well qa model reaching 61 % and 83 %,0.747227668762207
translation,11,32,results,61 % and 83 %,of,f1 score,61 % and 83 % of f1 score,0.5917731523513794
translation,11,32,results,61 % and 83 %,achieved by,fully - supervised setting,61 % and 83 % achieved by fully - supervised setting,0.6551639437675476
translation,11,32,results,fully - supervised setting,on,hybridqa and hotpotqa dataset,fully - supervised setting on hybridqa and hotpotqa dataset,0.5353103280067444
translation,11,32,results,human-labeled examples,has,generated questions,human-labeled examples has generated questions,0.5736992955207825
translation,11,32,results,results,Without using,human-labeled examples,results Without using human-labeled examples,0.6104668378829956
translation,11,133,results,unsupervised model mqa - qg,attains,30.5 f 1,unsupervised model mqa - qg attains 30.5 f 1,0.6651991009712219
translation,11,133,results,unsupervised model mqa - qg,attains,68.6 f 1,unsupervised model mqa - qg attains 68.6 f 1,0.6618157625198364
translation,11,133,results,unsupervised model mqa - qg,attains,outperforming,unsupervised model mqa - qg attains outperforming,0.6636932492256165
translation,11,133,results,30.5 f 1,on,hybridqa test set,30.5 f 1 on hybridqa test set,0.5557007193565369
translation,11,133,results,68.6 f 1,on,hotpotqa dev set,68.6 f 1 on hotpotqa dev set,0.5409310460090637
translation,11,133,results,outperforming,by,large margins,outperforming by large margins,0.5988951921463013
translation,11,133,results,"all the unsupervised baselines ( u1 , u4 , u5 , u6 )",by,large margins,"all the unsupervised baselines ( u1 , u4 , u5 , u6 ) by large margins",0.5535674691200256
translation,11,133,results,outperforming,has,"all the unsupervised baselines ( u1 , u4 , u5 , u6 )","outperforming has all the unsupervised baselines ( u1 , u4 , u5 , u6 )",0.5916710495948792
translation,11,133,results,results,has,unsupervised model mqa - qg,results has unsupervised model mqa - qg,0.5677478909492493
translation,11,134,results,f 1 gap,to,fully - supervised version,f 1 gap to fully - supervised version,0.5293828845024109
translation,11,134,results,fully - supervised version,is,19.5 and 14.2,fully - supervised version is 19.5 and 14.2,0.5121889114379883
translation,11,134,results,fully - supervised version,only,19.5 and 14.2,fully - supervised version only 19.5 and 14.2,0.6298425197601318
translation,11,134,results,19.5 and 14.2,for,hybridqa and hotpotqa,19.5 and 14.2 for hybridqa and hotpotqa,0.6497060656547546
translation,11,134,results,human-annotated training data,has,f 1 gap,human-annotated training data has f 1 gap,0.5538116097450256
translation,11,134,results,results,Without using,human-annotated training data,results Without using human-annotated training data,0.622355580329895
translation,11,135,results,two weak supervised baselines ( s1 and s2 ),in,hybridqa,two weak supervised baselines ( s1 and s2 ) in hybridqa,0.5503516793251038
translation,11,135,results,u2 and u3,has,outperform,u2 and u3 has outperform,0.6667213439941406
translation,11,135,results,outperform,has,two weak supervised baselines ( s1 and s2 ),outperform has two weak supervised baselines ( s1 and s2 ),0.6011298298835754
translation,11,135,results,results,results of,u2 and u3,results results of u2 and u3,0.829052209854126
translation,11,146,results,a1 - a3,achieves,low performance,a1 - a3 achieves low performance,0.7148935794830322
translation,11,146,results,low performance,of,em and f1,low performance of em and f1,0.623614490032196
translation,11,146,results,low performance,especially for,in - passage questions,low performance especially for in - passage questions,0.6634406447410583
translation,11,146,results,results,has,a1 - a3,results has a1 - a3,0.5582126975059509
translation,11,148,results,hotpotqa,observe that,benefit,hotpotqa observe that benefit,0.5915158987045288
translation,11,148,results,benefit,of,multihop questions,benefit of multihop questions,0.5868999361991882
translation,11,148,results,multihop questions,is,not as evident,multihop questions is not as evident,0.6032940149307251
translation,11,148,results,squad - transfer ( u6 ),achieves,relatively good f1,squad - transfer ( u6 ) achieves relatively good f1,0.692865252494812
translation,11,148,results,relatively good f1,of,62.8,relatively good f1 of 62.8,0.5395299196243286
translation,11,148,results,hybridqa,has,squad - transfer ( u6 ),hybridqa has squad - transfer ( u6 ),0.6169652342796326
translation,11,148,results,results,for,hotpotqa,results for hotpotqa,0.5984999537467957
translation,11,151,results,model,using,"single reasoning type ( a4 , a5 )","model using single reasoning type ( a4 , a5 )",0.7005324959754944
translation,11,151,results,models,using,"single reasoning type ( a4 , a5 )","models using single reasoning type ( a4 , a5 )",0.6979983448982239
translation,11,151,results,both reasoning types,has,),both reasoning types has ),0.635890007019043
translation,11,151,results,both reasoning types,has,model,both reasoning types has model,0.6343709230422974
translation,11,151,results,),has,model,) has model,0.6174131631851196
translation,11,151,results,results,Using,both reasoning types,results Using both reasoning types,0.5966272950172424
translation,11,155,results,gpt - 2 based model,filter out,most ungrammatical questions,gpt - 2 based model filter out most ungrammatical questions,0.7697750926017761
translation,11,155,results,valid yet unnatural questions,such as,where was the event,valid yet unnatural questions such as where was the event,0.6720843315124512
translation,11,155,results,results,find that,gpt - 2 based model,results find that gpt - 2 based model,0.648483395576477
translation,11,162,results,our model,performs,consistently better,our model performs consistently better,0.6439617276191711
translation,11,162,results,consistently better,than,model,consistently better than model,0.5857095122337341
translation,11,162,results,without unsupervised pretraining,for,datasets,without unsupervised pretraining for datasets,0.6352023482322693
translation,11,162,results,progressively larger training dataset sizes,has,our model,progressively larger training dataset sizes has our model,0.5532786846160889
translation,11,162,results,results,With,progressively larger training dataset sizes,results With progressively larger training dataset sizes,0.6061909794807434
translation,11,181,results,performance drop,for,hybridqa and the hotpotqa dataset,performance drop for hybridqa and the hotpotqa dataset,0.5920080542564392
translation,11,181,results,performance drop,with,4.3 and 4.8 decrease,performance drop with 4.3 and 4.8 decrease,0.6576730608940125
translation,11,181,results,4.3 and 4.8 decrease,in,f1,4.3 and 4.8 decrease in f1,0.5426931381225586
translation,11,182,results,paraphrasing,produces,more fluent questions,paraphrasing produces more fluent questions,0.6170558929443359
translation,11,182,results,more fluent questions,by rewriting,redundancy parts,more fluent questions by rewriting redundancy parts,0.7591869235038757
translation,11,182,results,redundancy parts,of,original questions,redundancy parts of original questions,0.5827378630638123
translation,11,182,results,redundancy parts,into,more concise expression,redundancy parts into more concise expression,0.5823885798454285
translation,11,182,results,results,observe,paraphrasing,results observe paraphrasing,0.584345281124115
translation,12,184,ablation-analysis,relative gap,of,accuracy,relative gap of accuracy,0.6024258732795715
translation,12,184,ablation-analysis,accuracy,between,two models,accuracy between two models,0.6999247074127197
translation,12,184,ablation-analysis,category occurrence frequency,has,decreases,category occurrence frequency has decreases,0.599595844745636
translation,12,184,ablation-analysis,decreases,has,relative gap,decreases has relative gap,0.6134779453277588
translation,12,184,ablation-analysis,ablation analysis,As,category occurrence frequency,ablation analysis As category occurrence frequency,0.5213078856468201
translation,12,69,baselines,bart,for,text generation,bart for text generation,0.6418297290802002
translation,12,69,baselines,bart,better models,correlations,bart better models correlations,0.6852636933326721
translation,12,69,baselines,correlations,between,input sentence,correlations between input sentence,0.6525965929031372
translation,12,69,baselines,correlations,between,output sentence,correlations between output sentence,0.6512100696563721
translation,12,69,baselines,output sentence,compared with,bert,output sentence compared with bert,0.7177897691726685
translation,12,83,baselines,"bart ( lewis et al. , 2020 )",is,denoising auto-encoder seq2seq model pre-training,"bart ( lewis et al. , 2020 ) is denoising auto-encoder seq2seq model pre-training",0.5170955061912537
translation,12,83,baselines,denoising auto-encoder seq2seq model pre-training,for,natural language generation,denoising auto-encoder seq2seq model pre-training for natural language generation,0.5622490048408508
translation,12,83,baselines,baselines,has,"bart ( lewis et al. , 2020 )","baselines has bart ( lewis et al. , 2020 )",0.5562279224395752
translation,12,135,baselines,non-bert models,has,"bert ( devlin et al. , 2019 b ) based models","non-bert models has bert ( devlin et al. , 2019 b ) based models",0.576978325843811
translation,12,137,baselines,baselines,has,non-bert models,baselines has non-bert models,0.5626470446586609
translation,12,138,baselines,baselines,For,acd,baselines For acd,0.6252615451812744
translation,12,125,experimental-setup,pre-trained bert - base 1 and bartbase 2 models,for,task fine-tuning,pre-trained bert - base 1 and bartbase 2 models for task fine-tuning,0.5748986005783081
translation,12,125,experimental-setup,experimental setup,use,pre-trained bert - base 1 and bartbase 2 models,experimental setup use pre-trained bert - base 1 and bartbase 2 models,0.5926806330680847
translation,12,126,experimental-setup,fine-tuning learning rate,from,"{ 4e - 5 , 2e - 5 , and 1e - 5 }","fine-tuning learning rate from { 4e - 5 , 2e - 5 , and 1e - 5 }",0.5161382555961609
translation,12,126,experimental-setup,batch size,from,"{ 8 , 16 , 24 }","batch size from { 8 , 16 , 24 }",0.5727025866508484
translation,12,126,experimental-setup,experimental setup,select,fine-tuning learning rate,experimental setup select fine-tuning learning rate,0.6331475377082825
translation,12,127,experimental-setup,dropout probability,is,0.1,dropout probability is 0.1,0.5777223110198975
translation,12,127,experimental-setup,experimental setup,has,dropout probability,experimental setup has dropout probability,0.5022250413894653
translation,12,82,model,"bert ( devlin et al. , 2019a )",is,encoder stack of transformer,"bert ( devlin et al. , 2019a ) is encoder stack of transformer",0.5848799347877502
translation,12,82,model,encoder stack of transformer,for,masked text filling,encoder stack of transformer for masked text filling,0.6224284768104553
translation,12,82,model,model,uses,context words,model uses context words,0.6300502419471741
translation,12,82,model,context words,to predict,masked words,context words to predict masked words,0.6910263895988464
translation,12,82,model,model,uses,context words,model uses context words,0.6300502419471741
translation,12,82,model,model,has,"bert ( devlin et al. , 2019a )","model has bert ( devlin et al. , 2019a )",0.5377529263496399
translation,12,41,results,generation methods,give,stronger performances,generation methods give stronger performances,0.6148622632026672
translation,12,41,results,stronger performances,than,mlm methods,stronger performances than mlm methods,0.5569735169410706
translation,12,41,results,outperforming,by,large margin,outperforming by large margin,0.6037299036979675
translation,12,41,results,previous stateof - the - art methods,by,large margin,previous stateof - the - art methods by large margin,0.5316175222396851
translation,12,41,results,outperforming,has,previous stateof - the - art methods,outperforming has previous stateof - the - art methods,0.5346779823303223
translation,12,41,results,results,has,generation methods,results has generation methods,0.48877739906311035
translation,12,42,results,generation method,jointly performing,acsa and acd,generation method jointly performing acsa and acd,0.7308716177940369
translation,12,42,results,acsa and acd,leads to,better results,acsa and acd leads to better results,0.6963412165641785
translation,12,42,results,better results,than,traditional pipeline,better results than traditional pipeline,0.5908809900283813
translation,12,42,results,results,using,generation method,results using generation method,0.6700745820999146
translation,12,153,results,performance,of,bert mlm and bart mlm,performance of bert mlm and bart mlm,0.6179044246673584
translation,12,153,results,bert mlm and bart mlm,better than,bert classification and bart classification,bert mlm and bart mlm better than bert classification and bart classification,0.7277905941009521
translation,12,153,results,results,see that,performance,results see that performance,0.6803712844848633
translation,12,154,results,bert mlm,gives,strong baseline,bert mlm gives strong baseline,0.6548277139663696
translation,12,154,results,outperforming,has,all non-bert and bert classification baselines,outperforming has all non-bert and bert classification baselines,0.5941150784492493
translation,12,154,results,results,has,bert mlm,results has bert mlm,0.5887685418128967
translation,12,155,results,pre-training,at,task level,pre-training at task level,0.5185614824295044
translation,12,155,results,pre-training,achieve,better results,pre-training achieve better results,0.5992971658706665
translation,12,155,results,better results,than,representation level,better results than representation level,0.5903685688972473
translation,12,155,results,better results,at,representation level,better results at representation level,0.545349657535553
translation,12,155,results,results,making use of,pre-training,results making use of pre-training,0.6568514704704285
translation,12,156,results,bart mlm and classification models,perform,better,bart mlm and classification models perform better,0.664948046207428
translation,12,156,results,better,than,corresponding bert models,better than corresponding bert models,0.5926271677017212
translation,12,156,results,results,has,bart mlm and classification models,results has bart mlm and classification models,0.5140392780303955
translation,12,157,results,bart generation,indicates,our model,bart generation indicates our model,0.6618809700012207
translation,12,157,results,outperforms,on,all three datasets,outperforms on all three datasets,0.5061540603637695
translation,12,157,results,all baselines,on,all three datasets,all baselines on all three datasets,0.46537455916404724
translation,12,157,results,our model,better detect,multiple sentiment polarities,our model better detect multiple sentiment polarities,0.6894687414169312
translation,12,157,results,multiple sentiment polarities,in,one sentence,multiple sentiment polarities in one sentence,0.5179785490036011
translation,12,157,results,bart generation,has,outperforms,bart generation has outperforms,0.6594122648239136
translation,12,157,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,12,157,results,results,has,bart generation,results has bart generation,0.5180215835571289
translation,12,158,results,bart generation,performs,significantly better,bart generation performs significantly better,0.6319781541824341
translation,12,158,results,significantly better,than,bart mlm,significantly better than bart mlm,0.6163365840911865
translation,12,158,results,bart mlm,giving,absolutely 3.89 % stronger accuracy,bart mlm giving absolutely 3.89 % stronger accuracy,0.6849068999290466
translation,12,158,results,absolutely 3.89 % stronger accuracy,on,mams,absolutely 3.89 % stronger accuracy on mams,0.5938272476196289
translation,12,158,results,results,has,bart generation,results has bart generation,0.5180215835571289
translation,12,162,results,performances,of,our model,performances of our model,0.5975624322891235
translation,12,162,results,our model,on,documentlevel acsa,our model on documentlevel acsa,0.5496394038200378
translation,12,162,results,lstm,has,bert classification,lstm has bert classification,0.5835450887680054
translation,12,162,results,bart classification,has,outperform,bart classification has outperform,0.6525818705558777
translation,12,162,results,outperform,has,all baselines,outperform has all baselines,0.594290554523468
translation,12,162,results,results,Compared with,lstm,results Compared with lstm,0.6540087461471558
translation,12,162,results,results,has,performances,results has performances,0.5711642503738403
translation,12,163,results,bert mlm and bart mlm,surpass,bert classification,bert mlm and bart mlm surpass bert classification,0.6972001791000366
translation,12,163,results,bert mlm and bart mlm,surpass,bart classification,bert mlm and bart mlm surpass bart classification,0.6941776275634766
translation,12,163,results,results,has,bert mlm and bart mlm,results has bert mlm and bart mlm,0.6095256805419922
translation,12,164,results,bart generation model,achieves,improvements,bart generation model achieves improvements,0.6524233222007751
translation,12,164,results,improvements,of,1.15 % and 0.70 %,improvements of 1.15 % and 0.70 %,0.6006409525871277
translation,12,164,results,1.15 % and 0.70 %,over,bart mlm,1.15 % and 0.70 % over bart mlm,0.657271683216095
translation,12,164,results,bart mlm,on,tripadvisor and beeradvocate,bart mlm on tripadvisor and beeradvocate,0.6123795509338379
translation,12,164,results,results,has,bart generation model,results has bart generation model,0.5506054162979126
translation,12,166,results,more than 95 % precision score,shows that,our model,more than 95 % precision score shows that our model,0.700539231300354
translation,12,166,results,our model,effectively exclude,aspect categories,our model effectively exclude aspect categories,0.6204511523246765
translation,12,166,results,results,has,more than 95 % precision score,results has more than 95 % precision score,0.5738056302070618
translation,12,168,results,bart generation,has,outperforms,bart generation has outperforms,0.6594122648239136
translation,12,168,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,12,168,results,results,shows,bart generation,results shows bart generation,0.5979900360107422
translation,12,172,results,joint bart generation,achieves,better results,joint bart generation achieves better results,0.6429491639137268
translation,12,172,results,improvements,over,pipeline bart generation,improvements over pipeline bart generation,0.7092363238334656
translation,12,172,results,results,find that,joint bart generation,results find that joint bart generation,0.6055278182029724
translation,12,173,results,all baselines,on,"precision , recall and f - 1 score","all baselines on precision , recall and f - 1 score",0.4967237710952759
translation,12,173,results,joint bart generation,has,outperforms,joint bart generation has outperforms,0.6098934412002563
translation,12,173,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,12,173,results,results,has,joint bart generation,results has joint bart generation,0.5024233460426331
translation,12,177,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,12,177,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,12,177,results,results,has,our method,results has our method,0.5589964985847473
translation,12,178,results,model,trained on,mams,model trained on mams,0.7777659296989441
translation,12,178,results,better performance,on,rest14,better performance on rest14,0.5721586346626282
translation,12,178,results,better performance,than,reverse zero-shot setting,better performance than reverse zero-shot setting,0.5896418690681458
translation,12,178,results,rest14,than,reverse zero-shot setting,rest14 than reverse zero-shot setting,0.5957276821136475
translation,12,178,results,mams,has,better performance,mams has better performance,0.5905833840370178
translation,12,178,results,results,has,model,results has model,0.5339115858078003
translation,12,185,results,zero frequency,gives,absolutely 8.03 % stronger accuracy,zero frequency gives absolutely 8.03 % stronger accuracy,0.6215311288833618
translation,12,185,results,our method,gives,absolutely 8.03 % stronger accuracy,our method gives absolutely 8.03 % stronger accuracy,0.5998790860176086
translation,12,185,results,absolutely 8.03 % stronger accuracy,than,bart classification,absolutely 8.03 % stronger accuracy than bart classification,0.5890213847160339
translation,12,185,results,zero frequency,has,our method,zero frequency has our method,0.5935516357421875
translation,12,185,results,results,In,zero frequency,results In zero frequency,0.5548290610313416
translation,13,34,ablation-analysis,graphmerge,introduces,more edges,graphmerge introduces more edges,0.6489356160163879
translation,13,34,ablation-analysis,more edges,in,graph,more edges in graph,0.5469774007797241
translation,13,34,ablation-analysis,more edges,when,parses differ,more edges when parses differ,0.676918089389801
translation,13,34,ablation-analysis,more edges,reduces,diameter of graphs,more edges reduces diameter of graphs,0.6399663686752319
translation,13,113,ablation-analysis,2 - layer gat / rgat models,turn out to be,best,2 - layer gat / rgat models turn out to be best,0.6270677447319031
translation,13,113,ablation-analysis,best,based on,dev set,best based on dev set,0.6246178150177002
translation,13,113,ablation-analysis,ablation analysis,has,2 - layer gat / rgat models,ablation analysis has 2 - layer gat / rgat models,0.5259899497032166
translation,13,123,ablation-analysis,syntax information,benefits,aspect-level sentiment classification,syntax information benefits aspect-level sentiment classification,0.5761436223983765
translation,13,123,ablation-analysis,ablation analysis,observe,syntax information,ablation analysis observe syntax information,0.6177419424057007
translation,13,137,ablation-analysis,three datasets,ablating,edge type,three datasets ablating edge type,0.5864700675010681
translation,13,137,ablation-analysis,edge type,degrades,performances,edge type degrades performances,0.8092421293258667
translation,13,137,ablation-analysis,ablation analysis,On,three datasets,ablation analysis On three datasets,0.5263423323631287
translation,13,138,ablation-analysis,syntactic dependency information,in,original dependency trees,syntactic dependency information in original dependency trees,0.4614669680595398
translation,13,138,ablation-analysis,syntactic dependency information,is,important,syntactic dependency information is important,0.5130676031112671
translation,13,138,ablation-analysis,ablation analysis,indicates that,syntactic dependency information,ablation analysis indicates that syntactic dependency information,0.6446502804756165
translation,13,140,ablation-analysis,position embeddings,hurts,performances,position embeddings hurts performances,0.6309233903884888
translation,13,140,ablation-analysis,ablation analysis,Removing,position embeddings,ablation analysis Removing position embeddings,0.7285666465759277
translation,13,153,ablation-analysis,single dependency tree,has,ensemble graph,single dependency tree has ensemble graph,0.5672294497489929
translation,13,153,ablation-analysis,ablation analysis,Compared with,single dependency tree,ablation analysis Compared with single dependency tree,0.6407431364059448
translation,13,154,ablation-analysis,shorter distance,between,term and opinion words,shorter distance between term and opinion words,0.6732131838798523
translation,13,154,ablation-analysis,term and opinion words,correlates with,better performance,term and opinion words correlates with better performance,0.6707562804222107
translation,13,154,ablation-analysis,ablation analysis,has,shorter distance,ablation analysis has shorter distance,0.522000253200531
translation,13,95,baselines,bert - spc,feeds,sentence and term pair,bert - spc feeds sentence and term pair,0.7239512205123901
translation,13,95,baselines,bert - spc,uses,bert outputs,bert - spc uses bert outputs,0.6253134608268738
translation,13,95,baselines,sentence and term pair,into,bert model,sentence and term pair into bert model,0.5577216148376465
translation,13,95,baselines,bert outputs,for,predictions,bert outputs for predictions,0.6758744120597839
translation,13,95,baselines,aen - bert,uses,bert,aen - bert uses bert,0.5615391135215759
translation,13,95,baselines,aen - bert,employs,several attention layers,aen - bert employs several attention layers,0.5660980939865112
translation,13,95,baselines,bert,as,encoder,bert as encoder,0.6353098154067993
translation,13,96,baselines,baselines,has,bert + dependency tree based models,baselines has bert + dependency tree based models,0.5668759346008301
translation,13,97,baselines,mutual biaffine module,to jointly consider,representations,mutual biaffine module to jointly consider representations,0.7026571035385132
translation,13,97,baselines,representations,learnt from,transformer and the gnn model,representations learnt from transformer and the gnn model,0.6941769123077393
translation,13,97,baselines,transformer and the gnn model,over,dependency tree,transformer and the gnn model over dependency tree,0.6496867537498474
translation,13,97,baselines,dependency tree,to,aspect-oriented tree,dependency tree to aspect-oriented tree,0.5231736898422241
translation,13,97,baselines,r-gat +bert,prunes,dependency tree,r-gat +bert prunes dependency tree,0.7394455671310425
translation,13,97,baselines,r-gat +bert,employs,rgat,r-gat +bert employs rgat,0.5867908596992493
translation,13,97,baselines,dependency tree,to,aspect-oriented tree,dependency tree to aspect-oriented tree,0.5231736898422241
translation,13,97,baselines,aspect-oriented tree,rooted at,aspect term,aspect-oriented tree rooted at aspect term,0.6586771607398987
translation,13,97,baselines,rgat,to encode,new tree,rgat to encode new tree,0.6977490782737732
translation,13,97,baselines,new tree,for,predictions,new tree for predictions,0.6316385865211487
translation,13,101,baselines,bert - baseline,feeds,sentence - term pair,bert - baseline feeds sentence - term pair,0.6954872012138367
translation,13,101,baselines,bert - baseline,applies,classifier,bert - baseline applies classifier,0.6293723583221436
translation,13,101,baselines,sentence - term pair,into,bert - base encoder,sentence - term pair into bert - base encoder,0.5488107204437256
translation,13,101,baselines,classifier,representation of,aspect term token,classifier representation of aspect term token,0.6592655181884766
translation,13,102,baselines,gat - baseline,with,stanza,gat - baseline with stanza,0.7077210545539856
translation,13,102,baselines,gat - baseline,with,stanza,gat - baseline with stanza,0.7077210545539856
translation,13,102,baselines,gat - baseline,employs,vanilla gat model,gat - baseline employs vanilla gat model,0.5407320261001587
translation,13,102,baselines,stanza,employs,vanilla gat model,stanza employs vanilla gat model,0.6040297746658325
translation,13,102,baselines,vanilla gat model,over,single dependency tree,vanilla gat model over single dependency tree,0.6665742993354797
translation,13,102,baselines,single dependency tree,obtained from,stanza,single dependency tree obtained from stanza,0.6180137395858765
translation,13,102,baselines,single dependency tree,without differentiating,edge types,single dependency tree without differentiating edge types,0.7251777648925781
translation,13,102,baselines,baselines,has,gat - baseline,baselines has gat - baseline,0.6142918467521667
translation,13,104,baselines,rgat,over,single dependency trees,rgat over single dependency trees,0.6040448546409607
translation,13,104,baselines,rgat,apply,rgat models,rgat apply rgat models,0.6659663319587708
translation,13,104,baselines,single dependency trees,apply,rgat models,single dependency trees apply rgat models,0.5871633291244507
translation,13,104,baselines,rgat models,with,parent-to-children and child - to - parent edge types,rgat models with parent-to-children and child - to - parent edge types,0.6287530660629272
translation,13,104,baselines,parent-to-children and child - to - parent edge types,over,different dependency trees,parent-to-children and child - to - parent edge types over different dependency trees,0.6478924751281738
translation,13,104,baselines,different dependency trees,from,"corenlp , stanza , and berkeley parsers","different dependency trees from corenlp , stanza , and berkeley parsers",0.5283025503158569
translation,13,104,baselines,baselines,has,rgat,baselines has rgat,0.5295360684394836
translation,13,75,hyperparameters,initial word node features,for,rgat,initial word node features for rgat,0.5640146136283875
translation,13,75,hyperparameters,initial word node features,obtained from,bert encoder,initial word node features obtained from bert encoder,0.5494562983512878
translation,13,75,hyperparameters,rgat,obtained from,bert encoder,rgat obtained from bert encoder,0.5874054431915283
translation,13,75,hyperparameters,bert encoder,with,positional information,bert encoder with positional information,0.6504552364349365
translation,13,75,hyperparameters,positional information,from,positional embeddings,positional information from positional embeddings,0.5405680537223816
translation,13,75,hyperparameters,hyperparameters,has,initial word node features,hyperparameters has initial word node features,0.4552958011627197
translation,13,110,hyperparameters,gat implementation,based on,deep graph library,gat implementation based on deep graph library,0.586361289024353
translation,13,110,hyperparameters,hyperparameters,has,gat implementation,hyperparameters has gat implementation,0.491947203874588
translation,13,111,hyperparameters,training,set,learning rate,training set learning rate,0.6812386512756348
translation,13,111,hyperparameters,training,set,batch size,training set batch size,0.6989448666572571
translation,13,111,hyperparameters,learning rate,=,10 ?5,learning rate = 10 ?5,0.6754002571105957
translation,13,111,hyperparameters,batch size,=,4,batch size = 4,0.668002188205719
translation,13,111,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,13,112,hyperparameters,dev data,to select,hidden dimension d h,dev data to select hidden dimension d h,0.6728765368461609
translation,13,112,hyperparameters,dev data,to select,head number,dev data to select head number,0.644463837146759
translation,13,112,hyperparameters,dev data,to select,gat / rgat layer,dev data to select gat / rgat layer,0.666178286075592
translation,13,112,hyperparameters,hidden dimension d h,for,gat / rgat,hidden dimension d h for gat / rgat,0.6142634153366089
translation,13,112,hyperparameters,hidden dimension d h,for,gat / rgat layer,hidden dimension d h for gat / rgat layer,0.5545336008071899
translation,13,112,hyperparameters,gat / rgat,from,"{ 64 , 128 , 256 }","gat / rgat from { 64 , 128 , 256 }",0.5700340867042542
translation,13,112,hyperparameters,head number,in,multi-head self-attention,head number in multi-head self-attention,0.5367976427078247
translation,13,112,hyperparameters,multi-head self-attention,from,"{ 4 , 8 }","multi-head self-attention from { 4 , 8 }",0.5801582336425781
translation,13,112,hyperparameters,gat / rgat layer,from,"{ 2 , 3 , 4 }","gat / rgat layer from { 2 , 3 , 4 }",0.5841525793075562
translation,13,112,hyperparameters,hyperparameters,use,dev data,hyperparameters use dev data,0.6299360394477844
translation,13,114,hyperparameters,best setting,from,"dropout rate range = [ 0.1 , 0.3 ]","best setting from dropout rate range = [ 0.1 , 0.3 ]",0.5072281360626221
translation,13,114,hyperparameters,hyperparameters,apply,"dropout ( srivastava et al. , 2014 )","hyperparameters apply dropout ( srivastava et al. , 2014 )",0.5427748560905457
translation,13,114,hyperparameters,hyperparameters,select,best setting,hyperparameters select best setting,0.6363351941108704
translation,13,115,hyperparameters,weight,of,l2 regularization,weight of l2 regularization,0.5119404792785645
translation,13,115,hyperparameters,l2 regularization,as,10 ?6,l2 regularization as 10 ?6,0.5382182002067566
translation,13,115,hyperparameters,hyperparameters,set,weight,hyperparameters set weight,0.6553722620010376
translation,13,116,hyperparameters,model,up to,5 epochs,model up to 5 epochs,0.6747093200683594
translation,13,116,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,13,5,model,syntactic information,propose,simple yet effective graph ensemble technique,syntactic information propose simple yet effective graph ensemble technique,0.5922301411628723
translation,13,5,model,simple yet effective graph ensemble technique,to make use of,predictions,simple yet effective graph ensemble technique to make use of predictions,0.686728298664093
translation,13,5,model,predictions,from,different parsers,predictions from different parsers,0.551298201084137
translation,13,5,model,syntactic information,has,in the face of unavoidable errors,syntactic information has in the face of unavoidable errors,0.5270102620124817
translation,13,5,model,model,propose,simple yet effective graph ensemble technique,model propose simple yet effective graph ensemble technique,0.6448866724967957
translation,13,6,model,dependency relations,from,different parses,dependency relations from different parses,0.5435981154441833
translation,13,6,model,different parses,before applying,gnns,different parses before applying gnns,0.7043335437774658
translation,13,6,model,gnns,over,resulting graph,gnns over resulting graph,0.6442453861236572
translation,13,22,model,technique,to help,dependency - based models,technique to help dependency - based models,0.6221726536750793
translation,13,22,model,dependency - based models,mitigate,effect of parsing errors,dependency - based models mitigate effect of parsing errors,0.6249195337295532
translation,13,22,model,model,propose,graphmerge,model propose graphmerge,0.6786664724349976
translation,13,23,model,model,based on,observation,model based on observation,0.7373062372207642
translation,13,27,model,gnn model,to,various graph hypotheses,gnn model to various graph hypotheses,0.5300034284591675
translation,13,27,model,edges,contribute more to,task,edges contribute more to task,0.7022247314453125
translation,13,27,model,model,exposes,gnn model,model exposes gnn model,0.6659009456634521
translation,13,28,model,syntactic dependency information,between,words,syntactic dependency information between words,0.5770754218101501
translation,13,28,model,syntactic dependency information,define,two different edge typesparent - to-children and children - to - parent,syntactic dependency information define two different edge typesparent - to-children and children - to - parent,0.5978922843933105
translation,13,28,model,relational graph attention networks ( rgat ),on,ensemble graph,relational graph attention networks ( rgat ) on ensemble graph,0.5421529412269592
translation,13,28,model,model,To retain,syntactic dependency information,model To retain syntactic dependency information,0.6213370561599731
translation,13,68,model,relational gat ( rgat ),to capture,edge type information,relational gat ( rgat ) to capture edge type information,0.7020111680030823
translation,13,68,model,model,adapt,relational gat ( rgat ),model adapt relational gat ( rgat ),0.8328621983528137
translation,13,126,model,ensemble models,benefit from,multiple parses,ensemble models benefit from multiple parses,0.6368953585624695
translation,13,126,model,model,has,ensemble models,model has ensemble models,0.5510801672935486
translation,13,139,model,differentiating edges,in,ensemble graph,differentiating edges in ensemble graph,0.5581567287445068
translation,13,139,model,differentiating edges,provides,more guidance,differentiating edges provides more guidance,0.6617158651351929
translation,13,139,model,more guidance,to,model,more guidance to model,0.6029775142669678
translation,13,139,model,model,about selecting,useful connections,model about selecting useful connections,0.6762166619300842
translation,13,139,model,useful connections,among,nodes,useful connections among nodes,0.6237766146659851
translation,13,139,model,model,has,differentiating edges,model has differentiating edges,0.5793299078941345
translation,13,141,model,bert encoder,incorporates,position information,bert encoder incorporates position information,0.7318935394287109
translation,13,141,model,position information,at,input,position information at input,0.5628607869148254
translation,13,141,model,dampened,over,layers of transformers,dampened over layers of transformers,0.6848694086074829
translation,13,141,model,model,has,bert encoder,model has bert encoder,0.6133244037628174
translation,13,120,results,graphmerge model,achieves,best performances,graphmerge model achieves best performances,0.6310296654701233
translation,13,120,results,best performances,on,all three datasets,best performances on all three datasets,0.44413381814956665
translation,13,120,results,results,see,graphmerge model,results see graphmerge model,0.5556976199150085
translation,13,120,results,results,has,graphmerge model,results has graphmerge model,0.5100879669189453
translation,13,121,results,graphmerge model,outperforms,baselines,graphmerge model outperforms baselines,0.7677662372589111
translation,13,121,results,baselines,by,at least 1.42 accuracy and 2.34 macro - f1,baselines by at least 1.42 accuracy and 2.34 macro - f1,0.558833658695221
translation,13,121,results,laptop dataset,has,graphmerge model,laptop dataset has graphmerge model,0.5173071622848511
translation,13,121,results,results,On,laptop dataset,results On laptop dataset,0.5323125123977661
translation,13,122,results,performance comparisons,of,graphmerge model,performance comparisons of graphmerge model,0.5822098851203918
translation,13,124,results,gat and rgat models,based on,dependency trees,gat and rgat models based on dependency trees,0.6418083310127258
translation,13,124,results,dependency trees,has,outperform,dependency trees has outperform,0.6000188589096069
translation,13,124,results,outperform,has,bert - baseline,outperform has bert - baseline,0.6434299349784851
translation,13,124,results,results,has,gat and rgat models,results has gat and rgat models,0.4795772135257721
translation,13,127,results,"label-ensemble , feature-ensemble , and graphmerge models",achieve,better performance,"label-ensemble , feature-ensemble , and graphmerge models achieve better performance",0.5993665456771851
translation,13,127,results,better performance,compared to,single dependency tree counterparts,better performance compared to single dependency tree counterparts,0.6869451999664307
translation,13,127,results,results,has,"label-ensemble , feature-ensemble , and graphmerge models","results has label-ensemble , feature-ensemble , and graphmerge models",0.4625625014305115
translation,13,129,results,graphmerge,achieves,best performance,graphmerge achieves best performance,0.6629643440246582
translation,13,129,results,results,has,graphmerge,results has graphmerge,0.5256472229957581
translation,13,130,results,proposed graphmerge model,shows,consistent improvements,proposed graphmerge model shows consistent improvements,0.6712770462036133
translation,13,130,results,consistent improvements,over,all single dependency tree models,consistent improvements over all single dependency tree models,0.6865928173065186
translation,13,130,results,other two ensemble models,without,additional parameters or computational overhead,other two ensemble models without additional parameters or computational overhead,0.6997092962265015
translation,13,130,results,other two ensemble models,compared to,single - tree models,other two ensemble models compared to single - tree models,0.6130866408348083
translation,13,130,results,surpasses,has,other two ensemble models,surpasses has other two ensemble models,0.582628071308136
translation,13,130,results,results,has,proposed graphmerge model,results has proposed graphmerge model,0.5797458291053772
translation,13,142,results,sequence order,before applying,rgat,sequence order before applying rgat,0.691429615020752
translation,13,142,results,rgat,benefits,task,rgat benefits task,0.6302824020385742
translation,13,142,results,results,Emphasizing,sequence order,results Emphasizing sequence order,0.6794664859771729
translation,13,155,results,accuracy,of,one- hop and two -hops cases,accuracy of one- hop and two -hops cases,0.6453485488891602
translation,13,155,results,accuracy,beats,all single dependency tree models,accuracy beats all single dependency tree models,0.7184486985206604
translation,13,155,results,one- hop and two -hops cases,beats,all single dependency tree models,one- hop and two -hops cases beats all single dependency tree models,0.6798479557037354
translation,13,155,results,ensemble graph,has,accuracy,ensemble graph has accuracy,0.5660654306411743
translation,13,155,results,results,With,ensemble graph,results With ensemble graph,0.6587294340133667
translation,13,157,results,shortening distance,correlates with,improved results,shortening distance correlates with improved results,0.7114192247390747
translation,13,157,results,shortening distance,not mean,closer distance,shortening distance not mean closer distance,0.7207760214805603
translation,13,157,results,closer distance,sufficient for,better performance,closer distance sufficient for better performance,0.7484618425369263
translation,14,85,baselines,aspect extraction,in,unified framework,aspect extraction in unified framework,0.5178180932998657
translation,14,85,baselines,unified framework,by attaching,aspect category,unified framework by attaching aspect category,0.6351127028465271
translation,14,85,baselines,unified framework,by attaching,sentiment polarity,unified framework by attaching sentiment polarity,0.6948440670967102
translation,14,85,baselines,sentiment polarity,to,review sentence,sentiment polarity to review sentence,0.5173960328102112
translation,14,85,baselines,input,of,bert,input of bert,0.6541942358016968
translation,14,85,baselines,baselines,has,tas - bert - acos,baselines has tas - bert - acos,0.6091861724853516
translation,14,12,experiments,https,:,//github.com / nustm /acos,https : //github.com / nustm /acos,0.6188725829124451
translation,14,37,experiments,representative approaches,in,aspect-opinion pair extraction,representative approaches in aspect-opinion pair extraction,0.48907455801963806
translation,14,37,experiments,representative approaches,in,aspect-category -opinion triple extraction,representative approaches in aspect-category -opinion triple extraction,0.5020285248756409
translation,14,37,experiments,representative approaches,in,aspect-opinion -sentiment triple extraction,representative approaches in aspect-opinion -sentiment triple extraction,0.5115067362785339
translation,14,37,experiments,representative approaches,to,acos quadruple extraction,representative approaches to acos quadruple extraction,0.5630016326904297
translation,14,210,experiments,tas - bert -acos,support,acos quadru-ple extraction,tas - bert -acos support acos quadru-ple extraction,0.5680415034294128
translation,14,210,experiments,extract- classify - acos,support,acos quadru-ple extraction,extract- classify - acos support acos quadru-ple extraction,0.5987457633018494
translation,14,210,experiments,acos quadru-ple extraction,in case of,implicit aspects,acos quadru-ple extraction in case of implicit aspects,0.671870768070221
translation,14,210,experiments,acos quadru-ple extraction,in case of,implicit opinions,acos quadru-ple extraction in case of implicit opinions,0.6316709518432617
translation,14,192,hyperparameters,training,use,adamw optimizer,training use adamw optimizer,0.6061614155769348
translation,14,192,hyperparameters,adamw optimizer,of,bert,adamw optimizer of bert,0.5866798162460327
translation,14,192,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,14,193,hyperparameters,maximum length,of,review sentence,maximum length of review sentence,0.5611175894737244
translation,14,193,hyperparameters,review sentence,set to,128,review sentence set to 128,0.7016410827636719
translation,14,193,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,14,194,hyperparameters,batch size and learning rates,in,aspect opinion co-extraction and category - sentiment classification,batch size and learning rates in aspect opinion co-extraction and category - sentiment classification,0.5312118530273438
translation,14,194,hyperparameters,aspect opinion co-extraction and category - sentiment classification,as,"[ 32 , 2e - 5 ] and [ 16 , 3e - 5 ]","aspect opinion co-extraction and category - sentiment classification as [ 32 , 2e - 5 ] and [ 16 , 3e - 5 ]",0.531126081943512
translation,14,194,hyperparameters,hyperparameters,set,batch size and learning rates,hyperparameters set batch size and learning rates,0.5781989693641663
translation,14,195,hyperparameters,dropout rate,set as,0.1,dropout rate set as 0.1,0.568564772605896
translation,14,195,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,14,196,hyperparameters,batch size and learning rate,in,category classification,batch size and learning rate in category classification,0.5192984938621521
translation,14,196,hyperparameters,batch size and learning rate,in,aspect-opinion pair filtering,batch size and learning rate in aspect-opinion pair filtering,0.5212805867195129
translation,14,196,hyperparameters,category classification,of,jet - acos,category classification of jet - acos,0.5875045657157898
translation,14,196,hyperparameters,aspect-opinion pair filtering,in,tas - bert - acos,aspect-opinion pair filtering in tas - bert - acos,0.5711202621459961
translation,14,196,hyperparameters,tas - bert - acos,set as,"[ 8 , 5e - 5 ]","tas - bert - acos set as [ 8 , 5e - 5 ]",0.6662594676017761
translation,14,196,hyperparameters,hyperparameters,has,batch size and learning rate,hyperparameters has batch size and learning rate,0.4734242260456085
translation,14,71,model,model,has,double-propagation-acos,model has double-propagation-acos,0.6019628047943115
translation,14,106,results,double-propagation - acos,gets,lowest performance,double-propagation - acos gets lowest performance,0.6062524318695068
translation,14,106,results,results,seen that,double-propagation - acos,results seen that double-propagation - acos,0.6688631176948547
translation,14,108,results,jet -acos and tas - bert - acos,achieve,comparable f 1 performance,jet -acos and tas - bert - acos achieve comparable f 1 performance,0.6202911138534546
translation,14,108,results,better,on,restaurant - acos dataset,better on restaurant - acos dataset,0.5340449213981628
translation,14,108,results,better,on,laptop - acos,better on laptop - acos,0.5874112844467163
translation,14,108,results,better,on,laptop - acos,better on laptop - acos,0.5874112844467163
translation,14,108,results,better,on,laptop - acos,better on laptop - acos,0.5874112844467163
translation,14,108,results,comparable f 1 performance,has,former,comparable f 1 performance has former,0.5953531265258789
translation,14,108,results,results,has,jet -acos and tas - bert - acos,results has jet -acos and tas - bert - acos,0.5225589871406555
translation,14,109,results,extract - classify -acos,achieves,best performance,extract - classify -acos achieves best performance,0.7148285508155823
translation,14,109,results,best performance,among,four baseline systems,best performance among four baseline systems,0.5579500198364258
translation,14,109,results,results,has,extract - classify -acos,results has extract - classify -acos,0.6461969614028931
translation,14,110,results,jet -acos,by,5.60 percentage points,jet -acos by 5.60 percentage points,0.5927243232727051
translation,14,110,results,jet -acos,by,8.49 percentage points,jet -acos by 8.49 percentage points,0.5910024046897888
translation,14,110,results,5.60 percentage points,on,restaurant - acos,5.60 percentage points on restaurant - acos,0.526894211769104
translation,14,110,results,tas - bert - acos,by,8.49 percentage points,tas - bert - acos by 8.49 percentage points,0.6036697626113892
translation,14,110,results,8.49 percentage points,on,laptop - acos,8.49 percentage points on laptop - acos,0.5038983821868896
translation,14,110,results,outperforms,has,jet -acos,outperforms has jet -acos,0.6036844253540039
translation,14,110,results,outperforms,has,tas - bert - acos,outperforms has tas - bert - acos,0.596897304058075
translation,14,110,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,14,113,results,f 1 score,of,extract - classify - acos,f 1 score of extract - classify - acos,0.6259444952011108
translation,14,113,results,extract - classify - acos,on,both datasets,extract - classify - acos on both datasets,0.5261023044586182
translation,14,119,results,double-propagation - acos and jet - acos,address,ea & eo,double-propagation - acos and jet - acos address ea & eo,0.5965660810470581
translation,14,119,results,tas - bert - acos and extract - classify - acos,support,implicit aspects,tas - bert - acos and extract - classify - acos support implicit aspects,0.6050727963447571
translation,14,119,results,tas - bert - acos and extract - classify - acos,support,implicit opinions,tas - bert - acos and extract - classify - acos support implicit opinions,0.5731101632118225
translation,14,121,results,extract - classify -acos,is,better,extract - classify -acos is better,0.6460993885993958
translation,14,121,results,extract - classify -acos,is,better,extract - classify -acos is better,0.6460993885993958
translation,14,121,results,extract - classify -acos,is,better,extract - classify -acos is better,0.6460993885993958
translation,14,121,results,better,in case of,ia,better in case of ia,0.706329345703125
translation,14,121,results,better,in case of,& eo,better in case of & eo,0.719199538230896
translation,14,121,results,better,in case of,ea & io,better in case of ea & io,0.758070707321167
translation,14,121,results,better,in case of,ia,better in case of ia,0.706329345703125
translation,14,121,results,better,in case of,& eo,better in case of & eo,0.719199538230896
translation,14,121,results,better,in case of,ia & io,better in case of ia & io,0.7592742443084717
translation,14,121,results,better,in case of,ia,better in case of ia,0.706329345703125
translation,14,121,results,better,in case of,& eo,better in case of & eo,0.719199538230896
translation,14,121,results,better,in case of,ia & io,better in case of ia & io,0.7592742443084717
translation,14,121,results,ia,on,restaurant - acos,ia on restaurant - acos,0.5859329104423523
translation,14,121,results,ia,on,laptop - acos,ia on laptop - acos,0.5437440276145935
translation,14,121,results,ia,is,& eo,ia is & eo,0.7506093382835388
translation,14,121,results,ia,in case of,& eo,ia in case of & eo,0.7459483742713928
translation,14,121,results,ia,on,laptop - acos,ia on laptop - acos,0.5437440276145935
translation,14,121,results,ea & io,on,restaurant - acos,ea & io on restaurant - acos,0.602824330329895
translation,14,121,results,tas - bert - acos,is,better,tas - bert - acos is better,0.6391618251800537
translation,14,121,results,better,in case of,ia,better in case of ia,0.706329345703125
translation,14,121,results,better,in case of,& eo,better in case of & eo,0.719199538230896
translation,14,121,results,better,in case of,ia & io,better in case of ia & io,0.7592742443084717
translation,14,121,results,ia,on,laptop - acos,ia on laptop - acos,0.5437440276145935
translation,14,121,results,ia & io,on,laptop - acos,ia & io on laptop - acos,0.582854688167572
translation,14,121,results,ia,has,& eo,ia has & eo,0.6866044998168945
translation,14,121,results,ia,has,& eo,ia has & eo,0.6866044998168945
translation,14,121,results,results,has,extract - classify -acos,results has extract - classify -acos,0.6461969614028931
translation,14,122,results,extract - classify -acos,performs,significantly better,extract - classify -acos performs significantly better,0.6822836399078369
translation,14,122,results,significantly better,in case of,ea & eo,significantly better in case of ea & eo,0.7223889231681824
translation,14,122,results,significantly better,in case of,two datasets,significantly better in case of two datasets,0.6589511632919312
translation,14,122,results,ea & eo,on,two datasets,ea & eo on two datasets,0.5271738171577454
translation,14,122,results,results,has,extract - classify -acos,results has extract - classify -acos,0.6461969614028931
translation,14,211,results,tas - bert - acos,performs,better,tas - bert - acos performs better,0.6742004752159119
translation,14,211,results,better,than,jet - acos,better than jet - acos,0.6253189444541931
translation,14,211,results,fails,case of,ia & eo,fails case of ia & eo,0.746558427810669
translation,14,211,results,results,has,tas - bert - acos,results has tas - bert - acos,0.5513836145401001
translation,14,212,results,extract - classify -acos,performs,best,extract - classify -acos performs best,0.6663133502006531
translation,14,212,results,extract - classify -acos,produces,more accurate predictions,extract - classify -acos produces more accurate predictions,0.6914169788360596
translation,14,212,results,more accurate predictions,in,all cases,more accurate predictions in all cases,0.5192742943763733
translation,14,212,results,results,has,extract - classify -acos,results has extract - classify -acos,0.6461969614028931
translation,15,7,model,multi-modal joint learning approach,with,auxiliary cross-modal relation detection,multi-modal joint learning approach with auxiliary cross-modal relation detection,0.5984765291213989
translation,15,7,model,auxiliary cross-modal relation detection,for,multi-modal aspect-level sentiment analysis ( malsa ),auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis ( malsa ),0.5983315706253052
translation,15,7,model,model,jointly perform,multi-modal ate ( mate ),model jointly perform multi-modal ate ( mate ),0.7019463181495667
translation,15,7,model,model,propose,multi-modal joint learning approach,model propose multi-modal joint learning approach,0.6546975374221802
translation,15,8,model,model,build,auxiliary text-image relation detection module,model build auxiliary text-image relation detection module,0.7121216654777527
translation,15,9,model,hierarchical framework,to bridge,multi-modal connection,hierarchical framework to bridge multi-modal connection,0.5640121102333069
translation,15,9,model,multi-modal connection,between,mate and masc,multi-modal connection between mate and masc,0.6692202687263489
translation,15,9,model,model,adopt,hierarchical framework,model adopt hierarchical framework,0.7038043141365051
translation,15,31,model,multi-modal joint learning approach,with,auxiliary cross-modal relation detection,multi-modal joint learning approach with auxiliary cross-modal relation detection,0.5984765291213989
translation,15,31,model,auxiliary cross-modal relation detection,namely,jml,auxiliary cross-modal relation detection namely jml,0.6903570294380188
translation,15,31,model,model,propose,multi-modal joint learning approach,model propose multi-modal joint learning approach,0.6546975374221802
translation,15,32,model,module,of,auxiliary cross-modal relation detection,module of auxiliary cross-modal relation detection,0.5417866706848145
translation,15,32,model,model,design,module,model design module,0.6778693199157715
translation,15,33,model,joint hierarchical framework,to separately attend to,effective visual information,joint hierarchical framework to separately attend to effective visual information,0.6688142418861389
translation,15,33,model,effective visual information,for,each sub-task,effective visual information for each sub-task,0.585896372795105
translation,15,33,model,each sub-task,instead of,collapsed tagging framework,each sub-task instead of collapsed tagging framework,0.6446578502655029
translation,15,33,model,model,leverage,joint hierarchical framework,model leverage joint hierarchical framework,0.711628794670105
translation,16,166,ablation-analysis,deployment,of,supervised contrastive learning objective,deployment of supervised contrastive learning objective,0.5226244330406189
translation,16,166,ablation-analysis,supervised contrastive learning objective,enhances,noise immunity,supervised contrastive learning objective enhances noise immunity,0.6298536062240601
translation,16,166,ablation-analysis,noise immunity,of,pre-training process,noise immunity of pre-training process,0.5706832408905029
translation,16,166,ablation-analysis,more effective,in learning,implicit sentiment,more effective in learning implicit sentiment,0.6686474680900574
translation,16,166,ablation-analysis,ablation analysis,has,deployment,ablation analysis has deployment,0.5221257209777832
translation,16,169,ablation-analysis,supervised contrastive learning loss ( - scl ),leads to,2.38 % performance drop,supervised contrastive learning loss ( - scl ) leads to 2.38 % performance drop,0.5976243019104004
translation,16,169,ablation-analysis,2.38 % performance drop,on,restaurant,2.38 % performance drop on restaurant,0.5399888157844543
translation,16,169,ablation-analysis,ablation analysis,removing,supervised contrastive learning loss ( - scl ),ablation analysis removing supervised contrastive learning loss ( - scl ),0.7084594368934631
translation,16,170,ablation-analysis,supervised contrastive learning,plays,primary role,supervised contrastive learning plays primary role,0.7317404747009277
translation,16,170,ablation-analysis,primary role,in,scapt,primary role in scapt,0.5582684278488159
translation,16,170,ablation-analysis,ablation analysis,verifies,supervised contrastive learning,ablation analysis verifies supervised contrastive learning,0.6188238263130188
translation,16,171,ablation-analysis,masked aspect prediction and review reconstruction objectives,brings about,performance drop,masked aspect prediction and review reconstruction objectives brings about performance drop,0.674258291721344
translation,16,171,ablation-analysis,ablation analysis,removing of,masked aspect prediction and review reconstruction objectives,ablation analysis removing of masked aspect prediction and review reconstruction objectives,0.6514630317687988
translation,16,126,baselines,knowledge-enhanced methods,has,"transcap ( chen and qian , 2019 )","knowledge-enhanced methods has transcap ( chen and qian , 2019 )",0.5757293701171875
translation,16,130,baselines,bertasp,Directly apply,aspect-aware finetuning,bertasp Directly apply aspect-aware finetuning,0.6333712339401245
translation,16,130,baselines,aspect-aware finetuning,on,bert - base,aspect-aware finetuning on bert - base,0.5649110078811646
translation,16,130,baselines,baselines,has,bertasp,baselines has bertasp,0.621363639831543
translation,16,142,experiments,transencasp+scapt,performs,better,transencasp+scapt performs better,0.7267311215400696
translation,16,142,experiments,better,than,most baselines,better than most baselines,0.5645020008087158
translation,16,142,experiments,most baselines,without,pre-trained knowledge,most baselines without pre-trained knowledge,0.6873877644538879
translation,16,147,experiments,scapt,good at learning,implicit sentiment,scapt good at learning implicit sentiment,0.7854706645011902
translation,16,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,6 layers,300 dimensional randomly initialized transformer encoder with 6 layers,0.5925702452659607
translation,16,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,6 heads,300 dimensional randomly initialized transformer encoder with 6 heads,0.6720916032791138
translation,16,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,bertbase - uncased,300 dimensional randomly initialized transformer encoder with bertbase - uncased,0.6486042737960815
translation,16,120,hyperparameters,hyperparameters,use,300 dimensional randomly initialized transformer encoder,hyperparameters use 300 dimensional randomly initialized transformer encoder,0.5940749049186707
translation,16,121,hyperparameters,pre-training,for,transformer encoder and bert,pre-training for transformer encoder and bert,0.6063214540481567
translation,16,121,hyperparameters,transformer encoder and bert,takes,80 and 8 epochs,transformer encoder and bert takes 80 and 8 epochs,0.6858963966369629
translation,16,121,hyperparameters,hyperparameters,has,pre-training,hyperparameters has pre-training,0.5166658163070679
translation,16,122,hyperparameters,"adam ( kingma and ba , 2015 )",with,warm - up,"adam ( kingma and ba , 2015 ) with warm - up",0.657189130783081
translation,16,122,hyperparameters,warm - up,to optimize,our models,warm - up to optimize our models,0.7518042922019958
translation,16,122,hyperparameters,our models,with,learning rate,our models with learning rate,0.6224355101585388
translation,16,122,hyperparameters,our models,with,5e? 5,our models with 5e? 5,0.7151073217391968
translation,16,122,hyperparameters,1e? 3,for,transformer encoder,1e? 3 for transformer encoder,0.6156834363937378
translation,16,122,hyperparameters,5e? 5,for,bert,5e? 5 for bert,0.6958458423614502
translation,16,122,hyperparameters,learning rate,has,1e? 3,learning rate has 1e? 3,0.5725879073143005
translation,16,122,hyperparameters,hyperparameters,adopt,"adam ( kingma and ba , 2015 )","hyperparameters adopt adam ( kingma and ba , 2015 )",0.6174468994140625
translation,16,123,hyperparameters,fine-tuned,by,aspect- aware fine-tuning,fine-tuned by aspect- aware fine-tuning,0.5026795268058777
translation,16,123,hyperparameters,aspect- aware fine-tuning,with,5e ? 5 learning rate,aspect- aware fine-tuning with 5e ? 5 learning rate,0.6483132243156433
translation,16,123,hyperparameters,hyperparameters,has,pre-trained models,hyperparameters has pre-trained models,0.5199089646339417
translation,16,7,model,supervised contrastive pre-training,on,large-scale sentimentannotated corpora,supervised contrastive pre-training on large-scale sentimentannotated corpora,0.49789318442344666
translation,16,7,model,large-scale sentimentannotated corpora,retrieved from,in - domain language resources,large-scale sentimentannotated corpora retrieved from in - domain language resources,0.5503294467926025
translation,16,7,model,model,adopt,supervised contrastive pre-training,model adopt supervised contrastive pre-training,0.6712905764579773
translation,16,47,model,aspect- aware finetuning,to enhance,ability,aspect- aware finetuning to enhance ability,0.6761718392372131
translation,16,47,model,ability,of,models,ability of models,0.60906982421875
translation,16,47,model,models,on,aspect-based sentiment identification,models on aspect-based sentiment identification,0.5239691138267517
translation,16,47,model,fine-tuning,has,aspect- aware finetuning,fine-tuning has aspect- aware finetuning,0.5304989218711853
translation,16,47,model,model,In,fine-tuning,model In fine-tuning,0.5134802460670471
translation,16,118,model,scapt,to,transformer encoder and bert,scapt to transformer encoder and bert,0.581540584564209
translation,16,118,model,fine-tuned,by,aspect- aware fine-tuning,fine-tuned by aspect- aware fine-tuning,0.5026795268058777
translation,16,118,model,model,apply,scapt,model apply scapt,0.707892656326294
translation,16,150,model,fine-tuning,serves as,complement,fine-tuning serves as complement,0.5968608856201172
translation,16,150,model,complement,to,scapt,complement to scapt,0.6791366338729858
translation,16,127,results,effect,of,scapt,effect of scapt,0.6632857322692871
translation,16,127,results,effect,of,aspectaware fine-tuning,effect of aspectaware fine-tuning,0.5997379422187805
translation,16,127,results,methods,on,restaurant and laptop,methods on restaurant and laptop,0.5144035816192627
translation,16,136,results,our model,capable to identify,implicit sentiment,our model capable to identify implicit sentiment,0.7125455737113953
translation,16,136,results,our model,attributes its effectiveness,supervised contrastive learning,our model attributes its effectiveness supervised contrastive learning,0.7150826454162598
translation,16,136,results,supervised contrastive learning,in,scapt,supervised contrastive learning in scapt,0.5956401228904724
translation,16,136,results,results,reveal,our model,results reveal our model,0.6899428963661194
translation,16,140,results,our model,achieves,sota performance,our model achieves sota performance,0.7261884808540344
translation,16,140,results,results,has,our model,results has our model,0.5871725678443909
translation,16,141,results,current sota model,by,1.97 %,current sota model by 1.97 %,0.5486602187156677
translation,16,141,results,current sota model,by,3.80 %,current sota model by 3.80 %,0.5549768209457397
translation,16,141,results,current sota model,on,restaurant / laptop,current sota model on restaurant / laptop,0.5028263330459595
translation,16,141,results,1.97 %,/,3.80 %,1.97 % / 3.80 %,0.6226115822792053
translation,16,141,results,3.80 %,on,restaurant / laptop,3.80 % on restaurant / laptop,0.49145713448524475
translation,16,141,results,bertasp+scapt,has,outperforms,bertasp+scapt has outperforms,0.6534441709518433
translation,16,141,results,outperforms,has,current sota model,outperforms has current sota model,0.6067540645599365
translation,16,141,results,results,has,bertasp+scapt,results has bertasp+scapt,0.5277132391929626
translation,16,143,results,bertasp +scapt,achieves,best performance,bertasp +scapt achieves best performance,0.7108955383300781
translation,16,143,results,best performance,on,ese / ise slices,best performance on ese / ise slices,0.5756258368492126
translation,16,143,results,ese / ise slices,of,two datasets,ese / ise slices of two datasets,0.5842375159263611
translation,16,143,results,results,has,bertasp +scapt,results has bertasp +scapt,0.5277132391929626
translation,16,144,results,improve significantly,on,absa tasks,improve significantly on absa tasks,0.5247881412506104
translation,16,144,results,scapt,has,models,scapt has models,0.6351556181907654
translation,16,144,results,models,has,improve significantly,models has improve significantly,0.5991112589836121
translation,16,144,results,results,pre-trained with,scapt,results pre-trained with scapt,0.6601470708847046
translation,16,145,results,bertasp,directly fine-tuned on,absa datasets,bertasp directly fine-tuned on absa datasets,0.7551881074905396
translation,16,145,results,bertasp +scapt,achieves,3.31 %,bertasp +scapt achieves 3.31 %,0.6630454063415527
translation,16,145,results,/4.23 % performance gain,on,restaurant / laptop,/4.23 % performance gain on restaurant / laptop,0.49835634231567383
translation,16,145,results,bertasp,has,bertasp +scapt,bertasp has bertasp +scapt,0.6582992672920227
translation,16,145,results,3.31 %,has,/4.23 % performance gain,3.31 % has /4.23 % performance gain,0.5715115666389465
translation,16,145,results,results,Compared with,bertasp,results Compared with bertasp,0.7134621143341064
translation,16,146,results,transencasp+scapt,is,6.29%/11.34 %,transencasp+scapt is 6.29%/11.34 %,0.5893526673316956
translation,16,146,results,6.29%/11.34 %,better that,transencasp,6.29%/11.34 % better that transencasp,0.6718003153800964
translation,16,146,results,results,has,transencasp+scapt,results has transencasp+scapt,0.5526043772697449
translation,16,149,results,performance,on,ese,performance on ese,0.6758587956428528
translation,16,149,results,bertasp +scapt,appears to be,much better,bertasp +scapt appears to be much better,0.6812191605567932
translation,16,149,results,much better,on,ise,much better on ise,0.6200690269470215
translation,16,149,results,performance,has,bertasp +scapt,performance has bertasp +scapt,0.5756464600563049
translation,16,149,results,ese,has,bertasp +scapt,ese has bertasp +scapt,0.6639666557312012
translation,16,151,results,aspectaware fine-tuning,perform,better,aspectaware fine-tuning perform better,0.6028879880905151
translation,16,151,results,better,on,ese slices,better on ese slices,0.5882712602615356
translation,16,151,results,ese slices,of,datasets,ese slices of datasets,0.6087610125541687
translation,16,152,results,bertasp,worse on,ise,bertasp worse on ise,0.7420403957366943
translation,16,152,results,bertasp,better on,ese,bertasp better on ese,0.6878868937492371
translation,16,152,results,ese,compared with,bert - spc,ese compared with bert - spc,0.7064796686172485
translation,16,152,results,results,has,bertasp,results has bertasp,0.6127899289131165
translation,16,158,results,transencasp + scapt,achieves,state - of- the - art,transencasp + scapt achieves state - of- the - art,0.65644770860672
translation,16,158,results,outperforms,that lack,external sentiment knowledge,outperforms that lack external sentiment knowledge,0.6682400703430176
translation,16,158,results,baselines,that lack,external sentiment knowledge,baselines that lack external sentiment knowledge,0.6462886333465576
translation,16,158,results,bertasp +scapt,achieves,state - of- the - art,bertasp +scapt achieves state - of- the - art,0.6594759821891785
translation,16,158,results,state - of- the - art,in,multi-aspect scenario,state - of- the - art in multi-aspect scenario,0.5262468457221985
translation,16,158,results,transencasp + scapt,has,outperforms,transencasp + scapt has outperforms,0.6450316905975342
translation,16,158,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,16,158,results,results,shows,transencasp + scapt,results shows transencasp + scapt,0.6362996101379395
translation,16,160,results,bertasp,performs,much more better,bertasp performs much more better,0.6329836845397949
translation,16,160,results,much more better,than,bert - spc,much more better than bert - spc,0.6357788443565369
translation,16,160,results,bert - spc,in,mams,bert - spc in mams,0.6424346566200256
translation,16,160,results,mams,than in,restaurant / laptop,mams than in restaurant / laptop,0.6318863034248352
translation,16,160,results,results,has,bertasp,results has bertasp,0.6127899289131165
translation,16,165,results,bertasp+scapt,is,4.49 %,bertasp+scapt is 4.49 %,0.5921393632888794
translation,16,165,results,4.49 %,better on,ise,4.49 % better on ise,0.6909835338592529
translation,16,165,results,better performance,on,whole tasks,better performance on whole tasks,0.5383940935134888
translation,16,165,results,bertasp+ cept,has,bertasp+scapt,bertasp+ cept has bertasp+scapt,0.6613691449165344
translation,16,165,results,results,Compared with,bertasp+ cept,results Compared with bertasp+ cept,0.674809992313385
translation,16,183,results,obvious performance drop,in,baseline models,obvious performance drop in baseline models,0.5077483654022217
translation,16,183,results,bertasp +scapt,performs,significantly better,bertasp +scapt performs significantly better,0.661868691444397
translation,16,183,results,significantly better,than,other models,significantly better than other models,0.5623962879180908
translation,16,183,results,other models,with,9.05 %,other models with 9.05 %,0.544816255569458
translation,16,183,results,other models,with,6.63 % decline,other models with 6.63 % decline,0.5696715712547302
translation,16,183,results,9.05 %,/,6.63 % decline,9.05 % / 6.63 % decline,0.634560227394104
translation,16,183,results,6.63 % decline,on,restaurant and laptop,6.63 % decline on restaurant and laptop,0.48437219858169556
translation,16,183,results,obvious performance drop,has,bertasp +scapt,obvious performance drop has bertasp +scapt,0.5879514813423157
translation,16,183,results,baseline models,has,bertasp +scapt,baseline models has bertasp +scapt,0.5681691765785217
translation,16,183,results,results,Comparing to,obvious performance drop,results Comparing to obvious performance drop,0.7432276606559753
translation,16,184,results,models,pre-trained with,scapt,models pre-trained with scapt,0.7336270213127136
translation,16,184,results,scapt,are,more robust,scapt are more robust,0.5943387746810913
translation,16,184,results,more robust,for,aspect-level perturbations,more robust for aspect-level perturbations,0.5785481929779053
translation,16,184,results,more robust,attribute to,better modeling,more robust attribute to better modeling,0.7601926922798157
translation,16,184,results,results,show that,models,results show that models,0.4730059802532196
translation,17,32,ablation-analysis,analysis,of,induced tree,analysis of induced tree,0.6220544576644897
translation,17,32,ablation-analysis,induced tree,from,ft - ptms,induced tree from ft - ptms,0.6094368696212769
translation,17,32,ablation-analysis,more sentimentword -oriented,making,aspect term,more sentimentword -oriented making aspect term,0.6491901278495789
translation,17,32,ablation-analysis,aspect term,directly connect to,sentiment adjectives,aspect term directly connect to sentiment adjectives,0.6312950849533081
translation,17,32,ablation-analysis,ablation analysis,of,induced tree,ablation analysis of induced tree,0.5767281651496887
translation,17,32,ablation-analysis,ablation analysis,has,analysis,ablation analysis has analysis,0.5138995051383972
translation,17,126,baselines,perturbed masking method,apply,chu-liu / edmonds ' algorithm,perturbed masking method apply chu-liu / edmonds ' algorithm,0.6163327693939209
translation,17,126,baselines,chu-liu / edmonds ' algorithm,for,tree decoding,chu-liu / edmonds ' algorithm for tree decoding,0.6110377311706543
translation,17,125,experiments,fine-tuning experiments,with,learning rate ? = 2e - 4,fine-tuning experiments with learning rate ? = 2e - 4,0.6471251249313354
translation,17,125,experiments,learning rate ? = 2e - 4,using,adamw optimizer,learning rate ? = 2e - 4 using adamw optimizer,0.6306223273277283
translation,17,125,experiments,adamw optimizer,with,default settings,adamw optimizer with default settings,0.605948269367218
translation,17,130,hyperparameters,) embeddings,for,english datasets,) embeddings for english datasets,0.61784428358078
translation,17,130,hyperparameters,300 - dimension glove,has,) embeddings,300 - dimension glove has ) embeddings,0.594301164150238
translation,17,130,hyperparameters,hyperparameters,use,300 - dimension glove,hyperparameters use 300 - dimension glove,0.6199774146080017
translation,17,131,hyperparameters,word embeddings,fixed to avoid,overfitting,word embeddings fixed to avoid overfitting,0.6790032386779785
translation,17,131,hyperparameters,hyperparameters,keep,word embeddings,hyperparameters keep word embeddings,0.5392046570777893
translation,17,27,results,models,with,trees,models with trees,0.6664887070655823
translation,17,27,results,models,with,trees,models with trees,0.6664887070655823
translation,17,27,results,trees,induced from,alsc fine - tuned roberta,trees induced from alsc fine - tuned roberta,0.6662574410438538
translation,17,27,results,trees,induced from,dependency parser,trees induced from dependency parser,0.632443904876709
translation,17,27,results,trees,from,dependency parser,trees from dependency parser,0.5584138035774231
translation,17,27,results,trees,from,dependency parser,trees from dependency parser,0.5584138035774231
translation,17,27,results,outperform,has,trees,outperform has trees,0.5950860381126404
translation,17,27,results,results,has,models,results has models,0.5335168838500977
translation,17,28,results,base roberta,with,mlp layer,base roberta with mlp layer,0.6632170081138611
translation,17,28,results,mlp layer,is enough to achieve,state-ofthe -art ( sota ) or near sota performance,mlp layer is enough to achieve state-ofthe -art ( sota ) or near sota performance,0.711737871170044
translation,17,28,results,state-ofthe -art ( sota ) or near sota performance,on,all six alsc datasets,state-ofthe -art ( sota ) or near sota performance on all six alsc datasets,0.5070962309837341
translation,17,28,results,all six alsc datasets,across,four languages,all six alsc datasets across four languages,0.7592256665229797
translation,17,28,results,tree structures,into,roberta - based alsc models,tree structures into roberta - based alsc models,0.5814844369888306
translation,17,28,results,results,find that,base roberta,results find that base roberta,0.6994147896766663
translation,17,31,results,models,using,induced trees,models using induced trees,0.5747367739677429
translation,17,31,results,induced trees,from,fine - tuned roberta,induced trees from fine - tuned roberta,0.5363574624061584
translation,17,31,results,fine - tuned roberta,has,outperform,fine - tuned roberta has outperform,0.6249097585678101
translation,17,31,results,outperform,has,other trees,outperform has other trees,0.5998445749282837
translation,17,31,results,results,has,models,results has models,0.5335168838500977
translation,17,33,results,sota or near sota performances,on,six alsc datasets,sota or near sota performances on six alsc datasets,0.5263545513153076
translation,17,33,results,six alsc datasets,across,four languages,six alsc datasets across four languages,0.717772901058197
translation,17,33,results,four languages,based on,roberta,four languages based on roberta,0.5916095972061157
translation,17,33,results,results,achieve,sota or near sota performances,results achieve sota or near sota performances,0.6414437294006348
translation,17,34,results,roberta,could,better adapt,roberta could better adapt,0.6602952480316162
translation,17,34,results,roberta,help,aspects,roberta help aspects,0.6986654996871948
translation,17,34,results,better adapt,to,alsc,better adapt to alsc,0.6497665643692017
translation,17,34,results,aspects,to find,sentiment words,aspects to find sentiment words,0.5392785668373108
translation,17,34,results,results,find that,roberta,results find that roberta,0.6364197134971619
translation,17,137,results,all the trees,incorporating,ft - roberta induced tree,all the trees incorporating ft - roberta induced tree,0.677737832069397
translation,17,137,results,ft - roberta induced tree,leads to,best results,ft - roberta induced tree leads to best results,0.669973611831665
translation,17,137,results,best results,on,all datasets,best results on all datasets,0.48451125621795654
translation,17,137,results,results,observe,all the trees,results observe all the trees,0.5778577327728271
translation,17,137,results,results,among,all the trees,results among all the trees,0.5728695392608643
translation,17,138,results,models,based on,ft - roberta induced tree,models based on ft - roberta induced tree,0.6897619366645813
translation,17,138,results,outperform,1.1 % in,accuracy,outperform 1.1 % in accuracy,0.6393346786499023
translation,17,138,results,ft - roberta induced tree,has,outperform,ft - roberta induced tree has outperform,0.6335973739624023
translation,17,138,results,outperform,has,dep.,outperform has dep.,0.612815797328949
translation,17,140,results,bert induced tree and roberta induced tree,show,small performance difference,bert induced tree and roberta induced tree show small performance difference,0.6512917876243591
translation,17,140,results,small performance difference,in,all but one dataset,small performance difference in all but one dataset,0.5132330656051636
translation,17,146,results,dependency trees,achieve,better performance,dependency trees achieve better performance,0.6211760640144348
translation,17,146,results,better performance,than,ptms induced trees,better performance than ptms induced trees,0.5845630168914795
translation,17,162,results,roberta induced tree,models incorporating,ft - roberta induced tree,roberta induced tree models incorporating ft - roberta induced tree,0.6811232566833496
translation,17,162,results,average accuracy improvement,of,1.56 %,average accuracy improvement of 1.56 %,0.5242767333984375
translation,17,162,results,results,compared with,roberta induced tree,results compared with roberta induced tree,0.6230570673942566
translation,17,205,results,roberta,with,mlp layer,roberta with mlp layer,0.6747660636901855
translation,17,205,results,mlp layer,achieve,sota or near sota performance,mlp layer achieve sota or near sota performance,0.6728405356407166
translation,17,205,results,all these sota alternatives,has,roberta,all these sota alternatives has roberta,0.6636983156204224
translation,17,205,results,results,Comparing with,all these sota alternatives,results Comparing with all these sota alternatives,0.6831919550895691
translation,17,206,results,other datasets,notice that,significant improvement,other datasets notice that significant improvement,0.5428991913795471
translation,17,206,results,significant improvement,obtained on,lap-top14 dataset,significant improvement obtained on lap-top14 dataset,0.5914492011070251
translation,17,206,results,results,compared to,other datasets,results compared to other datasets,0.6644323468208313
translation,17,213,results,trees,over,roberta,trees over roberta,0.662417471408844
translation,17,213,results,roberta,brings,no significant improvement,roberta brings no significant improvement,0.6428612470626831
translation,17,213,results,results,incorporating,trees,results incorporating trees,0.61252361536026
translation,18,44,ablation-analysis,language -specific knowledge,essential for,tackling,language -specific knowledge essential for tackling,0.6768878102302551
translation,18,44,ablation-analysis,tackling,has,cross-lingual absa task,tackling has cross-lingual absa task,0.5381428599357605
translation,18,44,ablation-analysis,ablation analysis,show,language -specific knowledge,ablation analysis show language -specific knowledge,0.6050970554351807
translation,18,165,ablation-analysis,assumption,that,language -specific knowledge,assumption that language -specific knowledge,0.626315176486969
translation,18,165,ablation-analysis,language -specific knowledge,tackling,cross-lingual absa task,language -specific knowledge tackling cross-lingual absa task,0.6035633683204651
translation,18,165,ablation-analysis,model,on,unlabeled target data,model on unlabeled target data,0.5046809315681458
translation,18,165,ablation-analysis,performance,could be,further improved,performance could be further improved,0.701343834400177
translation,18,165,ablation-analysis,model,has,performance,model has performance,0.5451148748397827
translation,18,165,ablation-analysis,ablation analysis,verifies,assumption,ablation analysis verifies assumption,0.7176623344421387
translation,18,139,experimental-setup,best training hyper-parameters,by conducting,grid search,best training hyper-parameters by conducting grid search,0.6774879693984985
translation,18,139,experimental-setup,grid search,combination of,batch size and learning rate,grid search combination of batch size and learning rate,0.6230162382125854
translation,18,139,experimental-setup,experimental setup,select,best training hyper-parameters,experimental setup select best training hyper-parameters,0.6389145851135254
translation,18,140,experimental-setup,learning rate,has,"{ 2e - 5 , 3e - 5 , 5e - 5 }","learning rate has { 2e - 5 , 3e - 5 , 5e - 5 }",0.5247387886047363
translation,18,140,experimental-setup,learning rate,has,batch size,learning rate has batch size,0.5358124375343323
translation,18,140,experimental-setup,batch size,has,"8 , 16 , 25 }","batch size has 8 , 16 , 25 }",0.6048581600189209
translation,18,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,18,142,experimental-setup,mbert,use,batch size,mbert use batch size,0.660168468952179
translation,18,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,18,142,experimental-setup,mbert,for,xlm -r,mbert for xlm -r,0.6584206819534302
translation,18,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,18,142,experimental-setup,learning rate,being,5e - 5,learning rate being 5e - 5,0.6260045170783997
translation,18,142,experimental-setup,learning rate,being,2e - 5,learning rate being 2e - 5,0.6295531988143921
translation,18,142,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,18,142,experimental-setup,batch size,being,16,batch size being 16,0.6230911612510681
translation,18,142,experimental-setup,batch size,being,8,batch size being 8,0.6359264850616455
translation,18,142,experimental-setup,xlm -r,use,learning rate,xlm -r use learning rate,0.6613675951957703
translation,18,142,experimental-setup,xlm -r,use,batch size,xlm -r use batch size,0.6992930173873901
translation,18,142,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,18,142,experimental-setup,batch size,being,8,batch size being 8,0.6359264850616455
translation,18,142,experimental-setup,experimental setup,For,mbert,experimental setup For mbert,0.6481549143791199
translation,18,142,experimental-setup,experimental setup,for,xlm -r,experimental setup for xlm -r,0.6217503547668457
translation,18,17,experiments,cross-lingual,has,absa,cross-lingual has absa,0.6385807394981384
translation,18,38,experiments,unlabeled target language data,via,knowledge distillation,unlabeled target language data via knowledge distillation,0.6681320667266846
translation,18,38,experiments,knowledge distillation,has,"hinton et al. , 2015 )","knowledge distillation has hinton et al. , 2015 )",0.5692386627197266
translation,18,141,experiments,best choices,selected by,performance,best choices selected by performance,0.7167204022407532
translation,18,141,experiments,performance,on,source language data,performance on source language data,0.5168934464454651
translation,18,7,model,alignment - free label projection method,to obtain,high-quality pseudolabeled data,alignment - free label projection method to obtain high-quality pseudolabeled data,0.5791402459144592
translation,18,7,model,high-quality pseudolabeled data,of,target language,high-quality pseudolabeled data of target language,0.5275003910064697
translation,18,7,model,high-quality pseudolabeled data,help of,translation system,high-quality pseudolabeled data help of translation system,0.6238883137702942
translation,18,7,model,high-quality pseudolabeled data,could preserve,more accurate task -specific knowledge,high-quality pseudolabeled data could preserve more accurate task -specific knowledge,0.7045449018478394
translation,18,7,model,more accurate task -specific knowledge,in,target language,more accurate task -specific knowledge in target language,0.45930418372154236
translation,18,7,model,model,propose,alignment - free label projection method,model propose alignment - free label projection method,0.6150904297828674
translation,18,8,model,aspect code-switching mechanism,to augment,training data,aspect code-switching mechanism to augment training data,0.6637360453605652
translation,18,8,model,training data,with,code-switched bilingual sentences,training data with code-switched bilingual sentences,0.596964418888092
translation,18,8,model,model,design,aspect code-switching mechanism,model design aspect code-switching mechanism,0.5897082090377808
translation,18,30,model,alignment - free label projection method,to obtain,high-quality pseudolabeled target language data,alignment - free label projection method to obtain high-quality pseudolabeled target language data,0.5651364326477051
translation,18,30,model,model,propose,alignment - free label projection method,model propose alignment - free label projection method,0.6150904297828674
translation,18,34,model,aspect code-switching ( acs ) mechanism,switches,aspect terms,aspect code-switching ( acs ) mechanism switches aspect terms,0.7601048946380615
translation,18,34,model,aspect terms,between,source and translated target sentences,aspect terms between source and translated target sentences,0.5266088247299194
translation,18,34,model,source and translated target sentences,to construct,two bilingual sentences,source and translated target sentences to construct two bilingual sentences,0.611903190612793
translation,18,34,model,model,propose,aspect code-switching ( acs ) mechanism,model propose aspect code-switching ( acs ) mechanism,0.6911861896514893
translation,18,208,model,high-quality labeled target data,design,alignment - free label projection method,high-quality labeled target data design alignment - free label projection method,0.505273699760437
translation,18,208,model,alignment - free label projection method,establishes,strong translation - based baseline,alignment - free label projection method establishes strong translation - based baseline,0.5604097843170166
translation,18,208,model,model,To obtain,high-quality labeled target data,model To obtain high-quality labeled target data,0.5788755416870117
translation,18,9,results,above model,on,unlabeled target language data,above model on unlabeled target language data,0.4859589636325836
translation,18,9,results,above model,improves,performance,above model improves performance,0.7639926671981812
translation,18,9,results,unlabeled target language data,improves,performance,unlabeled target language data improves performance,0.6390823125839233
translation,18,9,results,performance,to,same level,performance to same level,0.6317873597145081
translation,18,9,results,same level,of,supervised method,same level of supervised method,0.617517352104187
translation,18,156,results,zero - shot method,based on,mbert,zero - shot method based on mbert,0.7290583252906799
translation,18,156,results,mbert,is,relatively weak,mbert is relatively weak,0.5859703421592712
translation,18,156,results,xlm -r backbone,becomes,competitive baseline,xlm -r backbone becomes competitive baseline,0.5796166062355042
translation,18,158,results,bilingual -ta,trained with,source data,bilingual -ta trained with source data,0.7403568029403687
translation,18,158,results,bilingual -ta,trained with,labeled target data,bilingual -ta trained with labeled target data,0.7413628101348877
translation,18,158,results,labeled target data,from,translate - then - align paradigm,labeled target data from translate - then - align paradigm,0.5802510380744934
translation,18,158,results,even worse,than,zero - shot method,even worse than zero - shot method,0.6093063950538635
translation,18,158,results,results,has,bilingual -ta,results has bilingual -ta,0.5559729933738708
translation,18,159,results,proposed alignmentfree label projection method,establishes,strong baseline,proposed alignmentfree label projection method establishes strong baseline,0.5653381943702698
translation,18,159,results,strong baseline,for,crosslingual absa problem,strong baseline for crosslingual absa problem,0.5854579210281372
translation,18,159,results,results,has,proposed alignmentfree label projection method,results has proposed alignmentfree label projection method,0.5326226949691772
translation,18,160,results,zero - shot method,based on either,mbert or xlm -r,zero - shot method based on either mbert or xlm -r,0.7104638814926147
translation,18,160,results,much better performance,than,previous translation - based approach,much better performance than previous translation - based approach,0.5604400038719177
translation,18,160,results,outperforms,has,zero - shot method,outperforms has zero - shot method,0.5894685983657837
translation,18,161,results,translation -ta,obtains,6.84 % and 8.21 % absolute performance gains,translation -ta obtains 6.84 % and 8.21 % absolute performance gains,0.5980414748191833
translation,18,161,results,6.84 % and 8.21 % absolute performance gains,based on,mbert and xlm -r,6.84 % and 8.21 % absolute performance gains based on mbert and xlm -r,0.668215274810791
translation,18,161,results,results,Compared with,translation -ta,results Compared with translation -ta,0.6773470640182495
translation,18,163,results,proposed acs method,has,further outperforms,proposed acs method has further outperforms,0.5811654925346375
translation,18,163,results,further outperforms,has,translation - af baseline,further outperforms has translation - af baseline,0.5724828243255615
translation,18,163,results,results,has,proposed acs method,results has proposed acs method,0.574910044670105
translation,18,164,results,model,on,unlabeled data,model on unlabeled data,0.508256196975708
translation,18,164,results,unlabeled data,of,target language,unlabeled data of target language,0.5140315294265747
translation,18,164,results,proposed single - teacher distillation ( acs - distill -s ),achieve,greater performance,proposed single - teacher distillation ( acs - distill -s ) achieve greater performance,0.6692807078361511
translation,18,164,results,multi-teacher distillation ( acs - distill -m ),achieve,greater performance,multi-teacher distillation ( acs - distill -m ) achieve greater performance,0.6650066375732422
translation,18,164,results,model,has,proposed single - teacher distillation ( acs - distill -s ),model has proposed single - teacher distillation ( acs - distill -s ),0.5869888067245483
translation,18,164,results,results,By distilling,model,results By distilling model,0.6220720410346985
translation,18,166,results,model,trained with,multiple teachers,model trained with multiple teachers,0.7501482367515564
translation,18,166,results,model,achieves,slightly better performance,model achieves slightly better performance,0.6742340326309204
translation,18,166,results,slightly better performance,than,single -teacher model,slightly better performance than single -teacher model,0.5691604018211365
translation,18,166,results,results,has,model,results has model,0.5339115858078003
translation,18,170,results,multilingual pseudo-labeled translated target data,"our label projection method ( i.e. ,",mtl - af ),"multilingual pseudo-labeled translated target data our label projection method ( i.e. , mtl - af )",0.7247605919837952
translation,18,170,results,multilingual pseudo-labeled translated target data,sets up,quite strong baseline,multilingual pseudo-labeled translated target data sets up quite strong baseline,0.6229647994041443
translation,18,170,results,already outperform,has,mtl - ws,already outperform has mtl - ws,0.6208239793777466
translation,18,170,results,already outperform,has,previous state - of - the - art model,already outperform has previous state - of - the - art model,0.5582608580589294
translation,18,170,results,mtl - ws,has,previous state - of - the - art model,mtl - ws has previous state - of - the - art model,0.5748343467712402
translation,18,170,results,results,training on,multilingual pseudo-labeled translated target data,results training on multilingual pseudo-labeled translated target data,0.613915205001831
translation,18,171,results,distillation,on,unlabeled data,distillation on unlabeled data,0.5583367943763733
translation,18,171,results,distillation,improve,adaptation performance,distillation improve adaptation performance,0.682765543460846
translation,18,174,results,our model,distilled on,multilingual data ( mtl - acs - d ),our model distilled on multilingual data ( mtl - acs - d ),0.7273309826850891
translation,18,174,results,our model,achieves,65.33 average f1 scores,our model achieves 65.33 average f1 scores,0.6104311347007751
translation,18,174,results,65.33 average f1 scores,even close to,f1 scores,65.33 average f1 scores even close to f1 scores,0.6194704174995422
translation,18,174,results,65.33 average f1 scores,showing,superiority,65.33 average f1 scores showing superiority,0.674813985824585
translation,18,174,results,f1 scores,under,supervised setting,f1 scores under supervised setting,0.6583106517791748
translation,18,174,results,xlm -r as backbone,has,our model,xlm -r as backbone has our model,0.615821361541748
translation,18,174,results,results,With,xlm -r as backbone,results With xlm -r as backbone,0.5894785523414612
translation,18,177,results,translated data and the source data,is,most powerful method,translated data and the source data is most powerful method,0.5313620567321777
translation,18,177,results,results,combining,translated data and the source data,results combining translated data and the source data,0.6738789677619934
translation,18,178,results,best performance,when,target language,best performance when target language,0.6493076086044312
translation,18,178,results,target language,is,spanish,target language is spanish,0.5763013362884521
translation,18,190,results,af,produces,2.1 % partially missed aspects,af produces 2.1 % partially missed aspects,0.619411051273346
translation,18,190,results,2.1 % partially missed aspects,when facing,long aspect terms,2.1 % partially missed aspects when facing long aspect terms,0.5837658643722534
translation,18,190,results,results,notice,af,results notice af,0.5822162628173828
translation,19,23,baselines,re-ranking approach,uses,cross-attention,re-ranking approach uses cross-attention,0.5752674341201782
translation,19,23,baselines,cross-attention,between,question,cross-attention between question,0.6235948801040649
translation,19,23,baselines,cross-attention,between,candidate passage,cross-attention between candidate passage,0.6214585304260254
translation,19,6,experiments,successful re-ranking approach ( reconsider ),for,span-extraction tasks,successful re-ranking approach ( reconsider ) for span-extraction tasks,0.591367244720459
translation,19,6,experiments,successful re-ranking approach ( reconsider ),improves upon,performance,successful re-ranking approach ( reconsider ) improves upon performance,0.7632039189338684
translation,19,6,experiments,performance,of,mrc models,performance of mrc models,0.6044309735298157
translation,19,77,experiments,webq and trec,start,training,webq and trec start training,0.6912122964859009
translation,19,77,experiments,training,from,our trained nq model,training from our trained nq model,0.5676467418670654
translation,19,7,model,re,trained on,positive and negative examples,re trained on positive and negative examples,0.7466269731521606
translation,19,7,model,re,uses,in - passage span annotations,re uses in - passage span annotations,0.6341873407363892
translation,19,7,model,-consider,trained on,positive and negative examples,-consider trained on positive and negative examples,0.7189164161682129
translation,19,7,model,-consider,uses,in - passage span annotations,-consider uses in - passage span annotations,0.6420300006866455
translation,19,7,model,positive and negative examples,extracted from,high confidence mrc model predictions,positive and negative examples extracted from high confidence mrc model predictions,0.5735710263252258
translation,19,7,model,in - passage span annotations,to perform,span-focused reranking,in - passage span annotations to perform span-focused reranking,0.6485077142715454
translation,19,7,model,span-focused reranking,over,smaller candidate set,span-focused reranking over smaller candidate set,0.6272874474525452
translation,19,7,model,re,has,-consider,re has -consider,0.6640076637268066
translation,19,7,model,model,has,re,model has re,0.6371442675590515
translation,19,7,model,model,has,-consider,model has -consider,0.6387417316436768
translation,19,11,model,span selection components,of,mrc models,span selection components of mrc models,0.545511782169342
translation,19,11,model,mrc models,trained on,distantly supervised positive examples,mrc models trained on distantly supervised positive examples,0.7145074605941772
translation,19,11,model,distantly supervised positive examples,together with,heuristically chosen negative examples,distantly supervised positive examples together with heuristically chosen negative examples,0.641685426235199
translation,19,11,model,heuristically chosen negative examples,typically from,upstream retrieval models,heuristically chosen negative examples typically from upstream retrieval models,0.6608687043190002
translation,19,11,model,model,has,span selection components,model has span selection components,0.5406027436256409
translation,19,13,model,general approach,to make,answer reranking,general approach to make answer reranking,0.6395789384841919
translation,19,13,model,successful,for,span-extraction tasks,successful for span-extraction tasks,0.5765646696090698
translation,19,13,model,answer reranking,has,successful,answer reranking has successful,0.597324550151825
translation,19,13,model,model,develop,general approach,model develop general approach,0.6766122579574585
translation,19,32,model,mrc model performance,by making,re-ranking,mrc model performance by making re-ranking,0.6521157026290894
translation,19,32,model,successful,using,span-focused re-ranking,successful using span-focused re-ranking,0.6653711795806885
translation,19,32,model,span-focused re-ranking,of,highly confident predictions,span-focused re-ranking of highly confident predictions,0.5522466897964478
translation,19,32,model,significantly improve,has,mrc model performance,significantly improve has mrc model performance,0.5594813823699951
translation,19,32,model,re-ranking,has,successful,re-ranking has successful,0.6032386422157288
translation,19,27,results,broadly applicable span-focused reranking approach,on,models,broadly applicable span-focused reranking approach on models,0.5448279976844788
translation,19,27,results,state of the art,on,four qa datasets,state of the art on four qa datasets,0.4811742305755615
translation,19,27,results,45.5 %,on,opendomain setting,45.5 % on opendomain setting,0.4728088080883026
translation,19,27,results,opendomain setting,of,nq,opendomain setting of nq,0.533851146697998
translation,19,27,results,+ 1.6 %,on,small models,+ 1.6 % on small models,0.5425274968147278
translation,19,27,results,61.1 %,on,triviaqa,61.1 % on triviaqa,0.5116187930107117
translation,19,27,results,results,use,broadly applicable span-focused reranking approach,results use broadly applicable span-focused reranking approach,0.6396538019180298
translation,19,80,results,bert base version,of,reconsider,bert base version of reconsider,0.6129481196403503
translation,19,80,results,previous state- ofthe - art dpr model,by,1.6 %,previous state- ofthe - art dpr model by 1.6 %,0.525787353515625
translation,19,80,results,previous state- ofthe - art dpr model,by,? 2 %,previous state- ofthe - art dpr model by ? 2 %,0.5673642754554749
translation,19,80,results,previous state- ofthe - art dpr model,on,triv -iaqa and webq,previous state- ofthe - art dpr model on triv -iaqa and webq,0.5458025932312012
translation,19,80,results,1.6 %,on,nq,1.6 % on nq,0.6219786405563354
translation,19,80,results,? 2 %,on,triv -iaqa and webq,? 2 % on triv -iaqa and webq,0.6277931928634644
translation,19,80,results,reconsider,has,outperforms,reconsider has outperforms,0.6439379453659058
translation,19,80,results,outperforms,has,previous state- ofthe - art dpr model,outperforms has previous state- ofthe - art dpr model,0.5505332946777344
translation,19,80,results,results,has,bert base version,results has bert base version,0.5981032252311707
translation,19,82,results,effectiveness,of,coarse- to-fine approach,effectiveness of coarse- to-fine approach,0.6345826983451843
translation,19,82,results,coarse- to-fine approach,for selecting,negative passages,coarse- to-fine approach for selecting negative passages,0.7137360572814941
translation,19,82,results,coarse- to-fine approach,with,dense retrieval based negatives ( dpr ),coarse- to-fine approach with dense retrieval based negatives ( dpr ),0.6694352030754089
translation,19,82,results,coarse- to-fine approach,with,outperforming,coarse- to-fine approach with outperforming,0.6727323532104492
translation,19,82,results,dense retrieval based negatives ( dpr ),improved upon,our reranking approach,dense retrieval based negatives ( dpr ) improved upon our reranking approach,0.707410991191864
translation,19,82,results,dense retrieval based negatives ( dpr ),has,outperforming,dense retrieval based negatives ( dpr ) has outperforming,0.5999629497528076
translation,19,82,results,outperforming,has,bm25,outperforming has bm25,0.5997449159622192
translation,19,82,results,results,demonstrates,effectiveness,results demonstrates effectiveness,0.6364638805389404
translation,19,88,results,outperforms,by,?1 %,outperforms by ?1 %,0.6671167016029358
translation,19,88,results,?1 %,on,all datasets,?1 % on all datasets,0.5519053936004639
translation,19,88,results,large,has,outperforms,large has outperforms,0.617459774017334
translation,19,88,results,results,RECONSIDER,large,results RECONSIDER large,0.5578119158744812
translation,19,90,results,k=5 ( testing ),to be,best,k=5 ( testing ) to be best,0.6140562295913696
translation,19,90,results,best,for,all datasets,best for all datasets,0.5452767014503479
translation,19,90,results,little effect,on,accuracy,little effect on accuracy,0.5176700353622437
translation,19,90,results,little effect,training on,top - 100 predictions,little effect training on top - 100 predictions,0.7078843116760254
translation,19,90,results,results,find,k=5 ( testing ),results find k=5 ( testing ),0.6447532176971436
translation,20,169,ablation-analysis,vocabulary overlap,not,helping,vocabulary overlap not helping,0.7343826293945312
translation,20,169,ablation-analysis,helping,either of,models,helping either of models,0.5207620859146118
translation,20,169,ablation-analysis,models,in terms of,predictive performance,models in terms of predictive performance,0.6779448390007019
translation,20,169,ablation-analysis,ablation analysis,conclude that,vocabulary overlap,ablation analysis conclude that vocabulary overlap,0.5690854787826538
translation,20,5,experiments,checking factual correctness,of,textual summarization ( cfcs ),checking factual correctness of textual summarization ( cfcs ),0.5584978461265564
translation,20,171,experiments,race converted,on,converted forms,race converted on converted forms,0.586106538772583
translation,20,171,experiments,converted forms,of,mcrc datasets,converted forms of mcrc datasets,0.6132701635360718
translation,20,139,results,model,trained on,long- premise race converted dataset,model trained on long- premise race converted dataset,0.7300937175750732
translation,20,139,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,20,139,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,20,139,results,short- premise nli datasets,in,regular and segmented forms,short- premise nli datasets in regular and segmented forms,0.5072154402732849
translation,20,139,results,long- premise race converted dataset,has,outperforms,long- premise race converted dataset has outperforms,0.6029102206230164
translation,20,139,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,20,165,results,model,trained on,race converted,model trained on race converted,0.7553454041481018
translation,20,165,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,20,165,results,models,trained on,short- premise nli datasets,models trained on short- premise nli datasets,0.7033385634422302
translation,20,165,results,considerably outperforms,has,models,considerably outperforms has models,0.6634339094161987
translation,20,165,results,results,see that,model,results see that model,0.640961766242981
translation,20,168,results,performance,of,two models,performance of two models,0.6440985202789307
translation,20,168,results,two models,on,high vocabulary overlap subsets,two models on high vocabulary overlap subsets,0.529872715473175
translation,20,168,results,overall performances,on,respective datasets,overall performances on respective datasets,0.46470633149147034
translation,20,180,results,race converted model,performs,better,race converted model performs better,0.6330445408821106
translation,20,180,results,better,on,manually annotated subset,better on manually annotated subset,0.5731756687164307
translation,20,180,results,results,shows,race converted model,results shows race converted model,0.6634796857833862
translation,21,187,ablation-analysis,tokens,from,input sequence,tokens from input sequence,0.537002444267273
translation,21,187,ablation-analysis,input sequence,employing,transformer,input sequence employing transformer,0.6697494983673096
translation,21,187,ablation-analysis,qr model ( copy - transformer ),compared to,other existing generative models,qr model ( copy - transformer ) compared to other existing generative models,0.6406790614128113
translation,21,155,baselines,allenai coref,is,state - of - the - art model,allenai coref is state - of - the - art model,0.5738205909729004
translation,21,155,baselines,state - of - the - art model,for,coreference resolution task,state - of - the - art model for coreference resolution task,0.5268639326095581
translation,21,155,baselines,baselines,has,allenai coref,baselines has allenai coref,0.5751386880874634
translation,21,157,baselines,pointergenerator,uses,bi-lstm encoder,pointergenerator uses bi-lstm encoder,0.5481691360473633
translation,21,157,baselines,pointergenerator,uses,pointer- generator decoder,pointergenerator uses pointer- generator decoder,0.5834287405014038
translation,21,157,baselines,pointer- generator decoder,copy and generate,tokens,pointer- generator decoder copy and generate tokens,0.7169618010520935
translation,21,157,baselines,baselines,has,pointergenerator,baselines has pointergenerator,0.4947958290576935
translation,21,158,baselines,gecor,uses,two bi-gru encoders,gecor uses two bi-gru encoders,0.641804575920105
translation,21,158,baselines,gecor,uses,pointer - generator decoder,gecor uses pointer - generator decoder,0.630713701248169
translation,21,158,baselines,two bi-gru encoders,one for,user utterance,two bi-gru encoders one for user utterance,0.6435361504554749
translation,21,158,baselines,two bi-gru encoders,other for,dialogue context,two bi-gru encoders other for dialogue context,0.6116034984588623
translation,21,158,baselines,baselines,has,gecor,baselines has gecor,0.5851355195045471
translation,21,111,experimental-setup,scripts,for reproducing,passage collection,scripts for reproducing passage collection,0.6416325569152832
translation,21,111,experimental-setup,experimental setup,has,scripts,experimental setup has scripts,0.48326849937438965
translation,21,154,experimental-setup,transformer - based models,initialized with,pretrained weights,transformer - based models initialized with pretrained weights,0.7608480453491211
translation,21,154,experimental-setup,transformer - based models,further fine-tuned on,question rewrites,transformer - based models further fine-tuned on question rewrites,0.6879419684410095
translation,21,154,experimental-setup,pretrained weights,of,gpt - 2 ( english medium-size ),pretrained weights of gpt - 2 ( english medium-size ),0.5432412028312683
translation,21,154,experimental-setup,question rewrites,from,qrecc training set,question rewrites from qrecc training set,0.5556543469429016
translation,21,154,experimental-setup,experimental setup,has,transformer - based models,experimental setup has transformer - based models,0.5325118899345398
translation,21,4,experiments,14 k conversations,with,80 k question - answer pairs,14 k conversations with 80 k question - answer pairs,0.6449865102767944
translation,21,4,experiments,question rewriting in conversational context,has,),question rewriting in conversational context has ),0.5745674967765808
translation,21,22,experiments,qrecc,accompanied with,scripts,qrecc accompanied with scripts,0.6941855549812317
translation,21,22,experiments,scripts,for building,collection of passages,scripts for building collection of passages,0.7720090746879578
translation,21,22,experiments,collection of passages,from,common crawl,collection of passages from common crawl,0.5694857835769653
translation,21,22,experiments,collection of passages,from,wayback machine,collection of passages from wayback machine,0.5661803483963013
translation,21,22,experiments,wayback machine,for,passage retrieval,wayback machine for passage retrieval,0.557467520236969
translation,21,163,experiments,standard bm25 ranking,for,passage retrieval,standard bm25 ranking for passage retrieval,0.5305793881416321
translation,21,163,experiments,passage retrieval,with,"k 1 = 0.82 , b = 0.68","passage retrieval with k 1 = 0.82 , b = 0.68",0.6343065500259399
translation,21,23,model,model,has,qrecc,model has qrecc,0.6178826093673706
translation,21,144,model,qr model,to incorporate,conversational context,qr model to incorporate conversational context,0.6479210257530212
translation,21,156,model,qr,with,heuristic,qr with heuristic,0.6901621222496033
translation,21,156,model,heuristic,that substitutes,all coreference mentions,heuristic that substitutes all coreference mentions,0.6444606184959412
translation,21,156,model,all coreference mentions,with,corresponding antecedents,all coreference mentions with corresponding antecedents,0.6002618074417114
translation,21,156,model,corresponding antecedents,from,cluster,corresponding antecedents from cluster,0.5961775183677673
translation,21,156,model,model,adapt it for,qr,model adapt it for qr,0.7369700074195862
translation,21,159,model,generator,is,transformer decoder model,generator is transformer decoder model,0.5401466488838196
translation,21,159,model,transformer decoder model,with,language modeling head,transformer decoder model with language modeling head,0.5861710906028748
translation,21,159,model,linear layer,in the size of,vocabulary,linear layer in the size of vocabulary,0.6603825688362122
translation,21,159,model,second head,for,auxiliary classification task,second head for auxiliary classification task,0.5513402223587036
translation,21,159,model,second head,distinguishes between,correct rewrite,second head distinguishes between correct rewrite,0.6935244202613831
translation,21,159,model,second head,distinguishes between,several noisy rewrites,second head distinguishes between several noisy rewrites,0.6993820667266846
translation,21,159,model,several noisy rewrites,as,negative samples,several noisy rewrites as negative samples,0.5349355936050415
translation,21,159,model,language modeling head,has,linear layer,language modeling head has linear layer,0.5024048686027527
translation,21,159,model,generator + multiple-choice model,has,second head,generator + multiple-choice model has second head,0.5264959931373596
translation,21,159,model,model,has,generator,model has generator,0.5837957262992859
translation,21,160,model,copytransformer,one of,attention heads,copytransformer one of attention heads,0.6252089142799377
translation,21,160,model,attention heads,of,transformer,attention heads of transformer,0.6221866607666016
translation,21,160,model,pointer,to copy,tokens,pointer to copy tokens,0.5775867104530334
translation,21,160,model,tokens,from,input sequence directly,tokens from input sequence directly,0.5892200469970703
translation,21,160,model,two language modeling heads,produce,separate vocabulary distributions,two language modeling heads produce separate vocabulary distributions,0.610292375087738
translation,21,160,model,combined,via,parameterized weighted sum,combined via parameterized weighted sum,0.6448231339454651
translation,21,160,model,model,has,copytransformer,model has copytransformer,0.5907248258590698
translation,21,9,results,first baseline,for,qrecc dataset,first baseline for qrecc dataset,0.5780969262123108
translation,21,9,results,qrecc dataset,with,f1,qrecc dataset with f1,0.646390974521637
translation,21,9,results,qrecc dataset,compared to,human upper bound,qrecc dataset compared to human upper bound,0.6008018255233765
translation,21,9,results,f1,of,19.10,f1 of 19.10,0.5823016166687012
translation,21,9,results,human upper bound,of,75.45,human upper bound of 75.45,0.5456984639167786
translation,21,9,results,results,set,first baseline,results set first baseline,0.6128122806549072
translation,21,183,results,generative models,has,outperform,generative models has outperform,0.5889489650726318
translation,21,183,results,outperform,has,state- of- theart coreference resolution model ( allenai coref ),outperform has state- of- theart coreference resolution model ( allenai coref ),0.6161487102508545
translation,21,183,results,results,has,generative models,results has generative models,0.49864086508750916
translation,21,184,results,pointergenerator,employs,bi-lstm encoder,pointergenerator employs bi-lstm encoder,0.5431681275367737
translation,21,184,results,bi-lstm encoder,with,copy and generate mechanism,bi-lstm encoder with copy and generate mechanism,0.6528283953666687
translation,21,184,results,generator,using,transformer,generator using transformer,0.656103789806366
translation,21,184,results,copy and generate mechanism,has,outperforms,copy and generate mechanism has outperforms,0.6289238929748535
translation,21,184,results,outperforms,has,generator,outperforms has generator,0.6736419796943665
translation,21,184,results,results,noticed,pointergenerator,results noticed pointergenerator,0.6581062078475952
translation,21,185,results,improve,has,qr model effectiveness,improve has qr model effectiveness,0.538372278213501
translation,21,186,results,two separate bi-gru encoders,for,query and conversation context,two separate bi-gru encoders for query and conversation context,0.5600346922874451
translation,21,186,results,two separate bi-gru encoders,improved,qr effectiveness ( gecor ),two separate bi-gru encoders improved qr effectiveness ( gecor ),0.6863290667533875
translation,21,186,results,query and conversation context,improved,qr effectiveness ( gecor ),query and conversation context improved qr effectiveness ( gecor ),0.732134222984314
translation,21,186,results,results,Use of,two separate bi-gru encoders,results Use of two separate bi-gru encoders,0.6010259985923767
translation,21,204,results,human rewritten questions,more than double,effectiveness,human rewritten questions more than double effectiveness,0.6847395300865173
translation,21,204,results,effectiveness,of using,original questions,effectiveness of using original questions,0.5668979287147522
translation,21,204,results,results,see that,human rewritten questions,results see that human rewritten questions,0.6319023370742798
translation,21,205,results,human rewritten questions,using,transfomer ++,human rewritten questions using transfomer ++,0.7113434076309204
translation,21,205,results,results,In the absence of,human rewritten questions,results In the absence of human rewritten questions,0.6232506632804871
translation,22,176,ablation-analysis,hyperparameter selection,tries to find,sweet spot,hyperparameter selection tries to find sweet spot,0.7164741158485413
translation,22,176,ablation-analysis,sweet spot,for,effectiveness and efficiency,sweet spot for effectiveness and efficiency,0.5655063390731812
translation,22,176,ablation-analysis,sweet spot,points to,br,sweet spot points to br,0.7429855465888977
translation,22,176,ablation-analysis,br,of,1:1,br of 1:1,0.448786199092865
translation,22,176,ablation-analysis,ablation analysis,has,hyperparameter selection,ablation analysis has hyperparameter selection,0.49286210536956787
translation,22,160,baselines,control experiment,using,raw compsent - 19,control experiment using raw compsent - 19,0.7156354784965515
translation,22,160,baselines,raw compsent - 19,without,weighting or augmentation,raw compsent - 19 without weighting or augmentation,0.755687952041626
translation,22,160,baselines,original,has,row,original has row,0.5909149050712585
translation,22,8,model,sentiment analysis enhanced comparative network ( saecon ),improves,cpc accuracy,sentiment analysis enhanced comparative network ( saecon ) improves cpc accuracy,0.6990506649017334
translation,22,8,model,cpc accuracy,with,sentiment analyzer,cpc accuracy with sentiment analyzer,0.6271463632583618
translation,22,8,model,sentiment analyzer,learns,sentiments,sentiment analyzer learns sentiments,0.7260050177574158
translation,22,8,model,sentiments,to,individual entities,sentiments to individual entities,0.5242451429367065
translation,22,8,model,individual entities,via,domain adaptive knowledge transfer,individual entities via domain adaptive knowledge transfer,0.650031328201294
translation,22,8,model,model,propose,sentiment analysis enhanced comparative network ( saecon ),model propose sentiment analysis enhanced comparative network ( saecon ),0.6572006344795227
translation,22,25,model,sentiment analysis,has,enhanced comparative classification network ( saecon ),sentiment analysis has enhanced comparative classification network ( saecon ),0.6002821922302246
translation,22,25,model,sentiment analysis,has,cpc approach,sentiment analysis has cpc approach,0.5614159107208252
translation,22,25,model,enhanced comparative classification network ( saecon ),has,cpc approach,enhanced comparative classification network ( saecon ) has cpc approach,0.5912014842033386
translation,22,25,model,model,propose,sentiment analysis,model propose sentiment analysis,0.6620081067085266
translation,22,28,model,sentiments,towards,individual entities,sentiments towards individual entities,0.6550149917602539
translation,22,151,results,saecon with bert embeddings,achieves,highest f1 scores,saecon with bert embeddings achieves highest f1 scores,0.6740979552268982
translation,22,151,results,highest f1 scores,comparing with,all baselines,highest f1 scores comparing with all baselines,0.6457018256187439
translation,22,151,results,highest f1 scores,demonstrates,superior ability,highest f1 scores demonstrates superior ability,0.6358857154846191
translation,22,151,results,results,has,saecon with bert embeddings,results has saecon with bert embeddings,0.6201230883598328
translation,22,152,results,f1 scores,for,none,f1 scores for none,0.6516475081443787
translation,22,152,results,none,"i.e. , F1",),"none i.e. , F1 )",0.6470938324928284
translation,22,152,results,none,"i.e. , F1",consistently the highest,"none i.e. , F1 consistently the highest",0.593987226486206
translation,22,152,results,none,are,consistently the highest,none are consistently the highest,0.5812240839004517
translation,22,152,results,consistently the highest,in,all rows,consistently the highest in all rows,0.5546309351921082
translation,22,152,results,all rows,due to,data imbalance,all rows due to data imbalance,0.6688251495361328
translation,22,152,results,data imbalance,None accounts for,largest percentage,data imbalance None accounts for largest percentage,0.6582602262496948
translation,22,152,results,results,has,f1 scores,results has f1 scores,0.517913281917572
translation,22,163,results,upsampling,cannot provide,performance gain,upsampling cannot provide performance gain,0.651193380355835
translation,22,164,results,weighted loss,performs,bit worse,weighted loss performs bit worse,0.5871269106864929
translation,22,164,results,weighted loss,performs,consistently better,weighted loss performs consistently better,0.5755717158317566
translation,22,164,results,bit worse,on,f1 ( n ),bit worse on f1 ( n ),0.5833765268325806
translation,22,164,results,consistently better,on,other metrics,consistently better on other metrics,0.5205255746841431
translation,22,164,results,results,has,weighted loss,results has weighted loss,0.5577974915504456
translation,22,175,results,performance,is,better,performance is better,0.6231186985969543
translation,22,175,results,better,when,cpc batches,better when cpc batches,0.6598550081253052
translation,22,175,results,cpc batches,less than,absa ones,cpc batches less than absa ones,0.6903071999549866
translation,22,175,results,reported performances,has,differ slightly,reported performances has differ slightly,0.5651199817657471
translation,22,180,results,better domain adaptation,produces,higher f1 scores,better domain adaptation produces higher f1 scores,0.6189987659454346
translation,22,180,results,higher f1 scores,in,scenarios,higher f1 scores in scenarios,0.5285905003547668
translation,22,180,results,scenarios,where,datasets in the domain of interest,scenarios where datasets in the domain of interest,0.6271801590919495
translation,22,180,results,results,has,better domain adaptation,results has better domain adaptation,0.5491089224815369
translation,23,228,ablation-analysis,poor,at balancing between,high- degree and low-degree concepts,poor at balancing between high- degree and low-degree concepts,0.7940708994865417
translation,23,228,ablation-analysis,high- degree and low-degree concepts,in,knowledge graph,high- degree and low-degree concepts in knowledge graph,0.511637270450592
translation,23,228,ablation-analysis,ablation analysis,shows,duerquiz,ablation analysis shows duerquiz,0.6996009349822998
translation,23,201,experimental-setup,mip formulation,use,weights and hyper-parameters,mip formulation use weights and hyper-parameters,0.6016156077384949
translation,23,209,experimental-setup,ubuntu 18.04.5 lts machine,with,8 - core intel i7-8550u 1.80 ghz processors,ubuntu 18.04.5 lts machine with 8 - core intel i7-8550u 1.80 ghz processors,0.5190089344978333
translation,23,209,experimental-setup,ubuntu 18.04.5 lts machine,with,16 gb,ubuntu 18.04.5 lts machine with 16 gb,0.5373846888542175
translation,23,5,model,interview assistant system,select,optimal set of technical questions,interview assistant system select optimal set of technical questions,0.6982613205909729
translation,23,5,model,optimal set of technical questions,from,question banks ),optimal set of technical questions from question banks ),0.5705532431602478
translation,23,5,model,optimal set of technical questions,personalized for,candidate,optimal set of technical questions personalized for candidate,0.7187869548797607
translation,23,5,model,model,propose,interview assistant system,model propose interview assistant system,0.6447305083274841
translation,23,23,model,set of questions,for,human interviewer,set of questions for human interviewer,0.5880994200706482
translation,23,23,model,plan,for,upcoming interview,plan for upcoming interview,0.633746325969696
translation,23,23,model,model,propose,interview assistant system,model propose interview assistant system,0.6447305083274841
translation,23,32,model,knowledge graph,as,background knowledge,knowledge graph as background knowledge,0.4512254297733307
translation,23,32,model,knowledge graph,formulate,objective functions and constraints,knowledge graph formulate objective functions and constraints,0.6163210868835449
translation,23,32,model,model,use,knowledge graph,model use knowledge graph,0.5699092745780945
translation,23,32,model,model,formulate,objective functions and constraints,model formulate objective functions and constraints,0.6587404608726501
translation,23,225,results,improvement,against,br1,improvement against br1,0.7481210231781006
translation,23,225,results,improvement,shows,importance,improvement shows importance,0.6768685579299927
translation,23,225,results,importance,focusing on,resume,importance focusing on resume,0.593967616558075
translation,23,225,results,randomly selecting questions,related to,skill,randomly selecting questions related to skill,0.7245420217514038
translation,23,225,results,results,has,improvement,results has improvement,0.6248279809951782
translation,23,226,results,improvement,against,br3,improvement against br3,0.7909741997718811
translation,23,226,results,questions,related to,resume,questions related to resume,0.6057725548744202
translation,23,226,results,results,has,improvement,results has improvement,0.6248279809951782
translation,23,227,results,improvement,against,duerquiz,improvement against duerquiz,0.7159231305122375
translation,23,227,results,improvement,importance of,additional terms,improvement importance of additional terms,0.6402903199195862
translation,23,227,results,duerquiz,importance of,additional terms,duerquiz importance of additional terms,0.7070886492729187
translation,23,227,results,results,has,improvement,results has improvement,0.6248279809951782
translation,23,231,results,br1 and br3,perform,better,br1 and br3 perform better,0.6704995632171631
translation,23,231,results,better,than,duerquiz,better than duerquiz,0.6843308806419373
translation,23,231,results,duerquiz,in terms of,forward evaluation,duerquiz in terms of forward evaluation,0.7127740383148193
translation,23,231,results,results,note,br1 and br3,results note br1 and br3,0.6157127022743225
translation,23,232,results,duerquiz,better than,baselines,duerquiz better than baselines,0.7650855779647827
translation,23,232,results,duerquiz,in terms of,backward evaluation,duerquiz in terms of backward evaluation,0.7060588002204895
translation,23,232,results,baselines,in terms of,backward evaluation,baselines in terms of backward evaluation,0.6436949968338013
translation,23,232,results,results,has,duerquiz,results has duerquiz,0.5671965479850769
translation,23,234,results,duerquiz,in,both directions,duerquiz in both directions,0.5890520215034485
translation,23,234,results,ip,has,outperforms,ip has outperforms,0.664216935634613
translation,23,234,results,outperforms,has,duerquiz,outperforms has duerquiz,0.6724739074707031
translation,23,234,results,results,Note,ip,results Note ip,0.6108553409576416
translation,24,72,ablation-analysis,one epoch,improves,results,one epoch improves results,0.7385550737380981
translation,24,72,ablation-analysis,results,by,more than 0.3 %,results by more than 0.3 %,0.5675797462463379
translation,24,72,ablation-analysis,ablation analysis,trained for,one epoch,ablation analysis trained for one epoch,0.7766249775886536
translation,24,77,ablation-analysis,more efficient domain adaptation,with,nb - mlm,more efficient domain adaptation with nb - mlm,0.6513762474060059
translation,24,77,ablation-analysis,more efficient domain adaptation,reduces,difference,more efficient domain adaptation reduces difference,0.6775000095367432
translation,24,77,ablation-analysis,difference,to,0.2 %,difference to 0.2 %,0.5357705950737
translation,24,77,ablation-analysis,ablation analysis,has,more efficient domain adaptation,ablation analysis has more efficient domain adaptation,0.5249918699264526
translation,24,59,hyperparameters,validation,randomly selected,5 k positive and 5 k negative examples,validation randomly selected 5 k positive and 5 k negative examples,0.6593404412269592
translation,24,59,hyperparameters,hyperparameters,For,validation,hyperparameters For validation,0.5990188121795654
translation,24,60,hyperparameters,domain and task adaptation,used,batch size,domain and task adaptation used batch size,0.6025341153144836
translation,24,60,hyperparameters,batch size,of,1024,batch size of 1024,0.6398507952690125
translation,24,60,hyperparameters,fine-tuned,with,batch size,fine-tuned with batch size,0.6441797018051147
translation,24,60,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,24,60,hyperparameters,hyperparameters,For,domain and task adaptation,hyperparameters For domain and task adaptation,0.5491582155227661
translation,24,61,hyperparameters,learning rate,of,2e - 4,learning rate of 2e - 4,0.6387818455696106
translation,24,61,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,24,61,hyperparameters,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,24,61,hyperparameters,2e - 4,for,domain adaptation,2e - 4 for domain adaptation,0.6220231056213379
translation,24,61,hyperparameters,1e - 4,for,task adaptation,1e - 4 for task adaptation,0.6167691946029663
translation,24,61,hyperparameters,1e - 5,for,final fine-tuning,1e - 5 for final fine-tuning,0.6362829208374023
translation,24,61,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,24,8,model,technique,for,more efficient adaptation,technique for more efficient adaptation,0.6458348631858826
translation,24,8,model,more efficient adaptation,focuses on,predicting words,more efficient adaptation focuses on predicting words,0.7043801546096802
translation,24,8,model,predicting words,with,large weights,predicting words with large weights,0.6512245535850525
translation,24,8,model,large weights,of,naive bayes classifier,large weights of naive bayes classifier,0.5780720114707947
translation,24,8,model,naive bayes classifier,trained for,task at hand,naive bayes classifier trained for task at hand,0.788340151309967
translation,24,8,model,model,propose,technique,model propose technique,0.6912915110588074
translation,24,16,model,method,for,more efficient mlm adaptation,method for more efficient mlm adaptation,0.6295204758644104
translation,24,16,model,model,propose,method,model propose method,0.6280754208564758
translation,24,30,results,roberta,enhances,bert,roberta enhances bert,0.5494688749313354
translation,24,30,results,roberta,selecting,different target words,roberta selecting different target words,0.7372971177101135
translation,24,30,results,bert,by,pre-training,bert by pre-training,0.6439120769500732
translation,24,30,results,bert,getting rid of,next sentence prediction ( nsp ) task,bert getting rid of next sentence prediction ( nsp ) task,0.644635021686554
translation,24,30,results,longer,on,ten times larger corpora,longer on ten times larger corpora,0.5296316742897034
translation,24,30,results,next sentence prediction ( nsp ) task,during,pre-training,next sentence prediction ( nsp ) task during pre-training,0.6576100587844849
translation,24,30,results,different target words,to be,masked,different target words to be masked,0.5927153825759888
translation,24,30,results,pre-training,has,longer,pre-training has longer,0.5985509753227234
translation,24,30,results,each epoch,has,dynamic masking,each epoch has dynamic masking,0.597472071647644
translation,24,30,results,results,has,roberta,results has roberta,0.530095100402832
translation,24,36,results,our method,leveraging,large data,our method leveraging large data,0.7108040452003479
translation,24,36,results,more efficiently,makes,domain adaptation,more efficiently makes domain adaptation,0.6454546451568604
translation,24,36,results,domain adaptation,comparable to,task adaptation,domain adaptation comparable to task adaptation,0.6180518269538879
translation,24,36,results,combination,is,significantly better,combination is significantly better,0.5828802585601807
translation,24,36,results,large data,has,more efficiently,large data has more efficiently,0.5678417086601257
translation,24,36,results,results,find that,our method,results find that our method,0.6377817392349243
translation,24,64,results,our nb - mlm model,significantly helps for,domain adaptation,our nb - mlm model significantly helps for domain adaptation,0.7096347212791443
translation,24,64,results,domain adaptation,on,imdb,domain adaptation on imdb,0.5416913628578186
translation,24,64,results,results,has,our nb - mlm model,results has our nb - mlm model,0.5330774784088135
translation,24,67,results,improvements,from,nb - mlm,improvements from nb - mlm,0.6198707818984985
translation,24,67,results,nb - mlm,are,small but consistent,nb - mlm are small but consistent,0.586183488368988
translation,24,67,results,yelp,has,improvements,yelp has improvements,0.6260426044464111
translation,24,67,results,results,For,yelp,results For yelp,0.6341710686683655
translation,24,71,results,domain adaptation,with,nb - mlm,domain adaptation with nb - mlm,0.6471427083015442
translation,24,71,results,domain adaptation,obtains,results,domain adaptation obtains results,0.5867282748222351
translation,24,71,results,results,similar to,uniform mlm,results similar to uniform mlm,0.6781432032585144
translation,24,71,results,uniform mlm,in,5x fewer training steps and data,uniform mlm in 5x fewer training steps and data,0.5378034114837646
translation,24,71,results,imdb,has,domain adaptation,imdb has domain adaptation,0.5435937643051147
translation,24,71,results,results,For,imdb,results For imdb,0.657120406627655
translation,24,73,results,nb - mlm,gives,much smaller improvement,nb - mlm gives much smaller improvement,0.6327693462371826
translation,24,73,results,task adaptation,has,nb - mlm,task adaptation has nb - mlm,0.5680142045021057
translation,24,73,results,results,For,task adaptation,results For task adaptation,0.5286756157875061
translation,24,74,results,task adaptation,with,uniform mlm,task adaptation with uniform mlm,0.6500149369239807
translation,24,74,results,domain adaptation,that employs,much more data,domain adaptation that employs much more data,0.6375723481178284
translation,24,74,results,much more data,by,almost 0.5 %,much more data by almost 0.5 %,0.5805325508117676
translation,24,74,results,uniform mlm,has,outperforms,uniform mlm has outperforms,0.6258792877197266
translation,24,74,results,outperforms,has,domain adaptation,outperforms has domain adaptation,0.6127426028251648
translation,24,78,results,domain adaptation,followed by,task adaptation,domain adaptation followed by task adaptation,0.6005827188491821
translation,24,78,results,task adaptation,results in,best final performance,task adaptation results in best final performance,0.6419297456741333
translation,24,78,results,results,using,domain adaptation,results using domain adaptation,0.5978401303291321
translation,24,79,results,nb - mlm,gives,0.2 % improvement,nb - mlm gives 0.2 % improvement,0.6150553822517395
translation,24,79,results,nb - mlm,gives,0.1 %,nb - mlm gives 0.1 %,0.5813926458358765
translation,24,79,results,0.2 % improvement,for,short adaptation,0.2 % improvement for short adaptation,0.5954150557518005
translation,24,79,results,0.1 %,for,long adaptation,0.1 % for long adaptation,0.6159226894378662
translation,25,166,ablation-analysis,performance,of,zs - bert method,performance of zs - bert method,0.597926914691925
translation,25,166,ablation-analysis,drops significantly,cases of,cross-domain,drops significantly cases of cross-domain,0.7100430727005005
translation,25,166,ablation-analysis,performance,has,drops significantly,performance has drops significantly,0.611855685710907
translation,25,166,ablation-analysis,zs - bert method,has,drops significantly,zs - bert method has drops significantly,0.627281129360199
translation,25,166,ablation-analysis,ablation analysis,demonstrate that,performance,ablation analysis demonstrate that performance,0.655604898929596
translation,25,193,baselines,distribution,of,source and target features,distribution of source and target features,0.6145616173744202
translation,25,193,baselines,aligning,has,distribution,aligning has distribution,0.6003434062004089
translation,25,193,baselines,baselines,has,mmd,baselines has mmd,0.5868086218833923
translation,25,151,experiments,cross-domain sentiment analysis methods,namely,htan,cross-domain sentiment analysis methods namely htan,0.6866269111633301
translation,25,151,experiments,pblm,has,"ziser and reichart , 2018 )","pblm has ziser and reichart , 2018 )",0.5942057371139526
translation,25,151,experiments,htan,has,"li et al. , 2018 )","htan has li et al. , 2018 )",0.5460772514343262
translation,25,8,model,arabic cross-domain and crossdialect sentiment analysis,from,contextualized word embedding,arabic cross-domain and crossdialect sentiment analysis from contextualized word embedding,0.5268385410308838
translation,25,8,model,model,propose,new unsupervised domain adaptation,model propose new unsupervised domain adaptation,0.673429548740387
translation,25,32,model,adversarial - learned loss,for,domain adaptation ( alda ),adversarial - learned loss for domain adaptation ( alda ),0.6157107949256897
translation,25,32,model,model,introduce,unsupervised domain adaptation,model introduce unsupervised domain adaptation,0.637692391872406
translation,25,11,results,our method,increases,performance,our method increases performance,0.6949278712272644
translation,25,11,results,performance,by,improvement rate,performance by improvement rate,0.5463268160820007
translation,25,11,results,improvement rate,of,20.8 %,improvement rate of 20.8 %,0.5613632798194885
translation,25,11,results,20.8 %,over,zero-shot transfer learning,20.8 % over zero-shot transfer learning,0.6308294534683228
translation,25,11,results,zero-shot transfer learning,from,bert,zero-shot transfer learning from bert,0.5505845546722412
translation,25,73,results,zero-shot transfer,from,svm model,zero-shot transfer from svm model,0.5693212747573853
translation,25,73,results,zero-shot transfer,achieves,competitive results,zero-shot transfer achieves competitive results,0.7104361653327942
translation,25,73,results,competitive results,for,some datasets,competitive results for some datasets,0.5557728409767151
translation,25,132,results,trained dialect identification model,achieves,89 % accuracy,trained dialect identification model achieves 89 % accuracy,0.6928309798240662
translation,25,156,results,cross-dialect sentiment analysis,show,zs - bert,cross-dialect sentiment analysis show zs - bert,0.637886643409729
translation,25,156,results,zs - bert,shows,method,zs - bert shows method,0.661249041557312
translation,25,156,results,dann bow and adrl,in,most test cases,dann bow and adrl in most test cases,0.5193732976913452
translation,25,156,results,most test cases,of,cross-domain sentiment analysis ( 14 out of 20 cases,most test cases of cross-domain sentiment analysis ( 14 out of 20 cases,0.5678690671920776
translation,25,156,results,method,has,outperforms,method has outperforms,0.6569275856018066
translation,25,157,results,transfer performance,of,bert model,transfer performance of bert model,0.596116304397583
translation,25,157,results,three domain adaptation methods,has,"coral , mmd , and dann","three domain adaptation methods has coral , mmd , and dann",0.5620943903923035
translation,25,157,results,three domain adaptation methods,has,outperform,three domain adaptation methods has outperform,0.5797481536865234
translation,25,157,results,"coral , mmd , and dann",has,outperform,"coral , mmd , and dann has outperform",0.5930989980697632
translation,25,157,results,outperform,has,dann bow and adrl,outperform has dann bow and adrl,0.6264753341674805
translation,25,157,results,results,show that,three domain adaptation methods,results show that three domain adaptation methods,0.46125903725624084
translation,25,158,results,on a par,with,each other,on a par with each other,0.6983147859573364
translation,25,158,results,each other,in terms of,accuracy,each other in terms of accuracy,0.7113717198371887
translation,25,158,results,latter three methods,has,dann ),latter three methods has dann ),0.6239612102508545
translation,25,159,results,outperforms,as,zs - bert,outperforms as zs - bert,0.6055783629417419
translation,25,159,results,state - of - the - art methods ( dann bow and adrl ),as,zs - bert,state - of - the - art methods ( dann bow and adrl ) as zs - bert,0.5452503561973572
translation,25,159,results,zs - bert,by,average increment,zs - bert by average increment,0.6161015629768372
translation,25,159,results,average increment,of,19 % and 10.7 %,average increment of 19 % and 10.7 %,0.5667517781257629
translation,25,159,results,proposed method,has,outperforms,proposed method has outperforms,0.6315754055976868
translation,25,159,results,outperforms,has,state - of - the - art methods ( dann bow and adrl ),outperforms has state - of - the - art methods ( dann bow and adrl ),0.5918145775794983
translation,25,159,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,25,160,results,better performance,than,"coral , mmd , and","better performance than coral , mmd , and",0.5690097212791443
translation,25,160,results,better performance,than,dann,better performance than dann,0.5770220756530762
translation,25,160,results,dann,for,most source,dann for most source,0.6605378985404968
translation,25,160,results,arabic cross-domain sentiment analysis,using,arsentd - lev dataset,arabic cross-domain sentiment analysis using arsentd - lev dataset,0.619516134262085
translation,25,160,results,"coral , mmd , and",has,dann,"coral , mmd , and has dann",0.5730460286140442
translation,25,164,results,zero-shot transfer,has,from arabert ( zs - bert ),zero-shot transfer has from arabert ( zs - bert ),0.6073798537254333
translation,25,164,results,zero-shot transfer,has,outperforms,zero-shot transfer has outperforms,0.6105437278747559
translation,25,164,results,from arabert ( zs - bert ),has,outperforms,from arabert ( zs - bert ) has outperforms,0.6566685438156128
translation,25,164,results,outperforms,has,previous state - of- the - art methods ( pblm and htan ),outperforms has previous state - of- the - art methods ( pblm and htan ),0.5911202430725098
translation,25,165,results,evaluated domain adaptation methods,on top of,bert,evaluated domain adaptation methods on top of bert,0.7079451084136963
translation,25,165,results,arabert 's performance,for,all evaluated scenarios,arabert 's performance for all evaluated scenarios,0.5949963331222534
translation,25,165,results,evaluated domain adaptation methods,has,improve,evaluated domain adaptation methods has improve,0.600109875202179
translation,25,165,results,improve,has,arabert 's performance,improve has arabert 's performance,0.5766749978065491
translation,25,165,results,results,has,evaluated domain adaptation methods,results has evaluated domain adaptation methods,0.5353028774261475
translation,25,167,results,domain adaptation methods,show,more important improvements,domain adaptation methods show more important improvements,0.5984556674957275
translation,25,167,results,increment,of,7.4 %,increment of 7.4 %,0.6013587117195129
translation,25,167,results,more important improvements,has,increment,more important improvements has increment,0.5965419411659241
translation,25,167,results,results,has,domain adaptation methods,results has domain adaptation methods,0.48293596506118774
translation,25,174,results,all domain adaptation methods,achieve,comparable performances,all domain adaptation methods achieve comparable performances,0.5524654388427734
translation,25,174,results,outperform,for,all evaluated datasets,outperform for all evaluated datasets,0.6027163863182068
translation,25,174,results,outperform,by,average increment,outperform by average increment,0.644012987613678
translation,25,174,results,zs - bert method,for,all evaluated datasets,zs - bert method for all evaluated datasets,0.5017458200454712
translation,25,174,results,average increment,of,4.9 %,average increment of 4.9 %,0.5679084658622742
translation,25,174,results,"coral , mmd , and dann",achieve,comparable performances,"coral , mmd , and dann achieve comparable performances",0.6234233379364014
translation,25,174,results,comparable performances,for,most dialectal datasets,comparable performances for most dialectal datasets,0.552649974822998
translation,25,174,results,all domain adaptation methods,has,outperform,all domain adaptation methods has outperform,0.5811818838119507
translation,25,174,results,outperform,has,zs - bert method,outperform has zs - bert method,0.5952184796333313
translation,25,175,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,25,175,results,outperforms,has,all other domain adaptation methods,outperforms has all other domain adaptation methods,0.5570135712623596
translation,25,178,results,far better trans - fer performance,than,state - of - the - art methods,far better trans - fer performance than state - of - the - art methods,0.556451678276062
translation,25,178,results,state - of - the - art methods,based on,bag-of-words representation or pretrained word embedding,state - of - the - art methods based on bag-of-words representation or pretrained word embedding,0.5875752568244934
translation,25,179,results,bert - based domain adaptation methods,yield,far better transfer learning performance,bert - based domain adaptation methods yield far better transfer learning performance,0.694345235824585
translation,25,179,results,far better transfer learning performance,than,dann_bow and adrl methods,far better transfer learning performance than dann_bow and adrl methods,0.5272535681724548
translation,25,179,results,results,has,bert - based domain adaptation methods,results has bert - based domain adaptation methods,0.4964756369590759
translation,25,180,results,our method,achieves,better performance,our method achieves better performance,0.6458921432495117
translation,25,180,results,better performance,than,"coral , mmd , and dann","better performance than coral , mmd , and dann",0.5775853991508484
translation,25,180,results,better performance,implemented on top of,bert module,better performance implemented on top of bert module,0.6945006251335144
translation,25,180,results,"coral , mmd , and dann",implemented on top of,bert module,"coral , mmd , and dann implemented on top of bert module",0.729077160358429
translation,25,180,results,results,has,our method,results has our method,0.5589964985847473
translation,26,122,ablation-analysis,model ablations,that use,missing schema,model ablations that use missing schema,0.7088445425033569
translation,26,122,ablation-analysis,missing schema,produce,more diverse questions,missing schema produce more diverse questions,0.672563374042511
translation,26,122,ablation-analysis,ablation analysis,has,model ablations,ablation analysis has model ablations,0.5199047327041626
translation,26,147,ablation-analysis,questions,become,more relevant,questions become more relevant,0.5787608623504639
translation,26,147,ablation-analysis,our proposed model,obtains,highest relevance score,our proposed model obtains highest relevance score,0.6171995997428894
translation,26,147,ablation-analysis,"missing schema ( i.e. , bart + missinfo )",has,questions,"missing schema ( i.e. , bart + missinfo ) has questions",0.5817197561264038
translation,26,150,ablation-analysis,reduce further,with,pplm model,reduce further with pplm model,0.6641945838928223
translation,26,150,ablation-analysis,missing schema,has,fluency,missing schema has fluency,0.620036780834198
translation,26,150,ablation-analysis,fluency,has,decreases,fluency has decreases,0.6058558821678162
translation,26,150,ablation-analysis,score,has,reduce further,score has reduce further,0.6358086466789246
translation,26,150,ablation-analysis,ablation analysis,addition of,missing schema,ablation analysis addition of missing schema,0.6284905076026917
translation,26,6,model,difference,between,global and the local view,difference between global and the local view,0.6541563272476196
translation,26,6,model,difference,train,model,difference train model,0.6783961057662964
translation,26,6,model,model,for,clarification question generation,model for clarification question generation,0.6057416200637817
translation,26,17,model,model,propose,two -stage framework,model propose two -stage framework,0.6008333563804626
translation,26,78,model,pretrained base bart model,consisting of,six layer encoder,pretrained base bart model consisting of six layer encoder,0.7074418663978577
translation,26,78,model,pretrained base bart model,consisting of,six layer decoder,pretrained base bart model consisting of six layer decoder,0.7126094102859497
translation,26,78,model,model,start with,pretrained base bart model,model start with pretrained base bart model,0.6317967772483826
translation,26,188,model,plug-and - play - language - model,to tune,generations,plug-and - play - language - model to tune generations,0.6889910697937012
translation,26,188,model,generations,during,decoding,generations during decoding,0.7196161150932312
translation,26,188,model,model,use,plug-and - play - language - model,model use plug-and - play - language - model,0.6273614168167114
translation,26,118,results,results,has,automatic metric results,results has automatic metric results,0.5753768682479858
translation,26,119,results,retrieval model,performs,worst,retrieval model performs worst,0.691613495349884
translation,26,119,results,bleu -4 and meteor,has,retrieval model,bleu -4 and meteor has retrieval model,0.5478381514549255
translation,26,123,results,our model,i.e.,bart + missinfo + pplm,our model i.e. bart + missinfo + pplm,0.6417128443717957
translation,26,123,results,results,has,our model,results has our model,0.5871725678443909
translation,26,124,results,results,has,ubuntu,results has ubuntu,0.5379010438919067
translation,26,127,results,bleu -4 and meteor scores,find that,retrieval baseline,bleu -4 and meteor scores find that retrieval baseline,0.6091101765632629
translation,26,127,results,retrieval baseline,has,lowest scores,retrieval baseline has lowest scores,0.5544719696044922
translation,26,128,results,transformer baseline,has,outperforms,transformer baseline has outperforms,0.6296485066413879
translation,26,128,results,outperforms,has,retrieval baseline,outperforms has retrieval baseline,0.587454617023468
translation,26,128,results,lags behind,has,bart,lags behind has bart,0.6489230394363403
translation,26,128,results,results,has,transformer baseline,results has transformer baseline,0.6075185537338257
translation,26,130,results,distinct - 2 scores,find,same trend,distinct - 2 scores find same trend,0.6492016315460205
translation,26,130,results,same trend,with,retrieval model,same trend with retrieval model,0.6795862317085266
translation,26,130,results,same trend,with,our final model,same trend with our final model,0.7012905478477478
translation,26,130,results,retrieval model,being,most diverse,retrieval model being most diverse,0.6466839909553528
translation,26,130,results,our final model,has,outperforming,our final model has outperforming,0.5869978666305542
translation,26,130,results,outperforming,has,all other baselines,outperforming has all other baselines,0.5882422924041748
translation,26,130,results,results,Under,distinct - 2 scores,results Under distinct - 2 scores,0.5253536105155945
translation,26,132,results,human judgment results,on,model generations,human judgment results on model generations,0.5229528546333313
translation,26,132,results,model generations,for,300 randomly sampled product descriptions,model generations for 300 randomly sampled product descriptions,0.5720037817955017
translation,26,132,results,300 randomly sampled product descriptions,from,amazon test set,300 randomly sampled product descriptions from amazon test set,0.5323775410652161
translation,26,133,results,all models,score,reasonably,all models score reasonably,0.6979027390480042
translation,26,133,results,reasonably,with,our proposed model,reasonably with our proposed model,0.708439826965332
translation,26,133,results,our proposed model,producing,most relevant and fluent questions,our proposed model producing most relevant and fluent questions,0.6871406435966492
translation,26,133,results,relevancy and fluency,has,all models,relevancy and fluency has all models,0.5525310635566711
translation,26,133,results,results,Under,relevancy and fluency,results Under relevancy and fluency,0.5442346334457397
translation,26,134,results,bart model,finetuned on,context,bart model finetuned on context,0.7691946625709534
translation,26,134,results,context,instead of,missing schema,context instead of missing schema,0.6306244730949402
translation,26,134,results,missing information,has,bart model,missing information has bart model,0.5799833536148071
translation,26,134,results,missing schema,has,lowest score,missing schema has lowest score,0.572794497013092
translation,26,134,results,results,Under,missing information,results Under missing information,0.5003090500831604
translation,26,135,results,gan - utility,has,outperforms,gan - utility has outperforms,0.6366574168205261
translation,26,135,results,outperforms,has,bart,outperforms has bart,0.6937707662582397
translation,26,135,results,significantly lags behind,has,bart + missinfo,significantly lags behind has bart + missinfo,0.615752100944519
translation,26,135,results,significantly lags behind,has,bart + missinfo + pplm,significantly lags behind has bart + missinfo + pplm,0.5973802208900452
translation,26,135,results,results,has,gan - utility,results has gan - utility,0.5534266829490662
translation,26,140,results,our model,wins over,gan - utility,our model wins over gan - utility,0.784442126750946
translation,26,140,results,gan - utility,by,significant margin,gan - utility by significant margin,0.606596052646637
translation,26,140,results,gan - utility,with,humans preferring,gan - utility with humans preferring,0.6488078236579895
translation,26,140,results,significant margin,with,humans preferring,significant margin with humans preferring,0.7102558016777039
translation,26,140,results,humans preferring,has,our model- generated questions,humans preferring has our model- generated questions,0.5995359420776367
translation,26,140,results,results,find that,our model,results find that our model,0.6804299354553223
translation,26,141,results,our model,beats,bart - baseline,our model beats bart - baseline,0.7457100749015808
translation,26,141,results,bart - baseline,has,66 %,bart - baseline has 66 %,0.5815114378929138
translation,26,141,results,results,has,our model,results has our model,0.5871725678443909
translation,26,142,results,our model,beats,bart + missinfo model,our model beats bart + missinfo model,0.7008434534072876
translation,26,142,results,bart + missinfo model,has,61 % of the time,bart + missinfo model has 61 % of the time,0.5734856724739075
translation,26,142,results,results,has,our model,results has our model,0.5871725678443909
translation,26,146,results,relevance,find that,transformer and bart baselines,relevance find that transformer and bart baselines,0.6486666202545166
translation,26,146,results,transformer and bart baselines,produce,less relevant questions,transformer and bart baselines produce less relevant questions,0.6493300199508667
translation,26,146,results,results,In terms of,relevance,results In terms of relevance,0.6099230051040649
translation,26,149,results,transformer and bart baselines,obtain,high scores,transformer and bart baselines obtain high scores,0.5881056189537048
translation,26,149,results,fluency,has,transformer and bart baselines,fluency has transformer and bart baselines,0.600096583366394
translation,26,149,results,results,Under,fluency,results Under fluency,0.5011642575263977
translation,26,152,results,all models,perform,well,all models perform well,0.6411154866218567
translation,26,152,results,missing information,has,all models,missing information has all models,0.5246827602386475
translation,26,152,results,results,Under,missing information,results Under missing information,0.5003090500831604
translation,26,154,results,humans,choose,our model- generated questions,humans choose our model- generated questions,0.7077654004096985
translation,26,154,results,our model- generated questions,compared to,transformer or bart generated questions,our model- generated questions compared to transformer or bart generated questions,0.6716505289077759
translation,26,154,results,our model- generated questions,has,85 % of time,our model- generated questions has 85 % of time,0.5925788879394531
translation,26,154,results,results,find that,humans,results find that humans,0.6054678559303284
translation,26,160,results,our model,is,least variant,our model is least variant,0.6045898199081421
translation,26,160,results,least variant,toward,information available,least variant toward information available,0.5871681571006775
translation,26,160,results,more robust,for,amazon dataset,more robust for amazon dataset,0.5903050899505615
translation,26,160,results,results,shows,our model,results shows our model,0.7287026643753052
translation,27,9,ablation-analysis,efficiency,of,our method,efficiency of our method,0.5723916888237
translation,27,9,ablation-analysis,efficiency,use,layer - wise parameter sharing,efficiency use layer - wise parameter sharing,0.6709770560264587
translation,27,9,ablation-analysis,efficiency,use,factorized co-attention,efficiency use factorized co-attention,0.6752267479896545
translation,27,9,ablation-analysis,our method,use,layer - wise parameter sharing,our method use layer - wise parameter sharing,0.6301618814468384
translation,27,9,ablation-analysis,our method,use,factorized co-attention,our method use factorized co-attention,0.6295271515846252
translation,27,9,ablation-analysis,factorized co-attention,that share,parameters,factorized co-attention that share parameters,0.6505885720252991
translation,27,9,ablation-analysis,parameters,between,cross attention blocks,parameters between cross attention blocks,0.607753336429596
translation,27,9,ablation-analysis,minimal impact,on,task performance,minimal impact on task performance,0.5713351964950562
translation,27,9,ablation-analysis,ablation analysis,To further improve,efficiency,ablation analysis To further improve efficiency,0.7146132588386536
translation,27,174,ablation-analysis,parameter sharing,decrease,model size,parameter sharing decrease model size,0.7814997434616089
translation,27,174,ablation-analysis,model size,by,71 %,model size by 71 %,0.5674924850463867
translation,27,174,ablation-analysis,negligible impact,on,model accuracy,negligible impact on model accuracy,0.5483343005180359
translation,27,174,ablation-analysis,ablation analysis,indicate,parameter sharing,ablation analysis indicate parameter sharing,0.5838029384613037
translation,27,175,ablation-analysis,layer - wise parameter sharing,improves,performance,layer - wise parameter sharing improves performance,0.6869350075721741
translation,27,175,ablation-analysis,ablation analysis,has,layer - wise parameter sharing,ablation analysis has layer - wise parameter sharing,0.5132091045379639
translation,27,179,ablation-analysis,further sharing,reduces,size of the model,further sharing reduces size of the model,0.6871567368507385
translation,27,179,ablation-analysis,size of the model,by,70 %,size of the model by 70 %,0.5952891111373901
translation,27,179,ablation-analysis,70 %,compared with,our model,70 % compared with our model,0.6743667125701904
translation,27,179,ablation-analysis,1.5 % relative reduction,in,model accuracy,1.5 % relative reduction in model accuracy,0.5172541737556458
translation,27,179,ablation-analysis,ablation analysis,show,further sharing,ablation analysis show further sharing,0.6786255240440369
translation,27,197,ablation-analysis,advantages,of,sp - block,advantages of sp - block,0.5775008201599121
translation,27,197,ablation-analysis,sp - block,lead to,3.3 % increase,sp - block lead to 3.3 % increase,0.679323673248291
translation,27,197,ablation-analysis,sp - block,lead to,89 % reduction,sp - block lead to 89 % reduction,0.6748701930046082
translation,27,197,ablation-analysis,3.3 % increase,in,performance,3.3 % increase in performance,0.5454117059707642
translation,27,197,ablation-analysis,89 % reduction,in,parameters,89 % reduction in parameters,0.554908037185669
translation,27,197,ablation-analysis,ablation analysis,has,advantages,ablation analysis has advantages,0.5017535090446472
translation,27,159,baselines,spt,with,layer - wise parameter sharing,spt with layer - wise parameter sharing,0.6364359259605408
translation,27,159,baselines,spt,with,mixed sampling,spt with mixed sampling,0.6691460609436035
translation,27,159,baselines,spt,with,summation,spt with summation,0.693239688873291
translation,27,159,baselines,summation,for,cross-modal interactions,summation for cross-modal interactions,0.6335820555686951
translation,27,209,experimental-setup,largest batch size,executed on,single nvidia tesla v100,largest batch size executed on single nvidia tesla v100,0.7325364351272583
translation,27,209,experimental-setup,single nvidia tesla v100,with,16gb vram,single nvidia tesla v100 with 16gb vram,0.622830867767334
translation,27,6,model,model,propose,multimodal sparse phased transformer ( spt ),model propose multimodal sparse phased transformer ( spt ),0.6728518605232239
translation,27,7,model,spt,uses,sampling function,spt uses sampling function,0.654998779296875
translation,27,7,model,spt,compress,long sequence,spt compress long sequence,0.6941305994987488
translation,27,7,model,sampling function,to generate,sparse attention matrix,sampling function to generate sparse attention matrix,0.6983646154403687
translation,27,7,model,long sequence,to,shorter sequence,long sequence to shorter sequence,0.5937726497650146
translation,27,7,model,shorter sequence,of,hidden states,shorter sequence of hidden states,0.6162627935409546
translation,27,7,model,model,has,spt,model has spt,0.6132522821426392
translation,27,8,model,spt,concurrently captures,interactions,spt concurrently captures interactions,0.7560601234436035
translation,27,8,model,interactions,between,hidden states,interactions between hidden states,0.6528207659721375
translation,27,8,model,hidden states,of,different modalities,hidden states of different modalities,0.5819932222366333
translation,27,8,model,different modalities,at,every layer,different modalities at every layer,0.5361409187316895
translation,27,8,model,model,has,spt,model has spt,0.6132522821426392
translation,27,39,model,sp - attention,creates,sparse attention matrix,sp - attention creates sparse attention matrix,0.6078674793243408
translation,27,39,model,sp - attention,propose,multimodal sp - transformer,sp - attention propose multimodal sp - transformer,0.6641334295272827
translation,27,39,model,sparse attention matrix,improves,computational and sampling efficiency,sparse attention matrix improves computational and sampling efficiency,0.6373590230941772
translation,27,39,model,sparse attention matrix,propose,multimodal sp - transformer,sparse attention matrix propose multimodal sp - transformer,0.6294896006584167
translation,27,39,model,computational and sampling efficiency,propose,multimodal sp - transformer,computational and sampling efficiency propose multimodal sp - transformer,0.5816723704338074
translation,27,39,model,multimodal sp - transformer,uses,concurrent structure,multimodal sp - transformer uses concurrent structure,0.613895833492279
translation,27,39,model,concurrent structure,of,blocks,concurrent structure of blocks,0.6473026871681213
translation,27,39,model,blocks,in,each sub-layer,blocks in each sub-layer,0.5535777807235718
translation,27,39,model,blocks,to allow,multimodal signal,blocks to allow multimodal signal,0.7051376700401306
translation,27,39,model,model,propose,multimodal sp - transformer,model propose multimodal sp - transformer,0.6953314542770386
translation,27,39,model,model,has,sp - attention,model has sp - attention,0.5905196070671082
translation,27,40,model,spt,uses,input attention,spt uses input attention,0.5882565975189209
translation,27,40,model,spt,uses,cross-attention,spt uses cross-attention,0.5679013133049011
translation,27,40,model,spt,leverage,factorized co-attention,spt leverage factorized co-attention,0.7057104110717773
translation,27,40,model,input attention,on,source input sequence,input attention on source input sequence,0.5656351447105408
translation,27,40,model,cross-attention,on,hidden state pairs,cross-attention on hidden state pairs,0.5538804531097412
translation,27,40,model,hidden state pairs,of,different modalities,hidden state pairs of different modalities,0.5569576025009155
translation,27,40,model,hidden state pairs,of,self-attention,hidden state pairs of self-attention,0.5762320160865784
translation,27,40,model,self-attention,on,hidden states,self-attention on hidden states,0.5928837060928345
translation,27,40,model,hidden states,of,each modality,hidden states of each modality,0.5771176815032959
translation,27,40,model,factorized co-attention,that use,factorized form,factorized co-attention that use factorized form,0.6832307577133179
translation,27,40,model,factorized form,of,attention computation,factorized form of attention computation,0.5685617923736572
translation,27,40,model,attention computation,based on,affinity matrix,attention computation based on affinity matrix,0.6453105807304382
translation,27,40,model,affinity matrix,to further reduce,number of parameters,affinity matrix to further reduce number of parameters,0.6548743844032288
translation,27,40,model,number of parameters,for,cross attention block ( co - sp ),number of parameters for cross attention block ( co - sp ),0.6040151715278625
translation,27,40,model,model,leverage,factorized co-attention,model leverage factorized co-attention,0.7391678094863892
translation,27,40,model,model,has,spt,model has spt,0.6132522821426392
translation,27,41,model,improve efficiency,of,spt,improve efficiency of spt,0.6049662232398987
translation,27,41,model,spt,by,parameter sharing,spt by parameter sharing,0.577385425567627
translation,27,41,model,parameter sharing,across,all layers,parameter sharing across all layers,0.7275158166885376
translation,27,41,model,model,further,improve efficiency,model further improve efficiency,0.596920907497406
translation,27,77,model,hidden state sequences,for,each pair of two modalities,hidden state sequences for each pair of two modalities,0.5749309659004211
translation,27,77,model,hidden state sequences,interact through,cross attention,hidden state sequences interact through cross attention,0.7045464515686035
translation,27,77,model,each pair of two modalities,interact through,cross attention,each pair of two modalities interact through cross attention,0.7555387616157532
translation,27,77,model,cross attention,by,co-sp - block,cross attention by co-sp - block,0.583130955696106
translation,27,77,model,co-sp - block,using,factorized co-attention,co-sp - block using factorized co-attention,0.6907876133918762
translation,27,77,model,model,has,hidden state sequences,model has hidden state sequences,0.5438729524612427
translation,27,178,model,linear projection,to map,"audio , video , text inputs","linear projection to map audio , video , text inputs",0.7006237506866455
translation,27,178,model,"audio , video , text inputs",to,d model,"audio , video , text inputs to d model",0.5742656588554382
translation,27,178,model,model,use,linear projection,model use linear projection,0.6319994330406189
translation,27,194,model,spt,uses,input attention block,spt uses input attention block,0.5728753209114075
translation,27,194,model,),uses,input attention block,) uses input attention block,0.5911213159561157
translation,27,194,model,input attention block,followed by,self-attention block,input attention block followed by self-attention block,0.6419849395751953
translation,27,194,model,self-attention block,in,each layer,self-attention block in each layer,0.5132891535758972
translation,27,194,model,spt,has,),spt has ),0.6734700202941895
translation,27,194,model,model,has,spt,model has spt,0.6132522821426392
translation,27,223,model,multimodal sp - transformer,uses,sequence of hidden states,multimodal sp - transformer uses sequence of hidden states,0.615723729133606
translation,27,223,model,sequence of hidden states,to sample from,longer multimodal input sequences,sequence of hidden states to sample from longer multimodal input sequences,0.6896897554397583
translation,27,223,model,model,propose,multimodal sp - transformer,model propose multimodal sp - transformer,0.6953314542770386
translation,27,225,model,concurrent structure,enables,more effective capturing,concurrent structure enables more effective capturing,0.7086020112037659
translation,27,225,model,more effective capturing,of,multimodal interaction,more effective capturing of multimodal interaction,0.562516450881958
translation,27,225,model,more effective capturing,resulting in,higher performance,more effective capturing resulting in higher performance,0.6923470497131348
translation,27,225,model,model,has,concurrent structure,model has concurrent structure,0.5541890263557434
translation,27,169,results,concurrent structure,improves,accuracy,concurrent structure improves accuracy,0.7387406229972839
translation,27,169,results,accuracy,compared with,serial structure,accuracy compared with serial structure,0.686896800994873
translation,27,169,results,richer multimodal interactions,at,every layer,richer multimodal interactions at every layer,0.5348236560821533
translation,27,169,results,results,has,concurrent structure,results has concurrent structure,0.5364403128623962
translation,27,195,results,substantial difference,in,performance,substantial difference in performance,0.5666977167129517
translation,27,195,results,performance,for,sp - block,performance for sp - block,0.6560766696929932
translation,27,195,results,results,show,substantial difference,results show substantial difference,0.666009783744812
translation,27,208,results,training time,in,"unaligned cmu - mosi , cmu - mosei , and ur - funny datasets","training time in unaligned cmu - mosi , cmu - mosei , and ur - funny datasets",0.4930068850517273
translation,27,208,results,results,test,training time,results test training time,0.7812061309814453
translation,27,224,results,reduced computational complexity,through,sparse attention,reduced computational complexity through sparse attention,0.6641221642494202
translation,27,224,results,our model,has,reduced computational complexity,our model has reduced computational complexity,0.5680949091911316
translation,28,8,model,novel absa model,with,federated learning ( fl ),novel absa model with federated learning ( fl ),0.6584933400154114
translation,28,8,model,novel absa model,incorporate,topic memory ( tm ),novel absa model incorporate topic memory ( tm ),0.6914800405502319
translation,28,8,model,federated learning ( fl ),adopted to overcome,data isolation limitations,federated learning ( fl ) adopted to overcome data isolation limitations,0.751625120639801
translation,28,8,model,topic memory ( tm ),proposed to take,cases of data,topic memory ( tm ) proposed to take cases of data,0.7053586840629578
translation,28,8,model,cases of data,from,diverse sources ( domains ),cases of data from diverse sources ( domains ),0.5671743750572205
translation,28,8,model,model,propose,novel absa model,model propose novel absa model,0.6627575755119324
translation,28,9,model,tm,aims to identify,different isolated data sources,tm aims to identify different isolated data sources,0.6414865255355835
translation,28,9,model,different isolated data sources,due to,data inaccessibility,different isolated data sources due to data inaccessibility,0.7086924910545349
translation,28,9,model,data inaccessibility,by providing,useful categorical information,data inaccessibility by providing useful categorical information,0.6561563611030579
translation,28,9,model,useful categorical information,for,localized predictions,useful categorical information for localized predictions,0.61472487449646
translation,28,9,model,model,has,tm,model has tm,0.63670814037323
translation,28,26,model,neural model,based on,fl,neural model based on fl,0.7306357622146606
translation,28,26,model,neural model,based on,fl,neural model based on fl,0.7306357622146606
translation,28,26,model,neural model,with,topic memory,neural model with topic memory,0.6496762633323669
translation,28,26,model,fl,for,absa,fl for absa,0.7519382834434509
translation,28,26,model,fl,by providing,categorical ( topic ) information,fl by providing categorical ( topic ) information,0.6543477773666382
translation,28,26,model,absa,in,distributed environment,absa in distributed environment,0.5284864902496338
translation,28,26,model,distributed environment,namely,tm - fl,distributed environment namely tm - fl,0.6765508651733398
translation,28,26,model,topic memory,to enhance,fl,topic memory to enhance fl,0.709194004535675
translation,28,26,model,fl,by providing,categorical ( topic ) information,fl by providing categorical ( topic ) information,0.6543477773666382
translation,28,26,model,model,propose,neural model,model propose neural model,0.6627441048622131
translation,28,27,model,topic model,serves as,server-side component,topic model serves as server-side component,0.6213348507881165
translation,28,27,model,server-side component,to read,different inputs,server-side component to read different inputs,0.531682550907135
translation,28,27,model,server-side component,respond with,categorical weights,server-side component respond with categorical weights,0.7310680150985718
translation,28,27,model,different inputs,from,each node,different inputs from each node,0.5898248553276062
translation,28,27,model,categorical weights,to help,backbone absa classifier,categorical weights to help backbone absa classifier,0.6313904523849487
translation,28,27,model,model,has,topic model,model has topic model,0.5422954559326172
translation,28,28,model,absa,by leveraging,extra labeled data,absa by leveraging extra labeled data,0.6621009707450867
translation,28,28,model,extra labeled data,through,fl framework,extra labeled data through fl framework,0.6398017406463623
translation,28,28,model,fl framework,enhanced by,tm,fl framework enhanced by tm,0.7052283883094788
translation,29,192,ablation-analysis,underrepresentation,of,two tags,underrepresentation of two tags,0.5925464630126953
translation,29,192,ablation-analysis,underrepresentation,effects,performance,underrepresentation effects performance,0.7168313264846802
translation,29,192,ablation-analysis,two tags,in,emota dataset,two tags in emota dataset,0.5003433227539062
translation,29,192,ablation-analysis,two tags,for,7 - class set - up,two tags for 7 - class set - up,0.6511567831039429
translation,29,192,ablation-analysis,two tags,effects,performance,two tags effects performance,0.6680007576942444
translation,29,192,ablation-analysis,ablation analysis,has,underrepresentation,ablation analysis has underrepresentation,0.5507570505142212
translation,29,165,experiments,categorical crossentropy loss,used for,ta and sentiment channels,categorical crossentropy loss used for ta and sentiment channels,0.6456229090690613
translation,29,165,experiments,binary crossentropy loss function,used for,emotion channel,binary crossentropy loss function used for emotion channel,0.6532870531082153
translation,29,234,experiments,novel dataset emota,contains,pre-annotated tweets,novel dataset emota contains pre-annotated tweets,0.5971131324768066
translation,29,234,experiments,novel dataset emota,annotated with,tas and sentiment categories,novel dataset emota annotated with tas and sentiment categories,0.7310525178909302
translation,29,234,experiments,pre-annotated tweets,with,emotions,pre-annotated tweets with emotions,0.6421374678611755
translation,29,234,experiments,emotions,collected from,open-source dataset,emotions collected from open-source dataset,0.713185727596283
translation,29,110,hyperparameters,emoji2vec,provides,d v = 300 dimensional vector representation,emoji2vec provides d v = 300 dimensional vector representation,0.5735775828361511
translation,29,162,hyperparameters,bi-lstm layer,with,100 memory cells,bi-lstm layer with 100 memory cells,0.6322386860847473
translation,29,162,hyperparameters,different modalities,has,bi-lstm layer,different modalities has bi-lstm layer,0.5515227913856506
translation,29,162,hyperparameters,hyperparameters,To encode,different modalities,hyperparameters To encode different modalities,0.6876967549324036
translation,29,166,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,29,166,hyperparameters,learning rate,used in,final experimental setting,learning rate used in final experimental setting,0.6657951474189758
translation,29,166,hyperparameters,adam optimizer,used in,final experimental setting,adam optimizer used in final experimental setting,0.64727383852005
translation,29,166,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,29,9,model,dyadic attention mechanism ( dam ),has,"based multi-modal , adversarial multi-tasking framework","dyadic attention mechanism ( dam ) has based multi-modal , adversarial multi-tasking framework",0.5179898142814636
translation,29,9,model,model,propose,dyadic attention mechanism ( dam ),model propose dyadic attention mechanism ( dam ),0.6647624969482422
translation,29,10,model,dam,incorporates,intra-modal and inter-modal attention,dam incorporates intra-modal and inter-modal attention,0.6383128762245178
translation,29,10,model,dam,learns,generalized features,dam learns generalized features,0.7332034111022949
translation,29,10,model,intra-modal and inter-modal attention,to fuse,multiple modalities,intra-modal and inter-modal attention to fuse multiple modalities,0.6419222950935364
translation,29,10,model,generalized features,across,all the tasks,generalized features across all the tasks,0.6940180063247681
translation,29,10,model,model,has,dam,model has dam,0.6231600046157837
translation,29,37,model,based multi-task adversarial learning framework,for,"multi-modal tac , sa and er","based multi-task adversarial learning framework for multi-modal tac , sa and er",0.5926404595375061
translation,29,37,model,dyadic attention mechanism ( dam ),has,based multi-task adversarial learning framework,dyadic attention mechanism ( dam ) has based multi-task adversarial learning framework,0.5077179074287415
translation,29,37,model,model,propose,dyadic attention mechanism ( dam ),model propose dyadic attention mechanism ( dam ),0.6647624969482422
translation,29,38,model,dam,incorporate,intra-modal and inter-modal attention,dam incorporate intra-modal and inter-modal attention,0.6112880706787109
translation,29,38,model,dam,learn,generalized features,dam learn generalized features,0.7017776966094971
translation,29,38,model,intra-modal and inter-modal attention,to integrate,information,intra-modal and inter-modal attention to integrate information,0.6553454995155334
translation,29,38,model,intra-modal and inter-modal attention,learn,generalized features,intra-modal and inter-modal attention learn generalized features,0.639850914478302
translation,29,38,model,information,across,multiple modalities,information across multiple modalities,0.713330864906311
translation,29,38,model,generalized features,across,multiple tasks,generalized features across multiple tasks,0.7266848087310791
translation,29,38,model,model,In,dam,model In dam,0.60623699426651
translation,29,164,model,three channels,contain,"7 , 3 and 11 output neurons","three channels contain 7 , 3 and 11 output neurons",0.5454795956611633
translation,29,164,model,"7 , 3 and 11 output neurons",for,ta,"7 , 3 and 11 output neurons for ta",0.6415930390357971
translation,29,164,model,"7 , 3 and 11 output neurons",for,sentiment and emotion tags,"7 , 3 and 11 output neurons for sentiment and emotion tags",0.6092522144317627
translation,29,164,model,model,has,three channels,model has three channels,0.590981662273407
translation,29,235,model,adversarial multi-task framework,joint optimization of,"tas , sentiment and emotions","adversarial multi-task framework joint optimization of tas , sentiment and emotions",0.7000615000724792
translation,29,235,model,dyadic attention mechanism,has,based multi-modal ( emojis and text ),dyadic attention mechanism has based multi-modal ( emojis and text ),0.5938184857368469
translation,29,235,model,model,propose,dyadic attention mechanism,model propose dyadic attention mechanism,0.6390876173973083
translation,29,236,model,dam ( dyadic attention mechanism ) module,employs,intra-modal and inter-modal attention,dam ( dyadic attention mechanism ) module employs intra-modal and inter-modal attention,0.5388191342353821
translation,29,236,model,intra-modal and inter-modal attention,to fuse,multiple modalities,intra-modal and inter-modal attention to fuse multiple modalities,0.6419222950935364
translation,29,236,model,intra-modal and inter-modal attention,learn,generalized features,intra-modal and inter-modal attention learn generalized features,0.639850914478302
translation,29,236,model,generalized features,across,all the tasks,generalized features across all the tasks,0.6940180063247681
translation,29,236,model,model,has,dam ( dyadic attention mechanism ) module,model has dam ( dyadic attention mechanism ) module,0.5710484981536865
translation,29,11,results,proposed framework,boosts,performance,proposed framework boosts performance,0.6363128423690796
translation,29,11,results,performance,of,primary task,performance of primary task,0.5629546642303467
translation,29,11,results,performance,i.e.,ta classification ( tac ),performance i.e. ta classification ( tac ),0.6990882158279419
translation,29,11,results,performance,by benefitting from,two secondary tasks,performance by benefitting from two secondary tasks,0.69566810131073
translation,29,11,results,ta classification ( tac ),by benefitting from,two secondary tasks,ta classification ( tac ) by benefitting from two secondary tasks,0.6954102516174316
translation,29,11,results,two secondary tasks,i.e.,sentiment and emotion analysis,two secondary tasks i.e. sentiment and emotion analysis,0.6498808860778809
translation,29,11,results,sentiment and emotion analysis,compared to,uni-modal and single task tac ( tweet act classification ) variants,sentiment and emotion analysis compared to uni-modal and single task tac ( tweet act classification ) variants,0.6307427883148193
translation,29,11,results,results,indicate,proposed framework,results indicate proposed framework,0.588329017162323
translation,29,40,results,multi-modal and multi-task tac,performs,significantly better,multi-modal and multi-task tac performs significantly better,0.6118166446685791
translation,29,40,results,significantly better,than,uni-modal and single task tac variants,significantly better than uni-modal and single task tac variants,0.5625451803207397
translation,29,40,results,results,has,multi-modal and multi-task tac,results has multi-modal and multi-task tac,0.533682107925415
translation,29,186,results,non-verbal cues,in the form of,emojis,non-verbal cues in the form of emojis,0.7408797144889832
translation,29,186,results,improves,has,uni-modal textual baseline,improves has uni-modal textual baseline,0.6017552018165588
translation,29,186,results,uni-modal textual baseline,has,consistently,uni-modal textual baseline has consistently,0.5868269801139832
translation,29,186,results,results,addition of,non-verbal cues,results addition of non-verbal cues,0.5395450592041016
translation,29,189,results,emoji,as,textual feature,emoji as textual feature,0.49437415599823
translation,29,189,results,different modality,in,single task tac framework,different modality in single task tac framework,0.535312294960022
translation,29,189,results,results,for utilizing,emoji,results for utilizing emoji,0.7060969471931458
translation,29,190,results,five-class set - up,gave,better results,five-class set - up gave better results,0.649511456489563
translation,29,190,results,better results,than,seven - class set up,better results than seven - class set up,0.6144582629203796
translation,29,190,results,results,has,five-class set - up,results has five-class set - up,0.5363449454307556
translation,29,193,results,multi-task framework,with,"all the three tasks ( i.e. , tac + sa + er )","multi-task framework with all the three tasks ( i.e. , tac + sa + er )",0.6122452616691589
translation,29,193,results,multi-task framework,consistently gave,better results,multi-task framework consistently gave better results,0.7645144462585449
translation,29,193,results,better results,compared to,single task tac,better results compared to single task tac,0.6824324131011963
translation,29,194,results,tac + sa,shows,little improvement,tac + sa shows little improvement,0.6580514907836914
translation,29,194,results,little improvement,in,different metrics,little improvement in different metrics,0.48493295907974243
translation,29,194,results,tac + er,over and above,single task tac,tac + er over and above single task tac,0.671141505241394
translation,29,194,results,bi-task variant,has,tac + sa,bi-task variant has tac + sa,0.6120011210441589
translation,29,194,results,results,In,bi-task variant,results In bi-task variant,0.5356688499450684
translation,29,207,results,varying ways of multitasking,observed that,sp model,varying ways of multitasking observed that sp model,0.6849471926689148
translation,29,207,results,sp,with,adversarial loss ( adv ),sp with adversarial loss ( adv ),0.6499645709991455
translation,29,207,results,sp,observed that,sp model,sp observed that sp model,0.7064299583435059
translation,29,207,results,sp model,gave,better results,sp model gave better results,0.7055653929710388
translation,29,207,results,better results,compared to,fs model,better results compared to fs model,0.6971291899681091
translation,29,207,results,varying ways of multitasking,has,sp,varying ways of multitasking has sp,0.5639001131057739
translation,29,207,results,fs,has,sp,fs has sp,0.734394371509552
translation,29,207,results,results,In terms of,varying ways of multitasking,results In terms of varying ways of multitasking,0.6883631348609924
translation,29,208,results,adversarial loss,boosted,performance,adversarial loss boosted performance,0.6690315008163452
translation,29,208,results,performance,of,different multi-task models,performance of different multi-task models,0.590144693851471
translation,29,208,results,results,incorporating,adversarial loss,results incorporating adversarial loss,0.6813888549804688
translation,29,211,results,different attentions,used for,best performing multi-task model,different attentions used for best performing multi-task model,0.6448123455047607
translation,29,211,results,results,importance of,different attentions,results importance of different attentions,0.688970148563385
translation,30,5,baselines,sparse representation,can be,efficiently implemented,sparse representation can be efficiently implemented,0.5717396140098572
translation,30,5,baselines,efficiently implemented,as,inverted index,efficiently implemented as inverted index,0.5614404082298279
translation,30,29,experiments,first sparta,shows,strong domain generalization ability,first sparta shows strong domain generalization ability,0.5679142475128174
translation,30,29,experiments,first sparta,achieves,best performance,first sparta achieves best performance,0.6798684597015381
translation,30,29,experiments,best performance,compared to,classic ir method,best performance compared to classic ir method,0.6387693881988525
translation,30,29,experiments,best performance,compared to,other learning methods,best performance compared to other learning methods,0.6395224332809448
translation,30,29,experiments,other learning methods,in,low-resources domains,other learning methods in low-resources domains,0.4850558936595917
translation,30,4,model,model,introduce,sparta,model introduce sparta,0.6502888202667236
translation,30,22,model,sparta ( sparse transformer matching ),has,novel neural ranking model,sparta ( sparse transformer matching ) has novel neural ranking model,0.5463204979896545
translation,30,22,model,model,introduce,sparta ( sparse transformer matching ),model introduce sparta ( sparse transformer matching ),0.6416409611701965
translation,30,23,model,token - level interaction,between,every query and answer token pair,token - level interaction between every query and answer token pair,0.688866376876831
translation,30,23,model,token - level interaction,leading to,superior retrieval performance,token - level interaction leading to superior retrieval performance,0.6953116655349731
translation,30,24,model,sparse answer representations,model,potential interaction,sparse answer representations model potential interaction,0.7266682982444763
translation,30,24,model,potential interaction,between,every query term,potential interaction between every query term,0.6217591762542725
translation,30,27,results,proposed spartaqa system,achieves,new stateof - the- art results,proposed spartaqa system achieves new stateof - the- art results,0.6400958895683289
translation,30,27,results,new stateof - the- art results,across,15 different domains,new stateof - the- art results across 15 different domains,0.6536785364151001
translation,30,27,results,new stateof - the- art results,across,2 languages,new stateof - the- art results across 2 languages,0.6131555438041687
translation,30,27,results,2 languages,with,significant performance gain,2 languages with significant performance gain,0.647613525390625
translation,30,27,results,significant performance gain,including,opensquad,significant performance gain including opensquad,0.6948579549789429
translation,30,27,results,significant performance gain,including,opencmrc,significant performance gain including opencmrc,0.7091006636619568
translation,30,27,results,significant performance gain,including,etc,significant performance gain including etc,0.7216905951499939
translation,30,27,results,results,has,proposed spartaqa system,results has proposed spartaqa system,0.6210412979125977
translation,31,108,baselines,"lstm , bert - base , and bert - large",as,text encoder,"lstm , bert - base , and bert - large as text encoder",0.5029577612876892
translation,31,108,baselines,words,in,real samples,words in real samples,0.5081813931465149
translation,31,108,baselines,real samples,with,synonyms,real samples with synonyms,0.5969439744949341
translation,31,108,baselines,synonyms,from,wordnet,synonyms from wordnet,0.47392985224723816
translation,31,108,baselines,synonyms,to generate,synonymous samples,synonyms to generate synonymous samples,0.6089966893196106
translation,31,108,baselines,baselines,employ,"lstm , bert - base , and bert - large","baselines employ lstm , bert - base , and bert - large",0.5001193284988403
translation,31,109,baselines,back- tran,translates,real,back- tran translates real,0.715012788772583
translation,31,109,baselines,back- tran,translates it back to,source language,back- tran translates it back to source language,0.6790032982826233
translation,31,109,baselines,real,to,other language,real to other language,0.5759230852127075
translation,31,109,baselines,source language,to get,synonymous samples,source language to get synonymous samples,0.6582463979721069
translation,31,109,baselines,back- tran,has,),back- tran has ),0.6873273849487305
translation,31,109,baselines,baselines,has,back- tran,baselines has back- tran,0.6146595478057861
translation,31,110,baselines,language model,to obtain,synonyms,language model to obtain synonyms,0.5520969033241272
translation,31,110,baselines,language model,randomly replaces,words,language model randomly replaces words,0.6915287375450134
translation,31,110,baselines,synonyms,for,each word,synonyms for each word,0.5624315142631531
translation,31,110,baselines,words,with,synonyms,words with synonyms,0.5573168992996216
translation,31,110,baselines,words,to obtain,adversarial samples,words to obtain adversarial samples,0.5643887519836426
translation,31,111,baselines,vat,improves,model robustness,vat improves model robustness,0.6785385012626648
translation,31,111,baselines,"al. , 2017 )",improves,model robustness,"al. , 2017 ) improves model robustness",0.6107215881347656
translation,31,111,baselines,model robustness,by adding,random perturbation,model robustness by adding random perturbation,0.7124195098876953
translation,31,111,baselines,random perturbation,to,embedding layer,random perturbation to embedding layer,0.5275986194610596
translation,31,111,baselines,embedding layer,to obtain,new adversarial examples,embedding layer to obtain new adversarial examples,0.566064178943634
translation,31,111,baselines,baselines,has,vat,baselines has vat,0.5670900940895081
translation,31,112,baselines,"lexicalat ( xu et al. , 2019 )",first uses,generator,"lexicalat ( xu et al. , 2019 ) first uses generator",0.7552493214607239
translation,31,112,baselines,"lexicalat ( xu et al. , 2019 )",jointly optimizes,generator and the discriminator,"lexicalat ( xu et al. , 2019 ) jointly optimizes generator and the discriminator",0.732206404209137
translation,31,112,baselines,generator,to randomly replace,words,generator to randomly replace words,0.7517526149749756
translation,31,112,baselines,words,with,"synonym , hyponym or hypernym","words with synonym , hyponym or hypernym",0.6123741269111633
translation,31,112,baselines,"synonym , hyponym or hypernym",to obtain,new samples,"synonym , hyponym or hypernym to obtain new samples",0.5714682340621948
translation,31,112,baselines,generator and the discriminator,based on,adversarial learning,generator and the discriminator based on adversarial learning,0.6645207405090332
translation,31,112,baselines,baselines,has,"lexicalat ( xu et al. , 2019 )","baselines has lexicalat ( xu et al. , 2019 )",0.5249496102333069
translation,31,113,baselines,dsa,first replaces,original words,dsa first replaces original words,0.7079789638519287
translation,31,113,baselines,dsa,employs,original and antonymous samples,dsa employs original and antonymous samples,0.5448059439659119
translation,31,113,baselines,original words,with,antonyms,original words with antonyms,0.5326042771339417
translation,31,113,baselines,antonyms,from,word - net,antonyms from word - net,0.5408233404159546
translation,31,113,baselines,original and antonymous samples,for,dual sentiment analysis,original and antonymous samples for dual sentiment analysis,0.5897910594940186
translation,31,113,baselines,dual sentiment analysis,under,softmax regression,dual sentiment analysis under softmax regression,0.6267041563987732
translation,31,113,baselines,baselines,has,dsa,baselines has dsa,0.5477825403213501
translation,31,114,baselines,"agc ( wang and culotta , 2021 )",first uses,wordnet,"agc ( wang and culotta , 2021 ) first uses wordnet",0.6389485001564026
translation,31,114,baselines,"agc ( wang and culotta , 2021 )",first uses,word substitution method,"agc ( wang and culotta , 2021 ) first uses word substitution method",0.7161074280738831
translation,31,114,baselines,"agc ( wang and culotta , 2021 )",uses,word substitution method,"agc ( wang and culotta , 2021 ) uses word substitution method",0.6050508618354797
translation,31,114,baselines,wordnet,to obtain,antonyms,wordnet to obtain antonyms,0.5822159051895142
translation,31,114,baselines,antonyms,for,n most important words,antonyms for n most important words,0.5581554174423218
translation,31,114,baselines,n most important words,in,corpus,n most important words in corpus,0.5049900412559509
translation,31,114,baselines,word substitution method,to obtain,counterfactual samples,word substitution method to obtain counterfactual samples,0.5801947712898254
translation,31,114,baselines,counterfactual samples,to improve,model robustness,counterfactual samples to improve model robustness,0.6248697638511658
translation,31,114,baselines,baselines,has,"agc ( wang and culotta , 2021 )","baselines has agc ( wang and culotta , 2021 )",0.5473138093948364
translation,31,104,experiments,bert text encoder,set,batch size,bert text encoder set batch size,0.6978698372840881
translation,31,104,experiments,bert text encoder,set,learning rate,bert text encoder set learning rate,0.6678078174591064
translation,31,104,experiments,learning rate,to,8 and 2e - 5,learning rate to 8 and 2e - 5,0.5892360210418701
translation,31,101,hyperparameters,warm up stage,train,generator,warm up stage train generator,0.7232978343963623
translation,31,101,hyperparameters,warm up stage,train,both the generator and the antonymous sentiment predictor,warm up stage train both the generator and the antonymous sentiment predictor,0.6975128054618835
translation,31,101,hyperparameters,warm up stage,train,original sentiment predictor,warm up stage train original sentiment predictor,0.7139649391174316
translation,31,101,hyperparameters,warm up stage,train,both the generator and the antonymous sentiment predictor,warm up stage train both the generator and the antonymous sentiment predictor,0.6975128054618835
translation,31,101,hyperparameters,generator,for,40 epochs,generator for 40 epochs,0.5854642987251282
translation,31,101,hyperparameters,generator,train,original sentiment predictor,generator train original sentiment predictor,0.7097720503807068
translation,31,101,hyperparameters,original sentiment predictor,for,100 epochs,original sentiment predictor for 100 epochs,0.570968508720398
translation,31,101,hyperparameters,both the generator and the antonymous sentiment predictor,based on,reinforcement learning,both the generator and the antonymous sentiment predictor based on reinforcement learning,0.6504738926887512
translation,31,101,hyperparameters,reinforcement learning,for,60 epochs,reinforcement learning for 60 epochs,0.539469301700592
translation,31,101,hyperparameters,hyperparameters,In,warm up stage,hyperparameters In warm up stage,0.49243953824043274
translation,31,101,hyperparameters,hyperparameters,train,original sentiment predictor,hyperparameters train original sentiment predictor,0.6734943389892578
translation,31,101,hyperparameters,hyperparameters,train,both the generator and the antonymous sentiment predictor,hyperparameters train both the generator and the antonymous sentiment predictor,0.6621475219726562
translation,31,102,hyperparameters,generator,set,size,generator set size,0.7279884219169617
translation,31,102,hyperparameters,generator,set,batch size,generator set batch size,0.6991673707962036
translation,31,102,hyperparameters,generator,set,learning rate,generator set learning rate,0.6817117929458618
translation,31,102,hyperparameters,size,of,hidden dimension,size of hidden dimension,0.5746702551841736
translation,31,102,hyperparameters,size,of,batch size,size of batch size,0.6234161257743835
translation,31,102,hyperparameters,size,of,learning rate,size of learning rate,0.5890579223632812
translation,31,102,hyperparameters,size,of,sentence sampling times m,size of sentence sampling times m,0.6078203320503235
translation,31,102,hyperparameters,sentence sampling times m,to,"300 , 8 , 1e - 3 , and 32","sentence sampling times m to 300 , 8 , 1e - 3 , and 32",0.588293194770813
translation,31,102,hyperparameters,hyperparameters,For,generator,hyperparameters For generator,0.5813727974891663
translation,31,103,hyperparameters,lstm text encoder,set,batch size,lstm text encoder set batch size,0.6177190542221069
translation,31,103,hyperparameters,lstm text encoder,set,size of hidden dimension,lstm text encoder set size of hidden dimension,0.6077120900154114
translation,31,103,hyperparameters,lstm text encoder,set,learning rate,lstm text encoder set learning rate,0.5756711363792419
translation,31,103,hyperparameters,lstm text encoder,set,embedding drop rate,lstm text encoder set embedding drop rate,0.5912277698516846
translation,31,103,hyperparameters,lstm text encoder,set,representation dropout rate,lstm text encoder set representation dropout rate,0.5639720559120178
translation,31,103,hyperparameters,representation dropout rate,to,"64 , 300 , 1e -3 , 0.4 , and 0.1","representation dropout rate to 64 , 300 , 1e -3 , 0.4 , and 0.1",0.5469456911087036
translation,31,103,hyperparameters,hyperparameters,For,lstm text encoder,hyperparameters For lstm text encoder,0.5060368180274963
translation,31,106,hyperparameters,parameters,optimized with,adam optimizer,parameters optimized with adam optimizer,0.6940226554870605
translation,31,106,hyperparameters,parameters,tuned on,development set,parameters tuned on development set,0.7338297963142395
translation,31,106,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,31,6,model,end-toend reinforcement learning framework,jointly performs,counterfactual data generation,end-toend reinforcement learning framework jointly performs counterfactual data generation,0.7205265760421753
translation,31,6,model,end-toend reinforcement learning framework,jointly performs,dual sentiment classification,end-toend reinforcement learning framework jointly performs dual sentiment classification,0.72553551197052
translation,31,6,model,model,propose,end-toend reinforcement learning framework,model propose end-toend reinforcement learning framework,0.6835977435112
translation,31,7,model,generator,automatically generates,massive and diverse antonymous sentences,generator automatically generates massive and diverse antonymous sentences,0.7836265563964844
translation,31,7,model,generator,iteratively generate,higher -quality antonymous samples,generator iteratively generate higher -quality antonymous samples,0.7532157897949219
translation,31,7,model,discriminator,contains,original - side sentiment predictor,discriminator contains original - side sentiment predictor,0.5719584226608276
translation,31,7,model,discriminator,contains,antonymous -side sentiment predictor,discriminator contains antonymous -side sentiment predictor,0.6077946424484253
translation,31,7,model,discriminator,help,generator,discriminator help generator,0.680752158164978
translation,31,7,model,discriminator,directly used as,final sentiment classifier,discriminator directly used as final sentiment classifier,0.6858039498329163
translation,31,7,model,generator,iteratively generate,higher -quality antonymous samples,generator iteratively generate higher -quality antonymous samples,0.7532157897949219
translation,31,7,model,discriminator,directly used as,final sentiment classifier,discriminator directly used as final sentiment classifier,0.6858039498329163
translation,31,22,model,end-to - end reinforcement learning framework,named,reinforced counterfactual data augmentation ( rcda ),end-to - end reinforcement learning framework named reinforced counterfactual data augmentation ( rcda ),0.6712717413902283
translation,31,22,model,end-to - end reinforcement learning framework,for,dual sentiment classification,end-to - end reinforcement learning framework for dual sentiment classification,0.5502049326896667
translation,31,22,model,reinforced counterfactual data augmentation ( rcda ),for,joint counterfactual data augmentation,reinforced counterfactual data augmentation ( rcda ) for joint counterfactual data augmentation,0.6186631321907043
translation,31,22,model,reinforced counterfactual data augmentation ( rcda ),for,dual sentiment classification,reinforced counterfactual data augmentation ( rcda ) for dual sentiment classification,0.6277786493301392
translation,31,22,model,model,propose,end-to - end reinforcement learning framework,model propose end-to - end reinforcement learning framework,0.6760078072547913
translation,31,23,model,dual sentiment classification modules,regarded as,generator and a discriminator,dual sentiment classification modules regarded as generator and a discriminator,0.5756207704544067
translation,31,23,model,dual sentiment classification modules,integrated in,reinforcement learning framework,dual sentiment classification modules integrated in reinforcement learning framework,0.6362009048461914
translation,31,23,model,model,has,counterfactual sentence generation,model has counterfactual sentence generation,0.5366325974464417
translation,31,24,model,one-tomany antonym and synonym lists,obtained from,wordnet,one-tomany antonym and synonym lists obtained from wordnet,0.6053820848464966
translation,31,24,model,one-tomany antonym and synonym lists,to generate,massive antonymous candidates,one-tomany antonym and synonym lists to generate massive antonymous candidates,0.7121157646179199
translation,31,24,model,massive antonymous candidates,based on,multi-label learning,massive antonymous candidates based on multi-label learning,0.5807567238807678
translation,31,24,model,best antonymous sentence,based on,reinforcement learning,best antonymous sentence based on reinforcement learning,0.6167916655540466
translation,31,32,model,antonymous and original samples,as,pairs,antonymous and original samples as pairs,0.5899985432624817
translation,31,32,model,antonymous and original samples,feed them to,discriminator,antonymous and original samples feed them to discriminator,0.6295415163040161
translation,31,32,model,discriminator,for,dual training and prediction,discriminator for dual training and prediction,0.5969334244728088
translation,31,32,model,discriminator,alleviates,spurious association problem,discriminator alleviates spurious association problem,0.7287067174911499
translation,31,32,model,spurious association problem,in,sentiment classification,spurious association problem in sentiment classification,0.5145894289016724
translation,31,32,model,model,regard,antonymous and original samples,model regard antonymous and original samples,0.6203927993774414
translation,31,32,model,model,feed them to,discriminator,model feed them to discriminator,0.649038553237915
translation,31,117,results,all the compared systems,by using,"lstm , bert - base , and bert - large","all the compared systems by using lstm , bert - base , and bert - large",0.6449898481369019
translation,31,117,results,"lstm , bert - base , and bert - large",as,our text encoder,"lstm , bert - base , and bert - large as our text encoder",0.49499526619911194
translation,31,117,results,our rcda method,has,consistently outperforms,our rcda method has consistently outperforms,0.6045581698417664
translation,31,117,results,consistently outperforms,has,all the compared systems,consistently outperforms has all the compared systems,0.5988839864730835
translation,31,117,results,results,observe,our rcda method,results observe our rcda method,0.5373309850692749
translation,31,118,results,outperforms,by around,2 absolute percentage points,outperforms by around 2 absolute percentage points,0.7714290022850037
translation,31,118,results,baseline approach,by around,2 absolute percentage points,baseline approach by around 2 absolute percentage points,0.6854258179664612
translation,31,118,results,2 absolute percentage points,on,accuracy,2 absolute percentage points on accuracy,0.5429591536521912
translation,31,118,results,accuracy,for,each data set,accuracy for each data set,0.6605020761489868
translation,31,118,results,lstm text encoder,has,rcda,lstm text encoder has rcda,0.5364476442337036
translation,31,118,results,rcda,has,outperforms,rcda has outperforms,0.6361355781555176
translation,31,118,results,outperforms,has,baseline approach,outperforms has baseline approach,0.6255874633789062
translation,31,118,results,results,for,lstm text encoder,results for lstm text encoder,0.5615063309669495
translation,31,119,results,rcda,outperforms,bert - base,rcda outperforms bert - base,0.7588773965835571
translation,31,119,results,bert - base,by,0.46 %,bert - base by 0.46 %,0.5754464864730835
translation,31,119,results,bert - base,by,0.36 %,bert - base by 0.36 %,0.5784401297569275
translation,31,119,results,bert - base,by,1.09 %,bert - base by 1.09 %,0.5870839953422546
translation,31,119,results,bert - base,by,0.4 %,bert - base by 0.4 %,0.5949905514717102
translation,31,119,results,0.46 %,on,sst -2,0.46 % on sst -2,0.5776009559631348
translation,31,119,results,0.46 %,on,sst -5,0.46 % on sst -5,0.5974200367927551
translation,31,119,results,0.36 %,on,sst -5,0.36 % on sst -5,0.6010051965713501
translation,31,119,results,1.09 %,on,rt,1.09 % on rt,0.670504629611969
translation,31,119,results,0.4 %,on,yelp,0.4 % on yelp,0.5976778864860535
translation,31,119,results,bert text encoder,has,rcda,bert text encoder has rcda,0.5541983842849731
translation,31,119,results,results,For,bert text encoder,results For bert text encoder,0.596274733543396
translation,31,120,results,bert - large,reaches,highly competitive results,bert - large reaches highly competitive results,0.7518163919448853
translation,31,120,results,performance,across,four datasets,performance across four datasets,0.6978843212127686
translation,31,120,results,our rcda approach,has,significantly boost,our rcda approach has significantly boost,0.5751357078552246
translation,31,120,results,significantly boost,has,performance,significantly boost has performance,0.5819864869117737
translation,31,120,results,results,has,bert - large,results has bert - large,0.6052632927894592
translation,31,121,results,most existing data augmentation - based methods,including,synda,most existing data augmentation - based methods including synda,0.7194470763206482
translation,31,121,results,most existing data augmentation - based methods,including,conda,most existing data augmentation - based methods including conda,0.6667905449867249
translation,31,121,results,most existing data augmentation - based methods,including,vat,most existing data augmentation - based methods including vat,0.6963180303573608
translation,31,121,results,most existing data augmentation - based methods,including,dsa,most existing data augmentation - based methods including dsa,0.6888183951377869
translation,31,121,results,most existing data augmentation - based methods,including,agc,most existing data augmentation - based methods including agc,0.7091765999794006
translation,31,121,results,agc,across,four datasets,agc across four datasets,0.7461937665939331
translation,31,121,results,rcda approach,has,consistently outperforms,rcda approach has consistently outperforms,0.6248309016227722
translation,31,121,results,consistently outperforms,has,most existing data augmentation - based methods,consistently outperforms has most existing data augmentation - based methods,0.5539087057113647
translation,31,121,results,results,observe that,rcda approach,results observe that rcda approach,0.608716607093811
translation,31,122,results,our rcda method,achieve,better performance,our rcda method achieve better performance,0.6039889454841614
translation,31,122,results,better performance,across,four datasets,better performance across four datasets,0.6609357595443726
translation,31,122,results,bert - large,as,text encoder,bert - large as text encoder,0.5933511853218079
translation,32,6,ablation-analysis,several orthogonal strategies,to drastically reduce,footprint,several orthogonal strategies to drastically reduce footprint,0.7715303897857666
translation,32,6,ablation-analysis,footprint,of,retrieve -andread open-domain qa system,footprint of retrieve -andread open-domain qa system,0.5989022254943848
translation,32,6,ablation-analysis,ablation analysis,discuss,several orthogonal strategies,ablation analysis discuss several orthogonal strategies,0.5976371765136719
translation,32,109,ablation-analysis,process,of,unifying,process of unifying,0.6253196001052856
translation,32,109,ablation-analysis,retriever and reader,into,single model,retriever and reader into single model,0.6280770897865295
translation,32,109,ablation-analysis,drops,by,1.11,drops by 1.11,0.6329972743988037
translation,32,109,ablation-analysis,em,by,1.11,em by 1.11,0.5570625066757202
translation,32,109,ablation-analysis,increases,by,2.77 %,increases by 2.77 %,0.6498345136642456
translation,32,109,ablation-analysis,increases,with,iterative finetuning,increases with iterative finetuning,0.6966832876205444
translation,32,109,ablation-analysis,2.77 %,with,iterative finetuning,2.77 % with iterative finetuning,0.6380167007446289
translation,32,109,ablation-analysis,unifying,has,retriever and reader,unifying has retriever and reader,0.5986703038215637
translation,32,109,ablation-analysis,unifying,has,drops,unifying has drops,0.5945711731910706
translation,32,109,ablation-analysis,drops,has,em,drops has em,0.5538190603256226
translation,32,109,ablation-analysis,accuracy,has,increases,accuracy has increases,0.6037072539329529
translation,32,109,ablation-analysis,ablation analysis,has,process,ablation analysis has process,0.49672770500183105
translation,32,110,ablation-analysis,em,by,1.5 %,em by 1.5 %,0.6106031537055969
translation,32,110,ablation-analysis,em,by,0.38 %,em by 0.38 %,0.6245997548103333
translation,32,110,ablation-analysis,em,by,0.38 %,em by 0.38 %,0.6245997548103333
translation,32,110,ablation-analysis,l recon,drops,em,l recon drops em,0.5957598090171814
translation,32,110,ablation-analysis,em,by,0.38 %,em by 0.38 %,0.6245997548103333
translation,32,110,ablation-analysis,knowledge distillation step,has,drops,knowledge distillation step has drops,0.6251198053359985
translation,32,110,ablation-analysis,drops,has,em,drops has em,0.5538190603256226
translation,32,111,ablation-analysis,post-training compression techniques,reduces,system footprint,post-training compression techniques reduces system footprint,0.7023220658302307
translation,32,111,ablation-analysis,system footprint,by,large margin,system footprint by large margin,0.5835511684417725
translation,32,111,ablation-analysis,system footprint,sacrificing,little accuracy,system footprint sacrificing little accuracy,0.6807615160942078
translation,32,111,ablation-analysis,ablation analysis,Applying,post-training compression techniques,ablation analysis Applying post-training compression techniques,0.7533056735992432
translation,32,22,hyperparameters,passage filter,to reduce,corpus size,passage filter to reduce corpus size,0.6265087127685547
translation,32,22,hyperparameters,hyperparameters,train,passage filter,hyperparameters train passage filter,0.6900551915168762
translation,32,21,model,problem-specific techniques,to size down,conventional retrieve & read system,problem-specific techniques to size down conventional retrieve & read system,0.7450541257858276
translation,32,21,model,model,utilize,generic approaches,model utilize generic approaches,0.624180257320404
translation,32,21,model,model,combine them with,problem-specific techniques,model combine them with problem-specific techniques,0.7494996190071106
translation,32,23,model,knowledge distillation,to make,single-encoder lightweight model,knowledge distillation to make single-encoder lightweight model,0.5656766295433044
translation,32,23,model,single-encoder lightweight model,can perform,retrieval and reading,single-encoder lightweight model can perform retrieval and reading,0.6956042051315308
translation,32,23,model,model,apply,parameter sharing strategies,model apply parameter sharing strategies,0.6219104528427124
translation,32,23,model,model,apply,knowledge distillation,model apply knowledge distillation,0.656205952167511
translation,32,116,results,performance,of,our system,performance of our system,0.5908740162849426
translation,32,116,results,our system,higher than,all of the parametric baselines,our system higher than all of the parametric baselines,0.6154570579528809
translation,32,116,results,accuracy drop,from,dpr,accuracy drop from dpr,0.5608585476875305
translation,32,116,results,dpr,is,only 2.45 %,dpr is only 2.45 %,0.5647044777870178
translation,32,116,results,dpr,is,about 4 %,dpr is about 4 %,0.6099341511726379
translation,32,116,results,dpr,reducing,system footprint,dpr reducing system footprint,0.7149910926818848
translation,32,116,results,only 2.45 %,on,efficientqa dev set,only 2.45 % on efficientqa dev set,0.5702883005142212
translation,32,116,results,about 4 %,on,test set,about 4 % on test set,0.586182713508606
translation,32,116,results,system footprint,to,0.6 %,system footprint to 0.6 %,0.5121368765830994
translation,32,116,results,0.6 %,of,original size,0.6 % of original size,0.6153743267059326
translation,32,116,results,results,has,performance,results has performance,0.5972660779953003
translation,32,117,results,final system,achieves,first place,final system achieves first place,0.6840270161628723
translation,32,117,results,final system,achieves,second place,final system achieves second place,0.6718311905860901
translation,32,117,results,first place,in,human ( manual ) evaluation,first place in human ( manual ) evaluation,0.5115380883216858
translation,32,117,results,first place,in,automatic evaluation,first place in automatic evaluation,0.5216834545135498
translation,32,117,results,second place,in,automatic evaluation,second place in automatic evaluation,0.5364782810211182
translation,32,117,results,automatic evaluation,on,systems,automatic evaluation on systems,0.5866946578025818
translation,32,117,results,systems,Under,500 mb track,systems Under 500 mb track,0.6574097871780396
translation,32,117,results,500 mb track,of,efficientqa competition,500 mb track of efficientqa competition,0.5523959994316101
translation,32,117,results,results,has,final system,results has final system,0.6088947057723999
translation,32,119,results,possibly correct answers,counted as,correct,possibly correct answers counted as correct,0.6204431056976318
translation,32,119,results,accuracy,rises to,54.95 % ( 7.58 % higher,accuracy rises to 54.95 % ( 7.58 % higher,0.6448811292648315
translation,32,119,results,54.95 % ( 7.58 % higher,than,other system,54.95 % ( 7.58 % higher than other system,0.5507586002349854
translation,32,119,results,possibly correct answers,has,accuracy,possibly correct answers has accuracy,0.5900187492370605
translation,32,119,results,results,when,possibly correct answers,results when possibly correct answers,0.636158287525177
translation,33,148,baselines,two variants,of,our neural model,two variants of our neural model,0.5984857082366943
translation,33,148,baselines,our neural model,for,supervised clustering ( nsc ),our neural model for supervised clustering ( nsc ),0.6493991017341614
translation,33,148,baselines,nsc - cnn,using,word and word overlap embeddings,nsc - cnn using word and word overlap embeddings,0.6282162666320801
translation,33,148,baselines,nsc - cnn,using,bert embeddings,nsc - cnn using bert embeddings,0.6505032777786255
translation,33,148,baselines,nsc - cnn,using,bert embeddings,nsc - cnn using bert embeddings,0.6505032777786255
translation,33,148,baselines,nsc - bert,using,bert embeddings,nsc - bert using bert embeddings,0.6993831396102905
translation,33,148,baselines,bert embeddings,of,question pairs,bert embeddings of question pairs,0.61141437292099
translation,33,148,baselines,baselines,experiment with,two variants,baselines experiment with two variants,0.6694121956825256
translation,33,162,baselines,number of baselines,based on,pairwise query similarities,number of baselines based on pairwise query similarities,0.6019909977912903
translation,33,187,baselines,lsp py,is,lsp reimplementation,lsp py is lsp reimplementation,0.5792967081069946
translation,33,187,baselines,lsp reimplementation,in,python,lsp reimplementation in python,0.5080822706222534
translation,33,187,baselines,lsp reimplementation,using,text similarity,lsp reimplementation using text similarity,0.6500803828239441
translation,33,187,baselines,python,using,text similarity,python using text similarity,0.6690375804901123
translation,33,187,baselines,baselines,has,lsp py,baselines has lsp py,0.567891538143158
translation,33,149,experimental-setup,nsc - cnn,employ,fasttext 7 word embeddings,nsc - cnn employ fasttext 7 word embeddings,0.5307646989822388
translation,33,149,experimental-setup,fasttext 7 word embeddings,in dimension,300,fasttext 7 word embeddings in dimension 300,0.6696272492408752
translation,33,149,experimental-setup,fasttext 7 word embeddings,pre-trained for,english language,fasttext 7 word embeddings pre-trained for english language,0.7082991003990173
translation,33,149,experimental-setup,english language,on,wikipedia,english language on wikipedia,0.5288727283477783
translation,33,149,experimental-setup,experimental setup,For,nsc - cnn,experimental setup For nsc - cnn,0.5740327835083008
translation,33,150,experimental-setup,max length,of,questions,max length of questions,0.559765636920929
translation,33,150,experimental-setup,questions,to,50,questions to 50,0.6325157284736633
translation,33,150,experimental-setup,shorter questions,on,right,shorter questions on right,0.5139449834823608
translation,33,150,experimental-setup,experimental setup,set,max length,experimental setup set max length,0.6685183644294739
translation,33,150,experimental-setup,experimental setup,pad,shorter questions,experimental setup pad shorter questions,0.6545917987823486
translation,33,151,experimental-setup,hidden layer,set to,1 3,hidden layer set to 1 3,0.7305585741996765
translation,33,151,experimental-setup,1 3,of,size of the input layer,1 3 of size of the input layer,0.5994384288787842
translation,33,151,experimental-setup,experimental setup,size of,hidden layer,experimental setup size of hidden layer,0.6965184807777405
translation,33,152,experimental-setup,convolution filter width,varies from,1 to 3,convolution filter width varies from 1 to 3,0.6773598790168762
translation,33,152,experimental-setup,experimental setup,has,convolution filter width,experimental setup has convolution filter width,0.5086069107055664
translation,33,154,experimental-setup,gradients,to have,l,gradients to have l,0.6821033954620361
translation,33,154,experimental-setup,? norm,less than or equal to,1,? norm less than or equal to 1,0.4734882414340973
translation,33,154,experimental-setup,l,has,? norm,l has ? norm,0.6800710558891296
translation,33,188,experimental-setup,lsp py,for,100 epochs,lsp py for 100 epochs,0.6276310682296753
translation,33,188,experimental-setup,experimental setup,trained,lsp py,experimental setup trained lsp py,0.6762492656707764
translation,33,27,experiments,quora intent corpus,has,"haponchyk et al. , 2018 )","quora intent corpus has haponchyk et al. , 2018 )",0.5685083270072937
translation,33,153,experiments,nsc - bert,use,bert base model,nsc - bert use bert base model,0.6235024929046631
translation,33,153,experiments,bert base model,train for,3 epochs,bert base model train for 3 epochs,0.7730001211166382
translation,33,153,experiments,3 epochs,for,fine-tuning,3 epochs for fine-tuning,0.5750036835670471
translation,33,153,experiments,fine-tuning,on,question pair classification task,fine-tuning on question pair classification task,0.49982020258903503
translation,33,6,model,neural networks,based on,latent structured prediction loss and transformer models,neural networks based on latent structured prediction loss and transformer models,0.6626942157745361
translation,33,6,model,neural networks,to approach,supervised clustering,neural networks to approach supervised clustering,0.6256920099258423
translation,33,6,model,latent structured prediction loss and transformer models,to approach,supervised clustering,latent structured prediction loss and transformer models to approach supervised clustering,0.6760095953941345
translation,33,6,model,model,design,neural networks,model design neural networks,0.5537787675857544
translation,33,21,model,neural supervised clustering ( nsc ) models,using,structured prediction algorithms,neural supervised clustering ( nsc ) models using structured prediction algorithms,0.6736364960670471
translation,33,21,model,neural supervised clustering ( nsc ) models,using,lssvm,neural supervised clustering ( nsc ) models using lssvm,0.6631282567977905
translation,33,21,model,neural supervised clustering ( nsc ) models,using,lsp,neural supervised clustering ( nsc ) models using lsp,0.6841972470283508
translation,33,21,model,model,design,neural supervised clustering ( nsc ) models,model design neural supervised clustering ( nsc ) models,0.5513848066329956
translation,33,22,model,latent representation,of,clusters,latent representation of clusters,0.6015920042991638
translation,33,22,model,clusters,using,graph structures,clusters using graph structures,0.6245672106742859
translation,33,22,model,model,based on,latent representation,model based on latent representation,0.6574620604515076
translation,33,23,model,model score,to globally select,maxviolating constraint,model score to globally select maxviolating constraint,0.6873869299888611
translation,33,23,model,maxviolating constraint,at,each learning step,maxviolating constraint at each learning step,0.5369438529014587
translation,33,23,model,model,used together with,model score,model used together with model score,0.6413522958755493
translation,33,251,model,supervised neural clustering,based on,traditional lssvm and lsp models,supervised neural clustering based on traditional lssvm and lsp models,0.6740456223487854
translation,33,251,model,supervised neural clustering,optimizing,structural margin loss,supervised neural clustering optimizing structural margin loss,0.6704553961753845
translation,33,251,model,model,firstly proposed,supervised neural clustering,model firstly proposed supervised neural clustering,0.7286008596420288
translation,33,30,results,nsc,using,traditional cnn networks and transformer models,nsc using traditional cnn networks and transformer models,0.7131919860839844
translation,33,30,results,nsc,show,impressive boost,nsc show impressive boost,0.6910212635993958
translation,33,30,results,impressive boost,in,f1,impressive boost in f1,0.5375732183456421
translation,33,30,results,f1,of,our nsc - bert model,f1 of our nsc - bert model,0.5962566137313843
translation,33,171,results,ic&oos dataset,averaged over,10 different sample splits,ic&oos dataset averaged over 10 different sample splits,0.7036184072494507
translation,33,171,results,10 different sample splits,obtained with,10,10 different sample splits obtained with 10,0.5980986952781677
translation,33,171,results,results,on,ic&oos dataset,results on ic&oos dataset,0.554269015789032
translation,33,172,results,consistently improves,over,all the baselines,consistently improves over all the baselines,0.6866604089736938
translation,33,172,results,all the baselines,in terms of,f1 and ceaf,all the baselines in terms of f1 and ceaf,0.7290621399879456
translation,33,172,results,nsc,has,consistently improves,nsc has consistently improves,0.6356489658355713
translation,33,172,results,results,note that,nsc,results note that nsc,0.4804931581020355
translation,33,173,results,results,shows,good precision / recall balance,results shows good precision / recall balance,0.6695468425750732
translation,33,180,results,impressive performance,of,nsc,impressive performance of nsc,0.5889235138893127
translation,33,180,results,nsc,especially when fed by,bert,nsc especially when fed by bert,0.6628900170326233
translation,33,181,results,clustering f1,of,95.65,clustering f1 of 95.65,0.5470234751701355
translation,33,181,results,95.65,suggests that,nsc,95.65 suggests that nsc,0.6557906866073608
translation,33,181,results,nsc,replicate,clusters of questions,nsc replicate clusters of questions,0.7538431286811829
translation,33,181,results,results,has,clustering f1,results has clustering f1,0.5777261853218079
translation,33,189,results,nsc - cnn,improves over,state of the art,nsc - cnn improves over state of the art,0.7162679433822632
translation,33,189,results,state of the art,on,both the test sets,state of the art on both the test sets,0.5160079598426819
translation,33,189,results,state of the art,for,both measures,state of the art for both measures,0.5538114905357361
translation,33,189,results,results,has,nsc - cnn,results has nsc - cnn,0.5477761030197144
translation,33,190,results,nsc - bert,achieves,higher ceaf,nsc - bert achieves higher ceaf,0.7282519340515137
translation,33,190,results,manual test set,has,nsc - bert,manual test set has nsc - bert,0.6263229846954346
translation,33,190,results,results,On,manual test set,results On manual test set,0.5730615854263306
translation,33,192,results,evaluation,over,automatic test set,evaluation over automatic test set,0.6517324447631836
translation,33,192,results,evaluation,has,nsc - bert,evaluation has nsc - bert,0.6005058288574219
translation,33,192,results,automatic test set,has,nsc - bert,automatic test set has nsc - bert,0.5899143815040588
translation,33,192,results,nsc - bert,has,largely outperforms,nsc - bert has largely outperforms,0.6191164255142212
translation,33,192,results,largely outperforms,has,any model,largely outperforms has any model,0.5732402801513672
translation,33,192,results,results,on,evaluation,results on evaluation,0.5798072218894958
translation,33,211,results,results,of,nsc,results of nsc,0.4272690713405609
translation,33,212,results,improve,with respect to,completely disjoint setting,improve with respect to completely disjoint setting,0.645045280456543
translation,33,212,results,results,has,improve,results has improve,0.6249991655349731
translation,33,213,results,nsc - cnn,able to,almost replicate,nsc - cnn able to almost replicate,0.6026603579521179
translation,33,213,results,result,of,cnn classifier,result of cnn classifier,0.5843936204910278
translation,33,213,results,result,in terms of,f1,result in terms of f1,0.7416657209396362
translation,33,213,results,cnn classifier,in terms of,f1,cnn classifier in terms of f1,0.6861103177070618
translation,33,213,results,almost replicate,has,result,almost replicate has result,0.6236385107040405
translation,33,213,results,results,has,nsc - cnn,results has nsc - cnn,0.5477761030197144
translation,33,214,results,oos recall,is,more than 85 %,oos recall is more than 85 %,0.607610821723938
translation,33,214,results,more than 85 %,2 - 3 times the one of,classifiers,more than 85 % 2 - 3 times the one of classifiers,0.7534515857696533
translation,33,214,results,more than 85 %,means,85 %,more than 85 % means 85 %,0.6815455555915833
translation,33,214,results,results,has,oos recall,results has oos recall,0.6280154585838318
translation,33,216,results,nsc - bert,improves over,classifier model,nsc - bert improves over classifier model,0.6537011861801147
translation,33,216,results,classifier model,on,test samples,classifier model on test samples,0.557077944278717
translation,33,216,results,classifier model,by,1.5,classifier model by 1.5,0.5780717134475708
translation,33,216,results,1.5,in terms of,clustering f1,1.5 in terms of clustering f1,0.737334668636322
translation,33,216,results,results,has,nsc - bert,results has nsc - bert,0.5202484130859375
translation,33,218,results,nsc - bert,highly improves ( at least 2 times ),recall,nsc - bert highly improves ( at least 2 times ) recall,0.7383084297180176
translation,33,218,results,recall,of,classifier,recall of classifier,0.625652551651001
translation,33,218,results,classifier,for,oos task,classifier for oos task,0.6554524898529053
translation,33,218,results,results,has,nsc - bert,results has nsc - bert,0.5202484130859375
translation,33,232,results,our experiments,suggest,transformer model,our experiments suggest transformer model,0.6481996774673462
translation,33,232,results,transformer model,boosts,performance,transformer model boosts performance,0.7185078859329224
translation,33,232,results,performance,of,our clustering approach,performance of our clustering approach,0.5912233591079712
translation,33,253,results,our experiments,on,ic&oos and quora intent corpora,our experiments on ic&oos and quora intent corpora,0.5555076599121094
translation,33,253,results,ic&oos and quora intent corpora,show,impressive improvement,ic&oos and quora intent corpora show impressive improvement,0.6131967902183533
translation,33,253,results,impressive improvement,over,state of the art,impressive improvement over state of the art,0.630460262298584
translation,33,253,results,17.24 % absolute,over,unsupervised models,17.24 % absolute over unsupervised models,0.6472857594490051
translation,33,253,results,8 % points more,than,our proposed semi-supervised approaches,8 % points more than our proposed semi-supervised approaches,0.5519542694091797
translation,34,156,ablation-analysis,strongest positive effect,on,nsf 1 and sf 1,strongest positive effect on nsf 1 and sf 1,0.5443258881568909
translation,34,156,ablation-analysis,dep. edges,has,strongest positive effect,dep. edges has strongest positive effect,0.5032737851142883
translation,34,156,ablation-analysis,ablation analysis,has,dep. edges,ablation analysis has dep. edges,0.5514888167381287
translation,34,159,ablation-analysis,average benefit,is,5.2 and 4.2,average benefit is 5.2 and 4.2,0.5479064583778381
translation,34,159,ablation-analysis,5.2 and 4.2,for,nsf 1 and sf 1,5.2 and 4.2 for nsf 1 and sf 1,0.6625828742980957
translation,34,160,ablation-analysis,dep. edges,leads to,poorer scores,dep. edges leads to poorer scores,0.6547683477401733
translation,34,160,ablation-analysis,span extraction and targeted sentiment,has,dep. edges,span extraction and targeted sentiment has dep. edges,0.5673551559448242
translation,34,160,ablation-analysis,ablation analysis,On,span extraction and targeted sentiment,ablation analysis On span extraction and targeted sentiment,0.5702350735664368
translation,34,122,experiments,sentiment graph models,use,token - level mbert representations,sentiment graph models use token - level mbert representations,0.5540071129798889
translation,34,122,experiments,token - level mbert representations,addition to,word2vec skip-gram embeddings,token - level mbert representations addition to word2vec skip-gram embeddings,0.6059470176696777
translation,34,172,experiments,"english datasets ( mpqa , ds unis )",followed by,norec fine,"english datasets ( mpqa , ds unis ) followed by norec fine",0.6634604930877686
translation,34,172,experiments,"english datasets ( mpqa , ds unis )",followed by,multib ca,"english datasets ( mpqa , ds unis ) followed by multib ca",0.6624008417129517
translation,34,172,experiments,"english datasets ( mpqa , ds unis )",followed by,multib eu,"english datasets ( mpqa , ds unis ) followed by multib eu",0.6503387093544006
translation,34,123,hyperparameters,all models,for,100 epochs,all models for 100 epochs,0.6012416481971741
translation,34,123,hyperparameters,model,performs,best,model performs best,0.6411250829696655
translation,34,123,hyperparameters,best,regarding,lf 1,best regarding lf 1,0.6608026623725891
translation,34,123,hyperparameters,lf 1,on,dev set,lf 1 on dev set,0.6334023475646973
translation,34,123,hyperparameters,hyperparameters,train,all models,hyperparameters train all models,0.6430407166481018
translation,34,15,model,sentiment analysis,as,dependency graph parsing problem,sentiment analysis as dependency graph parsing problem,0.49894067645072937
translation,34,15,model,dependency graph parsing problem,where,sentiment expression,dependency graph parsing problem where sentiment expression,0.5695333480834961
translation,34,15,model,sentiment expression,is,root node,sentiment expression is root node,0.5675815343856812
translation,34,15,model,arcs,model,relationships,arcs model relationships,0.7689867615699768
translation,34,15,model,model,cast,sentiment analysis,model cast sentiment analysis,0.6455450057983398
translation,34,43,model,interactions,between,each pair of sub-tasks,interactions between each pair of sub-tasks,0.7102321982383728
translation,34,43,model,interactions,by creating,pairwise weighted attention representations,interactions by creating pairwise weighted attention representations,0.6865880489349365
translation,34,43,model,each pair of sub-tasks,has,target extraction,each pair of sub-tasks has target extraction,0.593519389629364
translation,34,128,results,"baselines imn , racl , and racl - bert",perform,well,"baselines imn , racl , and racl - bert perform well",0.6265014410018921
translation,34,128,results,well,at,extracting,well at extracting,0.6363306045532227
translation,34,128,results,well,at,expressions ( 48.7/55.4/56.3 ),well at expressions ( 48.7/55.4/56.3 ),0.5052710175514221
translation,34,128,results,struggle,with,full targeted sentiment task ( 18.0/20.1/30.3 ),struggle with full targeted sentiment task ( 18.0/20.1/30.3 ),0.6318184733390808
translation,34,128,results,norec fine,has,"baselines imn , racl , and racl - bert","norec fine has baselines imn , racl , and racl - bert",0.592727541923523
translation,34,128,results,extracting,has,"targets ( 35.9 , 45.6 , and 47.2 f 1","extracting has targets ( 35.9 , 45.6 , and 47.2 f 1",0.5061657428741455
translation,34,128,results,results,On,norec fine,results On norec fine,0.6369231939315796
translation,34,129,results,graphbased models,extract,targets,graphbased models extract targets,0.6536019444465637
translation,34,129,results,graphbased models,extract,comparable scores,graphbased models extract comparable scores,0.6718580722808838
translation,34,129,results,graphbased models,have,comparable scores,graphbased models have comparable scores,0.5325963497161865
translation,34,129,results,comparable scores,for,expressions ( 54.4/55.5 ),comparable scores for expressions ( 54.4/55.5 ),0.5770144462585449
translation,34,129,results,targets,has,better ( 50.1/54.8 ),targets has better ( 50.1/54.8 ),0.5170242786407471
translation,34,129,results,results,has,graphbased models,results has graphbased models,0.5286853909492493
translation,34,132,results,strongest baseline ( racl - bert ),on,targeted sentiment,strongest baseline ( racl - bert ) on targeted sentiment,0.5400292277336121
translation,34,132,results,targeted sentiment,on,all 5 datasets,targeted sentiment on all 5 datasets,0.49725672602653503
translation,34,132,results,graph models,has,outperform,graph models has outperform,0.6135796308517456
translation,34,132,results,outperform,has,strongest baseline ( racl - bert ),outperform has strongest baseline ( racl - bert ),0.6051915884017944
translation,34,132,results,results,has,graph models,results has graph models,0.5189014077186584
translation,34,136,results,imn,is,weakest baseline,imn is weakest baseline,0.5382088422775269
translation,34,136,results,weakest baseline,followed by,racl,weakest baseline followed by racl,0.6704828143119812
translation,34,136,results,weakest baseline,followed by,racl - bert,weakest baseline followed by racl - bert,0.6700998544692993
translation,34,136,results,results,On average,imn,results On average imn,0.7207638621330261
translation,34,137,results,racl - bert,gives over,racl,racl - bert gives over racl,0.7567471861839294
translation,34,137,results,racl,seen in,targeted metric,racl seen in targeted metric,0.6640105247497559
translation,34,137,results,results,has,main improvement,results has main improvement,0.5848939418792725
translation,34,138,results,proposed graph- based models,are,consistently the best models,proposed graph- based models are consistently the best models,0.5810885429382324
translation,34,138,results,consistently the best models,across,metrics and datasets,consistently the best models across metrics and datasets,0.7137957215309143
translation,34,138,results,results,has,proposed graph- based models,results has proposed graph- based models,0.57216477394104
translation,34,139,results,differences,between,head-first and head-final,differences between head-first and head-final,0.6827198266983032
translation,34,139,results,head-first and head-final,are,quite small,head-first and head-final are quite small,0.5691865682601929
translation,34,139,results,graph representations,has,differences,graph representations has differences,0.5930564999580383
translation,34,139,results,results,Regarding,graph representations,results Regarding graph representations,0.6097605228424072
translation,34,140,results,head-first,performs,better,head-first performs better,0.6772333979606628
translation,34,140,results,head-first,performs,slightly better,head-first performs slightly better,0.6238001585006714
translation,34,140,results,head-first,for,"others ( norec fine , mpqa , and ds unis )","head-first for others ( norec fine , mpqa , and ds unis )",0.6361637115478516
translation,34,140,results,better,on,multib ca,better on multib ca,0.6740003824234009
translation,34,140,results,better,on,multib eu,better on multib eu,0.6479859948158264
translation,34,140,results,slightly better,on,multib eu,slightly better on multib eu,0.6205353140830994
translation,34,140,results,"others ( norec fine , mpqa , and ds unis )",is,better,"others ( norec fine , mpqa , and ds unis ) is better",0.587238073348999
translation,34,140,results,head-final,is,better,head-final is better,0.6503175497055054
translation,34,140,results,"others ( norec fine , mpqa , and ds unis )",has,head-final,"others ( norec fine , mpqa , and ds unis ) has head-final",0.5641233325004578
translation,34,140,results,results,has,head-first,results has head-first,0.5373266339302063
translation,34,152,results,large effect,on,all metrics,large effect on all metrics,0.4967055320739746
translation,34,152,results,graph structure,has,large effect,graph structure has large effect,0.5668627023696899
translation,34,152,results,results,implementation of,graph structure,results implementation of graph structure,0.6708673238754272
translation,34,154,results,+ inlabel,tends to,improve,+ inlabel tends to improve,0.773246169090271
translation,34,154,results,improve,on,non-english datasets,improve on non-english datasets,0.5124187469482422
translation,34,154,results,results,on,non-english datasets,results on non-english datasets,0.5385290384292603
translation,34,154,results,improve,has,results,improve has results,0.5343002676963806
translation,34,154,results,results,has,+ inlabel,results has + inlabel,0.5690461993217468
translation,34,161,results,dep. labels,lead to,any consistent improvements,dep. labels lead to any consistent improvements,0.74281907081604
translation,34,161,results,results,has,dep. labels,results has dep. labels,0.5339462161064148
translation,34,163,results,do graph models perform better,on,sentences with multiple targets,do graph models perform better on sentences with multiple targets,0.5239006876945496
translation,34,163,results,do graph models perform better,predicting,full sentiment graph,do graph models perform better predicting full sentiment graph,0.6380904316902161
translation,34,163,results,full sentiment graph,may,larger effect,full sentiment graph may larger effect,0.65456622838974
translation,34,163,results,larger effect,on,sentences with multiple targets,larger effect on sentences with multiple targets,0.47036370635032654
translation,34,163,results,results,has,do graph models perform better,results has do graph models perform better,0.5684084296226501
translation,34,166,results,racl - bert,on,9 of 10 experiments,racl - bert on 9 of 10 experiments,0.5632165670394897
translation,34,166,results,head-first and head-final,has,outperform,head-first and head-final has outperform,0.6181967258453369
translation,34,166,results,outperform,has,racl - bert,outperform has racl - bert,0.748256266117096
translation,34,168,results,mbert features,leads to,average improvements,mbert features leads to average improvements,0.6387726664543152
translation,34,168,results,average improvements,in,all experiments,average improvements in all experiments,0.5117442011833191
translation,34,168,results,average improvements,for,extracting spans,average improvements for extracting spans,0.59653639793396
translation,34,168,results,average gain,of,4.1 pp,average gain of 4.1 pp,0.5954195857048035
translation,34,168,results,average gain,of,3.4,average gain of 3.4,0.6050191521644592
translation,34,168,results,average gain,of,3.1,average gain of 3.1,0.6062804460525513
translation,34,168,results,4.1 pp,for,holders,4.1 pp for holders,0.6061717867851257
translation,34,168,results,3.4,for,targets,3.4 for targets,0.611418604850769
translation,34,168,results,3.1,for,expressions,3.1 for expressions,0.561475932598114
translation,34,168,results,all experiments,has,average gain,all experiments has average gain,0.5689820051193237
translation,34,168,results,extracting spans,has,average gain,extracting spans has average gain,0.5935081243515015
translation,34,168,results,results,Adding,mbert features,results Adding mbert features,0.6383070945739746
translation,34,169,results,targeted sentiment,for,parsing graph metrics uf 1 and lf 1,targeted sentiment for parsing graph metrics uf 1 and lf 1,0.5808380842208862
translation,34,169,results,larger gain,of,4.2 pp,larger gain of 4.2 pp,0.5884018540382385
translation,34,169,results,gains,are,more limited ( 3.3 pp / 3.8 pp ),gains are more limited ( 3.3 pp / 3.8 pp ),0.5714882612228394
translation,34,169,results,parsing graph metrics uf 1 and lf 1,has,gains,parsing graph metrics uf 1 and lf 1 has gains,0.5805613994598389
translation,34,169,results,results,For,targeted sentiment,results For targeted sentiment,0.6564375162124634
translation,35,6,baselines,fvsqa,on,fvqa dataset,fvsqa on fvqa dataset,0.5563735961914062
translation,35,6,baselines,baselines,has,fvsqa,baselines has fvsqa,0.5499839186668396
translation,35,156,experimental-setup,embedding dimensions,for,both entity and relation embeddings,embedding dimensions for both entity and relation embeddings,0.5565252900123596
translation,35,156,experimental-setup,both entity and relation embeddings,are,n e = n r = 300,both entity and relation embeddings are n e = n r = 300,0.5815088748931885
translation,35,156,experimental-setup,experimental setup,has,embedding dimensions,experimental setup has embedding dimensions,0.521648645401001
translation,35,157,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,35,158,experimental-setup,ermlp,trained for,"25,000 epochs","ermlp trained for 25,000 epochs",0.7724790573120117
translation,35,158,experimental-setup,experimental setup,has,ermlp,experimental setup has ermlp,0.5932108163833618
translation,35,159,experimental-setup,adam optimizer,for which,learning rate,adam optimizer for which learning rate,0.5592728853225708
translation,35,159,experimental-setup,adam optimizer,scaled down by,factor,adam optimizer scaled down by factor,0.7608569264411926
translation,35,159,experimental-setup,learning rate,initialized as,0.01,learning rate initialized as 0.01,0.6600828170776367
translation,35,159,experimental-setup,learning rate,scaled down by,factor,learning rate scaled down by factor,0.7750404477119446
translation,35,159,experimental-setup,factor,of,0.1,factor of 0.1,0.6363644003868103
translation,35,159,experimental-setup,factor,after,"every 10,000 epochs","factor after every 10,000 epochs",0.7376020550727844
translation,35,159,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,35,160,experimental-setup,hyper-parameter search,for,learning rate,hyper-parameter search for learning rate,0.6164105534553528
translation,35,160,experimental-setup,learning rate,performed by,choosing,learning rate performed by choosing,0.5791056156158447
translation,35,160,experimental-setup,experimental setup,has,hyper-parameter search,experimental setup has hyper-parameter search,0.5220959782600403
translation,35,161,experimental-setup,temperature hyperparameter,for,self-adversarial probability parameterization,temperature hyperparameter for self-adversarial probability parameterization,0.5907952785491943
translation,35,161,experimental-setup,self-adversarial probability parameterization,set to,1,self-adversarial probability parameterization set to 1,0.6490086913108826
translation,35,161,experimental-setup,experimental setup,has,temperature hyperparameter,experimental setup has temperature hyperparameter,0.46332764625549316
translation,35,162,experimental-setup,number of adversarial samples n,generated for,each positive sample,number of adversarial samples n generated for each positive sample,0.6707566976547241
translation,35,162,experimental-setup,each positive sample,is,16,each positive sample is 16,0.6300180554389954
translation,35,162,experimental-setup,experimental setup,has,number of adversarial samples n,experimental setup has number of adversarial samples n,0.5106565356254578
translation,35,167,experimental-setup,all layers,are,fully connected,all layers are fully connected,0.5375716090202332
translation,35,167,experimental-setup,experimental setup,has,all layers,experimental setup has all layers,0.5241839289665222
translation,35,174,experimental-setup,optimizer,used,stochastic gradient descent,optimizer used stochastic gradient descent,0.5938571095466614
translation,35,174,experimental-setup,optimizer,is,stochastic gradient descent,optimizer is stochastic gradient descent,0.540865957736969
translation,35,174,experimental-setup,stochastic gradient descent,with,batch size,stochastic gradient descent with batch size,0.6473997831344604
translation,35,174,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,35,174,experimental-setup,fusion function,has,optimizer,fusion function has optimizer,0.559809684753418
translation,35,174,experimental-setup,experimental setup,To train,fusion function,experimental setup To train fusion function,0.6717165112495422
translation,35,175,experimental-setup,100 epochs,with,learning rate,100 epochs with learning rate,0.6320255994796753
translation,35,175,experimental-setup,100 epochs,with,weight decay,100 epochs with weight decay,0.6120365858078003
translation,35,175,experimental-setup,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,35,175,experimental-setup,weight decay,of,1e - 3,weight decay of 1e - 3,0.6349888443946838
translation,35,175,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,35,176,experimental-setup,fully - connected layers,use,dropout probability,fully - connected layers use dropout probability,0.5923046469688416
translation,35,176,experimental-setup,dropout probability,of,0.3,dropout probability of 0.3,0.5935159921646118
translation,35,176,experimental-setup,experimental setup,has,fully - connected layers,experimental setup has fully - connected layers,0.5195494890213013
translation,35,177,experimental-setup,gpu servers,provided by,google colab,gpu servers provided by google colab,0.6323619484901428
translation,35,177,experimental-setup,experimental setup,trained using,gpu servers,experimental setup trained using gpu servers,0.6251792907714844
translation,35,180,experiments,fvsqa,trained using,best performing kg embedding model,fvsqa trained using best performing kg embedding model,0.7361046075820923
translation,35,183,experiments,english,investigate,asr + text - based system,english investigate asr + text - based system,0.6417397260665894
translation,35,183,experiments,asr + text - based system,where,fvqa model,asr + text - based system where fvqa model,0.6152608394622803
translation,35,183,experiments,fvqa model,trained on,gold-standard textual questions,fvqa model trained on gold-standard textual questions,0.6776961088180542
translation,35,183,experiments,fvqa model,dur-ing,inference-time,fvqa model dur-ing inference-time,0.7068958282470703
translation,35,183,experiments,asr - converted speech transcript,of,question,asr - converted speech transcript of question,0.5709648132324219
translation,35,183,experiments,inference-time,has,asr - converted speech transcript,inference-time has asr - converted speech transcript,0.5511771440505981
translation,35,163,model,ermlp,parameterized as,three - layer neural network,ermlp parameterized as three - layer neural network,0.6975136995315552
translation,35,163,model,model,has,ermlp,model has ermlp,0.6216840744018555
translation,35,179,results,cross-lingual fvsqa,Aided by,ermlp,cross-lingual fvsqa Aided by ermlp,0.6213252544403076
translation,35,179,results,wow,able to perform,fvsqa,wow able to perform fvsqa,0.6724210977554321
translation,35,179,results,fvsqa,at,same levels of accuracy,fvsqa at same levels of accuracy,0.5532967448234558
translation,35,179,results,same levels of accuracy,across,"english , hindi , and turkish","same levels of accuracy across english , hindi , and turkish",0.663956344127655
translation,35,179,results,cross-lingual fvsqa,has,wow,cross-lingual fvsqa has wow,0.594577968120575
translation,35,179,results,ermlp,has,wow,ermlp has wow,0.688281238079071
translation,35,185,results,fvqa system,performs,better,fvqa system performs better,0.6071110367774963
translation,35,185,results,better,than,end-to - end system,better than end-to - end system,0.6097286343574524
translation,35,185,results,better,for,english,better for english,0.5867434740066528
translation,35,185,results,end-to - end system,for,english,end-to - end system for english,0.6521089673042297
translation,35,185,results,results,resulting,fvqa system,results resulting fvqa system,0.6638560891151428
translation,36,5,baselines,dynasent,combines,naturally occurring sentences,dynasent combines naturally occurring sentences,0.6980252265930176
translation,36,5,baselines,naturally occurring sentences,with,sentences,naturally occurring sentences with sentences,0.6146021485328674
translation,36,5,baselines,sentences,created using,open-source dynabench platform,sentences created using open-source dynabench platform,0.703345775604248
translation,36,5,baselines,baselines,has,dynasent,baselines has dynasent,0.61049884557724
translation,36,149,results,small performance drop,on,external datasets,small performance drop on external datasets,0.4823920726776123
translation,36,149,results,huge jump,in,performance,huge jump in performance,0.5637345910072327
translation,36,149,results,performance,on,our dataset,performance on our dataset,0.5095446705818176
translation,36,228,results,model performance,on,round 1 dev,model performance on round 1 dev,0.5734930634498596
translation,36,228,results,increases,for,all three labels,increases for all three labels,0.6515781283378601
translation,36,228,results,all three labels,given,more training examples,all three labels given more training examples,0.680781900882721
translation,36,228,results,round 1 dev,has,increases,round 1 dev has increases,0.6183427572250366
translation,36,228,results,results,has,model performance,results has model performance,0.5472817420959473
translation,36,229,results,f1 scores,for,positive and negative classes,f1 scores for positive and negative classes,0.6239114999771118
translation,36,229,results,positive and negative classes,remain,high,positive and negative classes remain high,0.6127724647521973
translation,36,229,results,positive and negative classes,begin to,drop slightly,positive and negative classes begin to drop slightly,0.6626428365707397
translation,36,229,results,drop slightly,with,larger samples,drop slightly with larger samples,0.7429721355438232
translation,36,229,results,results,has,f1 scores,results has f1 scores,0.517913281917572
translation,37,134,ablation-analysis,fully connected graph,removal of,aspect-aware words,fully connected graph removal of aspect-aware words,0.625834584236145
translation,37,134,ablation-analysis,aspect-aware words,reduces,performance,aspect-aware words reduces performance,0.6698822975158691
translation,37,134,ablation-analysis,performance,has,seriously,performance has seriously,0.6123871207237244
translation,37,134,ablation-analysis,ablation analysis,Note,fully connected graph,ablation analysis Note fully connected graph,0.5386282801628113
translation,37,136,ablation-analysis,model,without employing,dependency tree,model without employing dependency tree,0.7091712355613708
translation,37,136,ablation-analysis,relations,into,graph,relations into graph,0.6329939961433411
translation,37,136,ablation-analysis,ablation analysis,has,model,ablation analysis has model,0.4912438988685608
translation,37,144,ablation-analysis,1 - layer gcn block,performs,unsatisfactorily,1 - layer gcn block performs unsatisfactorily,0.5728732943534851
translation,37,144,ablation-analysis,ablation analysis,has,1 - layer gcn block,ablation analysis has 1 - layer gcn block,0.5034959316253662
translation,37,145,ablation-analysis,fluctuates,with,increasing layer number,fluctuates with increasing layer number,0.7164456248283386
translation,37,145,ablation-analysis,fluctuates,tends to,decline,fluctuates tends to decline,0.7768949866294861
translation,37,145,ablation-analysis,increasing layer number,of,gcn blocks,increasing layer number of gcn blocks,0.632689356803894
translation,37,145,ablation-analysis,decline,when,layer number,decline when layer number,0.675500214099884
translation,37,145,ablation-analysis,layer number,is,greater than 4,layer number is greater than 4,0.5928710103034973
translation,37,145,ablation-analysis,performance,has,fluctuates,performance has fluctuates,0.5909061431884766
translation,37,145,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,37,111,baselines,mimlln,has,"li et al. , 2020 b )","mimlln has li et al. , 2020 b )",0.6047934293746948
translation,37,112,baselines,various variants,of,proposed aagcn,various variants of proposed aagcn,0.578541100025177
translation,37,112,baselines,baselines,provide,various variants,baselines provide various variants,0.5712874531745911
translation,37,113,baselines,aagcn - bert,takes,input,aagcn - bert takes input,0.6954612135887146
translation,37,113,baselines,aagcn - bert,as,input,aagcn - bert as input,0.5758327841758728
translation,37,113,baselines,[ cls ] sentence [ sep ] aspect [ sep ],as,input,[ cls ] sentence [ sep ] aspect [ sep ] as input,0.5357915163040161
translation,37,113,baselines,baselines,has,aagcn - bert,baselines has aagcn - bert,0.6368136405944824
translation,37,101,hyperparameters,hidden vector dimension,is,300,hidden vector dimension is 300,0.6152721643447876
translation,37,101,hyperparameters,hyperparameters,has,hidden vector dimension,hyperparameters has hidden vector dimension,0.5345754623413086
translation,37,102,hyperparameters,gcn blocks number,is,2,gcn blocks number is 2,0.6196865439414978
translation,37,102,hyperparameters,hyperparameters,has,gcn blocks number,hyperparameters has gcn blocks number,0.503264307975769
translation,37,103,hyperparameters,hyperparameters,has,coefficients,hyperparameters has coefficients,0.5175530314445496
translation,37,104,hyperparameters,adam,utilized as,optimizer,adam utilized as optimizer,0.6164308786392212
translation,37,104,hyperparameters,adam,with,mini-batch,adam with mini-batch,0.6320023536682129
translation,37,104,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,37,104,hyperparameters,optimizer,with,mini-batch,optimizer with mini-batch,0.6316599249839783
translation,37,104,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,37,104,hyperparameters,mini-batch,of,16,mini-batch of 16,0.648587167263031
translation,37,104,hyperparameters,hyperparameters,has,adam,hyperparameters has adam,0.5514352917671204
translation,37,105,hyperparameters,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,37,105,hyperparameters,0.3,after,embedding layer,0.3 after embedding layer,0.6524583697319031
translation,37,105,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
translation,37,106,hyperparameters,bert - based models,use,"pre-trained uncased bert - base ( devlin et al. , 2019 )","bert - based models use pre-trained uncased bert - base ( devlin et al. , 2019 )",0.5971066951751709
translation,37,106,hyperparameters,bert - based models,use,learning rate,bert - based models use learning rate,0.623436450958252
translation,37,106,hyperparameters,"pre-trained uncased bert - base ( devlin et al. , 2019 )",with,768 dimensional embedding,"pre-trained uncased bert - base ( devlin et al. , 2019 ) with 768 dimensional embedding",0.6079452633857727
translation,37,106,hyperparameters,learning rate,is,0.00002,learning rate is 0.00002,0.5624144077301025
translation,37,106,hyperparameters,hyperparameters,For,bert - based models,hyperparameters For bert - based models,0.5891440510749817
translation,37,20,model,aspect- aware graph ( s ),for,context,aspect- aware graph ( s ) for context,0.5899978876113892
translation,37,20,model,context,with respect to,corresponding aspect,context with respect to corresponding aspect,0.7207880020141602
translation,37,20,model,model,investigate,acsa,model investigate acsa,0.6622636914253235
translation,37,21,model,distinct aspect,as,distinct,distinct aspect as distinct,0.5883125066757202
translation,37,21,model,aspect-related words,from,external knowledge,aspect-related words from external knowledge,0.5227487087249756
translation,37,21,model,substitutes,of,coarse- grained aspect,substitutes of coarse- grained aspect,0.642871618270874
translation,37,21,model,coarse- grained aspect,to construct,graph,coarse- grained aspect to construct graph,0.673783004283905
translation,37,21,model,graph,of,context,graph of context,0.581571102142334
translation,37,21,model,model,regard,distinct aspect,model regard distinct aspect,0.7176353931427002
translation,37,21,model,model,search,aspect-related words,model search aspect-related words,0.669811487197876
translation,37,31,model,aspect-aware graph convolutional network ( aagcn ) structure,to draw,contextual sentiment dependencies,aspect-aware graph convolutional network ( aagcn ) structure to draw contextual sentiment dependencies,0.6040297150611877
translation,37,31,model,contextual sentiment dependencies,to,aspect,contextual sentiment dependencies to aspect,0.5120059251785278
translation,37,31,model,aspect,for,acsa,aspect for acsa,0.742711067199707
translation,37,35,model,aspect-aware graph convolutional network,to draw,contextual sentiment dependencies,aspect-aware graph convolutional network to draw contextual sentiment dependencies,0.6174504160881042
translation,37,35,model,contextual sentiment dependencies,to,aspect,contextual sentiment dependencies to aspect,0.5120059251785278
translation,37,35,model,aspect,for,sentiment detection,aspect for sentiment detection,0.5783583521842957
translation,37,35,model,model,has,aspect-aware graph convolutional network,model has aspect-aware graph convolutional network,0.5292574167251587
translation,37,51,model,aspect- aware graphs construction,constructs,aspect-aware graphs,aspect- aware graphs construction constructs aspect-aware graphs,0.63124680519104
translation,37,51,model,aspect-aware graphs,of,context,aspect-aware graphs of context,0.5424070358276367
translation,37,107,model,"senticnet ( cambria et al. , 2020 )",contains,affective commonsense relations,"senticnet ( cambria et al. , 2020 ) contains affective commonsense relations",0.5903232097625732
translation,37,107,model,affective commonsense relations,between,words,affective commonsense relations between words,0.6438813209533691
translation,37,107,model,affective commonsense relations,to derive,aspect-aware words,affective commonsense relations to derive aspect-aware words,0.6094300746917725
translation,37,107,model,model,has,"senticnet ( cambria et al. , 2020 )","model has senticnet ( cambria et al. , 2020 )",0.5578114986419678
translation,37,123,results,6 datasets,demonstrate,our proposed model,6 datasets demonstrate our proposed model,0.5814072489738464
translation,37,123,results,our proposed model,performs,consistently better,our proposed model performs consistently better,0.6447668671607971
translation,37,123,results,consistently better,than,comparison models,consistently better than comparison models,0.5946530103683472
translation,37,123,results,comparison models,for,non-bert and bert - based models,comparison models for non-bert and bert - based models,0.638984203338623
translation,37,123,results,comparison models,for,e#a and e aspects,comparison models for e#a and e aspects,0.6361783742904663
translation,37,123,results,results,on,6 datasets,results on 6 datasets,0.4788196384906769
translation,37,125,results,models,without employing,distributions,models without employing distributions,0.7221059799194336
translation,37,125,results,distributions,to derive,aspect-aware weights,distributions to derive aspect-aware weights,0.6543532013893127
translation,37,125,results,overall improved,in,any distribution,overall improved in any distribution,0.5813660621643066
translation,37,125,results,models,has,performance,models has performance,0.5729507207870483
translation,37,125,results,distributions,has,performance,distributions has performance,0.5859212875366211
translation,37,125,results,aspect-aware weights,has,performance,aspect-aware weights has performance,0.5745195746421814
translation,37,125,results,results,Compared with,models,results Compared with models,0.6802673935890198
translation,37,127,results,our proposed aagcn,explores,beta distribution,our proposed aagcn explores beta distribution,0.6677573919296265
translation,37,127,results,beta distribution,to determine,aspect-aware weights,beta distribution to determine aspect-aware weights,0.625140905380249
translation,37,127,results,our proposed aagcn,has,outstandingly outperforms,our proposed aagcn has outstandingly outperforms,0.580000638961792
translation,37,127,results,outstandingly outperforms,has,several related distributions,outstandingly outperforms has several related distributions,0.5454792976379395
translation,37,129,results,both aagcn and aagcn -c,perform,overall better,both aagcn and aagcn -c perform overall better,0.641799807548523
translation,37,129,results,overall better,than,baselines,overall better than baselines,0.5951704978942871
translation,37,129,results,different external knowledge scenarios,has,both aagcn and aagcn -c,different external knowledge scenarios has both aagcn and aagcn -c,0.5881211161613464
translation,37,129,results,results,For,different external knowledge scenarios,results For different external knowledge scenarios,0.5877750515937805
translation,37,130,results,models,based on,conceptnet,models based on conceptnet,0.6861376762390137
translation,37,130,results,models,models with,senticnet,models models with senticnet,0.6836720705032349
translation,37,130,results,senticnet,reveal,considerable superiorities,senticnet reveal considerable superiorities,0.71808922290802
translation,37,130,results,considerable superiorities,for,non-bert and bert - based conditions,considerable superiorities for non-bert and bert - based conditions,0.6558704972267151
translation,37,130,results,results,compared with,models,results compared with models,0.6802673935890198
translation,37,130,results,results,models with,senticnet,results models with senticnet,0.7413524389266968
translation,38,129,ablation-analysis,tense,of,auxiliary sentence,tense of auxiliary sentence,0.5357276797294617
translation,38,129,ablation-analysis,tense,is,influential,tense is influential,0.6310055255889893
translation,38,129,ablation-analysis,auxiliary sentence,is,influential,auxiliary sentence is influential,0.5935754776000977
translation,38,129,ablation-analysis,ablation analysis,has,tense,ablation analysis has tense,0.5693032145500183
translation,38,52,baselines,baselines,has,auto encoding ( ae ),baselines has auto encoding ( ae ),0.5516456961631775
translation,38,55,baselines,baselines,has,sequence-to-sequence ( seq2seq ),baselines has sequence-to-sequence ( seq2seq ),0.5597500801086426
translation,38,56,hyperparameters,seq2seq model,initialized with,pre-trained weights,seq2seq model initialized with pre-trained weights,0.7552522420883179
translation,38,56,hyperparameters,pre-trained weights,of,"bart ( lewis et al. , 2020 )","pre-trained weights of bart ( lewis et al. , 2020 )",0.5397520661354065
translation,38,56,hyperparameters,pre-trained weights,includes,encoder and decoder,pre-trained weights includes encoder and decoder,0.6429527997970581
translation,38,56,hyperparameters,hyperparameters,has,seq2seq model,hyperparameters has seq2seq model,0.5068058371543884
translation,38,7,model,semantics - preservation data augmentation approach,by considering,importance,semantics - preservation data augmentation approach by considering importance,0.7372575998306274
translation,38,7,model,importance,of,each word,importance of each word,0.5962938666343689
translation,38,7,model,each word,in,textual sequence,each word in textual sequence,0.5284222364425659
translation,38,7,model,textual sequence,according to,related aspects and sentiments,textual sequence according to related aspects and sentiments,0.6653958559036255
translation,38,7,model,model,propose,semantics - preservation data augmentation approach,model propose semantics - preservation data augmentation approach,0.6638747453689575
translation,38,8,model,unimportant tokens,with,two replacement strategies,unimportant tokens with two replacement strategies,0.6573323607444763
translation,38,8,model,two replacement strategies,without altering,aspect-level polarity,two replacement strategies without altering aspect-level polarity,0.7079955339431763
translation,38,8,model,model,substitute,unimportant tokens,model substitute unimportant tokens,0.7188136577606201
translation,38,72,results,better performances,on,all settings,better performances on all settings,0.5182134509086609
translation,38,72,results,better performances,with,stable results,better performances with stable results,0.6310062408447266
translation,38,72,results,stable results,has,lower standard deviations ),stable results has lower standard deviations ),0.5044218301773071
translation,38,76,results,proposed spm,has,consistently outperforms,proposed spm has consistently outperforms,0.6170216798782349
translation,38,76,results,consistently outperforms,has,random masking strategy ( c- bert ),consistently outperforms has random masking strategy ( c- bert ),0.6157573461532593
translation,38,76,results,results,has,proposed spm,results has proposed spm,0.6180950999259949
translation,38,77,results,proposed token replacement strategy seq2seq,performs,well,proposed token replacement strategy seq2seq performs well,0.6390923261642456
translation,38,77,results,well,in,acsc and atsc,well in acsc and atsc,0.5778183937072754
translation,38,77,results,ae,achieves,best results,ae achieves best results,0.6488016247749329
translation,38,77,results,best results,in,ate,best results in ate,0.609075665473938
translation,38,77,results,results,has,proposed token replacement strategy seq2seq,results has proposed token replacement strategy seq2seq,0.604755699634552
translation,38,82,results,proposed method,improves,more performance,proposed method improves more performance,0.6925889849662781
translation,38,82,results,more performance,than,other data augmentation methods,more performance than other data augmentation methods,0.5404075980186462
translation,38,82,results,most languages,has,proposed method,most languages has proposed method,0.5716005563735962
translation,38,82,results,results,In,most languages,results In most languages,0.4964330494403839
translation,38,123,results,other data augmentation methods,in,price / risk movement prediction tasks,other data augmentation methods in price / risk movement prediction tasks,0.4736957550048828
translation,38,123,results,proposed methods,has,outperform,proposed methods has outperform,0.6184839606285095
translation,38,123,results,outperform,has,other data augmentation methods,outperform has other data augmentation methods,0.5342132449150085
translation,38,123,results,results,has,proposed methods,results has proposed methods,0.5503098368644714
translation,38,130,results,issue-specific term (   market   ),provide,slight improvement,issue-specific term (   market   ) provide slight improvement,0.6043741106987
translation,39,74,baselines,lstm,acts as,baseline,lstm acts as baseline,0.621769368648529
translation,39,74,baselines,lstm,acts as,baselines,lstm acts as baselines,0.6198511719703674
translation,39,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,39,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,39,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,39,74,baselines,models,without using,plm,models without using plm,0.721270740032196
translation,39,74,baselines,models,without using,corresponding plms,models without using corresponding plms,0.7297158241271973
translation,39,74,baselines,models,with,corresponding plms,models with corresponding plms,0.6355447173118591
translation,39,74,baselines,bert - spc and roberta - mlp,are,baselines,bert - spc and roberta - mlp are baselines,0.5908552408218384
translation,39,74,baselines,baselines,for,models,baselines for models,0.679167628288269
translation,39,74,baselines,models,with,corresponding plms,models with corresponding plms,0.6355447173118591
translation,39,74,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,39,6,model,grammatical sequential features,from,plm of bert,grammatical sequential features from plm of bert,0.582516610622406
translation,39,6,model,syntactic knowledge,from,dependency graphs,syntactic knowledge from dependency graphs,0.5260922908782959
translation,39,6,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,39,6,model,model,propose,bert4gcn,model propose bert4gcn,0.6858947277069092
translation,39,7,model,bert4gcn,utilizes,outputs,bert4gcn utilizes outputs,0.7180281281471252
translation,39,7,model,outputs,from,intermediate layers,outputs from intermediate layers,0.5442511439323425
translation,39,7,model,intermediate layers,of,bert,intermediate layers of bert,0.6367949843406677
translation,39,7,model,positional information,between,words,positional information between words,0.653178870677948
translation,39,7,model,words,to augment,gcn ( graph convolutional network ),words to augment gcn ( graph convolutional network ),0.6829253435134888
translation,39,7,model,gcn ( graph convolutional network ),to better encode,dependency graphs,gcn ( graph convolutional network ) to better encode dependency graphs,0.693901538848877
translation,39,7,model,dependency graphs,for,downstream classification,dependency graphs for downstream classification,0.5959062576293945
translation,39,7,model,model,has,bert4gcn,model has bert4gcn,0.6771584749221802
translation,39,21,model,context,with,bilstm ( bidirectional long short - term memory ),context with bilstm ( bidirectional long short - term memory ),0.6065624356269836
translation,39,21,model,context,to capture,contextual information,context to capture contextual information,0.6653507947921753
translation,39,21,model,contextual information,regarding,word orders,contextual information regarding word orders,0.5892640352249146
translation,39,21,model,model,encode,context,model encode context,0.737949550151825
translation,39,22,model,hidden states,of,bilstm,hidden states of bilstm,0.5628228187561035
translation,39,22,model,hidden states,employ,multi-layer gcn,hidden states employ multi-layer gcn,0.5325880646705627
translation,39,22,model,bilstm,to initiate,node representations,bilstm to initiate node representations,0.6218497157096863
translation,39,22,model,multi-layer gcn,on,dependency graph,multi-layer gcn on dependency graph,0.5227571725845337
translation,39,22,model,model,use,hidden states,model use hidden states,0.6849468350410461
translation,39,24,model,bert4gcn,fuse,grammatical sequential features,bert4gcn fuse grammatical sequential features,0.7118731141090393
translation,39,24,model,grammatical sequential features,with,graph - based representations,grammatical sequential features with graph - based representations,0.5962903499603271
translation,39,24,model,model,has,bert4gcn,model has bert4gcn,0.6771584749221802
translation,39,25,model,edge,of,dependency graph,edge of dependency graph,0.5540347695350647
translation,39,25,model,edge,of,dependency graph,edge of dependency graph,0.5540347695350647
translation,39,25,model,edge,based on,self-attention weights,edge based on self-attention weights,0.6998171806335449
translation,39,25,model,dependency graph,based on,self-attention weights,dependency graph based on self-attention weights,0.6414437294006348
translation,39,25,model,self-attention weights,in,"transformer encoder ( vaswani et al. , 2017 )","self-attention weights in transformer encoder ( vaswani et al. , 2017 )",0.4651518762111664
translation,39,25,model,"transformer encoder ( vaswani et al. , 2017 )",of,bert,"transformer encoder ( vaswani et al. , 2017 ) of bert",0.5931496024131775
translation,39,25,model,"transformer encoder ( vaswani et al. , 2017 )",to deal with,parsing errors,"transformer encoder ( vaswani et al. , 2017 ) to deal with parsing errors",0.6538543105125427
translation,39,25,model,bert,to deal with,parsing errors,bert to deal with parsing errors,0.7327443957328796
translation,39,25,model,model,prune and add,edge,model prune and add edge,0.7621790766716003
translation,39,26,model,method,incorporates,relative positional embedding,method incorporates relative positional embedding,0.7065366506576538
translation,39,26,model,relative positional embedding,in,node representations,relative positional embedding in node representations,0.5370160341262817
translation,40,58,baselines,gpt - 2,use,original causal lm ( clm ) objective,gpt - 2 use original causal lm ( clm ) objective,0.6342961192131042
translation,40,58,baselines,baselines,For,gpt - 2,baselines For gpt - 2,0.6106210350990295
translation,40,8,experiments,few-shot setting,for,semeval 2014,few-shot setting for semeval 2014,0.6195738911628723
translation,40,9,experiments,our method,of reformulating,atsc,our method of reformulating atsc,0.6943092346191406
translation,40,9,experiments,atsc,as,nli task,atsc as nli task,0.5204768180847168
translation,40,9,experiments,supervised sota approaches,by,up to 24.13 accuracy points,supervised sota approaches by up to 24.13 accuracy points,0.5890952348709106
translation,40,9,experiments,supervised sota approaches,by,33.14 macro f1 points,supervised sota approaches by 33.14 macro f1 points,0.5623603463172913
translation,40,9,experiments,task 4 laptop domain,has,our method,task 4 laptop domain has our method,0.553636372089386
translation,40,9,experiments,outperforms,has,supervised sota approaches,outperforms has supervised sota approaches,0.6014392375946045
translation,40,68,hyperparameters,bert - base model,pretrained on,mnli dataset,bert - base model pretrained on mnli dataset,0.782687246799469
translation,40,68,hyperparameters,hyperparameters,use,bert - base model,hyperparameters use bert - base model,0.6352326273918152
translation,40,26,model,two atsc models,based on,natural language prompts,two atsc models based on natural language prompts,0.6321611404418945
translation,40,26,model,first method,appends,cloze question prompts,first method appends cloze question prompts,0.523867130279541
translation,40,26,model,first method,predicts,likely,first method predicts likely,0.7591094970703125
translation,40,26,model,cloze question prompts,to,review text,cloze question prompts to review text,0.5740119814872742
translation,40,26,model,likely,to observe,"good , bad , and ok","likely to observe good , bad , and ok",0.6754434108734131
translation,40,26,model,"good , bad , and ok",as,next word,"good , bad , and ok as next word",0.5170080661773682
translation,40,26,model,model,propose,two atsc models,model propose two atsc models,0.661225438117981
translation,40,69,model,softmax layer,on top of,logits,softmax layer on top of logits,0.686877965927124
translation,40,69,model,softmax layer,to normalize,prediction scores,softmax layer to normalize prediction scores,0.6267380118370056
translation,40,69,model,softmax layer,to finetune,models,softmax layer to finetune models,0.6771717071533203
translation,40,69,model,prediction scores,of,three classes,prediction scores of three classes,0.5984625816345215
translation,40,69,model,models,with,cross-entropy loss,models with cross-entropy loss,0.6021761298179626
translation,40,69,model,cross-entropy loss,when,labeled data,cross-entropy loss when labeled data,0.5812923908233643
translation,40,69,model,labeled data,is,available,labeled data is available,0.6030660271644592
translation,40,69,model,model,apply,softmax layer,model apply softmax layer,0.599711000919342
translation,40,33,results,promptbased models,robust to,domain shifts,promptbased models robust to domain shifts,0.7618956565856934
translation,40,33,results,results,has,promptbased models,results has promptbased models,0.4686737358570099
translation,40,88,results,outperform,in,all cases,outperform in all cases,0.5809333920478821
translation,40,88,results,no-prompt baselines,in,all cases,no-prompt baselines in all cases,0.5485486388206482
translation,40,88,results,both target domains,has,our prompt- based bert models,both target domains has our prompt- based bert models,0.5923322439193726
translation,40,88,results,our prompt- based bert models,has,outperform,our prompt- based bert models has outperform,0.586745023727417
translation,40,88,results,outperform,has,no-prompt baselines,outperform has no-prompt baselines,0.6198127865791321
translation,40,88,results,results,for,both target domains,results for both target domains,0.535052478313446
translation,40,89,results,few-shots,achieve,larger performance gains,few-shots achieve larger performance gains,0.6333398222923279
translation,40,89,results,larger performance gains,as,fewer labels,larger performance gains as fewer labels,0.5346285700798035
translation,40,89,results,fewer labels,are,available,fewer labels are available,0.6033520102500916
translation,40,89,results,results,Especially for,few-shots,results Especially for few-shots,0.6653315424919128
translation,40,93,results,our methods,achieve,good performances,our methods achieve good performances,0.5499653220176697
translation,40,93,results,good performances,in,zero-shot cases,good performances in zero-shot cases,0.5420867204666138
translation,40,93,results,baselines,trained on,16 samples,baselines trained on 16 samples,0.7219739556312561
translation,40,93,results,significantly outperforming,has,baselines,significantly outperforming has baselines,0.6089772582054138
translation,40,93,results,results,observe that,our methods,results observe that our methods,0.5723729729652405
translation,40,95,results,prompt models,with,16 - shot cross-domain training,prompt models with 16 - shot cross-domain training,0.609455406665802
translation,40,95,results,prompt models,achieve,better performance,prompt models achieve better performance,0.6625387668609619
translation,40,95,results,better performance,than,in - and out-domain bert nsp,better performance than in - and out-domain bert nsp,0.6110102534294128
translation,40,96,results,have even exceeded in -domain,for,restaurants domain,have even exceeded in -domain for restaurants domain,0.6390429139137268
translation,40,96,results,crossdomain,has,have even exceeded in -domain,crossdomain has have even exceeded in -domain,0.6131864190101624
translation,40,96,results,results,interesting to note,crossdomain,results interesting to note crossdomain,0.5999849438667297
translation,40,99,results,bert lm model,trained with,merely 16 examples,bert lm model trained with merely 16 examples,0.7127271890640259
translation,40,99,results,bert lm model,achieves,about 77 % accuracy,bert lm model achieves about 77 % accuracy,0.7068583965301514
translation,40,99,results,about 77 % accuracy,on,acsc,about 77 % accuracy on acsc,0.6017430424690247
translation,40,99,results,results,see,bert lm model,results see bert lm model,0.5849559307098389
translation,40,100,results,bert nsp,achieves,around 65 %,bert nsp achieves around 65 %,0.7368992567062378
translation,40,100,results,bert nsp,has,no-prompt baseline,bert nsp has no-prompt baseline,0.5970755815505981
translation,40,100,results,results,has,bert nsp,results has bert nsp,0.561840295791626
translation,40,107,results,significant amount of improvements,over,noprompt baselines,significant amount of improvements over noprompt baselines,0.6997228860855103
translation,40,108,results,our nli model,performs,well,our nli model performs well,0.658847451210022
translation,40,108,results,well,with,lower amounts of training data,well with lower amounts of training data,0.6554457545280457
translation,40,108,results,does better,when,more labels,does better when more labels,0.6782758235931396
translation,40,108,results,bert lm model,has,does better,bert lm model has does better,0.6270426511764526
translation,40,108,results,more labels,has,are available,more labels has are available,0.5680786967277527
translation,41,156,ablation-analysis,counter-fitted embeddings ( ae + ls + cf ),appear to address,label preservation,counter-fitted embeddings ( ae + ls + cf ) appear to address label preservation,0.5441343188285828
translation,41,156,ablation-analysis,agr,improves from,0.12 to 0.46 to 0.64,agr improves from 0.12 to 0.46 to 0.64,0.6556984782218933
translation,41,156,ablation-analysis,label smoothing,has,+ ls ),label smoothing has + ls ),0.6078101992607117
translation,41,156,ablation-analysis,ablation analysis,introduction of,label smoothing,ablation analysis introduction of label smoothing,0.7129161357879639
translation,41,105,baselines,c-lstm,is,classifier,c-lstm is classifier,0.5760554075241089
translation,41,105,baselines,classifier,with,bidirectional lstm,classifier with bidirectional lstm,0.6175999641418457
translation,41,105,baselines,bidirectional lstm,followed by,self-attention layer,bidirectional lstm followed by self-attention layer,0.6807053685188293
translation,41,105,baselines,self-attention layer,to weigh,lstm hidden states,self-attention layer to weigh lstm hidden states,0.5532988905906677
translation,41,105,baselines,baselines,has,c-lstm,baselines has c-lstm,0.5209414958953857
translation,41,112,experiments,target classifiers,pretrain,three sentiment classification models,target classifiers pretrain three sentiment classification models,0.6883351802825928
translation,41,112,experiments,three sentiment classification models,using,yelp50,three sentiment classification models using yelp50,0.6023027300834656
translation,41,112,experiments,three sentiment classification models,using,c-cnn,three sentiment classification models using c-cnn,0.6194533705711365
translation,41,112,experiments,three sentiment classification models,using,c-bert,three sentiment classification models using c-bert,0.6421965956687927
translation,41,116,hyperparameters,number of hidden units,for,all classifiers,number of hidden units for all classifiers,0.5813358426094055
translation,41,116,hyperparameters,number of attention units,for,c-lstm,number of attention units for c-lstm,0.58887779712677
translation,41,116,hyperparameters,number of attention units,for,convolutional filter sizes,number of attention units for convolutional filter sizes,0.5743253231048584
translation,41,116,hyperparameters,dropout rates,for,c-cnn,dropout rates for c-cnn,0.5359574556350708
translation,41,116,hyperparameters,learning rate,has,batch size,learning rate has batch size,0.5358124375343323
translation,41,116,hyperparameters,hyperparameters,tune,learning rate,hyperparameters tune learning rate,0.7057108283042908
translation,41,116,hyperparameters,hyperparameters,tune,number of attention units,hyperparameters tune number of attention units,0.6873290538787842
translation,41,117,hyperparameters,auto-encoder,pre-train it to reconstruct,sentences,auto-encoder pre-train it to reconstruct sentences,0.7761346697807312
translation,41,117,hyperparameters,sentences,in,yelp50,sentences in yelp50,0.5612265467643738
translation,41,117,hyperparameters,hyperparameters,For,auto-encoder,hyperparameters For auto-encoder,0.5609650015830994
translation,41,121,hyperparameters,word embeddings,are,fixed,word embeddings are fixed,0.6090652346611023
translation,41,121,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,41,4,model,defence framework,for,sentiment classification,defence framework for sentiment classification,0.608810305595398
translation,41,4,model,model,introduce,grey- box adversarial attack,model introduce grey- box adversarial attack,0.6437088847160339
translation,41,4,model,model,introduce,defence framework,model introduce defence framework,0.6576252579689026
translation,41,19,model,grey - box framework,that generates,high quality textual adversarial examples,grey - box framework that generates high quality textual adversarial examples,0.6229444146156311
translation,41,19,model,grey - box framework,simultaneously trains,improved sentiment classifier,grey - box framework simultaneously trains improved sentiment classifier,0.7383520007133484
translation,41,19,model,improved sentiment classifier,for,adversarial defending,improved sentiment classifier for adversarial defending,0.6145883202552795
translation,41,19,model,model,propose,grey - box framework,model propose grey - box framework,0.6632116436958313
translation,41,24,model,more robust,than,adversarial defending,more robust than adversarial defending,0.5880779027938843
translation,41,24,model,adversarial defending,based on,adversarial examples augmentation,adversarial defending based on adversarial examples augmentation,0.6349107623100281
translation,41,24,model,model,show,classifier,model show classifier,0.6982285976409912
translation,41,100,model,simple copy mechanism,that helps,grey - box attack,simple copy mechanism that helps grey - box attack,0.5951220989227295
translation,41,100,model,grey - box attack,to produce,faithful reconstruction,grey - box attack to produce faithful reconstruction,0.7011643648147583
translation,41,100,model,faithful reconstruction,of,original sentences,faithful reconstruction of original sentences,0.58074551820755
translation,41,100,model,model,introduce,simple copy mechanism,model introduce simple copy mechanism,0.6812455654144287
translation,41,113,model,c-lstm,composed of,embedding layer,c-lstm composed of embedding layer,0.635206937789917
translation,41,113,model,c-lstm,composed of,2 - layer bidirectional lstms,c-lstm composed of 2 - layer bidirectional lstms,0.6734949350357056
translation,41,113,model,c-lstm,composed of,self-attention layer,c-lstm composed of self-attention layer,0.7045729756355286
translation,41,113,model,c-lstm,composed of,output layer,c-lstm composed of output layer,0.6428480744361877
translation,41,113,model,model,has,c-lstm,model has c-lstm,0.5582016706466675
translation,41,114,model,number of convolutional filters,of,varying sizes,number of convolutional filters of varying sizes,0.6234728693962097
translation,41,114,model,outputs,"concatenated , pooled and fed to",fully - connected layer,"outputs concatenated , pooled and fed to fully - connected layer",0.6760164499282837
translation,41,114,model,fully - connected layer,followed by,output layer,fully - connected layer followed by output layer,0.629397451877594
translation,41,114,model,c-cnn,has,number of convolutional filters,c-cnn has number of convolutional filters,0.5718389749526978
translation,41,114,model,model,has,c-cnn,model has c-cnn,0.5524232983589172
translation,41,115,model,c-bert,obtained by,fine-tuning,c-bert obtained by fine-tuning,0.6467083692550659
translation,41,115,model,bert - base model,for,sentiment classification,bert - base model for sentiment classification,0.6163033246994019
translation,41,115,model,fine-tuning,has,bert - base model,fine-tuning has bert - base model,0.564826250076294
translation,41,115,model,model,has,c-bert,model has c-bert,0.6673047542572021
translation,41,153,results,  pos   and   neg   performance,of,ae and ae + bal,  pos   and   neg   performance of ae and ae + bal,0.5826168060302734
translation,41,153,results,  pos   and   neg   performance,of,ae + bal,  pos   and   neg   performance of ae + bal,0.5807332396507263
translation,41,153,results,  pos   and   neg   performance,see that,ae + bal,  pos   and   neg   performance see that ae + bal,0.6100557446479797
translation,41,153,results,ae and ae + bal,see that,ae + bal,ae and ae + bal see that ae + bal,0.6424611210823059
translation,41,153,results,effective,in creating,more balanced performance,effective in creating more balanced performance,0.7386100888252258
translation,41,153,results,more balanced performance,for,positive - to-negative and negative - to - positive attacks,more balanced performance for positive - to-negative and negative - to - positive attacks,0.6531312465667725
translation,41,153,results,results,Looking at,  pos   and   neg   performance,results Looking at   pos   and   neg   performance,0.5467885136604309
translation,41,157,results,copy mechanism ( ae + ls + cf + cpy ),provides,some marginal improvement,copy mechanism ( ae + ls + cf + cpy ) provides some marginal improvement,0.6474308967590332
translation,41,157,results,results,Adding,copy mechanism ( ae + ls + cf + cpy ),results Adding copy mechanism ( ae + ls + cf + cpy ),0.7412644028663635
translation,41,165,results,c-lstm and c- cnn,found that,c-cnn,c-lstm and c- cnn found that c-cnn,0.6280491352081299
translation,41,165,results,c-cnn,is,easier classifier,c-cnn is easier classifier,0.5644590258598328
translation,41,165,results,easier classifier,to attack,bleu,easier classifier to attack bleu,0.7881542444229126
translation,41,165,results,easier classifier,as,bleu,easier classifier as bleu,0.5589718818664551
translation,41,165,results,scores,for,same threshold,scores for same threshold,0.667961597442627
translation,41,165,results,same threshold,are,higher,same threshold are higher,0.6676276326179504
translation,41,165,results,results,Comparing,c-lstm and c- cnn,results Comparing c-lstm and c- cnn,0.6554955840110779
translation,41,166,results,textfooler,appears to be,ineffective,textfooler appears to be ineffective,0.7296895980834961
translation,41,166,results,ineffective,for,attacking,ineffective for attacking,0.712550163269043
translation,41,166,results,attacking,has,c-cnn,attacking has c-cnn,0.5274375081062317
translation,41,166,results,results,has,textfooler,results has textfooler,0.566886305809021
translation,41,168,results,ae +ls +cf + cpy,performs,relatively well,ae +ls +cf + cpy performs relatively well,0.6300984621047974
translation,41,168,results,ae +ls +cf + cpy,far behind,textfooler,ae +ls +cf + cpy far behind textfooler,0.7186169028282166
translation,41,168,results,results,has,ae +ls +cf + cpy,results has ae +ls +cf + cpy,0.550938069820404
translation,41,169,results,hotflip,produces,good bleu scores,hotflip produces good bleu scores,0.5809932351112366
translation,41,169,results,hotflip,produces,substantially worse use scores,hotflip produces substantially worse use scores,0.6317158341407776
translation,41,169,results,results,has,hotflip,results has hotflip,0.5650033950805664
translation,41,170,results,tyc,is,worst performing model,tyc is worst performing model,0.5776455998420715
translation,41,170,results,results,has,tyc,results has tyc,0.5550491809844971
translation,41,171,results,most methods,do not produce,adversarial examples,most methods do not produce adversarial examples,0.7319739460945129
translation,41,171,results,adversarial examples,that are,very effective,adversarial examples that are very effective,0.5951603055000305
translation,41,171,results,very effective,at attacking,c-bert,very effective at attacking c-bert,0.7629233598709106
translation,41,171,results,results,has,most methods,results has most methods,0.49081864953041077
translation,41,174,results,tyc,produces,largely gibberish output,tyc produces largely gibberish output,0.6507446765899658
translation,41,174,results,results,has,tyc,results has tyc,0.5550491809844971
translation,41,175,results,hotflip,replace,words,hotflip replace words,0.6603380441665649
translation,41,175,results,hotflip,explains,low use and acpt scores,hotflip explains low use and acpt scores,0.6339836120605469
translation,41,175,results,words,with,low semantic similarity,words with low semantic similarity,0.6007913947105408
translation,41,175,results,low semantic similarity,with,original words,low semantic similarity with original words,0.5785092711448669
translation,41,175,results,results,has,hotflip,results has hotflip,0.5650033950805664
translation,41,181,results,our model,is,fastest method,our model is fastest method,0.578346312046051
translation,41,181,results,ae +ls +cf + cpy,is,fastest method,ae +ls +cf + cpy is fastest method,0.5494185090065002
translation,41,181,results,about an order of magnitude faster,compared to,next best method hotflip,about an order of magnitude faster compared to next best method hotflip,0.6544039845466614
translation,41,181,results,our model,has,ae +ls +cf + cpy,our model has ae +ls +cf + cpy,0.582347571849823
translation,41,181,results,fastest method,has,about an order of magnitude faster,fastest method has about an order of magnitude faster,0.47661545872688293
translation,41,181,results,results,has,our model,results has our model,0.5871725678443909
translation,41,199,results,hotflip,is,least impressive method,hotflip is least impressive method,0.5748519897460938
translation,41,199,results,results,has,hotflip,results has hotflip,0.5650033950805664
translation,41,200,results,label preservation,has,our method ae +ls +cf + cpy,label preservation has our method ae +ls +cf + cpy,0.5582352876663208
translation,41,200,results,our method ae +ls +cf + cpy,has,best performance,our method ae +ls +cf + cpy has best performance,0.5300251245498657
translation,41,200,results,results,On,label preservation,results On label preservation,0.5108165144920349
translation,42,131,ablation-analysis,albert,helps achieve,best result,albert helps achieve best result,0.6894986629486084
translation,42,131,ablation-analysis,best result,among,four plms,best result among four plms,0.619240403175354
translation,42,131,ablation-analysis,ablation analysis,shows,albert,ablation analysis shows albert,0.5740904211997986
translation,42,140,ablation-analysis,number of encoder layers,exceeds,3,number of encoder layers exceeds 3,0.6473464369773865
translation,42,140,ablation-analysis,performance,shows,continuous decreasing trend,performance shows continuous decreasing trend,0.6757770776748657
translation,42,140,ablation-analysis,continuous decreasing trend,except,16 rest,continuous decreasing trend except 16 rest,0.764390766620636
translation,42,140,ablation-analysis,16 rest,when,number of encoder layers,16 rest when number of encoder layers,0.6174058318138123
translation,42,140,ablation-analysis,number of encoder layers,increased to,7,number of encoder layers increased to 7,0.6869590878486633
translation,42,140,ablation-analysis,increases,by,nearly 2.5 absolute f 1 score,increases by nearly 2.5 absolute f 1 score,0.6393924355506897
translation,42,140,ablation-analysis,number of encoder layers,has,performance,number of encoder layers has performance,0.5806815028190613
translation,42,140,ablation-analysis,3,has,performance,3 has performance,0.5494488477706909
translation,42,140,ablation-analysis,16 rest,has,performance,16 rest has performance,0.5851522088050842
translation,42,140,ablation-analysis,performance,has,increases,performance has increases,0.5947421193122864
translation,42,140,ablation-analysis,ablation analysis,when,number of encoder layers,ablation analysis when number of encoder layers,0.6506556272506714
translation,42,109,baselines,rinante +,has,"peng et al. , 2020","rinante + has peng et al. , 2020",0.5990360379219055
translation,42,109,baselines,cmla +,has,"peng et al. , 2020 )","cmla + has peng et al. , 2020 )",0.6089118719100952
translation,42,109,baselines,li-unified -r,has,"peng et al. , 2020 )","li-unified -r has peng et al. , 2020 )",0.543563723564148
translation,42,109,baselines,ote-mtl,has,"zhang et al. , 2020a","ote-mtl has zhang et al. , 2020a",0.6091349124908447
translation,42,7,model,joint absa model,focuses on,difference,joint absa model focuses on difference,0.7471405267715454
translation,42,7,model,benefits,of,encoder sharing,benefits of encoder sharing,0.5853569507598877
translation,42,7,model,difference,to improve,effectiveness,difference to improve effectiveness,0.6964927911758423
translation,42,7,model,model,propose,joint absa model,model propose joint absa model,0.6526687145233154
translation,42,8,model,dual-encoder design,in which,pair encoder,dual-encoder design in which pair encoder,0.6268938779830933
translation,42,8,model,pair encoder,focuses on,candidate aspect-opinion pair classification,pair encoder focuses on candidate aspect-opinion pair classification,0.6995263695716858
translation,42,8,model,original encoder,keeps attention on,sequence labeling,original encoder keeps attention on sequence labeling,0.6974812746047974
translation,42,8,model,model,introduce,dual-encoder design,model introduce dual-encoder design,0.6448301076889038
translation,42,32,model,dual-encoder model,based on,pretrained language model,dual-encoder model based on pretrained language model,0.6066051721572876
translation,42,32,model,multiple encoders,on,absa task,multiple encoders on absa task,0.5783410668373108
translation,42,32,model,model,propose,dual-encoder model,model propose dual-encoder model,0.6623518466949463
translation,42,33,model,shared sequence encoder,to represent,aspect terms and opinion terms,shared sequence encoder to represent aspect terms and opinion terms,0.6369184255599976
translation,42,33,model,aspect terms and opinion terms,in,same embedding space,aspect terms and opinion terms in same embedding space,0.5003525614738464
translation,42,34,model,pair encoder,to represent,aspectopinion pair,pair encoder to represent aspectopinion pair,0.6634731292724609
translation,42,34,model,aspectopinion pair,on,span level,aspectopinion pair on span level,0.5438784956932068
translation,42,34,model,model,introduce,pair encoder,model introduce pair encoder,0.7017258405685425
translation,42,107,results,results,on,aste task,results on aste task,0.5079299807548523
translation,42,110,results,our bertbased dual-encoder model,achieves,improvement,our bertbased dual-encoder model achieves improvement,0.6626638174057007
translation,42,110,results,improvement,by,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score","improvement by 1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score",0.5568260550498962
translation,42,110,results,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score",on,benchmark datasets,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score on benchmark datasets",0.476910799741745
translation,42,110,results,"best baseline model ( huang et al. , 2021 )",has,our bertbased dual-encoder model,"best baseline model ( huang et al. , 2021 ) has our bertbased dual-encoder model",0.5034582614898682
translation,42,110,results,results,Compared with,"best baseline model ( huang et al. , 2021 )","results Compared with best baseline model ( huang et al. , 2021 )",0.6177413463592529
translation,42,111,results,our dual-encoder model,capable of capturing,difference,our dual-encoder model capable of capturing difference,0.7243567705154419
translation,42,111,results,difference,between,at / ot extraction subtask and sc subtask,difference between at / ot extraction subtask and sc subtask,0.6501572728157043
translation,42,111,results,at / ot extraction subtask and sc subtask,with the help of,additional pair encoder,at / ot extraction subtask and sc subtask with the help of additional pair encoder,0.6715391278266907
translation,42,111,results,results,signifies that,our dual-encoder model,results signifies that our dual-encoder model,0.6435863375663757
translation,42,112,results,all the other competitive methods,on,most metrics,all the other competitive methods on most metrics,0.4642323851585388
translation,42,112,results,all the other competitive methods,except for,precision score,all the other competitive methods except for precision score,0.5820450782775879
translation,42,112,results,most metrics,of,"4 datasets 14 rest , 14 lap , 15 rest and 16 rest","most metrics of 4 datasets 14 rest , 14 lap , 15 rest and 16 rest",0.5214417576789856
translation,42,112,results,albert - based model,has,significantly outperforms,albert - based model has significantly outperforms,0.6131628751754761
translation,42,112,results,significantly outperforms,has,all the other competitive methods,significantly outperforms has all the other competitive methods,0.5785022974014282
translation,42,112,results,results,has,albert - based model,results has albert - based model,0.5318671464920044
translation,42,113,results,albert - based model,achieves,improvement,albert - based model achieves improvement,0.6460460424423218
translation,42,113,results,improvement,of,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score","improvement of 6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score",0.5608571767807007
translation,42,113,results,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score",over,all the baseline models,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score over all the baseline models",0.6179499626159668
translation,42,113,results,all the baseline models,on,four benchmark datasets,all the baseline models on four benchmark datasets,0.4489745795726776
translation,42,113,results,results,has,albert - based model,results has albert - based model,0.5318671464920044
translation,42,120,results,aesc task,For,aesc task,aesc task For aesc task,0.6281108856201172
translation,42,120,results,results,on,aesc task,results on aesc task,0.5013177394866943
translation,42,125,results,performance,of,our dual-encoder model,performance of our dual-encoder model,0.5757023692131042
translation,42,125,results,our dual-encoder model,is,comparable,our dual-encoder model is comparable,0.5772272944450378
translation,42,125,results,comparable,on,aesc task,comparable on aesc task,0.5702894926071167
translation,42,125,results,aesc task,than,single - encoder structure,aesc task than single - encoder structure,0.5581415295600891
translation,42,125,results,results,see that,performance,results see that performance,0.6803712844848633
translation,42,134,results,absolute f 1 score,between,"bert and roberta , albert","absolute f 1 score between bert and roberta , albert",0.6356632709503174
translation,42,134,results,"bert and roberta , albert",is,1.04 and 4.19,"bert and roberta , albert is 1.04 and 4.19",0.5914108753204346
translation,42,134,results,results,has,absolute f 1 score,results has absolute f 1 score,0.5565140247344971
translation,42,144,results,quaddirectional setting,has,significantly outperforms,quaddirectional setting has significantly outperforms,0.6145737171173096
translation,42,144,results,significantly outperforms,has,other two settings,significantly outperforms has other two settings,0.551098108291626
translation,42,144,results,results,observe,quaddirectional setting,results observe quaddirectional setting,0.6090797185897827
translation,43,15,model,minimal prediction preserving inputs ( mppis ),iteratively removing,least important word,minimal prediction preserving inputs ( mppis ) iteratively removing least important word,0.7037027478218079
translation,43,15,model,least important word,from,query,least important word from query,0.576629638671875
translation,43,15,model,least important word,to obtain,shortest sequence,least important word to obtain shortest sequence,0.5896921753883362
translation,43,15,model,shortest sequence,for which,model 's prediction,shortest sequence for which model 's prediction,0.671638011932373
translation,43,15,model,model 's prediction,has,remains unchanged,model 's prediction has remains unchanged,0.5905545353889465
translation,43,15,model,model,has,minimal prediction preserving inputs ( mppis ),model has minimal prediction preserving inputs ( mppis ),0.5915274620056152
translation,43,9,results,over-confidence,on,mppis,over-confidence on mppis,0.6038600206375122
translation,43,9,results,over-confidence,fails to improve,generalization,over-confidence fails to improve generalization,0.7433559894561768
translation,43,9,results,over-confidence,fails to improve,adversarial robustness,over-confidence fails to improve adversarial robustness,0.7073751091957092
translation,43,9,results,results,penalizing,over-confidence,results penalizing over-confidence,0.7378342747688293
translation,43,23,results,largescale pretraining,does not produce,more human interpretable mppis,largescale pretraining does not produce more human interpretable mppis,0.7224672436714172
translation,43,23,results,results,find,largescale pretraining,results find largescale pretraining,0.5352010130882263
translation,43,90,results,performance,on,out-domain mppis,performance on out-domain mppis,0.5679840445518494
translation,43,90,results,out-domain mppis,46.6 % closer to,original performance,out-domain mppis 46.6 % closer to original performance,0.7155277729034424
translation,43,90,results,original performance,than on,random mppis,original performance than on random mppis,0.5582692623138428
translation,43,90,results,results,shows,performance,results shows performance,0.7266349792480469
translation,43,97,results,adversarial robustness f1,on,adversarial squad ( ar ),adversarial robustness f1 on adversarial squad ( ar ),0.5308582782745361
translation,43,97,results,decline slightly,on average,mppi regularization,decline slightly on average mppi regularization,0.6919354200363159
translation,43,97,results,decline slightly,with,mppi regularization,decline slightly with mppi regularization,0.6678054332733154
translation,43,97,results,mppi regularization,by,"0.2 % , 2.7 % , and 0.6 %","mppi regularization by 0.2 % , 2.7 % , and 0.6 %",0.5628207921981812
translation,43,97,results,adversarial squad ( ar ),has,decline slightly,adversarial squad ( ar ) has decline slightly,0.6004939079284668
translation,44,24,ablation-analysis,round-trip translation,at,test time,round-trip translation at test time,0.5310289263725281
translation,44,24,ablation-analysis,round-trip translation,consistently reduces,fairness gap,round-trip translation consistently reduces fairness gap,0.7996982336044312
translation,44,24,ablation-analysis,round-trip translation,for,our best models,round-trip translation for our best models,0.5780915021896362
translation,44,24,ablation-analysis,our best models,both,training and test data,our best models both training and test data,0.6726540923118591
translation,44,24,ablation-analysis,svms,stacked on,bert representations,svms stacked on bert representations,0.7008928656578064
translation,44,24,ablation-analysis,disappears,when,training and test data,disappears when training and test data,0.6543428301811218
translation,44,24,ablation-analysis,disappears,both,training and test data,disappears both training and test data,0.712655246257782
translation,44,24,ablation-analysis,training and test data,translated into,foreign language and back,training and test data translated into foreign language and back,0.7183794975280762
translation,44,24,ablation-analysis,our best models,has,svms,our best models has svms,0.5638766884803772
translation,44,24,ablation-analysis,our best models,has,effect,our best models has effect,0.613236129283905
translation,44,24,ablation-analysis,effect,has,disappears,effect has disappears,0.6273092031478882
translation,44,24,ablation-analysis,ablation analysis,find that,round-trip translation,ablation analysis find that round-trip translation,0.6642451286315918
translation,44,47,ablation-analysis,significant decrease,in,kl - divergence,significant decrease in kl - divergence,0.5088932514190674
translation,44,47,ablation-analysis,kl - divergence,for,all groups,kl - divergence for all groups,0.6074047684669495
translation,44,47,ablation-analysis,all groups,after,round -trip translating,all groups after round -trip translating,0.7303268313407898
translation,44,49,ablation-analysis,number of unique words,dropped by,36 %,number of unique words dropped by 36 %,0.6814844012260437
translation,44,49,ablation-analysis,36 %,after,round -trip translation,36 % after round -trip translation,0.6829888224601746
translation,44,49,ablation-analysis,ablation analysis,see that,number of unique words,ablation analysis see that number of unique words,0.6057654619216919
translation,44,58,ablation-analysis,test time normalization,with,round trip translation,test time normalization with round trip translation,0.6487547159194946
translation,44,58,ablation-analysis,overall positive effect,on,cross-group generalization,overall positive effect on cross-group generalization,0.511343240737915
translation,44,58,ablation-analysis,fairness gap,with,up to ? 27 %,fairness gap with up to ? 27 %,0.6602929830551147
translation,44,58,ablation-analysis,test time normalization,has,overall positive effect,test time normalization has overall positive effect,0.5662668347358704
translation,44,58,ablation-analysis,round trip translation,has,overall positive effect,round trip translation has overall positive effect,0.5779203772544861
translation,44,58,ablation-analysis,ablation analysis,has,test time normalization,ablation analysis has test time normalization,0.528400182723999
translation,44,61,ablation-analysis,translating the data,reduces,overall accuracy,translating the data reduces overall accuracy,0.7089628577232361
translation,44,61,ablation-analysis,overall accuracy,of,our document classifiers,overall accuracy of our document classifiers,0.5305272340774536
translation,44,61,ablation-analysis,process of round-trip,has,translating the data,process of round-trip has translating the data,0.5429555177688599
translation,44,66,baselines,round -trip translation,to reduce,group disparity,round -trip translation to reduce group disparity,0.7106284499168396
translation,44,66,baselines,group disparity,of,sentiment classifiers,group disparity of sentiment classifiers,0.6175428032875061
translation,44,66,baselines,sentiment classifiers,for,danish,sentiment classifiers for danish,0.6394626498222351
translation,44,52,hyperparameters,two different pretrained language models,namely,"multilingual laser model ( artetxe and schwenk , 2019 )","two different pretrained language models namely multilingual laser model ( artetxe and schwenk , 2019 )",0.6330909729003906
translation,44,52,hyperparameters,two different pretrained language models,namely,"monolingual bert ( devlin et al. , 2019 )","two different pretrained language models namely monolingual bert ( devlin et al. , 2019 )",0.6247427463531494
translation,44,52,hyperparameters,"monolingual bert ( devlin et al. , 2019 )",trained for,danish,"monolingual bert ( devlin et al. , 2019 ) trained for danish",0.7341945767402649
translation,44,52,hyperparameters,hyperparameters,use,two different pretrained language models,hyperparameters use two different pretrained language models,0.5140702724456787
translation,44,52,hyperparameters,hyperparameters,trained for,danish,hyperparameters trained for danish,0.7159761786460876
translation,44,6,results,impact,of,round-trip translation,impact of round-trip translation,0.6108768582344055
translation,44,6,results,round-trip translation,on,demographic parity,round-trip translation on demographic parity,0.5237032771110535
translation,44,6,results,round-trip translation,show,round -trip translation,round-trip translation show round -trip translation,0.6608357429504395
translation,44,6,results,demographic parity,of,sentiment classifiers,demographic parity of sentiment classifiers,0.5852938890457153
translation,44,6,results,classification fairness,at,test time,classification fairness at test time,0.5175735354423523
translation,44,6,results,round -trip translation,has,consistently improves,round -trip translation has consistently improves,0.6124139428138733
translation,44,6,results,consistently improves,has,classification fairness,consistently improves has classification fairness,0.5590733885765076
translation,44,6,results,test time,has,reducing,test time has reducing,0.5751363039016724
translation,44,6,results,reducing,has,up to 47 %,reducing has up to 47 %,0.6123502254486084
translation,44,6,results,results,explore,impact,results explore impact,0.6217379570007324
translation,44,59,results,increases,up to,39 %,increases up to 39 %,0.6934587359428406
translation,44,59,results,increases,up to,47 %,increases up to 47 %,0.6752683520317078
translation,44,59,results,decreases,up to,47 %,decreases up to 47 %,0.6795217394828796
translation,44,77,results,2/4 classifiers,saw,improvements,2/4 classifiers saw improvements,0.6917025446891785
translation,44,77,results,improvements,for,majority groups,improvements for majority groups,0.615971028804779
translation,44,77,results,results,For,2/4 classifiers,results For 2/4 classifiers,0.5625653266906738
translation,45,174,baselines,system,convert,qa pairs,system convert qa pairs,0.7381657361984253
translation,45,174,baselines,bert ( stories only ; mlm ),convert,qa pairs,bert ( stories only ; mlm ) convert qa pairs,0.6732285022735596
translation,45,174,baselines,stories,of,spartqa - auto,stories of spartqa - auto,0.599675178527832
translation,45,174,baselines,stories,of,spartqa - auto,stories of spartqa - auto,0.599675178527832
translation,45,174,baselines,bert ( spartqa - auto ; mlm ),convert,qa pairs,bert ( spartqa - auto ; mlm ) convert qa pairs,0.694457471370697
translation,45,174,baselines,qa pairs,in,spartqa - auto,qa pairs in spartqa - auto,0.5390599370002747
translation,45,174,baselines,qa pairs,into,textual statements,qa pairs into textual statements,0.576953649520874
translation,45,174,baselines,spartqa - auto,into,textual statements,spartqa - auto into textual statements,0.5609510540962219
translation,45,174,baselines,text,as,mlm,text as mlm,0.6077252626419067
translation,45,174,baselines,system,has,bert ( stories only ; mlm ),system has bert ( stories only ; mlm ),0.6140885353088379
translation,45,174,baselines,baselines,has,system,baselines has system,0.6421393752098083
translation,45,174,baselines,baselines,has,bert ( stories only ; mlm ),baselines has bert ( stories only ; mlm ),0.5835835933685303
translation,45,27,experiments,spatial language,grounded to,geometry of visual scenes,spatial language grounded to geometry of visual scenes,0.6908935904502869
translation,45,167,experiments,spartqa - auto,improves,spatial reasoning,spartqa - auto improves spatial reasoning,0.6813230514526367
translation,45,167,experiments,performance,on,spartqa - human,performance on spartqa - human,0.5267999172210693
translation,45,167,experiments,spartqa - human,in,low-resource setting,spartqa - human in low-resource setting,0.5728551745414734
translation,45,167,experiments,0.6 k qa pairs,from,spartqa - human,0.6 k qa pairs from spartqa - human,0.5232541561126709
translation,45,167,experiments,0.6 k qa pairs,used for,fine-tuning,0.6 k qa pairs used for fine-tuning,0.6437633633613586
translation,45,167,experiments,0.5 k,for,testing,0.5 k for testing,0.7111654281616211
translation,45,167,experiments,fine-tuning,has,lms,fine-tuning has lms,0.5815005898475647
translation,45,167,experiments,fine-tuning,has,0.5 k,fine-tuning has 0.5 k,0.5633535981178284
translation,45,165,hyperparameters,adamw,with,2 ? 10 ?6 learning rate,adamw with 2 ? 10 ?6 learning rate,0.6125576496124268
translation,45,165,hyperparameters,adamw,with,focal loss,adamw with focal loss,0.6475701332092285
translation,45,165,hyperparameters,adamw,with,? = 2,adamw with ? = 2,0.6792542934417725
translation,45,165,hyperparameters,adamw,with,? = 2,adamw with ? = 2,0.6792542934417725
translation,45,165,hyperparameters,? = 2,for training,models,? = 2 for training models,0.8317444920539856
translation,45,50,model,scene structure,of,images,scene structure of images,0.6173290014266968
translation,45,50,model,scene structure,design,novel cfgs,scene structure design novel cfgs,0.5686478018760681
translation,45,50,model,spatial reasoning rules,to automatically generate,"data ( i.e. , spartqa - auto )","spatial reasoning rules to automatically generate data ( i.e. , spartqa - auto )",0.7071611285209656
translation,45,50,model,"data ( i.e. , spartqa - auto )",to obtain,distant supervision signals,"data ( i.e. , spartqa - auto ) to obtain distant supervision signals",0.5843183398246765
translation,45,50,model,distant supervision signals,for,spatial reasoning,distant supervision signals for spatial reasoning,0.5492472648620605
translation,45,50,model,spatial reasoning,over,text,spatial reasoning over text,0.6396961212158203
translation,45,50,model,model,exploit,scene structure,model exploit scene structure,0.7025305032730103
translation,45,51,results,spartqa - auto,rich source of,spatial knowledge,spartqa - auto rich source of spatial knowledge,0.6788218021392822
translation,45,51,results,spatial knowledge,improved,performance,spatial knowledge improved performance,0.6605084538459778
translation,45,51,results,performance,of,lms,performance of lms,0.6617756485939026
translation,45,51,results,performance,on,different data domains,performance on different data domains,0.5535700917243958
translation,45,51,results,lms,on,spartqa - human,lms on spartqa - human,0.5971340537071228
translation,45,51,results,different data domains,such as,babi and boolq,different data domains such as babi and boolq,0.6247313618659973
translation,45,51,results,results,has,spartqa - auto,results has spartqa - auto,0.5716547966003418
translation,45,89,results,expert performance,on,100 examples,expert performance on 100 examples,0.5134113430976868
translation,45,89,results,expert performance,measured by,accuracy,expert performance measured by accuracy,0.7488168478012085
translation,45,89,results,100 examples,of,spartqa - human 's test set,100 examples of spartqa - human 's test set,0.5396487712860107
translation,45,89,results,accuracy,of answering,questions,accuracy of answering questions,0.7373809814453125
translation,45,89,results,questions,is,92 %,questions is 92 %,0.593120813369751
translation,45,89,results,92 %,across,four q-types,92 % across four q-types,0.6774555444717407
translation,45,89,results,results,has,expert performance,results has expert performance,0.5758697986602783
translation,45,171,results,system 2,performs,consistently lower,system 2 performs consistently lower,0.6633114814758301
translation,45,171,results,consistently lower,than,system 5,consistently lower than system 5,0.6511703133583069
translation,45,171,results,results,see that,system 2,results see that system 2,0.6214669346809387
translation,45,172,results,performing better,than,majority baseline,performing better than majority baseline,0.5663770437240601
translation,45,172,results,majority baseline,on,yn q-type,majority baseline on yn q-type,0.556928277015686
translation,45,177,results,system 3,slightly improves over,system 2,system 3 slightly improves over system 2,0.7193133234977722
translation,45,177,results,results,see that,system 3,results see that system 3,0.6622195243835449
translation,45,180,results,proposed system,performs,better,proposed system performs better,0.6328778862953186
translation,45,180,results,better,than,other three baseline systems,better than other three baseline systems,0.5193246603012085
translation,45,180,results,accuracy,on,yn,accuracy on yn,0.598242998123169
translation,45,196,results,xlnet,performs,best,xlnet performs best,0.6713972687721252
translation,45,196,results,best,on,all q-types,best on all q-types,0.5154730081558228
translation,45,196,results,all q-types,except,accuracy,all q-types except accuracy,0.7259818911552429
translation,45,196,results,accuracy,on,spartqa - human 's yn section,accuracy on spartqa - human 's yn section,0.5347857475280762
translation,45,196,results,results,has,xlnet,results has xlnet,0.5467540621757507
translation,46,46,ablation-analysis,embedding based analysis,shows that,reranking,embedding based analysis shows that reranking,0.6576732397079468
translation,46,46,ablation-analysis,reranking,of,wair evidence chains,reranking of wair evidence chains,0.488998144865036
translation,46,46,ablation-analysis,reranking,helps,reranker,reranking helps reranker,0.6935743689537048
translation,46,46,ablation-analysis,wair evidence chains,helps,reranker,wair evidence chains helps reranker,0.6326484680175781
translation,46,46,ablation-analysis,reranker,to project,embedding representations,reranker to project embedding representations,0.6616126298904419
translation,46,46,ablation-analysis,embedding representations,of,evidence facts,embedding representations of evidence facts,0.555117130279541
translation,46,46,ablation-analysis,complementary knowledge aggregation,during,qa stage,complementary knowledge aggregation during qa stage,0.6946861743927002
translation,46,46,ablation-analysis,qa stage,necessary for,multi-hop reasoning,qa stage necessary for multi-hop reasoning,0.7244789600372314
translation,46,46,ablation-analysis,evidence facts,has,differently,evidence facts has differently,0.6097002029418945
translation,46,46,ablation-analysis,ablation analysis,has,embedding based analysis,ablation analysis has embedding based analysis,0.5732536315917969
translation,46,162,ablation-analysis,two-step evidence retrieval process,substantially impacts,qa performance,two-step evidence retrieval process substantially impacts qa performance,0.7435903549194336
translation,46,163,ablation-analysis,top reranked wair chain,leads to,higher qa performance,top reranked wair chain leads to higher qa performance,0.6545987725257874
translation,46,163,ablation-analysis,top reranked wair chain,leads to,2.3 % f1,top reranked wair chain leads to 2.3 % f1,0.6255671977996826
translation,46,163,ablation-analysis,higher qa performance,on,qasc,higher qa performance on qasc,0.6017559170722961
translation,46,163,ablation-analysis,5.2 %,on,qasc,5.2 % on qasc,0.5727489590644836
translation,46,163,ablation-analysis,2.3 % f1,on,multirc,2.3 % f1 on multirc,0.6104291081428528
translation,46,163,ablation-analysis,higher qa performance,has,5.2 %,higher qa performance has 5.2 %,0.5741826891899109
translation,46,163,ablation-analysis,higher qa performance,has,2.3 % f1,higher qa performance has 2.3 % f1,0.5735984444618225
translation,46,163,ablation-analysis,ablation analysis,has,top reranked wair chain,ablation analysis has top reranked wair chain,0.5782157778739929
translation,46,194,ablation-analysis,inter-justification alignment similarity score,of,jointrr,inter-justification alignment similarity score of jointrr,0.5389304757118225
translation,46,194,ablation-analysis,substantially lower,majority of the layers after,layer 3,substantially lower majority of the layers after layer 3,0.8210254311561584
translation,46,194,ablation-analysis,ablation analysis,has,inter-justification alignment similarity score,ablation analysis has inter-justification alignment similarity score,0.5151121020317078
translation,46,197,ablation-analysis,singlerr,learns to consider,both sentences,singlerr learns to consider both sentences,0.730543315410614
translation,46,197,ablation-analysis,qa performance,by,4.3 % em0,qa performance by 4.3 % em0,0.5951627492904663
translation,46,197,ablation-analysis,both sentences,has,similar,both sentences has similar,0.6262670159339905
translation,46,197,ablation-analysis,hurts,has,qa performance,hurts has qa performance,0.6009311079978943
translation,46,197,ablation-analysis,ablation analysis,has,singlerr,ablation analysis has singlerr,0.5682993531227112
translation,46,106,baselines,baselines,has,supervised evidence reranking,baselines has supervised evidence reranking,0.5424639582633972
translation,46,114,baselines,all the evidence sentences,having,probability,all the evidence sentences having probability,0.6555046439170837
translation,46,114,baselines,probability,larger than,0.5,probability larger than 0.5,0.7276877760887146
translation,46,114,baselines,concatenated,to create,final evidence text,concatenated to create final evidence text,0.6898650527000427
translation,46,114,baselines,singlerr approach,has,all the evidence sentences,singlerr approach has all the evidence sentences,0.5962749719619751
translation,46,119,baselines,multiple -choice question answering ( mcqa ) architecture,of,roberta,multiple -choice question answering ( mcqa ) architecture of roberta,0.4872111976146698
translation,46,119,baselines,multiple -choice question answering ( mcqa ) architecture,where,softmax,multiple -choice question answering ( mcqa ) architecture where softmax,0.5597648620605469
translation,46,119,baselines,roberta,for,qasc,roberta for qasc,0.5931457877159119
translation,46,119,baselines,discriminate,among,eight answer choices,discriminate among eight answer choices,0.6041516661643982
translation,46,65,experiments,candidate evidence chain,from,wair,candidate evidence chain from wair,0.606724202632904
translation,46,65,experiments,candidate evidence chain,assist,reranker method,candidate evidence chain assist reranker method,0.5998982191085815
translation,46,113,hyperparameters,roberta - base,with,learning rate,roberta - base with learning rate,0.6535390019416809
translation,46,113,hyperparameters,roberta - base,with,maximum sequence length,roberta - base with maximum sequence length,0.6648420095443726
translation,46,113,hyperparameters,roberta - base,with,batch size,roberta - base with batch size,0.674845278263092
translation,46,113,hyperparameters,roberta - base,with,4 epochs,roberta - base with 4 epochs,0.6738254427909851
translation,46,113,hyperparameters,learning rate,of,1e?5,learning rate of 1e?5,0.6333191394805908
translation,46,113,hyperparameters,maximum sequence length,of,256 5,maximum sequence length of 256 5,0.6602045893669128
translation,46,113,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,46,120,hyperparameters,inputs,to,roberta -mcqa,inputs to roberta -mcqa,0.6049313545227051
translation,46,120,hyperparameters,roberta -mcqa,consist of,eight queries,roberta -mcqa consist of eight queries,0.6798197031021118
translation,46,120,hyperparameters,hyperparameters,has,inputs,hyperparameters has inputs,0.5048274993896484
translation,46,5,model,simple approach,retrieves and reranks,set,simple approach retrieves and reranks set,0.735321581363678
translation,46,5,model,set,of,evidence facts,set of evidence facts,0.5981993079185486
translation,46,5,model,evidence facts,has,jointly,evidence facts has jointly,0.6253054738044739
translation,46,32,model,compositional evidence,propose,simple unsupervised retriever - weighted alignment - based information retrieval algorithm ( wair ),compositional evidence propose simple unsupervised retriever - weighted alignment - based information retrieval algorithm ( wair ),0.6297994256019592
translation,46,32,model,simple unsupervised retriever - weighted alignment - based information retrieval algorithm ( wair ),generates,candidate evidence chains,simple unsupervised retriever - weighted alignment - based information retrieval algorithm ( wair ) generates candidate evidence chains,0.6415082812309265
translation,46,32,model,candidate evidence chains,based on,two key heuristics,candidate evidence chains based on two key heuristics,0.6458977460861206
translation,46,32,model,candidate evidence chains,based on,associativity,candidate evidence chains based on associativity,0.6396178007125854
translation,46,32,model,model,For retrieving,compositional evidence,model For retrieving compositional evidence,0.6889820694923401
translation,46,39,model,our unsupervised wair approach,weighs down,query terms,our unsupervised wair approach weighs down query terms,0.585517168045044
translation,46,39,model,our unsupervised wair approach,increases,weights,our unsupervised wair approach increases weights,0.6372262239456177
translation,46,39,model,query terms,that have already been covered by,previously retrieved sentences,query terms that have already been covered by previously retrieved sentences,0.6360004544258118
translation,46,39,model,weights,of,reformulated query terms,weights of reformulated query terms,0.537747323513031
translation,46,39,model,2 iterations,has,our unsupervised wair approach,2 iterations has our unsupervised wair approach,0.4458792805671692
translation,46,39,model,reformulated query terms,has,that have not been covered yet,reformulated query terms has that have not been covered yet,0.5829895734786987
translation,46,39,model,model,In,2 iterations,model In 2 iterations,0.5635144710540771
translation,46,40,model,clusters,of,evidence sentences,clusters of evidence sentences,0.5987820625305176
translation,46,40,model,evidence sentences,generated by,wair,evidence sentences generated by wair,0.6776794791221619
translation,46,42,model,top reranked set of sentences,fed into,answer classification component,top reranked set of sentences fed into answer classification component,0.6648228764533997
translation,46,42,model,model,has,top reranked set of sentences,model has top reranked set of sentences,0.5872845649719238
translation,46,35,results,candidate evidence chain,WAIR to,roberta reranker,candidate evidence chain WAIR to roberta reranker,0.6056965589523315
translation,46,35,results,candidate evidence chain,achieves,substantially better performance,candidate evidence chain achieves substantially better performance,0.6756148934364319
translation,46,35,results,substantially better performance,than,same reranker,substantially better performance than same reranker,0.6213383674621582
translation,46,35,results,same reranker,instead fed with,individual candidate sentences,same reranker instead fed with individual candidate sentences,0.6531420946121216
translation,46,35,results,results,feeding,candidate evidence chain,results feeding candidate evidence chain,0.6900110244750977
translation,46,47,results,just the simple construction,of,candidate evidence,just the simple construction of candidate evidence,0.5660043358802795
translation,46,47,results,candidate evidence,using,wair,candidate evidence using wair,0.7427722811698914
translation,46,47,results,wair,leads to,substantial higher ( 10.2 % recall@2,wair leads to substantial higher ( 10.2 % recall@2,0.6671845316886902
translation,46,47,results,substantial higher ( 10.2 % recall@2,on,"qasc ( khot et al. , 2019a )","substantial higher ( 10.2 % recall@2 on qasc ( khot et al. , 2019a )",0.5176805853843689
translation,46,47,results,3.6 % f1,on,multirc ) evidence selection performance,3.6 % f1 on multirc ) evidence selection performance,0.5166981816291809
translation,46,47,results,multirc ) evidence selection performance,with,same roberta reranker,multirc ) evidence selection performance with same roberta reranker,0.6471989154815674
translation,46,47,results,same roberta reranker,fed with,individual candidate sentences,same roberta reranker fed with individual candidate sentences,0.6358420252799988
translation,46,47,results,results,show that,just the simple construction,results show that just the simple construction,0.5262314081192017
translation,46,48,results,evidence selection,on,two multi-hop qa datasets,evidence selection on two multi-hop qa datasets,0.5204152464866638
translation,46,48,results,30.5 % recall@2,on,qasc,30.5 % recall@2 on qasc,0.533154308795929
translation,46,48,results,68.0 %,on,multirc,68.0 % on multirc,0.5690886378288269
translation,46,48,results,two multi-hop qa datasets,has,30.5 % recall@2,two multi-hop qa datasets has 30.5 % recall@2,0.5616757869720459
translation,46,50,results,qa performance,improves,2.3 % em0,qa performance improves 2.3 % em0,0.7103434801101685
translation,46,50,results,qa performance,improves,5.2 % accuracy,qa performance improves 5.2 % accuracy,0.7091017961502075
translation,46,50,results,2.3 % em0,in,multirc,2.3 % em0 in multirc,0.6010735034942627
translation,46,50,results,5.2 % accuracy,in,qasc,5.2 % accuracy in qasc,0.566913366317749
translation,46,50,results,top reranked wair evidence chain,fed to,qa module,top reranked wair evidence chain fed to qa module,0.6939336061477661
translation,46,50,results,results,show that,qa performance,results show that qa performance,0.4906654953956604
translation,46,51,results,top reranked wair evidence chain,achieve,state - of - the - art qa performance,top reranked wair evidence chain achieve state - of - the - art qa performance,0.6063045263290405
translation,46,51,results,top reranked wair evidence chain,achieve,second best qa results,top reranked wair evidence chain achieve second best qa results,0.6243439316749573
translation,46,51,results,state - of - the - art qa performance,on,multirc,state - of - the - art qa performance on multirc,0.5703054666519165
translation,46,51,results,second best qa results,on,qasc,second best qa results on qasc,0.5889163613319397
translation,46,51,results,results,feeding,top reranked wair evidence chain,results feeding top reranked wair evidence chain,0.7058036923408508
translation,46,64,results,evidence retrieval performance,of,same reranker,evidence retrieval performance of same reranker,0.5817751288414001
translation,46,64,results,same reranker,is,substantially improved,same reranker is substantially improved,0.6012428998947144
translation,46,64,results,substantially improved,resulting in,state - of - the - art performance,substantially improved resulting in state - of - the - art performance,0.6308621764183044
translation,46,64,results,outperforming,has,all the previous approaches,outperforming has all the previous approaches,0.5871257781982422
translation,46,64,results,results,has,evidence retrieval performance,results has evidence retrieval performance,0.553883969783783
translation,46,145,results,alignmentbased evidence retrieval approach ( wair ),has,outperforms,alignmentbased evidence retrieval approach ( wair ) has outperforms,0.6036977767944336
translation,46,145,results,outperforms,has,other ir benchmarks ( bm25 and alignment ),outperforms has other ir benchmarks ( bm25 and alignment ),0.5948333740234375
translation,46,146,results,two-step ir - based methods,for,evidence retrieval,two-step ir - based methods for evidence retrieval,0.6115721464157104
translation,46,146,results,outperforms,has,two-step ir - based methods,outperforms has two-step ir - based methods,0.5792059302330017
translation,46,147,results,reranking,leads to,absolute 10.4 %,reranking leads to absolute 10.4 %,0.6485481858253479
translation,46,147,results,reranking,leads to,3.6 % f1 improvement,reranking leads to 3.6 % f1 improvement,0.6179239153862
translation,46,147,results,absolute 10.4 %,on,qasc,absolute 10.4 % on qasc,0.5690735578536987
translation,46,147,results,3.6 % f1 improvement,on,multirc,3.6 % f1 improvement on multirc,0.550413191318512
translation,46,147,results,same reranker,fed with,individual sentences ( singlerr ),same reranker fed with individual sentences ( singlerr ),0.6528550982475281
translation,46,147,results,supervised reranking,has,reranking,supervised reranking has reranking,0.5869442224502563
translation,46,147,results,reranking,has,wair,reranking has wair,0.6303401589393616
translation,46,147,results,wair,has,candidate evidence chains ( jointrr ),wair has candidate evidence chains ( jointrr ),0.5998183488845825
translation,46,147,results,results,has,supervised reranking,results has supervised reranking,0.552692711353302
translation,46,149,results,results,has,recall comparison,results has recall comparison,0.5746797323226929
translation,46,150,results,just feeding wair candidate chains,result in,higher performance,just feeding wair candidate chains result in higher performance,0.6682645678520203
translation,46,150,results,higher performance,for retrieving,complete evidence,higher performance for retrieving complete evidence,0.7308197617530823
translation,46,150,results,complete evidence,than,singlerr,complete evidence than singlerr,0.5432824492454529
translation,46,150,results,singlerr,especially for,low recall scenarios,singlerr especially for low recall scenarios,0.6993425488471985
translation,46,150,results,results,has,just feeding wair candidate chains,results has just feeding wair candidate chains,0.6048758029937744
translation,46,155,results,performance,of,jointrr approach,performance of jointrr approach,0.6111735701560974
translation,46,155,results,performance,is,substantially improved,performance is substantially improved,0.6074433922767639
translation,46,155,results,substantially improved,when,gold evidence sentences,substantially improved when gold evidence sentences,0.6269392371177673
translation,46,155,results,gold evidence sentences,retrieved in,initial wair pool,gold evidence sentences retrieved in initial wair pool,0.5812721252441406
translation,46,158,results,top reranked wair chain,achieves,30.5 % recall@2,top reranked wair chain achieves 30.5 % recall@2,0.6563820242881775
translation,46,158,results,top reranked wair chain,achieves,67.6 % f1,top reranked wair chain achieves 67.6 % f1,0.6549636721611023
translation,46,158,results,30.5 % recall@2,on,qasc,30.5 % recall@2 on qasc,0.533154308795929
translation,46,158,results,67.6 % f1,on,multirc,67.6 % f1 on multirc,0.5793882012367249
translation,46,158,results,results,has,top reranked wair chain,results has top reranked wair chain,0.6118711233139038
translation,46,161,results,results,has,impact of two -step evidence retrieval,results has impact of two -step evidence retrieval,0.5336183309555054
translation,46,164,results,results,has,impact of retrieval recall,results has impact of retrieval recall,0.5989493727684021
translation,46,165,results,jointrr,achieves,higher recall@n score,jointrr achieves higher recall@n score,0.6835120916366577
translation,46,165,results,higher recall@n score,for finding,both ( or complete ) evidence,higher recall@n score for finding both ( or complete ) evidence,0.7069639563560486
translation,46,165,results,results,has,jointrr,results has jointrr,0.5240598320960999
translation,46,167,results,singlerr,achieves,marginally better performance,singlerr achieves marginally better performance,0.7099135518074036
translation,46,167,results,marginally better performance,on finding,atleast 1 evidence sentence,marginally better performance on finding atleast 1 evidence sentence,0.6910268068313599
translation,46,167,results,results,has,singlerr,results has singlerr,0.5339357256889343
translation,46,168,results,best qa performance,achieved at,higher recalls,best qa performance achieved at higher recalls,0.6759909987449646
translation,46,168,results,results,has,best qa performance,results has best qa performance,0.5786038041114807
translation,46,169,results,results,has,ceiling performance,results has ceiling performance,0.5298755168914795
translation,46,170,results,qa scores,of,jointrr,qa scores of jointrr,0.6109994649887085
translation,46,170,results,jointrr,approaches,human performance,jointrr approaches human performance,0.6747612953186035
translation,46,170,results,( pseudo ) oracle retriever,has,qa scores,( pseudo ) oracle retriever has qa scores,0.5938962697982788
translation,46,170,results,results,coupled with,( pseudo ) oracle retriever,results coupled with ( pseudo ) oracle retriever,0.7241377830505371
translation,46,172,results,state - of - the - art qa performance,on,multirc development and test sets,state - of - the - art qa performance on multirc development and test sets,0.5397953987121582
translation,46,172,results,top qa performance,has,roberta,top qa performance has roberta,0.5857771635055542
translation,46,172,results,results,has,top qa performance,results has top qa performance,0.5801218152046204
translation,46,173,results,second and third best results,on,qasc development and test sets,second and third best results on qasc development and test sets,0.5429289937019348
translation,46,173,results,results,achieves,second and third best results,results achieves second and third best results,0.6033098101615906
translation,46,189,results,jointrr,attends,considerably more,jointrr attends considerably more,0.7185983657836914
translation,46,189,results,considerably more,to,linking terms,considerably more to linking terms,0.5510923862457275
translation,46,189,results,results,has,jointrr,results has jointrr,0.5240598320960999
translation,47,169,ablation-analysis,twin confidence level threshold,to,? = 0.9,twin confidence level threshold to ? = 0.9,0.5897783637046814
translation,47,169,ablation-analysis,precision,of,q2q model,precision of q2q model,0.6113348007202148
translation,47,169,ablation-analysis,q2q model,raises to,89.9 %,q2q model raises to 89.9 %,0.6284483671188354
translation,47,169,ablation-analysis,89.9 %,with,recall,89.9 % with recall,0.6652593612670898
translation,47,169,ablation-analysis,recall,of,69.5 %,recall of 69.5 %,0.5714939832687378
translation,47,169,ablation-analysis,twin confidence level threshold,has,precision,twin confidence level threshold has precision,0.5533918142318726
translation,47,169,ablation-analysis,? = 0.9,has,precision,? = 0.9 has precision,0.6252602934837341
translation,47,169,ablation-analysis,ablation analysis,setting,twin confidence level threshold,ablation analysis setting twin confidence level threshold,0.4599058926105499
translation,47,170,baselines,performance,of,q2q similarity classifier,performance of q2q similarity classifier,0.571414589881897
translation,47,170,baselines,performance,namely,cosine similarity,performance namely cosine similarity,0.7177948355674744
translation,47,170,baselines,cosine similarity,over,use embedding,cosine similarity over use embedding,0.6763504147529602
translation,47,170,baselines,cosine similarity,over,roberta 15 embedding,cosine similarity over roberta 15 embedding,0.676990807056427
translation,47,170,baselines,cosine similarity,over,roberta 15 embedding,cosine similarity over roberta 15 embedding,0.676990807056427
translation,47,170,baselines,cosine similarity,over,roberta 15 embedding,cosine similarity over roberta 15 embedding,0.676990807056427
translation,47,15,experimental-setup,https,://,registry.opendata.aws,https :// registry.opendata.aws,0.49982038140296936
translation,47,15,experimental-setup,names,has,amazon - pqsim,names has amazon - pqsim,0.615315854549408
translation,47,162,experimental-setup,annoy 11 python library,for,implementation,annoy 11 python library for implementation,0.5485365390777588
translation,47,162,experimental-setup,implementation,of,efficient aknn retrieval,implementation of efficient aknn retrieval,0.5714251399040222
translation,47,162,experimental-setup,experimental setup,use,annoy 11 python library,experimental setup use annoy 11 python library,0.6080875992774963
translation,47,166,experimental-setup,q2q model,apply,"standard pretrained roberta ( liu et al. , 2019 ) classifier","q2q model apply standard pretrained roberta ( liu et al. , 2019 ) classifier",0.5735034346580505
translation,47,166,experimental-setup,experimental setup,For,q2q model,experimental setup For q2q model,0.6328625082969666
translation,47,262,experimental-setup,incorporated roberta models,use,maximum sequence length,incorporated roberta models use maximum sequence length,0.6532389521598816
translation,47,262,experimental-setup,incorporated roberta models,use,dropout,incorporated roberta models use dropout,0.6650442481040955
translation,47,262,experimental-setup,incorporated roberta models,use,32 batch size,incorporated roberta models use 32 batch size,0.6801190972328186
translation,47,262,experimental-setup,maximum sequence length,of,256,maximum sequence length of 256,0.6090841293334961
translation,47,262,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,47,262,experimental-setup,32 batch size,for,training,32 batch size for training,0.561586856842041
translation,47,319,experimental-setup,maximum sequence length,of,128,maximum sequence length of 128,0.6129645705223083
translation,47,319,experimental-setup,batch size,has,32,batch size has 32,0.630264163017273
translation,47,319,experimental-setup,batch size,has,maximum sequence length,batch size has maximum sequence length,0.5512473583221436
translation,47,319,experimental-setup,experimental setup,learning rate,5e - 5,experimental setup learning rate 5e - 5,0.678176760673523
translation,47,319,experimental-setup,experimental setup,learning rate,3 epochs,experimental setup learning rate 3 epochs,0.6796107292175293
translation,47,8,experiments,contextual similarity,between,products,contextual similarity between products,0.661584198474884
translation,47,8,experiments,products,based on,answers,products based on answers,0.6568266749382019
translation,47,8,experiments,answers,provide,same question,answers provide same question,0.6690645217895508
translation,47,7,model,novel and complementary approach,for predicting,answer,novel and complementary approach for predicting answer,0.7787117958068848
translation,47,7,model,answer,for,such questions,answer for such questions,0.6276300549507141
translation,47,7,model,answer,for,similar questions,answer for similar questions,0.6285467147827148
translation,47,7,model,similar questions,asked on,similar products,similar questions asked on similar products,0.7588561177253723
translation,47,7,model,model,propose,novel and complementary approach,model propose novel and complementary approach,0.6780610084533691
translation,47,233,model,novel answer prediction approach,in,pqa domain,novel answer prediction approach in pqa domain,0.5247637033462524
translation,47,233,model,novel answer prediction approach,directly leverages,similar questions,novel answer prediction approach directly leverages similar questions,0.7351625561714172
translation,47,233,model,answered,with respect to,other products,answered with respect to other products,0.6959016919136047
translation,47,233,model,simba,has,novel answer prediction approach,simba has novel answer prediction approach,0.6124632358551025
translation,47,233,model,similar questions,has,answered,similar questions has answered,0.6277624368667603
translation,47,233,model,model,presented,simba,model presented simba,0.6847927570343018
translation,47,171,results,q2q model,has,significantly outperforms,q2q model has significantly outperforms,0.6130362153053284
translation,47,171,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,47,171,results,results,showing,q2q model,results showing q2q model,0.6779796481132507
translation,47,186,results,model,achieves,relatively high accuracy,model achieves relatively high accuracy,0.6751955151557922
translation,47,186,results,relatively high accuracy,with,macro average,relatively high accuracy with macro average,0.666516125202179
translation,47,186,results,relatively high accuracy,presenting,significant lift,relatively high accuracy presenting significant lift,0.7079623937606812
translation,47,186,results,macro average,of,77.2 %,macro average of 77.2 %,0.5476592183113098
translation,47,186,results,77.2 %,over,all categories,77.2 % over all categories,0.5784231424331665
translation,47,186,results,significant lift,of,9.7 %,significant lift of 9.7 %,0.5558685660362244
translation,47,186,results,9.7 %,over,majority decision baseline,9.7 % over majority decision baseline,0.6269237399101257
translation,47,186,results,results,has,model,results has model,0.5339115858078003
translation,47,216,results,drop,in,accuracy,drop in accuracy,0.5577701330184937
translation,47,216,results,accuracy,of,majority baseline,accuracy of majority baseline,0.5522401332855225
translation,47,216,results,majority baseline,as,number of twins,majority baseline as number of twins,0.5073954463005066
translation,47,216,results,number of twins,has,grows,number of twins has grows,0.5942548513412476
translation,47,216,results,results,see,drop,results see drop,0.5868945717811584
translation,47,217,results,accuracy,of,qso,accuracy of qso,0.5880580544471741
translation,47,217,results,significantly higher,than,majority - vote baseline,significantly higher than majority - vote baseline,0.5716491341590881
translation,47,217,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,47,221,results,pso method,does n't improve,qso,pso method does n't improve qso,0.6390128135681152
translation,47,221,results,results,find that,pso method,results find that pso method,0.6743136048316956
translation,47,224,results,simba,see,large performance improvement,simba see large performance improvement,0.5885109901428223
translation,47,224,results,large performance improvement,over,qso and pso methods,large performance improvement over qso and pso methods,0.6735102534294128
translation,47,224,results,results,Moving to,simba,results Moving to simba,0.5940901041030884
translation,47,225,results,consistent improvement,in,accuracy,consistent improvement in accuracy,0.5118999481201172
translation,47,225,results,accuracy,with,number of twins,accuracy with number of twins,0.6004739999771118
translation,47,225,results,results,see,consistent improvement,results see consistent improvement,0.6371880769729614
translation,47,226,results,apc method,performs,very well,apc method performs very well,0.6229100823402405
translation,47,226,results,greatly outperforms,has,majority - vote,greatly outperforms has majority - vote,0.5761461853981018
translation,47,226,results,greatly outperforms,has,the qso and pso baselines,greatly outperforms has the qso and pso baselines,0.5940273404121399
translation,47,226,results,results,has,apc method,results has apc method,0.5290288925170898
translation,47,227,results,segment of questions,with,less than 10 twins,segment of questions with less than 10 twins,0.6611029505729675
translation,47,227,results,segment of questions,has,apc,segment of questions has apc,0.6470718383789062
translation,47,227,results,less than 10 twins,has,apc,less than 10 twins has apc,0.5883151888847351
translation,47,227,results,apc,has,outperforms,apc has outperforms,0.6410955786705017
translation,47,227,results,outperforms,has,simba method,outperforms has simba method,0.6041325926780701
translation,47,227,results,results,For,segment of questions,results For segment of questions,0.6073647141456604
translation,47,229,results,segment of questions,with,60 or more twins,segment of questions with 60 or more twins,0.628803551197052
translation,47,229,results,13.6 %,of,questions,13.6 % of questions,0.5524526238441467
translation,47,229,results,consistently outperforms,by,1 - 2 %,consistently outperforms by 1 - 2 %,0.6177442073822021
translation,47,229,results,inductive baseline,by,1 - 2 %,inductive baseline by 1 - 2 %,0.6057842969894409
translation,47,229,results,segment of questions,has,simba method,segment of questions has simba method,0.5956605076789856
translation,47,229,results,simba method,has,consistently outperforms,simba method has consistently outperforms,0.6325129270553589
translation,47,229,results,consistently outperforms,has,inductive baseline,consistently outperforms has inductive baseline,0.6037949919700623
translation,47,229,results,results,for,segment of questions,results for segment of questions,0.6073647141456604
translation,47,230,results,all questions,with,at least 1 twin,all questions with at least 1 twin,0.630145788192749
translation,47,230,results,all questions,with,at least 60 twins,all questions with at least 60 twins,0.6361415982246399
translation,47,230,results,all questions,with,at least 60 twins,all questions with at least 60 twins,0.6361415982246399
translation,47,230,results,apc method,dominates in,7 out of the 11 categories,apc method dominates in 7 out of the 11 categories,0.7667209506034851
translation,47,230,results,apc method,for,questions,apc method for questions,0.6298332810401917
translation,47,230,results,questions,with,at least 60 twins,questions with at least 60 twins,0.6401470899581909
translation,47,230,results,simba method,dominates in,6 out of the 11 categories,simba method dominates in 6 out of the 11 categories,0.7714837789535522
translation,47,230,results,all questions,has,apc method,all questions has apc method,0.572248101234436
translation,47,230,results,all questions,has,simba method,all questions has simba method,0.5746462345123291
translation,47,230,results,questions,has,simba method,questions has simba method,0.6020087599754333
translation,47,230,results,at least 60 twins,has,simba method,at least 60 twins has simba method,0.5728181004524231
translation,47,230,results,results,considering,all questions,results considering all questions,0.5899239182472229
translation,47,230,results,results,for,questions,results for questions,0.5702887773513794
translation,47,231,results,two approaches,can be,effectively joined,two approaches can be effectively joined,0.6885617971420288
translation,47,231,results,outperforms,over,all subsets,outperforms over all subsets,0.6860272884368896
translation,47,231,results,two approaches,has,compliment,two approaches has compliment,0.5708709359169006
translation,47,231,results,compliment,has,each other,compliment has each other,0.5492910146713257
translation,47,231,results,simba + apc method,has,outperforms,simba + apc method has outperforms,0.6350163221359253
translation,47,231,results,results,see that,two approaches,results see that two approaches,0.6673964858055115
translation,48,160,baselines,atae -lstm,utilizes,aspect embedding,atae -lstm utilizes aspect embedding,0.5941663384437561
translation,48,160,baselines,atae -lstm,utilizes,attention mechanism,atae -lstm utilizes attention mechanism,0.6145544052124023
translation,48,160,baselines,attention mechanism,in,aspectlevel sentiment classification,attention mechanism in aspectlevel sentiment classification,0.502601683139801
translation,48,160,baselines,baselines,has,atae -lstm,baselines has atae -lstm,0.5194156169891357
translation,48,161,baselines,ian,employs,two lstms,ian employs two lstms,0.6283530592918396
translation,48,161,baselines,ian,employs,interactive attention mechanism,ian employs interactive attention mechanism,0.6440051794052124
translation,48,161,baselines,interactive attention mechanism,to generate,representations,interactive attention mechanism to generate representations,0.715827465057373
translation,48,161,baselines,representations,for,aspect and sentence,representations for aspect and sentence,0.6712110638618469
translation,48,161,baselines,baselines,has,ian,baselines has ian,0.603387176990509
translation,48,162,baselines,ram,uses,multiple attention and memory networks,ram uses multiple attention and memory networks,0.6333059072494507
translation,48,162,baselines,multiple attention and memory networks,to learn,sentence representation,multiple attention and memory networks to learn sentence representation,0.6204143166542053
translation,48,162,baselines,ram,has,"chen et al. , 2017 )","ram has chen et al. , 2017 )",0.5851209759712219
translation,48,162,baselines,baselines,has,ram,baselines has ram,0.6504290103912354
translation,48,163,baselines,mgan,designs,multigrained attention mechanism,mgan designs multigrained attention mechanism,0.613237738609314
translation,48,163,baselines,multigrained attention mechanism,to capture,word-level interactions,multigrained attention mechanism to capture word-level interactions,0.6775348782539368
translation,48,163,baselines,word-level interactions,between,aspect and context,word-level interactions between aspect and context,0.652218222618103
translation,48,163,baselines,baselines,has,mgan,baselines has mgan,0.6378287076950073
translation,48,164,baselines,tnet,transforms,bilstm embeddings,tnet transforms bilstm embeddings,0.6699641346931458
translation,48,164,baselines,tnet,uses,cnn,tnet uses cnn,0.6224582195281982
translation,48,164,baselines,bilstm embeddings,into,target-specific embeddings,bilstm embeddings into target-specific embeddings,0.5063744783401489
translation,48,164,baselines,cnn,to extract,final embeddings,cnn to extract final embeddings,0.664302408695221
translation,48,164,baselines,final embeddings,for,classification,final embeddings for classification,0.6056102514266968
translation,48,164,baselines,baselines,has,tnet,baselines has tnet,0.5590061545372009
translation,48,165,baselines,baselines,has,asgcn,baselines has asgcn,0.6009701490402222
translation,48,168,baselines,bigcn,uses,hierarchical graph structure,bigcn uses hierarchical graph structure,0.6033093929290771
translation,48,168,baselines,"zhang and qian , 2020 )",uses,hierarchical graph structure,"zhang and qian , 2020 ) uses hierarchical graph structure",0.6096274256706238
translation,48,168,baselines,hierarchical graph structure,to integrate,word co-occurrence information and dependency type information,hierarchical graph structure to integrate word co-occurrence information and dependency type information,0.6854898929595947
translation,48,168,baselines,bigcn,has,"zhang and qian , 2020 )","bigcn has zhang and qian , 2020 )",0.5964411497116089
translation,48,168,baselines,baselines,has,bigcn,baselines has bigcn,0.5481768846511841
translation,48,171,baselines,intergcn,utilizes,gcn,intergcn utilizes gcn,0.6085629463195801
translation,48,171,baselines,gcn,over,dependency tree,gcn over dependency tree,0.6272807121276855
translation,48,171,baselines,dependency tree,to learn,aspect representations,dependency tree to learn aspect representations,0.5585359334945679
translation,48,171,baselines,aspect representations,with,syntactic information,aspect representations with syntactic information,0.5817540884017944
translation,48,171,baselines,baselines,has,intergcn,baselines has intergcn,0.5795003771781921
translation,48,176,baselines,"bert ( devlin et al. , 2019 )",is,vanilla bert model,"bert ( devlin et al. , 2019 ) is vanilla bert model",0.5385352373123169
translation,48,176,baselines,"bert ( devlin et al. , 2019 )",using,representation of [ cls ],"bert ( devlin et al. , 2019 ) using representation of [ cls ]",0.6895939707756042
translation,48,176,baselines,"bert ( devlin et al. , 2019 )",for,predictions,"bert ( devlin et al. , 2019 ) for predictions",0.6809778213500977
translation,48,176,baselines,vanilla bert model,by feeding,sentence -aspect pair,vanilla bert model by feeding sentence -aspect pair,0.7016799449920654
translation,48,176,baselines,vanilla bert model,for,predictions,vanilla bert model for predictions,0.666904628276825
translation,48,176,baselines,representation of [ cls ],for,predictions,representation of [ cls ] for predictions,0.6138190031051636
translation,48,176,baselines,baselines,has,"bert ( devlin et al. , 2019 )","baselines has bert ( devlin et al. , 2019 )",0.5393238067626953
translation,48,178,baselines,r-gat model,uses,pre-trained bert,r-gat model uses pre-trained bert,0.5819488763809204
translation,48,178,baselines,pre-trained bert,to replace,bilstm,pre-trained bert to replace bilstm,0.6882497668266296
translation,48,178,baselines,bilstm,as,encoder,bilstm as encoder,0.5479370951652527
translation,48,180,baselines,dgedt model,uses,pre-trained bert,dgedt model uses pre-trained bert,0.5682119131088257
translation,48,180,baselines,pre-trained bert,to replace,bilstm,pre-trained bert to replace bilstm,0.6882497668266296
translation,48,180,baselines,bilstm,as,encoder,bilstm as encoder,0.5479370951652527
translation,48,146,experimental-setup,lal - parser,used for,dependency parsing,lal - parser used for dependency parsing,0.6118893027305603
translation,48,146,experimental-setup,lal - parser,provides,off-the-shelf parser,lal - parser provides off-the-shelf parser,0.6294187903404236
translation,48,146,experimental-setup,lal - parser,use,pretrained 300 - dimensional glove 3 vectors,lal - parser use pretrained 300 - dimensional glove 3 vectors,0.5749433636665344
translation,48,146,experimental-setup,pretrained 300 - dimensional glove 3 vectors,to initialize,word embeddings,pretrained 300 - dimensional glove 3 vectors to initialize word embeddings,0.6678568720817566
translation,48,146,experimental-setup,experimental setup,has,lal - parser,experimental setup has lal - parser,0.5726666450500488
translation,48,147,experimental-setup,dimensionality,of,position,dimensionality of position,0.6051093339920044
translation,48,147,experimental-setup,part-of-speech ( pos ) embeddings,set to,30,part-of-speech ( pos ) embeddings set to 30,0.6443527936935425
translation,48,147,experimental-setup,experimental setup,has,dimensionality,experimental setup has dimensionality,0.4790812134742737
translation,48,148,experimental-setup,"word , pos and position embeddings",input them into,bilstm model,"word , pos and position embeddings input them into bilstm model",0.636887788772583
translation,48,148,experimental-setup,bilstm model,whose,hidden size,bilstm model whose hidden size,0.5814087986946106
translation,48,148,experimental-setup,hidden size,set to,50,hidden size set to 50,0.7490441203117371
translation,48,148,experimental-setup,experimental setup,concatenate,"word , pos and position embeddings","experimental setup concatenate word , pos and position embeddings",0.6597985029220581
translation,48,149,experimental-setup,overfitting,apply,dropout,overfitting apply dropout,0.6489628553390503
translation,48,149,experimental-setup,dropout,rate of,0.7,dropout rate of 0.7,0.6630628108978271
translation,48,149,experimental-setup,dropout,to,input word embeddings,dropout to input word embeddings,0.47961774468421936
translation,48,149,experimental-setup,0.7,to,input word embeddings,0.7 to input word embeddings,0.5002456903457642
translation,48,149,experimental-setup,input word embeddings,of,bilstm,input word embeddings of bilstm,0.4824706017971039
translation,48,149,experimental-setup,experimental setup,To alleviate,overfitting,experimental setup To alleviate overfitting,0.6469776034355164
translation,48,150,experimental-setup,dropout rate,of,syngcn and semgcn modules,dropout rate of syngcn and semgcn modules,0.59433513879776
translation,48,150,experimental-setup,dropout rate,number of,syngcn and semgcn layers,dropout rate number of syngcn and semgcn layers,0.6930668354034424
translation,48,150,experimental-setup,syngcn and semgcn modules,set to,0.1,syngcn and semgcn modules set to 0.1,0.6725823283195496
translation,48,150,experimental-setup,syngcn and semgcn layers,set to,2,syngcn and semgcn layers set to 2,0.702946662902832
translation,48,150,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,48,151,experimental-setup,model weights,initialized from,uniform distribution,model weights initialized from uniform distribution,0.7345596551895142
translation,48,151,experimental-setup,experimental setup,has,model weights,experimental setup has model weights,0.5362875461578369
translation,48,152,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,48,152,experimental-setup,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,48,152,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,48,153,experimental-setup,dualgcn model,trained in,50 epochs,dualgcn model trained in 50 epochs,0.7314337491989136
translation,48,153,experimental-setup,50 epochs,with,batch size,50 epochs with batch size,0.596022367477417
translation,48,153,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,48,153,experimental-setup,experimental setup,has,dualgcn model,experimental setup has dualgcn model,0.5438632965087891
translation,48,154,experimental-setup,dualgcn + bert,use,bert- base-uncased,dualgcn + bert use bert- base-uncased,0.6607987284660339
translation,48,154,experimental-setup,experimental setup,For,dualgcn + bert,experimental setup For dualgcn + bert,0.6357911229133606
translation,48,154,experimental-setup,experimental setup,has,regularization coefficients,experimental setup has regularization coefficients,0.478807657957077
translation,48,7,model,dual graph convolutional networks ( du-algcn ) model,considers,complementarity,dual graph convolutional networks ( du-algcn ) model considers complementarity,0.6527675986289978
translation,48,7,model,complementarity,of,syntax structures and semantic correlations simultaneously,complementarity of syntax structures and semantic correlations simultaneously,0.6108920574188232
translation,48,7,model,model,propose,dual graph convolutional networks ( du-algcn ) model,model propose dual graph convolutional networks ( du-algcn ) model,0.6263264417648315
translation,48,8,model,dependency parsing errors,design,syngcn module,dependency parsing errors design syngcn module,0.535831868648529
translation,48,8,model,syngcn module,with,rich syntactic knowledge,syngcn module with rich syntactic knowledge,0.6019936800003052
translation,48,8,model,model,to alleviate,dependency parsing errors,model to alleviate dependency parsing errors,0.6234425902366638
translation,48,9,model,semantic correlations,design,semgcn module,semantic correlations design semgcn module,0.5511423349380493
translation,48,9,model,semgcn module,with,self-attention mechanism,semgcn module with self-attention mechanism,0.5640550255775452
translation,48,9,model,model,To capture,semantic correlations,model To capture semantic correlations,0.7287741303443909
translation,48,10,model,orthogonal and differential regularizers,to capture,semantic correlations,orthogonal and differential regularizers to capture semantic correlations,0.6621938347816467
translation,48,10,model,semantic correlations,between,words,semantic correlations between words,0.6386998295783997
translation,48,10,model,attention scores,in,semgcn module,attention scores in semgcn module,0.4818611145019531
translation,48,10,model,model,propose,orthogonal and differential regularizers,model propose orthogonal and differential regularizers,0.634850800037384
translation,48,11,model,orthogonal regularizer,encourages,semgcn,orthogonal regularizer encourages semgcn,0.6364517211914062
translation,48,11,model,semgcn,to learn,semantically correlated words,semgcn to learn semantically correlated words,0.5831054449081421
translation,48,11,model,semantically correlated words,with,less overlap,semantically correlated words with less overlap,0.6388964653015137
translation,48,11,model,less overlap,for,each word,less overlap for each word,0.6238483786582947
translation,48,11,model,model,has,orthogonal regularizer,model has orthogonal regularizer,0.5549805164337158
translation,48,12,model,differential regularizer,encourages,semgcn,differential regularizer encourages semgcn,0.6563831567764282
translation,48,12,model,semgcn,to learn,semantic features,semgcn to learn semantic features,0.6190429925918579
translation,48,12,model,model,has,differential regularizer,model has differential regularizer,0.5566549301147461
translation,48,31,model,novel architecture,has,dual graph convolution network ( dualgcn ),novel architecture has dual graph convolution network ( dualgcn ),0.5694978833198547
translation,48,31,model,model,propose,novel architecture,model propose novel architecture,0.7315564155578613
translation,48,32,model,probability matrix,of,all dependency arcs,probability matrix of all dependency arcs,0.5560622811317444
translation,48,32,model,all dependency arcs,from,dependency parser,all dependency arcs from dependency parser,0.5606565475463867
translation,48,32,model,model,use,probability matrix,model use probability matrix,0.6583688259124756
translation,48,34,model,semantic correlation - based graph convolutional network ( semgcn ),by utilizing,self-attention mechanism,semantic correlation - based graph convolutional network ( semgcn ) by utilizing self-attention mechanism,0.685248076915741
translation,48,34,model,model,construct,semantic correlation - based graph convolutional network ( semgcn ),model construct semantic correlation - based graph convolutional network ( semgcn ),0.6975327134132385
translation,48,35,model,attention matrix,shaped by,self-attending,attention matrix shaped by self-attending,0.7310308814048767
translation,48,35,model,attention matrix,viewed as,edge-weighted directed graph,attention matrix viewed as edge-weighted directed graph,0.5581359267234802
translation,48,35,model,attention matrix,represent,semantic correlations,attention matrix represent semantic correlations,0.5985637903213501
translation,48,35,model,semantic correlations,between,words,semantic correlations between words,0.6386998295783997
translation,48,36,model,biaffine module,to bridge,relevant information,biaffine module to bridge relevant information,0.5737254619598389
translation,48,36,model,relevant information,between,syngcn and semgcn modules,relevant information between syngcn and semgcn modules,0.6233780980110168
translation,48,36,model,model,utilize,biaffine module,model utilize biaffine module,0.6618334054946899
translation,48,37,model,two regularizers,to enhance,dualgcn model,two regularizers to enhance dualgcn model,0.6464191675186157
translation,48,37,model,model,design,two regularizers,model design two regularizers,0.5914490222930908
translation,48,46,model,dualgcn,integrates,syngcn and semgcn networks,dualgcn integrates syngcn and semgcn networks,0.6455587148666382
translation,48,46,model,syngcn and semgcn networks,through,mutual biaffine module,syngcn and semgcn networks through mutual biaffine module,0.6610059142112732
translation,48,46,model,mutual biaffine module,propose,orthogonal and differential regularizers,mutual biaffine module propose orthogonal and differential regularizers,0.5763040781021118
translation,48,46,model,model,has,dualgcn,model has dualgcn,0.6031571626663208
translation,48,47,model,orthogonal regularizer,encourages,semgcn network,orthogonal regularizer encourages semgcn network,0.6096652746200562
translation,48,47,model,orthogonal regularizer,encourages,semgcn network,orthogonal regularizer encourages semgcn network,0.6096652746200562
translation,48,47,model,semgcn network,to learn,orthogonal semantic attention matrix,semgcn network to learn orthogonal semantic attention matrix,0.621911883354187
translation,48,47,model,semgcn network,to learn,semantic features,semgcn network to learn semantic features,0.6254303455352783
translation,48,47,model,differential regularizer,encourages,semgcn network,differential regularizer encourages semgcn network,0.6064865589141846
translation,48,47,model,semgcn network,to learn,semantic features,semgcn network to learn semantic features,0.6254303455352783
translation,48,47,model,semantic features,distinct from,syntactic ones,semantic features distinct from syntactic ones,0.6123568415641785
translation,48,47,model,syntactic ones,built from,syngcn network,syntactic ones built from syngcn network,0.6937333941459656
translation,48,47,model,model,has,orthogonal regularizer,model has orthogonal regularizer,0.5549805164337158
translation,48,71,model,gcn based method,combining,syntactic and semantic features,gcn based method combining syntactic and semantic features,0.7042787671089172
translation,48,71,model,model,propose,gcn based method,model propose gcn based method,0.6799889802932739
translation,48,72,model,dependency probability matrix,with,richer syntactic information,dependency probability matrix with richer syntactic information,0.5965169072151184
translation,48,72,model,dependency probability matrix,elaborately design,orthogonal and differential regularizers,dependency probability matrix elaborately design orthogonal and differential regularizers,0.6704121232032776
translation,48,72,model,orthogonal and differential regularizers,to enhance,ability,orthogonal and differential regularizers to enhance ability,0.6340150833129883
translation,48,72,model,ability,to precisely capture,semantic associations,ability to precisely capture semantic associations,0.5912325978279114
translation,48,72,model,model,use,dependency probability matrix,model use dependency probability matrix,0.6356833577156067
translation,48,72,model,model,elaborately design,orthogonal and differential regularizers,model elaborately design orthogonal and differential regularizers,0.7247058749198914
translation,48,73,model,graph convolutional network ( gcn ),has,gcn,graph convolutional network ( gcn ) has gcn,0.5599333643913269
translation,48,73,model,model,has,graph convolutional network ( gcn ),model has graph convolutional network ( gcn ),0.5609598755836487
translation,48,186,model,dualgcn model,utilizes,syntactic knowledge,dualgcn model utilizes syntactic knowledge,0.6046993732452393
translation,48,186,model,dualgcn model,avoid,noises,dualgcn model avoid noises,0.6216912865638733
translation,48,186,model,syntactic knowledge,to establish,dependencies,syntactic knowledge to establish dependencies,0.6356699466705322
translation,48,186,model,dependencies,between,words,dependencies between words,0.6616805791854858
translation,48,186,model,noises,introduced by,attention mechanism,noises introduced by attention mechanism,0.676682710647583
translation,48,186,model,atae,has,dualgcn model,atae has dualgcn model,0.5662005543708801
translation,48,183,results,consistently outperforms,on,"restaurant , laptop and twitter datasets","consistently outperforms on restaurant , laptop and twitter datasets",0.5258811116218567
translation,48,183,results,all attention - based and syntax - based methods,on,"restaurant , laptop and twitter datasets","all attention - based and syntax - based methods on restaurant , laptop and twitter datasets",0.4689352810382843
translation,48,183,results,our dualgcn model,has,consistently outperforms,our dualgcn model has consistently outperforms,0.5951640605926514
translation,48,183,results,consistently outperforms,has,all attention - based and syntax - based methods,consistently outperforms has all attention - based and syntax - based methods,0.5853135585784912
translation,48,185,results,dualgcn,accurately fits,datasets,dualgcn accurately fits datasets,0.7130246162414551
translation,48,185,results,datasets,contain,"formal , informal or complicated reviews","datasets contain formal , informal or complicated reviews",0.5714069604873657
translation,48,185,results,results,has,dualgcn,results has dualgcn,0.5791388750076294
translation,48,188,results,informal or complicated sentences,using,syntactic knowledge,informal or complicated sentences using syntactic knowledge,0.6319848895072937
translation,48,188,results,syntactic knowledge,results in,poor performance,syntactic knowledge results in poor performance,0.6073233485221863
translation,48,188,results,results,when considering,informal or complicated sentences,results when considering informal or complicated sentences,0.7092292904853821
translation,48,189,results,most of the models,based on,static word embedding,most of the models based on static word embedding,0.6068204045295715
translation,48,189,results,basic bert,has,outperforms,basic bert has outperforms,0.6700303554534912
translation,48,189,results,outperforms,has,most of the models,outperforms has most of the models,0.6008950471878052
translation,48,190,results,our dualgcn + bert,achieves,better performance,our dualgcn + bert achieves better performance,0.6936873197555542
translation,48,190,results,bert,has,our dualgcn + bert,bert has our dualgcn + bert,0.6650874018669128
translation,48,190,results,results,based on,bert,results based on bert,0.5299289226531982
translation,48,217,results,our model,with,two dualgcn layers,our model with two dualgcn layers,0.603084921836853
translation,48,217,results,two dualgcn layers,performs,best,two dualgcn layers performs best,0.6424705982208252
translation,49,164,ablation-analysis,mmai module ( w/ o mmai ) and multi-gnn module ( w/ o mgnn ),adversely affect,model results,mmai module ( w/ o mmai ) and multi-gnn module ( w/ o mgnn ) adversely affect model results,0.7118573784828186
translation,49,164,ablation-analysis,ablation analysis,removal of,mmai module ( w/ o mmai ) and multi-gnn module ( w/ o mgnn ),ablation analysis removal of mmai module ( w/ o mmai ) and multi-gnn module ( w/ o mgnn ),0.678043007850647
translation,49,165,ablation-analysis,mmai module,with,coatt,mmai module with coatt,0.6849707961082458
translation,49,165,ablation-analysis,model performance,found to be,slightly worse,model performance found to be slightly worse,0.5996069312095642
translation,49,165,ablation-analysis,slightly worse,than,mgnns module,slightly worse than mgnns module,0.5793083906173706
translation,49,165,ablation-analysis,mmai module,has,model performance,mmai module has model performance,0.5289040803909302
translation,49,165,ablation-analysis,coatt,has,model performance,coatt has model performance,0.5535416007041931
translation,49,165,ablation-analysis,ablation analysis,replacing,mmai module,ablation analysis replacing mmai module,0.6759082078933716
translation,49,167,ablation-analysis,one of the object views ( w/ o object ) or scene views ( w/ o scene ),is,removed,one of the object views ( w/ o object ) or scene views ( w/ o scene ) is removed,0.5765247344970703
translation,49,167,ablation-analysis,performance,of,model,performance of model,0.6080846190452576
translation,49,167,ablation-analysis,removed,has,performance,removed has performance,0.6348581910133362
translation,49,167,ablation-analysis,model,has,declines,model has declines,0.6094205379486084
translation,49,167,ablation-analysis,ablation analysis,When,one of the object views ( w/ o object ) or scene views ( w/ o scene ),ablation analysis When one of the object views ( w/ o object ) or scene views ( w/ o scene ),0.6619083285331726
translation,49,183,ablation-analysis,dataset gap,is,relatively wide,dataset gap is relatively wide,0.5583393573760986
translation,49,183,ablation-analysis,transferability,of,text graphs,transferability of text graphs,0.5481693744659424
translation,49,183,ablation-analysis,transferability,is,worse,transferability is worse,0.6276441216468811
translation,49,183,ablation-analysis,text graphs,is,worse,text graphs is worse,0.6412108540534973
translation,49,183,ablation-analysis,dataset gap,has,transferability,dataset gap has transferability,0.5617534518241882
translation,49,183,ablation-analysis,ablation analysis,When,dataset gap,ablation analysis When dataset gap,0.6719913482666016
translation,49,144,baselines,"cnn ( kim , 2014 ) and bi-lstm ( zhou et al. , 2016 )",are,well -known models,"cnn ( kim , 2014 ) and bi-lstm ( zhou et al. , 2016 ) are well -known models",0.4886319637298584
translation,49,144,baselines,well -known models,for,text classification tasks,well -known models for text classification tasks,0.4826231002807617
translation,49,144,baselines,"biacnn ( lai et al. , 2015 )",incorporates,cnn and bilstm models,"biacnn ( lai et al. , 2015 ) incorporates cnn and bilstm models",0.6462265849113464
translation,49,144,baselines,cnn and bilstm models,with,attention mechanism,cnn and bilstm models with attention mechanism,0.5673370957374573
translation,49,144,baselines,attention mechanism,for,text sentiment analysis,attention mechanism for text sentiment analysis,0.5622244477272034
translation,49,144,baselines,text modality,has,"cnn ( kim , 2014 ) and bi-lstm ( zhou et al. , 2016 )","text modality has cnn ( kim , 2014 ) and bi-lstm ( zhou et al. , 2016 )",0.5031998157501221
translation,49,148,baselines,"mvan ( yang et al. , 2020 )",is,multi-view attentional network,"mvan ( yang et al. , 2020 ) is multi-view attentional network",0.5172069668769836
translation,49,148,baselines,multi-view attentional network,utilizes,memory network,multi-view attentional network utilizes memory network,0.5185374617576599
translation,49,148,baselines,memory network,for,multimodal emotion analysis,memory network for multimodal emotion analysis,0.5516530275344849
translation,49,148,baselines,baselines,has,"mvan ( yang et al. , 2020 )","baselines has mvan ( yang et al. , 2020 )",0.5320668816566467
translation,49,163,baselines,performance,of,multi-gnn module,performance of multi-gnn module,0.5649635195732117
translation,49,163,baselines,multi-gnn module,replace,text - gnn,multi-gnn module replace text - gnn,0.6238172650337219
translation,49,163,baselines,multi-gnn module,replace,image - gcn,multi-gnn module replace image - gcn,0.625378429889679
translation,49,163,baselines,text - gnn,with,cnn,text - gnn with cnn,0.6475139260292053
translation,49,163,baselines,image - gcn,with,pretrained resnet,image - gcn with pretrained resnet,0.6314272284507751
translation,49,225,baselines,duig,is,image graph convolutional neural network,duig is image graph convolutional neural network,0.5095068216323853
translation,49,225,baselines,image graph convolutional neural network,with,dual views,image graph convolutional neural network with dual views,0.6142120957374573
translation,49,225,baselines,dual views,e.g.,scene,dual views e.g. scene,0.6832217574119568
translation,49,225,baselines,baselines,has,duig,baselines has duig,0.6099401116371155
translation,49,226,baselines,muiltimodal baselines,is,hierarchical semantic attentional network,muiltimodal baselines is hierarchical semantic attentional network,0.5514606833457947
translation,49,226,baselines,"hsan ( xu , 2017 )",is,hierarchical semantic attentional network,"hsan ( xu , 2017 ) is hierarchical semantic attentional network",0.5349317789077759
translation,49,226,baselines,hierarchical semantic attentional network,based on,image captions,hierarchical semantic attentional network based on image captions,0.6026989817619324
translation,49,226,baselines,image captions,for,multimodal sentiment analysis,image captions for multimodal sentiment analysis,0.5676737427711487
translation,49,226,baselines,baselines,has,muiltimodal baselines,baselines has muiltimodal baselines,0.5941405892372131
translation,49,136,experimental-setup,experimental setup,adopt,cross-entropy loss function,experimental setup adopt cross-entropy loss function,0.602081835269928
translation,49,20,experiments,multimodal sentiment detection,for,image - text pairs in social media posts,multimodal sentiment detection for image - text pairs in social media posts,0.5283255577087402
translation,49,146,experiments,osda,),deep semantic network,osda ) deep semantic network,0.5464111566543579
translation,49,146,experiments,osda,is,deep semantic network,osda is deep semantic network,0.4819057583808899
translation,49,146,experiments,deep semantic network,with,attention,deep semantic network with attention,0.5787767767906189
translation,49,146,experiments,attention,for,multimodal sentiment analysis,attention for multimodal sentiment analysis,0.5411992073059082
translation,49,146,experiments,image modality,has,osda,image modality has osda,0.5258373618125916
translation,49,7,model,sentiment - awareness ( mgnns ),for,imagetext sentiment detection,sentiment - awareness ( mgnns ) for imagetext sentiment detection,0.6031159162521362
translation,49,8,model,different modalities,to capture,hidden representations,different modalities to capture hidden representations,0.652916669845581
translation,49,8,model,model,encode,different modalities,model encode different modalities,0.7811754941940308
translation,49,9,model,multichannel graph neural networks,to learn,multimodal representations,multichannel graph neural networks to learn multimodal representations,0.6001760363578796
translation,49,9,model,multimodal representations,based on,global characteristics,multimodal representations based on global characteristics,0.6090373992919922
translation,49,9,model,global characteristics,of,dataset,global characteristics of dataset,0.5999755859375
translation,49,9,model,model,introduce,multichannel graph neural networks,model introduce multichannel graph neural networks,0.6306357979774475
translation,49,38,model,two graphs,for,scenes and objects,two graphs for scenes and objects,0.6218830943107605
translation,49,38,model,co-occurrences,in,datasets,co-occurrences in datasets,0.48483604192733765
translation,49,38,model,graph convolutional network ( gcn ) models,over,two graphs,graph convolutional network ( gcn ) models over two graphs,0.6394574642181396
translation,49,38,model,two graphs,to represent,images,two graphs to represent images,0.7013307809829712
translation,49,38,model,model,explicitly build,two graphs,model explicitly build two graphs,0.7019568681716919
translation,49,38,model,model,propose,graph convolutional network ( gcn ) models,model propose graph convolutional network ( gcn ) models,0.6193767189979553
translation,49,39,model,isolated feature problem,build,multiple graphs,isolated feature problem build multiple graphs,0.6443682909011841
translation,49,39,model,multiple graphs,for,different modalities,multiple graphs for different modalities,0.6070390343666077
translation,49,39,model,multiple graphs,propose,multi-channel graph neural networks ( multi - gnn ) module,multiple graphs propose multi-channel graph neural networks ( multi - gnn ) module,0.6483498215675354
translation,49,39,model,multi-channel graph neural networks ( multi - gnn ) module,to capture,depth global characteristics,multi-channel graph neural networks ( multi - gnn ) module to capture depth global characteristics,0.6779288053512573
translation,49,39,model,depth global characteristics,of,data,depth global characteristics of data,0.592219889163971
translation,49,39,model,model,to tackle,isolated feature problem,model to tackle isolated feature problem,0.7188785672187805
translation,49,147,model,"- mem ( xu et al. , 2018 )",is,co-memory network,"- mem ( xu et al. , 2018 ) is co-memory network",0.5377804040908813
translation,49,147,model,co-memory network,for iteratively modeling,interactions,co-memory network for iteratively modeling interactions,0.7139174342155457
translation,49,147,model,interactions,between,multiple modalities,interactions between multiple modalities,0.6793796420097351
translation,49,147,model,co,has,"- mem ( xu et al. , 2018 )","co has - mem ( xu et al. , 2018 )",0.546441376209259
translation,49,147,model,model,has,co,model has co,0.6222068667411804
translation,49,147,model,model,has,"- mem ( xu et al. , 2018 )","model has - mem ( xu et al. , 2018 )",0.5444806218147278
translation,49,200,model,novel model,built based on,global characteristics,novel model built based on global characteristics,0.719876229763031
translation,49,200,model,global characteristics,of,dataset,global characteristics of dataset,0.5999755859375
translation,49,200,model,dataset,for,multimodal sentiment detection tasks,dataset for multimodal sentiment detection tasks,0.5164137482643127
translation,49,200,model,novel model,has,mgnns,novel model has mgnns,0.5802369713783264
translation,49,200,model,model,proposes,novel model,model proposes novel model,0.7164400219917297
translation,49,149,results,state - of - the - art performance,on,image -text multimodal sentiment classification,state - of - the - art performance on image -text multimodal sentiment classification,0.5103690028190613
translation,49,154,results,our model ( mgnns ),is,competitive,our model ( mgnns ) is competitive,0.6114121675491333
translation,49,154,results,competitive,with,other strong baseline models,competitive with other strong baseline models,0.6473057866096497
translation,49,154,results,other strong baseline models,on,three datasets,other strong baseline models on three datasets,0.47851529717445374
translation,49,154,results,results,has,our model ( mgnns ),results has our model ( mgnns ),0.5470564365386963
translation,49,157,results,perform better,than,most of the unimodal sentiment analysis models,perform better than most of the unimodal sentiment analysis models,0.5468902587890625
translation,49,157,results,most of the unimodal sentiment analysis models,on,all three datasets,most of the unimodal sentiment analysis models on all three datasets,0.49894627928733826
translation,49,157,results,multimodal sentiment analysis models,has,perform better,multimodal sentiment analysis models has perform better,0.5852311253547668
translation,49,157,results,results,has,multimodal sentiment analysis models,results has multimodal sentiment analysis models,0.5488684773445129
translation,49,158,results,segmental indictors,are,difficult to capture,segmental indictors are difficult to capture,0.5476892590522766
translation,49,158,results,difficult to capture,for,images,difficult to capture for images,0.622764527797699
translation,49,158,results,images,owing to,low information density,images owing to low information density,0.663398265838623
translation,49,158,results,sentiment analysis,on,image modality,sentiment analysis on image modality,0.5436317920684814
translation,49,158,results,sentiment analysis,achieves,worst results,sentiment analysis achieves worst results,0.669472873210907
translation,49,158,results,image modality,achieves,worst results,image modality achieves worst results,0.6402111649513245
translation,49,158,results,results,has,segmental indictors,results has segmental indictors,0.4973827302455902
translation,49,159,results,tgnn unimodal model,has,outperforms,tgnn unimodal model has outperforms,0.6175086498260498
translation,49,159,results,outperforms,has,hsan multimodal model,outperforms has hsan multimodal model,0.5928154587745667
translation,49,159,results,results,has,tgnn unimodal model,results has tgnn unimodal model,0.5553473830223083
translation,49,162,results,whole mgnns model,achieves,best performance,whole mgnns model achieves best performance,0.6802752017974854
translation,49,162,results,best performance,among,all models,best performance among all models,0.607068657875061
translation,49,162,results,results,shows,whole mgnns model,results shows whole mgnns model,0.6397005915641785
translation,49,172,results,graph,constructed based on,own dataset,graph constructed based on own dataset,0.7255937457084656
translation,49,172,results,graph,transferred from,other datasets,graph transferred from other datasets,0.695136308670044
translation,49,172,results,experimental results,calculated based on,graphs,experimental results calculated based on graphs,0.7006380558013916
translation,49,172,results,graphs,transferred from,other datasets,graphs transferred from other datasets,0.6744182705879211
translation,49,172,results,other datasets,are,worse,other datasets are worse,0.5814388990402222
translation,49,172,results,graph,has,experimental results,graph has experimental results,0.5666597485542297
translation,50,138,ablation-analysis,weights,to,"longformer ( beltagy et al. , 2020 )","weights to longformer ( beltagy et al. , 2020 )",0.5788964629173279
translation,50,138,ablation-analysis,breadth first reasoning graph,has,performance,breadth first reasoning graph has performance,0.5624347925186157
translation,50,138,ablation-analysis,"longformer ( beltagy et al. , 2020 )",has,performance,"longformer ( beltagy et al. , 2020 ) has performance",0.538453221321106
translation,50,138,ablation-analysis,ablation analysis,add,breadth first reasoning graph,ablation analysis add breadth first reasoning graph,0.5707757472991943
translation,50,146,ablation-analysis,joint f1,drops at,about 6 %,joint f1 drops at about 6 %,0.7381503582000732
translation,50,146,ablation-analysis,about 6 %,causes,over-smoothing,about 6 % causes over-smoothing,0.6451454758644104
translation,50,146,ablation-analysis,4 layers,has,joint f1,4 layers has joint f1,0.592254638671875
translation,50,146,ablation-analysis,ablation analysis,replace,4 layers,ablation analysis replace 4 layers,0.6324459314346313
translation,50,125,experiments,our model,shows,close performances,our model shows close performances,0.6576852798461914
translation,50,125,experiments,close performances,to,hgn - large,close performances to hgn - large,0.5885443091392517
translation,50,125,experiments,supporting sentence prediction and joint prediction,has,our model,supporting sentence prediction and joint prediction has our model,0.5874292254447937
translation,50,6,model,novel model,of,breadth first reasoning graph ( bfr - graph ),novel model of breadth first reasoning graph ( bfr - graph ),0.5574309229850769
translation,50,6,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,50,7,model,reasoning message,start from,question node,reasoning message start from question node,0.6884185671806335
translation,50,7,model,reasoning message,pass to,next sentences node,reasoning message pass to next sentences node,0.668165385723114
translation,50,7,model,bfr - graph,has,reasoning message,bfr - graph has reasoning message,0.6287450194358826
translation,50,7,model,model,In,bfr - graph,model In bfr - graph,0.5587303042411804
translation,50,28,model,novel model,of,breadth first reasoning graph ( bfr - graph ),novel model of breadth first reasoning graph ( bfr - graph ),0.5574309229850769
translation,50,28,model,novel model,to effectively adapt,gnn,novel model to effectively adapt gnn,0.697044849395752
translation,50,28,model,gnn,to,multihop qa,gnn to multihop qa,0.6007721424102783
translation,50,28,model,model,proposed,novel model,model proposed novel model,0.7660740613937378
translation,50,29,model,bfr - graph,is,weighted graph,bfr - graph is weighted graph,0.5554757714271545
translation,50,29,model,weighted graph,in which,weight,weighted graph in which weight,0.6552664637565613
translation,50,29,model,weight,of,edge,weight of edge,0.5836401581764221
translation,50,29,model,"other relational information ( e.g. , cooccurrence entities and distance )",of,connected sentences,"other relational information ( e.g. , cooccurrence entities and distance ) of connected sentences",0.5278699398040771
translation,50,29,model,model,proposed,bfr - graph,model proposed bfr - graph,0.7258521318435669
translation,50,53,model,novel model,of,breadth first reasoning graph ( bfr - graph ),novel model of breadth first reasoning graph ( bfr - graph ),0.5574309229850769
translation,50,53,model,breadth first reasoning graph ( bfr - graph ),for,multi-hop qa,breadth first reasoning graph ( bfr - graph ) for multi-hop qa,0.6258614659309387
translation,50,53,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,50,54,model,new restrictions,on,message passing,new restrictions on message passing,0.5220213532447815
translation,50,54,model,message,starts from,question,message starts from question,0.6975935101509094
translation,50,54,model,message,passes to,latter sentence nodes,message passes to latter sentence nodes,0.7331551313400269
translation,50,54,model,message passing,has,message,message passing has message,0.6099917888641357
translation,50,54,model,latter sentence nodes,has,hop by hop,latter sentence nodes has hop by hop,0.5649093985557556
translation,50,55,model,weighted graph,considering,co-occurrence entities and distance,weighted graph considering co-occurrence entities and distance,0.6450052857398987
translation,50,55,model,co-occurrence entities and distance,between,sentences,co-occurrence entities and distance between sentences,0.6026660203933716
translation,50,124,results,our model,improves,0.84 % ans-em ( exact match ),our model improves 0.84 % ans-em ( exact match ),0.6769370436668396
translation,50,124,results,0.84 % ans-em ( exact match ),than,hgn - large,0.84 % ans-em ( exact match ) than hgn - large,0.5751611590385437
translation,50,124,results,results,has,our model,results has our model,0.5871725678443909
translation,51,20,ablation-analysis,first-stage retriever,significantly affects,final qa performance,first-stage retriever significantly affects final qa performance,0.7552026510238647
translation,51,20,ablation-analysis,recall component,has,first-stage retriever,recall component has first-stage retriever,0.5775500535964966
translation,51,20,ablation-analysis,ablation analysis,As,recall component,ablation analysis As recall component,0.5355434417724609
translation,51,244,ablation-analysis,performance,of,retriever,performance of retriever,0.659676194190979
translation,51,244,ablation-analysis,significantly decreases,by introducing,hard negatives,significantly decreases by introducing hard negatives,0.6536787152290344
translation,51,244,ablation-analysis,retriever,has,significantly decreases,retriever has significantly decreases,0.6421340107917786
translation,51,244,ablation-analysis,hard negatives,has,without denoising,hard negatives has without denoising,0.6205457448959351
translation,51,254,ablation-analysis,data augmentation strategy,has,performance,data augmentation strategy has performance,0.5646336078643799
translation,51,254,ablation-analysis,ablation analysis,integrated with,data augmentation strategy,ablation analysis integrated with data augmentation strategy,0.7425723075866699
translation,51,220,baselines,sparse retrievers,include,traditional retriever bm25,sparse retrievers include traditional retriever bm25,0.5564054846763611
translation,51,220,baselines,sparse retrievers,include,four traditional retrievers,sparse retrievers include four traditional retrievers,0.5858803391456604
translation,51,220,baselines,four traditional retrievers,enhanced by,neural networks,four traditional retrievers enhanced by neural networks,0.6850356459617615
translation,51,220,baselines,baselines,has,sparse retrievers,baselines has sparse retrievers,0.5839353799819946
translation,51,225,baselines,drp and me - bert,use,in - batch random sampling,drp and me - bert use in - batch random sampling,0.6142252087593079
translation,51,225,baselines,drp and me - bert,use,hard negative sampling,drp and me - bert use hard negative sampling,0.6549580097198486
translation,51,225,baselines,hard negative sampling,from,results,hard negative sampling from results,0.5928301811218262
translation,51,225,baselines,hard negative sampling,by using,dense retriever,hard negative sampling by using dense retriever,0.6502951383590698
translation,51,225,baselines,results,retrieved by,bm25,results retrieved by bm25,0.509337306022644
translation,51,225,baselines,ance,enhances,hard negative sampling,ance enhances hard negative sampling,0.6979737877845764
translation,51,225,baselines,hard negative sampling,by using,dense retriever,hard negative sampling by using dense retriever,0.6502951383590698
translation,51,225,baselines,baselines,has,drp and me - bert,baselines has drp and me - bert,0.6274124979972839
translation,51,169,experiments,experiments,with,deep learning framework paddlepaddle,experiments with deep learning framework paddlepaddle,0.6085469126701355
translation,51,169,experiments,deep learning framework paddlepaddle,on,up to eight nvidia tesla v100 gpus,deep learning framework paddlepaddle on up to eight nvidia tesla v100 gpus,0.49237948656082153
translation,51,169,experiments,up to eight nvidia tesla v100 gpus,with,32g ram,up to eight nvidia tesla v100 gpus with 32g ram,0.6059490442276001
translation,51,171,hyperparameters,dual-encoder,initialized with,parameters of ernie 2.0 base,dual-encoder initialized with parameters of ernie 2.0 base,0.7386052012443542
translation,51,171,hyperparameters,cross-encoder,initialized with,ernie 2.0 large,cross-encoder initialized with ernie 2.0 large,0.7743119597434998
translation,51,171,hyperparameters,hyperparameters,has,dual-encoder,hyperparameters has dual-encoder,0.5646492838859558
translation,51,171,hyperparameters,hyperparameters,has,cross-encoder,hyperparameters has cross-encoder,0.5363478660583496
translation,51,176,hyperparameters,cross- batch negative sampling,implemented with,differentiable all - gather operation,cross- batch negative sampling implemented with differentiable all - gather operation,0.7243303656578064
translation,51,176,hyperparameters,differentiable all - gather operation,provided in,"fleetx ( dong , 2020 )","differentiable all - gather operation provided in fleetx ( dong , 2020 )",0.683784008026123
translation,51,176,hyperparameters,differentiable all - gather operation,is,highly scalable distributed training engine,differentiable all - gather operation is highly scalable distributed training engine,0.560949981212616
translation,51,176,hyperparameters,highly scalable distributed training engine,of,paddlepaddle,highly scalable distributed training engine of paddlepaddle,0.568953812122345
translation,51,176,hyperparameters,hyperparameters,has,cross- batch negative sampling,hyperparameters has cross- batch negative sampling,0.5260655879974365
translation,51,192,hyperparameters,dual-encoders,trained with,batch sizes,dual-encoders trained with batch sizes,0.7512705326080322
translation,51,192,hyperparameters,batch sizes,of,512 ? 8 and 512 ? 2,batch sizes of 512 ? 8 and 512 ? 2,0.6314955353736877
translation,51,192,hyperparameters,512 ? 8 and 512 ? 2,on,ms - marco and nq,512 ? 8 and 512 ? 2 on ms - marco and nq,0.6157416701316833
translation,51,192,hyperparameters,hyperparameters,has,dual-encoders,hyperparameters has dual-encoders,0.5567411184310913
translation,51,194,hyperparameters,cross-encoders,trained with,batch sizes,cross-encoders trained with batch sizes,0.7498326897621155
translation,51,194,hyperparameters,batch sizes,of,64 ? 4 and 64,batch sizes of 64 ? 4 and 64,0.6367188096046448
translation,51,194,hyperparameters,64 ? 4 and 64,on,msmarco and nq,64 ? 4 and 64 on msmarco and nq,0.6395875215530396
translation,51,194,hyperparameters,hyperparameters,has,cross-encoders,hyperparameters has cross-encoders,0.5313653349876404
translation,51,195,hyperparameters,automatic mixed precision and gradient checkpoint 4 functionality,in,fleetx,automatic mixed precision and gradient checkpoint 4 functionality in fleetx,0.5475448369979858
translation,51,195,hyperparameters,models,using,large batch sizes,models using large batch sizes,0.6531232595443726
translation,51,195,hyperparameters,large batch sizes,with,limited resources,large batch sizes with limited resources,0.6232112050056458
translation,51,195,hyperparameters,hyperparameters,use,automatic mixed precision and gradient checkpoint 4 functionality,hyperparameters use automatic mixed precision and gradient checkpoint 4 functionality,0.6257232427597046
translation,51,197,hyperparameters,dual-encoders,trained on,msmarco,dual-encoders trained on msmarco,0.7685521841049194
translation,51,197,hyperparameters,msmarco,for,"40 , 10 and 10 epochs","msmarco for 40 , 10 and 10 epochs",0.6107109189033508
translation,51,197,hyperparameters,"40 , 10 and 10 epochs",in,three steps,"40 , 10 and 10 epochs in three steps",0.5752086639404297
translation,51,197,hyperparameters,three steps,of,rocketqa,three steps of rocketqa,0.6202911734580994
translation,51,197,hyperparameters,hyperparameters,has,dual-encoders,hyperparameters has dual-encoders,0.5567411184310913
translation,51,198,hyperparameters,dualencoders,trained on,nq,dualencoders trained on nq,0.7337130904197693
translation,51,198,hyperparameters,nq,for,30 epochs,nq for 30 epochs,0.6631354093551636
translation,51,198,hyperparameters,nq,in,all steps,nq in all steps,0.5470165014266968
translation,51,198,hyperparameters,30 epochs,in,all steps,30 epochs in all steps,0.5435925722122192
translation,51,198,hyperparameters,all steps,of,rocketqa,all steps of rocketqa,0.6031638979911804
translation,51,198,hyperparameters,hyperparameters,has,dualencoders,hyperparameters has dualencoders,0.5270931720733643
translation,51,199,hyperparameters,cross-encoders,trained for,2 epochs,cross-encoders trained for 2 epochs,0.774083137512207
translation,51,199,hyperparameters,2 epochs,on,msmarco and nq,2 epochs on msmarco and nq,0.5667834281921387
translation,51,199,hyperparameters,hyperparameters,has,cross-encoders,hyperparameters has cross-encoders,0.5313653349876404
translation,51,203,hyperparameters,learning rate,of,dual-encoder,learning rate of dual-encoder,0.5652797222137451
translation,51,203,hyperparameters,learning rate,of,cross-encoder,learning rate of cross-encoder,0.5587448477745056
translation,51,203,hyperparameters,learning rate,rate of,cross-encoder,learning rate rate of cross-encoder,0.6839441657066345
translation,51,203,hyperparameters,learning rate,of,cross-encoder,learning rate of cross-encoder,0.5587448477745056
translation,51,203,hyperparameters,dual-encoder,set to,3e - 5,dual-encoder set to 3e - 5,0.715507447719574
translation,51,203,hyperparameters,linear scheduling warm - up,set to,0.1,linear scheduling warm - up set to 0.1,0.6685783863067627
translation,51,203,hyperparameters,learning rate,of,cross-encoder,learning rate of cross-encoder,0.5587448477745056
translation,51,203,hyperparameters,cross-encoder,set to,1e - 5,cross-encoder set to 1e - 5,0.7113935947418213
translation,51,203,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,51,203,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,51,205,hyperparameters,maximal length,of,questions and passages,maximal length of questions and passages,0.5851740837097168
translation,51,205,hyperparameters,maximal length,as,32 and 128,maximal length as 32 and 128,0.5520910024642944
translation,51,205,hyperparameters,questions and passages,as,32 and 128,questions and passages as 32 and 128,0.573596179485321
translation,51,205,hyperparameters,hyperparameters,set,maximal length,hyperparameters set maximal length,0.6253907680511475
translation,51,7,model,optimized training approach,called,rocketqa,optimized training approach called rocketqa,0.7061154842376709
translation,51,7,model,optimized training approach,to improving,dense passage retrieval,optimized training approach to improving dense passage retrieval,0.6275440454483032
translation,51,7,model,model,propose,optimized training approach,model propose optimized training approach,0.6745197772979736
translation,51,127,model,unlabeled questions,for,data augmentation,unlabeled questions for data augmentation,0.5522908568382263
translation,51,269,model,novelty,mainly lies in,passage retrieval component,novelty mainly lies in passage retrieval component,0.6807722449302673
translation,51,269,model,passage retrieval component,i.e.,rocketqa approach,passage retrieval component i.e. rocketqa approach,0.6423177719116211
translation,51,269,model,prior solutions,has,novelty,prior solutions has novelty,0.5565969944000244
translation,51,269,model,model,Compared with,prior solutions,model Compared with prior solutions,0.6951514482498169
translation,51,272,model,optimized training approach,to improving,dense passage retrieval,optimized training approach to improving dense passage retrieval,0.6275440454483032
translation,51,272,model,model,presented,optimized training approach,model presented optimized training approach,0.6976490616798401
translation,51,227,results,all the baselines,on,msmarco and nq datasets,all the baselines on msmarco and nq datasets,0.5051813721656799
translation,51,227,results,rocketqa,has,significantly outperforms,rocketqa has significantly outperforms,0.6262931823730469
translation,51,227,results,significantly outperforms,has,all the baselines,significantly outperforms has all the baselines,0.5936449766159058
translation,51,227,results,results,see that,rocketqa,results see that rocketqa,0.6309000849723816
translation,51,228,results,dense retrievers,are,better,dense retrievers are better,0.5955894589424133
translation,51,228,results,better,than,sparse retrievers,better than sparse retrievers,0.6060550808906555
translation,51,228,results,results,has,observation,results has observation,0.5461027026176453
translation,51,234,results,performance,of,dense retriever,performance of dense retriever,0.5862166881561279
translation,51,234,results,improved,with,more negatives,improved with more negatives,0.685470461845398
translation,51,234,results,more negatives,by,cross- batch negatives,more negatives by cross- batch negatives,0.5329248309135437
translation,51,250,results,denoised negatives,improve,performance,denoised negatives improve performance,0.68172687292099
translation,51,250,results,performance,of,dense retriever,performance of dense retriever,0.5862166881561279
translation,51,268,results,our retriever,leads to,better qa performance,our retriever leads to better qa performance,0.65346759557724
translation,51,268,results,results,see that,our retriever,results see that our retriever,0.6728084683418274
translation,52,159,ablation-analysis,ablations,on,sentiment polarity and aspect category,ablations on sentiment polarity and aspect category,0.5789309740066528
translation,52,159,ablation-analysis,model,suffers,more,model suffers more,0.7654182314872742
translation,52,159,ablation-analysis,more,when,aspect category,more when aspect category,0.6228774189949036
translation,52,159,ablation-analysis,aspect category,projected to,indexed symbol,aspect category projected to indexed symbol,0.6495500802993774
translation,52,159,ablation-analysis,ablations,has,model,ablations has model,0.5778318643569946
translation,52,159,ablation-analysis,sentiment polarity and aspect category,has,model,sentiment polarity and aspect category has model,0.5659159421920776
translation,52,159,ablation-analysis,ablation analysis,Comparing,ablations,ablation analysis Comparing ablations,0.7068380117416382
translation,52,159,ablation-analysis,ablation analysis,on,sentiment polarity and aspect category,ablation analysis on sentiment polarity and aspect category,0.5668892860412598
translation,52,131,baselines,models,in,pipeline manner,models in pipeline manner,0.5719671845436096
translation,52,131,baselines,pipeline manner,for,quad prediction,pipeline manner for quad prediction,0.6514960527420044
translation,52,131,baselines,"hgcn ( cai et al. , 2020 )",for jointly detecting,aspect category and sentiment polarity,"hgcn ( cai et al. , 2020 ) for jointly detecting aspect category and sentiment polarity",0.6916248798370361
translation,52,131,baselines,aspect category and sentiment polarity,followed by,bertbased model,aspect category and sentiment polarity followed by bertbased model,0.6389782428741455
translation,52,131,baselines,bertbased model,extracting,aspect and opinion term,bertbased model extracting aspect and opinion term,0.7021769285202026
translation,52,131,baselines,aspect and opinion term,given,predicted aspect category and sentiment,aspect and opinion term given predicted aspect category and sentiment,0.6832235455513
translation,52,131,baselines,quad prediction,has,"hgcn ( cai et al. , 2020 )","quad prediction has hgcn ( cai et al. , 2020 )",0.5500532984733582
translation,52,131,baselines,baselines,cascade,models,baselines cascade models,0.8000946044921875
translation,52,136,baselines,generation - type baseline gas,to directly treat,sentiment quads sequence,generation - type baseline gas to directly treat sentiment quads sequence,0.6704021096229553
translation,52,136,baselines,sentiment quads sequence,as,target,sentiment quads sequence as target,0.563845694065094
translation,52,136,baselines,target,for learning,generation model,target for learning generation model,0.7264538407325745
translation,52,165,baselines,pipeline - based methods,including,"cmla + ( wang et al. , 2017 )","pipeline - based methods including cmla + ( wang et al. , 2017 )",0.6434886455535889
translation,52,165,baselines,pipeline - based methods,including,li-unified -r,pipeline - based methods including li-unified -r,0.6776021122932434
translation,52,165,baselines,pipeline - based methods,including,"peng-pipeline ( peng et al. , 2020 )","pipeline - based methods including peng-pipeline ( peng et al. , 2020 )",0.6618529558181763
translation,52,165,baselines,pipeline - based methods,firstly extract,aspect and opinion terms,pipeline - based methods firstly extract aspect and opinion terms,0.7260157465934753
translation,52,165,baselines,pipeline - based methods,conduct,pairing,pipeline - based methods conduct pairing,0.7086511254310608
translation,52,165,baselines,"peng-pipeline ( peng et al. , 2020 )",firstly extract,aspect and opinion terms,"peng-pipeline ( peng et al. , 2020 ) firstly extract aspect and opinion terms",0.7227712273597717
translation,52,165,baselines,"peng-pipeline ( peng et al. , 2020 )",conduct,pairing,"peng-pipeline ( peng et al. , 2020 ) conduct pairing",0.6144481897354126
translation,52,165,baselines,two -stage,proposes,two -stage method,two -stage proposes two -stage method,0.6682748794555664
translation,52,165,baselines,two -stage method,to enhance,correlation,two -stage method to enhance correlation,0.7377310395240784
translation,52,165,baselines,correlation,between,aspects and opinions,correlation between aspects and opinions,0.6580012440681458
translation,52,165,baselines,end-to - end models,including,"gts ( wu et al. , 2020 )","end-to - end models including gts ( wu et al. , 2020 )",0.6965441703796387
translation,52,165,baselines,end-to - end models,including,jet,end-to - end models including jet,0.7133195400238037
translation,52,165,baselines,end-to - end models,designing,unified tagging schemes,end-to - end models designing unified tagging schemes,0.6608056426048279
translation,52,165,baselines,jet,designing,unified tagging schemes,jet designing unified tagging schemes,0.575553834438324
translation,52,165,baselines,baselines,adopt,two types of baselines,baselines adopt two types of baselines,0.6648445129394531
translation,52,167,baselines,two bert based models,including,tas - crf,two bert based models including tas - crf,0.6410709619522095
translation,52,167,baselines,two bert based models,including,"tas -to ( wan et al. , 2020 )","two bert based models including tas -to ( wan et al. , 2020 )",0.6370678544044495
translation,52,167,baselines,recent model mejd,utilizes,graph structure,recent model mejd utilizes graph structure,0.5649315714836121
translation,52,167,baselines,graph structure,to model,dependency,graph structure to model dependency,0.7181598544120789
translation,52,167,baselines,dependency,among,sentiment elements,dependency among sentiment elements,0.5981006622314453
translation,52,167,baselines,pipeline - type baseline method,has,two bert based models,pipeline - type baseline method has two bert based models,0.54000324010849
translation,52,124,experimental-setup,"t5 - base ( raffel et al. , 2020 )",as,pretrained generative model,"t5 - base ( raffel et al. , 2020 ) as pretrained generative model",0.47154155373573303
translation,52,124,experimental-setup,pretrained generative model,adopts,classical transformer encoder -decoder network architecture,pretrained generative model adopts classical transformer encoder -decoder network architecture,0.5559453368186951
translation,52,124,experimental-setup,experimental setup,adopt,"t5 - base ( raffel et al. , 2020 )","experimental setup adopt t5 - base ( raffel et al. , 2020 )",0.5997410416603088
translation,52,125,experimental-setup,training,use,batch size,training use batch size,0.657907247543335
translation,52,125,experimental-setup,training,use,learning rate,training use learning rate,0.6668137311935425
translation,52,125,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,52,125,experimental-setup,learning rate,being,3e - 4,learning rate being 3e - 4,0.620473325252533
translation,52,125,experimental-setup,experimental setup,Regarding,training,experimental setup Regarding training,0.5939168930053711
translation,52,126,experimental-setup,training epochs,is,20,training epochs is 20,0.6059380769729614
translation,52,166,experiments,tasd task,adopt,dataset,tasd task adopt dataset,0.6230233311653137
translation,52,166,experiments,dataset,prepared by,wan et al . ( 2020 ),dataset prepared by wan et al . ( 2020 ),0.5543379187583923
translation,52,7,model,novel paraphrase modeling paradigm,to cast,asqp task,novel paraphrase modeling paradigm to cast asqp task,0.5827621817588806
translation,52,7,model,asqp task,to,paraphrase generation process,asqp task to paraphrase generation process,0.5408644676208496
translation,52,7,model,model,propose,novel paraphrase modeling paradigm,model propose novel paraphrase modeling paradigm,0.6414596438407898
translation,52,8,model,generation formulation,allows solving,asqp,generation formulation allows solving asqp,0.7591036558151245
translation,52,8,model,asqp,in,end-to - end manner,asqp in end-to - end manner,0.5673540234565735
translation,52,8,model,asqp,alleviating,potential error propagation,asqp alleviating potential error propagation,0.7385818958282471
translation,52,8,model,model,has,generation formulation,model has generation formulation,0.5056276917457581
translation,52,50,model,paraphrase modeling,better utilize,knowledge,paraphrase modeling better utilize knowledge,0.6799531579017639
translation,52,50,model,knowledge,of,pre-trained model,knowledge of pre-trained model,0.5471216440200806
translation,52,50,model,pre-trained model,via casting,original task,pre-trained model via casting original task,0.7194086313247681
translation,52,50,model,original task,to,paraphrase generation process,original task to paraphrase generation process,0.506069540977478
translation,52,50,model,model,propose,paraphrase modeling,model propose paraphrase modeling,0.6588060259819031
translation,52,127,model,inference,utilize,greedy decoding,inference utilize greedy decoding,0.5734617114067078
translation,52,127,model,greedy decoding,for generating,output sequence,greedy decoding for generating output sequence,0.7009100317955017
translation,52,127,model,model,During,inference,model During inference,0.6954593062400818
translation,52,132,model,latter one,equipped with,linear layer ( bert - linear ),latter one equipped with linear layer ( bert - linear ),0.7161681056022644
translation,52,132,model,latter one,equipped with,transformer block ( bert - tfm ),latter one equipped with transformer block ( bert - tfm ),0.7400059700012207
translation,52,132,model,transformer block ( bert - tfm ),on top,unified model,transformer block ( bert - tfm ) on top unified model,0.6276261210441589
translation,52,132,model,unified model,modify,"tas ( wan et al. , 2020 )","unified model modify tas ( wan et al. , 2020 )",0.6647282242774963
translation,52,132,model,unified model,modify,state - of - the - art unified model,unified model modify state - of - the - art unified model,0.6209748387336731
translation,52,132,model,"tas ( wan et al. , 2020 )",has,state - of - the - art unified model,"tas ( wan et al. , 2020 ) has state - of - the - art unified model",0.5466716885566711
translation,52,132,model,state - of - the - art unified model,has,"to extract ( c , a , p ) triplet","state - of - the - art unified model has to extract ( c , a , p ) triplet",0.5446453094482422
translation,52,132,model,model,modify,"tas ( wan et al. , 2020 )","model modify tas ( wan et al. , 2020 )",0.7010951042175293
translation,52,132,model,model,has,latter one,model has latter one,0.6439481377601624
translation,52,133,model,tas,expands,each original data sample,tas expands each original data sample,0.7304454445838928
translation,52,133,model,tas,to solve,task,tas to solve task,0.676630437374115
translation,52,133,model,each original data sample,into,multiple samples,each original data sample into multiple samples,0.6060016751289368
translation,52,133,model,specific aspect category and sentiment polarity pair,to solve,task,specific aspect category and sentiment polarity pair to solve task,0.6329760551452637
translation,52,133,model,task,in,end-to - end manner,task in end-to - end manner,0.5783459544181824
translation,52,133,model,model,has,tas,model has tas,0.6357436180114746
translation,52,134,model,tagging schema,to predict,aspect and opinion term,tagging schema to predict aspect and opinion term,0.6489995121955872
translation,52,134,model,tagging schema,for constructing,unified model,tagging schema for constructing unified model,0.6427550911903381
translation,52,134,model,aspect and opinion term,for constructing,unified model,aspect and opinion term for constructing unified model,0.6706347465515137
translation,52,134,model,unified model,to predict,quad,unified model to predict quad,0.7724398970603943
translation,52,134,model,quad,denoted as,taso ( tas with opinion ),quad denoted as taso ( tas with opinion ),0.7299804091453552
translation,52,134,model,model,change,tagging schema,model change tagging schema,0.7093689441680908
translation,52,135,model,two variants,in terms of,prediction layer,two variants in terms of prediction layer,0.6764435768127441
translation,52,135,model,two variants,using,linear classification layer ( taso - linear ),two variants using linear classification layer ( taso - linear ),0.6858359575271606
translation,52,135,model,two variants,using,crf layer ( taso - crf ),two variants using crf layer ( taso - crf ),0.6864288449287415
translation,52,148,model,asqp problem,in,end-to - end manner,asqp problem in end-to - end manner,0.5271927118301392
translation,52,148,model,asqp problem,alleviating,possible error propagation,asqp problem alleviating possible error propagation,0.7229214906692505
translation,52,148,model,possible error propagation,in,pipeline solutions,possible error propagation in pipeline solutions,0.5252411961555481
translation,52,202,model,novel para - phrase modeling paradigm,tackles,original quad prediction,novel para - phrase modeling paradigm tackles original quad prediction,0.712007462978363
translation,52,202,model,original quad prediction,as,paraphrase generation problem,original quad prediction as paraphrase generation problem,0.515880286693573
translation,52,202,model,model,propose,novel para - phrase modeling paradigm,model propose novel para - phrase modeling paradigm,0.6805191040039062
translation,52,21,results,new task,compensates for,drawbacks,new task compensates for drawbacks,0.7700057029724121
translation,52,21,results,new task,comprehensively understand,user 's aspect-level opinions,new task comprehensively understand user 's aspect-level opinions,0.5995097756385803
translation,52,21,results,drawbacks,of,previous tasks,drawbacks of previous tasks,0.5363020896911621
translation,52,21,results,results,has,new task,results has new task,0.5799921751022339
translation,52,142,results,performance,of,pipeline methods,performance of pipeline methods,0.6009612679481506
translation,52,142,results,performance,far from,satisfactory,performance far from satisfactory,0.7129362225532532
translation,52,142,results,pipeline methods,far from,satisfactory,pipeline methods far from satisfactory,0.7333899736404419
translation,52,142,results,results,has,performance,results has performance,0.5972660779953003
translation,52,143,results,bert,as,backbone,bert as backbone,0.6227380037307739
translation,52,143,results,"unified methods ( e.g. , taso - bert - linear )",perform,much better,"unified methods ( e.g. , taso - bert - linear ) perform much better",0.5744190812110901
translation,52,143,results,much better,than,"pipeline ones ( e.g. , hgcn - bert + bert - linear )","much better than pipeline ones ( e.g. , hgcn - bert + bert - linear )",0.57045978307724
translation,52,143,results,bert,has,"unified methods ( e.g. , taso - bert - linear )","bert has unified methods ( e.g. , taso - bert - linear )",0.5532359480857849
translation,52,143,results,backbone,has,"unified methods ( e.g. , taso - bert - linear )","backbone has unified methods ( e.g. , taso - bert - linear )",0.5492798686027527
translation,52,143,results,results,adopting,bert,results adopting bert,0.5397054553031921
translation,52,145,results,outperforms,showing,effectiveness,outperforms showing effectiveness,0.7438272833824158
translation,52,145,results,two variants of taso,by,large margin,two variants of taso by large margin,0.576762855052948
translation,52,145,results,two variants of taso,showing,effectiveness,two variants of taso showing effectiveness,0.6436799168586731
translation,52,145,results,effectiveness,of,sequenceto-sequence modeling,effectiveness of sequenceto-sequence modeling,0.6262339353561401
translation,52,145,results,sequenceto-sequence modeling,for,asqp task,sequenceto-sequence modeling for asqp task,0.626503586769104
translation,52,145,results,unified methods,has,gas,unified methods has gas,0.5819418430328369
translation,52,145,results,gas,has,outperforms,gas has outperforms,0.6331851482391357
translation,52,145,results,outperforms,has,two variants of taso,outperforms has two variants of taso,0.6019713282585144
translation,52,145,results,results,among,unified methods,results among unified methods,0.5511689186096191
translation,52,147,results,paraphrase modeling,achieves,best performance,paraphrase modeling achieves best performance,0.6719805002212524
translation,52,147,results,best performance,on,all metrics,best performance on all metrics,0.5029283165931702
translation,52,147,results,all metrics,across,two datasets,all metrics across two datasets,0.7190589904785156
translation,52,147,results,our proposed method,has,paraphrase modeling,our proposed method has paraphrase modeling,0.5636901259422302
translation,52,147,results,results,see that,our proposed method,results see that our proposed method,0.6374303698539734
translation,52,149,results,gas method,using,same pre-trained model,gas method using same pre-trained model,0.7152330279350281
translation,52,149,results,our paraphrase,achieves,superior results,our paraphrase achieves superior results,0.6979058980941772
translation,52,149,results,gas method,has,our paraphrase,gas method has our paraphrase,0.6315910220146179
translation,52,149,results,same pre-trained model,has,our paraphrase,same pre-trained model has our paraphrase,0.5995162725448608
translation,52,149,results,results,compared with,gas method,results compared with gas method,0.7018167972564697
translation,52,162,results,results,on,aste and tasd tasks,results on aste and tasd tasks,0.4868595004081726
translation,52,163,results,proposed para - phrase modeling,provides,unified framework,proposed para - phrase modeling provides unified framework,0.5809506177902222
translation,52,163,results,unified framework,to tackle,absa problem,unified framework to tackle absa problem,0.6912480592727661
translation,52,163,results,unified framework,test it on,aste and tasd tasks,unified framework test it on aste and tasd tasks,0.6966440081596375
translation,52,163,results,absa problem,test it on,aste and tasd tasks,absa problem test it on aste and tasd tasks,0.7181704640388489
translation,52,171,results,previous state - of - the - art models,across,all datasets,previous state - of - the - art models across all datasets,0.6695926785469055
translation,52,171,results,all datasets,in,two tasks,all datasets in two tasks,0.4903246760368347
translation,52,171,results,proposed para - phrase method,has,consistently outperforms,proposed para - phrase method has consistently outperforms,0.6155933737754822
translation,52,171,results,consistently outperforms,has,previous state - of - the - art models,consistently outperforms has previous state - of - the - art models,0.5564045310020447
translation,52,171,results,results,observe,proposed para - phrase method,results observe proposed para - phrase method,0.6014822721481323
translation,53,135,ablation-analysis,qa context and kg,cannot mutually update,representations,qa context and kg cannot mutually update representations,0.7961955070495605
translation,53,135,ablation-analysis,hurting,has,performance,hurting has performance,0.5870582461357117
translation,53,135,ablation-analysis,performance,has,76.5 % ?74.8 %,performance has 76.5 % ?74.8 %,0.5787467360496521
translation,53,118,baselines,mhgrn,is,existing top performance model,mhgrn is existing top performance model,0.5821434855461121
translation,53,118,baselines,existing top performance model,under,lm + kg framework,existing top performance model under lm + kg framework,0.6581199765205383
translation,53,118,baselines,baselines,has,mhgrn,baselines has mhgrn,0.6027963161468506
translation,53,105,experimental-setup,dimension ( d = 200 ) and number of layers ( l = 5 ),of,gnn module,dimension ( d = 200 ) and number of layers ( l = 5 ) of gnn module,0.5909621715545654
translation,53,105,experimental-setup,dimension ( d = 200 ) and number of layers ( l = 5 ),with,dropout rate,dimension ( d = 200 ) and number of layers ( l = 5 ) with dropout rate,0.6230986714363098
translation,53,105,experimental-setup,0.2,applied to,each layer,0.2 applied to each layer,0.658157229423523
translation,53,105,experimental-setup,dropout rate,has,0.2,dropout rate has 0.2,0.508906900882721
translation,53,105,experimental-setup,experimental setup,set,dimension ( d = 200 ) and number of layers ( l = 5 ),experimental setup set dimension ( d = 200 ) and number of layers ( l = 5 ),0.6276434659957886
translation,53,106,experimental-setup,parameters,optimized by,radam,parameters optimized by radam,0.7679793834686279
translation,53,106,experimental-setup,radam,with,batch size,radam with batch size,0.6683125495910645
translation,53,106,experimental-setup,radam,with,gradient clipping 1.0,radam with gradient clipping 1.0,0.6206299066543579
translation,53,106,experimental-setup,1e - 5 and 1e - 3,for,lm and gnn components,1e - 5 and 1e - 3 for lm and gnn components,0.6725971102714539
translation,53,106,experimental-setup,batch size,has,128,batch size has 128,0.6398258209228516
translation,53,106,experimental-setup,learning rate,has,1e - 5 and 1e - 3,learning rate has 1e - 5 and 1e - 3,0.573226809501648
translation,53,106,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,53,107,experimental-setup,two gpus ( gtx titan x ),takes,?20 hours,two gpus ( gtx titan x ) takes ?20 hours,0.6299989819526672
translation,53,5,model,relevance scoring,where,lms,relevance scoring where lms,0.666125476360321
translation,53,5,model,relevance scoring,use,lms,relevance scoring use lms,0.6429979205131531
translation,53,5,model,kg nodes,to form,joint graph,kg nodes to form joint graph,0.6192983388900757
translation,53,5,model,joint reasoning,connect,qa context and kg,joint reasoning connect qa context and kg,0.6952099800109863
translation,53,5,model,qa context and kg,to form,joint graph,qa context and kg to form joint graph,0.616638720035553
translation,53,5,model,representations,through,graph - based message passing,representations through graph - based message passing,0.6508306860923767
translation,53,5,model,model,propose,qa - gnn,model propose qa - gnn,0.7122156023979187
translation,53,30,model,feature,of,each node,feature of each node,0.6375957727432251
translation,53,30,model,each node,with,relevance score,each node with relevance score,0.6118701100349426
translation,53,30,model,new attention - based gnn module,for,reasoning,new attention - based gnn module for reasoning,0.6126775741577148
translation,53,30,model,model,augment,feature,model augment feature,0.7263062000274658
translation,53,30,model,model,design,new attention - based gnn module,model design new attention - based gnn module,0.5626288652420044
translation,53,31,model,joint reasoning algorithm,on,working graph,joint reasoning algorithm on working graph,0.5430496335029602
translation,53,31,model,joint reasoning algorithm,simultaneously updates,representation,joint reasoning algorithm simultaneously updates representation,0.7555167078971863
translation,53,31,model,working graph,simultaneously updates,representation,working graph simultaneously updates representation,0.7769919037818909
translation,53,31,model,representation,of both,kg entities,representation of both kg entities,0.6875184774398804
translation,53,31,model,representation,of both,qa context node,representation of both qa context node,0.630826473236084
translation,53,31,model,model,has,joint reasoning algorithm,model has joint reasoning algorithm,0.5235112309455872
translation,53,32,results,outperforms,has,strong fine- tuned lm baselines,outperforms has strong fine- tuned lm baselines,0.5728601813316345
translation,53,32,results,results,evaluate,qa,results evaluate qa,0.48974260687828064
translation,53,33,results,qa - gnn,exhibits,improved performance,qa - gnn exhibits improved performance,0.7211683988571167
translation,53,33,results,improved performance,on,some forms of structured reasoning,improved performance on some forms of structured reasoning,0.4964044690132141
translation,53,33,results,improved performance,achieves,4.6 % improvement,improved performance achieves 4.6 % improvement,0.6388002038002014
translation,53,33,results,4.6 % improvement,over,fine- tuned lms,4.6 % improvement over fine- tuned lms,0.6522716879844666
translation,53,33,results,fine- tuned lms,on,questions,fine- tuned lms on questions,0.5659245252609253
translation,53,33,results,+ 0.6 %,over,fine- tuned lms,+ 0.6 % over fine- tuned lms,0.6716353893280029
translation,53,33,results,results,has,qa - gnn,results has qa - gnn,0.5782915949821472
translation,53,127,results,competitive results,to,other systems,competitive results to other systems,0.5581709146499634
translation,53,127,results,competitive results,on,official leaderboards,competitive results on official leaderboards,0.5233601331710815
translation,53,127,results,other systems,on,official leaderboards,other systems on official leaderboards,0.5280522108078003
translation,53,127,results,results,achieve,competitive results,results achieve competitive results,0.6465562582015991
translation,53,129,results,our model,achieves,top performance,our model achieves top performance,0.6796824336051941
translation,53,129,results,comparable in size and amount of data,to,other systems,comparable in size and amount of data to other systems,0.5538395047187805
translation,53,129,results,top performance,on,two datasets,top performance on two datasets,0.48105186223983765
translation,53,129,results,systems,has,our model,systems has our model,0.6282470226287842
translation,53,156,results,limited improvements,over,roberta,limited improvements over roberta,0.6017455458641052
translation,53,156,results,roberta,on,questions,roberta on questions,0.5983307957649231
translation,53,156,results,questions,with,negation ( + 0.6 % ),questions with negation ( + 0.6 % ),0.6065779328346252
translation,53,156,results,qa - gnn,exhibits,bigger boost,qa - gnn exhibits bigger boost,0.6978716850280762
translation,53,156,results,bigger boost,has,+ 4.6 % ),bigger boost has + 4.6 % ),0.5937076210975647
translation,54,36,model,novel masked language model,for helping improve,fluency and grammar correctness,novel masked language model for helping improve fluency and grammar correctness,0.6845620274543762
translation,54,36,model,fluency and grammar correctness,of,generated cad,fluency and grammar correctness of generated cad,0.5995137095451355
translation,54,36,model,model,introduce,novel masked language model,model introduce novel masked language model,0.6018776297569275
translation,54,37,model,fine-tuned model,as,discriminator,fine-tuned model as discriminator,0.5251905918121338
translation,54,37,model,discriminator,for automatically evaluating,edit-distance,discriminator for automatically evaluating edit-distance,0.7158894538879395
translation,54,37,model,edit-distance,using,data,edit-distance using data,0.6975882053375244
translation,54,37,model,data,generated with,minimal and fluent edits,data generated with minimal and fluent edits,0.6976696848869324
translation,54,37,model,model,add,fine-tuned model,model add fine-tuned model,0.6239851117134094
translation,54,35,results,cad 's per-formance benefit,tends to,decrease,cad 's per-formance benefit tends to decrease,0.7417270541191101
translation,54,35,results,increasing parameter spaces,has,cad 's per-formance benefit,increasing parameter spaces has cad 's per-formance benefit,0.5891275405883789
translation,54,35,results,results,found that,increasing parameter spaces,results found that increasing parameter spaces,0.6695753335952759
translation,54,35,results,results,for,increasing parameter spaces,results for increasing parameter spaces,0.6343483328819275
translation,55,78,ablation-analysis,gap,in,kgqa,gap in kgqa,0.5805284380912781
translation,55,78,ablation-analysis,65 % f1 score,in,english ( src ),65 % f1 score in english ( src ),0.5082931518554688
translation,55,78,ablation-analysis,65 % f1 score,vs.,54 %,65 % f1 score vs. 54 %,0.7182174921035767
translation,55,78,ablation-analysis,english ( src ),vs.,54 %,english ( src ) vs. 54 %,0.696116030216217
translation,55,78,ablation-analysis,54 %,in,italian ( tgt ),54 % in italian ( tgt ),0.533668577671051
translation,55,78,ablation-analysis,54 %,observed by,mbert zero-shot transfer,54 % observed by mbert zero-shot transfer,0.7343451380729675
translation,55,78,ablation-analysis,mbert zero-shot transfer,in,our pipeline,mbert zero-shot transfer in our pipeline,0.5362146496772766
translation,55,78,ablation-analysis,mbert zero-shot transfer,without,multilingual augmenting,mbert zero-shot transfer without multilingual augmenting,0.6834704279899597
translation,55,78,ablation-analysis,gap,has,65 % f1 score,gap has 65 % f1 score,0.5967653393745422
translation,55,78,ablation-analysis,kgqa,has,65 % f1 score,kgqa has 65 % f1 score,0.5952323079109192
translation,55,78,ablation-analysis,ablation analysis,emphasize,gap,ablation analysis emphasize gap,0.7013121247291565
translation,55,178,ablation-analysis,performance,of,our approach,performance of our approach,0.5712711215019226
translation,55,178,ablation-analysis,more noise added,has,performance,more noise added has performance,0.5691373944282532
translation,55,178,ablation-analysis,our approach,has,drops,our approach has drops,0.6333293318748474
translation,55,165,experiments,xlm -r,as,multilingual encoder,xlm -r as multilingual encoder,0.530677080154419
translation,55,152,hyperparameters,embedding and hidden size,set to,768,embedding and hidden size set to 768,0.7439280152320862
translation,55,152,hyperparameters,hyperparameters,has,embedding and hidden size,hyperparameters has embedding and hidden size,0.5468842387199402
translation,55,153,hyperparameters,adam optimizer,to optimize,kgqa loss,adam optimizer to optimize kgqa loss,0.6684078574180603
translation,55,153,hyperparameters,adam optimizer,to optimize,linear warmup,adam optimizer to optimize linear warmup,0.6765668392181396
translation,55,153,hyperparameters,adam optimizer,with,linear warmup,adam optimizer with linear warmup,0.5890266299247742
translation,55,153,hyperparameters,kgqa loss,with,learning rate,kgqa loss with learning rate,0.5635788440704346
translation,55,153,hyperparameters,kgqa loss,with,linear warmup,kgqa loss with linear warmup,0.6224173307418823
translation,55,153,hyperparameters,learning rate,of,5 ? 10 ?5,learning rate of 5 ? 10 ?5,0.6658074855804443
translation,55,153,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,55,154,hyperparameters,batch size,set to,"35 , 3 , and 32","batch size set to 35 , 3 , and 32",0.7301909923553467
translation,55,154,hyperparameters,hyperparameters,has,maximum training epoch,hyperparameters has maximum training epoch,0.4844345152378082
translation,55,154,hyperparameters,hyperparameters,has,warm - up epoch,hyperparameters has warm - up epoch,0.5073937177658081
translation,55,154,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,55,155,hyperparameters,discriminator,trained along with,each module 's objective,discriminator trained along with each module 's objective,0.6931278705596924
translation,55,155,hyperparameters,discriminator,set to,5 ? 10 ?4,discriminator set to 5 ? 10 ?4,0.7131747007369995
translation,55,155,hyperparameters,5 ? 10 ?4,for,learning to fool,5 ? 10 ?4 for learning to fool,0.6734492182731628
translation,55,155,hyperparameters,hyperparameters,has,discriminator,hyperparameters has discriminator,0.49870187044143677
translation,55,156,hyperparameters,discriminator,optimized via,adam optimizer,discriminator optimized via adam optimizer,0.7104576230049133
translation,55,156,hyperparameters,discriminator,for,type constraint model,discriminator for type constraint model,0.5844286680221558
translation,55,156,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,55,156,hyperparameters,adam optimizer,for,type constraint model,adam optimizer for type constraint model,0.5871578454971313
translation,55,156,hyperparameters,type constraint model,set to,0.7,type constraint model set to 0.7,0.6827227473258972
translation,55,156,hyperparameters,hyperparameters,has,discriminator,hyperparameters has discriminator,0.49870187044143677
translation,55,8,model,unsupervised bilingual lexicon induction ( bli ),to map,training questions,unsupervised bilingual lexicon induction ( bli ) to map training questions,0.6694692969322205
translation,55,8,model,training questions,in,source language,training questions in source language,0.4910217523574829
translation,55,8,model,source language,into those in,target language,source language into those in target language,0.6338541507720947
translation,55,8,model,target language,as,augmented training data,target language as augmented training data,0.49352899193763733
translation,55,8,model,model,exploit,unsupervised bilingual lexicon induction ( bli ),model exploit unsupervised bilingual lexicon induction ( bli ),0.7255443334579468
translation,55,9,model,adversarial learning strategy,to alleviate,syntax - disorder,adversarial learning strategy to alleviate syntax - disorder,0.6831851601600647
translation,55,9,model,syntax - disorder,of,augmented data,syntax - disorder of augmented data,0.6065139770507812
translation,55,9,model,model,propose,adversarial learning strategy,model propose adversarial learning strategy,0.6857652068138123
translation,55,29,model,full-supervised machine translator,with,unsupervised bilingual lexicon induction ( bli ),full-supervised machine translator with unsupervised bilingual lexicon induction ( bli ),0.6242346167564392
translation,55,29,model,unsupervised bilingual lexicon induction ( bli ),for,word -level translation,unsupervised bilingual lexicon induction ( bli ) for word -level translation,0.5667163133621216
translation,55,29,model,model,replace,full-supervised machine translator,model replace full-supervised machine translator,0.5830680131912231
translation,55,30,model,bli model,first trained on,non-parallel bilingual corpora,bli model first trained on non-parallel bilingual corpora,0.7149970531463623
translation,55,31,model,bilingual word alignments,in,bli,bilingual word alignments in bli,0.5580441951751709
translation,55,31,model,bilingual word alignments,map,training questions,bilingual word alignments map training questions,0.6813349723815918
translation,55,31,model,training questions,in,source language,training questions in source language,0.4910217523574829
translation,55,31,model,source language,into,target languages,source language into target languages,0.5591976642608643
translation,55,31,model,model,via,bilingual word alignments,model via bilingual word alignments,0.5939096212387085
translation,55,35,model,adversarial strategy,to mitigate,syntax - disorder,adversarial strategy to mitigate syntax - disorder,0.6907746195793152
translation,55,35,model,syntax - disorder,caused by,bli,syntax - disorder caused by bli,0.7162957191467285
translation,55,35,model,model,propose,adversarial strategy,model propose adversarial strategy,0.7111575603485107
translation,55,36,model,discriminator,on top of,encoder,discriminator on top of encoder,0.7140843272209167
translation,55,36,model,discriminator,trained to distinguish,input,discriminator trained to distinguish input,0.6757391095161438
translation,55,36,model,input,is,grammatical question,input is grammatical question,0.5950849652290344
translation,55,36,model,input,is,bli - translated one in target language,input is bli - translated one in target language,0.5738543272018433
translation,55,36,model,input,in,bli - translated one in target language,input in bli - translated one in target language,0.5339335799217224
translation,55,36,model,grammatical question,in,source language,grammatical question in source language,0.46117517352104187
translation,55,36,model,model,present,discriminator,model present discriminator,0.730438232421875
translation,55,37,model,fine- tuned,to fool,discriminator,fine- tuned to fool discriminator,0.7056232690811157
translation,55,37,model,discriminator,so that,questions ' representations,discriminator so that questions ' representations,0.5820456147193909
translation,55,37,model,questions ' representations,are both,language - and syntax - agnostic,questions ' representations are both language - and syntax - agnostic,0.6566942930221558
translation,55,37,model,kgqa goal,has,encoder,kgqa goal has encoder,0.5087802410125732
translation,55,37,model,model,jointly with,kgqa goal,model jointly with kgqa goal,0.5734542012214661
translation,55,161,results,baseline,on,both datasets,baseline on both datasets,0.5031464099884033
translation,55,161,results,both datasets,for,all languages,both datasets for all languages,0.5407257676124573
translation,55,161,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,55,161,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,55,161,results,baseline,has,significantly,baseline has significantly,0.5920042991638184
translation,55,162,results,ica,improved by,"1 % - 4 % , and 2.9 %","ica improved by 1 % - 4 % , and 2.9 %",0.6110491156578064
translation,55,162,results,"1 % - 4 % , and 2.9 %",on average,qald dataset,"1 % - 4 % , and 2.9 % on average qald dataset",0.7073066234588623
translation,55,162,results,"1 % - 4 % , and 2.9 %",on,qald dataset,"1 % - 4 % , and 2.9 % on qald dataset",0.5718694925308228
translation,55,162,results,results,has,ica,results has ica,0.43260952830314636
translation,55,163,results,improvement,on,lc - quad,improvement on lc - quad,0.5824068784713745
translation,55,163,results,lc - quad,is,even larger,lc - quad is even larger,0.610281765460968
translation,55,163,results,averaged ica and f1 score,of,all languages,averaged ica and f1 score of all languages,0.5232070088386536
translation,55,163,results,all languages,increased by,around 7 % and 4 %,all languages increased by around 7 % and 4 %,0.7002360820770264
translation,55,163,results,results,has,improvement,results has improvement,0.6248279809951782
translation,55,164,results,performance,of,"source - language ( i.e. , english ) questions","performance of source - language ( i.e. , english ) questions",0.5414020419120789
translation,55,164,results,performance,increased by,large margin,performance increased by large margin,0.717343807220459
translation,55,164,results,"source - language ( i.e. , english ) questions",increased by,large margin,"source - language ( i.e. , english ) questions increased by large margin",0.6518680453300476
translation,55,164,results,"source - language ( i.e. , english ) questions",from,80 % to 85 %,"source - language ( i.e. , english ) questions from 80 % to 85 %",0.49457409977912903
translation,55,164,results,increases,from,65 % to 66.7 %,increases from 65 % to 66.7 %,0.5925999879837036
translation,55,164,results,increases,from,80 % to 85 %,increases from 80 % to 85 %,0.6307879686355591
translation,55,164,results,65 % to 66.7 %,on,qald,65 % to 66.7 % on qald,0.5292373299598694
translation,55,164,results,80 % to 85 %,on,lc - quad,80 % to 85 % on lc - quad,0.5834957957267761
translation,55,164,results,syntax - agnostic adversarial learning,has,performance,syntax - agnostic adversarial learning has performance,0.5284474492073059
translation,55,164,results,results,BLI - augmented data and,syntax - agnostic adversarial learning,results BLI - augmented data and syntax - agnostic adversarial learning,0.6968818306922913
translation,55,167,results,similar improvements,as in,mbert,similar improvements as in mbert,0.7386966943740845
translation,55,167,results,averaged ica and f1 score,increased by,around 1 %,averaged ica and f1 score increased by around 1 %,0.6975920796394348
translation,55,167,results,results,observe,similar improvements,results observe similar improvements,0.6068282127380371
translation,55,171,results,target - languages,on,qald - multilingual,target - languages on qald - multilingual,0.5646947026252747
translation,55,171,results,target - languages,on,lc - quad - multilingual,target - languages on lc - quad - multilingual,0.5621581077575684
translation,55,172,results,our approach,increases,ica score,our approach increases ica score,0.7223571538925171
translation,55,172,results,ica score,on,qald,ica score on qald,0.5726414322853088
translation,55,172,results,ica score,by,1.7 %,ica score by 1.7 %,0.5416218638420105
translation,55,172,results,syntax - agnostic adversarial learning,improves,1.2 %,syntax - agnostic adversarial learning improves 1.2 %,0.6349433660507202
translation,55,172,results,bli - based data augmentation,has,our approach,bli - based data augmentation has our approach,0.5722675919532776
translation,55,173,results,improvements,observed,lc - quad,improvements observed lc - quad,0.7194985747337341
translation,55,173,results,lc - quad,verifies,effectiveness of both components,lc - quad verifies effectiveness of both components,0.630561351776123
translation,55,173,results,results,has,improvements,results has improvements,0.615561842918396
translation,55,179,results,50 %,of,translated words,50 % of translated words,0.5914204120635986
translation,55,179,results,translated words,are,noisy,translated words are noisy,0.6105737090110779
translation,55,179,results,translated words,has,our method,translated words has our method,0.6061103343963623
translation,55,179,results,our method,has,still outperforms,our method has still outperforms,0.6156130433082581
translation,55,179,results,still outperforms,has,baseline model,still outperforms has baseline model,0.582332968711853
translation,55,179,results,results,when,50 %,results when 50 %,0.6813443303108215
translation,56,207,ablation-analysis,average drop,of,9.3 %,average drop of 9.3 %,0.5640653967857361
translation,56,207,ablation-analysis,9.3 %,in,f 1 scores,9.3 % in f 1 scores,0.588631808757782
translation,56,207,ablation-analysis,f 1 scores,for,both our model variants,f 1 scores for both our model variants,0.623725175857544
translation,56,207,ablation-analysis,ablation analysis,has,average drop,ablation analysis has average drop,0.5845745801925659
translation,56,208,ablation-analysis,pos and dep features,observe,average drop,pos and dep features observe average drop,0.5755753517150879
translation,56,208,ablation-analysis,average drop,of,2.3 %,average drop of 2.3 %,0.5649719834327698
translation,56,208,ablation-analysis,2.3 %,in,f 1 scores,2.3 % in f 1 scores,0.5689826607704163
translation,56,161,baselines,jet,is,first end-to - end approach,jet is first end-to - end approach,0.6047693490982056
translation,56,161,baselines,first end-to - end approach,for,task of aste,first end-to - end approach for task of aste,0.6369946599006653
translation,56,161,baselines,task of aste,leverages,novel position - aware tagging scheme,task of aste leverages novel position - aware tagging scheme,0.6954044103622437
translation,56,161,baselines,baselines,has,jet,baselines has jet,0.6161085963249207
translation,56,175,baselines,gts,uses,double embeddings,gts uses double embeddings,0.6561480164527893
translation,56,175,baselines,double embeddings,has,general glove vectors,double embeddings has general glove vectors,0.5692175626754761
translation,56,175,baselines,baselines,has,gts,baselines has gts,0.5884730219841003
translation,56,145,experimental-setup,word embeddings,using,pre-trained 300 - dim,word embeddings using pre-trained 300 - dim,0.5565847158432007
translation,56,145,experimental-setup,non-bert experiments,has,word embeddings,non-bert experiments has word embeddings,0.5418511629104614
translation,56,145,experimental-setup,experimental setup,For,non-bert experiments,experimental setup For non-bert experiments,0.631851077079773
translation,56,146,experimental-setup,d w,set to,300,d w set to 300,0.6955188512802124
translation,56,146,experimental-setup,experimental setup,has,glove vectors,experimental setup has glove vectors,0.5314568281173706
translation,56,147,experimental-setup,d pos and d dep,set to,50,d pos and d dep set to 50,0.7610130310058594
translation,56,147,experimental-setup,experimental setup,has,d pos and d dep,experimental setup has d pos and d dep,0.5663369297981262
translation,56,148,experimental-setup,d h,set to,300,d h set to 300,0.7320505976676941
translation,56,148,experimental-setup,hidden state dimensions,of,both the lstms ( backward and forward ),hidden state dimensions of both the lstms ( backward and forward ),0.5483797192573547
translation,56,148,experimental-setup,hidden state dimensions,of,bi-lstm - based encoder,hidden state dimensions of bi-lstm - based encoder,0.5531929135322571
translation,56,148,experimental-setup,hidden state dimensions,of,bi-lstm - based encoder,hidden state dimensions of bi-lstm - based encoder,0.5531929135322571
translation,56,148,experimental-setup,hidden state dimensions,set to,150,hidden state dimensions set to 150,0.7180522680282593
translation,56,148,experimental-setup,both the lstms ( backward and forward ),of,bi-lstm - based encoder,both the lstms ( backward and forward ) of bi-lstm - based encoder,0.5629065036773682
translation,56,148,experimental-setup,bi-lstm - based encoder,set to,150,bi-lstm - based encoder set to 150,0.6509420275688171
translation,56,148,experimental-setup,experimental setup,has,d h,experimental setup has d h,0.5988399982452393
translation,56,149,experimental-setup,d p,set to,300,d p set to 300,0.6909915208816528
translation,56,149,experimental-setup,experimental setup,has,d p,experimental setup has d p,0.5736496448516846
translation,56,150,experimental-setup,uncased version,of,"pre-trained bert - base ( devlin et al. , 2019 )","uncased version of pre-trained bert - base ( devlin et al. , 2019 )",0.5130608677864075
translation,56,150,experimental-setup,fine-tuned,to encode,each sentence,fine-tuned to encode each sentence,0.7240579724311829
translation,56,150,experimental-setup,bert experiments,has,uncased version,bert experiments has uncased version,0.5623969435691833
translation,56,150,experimental-setup,experimental setup,For,bert experiments,experimental setup For bert experiments,0.6180595755577087
translation,56,151,experimental-setup,model variants,trained,end-to-end,model variants trained end-to-end,0.7565562129020691
translation,56,151,experimental-setup,end-to-end,on,tesla p100 - pcie 16gb gpu,end-to-end on tesla p100 - pcie 16gb gpu,0.5071946978569031
translation,56,151,experimental-setup,tesla p100 - pcie 16gb gpu,with,adam optimizer,tesla p100 - pcie 16gb gpu with adam optimizer,0.6110146045684814
translation,56,151,experimental-setup,adam optimizer,has,learning rate,adam optimizer has learning rate,0.5073457360267639
translation,56,151,experimental-setup,learning rate,has,10 ?3,learning rate has 10 ?3,0.6000961661338806
translation,56,151,experimental-setup,weight decay,has,10 ?5,weight decay has 10 ?5,0.5990263223648071
translation,56,151,experimental-setup,experimental setup,has,model variants,experimental setup has model variants,0.5390467047691345
translation,56,152,experimental-setup,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,56,152,experimental-setup,dropout rate,applied on,embeddings,dropout rate applied on embeddings,0.6108910441398621
translation,56,152,experimental-setup,0.5,applied on,embeddings,0.5 applied on embeddings,0.6762612462043762
translation,56,152,experimental-setup,embeddings,to avoid,overfitting,embeddings to avoid overfitting,0.6304114460945129
translation,56,152,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,56,8,model,tagging -free solution,for,task,tagging -free solution for task,0.6070723533630371
translation,56,8,model,model,present,tagging -free solution,model present tagging -free solution,0.689847469329834
translation,56,9,model,encoder-decoder architecture,with,pointer network - based decoding framework,encoder-decoder architecture with pointer network - based decoding framework,0.6420943140983582
translation,56,9,model,entire opinion triplet,at,each time step,entire opinion triplet at each time step,0.5430303812026978
translation,56,9,model,model,adapt,encoder-decoder architecture,model adapt encoder-decoder architecture,0.7552173137664795
translation,56,10,model,interactions,between,aspects and opinions,interactions between aspects and opinions,0.674423098564148
translation,56,10,model,interactions,effectively captured by,decoder,interactions effectively captured by decoder,0.7831124663352966
translation,56,10,model,aspects and opinions,effectively captured by,decoder,aspects and opinions effectively captured by decoder,0.7726924419403076
translation,56,10,model,decoder,by considering,entire detected spans,decoder by considering entire detected spans,0.7307143211364746
translation,56,10,model,entire detected spans,while predicting,connecting sentiment,entire detected spans while predicting connecting sentiment,0.7736376523971558
translation,56,10,model,model,has,interactions,model has interactions,0.5665457248687744
translation,56,46,model,semantic role labeling,propose,paste,semantic role labeling propose paste,0.6021109223365784
translation,56,46,model,model,propose,paste,model propose paste,0.6608495712280273
translation,56,47,model,pointer network,effectively captures,aspect-opinion interdependence,pointer network effectively captures aspect-opinion interdependence,0.7575364112854004
translation,56,47,model,aspect-opinion interdependence,while detecting,respective spans,aspect-opinion interdependence while detecting respective spans,0.7337833046913147
translation,56,47,model,model,has,pointer network,model has pointer network,0.5618420243263245
translation,56,54,model,architecture,exploits,aspect-opinion interdependence,architecture exploits aspect-opinion interdependence,0.748745322227478
translation,56,54,model,architecture,models,span-level interactions,architecture models span-level interactions,0.784639835357666
translation,56,54,model,architecture,propose,position - based scheme,architecture propose position - based scheme,0.6931284666061401
translation,56,54,model,aspect-opinion interdependence,during,span detection process,aspect-opinion interdependence during span detection process,0.6107187271118164
translation,56,54,model,span-level interactions,for,sentiment prediction,span-level interactions for sentiment prediction,0.5939781665802002
translation,56,54,model,span-level interactions,propose,position - based scheme,span-level interactions propose position - based scheme,0.6662407517433167
translation,56,54,model,position - based scheme,to uniformly represent,opinion triplet,position - based scheme to uniformly represent opinion triplet,0.7196758389472961
translation,56,54,model,model,propose,position - based scheme,model propose position - based scheme,0.68131422996521
translation,56,179,results,both our variants,perform,comparably,both our variants perform comparably,0.5851083397865295
translation,56,179,results,substantially outperform,has,all the non-bert baselines,substantially outperform has all the non-bert baselines,0.5952956676483154
translation,56,180,results,laptop,achieve,13.1 % f 1 gains,laptop achieve 13.1 % f 1 gains,0.6625638008117676
translation,56,180,results,laptop,achieve,2.2 % f 1 gains,laptop achieve 2.2 % f 1 gains,0.6509658694267273
translation,56,180,results,laptop,on,restaurant,laptop on restaurant,0.541511595249176
translation,56,180,results,laptop,obtain,2.2 % f 1 gains,laptop obtain 2.2 % f 1 gains,0.581407904624939
translation,56,180,results,13.1 % f 1 gains,over,ote - mtl,13.1 % f 1 gains over ote - mtl,0.6505237817764282
translation,56,180,results,13.1 % f 1 gains,over,gts - bilstm,13.1 % f 1 gains over gts - bilstm,0.6575813293457031
translation,56,180,results,restaurant,obtain,2.2 % f 1 gains,restaurant obtain 2.2 % f 1 gains,0.6150373220443726
translation,56,180,results,2.2 % f 1 gains,over,gts - bilstm,2.2 % f 1 gains over gts - bilstm,0.6599964499473572
translation,56,180,results,results,On,laptop,results On laptop,0.5542846322059631
translation,56,180,results,results,on,restaurant,results on restaurant,0.5353488922119141
translation,56,182,results,better performance,attributed to,better recall scores,better performance attributed to better recall scores,0.6396190524101257
translation,56,182,results,better recall scores,with,around 15.6 % recall gains,better recall scores with around 15.6 % recall gains,0.6268534660339355
translation,56,182,results,around 15.6 % recall gains,over,respective strongest baselines,around 15.6 % recall gains over respective strongest baselines,0.6757492423057556
translation,56,182,results,respective strongest baselines,on,laptop and restaurant datasets,respective strongest baselines on laptop and restaurant datasets,0.49875789880752563
translation,56,182,results,results,has,better performance,results has better performance,0.6032086610794067
translation,56,184,results,jet,on,all the datasets,jet on all the datasets,0.5756558179855347
translation,56,184,results,bert,has,comfortably outperform,bert has comfortably outperform,0.6442790627479553
translation,56,184,results,comfortably outperform,has,jet,comfortably outperform has jet,0.6147428750991821
translation,56,184,results,results,With,bert,results With bert,0.4931848347187042
translation,56,185,results,gts - bert,on,laptop,gts - bert on laptop,0.5296063423156738
translation,56,185,results,outperforms,on,all the restaurant datasets,outperforms on all the restaurant datasets,0.5097989439964294
translation,56,194,results,baselines,on,laptop and restaurant datasets,baselines on laptop and restaurant datasets,0.4765833020210266
translation,56,194,results,our core architecture ( w/ o bert ),has,paste,our core architecture ( w/ o bert ) has paste,0.5990115404129028
translation,56,194,results,paste,has,consistently outperforms,paste has consistently outperforms,0.6407350897789001
translation,56,194,results,consistently outperforms,has,baselines,consistently outperforms has baselines,0.6030921339988708
translation,56,195,results,paste,better than,previous tagging - based approaches,paste better than previous tagging - based approaches,0.6442617774009705
translation,56,195,results,aspect-opinion span-level interdependence,during,extraction process,aspect-opinion span-level interdependence during extraction process,0.59868323802948
translation,56,203,results,our baselines,on,aspect and opinion span detection sub-tasks,our baselines on aspect and opinion span detection sub-tasks,0.49138176441192627
translation,56,203,results,substantially outperform,has,our baselines,substantially outperform has our baselines,0.5874319076538086
translation,56,204,results,outperformed,when it comes to,sentiment detection,outperformed when it comes to sentiment detection,0.70235276222229
translation,57,197,baselines,baselines,has,non-continual learning ( nl ),baselines has non-continual learning ( nl ),0.5843030214309692
translation,57,199,baselines,3 baselines,under,nl,3 baselines under nl,0.6212452054023743
translation,57,199,baselines,3 baselines,under,w2v ( word2vec embeddings ),3 baselines under w2v ( word2vec embeddings ),0.5734305381774902
translation,57,199,baselines,baselines,under,nl,baselines under nl,0.6989196538925171
translation,57,200,baselines,bert,use,trainable bert,bert use trainable bert,0.6047078967094421
translation,57,200,baselines,trainable bert,to perform,asc,trainable bert to perform asc,0.7143428921699524
translation,57,200,baselines,adapter-bert,adapts,bert,adapter-bert adapts bert,0.728184700012207
translation,57,200,baselines,w2v,uses,embeddings,w2v uses embeddings,0.6832162141799927
translation,57,200,baselines,embeddings,trained on,amazon review data,embeddings trained on amazon review data,0.725562572479248
translation,57,200,baselines,amazon review data,using,fast- text,amazon review data using fast- text,0.6456372737884521
translation,57,200,baselines,baselines,For,bert,baselines For bert,0.6953617334365845
translation,57,202,baselines,baselines,has,continual learning ( cl ),baselines has continual learning ( cl ),0.5845546722412109
translation,57,203,baselines,12 baselines,from,6,12 baselines from 6,0.6133310794830322
translation,57,203,baselines,3 baselines,has,without dealing with forgetting ( wdf ),3 baselines has without dealing with forgetting ( wdf ),0.5583510994911194
translation,57,204,baselines,wdf baselines,greedily learn,sequence of tasks,wdf baselines greedily learn sequence of tasks,0.5966076850891113
translation,57,204,baselines,sequence of tasks,without explicitly tackling,forgetting,sequence of tasks without explicitly tackling forgetting,0.6866227984428406
translation,57,204,baselines,sequence of tasks,without explicitly tackling,knowledge transfer,sequence of tasks without explicitly tackling knowledge transfer,0.6522963643074036
translation,57,204,baselines,incrementally,without explicitly tackling,knowledge transfer,incrementally without explicitly tackling knowledge transfer,0.7359265089035034
translation,57,204,baselines,sequence of tasks,has,incrementally,sequence of tasks has incrementally,0.6141365170478821
translation,57,204,baselines,baselines,has,wdf baselines,baselines has wdf baselines,0.5954494476318359
translation,57,205,baselines,3 baselines,under,wdf,3 baselines under wdf,0.588284432888031
translation,57,205,baselines,wdf,are,bert,wdf are bert,0.6894651651382446
translation,57,205,baselines,wdf,are,adapter - bert,wdf are adapter - bert,0.6345420479774475
translation,57,205,baselines,wdf,are,w2v,wdf are w2v,0.6070438623428345
translation,57,205,baselines,baselines,under,wdf,baselines under wdf,0.6443697214126587
translation,57,205,baselines,baselines,has,3 baselines,baselines has 3 baselines,0.6335406303405762
translation,57,206,baselines,6 state- of- the - art,has,cl systems,6 state- of- the - art has cl systems,0.4974624216556549
translation,57,206,baselines,baselines,has,6 state- of- the - art,baselines has 6 state- of- the - art,0.5597833395004272
translation,57,212,baselines,ewc,is,popular regularization - based class incremental learning ( cil ) method,ewc is popular regularization - based class incremental learning ( cil ) method,0.5436784029006958
translation,57,212,baselines,popular regularization - based class incremental learning ( cil ) method,adapted for,til,popular regularization - based class incremental learning ( cil ) method adapted for til,0.6276266574859619
translation,57,212,baselines,specific task id,during,training,specific task id during training,0.7255362868309021
translation,57,212,baselines,corresponding head 's prediction,during,testing,corresponding head 's prediction during testing,0.7073137164115906
translation,57,212,baselines,baselines,has,ewc,baselines has ewc,0.5811472535133362
translation,57,213,baselines,"owm ( zeng et al. , 2019 )",is,state - of- theart cil method,"owm ( zeng et al. , 2019 ) is state - of- theart cil method",0.5485806465148926
translation,57,213,baselines,state - of- theart cil method,adapt to,til,state - of- theart cil method adapt to til,0.6630411744117737
translation,57,213,baselines,baselines,has,"owm ( zeng et al. , 2019 )","baselines has owm ( zeng et al. , 2019 )",0.530544638633728
translation,57,214,baselines,6 baselines,using,w2v embeddings,6 baselines using w2v embeddings,0.5471423864364624
translation,57,214,baselines,6 baselines,using,bert,6 baselines using bert,0.6136292815208435
translation,57,214,baselines,6 baselines,using,bert,6 baselines using bert,0.6136292815208435
translation,57,214,baselines,w2v embeddings,with,aspect term,w2v embeddings with aspect term,0.5580676198005676
translation,57,214,baselines,aspect term,added before,sentence,aspect term added before sentence,0.6220335364341736
translation,57,214,baselines,6 baselines,using,bert,6 baselines using bert,0.6136292815208435
translation,57,214,baselines,bert,has,frozen ),bert has frozen ),0.649966835975647
translation,57,218,hyperparameters,task sharing module,employ,2 layers of fully connected network,task sharing module employ 2 layers of fully connected network,0.5312902927398682
translation,57,218,hyperparameters,2 layers of fully connected network,dimensions,768,2 layers of fully connected network dimensions 768,0.585332453250885
translation,57,218,hyperparameters,768,has,in tcl,768 has in tcl,0.68345046043396
translation,57,218,hyperparameters,hyperparameters,for,task sharing module,hyperparameters for task sharing module,0.548771321773529
translation,57,220,hyperparameters,dynamic routing,repeated for,3 iterations,dynamic routing repeated for 3 iterations,0.6991627812385559
translation,57,220,hyperparameters,hyperparameters,has,dynamic routing,hyperparameters has dynamic routing,0.5654545426368713
translation,57,221,hyperparameters,task-specific module,employ,embedding,task-specific module employ embedding,0.5945330262184143
translation,57,221,hyperparameters,embedding,with,2000 dimensions,embedding with 2000 dimensions,0.7254818081855774
translation,57,221,hyperparameters,embedding,as,final and hidden layer,embedding as final and hidden layer,0.5717828273773193
translation,57,221,hyperparameters,final and hidden layer,of,tsm,final and hidden layer of tsm,0.6404836773872375
translation,57,221,hyperparameters,hyperparameters,For,task-specific module,hyperparameters For task-specific module,0.5334304571151733
translation,57,222,hyperparameters,task id embeddings,have,2000 dimensions,task id embeddings have 2000 dimensions,0.5407152771949768
translation,57,222,hyperparameters,hyperparameters,has,task id embeddings,hyperparameters has task id embeddings,0.4907762408256531
translation,57,225,hyperparameters,training,of,"bert , adapter - bert and b - cl","training of bert , adapter - bert and b - cl",0.5978651642799377
translation,57,225,hyperparameters,"bert , adapter - bert and b - cl",follow,"xu et al. , 2019 )","bert , adapter - bert and b - cl follow xu et al. , 2019 )",0.5808050036430359
translation,57,225,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,57,227,hyperparameters,maximum length,of,sum of sentence and aspect,maximum length of sum of sentence and aspect,0.5727568864822388
translation,57,227,hyperparameters,sum of sentence and aspect,set to,128,sum of sentence and aspect set to 128,0.6767392754554749
translation,57,227,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,57,228,hyperparameters,adam optimizer,set,learning rate,adam optimizer set learning rate,0.6309201717376709
translation,57,228,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,57,229,hyperparameters,semeval datasets,for,all other datasets,semeval datasets for all other datasets,0.5257278084754944
translation,57,229,hyperparameters,30 epochs,used,results,30 epochs used results,0.6193733215332031
translation,57,229,hyperparameters,30 epochs,based on,results,30 epochs based on results,0.6440513134002686
translation,57,229,hyperparameters,results,from,validation data,results from validation data,0.548320472240448
translation,57,229,hyperparameters,semeval datasets,has,10 epochs,semeval datasets has 10 epochs,0.5844972133636475
translation,57,229,hyperparameters,semeval datasets,has,30 epochs,semeval datasets has 30 epochs,0.5780022740364075
translation,57,229,hyperparameters,all other datasets,has,30 epochs,all other datasets has 30 epochs,0.5683358907699585
translation,57,229,hyperparameters,hyperparameters,For,semeval datasets,hyperparameters For semeval datasets,0.5243437886238098
translation,57,229,hyperparameters,hyperparameters,for,all other datasets,hyperparameters for all other datasets,0.47799068689346313
translation,57,230,hyperparameters,runs,use,batch size,runs use batch size,0.6805530786514282
translation,57,230,hyperparameters,batch size,has,32,batch size has 32,0.630264163017273
translation,57,230,hyperparameters,hyperparameters,has,runs,hyperparameters has runs,0.5537269711494446
translation,57,231,hyperparameters,cl baselines,train,all models,cl baselines train all models,0.6892093420028687
translation,57,231,hyperparameters,all models,with,learning rate,all models with learning rate,0.60528564453125
translation,57,231,hyperparameters,learning rate,of,0.05,learning rate of 0.05,0.6214359998703003
translation,57,231,hyperparameters,hyperparameters,For,cl baselines,hyperparameters For cl baselines,0.5964531898498535
translation,57,232,hyperparameters,training,when,no improvement,training when no improvement,0.6502584218978882
translation,57,232,hyperparameters,no improvement,in,validation loss,no improvement in validation loss,0.4878521263599396
translation,57,232,hyperparameters,validation loss,for,5 epochs,validation loss for 5 epochs,0.5681278705596924
translation,57,232,hyperparameters,hyperparameters,early - stop,training,hyperparameters early - stop training,0.6565902233123779
translation,57,233,hyperparameters,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,57,233,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,57,7,model,novel capsule network based model,called,b - cl,novel capsule network based model called b - cl,0.6859564781188965
translation,57,7,model,model,proposes,novel capsule network based model,model proposes novel capsule network based model,0.6856325268745422
translation,57,41,model,novel model,called,b-cl ( bert - based continual learning ),novel model called b-cl ( bert - based continual learning ),0.6523751616477966
translation,57,41,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,57,42,model,key novelty,is,building block,key novelty is building block,0.5981903076171875
translation,57,42,model,building block,called,continual learning adapter ( cla ),building block called continual learning adapter ( cla ),0.707611083984375
translation,57,42,model,building block,leverages,capsules and dynamic routing,building block leverages capsules and dynamic routing,0.7120996117591858
translation,57,42,model,building block,exploit,shared knowledge,building block exploit shared knowledge,0.7555518746376038
translation,57,42,model,building block,uses,task masks,building block uses task masks,0.6713336706161499
translation,57,42,model,cla,leverages,capsules and dynamic routing,cla leverages capsules and dynamic routing,0.6961756348609924
translation,57,42,model,cla,exploit,shared knowledge,cla exploit shared knowledge,0.7592064142227173
translation,57,42,model,capsules and dynamic routing,to identify,previous tasks,capsules and dynamic routing to identify previous tasks,0.7003937363624573
translation,57,42,model,previous tasks,similar to,new task,previous tasks similar to new task,0.6603112816810608
translation,57,42,model,previous tasks,exploit,shared knowledge,previous tasks exploit shared knowledge,0.6666850447654724
translation,57,42,model,shared knowledge,to help,new task learning,shared knowledge to help new task learning,0.5999647378921509
translation,57,42,model,task masks,to protect,task -specific knowledge,task masks to protect task -specific knowledge,0.68352210521698
translation,57,42,model,task -specific knowledge,to avoid,forgetting,task -specific knowledge to avoid forgetting,0.6668534278869629
translation,57,42,model,model,has,key novelty,model has key novelty,0.5159850716590881
translation,57,46,model,new model b-cl,with,novel adapter cla,new model b-cl with novel adapter cla,0.687727689743042
translation,57,46,model,novel adapter cla,incorporated in,pre-trained bert,novel adapter cla incorporated in pre-trained bert,0.7399556636810303
translation,57,46,model,pre-trained bert,to enable,asc continual learning,pre-trained bert to enable asc continual learning,0.7510701417922974
translation,57,46,model,model,proposes,new model b-cl,model proposes new model b-cl,0.6993173956871033
translation,57,86,model,adapter idea and the capsule network,to achieve,effective cl,adapter idea and the capsule network to achieve effective cl,0.7149211168289185
translation,57,86,model,effective cl,for,asc tasks,effective cl for asc tasks,0.6168349981307983
translation,57,86,model,model,exploit,adapter idea and the capsule network,model exploit adapter idea and the capsule network,0.715977132320404
translation,57,88,model,scalar feature detectors,with,vector capsules,scalar feature detectors with vector capsules,0.6646732091903687
translation,57,88,model,scalar feature detectors,preserve,additional information,scalar feature detectors preserve additional information,0.7650582790374756
translation,57,88,model,vector capsules,preserve,additional information,vector capsules preserve additional information,0.630142867565155
translation,57,88,model,additional information,such as,position and thickness,additional information such as position and thickness,0.5935906767845154
translation,57,88,model,position and thickness,in,images,position and thickness in images,0.5716755986213684
translation,57,88,model,model,has,capsule network ( capsnet ),model has capsule network ( capsnet ),0.6032207012176514
translation,57,114,model,knowledge sharing module ( ksm ),groups,similar tasks,knowledge sharing module ( ksm ) groups similar tasks,0.6842383742332458
translation,57,114,model,shared knowledge ( features ),to enable,knowledge transfer,shared knowledge ( features ) to enable knowledge transfer,0.7110363841056824
translation,57,114,model,knowledge transfer,among,similar tasks,knowledge transfer among similar tasks,0.5721805691719055
translation,57,114,model,model,has,knowledge sharing module ( ksm ),model has knowledge sharing module ( ksm ),0.5705783367156982
translation,57,115,model,dynamic routing algorithm,of,capsule network,dynamic routing algorithm of capsule network,0.5547851920127869
translation,57,115,model,two capsule layers,has,task capsule layer,two capsule layers has task capsule layer,0.5950135588645935
translation,57,115,model,model,achieved through,two capsule layers,model achieved through two capsule layers,0.6902794241905212
translation,57,115,model,model,achieved through,dynamic routing algorithm,model achieved through dynamic routing algorithm,0.6648895144462585
translation,57,155,model,task specific module,consists of,differentiable layers,task specific module consists of differentiable layers,0.6317060589790344
translation,57,155,model,differentiable layers,has,cla,differentiable layers has cla,0.6546182632446289
translation,57,155,model,model,has,task specific module,model has task specific module,0.566531240940094
translation,57,201,model,asc classification network,takes,aspect term and review sentence,asc classification network takes aspect term and review sentence,0.6143860220909119
translation,57,201,model,aspect term and review sentence,as,input,aspect term and review sentence as input,0.49519234895706177
translation,57,201,model,model,adopt,asc classification network,model adopt asc classification network,0.6721659302711487
translation,57,223,model,fully connected layer,with,softmax output,fully connected layer with softmax output,0.5902058482170105
translation,57,223,model,softmax output,used as,classification heads,softmax output used as classification heads,0.6529909372329712
translation,57,223,model,classification heads,in,last layer,classification heads in last layer,0.5644072890281677
translation,57,223,model,last layer,of,bert,last layer of bert,0.6522579789161682
translation,57,223,model,model,has,fully connected layer,model has fully connected layer,0.5389924645423889
translation,57,239,results,b -cl,has,outperforms,b -cl has outperforms,0.6579309701919556
translation,57,239,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,57,239,results,all baselines,has,markedly,all baselines has markedly,0.5953255891799927
translation,57,239,results,results,shows,b -cl,results shows b -cl,0.679310142993927
translation,57,242,results,nl,see,wdf,nl see wdf,0.633635401725769
translation,57,242,results,nl,for,w2v,nl for w2v,0.6656643152236938
translation,57,242,results,wdf,see,wdf,wdf see wdf,0.6275951862335205
translation,57,242,results,wdf,much better than,nl,wdf much better than nl,0.6923727989196777
translation,57,242,results,wdf,much better than,nl,wdf much better than nl,0.6923727989196777
translation,57,242,results,nl,for,w2v,nl for w2v,0.6656643152236938
translation,57,242,results,results,Comparing,nl,results Comparing nl,0.534936249256134
translation,57,242,results,results,Comparing,wdf,results Comparing wdf,0.685333788394928
translation,57,248,results,our b - cl,do,very well,our b - cl do very well,0.49372953176498413
translation,57,248,results,very well,in,forgetting avoidance,very well in forgetting avoidance,0.5008916854858398
translation,57,248,results,very well,in,knowledge transfer,very well in knowledge transfer,0.4911331832408905
translation,57,248,results,outperforming,has,all baselines,outperforming has all baselines,0.6048299670219421
translation,57,249,results,significantly poorer,than,b-cl,significantly poorer than b-cl,0.5959507822990417
translation,57,249,results,state - of- the - art cl baselines,has,ewc,state - of- the - art cl baselines has ewc,0.5351729393005371
translation,57,249,results,state - of- the - art cl baselines,has,ucl,state - of- the - art cl baselines has ucl,0.5392849445343018
translation,57,249,results,state - of- the - art cl baselines,has,owm,state - of- the - art cl baselines has owm,0.5432252287864685
translation,57,249,results,state - of- the - art cl baselines,has,hat,state - of- the - art cl baselines has hat,0.5582152605056763
translation,57,249,results,results,For,state - of- the - art cl baselines,results For state - of- the - art cl baselines,0.5720596313476562
translation,57,260,results,b - cl,slightly improves,performance,b - cl slightly improves performance,0.7281337976455688
translation,57,260,results,backward transfer,has,b - cl,backward transfer has b - cl,0.632097601890564
translation,57,260,results,results,For,backward transfer,results For backward transfer,0.6224793195724487
translation,57,265,results,full b - cl system,gives,best overall results,full b - cl system gives best overall results,0.6225740313529968
translation,57,265,results,results,shows,full b - cl system,results shows full b - cl system,0.6583369970321655
translation,58,90,ablation-analysis,different components,i.e.,type-aware graph ( tg ),different components i.e. type-aware graph ( tg ),0.6726588010787964
translation,58,90,ablation-analysis,different components,i.e.,attention ( att ),different components i.e. attention ( att ),0.6853426694869995
translation,58,90,ablation-analysis,different components,i.e.,ale,different components i.e. ale,0.711610734462738
translation,58,90,ablation-analysis,best model,i.e.,t-gcn,best model i.e. t-gcn,0.6472682952880859
translation,58,90,ablation-analysis,t-gcn,on,bert - large encoder,t-gcn on bert - large encoder,0.5446509718894958
translation,58,90,ablation-analysis,bert - large encoder,with,sentence - aspect pair input,bert - large encoder with sentence - aspect pair input,0.6120577454566956
translation,58,90,ablation-analysis,ablation analysis,based on,best model,ablation analysis based on best model,0.6790611743927002
translation,58,95,ablation-analysis,drops,on,all datasets,drops on all datasets,0.5722501873970032
translation,58,95,ablation-analysis,all datasets,if,any component,all datasets if any component,0.5587804317474365
translation,58,95,ablation-analysis,any component,excluded from,full model,any component excluded from full model,0.6990002989768982
translation,58,95,ablation-analysis,model performance,has,drops,model performance has drops,0.5833732485771179
translation,58,97,ablation-analysis,each single components,compared with,results,each single components compared with results,0.7321183681488037
translation,58,97,ablation-analysis,each single components,results from,gcn baseline ( id : 8 ),each single components results from gcn baseline ( id : 8 ),0.6816698908805847
translation,58,97,ablation-analysis,results,from,models,results from models,0.5280749201774597
translation,58,97,ablation-analysis,models,with,particular module ( id : 5 - 7 ),models with particular module ( id : 5 - 7 ),0.6635183691978455
translation,58,97,ablation-analysis,attention mechanism,is,most important one,attention mechanism is most important one,0.5510169267654419
translation,58,97,ablation-analysis,most important one,to improve,model performance,most important one to improve model performance,0.7569800615310669
translation,58,97,ablation-analysis,each single components,has,results,each single components has results,0.5941300392150879
translation,58,97,ablation-analysis,each single components,has,model,each single components has model,0.6063256859779358
translation,58,97,ablation-analysis,gcn baseline ( id : 8 ),has,results,gcn baseline ( id : 8 ) has results,0.5575603246688843
translation,58,97,ablation-analysis,ablation analysis,for,each single components,ablation analysis for each single components,0.6269057989120483
translation,58,97,ablation-analysis,ablation analysis,from,models,ablation analysis from models,0.5515545010566711
translation,58,103,ablation-analysis,all layers,contribute to,final prediction,all layers contribute to final prediction,0.6871142983436584
translation,58,103,ablation-analysis,final prediction,for,absa,final prediction for absa,0.6609385013580322
translation,58,103,ablation-analysis,ablation analysis,has,all layers,ablation analysis has all layers,0.544579803943634
translation,58,105,ablation-analysis,second layer of t-gcn,contributes,most,second layer of t-gcn contributes most,0.6614766120910645
translation,58,105,ablation-analysis,most,among,all three layers,most among all three layers,0.6868903636932373
translation,58,105,ablation-analysis,"most datasets ( i.e. lap14 , rest14 , rest15 , rest16 , and mams )",has,second layer of t-gcn,"most datasets ( i.e. lap14 , rest14 , rest15 , rest16 , and mams ) has second layer of t-gcn",0.5357203483581543
translation,58,76,baselines,"each encoder ( i.e. , bert base and large )",run,two baselines,"each encoder ( i.e. , bert base and large ) run two baselines",0.683322548866272
translation,58,76,baselines,bert,with,normal gcn,bert with normal gcn,0.6948896646499634
translation,58,76,baselines,normal gcn,where,all edges,normal gcn where all edges,0.6001610159873962
translation,58,76,baselines,all edges,are,equally treated,all edges are equally treated,0.5874935388565063
translation,58,76,baselines,predicted,output of,last gcn layer,predicted output of last gcn layer,0.7352679371833801
translation,58,73,hyperparameters,pre-trained parameters,of,bert,pre-trained parameters of bert,0.5855001211166382
translation,58,73,hyperparameters,other trainable parameters,by,"xavier ( glorot and bengio , 2010 )","other trainable parameters by xavier ( glorot and bengio , 2010 )",0.5213328003883362
translation,58,73,hyperparameters,hyperparameters,initialize,other trainable parameters,hyperparameters initialize other trainable parameters,0.7340154647827148
translation,58,6,model,dependency types,for,absa,dependency types for absa,0.6149440407752991
translation,58,6,model,dependency types,where,attention,dependency types where attention,0.5815339088439941
translation,58,6,model,dependency types,where,attentive layer ensemble,dependency types where attentive layer ensemble,0.5398315191268921
translation,58,6,model,absa,with,type -aware graph convolutional networks ( t- gcn ),absa with type -aware graph convolutional networks ( t- gcn ),0.6499036550521851
translation,58,6,model,attention,used in,t-gcn,attention used in t-gcn,0.7527058124542236
translation,58,6,model,t-gcn,to distinguish,different edges ( relations ),t-gcn to distinguish different edges ( relations ),0.6850337982177734
translation,58,6,model,different edges ( relations ),in,graph,different edges ( relations ) in graph,0.5237339735031128
translation,58,23,model,type-aware graph convolutional networks ( t- gcn ),with,multiple layers,type-aware graph convolutional networks ( t- gcn ) with multiple layers,0.6303359866142273
translation,58,23,model,multiple layers,to enhance,absa,multiple layers to enhance absa,0.713712751865387
translation,58,23,model,absa,by incorporating,word relations and their dependency types,absa by incorporating word relations and their dependency types,0.6321781873703003
translation,58,23,model,absa,to comprehensively learn from,dependency parsing results,absa to comprehensively learn from dependency parsing results,0.6022341847419739
translation,58,23,model,word relations and their dependency types,to comprehensively learn from,dependency parsing results,word relations and their dependency types to comprehensively learn from dependency parsing results,0.5475620031356812
translation,58,23,model,model,propose,type-aware graph convolutional networks ( t- gcn ),model propose type-aware graph convolutional networks ( t- gcn ),0.6307380795478821
translation,58,24,model,dependency parsing results,of,input texts,dependency parsing results of input texts,0.5061821937561035
translation,58,24,model,input texts,through,off - the-shelf toolkits,input texts through off - the-shelf toolkits,0.6411269903182983
translation,58,24,model,off - the-shelf toolkits,build,graph,off - the-shelf toolkits build graph,0.7176958322525024
translation,58,24,model,off - the-shelf toolkits,build,graph,off - the-shelf toolkits build graph,0.7176958322525024
translation,58,24,model,graph,over,dependency tree,graph over dependency tree,0.6357588171958923
translation,58,24,model,graph,to weight,all edges,graph to weight all edges,0.7328232526779175
translation,58,24,model,dependency tree,with,each edge,dependency tree with each edge,0.6234830617904663
translation,58,24,model,dependency tree,with,all edges,dependency tree with all edges,0.6190745234489441
translation,58,24,model,each edge,labeled by,corresponding dependency type,each edge labeled by corresponding dependency type,0.6703599691390991
translation,58,24,model,corresponding dependency type,between,two connected words,corresponding dependency type between two connected words,0.6159906387329102
translation,58,24,model,attention mechanism,to,graph,attention mechanism to graph,0.5426735877990723
translation,58,24,model,graph,to weight,all edges,graph to weight all edges,0.7328232526779175
translation,58,24,model,all edges,according to,contributions,all edges according to contributions,0.7040921449661255
translation,58,24,model,attentive layer ensemble,to weight and combine,contextual information,attentive layer ensemble to weight and combine contextual information,0.7333319783210754
translation,58,24,model,contextual information,learned from,different gcn layers,contextual information learned from different gcn layers,0.6684632897377014
translation,58,24,model,model,firstly obtain,dependency parsing results,model firstly obtain dependency parsing results,0.5621690154075623
translation,58,24,model,model,use,attentive layer ensemble,model use attentive layer ensemble,0.6446933746337891
translation,58,80,results,our models,further improve,performance,our models further improve performance,0.7310587763786316
translation,58,80,results,performance,in,accuracy and f1 socres,performance in accuracy and f1 socres,0.5546317100524902
translation,58,80,results,performance,on,all datasets,performance on all datasets,0.4679260551929474
translation,58,80,results,accuracy and f1 socres,on,all datasets,accuracy and f1 socres on all datasets,0.4899418354034424
translation,58,80,results,bert - base and bert - large encoders,has,our models,bert - base and bert - large encoders has our models,0.6044878363609314
translation,58,80,results,results,for,bert - base and bert - large encoders,results for bert - base and bert - large encoders,0.5904704332351685
translation,58,82,results,our models,encode,sentence - aspect pair,our models encode sentence - aspect pair,0.7065483927726746
translation,58,82,results,sentence - aspect pair,achieve,higher results,sentence - aspect pair achieve higher results,0.6083510518074036
translation,58,82,results,higher results,than,ones,higher results than ones,0.6495943665504456
translation,58,82,results,ones,encoding,single sentence,ones encoding single sentence,0.8239990472793579
translation,58,82,results,most cases,has,our models,most cases has our models,0.5915719270706177
translation,58,82,results,results,in,most cases,results in most cases,0.5547818541526794
translation,58,82,results,results,has,our models,results has our models,0.5733726620674133
translation,58,84,results,performance,of,our best model,performance of our best model,0.5762022733688354
translation,58,84,results,bert - large encoder,with,sentence - aspect pair input,bert - large encoder with sentence - aspect pair input,0.6120577454566956
translation,58,84,results,results,compare,performance,results compare performance,0.7135117650032043
translation,59,153,ablation-analysis,vital role,in,joint sentiment and event type prediction,vital role in joint sentiment and event type prediction,0.5138718485832214
translation,59,153,ablation-analysis,event type,has,extra annotated information,event type has extra annotated information,0.5816475749015808
translation,59,153,ablation-analysis,ablation analysis,has,event type,ablation analysis has event type,0.4587600827217102
translation,59,138,baselines,baselines,has,"bilstm + orthogonalatt ( wei et al. , 2020 )","baselines has bilstm + orthogonalatt ( wei et al. , 2020 )",0.4720533788204193
translation,59,142,baselines,baselines,has,"ntn ( weber et al. , 2017 )","baselines has ntn ( weber et al. , 2017 )",0.5105112791061401
translation,59,145,baselines,ntn model,with,bert,ntn model with bert,0.7126442193984985
translation,59,145,baselines,ntn model,using,fine - tuned bert,ntn model using fine - tuned bert,0.6585178375244141
translation,59,145,baselines,bert,as,encoder,bert as encoder,0.6353098154067993
translation,59,145,baselines,fine - tuned bert,to get,hidden states,fine - tuned bert to get hidden states,0.6745864152908325
translation,59,145,baselines,hidden states,of,word sequence,hidden states of word sequence,0.5930060744285583
translation,59,145,baselines,bertntn,has,ntn model,bertntn has ntn model,0.598125159740448
translation,59,145,baselines,baselines,has,bertntn,baselines has bertntn,0.6474811434745789
translation,59,146,baselines,baselines,has,sentence and event triplet,baselines has sentence and event triplet,0.5838185548782349
translation,59,159,baselines,full model,are,htc + mtl + sen_event,full model are htc + mtl + sen_event,0.5544074773788452
translation,59,159,baselines,full model,are,htc + sen_event,full model are htc + sen_event,0.5749099850654602
translation,59,159,baselines,htc + mtl + sen_event,for,evesa,htc + mtl + sen_event for evesa,0.6590927243232727
translation,59,159,baselines,htc + mtl + sen_event,for,semeval17 task4,htc + mtl + sen_event for semeval17 task4,0.6154590249061584
translation,59,159,baselines,htc + mtl + sen_event,for,semeval17 task4,htc + mtl + sen_event for semeval17 task4,0.6154590249061584
translation,59,159,baselines,htc + sen_event,for,semeval17 task4,htc + sen_event for semeval17 task4,0.6341845393180847
translation,59,159,baselines,baselines,has,full model,baselines has full model,0.5727254152297974
translation,59,126,hyperparameters,dimension,of,hidden states,dimension of hidden states,0.5819122791290283
translation,59,126,hyperparameters,hidden states,is,768,hidden states is 768,0.6309125423431396
translation,59,126,hyperparameters,batch size,is,32,batch size is 32,0.6284153461456299
translation,59,126,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,59,126,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,59,127,hyperparameters,number of matrices,in,tensor compositions,number of matrices in tensor compositions,0.5098816156387329
translation,59,127,hyperparameters,tensor compositions,set to,100,tensor compositions set to 100,0.6994795799255371
translation,59,127,hyperparameters,hyperparameters,has,number of matrices,hyperparameters has number of matrices,0.5326805114746094
translation,59,128,hyperparameters,number of epochs,is,5,number of epochs is 5,0.6167141795158386
translation,59,128,hyperparameters,hyperparameters,has,number of epochs,hyperparameters has number of epochs,0.5152985453605652
translation,59,129,hyperparameters,"l e , l s and l sim",set to,"5 , 1 , and 1","l e , l s and l sim set to 5 , 1 , and 1",0.7193502187728882
translation,59,129,hyperparameters,hyperparameters,has,"l e , l s and l sim","hyperparameters has l e , l s and l sim",0.5776627659797668
translation,59,130,hyperparameters,loss function,minimized using,"adam optimizer ( kingma and ba , 2014 )","loss function minimized using adam optimizer ( kingma and ba , 2014 )",0.6829732656478882
translation,59,130,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,59,130,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",with,dropout rate,"adam optimizer ( kingma and ba , 2014 ) with dropout rate",0.6022156476974487
translation,59,130,hyperparameters,learning rate,of,1e 5,learning rate of 1e 5,0.650136411190033
translation,59,130,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,59,130,hyperparameters,hyperparameters,has,loss function,hyperparameters has loss function,0.4492484927177429
translation,59,136,hyperparameters,sentence length,to,last 40 tokens,sentence length to last 40 tokens,0.5257707834243774
translation,59,136,hyperparameters,last 40 tokens,to allow,larger batch size,last 40 tokens to allow larger batch size,0.6502677798271179
translation,59,136,hyperparameters,hyperparameters,limit,sentence length,hyperparameters limit sentence length,0.6229425668716431
translation,59,137,hyperparameters,bert - base,due to,memory limit,bert - base due to memory limit,0.703201174736023
translation,59,137,hyperparameters,memory limit,of,our gpu,memory limit of our gpu,0.52602219581604
translation,59,137,hyperparameters,hyperparameters,use,bert - base,hyperparameters use bert - base,0.6454315185546875
translation,59,144,hyperparameters,glove embeddings,for,word sequence,glove embeddings for word sequence,0.5714972615242004
translation,59,144,hyperparameters,hyperparameters,use,glove embeddings,hyperparameters use glove embeddings,0.5820891261100769
translation,59,24,model,corpus,in which,event triplets,corpus in which event triplets,0.6295045018196106
translation,59,24,model,event triplets,in the form of,"< subject , predicate , object >","event triplets in the form of < subject , predicate , object >",0.7072286009788513
translation,59,24,model,event triplets,in the form of,corresponding types,event triplets in the form of corresponding types,0.6919238567352295
translation,59,24,model,construct,has,corpus,construct has corpus,0.6004835367202759
translation,59,24,model,annotated in,has,sentences,annotated in has sentences,0.6388204097747803
translation,59,30,model,method,for,implicit sentiment analysis,method for implicit sentiment analysis,0.5574229955673218
translation,59,30,model,implicit sentiment analysis,built on,hierarchical tensor-based compositions,implicit sentiment analysis built on hierarchical tensor-based compositions,0.6597251296043396
translation,59,30,model,interaction,between,subject,interaction between subject,0.6652243733406067
translation,59,30,model,model,propose,method,model propose method,0.6280754208564758
translation,59,77,model,"event triplet < subject , predicate , object >",with,sentence,"event triplet < subject , predicate , object > with sentence",0.5827311873435974
translation,59,77,model,"event triplet < subject , predicate , object >",has,hierarchical tensor-based compositions,"event triplet < subject , predicate , object > has hierarchical tensor-based compositions",0.5602151155471802
translation,59,77,model,model,To integrate,"event triplet < subject , predicate , object >","model To integrate event triplet < subject , predicate , object >",0.6865662932395935
translation,59,125,model,fine-tuned,in,training process,fine-tuned in training process,0.5356848835945129
translation,59,125,model,model,has,bert encoder,model has bert encoder,0.6133244037628174
translation,59,150,results,proposed model,performs,remarkably better,proposed model performs remarkably better,0.6133496165275574
translation,59,150,results,remarkably better,than,other baselines,remarkably better than other baselines,0.5562923550605774
translation,59,150,results,other baselines,across,all input categories,other baselines across all input categories,0.7201889753341675
translation,59,150,results,all evaluation metrics,including,recently proposed implicit sentiment analysis model,all evaluation metrics including recently proposed implicit sentiment analysis model,0.6457906365394592
translation,59,150,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,59,151,results,event representation,in the form of,event triplet,event representation in the form of event triplet,0.7124059796333313
translation,59,151,results,event representation,leads to,improved performance,event representation leads to improved performance,0.6573217511177063
translation,59,151,results,improved performance,in,sentiment classification,improved performance in sentiment classification,0.5226617455482483
translation,59,151,results,improved performance,evidenced,lower performance,improved performance evidenced lower performance,0.6483954191207886
translation,59,151,results,lower performance,of,bert,lower performance of bert,0.5873644948005676
translation,59,151,results,bert,using,sentence,bert using sentence,0.745249330997467
translation,59,151,results,results,has,event representation,results has event representation,0.543364942073822
translation,59,152,results,models,using,event triplets,models using event triplets,0.6707826256752014
translation,59,152,results,event triplets,as,only input,event triplets as only input,0.5273926258087158
translation,59,152,results,only input,such as,bert,only input such as bert,0.6840032935142517
translation,59,152,results,ntn,give,inferior results,ntn give inferior results,0.6265679597854614
translation,59,160,results,event triplets,as,input only ( bertntn ),event triplets as input only ( bertntn ),0.512157678604126
translation,59,160,results,input only ( bertntn ),gives,worst results,input only ( bertntn ) gives worst results,0.6346742510795593
translation,59,160,results,results,using,event triplets,results using event triplets,0.6213038563728333
translation,59,161,results,bert,trained on,tweets,bert trained on tweets,0.7520341873168945
translation,59,161,results,bert,directly improves upon,bertntn,bert directly improves upon bertntn,0.536825954914093
translation,59,161,results,tweets,directly improves upon,bertntn,tweets directly improves upon bertntn,0.6981900334358215
translation,59,161,results,bertntn,has,quite substantially,bertntn has quite substantially,0.662097692489624
translation,59,161,results,results,has,bert,results has bert,0.43097156286239624
translation,59,162,results,outputs,from,bert and bertntn,outputs from bert and bertntn,0.6509569883346558
translation,59,162,results,outputs,from,bert,outputs from bert,0.620404064655304
translation,59,162,results,worse,than,bert,worse than bert,0.7024371027946472
translation,59,163,results,more effective,in encod - ing,sentence contextual information,more effective in encod - ing sentence contextual information,0.6515589356422424
translation,59,163,results,more effective,in encod - ing,event triplets,more effective in encod - ing event triplets,0.6834821701049805
translation,59,163,results,more effective,in encod - ing,outperforming,more effective in encod - ing outperforming,0.7212703227996826
translation,59,163,results,outperforming,has,bert,outperforming has bert,0.6190991401672363
translation,59,163,results,results,Using,hierarchical tensor compositions ( htc ),results Using hierarchical tensor compositions ( htc ),0.6241819262504578
translation,59,164,results,multi-task learning,improves,performance,multi-task learning improves performance,0.6543582081794739
translation,59,164,results,performance,of,htc,performance of htc,0.6151379942893982
translation,59,164,results,results,has,multi-task learning,results has multi-task learning,0.5150524973869324
translation,59,165,results,all three components,achieves,best results,all three components achieves best results,0.6046836972236633
translation,59,165,results,best results,on,datasets,best results on datasets,0.5370998978614807
translation,59,165,results,results,combination of,all three components,results combination of all three components,0.6059733033180237
translation,60,159,ablation-analysis,multi-word triplets,pose,challenges,multi-word triplets pose challenges,0.6798458695411682
translation,60,159,ablation-analysis,f 1 results,drop by,more than 10 points,f 1 results drop by more than 10 points,0.768962025642395
translation,60,159,ablation-analysis,gts and our model,has,multi-word triplets,gts and our model has multi-word triplets,0.567939817905426
translation,60,159,ablation-analysis,ablation analysis,For,gts and our model,ablation analysis For gts and our model,0.6465741991996765
translation,60,189,ablation-analysis,reduced,when removing,span width and distance embedding,reduced when removing span width and distance embedding,0.63778156042099
translation,60,59,baselines,two encoding methods,to obtain,contextualized representation,two encoding methods to obtain contextualized representation,0.5762055516242981
translation,60,59,baselines,contextualized representation,for,each word,contextualized representation for each word,0.5866222381591797
translation,60,59,baselines,each word,in,sentence,each word in sentence,0.5183047652244568
translation,60,103,baselines,uncased version,of,bert base,uncased version of bert base,0.6223145127296448
translation,60,117,baselines,cmla +,employs,attention mechanism,cmla + employs attention mechanism,0.6386487483978271
translation,60,117,baselines,attention mechanism,to consider,interaction,attention mechanism to consider interaction,0.650738000869751
translation,60,117,baselines,interaction,between,aspect terms and opinion terms,interaction between aspect terms and opinion terms,0.6482553482055664
translation,60,117,baselines,baselines,has,cmla +,baselines has cmla +,0.629122257232666
translation,60,118,baselines,rinante +,adopts,bilstm - crf model,rinante + adopts bilstm - crf model,0.643204391002655
translation,60,118,baselines,bilstm - crf model,with,mined rules,bilstm - crf model with mined rules,0.6202457547187805
translation,60,118,baselines,mined rules,to capture,dependency relations,mined rules to capture dependency relations,0.6175315380096436
translation,60,118,baselines,baselines,has,rinante +,baselines has rinante +,0.6456159353256226
translation,60,256,experiments,model experiments,on,nvidia tesla v100 gpu,model experiments on nvidia tesla v100 gpu,0.5150201320648193
translation,60,256,experiments,nvidia tesla v100 gpu,with,cuda version 10.2,nvidia tesla v100 gpu with cuda version 10.2,0.5763818025588989
translation,60,256,experiments,nvidia tesla v100 gpu,with,pytorch version 1.6.0,nvidia tesla v100 gpu with pytorch version 1.6.0,0.5495226979255676
translation,60,101,hyperparameters,hidden size,of,bilstm encoder,hidden size of bilstm encoder,0.5392170548439026
translation,60,101,hyperparameters,bilstm encoder,is,300,bilstm encoder is 300,0.5813038349151611
translation,60,101,hyperparameters,dropout rate,is,0.5,dropout rate is 0.5,0.5554119348526001
translation,60,101,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,60,101,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,60,104,hyperparameters,model,trained for,10 epochs,model trained for 10 epochs,0.8151181936264038
translation,60,104,hyperparameters,10 epochs,with,linear warmup,10 epochs with linear warmup,0.659950315952301
translation,60,104,hyperparameters,linear warmup,for,10 %,linear warmup for 10 %,0.656609833240509
translation,60,104,hyperparameters,linear warmup,followed by,linear decay,linear warmup followed by linear decay,0.6578307747840881
translation,60,104,hyperparameters,10 %,of,training steps,10 % of training steps,0.6151220798492432
translation,60,104,hyperparameters,linear decay,of,learning rate,linear decay of learning rate,0.5832511782646179
translation,60,104,hyperparameters,learning rate,to,0,learning rate to 0,0.6213033199310303
translation,60,104,hyperparameters,hyperparameters,trained for,10 epochs,hyperparameters trained for 10 epochs,0.6916821002960205
translation,60,104,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,60,105,hyperparameters,adamw,as,optimizer,adamw as optimizer,0.5229285955429077
translation,60,105,hyperparameters,optimizer,with,maximum learning rate,optimizer with maximum learning rate,0.6169201731681824
translation,60,105,hyperparameters,optimizer,with,weight decay,optimizer with weight decay,0.62936931848526
translation,60,105,hyperparameters,maximum learning rate,of,5e - 5,maximum learning rate of 5e - 5,0.6307420134544373
translation,60,105,hyperparameters,5e - 5,for,transformer weights,5e - 5 for transformer weights,0.620119035243988
translation,60,105,hyperparameters,weight decay,of,1e - 2,weight decay of 1e - 2,0.6325306296348572
translation,60,105,hyperparameters,hyperparameters,employ,adamw,hyperparameters employ adamw,0.5531696677207947
translation,60,106,hyperparameters,other parameter groups,use,learning rate,other parameter groups use learning rate,0.6757786273956299
translation,60,106,hyperparameters,learning rate,of,1e - 3,learning rate of 1e - 3,0.6364982724189758
translation,60,106,hyperparameters,learning rate,with,weight decay,learning rate with weight decay,0.636817991733551
translation,60,106,hyperparameters,1e - 3,with,weight decay,1e - 3 with weight decay,0.6632198095321655
translation,60,107,hyperparameters,maximum span length l,set as,8,maximum span length l set as 8,0.6548327803611755
translation,60,107,hyperparameters,hyperparameters,has,maximum span length l,hyperparameters has maximum span length l,0.48534008860588074
translation,60,108,hyperparameters,span pruning threshold z,set as,0.5,span pruning threshold z set as 0.5,0.6325692534446716
translation,60,108,hyperparameters,hyperparameters,has,span pruning threshold z,hyperparameters has span pruning threshold z,0.5264452695846558
translation,60,259,hyperparameters,feed -forward neural networks,in,mention module and triplet module,feed -forward neural networks in mention module and triplet module,0.5454168319702148
translation,60,259,hyperparameters,feed -forward neural networks,have,hidden size,feed -forward neural networks have hidden size,0.5491156578063965
translation,60,259,hyperparameters,mention module and triplet module,have,2 hidden layers,mention module and triplet module have 2 hidden layers,0.5733149647712708
translation,60,259,hyperparameters,hidden size,of,150,hidden size of 150,0.6804213523864746
translation,60,259,hyperparameters,hyperparameters,has,feed -forward neural networks,hyperparameters has feed -forward neural networks,0.5635887980461121
translation,60,260,hyperparameters,dropout,of,0.4,dropout of 0.4,0.6157568693161011
translation,60,260,hyperparameters,0.4,after,each hidden layer,0.4 after each hidden layer,0.6719397902488708
translation,60,260,hyperparameters,hyperparameters,use,relu activation,hyperparameters use relu activation,0.6096047759056091
translation,60,260,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,60,261,hyperparameters,xavier normal weight initialization,for,feed-forward parameters,xavier normal weight initialization for feed-forward parameters,0.572407066822052
translation,60,261,hyperparameters,hyperparameters,use,xavier normal weight initialization,hyperparameters use xavier normal weight initialization,0.5857784152030945
translation,60,262,hyperparameters,span width and distance embeddings,have,20 and 128 dimensions,span width and distance embeddings have 20 and 128 dimensions,0.5780469179153442
translation,60,262,hyperparameters,hyperparameters,has,span width and distance embeddings,hyperparameters has span width and distance embeddings,0.5277999639511108
translation,60,263,hyperparameters,input values,are,"bucketed ( gardner et al. , 2017 )","input values are bucketed ( gardner et al. , 2017 )",0.5598690509796143
translation,60,263,hyperparameters,"bucketed ( gardner et al. , 2017 )",before being fed to,embedding matrix lookup,"bucketed ( gardner et al. , 2017 ) before being fed to embedding matrix lookup",0.6151427626609802
translation,60,263,hyperparameters,hyperparameters,has,input values,hyperparameters has input values,0.5027667880058289
translation,60,264,hyperparameters,model parameters,updated after,each sentence,model parameters updated after each sentence,0.6633955836296082
translation,60,264,hyperparameters,each sentence,results in,batch size,each sentence results in batch size,0.5903233885765076
translation,60,264,hyperparameters,batch size,of,1,batch size of 1,0.6655300855636597
translation,60,264,hyperparameters,training,has,model parameters,training has model parameters,0.5273199081420898
translation,60,264,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,60,264,hyperparameters,hyperparameters,has,model parameters,hyperparameters has model parameters,0.45928311347961426
translation,60,9,model,dual-channel span pruning strategy,by incorporating,supervision,dual-channel span pruning strategy by incorporating supervision,0.722923994064331
translation,60,9,model,supervision,from,aspect term extraction ( ate ),supervision from aspect term extraction ( ate ),0.5647532939910889
translation,60,9,model,supervision,from,opinion term extraction ( ote ) tasks,supervision from opinion term extraction ( ote ) tasks,0.5341883301734924
translation,60,9,model,model,propose,dual-channel span pruning strategy,model propose dual-channel span pruning strategy,0.655137300491333
translation,60,39,model,span-based model,for,aste (,span-based model for aste (,0.6349520087242126
translation,60,39,model,sentiment relation,of,aspect target and opinion pair,sentiment relation of aspect target and opinion pair,0.5723589658737183
translation,60,39,model,aste (,has,),aste ( has ),0.6618462800979614
translation,60,39,model,model,propose,span-based model,model propose span-based model,0.6320574879646301
translation,60,102,model,"pre-trained bert ( devlin et al. , 2019 )",to encode,each sentence,"pre-trained bert ( devlin et al. , 2019 ) to encode each sentence",0.7066152095794678
translation,60,205,model,span-level approach - span - aste,to learn,interactions,span-level approach - span - aste to learn interactions,0.6679157614707947
translation,60,205,model,interactions,between,target spans and opinion spans,interactions between target spans and opinion spans,0.6988096833229065
translation,60,205,model,target spans and opinion spans,for,aste task,target spans and opinion spans for aste task,0.5839306712150574
translation,60,205,model,model,propose,span-level approach - span - aste,model propose span-level approach - span - aste,0.6866393089294434
translation,60,11,results,our framework,simultaneously achieves,strong performance,our framework simultaneously achieves strong performance,0.6704097986221313
translation,60,11,results,strong performance,for,aste as well as ate and ote tasks,strong performance for aste as well as ate and ote tasks,0.6601468920707703
translation,60,11,results,results,has,our framework,results has our framework,0.6097875237464905
translation,60,126,results,consistently outperforms,for,bilstm and bert sentence encoders,consistently outperforms for bilstm and bert sentence encoders,0.5962936282157898
translation,60,126,results,previous works,for,bilstm and bert sentence encoders,previous works for bilstm and bert sentence encoders,0.5446714758872986
translation,60,126,results,f 1 metric,has,our model,f 1 metric has our model,0.6083639860153198
translation,60,126,results,our model,has,consistently outperforms,our model has consistently outperforms,0.6076288819313049
translation,60,126,results,consistently outperforms,has,previous works,consistently outperforms has previous works,0.5864814519882202
translation,60,127,results,significantly out -performs,in,precision and recall,significantly out -performs in precision and recall,0.5717372298240662
translation,60,127,results,other end-to - end methods,in,precision and recall,other end-to - end methods in precision and recall,0.45556554198265076
translation,60,127,results,other end-to - end methods,both,precision and recall,other end-to - end methods both precision and recall,0.5938535332679749
translation,60,127,results,our model,has,significantly out -performs,our model has significantly out -performs,0.5800602436065674
translation,60,127,results,significantly out -performs,has,other end-to - end methods,significantly out -performs has other end-to - end methods,0.5750085711479187
translation,60,127,results,results,has,our model,results has our model,0.5871725678443909
translation,60,131,results,other end-to - end methods,are,more competitive,other end-to - end methods are more competitive,0.5503902435302734
translation,60,131,results,more competitive,than,pipeline methods,more competitive than pipeline methods,0.5873119831085205
translation,60,131,results,results,has,other end-to - end methods,results has other end-to - end methods,0.530074417591095
translation,60,133,results,all three end-to - end models,achieve,much stronger performance,all three end-to - end models achieve much stronger performance,0.6357948184013367
translation,60,133,results,much stronger performance,than,lstm - based versions,much stronger performance than lstm - based versions,0.589474081993103
translation,60,133,results,bert encoder,has,all three end-to - end models,bert encoder has all three end-to - end models,0.5817113518714905
translation,60,133,results,results,With,bert encoder,results With bert encoder,0.6525373458862305
translation,60,134,results,our approach,outperforms,"previous best results gts ( wu et al. , 2020 )","our approach outperforms previous best results gts ( wu et al. , 2020 )",0.7251538038253784
translation,60,134,results,"previous best results gts ( wu et al. , 2020 )",by,"4.35 , 5.02 , 3.12 , and 2.33 f 1 points","previous best results gts ( wu et al. , 2020 ) by 4.35 , 5.02 , 3.12 , and 2.33 f 1 points",0.5272852182388306
translation,60,134,results,"4.35 , 5.02 , 3.12 , and 2.33 f 1 points",on,four datasets,"4.35 , 5.02 , 3.12 , and 2.33 f 1 points on four datasets",0.5315126776695251
translation,60,134,results,results,has,our approach,results has our approach,0.6050099730491638
translation,60,139,results,our model,directly address,ate and ote tasks,our model directly address ate and ote tasks,0.6723875403404236
translation,60,139,results,ate and ote tasks,with,significant performance improvement,ate and ote tasks with significant performance improvement,0.6399592161178589
translation,60,139,results,significant performance improvement,than,gts,significant performance improvement than gts,0.6300693154335022
translation,60,139,results,f 1 scores,on,both tasks,f 1 scores on both tasks,0.48441535234451294
translation,60,140,results,gts,shows,better recall score,gts shows better recall score,0.6804850101470947
translation,60,140,results,better recall score,on,rest 16 dataset,better recall score on rest 16 dataset,0.518442690372467
translation,60,140,results,low precision score,results in,worse f 1 performance,low precision score results in worse f 1 performance,0.6239408850669861
translation,60,154,results,our method,shows,consistent improvement,our method shows consistent improvement,0.6471164226531982
translation,60,154,results,consistent improvement,in terms of,precision and recall score,consistent improvement in terms of precision and recall score,0.683912456035614
translation,60,154,results,consistent improvement,results in,improvement,consistent improvement results in improvement,0.5934396386146545
translation,60,154,results,precision and recall score,on,four datasets,precision and recall score on four datasets,0.4814586937427521
translation,60,154,results,improvement,of,f 1 score,improvement of f 1 score,0.6051212549209595
translation,60,154,results,single- word setting,has,our method,single- word setting has our method,0.5435657501220703
translation,60,154,results,results,For,single- word setting,results For single- word setting,0.5548301339149475
translation,60,155,results,evaluations,for,multi-word triplets,evaluations for multi-word triplets,0.6219040155410767
translation,60,155,results,our model,achieves,more significant improvements,our model achieves more significant improvements,0.6404632329940796
translation,60,155,results,more significant improvements,for,f 1 scores,more significant improvements for f 1 scores,0.6469207406044006
translation,60,155,results,evaluations,has,our model,evaluations has our model,0.6188586354255676
translation,60,155,results,multi-word triplets,has,our model,multi-word triplets has our model,0.5643665790557861
translation,60,155,results,results,compare,evaluations,results compare evaluations,0.6524370908737183
translation,60,156,results,our recall,shows,greater improvement,our recall shows greater improvement,0.6766737103462219
translation,60,156,results,greater improvement,over,gts approach,greater improvement over gts approach,0.7126138210296631
translation,60,156,results,precision,has,our recall,precision has our recall,0.6068307161331177
translation,60,156,results,results,Compared to,precision,results Compared to precision,0.6853222250938416
translation,60,158,results,our span enumeration,naturally benefits,recall,our span enumeration naturally benefits recall,0.7560834884643555
translation,60,158,results,recall,of,multi-word spans,recall of multi-word spans,0.5732074975967407
translation,60,158,results,results,has,our span enumeration,results has our span enumeration,0.5751441121101379
translation,60,208,results,significantly outperforms,for,aste,significantly outperforms for aste,0.6598395705223083
translation,60,208,results,previous methods,for,aste,previous methods for aste,0.6483427882194519
translation,60,208,results,significantly outperforms,has,previous methods,significantly outperforms has previous methods,0.5611478686332703
translation,60,209,results,strong performance,on,aste task,strong performance on aste task,0.5773632526397705
translation,60,209,results,performance,attributed to,improvement,performance attributed to improvement,0.6878548264503479
translation,60,209,results,improvement,on,multi-word triplets,improvement on multi-word triplets,0.5545921325683594
translation,60,209,results,strong performance,has,performance,strong performance has performance,0.6060401797294617
translation,60,209,results,results,achieve,strong performance,results achieve strong performance,0.6931333541870117
translation,61,163,ablation-analysis,peer attention model,applied to,best representations,peer attention model applied to best representations,0.6868752837181091
translation,61,163,ablation-analysis,best representations,boost them,even more,best representations boost them even more,0.6619125604629517
translation,61,163,ablation-analysis,ablation analysis,has,peer attention model,ablation analysis has peer attention model,0.5288307666778564
translation,61,5,baselines,ava,uses,transformer - based language models,ava uses transformer - based language models,0.5706834197044373
translation,61,5,baselines,transformer - based language models,to encode,"question , answer , and reference texts","transformer - based language models to encode question , answer , and reference texts",0.6797321438789368
translation,61,5,baselines,baselines,has,ava,baselines has ava,0.5687152147293091
translation,61,35,experiments,point-wise estimation,of,qa system output,point-wise estimation of qa system output,0.6315008401870728
translation,61,138,hyperparameters,roberta - base,as,initial pre-trained model,roberta - base as initial pre-trained model,0.5315200686454773
translation,61,138,hyperparameters,initial pre-trained model,for,each b instance,initial pre-trained model for each b instance,0.5866754055023193
translation,61,138,hyperparameters,initial pre-trained model,with,default hyper-parameter setting,initial pre-trained model with default hyper-parameter setting,0.5961832404136658
translation,61,138,hyperparameters,default hyper-parameter setting,of,glue trainings,default hyper-parameter setting of glue trainings,0.5838454961776733
translation,61,138,hyperparameters,adamw variant,),optimizer,adamw variant ) optimizer,0.5659360289573669
translation,61,138,hyperparameters,adamw variant,as,optimizer,adamw variant as optimizer,0.5081870555877686
translation,61,138,hyperparameters,learning rate,of,1e - 06,learning rate of 1e - 06,0.6203526258468628
translation,61,138,hyperparameters,1e - 06,for,fine-tuning exercises,1e - 06 for fine-tuning exercises,0.6362074017524719
translation,61,138,hyperparameters,maximum sequence length,set to,128,maximum sequence length set to 128,0.6947898864746094
translation,61,139,hyperparameters,number of iterations,is,two,number of iterations is two,0.612697184085846
translation,61,139,hyperparameters,hyperparameters,has,number of iterations,hyperparameters has number of iterations,0.5118768215179443
translation,61,34,model,several baselines,based on,pre-trained transformer models,several baselines based on pre-trained transformer models,0.6201525330543518
translation,61,34,model,pre-trained transformer models,to encode,triple,pre-trained transformer models to encode triple,0.7512906193733215
translation,61,34,model,new attention mechanism,to model,interaction,new attention mechanism to model interaction,0.7185190916061401
translation,61,34,model,interaction,between,t and r,interaction between t and r,0.7144364714622498
translation,61,34,model,new attention mechanism,has,peer attention,new attention mechanism has peer attention,0.5489632487297058
translation,61,34,model,model,build,several baselines,model build several baselines,0.7486685514450073
translation,61,34,model,model,propose,new attention mechanism,model propose new attention mechanism,0.6747162938117981
translation,61,37,results,ava,rank,systems,ava rank systems,0.7300755381584167
translation,61,37,results,systems,in terms of,accuracy,systems in terms of accuracy,0.6934451460838318
translation,61,37,results,results,has,ava,results has ava,0.5655847191810608
translation,61,188,results,error,ranging from,2 to 4.1 points,error ranging from 2 to 4.1 points,0.6273473501205444
translation,61,188,results,trec - qa test set,has,ava,trec - qa test set has ava,0.6027982234954834
translation,61,188,results,ava,has,error,ava has error,0.5520049929618835
translation,61,188,results,results,On,trec - qa test set,results On trec - qa test set,0.548128604888916
translation,61,189,results,higher,reaching,9.5 %,higher reaching 9.5 %,0.714726448059082
translation,61,189,results,wikiqa test set,has,error,wikiqa test set has error,0.552680253982544
translation,61,189,results,results,On,wikiqa test set,results On wikiqa test set,0.5386223196983337
translation,61,233,results,much more accurate,than,overfitted reranker,much more accurate than overfitted reranker,0.6156213879585266
translation,61,233,results,overfitted reranker,has,66 % versus 25 %,overfitted reranker has 66 % versus 25 %,0.5636593699455261
translation,61,233,results,results,has,ava,results has ava,0.5655847191810608
translation,62,156,baselines,lang - qap,is,language-only ( video-blind ) model,lang - qap is language-only ( video-blind ) model,0.5617920756340027
translation,62,156,baselines,language-only ( video-blind ) model,using,query input,language-only ( video-blind ) model using query input,0.6314498782157898
translation,62,156,baselines,baselines,has,lang - qap,baselines has lang - qap,0.6078855395317078
translation,62,159,baselines,baselines,has,butd -qap,baselines has butd -qap,0.6237136721611023
translation,62,16,model,input,to,vidqap,input to vidqap,0.600758969783783
translation,62,16,model,vidqap,consists of,query expression,vidqap consists of query expression,0.6217182874679565
translation,62,16,model,query expression,with,query-token,query expression with query-token,0.6437632441520691
translation,62,33,model,vidqap,extend,three visionlanguage models,vidqap extend three visionlanguage models,0.6393567323684692
translation,62,33,model,three visionlanguage models,namely,"bottom - up-top- down ( anderson et al. , 2018 )","three visionlanguage models namely bottom - up-top- down ( anderson et al. , 2018 )",0.6284610629081726
translation,62,33,model,three visionlanguage models,namely,"vognet ( sadhu et al. , 2020 )","three visionlanguage models namely vognet ( sadhu et al. , 2020 )",0.6419515609741211
translation,62,33,model,three visionlanguage models,namely,multi-modal transformer,three visionlanguage models namely multi-modal transformer,0.6635572910308838
translation,62,33,model,multi-modal transformer,by replacing,classification heads,multi-modal transformer by replacing classification heads,0.57927006483078
translation,62,33,model,classification heads,with,"transformer ( vaswani et al. , 2017 ) based language decoder","classification heads with transformer ( vaswani et al. , 2017 ) based language decoder",0.6098660230636597
translation,62,33,model,model,To investigate,vidqap,model To investigate vidqap,0.6485010385513306
translation,62,36,results,certain roles,hardly benefit from,visionlanguage models,certain roles hardly benefit from visionlanguage models,0.6596257090568542
translation,62,36,results,visionlanguage models,suggesting room for,improvement,visionlanguage models suggesting room for improvement,0.690361738204956
translation,62,36,results,results,has,certain roles,results has certain roles,0.5825421214103699
translation,62,201,results,n-way classification baselines,on,asrl - qa and charades - srl - qa,n-way classification baselines on asrl - qa and charades - srl - qa,0.5405194759368896
translation,62,203,results,r- bertscore,shows,higher relative improvement,r- bertscore shows higher relative improvement,0.7035718560218811
translation,62,203,results,other metrics,has,r- bertscore,other metrics has r- bertscore,0.5793989300727844
translation,62,203,results,results,compared to,other metrics,results compared to other metrics,0.6189091205596924
translation,62,206,results,performance,on,both datasets,performance on both datasets,0.4974806010723114
translation,62,206,results,performance,follow,very similar trends,performance follow very similar trends,0.6417884230613708
translation,62,206,results,very similar trends,across,all metrics,very similar trends across all metrics,0.6945637464523315
translation,62,206,results,results,find that,performance,results find that performance,0.6555676460266113
translation,62,207,results,slightly higher scores,compared to,asrl - qa,slightly higher scores compared to asrl - qa,0.696185827255249
translation,62,207,results,charades -srl -qa,has,slightly higher scores,charades -srl -qa has slightly higher scores,0.5680089592933655
translation,62,207,results,results,has,charades -srl -qa,results has charades -srl -qa,0.5334880352020264
translation,62,208,results,results,has,comparison within n-way classification,results has comparison within n-way classification,0.5642409324645996
translation,62,209,results,fixed set of phrases,used,classification models,fixed set of phrases used classification models,0.6396241188049316
translation,62,209,results,classification models,show,very limited performance,classification models show very limited performance,0.6131704449653625
translation,62,209,results,1 k,has,fixed set of phrases,1 k has fixed set of phrases,0.5947731733322144
translation,62,209,results,results,when,1 k,results when 1 k,0.6929494142532349
translation,62,210,results,10 k phrases,gives,significant improvement,10 k phrases gives significant improvement,0.6748366951942444
translation,62,210,results,significant improvement,in,performance,significant improvement in performance,0.5336322784423828
translation,62,210,results,performance,on,charades - srl - qa,performance on charades - srl - qa,0.577967643737793
translation,62,210,results,results,Allowing,10 k phrases,results Allowing 10 k phrases,0.6967567205429077
translation,62,215,results,outperform,namely,lang -cl,outperform namely lang -cl,0.7644185423851013
translation,62,215,results,classification counterparts,namely,lang -cl,classification counterparts namely lang -cl,0.7322572469711304
translation,62,215,results,classification counterparts,namely,mtx - cl,classification counterparts namely mtx - cl,0.7350579500198364
translation,62,215,results,lang-qap and mtx - qap,has,outperform,lang-qap and mtx - qap has outperform,0.6418797373771667
translation,62,215,results,outperform,has,classification counterparts,outperform has classification counterparts,0.5949836373329163
translation,62,215,results,results,shows,lang-qap and mtx - qap,results shows lang-qap and mtx - qap,0.645083487033844
translation,62,215,results,results,shows,outperform,results shows outperform,0.7696700096130371
translation,62,218,results,multi-modal models,has,outperform,multi-modal models has outperform,0.5982056260108948
translation,62,218,results,outperform,has,language-only baseline,outperform has language-only baseline,0.5919138193130493
translation,62,218,results,results,find that,multi-modal models,results find that multi-modal models,0.6105462908744812
translation,63,6,experiments,vision - language question answering task,based on,"clevr ( johnson et al. , 2017a ) dataset","vision - language question answering task based on clevr ( johnson et al. , 2017a ) dataset",0.5647618174552917
translation,63,5,model,model,take,visual understanding,model take visual understanding,0.6412723660469055
translation,64,256,ablation-analysis,bert ( frozen ) and w2v variants,are,weaker,bert ( frozen ) and w2v variants are weaker,0.602854311466217
translation,64,256,ablation-analysis,ablation analysis,has,bert ( frozen ) and w2v variants,ablation analysis has bert ( frozen ) and w2v variants,0.5541306138038635
translation,64,276,ablation-analysis,both forward and backward transfers,are,effective,both forward and backward transfers are effective,0.6026939749717712
translation,64,276,ablation-analysis,ablation analysis,see,both forward and backward transfers,ablation analysis see both forward and backward transfers,0.6038540601730347
translation,64,279,ablation-analysis,each of the components,is,effective,each of the components is effective,0.6032114028930664
translation,64,279,ablation-analysis,each of the components,work,in concert,each of the components work in concert,0.6175940632820129
translation,64,279,ablation-analysis,in concert,to produce,best final result,in concert to produce best final result,0.7318052053451538
translation,64,279,ablation-analysis,ablation analysis,shows,each of the components,ablation analysis shows each of the components,0.6427532434463501
translation,64,203,baselines,bert,with,fine-tuning,bert with fine-tuning,0.6896287798881531
translation,64,203,baselines,bert ( frozen ),without finetuning,adapter-bert,bert ( frozen ) without finetuning adapter-bert,0.7585071921348572
translation,64,203,baselines,w2v,trained with,amazon review data,w2v trained with amazon review data,0.7338240146636963
translation,64,203,baselines,w2v,using,"fasttext ( grave et al. , 2018 )","w2v using fasttext ( grave et al. , 2018 )",0.6073086261749268
translation,64,203,baselines,word2vec embeddings,trained with,amazon review data,word2vec embeddings trained with amazon review data,0.6618720889091492
translation,64,203,baselines,amazon review data,using,"fasttext ( grave et al. , 2018 )","amazon review data using fasttext ( grave et al. , 2018 )",0.6401327252388
translation,64,203,baselines,adapter-bert,has,"houlsby et al. , 2019 )","adapter-bert has houlsby et al. , 2019 )",0.6043873429298401
translation,64,203,baselines,w2v,has,word2vec embeddings,w2v has word2vec embeddings,0.5312937498092651
translation,64,204,baselines,csc,creates,another 4 variants,csc creates another 4 variants,0.6860532760620117
translation,64,204,baselines,contrastive supervised learning,of,current task,contrastive supervised learning of current task,0.5625358819961548
translation,64,204,baselines,csc,has,contrastive supervised learning,csc has contrastive supervised learning,0.539366602897644
translation,64,205,baselines,asc network,taking,aspect term and review sentence,asc network taking aspect term and review sentence,0.6422182321548462
translation,64,205,baselines,aspect term and review sentence,as,input,aspect term and review sentence as input,0.49519234895706177
translation,64,205,baselines,input,for,bert variants,input for bert variants,0.6873489022254944
translation,64,206,baselines,w2v variants,use,concatenation,w2v variants use concatenation,0.6444248557090759
translation,64,207,baselines,baselines,has,continual learning ( cl ),baselines has continual learning ( cl ),0.5845546722412109
translation,64,208,baselines,38 baselines,in,5 categories,38 baselines in 5 categories,0.4990047216415405
translation,64,208,baselines,cl setting,has,38 baselines,cl setting has 38 baselines,0.5829006433486938
translation,64,208,baselines,baselines,has,cl setting,baselines has cl setting,0.531460165977478
translation,64,215,baselines,original image classification networks,with,cnn,original image classification networks with cnn,0.598374605178833
translation,64,215,baselines,cnn,for,text classification,cnn for text classification,0.5836858153343201
translation,64,217,baselines,ucl,is,recent til method,ucl is recent til method,0.6094633340835571
translation,64,217,baselines,baselines,has,ucl,baselines has ucl,0.6063944697380066
translation,64,235,hyperparameters,adapter,uses,2 layers of fully connected network,adapter uses 2 layers of fully connected network,0.6372610330581665
translation,64,235,hyperparameters,2 layers of fully connected network,dimensions,2000,2 layers of fully connected network dimensions 2000,0.6315926313400269
translation,64,235,hyperparameters,hyperparameters,has,adapter,hyperparameters has adapter,0.5577154755592346
translation,64,236,hyperparameters,task id embeddings,have,2000 dimensions,task id embeddings have 2000 dimensions,0.5407152771949768
translation,64,236,hyperparameters,hyperparameters,has,task id embeddings,hyperparameters has task id embeddings,0.4907762408256531
translation,64,238,hyperparameters,400,for,s max,400 for s max,0.6603901982307434
translation,64,238,hyperparameters,400,for,dropout,400 for dropout,0.6870142221450806
translation,64,238,hyperparameters,dropout,of,0.5,dropout of 0.5,0.6125851273536682
translation,64,238,hyperparameters,0.5,between,fully connected layers,0.5 between fully connected layers,0.6038256883621216
translation,64,238,hyperparameters,hyperparameters,use,400,hyperparameters use 400,0.6339437961578369
translation,64,238,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,64,239,hyperparameters,temperature,in,each contrastive objective,temperature in each contrastive objective,0.5396864414215088
translation,64,239,hyperparameters,each contrastive objective,set to,1,each contrastive objective set to 1,0.7198143601417542
translation,64,239,hyperparameters,hyperparameters,has,temperature,hyperparameters has temperature,0.5350812077522278
translation,64,240,hyperparameters,weight,of,each objective,weight of each objective,0.5895471572875977
translation,64,240,hyperparameters,each objective,set to,1,each objective set to 1,0.6929543018341064
translation,64,240,hyperparameters,hyperparameters,has,weight,hyperparameters has weight,0.5009066462516785
translation,64,245,hyperparameters,max length,of,sum of sentence and aspect,max length of sum of sentence and aspect,0.5790707468986511
translation,64,245,hyperparameters,sum of sentence and aspect,is,128,sum of sentence and aspect is 128,0.579586923122406
translation,64,245,hyperparameters,hyperparameters,has,max length,hyperparameters has max length,0.5074495077133179
translation,64,246,hyperparameters,adam optimizer,set,learning rate,adam optimizer set learning rate,0.6309201717376709
translation,64,246,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,64,247,hyperparameters,semeval datasets,for,all other datasets,semeval datasets for all other datasets,0.5257278084754944
translation,64,247,hyperparameters,30 epochs,used,results,30 epochs used results,0.6193733215332031
translation,64,247,hyperparameters,30 epochs,based on,results,30 epochs based on results,0.6440513134002686
translation,64,247,hyperparameters,results,from,validation data,results from validation data,0.548320472240448
translation,64,247,hyperparameters,semeval datasets,has,10 epochs,semeval datasets has 10 epochs,0.5844972133636475
translation,64,247,hyperparameters,semeval datasets,has,30 epochs,semeval datasets has 30 epochs,0.5780022740364075
translation,64,247,hyperparameters,all other datasets,has,30 epochs,all other datasets has 30 epochs,0.5683358907699585
translation,64,247,hyperparameters,hyperparameters,For,semeval datasets,hyperparameters For semeval datasets,0.5243437886238098
translation,64,247,hyperparameters,hyperparameters,for,all other datasets,hyperparameters for all other datasets,0.47799068689346313
translation,64,248,hyperparameters,runs,use,batch size,runs use batch size,0.6805530786514282
translation,64,248,hyperparameters,batch size,has,32,batch size has 32,0.630264163017273
translation,64,248,hyperparameters,hyperparameters,has,runs,hyperparameters has runs,0.5537269711494446
translation,64,249,hyperparameters,cl baselines,train,all models,cl baselines train all models,0.6892093420028687
translation,64,249,hyperparameters,all models,with,learning rate,all models with learning rate,0.60528564453125
translation,64,249,hyperparameters,all models,with,early - stop training,all models with early - stop training,0.6125345826148987
translation,64,249,hyperparameters,all models,set,batch size,all models set batch size,0.7037363052368164
translation,64,249,hyperparameters,learning rate,of,0.05,learning rate of 0.05,0.6214359998703003
translation,64,249,hyperparameters,early - stop training,when,no improvement,early - stop training when no improvement,0.6379125714302063
translation,64,249,hyperparameters,no improvement,in,validation loss,no improvement in validation loss,0.4878521263599396
translation,64,249,hyperparameters,validation loss,for,5 epochs,validation loss for 5 epochs,0.5681278705596924
translation,64,249,hyperparameters,hyperparameters,For,cl baselines,hyperparameters For cl baselines,0.5964531898498535
translation,64,8,model,novel model,called,classic,novel model called classic,0.7164389491081238
translation,64,8,model,model,proposes,novel model,model proposes novel model,0.7164400219917297
translation,64,9,model,key novelty,is,contrastive continual learning method,key novelty is contrastive continual learning method,0.5120899081230164
translation,64,9,model,contrastive continual learning method,enables,knowledge transfer,contrastive continual learning method enables knowledge transfer,0.6422131657600403
translation,64,9,model,contrastive continual learning method,enables,knowledge distillation,contrastive continual learning method enables knowledge distillation,0.6401556730270386
translation,64,9,model,knowledge transfer,across,tasks,knowledge transfer across tasks,0.6109059453010559
translation,64,9,model,knowledge distillation,from,old tasks,knowledge distillation from old tasks,0.5357193946838379
translation,64,9,model,model,has,key novelty,model has key novelty,0.5159850716590881
translation,64,37,model,novel model,called,classic,novel model called classic,0.7164389491081238
translation,64,37,model,novel model,in,dil setting,novel model in dil setting,0.5857995748519897
translation,64,37,model,continual and contrastive learning,for,aspect sentiment classification,continual and contrastive learning for aspect sentiment classification,0.6144641637802124
translation,64,37,model,classic,has,continual and contrastive learning,classic has continual and contrastive learning,0.5895887613296509
translation,64,37,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,64,44,model,new model,called,classic,new model called classic,0.7334461212158203
translation,64,44,model,new model,uses,adapters,new model uses adapters,0.6215675473213196
translation,64,44,model,adapters,to incorporate,pretrained bert,adapters to incorporate pretrained bert,0.7020784616470337
translation,64,44,model,pretrained bert,into,asc continual learning,pretrained bert into asc continual learning,0.6388400197029114
translation,64,44,model,novel contrastive continual learning method,for,knowledge transfer and distillation,novel contrastive continual learning method for knowledge transfer and distillation,0.5947847366333008
translation,64,44,model,task masks,to isolate,task -specific knowledge,task masks to isolate task -specific knowledge,0.6905819177627563
translation,64,44,model,task -specific knowledge,to avoid,cf,task -specific knowledge to avoid cf,0.6976957321166992
translation,64,44,model,model,proposes,new model,model proposes new model,0.7315190434455872
translation,64,44,model,model,called,classic,model called classic,0.73288893699646
translation,64,91,model,classic,uses,three sub-systems,classic uses three sub-systems,0.6446373462677002
translation,64,91,model,contrastive ensemble distillation,for mitigating,cf,contrastive ensemble distillation for mitigating cf,0.7023903131484985
translation,64,91,model,knowledge,of,previous tasks,knowledge of previous tasks,0.5323836207389832
translation,64,91,model,knowledge,to,current task model,knowledge to current task model,0.5062599778175354
translation,64,91,model,previous tasks,to,current task model,previous tasks to current task model,0.5148934721946716
translation,64,91,model,contrastive knowledge sharing ( cks ),to encourage,knowledge transfer,contrastive knowledge sharing ( cks ) to encourage knowledge transfer,0.7008083462715149
translation,64,91,model,contrastive supervised learning,on,current task model ( csc ),contrastive supervised learning on current task model ( csc ),0.5712440609931946
translation,64,91,model,contrastive supervised learning,to improve,current task model accuracy,contrastive supervised learning to improve current task model accuracy,0.635350227355957
translation,64,91,model,current task model ( csc ),to improve,current task model accuracy,current task model ( csc ) to improve current task model accuracy,0.6345348358154297
translation,64,91,model,contrastive ensemble distillation,has,ced ),contrastive ensemble distillation has ced ),0.6175643801689148
translation,64,91,model,model,uses,three sub-systems,model uses three sub-systems,0.5931350588798523
translation,64,91,model,model,has,classic,model has classic,0.5846225619316101
translation,64,198,model,dil,adapt,recent til systems,dil adapt recent til systems,0.810191810131073
translation,64,198,model,dil,by merging,classification heads,dil by merging classification heads,0.7636361122131348
translation,64,198,model,recent til systems,to,dil,recent til systems to dil,0.6408712863922119
translation,64,198,model,dil,by merging,classification heads,dil by merging classification heads,0.7636361122131348
translation,64,198,model,classification heads,to form,dil systems,classification heads to form dil systems,0.6715396642684937
translation,64,198,model,model,adapt,recent til systems,model adapt recent til systems,0.7559109330177307
translation,64,237,model,fully connected layer,with,softmax output,fully connected layer with softmax output,0.5902058482170105
translation,64,237,model,softmax output,used as,classification head,softmax output used as classification head,0.6461495757102966
translation,64,237,model,classification head,in,last layer of bert,classification head in last layer of bert,0.5743628144264221
translation,64,237,model,model,has,fully connected layer,model has fully connected layer,0.5389924645423889
translation,64,254,results,classic,has,outperforms,classic has outperforms,0.6287333369255066
translation,64,254,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,64,254,results,all baselines,has,markedly,all baselines has markedly,0.5953255891799927
translation,64,254,results,results,shows,classic,results shows classic,0.49953052401542664
translation,64,255,results,adapter - bert,performs,similarly,adapter - bert performs similarly,0.7055947184562683
translation,64,255,results,similarly,to,bert,similarly to bert,0.6434803605079651
translation,64,255,results,non-continual learning baselines,has,adapter - bert,non-continual learning baselines has adapter - bert,0.6144148707389832
translation,64,255,results,results,For,non-continual learning baselines,results For non-continual learning baselines,0.5887065529823303
translation,64,257,results,one variants and ncl variants,see that,w2v,one variants and ncl variants see that w2v,0.6517987251281738
translation,64,257,results,one variants and ncl variants,under,w2v,one variants and ncl variants under w2v,0.6426141858100891
translation,64,257,results,ncl variants,much bet-ter than,one variants,ncl variants much bet-ter than one variants,0.618475615978241
translation,64,257,results,one variants and ncl variants,has,ncl variants,one variants and ncl variants has ncl variants,0.569621205329895
translation,64,257,results,w2v,has,ncl variants,w2v has ncl variants,0.5848621129989624
translation,64,257,results,results,Comparing,one variants and ncl variants,results Comparing one variants and ncl variants,0.6010481119155884
translation,64,263,results,various continual learning ( cl ) baselines,with,bert ( frozen ),various continual learning ( cl ) baselines with bert ( frozen ),0.6410025358200073
translation,64,263,results,markedly weaker,than,classic,markedly weaker than classic,0.6137980222702026
translation,64,263,results,results,has,various continual learning ( cl ) baselines,results has various continual learning ( cl ) baselines,0.5260397791862488
translation,64,266,results,w2v based cl baselines,are,even weaker,w2v based cl baselines are even weaker,0.5405018925666809
translation,64,266,results,results,has,w2v based cl baselines,results has w2v based cl baselines,0.5374657511711121
translation,64,282,results,classic,has,outperforms,classic has outperforms,0.6287333369255066
translation,64,282,results,outperforms,has,state - of - the - art baselines,outperforms has state - of - the - art baselines,0.5583442449569702
translation,65,188,ablation-analysis,representation model,for,column classifier,representation model for column classifier,0.6409322023391724
translation,65,188,ablation-analysis,representation model,reduces,performance,representation model reduces performance,0.6878907084465027
translation,65,188,ablation-analysis,performance,by,two percent,performance by two percent,0.6412039399147034
translation,65,188,ablation-analysis,performance,by,less than three percent,performance by less than three percent,0.6107740998268127
translation,65,188,ablation-analysis,performance,by,up to seven percent,performance by up to seven percent,0.6152383685112
translation,65,188,ablation-analysis,performance,less than,two percent,performance less than two percent,0.6707556843757629
translation,65,188,ablation-analysis,two percent,on,wikisql,two percent on wikisql,0.5765411853790283
translation,65,188,ablation-analysis,less than three percent,on,tabmcq,less than three percent on tabmcq,0.6270323395729065
translation,65,188,ablation-analysis,up to seven percent,on,wikitablequestions,up to seven percent on wikitablequestions,0.5752981305122375
translation,65,188,ablation-analysis,ablation analysis,Using,representation model,ablation analysis Using representation model,0.6735730171203613
translation,65,198,experiments,mrc,on,wikisql benchmark,mrc on wikisql benchmark,0.5372568368911743
translation,65,198,experiments,row prediction,is,more accurate,row prediction is more accurate,0.5871528387069702
translation,65,198,experiments,more accurate,than,column prediction,more accurate than column prediction,0.5607819557189941
translation,65,198,experiments,mrc,has,row prediction,mrc has row prediction,0.5991109609603882
translation,65,198,experiments,wikisql benchmark,has,row prediction,wikisql benchmark has row prediction,0.5617760419845581
translation,65,6,model,model,propose,two novel approaches,model propose two novel approaches,0.7303816080093384
translation,65,7,model,rci interaction,leverages,transformer based architecture,rci interaction leverages transformer based architecture,0.7372463345527649
translation,65,7,model,transformer based architecture,independently classifies,rows and columns,transformer based architecture independently classifies rows and columns,0.7631517052650452
translation,65,7,model,rows and columns,to identify,relevant cells,rows and columns to identify relevant cells,0.6637727618217468
translation,65,7,model,model,called,rci interaction,model called rci interaction,0.7004464268684387
translation,65,21,model,new approach,to,table qa,new approach to table qa,0.6232290267944336
translation,65,21,model,new approach,independently predicts,probability,new approach independently predicts probability,0.7766625285148621
translation,65,21,model,probability,of containing,answer,probability of containing answer,0.7087518572807312
translation,65,21,model,answer,to,question,answer to question,0.6327336430549622
translation,65,21,model,question,in,each row and column,question in each row and column,0.5831618309020996
translation,65,21,model,model,propose,new approach,model propose new approach,0.7215121984481812
translation,65,22,model,row and column intersection ( rci ),of,probabilistic predictions,row and column intersection ( rci ) of probabilistic predictions,0.5931995511054993
translation,65,22,model,row and column intersection ( rci ),gives,probability,row and column intersection ( rci ) gives probability,0.6163878440856934
translation,65,22,model,rci,gives,probability,rci gives probability,0.667920708656311
translation,65,22,model,probability,for,each cell,probability for each cell,0.6396989822387695
translation,65,22,model,each cell,of,table,each cell of table,0.6169537901878357
translation,65,22,model,row and column intersection ( rci ),has,rci,row and column intersection ( rci ) has rci,0.5825573205947876
translation,65,22,model,probabilistic predictions,has,rci,probabilistic predictions has rci,0.6051397919654846
translation,65,22,model,model,taking,row and column intersection ( rci ),model taking row and column intersection ( rci ),0.7152097225189209
translation,65,35,model,squad,then on,natural questions,squad then on natural questions,0.6636191606521606
translation,65,38,model,model,for,table qa task,model for table qa task,0.6141225099563599
translation,65,38,model,model,concatenates,textual representation,model concatenates textual representation,0.6699580550193787
translation,65,38,model,textual representation,of,each row ( or column ),textual representation of each row ( or column ),0.5930555462837219
translation,65,38,model,textual representation,classifies,sequence pair,textual representation classifies sequence pair,0.6515862345695496
translation,65,38,model,each row ( or column ),to,text,each row ( or column ) to text,0.5920904278755188
translation,65,38,model,text,of,question,text of question,0.6178677678108215
translation,65,38,model,sequence pair,as,positive,sequence pair as positive,0.6095650792121887
translation,65,38,model,sequence pair,as,negative,sequence pair as negative,0.6094285845756531
translation,65,38,model,model,propose,model,model propose model,0.6740307211875916
translation,65,38,model,model,for,table qa task,model for table qa task,0.6141225099563599
translation,65,61,model,lstm encoderdecoder model,where,tables,lstm encoderdecoder model where tables,0.6327582597732544
translation,65,61,model,first converted,to,knowledge - graph,first converted to knowledge - graph,0.5808365941047668
translation,65,61,model,first converted,to,word tokens,first converted to word tokens,0.551520824432373
translation,65,61,model,word tokens,in,questions,word tokens in questions,0.5297849774360657
translation,65,62,model,questions and linked table entities,encoded into,representation vectors,questions and linked table entities encoded into representation vectors,0.714800238609314
translation,65,62,model,representation vectors,decoded to,executable ?-,representation vectors decoded to executable ?-,0.7451181411743164
translation,65,62,model,executable ?-,has,dcs logical forms,executable ?- has dcs logical forms,0.6004434823989868
translation,65,62,model,model,has,questions and linked table entities,model has questions and linked table entities,0.6173447370529175
translation,65,8,results,cell values,on,recent benchmarks,cell values on recent benchmarks,0.49422508478164673
translation,65,8,results,significant efficiency advantage,for,online qa systems,significant efficiency advantage for online qa systems,0.6210353970527649
translation,65,8,results,significant efficiency advantage,by materializing,embeddings,significant efficiency advantage by materializing embeddings,0.7395016551017761
translation,65,8,results,online qa systems,over,tables,online qa systems over tables,0.6396653056144714
translation,65,8,results,embeddings,for,existing tables,embeddings for existing tables,0.6275597214698792
translation,65,8,results,results,called,rci representation,results called rci representation,0.6443313360214233
translation,65,30,results,outperforms,achieving,?3.4 % and ?18.86 % additional precision improvement,outperforms achieving ?3.4 % and ?18.86 % additional precision improvement,0.6429881453514099
translation,65,30,results,state - of - the - art transformer based approaches,achieving,?3.4 % and ?18.86 % additional precision improvement,state - of - the - art transformer based approaches achieving ?3.4 % and ?18.86 % additional precision improvement,0.609451949596405
translation,65,30,results,?3.4 % and ?18.86 % additional precision improvement,on,standard wik-isql benchmark,?3.4 % and ?18.86 % additional precision improvement on standard wik-isql benchmark,0.5137985944747925
translation,65,30,results,?3.4 % and ?18.86 % additional precision improvement,containing,lookup and aggregation questions,?3.4 % and ?18.86 % additional precision improvement containing lookup and aggregation questions,0.6700804829597473
translation,65,30,results,interaction model,has,outperforms,interaction model has outperforms,0.6426767706871033
translation,65,30,results,outperforms,has,state - of - the - art transformer based approaches,outperforms has state - of - the - art transformer based approaches,0.5840674042701721
translation,65,30,results,results,has,interaction model,results has interaction model,0.5140554904937744
translation,65,175,results,representation model,found,comparable performance,representation model found comparable performance,0.6339083313941956
translation,65,175,results,representation model,found,much lower performance,representation model found much lower performance,0.6171501874923706
translation,65,175,results,comparable performance,on,column classifier,comparable performance on column classifier,0.5495678782463074
translation,65,175,results,comparable performance,on,row classifier,comparable performance on row classifier,0.5549712181091309
translation,65,175,results,much lower performance,on,row classifier,much lower performance on row classifier,0.540363609790802
translation,65,175,results,results,For,representation model,results For representation model,0.6340265870094299
translation,65,186,results,lookup versions,of,wikisql,lookup versions of wikisql,0.5736832022666931
translation,65,186,results,lookup versions,of,tabmcq,lookup versions of tabmcq,0.6276492476463318
translation,65,186,results,lookup versions,of,wikitablequestions,lookup versions of wikitablequestions,0.5620877742767334
translation,65,186,results,results,on,lookup versions,results on lookup versions,0.5653499960899353
translation,65,187,results,interaction and the representation models,of,rci,interaction and the representation models of rci,0.608933687210083
translation,65,187,results,all other methods,on,wikisql,all other methods on wikisql,0.49259695410728455
translation,65,187,results,all other methods,on,tabmcq,all other methods on tabmcq,0.5035842061042786
translation,65,187,results,all other methods,on,wikitablequestions,all other methods on wikitablequestions,0.513929009437561
translation,65,187,results,rci,has,outperform,rci has outperform,0.6762089133262634
translation,65,187,results,outperform,has,all other methods,outperform has all other methods,0.5582208037376404
translation,65,187,results,results,has,interaction and the representation models,results has interaction and the representation models,0.5305460691452026
translation,65,189,results,outperform,with,computational cost,outperform with computational cost,0.6787654757499695
translation,65,189,results,mrc xxl,with,far fewer parameters,mrc xxl with far fewer parameters,0.6536400318145752
translation,65,189,results,mrc xxl,with,computational cost,mrc xxl with computational cost,0.6160160303115845
translation,65,189,results,outperform,has,mrc xxl,outperform has mrc xxl,0.6544551253318787
translation,65,189,results,results,On,two of the three datasets,results On two of the three datasets,0.48310062289237976
translation,65,190,results,rci,with,albert - base,rci with albert - base,0.6832436919212341
translation,65,190,results,outperforms,large version of,tapas,outperforms large version of tapas,0.7271900773048401
translation,65,190,results,tapas,trained on,wikisql,tapas trained on wikisql,0.7227839827537537
translation,65,190,results,94.6 % hit@1,compared to,89.43 % hit@1,94.6 % hit@1 compared to 89.43 % hit@1,0.653762698173523
translation,65,190,results,89.43 % hit@1,of,tapas large,89.43 % hit@1 of tapas large,0.5782203674316406
translation,65,190,results,albert - base,has,outperforms,albert - base has outperforms,0.6259034872055054
translation,65,190,results,results,has,rci,results has rci,0.5690315365791321
translation,65,201,results,rci inter model,is,best,rci inter model is best,0.5756218433380127
translation,65,201,results,best,at,both tasks,best at both tasks,0.5150783658027649
translation,65,201,results,rci,having,same performance,rci having same performance,0.6927276253700256
translation,65,201,results,same performance,at,row level task,same performance at row level task,0.5747276544570923
translation,65,201,results,results,has,rci inter model,results has rci inter model,0.5507699251174927
translation,65,202,results,tabmcq column level performance,of,mrc,tabmcq column level performance of mrc,0.6288072466850281
translation,65,202,results,two percent,of,rci inter,two percent of rci inter,0.7164127826690674
translation,65,202,results,results,has,tabmcq column level performance,results has tabmcq column level performance,0.5696037411689758
translation,66,147,ablation-analysis,manifest performance degradation,after removing,part of the mi loss,manifest performance degradation after removing part of the mi loss,0.6968626379966736
translation,66,147,ablation-analysis,results,are,even worse,results are even worse,0.6078733801841736
translation,66,147,ablation-analysis,even worse,when removing,all terms,even worse when removing all terms,0.732720673084259
translation,66,147,ablation-analysis,all terms,in,one loss,all terms in one loss,0.5124179124832153
translation,66,147,ablation-analysis,ablation analysis,note,manifest performance degradation,ablation analysis note manifest performance degradation,0.6069875359535217
translation,66,148,ablation-analysis,current optimization target pairs,in,inter-modality mi,current optimization target pairs in inter-modality mi,0.5092269778251648
translation,66,148,ablation-analysis,inter-modality mi,with,single pair,inter-modality mi with single pair,0.6680952906608582
translation,66,148,ablation-analysis,inter-modality mi,with,other pair combinations,inter-modality mi with other pair combinations,0.6673215627670288
translation,66,148,ablation-analysis,ablation analysis,replacing,current optimization target pairs,ablation analysis replacing current optimization target pairs,0.6336864829063416
translation,66,121,baselines,low-rank multimodal fusion,decomposes,stacked high-order tensors,low-rank multimodal fusion decomposes stacked high-order tensors,0.7636279463768005
translation,66,121,baselines,low-rank multimodal fusion,performs,efficient fusion,low-rank multimodal fusion performs efficient fusion,0.571571946144104
translation,66,121,baselines,stacked high-order tensors,into,many low rank factors,stacked high-order tensors into many low rank factors,0.5765082240104675
translation,66,122,baselines,mfm,"et al. , 2019",multimodal factorization,"mfm et al. , 2019 multimodal factorization",0.6185889840126038
translation,66,122,baselines,baselines,has,mfm,baselines has mfm,0.5705968737602234
translation,66,124,baselines,multimodal transformer,constructs,architecture unimodal and crossmodal transformer networks,multimodal transformer constructs architecture unimodal and crossmodal transformer networks,0.6086102724075317
translation,66,124,baselines,multimodal transformer,constructs,complete fusion process,multimodal transformer constructs complete fusion process,0.5968261361122131
translation,66,124,baselines,complete fusion process,by,attention,complete fusion process by attention,0.6071861386299133
translation,66,124,baselines,mult,has,"tsai et al. , 2019a","mult has tsai et al. , 2019a",0.6148602366447449
translation,66,124,baselines,mult,has,multimodal transformer,mult has multimodal transformer,0.6693370342254639
translation,66,124,baselines,"tsai et al. , 2019a",has,multimodal transformer,"tsai et al. , 2019a has multimodal transformer",0.5684776306152344
translation,66,124,baselines,baselines,has,mult,baselines has mult,0.6613882184028625
translation,66,130,baselines,self-supervised multi-task learning,assigns,each modality,self-supervised multi-task learning assigns each modality,0.6352325081825256
translation,66,130,baselines,unimodal training task,with,automatically generated labels,unimodal training task with automatically generated labels,0.5979393124580383
translation,66,130,baselines,"mag - bert self -mm ( yu et al. , 2021 )",has,self-supervised multi-task learning,"mag - bert self -mm ( yu et al. , 2021 ) has self-supervised multi-task learning",0.5513402819633484
translation,66,130,baselines,each modality,has,unimodal training task,each modality has unimodal training task,0.5531008243560791
translation,66,130,baselines,baselines,has,"mag - bert self -mm ( yu et al. , 2021 )","baselines has mag - bert self -mm ( yu et al. , 2021 )",0.5375633239746094
translation,66,134,experimental-setup,our model,on,single rtx 2080 ti gpu,our model on single rtx 2080 ti gpu,0.5485325455665588
translation,66,134,experimental-setup,grid search,for,best set of hyper-parameters,grid search for best set of hyper-parameters,0.5852640271186829
translation,66,134,experimental-setup,experimental setup,trained,our model,experimental setup trained our model,0.7027952075004578
translation,66,134,experimental-setup,experimental setup,ran,grid search,experimental setup ran grid search,0.590301513671875
translation,66,137,experimental-setup,gridsearch,for,best set of hyper-parameters,gridsearch for best set of hyper-parameters,0.6017590165138245
translation,66,137,experimental-setup,gridsearch,in,"{ 0.05 , 0.1 , 0.3 }","gridsearch in { 0.05 , 0.1 , 0.3 }",0.5382668972015381
translation,66,137,experimental-setup,batch size,in,"{ 32 , 64 }","batch size in { 32 , 64 }",0.5314143300056458
translation,66,137,experimental-setup,? lld,in,"{ 1e- 3,5e -3 }","? lld in { 1e- 3,5e -3 }",0.6025366187095642
translation,66,137,experimental-setup,? lld,in,"{ 0.05 , 0.1 , 0.3 }","? lld in { 0.05 , 0.1 , 0.3 }",0.5751696228981018
translation,66,137,experimental-setup,? main,in,"{ 0.05 , 0.1 , 0.3 }","? main in { 0.05 , 0.1 , 0.3 }",0.5567386746406555
translation,66,137,experimental-setup,hidden dim,in,"{ 32 , 64 }","hidden dim in { 32 , 64 }",0.5789968967437744
translation,66,137,experimental-setup,memory size,in,"{ 1 , 2 , 3 } batches","memory size in { 1 , 2 , 3 } batches",0.549605667591095
translation,66,137,experimental-setup,gradient clipping value,fixed at,5.0,gradient clipping value fixed at 5.0,0.6665729284286499
translation,66,137,experimental-setup,learning rate,for,bert fine - tuning,learning rate for bert fine - tuning,0.6402745842933655
translation,66,137,experimental-setup,learning rate,is,5e - 5,learning rate is 5e - 5,0.6211683750152588
translation,66,137,experimental-setup,bert fine - tuning,is,5e - 5,bert fine - tuning is 5e - 5,0.6527397036552429
translation,66,137,experimental-setup,experimental setup,perform,gridsearch,experimental setup perform gridsearch,0.5916770100593567
translation,66,137,experimental-setup,experimental setup,perform,5e - 5,experimental setup perform 5e - 5,0.5979979634284973
translation,66,137,experimental-setup,experimental setup,perform,bert embedding size,experimental setup perform bert embedding size,0.5446315407752991
translation,66,133,experiments,visual and acoustic,use,covarep,visual and acoustic use covarep,0.646614670753479
translation,66,133,experiments,visual and acoustic,use,p2fa,visual and acoustic use p2fa,0.5879372954368591
translation,66,133,experiments,p2fa,has,"yuan and liberman , 2008 )","p2fa has yuan and liberman , 2008 )",0.5987759232521057
translation,66,7,model,framework,named,multimodal infomax ( mmim ),framework named multimodal infomax ( mmim ),0.715107798576355
translation,66,7,model,mutual information ( mi ),in,unimodal input pairs ( inter-modality ),mutual information ( mi ) in unimodal input pairs ( inter-modality ),0.5059155821800232
translation,66,7,model,mutual information ( mi ),between,multimodal fusion result,mutual information ( mi ) between multimodal fusion result,0.5994167923927307
translation,66,7,model,mutual information ( mi ),between,unimodal input,mutual information ( mi ) between unimodal input,0.6234382390975952
translation,66,7,model,unimodal input,to maintain,taskrelated information,unimodal input to maintain taskrelated information,0.6055822372436523
translation,66,7,model,taskrelated information,through,multimodal fusion,taskrelated information through multimodal fusion,0.6481028199195862
translation,66,7,model,model,propose,framework,model propose framework,0.666053295135498
translation,66,8,model,framework,jointly trained with,main task ( msa ),framework jointly trained with main task ( msa ),0.7515382170677185
translation,66,8,model,main task ( msa ),to improve,performance,main task ( msa ) to improve performance,0.702183187007904
translation,66,8,model,performance,of,downstream msa task,performance of downstream msa task,0.5492212176322937
translation,66,8,model,model,has,framework,model has framework,0.5441871285438538
translation,66,25,model,mutual information,in,multimodal fusion,mutual information in multimodal fusion,0.5240428447723389
translation,66,25,model,model,propose,multimodal infomax ( mmim ),model propose multimodal infomax ( mmim ),0.6608928442001343
translation,66,26,model,two types of mutual information,in,representation pairs,two types of mutual information in representation pairs,0.5125570893287659
translation,66,26,model,two types of mutual information,between,unimodal representations,two types of mutual information between unimodal representations,0.6076692342758179
translation,66,26,model,two types of mutual information,between,fusion results and their low-level unimodal representations,two types of mutual information between fusion results and their low-level unimodal representations,0.5958448648452759
translation,66,26,model,model,enhance,two types of mutual information,model enhance two types of mutual information,0.621779203414917
translation,66,127,model,modality -invariant and - specific representations,projects,features,modality -invariant and - specific representations projects features,0.7484485507011414
translation,66,127,model,features,into,separate two spaces,features into separate two spaces,0.6269009113311768
translation,66,127,model,separate two spaces,with,special limitations,separate two spaces with special limitations,0.6122433543205261
translation,66,127,model,misa,has,modality -invariant and - specific representations,misa has modality -invariant and - specific representations,0.5643167495727539
translation,66,127,model,model,has,misa,model has misa,0.6026583909988403
translation,66,140,results,mmim,yields,better or comparable results,mmim yields better or comparable results,0.6956554651260376
translation,66,140,results,better or comparable results,to,many baseline methods,better or comparable results to many baseline methods,0.4835072159767151
translation,66,140,results,results,find that,mmim,results find that mmim,0.6468390822410583
translation,66,141,results,significantly outperforms,in,"( non - 0 ) acc - 7 , ( non - 0 ) acc - 2","significantly outperforms in ( non - 0 ) acc - 7 , ( non - 0 ) acc - 2",0.5779319405555725
translation,66,141,results,significantly outperforms,in,f1 score,significantly outperforms in f1 score,0.5195156931877136
translation,66,141,results,significantly outperforms,in,"( non - 0 ) acc - 7 , ( non - 0 ) acc - 2","significantly outperforms in ( non - 0 ) acc - 7 , ( non - 0 ) acc - 2",0.5779319405555725
translation,66,141,results,significantly outperforms,in,f1 score,significantly outperforms in f1 score,0.5195156931877136
translation,66,141,results,sota,in,all metrics,sota in all metrics,0.5317621827125549
translation,66,141,results,sota,in,"( non - 0 ) acc - 7 , ( non - 0 ) acc - 2","sota in ( non - 0 ) acc - 7 , ( non - 0 ) acc - 2",0.5702806115150452
translation,66,141,results,sota,in,f1 score,sota in f1 score,0.5284654498100281
translation,66,141,results,sota,in,"( non - 0 ) acc - 7 , ( non - 0 ) acc - 2","sota in ( non - 0 ) acc - 7 , ( non - 0 ) acc - 2",0.5702806115150452
translation,66,141,results,f1 score,on,cmu - mosei,f1 score on cmu - mosei,0.5494669675827026
translation,66,141,results,significantly outperforms,has,sota,significantly outperforms has sota,0.6008003354072571
translation,66,142,results,mmim,achieves,very closed performance ( ?0.5 % ),mmim achieves very closed performance ( ?0.5 % ),0.6665380001068115
translation,66,142,results,very closed performance ( ?0.5 % ),to,sota,very closed performance ( ?0.5 % ) to sota,0.6154642105102539
translation,66,142,results,other metrics,has,mmim,other metrics has mmim,0.5524120330810547
translation,66,142,results,results,For,other metrics,results For other metrics,0.5648709535598755
translation,67,187,ablation-analysis,takeaway,has,careful held - out curation,takeaway has careful held - out curation,0.5875614285469055
translation,67,187,ablation-analysis,ablation analysis,has,takeaway,ablation analysis has takeaway,0.5774329304695129
translation,67,332,ablation-analysis,repeat 1 / f times column,notice,gap,repeat 1 / f times column notice gap,0.6936565041542053
translation,67,332,ablation-analysis,gap,between,our model 's original generation ( 24.4 rouge -l ),gap between our model 's original generation ( 24.4 rouge -l ),0.6102437973022461
translation,67,332,ablation-analysis,gap,between,equallength truncated generations,gap between equallength truncated generations,0.6766211986541748
translation,67,332,ablation-analysis,equallength truncated generations,with,repetition,equallength truncated generations with repetition,0.6351169347763062
translation,67,332,ablation-analysis,ablation analysis,In,repeat 1 / f times column,ablation analysis In repeat 1 / f times column,0.5376617908477783
translation,67,40,baselines,state - of - the - art system,for,eli5,state - of - the - art system for eli5,0.6316871643066406
translation,67,40,baselines,eli5,by using,"sparse transformer variant ( roy et al. , 2020 )","eli5 by using sparse transformer variant ( roy et al. , 2020 )",0.6476595997810364
translation,67,40,baselines,"sparse transformer variant ( roy et al. , 2020 )",to condition over,wikipedia paragraphs,"sparse transformer variant ( roy et al. , 2020 ) to condition over wikipedia paragraphs",0.7512887716293335
translation,67,40,baselines,wikipedia paragraphs,returned by,"realm - style retriever ( guu et al. , 2020 )","wikipedia paragraphs returned by realm - style retriever ( guu et al. , 2020 )",0.7032669186592102
translation,67,72,experimental-setup,inference,perform,maximum inner-product search ( mips ),inference perform maximum inner-product search ( mips ),0.6160827875137329
translation,67,72,experimental-setup,maximum inner-product search ( mips ),with,"scann library ( guo et al. , 2020 )","maximum inner-product search ( mips ) with scann library ( guo et al. , 2020 )",0.5728762745857239
translation,67,72,experimental-setup,"scann library ( guo et al. , 2020 )",to efficiently find,top k documents,"scann library ( guo et al. , 2020 ) to efficiently find top k documents",0.6780843138694763
translation,67,72,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,67,251,experimental-setup,early stopping,on,validation data,early stopping on validation data,0.5751675367355347
translation,67,251,experimental-setup,experimental setup,do,early stopping,experimental setup do early stopping,0.46015220880508423
translation,67,254,experimental-setup,generator,trained on,64 google cloud tpus,generator trained on 64 google cloud tpus,0.7086352705955505
translation,67,254,experimental-setup,steps,on,eli5 training set,steps on eli5 training set,0.5826351046562195
translation,67,254,experimental-setup,experimental setup,has,generator,experimental setup has generator,0.5087665319442749
translation,67,255,experimental-setup,_local_cluster8 k configuration,available in,routing transformer implementation,_local_cluster8 k configuration available in routing transformer implementation,0.6833239197731018
translation,67,255,experimental-setup,pg19,has,_local_cluster8 k configuration,pg19 has _local_cluster8 k configuration,0.643429160118103
translation,67,255,experimental-setup,experimental setup,use,pg19,experimental setup use pg19,0.6160019636154175
translation,67,256,experimental-setup,"15 % input , attention and relu dropout",critical to prevent,overfitting,"15 % input , attention and relu dropout critical to prevent overfitting",0.6566796898841858
translation,67,256,experimental-setup,overfitting,on,training set,overfitting on training set,0.5583126544952393
translation,67,257,experimental-setup,learning rate,of,5e - 5,learning rate of 5e - 5,0.6587256789207458
translation,67,258,experimental-setup,truncated / padded,to,288 subword tokens,truncated / padded to 288 subword tokens,0.5625106692314148
translation,67,258,experimental-setup,experimental setup,has,"our retrievals , questions and answers","experimental setup has our retrievals , questions and answers",0.5586737394332886
translation,67,182,experiments,outperforming,has,all others,outperforming has all others,0.5762148499488831
translation,67,183,experiments,eli5 performance,on,overlapping qa,eli5 performance on overlapping qa,0.589911937713623
translation,67,250,experiments,retriever,trained on,64 google cloud tpus,retriever trained on 64 google cloud tpus,0.7310593724250793
translation,67,250,experiments,retriever,trained on,batch size,retriever trained on batch size,0.6948064565658569
translation,67,250,experiments,64 google cloud tpus,for,total of 4 k steps,64 google cloud tpus for total of 4 k steps,0.6327486038208008
translation,67,250,experiments,batch size,of,12288,batch size of 12288,0.6345420479774475
translation,67,50,results,our system,achieves,higher rouge -l,our system achieves higher rouge -l,0.7337109446525574
translation,67,50,results,higher rouge -l,than,reference human-written answers,higher rouge -l than reference human-written answers,0.5977909564971924
translation,67,50,results,results,has,our system,results has our system,0.5954442024230957
translation,67,94,results,outperform,on,generation & combined scores,outperform on generation & combined scores,0.577609121799469
translation,67,94,results,prior work,on,generation & combined scores,prior work on generation & combined scores,0.5354978442192078
translation,67,94,results,outperform,has,prior work,outperform has prior work,0.5811013579368591
translation,67,94,results,results,has,outperform,results has outperform,0.642206609249115
translation,67,119,results,proposed rt +c-realm system,achieves,new state - of- the - art,proposed rt +c-realm system achieves new state - of- the - art,0.6654549837112427
translation,67,119,results,new state - of- the - art,on,combined performance ( kilt r -l ),new state - of- the - art on combined performance ( kilt r -l ),0.5407241582870483
translation,67,119,results,results,has,proposed rt +c-realm system,results has proposed rt +c-realm system,0.6097667813301086
translation,67,133,results,similar rouge -l scores,as,our predicted system,similar rouge -l scores as our predicted system,0.49689069390296936
translation,67,133,results,random retrieval model,has,similar rouge -l scores,random retrieval model has similar rouge -l scores,0.5766910314559937
translation,67,186,results,large differences,of,"6.6 rprec , 8.1 r@5","large differences of 6.6 rprec , 8.1 r@5",0.5926085114479065
translation,67,186,results,"6.6 rprec , 8.1 r@5",in,retrieval performance,"6.6 rprec , 8.1 r@5 in retrieval performance",0.508790910243988
translation,67,186,results,retrieval performance,favoring,overlap subset,retrieval performance favoring overlap subset,0.664161205291748
translation,67,186,results,small generation score gain,of,"0.8 f1 , 0.4 r -l","small generation score gain of 0.8 f1 , 0.4 r -l",0.5987040400505066
translation,67,186,results,results,notice,large differences,results notice large differences,0.6904535293579102
translation,67,205,results,"both our lower bounds ( random training answer , copy input )",are,quite competitive,"both our lower bounds ( random training answer , copy input ) are quite competitive",0.5500029921531677
translation,67,205,results,close,to,"bart + dpr ( petroni et al. , 2020 )","close to bart + dpr ( petroni et al. , 2020 )",0.5787523984909058
translation,67,205,results,outperforming,has,rag,outperforming has rag,0.6268864274024963
translation,67,205,results,outperforming,has,"lewis et al. , 2020 c )","outperforming has lewis et al. , 2020 c )",0.5978236794471741
translation,67,205,results,rag,has,"lewis et al. , 2020 c )","rag has lewis et al. , 2020 c )",0.5720431208610535
translation,67,205,results,results,has,"both our lower bounds ( random training answer , copy input )","results has both our lower bounds ( random training answer , copy input )",0.5727999806404114
translation,67,252,results,our model,converges,quite fast,our model converges quite fast,0.7412150502204895
translation,67,252,results,our model,reaching,best performance,our model reaching best performance,0.6774389743804932
translation,67,252,results,our model,needing,103 minutes,our model needing 103 minutes,0.6774812936782837
translation,67,252,results,quite fast,reaching,best performance,quite fast reaching best performance,0.7236155271530151
translation,67,252,results,best performance,in,1.5 k steps,best performance in 1.5 k steps,0.5290250182151794
translation,67,252,results,best performance,in,43 minutes,best performance in 43 minutes,0.5073941946029663
translation,67,252,results,best performance,in,43 minutes,best performance in 43 minutes,0.5073941946029663
translation,67,252,results,103 minutes,for,full set of 4 k steps,103 minutes for full set of 4 k steps,0.6313361525535583
translation,67,252,results,results,has,our model,results has our model,0.5871725678443909
translation,68,84,baselines,amazon,in,6 languages,amazon in 6 languages,0.5374522805213928
translation,68,84,baselines,dataset,in,6 languages,dataset in 6 languages,0.4955856204032898
translation,68,84,baselines,6 languages,with,210k reviews per language,6 languages with 210k reviews per language,0.6141824126243591
translation,68,84,baselines,amazon,has,dataset,amazon has dataset,0.545841634273529
translation,68,84,baselines,baselines,has,amazon,baselines has amazon,0.6271703839302063
translation,68,182,baselines,pipelined system,with,domain-specific language models,pipelined system with domain-specific language models,0.6220173239707947
translation,68,182,baselines,domain-specific language models,augmented with,adversarial data,domain-specific language models augmented with adversarial data,0.7024226188659668
translation,68,182,baselines,bat,has,"11 ( karimi et al. , 2020 )","bat has 11 ( karimi et al. , 2020 )",0.6086733341217041
translation,68,182,baselines,bat,has,pipelined system,bat has pipelined system,0.6274517178535461
translation,68,182,baselines,"11 ( karimi et al. , 2020 )",has,pipelined system,"11 ( karimi et al. , 2020 ) has pipelined system",0.5910707116127014
translation,68,182,baselines,baselines,has,bat,baselines has bat,0.6228921413421631
translation,68,183,baselines,end-to- end model,based on,song et al . ( 2019 ),end-to- end model based on song et al . ( 2019 ),0.6764373183250427
translation,68,183,baselines,end-to- end model,based on,domain adaptation,end-to- end model based on domain adaptation,0.6692982912063599
translation,68,183,baselines,end-to- end model,based on,local context focus mechanism,end-to- end model based on local context focus mechanism,0.6594476699829102
translation,68,183,baselines,end-to- end model,with,domain adaptation,end-to- end model with domain adaptation,0.6393902897834778
translation,68,183,baselines,end-to- end model,with,local context focus mechanism,end-to- end model with local context focus mechanism,0.6351919174194336
translation,68,183,baselines,lcf,has,"12 ( yang et al. , 2020 )","lcf has 12 ( yang et al. , 2020 )",0.5879112482070923
translation,68,183,baselines,lcf,has,end-to- end model,lcf has end-to- end model,0.5444955229759216
translation,68,183,baselines,"12 ( yang et al. , 2020 )",has,end-to- end model,"12 ( yang et al. , 2020 ) has end-to- end model",0.5576976537704468
translation,68,183,baselines,baselines,has,lcf,baselines has lcf,0.56271892786026
translation,68,184,baselines,racl,has,"13 ( chen and qian , 2020 )","racl has 13 ( chen and qian , 2020 )",0.6166187524795532
translation,68,184,baselines,baselines,has,racl,baselines has racl,0.6466894745826721
translation,68,187,baselines,bert - e2e,has,14,bert - e2e has 14,0.6735124588012695
translation,68,187,baselines,bert - e2e,has,bert - based end-to - end sequence labeling system,bert - e2e has bert - based end-to - end sequence labeling system,0.5556907653808594
translation,68,187,baselines,14,has,bert - based end-to - end sequence labeling system,14 has bert - based end-to - end sequence labeling system,0.5413301587104797
translation,68,187,baselines,baselines,has,bert - e2e,baselines has bert - e2e,0.6101173758506775
translation,68,189,baselines,pipeline,of,mcrf - sa,pipeline of mcrf - sa,0.6171498894691467
translation,68,189,baselines,sc system,utilizing,multiple crf - based structured attention models,sc system utilizing multiple crf - based structured attention models,0.633929967880249
translation,68,189,baselines,hast + mcrf,has,pipeline,hast + mcrf has pipeline,0.6547242999076843
translation,68,189,baselines,hast,has,te system,hast has te system,0.6846380233764648
translation,68,189,baselines,mcrf - sa,has,sc system,mcrf - sa has sc system,0.5742834210395813
translation,68,189,baselines,baselines,has,hast + mcrf,baselines has hast + mcrf,0.6032893061637878
translation,68,10,experiments,github.com,has,/ibm,github.com has /ibm,0.5383796691894531
translation,68,10,experiments,/ibm,has,/ yaso-tsa,/ibm has / yaso-tsa,0.6619590520858765
translation,68,186,hyperparameters,racl - glove variant,based on,pre-trained word embeddings,racl - glove variant based on pre-trained word embeddings,0.5928137898445129
translation,68,186,hyperparameters,hyperparameters,used,racl - glove variant,hyperparameters used racl - glove variant,0.6042591333389282
translation,68,6,model,yaso - a new tsa evaluation dataset,of,open-domain user reviews,yaso - a new tsa evaluation dataset of open-domain user reviews,0.5428785681724548
translation,68,6,model,model,present,yaso - a new tsa evaluation dataset,model present yaso - a new tsa evaluation dataset,0.6651991605758667
translation,68,188,model,bert + linear architecture,computes,pertoken labels,bert + linear architecture computes pertoken labels,0.7619082927703857
translation,68,188,model,pertoken labels,using,linear classification layer,pertoken labels using linear classification layer,0.6571409702301025
translation,68,188,model,model,used,bert + linear architecture,model used bert + linear architecture,0.5843218564987183
translation,68,204,results,bat,trained on,restaurants data,bat trained on restaurants data,0.7305490374565125
translation,68,204,results,restaurants data,was,best-performing system,restaurants data was best-performing system,0.5932638049125671
translation,68,204,results,best-performing system,for,te and the full tsa tasks,best-performing system for te and the full tsa tasks,0.5822087526321411
translation,68,204,results,best-performing system,on,three of the four datasets,best-performing system on three of the four datasets,0.47724759578704834
translation,68,204,results,results,has,bat,results has bat,0.4547613859176636
translation,68,205,results,bert - e2e,was,best model,bert - e2e was best model,0.6322566270828247
translation,68,205,results,best model,on,three datasets,best model on three datasets,0.5135152339935303
translation,68,205,results,sc,has,bert - e2e,sc has bert - e2e,0.6769158840179443
translation,68,205,results,results,For,sc,results For sc,0.6267269849777222
translation,68,206,results,results,for,sc,results for sc,0.6267269849777222
translation,68,206,results,sc,were,relatively high,sc were relatively high,0.6264773011207581
translation,68,206,results,te results,by,some models,te results by some models,0.6270047426223755
translation,68,206,results,some models,may be,very low,some models may be very low,0.685539960861206
translation,68,206,results,results,for,sc,results for sc,0.6267269849777222
translation,68,206,results,results,has,results,results has results,0.48582205176353455
translation,69,160,ablation-analysis,text,was,missing,text was missing,0.6459832191467285
translation,69,160,ablation-analysis,relatively large performance drop,implies,language modality,relatively large performance drop implies language modality,0.6104033589363098
translation,69,160,ablation-analysis,text,has,model,text has model,0.6128824353218079
translation,69,160,ablation-analysis,missing,has,model,missing has model,0.606469452381134
translation,69,160,ablation-analysis,model,has,relatively large performance drop,model has relatively large performance drop,0.5507715940475464
translation,69,160,ablation-analysis,ablation analysis,when,text,ablation analysis when text,0.6953706741333008
translation,69,141,baselines,sequence to sequence,for,sentiment ( seq2seq2sent ),sequence to sequence for sentiment ( seq2seq2sent ),0.6262989640235901
translation,69,141,baselines,multimodal sentiment analysis,with,transformer ( transmodality ),multimodal sentiment analysis with transformer ( transmodality ),0.6417288780212402
translation,69,141,baselines,translation - based,has,multimodal cyclic translation network ( mctn ),translation - based has multimodal cyclic translation network ( mctn ),0.5882948040962219
translation,69,141,baselines,translation - based,has,sequence to sequence,translation - based has sequence to sequence,0.6167308688163757
translation,69,141,baselines,translation - based,has,multimodal sentiment analysis,translation - based has multimodal sentiment analysis,0.5188969373703003
translation,69,141,baselines,multimodal cyclic translation network ( mctn ),has,sequence to sequence,multimodal cyclic translation network ( mctn ) has sequence to sequence,0.6179496645927429
translation,69,141,baselines,multimodal cyclic translation network ( mctn ),has,multimodal sentiment analysis,multimodal cyclic translation network ( mctn ) has multimodal sentiment analysis,0.5465080142021179
translation,69,141,baselines,baselines,has,translation - based,baselines has translation - based,0.582858681678772
translation,69,142,baselines,baselines,has,non-translation based,baselines has non-translation based,0.5552377104759216
translation,69,167,experiments,translation text ? audio,achieves,better performance,translation text ? audio achieves better performance,0.66877281665802
translation,69,167,experiments,better performance,than,audio ? text,better performance than audio ? text,0.601737916469574
translation,69,167,experiments,"( audio , text ) instance",has,translation text ? audio,"( audio , text ) instance has translation text ? audio",0.5981679558753967
translation,69,178,experiments,model,achieves,best performance,model achieves best performance,0.6728841066360474
translation,69,178,experiments,best performance,at,layer 1,best performance at layer 1,0.5768613219261169
translation,69,178,experiments,meld ( sentiment ),has,model,meld ( sentiment ) has model,0.601706862449646
translation,69,7,model,coupled - translation fusion network ( ctfn ),proposed to model,bi-direction interplay,coupled - translation fusion network ( ctfn ) proposed to model bi-direction interplay,0.7714049220085144
translation,69,7,model,bi-direction interplay,via,couple learning,bi-direction interplay via couple learning,0.7060958743095398
translation,69,7,model,couple learning,ensuring,robustness,couple learning ensuring robustness,0.745133101940155
translation,69,7,model,robustness,in respect to,missing modalities,robustness in respect to missing modalities,0.6480712294578552
translation,69,8,model,cyclic consistency constraint,presented to improve,translation performance,cyclic consistency constraint presented to improve translation performance,0.6786308884620667
translation,69,8,model,model,has,cyclic consistency constraint,model has cyclic consistency constraint,0.5726240873336792
translation,69,11,model,hierarchical architecture,to exploit,multiple bi-direction translations,hierarchical architecture to exploit multiple bi-direction translations,0.6765353679656982
translation,69,11,model,multiple bi-direction translations,leading to,double multimodal fusing embeddings,multiple bi-direction translations leading to double multimodal fusing embeddings,0.6555283069610596
translation,69,11,model,double multimodal fusing embeddings,compared with,traditional translation methods,double multimodal fusing embeddings compared with traditional translation methods,0.6318324208259583
translation,69,11,model,ctfn,has,hierarchical architecture,ctfn has hierarchical architecture,0.5749261379241943
translation,69,11,model,model,Based on,ctfn,model Based on ctfn,0.7165889739990234
translation,69,36,model,cyclic consistency constraint,proposed to improve,translation performance,cyclic consistency constraint proposed to improve translation performance,0.6906856298446655
translation,69,36,model,model,has,cyclic consistency constraint,model has cyclic consistency constraint,0.5726240873336792
translation,69,146,results,ctfn,exceeded,previous best transmodality,ctfn exceeded previous best transmodality,0.6864824891090393
translation,69,146,results,previous best transmodality,by,margin,previous best transmodality by margin,0.565902054309845
translation,69,146,results,margin,of,4.51,margin of 4.51,0.6084176301956177
translation,69,146,results,cmu - mosi dataset,has,ctfn,cmu - mosi dataset has ctfn,0.589803159236908
translation,69,146,results,previous best transmodality,has,"on ( video , audio )","previous best transmodality has on ( video , audio )",0.5938244462013245
translation,69,146,results,results,on,cmu - mosi dataset,results on cmu - mosi dataset,0.5326117873191833
translation,69,147,results,empirical improvement,of,ctfn,empirical improvement of ctfn,0.6039842963218689
translation,69,147,results,ctfn,was,0.78,ctfn was 0.78,0.6405874490737915
translation,69,147,results,meld ( sentiment ) dataset,has,empirical improvement,meld ( sentiment ) dataset has empirical improvement,0.5548974275588989
translation,69,147,results,results,on,meld ( sentiment ) dataset,results on meld ( sentiment ) dataset,0.5517246127128601
translation,69,148,results,"( video , audio )",is,more significant,"( video , audio ) is more significant",0.5701761245727539
translation,69,148,results,more significant,than,"( text , video )","more significant than ( text , video )",0.6010249853134155
translation,69,148,results,more significant,than,"( text , audio )","more significant than ( text , audio )",0.5982791185379028
translation,69,148,results,results,improvement of,"( video , audio )","results improvement of ( video , audio )",0.7238147258758545
translation,69,150,results,ctfn,exceeds,previous best transmodality,ctfn exceeds previous best transmodality,0.6430289149284363
translation,69,150,results,previous best transmodality,with,improvement,previous best transmodality with improvement,0.6132492423057556
translation,69,150,results,improvement,of,0.06,improvement of 0.06,0.5992418527603149
translation,69,150,results,0.06,leading to,comparable performance,0.06 leading to comparable performance,0.6506638526916504
translation,69,150,results,"text , audio , video )",has,ctfn,"text , audio , video ) has ctfn",0.6198099255561829
translation,69,153,results,tri-modality case,achieved,improvement,tri-modality case achieved improvement,0.7059566974639893
translation,69,153,results,improvement,of,0.61,improvement of 0.61,0.5649508833885193
translation,69,153,results,improvement,indicating,benefits,improvement indicating benefits,0.683861255645752
translation,69,153,results,bi-modality setting,has,tri-modality case,bi-modality setting has tri-modality case,0.6033535003662109
translation,69,153,results,results,compared to,bi-modality setting,results compared to bi-modality setting,0.6452452540397644
translation,69,159,results,"setting ( text , audio , video )",reach,comparable result,"setting ( text , audio , video ) reach comparable result",0.7443575263023376
translation,69,159,results,text - based settings,reach,comparable result,text - based settings reach comparable result,0.7325178384780884
translation,69,159,results,comparable result,with,relatively small performance drop,comparable result with relatively small performance drop,0.6714737415313721
translation,69,159,results,results,compared to,"setting ( text , audio , video )","results compared to setting ( text , audio , video )",0.6853514313697815
translation,69,161,results,"( audio , video , text )",demonstrates,hierarchical ctfn,"( audio , video , text ) demonstrates hierarchical ctfn",0.6466377973556519
translation,69,161,results,hierarchical ctfn,able to maintain,robustness and consistency,hierarchical ctfn able to maintain robustness and consistency,0.6710327863693237
translation,69,161,results,robustness and consistency,when considering,single input modality,robustness and consistency when considering single input modality,0.7174479365348816
