topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,105,ablation-analysis,softmax tempering,is,very important,softmax tempering is very important,0.5217532515525818
translation,0,105,ablation-analysis,softmax tempering,not that important in,highresource settings,softmax tempering not that important in highresource settings,0.6795362234115601
translation,0,105,ablation-analysis,very important,in,low-resource settings,very important in low-resource settings,0.5490085482597351
translation,0,158,ablation-analysis,improvements,of,1.0 to 2.0 bleu points,improvements of 1.0 to 2.0 bleu points,0.574353814125061
translation,0,158,ablation-analysis,settings,has,improvements,settings has improvements,0.6147841811180115
translation,0,165,ablation-analysis,label smoothing and sem,without,tempering,label smoothing and sem without tempering,0.6995123624801636
translation,0,165,ablation-analysis,sem loss,seems to matter less,lower values for sem loss,sem loss seems to matter less lower values for sem loss,0.6577257513999939
translation,0,165,ablation-analysis,label smoothing and sem,has,translation quality,label smoothing and sem has translation quality,0.5404294729232788
translation,0,165,ablation-analysis,combined,has,translation quality,combined has translation quality,0.5752008557319641
translation,0,165,ablation-analysis,tempering,has,translation quality,tempering has translation quality,0.5371434092521667
translation,0,165,ablation-analysis,translation quality,has,improves,translation quality has improves,0.6089749336242676
translation,0,165,ablation-analysis,ablation analysis,When,label smoothing and sem,ablation analysis When label smoothing and sem,0.6260362863540649
translation,0,12,baselines,nmt for high- resource language pairs,choose,nmt architecture,nmt for high- resource language pairs choose nmt architecture,0.6819647550582886
translation,0,12,baselines,nmt for high- resource language pairs,train,model,nmt for high- resource language pairs train model,0.6272547841072083
translation,0,12,baselines,model,on,all existing data,model on all existing data,0.4970008432865143
translation,0,12,baselines,model,by minimizing,softmax cross-entropy loss,model by minimizing softmax cross-entropy loss,0.639188289642334
translation,0,12,baselines,baselines,has,nmt for high- resource language pairs,baselines has nmt for high- resource language pairs,0.551098108291626
translation,0,78,experimental-setup,transformer base   and   transformer big   models,used,hyper-parameter settings,transformer base   and   transformer big   models used hyper-parameter settings,0.6339563727378845
translation,0,78,experimental-setup,hyper-parameter settings,in,transformer base single gpu and transformer big single gpu,hyper-parameter settings in transformer base single gpu and transformer big single gpu,0.5264527797698975
translation,0,78,experimental-setup,experimental setup,For,transformer base   and   transformer big   models,experimental setup For transformer base   and   transformer big   models,0.5770204067230225
translation,0,79,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,0,79,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,0,80,experimental-setup,internal sub-word tokenization mechanism,of,tensor2tensor,internal sub-word tokenization mechanism of tensor2tensor,0.5537843704223633
translation,0,80,experimental-setup,tensor2tensor,with,separate source and target language vocabularies,tensor2tensor with separate source and target language vocabularies,0.603532075881958
translation,0,80,experimental-setup,separate source and target language vocabularies,of size,"8,192 and 32,768","separate source and target language vocabularies of size 8,192 and 32,768",0.6931143999099731
translation,0,80,experimental-setup,"8,192 and 32,768",for,low-resource and high- resource settings,"8,192 and 32,768 for low-resource and high- resource settings",0.6196557879447937
translation,0,80,experimental-setup,experimental setup,used,internal sub-word tokenization mechanism,experimental setup used internal sub-word tokenization mechanism,0.5801734328269958
translation,0,81,experimental-setup,our models,for,softmax temperature values,our models for softmax temperature values,0.6055556535720825
translation,0,81,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,0,30,results,greedy - search performance,of,models,greedy - search performance of models,0.622206449508667
translation,0,30,results,greedy - search performance,of,models,greedy - search performance of models,0.622206449508667
translation,0,30,results,greedy - search performance,of,models,greedy - search performance of models,0.622206449508667
translation,0,30,results,models,trained with,softmax tempering,models trained with softmax tempering,0.7357732057571411
translation,0,30,results,models,trained with,softmax tempering,models trained with softmax tempering,0.7357732057571411
translation,0,30,results,models,trained without,softmax tempering,models trained without softmax tempering,0.7276356816291809
translation,0,30,results,softmax tempering,becomes,comparable to or better,softmax tempering becomes comparable to or better,0.632431149482727
translation,0,30,results,comparable to or better,than,beam-search performance,comparable to or better than beam-search performance,0.60617995262146
translation,0,30,results,beam-search performance,of,models,beam-search performance of models,0.5921302437782288
translation,0,30,results,models,trained without,softmax tempering,models trained without softmax tempering,0.7276356816291809
translation,0,30,results,results,has,greedy - search performance,results has greedy - search performance,0.5631703734397888
translation,0,163,results,label smoothing and temperature,kept to,default values,label smoothing and temperature kept to default values,0.6701947450637817
translation,0,163,results,default values,giving,high weight,default values giving high weight,0.713301956653595
translation,0,163,results,high weight,to,sem loss,high weight to sem loss,0.6020211577415466
translation,0,163,results,high weight,gives,improvements,high weight gives improvements,0.615574300289154
translation,0,163,results,improvements,of,over 1.0 bleu points,improvements of over 1.0 bleu points,0.5595738291740417
translation,2,151,ablation-analysis,spm,makes,slight contributions,spm makes slight contributions,0.5432291626930237
translation,2,151,ablation-analysis,slight contributions,to,nct model,slight contributions to nct model,0.5489016771316528
translation,2,151,ablation-analysis,nct model,in terms of,bleu,nct model in terms of bleu,0.6274317502975464
translation,2,151,ablation-analysis,less significant,than,dcm,less significant than dcm,0.6086525321006775
translation,2,151,ablation-analysis,ablation analysis,has,spm,ablation analysis has spm,0.5418246984481812
translation,2,129,baselines,original model,is,rnn - based,original model is rnn - based,0.5766943097114563
translation,2,129,baselines,additional encoder,to incorporate,mixed - language dialogue history,additional encoder to incorporate mixed - language dialogue history,0.7032874822616577
translation,2,129,baselines,context - aware nmt systems,has,original model,context - aware nmt systems has original model,0.5628893375396729
translation,2,110,experimental-setup,transformer models,contain,l = 6 encoder layers,transformer models contain l = 6 encoder layers,0.5930576324462891
translation,2,110,experimental-setup,transformer models,contain,l = 6 decoder layers,transformer models contain l = 6 decoder layers,0.5789194107055664
translation,2,110,experimental-setup,experimental setup,has,transformer models,experimental setup has transformer models,0.5094819664955139
translation,2,111,experimental-setup,training step,for,first pre-training stage,training step for first pre-training stage,0.6458976864814758
translation,2,111,experimental-setup,training step,for,second fine-tuning stage,training step for second fine-tuning stage,0.6231564283370972
translation,2,111,experimental-setup,first pre-training stage,set to,"t 1 = 200,000","first pre-training stage set to t 1 = 200,000",0.7158580422401428
translation,2,111,experimental-setup,second fine-tuning stage,set to,"t 2 = 5,000","second fine-tuning stage set to t 2 = 5,000",0.7172166109085083
translation,2,111,experimental-setup,experimental setup,has,training step,experimental setup has training step,0.5325546264648438
translation,2,112,experimental-setup,batch size,for,each gpu,batch size for each gpu,0.6036682724952698
translation,2,112,experimental-setup,each gpu,set to,4096 tokens,each gpu set to 4096 tokens,0.6593772172927856
translation,2,112,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,2,113,experimental-setup,experiments,in,first stage,experiments in first stage,0.5838447213172913
translation,2,113,experimental-setup,first stage,conducted utilizing,8 nvidia tesla v100 gpus,first stage conducted utilizing 8 nvidia tesla v100 gpus,0.6434941291809082
translation,2,113,experimental-setup,4 gpus,for,second stage,4 gpus for second stage,0.6077414155006409
translation,2,115,experimental-setup,"adam ( kingma and ba , 2014 )",with,? 1 = 0.9 and ? 2 = 0.998,"adam ( kingma and ba , 2014 ) with ? 1 = 0.9 and ? 2 = 0.998",0.6532140970230103
translation,2,115,experimental-setup,learning rate,set to,1.0,learning rate set to 1.0,0.7091217041015625
translation,2,115,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,2,116,experimental-setup,label smoothing,set to,0.1,label smoothing set to 0.1,0.6640933752059937
translation,2,116,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,2,117,experimental-setup,dropout,of,0.1/0.3,dropout of 0.1/0.3,0.6290892958641052
translation,2,117,experimental-setup,0.1/0.3,for,base and big setting,0.1/0.3 for base and big setting,0.6533445715904236
translation,2,118,experimental-setup,| t |,set to,10,| t | set to 10,0.7035831809043884
translation,2,118,experimental-setup,experimental setup,has,| t |,experimental setup has | t |,0.5305099487304688
translation,2,121,experimental-setup,beam size,set to,4,beam size set to 4,0.7776850461959839
translation,2,121,experimental-setup,length penalty,is,0.6,length penalty is 0.6,0.5568951964378357
translation,2,121,experimental-setup,inference,has,beam size,inference has beam size,0.5730389356613159
translation,2,121,experimental-setup,inference,has,length penalty,inference has length penalty,0.5228193998336792
translation,2,121,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,2,121,experimental-setup,experimental setup,has,length penalty,experimental setup has length penalty,0.4614953398704529
translation,2,7,experiments,four auxiliary tasks,including,monolingual response generation,four auxiliary tasks including monolingual response generation,0.6104792356491089
translation,2,7,experiments,four auxiliary tasks,including,cross-lingual response generation,four auxiliary tasks including cross-lingual response generation,0.6146933436393738
translation,2,7,experiments,four auxiliary tasks,including,next utterance discrimination,four auxiliary tasks including next utterance discrimination,0.6671242713928223
translation,2,7,experiments,four auxiliary tasks,including,speaker identification,four auxiliary tasks including speaker identification,0.684770941734314
translation,2,8,experiments,nct model,through,training objectives,nct model through training objectives,0.6827154755592346
translation,2,30,experiments,csa - nct framework,datasets of,different language pairs,csa - nct framework datasets of different language pairs,0.7291982769966125
translation,2,114,experiments,8*4096 and 4*4096 tokens,per,update,8*4096 and 4*4096 tokens per update,0.661396324634552
translation,2,114,experiments,update,for,all experiments,update for all experiments,0.5762354731559753
translation,2,114,experiments,all experiments,in,first-stage and second-stage,all experiments in first-stage and second-stage,0.5438341498374939
translation,2,6,model,chat translation,by introducing,modeling of dialogue characteristics,chat translation by introducing modeling of dialogue characteristics,0.6830337047576904
translation,2,6,model,modeling of dialogue characteristics,into,nct model,modeling of dialogue characteristics into nct model,0.6047919988632202
translation,2,6,model,model,promote,chat translation,model promote chat translation,0.6666784286499023
translation,2,21,model,coherence - speaker - aware nct ( csa - nct ) training framework,to improve,nct model,coherence - speaker - aware nct ( csa - nct ) training framework to improve nct model,0.6303644776344299
translation,2,21,model,nct model,by making use of,dialogue characteristics,nct model by making use of dialogue characteristics,0.7000667452812195
translation,2,21,model,dialogue characteristics,in,conversations,dialogue characteristics in conversations,0.5127557516098022
translation,2,21,model,model,propose,coherence - speaker - aware nct ( csa - nct ) training framework,model propose coherence - speaker - aware nct ( csa - nct ) training framework,0.6320511102676392
translation,2,139,results,substantially outperforms,by,large margin,substantially outperforms by large margin,0.6206852197647095
translation,2,139,results,sentencelevel / context - aware baselines,by,large margin,sentencelevel / context - aware baselines by large margin,0.5454240441322327
translation,2,139,results,1.02,on,en?de,1.02 on en?de,0.6750983595848083
translation,2,139,results,base setting,has,our model,base setting has our model,0.5759850144386292
translation,2,139,results,our model,has,substantially outperforms,our model has substantially outperforms,0.6102578043937683
translation,2,139,results,substantially outperforms,has,sentencelevel / context - aware baselines,substantially outperforms has sentencelevel / context - aware baselines,0.5883991718292236
translation,2,140,results,csa - nct,performs,better,csa - nct performs better,0.6825180649757385
translation,2,140,results,better,on,two directions,better on two directions,0.5721273422241211
translation,2,140,results,lower,than,gate - transformer + ft,lower than gate - transformer + ft,0.617149293422699
translation,2,140,results,ter,has,csa - nct,ter has csa - nct,0.7132515907287598
translation,2,140,results,0.9 ? and 0.7 ?,has,lower,0.9 ? and 0.7 ? has lower,0.5784598588943481
translation,2,140,results,results,In term of,ter,results In term of ter,0.6863987445831299
translation,2,141,results,big setting,on,en?de and de?en,big setting on en?de and de?en,0.6401310563087463
translation,2,141,results,big setting,on,our model,big setting on our model,0.5685731172561646
translation,2,141,results,big setting,on,other existing systems,big setting on other existing systems,0.5768741965293884
translation,2,141,results,our model,consistently surpasses,baselines,our model consistently surpasses baselines,0.7652765512466431
translation,2,141,results,our model,consistently surpasses,other existing systems,our model consistently surpasses other existing systems,0.7454800605773926
translation,2,141,results,big setting,has,our model,big setting has our model,0.5874620079994202
translation,2,141,results,en?de and de?en,has,our model,en?de and de?en has our model,0.6817398071289062
translation,2,141,results,results,Under,big setting,results Under big setting,0.5601499676704407
translation,2,144,results,our model,presents,notable improvements,our model presents notable improvements,0.6498327851295471
translation,2,144,results,notable improvements,over,all comparison models,notable improvements over all comparison models,0.6919194459915161
translation,2,144,results,all comparison models,by,at least 2.43 ? and 0.77 ? bleu gains,all comparison models by at least 2.43 ? and 0.77 ? bleu gains,0.5737617611885071
translation,2,144,results,all comparison models,by,1.73 ? and 1.43 ? bleu gains,all comparison models by 1.73 ? and 1.43 ? bleu gains,0.5387669205665588
translation,2,144,results,at least 2.43 ? and 0.77 ? bleu gains,under,base setting,at least 2.43 ? and 0.77 ? bleu gains under base setting,0.6393013000488281
translation,2,144,results,1.73 ? and 1.43 ? bleu gains,under,big setting,1.73 ? and 1.43 ? bleu gains under big setting,0.6252944469451904
translation,2,144,results,en?zh and zh?en,has,our model,en?zh and zh?en has our model,0.6426789164543152
translation,2,144,results,results,on,en?zh and zh?en,results on en?zh and zh?en,0.5691802501678467
translation,2,149,results,results,under,big setting,results under big setting,0.5601499676704407
translation,2,150,results,dcm,substantially improves,nct model,dcm substantially improves nct model,0.7042081952095032
translation,2,150,results,nct model,in terms of,bleu and ter metrics,nct model in terms of bleu and ter metrics,0.6644065976142883
translation,2,167,results,our model,generates,"more coherent , speaker - relevant , and fluent translations","our model generates more coherent , speaker - relevant , and fluent translations",0.5901129245758057
translation,2,167,results,"more coherent , speaker - relevant , and fluent translations",compared with,other models,"more coherent , speaker - relevant , and fluent translations compared with other models",0.61504065990448
translation,2,167,results,results,show,our model,results show our model,0.6888449192047119
translation,3,94,ablation-analysis,fine -tuning,leads to,increase,fine -tuning leads to increase,0.6822187900543213
translation,3,94,ablation-analysis,increase,of,0.4 bleu,increase of 0.4 bleu,0.6070907711982727
translation,3,94,ablation-analysis,english ? german,has,fine -tuning,english ? german has fine -tuning,0.5869495868682861
translation,3,94,ablation-analysis,ablation analysis,On,english ? german,ablation analysis On english ? german,0.5559094548225403
translation,3,96,ablation-analysis,model combinations,found through,ergodic approach,model combinations found through ergodic approach,0.6900381445884705
translation,3,96,ablation-analysis,ergodic approach,contribute to,0.7 bleu increase,ergodic approach contribute to 0.7 bleu increase,0.6591529846191406
translation,3,96,ablation-analysis,ergodic approach,contribute to,0.6 bleu,ergodic approach contribute to 0.6 bleu,0.6262807250022888
translation,3,96,ablation-analysis,0.7 bleu increase,for,german ? english,0.7 bleu increase for german ? english,0.5764787793159485
translation,3,96,ablation-analysis,0.7 bleu increase,for,english ? german,0.7 bleu increase for english ? german,0.5725522637367249
translation,3,96,ablation-analysis,0.6 bleu,for,english ? german,0.6 bleu for english ? german,0.5595396161079407
translation,3,38,baselines,deep 25 - 6 model,features,25 - layer encoder,deep 25 - 6 model features 25 - layer encoder,0.6572901010513306
translation,3,38,baselines,deep 25 - 6 model,features,6 - layer decoder,deep 25 - 6 model features 6 - layer decoder,0.6708788275718689
translation,3,38,baselines,deep 25 - 6 model,features,1024 dimensions,deep 25 - 6 model features 1024 dimensions,0.6676256656646729
translation,3,38,baselines,deep 25 - 6 model,features,4096 - hidden-state,deep 25 - 6 model features 4096 - hidden-state,0.6587628126144409
translation,3,38,baselines,deep 25 - 6 model,features,16 - head self-attention,deep 25 - 6 model features 16 - head self-attention,0.6753430366516113
translation,3,38,baselines,deep 25 - 6 model,features,layer normalization,deep 25 - 6 model features layer normalization,0.6900547742843628
translation,3,38,baselines,deep 25 - 6 model,of,4096 - hidden-state,deep 25 - 6 model of 4096 - hidden-state,0.5823293924331665
translation,3,38,baselines,deep 25 - 6 model,of,layer normalization,deep 25 - 6 model of layer normalization,0.5628775358200073
translation,3,38,baselines,1024 dimensions,of,word vector,1024 dimensions of word vector,0.5840649008750916
translation,3,38,baselines,1024 dimensions,of,4096 - hidden-state,1024 dimensions of 4096 - hidden-state,0.6010226607322693
translation,3,38,baselines,1024 dimensions,of,layer normalization,1024 dimensions of layer normalization,0.5495730638504028
translation,3,38,baselines,baselines,has,two transformer deep-large model architectures,baselines has two transformer deep-large model architectures,0.5530699491500854
translation,3,34,experimental-setup,byte pair encoding ( bpe ),adopted for,chinese and english sub-word segmentation,byte pair encoding ( bpe ) adopted for chinese and english sub-word segmentation,0.6839948296546936
translation,3,34,experimental-setup,experimental setup,has,byte pair encoding ( bpe ),experimental setup has byte pair encoding ( bpe ),0.5415831804275513
translation,3,35,experimental-setup,bpe models,with,"32,000 merge operations","bpe models with 32,000 merge operations",0.664877712726593
translation,3,35,experimental-setup,"32,000 merge operations",for,both the source and target sides,"32,000 merge operations for both the source and target sides",0.5956828594207764
translation,3,35,experimental-setup,experimental setup,train,bpe models,experimental setup train bpe models,0.6794427037239075
translation,3,39,experimental-setup,"open-source fairseq ( ott et al. , 2019 )",for,training,"open-source fairseq ( ott et al. , 2019 ) for training",0.5262494087219238
translation,3,39,experimental-setup,experimental setup,use,"open-source fairseq ( ott et al. , 2019 )","experimental setup use open-source fairseq ( ott et al. , 2019 )",0.563403308391571
translation,3,42,experimental-setup,size,of,each batch,size of each batch,0.6566802859306335
translation,3,42,experimental-setup,size,of,parameter update frequency,size of parameter update frequency,0.5662137866020203
translation,3,42,experimental-setup,each batch,set as,2048,each batch set as 2048,0.670348048210144
translation,3,42,experimental-setup,parameter update frequency,as,32,parameter update frequency as 32,0.5660668611526489
translation,3,42,experimental-setup,learning rate,as,5e - 4,learning rate as 5e - 4,0.5762070417404175
translation,3,42,experimental-setup,label smoothing,as,0.1,label smoothing as 0.1,0.5090335607528687
translation,3,42,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,3,43,experimental-setup,number of warmup steps,is,4000,number of warmup steps is 4000,0.5959648489952087
translation,3,43,experimental-setup,dropout,is,0.1,dropout is 0.1,0.5574049949645996
translation,3,43,experimental-setup,experimental setup,has,number of warmup steps,experimental setup has number of warmup steps,0.5219058394432068
translation,3,43,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,3,63,experimental-setup,in- domain training data,to conduct,incremental training,in- domain training data to conduct incremental training,0.5856659412384033
translation,3,63,experimental-setup,incremental training,with,baseline models,incremental training with baseline models,0.6394542455673218
translation,3,63,experimental-setup,baseline models,trained by,wmt21 news data,baseline models trained by wmt21 news data,0.7138485312461853
translation,3,63,experimental-setup,wmt21 news data,for,domain transfer,wmt21 news data for domain transfer,0.6154999136924744
translation,3,63,experimental-setup,experimental setup,use,in- domain training data,experimental setup use in- domain training data,0.6088082790374756
translation,3,7,experiments,wmt21 biomedical translation task,Our systems in,english ? chinese and english ? german directions,wmt21 biomedical translation task Our systems in english ? chinese and english ? german directions,0.6779013276100159
translation,3,7,experiments,highest bleu scores,among,all submissions,highest bleu scores among all submissions,0.529674768447876
translation,3,32,experiments,german ? english translation task,employ,joint sentencepiece model ( spm ),german ? english translation task employ joint sentencepiece model ( spm ),0.5091531872749329
translation,3,32,experiments,joint sentencepiece model ( spm ),for,word segmentation,joint sentencepiece model ( spm ) for word segmentation,0.5850542783737183
translation,3,32,experiments,joint sentencepiece model ( spm ),with,size,joint sentencepiece model ( spm ) with size,0.6372353434562683
translation,3,32,experiments,word segmentation,with,size,word segmentation with size,0.6619372367858887
translation,3,32,experiments,size,of,vocabulary,size of vocabulary,0.5929076671600342
translation,3,32,experiments,vocabulary,set to,32k,vocabulary set to 32k,0.765925407409668
translation,3,33,experiments,jieba tokenizer,used for,chinese word segmentation,jieba tokenizer used for chinese word segmentation,0.5751668214797974
translation,3,33,experiments,moses tokenizer,for,english word segmentation,moses tokenizer for english word segmentation,0.544731080532074
translation,3,44,experiments,adam optimizer,with,"?1 = 0.9 , ?2 = 0.98","adam optimizer with ?1 = 0.9 , ?2 = 0.98",0.6221903562545776
translation,3,93,experiments,german ? english,fine - tune,model,german ? english fine - tune model,0.7205712199211121
translation,3,93,experiments,german ? english,see,improvement,german ? english see improvement,0.6444352269172668
translation,3,93,experiments,model,with,wmt18 and wmt19 test sets,model with wmt18 and wmt19 test sets,0.6354125738143921
translation,3,93,experiments,improvement,of,1.1 bleu,improvement of 1.1 bleu,0.5174891352653503
translation,3,12,model,data diversification,to generate,synthetic in- domain corpora,data diversification to generate synthetic in- domain corpora,0.685127317905426
translation,3,12,model,model,leverage,data diversification,model leverage data diversification,0.7908057570457458
translation,3,37,model,"transformer ( vaswani et al. , 2017 ) model architecture",adopts,full self-attention mechanism,"transformer ( vaswani et al. , 2017 ) model architecture adopts full self-attention mechanism",0.5705181360244751
translation,3,37,model,full self-attention mechanism,to realize,algorithm parallelism,full self-attention mechanism to realize algorithm parallelism,0.7123944163322449
translation,3,37,model,full self-attention mechanism,accelerate,model training speed,full self-attention mechanism accelerate model training speed,0.6369971632957458
translation,3,64,model,three monolingual enhancement strategies,to create,synthetic data,three monolingual enhancement strategies to create synthetic data,0.6059100031852722
translation,3,64,model,three monolingual enhancement strategies,perform,incremental training,three monolingual enhancement strategies perform incremental training,0.530802845954895
translation,3,64,model,synthetic data,add them to,in-domain training data,synthetic data add them to in-domain training data,0.6384685635566711
translation,3,64,model,in-domain training data,to further expand,scale,in-domain training data to further expand scale,0.6655837297439575
translation,3,64,model,scale,of,training data,scale of training data,0.6151946783065796
translation,3,64,model,model,use,three monolingual enhancement strategies,model use three monolingual enhancement strategies,0.6628091931343079
translation,3,65,results,our models,with,test sets,our models with test sets,0.6692066788673401
translation,3,65,results,test sets,from,previous years,test sets from previous years,0.559079110622406
translation,3,65,results,test sets,of further improving,in -domain performances,test sets of further improving in -domain performances,0.6884379386901855
translation,3,65,results,results,fine- tune,our models,results fine- tune our models,0.7262327075004578
translation,3,82,results,last year 's official best results,in,three language directions,last year 's official best results in three language directions,0.533032238483429
translation,3,82,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,3,82,results,outperform,has,last year 's official best results,outperform has last year 's official best results,0.6029011607170105
translation,3,82,results,results,has,our models,results has our models,0.5733726620674133
translation,3,86,results,our baseline models,trained by,wmt news data,our baseline models trained by wmt news data,0.7270055413246155
translation,3,86,results,our baseline models,may also perform,quite well,our baseline models may also perform quite well,0.6486797332763672
translation,3,86,results,quite well,in,biomedical field,quite well in biomedical field,0.49618402123451233
translation,3,86,results,results,notice that,our baseline models,results notice that our baseline models,0.6112399697303772
translation,3,91,results,data augmentation,results in,significant performance improvements,data augmentation results in significant performance improvements,0.6383220553398132
translation,3,91,results,significant performance improvements,with,1.1 bleu and 1.7 bleu,significant performance improvements with 1.1 bleu and 1.7 bleu,0.5808021426200867
translation,3,91,results,1.1 bleu and 1.7 bleu,on,german ? english and english ? german,1.1 bleu and 1.7 bleu on german ? english and english ? german,0.5037747621536255
translation,3,91,results,results,has,german ? english experiment,results has german ? english experiment,0.5615265965461731
translation,3,92,results,fine-tuning with previous years ' test sets,has,improved the quality of in-domain translations,fine-tuning with previous years ' test sets has improved the quality of in-domain translations,0.5690301656723022
translation,3,92,results,results,has,fine-tuning with previous years ' test sets,results has fine-tuning with previous years ' test sets,0.5717219114303589
translation,4,6,model,english - to - hindi,to translate,product reviews,english - to - hindi to translate product reviews,0.731934666633606
translation,4,6,model,neural machine translation ( nmt ) system,to translate,product reviews,neural machine translation ( nmt ) system to translate product reviews,0.6833544373512268
translation,4,6,model,product reviews,available on,e-commerce websites,product reviews available on e-commerce websites,0.5338107347488403
translation,4,6,model,product reviews,by creating,in-domain parallel corpora,product reviews by creating in-domain parallel corpora,0.6329001188278198
translation,4,6,model,various types of noise,in,reviews,various types of noise in reviews,0.5327246189117432
translation,4,6,model,reviews,via,two data augmentation techniques,reviews via two data augmentation techniques,0.666820764541626
translation,4,6,model,english - to - hindi,has,neural machine translation ( nmt ) system,english - to - hindi has neural machine translation ( nmt ) system,0.5505895018577576
translation,4,6,model,model,presenting,english - to - hindi,model presenting english - to - hindi,0.6707174181938171
translation,4,28,model,novel attention guided noise augmentation ( attnnoise ) technique,to make,nmt model robust,novel attention guided noise augmentation ( attnnoise ) technique to make nmt model robust,0.5801647305488586
translation,4,28,model,nmt model robust,towards,noisy inputs,nmt model robust towards noisy inputs,0.6713114380836487
translation,4,28,model,model,introduce,novel attention guided noise augmentation ( attnnoise ) technique,model introduce novel attention guided noise augmentation ( attnnoise ) technique,0.6595174074172974
translation,4,27,results,part-of- speech ( pos ) guided word embedding based and context aware word augmentation techniques,for,synthetic data creation,part-of- speech ( pos ) guided word embedding based and context aware word augmentation techniques for synthetic data creation,0.5734679102897644
translation,4,27,results,results,use,part-of- speech ( pos ) guided word embedding based and context aware word augmentation techniques,results use part-of- speech ( pos ) guided word embedding based and context aware word augmentation techniques,0.5723562836647034
translation,4,29,results,attnnoise method,has,significantly outperforms,attnnoise method has significantly outperforms,0.6141468286514282
translation,4,29,results,significantly outperforms,has,random noise injection ( rndnoise ) techniques,significantly outperforms has random noise injection ( rndnoise ) techniques,0.5856219530105591
translation,4,29,results,results,show,attnnoise method,results show attnnoise method,0.6550224423408508
translation,5,37,ablation-analysis,more impact,to,nmt,more impact to nmt,0.6227063536643982
translation,5,37,ablation-analysis,nmt,than,speaker information,nmt than speaker information,0.5037350058555603
translation,5,37,ablation-analysis,translator information,has,more impact,translator information has more impact,0.5503824949264526
translation,5,37,ablation-analysis,ablation analysis,show that,translator information,ablation analysis show that translator information,0.4996856153011322
translation,5,87,ablation-analysis,ineffective,to add,translator token,ineffective to add translator token,0.7304913997650146
translation,5,87,ablation-analysis,translator token,into,decoder,translator token into decoder,0.6077971458435059
translation,5,89,ablation-analysis,token,to,"encoder ( src - tok , enc - emb )","token to encoder ( src - tok , enc - emb )",0.5298718214035034
translation,5,89,ablation-analysis,token,are,significantly more effective,token are significantly more effective,0.5863289833068848
translation,5,124,ablation-analysis,all four directions,show,translator token,all four directions show translator token,0.6699092388153076
translation,5,124,ablation-analysis,translator token,has,more impact,translator token has more impact,0.5877870321273804
translation,5,124,ablation-analysis,translator token,has,more impact,translator token has more impact,0.5877870321273804
translation,5,124,ablation-analysis,ablation analysis,on,all four directions,ablation analysis on all four directions,0.5359702706336975
translation,5,22,baselines,discrete translator token,to model and control,translator -related stylistic variations in translation,discrete translator token to model and control translator -related stylistic variations in translation,0.7080738544464111
translation,5,69,experimental-setup,preprocessing,employ,"moses ( koehn et al. , 2007 ) tool","preprocessing employ moses ( koehn et al. , 2007 ) tool",0.604719877243042
translation,5,69,experimental-setup,"moses ( koehn et al. , 2007 ) tool",for,tokenization,"moses ( koehn et al. , 2007 ) tool for tokenization",0.5901491045951843
translation,5,69,experimental-setup,"moses ( koehn et al. , 2007 ) tool",apply,subword - nmt,"moses ( koehn et al. , 2007 ) tool apply subword - nmt",0.6455883383750916
translation,5,69,experimental-setup,subword - nmt,to learn,subword representations,subword - nmt to learn subword representations,0.5935890078544617
translation,5,69,experimental-setup,experimental setup,For,preprocessing,experimental setup For preprocessing,0.5661813020706177
translation,5,69,experimental-setup,experimental setup,apply,subword - nmt,experimental setup apply subword - nmt,0.6186390519142151
translation,5,72,experimental-setup,adam optimizer,to update,model parameters,adam optimizer to update model parameters,0.7320086359977722
translation,5,72,experimental-setup,experimental setup,employ,adam optimizer,experimental setup employ adam optimizer,0.5494607090950012
translation,5,73,experimental-setup,model,by,linearly increasing,model by linearly increasing,0.6365116834640503
translation,5,73,experimental-setup,model,by,decay it,model by decay it,0.6147608160972595
translation,5,73,experimental-setup,learning rate,from,1 ? 10 ?7 to 5 ? 10 ?4,learning rate from 1 ? 10 ?7 to 5 ? 10 ?4,0.5781847834587097
translation,5,73,experimental-setup,1 ? 10 ?7 to 5 ? 10 ?4,for,4000 updates,1 ? 10 ?7 to 5 ? 10 ?4 for 4000 updates,0.6918026208877563
translation,5,73,experimental-setup,decay it,with,inverse square root,decay it with inverse square root,0.7083251476287842
translation,5,73,experimental-setup,inverse square root,of,rest training steps,inverse square root of rest training steps,0.582011878490448
translation,5,73,experimental-setup,inverse square root,by,rate,inverse square root by rate,0.6276535391807556
translation,5,73,experimental-setup,rate,of,1 ? 10 ?4,rate of 1 ? 10 ?4,0.6188286542892456
translation,5,73,experimental-setup,linearly increasing,has,learning rate,linearly increasing has learning rate,0.5804010629653931
translation,5,73,experimental-setup,experimental setup,warm up,model,experimental setup warm up model,0.6913962364196777
translation,5,74,experimental-setup,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,5,74,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,5,74,experimental-setup,0.3,for,en-de,0.3 for en-de,0.6264965534210205
translation,5,74,experimental-setup,0.1,for,en-fr and en-it,0.1 for en-fr and en-it,0.6674880981445312
translation,5,74,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,5,75,experimental-setup,all mt systems,load,weights,all mt systems load weights,0.7859633564949036
translation,5,75,experimental-setup,weights,from,pretrained models,weights from pretrained models,0.5355296730995178
translation,5,75,experimental-setup,experimental setup,For,all mt systems,experimental setup For all mt systems,0.586317241191864
translation,5,76,experimental-setup,models,pretrained on,wmt data,models pretrained on wmt data,0.7430699467658997
translation,5,76,experimental-setup,models,pretrain,models,models pretrain models,0.6685341000556946
translation,5,76,experimental-setup,models,pretrain,en-it and en-es,models pretrain en-it and en-es,0.6953062415122986
translation,5,76,experimental-setup,models,for,en-it and en-es,models for en-it and en-es,0.7082003951072693
translation,5,76,experimental-setup,wmt data,for,en-de and en-fr,wmt data for en-de and en-fr,0.6227352619171143
translation,5,76,experimental-setup,models,for,en-it and en-es,models for en-it and en-es,0.7082003951072693
translation,5,76,experimental-setup,en-it and en-es,using,large in - house out - of - domain data,en-it and en-es using large in - house out - of - domain data,0.6845834851264954
translation,5,76,experimental-setup,experimental setup,employ,models,experimental setup employ models,0.5601065754890442
translation,5,76,experimental-setup,experimental setup,pretrain,models,experimental setup pretrain models,0.6920296549797058
translation,5,77,experimental-setup,models,on,ted talk data,models on ted talk data,0.5830769538879395
translation,5,77,experimental-setup,ted talk data,for,10 epochs,ted talk data for 10 epochs,0.635165810585022
translation,5,77,experimental-setup,best model,based on,validation loss,best model based on validation loss,0.5900523662567139
translation,5,77,experimental-setup,experimental setup,finetune,models,experimental setup finetune models,0.6702819466590881
translation,5,78,experimental-setup,inference,employ,beam search,inference employ beam search,0.570101797580719
translation,5,78,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,5,78,experimental-setup,beam search,add,length penalty,beam search add length penalty,0.6381945610046387
translation,5,78,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,5,78,experimental-setup,length penalty,of,0.4,length penalty of 0.4,0.5968765020370483
translation,5,78,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,5,70,experiments,"transformer ( vaswani et al. , 2017 )",as,baseline,"transformer ( vaswani et al. , 2017 ) as baseline",0.5852053165435791
translation,5,71,model,transformer model,comprised of,6 layers of encoder-decoder network,transformer model comprised of 6 layers of encoder-decoder network,0.6280705332756042
translation,5,71,model,each layer,contains,16 heads,each layer contains 16 heads,0.7026427388191223
translation,5,71,model,16 heads,with,self-attention hidden state,16 heads with self-attention hidden state,0.6437046527862549
translation,5,71,model,16 heads,with,feedforward hidden state,16 heads with feedforward hidden state,0.6494218111038208
translation,5,71,model,self-attention hidden state,of size,1024,self-attention hidden state of size 1024,0.6868571639060974
translation,5,71,model,feedforward hidden state,of size,4096,feedforward hidden state of size 4096,0.7084177732467651
translation,5,71,model,model,has,transformer model,model has transformer model,0.5662795305252075
translation,5,8,results,style-augmented translation models,able to capture,style variations,style-augmented translation models able to capture style variations,0.6955226063728333
translation,5,8,results,style-augmented translation models,to generate,translations,style-augmented translation models to generate translations,0.6654003858566284
translation,5,8,results,style variations,of,translators,style variations of translators,0.5719966888427734
translation,5,8,results,translations,with,different styles,translations with different styles,0.6471002697944641
translation,5,8,results,different styles,on,new data,different styles on new data,0.5648952722549438
translation,5,8,results,results,show,style-augmented translation models,results show style-augmented translation models,0.5712858438491821
translation,5,27,results,translator information,to,decoder,translator information to decoder,0.5827013850212097
translation,5,27,results,translator information,result in,nmt,translator information result in nmt,0.6882444620132446
translation,5,27,results,fully ignores,has,additional knowledge,fully ignores has additional knowledge,0.6034588813781738
translation,5,90,results,consistently better,at most by,0.4 bleu,consistently better at most by 0.4 bleu,0.566601037979126
translation,5,90,results,consistently better,than with,transformer baseline,consistently better than with transformer baseline,0.6828432083129883
translation,5,90,results,results,has,translation accuracy,results has translation accuracy,0.5236368179321289
translation,5,98,results,significantly higher,under,src -tok and enc -emb,significantly higher under src -tok and enc -emb,0.6478500366210938
translation,5,98,results,up to + 12 % relative ),under,src -tok and enc -emb,up to + 12 % relative ) under src -tok and enc -emb,0.680205762386322
translation,5,98,results,significantly higher,has,up to + 12 % relative ),significantly higher has up to + 12 % relative ),0.582524836063385
translation,5,98,results,results,has,classification accuracy,results has classification accuracy,0.540421724319458
translation,5,100,results,higher accuracy,achieved with,reference translations,higher accuracy achieved with reference translations,0.631085216999054
translation,6,62,ablation-analysis,out - of- domain data,in,in - 221 domain task,out - of- domain data in in - 221 domain task,0.5375402569770813
translation,6,62,ablation-analysis,out - of- domain data,caused,dramatic drop,out - of- domain data caused dramatic drop,0.6872674822807312
translation,6,62,ablation-analysis,dramatic drop,in,belu 222 score,dramatic drop in belu 222 score,0.5769972205162048
translation,6,62,ablation-analysis,ablation analysis,using,out - of- domain data,ablation analysis using out - of- domain data,0.678330659866333
translation,6,40,hyperparameters,vocabulary size,of,50 k tokens,vocabulary size of 50 k tokens,0.5921016335487366
translation,6,40,hyperparameters,data,by,transformer model,data by transformer model,0.597370982170105
translation,6,40,hyperparameters,transformer model,with,default parameters,transformer model with default parameters,0.5983361005783081
translation,6,40,hyperparameters,default parameters,using,"open-nmt ( klein et al. , 2017 )","default parameters using open-nmt ( klein et al. , 2017 )",0.5502734184265137
translation,6,40,hyperparameters,"open-nmt ( klein et al. , 2017 )",has,neural machine translation framework,"open-nmt ( klein et al. , 2017 ) has neural machine translation framework",0.5389968156814575
translation,6,40,hyperparameters,hyperparameters,selected,vocabulary size,hyperparameters selected vocabulary size,0.5734796524047852
translation,6,61,results,worst results,belong to,word level,worst results belong to word level,0.666754961013794
translation,6,61,results,worst results,belong to,hybrid wordlevel + subword level trainset,worst results belong to hybrid wordlevel + subword level trainset,0.6381462216377258
translation,6,61,results,results,has,worst results,results has worst results,0.5692225098609924
translation,7,22,hyperparameters,"mbart50 ( tang et al. , 2020 )",pre-trained with,50 languages,"mbart50 ( tang et al. , 2020 ) pre-trained with 50 languages",0.7408673763275146
translation,7,22,hyperparameters,hyperparameters,employ,pre-trained mbart model,hyperparameters employ pre-trained mbart model,0.5071061849594116
translation,7,22,hyperparameters,hyperparameters,employ,"mbart50 ( tang et al. , 2020 )","hyperparameters employ mbart50 ( tang et al. , 2020 )",0.5245257019996643
translation,7,23,hyperparameters,mbart50,on,both translation directions,mbart50 on both translation directions,0.5297203660011292
translation,7,23,hyperparameters,mbart50,to obtain,single multilingual model,mbart50 to obtain single multilingual model,0.5912579894065857
translation,7,23,hyperparameters,both translation directions,to obtain,single multilingual model,both translation directions to obtain single multilingual model,0.59438556432724
translation,7,23,hyperparameters,single multilingual model,for,task,single multilingual model for task,0.5777736306190491
translation,7,23,hyperparameters,hyperparameters,fine- tune,mbart50,hyperparameters fine- tune mbart50,0.7220877408981323
translation,7,8,results,fine-tuning mbart50,results in,31.69 bleu,fine-tuning mbart50 results in 31.69 bleu,0.5523003339767456
translation,7,8,results,fine-tuning mbart50,results in,23.63 bleu,fine-tuning mbart50 results in 23.63 bleu,0.55832839012146
translation,7,8,results,fine-tuning mbart50,increases,2.71 and 1.90 bleu,fine-tuning mbart50 increases 2.71 and 1.90 bleu,0.7054397463798523
translation,7,8,results,31.69 bleu,for,de- fr,31.69 bleu for de- fr,0.6424897909164429
translation,7,8,results,31.69 bleu,for,fr- de,31.69 bleu for fr- de,0.6251965761184692
translation,7,8,results,23.63 bleu,for,fr- de,23.63 bleu for fr- de,0.6464328169822693
translation,7,8,results,23.63 bleu,increases,2.71 and 1.90 bleu,23.63 bleu increases 2.71 and 1.90 bleu,0.6691531538963318
translation,7,25,results,our experiments,show,fine-tuned mbart50,our experiments show fine-tuned mbart50,0.6762139201164246
translation,7,25,results,fine-tuned mbart50,achieve,better translation quality,fine-tuned mbart50 achieve better translation quality,0.5835328698158264
translation,7,25,results,better translation quality,in,both directions,better translation quality in both directions,0.4864448010921478
translation,7,25,results,better translation quality,with,improvements,better translation quality with improvements,0.5830565690994263
translation,7,25,results,improvements,of,2.71,improvements of 2.71,0.5861209034919739
translation,7,25,results,improvements,of,1.9,improvements of 1.9,0.5924991369247437
translation,7,25,results,2.71,for,de- fr,2.71 for de- fr,0.6500122547149658
translation,7,25,results,1.9,for,fr- de,1.9 for fr- de,0.6364433765411377
translation,8,53,baselines,baselines,consider,three models,baselines consider three models,0.7083951234817505
translation,8,54,baselines,lstm encoderdecoder,is,single- layer gnmt,lstm encoderdecoder is single- layer gnmt,0.5161916613578796
translation,8,62,experimental-setup,separate source and target vocabularies,of about,32 k tokens,separate source and target vocabularies of about 32 k tokens,0.5664212703704834
translation,8,62,experimental-setup,32 k tokens,for,encoder- decoder models,32 k tokens for encoder- decoder models,0.5885951519012451
translation,8,62,experimental-setup,of about 50 k tokens,for,lm -style models,of about 50 k tokens for lm -style models,0.6217566728591919
translation,8,62,experimental-setup,joint vocabulary,has,of about 50 k tokens,joint vocabulary has of about 50 k tokens,0.6014222502708435
translation,8,170,experimental-setup,teacher,is,standard transformer - base,teacher is standard transformer - base,0.5952377915382385
translation,8,170,experimental-setup,standard transformer - base,from,fairseq,standard transformer - base from fairseq,0.596108615398407
translation,8,170,experimental-setup,experimental setup,has,teacher,experimental setup has teacher,0.5659071207046509
translation,8,177,results,earlier checkpoint,after,40 k iterations,earlier checkpoint after 40 k iterations,0.718743622303009
translation,8,177,results,earlier checkpoint,improve,nat quality,earlier checkpoint improve nat quality,0.6894490122795105
translation,8,177,results,nat quality,by,1.1 bleu,nat quality by 1.1 bleu,0.5374400019645691
translation,8,177,results,results,taking,earlier checkpoint,results taking earlier checkpoint,0.6973093748092651
translation,9,76,experimental-setup,shared vocabulary,for,all language pairs,shared vocabulary for all language pairs,0.621630072593689
translation,9,76,experimental-setup,"60,000 subword tokens",based on,bpe,"60,000 subword tokens based on bpe",0.6288573741912842
translation,9,76,experimental-setup,experimental setup,used,shared vocabulary,experimental setup used shared vocabulary,0.6133263111114502
translation,9,78,experimental-setup,( 2019 ),for,unmt,( 2019 ) for unmt,0.7306182980537415
translation,9,78,experimental-setup,six layers,for,encoder and the decoder,six layers for encoder and the decoder,0.6445223093032837
translation,9,78,experimental-setup,unmt,has,six layers,unmt has six layers,0.6322143077850342
translation,9,78,experimental-setup,experimental setup,used,transformer - based xlm toolkit,experimental setup used transformer - based xlm toolkit,0.580180287361145
translation,9,79,experimental-setup,dimensions,of,hidden layers,dimensions of hidden layers,0.5607795715332031
translation,9,79,experimental-setup,dimensions,set to,1024,dimensions set to 1024,0.7146978974342346
translation,9,79,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,9,80,experimental-setup,batch size,set to,2000 tokens,batch size set to 2000 tokens,0.6770256757736206
translation,9,80,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,9,81,experimental-setup,adam optimizer,to optimize,model parameters,adam optimizer to optimize model parameters,0.6930604577064514
translation,9,81,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,9,83,experimental-setup,specific cross-lingual language model,for,each different training dataset,specific cross-lingual language model for each different training dataset,0.5594722032546997
translation,9,83,experimental-setup,experimental setup,trained,specific cross-lingual language model,experimental setup trained specific cross-lingual language model,0.6905291676521301
translation,9,85,experimental-setup,eight v100 gpus,to train,all unmt models,eight v100 gpus to train all unmt models,0.6874677538871765
translation,9,85,experimental-setup,experimental setup,has,eight v100 gpus,experimental setup has eight v100 gpus,0.5365998148918152
translation,9,17,model,self-training mechanism,for,unmt,self-training mechanism for unmt,0.6339877247810364
translation,9,17,model,model,propose,self-training mechanism,model propose self-training mechanism,0.6587016582489014
translation,9,18,model,selftraining with unsupervised training ( st - ut ),self-training with,pseudo-supervised training ( st - pt ) strategies,selftraining with unsupervised training ( st - ut ) self-training with pseudo-supervised training ( st - pt ) strategies,0.7724719047546387
translation,9,18,model,pseudo-supervised training ( st - pt ) strategies,to train,robust unmt system,pseudo-supervised training ( st - pt ) strategies to train robust unmt system,0.6784910559654236
translation,9,18,model,robust unmt system,performs,better,robust unmt system performs better,0.6087504625320435
translation,9,18,model,model,propose,selftraining with unsupervised training ( st - ut ),model propose selftraining with unsupervised training ( st - ut ),0.6574345231056213
translation,9,18,model,model,self-training with,pseudo-supervised training ( st - pt ) strategies,model self-training with pseudo-supervised training ( st - pt ) strategies,0.7490589022636414
translation,9,92,results,corresponding baseline,in,all language pairs,corresponding baseline in all language pairs,0.535042941570282
translation,9,92,results,corresponding baseline,by,2 - 4 bleu points,corresponding baseline by 2 - 4 bleu points,0.5877724289894104
translation,9,92,results,proposed self-training mechanism,has,substantially outperformed,proposed self-training mechanism has substantially outperformed,0.5981351733207703
translation,9,92,results,substantially outperformed,has,corresponding baseline,substantially outperformed has corresponding baseline,0.6129670739173889
translation,9,92,results,results,has,proposed self-training mechanism,results has proposed self-training mechanism,0.580866813659668
translation,9,93,results,st - pt strategy,performed better,st - ut strategy,st - pt strategy performed better st - ut strategy,0.6946463584899902
translation,9,93,results,st - ut strategy,by,1 bleu point,st - ut strategy by 1 bleu point,0.593367874622345
translation,9,96,results,synthetic parallel data,could improve,translation performance,synthetic parallel data could improve translation performance,0.6331241726875305
translation,9,96,results,results,has,synthetic parallel data,results has synthetic parallel data,0.5282321572303772
translation,10,149,experimental-setup,translation models,adopt,encoderdecoder transformer,translation models adopt encoderdecoder transformer,0.6323831081390381
translation,10,149,experimental-setup,encoderdecoder transformer,) architecture,implementation,encoderdecoder transformer ) architecture implementation,0.7215557098388672
translation,10,150,experimental-setup,transformer - base architectures,has,6 encoder and decoder layers,transformer - base architectures has 6 encoder and decoder layers,0.5635956525802612
translation,10,150,experimental-setup,6 encoder and decoder layers,with,hidden dimension size,6 encoder and decoder layers with hidden dimension size,0.5922811031341553
translation,10,150,experimental-setup,hidden dimension size,being,512 and 8 attention heads,hidden dimension size being 512 and 8 attention heads,0.5857166051864624
translation,10,152,experimental-setup,model,trained for,200 k and 300k steps,model trained for 200 k and 300k steps,0.7982750535011292
translation,10,152,experimental-setup,200 k and 300k steps,for,ted and wmt,200 k and 300k steps for ted and wmt,0.6660516262054443
translation,10,152,experimental-setup,200 k and 300k steps,with,batch size,200 k and 300k steps with batch size,0.6775351762771606
translation,10,152,experimental-setup,batch size,of,"65,536 tokens","batch size of 65,536 tokens",0.5940939784049988
translation,10,152,experimental-setup,experimental setup,trained for,200 k and 300k steps,experimental setup trained for 200 k and 300k steps,0.7219254970550537
translation,10,152,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,10,153,experimental-setup,"sentencepiece ( kudo and richardson , 2018 ) vocabulary",for,english,"sentencepiece ( kudo and richardson , 2018 ) vocabulary for english",0.5584158301353455
translation,10,153,experimental-setup,"sentencepiece ( kudo and richardson , 2018 ) vocabulary",for,combined corpus of other languages,"sentencepiece ( kudo and richardson , 2018 ) vocabulary for combined corpus of other languages",0.5487127900123596
translation,10,154,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,10,154,experimental-setup,beam search,report,sacre-bleu score,beam search report sacre-bleu score,0.6075326800346375
translation,10,154,experimental-setup,beam size 5,for,decoding,beam size 5 for decoding,0.6803706288337708
translation,10,154,experimental-setup,experimental setup,report,sacre-bleu score,experimental setup report sacre-bleu score,0.5816199779510498
translation,10,6,model,new learning objective,for,mnmt,new learning objective for mnmt,0.6407970190048218
translation,10,6,model,new learning objective,minimizes,worst - case expected loss,new learning objective minimizes worst - case expected loss,0.6586620211601257
translation,10,6,model,mnmt,based on,distributionally robust optimization,mnmt based on distributionally robust optimization,0.5815305709838867
translation,10,6,model,worst - case expected loss,over,set of language pairs,worst - case expected loss over set of language pairs,0.6708610653877258
translation,10,6,model,model,propose,new learning objective,model propose new learning objective,0.7139577269554138
translation,10,7,model,large translation corpora,using,iterated best response scheme,large translation corpora using iterated best response scheme,0.6030014157295227
translation,10,7,model,iterated best response scheme,incurs,negligible additional computational cost,iterated best response scheme incurs negligible additional computational cost,0.7462044358253479
translation,10,7,model,negligible additional computational cost,compared to,standard empirical risk minimization,negligible additional computational cost compared to standard empirical risk minimization,0.6135967969894409
translation,10,20,model,proce-dure,for,multilingual translation,proce-dure for multilingual translation,0.6313880085945129
translation,10,21,model,learning,as,game,learning as game,0.6111995577812195
translation,10,21,model,game,between,learner,game between learner,0.6677811145782471
translation,10,21,model,game,between,adversary,game between adversary,0.6578925848007202
translation,10,21,model,hardest data distribution,within,uncertainty set q,hardest data distribution within uncertainty set q,0.6384111642837524
translation,10,25,model,corresponding optimization algorithm,amenable to,multilingual setting,corresponding optimization algorithm amenable to multilingual setting,0.7535126805305481
translation,10,25,model,model,propose,novel training objective,model propose novel training objective,0.6896026134490967
translation,10,25,model,model,propose,corresponding optimization algorithm,model propose corresponding optimization algorithm,0.661339282989502
translation,10,5,results,heavy data imbalance,between,languages,heavy data imbalance between languages,0.6275341510772705
translation,10,5,results,heavy data imbalance,hinders,model,heavy data imbalance hinders model,0.7025781869888306
translation,10,5,results,model,from performing uniformly across,language pairs,model from performing uniformly across language pairs,0.7399486899375916
translation,10,5,results,results,has,heavy data imbalance,results has heavy data imbalance,0.56809002161026
translation,10,23,results,existing methods,to,multilingual learning,existing methods to multilingual learning,0.49040448665618896
translation,10,23,results,existing methods,yields,inferior results,existing methods yields inferior results,0.6966468095779419
translation,10,23,results,multilingual learning,yields,inferior results,multilingual learning yields inferior results,0.7095313668251038
translation,10,23,results,inferior results,to,erm,inferior results to erm,0.5752959251403809
translation,10,23,results,results,naively applying,existing methods,results naively applying existing methods,0.6684342622756958
translation,10,162,results,outperforms,in terms of,average bleu score,outperforms in terms of average bleu score,0.6344548463821411
translation,10,162,results,all the other baseline methods,in terms of,average bleu score,all the other baseline methods in terms of average bleu score,0.6005671620368958
translation,10,162,results,average bleu score,over,all language pairs,average bleu score over all language pairs,0.6354020237922668
translation,10,162,results,outperforms,has,all the other baseline methods,outperforms has all the other baseline methods,0.560529351234436
translation,10,162,results,results,for,ted and wmt datasets,results for ted and wmt datasets,0.5890293717384338
translation,10,163,results,bleu scores,for,each individual language pairs,bleu scores for each individual language pairs,0.5578023791313171
translation,10,163,results,improves,over,almost all the language pairs,improves over almost all the language pairs,0.6808792948722839
translation,10,163,results,improves,for,both translation directions,improves for both translation directions,0.6052237153053284
translation,10,163,results,almost all the language pairs,for,both translation directions,almost all the language pairs for both translation directions,0.6021206974983215
translation,10,163,results,both translation directions,compared to,erm,both translation directions compared to erm,0.7390576601028442
translation,10,163,results,bleu scores,has,?- ibr,bleu scores has ?- ibr,0.6297919750213623
translation,10,163,results,each individual language pairs,has,?- ibr,each individual language pairs has ?- ibr,0.638796329498291
translation,10,163,results,?- ibr,has,improves,?- ibr has improves,0.7394167184829712
translation,10,169,results,improvements,are,larger,improvements are larger,0.6279048323631287
translation,10,169,results,larger,on,lrls,larger on lrls,0.6303174495697021
translation,10,169,results,improvements,on,hrls,improvements on hrls,0.5571517944335938
translation,10,169,results,results,consistently observe,improvements,results consistently observe improvements,0.7146520018577576
translation,10,171,results,?-ibr,achieves,more significant improvements,?-ibr achieves more significant improvements,0.7083312273025513
translation,10,171,results,more significant improvements,in,en?any direction,more significant improvements in en?any direction,0.5673503875732422
translation,10,171,results,en?any direction,than in,any ? en direction,en?any direction than in any ? en direction,0.700792670249939
translation,10,171,results,results,observe,?-ibr,results observe ?-ibr,0.6262096762657166
translation,10,197,results,none of the dro objectives,are,competitive,none of the dro objectives are competitive,0.5928722620010376
translation,10,197,results,competitive,with,temperature - weighted erm,competitive with temperature - weighted erm,0.6506152749061584
translation,10,200,results,cvar objective,on,all but one language pairs,cvar objective on all but one language pairs,0.5471940040588379
translation,10,200,results,fixed optimization algorithm,has,? 2 - group dro,fixed optimization algorithm has ? 2 - group dro,0.6129224896430969
translation,10,200,results,? 2 - group dro,has,outperforms,? 2 - group dro has outperforms,0.7002333402633667
translation,10,200,results,outperforms,has,cvar objective,outperforms has cvar objective,0.5772185921669006
translation,10,200,results,results,note,fixed optimization algorithm,results note fixed optimization algorithm,0.6413694024085999
translation,10,200,results,results,for,fixed optimization algorithm,results for fixed optimization algorithm,0.6322571039199829
translation,11,119,ablation-analysis,proper nouns and pronouns,not as important as,nouns,proper nouns and pronouns not as important as nouns,0.6338124871253967
translation,11,119,ablation-analysis,wsd,has,proper nouns and pronouns,wsd has proper nouns and pronouns,0.5232520699501038
translation,11,119,ablation-analysis,ablation analysis,For,wsd,ablation analysis For wsd,0.6113523840904236
translation,11,181,ablation-analysis,dot product alignment,over,attention,dot product alignment over attention,0.7232328057289124
translation,11,181,ablation-analysis,attention,in,decoder,attention in decoder,0.538677453994751
translation,11,182,ablation-analysis,model,learned,better alignment,model learned better alignment,0.7144197225570679
translation,11,182,ablation-analysis,better alignment,for,attention,better alignment for attention,0.615350067615509
translation,11,182,ablation-analysis,attention,in,decoder,attention in decoder,0.538677453994751
translation,11,185,ablation-analysis,encoder self-attention,seems to contribute,most,encoder self-attention seems to contribute most,0.7383836507797241
translation,11,185,ablation-analysis,most,to,translation performance,most to translation performance,0.5229630470275879
translation,11,185,ablation-analysis,most,to,contrastive evaluation,most to contrastive evaluation,0.5961734056472778
translation,11,185,ablation-analysis,ablation analysis,has,encoder self-attention,ablation analysis has encoder self-attention,0.5438876152038574
translation,11,191,ablation-analysis,attention regularization,especially,attnreg-,attention regularization especially attnreg-,0.7017444968223572
translation,11,191,ablation-analysis,attnreg-,observe,large drop,attnreg- observe large drop,0.6869217753410339
translation,11,191,ablation-analysis,large drop,in,contrastive performance,large drop in contrastive performance,0.5674084424972534
translation,11,191,ablation-analysis,contrastive performance,when,supporting context,contrastive performance when supporting context,0.6511393785476685
translation,11,191,ablation-analysis,supporting context,is,masked,supporting context is masked,0.5840147137641907
translation,11,192,ablation-analysis,score,after,masking,score after masking,0.74737548828125
translation,11,192,ablation-analysis,masking,is,significantly lower,masking is significantly lower,0.6124875545501709
translation,11,192,ablation-analysis,significantly lower,than,masking,significantly lower than masking,0.6153599619865417
translation,11,192,ablation-analysis,attnreg- pre,has,score,attnreg- pre has score,0.6155459880828857
translation,11,192,ablation-analysis,masking,has,supporting context,masking has supporting context,0.5932413339614868
translation,11,192,ablation-analysis,masking,has,all context,masking has all context,0.6092038750648499
translation,11,192,ablation-analysis,ablation analysis,for,attnreg- pre,ablation analysis for attnreg- pre,0.6904588341712952
translation,11,49,baselines,method,to regularize,attention,method to regularize attention,0.7143000364303589
translation,11,49,baselines,attention,towards,human-annotated disambiguating context ( ?5 ),attention towards human-annotated disambiguating context ( ?5 ),0.66822350025177
translation,11,158,baselines,baseline model,trained without,attention regularization,baseline model trained without attention regularization,0.7123790383338928
translation,11,158,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,11,159,baselines,two models,with,attention regularization,two models with attention regularization,0.5782536864280701
translation,11,159,baselines,attnreg-rand,jointly train on,mt objective,attnreg-rand jointly train on mt objective,0.7112153768539429
translation,11,159,baselines,attention,on,randomly initialized model,attention on randomly initialized model,0.5590416193008423
translation,11,159,baselines,attnreg - pre,first pre-train,model,attnreg - pre first pre-train model,0.7878482341766357
translation,11,159,baselines,attnreg - pre,jointly train on,mt objective,attnreg - pre jointly train on mt objective,0.6596673727035522
translation,11,159,baselines,attnreg - pre,regularize,attention,attnreg - pre regularize attention,0.7460023760795593
translation,11,159,baselines,model,solely on,mt objective,model solely on mt objective,0.5918439626693726
translation,11,159,baselines,model,solely on,mt objective,model solely on mt objective,0.5918439626693726
translation,11,159,baselines,model,jointly train on,mt objective,model jointly train on mt objective,0.6882693767547607
translation,11,50,results,attention regularization,is,effective technique,attention regularization is effective technique,0.5178874731063843
translation,11,50,results,effective technique,to encourage,models,effective technique to encourage models,0.7417001128196716
translation,11,50,results,models,to pay,more attention,models to pay more attention,0.7131180167198181
translation,11,50,results,more attention,to,words,more attention to words,0.5591425895690918
translation,11,50,results,words,humans find useful to resolve,ambiguity,words humans find useful to resolve ambiguity,0.7495456337928772
translation,11,50,results,ambiguity,in,translations,ambiguity in translations,0.5808462500572205
translation,11,50,results,results,find,attention regularization,results find attention regularization,0.5808096528053284
translation,11,51,results,our models,with,regularized attention,our models with regularized attention,0.615872323513031
translation,11,51,results,regularized attention,improving,translation quality,regularized attention improving translation quality,0.675177276134491
translation,11,51,results,regularized attention,yielding,relative improvement,regularized attention yielding relative improvement,0.6396337151527405
translation,11,51,results,translation quality,by,0.54 bleu,translation quality by 0.54 bleu,0.533362627029419
translation,11,51,results,relative improvement,of,14.7 %,relative improvement of 14.7 %,0.550304114818573
translation,11,51,results,regularized attention,has,outperform,regularized attention has outperform,0.5889331698417664
translation,11,51,results,outperform,has,previous context- aware baselines,outperform has previous context- aware baselines,0.5690655708312988
translation,11,51,results,results,has,our models,results has our models,0.5733726620674133
translation,11,97,results,target - side context,seems,more useful,target - side context seems more useful,0.7074694037437439
translation,11,97,results,more useful,than,source,more useful than source,0.579865574836731
translation,11,97,results,only target - side context,gives,higher answer accuracy,only target - side context gives higher answer accuracy,0.6047521233558655
translation,11,97,results,higher answer accuracy,than,only source -side context,higher answer accuracy than only source -side context,0.5492959022521973
translation,11,97,results,does not increase significantly,by having,both previous sentences,does not increase significantly by having both previous sentences,0.6550264954566956
translation,11,97,results,source,has,only target - side context,source has only target - side context,0.5647581219673157
translation,11,97,results,accuracy,has,does not increase significantly,accuracy has does not increase significantly,0.6134041547775269
translation,11,97,results,results,has,target - side context,results has target - side context,0.49236932396888733
translation,11,98,results,wsd,not observe,significant differences,wsd not observe significant differences,0.756019651889801
translation,11,98,results,significant differences,in,answer accuracy and confidence,significant differences in answer accuracy and confidence,0.5260260105133057
translation,11,98,results,answer accuracy and confidence,between,different context levels,answer accuracy and confidence between different context levels,0.6433932185173035
translation,11,117,results,nominals,are,most useful,nominals are most useful,0.5537030100822449
translation,11,117,results,most useful,for,par,most useful for par,0.7167408466339111
translation,11,117,results,results,find that,nominals,results find that nominals,0.661797046661377
translation,11,170,results,regularizing self-attention,in,top decoder layer,regularizing self-attention in top decoder layer,0.4690306484699249
translation,11,170,results,regularizing self-attention,gives,best scores,regularizing self-attention gives best scores,0.5668507218360901
translation,11,170,results,attnreg- pre,has,regularizing self-attention,attnreg- pre has regularizing self-attention,0.6044725775718689
translation,11,170,results,results,For,attnreg- pre,results For attnreg- pre,0.6868107914924622
translation,11,172,results,attnreg- rand,improves on,all metrics,attnreg- rand improves on all metrics,0.7080050706863403
translation,11,173,results,attnreg- pre,yields,considerable gains,attnreg- pre yields considerable gains,0.7662679553031921
translation,11,173,results,attnreg- pre,achieves,some improvement,attnreg- pre achieves some improvement,0.6963271498680115
translation,11,173,results,considerable gains,in,word f-measure,considerable gains in word f-measure,0.5549747943878174
translation,11,173,results,word f-measure,on,ambiguous pronouns,word f-measure on ambiguous pronouns,0.5118693113327026
translation,11,173,results,some improvement,over,baseline,some improvement over baseline,0.7137776613235474
translation,11,173,results,contrastive evaluation,on,big-par and par,contrastive evaluation on big-par and par,0.579357922077179
translation,11,173,results,attnreg- pre,has,does not improve,attnreg- pre has does not improve,0.6585985422134399
translation,11,173,results,does not improve,has,gen-eral translation scores significantly,does not improve has gen-eral translation scores significantly,0.6067320704460144
translation,11,173,results,results,has,attnreg- pre,results has attnreg- pre,0.6230554580688477
translation,11,174,results,attention regularization,with,supporting context,attention regularization with supporting context,0.5900953412055969
translation,11,174,results,supporting context,for,par,supporting context for par,0.7146077752113342
translation,11,174,results,results,has,attention regularization,results has attention regularization,0.5502508282661438
translation,11,176,results,both models,with,attention regularization,both models with attention regularization,0.5664086937904358
translation,11,176,results,no significant gains,in,wsd,no significant gains in wsd,0.5939432382583618
translation,11,176,results,results,For,both models,results For both models,0.580047070980072
translation,11,183,results,better alignment,than,attnreg-rand,better alignment than attnreg-rand,0.6034685373306274
translation,11,183,results,results,has,attnreg- pre,results has attnreg- pre,0.6230554580688477
translation,11,186,results,higher scores,on,metrics,higher scores on metrics,0.5195187926292419
translation,11,186,results,metrics,targeted to,pronoun translation,metrics targeted to pronoun translation,0.642218291759491
translation,11,190,results,varies little,when,supporting context,varies little when supporting context,0.6964359879493713
translation,11,190,results,supporting context,is,masked,supporting context is masked,0.5840147137641907
translation,11,190,results,baseline,has,varies little,baseline has varies little,0.5961495041847229
translation,11,190,results,results,find that,baseline,results find that baseline,0.6338460445404053
translation,11,193,results,both baseline and attnreg- rand,seem to rely more on,source context,both baseline and attnreg- rand seem to rely more on source context,0.693496584892273
translation,11,193,results,source context,than,target context,source context than target context,0.5462597012519836
translation,11,193,results,results,has,interesting finding,results has interesting finding,0.566483199596405
translation,12,86,ablation-analysis,simplified denoising training strategy,increase of,0.7 bleu,simplified denoising training strategy increase of 0.7 bleu,0.6591625213623047
translation,12,86,ablation-analysis,ablation analysis,has,simplified denoising training strategy,ablation analysis has simplified denoising training strategy,0.5065947771072388
translation,12,87,ablation-analysis,data augmentation techniques,result in,significant increase,data augmentation techniques result in significant increase,0.7087303400039673
translation,12,87,ablation-analysis,labse denoising,on,ru2zh data,labse denoising on ru2zh data,0.5613494515419006
translation,12,87,ablation-analysis,significant increase,of,1.9 bleu,significant increase of 1.9 bleu,0.5813084840774536
translation,12,87,ablation-analysis,data augmentation techniques,has,ftst method,data augmentation techniques has ftst method,0.549613893032074
translation,12,87,ablation-analysis,ablation analysis,noted,data augmentation techniques,ablation analysis noted data augmentation techniques,0.6306575536727905
translation,12,88,ablation-analysis,increase,of,0.6 bleu,increase of 0.6 bleu,0.609619677066803
translation,12,88,ablation-analysis,0.6 bleu,gained,ensemble,0.6 bleu gained ensemble,0.578992486000061
translation,12,88,ablation-analysis,0.6 bleu,via,ensemble,0.6 bleu via ensemble,0.6269869804382324
translation,12,44,baselines,"labse ( feng et al. , 2020 )",is,multilingual bert embedding model,"labse ( feng et al. , 2020 ) is multilingual bert embedding model",0.5113776326179504
translation,12,44,baselines,multilingual bert embedding model,measure,semantic similarities,multilingual bert embedding model measure semantic similarities,0.5852689146995544
translation,12,44,baselines,semantic similarities,across,languages,semantic similarities across languages,0.6960606575012207
translation,12,44,baselines,baselines,has,"labse ( feng et al. , 2020 )","baselines has labse ( feng et al. , 2020 )",0.5524108409881592
translation,12,83,baselines,baseline model,trained with,data,baseline model trained with data,0.7125710248947144
translation,12,83,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,12,60,experimental-setup,size,of,each batch,size of each batch,0.6566802859306335
translation,12,60,experimental-setup,size,of,parameter update frequency,size of parameter update frequency,0.5662137866020203
translation,12,60,experimental-setup,each batch,set as,2048,each batch set as 2048,0.670348048210144
translation,12,60,experimental-setup,parameter update frequency,as,32,parameter update frequency as 32,0.5660668611526489
translation,12,60,experimental-setup,learning rate,as,5e - 4,learning rate as 5e - 4,0.5762070417404175
translation,12,60,experimental-setup,label smoothing,as,0.1,label smoothing as 0.1,0.5090335607528687
translation,12,60,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,12,61,experimental-setup,number of warmup steps,is,4000,number of warmup steps is 4000,0.5959648489952087
translation,12,61,experimental-setup,dropout,is,0.1,dropout is 0.1,0.5574049949645996
translation,12,61,experimental-setup,experimental setup,has,number of warmup steps,experimental setup has number of warmup steps,0.5219058394432068
translation,12,61,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,12,62,experimental-setup,joint sentencepiece model,for,word segmentation,joint sentencepiece model for word segmentation,0.5829629898071289
translation,12,62,experimental-setup,size,of,vocabulary,size of vocabulary,0.5929076671600342
translation,12,62,experimental-setup,vocabulary,set to,32 k,vocabulary set to 32 k,0.7832134962081909
translation,12,62,experimental-setup,experimental setup,employ,joint sentencepiece model,experimental setup employ joint sentencepiece model,0.5187272429466248
translation,12,65,experimental-setup,inference phase,use,opensource marian,inference phase use opensource marian,0.6221617460250854
translation,12,65,experimental-setup,experimental setup,In,inference phase,experimental setup In inference phase,0.5257077217102051
translation,12,22,experiments,transformer - deep,features,35 - layer encoder,transformer - deep features 35 - layer encoder,0.653161346912384
translation,12,22,experiments,transformer - deep,features,6 - layer decoder,transformer - deep features 6 - layer decoder,0.6526795029640198
translation,12,22,experiments,transformer - deep,features,768 dimensions,transformer - deep features 768 dimensions,0.682805061340332
translation,12,22,experiments,transformer - deep,features,3072 - hidden-state,transformer - deep features 3072 - hidden-state,0.6531589031219482
translation,12,22,experiments,transformer - deep,features,16 - head self-attention,transformer - deep features 16 - head self-attention,0.6588339805603027
translation,12,22,experiments,transformer - deep,features,pre-norm,transformer - deep features pre-norm,0.6629202961921692
translation,12,22,experiments,768 dimensions,of,word vector,768 dimensions of word vector,0.5900354385375977
translation,12,63,experiments,jieba tokenizer,used for,chinese word segmentation,jieba tokenizer used for chinese word segmentation,0.5751668214797974
translation,12,63,experiments,moses tokenizer,for,english and russian word segmentation,moses tokenizer for english and russian word segmentation,0.5635706782341003
translation,12,7,hyperparameters,filtering,on,provided large-scale bilingual data,filtering on provided large-scale bilingual data,0.5218508243560791
translation,12,7,hyperparameters,hyperparameters,perform,detailed data pre-processing,hyperparameters perform detailed data pre-processing,0.6067922711372375
translation,12,13,model,multiple data filtering strategies,to enhance,data quality,multiple data filtering strategies to enhance data quality,0.6977945566177368
translation,12,13,model,multilingual model,has,pivot language,multilingual model has pivot language,0.5148653388023376
translation,12,13,model,data denoising,has,"wang et al. , 2018 ) strategies","data denoising has wang et al. , 2018 ) strategies",0.5324145555496216
translation,12,13,model,model,perform,multiple data filtering strategies,model perform multiple data filtering strategies,0.659120500087738
translation,12,13,model,model,leverage,multilingual model,model leverage multilingual model,0.719048798084259
translation,12,13,model,model,leverage,data denoising,model leverage data denoising,0.801101803779602
translation,12,67,model,multi-stage denoising training,with,data augmentation methods,multi-stage denoising training with data augmentation methods,0.5866475701332092
translation,12,67,model,model,combine,multi-stage denoising training,model combine multi-stage denoising training,0.6528597474098206
translation,12,9,results,our system,obtains,32.5 bleu,our system obtains 32.5 bleu,0.5281372666358948
translation,12,9,results,our system,obtains,27.7 bleu,our system obtains 27.7 bleu,0.5311981439590454
translation,12,9,results,our system,obtains,highest score,our system obtains highest score,0.5651198029518127
translation,12,9,results,32.5 bleu,on,dev set,32.5 bleu on dev set,0.5240519642829895
translation,12,9,results,27.7 bleu,on,test set,27.7 bleu on test set,0.49901115894317627
translation,12,9,results,results,has,our system,results has our system,0.5954442024230957
translation,12,84,results,bleu score,of,baseline model,bleu score of baseline model,0.5294237732887268
translation,12,84,results,baseline model,on,dev set,baseline model on dev set,0.534393310546875
translation,12,84,results,dev set,is,26.6,dev set is 26.6,0.6120631098747253
translation,12,84,results,results,has,bleu score,results has bleu score,0.5436024069786072
translation,12,85,results,our multilingual strategy,leads to,huge improvement,our multilingual strategy leads to huge improvement,0.6314758062362671
translation,12,85,results,huge improvement,of,2.7 bleu,huge improvement of 2.7 bleu,0.5212859511375427
translation,12,85,results,baseline model,has,our multilingual strategy,baseline model has our multilingual strategy,0.5744357109069824
translation,12,85,results,results,Comparing with,baseline model,results Comparing with baseline model,0.6776164174079895
translation,12,89,results,submitted system,gain,32.5 bleu,submitted system gain 32.5 bleu,0.691709041595459
translation,12,89,results,32.5 bleu,on,dev set,32.5 bleu on dev set,0.5240519642829895
translation,12,89,results,results,has,submitted system,results has submitted system,0.6237063407897949
translation,12,90,results,submitted model,gains,27.7 bleu,submitted model gains 27.7 bleu,0.7104120850563049
translation,12,90,results,27.7 bleu,on,wmt21 test set,27.7 bleu on wmt21 test set,0.5007456541061401
translation,12,97,results,results,of,baseline and denoising training model,results of baseline and denoising training model,0.5353672504425049
translation,12,97,results,baseline and denoising training model,see,increase,baseline and denoising training model see increase,0.5947151184082031
translation,12,97,results,increase,of,1.4 bleu,increase of 1.4 bleu,0.592400848865509
translation,12,97,results,results,compares,results,results compares results,0.6917222142219543
translation,12,97,results,results,of,baseline and denoising training model,results of baseline and denoising training model,0.5353672504425049
translation,12,100,results,our experiment,shows,full- data denoising training,our experiment shows full- data denoising training,0.6082324385643005
translation,12,100,results,full- data denoising training,leads to,increase,full- data denoising training leads to increase,0.6589175462722778
translation,12,100,results,full- data denoising training,leads to,increase,full- data denoising training leads to increase,0.6589175462722778
translation,12,100,results,increase,of,0.7 bleu,increase of 0.7 bleu,0.6080595850944519
translation,12,100,results,increase,of,0.5 bleu,increase of 0.5 bleu,0.6166480183601379
translation,12,100,results,increase,of,0.5 bleu,increase of 0.5 bleu,0.6166480183601379
translation,12,100,results,ru2zh data denoising,leads to,increase,ru2zh data denoising leads to increase,0.6703024506568909
translation,12,100,results,increase,of,0.5 bleu,increase of 0.5 bleu,0.6166480183601379
translation,12,100,results,results,has,our experiment,results has our experiment,0.5979328155517578
translation,12,102,results,data augmentation,leads to,huge bleu improvements,data augmentation leads to huge bleu improvements,0.6270713210105896
translation,12,102,results,system performance,leads to,huge bleu improvements,system performance leads to huge bleu improvements,0.6199032664299011
translation,12,102,results,data augmentation strategy,leads to,huge bleu improvements,data augmentation strategy leads to huge bleu improvements,0.645629346370697
translation,12,102,results,system performance,has,data augmentation strategy,system performance has data augmentation strategy,0.5584248304367065
translation,12,102,results,results,has,data augmentation,results has data augmentation,0.5241343379020691
translation,12,106,results,sampling back translation,lead to,better results,sampling back translation lead to better results,0.6840815544128418
translation,12,106,results,better results,about,0.3 bleu,better results about 0.3 bleu,0.5556557774543762
translation,12,109,results,combination of sampling bt and ft data ( ftst ),produce,best data augmentation effect,combination of sampling bt and ft data ( ftst ) produce best data augmentation effect,0.6313843131065369
translation,12,109,results,results,show,combination of sampling bt and ft data ( ftst ),results show combination of sampling bt and ft data ( ftst ),0.5952227115631104
translation,14,38,experimental-setup,pc,with,nvidia rtx 3090 gpu,pc with nvidia rtx 3090 gpu,0.5987768769264221
translation,14,38,experimental-setup,nvidia rtx 3090 gpu,supported by,i9 cpu,nvidia rtx 3090 gpu supported by i9 cpu,0.4925895631313324
translation,14,38,experimental-setup,experimental setup,used,pc,experimental setup used pc,0.6013845205307007
translation,14,51,experimental-setup,byte- pair encoding,used,rico sennrich 's python program bpe,byte- pair encoding used rico sennrich 's python program bpe,0.5891669392585754
translation,14,51,experimental-setup,experimental setup,For,byte- pair encoding,experimental setup For byte- pair encoding,0.5882764458656311
translation,14,39,experiments,24 gb,of,memory,24 gb of memory,0.5464227199554443
translation,14,39,experiments,24 gb,of,28.3 billion transistors,24 gb of 28.3 billion transistors,0.5700531005859375
translation,14,39,experiments,24 gb,of,35.58 tflops fp32 ( float ) performance,24 gb of 35.58 tflops fp32 ( float ) performance,0.5389176607131958
translation,14,39,experiments,24 gb,has,28.3 billion transistors,24 gb has 28.3 billion transistors,0.5480251908302307
translation,14,39,experiments,memory,has,28.3 billion transistors,memory has 28.3 billion transistors,0.5390148162841797
translation,14,39,experiments,state of the art,has,in 2021,state of the art has in 2021,0.5640749931335449
translation,14,83,results,our primary submissions,won,competition,our primary submissions won competition,0.7026678323745728
translation,14,83,results,our primary submissions,ranked,second,our primary submissions ranked second,0.7006562948226929
translation,14,83,results,byte pair encoding vocabulary size,of,"40,000","byte pair encoding vocabulary size of 40,000",0.5938888788223267
translation,14,83,results,competition,for,spanish ? catalan,competition for spanish ? catalan,0.6603516340255737
translation,14,83,results,second,for,other three language pairs,second for other three language pairs,0.618746817111969
translation,14,83,results,our primary submissions,has,byte pair encoding vocabulary size,our primary submissions has byte pair encoding vocabulary size,0.5356394648551941
translation,14,84,results,translation quality,to the most part,very good,translation quality to the most part very good,0.5808919668197632
translation,15,173,ablation-analysis,size of bpe,affects,model performance,size of bpe affects model performance,0.7283728122711182
translation,15,173,ablation-analysis,model performance,has,in low-resource settings,model performance has in low-resource settings,0.5332192182540894
translation,15,216,ablation-analysis,volt,reduces,softmax computations,volt reduces softmax computations,0.7019363045692444
translation,15,216,ablation-analysis,volt,does not significantly boost,softmax running time,volt does not significantly boost softmax running time,0.7061269879341125
translation,15,216,ablation-analysis,softmax running time,due to,optimized parallel computation,softmax running time due to optimized parallel computation,0.6414086818695068
translation,15,216,ablation-analysis,optimized parallel computation,in,gpus,optimized parallel computation in gpus,0.5015283823013306
translation,15,176,baselines,volt and bpe - 1 k,on,x-to - english bilingual setting,volt and bpe - 1 k on x-to - english bilingual setting,0.5679821968078613
translation,15,12,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
translation,15,12,experimental-setup,https,:,jingjing-nlp,https : jingjing-nlp,0.6523337960243225
translation,15,12,experimental-setup,https,/,jingjing-nlp,https / jingjing-nlp,0.678636908531189
translation,15,12,experimental-setup,//github.com,/,jingjing-nlp,//github.com / jingjing-nlp,0.5906397700309753
translation,15,12,experimental-setup,jingjing-nlp,has,/volt,jingjing-nlp has /volt,0.6800296306610107
translation,15,213,experimental-setup,maximum epochs,to,100,maximum epochs to 100,0.6341941952705383
translation,15,213,experimental-setup,last five models,as,final network,last five models as final network,0.5229575634002686
translation,15,213,experimental-setup,experimental setup,set,maximum epochs,experimental setup set maximum epochs,0.653889000415802
translation,15,214,experimental-setup,same environment,has,2 tesla-v100 - gpus + 1 gold-6130 - cpu,same environment has 2 tesla-v100 - gpus + 1 gold-6130 - cpu,0.5623219013214111
translation,15,202,experiments,volt,Beats,sentencepiece and wordpiece,volt Beats sentencepiece and wordpiece,0.781787633895874
translation,15,7,model,quest of vocabularization,finding,best token dictionary,quest of vocabularization finding best token dictionary,0.6776068210601807
translation,15,7,model,best token dictionary,as,optimal transport ( ot ) problem,best token dictionary as optimal transport ( ot ) problem,0.5425126552581787
translation,15,7,model,model,formulate,quest of vocabularization,model formulate quest of vocabularization,0.6781703233718872
translation,15,8,model,simple and efficient solution,without,trial training,simple and efficient solution without trial training,0.6902363300323486
translation,15,8,model,volt,has,simple and efficient solution,volt has simple and efficient solution,0.5897176265716553
translation,15,8,model,model,propose,volt,model propose volt,0.6557360887527466
translation,15,74,model,sequence of vocabularies,with,incremental sizes,sequence of vocabularies with incremental sizes,0.6566010117530823
translation,15,74,model,incremental sizes,via,bpe,incremental sizes via bpe,0.7141529321670532
translation,15,74,model,model,generate,sequence of vocabularies,model generate sequence of vocabularies,0.6932441592216492
translation,15,178,results,volt,find,good vocabulary,volt find good vocabulary,0.7016892433166504
translation,15,178,results,good vocabulary,on par with,heuristically searched vocabularies,good vocabulary on par with heuristically searched vocabularies,0.6867548823356628
translation,15,178,results,heuristically searched vocabularies,in terms of,bleu scores,heuristically searched vocabularies in terms of bleu scores,0.603899359703064
translation,15,178,results,results,see that,volt,results see that volt,0.5828496813774109
translation,15,200,results,volt,achieves,almost the best performance,volt achieves almost the best performance,0.730728030204773
translation,15,200,results,almost the best performance,with,much smaller vocabulary,almost the best performance with much smaller vocabulary,0.6700276136398315
translation,15,207,results,volt,outperforms,sentencepiece and wordpiece,volt outperforms sentencepiece and wordpiece,0.7677856087684631
translation,15,207,results,sentencepiece and wordpiece,by,large margin,sentencepiece and wordpiece by large margin,0.5826756358146667
translation,15,207,results,large margin,with,over 1 bleu improvements,large margin with over 1 bleu improvements,0.6526550650596619
translation,15,207,results,results,observe,volt,results observe volt,0.5193942189216614
translation,16,120,experiments,monolingual word and semantic sentence embeddings,use,roberta - large model,monolingual word and semantic sentence embeddings use roberta - large model,0.5576992630958557
translation,16,120,experiments,roberta - large model,shown,best performance,roberta - large model shown best performance,0.6689419746398926
translation,16,138,experiments,sss + wmd,achieves,best result,sss + wmd achieves best result,0.6941767334938049
translation,16,138,experiments,best result,in,monolingual multi-30 k tasks,best result in monolingual multi-30 k tasks,0.5054264664649963
translation,16,138,experiments,best result,using,xlm - roberta - base embeddings,best result using xlm - roberta - base embeddings,0.6822381019592285
translation,16,138,experiments,monolingual multi-30 k tasks,for,french to french,monolingual multi-30 k tasks for french to french,0.5817066431045532
translation,16,138,experiments,french to french,using,xlm - roberta - base embeddings,french to french using xlm - roberta - base embeddings,0.7196240425109863
translation,16,138,experiments,machine generated sentence to reference sentence case,has,sss + wmd,machine generated sentence to reference sentence case has sss + wmd,0.6054553389549255
translation,16,116,hyperparameters,semantic sentence embedding,used,xlm - roberta - base embeddings,semantic sentence embedding used xlm - roberta - base embeddings,0.5603360533714294
translation,16,116,hyperparameters,xlm - roberta - base embeddings,from,sentence transformer,xlm - roberta - base embeddings from sentence transformer,0.5675439834594727
translation,16,116,hyperparameters,xlm - roberta - base embeddings,trained on,"snli ( bowman et al. , 2015 ) + multinli ( williams et al. , 2018 )","xlm - roberta - base embeddings trained on snli ( bowman et al. , 2015 ) + multinli ( williams et al. , 2018 )",0.7009734511375427
translation,16,116,hyperparameters,xlm - roberta - base embeddings,fine-tuned on,sts benchmark training data,xlm - roberta - base embeddings fine-tuned on sts benchmark training data,0.704723596572876
translation,16,116,hyperparameters,hyperparameters,For,semantic sentence embedding,hyperparameters For semantic sentence embedding,0.5424250364303589
translation,16,5,model,well performing unsupervised alternative sentsim,relying on,strong pretrained multilingual word and sentence representations,well performing unsupervised alternative sentsim relying on strong pretrained multilingual word and sentence representations,0.6652872562408447
translation,16,5,model,more cost-effective,has,well performing unsupervised alternative sentsim,more cost-effective has well performing unsupervised alternative sentsim,0.632135808467865
translation,16,5,model,model,propose,more cost-effective,model propose more cost-effective,0.65093594789505
translation,16,25,model,bag-of-embeddings distance metrics,such as,bertscore,bag-of-embeddings distance metrics such as bertscore,0.5777259469032288
translation,16,25,model,model,introduce,sentence semantic similarity ( sss ),model introduce sentence semantic similarity ( sss ),0.6126379370689392
translation,16,26,model,semantic similarity,sentence levelbased on,sentence embeddings,semantic similarity sentence levelbased on sentence embeddings,0.7757964134216309
translation,16,26,model,semantic similarity,linearly combine it with,existing metrics,semantic similarity linearly combine it with existing metrics,0.6126612424850464
translation,16,26,model,existing metrics,that use,word embeddings,existing metrics that use word embeddings,0.634230375289917
translation,16,26,model,model,explore,semantic similarity,model explore semantic similarity,0.6423714756965637
translation,16,157,model,best layer,seems to be,17,best layer seems to be 17,0.6800263524055481
translation,16,157,model,24 output layers,has,best layer,24 output layers has best layer,0.5644987225532532
translation,16,157,model,model,Among,24 output layers,model Among 24 output layers,0.6095651388168335
translation,16,6,results,word mover 's distance,incorporating,notion of sentence semantic similarity,word mover 's distance incorporating notion of sentence semantic similarity,0.6795760989189148
translation,16,38,results,outperforms,on,various mt datasets,outperforms on various mt datasets,0.5279279947280884
translation,16,38,results,existing metrics,on,various mt datasets,existing metrics on various mt datasets,0.4635465145111084
translation,16,38,results,various mt datasets,in,monolingual and crosslingual settings,various mt datasets in monolingual and crosslingual settings,0.48217421770095825
translation,16,38,results,sentsim metric,has,outperforms,sentsim metric has outperforms,0.6285004019737244
translation,16,38,results,outperforms,has,existing metrics,outperforms has existing metrics,0.5908446311950684
translation,16,38,results,results,has,sentsim metric,results has sentsim metric,0.5925389528274536
translation,16,125,results,sss + wmd,has,outperforms,sss + wmd has outperforms,0.6310788989067078
translation,16,125,results,outperforms,has,all individual metrics,outperforms has all individual metrics,0.583467960357666
translation,16,125,results,outperforms,has,other combined metrics,outperforms has other combined metrics,0.5946329832077026
translation,16,125,results,results,reveals that,sss + wmd,results reveals that sss + wmd,0.662743330001831
translation,16,126,results,sss,better than both,wmd,sss better than both wmd,0.7329959869384766
translation,16,126,results,sss,better than both,bertscore,sss better than both bertscore,0.711757481098175
translation,16,126,results,outperforming,has,bertscore,outperforming has bertscore,0.5988250374794006
translation,16,128,results,outperforms,has,wmd and bertscore,outperforms has wmd and bertscore,0.6048978567123413
translation,16,129,results,sss + bertscore,showed,best and more stable performance,sss + bertscore showed best and more stable performance,0.7163011431694031
translation,16,129,results,best and more stable performance,for,all language pairs,best and more stable performance for all language pairs,0.5734347701072693
translation,16,129,results,all language pairs,in,wmt - 17 dataset,all language pairs in wmt - 17 dataset,0.4935624599456787
translation,16,129,results,results,has,sss + bertscore,results has sss + bertscore,0.5479956269264221
translation,16,134,results,outperforms,in,wmt - 20 dataset,outperforms in wmt - 20 dataset,0.5370739102363586
translation,16,134,results,wmd and bertscore,in,wmt - 20 dataset,wmd and bertscore in wmt - 20 dataset,0.5505285263061523
translation,16,134,results,sss,has,outperforms,sss has outperforms,0.6529795527458191
translation,16,134,results,outperforms,has,wmd and bertscore,outperforms has wmd and bertscore,0.6048978567123413
translation,16,134,results,results,has,sss,results has sss,0.5827586650848389
translation,16,135,results,sss + bertscore,reaches,best performance,sss + bertscore reaches best performance,0.749846875667572
translation,16,135,results,sss + bertscore,reaches,best metric,sss + bertscore reaches best metric,0.718230128288269
translation,16,135,results,best performance,in,three out of seven language pairs,best performance in three out of seven language pairs,0.4972335398197174
translation,16,135,results,best metric,in comparison with,bertscore or wmd alone,best metric in comparison with bertscore or wmd alone,0.6616244316101074
translation,16,135,results,results,has,sss + bertscore,results has sss + bertscore,0.5479956269264221
translation,16,142,results,two variants,of,combined metrics,two variants of combined metrics,0.5833961367607117
translation,16,142,results,outperform,has,any metric,outperform has any metric,0.622140109539032
translation,16,142,results,results,has,two variants,results has two variants,0.5471349358558655
translation,16,146,results,metrics ' performances,in the case of,src - mt,metrics ' performances in the case of src - mt,0.6163551211357117
translation,16,146,results,src - mt,are,much lower,src - mt are much lower,0.6116762161254883
translation,16,146,results,much lower,than in,mt - ref setting,much lower than in mt - ref setting,0.7015343308448792
translation,16,146,results,results,has,metrics ' performances,results has metrics ' performances,0.5484830141067505
translation,16,181,results,results,has,overall performance,results has overall performance,0.5951287150382996
translation,16,182,results,smd metric performance,combined with,token - level metrics,smd metric performance combined with token - level metrics,0.6536080837249756
translation,16,182,results,improves,combined with,token - level metrics,improves combined with token - level metrics,0.6886820197105408
translation,16,182,results,smd metric performance,has,improves,smd metric performance has improves,0.6019209027290344
translation,17,147,ablation-analysis,amount,of,supervised samples,amount of supervised samples,0.6218007206916809
translation,17,147,ablation-analysis,supervised samples,of,block,supervised samples of block,0.6041420102119446
translation,17,147,ablation-analysis,supervised samples,changes in,bleu score,supervised samples changes in bleu score,0.5723734498023987
translation,17,147,ablation-analysis,block,recommended by,proposed sampling techniques,block recommended by proposed sampling techniques,0.6695178151130676
translation,17,147,ablation-analysis,proposed sampling techniques,with,"20 , 40 , 60 and 80 %","proposed sampling techniques with 20 , 40 , 60 and 80 %",0.680676281452179
translation,17,147,ablation-analysis,ablation analysis,increasing,amount,ablation analysis increasing amount,0.7311774492263794
translation,17,200,ablation-analysis,reduction,of,human efforts,reduction of human efforts,0.5917996764183044
translation,17,200,ablation-analysis,human efforts,in terms of,word stroke ratio ( wsr ),human efforts in terms of word stroke ratio ( wsr ),0.702509343624115
translation,17,200,ablation-analysis,ablation analysis,observed,reduction,ablation analysis observed reduction,0.663178026676178
translation,17,210,ablation-analysis,"english-to - german , english-to -hindi and spanish - to - english translation",reduction in,wsr,"english-to - german , english-to -hindi and spanish - to - english translation reduction in wsr",0.6428934335708618
translation,17,210,ablation-analysis,wsr,by,"5 , 7 and 4 points","wsr by 5 , 7 and 4 points",0.6151672005653381
translation,17,210,ablation-analysis,wsr,over,ads,wsr over ads,0.7146399617195129
translation,17,210,ablation-analysis,ablation analysis,For,"english-to - german , english-to -hindi and spanish - to - english translation","ablation analysis For english-to - german , english-to -hindi and spanish - to - english translation",0.5923417210578918
translation,17,205,baselines,english-to-hindi and spanish-to - english,along with,qbc,english-to-hindi and spanish-to - english along with qbc,0.643750011920929
translation,17,205,baselines,second best-performing sampling techniques,are,nec and sim emb,second best-performing sampling techniques are nec and sim emb,0.6139819622039795
translation,17,205,baselines,english-to-hindi and spanish-to - english,has,second best-performing sampling techniques,english-to-hindi and spanish-to - english has second best-performing sampling techniques,0.5750670433044434
translation,17,205,baselines,qbc,has,second best-performing sampling techniques,qbc has second best-performing sampling techniques,0.5951802730560303
translation,17,154,experiments,sim emb,performs,best,sim emb performs best,0.639964759349823
translation,17,154,experiments,sim emb,achieves,26.90,sim emb achieves 26.90,0.6788107752799988
translation,17,154,experiments,26.90,which is,1.59 bleu,26.90 which is 1.59 bleu,0.6232244372367859
translation,17,156,experiments,german-to - english translation,observed,bleu score,german-to - english translation observed bleu score,0.6114534735679626
translation,17,156,experiments,bleu score,of,24.08,bleu score of 24.08,0.5272129774093628
translation,17,156,experiments,24.08,without using,active learning,24.08 without using active learning,0.6686652302742004
translation,17,160,experiments,initial bleu score,observed to be,25.76,initial bleu score observed to be 25.76,0.6275213956832886
translation,17,160,experiments,english-to - hindi translation,has,initial bleu score,english-to - hindi translation has initial bleu score,0.535216748714447
translation,17,172,experiments,spanish -to - english translation,see that,sim emb,spanish -to - english translation see that sim emb,0.6558879613876343
translation,17,172,experiments,random sampling,by,0.77 bleu points,random sampling by 0.77 bleu points,0.5659865140914917
translation,17,172,experiments,sim emb,has,significantly outperforms,sim emb has significantly outperforms,0.6446043848991394
translation,17,172,experiments,significantly outperforms,has,random sampling,significantly outperforms has random sampling,0.5973073244094849
translation,17,134,hyperparameters,hidden sizes,set to,512,hidden sizes set to 512,0.7126492857933044
translation,17,134,hyperparameters,dropout rate,set to,0.1,dropout rate set to 0.1,0.6670177578926086
translation,17,134,hyperparameters,hyperparameters,has,embedding size,hyperparameters has embedding size,0.4976881444454193
translation,17,134,hyperparameters,hyperparameters,has,hidden sizes,hyperparameters has hidden sizes,0.5404813885688782
translation,17,134,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,17,136,hyperparameters,adam optimizer,used for,training,adam optimizer used for training,0.6588284969329834
translation,17,136,hyperparameters,training,with,"8,000 warm up steps","training with 8,000 warm up steps",0.6378626823425293
translation,17,136,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,17,137,hyperparameters,"bpe ( sennrich et al. , 2016 )",with,vocabulary size,"bpe ( sennrich et al. , 2016 ) with vocabulary size",0.5659102201461792
translation,17,137,hyperparameters,vocabulary size,of,40k,vocabulary size of 40k,0.6079103350639343
translation,17,137,hyperparameters,hyperparameters,used,"bpe ( sennrich et al. , 2016 )","hyperparameters used bpe ( sennrich et al. , 2016 )",0.5566585659980774
translation,17,138,hyperparameters,models,trained with,opennmt toolkit,models trained with opennmt toolkit,0.7091933488845825
translation,17,138,hyperparameters,opennmt toolkit,with,batch size,opennmt toolkit with batch size,0.6115517616271973
translation,17,138,hyperparameters,batch size,of,"2,048 tokens","batch size of 2,048 tokens",0.5985899567604065
translation,17,138,hyperparameters,"2,048 tokens",till,convergence,"2,048 tokens till convergence",0.6558350324630737
translation,17,138,hyperparameters,opennmt toolkit,has,"( klein et al. , 2020 )","opennmt toolkit has ( klein et al. , 2020 )",0.5308389067649841
translation,17,138,hyperparameters,hyperparameters,trained with,opennmt toolkit,hyperparameters trained with opennmt toolkit,0.705898642539978
translation,17,138,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,17,139,hyperparameters,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,17,139,hyperparameters,inference,has,beam size,inference has beam size,0.5730389356613159
translation,17,139,hyperparameters,hyperparameters,During,inference,hyperparameters During inference,0.6701886057853699
translation,17,4,model,model,has,interactive - predictive translation,model has interactive - predictive translation,0.603775680065155
translation,17,6,model,sentence similarity ( sim ) ) sampling techniques,to find,ideal candidates,sentence similarity ( sim ) ) sampling techniques to find ideal candidates,0.6194797158241272
translation,17,6,model,ideal candidates,from,incoming data,ideal candidates from incoming data,0.5966468453407288
translation,17,6,model,ideal candidates,for,human supervision,ideal candidates for human supervision,0.5851313471794128
translation,17,6,model,ideal candidates,for,mt model 's weight updation,ideal candidates for mt model 's weight updation,0.6121958494186401
translation,17,6,model,model,explore,term based ( named entity count ( nec ) ),model explore term based ( named entity count ( nec ) ),0.6759713888168335
translation,17,6,model,model,explore,quality based ( quality estimation ( qe ),model explore quality based ( quality estimation ( qe ),0.6765451431274414
translation,17,6,model,model,explore,sentence similarity ( sim ) ) sampling techniques,model explore sentence similarity ( sim ) ) sampling techniques,0.6780064702033997
translation,17,133,model,6 layered encoder - decoder stacks,with,8 attention heads,6 layered encoder - decoder stacks with 8 attention heads,0.6075465679168701
translation,17,133,model,model,used,6 layered encoder - decoder stacks,model used 6 layered encoder - decoder stacks,0.6020259261131287
translation,17,135,model,feed-forward layer,consists of,"2,048 cells","feed-forward layer consists of 2,048 cells",0.647330105304718
translation,17,135,model,model,has,feed-forward layer,model has feed-forward layer,0.5812851190567017
translation,17,9,results,our proposed sampling technique,yields,"1.82 , 0.77 and 0.81 bleu points","our proposed sampling technique yields 1.82 , 0.77 and 0.81 bleu points",0.6878819465637207
translation,17,9,results,"1.82 , 0.77 and 0.81 bleu points",over,random sampling based baseline,"1.82 , 0.77 and 0.81 bleu points over random sampling based baseline",0.6322333812713623
translation,17,9,results,improvements,for,"german-english , spanish -english and hindi-english","improvements for german-english , spanish -english and hindi-english",0.6258934140205383
translation,17,9,results,"1.82 , 0.77 and 0.81 bleu points",has,improvements,"1.82 , 0.77 and 0.81 bleu points has improvements",0.5555071234703064
translation,17,9,results,results,has,our proposed sampling technique,results has our proposed sampling technique,0.5687376260757446
translation,17,152,results,trained nmt,to,new samples,trained nmt to new samples,0.5473305583000183
translation,17,152,results,new samples,recommended by,random sampling,new samples recommended by random sampling,0.6463253498077393
translation,17,152,results,increases,upto,25.31,increases upto 25.31,0.7117736339569092
translation,17,152,results,25.31,when,80 %,25.31 when 80 %,0.6318950057029724
translation,17,152,results,2.03 bleu points improvement,over,initial score,2.03 bleu points improvement over initial score,0.605983555316925
translation,17,152,results,trained nmt,has,bleu score,trained nmt has bleu score,0.5246474742889404
translation,17,152,results,new samples,has,bleu score,new samples has bleu score,0.5746071934700012
translation,17,152,results,bleu score,has,increases,bleu score has increases,0.6288427114486694
translation,17,152,results,80 %,has,of the samples of block,80 % has of the samples of block,0.6212463974952698
translation,17,152,results,results,adapting,trained nmt,results adapting trained nmt,0.736375093460083
translation,17,153,results,random sampling,proposed sampling techniques,qe,random sampling proposed sampling techniques qe,0.7917847633361816
translation,17,153,results,random sampling,proposed sampling techniques,sim f uzzy,random sampling proposed sampling techniques sim f uzzy,0.7423056364059448
translation,17,153,results,random sampling,proposed sampling techniques,nec,random sampling proposed sampling techniques nec,0.780846893787384
translation,17,153,results,nec,yield,"26.17 , 26.90 , 26.68 and 26.84 bleu scores","nec yield 26.17 , 26.90 , 26.68 and 26.84 bleu scores",0.7066997289657593
translation,17,153,results,80 %,of,samples,80 % of samples,0.6814351081848145
translation,17,153,results,samples,in,block,samples in block,0.6006568074226379
translation,17,153,results,results,Compared to,random sampling,results Compared to random sampling,0.6394362449645996
translation,17,155,results,combined opinion of sampling techniques ( i.e. qbc ),produced,27.13 bleu points,combined opinion of sampling techniques ( i.e. qbc ) produced 27.13 bleu points,0.6162680983543396
translation,17,155,results,27.13 bleu points,is,1.82 bleu improvement,27.13 bleu points is 1.82 bleu improvement,0.5432263016700745
translation,17,155,results,outperformed,has,other methods,outperformed has other methods,0.615522027015686
translation,17,157,results,baseline inmt system,brought about,27.05 bleu points,baseline inmt system brought about 27.05 bleu points,0.5944115519523621
translation,17,157,results,27.05 bleu points,on,test set,27.05 bleu points on test set,0.5049206614494324
translation,17,157,results,results,has,baseline inmt system,results has baseline inmt system,0.5842126607894897
translation,17,158,results,inmt system,with,sentence -similarity sampling feature ( i.e. sim emb ),inmt system with sentence -similarity sampling feature ( i.e. sim emb ),0.613142192363739
translation,17,158,results,sentence -similarity sampling feature ( i.e. sim emb ),surpassed,baseline,sentence -similarity sampling feature ( i.e. sim emb ) surpassed baseline,0.7365113496780396
translation,17,158,results,baseline,by,0.94 bleu points,baseline by 0.94 bleu points,0.5356763005256653
translation,17,158,results,results,has,inmt system,results has inmt system,0.5846540331840515
translation,17,159,results,all the other sampling methods,achieve,28.13 bleu points,all the other sampling methods achieve 28.13 bleu points,0.592253565788269
translation,17,159,results,28.13 bleu points,on,test set,28.13 bleu points on test set,0.4939424991607666
translation,17,159,results,improvement,of,1.08 points,improvement of 1.08 points,0.5385465025901794
translation,17,159,results,1.08 points,over,random sampling technique,1.08 points over random sampling technique,0.6450997591018677
translation,17,159,results,qbc method,has,outperforms,qbc method has outperforms,0.6276921033859253
translation,17,159,results,outperforms,has,all the other sampling methods,outperforms has all the other sampling methods,0.5866941809654236
translation,17,159,results,results,has,qbc method,results has qbc method,0.5404346585273743
translation,17,166,results,qbc,found to be,best performing sampling method,qbc found to be best performing sampling method,0.6445761322975159
translation,17,166,results,qbc,provides,gain,qbc provides gain,0.6881579160690308
translation,17,166,results,gain,of,0.81 bleu points,gain of 0.81 bleu points,0.5381298661231995
translation,17,166,results,0.81 bleu points,over,baseline,0.81 bleu points over baseline,0.6275511980056763
translation,17,167,results,sim emb,yields,comparable score,sim emb yields comparable score,0.7368119359016418
translation,17,167,results,spanish -to - english translation,has,sim emb,spanish -to - english translation has sim emb,0.5995517373085022
translation,17,167,results,results,for,spanish -to - english translation,results for spanish -to - english translation,0.5999910235404968
translation,17,170,results,baseline inmt model,produces,26.83 bleu points,baseline inmt model produces 26.83 bleu points,0.5613375306129456
translation,17,170,results,26.83 bleu points,on,test set,26.83 bleu points on test set,0.48945337533950806
translation,17,170,results,26.83 bleu points,corresponds to,absolute improvement,26.83 bleu points corresponds to absolute improvement,0.5985565185546875
translation,17,170,results,absolute improvement,of,1.07 bleu points,absolute improvement of 1.07 bleu points,0.5123637318611145
translation,17,170,results,1.07 bleu points,over,vanilla nmt system,1.07 bleu points over vanilla nmt system,0.6543810963630676
translation,17,170,results,english-to- hindi,has,baseline inmt model,english-to- hindi has baseline inmt model,0.5520187616348267
translation,17,170,results,results,For,english-to- hindi,results For english-to- hindi,0.5825427174568176
translation,17,171,results,nec,found to be,best-performing sampling technique,nec found to be best-performing sampling technique,0.6419731974601746
translation,17,171,results,nec,yields,27.64 bleu points,nec yields 27.64 bleu points,0.7148349285125732
translation,17,171,results,27.64 bleu points,with,absolute improvement,27.64 bleu points with absolute improvement,0.5828731060028076
translation,17,171,results,absolute improvement,of,0.82 bleu points,absolute improvement of 0.82 bleu points,0.5220708847045898
translation,17,171,results,0.82 bleu points,over,baseline ( random sampling ),0.82 bleu points over baseline ( random sampling ),0.6213868856430054
translation,17,171,results,results,has,nec,results has nec,0.475076287984848
translation,17,173,results,respective best-performing sampling techniques,bring about,gains,respective best-performing sampling techniques bring about gains,0.688592255115509
translation,17,173,results,gains,over,"ads ( peris and casacuberta , 2018 )","gains over ads ( peris and casacuberta , 2018 )",0.7124062776565552
translation,17,173,results,gains,by,"0.35 , 0.06 and 0.12 bleu scores","gains by 0.35 , 0.06 and 0.12 bleu scores",0.5553537607192993
translation,17,173,results,"ads ( peris and casacuberta , 2018 )",by,"0.35 , 0.06 and 0.12 bleu scores","ads ( peris and casacuberta , 2018 ) by 0.35 , 0.06 and 0.12 bleu scores",0.5317873358726501
translation,17,173,results,"english-to - german , english -to -hindi and spanish - to - english",has,respective best-performing sampling techniques,"english-to - german , english -to -hindi and spanish - to - english has respective best-performing sampling techniques",0.568699836730957
translation,17,173,results,results,for,"english-to - german , english -to -hindi and spanish - to - english","results for english-to - german , english -to -hindi and spanish - to - english",0.5346584916114807
translation,17,193,results,interactive - predictive translation setup,with,qbc,interactive - predictive translation setup with qbc,0.6440061926841736
translation,17,193,results,qbc,surpassed,baseline setup,qbc surpassed baseline setup,0.693372368812561
translation,17,193,results,baseline setup,by,"5.67 % , 4.85 % , 8.32 % and 4.24 % accuracies","baseline setup by 5.67 % , 4.85 % , 8.32 % and 4.24 % accuracies",0.5514813661575317
translation,17,193,results,"5.67 % , 4.85 % , 8.32 % and 4.24 % accuracies",in terms of,wpa,"5.67 % , 4.85 % , 8.32 % and 4.24 % accuracies in terms of wpa",0.7113877534866333
translation,17,193,results,wpa,for,"english-to - german , german- to - english , english -to -hindi and spanish - to - english translation tasks","wpa for english-to - german , german- to - english , english -to -hindi and spanish - to - english translation tasks",0.5711706280708313
translation,17,194,results,wsr scores,obtained by,different sampling techniques,wsr scores obtained by different sampling techniques,0.6375601291656494
translation,17,194,results,results,show,wsr scores,results show wsr scores,0.555205762386322
translation,17,198,results,qbc,achieves,statistically significantly absolute improvement,qbc achieves statistically significantly absolute improvement,0.7031140327453613
translation,17,198,results,statistically significantly absolute improvement,of,1.82 bleu points,statistically significantly absolute improvement of 1.82 bleu points,0.5171698331832886
translation,17,198,results,1.82 bleu points,over,baseline,1.82 bleu points over baseline,0.622681200504303
translation,17,198,results,english -to - german translation,has,qbc,english -to - german translation has qbc,0.583413302898407
translation,17,199,results,nec and sim emb,yield,0.81 and 0.77 bleu,nec and sim emb yield 0.81 and 0.77 bleu,0.6971025466918945
translation,17,199,results,0.81 and 0.77 bleu,over,baseline,0.81 and 0.77 bleu over baseline,0.6638987064361572
translation,17,199,results,improvements,over,baseline,improvements over baseline,0.7402786016464233
translation,17,199,results,english-to-hindi and spanish - to - english,has,nec and sim emb,english-to-hindi and spanish - to - english has nec and sim emb,0.6188892126083374
translation,17,199,results,0.81 and 0.77 bleu,has,improvements,0.81 and 0.77 bleu has improvements,0.5821144580841064
translation,17,201,results,"english-to - german , english-to-hindi and spanish - to - english",reduction in,wsr,"english-to - german , english-to-hindi and spanish - to - english reduction in wsr",0.647405207157135
translation,17,201,results,wsr,of,"9 % , 23 % and 10 %","wsr of 9 % , 23 % and 10 %",0.6000192165374756
translation,17,201,results,wsr,over,baseline,wsr over baseline,0.7287663817405701
translation,17,201,results,"9 % , 23 % and 10 %",over,baseline,"9 % , 23 % and 10 % over baseline",0.6443713307380676
translation,17,201,results,results,For,"english-to - german , english-to-hindi and spanish - to - english","results For english-to - german , english-to-hindi and spanish - to - english",0.5346584916114807
translation,17,203,results,qbc,performs,best,qbc performs best,0.6400716304779053
translation,17,203,results,best,with respect to,wsr reduction,best with respect to wsr reduction,0.700232744216919
translation,17,203,results,english -to - german translation,has,qbc,english -to - german translation has qbc,0.583413302898407
translation,17,203,results,results,for,english -to - german translation,results for english -to - german translation,0.5871089100837708
translation,17,204,results,sim emb,found to be,best-performing strategies,sim emb found to be best-performing strategies,0.6181407570838928
translation,17,204,results,german-to- english,has,qbc,german-to- english has qbc,0.6131355166435242
translation,17,204,results,results,For,german-to- english,results For german-to- english,0.5590099096298218
translation,17,206,results,sim emb,is,best-performing method,sim emb is best-performing method,0.5907029509544373
translation,17,206,results,sim emb,not,best-performing method,sim emb not best-performing method,0.6848282814025879
translation,18,100,ablation-analysis,in- domain self-supervised training ( id - st ),improve,model 's performance,in- domain self-supervised training ( id - st ) improve model 's performance,0.6372117400169373
translation,18,100,ablation-analysis,model 's performance,substantially more than,increased model parameters,model 's performance substantially more than increased model parameters,0.6656843423843384
translation,18,100,ablation-analysis,ablation analysis,has,in - domain back - translation ( id - bt ),ablation analysis has in - domain back - translation ( id - bt ),0.5706728100776672
translation,18,74,experimental-setup,joint byte pair encoding ( bpe ),with,44 k operations,joint byte pair encoding ( bpe ) with 44 k operations,0.630263090133667
translation,18,74,experimental-setup,44 k operations,for,subword vocabulary,44 k operations for subword vocabulary,0.5752072334289551
translation,18,74,experimental-setup,subword vocabulary,in,english and chinese,subword vocabulary in english and chinese,0.49091020226478577
translation,18,74,experimental-setup,experimental setup,adopt,joint byte pair encoding ( bpe ),experimental setup adopt joint byte pair encoding ( bpe ),0.613132894039154
translation,18,86,experimental-setup,sgd optimizer,utilized for,optimization training,sgd optimizer utilized for optimization training,0.6434723734855652
translation,18,86,experimental-setup,optimization training,when,switching to dsd loss,optimization training when switching to dsd loss,0.663048267364502
translation,18,86,experimental-setup,experimental setup,Except for,switching training phase,experimental setup Except for switching training phase,0.6271671652793884
translation,18,87,experimental-setup,learning rate,scheduled using,inverse sqrt scheduler,learning rate scheduled using inverse sqrt scheduler,0.7213821411132812
translation,18,87,experimental-setup,inverse sqrt scheduler,with,4000 warm - up steps,inverse sqrt scheduler with 4000 warm - up steps,0.636646568775177
translation,18,87,experimental-setup,inverse sqrt scheduler,with,maximum learning rate,inverse sqrt scheduler with maximum learning rate,0.613635778427124
translation,18,87,experimental-setup,inverse sqrt scheduler,with,"betas ( 0.9 , 0.98 )","inverse sqrt scheduler with betas ( 0.9 , 0.98 )",0.5894660949707031
translation,18,87,experimental-setup,baseline model training process,has,learning rate,baseline model training process has learning rate,0.4726938009262085
translation,18,87,experimental-setup,maximum learning rate,has,5e - 4,maximum learning rate has 5e - 4,0.5801246166229248
translation,18,87,experimental-setup,experimental setup,During,baseline model training process,experimental setup During baseline model training process,0.6561172604560852
translation,18,88,experimental-setup,8 nvidia v100 gpus,with,batch size,8 nvidia v100 gpus with batch size,0.6109968423843384
translation,18,88,experimental-setup,batch size,limited to,8192 tokens per gpu,batch size limited to 8192 tokens per gpu,0.5971709489822388
translation,18,89,experimental-setup,emploted,to save,gpu memory,emploted to save gpu memory,0.6460756659507751
translation,18,89,experimental-setup,speed up,has,calculations,speed up has calculations,0.5859144926071167
translation,18,89,experimental-setup,experimental setup,has,fp16,experimental setup has fp16,0.5959469079971313
translation,18,90,experimental-setup,virtual batch size,set,gradient update steps,virtual batch size set gradient update steps,0.6556541323661804
translation,18,90,experimental-setup,gradient update steps,to,8,gradient update steps to 8,0.5178824067115784
translation,18,90,experimental-setup,gradient update steps,during,training phase,gradient update steps during training phase,0.6465002298355103
translation,18,90,experimental-setup,8,during,training phase,8 during training phase,0.6492708921432495
translation,18,90,experimental-setup,experimental setup,increase,virtual batch size,experimental setup increase virtual batch size,0.6932790875434875
translation,18,91,experimental-setup,label smoothing and dropout values,set to,0.1,label smoothing and dropout values set to 0.1,0.659554123878479
translation,18,91,experimental-setup,experimental setup,has,label smoothing and dropout values,experimental setup has label smoothing and dropout values,0.5059936046600342
translation,18,92,experimental-setup,finetuning stage,utilize,smaller batch size,finetuning stage utilize smaller batch size,0.603618860244751
translation,18,92,experimental-setup,finetuning stage,train,model,finetuning stage train model,0.6988517642021179
translation,18,92,experimental-setup,smaller batch size,per,gpu,smaller batch size per gpu,0.5979165434837341
translation,18,92,experimental-setup,model,at,fixed learning rate,model at fixed learning rate,0.5805878043174744
translation,18,92,experimental-setup,fixed learning rate,of,1e - 4,fixed learning rate of 1e - 4,0.6451683044433594
translation,18,92,experimental-setup,smaller batch size,has,"4,096 tokens","smaller batch size has 4,096 tokens",0.596315860748291
translation,18,92,experimental-setup,experimental setup,In,finetuning stage,experimental setup In finetuning stage,0.5298711657524109
translation,18,94,experimental-setup,ffn size,are,768/12/12/3072,ffn size are 768/12/12/3072,0.6234674453735352
translation,18,94,experimental-setup,experimental setup,has,"hidden size , heads , hidden layers ,","experimental setup has hidden size , heads , hidden layers ,",0.533263087272644
translation,18,53,experiments,large scale monolingual corpus,train,our own monolingual and multilingual sentence encoder,large scale monolingual corpus train our own monolingual and multilingual sentence encoder,0.6309764981269836
translation,18,53,experiments,fine-tuned,to maximize,cosine similarity,fine-tuned to maximize cosine similarity,0.6613405346870422
translation,18,53,experiments,cosine similarity,between,similar sentences,cosine similarity between similar sentences,0.6500066518783569
translation,18,23,model,self - supervised training,combine,data-dependent gaussian prior objective ( d2 gpo ) objective,self - supervised training combine data-dependent gaussian prior objective ( d2 gpo ) objective,0.6263964176177979
translation,18,23,model,data-dependent gaussian prior objective ( d2 gpo ) objective,to alleviate,collapse,data-dependent gaussian prior objective ( d2 gpo ) objective to alleviate collapse,0.669945240020752
translation,18,23,model,collapse,due to,non-golden targets,collapse due to non-golden targets,0.721616268157959
translation,18,23,model,model,In,self - supervised training,model In self - supervised training,0.528734028339386
translation,18,24,model,finetune stage,with,domainrelated parallel corpus,finetune stage with domainrelated parallel corpus,0.6250703930854797
translation,18,24,model,finetune stage,adopted,training strategy,finetune stage adopted training strategy,0.6619417071342468
translation,18,24,model,training strategy,of switching,optimization objective,training strategy of switching optimization objective,0.6963409781455994
translation,18,24,model,optimization objective,from,mle,optimization objective from mle,0.5432961583137512
translation,18,24,model,mle,to,our proposed dual skew divergence ( dsd ),mle to our proposed dual skew divergence ( dsd ),0.5540998578071594
translation,18,24,model,model,In,finetune stage,model In finetune stage,0.5502551198005676
translation,18,93,model,sentence encoder models,developed with,xlm toolkit,sentence encoder models developed with xlm toolkit,0.6485131978988647
translation,18,93,model,architecture,based on,bert - base,architecture based on bert - base,0.6709699034690857
translation,18,93,model,model,has,sentence encoder models,model has sentence encoder models,0.532440721988678
translation,18,7,results,improve,has,translation performance,improve has translation performance,0.5398982167243958
translation,18,7,results,improve,has,performance,improve has performance,0.5578044652938843
translation,18,19,results,significant impact,on,performance,significant impact on performance,0.5626106262207031
translation,18,19,results,development set,has,domain,development set has domain,0.5650594830513
translation,18,19,results,domain,has,significant impact,domain has significant impact,0.5704123377799988
translation,18,19,results,results,on,development set,results on development set,0.588634192943573
translation,18,25,results,dsd objective,resulted in,improved convergence,dsd objective resulted in improved convergence,0.6701658368110657
translation,18,25,results,results,switching to,dsd objective,results switching to dsd objective,0.6819950938224792
translation,18,26,results,substantial improvements,over,strong baseline,substantial improvements over strong baseline,0.6683504581451416
translation,18,26,results,strong baseline,with,"4.3 ( en ? zh ) , 4.8 ( zh ? en ) , 3.2 ( ja ? en ) bleu scores","strong baseline with 4.3 ( en ? zh ) , 4.8 ( zh ? en ) , 3.2 ( ja ? en ) bleu scores",0.5749638080596924
translation,18,26,results,"4.3 ( en ? zh ) , 4.8 ( zh ? en ) , 3.2 ( ja ? en ) bleu scores",on,development sets,"4.3 ( en ? zh ) , 4.8 ( zh ? en ) , 3.2 ( ja ? en ) bleu scores on development sets",0.5017934441566467
translation,18,26,results,results,observe,substantial improvements,results observe substantial improvements,0.6126725673675537
translation,18,97,results,"deep transformer , wide transformer , and transformer - big",observed,widening,"deep transformer , wide transformer , and transformer - big observed widening",0.6396351456642151
translation,18,97,results,"deep transformer , wide transformer , and transformer - big",increasing,number of model layers,"deep transformer , wide transformer , and transformer - big increasing number of model layers",0.6366912126541138
translation,18,97,results,model,to increase,number of model parameters,model to increase number of model parameters,0.6880335211753845
translation,18,97,results,number of model parameters,result in,large performance benefits,number of model parameters result in large performance benefits,0.677596926689148
translation,18,97,results,widening,has,model,widening has model,0.6005424857139587
translation,18,97,results,results,comparing,"deep transformer , wide transformer , and transformer - big","results comparing deep transformer , wide transformer , and transformer - big",0.6612434983253479
translation,18,99,results,deep transformer w/ rpe model,has,outperforms,deep transformer w/ rpe model has outperforms,0.6135702729225159
translation,18,99,results,outperforms,has,deep transformer model,outperforms has deep transformer model,0.5777761936187744
translation,18,99,results,results,has,deep transformer w/ rpe model,results has deep transformer w/ rpe model,0.5307444930076599
translation,19,112,ablation-analysis,larger model capacity,helps,12layered encoder transformer - big,larger model capacity helps 12layered encoder transformer - big,0.6785947680473328
translation,19,112,ablation-analysis,12layered encoder transformer - big,performs,worse,12layered encoder transformer - big performs worse,0.6839583516120911
translation,19,112,ablation-analysis,worse,than,6 - layered one,worse than 6 - layered one,0.6135973930358887
translation,19,112,ablation-analysis,more data,has,larger model capacity,more data has larger model capacity,0.5576037764549255
translation,19,112,ablation-analysis,ablation analysis,after adding,more data,ablation analysis after adding more data,0.6678298115730286
translation,19,79,baselines,external baselines,include,google translate,external baselines include google translate,0.5693284869194031
translation,19,79,baselines,external baselines,include,romance multilingual model,external baselines include romance multilingual model,0.543248176574707
translation,19,79,baselines,external baselines,include,apertium rule- based machine translation system,external baselines include apertium rule- based machine translation system,0.5624025464057922
translation,19,79,baselines,google translate,for,romanian and italian ),google translate for romanian and italian ),0.6093252301216125
translation,19,79,baselines,romance multilingual model,from,opus,romance multilingual model from opus,0.5636785626411438
translation,19,79,baselines,baselines,has,external baselines,baselines has external baselines,0.5802140831947327
translation,19,27,experimental-setup,"transformer architecture ( vaswani et al. , 2017 )",implemented in,mariannmt,"transformer architecture ( vaswani et al. , 2017 ) implemented in mariannmt",0.6800416707992554
translation,19,73,experimental-setup,mariannmt,to train,models,mariannmt to train models,0.7384508848190308
translation,19,73,experimental-setup,bleu and chrf scores,computed using,"sacrebleu ( post , 2018 )","bleu and chrf scores computed using sacrebleu ( post , 2018 )",0.6813177466392517
translation,19,73,experimental-setup,mariannmt,has,", 2018 )","mariannmt has , 2018 )",0.595075249671936
translation,19,73,experimental-setup,experimental setup,use,mariannmt,experimental setup use mariannmt,0.5695306658744812
translation,19,77,experimental-setup,sentencepiece preprocessing,with,8 k subword models,sentencepiece preprocessing with 8 k subword models,0.5812535881996155
translation,19,77,experimental-setup,experimental setup,use,sentencepiece preprocessing,experimental setup use sentencepiece preprocessing,0.5658365488052368
translation,19,74,experiments,g2p conversion,used,phonemizer wrapper script,g2p conversion used phonemizer wrapper script,0.584635853767395
translation,19,74,experiments,phonemizer wrapper script,around,espeak -ng speech synthesizer,phonemizer wrapper script around espeak -ng speech synthesizer,0.6365883946418762
translation,19,74,experiments,phonemizer wrapper script,to produce,phonemic representation,phonemizer wrapper script to produce phonemic representation,0.6608691811561584
translation,19,74,experiments,phonemic representation,of,texts,phonemic representation of texts,0.5766958594322205
translation,19,78,experiments,models,to,translate,models to translate,0.645076334476471
translation,19,78,experiments,models,from,english,models from english,0.5999309420585632
translation,19,78,experiments,models,do,pivoted translation,models do pivoted translation,0.4882555902004242
translation,19,78,experiments,translate,from,catalan to english,translate from catalan to english,0.6012274622917175
translation,19,78,experiments,translate,from,english,translate from english,0.6458873152732849
translation,19,78,experiments,english,to,target languages,english to target languages,0.5542455911636353
translation,19,7,results,joint model,for,multiple similar language pairs,joint model for multiple similar language pairs,0.583549976348877
translation,19,7,results,joint model,improves upon,translation quality,joint model improves upon translation quality,0.6922817230224609
translation,19,7,results,translation quality,in,each pair,translation quality in each pair,0.5133421421051025
translation,19,7,results,results,show,joint model,results show joint model,0.629665195941925
translation,19,7,results,results,using,joint model,results using joint model,0.6351441740989685
translation,19,81,results,all other baselines,aside from,apertium,all other baselines aside from apertium,0.7004203200340271
translation,19,81,results,apertium,on,catalan -occitan,apertium on catalan -occitan,0.6007588505744934
translation,19,81,results,bilingual baselines,has,outperform,bilingual baselines has outperform,0.6092921495437622
translation,19,81,results,outperform,has,all other baselines,outperform has all other baselines,0.5772116780281067
translation,19,81,results,results,see that,bilingual baselines,results see that bilingual baselines,0.6282001733779907
translation,19,89,results,backtranslation,improves,results,backtranslation improves results,0.6523568630218506
translation,19,89,results,results,for,all the language pairs,results for all the language pairs,0.5752717852592468
translation,19,89,results,works better,than,general corpora backtranslation,works better than general corpora backtranslation,0.5521589517593384
translation,19,89,results,general corpora backtranslation,Occitan sides of,"other parallel corpora ( en- oc , fr- oc and es - oc )","general corpora backtranslation Occitan sides of other parallel corpora ( en- oc , fr- oc and es - oc )",0.6197476387023926
translation,19,89,results,occitan,has,wiki translation,occitan has wiki translation,0.6016437411308289
translation,19,89,results,wiki translation,has,works better,wiki translation has works better,0.5903868079185486
translation,19,89,results,results,see that,backtranslation,results see that backtranslation,0.6672195196151733
translation,19,90,results,performance,is,better,performance is better,0.6231186985969543
translation,19,90,results,results,observe,performance,results observe performance,0.6366938948631287
translation,19,94,results,character - level training,works,best,character - level training works best,0.618199348449707
translation,19,94,results,best,for,catalan to occitan translation,best for catalan to occitan translation,0.6156577467918396
translation,19,94,results,results,see that,character - level training,results see that character - level training,0.600231409072876
translation,19,104,results,joint model,obtained,improved results,joint model obtained improved results,0.7068398594856262
translation,19,104,results,improved results,for,all language pairs,improved results for all language pairs,0.5931017398834229
translation,19,104,results,results,by using,joint model,results by using joint model,0.6476263999938965
translation,19,107,results,first glance,including,additional related languages,first glance including additional related languages,0.7274463176727295
translation,19,107,results,performance,for,catalan- occitan,performance for catalan- occitan,0.6352003216743469
translation,19,107,results,performance,for,catalan- occitan,performance for catalan- occitan,0.6352003216743469
translation,19,107,results,not improve,has,performance,not improve has performance,0.587424099445343
translation,19,107,results,hurts,has,performance,hurts has performance,0.6043882966041565
translation,19,107,results,results,At,first glance,results At first glance,0.5233327746391296
translation,19,107,results,results,including,additional related languages,results including additional related languages,0.6487975716590881
translation,19,108,results,smaller training corpora,to have,same number of sentences,smaller training corpora to have same number of sentences,0.6078439354896545
translation,19,108,results,same number of sentences,as,largest one,same number of sentences as largest one,0.5703434944152832
translation,19,108,results,performance,of,model,performance of model,0.6080846190452576
translation,19,108,results,results,After oversampling,smaller training corpora,results After oversampling smaller training corpora,0.6664940714836121
translation,19,109,results,backtranslated wikipedia,results in,worse scores,backtranslated wikipedia results in worse scores,0.602052628993988
translation,19,109,results,results,adding,backtranslated wikipedia,results adding backtranslated wikipedia,0.73237144947052
translation,19,129,results,our submissions,ranked first in,all directions,our submissions ranked first in all directions,0.6276742815971375
translation,19,129,results,all directions,with respect to,all metrics,all directions with respect to all metrics,0.6562591791152954
translation,19,129,results,all metrics,except for,catalan - romanian bleu score,all metrics except for catalan - romanian bleu score,0.5788806080818176
translation,19,129,results,catalan - romanian bleu score,where,m2m model,catalan - romanian bleu score where m2m model,0.5692825317382812
translation,19,129,results,m2m model,was,0.2 points better,m2m model was 0.2 points better,0.611956775188446
translation,19,129,results,results,has,our submissions,results has our submissions,0.5741304159164429
translation,19,139,results,vertical combination of texts and phonemes,appears to perform,better,vertical combination of texts and phonemes appears to perform better,0.6910062432289124
translation,19,139,results,better,than,horizontal one,better than horizontal one,0.659693717956543
translation,19,139,results,results,has,vertical combination of texts and phonemes,results has vertical combination of texts and phonemes,0.5564590096473694
translation,19,147,results,our submission,ranked first in,all metrics,our submission ranked first in all metrics,0.675094485282898
translation,19,147,results,submitted catalan-occitan systems,has,our submission,submitted catalan-occitan systems has our submission,0.5983312129974365
translation,19,147,results,results,Within,submitted catalan-occitan systems,results Within submitted catalan-occitan systems,0.6136085987091064
translation,20,264,ablation-analysis,other effects,of,referencebased da,other effects of referencebased da,0.517859697341919
translation,20,264,ablation-analysis,other effects,of,start playing role,other effects of start playing role,0.5838967561721802
translation,20,264,ablation-analysis,referencebased da,in,czech ? english,referencebased da in czech ? english,0.5355974435806274
translation,20,264,ablation-analysis,start playing role,when,candidate and reference are human,start playing role when candidate and reference are human,0.6656649708747864
translation,20,264,ablation-analysis,start playing role,when,only the reference is human,start playing role when only the reference is human,0.6814965009689331
translation,20,264,ablation-analysis,start playing role,both,candidate and reference are human,start playing role both candidate and reference are human,0.6509618163108826
translation,20,264,ablation-analysis,other effects,has,start playing role,other effects has start playing role,0.5950626134872437
translation,20,264,ablation-analysis,referencebased da,has,start playing role,referencebased da has start playing role,0.5820310115814209
translation,20,264,ablation-analysis,ablation analysis,possible that,other effects,ablation analysis possible that other effects,0.7559962868690491
translation,20,423,baselines,direct - transformer model,trained on,entire russian / chinese parallel dataset,direct - transformer model trained on entire russian / chinese parallel dataset,0.7018676996231079
translation,20,423,baselines,direct - transformer model,decoded with,? = 1.0,direct - transformer model decoded with ? = 1.0,0.7124375700950623
translation,20,423,baselines,direct - transformer model,decoded with,beam_size=4,direct - transformer model decoded with beam_size=4,0.7361312508583069
translation,20,423,baselines,2 simple baselines,has,direct - transformer model,2 simple baselines has direct - transformer model,0.45937609672546387
translation,20,424,baselines,pivot model,trained with,corresponding parallel data,pivot model trained with corresponding parallel data,0.7463688254356384
translation,20,424,baselines,pivot model,has,2 mt systems,pivot model has 2 mt systems,0.5706251859664917
translation,20,424,baselines,2 mt systems,has,russian - to - english and englishto - chinese,2 mt systems has russian - to - english and englishto - chinese,0.4814050495624542
translation,20,424,baselines,baselines,has,pivot model,baselines has pivot model,0.5505384206771851
translation,20,207,experiments,microsoft,contributed with,42 %,microsoft contributed with 42 %,0.6093587279319763
translation,20,207,experiments,wmt news participants,contributed with,37 %,wmt news participants contributed with 37 %,0.6070600152015686
translation,20,207,experiments,toloka platform,with,21 %,toloka platform with 21 %,0.6223652958869934
translation,20,207,experiments,21 %,of,all valid annotations,21 % of all valid annotations,0.5574027895927429
translation,20,207,experiments,bilingual human evaluation,has,microsoft,bilingual human evaluation has microsoft,0.5678749680519104
translation,20,207,experiments,bilingual human evaluation,has,wmt news participants,bilingual human evaluation has wmt news participants,0.5308343172073364
translation,20,244,results,human reference translations,end up in,top-scoring cluster,human reference translations end up in top-scoring cluster,0.6342861652374268
translation,20,244,results,all language pairs,has,human reference translations,all language pairs has human reference translations,0.5465756058692932
translation,20,244,results,( relatively ) high quality,has,of these references,( relatively ) high quality has of these references,0.5756061673164368
translation,20,244,results,results,Across,all language pairs,results Across all language pairs,0.639985203742981
translation,20,245,results,language pairs,with,large numbers of submissions,language pairs with large numbers of submissions,0.5971714854240417
translation,20,245,results,language pairs,observe,little to no clustering,language pairs observe little to no clustering,0.573807954788208
translation,20,245,results,results,For,language pairs,results For language pairs,0.5562822222709656
translation,20,252,results,better clustering,for,contr : sr?dc,better clustering for contr : sr?dc,0.6750087141990662
translation,20,255,results,human reference translations,scored,worse,human reference translations scored worse,0.6826080083847046
translation,20,255,results,significantly worse,than,top cluster,significantly worse than top cluster,0.5907455682754517
translation,20,352,results,primary systems,with,smaller vocabulary size,primary systems with smaller vocabulary size,0.6320048570632935
translation,20,352,results,primary systems,performed,better,primary systems performed better,0.3049895167350769
translation,20,352,results,better,in terms of,bleu scores,better in terms of bleu scores,0.6192295551300049
translation,20,368,results,best results,for,spanish - catalan language pair,best results for spanish - catalan language pair,0.6012496948242188
translation,21,54,experiments,cascade and direct st systems,for,en-de / es / it,cascade and direct st systems for en-de / es / it,0.7068151831626892
translation,21,54,experiments,cascade and direct st systems,with,same core technology,cascade and direct st systems with same core technology,0.6884467005729675
translation,21,54,experiments,en-de / es / it,with,same core technology,en-de / es / it with same core technology,0.7244164347648621
translation,21,145,results,systems,show,pretty much,systems show pretty much,0.6570025086402893
translation,21,145,results,same number of errors,with,slight percentage gain ( + 1.1 ),same number of errors with slight percentage gain ( + 1.1 ),0.6542119979858398
translation,21,145,results,slight percentage gain ( + 1.1 ),in favor of,cascade,slight percentage gain ( + 1.1 ) in favor of cascade,0.7640140652656555
translation,21,145,results,en-it,has,systems,en-it has systems,0.6570487022399902
translation,21,145,results,pretty much,has,same number of errors,pretty much has same number of errors,0.6174860000610352
translation,21,145,results,results,On,en-it,results On en-it,0.7099568247795105
translation,21,191,results,cascade output,exhibits,higher lexical diversity,cascade output exhibits higher lexical diversity,0.7005496025085449
translation,21,191,results,higher lexical diversity,on,all languages,higher lexical diversity on all languages,0.5313971042633057
translation,21,191,results,smaller differences,on,en-de and en-es,smaller differences on en-de and en-es,0.6090527772903442
translation,21,191,results,results,shows,cascade output,results shows cascade output,0.7372120022773743
translation,22,110,ablation-analysis,extra embeddings,result in,greatly improved results,extra embeddings result in greatly improved results,0.6370707750320435
translation,22,110,ablation-analysis,greatly improved results,for,si- en,greatly improved results for si- en,0.6696203351020813
translation,22,110,ablation-analysis,greatly improved results,by,0.25 points,greatly improved results by 0.25 points,0.5855128765106201
translation,22,110,ablation-analysis,greatly improved results,by,0.05 points,greatly improved results by 0.05 points,0.5951810479164124
translation,22,110,ablation-analysis,0.05 points,for,km- en,0.05 points for km- en,0.6483203768730164
translation,22,110,ablation-analysis,zero-shot,has,extra embeddings,zero-shot has extra embeddings,0.6008976101875305
translation,22,110,ablation-analysis,ablation analysis,In,zero-shot,ablation analysis In zero-shot,0.5454408526420593
translation,22,115,ablation-analysis,mbert,extend,model,mbert extend model,0.7328410744667053
translation,22,115,ablation-analysis,model,with,language adapters,model with language adapters,0.5885448455810547
translation,22,115,ablation-analysis,additional embeddings,for,sinhala and khmer,additional embeddings for sinhala and khmer,0.6504870057106018
translation,22,115,ablation-analysis,language adapters,has,la ),language adapters has la ),0.6052196621894836
translation,22,115,ablation-analysis,language adapters,has,additional embeddings,language adapters has additional embeddings,0.5683887600898743
translation,22,115,ablation-analysis,ablation analysis,For,mbert,ablation analysis For mbert,0.6306974291801453
translation,22,116,ablation-analysis,three pairs,has,with unseen languages,three pairs has with unseen languages,0.5944350361824036
translation,22,116,ablation-analysis,three pairs,has,language adapters,three pairs has language adapters,0.606080949306488
translation,22,116,ablation-analysis,with unseen languages,has,language adapters,with unseen languages has language adapters,0.5854439735412598
translation,22,116,ablation-analysis,language adapters,has,greatly improve,language adapters has greatly improve,0.5779109001159668
translation,22,116,ablation-analysis,greatly improve,has,performance,greatly improve has performance,0.5732364654541016
translation,22,116,ablation-analysis,ablation analysis,For,three pairs,ablation analysis For three pairs,0.6405339241027832
translation,22,138,ablation-analysis,only the ( sinhala ) source,witness,expected drop,only the ( sinhala ) source witness expected drop,0.8160136938095093
translation,22,138,ablation-analysis,expected drop,in,performance,expected drop in performance,0.5590996742248535
translation,22,138,ablation-analysis,ablation analysis,training with,only the ( sinhala ) source,ablation analysis training with only the ( sinhala ) source,0.7984755635261536
translation,22,32,hyperparameters,adapters,are,randomly initialized weights,adapters are randomly initialized weights,0.5875763893127441
translation,22,32,hyperparameters,every layer,of,pre-trained transformer model,every layer of pre-trained transformer model,0.5919679403305054
translation,22,32,hyperparameters,hyperparameters,has,adapters,hyperparameters has adapters,0.5432923436164856
translation,22,72,hyperparameters,our models,with,mbert and xlm -r,our models with mbert and xlm -r,0.6698116064071655
translation,22,72,hyperparameters,hyperparameters,initialize,our models,hyperparameters initialize our models,0.7312972545623779
translation,22,73,hyperparameters,reduction factor r,of,8,reduction factor r of 8,0.6672627925872803
translation,22,73,hyperparameters,reduction factor r,for,our task adapters,reduction factor r for our task adapters,0.5782470107078552
translation,22,73,hyperparameters,8,for,our task adapters,8 for our task adapters,0.5957643389701843
translation,22,74,hyperparameters,language adapters,use,r = 2,language adapters use r = 2,0.6582779288291931
translation,22,74,hyperparameters,language adapters,trained on,wikipedia articles,language adapters trained on wikipedia articles,0.7362864017486572
translation,22,74,hyperparameters,hyperparameters,has,language adapters,hyperparameters has language adapters,0.5340996980667114
translation,22,75,hyperparameters,additional embeddings,for,khmer and sinhala,additional embeddings for khmer and sinhala,0.6390891671180725
translation,22,75,hyperparameters,additional embeddings,fine-tuned together with,respective las,additional embeddings fine-tuned together with respective las,0.7524265646934509
translation,22,75,hyperparameters,khmer and sinhala,contain,10 k tokens each,khmer and sinhala contain 10 k tokens each,0.6083926558494568
translation,22,75,hyperparameters,respective las,on,wikipedia data,respective las on wikipedia data,0.5936073064804077
translation,22,75,hyperparameters,hyperparameters,has,additional embeddings,hyperparameters has additional embeddings,0.5262640118598938
translation,22,76,hyperparameters,our models,using,"adamw ( loshchilov and hutter , 2019 )","our models using adamw ( loshchilov and hutter , 2019 )",0.6591467261314392
translation,22,76,hyperparameters,"adamw ( loshchilov and hutter , 2019 )",with,linear learning rate schedule,"adamw ( loshchilov and hutter , 2019 ) with linear learning rate schedule",0.6138474345207214
translation,22,76,hyperparameters,linear learning rate schedule,without,warm - up,linear learning rate schedule without warm - up,0.7795531749725342
translation,22,76,hyperparameters,hyperparameters,fine- tune,our models,hyperparameters fine- tune our models,0.7166115045547485
translation,22,77,hyperparameters,early stopping,by storing,checkpoint,early stopping by storing checkpoint,0.8030490875244141
translation,22,77,hyperparameters,checkpoint,with,best dev set performance,checkpoint with best dev set performance,0.6673009395599365
translation,22,77,hyperparameters,best dev set performance,evaluating,every 500 steps,best dev set performance evaluating every 500 steps,0.6752482056617737
translation,22,77,hyperparameters,hyperparameters,simulate,early stopping,hyperparameters simulate early stopping,0.6122740507125854
translation,22,78,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,22,78,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,22,78,hyperparameters,hyperparameters,use,learning rate,hyperparameters use learning rate,0.6108506321907043
translation,22,79,hyperparameters,each model,for,8 k steps,each model for 8 k steps,0.656547486782074
translation,22,79,hyperparameters,hyperparameters,train,each model,hyperparameters train each model,0.6649250984191895
translation,22,80,hyperparameters,hyperparameters,based on,wmt20 dev set performance,hyperparameters based on wmt20 dev set performance,0.5771656036376953
translation,22,81,hyperparameters,reduction factor r,for,task adapters,reduction factor r for task adapters,0.5742160677909851
translation,22,81,hyperparameters,learning rate,has,"5e - 5 , 1e - 4 , 2e - 4 , 5e - 4","learning rate has 5e - 5 , 1e - 4 , 2e - 4 , 5e - 4",0.5510082244873047
translation,22,81,hyperparameters,learning rate,has,batch size,learning rate has batch size,0.5358124375343323
translation,22,81,hyperparameters,learning rate,has,reduction factor r,learning rate has reduction factor r,0.510230302810669
translation,22,81,hyperparameters,batch size,has,"4 , 8 , 16 , 32 , 96 }","batch size has 4 , 8 , 16 , 32 , 96 }",0.5947847962379456
translation,22,81,hyperparameters,task adapters,has,"4 , 8 , 16 }","task adapters has 4 , 8 , 16 }",0.5858569741249084
translation,22,81,hyperparameters,training steps,has,"{ 2 k , 3k , 5 k , 8 k , 10 k }.","training steps has { 2 k , 3k , 5 k , 8 k , 10 k }.",0.48032405972480774
translation,22,7,results,model,to,new languages and unseen scripts,model to new languages and unseen scripts,0.5461056232452393
translation,22,7,results,model,achieve,on par performance or even surpass,model achieve on par performance or even surpass,0.6968564391136169
translation,22,7,results,new languages and unseen scripts,using,recent adapter-based methods,new languages and unseen scripts using recent adapter-based methods,0.6105652451515198
translation,22,7,results,models,pre-trained on,respective languages,models pre-trained on respective languages,0.6424360871315002
translation,22,7,results,on par performance or even surpass,has,models,on par performance or even surpass has models,0.6246615052223206
translation,22,7,results,results,extend,model,results extend model,0.5859024524688721
translation,22,107,results,additional embeddings,give,relatively small performance boost,additional embeddings give relatively small performance boost,0.6142240166664124
translation,22,107,results,relatively small performance boost,of,0.04 points,relatively small performance boost of 0.04 points,0.5583288073539734
translation,22,107,results,0.04 points,on top of,already quite good results,0.04 points on top of already quite good results,0.635812520980835
translation,22,107,results,si-en data,has,additional embeddings,si-en data has additional embeddings,0.6094387769699097
translation,22,117,results,zero-shot situations,gain,0.18 points,zero-shot situations gain 0.18 points,0.7537089586257935
translation,22,117,results,zero-shot situations,gain,0.07,zero-shot situations gain 0.07,0.7378015518188477
translation,22,117,results,zero-shot situations,gain,0.28 points,zero-shot situations gain 0.28 points,0.7548700571060181
translation,22,117,results,0.18 points,for,si-en,0.18 points for si-en,0.6606740951538086
translation,22,117,results,0.07,for,km-en,0.07 for km-en,0.6470996141433716
translation,22,117,results,0.28 points,for,ps- en,0.28 points for ps- en,0.6747519373893738
translation,22,117,results,results,In,zero-shot situations,results In zero-shot situations,0.5283589959144592
translation,22,118,results,0.03 points more,with,language adapters,0.03 points more with language adapters,0.5970640778541565
translation,22,144,results,adapter ensemble a-xlmrlarge ensemble,brings,slight performance boost,adapter ensemble a-xlmrlarge ensemble brings slight performance boost,0.5726762413978577
translation,22,144,results,results,has,adapter ensemble a-xlmrlarge ensemble,results has adapter ensemble a-xlmrlarge ensemble,0.5474370718002319
translation,22,149,results,competitive results,with,fully fine-tuned models,competitive results with fully fine-tuned models,0.612909734249115
translation,22,149,results,fully fine-tuned models,that do not employ,additional techniques,fully fine-tuned models that do not employ additional techniques,0.7463338971138
translation,22,149,results,additional techniques,like,ensembles,additional techniques like ensembles,0.655788779258728
translation,22,149,results,ensembles,in both,all and zero setups,ensembles in both all and zero setups,0.6504402160644531
translation,22,149,results,results,achieve,competitive results,results achieve competitive results,0.6465562582015991
translation,23,69,ablation-analysis,two iterations,of,knowledge distillation,two iterations of knowledge distillation,0.5836777687072754
translation,23,69,ablation-analysis,knowledge distillation,deliver,best performance,knowledge distillation deliver best performance,0.6226670742034912
translation,23,69,ablation-analysis,ablation analysis,for,nine tasks,ablation analysis for nine tasks,0.6076607704162598
translation,23,131,ablation-analysis,back - translation,useful for,en ? ja task,back - translation useful for en ? ja task,0.7145976424217224
translation,23,131,ablation-analysis,knowledge distillation,helpful for,ja ?en task,knowledge distillation helpful for ja ?en task,0.6611182689666748
translation,23,6,baselines,primary systems,built on,several effective variants of transformer,primary systems built on several effective variants of transformer,0.7019208073616028
translation,23,6,baselines,several effective variants of transformer,e.g.,transformer - dlcl,several effective variants of transformer e.g. transformer - dlcl,0.7163668870925903
translation,23,6,baselines,several effective variants of transformer,e.g.,ode - transformer,several effective variants of transformer e.g. ode - transformer,0.7054544687271118
translation,23,6,baselines,baselines,has,primary systems,baselines has primary systems,0.5757710337638855
translation,23,13,baselines,system combination,adopt,"post-ensemble ( kobayashi , 2018 )","system combination adopt post-ensemble ( kobayashi , 2018 )",0.6465966105461121
translation,23,13,baselines,"post-ensemble ( kobayashi , 2018 )",to find,most similar hypothesis,"post-ensemble ( kobayashi , 2018 ) to find most similar hypothesis",0.5947564244270325
translation,23,13,baselines,most similar hypothesis,among,several ensemble outputs,most similar hypothesis among several ensemble outputs,0.5974496603012085
translation,23,13,baselines,most similar hypothesis,regarded as,reranking technique,most similar hypothesis regarded as reranking technique,0.6070590615272522
translation,23,13,baselines,reranking technique,without,pre-training,reranking technique without pre-training,0.7058494687080383
translation,23,13,baselines,baselines,For,system combination,baselines For system combination,0.6169124841690063
translation,23,92,experimental-setup,models,based on,"fairseq ( ott et al. , 2019 )","models based on fairseq ( ott et al. , 2019 )",0.6563156247138977
translation,23,92,experimental-setup,experimental setup,implementation of,models,experimental setup implementation of models,0.7030203938484192
translation,23,93,experimental-setup,experimental setup,trained on,8 rtx 2080 ti gpus,experimental setup trained on 8 rtx 2080 ti gpus,0.6696639060974121
translation,23,94,experimental-setup,pre-norm transformer - base,as,baseline,pre-norm transformer - base as baseline,0.5305037498474121
translation,23,94,experimental-setup,pre-norm transformer - base,enhanced,deep or wide models,pre-norm transformer - base enhanced deep or wide models,0.7252023220062256
translation,23,94,experimental-setup,deep or wide models,by enlarging,model depth and the hidden size,deep or wide models by enlarging model depth and the hidden size,0.7233852744102478
translation,23,94,experimental-setup,experimental setup,selected,pre-norm transformer - base,experimental setup selected pre-norm transformer - base,0.5933008790016174
translation,23,95,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,"? 1 = 0.9 , ? 2 = 0.997","adam optimizer ( kingma and ba , 2014 ) with ? 1 = 0.9 , ? 2 = 0.997",0.614234983921051
translation,23,95,experimental-setup,"? 1 = 0.9 , ? 2 = 0.997",during,training,"? 1 = 0.9 , ? 2 = 0.997 during training",0.6186200380325317
translation,23,95,experimental-setup,experimental setup,used,"adam optimizer ( kingma and ba , 2014 )","experimental setup used adam optimizer ( kingma and ba , 2014 )",0.5960938334465027
translation,23,99,experimental-setup,max learning rate and warmup step,set to,0.002 and 8000,max learning rate and warmup step set to 0.002 and 8000,0.7235158681869507
translation,23,99,experimental-setup,max learning rate and warmup step,set to,0.0016 and 16000,max learning rate and warmup step set to 0.0016 and 16000,0.7004673480987549
translation,23,99,experimental-setup,0.002 and 8000,for,deep models,0.002 and 8000 for deep models,0.6292662024497986
translation,23,99,experimental-setup,0.0016 and 16000,for,deep and wide models,0.0016 and 16000 for deep and wide models,0.641063928604126
translation,23,99,experimental-setup,experimental setup,has,max learning rate and warmup step,experimental setup has max learning rate and warmup step,0.4684601128101349
translation,23,100,experimental-setup,dropout probabilities,set to,0.1,dropout probabilities set to 0.1,0.6957004070281982
translation,23,100,experimental-setup,0.1,including,residual dropout,0.1 including residual dropout,0.6920341849327087
translation,23,100,experimental-setup,0.1,including,attention dropout,0.1 including attention dropout,0.7009284496307373
translation,23,100,experimental-setup,0.1,including,relu dropout,0.1 including relu dropout,0.6676729321479797
translation,23,100,experimental-setup,experimental setup,including,residual dropout,experimental setup including residual dropout,0.6629257202148438
translation,23,100,experimental-setup,experimental setup,has,dropout probabilities,experimental setup has dropout probabilities,0.508775532245636
translation,23,116,experimental-setup,knowledge distillations,used,newstest2017 - 2019,knowledge distillations used newstest2017 - 2019,0.6057845950126648
translation,23,116,experimental-setup,newstest2017 - 2019,to fine - tune,our models,newstest2017 - 2019 to fine - tune our models,0.7264387607574463
translation,23,116,experimental-setup,our models,for,five epochs,our models for five epochs,0.6059447526931763
translation,23,116,experimental-setup,five epochs,with,0.0001 learning rate,five epochs with 0.0001 learning rate,0.5980732440948486
translation,23,116,experimental-setup,experimental setup,After,knowledge distillations,experimental setup After knowledge distillations,0.6700191497802734
translation,23,117,experimental-setup,final stage,add,newstest2020,final stage add newstest2020,0.7066758275032043
translation,23,117,experimental-setup,newstest2020,to,fine-tuning data,newstest2020 to fine-tuning data,0.5625203251838684
translation,23,117,experimental-setup,experimental setup,In,final stage,experimental setup In final stage,0.553928554058075
translation,23,156,experimental-setup,iterative kd,sampled,3 million,iterative kd sampled 3 million,0.7582387924194336
translation,23,156,experimental-setup,in- domain source data,according to,wmt2021 development sets,in- domain source data according to wmt2021 development sets,0.6515217423439026
translation,23,156,experimental-setup,iterative kd,has,two times,iterative kd has two times,0.6139585375785828
translation,23,156,experimental-setup,3 million,has,in- domain source data,3 million has in- domain source data,0.5397323369979858
translation,23,156,experimental-setup,experimental setup,implemented,iterative kd,experimental setup implemented iterative kd,0.7159606218338013
translation,23,141,experiments,en?ru,used,two parallel datasets,en?ru used two parallel datasets,0.6024747490882874
translation,23,141,experiments,two parallel datasets,including,paracrawl v8,two parallel datasets including paracrawl v8,0.6347501873970032
translation,23,141,experiments,two parallel datasets,including,news commentary,two parallel datasets including news commentary,0.6647242903709412
translation,23,11,hyperparameters,performance,of,single model,performance of single model,0.6403614282608032
translation,23,11,hyperparameters,performance,choose,pre-normalized transformer - dlcl,performance choose pre-normalized transformer - dlcl,0.6949375867843628
translation,23,11,hyperparameters,performance,choose,"ode - transformer ( li et al. , 2021a )","performance choose ode - transformer ( li et al. , 2021a )",0.7074052095413208
translation,23,11,hyperparameters,single model,choose,pre-normalized transformer - dlcl,single model choose pre-normalized transformer - dlcl,0.7111220359802246
translation,23,11,hyperparameters,single model,choose,"ode - transformer ( li et al. , 2021a )","single model choose ode - transformer ( li et al. , 2021a )",0.7055369019508362
translation,23,11,hyperparameters,pre-normalized transformer - dlcl,as,backbone,pre-normalized transformer - dlcl as backbone,0.6033992171287537
translation,23,11,hyperparameters,"ode - transformer ( li et al. , 2021a )",as,backbone,"ode - transformer ( li et al. , 2021a ) as backbone",0.5784563422203064
translation,23,11,hyperparameters,hyperparameters,To enhance,performance,hyperparameters To enhance performance,0.6798674464225769
translation,23,12,model,relative position representation,due to,strong performance,relative position representation due to strong performance,0.6608914732933044
translation,23,20,model,back - translation stage,leverage,target -side monolingual sentences,back - translation stage leverage target -side monolingual sentences,0.6718602180480957
translation,23,20,model,back - translation stage,use,"nucleus sampling ( holtzman et al. , 2019 ) decoding strategy","back - translation stage use nucleus sampling ( holtzman et al. , 2019 ) decoding strategy",0.627958357334137
translation,23,20,model,target -side monolingual sentences,to generate,source -side pseudo sentences,target -side monolingual sentences to generate source -side pseudo sentences,0.6312235593795776
translation,23,20,model,"nucleus sampling ( holtzman et al. , 2019 ) decoding strategy",to improve,generalization ability,"nucleus sampling ( holtzman et al. , 2019 ) decoding strategy to improve generalization ability",0.624927818775177
translation,23,20,model,model,In,back - translation stage,model In back - translation stage,0.522542417049408
translation,23,21,model,in- domain source-side monolingual data,by applying,iterative knowledge distillation,in- domain source-side monolingual data by applying iterative knowledge distillation,0.6680893301963806
translation,23,21,model,model,leverage,in- domain source-side monolingual data,model leverage in- domain source-side monolingual data,0.706297755241394
translation,23,51,model,relative position representation ( rpr ),into,self-attention mechanism,relative position representation ( rpr ) into self-attention mechanism,0.5582569241523743
translation,23,51,model,self-attention mechanism,on,encoder and decoder sides,self-attention mechanism on encoder and decoder sides,0.5764247179031372
translation,23,51,model,self-attention mechanism,both,encoder and decoder sides,self-attention mechanism both encoder and decoder sides,0.6942483186721802
translation,23,51,model,model,incorporate,relative position representation ( rpr ),model incorporate relative position representation ( rpr ),0.7099581956863403
translation,23,111,model,knowledge distillation,to iteratively enhance,single model,knowledge distillation to iteratively enhance single model,0.700107753276825
translation,23,111,model,twice,to iteratively enhance,single model,twice to iteratively enhance single model,0.7321015000343323
translation,23,111,model,single model,with,ensemble outputs,single model with ensemble outputs,0.6413695812225342
translation,23,111,model,knowledge distillation,has,twice,knowledge distillation has twice,0.6217775940895081
translation,23,111,model,model,implemented,knowledge distillation,model implemented knowledge distillation,0.6874175667762756
translation,23,74,results,iterative fine-tuning,can,better improve,iterative fine-tuning can better improve,0.6431429386138916
translation,23,74,results,translation quality,of,names,translation quality of names,0.5896246433258057
translation,23,74,results,names,of,news organizations,names of news organizations,0.5368629097938538
translation,23,74,results,news organizations,in,news field,news organizations in news field,0.4901377558708191
translation,23,74,results,better improve,has,translation quality,better improve has translation quality,0.574165940284729
translation,23,110,results,zh?en and en ?zh,got,bleu improvements,zh?en and en ?zh got bleu improvements,0.6194310784339905
translation,23,110,results,zh?en and en ?zh,got,further bleu improvements,zh?en and en ?zh got further bleu improvements,0.6087052822113037
translation,23,110,results,bleu improvements,of,1.8 and 2.9,bleu improvements of 1.8 and 2.9,0.5768886208534241
translation,23,110,results,bleu improvements,of,0.5 and 0.8,bleu improvements of 0.5 and 0.8,0.6218944191932678
translation,23,110,results,bleu improvements,of,0.5 and 0.8,bleu improvements of 0.5 and 0.8,0.6218944191932678
translation,23,110,results,1.8 and 2.9,in,first back -translation,1.8 and 2.9 in first back -translation,0.533347487449646
translation,23,110,results,further bleu improvements,of,0.5 and 0.8,further bleu improvements of 0.5 and 0.8,0.6120514273643494
translation,23,110,results,0.5 and 0.8,in,second back - translation,0.5 and 0.8 in second back - translation,0.5404043793678284
translation,23,110,results,results,For,zh?en and en ?zh,results For zh?en and en ?zh,0.649234414100647
translation,23,115,results,bleu improvements,of,1.1 and 0.6,bleu improvements of 1.1 and 0.6,0.5945099592208862
translation,23,115,results,bleu improvements,of,0.6 and 0.3,bleu improvements of 0.6 and 0.3,0.6085894107818604
translation,23,115,results,bleu improvements,of,0.6 and 0.3,bleu improvements of 0.6 and 0.3,0.6085894107818604
translation,23,115,results,1.1 and 0.6,in,first knowledge distillation,1.1 and 0.6 in first knowledge distillation,0.5344839692115784
translation,23,115,results,1.1 and 0.6,in,second knowledge distillation,1.1 and 0.6 in second knowledge distillation,0.5382605791091919
translation,23,115,results,1.1 and 0.6,in,second knowledge distillation,1.1 and 0.6 in second knowledge distillation,0.5382605791091919
translation,23,115,results,further bleu improvements,of,0.6 and 0.3,further bleu improvements of 0.6 and 0.3,0.5948141813278198
translation,23,115,results,0.6 and 0.3,in,second knowledge distillation,0.6 and 0.3 in second knowledge distillation,0.5374656915664673
translation,23,115,results,second knowledge distillation,in,zh?en and en?zh,second knowledge distillation in zh?en and en?zh,0.5805811882019043
translation,23,115,results,results,got,bleu improvements,results got bleu improvements,0.5887556672096252
translation,23,115,results,results,got,further bleu improvements,results got further bleu improvements,0.6144433617591858
translation,23,119,results,post-ensemble,brought us,+ 0.2 and + 0.3 bleu,post-ensemble brought us + 0.2 and + 0.3 bleu,0.5288757681846619
translation,23,119,results,+ 0.2 and + 0.3 bleu,in,zh?en and en ? zh directions,+ 0.2 and + 0.3 bleu in zh?en and en ? zh directions,0.5871551632881165
translation,23,119,results,ensemble method,has,post-ensemble,ensemble method has post-ensemble,0.5593234300613403
translation,23,119,results,results,Based on,ensemble method,results Based on ensemble method,0.6959848403930664
translation,23,120,results,iterative fine - tune,are,effective methods,iterative fine - tune are effective methods,0.5898647904396057
translation,23,120,results,effective methods,to get,significant improvements,effective methods to get significant improvements,0.6231909394264221
translation,23,120,results,results,find that,iterative back - translation,results find that iterative back - translation,0.6460726857185364
translation,23,139,results,+ 0.7 bleu,in,ja,+ 0.7 bleu in ja,0.6185786724090576
translation,23,139,results,+ 0.7 bleu,in,?en task,+ 0.7 bleu in ?en task,0.5403391122817993
translation,23,139,results,ja,has,?en task,ja has ?en task,0.6934190988540649
translation,23,155,results,significant improvements,of,6.1 and 4.4 bleu,significant improvements of 6.1 and 4.4 bleu,0.5510850548744202
translation,23,155,results,6.1 and 4.4 bleu,in,en ?is and is ?en directions,6.1 and 4.4 bleu in en ?is and is ?en directions,0.5951869487762451
translation,23,155,results,results,obtained,significant improvements,results obtained significant improvements,0.6995248198509216
translation,23,157,results,very effective method,to get,2.2 and 1.1 improvements,very effective method to get 2.2 and 1.1 improvements,0.6609475612640381
translation,23,157,results,results,shows,very effective method,results shows very effective method,0.6759159564971924
translation,23,159,results,two ensemble combinations,to decode,sentences,two ensemble combinations to decode sentences,0.7722270488739014
translation,23,159,results,model ensemble,gained,0.7 and 0.8 improvements,model ensemble gained 0.7 and 0.8 improvements,0.6569514870643616
translation,23,159,results,two ensemble combinations,has,model ensemble,two ensemble combinations has model ensemble,0.5840132236480713
translation,23,159,results,sentences,has,model ensemble,sentences has model ensemble,0.564422070980072
translation,23,159,results,results,implementing,two ensemble combinations,results implementing two ensemble combinations,0.6482847332954407
translation,23,166,results,still effective,in,low-resource language pairs,still effective in low-resource language pairs,0.5365821719169617
translation,23,166,results,results,observe that,wide and deep models,results observe that wide and deep models,0.591938853263855
translation,23,167,results,backtranslation and knowledge distillation techniques,gain,4.6 and 1.7 bleu improvements,backtranslation and knowledge distillation techniques gain 4.6 and 1.7 bleu improvements,0.694290816783905
translation,23,167,results,results,Through,backtranslation and knowledge distillation techniques,results Through backtranslation and knowledge distillation techniques,0.5840989351272583
translation,23,171,results,all of our systems,performed,competitively,all of our systems performed competitively,0.29701343178749084
translation,23,171,results,competitively,especially in,eh ?is and ru?en directions,competitively especially in eh ?is and ru?en directions,0.7114301323890686
translation,23,172,results,different methods,perform,differently,different methods perform differently,0.6439406871795654
translation,23,172,results,differently,on,nine tasks,differently on nine tasks,0.5593599677085876
translation,23,172,results,results,found that,different methods,results found that different methods,0.6674777865409851
translation,23,173,results,effective,for,almost all tasks,effective for almost all tasks,0.6096286773681641
translation,23,173,results,almost all tasks,except for,ja ?en task,almost all tasks except for ja ?en task,0.6506103873252869
translation,23,173,results,results,has,iterative bt,results has iterative bt,0.5525867342948914
translation,23,174,results,iterative kd,performs,better,iterative kd performs better,0.653288722038269
translation,23,174,results,better,for,"en?zh , en?ja and eh ? is tasks","better for en?zh , en?ja and eh ? is tasks",0.6399163603782654
translation,23,174,results,fine -tune,more suitable for,zn?en and en ? ru tasks,fine -tune more suitable for zn?en and en ? ru tasks,0.6595092415809631
translation,23,174,results,results,has,iterative kd,results has iterative kd,0.5756613612174988
translation,24,29,ablation-analysis,mtl,tends to generate,similar model representations,mtl tends to generate similar model representations,0.762453019618988
translation,24,29,ablation-analysis,mtl,preserves,more information,mtl preserves more information,0.6629449725151062
translation,24,29,ablation-analysis,similar model representations,for,different input modalities,similar model representations for different input modalities,0.5707110166549683
translation,24,29,ablation-analysis,similar model representations,preserves,more information,similar model representations preserves more information,0.7149743437767029
translation,24,29,ablation-analysis,more information,from,pretrained mt modules,more information from pretrained mt modules,0.5463232398033142
translation,24,29,ablation-analysis,ablation analysis,confirms,mtl,ablation analysis confirms mtl,0.6523818969726562
translation,24,31,ablation-analysis,more parameters,helpful to,transfer knowledge,more parameters helpful to transfer knowledge,0.7227939963340759
translation,24,31,ablation-analysis,transfer knowledge,to,primary st task,transfer knowledge to primary st task,0.5648636221885681
translation,24,31,ablation-analysis,ablation analysis,Sharing,more parameters,ablation analysis Sharing more parameters,0.7039914131164551
translation,24,32,ablation-analysis,top layers,in,st decoder,top layers in st decoder,0.5516842007637024
translation,24,32,ablation-analysis,top layers,are,more critical,top layers are more critical,0.6282019019126892
translation,24,32,ablation-analysis,top layers,are,more sensitive,top layers are more sensitive,0.6025441288948059
translation,24,32,ablation-analysis,st decoder,are,more critical,st decoder are more critical,0.6055001020431519
translation,24,32,ablation-analysis,st decoder,are,more sensitive,st decoder are more sensitive,0.6020495295524597
translation,24,32,ablation-analysis,more critical,to,translation performance,more critical to translation performance,0.5268663167953491
translation,24,32,ablation-analysis,more sensitive,to,modality difference,more sensitive to modality difference,0.5748683214187622
translation,24,32,ablation-analysis,ablation analysis,has,top layers,ablation analysis has top layers,0.551328182220459
translation,24,11,model,parameter sharing and initialization strategy,to enhance,information sharing,parameter sharing and initialization strategy to enhance information sharing,0.6777786612510681
translation,24,11,model,information sharing,between,tasks,information sharing between tasks,0.6298931837081909
translation,24,12,model,novel attention - based regularization,proposed for,encoders,novel attention - based regularization proposed for encoders,0.6494475603103638
translation,24,12,model,novel attention - based regularization,pulls,representations,novel attention - based regularization pulls representations,0.6683875918388367
translation,24,12,model,representations,from,different modalities,representations from different modalities,0.5796712040901184
translation,24,12,model,different modalities,has,closer,different modalities has closer,0.59036785364151
translation,24,13,model,online knowledge distillation,enhance,knowledge transfer,online knowledge distillation enhance knowledge transfer,0.606365442276001
translation,24,13,model,knowledge transfer,from,text to the speech task,knowledge transfer from text to the speech task,0.5766596794128418
translation,24,13,model,model,has,online knowledge distillation,model has online knowledge distillation,0.563926637172699
translation,24,35,model,parameter sharing,between,st and mt tasks,parameter sharing between st and mt tasks,0.66855788230896
translation,24,35,model,model,maximize,parameter sharing,model maximize parameter sharing,0.7294073700904846
translation,24,37,model,cross-attentive regularization,introduced for,encoders,cross-attentive regularization introduced for encoders,0.6943118572235107
translation,24,37,model,model,has,cross-attentive regularization,model has cross-attentive regularization,0.5597781538963318
translation,24,38,model,l2 distance,between,two reconstructed encoder output sequences,l2 distance between two reconstructed encoder output sequences,0.6044589281082153
translation,24,38,model,encoder outputs,from,different modalities,encoder outputs from different modalities,0.5655372142791748
translation,24,38,model,different modalities,closer to,each other,different modalities closer to each other,0.7140477895736694
translation,24,38,model,model,minimizes,l2 distance,model minimizes l2 distance,0.6969890594482422
translation,24,39,model,online knowledge distillation learning,introduced for,mtl,online knowledge distillation learning introduced for mtl,0.687713086605072
translation,24,39,model,knowledge transfer,from,mt to the st task,knowledge transfer from mt to the st task,0.5477644205093384
translation,24,39,model,model,has,online knowledge distillation learning,model has online knowledge distillation learning,0.5477019548416138
translation,24,42,model,online knowledge distillation,reduce,model representation difference,online knowledge distillation reduce model representation difference,0.6702708005905151
translation,24,42,model,online knowledge distillation,enhance,knowledge transfer,online knowledge distillation enhance knowledge transfer,0.606365442276001
translation,24,42,model,model representation difference,between,different modalities,model representation difference between different modalities,0.6575236916542053
translation,24,42,model,knowledge transfer,from,mt task,knowledge transfer from mt task,0.5050674676895142
translation,24,42,model,model,has,cross-attentive regularization,model has cross-attentive regularization,0.5597781538963318
translation,24,8,results,minimal negative transfer effect,between,two tasks,minimal negative transfer effect between two tasks,0.5995805859565735
translation,24,8,results,more parameters,helpful to transfer,knowledge,more parameters helpful to transfer knowledge,0.7099224328994751
translation,24,8,results,knowledge,from,text task,knowledge from text task,0.5534712672233582
translation,24,8,results,results,observe,minimal negative transfer effect,results observe minimal negative transfer effect,0.6291437745094299
translation,24,30,results,significant negative transfer effect,from,mt task,significant negative transfer effect from mt task,0.49520301818847656
translation,24,30,results,mt task,to,corresponding st task,mt task to corresponding st task,0.5976511240005493
translation,24,30,results,results,observe,significant negative transfer effect,results observe significant negative transfer effect,0.5767500996589661
translation,24,43,results,state of the art results,on,"must -c english - german ( en - de ) , english - french ( en - fr ) and english -spanish ( en - es ) language pairs","state of the art results on must -c english - german ( en - de ) , english - french ( en - fr ) and english -spanish ( en - es ) language pairs",0.4854757487773895
translation,24,43,results,2 or more bleu gains,over,strong baselines,2 or more bleu gains over strong baselines,0.6577431559562683
translation,25,113,ablation-analysis,number of iterations,taken during,decoding,number of iterations taken during decoding,0.6896804571151733
translation,25,113,ablation-analysis,significantly decreases,with respect to,our proposed rewritenat,significantly decreases with respect to our proposed rewritenat,0.7609314322471619
translation,25,113,ablation-analysis,previous iterative nats,has,number of iterations,previous iterative nats has number of iterations,0.5998257398605347
translation,25,113,ablation-analysis,decoding,has,significantly decreases,decoding has significantly decreases,0.6291800737380981
translation,25,113,ablation-analysis,ablation analysis,compared with,previous iterative nats,ablation analysis compared with previous iterative nats,0.6555565595626831
translation,25,83,experimental-setup,hyperparameters,for,"transformer - base ( vaswani et al. , 2017 )","hyperparameters for transformer - base ( vaswani et al. , 2017 )",0.5231420397758484
translation,25,84,experimental-setup,weight initialization schema,from,"bert ( devlin et al. , 2019 )","weight initialization schema from bert ( devlin et al. , 2019 )",0.5367968678474426
translation,25,84,experimental-setup,weights,from,"n ( 0 , 0.02 )","weights from n ( 0 , 0.02 )",0.5824266076087952
translation,25,84,experimental-setup,weights,set,biases,weights set biases,0.7025505304336548
translation,25,84,experimental-setup,biases,to,zero,biases to zero,0.606534481048584
translation,25,84,experimental-setup,layer normalization parameters,to,? = 0 and ? = 1,layer normalization parameters to ? = 0 and ? = 1,0.5179920196533203
translation,25,84,experimental-setup,regularization,use,dropout,regularization use dropout,0.6589788198471069
translation,25,84,experimental-setup,regularization,use,0.01 l 2 weight decay,regularization use 0.01 l 2 weight decay,0.6022562980651855
translation,25,84,experimental-setup,regularization,use,smoothed cross validation loss,regularization use smoothed cross validation loss,0.5480854511260986
translation,25,84,experimental-setup,smoothed cross validation loss,with,? = 0.1,smoothed cross validation loss with ? = 0.1,0.6245825290679932
translation,25,84,experimental-setup,dropout,has,"en? de and en?ro : 0.3 , en?fr : 0.1 , en?zh : 0.25 )","dropout has en? de and en?ro : 0.3 , en?fr : 0.1 , en?zh : 0.25 )",0.6012393832206726
translation,25,84,experimental-setup,experimental setup,follow,weight initialization schema,experimental setup follow weight initialization schema,0.562528669834137
translation,25,84,experimental-setup,experimental setup,follow,"? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8","experimental setup follow ? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8",0.5741896629333496
translation,25,84,experimental-setup,experimental setup,sample,weights,experimental setup sample weights,0.7797411680221558
translation,25,84,experimental-setup,experimental setup,sample,smoothed cross validation loss,experimental setup sample smoothed cross validation loss,0.693916380405426
translation,25,84,experimental-setup,experimental setup,sample,"? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8","experimental setup sample ? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8",0.6973596811294556
translation,25,84,experimental-setup,experimental setup,For,regularization,experimental setup For regularization,0.5391446948051453
translation,25,84,experimental-setup,experimental setup,adopt,adam optimizer,experimental setup adopt adam optimizer,0.6267623901367188
translation,25,84,experimental-setup,experimental setup,adopt,"? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8","experimental setup adopt ? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8",0.6313562989234924
translation,25,84,experimental-setup,experimental setup,using,"? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8","experimental setup using ? 1 = 0.9 , ? 2 = 0.98 , = 1e ?8",0.637800395488739
translation,25,85,experimental-setup,learning rate,scheduled using,inverse_sqrt,learning rate scheduled using inverse_sqrt,0.6873893141746521
translation,25,85,experimental-setup,inverse_sqrt,with,maximum learning rate 0.0005,inverse_sqrt with maximum learning rate 0.0005,0.6340360641479492
translation,25,85,experimental-setup,inverse_sqrt,with,"10,000 warmup steps","inverse_sqrt with 10,000 warmup steps",0.6389943957328796
translation,25,85,experimental-setup,steps,as,4000,steps as 4000,0.628239095211029
translation,25,85,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,25,86,experimental-setup,models,run on,8 tesla v100 gpus,models run on 8 tesla v100 gpus,0.7342218160629272
translation,25,86,experimental-setup,8 tesla v100 gpus,for,"300,000 updates","8 tesla v100 gpus for 300,000 updates",0.5680520534515381
translation,25,86,experimental-setup,"300,000 updates",with,effective batch size,"300,000 updates with effective batch size",0.6378687024116516
translation,25,86,experimental-setup,effective batch size,of,"128,000 tokens","effective batch size of 128,000 tokens",0.6054466366767883
translation,25,86,experimental-setup,experimental setup,run on,8 tesla v100 gpus,experimental setup run on 8 tesla v100 gpus,0.725068211555481
translation,25,86,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,25,87,experimental-setup,decoding,use,beam size,decoding use beam size,0.66057288646698
translation,25,87,experimental-setup,decoding,use,"length beam ( ghazvininejad et al. , 2019 )","decoding use length beam ( ghazvininejad et al. , 2019 )",0.597696840763092
translation,25,87,experimental-setup,beam size,of,b = 5,beam size of b = 5,0.6777058243751526
translation,25,87,experimental-setup,b = 5,for,autoregressive decoding,b = 5 for autoregressive decoding,0.640389084815979
translation,25,87,experimental-setup,"length beam ( ghazvininejad et al. , 2019 )",applied to obtain,translation,"length beam ( ghazvininejad et al. , 2019 ) applied to obtain translation",0.646533191204071
translation,25,87,experimental-setup,translation,with respect to,non-autoregressive counterpart,translation with respect to non-autoregressive counterpart,0.7343258857727051
translation,25,87,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,25,7,model,architecture,named,rewritenat,architecture named rewritenat,0.717950701713562
translation,25,7,model,rewritenat,to explicitly learn to rewrite,erroneous translation pieces,rewritenat to explicitly learn to rewrite erroneous translation pieces,0.7609038949012756
translation,25,7,model,model,propose,architecture,model propose architecture,0.697865903377533
translation,25,8,model,rewritenat,utilizes,locator module,rewritenat utilizes locator module,0.6994367837905884
translation,25,8,model,locator module,to locate,erroneous ones,locator module to locate erroneous ones,0.6266984939575195
translation,25,8,model,erroneous ones,revised into,correct ones,erroneous ones revised into correct ones,0.7354435324668884
translation,25,8,model,correct ones,by,revisor module,correct ones by revisor module,0.5603204369544983
translation,25,8,model,model,has,rewritenat,model has rewritenat,0.6633418202400208
translation,25,9,model,consistency,of,data distribution,consistency of data distribution,0.5972899794578552
translation,25,9,model,data distribution,with,iterative decoding,data distribution with iterative decoding,0.641104519367218
translation,25,9,model,consistency,has,iterative training strategy,consistency has iterative training strategy,0.582903265953064
translation,25,9,model,data distribution,has,iterative training strategy,data distribution has iterative training strategy,0.5448996424674988
translation,25,9,model,iterative decoding,has,iterative training strategy,iterative decoding has iterative training strategy,0.5458263158798218
translation,25,9,model,model,Towards keeping,consistency,model Towards keeping consistency,0.594831109046936
translation,25,26,model,architecture,named,rewritenat,architecture named rewritenat,0.717950701713562
translation,25,26,model,rewritenat,explicitly learns to rewrite,erroneous translation pieces,rewritenat explicitly learns to rewrite erroneous translation pieces,0.8213405013084412
translation,25,26,model,model,propose,architecture,model propose architecture,0.697865903377533
translation,25,27,model,locator module,to locate,incorrect words,locator module to locate incorrect words,0.6185792088508606
translation,25,27,model,incorrect words,within,previously generated translation,incorrect words within previously generated translation,0.5672800540924072
translation,25,27,model,model,introduce,locator module,model introduce locator module,0.6800999641418457
translation,25,91,results,rewritenat,obtain,substantial improvements,rewritenat obtain substantial improvements,0.633873701095581
translation,25,91,results,substantial improvements,than,strong iterative nat baselines,substantial improvements than strong iterative nat baselines,0.5473079085350037
translation,25,91,results,substantial improvements,while reducing,number of iterations,substantial improvements while reducing number of iterations,0.6849937438964844
translation,25,91,results,results,has,rewritenat,results has rewritenat,0.6379015445709229
translation,25,93,results,rewritenat,obtain,same performance,rewritenat obtain same performance,0.65084308385849
translation,25,93,results,substantially higher speedup,than,iterative nat baselines,substantially higher speedup than iterative nat baselines,0.599690318107605
translation,25,93,results,results,observed,rewritenat,results observed rewritenat,0.6958420872688293
translation,25,94,results,maximum iteration,set as,"2 ( i.e. , t = 2 )","maximum iteration set as 2 ( i.e. , t = 2 )",0.6544687747955322
translation,25,94,results,rewrite - nat,obtains,competitive result,rewrite - nat obtains competitive result,0.6629014611244202
translation,25,94,results,competitive result,to,cmlm and transformer,competitive result to cmlm and transformer,0.5773336887359619
translation,25,94,results,competitive result,with,"higher speedup ( i.e. , 7.02 ? )","competitive result with higher speedup ( i.e. , 7.02 ? )",0.5952892303466797
translation,25,94,results,cmlm and transformer,with,"b = 1 ( i.e. , 27.03 vs. 27.05 )","cmlm and transformer with b = 1 ( i.e. , 27.03 vs. 27.05 )",0.6406811475753784
translation,25,94,results,maximum iteration,has,rewrite - nat,maximum iteration has rewrite - nat,0.6112101078033447
translation,25,94,results,"2 ( i.e. , t = 2 )",has,rewrite - nat,"2 ( i.e. , t = 2 ) has rewrite - nat",0.5904508829116821
translation,25,94,results,results,When,maximum iteration,results When maximum iteration,0.7062197327613831
translation,25,103,results,repetitive words,with respect to,rewritenat,repetitive words with respect to rewritenat,0.7055436968803406
translation,25,103,results,significantly lower,than,most relevant cmlm baseline,significantly lower than most relevant cmlm baseline,0.5662093758583069
translation,25,103,results,decoding,using,single iteration ( - 6.05 % ),decoding using single iteration ( - 6.05 % ),0.7071978449821472
translation,25,103,results,results,proportion of,repetitive words,results proportion of repetitive words,0.6851829886436462
translation,25,104,results,rewrite - nat,achieve,substantial performance,rewrite - nat achieve substantial performance,0.6688397526741028
translation,25,104,results,substantial performance,over,cmlm,substantial performance over cmlm,0.7308014035224915
translation,25,104,results,results,has,rewrite - nat,results has rewrite - nat,0.5827599167823792
translation,26,156,ablation-analysis,significant contribution,to,final model,significant contribution to final model,0.5323574542999268
translation,26,156,ablation-analysis,each approach,has,significant contribution,each approach has significant contribution,0.45253050327301025
translation,26,156,ablation-analysis,ablation analysis,shows,each approach,ablation analysis shows each approach,0.6247193217277527
translation,26,87,baselines,performance,of,low-resource language pairs,performance of low-resource language pairs,0.6204391121864319
translation,26,87,baselines,performance,adopt,pivot-based translation method,performance adopt pivot-based translation method,0.6350993514060974
translation,26,87,baselines,low-resource language pairs,for,x?y directions,low-resource language pairs for x?y directions,0.6239662170410156
translation,26,87,baselines,low-resource language pairs,adopt,pivot-based translation method,low-resource language pairs adopt pivot-based translation method,0.5867435336112976
translation,26,16,experimental-setup,public available deltalm - large checkpoint,to initialize,model,public available deltalm - large checkpoint to initialize model,0.7348082065582275
translation,26,16,experimental-setup,experimental setup,use,public available deltalm - large checkpoint,experimental setup use public available deltalm - large checkpoint,0.6132376790046692
translation,26,60,experimental-setup,deltalm_large architecture,as,backbone model,deltalm_large architecture as backbone model,0.5351282358169556
translation,26,60,experimental-setup,12 inter-leaved decoder layers,with,embedding size,12 inter-leaved decoder layers with embedding size,0.5910120010375977
translation,26,60,experimental-setup,embedding size,of,1024,embedding size of 1024,0.6278319954872131
translation,26,60,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,26,60,experimental-setup,feed -forward network size,of,4096,feed -forward network size of 4096,0.606183648109436
translation,26,60,experimental-setup,16,has,attention heads,16 has attention heads,0.6201440095901489
translation,26,60,experimental-setup,experimental setup,adopt,deltalm_large architecture,experimental setup adopt deltalm_large architecture,0.6453878879547119
translation,26,61,experimental-setup,our model,with,public available deltalm large checkpoint,our model with public available deltalm large checkpoint,0.6611695289611816
translation,26,61,experimental-setup,experimental setup,directly initialize,our model,experimental setup directly initialize our model,0.7078383564949036
translation,26,82,experimental-setup,learning rate,set as,1e - 4,learning rate set as 1e - 4,0.6218491196632385
translation,26,82,experimental-setup,1e - 4,with,warmup step,1e - 4 with warmup step,0.6568119525909424
translation,26,82,experimental-setup,warmup step,of,4,warmup step of 4,0.5935348868370056
translation,26,82,experimental-setup,warmup step,of,000,warmup step of 000,0.6268640756607056
translation,26,82,experimental-setup,warmup step,",",000,"warmup step , 000",0.6077020168304443
translation,26,82,experimental-setup,4,",",000,"4 , 000",0.7578036189079285
translation,26,82,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,26,83,experimental-setup,label smoothing,with,ratio,label smoothing with ratio,0.6687318086624146
translation,26,83,experimental-setup,ratio,of,0.1,ratio of 0.1,0.6428002119064331
translation,26,83,experimental-setup,experimental setup,trained with,label smoothing,experimental setup trained with label smoothing,0.6765040755271912
translation,26,84,experimental-setup,experiments,conducted on,64 nvidia v100 or 32 a100 gpus,experiments conducted on 64 nvidia v100 or 32 a100 gpus,0.6156694889068604
translation,26,84,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,26,85,experimental-setup,batch size,is,1536 or 2048 tokens,batch size is 1536 or 2048 tokens,0.5910415053367615
translation,26,85,experimental-setup,1536 or 2048 tokens,per,gpu,1536 or 2048 tokens per gpu,0.6774399876594543
translation,26,85,experimental-setup,model,updated,32,model updated 32,0.6910762190818787
translation,26,85,experimental-setup,32,for,64 v100 gpus,32 for 64 v100 gpus,0.5073964595794678
translation,26,85,experimental-setup,64,for,32 a100 gpus,64 for 32 a100 gpus,0.5449071526527405
translation,26,85,experimental-setup,steps,to simulate,large batch size,steps to simulate large batch size,0.6695898175239563
translation,26,85,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,26,85,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,26,95,experimental-setup,beam search strategy,performed with,beam size,beam search strategy performed with beam size,0.6479254961013794
translation,26,95,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,26,95,experimental-setup,4,for,target sentence generation,4 for target sentence generation,0.44605445861816406
translation,26,95,experimental-setup,inference,has,beam search strategy,inference has beam search strategy,0.5653661489486694
translation,26,95,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,26,96,experimental-setup,length penalty,as,1.0,length penalty as 1.0,0.5041438937187195
translation,26,96,experimental-setup,experimental setup,set,length penalty,experimental setup set length penalty,0.6013389825820923
translation,26,14,experiments,effects of different advanced approaches,for,multilingual machine translation,effects of different advanced approaches for multilingual machine translation,0.5503696203231812
translation,26,14,experiments,effects of different advanced approaches,especially on,large-scale dataset,effects of different advanced approaches especially on large-scale dataset,0.6319621205329895
translation,26,81,experiments,multilingual models,with,"adam optimizer ( kingma and ba , 2014 ) ( ? 1 = 0.9 , ? 2 = 0.98 )","multilingual models with adam optimizer ( kingma and ba , 2014 ) ( ? 1 = 0.9 , ? 2 = 0.98 )",0.5682350993156433
translation,26,15,model,pre-trained language models,trained with,large-scale monolingual data,pre-trained language models trained with large-scale monolingual data,0.7274079322814941
translation,26,15,model,model,leverage,pre-trained language models,model leverage pre-trained language models,0.6390557885169983
translation,26,20,model,model,with,24 encoder layers,model with 24 encoder layers,0.6159191727638245
translation,26,20,model,12 layers,on the top of,encoder,12 layers on the top of encoder,0.7544810175895691
translation,26,20,model,model,train,model,model train model,0.6881781220436096
translation,26,21,model,iterative back - translation,back - translates,data,iterative back - translation back - translates data,0.850853681564331
translation,26,21,model,data,for,multiple rounds,data for multiple rounds,0.6820985078811646
translation,26,76,model,shallow translation model,with,24 encoder layers,shallow translation model with 24 encoder layers,0.5774952173233032
translation,26,76,model,shallow translation model,with,12 decoder layers,shallow translation model with 12 decoder layers,0.5936877727508545
translation,26,76,model,12 decoder layers,fine-tuned on,all available multilingual corpora,12 decoder layers fine-tuned on all available multilingual corpora,0.6621919274330139
translation,26,76,model,model,has,shallow translation model,model has shallow translation model,0.5276093482971191
translation,26,77,model,second stage,increase,depth of the encoder,second stage increase depth of the encoder,0.72471684217453
translation,26,77,model,depth of the encoder,from,24 layers,depth of the encoder from 24 layers,0.5519052147865295
translation,26,77,model,24 layers,to,36 layers,24 layers to 36 layers,0.5889800786972046
translation,26,77,model,bottom 24 layers of the encoder,initialized with,shallow model 's encoder,bottom 24 layers of the encoder initialized with shallow model 's encoder,0.7413119077682495
translation,26,101,results,our model,with,36 encoder layers,our model with 36 encoder layers,0.6037713885307312
translation,26,101,results,shallow counterpart,with,24 encoder layers,shallow counterpart with 24 encoder layers,0.6364694833755493
translation,26,101,results,36 encoder layers,has,significantly outperforms,36 encoder layers has significantly outperforms,0.6039637327194214
translation,26,101,results,significantly outperforms,has,shallow counterpart,significantly outperforms has shallow counterpart,0.5948946475982666
translation,26,101,results,results,has,our model,results has our model,0.5871725678443909
translation,26,102,results,our model,with,hybrid strategy,our model with hybrid strategy,0.695648193359375
translation,26,102,results,hybrid strategy,gets,best performance,hybrid strategy gets best performance,0.6495063900947571
translation,26,102,results,best performance,with,less inference cost,best performance with less inference cost,0.6307896971702576
translation,26,102,results,best performance,costs,double inference time,best performance costs double inference time,0.5854105949401855
translation,26,102,results,less inference cost,than,pivot-based translation,less inference cost than pivot-based translation,0.5508422255516052
translation,26,102,results,double inference time,compared to,direct translation,double inference time compared to direct translation,0.6258300542831421
translation,26,102,results,results,shows,our model,results shows our model,0.7287026643753052
translation,26,110,results,significant improvement,of,+ 9.41 bleu points,significant improvement of + 9.41 bleu points,0.5143883228302002
translation,26,110,results,+ 9.41 bleu points,over,strong m2 m baseline,+ 9.41 bleu points over strong m2 m baseline,0.620823323726654
translation,26,110,results,largest model ( 36 encoder layers and 12 decoder layers ),has,significant improvement,largest model ( 36 encoder layers and 12 decoder layers ) has significant improvement,0.5243950486183167
translation,26,110,results,results,observe that,largest model ( 36 encoder layers and 12 decoder layers ),results observe that largest model ( 36 encoder layers and 12 decoder layers ),0.5472433567047119
translation,26,119,results,deltalm + zcode ( hybrid ),outperforms,direct and pivot-based translation,deltalm + zcode ( hybrid ) outperforms direct and pivot-based translation,0.7723091840744019
translation,26,119,results,direct and pivot-based translation,by about,+ 0.5 bleu points,direct and pivot-based translation by about + 0.5 bleu points,0.5648958683013916
translation,26,121,results,deep model,with,36 encoder layers and 12 decoder layers,deep model with 36 encoder layers and 12 decoder layers,0.5893523693084717
translation,26,121,results,deep model,with,24 encoder layers and 12 decoder layers,deep model with 24 encoder layers and 12 decoder layers,0.5857471823692322
translation,26,121,results,deep model,with,24 encoder layers and 12 decoder layers,deep model with 24 encoder layers and 12 decoder layers,0.5857471823692322
translation,26,121,results,comparable performance,with,shallow model,comparable performance with shallow model,0.6773046255111694
translation,26,121,results,shallow model,with,24 encoder layers and 12 decoder layers,shallow model with 24 encoder layers and 12 decoder layers,0.5968617796897888
translation,26,121,results,36 encoder layers and 12 decoder layers,has,comparable performance,36 encoder layers and 12 decoder layers has comparable performance,0.5394617319107056
translation,26,121,results,results,has,deep model,results has deep model,0.5769193172454834
translation,26,161,results,deltalm + zcode,ranks,first,deltalm + zcode ranks first,0.7450199127197266
translation,26,161,results,first,across,three tracks,first across three tracks,0.7471338510513306
translation,26,161,results,board,has,deltalm + zcode,board has deltalm + zcode,0.5938732624053955
translation,27,56,experimental-setup,1 - 3 gpus,for,training,1 - 3 gpus for training,0.6054736971855164
translation,27,56,experimental-setup,batch size,is,2048,batch size is 2048,0.6247645020484924
translation,27,56,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,27,57,experimental-setup,embedding size and hidden size,set to,1024,embedding size and hidden size set to 1024,0.7366647124290466
translation,27,57,experimental-setup,dimension,of,feed -forward layer,dimension of feed -forward layer,0.6009939312934875
translation,27,57,experimental-setup,feed -forward layer,is,4096,feed -forward layer is 4096,0.5811145901679993
translation,27,57,experimental-setup,experimental setup,has,embedding size and hidden size,experimental setup has embedding size and hidden size,0.5544222593307495
translation,27,57,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,27,59,experimental-setup,dropout probabilities,set to,0.3,dropout probabilities set to 0.3,0.6935458183288574
translation,27,59,experimental-setup,experimental setup,has,"dropout mechanism ( provilkov et al. , 2020 )","experimental setup has dropout mechanism ( provilkov et al. , 2020 )",0.4964912235736847
translation,27,59,experimental-setup,experimental setup,has,dropout probabilities,experimental setup has dropout probabilities,0.508775532245636
translation,27,60,experimental-setup,"bpe ( sennrich et al. , 2016 )",used in,all experiments,"bpe ( sennrich et al. , 2016 ) used in all experiments",0.6863671541213989
translation,27,60,experimental-setup,"bpe ( sennrich et al. , 2016 )",where,merge operations,"bpe ( sennrich et al. , 2016 ) where merge operations",0.5756704807281494
translation,27,60,experimental-setup,merge operations,set to,32000,merge operations set to 32000,0.6863576769828796
translation,27,60,experimental-setup,experimental setup,has,"bpe ( sennrich et al. , 2016 )","experimental setup has bpe ( sennrich et al. , 2016 )",0.4959581792354584
translation,27,61,experimental-setup,maximum number of tokens,set to,4096,maximum number of tokens set to 4096,0.6882020831108093
translation,27,61,experimental-setup,experimental setup,has,maximum number of tokens,experimental setup has maximum number of tokens,0.5387575626373291
translation,27,62,experimental-setup,loss function,set to,label_smoothed_cross_entropy,loss function set to label_smoothed_cross_entropy,0.665013313293457
translation,27,62,experimental-setup,experimental setup,has,loss function,experimental setup has loss function,0.48357120156288147
translation,27,63,experimental-setup,adam_betas,set to,"( 0.9 , 0.997 )","adam_betas set to ( 0.9 , 0.997 )",0.6932135224342346
translation,27,63,experimental-setup,experimental setup,has,parameter,experimental setup has parameter,0.513629138469696
translation,27,64,experimental-setup,initial learning rate,is,0.0007,initial learning rate is 0.0007,0.5648828148841858
translation,27,64,experimental-setup,warm - up steps,set to,4000,warm - up steps set to 4000,0.7306844592094421
translation,27,64,experimental-setup,maximum epoch number,set to,15,maximum epoch number set to 15,0.723583459854126
translation,27,64,experimental-setup,baseline system,has,initial learning rate,baseline system has initial learning rate,0.4992057979106903
translation,27,64,experimental-setup,baseline system,has,warm - up steps,baseline system has warm - up steps,0.5715032815933228
translation,27,64,experimental-setup,baseline system,has,maximum epoch number,baseline system has maximum epoch number,0.5535585284233093
translation,27,64,experimental-setup,experimental setup,For,baseline system,experimental setup For baseline system,0.6031438708305359
translation,27,65,experimental-setup,initial learning rate,is,0.0001,initial learning rate is 0.0001,0.5475426316261292
translation,27,65,experimental-setup,warm - up steps,set to,4000,warm - up steps set to 4000,0.7306844592094421
translation,27,65,experimental-setup,maximum epoch number,set to,10,maximum epoch number set to 10,0.740292489528656
translation,27,65,experimental-setup,encoder combination system and decoder combination system,has,initial learning rate,encoder combination system and decoder combination system has initial learning rate,0.5210020542144775
translation,27,65,experimental-setup,encoder combination system and decoder combination system,has,warm - up steps,encoder combination system and decoder combination system has warm - up steps,0.5923826694488525
translation,27,65,experimental-setup,encoder combination system and decoder combination system,has,maximum epoch number,encoder combination system and decoder combination system has maximum epoch number,0.5632962584495544
translation,27,65,experimental-setup,experimental setup,For,encoder combination system and decoder combination system,experimental setup For encoder combination system and decoder combination system,0.6221386194229126
translation,27,5,model,pivot method,used in,our system,pivot method used in our system,0.6903260946273804
translation,27,5,model,our system,pipelines,russian - to - english translator and the english - to - chinese translator,our system pipelines russian - to - english translator and the english - to - chinese translator,0.7077715396881104
translation,27,5,model,russian - to - english translator and the english - to - chinese translator,to form,russian - to - chinese translator,russian - to - english translator and the english - to - chinese translator to form russian - to - chinese translator,0.6324995756149292
translation,27,58,model,six self-attention layers,for,both encoder and decoder,six self-attention layers for both encoder and decoder,0.5763852596282959
translation,27,58,model,multi-head self-attention mechanism,has,16 heads,multi-head self-attention mechanism has 16 heads,0.567532479763031
translation,27,58,model,model,use,six self-attention layers,model use six self-attention layers,0.6417887210845947
translation,27,71,results,primary submission,achieves,bleu score,primary submission achieves bleu score,0.6842708587646484
translation,27,71,results,primary submission,ranked,fourth,primary submission ranked fourth,0.7053260803222656
translation,27,71,results,bleu score,of,19.2,bleu score of 19.2,0.5436838865280151
translation,27,71,results,fourth,among,all participating teams,fourth among all participating teams,0.655359148979187
translation,27,71,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,27,72,results,contrast submission,achieves,bleu score,contrast submission achieves bleu score,0.6775712370872498
translation,27,72,results,bleu score,of,18.1,bleu score of 18.1,0.5467473864555359
translation,27,72,results,results,has,contrast submission,results has contrast submission,0.536990225315094
translation,28,43,experimental-setup,input,using,tokenizer,input using tokenizer,0.7143421769142151
translation,28,43,experimental-setup,input,using,"spacy ( honnibal et al. , 2020 )","input using spacy ( honnibal et al. , 2020 )",0.6662746667861938
translation,28,43,experimental-setup,"spacy ( honnibal et al. , 2020 )",for,english side,"spacy ( honnibal et al. , 2020 ) for english side",0.5807218551635742
translation,28,43,experimental-setup,experimental setup,pretokenized,input,experimental setup pretokenized input,0.7399957776069641
translation,28,44,experimental-setup,beam width,of,4,beam width of 4,0.7052165865898132
translation,28,44,experimental-setup,4,used for,beam search,4 used for beam search,0.5549005270004272
translation,28,44,experimental-setup,beam search,during,backtranslation,beam search during backtranslation,0.7039390802383423
translation,28,44,experimental-setup,experimental setup,has,beam width,experimental setup has beam width,0.4580118954181671
translation,28,45,experimental-setup,training iteration,took,approximately one week,training iteration took approximately one week,0.6668769717216492
translation,28,45,experimental-setup,approximately one week,on,single gtx 1080 graphics card,approximately one week on single gtx 1080 graphics card,0.5051144361495972
translation,28,45,experimental-setup,experimental setup,has,training iteration,experimental setup has training iteration,0.5293617844581604
translation,28,59,experimental-setup,initial learning rate,set to,3e - 4,initial learning rate set to 3e - 4,0.7237129807472229
translation,28,59,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,28,60,experimental-setup,sixteen 32 gb nvidia v100 gpus,connected with,infiniband,sixteen 32 gb nvidia v100 gpus connected with infiniband,0.6468278765678406
translation,28,60,experimental-setup,sixteen 32 gb nvidia v100 gpus,used for,training,sixteen 32 gb nvidia v100 gpus used for training,0.6568974852561951
translation,28,60,experimental-setup,infiniband,used for,training,infiniband used for training,0.6901049017906189
translation,28,60,experimental-setup,experimental setup,has,sixteen 32 gb nvidia v100 gpus,experimental setup has sixteen 32 gb nvidia v100 gpus,0.5613036751747131
translation,28,61,experimental-setup,effective batch size,around,10 k sequences,effective batch size around 10 k sequences,0.6825454235076904
translation,28,61,experimental-setup,4 days,of,wall clock time,4 days of wall clock time,0.5869848132133484
translation,28,61,experimental-setup,experimental setup,has,effective batch size,experimental setup has effective batch size,0.5394346117973328
translation,28,61,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,29,8,model,sequence - to-sequence architecture,with,character - level decoder,sequence - to-sequence architecture with character - level decoder,0.628020167350769
translation,29,8,model,character - level decoder,takes,lemma,character - level decoder takes lemma,0.6788164973258972
translation,29,8,model,character - level decoder,takes,lemma,character - level decoder takes lemma,0.6788164973258972
translation,29,8,model,lemma,of,user-specified term,lemma of user-specified term,0.5855775475502014
translation,29,8,model,lemma,of,words,lemma of words,0.6239060759544373
translation,29,8,model,words,generated from,word - level decoder,words generated from word - level decoder,0.6279655694961548
translation,29,8,model,correct inflected form,of,lemma,correct inflected form of lemma,0.5615507364273071
translation,29,8,model,model,extend,sequence - to-sequence architecture,model extend sequence - to-sequence architecture,0.7164607048034668
translation,29,30,model,model,automatically inflects,pre-specified term,model automatically inflects pre-specified term,0.7700881958007812
translation,29,30,model,pre-specified term,according to,context,pre-specified term according to context,0.6887123584747314
translation,29,30,model,context,of,produced translation,context of produced translation,0.6014434099197388
translation,29,30,model,model,propose,model,model propose model,0.6740307211875916
translation,29,30,model,model,automatically inflects,pre-specified term,model automatically inflects pre-specified term,0.7700881958007812
translation,29,31,model,sequence - to-sequence encoder and decoder,with,additional character - level decoder,sequence - to-sequence encoder and decoder with additional character - level decoder,0.6335574388504028
translation,29,31,model,additional character - level decoder,predicts,inflected form,additional character - level decoder predicts inflected form,0.691261351108551
translation,29,31,model,inflected form,of,pre-specified term,inflected form of pre-specified term,0.581291139125824
translation,29,31,model,model,extend,sequence - to-sequence encoder and decoder,model extend sequence - to-sequence encoder and decoder,0.7090439200401306
translation,29,32,model,advantages,of both,placeholder and the code-switching methods,advantages of both placeholder and the code-switching methods,0.7150427103042603
translation,29,32,model,faithfulness,to,lexical constraints,faithfulness to lexical constraints,0.5516926646232605
translation,29,32,model,flexibility,of dynamically deciding,word form,flexibility of dynamically deciding word form,0.6746863722801208
translation,29,32,model,word form,in,output,word form in output,0.5353031754493713
translation,29,32,model,placeholder and the code-switching methods,has,faithfulness,placeholder and the code-switching methods has faithfulness,0.5825191736221313
translation,29,163,results,ph ( oracle ),exhibits,nearly perfect specified term use rates,ph ( oracle ) exhibits nearly perfect specified term use rates,0.6787776350975037
translation,29,163,results,nearly perfect specified term use rates,has,more 98 %,nearly perfect specified term use rates has more 98 %,0.562982976436615
translation,29,163,results,results,observe,ph ( oracle ),results observe ph ( oracle ),0.6579515933990479
translation,29,164,results,more successful,at incorporating,specified term,more successful at incorporating specified term,0.6899821162223816
translation,29,164,results,specified term,into,translation,specified term into translation,0.6396782398223877
translation,29,164,results,translation,than,cs ( oracle ),translation than cs ( oracle ),0.5921494960784912
translation,29,164,results,translation,in the setting of,one constraint,translation in the setting of one constraint,0.6758390665054321
translation,29,164,results,results,is,more successful,results is more successful,0.5652786493301392
translation,29,165,results,results,are,quite mixed,results are quite mixed,0.5640879273414612
translation,29,165,results,quite mixed,for,noun,quite mixed for noun,0.6583108901977539
translation,29,165,results,inflection,has,results,inflection has results,0.563888669013977
translation,29,165,results,results,are,quite mixed,results are quite mixed,0.5640879273414612
translation,29,166,results,simple strategy,of predicting,most common inflection,simple strategy of predicting most common inflection,0.681344747543335
translation,29,166,results,most common inflection,achieves,better specified term use rates,most common inflection achieves better specified term use rates,0.6138595342636108
translation,29,166,results,better specified term use rates,than,most of the other sophisticated models,better specified term use rates than most of the other sophisticated models,0.5427649021148682
translation,29,166,results,results,has,simple strategy,results has simple strategy,0.5811981558799744
translation,29,169,results,ph ( morph ),performs,best,ph ( morph ) performs best,0.6561885476112366
translation,29,169,results,verb dictionary,has,ph ( morph ),verb dictionary has ph ( morph ),0.6043027639389038
translation,29,171,results,more restricted setting,has,our proposed model,more restricted setting has our proposed model,0.5639629364013672
translation,29,171,results,our proposed model,has,outperforms,our proposed model has outperforms,0.6381282210350037
translation,29,171,results,outperforms,has,comparable codeswitching model ( cs ( lemma ) ),outperforms has comparable codeswitching model ( cs ( lemma ) ),0.6183264255523682
translation,29,171,results,results,In,more restricted setting,results In more restricted setting,0.5538152456283569
translation,29,172,results,proposed model,is,more robust,proposed model is more robust,0.5976237654685974
translation,29,172,results,more robust,than,cs ( lemma ),more robust than cs ( lemma ),0.5893840789794922
translation,29,172,results,more robust,observe,consistent tendency,more robust observe consistent tendency,0.6196362376213074
translation,29,172,results,cs ( lemma ),to,unseen,cs ( lemma ) to unseen,0.6125655770301819
translation,29,172,results,consistent tendency,that,specified term use rate,consistent tendency that specified term use rate,0.6344308257102966
translation,29,172,results,degrades,when,entries,degrades when entries,0.6877576112747192
translation,29,172,results,are unseen,during,training,are unseen during training,0.6652631759643555
translation,29,172,results,training,especially with,cs ( lemma ) and verb entries ( 81.7 to 42.1 ),training especially with cs ( lemma ) and verb entries ( 81.7 to 42.1 ),0.653192937374115
translation,29,172,results,unseen,has,specified terms,unseen has specified terms,0.6067163944244385
translation,29,172,results,specified term use rate,has,degrades,specified term use rate has degrades,0.5989985466003418
translation,29,172,results,entries,has,are unseen,entries has are unseen,0.6455342769622803
translation,29,172,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,30,11,baselines,series of baseline systems,trained on,data,series of baseline systems trained on data,0.7739588618278503
translation,30,11,baselines,data,from,2020 edition,data from 2020 edition,0.6045732498168945
translation,30,11,baselines,data,for,translation,data for translation,0.6122652888298035
translation,30,11,baselines,translation,between,upper sorbian ( hsb ) and german ( de ),translation between upper sorbian ( hsb ) and german ( de ),0.616521418094635
translation,30,93,baselines,translation language modeling ( tlm ),concatenate,source and target sentences,translation language modeling ( tlm ) concatenate source and target sentences,0.6966647505760193
translation,30,93,baselines,source and target sentences,from,parallel corpus,source and target sentences from parallel corpus,0.5228410959243774
translation,30,93,baselines,mask algorithm,to,each one,mask algorithm to each one,0.6282576322555542
translation,30,93,baselines,baselines,has,translation language modeling ( tlm ),baselines has translation language modeling ( tlm ),0.557550311088562
translation,30,33,experimental-setup,gra-dients,over,2 batches,gra-dients over 2 batches,0.6996011137962341
translation,30,33,experimental-setup,gra-dients,train on,2 gpus,gra-dients train on 2 gpus,0.6833721399307251
translation,30,33,experimental-setup,2 gpus,with,batch_size,2 gpus with batch_size,0.6050093173980713
translation,30,33,experimental-setup,2 gpus,with,2 k,2 gpus with 2 k,0.6453481316566467
translation,30,33,experimental-setup,batch_size,of,1 k,batch_size of 1 k,0.6407611966133118
translation,30,33,experimental-setup,batch_size,of,2 k,batch_size of 2 k,0.650909960269928
translation,30,33,experimental-setup,1 k,for,base,1 k for base,0.6856658458709717
translation,30,33,experimental-setup,2 k,for,big,2 k for big,0.7457172870635986
translation,30,33,experimental-setup,experimental setup,accumulate,gra-dients,experimental setup accumulate gra-dients,0.6189201474189758
translation,30,34,experimental-setup,  noam   learning rate schedule,with,8 k warmup steps,  noam   learning rate schedule with 8 k warmup steps,0.6607671976089478
translation,30,34,experimental-setup,experimental setup,use,  noam   learning rate schedule,experimental setup use   noam   learning rate schedule,0.6268842816352844
translation,30,65,results,baseline hsb ? de model,with,combined 2021 and 2020 data,baseline hsb ? de model with combined 2021 and 2020 data,0.6137736439704895
translation,30,65,results,baseline hsb ? de model,reaches,bleu scores,baseline hsb ? de model reaches bleu scores,0.6079544425010681
translation,30,65,results,baseline hsb ? de model,by ensembling,best 4 saved checkpoints,baseline hsb ? de model by ensembling best 4 saved checkpoints,0.7469001412391663
translation,30,65,results,bleu scores,of,59.29,bleu scores of 59.29,0.5315316915512085
translation,30,65,results,bleu scores,of,51.86,bleu scores of 51.86,0.5307912230491638
translation,30,65,results,59.29,on,' dev ' set,59.29 on ' dev ' set,0.6293705701828003
translation,30,65,results,51.86,on,devtest ' set,51.86 on devtest ' set,0.5394037961959839
translation,30,65,results,devtest ' set,after,training,devtest ' set after training,0.7047805786132812
translation,30,65,results,training,for,"150,000 steps","training for 150,000 steps",0.6439037919044495
translation,30,65,results,results,has,baseline hsb ? de model,results has baseline hsb ? de model,0.5788149833679199
translation,30,72,results,best 4 checkpoints,reach,bleu scores,best 4 checkpoints reach bleu scores,0.6925023198127747
translation,30,72,results,bleu scores,of,8.25,bleu scores of 8.25,0.553505003452301
translation,30,72,results,8.25,/,8.22,8.25 / 8.22,0.5967952609062195
translation,30,72,results,ensembling,has,best 4 checkpoints,ensembling has best 4 checkpoints,0.5905771851539612
translation,30,72,results,results,When,ensembling,results When ensembling,0.6738938689231873
translation,30,139,results,our child de ?hsb models,show,scheduled training,our child de ?hsb models show scheduled training,0.7287836670875549
translation,30,139,results,scheduled training,improves,results,scheduled training improves results,0.7013694047927856
translation,30,139,results,results,over,baseline,results over baseline,0.6579297780990601
translation,30,139,results,results,has,our child de ?hsb models,results has our child de ?hsb models,0.5365089178085327
translation,30,140,results,hsb ? de model,with,training schedule,hsb ? de model with training schedule,0.6537731289863586
translation,30,140,results,hsb ? de model,trained with,lighter architecture ( base vs. big ),hsb ? de model trained with lighter architecture ( base vs. big ),0.7012498378753662
translation,30,140,results,hsb ? de model,trained with,lower quality parent model ( 19.8 vs. 24.5 ),hsb ? de model trained with lower quality parent model ( 19.8 vs. 24.5 ),0.6870480179786682
translation,30,140,results,hsb ? de model,achieves,higher bleu score,hsb ? de model achieves higher bleu score,0.6572851538658142
translation,30,140,results,hsb ? de model,quality of,back - translated data,hsb ? de model quality of back - translated data,0.6084946393966675
translation,30,140,results,back - translated data,with,hsb ? de model,back - translated data with hsb ? de model,0.6491156816482544
translation,30,140,results,improved slightly,with,addition of the mask monolingual task,improved slightly with addition of the mask monolingual task,0.6115408539772034
translation,30,140,results,hsb ? de model,has,improved slightly,hsb ? de model has improved slightly,0.6007344126701355
translation,30,140,results,results,has,hsb ? de model,results has hsb ? de model,0.5977747440338135
translation,30,141,results,models,trained on,de?hsb task,models trained on de?hsb task,0.7846664190292358
translation,30,141,results,improved,from,8.7 to 9.6,improved from 8.7 to 9.6,0.5459917783737183
translation,30,141,results,improved,on,' devtest ' set,improved on ' devtest ' set,0.5923246741294861
translation,30,141,results,8.7 to 9.6,on,' devtest ' set,8.7 to 9.6 on ' devtest ' set,0.5844295620918274
translation,30,141,results,ensemble,has,models,ensemble has models,0.601379930973053
translation,30,141,results,ensemble,has,scores,ensemble has scores,0.5858426094055176
translation,30,141,results,models,has,scores,models has scores,0.6379629373550415
translation,30,141,results,scores,has,improved,scores has improved,0.6000511050224304
translation,30,141,results,results,including in,ensemble,results including in ensemble,0.6629432439804077
translation,30,143,results,scheduled multi-task training,improved over,our best performing baselines,scheduled multi-task training improved over our best performing baselines,0.6847718954086304
translation,30,143,results,our best performing baselines,for,all directions,our best performing baselines for all directions,0.5971638560295105
translation,30,143,results,all directions,of,low-resource mt task,all directions of low-resource mt task,0.5677828192710876
translation,30,143,results,results,strategy of,diverse ensembles,results strategy of diverse ensembles,0.7033514976501465
translation,31,204,ablation-analysis,knowledge distillation,improve,baseline,knowledge distillation improve baseline,0.6862940192222595
translation,31,204,ablation-analysis,baseline,from,35.78,baseline from 35.78,0.504065215587616
translation,31,204,ablation-analysis,35.78,to,36.66,35.78 to 36.66,0.5939072966575623
translation,31,204,ablation-analysis,ablation analysis,has,back-translation,ablation analysis has back-translation,0.5684170722961426
translation,31,210,ablation-analysis,advanced finetuning methods,to further get,0.19 bleu improvements,advanced finetuning methods to further get 0.19 bleu improvements,0.6186721920967102
translation,31,210,ablation-analysis,ablation analysis,apply,advanced finetuning methods,ablation analysis apply advanced finetuning methods,0.6516459584236145
translation,31,215,ablation-analysis,more bt data,improve,bleu score,more bt data improve bleu score,0.6547167301177979
translation,31,215,ablation-analysis,bleu score,from,20.82 to 22.11,bleu score from 20.82 to 22.11,0.5065590739250183
translation,31,215,ablation-analysis,ablation analysis,has,knowledge distillation,ablation analysis has knowledge distillation,0.5629406571388245
translation,31,225,ablation-analysis,knowledge distillation,boosts,bleu score,knowledge distillation boosts bleu score,0.6415325999259949
translation,31,225,ablation-analysis,bleu score,to,36.58,bleu score to 36.58,0.5363497734069824
translation,31,225,ablation-analysis,ablation analysis,has,knowledge distillation,ablation analysis has knowledge distillation,0.5629406571388245
translation,31,226,ablation-analysis,finetuning,brings in,2.63 improvements,finetuning brings in 2.63 improvements,0.6909316182136536
translation,31,226,ablation-analysis,ablation analysis,has,finetuning,ablation analysis has finetuning,0.5503973364830017
translation,31,56,baselines,decreasing weights,with,position increasing,decreasing weights with position increasing,0.6678979992866516
translation,31,56,baselines,decreasing weights,with,learnable weights,decreasing weights with learnable weights,0.6408783793449402
translation,31,56,baselines,decreasing weights,with,exponential weights,decreasing weights with exponential weights,0.6653430461883545
translation,31,38,experimental-setup,multiple model configurations,with,20,multiple model configurations with 20,0.6477943658828735
translation,31,38,experimental-setup,multiple model configurations,with,25 - layer encoders,multiple model configurations with 25 - layer encoders,0.6240687966346741
translation,31,38,experimental-setup,20,/,25 - layer encoders,20 / 25 - layer encoders,0.6312918066978455
translation,31,38,experimental-setup,25 - layer encoders,for,deeper models,25 - layer encoders for deeper models,0.5989901423454285
translation,31,38,experimental-setup,hidden size,set to,1024,hidden size set to 1024,0.740225613117218
translation,31,38,experimental-setup,1024,for,all models,1024 for all models,0.6813985109329224
translation,31,38,experimental-setup,experimental setup,use,multiple model configurations,experimental setup use multiple model configurations,0.641724169254303
translation,31,40,experimental-setup,wider models,adopt,8/12/15 encoder layers,wider models adopt 8/12/15 encoder layers,0.6251184940338135
translation,31,40,experimental-setup,wider models,adopt,1024/2048,wider models adopt 1024/2048,0.6979981660842896
translation,31,40,experimental-setup,1024/2048,for,hidden size,1024/2048 for hidden size,0.6154640316963196
translation,31,40,experimental-setup,experimental setup,For,wider models,experimental setup For wider models,0.613762617111206
translation,31,41,experimental-setup,filter sizes,of,models,filter sizes of models,0.6075341701507568
translation,31,41,experimental-setup,experimental setup,has,filter sizes,experimental setup has filter sizes,0.5120683312416077
translation,31,103,experimental-setup,4 models,with,different architectures,4 models with different architectures,0.5802121162414551
translation,31,103,experimental-setup,4 models,with,training data,4 models with training data,0.5653160810470581
translation,31,103,experimental-setup,experimental setup,use,4 models,experimental setup use 4 models,0.6065812110900879
translation,31,168,experimental-setup,fairseq,for,en?zh and en? de,fairseq for en?zh and en? de,0.7130058407783508
translation,31,168,experimental-setup,experimental setup,has,implementation,experimental setup has implementation,0.5296319723129272
translation,31,169,experimental-setup,single models,carried out on,8 nvidia v100 gpus,single models carried out on 8 nvidia v100 gpus,0.6116848587989807
translation,31,169,experimental-setup,experimental setup,has,single models,experimental setup has single models,0.5755003094673157
translation,31,170,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.998","adam optimizer with ? 1 = 0.9 , ? 2 = 0.998",0.619918167591095
translation,31,170,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,31,171,experimental-setup,gradient accumulation,used,high gpu memory consumption,gradient accumulation used high gpu memory consumption,0.5770444273948669
translation,31,171,experimental-setup,gradient accumulation,due to,high gpu memory consumption,gradient accumulation due to high gpu memory consumption,0.65511155128479
translation,31,171,experimental-setup,experimental setup,has,gradient accumulation,experimental setup has gradient accumulation,0.4636458456516266
translation,31,172,experimental-setup,batch size,set to,8192 tokens,batch size set to 8192 tokens,0.6836884021759033
translation,31,172,experimental-setup,8192 tokens,per,gpu,8192 tokens per gpu,0.6427332758903503
translation,31,172,experimental-setup,update-freq,in,fairseq,update-freq in fairseq,0.585460901260376
translation,31,172,experimental-setup,parameter,in,fairseq,parameter in fairseq,0.5732161998748779
translation,31,172,experimental-setup,fairseq,to,2,fairseq to 2,0.669166088104248
translation,31,172,experimental-setup,update-freq,has,parameter,update-freq has parameter,0.5927608013153076
translation,31,172,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,31,173,experimental-setup,learning rate,set to,0.0005,learning rate set to 0.0005,0.6983845829963684
translation,31,173,experimental-setup,learning rate,set to,2.0,learning rate set to 2.0,0.7214587926864624
translation,31,173,experimental-setup,0.0005,for,fairseq,0.0005 for fairseq,0.6504055857658386
translation,31,173,experimental-setup,2.0,for,opennmt,2.0 for opennmt,0.613677978515625
translation,31,173,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,31,174,experimental-setup,warmup step,=,4000,warmup step = 4000,0.6246213316917419
translation,31,184,experimental-setup,english and german sentences,segmented by,moses,english and german sentences segmented by moses,0.7246012687683105
translation,31,184,experimental-setup,japanese,use,mecab,japanese use mecab,0.6656211614608765
translation,31,184,experimental-setup,mecab,for,segmentation,mecab for segmentation,0.6759904623031616
translation,31,184,experimental-setup,experimental setup,Pre-processing and Post-processing,english and german sentences,experimental setup Pre-processing and Post-processing english and german sentences,0.7255121469497681
translation,31,188,experimental-setup,byte pair encoding bpe,with,32 k operations,byte pair encoding bpe with 32 k operations,0.6604989767074585
translation,31,188,experimental-setup,32 k operations,for,all the languages,32 k operations for all the languages,0.6124697327613831
translation,31,188,experimental-setup,experimental setup,use,byte pair encoding bpe,experimental setup use byte pair encoding bpe,0.6201653480529785
translation,31,189,experimental-setup,de-tokenizing,on,english and german translations,de-tokenizing on english and german translations,0.5302900671958923
translation,31,189,experimental-setup,english and german translations,with,scripts,english and german translations with scripts,0.6248658299446106
translation,31,189,experimental-setup,scripts,provided in,moses,scripts provided in moses,0.7028868198394775
translation,31,228,experimental-setup,post-processing procedure,on,en?de,post-processing procedure on en?de,0.6289430856704712
translation,31,228,experimental-setup,experimental setup,apply,post-processing procedure,experimental setup apply post-processing procedure,0.5903792381286621
translation,31,6,experiments,data filtering,has,large-scale synthetic data generation,data filtering has large-scale synthetic data generation,0.5179373025894165
translation,31,22,experiments,synthetic data generation,exploit,large-scale back - translation,synthetic data generation exploit large-scale back - translation,0.6633849740028381
translation,31,22,experiments,synthetic data generation,exploit,sequence - level knowledge distillation,synthetic data generation exploit sequence - level knowledge distillation,0.7139203548431396
translation,31,22,experiments,large-scale back - translation,to leverage,target-side monolingual data,large-scale back - translation to leverage target-side monolingual data,0.6544015407562256
translation,31,22,experiments,", 2016a ) method",to leverage,target-side monolingual data,", 2016a ) method to leverage target-side monolingual data",0.6777335405349731
translation,31,22,experiments,sequence - level knowledge distillation,to leverage,source-side of bilingual data,sequence - level knowledge distillation to leverage source-side of bilingual data,0.6740046143531799
translation,31,22,experiments,"rush , 2016 )",to leverage,source-side of bilingual data,"rush , 2016 ) to leverage source-side of bilingual data",0.7370275259017944
translation,31,22,experiments,large-scale back - translation,has,", 2016a ) method","large-scale back - translation has , 2016a ) method",0.5401012301445007
translation,31,23,experiments,source-side monolingual data,explore,forward -translation,source-side monolingual data explore forward -translation,0.6161616444587708
translation,31,23,experiments,forward -translation,by,ensemble models,forward -translation by ensemble models,0.5497410297393799
translation,31,23,experiments,ensemble models,to get,general domain synthetic data,ensemble models to get general domain synthetic data,0.5919004678726196
translation,31,49,experiments,average attention transformer ( aan ),to introduce,more model diversity,average attention transformer ( aan ) to introduce more model diversity,0.6654317378997803
translation,31,187,experiments,truecasing,applied to,english ? japanese,truecasing applied to english ? japanese,0.7038252949714661
translation,31,187,experiments,truecasing,applied to,english ? german,truecasing applied to english ? german,0.7028660774230957
translation,31,203,experiments,en?ja task,filter out,sentence pairs,en?ja task filter out sentence pairs,0.707054853439331
translation,31,203,experiments,sentence pairs,containing,japanese characters,sentence pairs containing japanese characters,0.6279662847518921
translation,31,203,experiments,japanese characters,in,english side,japanese characters in english side,0.5224990844726562
translation,31,217,experiments,two -turn in - domain knowledge transfer,boost,bleu score,two -turn in - domain knowledge transfer boost bleu score,0.5981853604316711
translation,31,217,experiments,bleu score,to,25.89,bleu score to 25.89,0.5414509177207947
translation,31,19,model,some novel initialization methods,to alleviate,gradient vanishing problem,some novel initialization methods to alleviate gradient vanishing problem,0.6100358366966248
translation,31,19,model,gradient vanishing problem,of,post- norm transformer,gradient vanishing problem of post- norm transformer,0.5312879085540771
translation,31,19,model,model,adopt,some novel initialization methods,model adopt some novel initialization methods,0.7029532790184021
translation,31,20,model,"multi-head-attention ( vaswani et al. , 2017 )",series of,effective and diverse model variants,"multi-head-attention ( vaswani et al. , 2017 ) series of effective and diverse model variants",0.6625939607620239
translation,31,20,model,model,combine,average attention transformer ( aan ),model combine average attention transformer ( aan ),0.6975995898246765
translation,31,20,model,model,combine,"multi-head-attention ( vaswani et al. , 2017 )","model combine multi-head-attention ( vaswani et al. , 2017 )",0.6643193960189819
translation,31,24,model,iterative in-domain knowledge transfer,to generate,in- domain data,iterative in-domain knowledge transfer to generate in- domain data,0.6794762015342712
translation,31,24,model,model,use,iterative in-domain knowledge transfer,model use iterative in-domain knowledge transfer,0.6210338473320007
translation,31,28,model,search algorithm,based on,self - bleu scores,search algorithm based on self - bleu scores,0.6313942670822144
translation,31,28,model,self - bleu scores,between,candidate models,self - bleu scores between candidate models,0.6079211831092834
translation,31,28,model,candidate models,with,selected models,candidate models with selected models,0.5955444574356079
translation,31,28,model,model,propose,search algorithm,model propose search algorithm,0.7058070302009583
translation,31,104,model,source language sentences,with,generated in - domain target language sentences,source language sentences with generated in - domain target language sentences,0.5840261578559875
translation,31,104,model,generated in - domain target language sentences,as,pseudoparallel corpus,generated in - domain target language sentences as pseudoparallel corpus,0.5102152824401855
translation,31,104,model,model,combine,source language sentences,model combine source language sentences,0.6943995356559753
translation,31,121,model,several advanced finetuning approaches,to strengthen,effects of domain adaption,several advanced finetuning approaches to strengthen effects of domain adaption,0.6754096150398254
translation,31,121,model,several advanced finetuning approaches,to strengthen,ease,several advanced finetuning approaches to strengthen ease,0.710403323173523
translation,31,121,model,ease,has,exposure bias issue,ease has exposure bias issue,0.5370862483978271
translation,31,121,model,model,explore,several advanced finetuning approaches,model explore several advanced finetuning approaches,0.7340500950813293
translation,31,7,results,constrained systems,achieve,"36.9 , 46.9 , 27.8 and 31.3","constrained systems achieve 36.9 , 46.9 , 27.8 and 31.3",0.5883070826530457
translation,31,7,results,constrained systems,achieve,casesensitive bleu scores,constrained systems achieve casesensitive bleu scores,0.6146035194396973
translation,31,7,results,casesensitive bleu scores,on,"english ? chinese , english ? japanese , japanese ? english and english ? german","casesensitive bleu scores on english ? chinese , english ? japanese , japanese ? english and english ? german",0.5041454434394836
translation,31,7,results,"36.9 , 46.9 , 27.8 and 31.3",has,casesensitive bleu scores,"36.9 , 46.9 , 27.8 and 31.3 has casesensitive bleu scores",0.5722914934158325
translation,31,7,results,results,has,constrained systems,results has constrained systems,0.5753893852233887
translation,31,8,results,bleu scores,of,"english ? chinese , english ? japanese and japanese ? english","bleu scores of english ? chinese , english ? japanese and japanese ? english",0.5464146137237549
translation,31,8,results,bleu scores,of,english ? german,bleu scores of english ? german,0.5484792590141296
translation,31,8,results,"english ? chinese , english ? japanese and japanese ? english",are,highest,"english ? chinese , english ? japanese and japanese ? english are highest",0.5499112010002136
translation,31,8,results,highest,among,all submissions,highest among all submissions,0.5426722764968872
translation,31,8,results,highest,among,all constrained submissions,highest among all constrained submissions,0.5720680356025696
translation,31,8,results,english ? german,is,highest,english ? german is highest,0.5725212097167969
translation,31,8,results,highest,among,all constrained submissions,highest among all constrained submissions,0.5720680356025696
translation,31,8,results,results,has,bleu scores,results has bleu scores,0.5230661034584045
translation,31,39,results,decoder depth,from,6 to 8 and 10,decoder depth from 6 to 8 and 10,0.6199779510498047
translation,31,39,results,deeper depths,give,limited performance gains,deeper depths give limited performance gains,0.6282464265823364
translation,31,61,results,four mixed - aan models,reach,better ensemble result,four mixed - aan models reach better ensemble result,0.6796588897705078
translation,31,61,results,better ensemble result,than,result,better ensemble result than result,0.535007119178772
translation,31,61,results,result,with,ten models,result with ten models,0.6564785838127136
translation,31,61,results,ten models,consist of,deeper and wider standard transformer,ten models consist of deeper and wider standard transformer,0.701988160610199
translation,31,61,results,results,With,four mixed - aan models,results With four mixed - aan models,0.5030575394630432
translation,31,193,results,sentence pairs,containing,english characters,sentence pairs containing english characters,0.6567073464393616
translation,31,193,results,english characters,in,chinese sentences,english characters in chinese sentences,0.5007074475288391
translation,31,193,results,significant improvement,in,valid set,significant improvement in valid set,0.5403746366500854
translation,31,193,results,results,For,en?zh task,results For en?zh task,0.6460443735122681
translation,31,194,results,large-scale back - translation,obtain,+ 2.0 bleu score,large-scale back - translation obtain + 2.0 bleu score,0.4991116523742676
translation,31,194,results,+ 2.0 bleu score,on,baseline,+ 2.0 bleu score on baseline,0.5365613102912903
translation,31,194,results,results,After applying,large-scale back - translation,results After applying large-scale back - translation,0.6886907815933228
translation,31,195,results,+ 0.62 bleu score,after applying,knowledge distillation,+ 0.62 bleu score after applying knowledge distillation,0.7012912631034851
translation,31,195,results,+ 0.24 bleu,from,forward - translation,+ 0.24 bleu from forward - translation,0.4994933009147644
translation,31,195,results,results,further gain,+ 0.62 bleu score,results further gain + 0.62 bleu score,0.6704731583595276
translation,31,199,results,better model combination,with,less number of models,better model combination with less number of models,0.6084756255149841
translation,31,199,results,better model combination,achieve,50.94 bleu score,better model combination achieve 50.94 bleu score,0.5734371542930603
translation,31,199,results,less number of models,are,quickly searched,less number of models are quickly searched,0.5798994302749634
translation,31,199,results,bsbe,has,better model combination,bsbe has better model combination,0.5788196921348572
translation,31,199,results,results,With,bsbe,results With bsbe,0.681727409362793
translation,31,200,results,wmt2021 english ? chinese submission,achieves,sacre-bleu score,wmt2021 english ? chinese submission achieves sacre-bleu score,0.6645446419715881
translation,31,200,results,sacre-bleu score,of,36.9,sacre-bleu score of 36.9,0.53208327293396
translation,31,200,results,36.9,is,highest,36.9 is highest,0.582553505897522
translation,31,200,results,highest,among,all submissions,highest among all submissions,0.5426722764968872
translation,31,200,results,results,has,wmt2021 english ? chinese submission,results has wmt2021 english ? chinese submission,0.5542348027229309
translation,31,202,results,en? ja,on,newstest 2020,en? ja on newstest 2020,0.6264292001724243
translation,31,202,results,results,of,en? ja,results of en? ja,0.6876324415206909
translation,31,202,results,results,of,newstest 2020,results of newstest 2020,0.6093849539756775
translation,31,202,results,results,on,newstest 2020,results on newstest 2020,0.5825028419494629
translation,31,205,results,more bt data,brings in,0.56 improvements,more bt data brings in 0.56 improvements,0.6732049584388733
translation,31,205,results,results,Adding,more bt data,results Adding more bt data,0.7239001989364624
translation,31,208,results,two in-domain knowledge transfers,improve,bleu score,two in-domain knowledge transfers improve bleu score,0.6063787341117859
translation,31,208,results,bleu score,from,37.22,bleu score from 37.22,0.4993571639060974
translation,31,208,results,37.22,to,43.69,37.22 to 43.69,0.582478940486908
translation,31,208,results,results,has,two in-domain knowledge transfers,results has two in-domain knowledge transfers,0.5330737829208374
translation,31,209,results,normal finetune,provides,0.54 improvements,normal finetune provides 0.54 improvements,0.6611529588699341
translation,31,209,results,0.54 improvements,after,in- domain knowledge transfer,0.54 improvements after in- domain knowledge transfer,0.5877138376235962
translation,31,209,results,results,has,normal finetune,results has normal finetune,0.6022540926933289
translation,31,211,results,final ensemble result,has,outperforms,final ensemble result has outperforms,0.6038177609443665
translation,31,211,results,outperforms,has,baseline 9.57 bleu,outperforms has baseline 9.57 bleu,0.5692363977432251
translation,31,211,results,results,has,final ensemble result,results has final ensemble result,0.5709764361381531
translation,31,214,results,back-translation,provide,1.11 bleu improvements,back-translation provide 1.11 bleu improvements,0.5269651412963867
translation,31,214,results,1.11 bleu improvements,from,baseline,1.11 bleu improvements from baseline,0.5081259608268738
translation,31,218,results,bleu score,achieves,26.,bleu score achieves 26.,0.6610097885131836
translation,31,218,results,normal finetuning,has,bleu score,normal finetuning has bleu score,0.592357337474823
translation,31,218,results,26.,has,27,26. has 27,0.5888119339942932
translation,31,218,results,results,After,normal finetuning,results After normal finetuning,0.691420316696167
translation,31,219,results,advanced finetuning methods,provide,slight improvement,advanced finetuning methods provide slight improvement,0.6209242343902588
translation,31,219,results,slight improvement,on,ja?en,slight improvement on ja?en,0.6378814578056335
translation,31,219,results,results,has,advanced finetuning methods,results has advanced finetuning methods,0.5079400539398193
translation,31,220,results,ensemble,achieve,28.24 bleu,ensemble achieve 28.24 bleu,0.602035403251648
translation,31,220,results,28.24 bleu,in,newstest 2020,28.24 bleu in newstest 2020,0.5296887159347534
translation,31,220,results,results,After,ensemble,results After ensemble,0.6688900589942932
translation,31,223,results,en?de,on,newstest2020,en?de on newstest2020,0.650360643863678
translation,31,223,results,results,of,en?de,results of en?de,0.7134842872619629
translation,31,223,results,results,of,newstest2020,results of newstest2020,0.5957702398300171
translation,31,224,results,back - translation,improve,bleu score,back - translation improve bleu score,0.6370320916175842
translation,31,224,results,bleu score,from,33.28,bleu score from 33.28,0.49855801463127136
translation,31,224,results,33.28,has,to 35.28,33.28 has to 35.28,0.5471287965774536
translation,31,224,results,results,After adding,back - translation,results After adding back - translation,0.6932204365730286
translation,31,227,results,in-domain knowledge,into,monolingual corpus,in-domain knowledge into monolingual corpus,0.5041155815124512
translation,31,227,results,in-domain knowledge,get,another 0.31 bleu gain,in-domain knowledge get another 0.31 bleu gain,0.5466156005859375
translation,31,227,results,results,After injecting,in-domain knowledge,results After injecting in-domain knowledge,0.7029600143432617
translation,33,113,baselines,+ amt method,is,qe system,+ amt method is qe system,0.6208906769752502
translation,33,113,baselines,qe system,with,amt data,qe system with amt data,0.6584497690200806
translation,33,113,baselines,baselines,has,+ amt method,baselines has + amt method,0.5855575799942017
translation,33,103,experimental-setup,predictor and the estimator,on,multilingual qe da dataset,predictor and the estimator on multilingual qe da dataset,0.577931821346283
translation,33,103,experimental-setup,multilingual qe da dataset,using,"adam ( kingma and ba , 2015 )","multilingual qe da dataset using adam ( kingma and ba , 2015 )",0.621337890625
translation,33,103,experimental-setup,"adam ( kingma and ba , 2015 )",as optimizer,constant learning rate,"adam ( kingma and ba , 2015 ) as optimizer constant learning rate",0.7396700978279114
translation,33,103,experimental-setup,"adam ( kingma and ba , 2015 )",as optimizer,training batch size,"adam ( kingma and ba , 2015 ) as optimizer training batch size",0.7649933695793152
translation,33,103,experimental-setup,"adam ( kingma and ba , 2015 )",with,training batch size,"adam ( kingma and ba , 2015 ) with training batch size",0.5985226631164551
translation,33,103,experimental-setup,constant learning rate,of,1e ?6,constant learning rate of 1e ?6,0.636241614818573
translation,33,103,experimental-setup,training batch size,of,16,training batch size of 16,0.6191109418869019
translation,33,103,experimental-setup,experimental setup,train,predictor and the estimator,experimental setup train predictor and the estimator,0.6238553524017334
translation,33,104,experimental-setup,experimental setup,trained on,nvidia tesla v100 gpu,experimental setup trained on nvidia tesla v100 gpu,0.7058739066123962
translation,33,120,hyperparameters,ensemble settings,ensemble,multiple models,ensemble settings ensemble multiple models,0.811079740524292
translation,33,120,hyperparameters,multiple models,with,different pre-trained models,multiple models with different pre-trained models,0.6045088768005371
translation,33,120,hyperparameters,multiple models,with,classification layers,multiple models with classification layers,0.6323779821395874
translation,33,120,hyperparameters,classification layers,using,voting - based method,classification layers using voting - based method,0.7299567461013794
translation,33,120,hyperparameters,hyperparameters,For,ensemble settings,hyperparameters For ensemble settings,0.5355587601661682
translation,33,6,model,predictor-estimator,with,pre-trained xlm - roberta,predictor-estimator with pre-trained xlm - roberta,0.6798284649848938
translation,33,6,model,predictor-estimator,with,regressor,predictor-estimator with regressor,0.6311862468719482
translation,33,6,model,pre-trained xlm - roberta,as,predictor,pre-trained xlm - roberta as predictor,0.5517999529838562
translation,33,6,model,pre-trained xlm - roberta,as,task -specific classifier,pre-trained xlm - roberta as task -specific classifier,0.5346214771270752
translation,33,6,model,regressor,as,estimator,regressor as estimator,0.5711961984634399
translation,33,20,model,pe assisted qe ( peaqe ),by integrating,real pe,pe assisted qe ( peaqe ) by integrating real pe,0.603971540927887
translation,33,20,model,pe assisted qe ( peaqe ),by integrating,addtional high-quality translation,pe assisted qe ( peaqe ) by integrating addtional high-quality translation,0.6443730592727661
translation,33,20,model,addtional high-quality translation,in the way of,multitask learning,addtional high-quality translation in the way of multitask learning,0.6428580284118652
translation,33,20,model,model,extend,pe assisted qe ( peaqe ),model extend pe assisted qe ( peaqe ),0.7226861119270325
translation,33,21,model,data augmentation method,based on,monte carlo ( mc ) dropout,data augmentation method based on monte carlo ( mc ) dropout,0.6726226210594177
translation,33,21,model,performance,of,zero-shot language pairs,performance of zero-shot language pairs,0.5692723393440247
translation,33,21,model,model,explore,data augmentation method,model explore data augmentation method,0.6824847459793091
translation,33,7,results,our systems,by incorporating,post-edit sentence,our systems by incorporating post-edit sentence,0.6634693145751953
translation,33,7,results,our systems,by incorporating,additional high-quality translation sentence,our systems by incorporating additional high-quality translation sentence,0.6842713356018066
translation,33,7,results,additional high-quality translation sentence,in the way of,multitask learning,additional high-quality translation sentence in the way of multitask learning,0.6084941625595093
translation,33,8,results,our data augmentation strategy,based on,monte-,our data augmentation strategy based on monte-,0.7739370465278625
translation,33,8,results,carlo dropout,brings up,significant improvement,carlo dropout brings up significant improvement,0.6909381747245789
translation,33,8,results,significant improvement,on,da sub-task,significant improvement on da sub-task,0.5615776181221008
translation,33,8,results,zero-shot setting,has,our data augmentation strategy,zero-shot setting has our data augmentation strategy,0.5722467303276062
translation,33,8,results,monte-,has,carlo dropout,monte- has carlo dropout,0.5676981210708618
translation,33,8,results,results,in,zero-shot setting,results in zero-shot setting,0.5531973242759705
translation,33,9,results,our submissions,achieve,remarkable results,our submissions achieve remarkable results,0.6103018522262573
translation,33,9,results,remarkable results,over,all tasks,remarkable results over all tasks,0.6633147597312927
translation,33,9,results,results,has,our submissions,results has our submissions,0.5741304159164429
translation,33,22,results,our methods,achieve,impressive performance,our methods achieve impressive performance,0.5938833951950073
translation,33,22,results,impressive performance,on,word and sentence level tasks,impressive performance on word and sentence level tasks,0.4777657091617584
translation,33,22,results,results,has,our methods,results has our methods,0.5312396883964539
translation,33,118,results,best results,obtained,xlmr - large and ffn layer,best results obtained xlmr - large and ffn layer,0.6355835199356079
translation,33,118,results,best results,when applying,xlmr - large and ffn layer,best results when applying xlmr - large and ffn layer,0.6770506501197815
translation,33,118,results,xlmr - large and ffn layer,on,development set,xlmr - large and ffn layer on development set,0.5578129291534424
translation,33,119,results,involvement of amt,brings,significant improvement,involvement of amt brings significant improvement,0.577008068561554
translation,33,119,results,significant improvement,over,all language pairs,significant improvement over all language pairs,0.6741988062858582
translation,33,119,results,results,has,involvement of amt,results has involvement of amt,0.5515486001968384
translation,34,91,ablation-analysis,performance gap,between,different models,performance gap between different models,0.6541216373443604
translation,34,91,ablation-analysis,different models,is,not so noticeable,different models is not so noticeable,0.6034621000289917
translation,34,91,ablation-analysis,not so noticeable,for,et- en,not so noticeable for et- en,0.7611353397369385
translation,34,91,ablation-analysis,not so noticeable,for,en- de,not so noticeable for en- de,0.7666336297988892
translation,34,91,ablation-analysis,not so noticeable,for,en- de,not so noticeable for en- de,0.7666336297988892
translation,34,91,ablation-analysis,difference,is,significant,difference is significant,0.5639302134513855
translation,34,91,ablation-analysis,en- de,has,difference,en- de has difference,0.6486378312110901
translation,34,60,baselines,cnn - hter,train,model,cnn - hter train model,0.6654403805732727
translation,34,60,baselines,model,with,data,model with data,0.6407009959220886
translation,34,60,baselines,data,provided by,shared task organizers,data provided by shared task organizers,0.691639244556427
translation,34,60,baselines,cnn - hterart,use,synthetically computed hter,cnn - hterart use synthetically computed hter,0.5949722528457642
translation,34,60,baselines,synthetically computed hter,between,translation and reference,synthetically computed hter between translation and reference,0.6260933876037598
translation,34,60,baselines,baselines,has,cnn - hter,baselines has cnn - hter,0.5446193218231201
translation,34,41,experimental-setup,sklearn 2 library,set,randomized search,sklearn 2 library set randomized search,0.6584254503250122
translation,34,41,experimental-setup,sklearn 2 library,performed,5 - fold crossvalidation,sklearn 2 library performed 5 - fold crossvalidation,0.26210448145866394
translation,34,41,experimental-setup,randomized search,on,hyperparameters,randomized search on hyperparameters,0.5682629346847534
translation,34,41,experimental-setup,experimental setup,used,sklearn 2 library,experimental setup used sklearn 2 library,0.5840173363685608
translation,34,51,experimental-setup,relu,as,activation function,relu as activation function,0.5622032880783081
translation,34,51,experimental-setup,experimental setup,use,relu,experimental setup use relu,0.6114016175270081
translation,34,66,experiments,both cnn - bleurt models,show,better results,both cnn - bleurt models show better results,0.6059189438819885
translation,34,66,experiments,better results,compared to,ml - ent models,better results compared to ml - ent models,0.6633512377738953
translation,34,66,experiments,cnn model,trained only on,da score,cnn model trained only on da score,0.6940244436264038
translation,34,66,experiments,et - en language pair,has,both cnn - bleurt models,et - en language pair has both cnn - bleurt models,0.5308732390403748
translation,34,6,model,glassbox approach,based on,attention weights,glassbox approach based on attention weights,0.6454616189002991
translation,34,6,model,attention weights,extracted from,machine translation systems,attention weights extracted from machine translation systems,0.5270015597343445
translation,34,6,model,model,propose,glassbox approach,model propose glassbox approach,0.6688717603683472
translation,34,7,model,attention weight matrices,without replacing them with,general metrics,attention weight matrices without replacing them with general metrics,0.6760600805282593
translation,34,7,model,general metrics,has,like entropy ),general metrics has like entropy ),0.5834055542945862
translation,34,7,model,model,directly explore,attention weight matrices,model directly explore attention weight matrices,0.6817328333854675
translation,34,14,model,model,propose,lightweight glass - box approach,model propose lightweight glass - box approach,0.7016900181770325
translation,34,25,model,attention weights,obtained from,transformer mt models,attention weights obtained from transformer mt models,0.5971658825874329
translation,34,25,model,attention weights,feed,all encoder-decoder attentions weights,attention weights feed all encoder-decoder attentions weights,0.7306509017944336
translation,34,25,model,all encoder-decoder attentions weights,into,convolutional neural network ( cnn ),all encoder-decoder attentions weights into convolutional neural network ( cnn ),0.555406391620636
translation,34,25,model,convolutional neural network ( cnn ),to get,qe score,convolutional neural network ( cnn ) to get qe score,0.6197566390037537
translation,34,25,model,model,feed,all encoder-decoder attentions weights,model feed all encoder-decoder attentions weights,0.6919481754302979
translation,34,52,model,variable size,of,input batches,variable size of input batches,0.6278406977653503
translation,34,52,model,variable size,use,adaptive max pooling layer,variable size use adaptive max pooling layer,0.5824613571166992
translation,34,52,model,model,To handle,variable size,model To handle variable size,0.6984372735023499
translation,34,26,results,approach,in,supervised setting,approach in supervised setting,0.5559064149856567
translation,34,26,results,zero-shot scenario,when,training data,zero-shot scenario when training data,0.6140506863594055
translation,34,26,results,training data,for,required language pair,training data for required language pair,0.6171367168426514
translation,34,26,results,required language pair,is,not available,required language pair is not available,0.5499262809753418
translation,34,26,results,results,test,approach,results test approach,0.7081139087677002
translation,34,67,results,results,For,en- de,results For en- de,0.7156954407691956
translation,34,68,results,cnn - da model,shows,abysmal performance,cnn - da model shows abysmal performance,0.6913001537322998
translation,34,68,results,abysmal performance,compared to,cnn - bleurt,abysmal performance compared to cnn - bleurt,0.612676203250885
translation,34,68,results,abysmal performance,compared to,ml - ent models,abysmal performance compared to ml - ent models,0.6694599986076355
translation,34,68,results,results,has,cnn - da model,results has cnn - da model,0.5486936569213867
translation,34,69,results,performance,of,cnn - bleurt and ml - ent models,performance of cnn - bleurt and ml - ent models,0.537718653678894
translation,34,69,results,cnn - bleurt and ml - ent models,is,comparable,cnn - bleurt and ml - ent models is comparable,0.5710221529006958
translation,34,70,results,zero-shot en- cs,are,not impressive,zero-shot en- cs are not impressive,0.627204418182373
translation,34,70,results,results,for,zero-shot en- cs,results for zero-shot en- cs,0.6188912391662598
translation,34,84,results,both discussed approaches,show,comparable high performance,both discussed approaches show comparable high performance,0.6645542979240417
translation,34,84,results,comparable high performance,with,small amount of training / validation data,comparable high performance with small amount of training / validation data,0.6474741101264954
translation,34,85,results,performance,of,ml - ent models,performance of ml - ent models,0.6198248863220215
translation,34,85,results,ml - ent models,for,et- en ( top ) and en-de ( bottom ) language pairs,ml - ent models for et- en ( top ) and en-de ( bottom ) language pairs,0.6176971793174744
translation,34,85,results,slightly worsens,with,decreased amounts of training data,slightly worsens with decreased amounts of training data,0.6642407774925232
translation,34,87,results,all models,demonstrate,moderate linear correlation,all models demonstrate moderate linear correlation,0.5559413433074951
translation,34,87,results,moderate linear correlation,with,post-editing effort,moderate linear correlation with post-editing effort,0.6269219517707825
translation,34,87,results,1750,for,training,1750 for training,0.7272190451622009
translation,34,87,results,250,for,validation,250 for validation,0.6983571648597717
translation,34,87,results,en- et language pair,has,all models,en- et language pair has all models,0.5821366310119629
translation,34,87,results,2000 training / validation examples,has,1750,2000 training / validation examples has 1750,0.5878976583480835
translation,34,87,results,results,in case of,en- et language pair,results in case of en- et language pair,0.6859362125396729
translation,34,90,results,models,with,last layers,models with last layers,0.6538094282150269
translation,34,90,results,models,with,first layers,models with first layers,0.6628681421279907
translation,34,90,results,last layers,comparable to,  all layers   models,last layers comparable to   all layers   models,0.6553745865821838
translation,34,90,results,last layers,comparable to,  all layers   models,last layers comparable to   all layers   models,0.6553745865821838
translation,34,90,results,  all layers   models,is,more noticeable,  all layers   models is more noticeable,0.6197537183761597
translation,34,90,results,results,According to,models,results According to models,0.686180055141449
translation,35,96,ablation-analysis,overall translation quality,in terms of,bleu ),overall translation quality in terms of bleu ),0.5937632918357849
translation,35,96,ablation-analysis,overall translation quality,see that,term filtering,overall translation quality see that term filtering,0.5402526259422302
translation,35,96,ablation-analysis,term filtering,using,idf - based filter,term filtering using idf - based filter,0.681052029132843
translation,35,96,ablation-analysis,idf - based filter,is,crucial,idf - based filter is crucial,0.5906797051429749
translation,35,96,ablation-analysis,crucial,has,when relying on very noisy and automatically acquired term collections,crucial has when relying on very noisy and automatically acquired term collections,0.5625402331352234
translation,35,96,ablation-analysis,ablation analysis,analysing,overall translation quality,ablation analysis analysing overall translation quality,0.6734016537666321
translation,35,87,results,alignment - based term collections,show,better translation results,alignment - based term collections show better translation results,0.6219260096549988
translation,35,87,results,better translation results,in terms of,bleu,better translation results in terms of bleu,0.6127508282661438
translation,35,88,results,overall terminology translation quality,already,relatively high,overall terminology translation quality already relatively high,0.6038157939910889
translation,35,88,results,relatively high,for,baseline systems,relatively high for baseline systems,0.6508296728134155
translation,35,88,results,relatively high,ranging from,76 %,relatively high ranging from 76 %,0.6588608026504517
translation,35,88,results,76 %,for,en - ru,76 % for en - ru,0.6494890451431274
translation,35,88,results,en - ru,till,88.5 %,en - ru till 88.5 %,0.6122583150863647
translation,35,88,results,88.5 %,for,en - fr,88.5 % for en - fr,0.6517540216445923
translation,35,88,results,results,see that,overall terminology translation quality,results see that overall terminology translation quality,0.5916846990585327
translation,35,97,results,translation quality,drops by,3 bleu points,translation quality drops by 3 bleu points,0.708628237247467
translation,35,97,results,3 bleu points,when using,unfiltered term collections,3 bleu points when using unfiltered term collections,0.6049188375473022
translation,35,97,results,results,show that,translation quality,results show that translation quality,0.4155885875225067
translation,35,99,results,overall translation quality change,is,marginal,overall translation quality change is marginal,0.5554167628288269
translation,35,99,results,marginal,for,translation directions,marginal for translation directions,0.6577063798904419
translation,35,99,results,translation directions,featured,human-created term collections ( en - fr and en - ru ),translation directions featured human-created term collections ( en - fr and en - ru ),0.7224664092063904
translation,35,99,results,results,has,overall translation quality change,results has overall translation quality change,0.5179839134216309
translation,35,105,results,automatic metrics,show,our baseline systems,automatic metrics show our baseline systems,0.5875760912895203
translation,35,105,results,our baseline systems,are,already well equipped,our baseline systems are already well equipped,0.5645083785057068
translation,35,105,results,already well equipped,to translate,development and test sets,already well equipped to translate development and test sets,0.6201322674751282
translation,35,105,results,results,Results of,automatic metrics,results Results of automatic metrics,0.7535143494606018
translation,36,106,baselines,baseline m2m - 124 model,has,"fan et al. , 2020 )","baseline m2m - 124 model has fan et al. , 2020 )",0.516241192817688
translation,36,107,baselines,two improvements,uses,relus,two improvements uses relus,0.6541985869407654
translation,36,107,baselines,finetuning,over,multiccaligned,finetuning over multiccaligned,0.6864780783653259
translation,36,107,baselines,relus,improve,+ 0.8 bleu,relus improve + 0.8 bleu,0.6783392429351807
translation,36,107,baselines,+ 0.8 bleu,over,gelus,+ 0.8 bleu over gelus,0.6548596620559692
translation,36,107,baselines,two improvements,has,finetuning,two improvements has finetuning,0.6023430824279785
translation,36,131,experiments,central / south - east european languages,model pretrained with,deltalm,central / south - east european languages model pretrained with deltalm,0.6339027881622314
translation,36,131,experiments,deltalm,performed,best,deltalm performed best,0.26829424500465393
translation,36,131,experiments,best,followed by,ebay 's model,best followed by ebay 's model,0.6444945931434631
translation,36,131,experiments,ebay 's model,by,margin,ebay 's model by margin,0.6285278797149658
translation,36,131,experiments,margin,of,2.6 bleu,margin of 2.6 bleu,0.5968251824378967
translation,36,152,experiments,top performing pairs ( most progress ),are,into and out of english,top performing pairs ( most progress ) are into and out of english,0.5549174547195435
translation,36,152,experiments,worst performing ones,include,croatian and macedonian,worst performing ones include croatian and macedonian,0.565630316734314
translation,36,169,experiments,best scores,in,competition,best scores in competition,0.5450462102890015
translation,36,169,experiments,competition,for,each language pair,competition for each language pair,0.6878151893615723
translation,36,128,results,deltalm model,from,microsoft team,deltalm model from microsoft team,0.5592133402824402
translation,36,128,results,deltalm model,performs,best,deltalm model performs best,0.6196954846382141
translation,36,128,results,best,by,large margin,best by large margin,0.5930963158607483
translation,36,128,results,large margin,on,small - task1 ( + 2.6 bleu ) and full - task ( + 9.1 bleu ),large margin on small - task1 ( + 2.6 bleu ) and full - task ( + 9.1 bleu ),0.5116230845451355
translation,36,128,results,smaller,for,small - task2 ( 0.6 bleu ),smaller for small - task2 ( 0.6 bleu ),0.585811197757721
translation,36,128,results,results,observe that,deltalm model,results observe that deltalm model,0.6085187792778015
translation,36,132,results,progress,between,m2m -615 baseline and the next best system,progress between m2m -615 baseline and the next best system,0.6383712887763977
translation,36,132,results,m2m -615 baseline and the next best system,of,3.6 bleu,m2m -615 baseline and the next best system of 3.6 bleu,0.507387638092041
translation,36,132,results,results,observe that,progress,results observe that progress,0.6282564401626587
translation,36,154,results,average progress,for,languages,average progress for languages,0.6446918845176697
translation,36,154,results,more progress,in,translat - ing,more progress in translat - ing,0.5741637349128723
translation,36,154,results,translat - ing,from,english,translat - ing from english,0.6593331694602966
translation,36,154,results,english,than,any other language,english than any other language,0.48524999618530273
translation,36,154,results,results,present,average progress,results present average progress,0.7183470129966736
translation,36,159,results,significant progress,on,languages,significant progress on languages,0.5944040417671204
translation,36,159,results,languages,like,tamil ( tam ),languages like tamil ( tam ),0.6387104988098145
translation,36,159,results,languages,like,tagalog ( tgl ),languages like tagalog ( tgl ),0.6455475687980652
translation,36,160,results,progress,of,30 + bleu,progress of 30 + bleu,0.6082292795181274
translation,36,160,results,30 + bleu,for,translation,30 + bleu for translation,0.599527895450592
translation,36,160,results,translation,between,tamil,translation between tamil,0.6974919438362122
translation,36,164,results,improvement,of,21.8,improvement of 21.8,0.58941251039505
translation,36,164,results,21.8,across,all directions,21.8 across all directions,0.6378926038742065
translation,36,164,results,results,see,improvement,results see improvement,0.6593043208122253
translation,36,170,results,significant improvements,for,certain languages,significant improvements for certain languages,0.624474287033081
translation,36,170,results,certain languages,particularly,welsh ( cym ),certain languages particularly welsh ( cym ),0.6899776458740234
translation,36,170,results,certain languages,particularly,irish ( gle ),certain languages particularly irish ( gle ),0.6903212070465088
translation,36,170,results,certain languages,particularly,maltese ( mlt ),certain languages particularly maltese ( mlt ),0.657986044883728
translation,36,170,results,certain languages,particularly,pairings with english,certain languages particularly pairings with english,0.6894535422325134
translation,36,182,results,big improvements,coming from,other indo-european,big improvements coming from other indo-european,0.62486732006073
translation,36,182,results,big improvements,coming from,dravidian,big improvements coming from dravidian,0.7052617073059082
translation,36,182,results,big improvements,coming from,austronesian,big improvements coming from austronesian,0.6550062298774719
translation,36,182,results,other indo-european,influenced by,"irish , welsh","other indo-european influenced by irish , welsh",0.6786826252937317
translation,36,182,results,dravidian,influenced by,"tamil , telugu","dravidian influenced by tamil , telugu",0.6931175589561462
translation,36,182,results,austronesian,influenced by,tagalog,austronesian influenced by tagalog,0.7052142024040222
translation,36,182,results,results,see,big improvements,results see big improvements,0.5913679599761963
translation,36,183,results,very little progress,for,african languages,very little progress for african languages,0.5626261830329895
translation,36,183,results,african languages,represented by,bantu and nilotic subgroups,african languages represented by bantu and nilotic subgroups,0.6823272705078125
translation,36,207,results,mmtafrica,improves over,flo - res 101 benchmarks,mmtafrica improves over flo - res 101 benchmarks,0.7252140641212463
translation,36,207,results,bleu gains,ranging from,+ 0.58,bleu gains ranging from + 0.58,0.600054144859314
translation,36,207,results,bleu gains,ranging from,+ 19.46,bleu gains ranging from + 19.46,0.5922699570655823
translation,36,207,results,+ 0.58,in,swahili to french,+ 0.58 in swahili to french,0.4796527922153473
translation,36,207,results,+ 19.46,in,french to xhosa,+ 19.46 in french to xhosa,0.5136059522628784
translation,36,207,results,results,has,mmtafrica,results has mmtafrica,0.5309880971908569
translation,37,156,ablation-analysis,ablation analysis,When removing,latent variables,ablation analysis When removing latent variables,0.7556514739990234
translation,37,157,ablation-analysis,all latent variables,make,substantial contributions,all latent variables make substantial contributions,0.5937169194221497
translation,37,157,ablation-analysis,substantial contributions,to,performance,substantial contributions to performance,0.552266001701355
translation,37,157,ablation-analysis,ablation analysis,has,all latent variables,ablation analysis has all latent variables,0.5256156921386719
translation,37,158,ablation-analysis,combination effect,of,three latent variables,combination effect of three latent variables,0.5631604790687561
translation,37,158,ablation-analysis,ablation analysis,show,combination effect,ablation analysis show combination effect,0.6338967084884644
translation,37,135,baselines,de-facto nmt model,does not fine -tune,chat translation data,de-facto nmt model does not fine -tune chat translation data,0.726711630821228
translation,37,135,baselines,"transformer ( vaswani et al. , 2017 )",has,de-facto nmt model,"transformer ( vaswani et al. , 2017 ) has de-facto nmt model",0.5795809030532837
translation,37,135,baselines,baselines,has,"transformer ( vaswani et al. , 2017 )","baselines has transformer ( vaswani et al. , 2017 )",0.5352681279182434
translation,37,136,baselines,fine-tuning,on,chat translation data,fine-tuning on chat translation data,0.5282935500144958
translation,37,136,baselines,transformer + ft,has,fine-tuning,transformer + ft has fine-tuning,0.6196434497833252
translation,37,136,baselines,baselines,has,transformer + ft,baselines has transformer + ft,0.6311089992523193
translation,37,138,baselines,stateof - the- art document- level nmt model,based on,transformer,stateof - the- art document- level nmt model based on transformer,0.6271679997444153
translation,37,138,baselines,stateof - the- art document- level nmt model,sharing,first encoder layer,stateof - the- art document- level nmt model sharing first encoder layer,0.577063262462616
translation,37,138,baselines,transformer,sharing,first encoder layer,transformer sharing first encoder layer,0.6886062026023865
translation,37,138,baselines,first encoder layer,to incorporate,bilingual dialogue history,first encoder layer to incorporate bilingual dialogue history,0.6439892053604126
translation,37,138,baselines,"doc-transformer +ft ( ma et al. , 2020 )",has,stateof - the- art document- level nmt model,"doc-transformer +ft ( ma et al. , 2020 ) has stateof - the- art document- level nmt model",0.5526798367500305
translation,37,138,baselines,baselines,has,"doc-transformer +ft ( ma et al. , 2020 )","baselines has doc-transformer +ft ( ma et al. , 2020 )",0.5828866362571716
translation,37,139,baselines,dia-transformer +ft,using,additional rnn - based,dia-transformer +ft using additional rnn - based,0.7277265787124634
translation,37,139,baselines,baselines,has,dia-transformer +ft,baselines has dia-transformer +ft,0.6115756034851074
translation,37,129,hyperparameters,transformer - base,use,512,transformer - base use 512,0.6588214635848999
translation,37,129,hyperparameters,transformer - base,use,2048,transformer - base use 2048,0.6566601395606995
translation,37,129,hyperparameters,transformer - base,use,8 heads,transformer - base use 8 heads,0.6541709899902344
translation,37,129,hyperparameters,512,as,"hidden size ( i.e. , d )","512 as hidden size ( i.e. , d )",0.5622899532318115
translation,37,129,hyperparameters,2048,as,filter size,2048 as filter size,0.5478007197380066
translation,37,129,hyperparameters,8 heads,in,multi-head attention,8 heads in multi-head attention,0.5525535345077515
translation,37,129,hyperparameters,hyperparameters,In,transformer - base,hyperparameters In transformer - base,0.5060569643974304
translation,37,130,hyperparameters,transformer - big,use,1024,transformer - big use 1024,0.6598017811775208
translation,37,130,hyperparameters,transformer - big,use,4096,transformer - big use 4096,0.6518111824989319
translation,37,130,hyperparameters,transformer - big,use,16 heads,transformer - big use 16 heads,0.6631758213043213
translation,37,130,hyperparameters,1024,as,hidden size,1024 as hidden size,0.5615423321723938
translation,37,130,hyperparameters,4096,as,filter size,4096 as filter size,0.562114953994751
translation,37,130,hyperparameters,16 heads,in,multi-head attention,16 heads in multi-head attention,0.5554214715957642
translation,37,130,hyperparameters,hyperparameters,In,transformer - big,hyperparameters In transformer - big,0.5230147838592529
translation,37,131,hyperparameters,transformer models,contain,n e = 6 encoder layers,transformer models contain n e = 6 encoder layers,0.6058838367462158
translation,37,131,hyperparameters,transformer models,contain,n d = 6 decoder layers,transformer models contain n d = 6 decoder layers,0.5829508900642395
translation,37,131,hyperparameters,hyperparameters,has,transformer models,hyperparameters has transformer models,0.5544906258583069
translation,37,163,hyperparameters,dimensionality,of,word embeddings,dimensionality of word embeddings,0.5177170038223267
translation,37,163,hyperparameters,word embeddings,to,100,word embeddings to 100,0.5957393050193787
translation,37,163,hyperparameters,hyperparameters,set,dimensionality,hyperparameters set dimensionality,0.6572375893592834
translation,37,7,model,three latent variational modules,to learn,distributions of bilingual conversational characteristics,three latent variational modules to learn distributions of bilingual conversational characteristics,0.5178379416465759
translation,37,7,model,model,design,three latent variational modules,model design three latent variational modules,0.5707518458366394
translation,37,33,model,model,named,cpcc,model named cpcc,0.7817204594612122
translation,37,33,model,model,to capture,role preference,model to capture role preference,0.7252652645111084
translation,37,33,model,model,to capture,dialogue coherence,model to capture dialogue coherence,0.7239768505096436
translation,37,33,model,model,to capture,translation consistency,model to capture translation consistency,0.686330258846283
translation,37,33,model,translation consistency,with,latent variables,translation consistency with latent variables,0.5415880680084229
translation,37,33,model,latent variables,learned by,cvae,latent variables learned by cvae,0.727123498916626
translation,37,33,model,cvae,for,neural chat translation,cvae for neural chat translation,0.6406939029693604
translation,37,33,model,model,propose,model,model propose model,0.6740307211875916
translation,37,33,model,model,named,cpcc,model named cpcc,0.7817204594612122
translation,37,34,model,cpcc,contains,three specific latent variational modules,cpcc contains three specific latent variational modules,0.5953824520111084
translation,37,34,model,three specific latent variational modules,to learn,distributions,three specific latent variational modules to learn distributions,0.5623683929443359
translation,37,34,model,distributions,of,role preference,distributions of role preference,0.5863990783691406
translation,37,34,model,distributions,of,dialogue coherence,distributions of dialogue coherence,0.57867830991745
translation,37,34,model,distributions,of,translation consistency,distributions of translation consistency,0.5817649960517883
translation,37,34,model,model,has,cpcc,model has cpcc,0.611195981502533
translation,37,35,model,one role-tailored latent variable,sampled from,learned distribution,one role-tailored latent variable sampled from learned distribution,0.6646069884300232
translation,37,35,model,learned distribution,conditioned only on,utterances,learned distribution conditioned only on utterances,0.7075915932655334
translation,37,35,model,utterances,from,this role,utterances from this role,0.6444281935691833
translation,37,35,model,model,firstly use,one role-tailored latent variable,model firstly use one role-tailored latent variable,0.6934509873390198
translation,37,36,model,another latent variable,generated by,distribution,another latent variable generated by distribution,0.6529359817504883
translation,37,36,model,another latent variable,to maintain,coherence,another latent variable to maintain coherence,0.6574091911315918
translation,37,36,model,distribution,conditioned on,source - language dialogue history,distribution conditioned on source - language dialogue history,0.683860182762146
translation,37,36,model,model,utilize,another latent variable,model utilize another latent variable,0.6273733973503113
translation,37,137,model,model,has,context - aware nmt models,model has context - aware nmt models,0.5594804883003235
translation,37,144,results,substantially outperforms,by,large margin,substantially outperforms by large margin,0.6206852197647095
translation,37,144,results,baselines,by,large margin,baselines by large margin,0.6198195815086365
translation,37,144,results,large margin,with,1.70 ? and 1.48 ? bleu scores,large margin with 1.70 ? and 1.48 ? bleu scores,0.6322294473648071
translation,37,144,results,1.70 ? and 1.48 ? bleu scores,on,en?de and de?en,1.70 ? and 1.48 ? bleu scores on en?de and de?en,0.579203188419342
translation,37,144,results,base setting,has,cpcc,base setting has cpcc,0.579071044921875
translation,37,144,results,cpcc,has,substantially outperforms,cpcc has substantially outperforms,0.613275408744812
translation,37,144,results,substantially outperforms,has,baselines,substantially outperforms has baselines,0.6038005948066711
translation,37,145,results,our cpcc,achieves,significant improvement,our cpcc achieves significant improvement,0.685575008392334
translation,37,145,results,significant improvement,of,1.3 points,significant improvement of 1.3 points,0.491127610206604
translation,37,145,results,1.3 points,in,both language pairs,1.3 points in both language pairs,0.508723795413971
translation,37,145,results,ter,has,our cpcc,ter has our cpcc,0.6857856512069702
translation,37,145,results,results,On,ter,results On ter,0.5739683508872986
translation,37,146,results,our cpcc,consistently boosts,performance,our cpcc consistently boosts performance,0.8067801594734192
translation,37,146,results,performance,in,both direc-tions,performance in both direc-tions,0.5679042935371399
translation,37,146,results,big setting,has,our cpcc,big setting has our cpcc,0.6167240738868713
translation,37,146,results,results,Under,big setting,results Under big setting,0.5601499676704407
translation,37,147,results,significantly surpasses,about,1.39? 1.59 ? bleu scores,significantly surpasses about 1.39? 1.59 ? bleu scores,0.5799477100372314
translation,37,147,results,significantly surpasses,about,0.6?0.9 ? ter scores,significantly surpasses about 0.6?0.9 ? ter scores,0.6415245532989502
translation,37,147,results,both language directions,under,base and big settings,both language directions under base and big settings,0.6763319373130798
translation,37,147,results,strong context- aware nmt systems,has,our cpcc,strong context- aware nmt systems has our cpcc,0.6154781579971313
translation,37,147,results,our cpcc,has,significantly surpasses,our cpcc has significantly surpasses,0.5841124653816223
translation,37,147,results,results,Compared against,strong context- aware nmt systems,results Compared against strong context- aware nmt systems,0.7288604378700256
translation,37,150,results,our cpcc,presents,remarkable bleu improvements,our cpcc presents remarkable bleu improvements,0.5972840785980225
translation,37,150,results,remarkable bleu improvements,over,  transformer + ft  ,remarkable bleu improvements over   transformer + ft  ,0.7051686644554138
translation,37,150,results,remarkable bleu improvements,by,large margin,remarkable bleu improvements by large margin,0.5510874390602112
translation,37,150,results,remarkable bleu improvements,by,2.03 ? and 0.83 ? bleu gains,remarkable bleu improvements by 2.03 ? and 0.83 ? bleu gains,0.5519824624061584
translation,37,150,results,  transformer + ft  ,by,large margin,  transformer + ft   by large margin,0.6089034676551819
translation,37,150,results,large margin,in,two directions,large margin in two directions,0.5644595623016357
translation,37,150,results,2.33 ? and 0.91 ? bleu gains,under,base setting,2.33 ? and 0.91 ? bleu gains under base setting,0.6326626539230347
translation,37,150,results,2.03 ? and 0.83 ? bleu gains,in,both directions,2.03 ? and 0.83 ? bleu gains in both directions,0.5446448922157288
translation,37,150,results,2.03 ? and 0.83 ? bleu gains,under,big setting,2.03 ? and 0.83 ? bleu gains under big setting,0.6308806538581848
translation,37,150,results,both directions,under,big setting,both directions under big setting,0.6487137079238892
translation,37,150,results,results,has,our cpcc,results has our cpcc,0.5866038203239441
translation,37,152,results,both language directions,under,base and big settings,both language directions under base and big settings,0.6763319373130798
translation,37,159,results,explicitly modeling,has,bilingual conversational characteristics,explicitly modeling has bilingual conversational characteristics,0.48182740807533264
translation,37,159,results,bilingual conversational characteristics,has,significantly outperforms,bilingual conversational characteristics has significantly outperforms,0.593491792678833
translation,37,159,results,significantly outperforms,has,implicit modeling,significantly outperforms has implicit modeling,0.5977449417114258
translation,37,172,results,results,shows,our cpcc,results shows our cpcc,0.7138299942016602
translation,38,44,experimental-setup,training,used,opennmt - py framework,training used opennmt - py framework,0.5741868019104004
translation,38,44,experimental-setup,opennmt - py framework,together with,sentencepiece tokenizer,opennmt - py framework together with sentencepiece tokenizer,0.5908083915710449
translation,38,44,experimental-setup,) unigram model,of size,32 k,) unigram model of size 32 k,0.7252518534660339
translation,38,44,experimental-setup,sentencepiece tokenizer,has,) unigram model,sentencepiece tokenizer has ) unigram model,0.5804575681686401
translation,38,44,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,38,45,experimental-setup,models,on,gpu,models on gpu,0.5813058614730835
translation,38,45,experimental-setup,gpu,for,210k steps,gpu for 210k steps,0.610713541507721
translation,38,45,experimental-setup,experimental setup,trained,models,experimental setup trained models,0.6523525714874268
translation,38,46,experimental-setup,experimental setup,used,half -precision,experimental setup used half -precision,0.5778513550758362
translation,38,47,experimental-setup,optimization,used,"adam ( kingma and ba , 2014 )","optimization used adam ( kingma and ba , 2014 )",0.573039710521698
translation,38,47,experimental-setup,"adam ( kingma and ba , 2014 )",with,linear warmup,"adam ( kingma and ba , 2014 ) with linear warmup",0.6324852108955383
translation,38,47,experimental-setup,linear warmup,for,learning rate,linear warmup for learning rate,0.61821049451828
translation,38,47,experimental-setup,linear warmup,for,inverse square root decay,linear warmup for inverse square root decay,0.6399133205413818
translation,38,47,experimental-setup,learning rate,for,15 k steps,learning rate for 15 k steps,0.6239137053489685
translation,38,47,experimental-setup,15 k steps,up to,0.0005,15 k steps up to 0.0005,0.6519817113876343
translation,38,47,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,38,48,experimental-setup,models,were,randomly initialized,models were randomly initialized,0.6806023120880127
translation,39,27,ablation-analysis,well,on,general - domain data,well on general - domain data,0.5214187502861023
translation,39,27,ablation-analysis,well,adjust,model,well adjust model,0.7366830110549927
translation,39,27,ablation-analysis,model,on,in- domain data,model on in- domain data,0.5670864582061768
translation,39,27,ablation-analysis,in- domain data,with,knowledge distillation,in- domain data with knowledge distillation,0.6459905505180359
translation,39,27,ablation-analysis,knowledge distillation,where,original whole model,knowledge distillation where original whole model,0.5916674733161926
translation,39,27,ablation-analysis,knowledge distillation,where,pruned model,knowledge distillation where pruned model,0.6209372282028198
translation,39,27,ablation-analysis,original whole model,used as,teacher,original whole model used as teacher,0.7132461071014404
translation,39,27,ablation-analysis,pruned model,as,student,pruned model as student,0.5812788605690002
translation,39,27,ablation-analysis,ablation analysis,adjust,model,ablation analysis adjust model,0.6546985507011414
translation,39,152,baselines,baselines,has,multiple-output layer learning ( mll ),baselines has multiple-output layer learning ( mll ),0.5627908110618591
translation,39,119,experiments,44 k size,of,chinese dictionary,44 k size of chinese dictionary,0.5579633712768555
translation,39,119,experiments,33 k size,of,english dictionary,33 k size of english dictionary,0.5987242460250854
translation,39,119,experiments,english dictionary,built based on,general - domain data,english dictionary built based on general - domain data,0.6958702802658081
translation,39,119,experiments,zh-en translation task,has,44 k size,zh-en translation task has 44 k size,0.5680515170097351
translation,39,117,hyperparameters,integrating operations,of,"32k , 32k , and 30 k","integrating operations of 32k , 32k , and 30 k",0.6132134795188904
translation,39,117,hyperparameters,"32k , 32k , and 30 k",performed to learn,bpe,"32k , 32k , and 30 k performed to learn bpe",0.6272286176681519
translation,39,117,hyperparameters,bpe,on,general- domain data,bpe on general- domain data,0.5129220485687256
translation,39,117,hyperparameters,bpe,applied to,general - domain and in-domain data,bpe applied to general - domain and in-domain data,0.725225031375885
translation,39,117,hyperparameters,"sennrich et al. , 2016 )",on,general- domain data,"sennrich et al. , 2016 ) on general- domain data",0.5354510545730591
translation,39,117,hyperparameters,bpe,has,"sennrich et al. , 2016 )","bpe has sennrich et al. , 2016 )",0.5594024062156677
translation,39,117,hyperparameters,hyperparameters,has,integrating operations,hyperparameters has integrating operations,0.5761168003082275
translation,39,120,hyperparameters,32 k size,of,dictionaries,32 k size of dictionaries,0.5880147218704224
translation,39,120,hyperparameters,dictionaries,for,source and target languages,dictionaries for source and target languages,0.5531042218208313
translation,39,120,hyperparameters,source and target languages,built on,corresponding general - domain data,source and target languages built on corresponding general - domain data,0.6622403264045715
translation,39,120,hyperparameters,en-fr and en- de tasks,has,32 k size,en-fr and en- de tasks has 32 k size,0.586890459060669
translation,39,120,hyperparameters,hyperparameters,For,en-fr and en- de tasks,hyperparameters For en-fr and en- de tasks,0.5834453701972961
translation,39,151,hyperparameters,adapter size,set to,64,adapter size set to 64,0.7191089987754822
translation,39,151,hyperparameters,hyperparameters,has,adapter size,hyperparameters has adapter size,0.5228874087333679
translation,39,162,hyperparameters,learning rate,during,fine- tuning process,learning rate during fine- tuning process,0.6538194417953491
translation,39,162,hyperparameters,learning rate,to,7.5 ? 10 ?5,learning rate to 7.5 ? 10 ?5,0.5824729800224304
translation,39,162,hyperparameters,fine- tuning process,to,7.5 ? 10 ?5,fine- tuning process to 7.5 ? 10 ?5,0.5875194072723389
translation,39,162,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,39,166,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,39,166,hyperparameters,beam size,of,4,beam size of 4,0.6962505578994751
translation,39,166,hyperparameters,4,during,decoding process,4 during decoding process,0.5981552004814148
translation,39,166,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
translation,39,202,hyperparameters,ewc and mll method,vary ?,0.25 to 2.5,ewc and mll method vary ? 0.25 to 2.5,0.7209612131118774
translation,39,202,hyperparameters,hyperparameters,For,ewc and mll method,hyperparameters For ewc and mll method,0.5995401740074158
translation,39,203,hyperparameters,pruning proportion,from,5 % to 30 %,pruning proportion from 5 % to 30 %,0.5727197527885437
translation,39,203,hyperparameters,pruning proportion,from,10 % to 50 %,pruning proportion from 10 % to 50 %,0.5716336965560913
translation,39,203,hyperparameters,pruning proportion,from,10 % to 50 %,pruning proportion from 10 % to 50 %,0.5716336965560913
translation,39,203,hyperparameters,5 % to 30 %,for,our neuron- pruning method,5 % to 30 % for our neuron- pruning method,0.6332329511642456
translation,39,203,hyperparameters,10 % to 50 %,for,our weight - pruning method,10 % to 50 % for our weight - pruning method,0.6357375979423523
translation,39,203,hyperparameters,hyperparameters,vary,pruning proportion,hyperparameters vary pruning proportion,0.7209262251853943
translation,39,203,hyperparameters,hyperparameters,from,10 % to 50 %,hyperparameters from 10 % to 50 %,0.5598834753036499
translation,39,9,model,model,to,original size,model to original size,0.549666702747345
translation,39,9,model,model,fine - tune,added parameters,model fine - tune added parameters,0.6985934376716614
translation,39,9,model,added parameters,for,in-domain translation,added parameters for in-domain translation,0.5984472632408142
translation,39,9,model,model,expand,model,model expand model,0.6578519940376282
translation,39,9,model,model,fine - tune,added parameters,model fine - tune added parameters,0.6985934376716614
translation,39,61,model,our model,under,framework,our model under framework,0.6573410630226135
translation,39,61,model,our model,with,unpruned model,our model with unpruned model,0.6350268125534058
translation,39,61,model,our model,with,pruned model,our model with pruned model,0.6610230803489685
translation,39,61,model,framework,of,"knowledge distillation ( hinton et al. , 2015 )","framework of knowledge distillation ( hinton et al. , 2015 )",0.5172052979469299
translation,39,61,model,"knowledge distillation ( hinton et al. , 2015 )",on,in-domain,"knowledge distillation ( hinton et al. , 2015 ) on in-domain",0.5272923707962036
translation,39,61,model,in-domain,with,pruned model,in-domain with pruned model,0.6669619679450989
translation,39,61,model,unpruned model,as,teacher,unpruned model as teacher,0.599169909954071
translation,39,61,model,unpruned model,as,student,unpruned model as student,0.5805261135101318
translation,39,61,model,pruned model,as,student,pruned model as student,0.5812788605690002
translation,39,61,model,model,adjust,our model,model adjust our model,0.6575074195861816
translation,39,61,model,model,under,framework,model under framework,0.6366250514984131
translation,39,67,model,neuron pruning,prune,unimportant neurons and relevant parameters,neuron pruning prune unimportant neurons and relevant parameters,0.7553943991661072
translation,39,67,model,model,first,neuron pruning,model first neuron pruning,0.7737264037132263
translation,39,163,model,layer - normalization layers,in,encoder and decoder,layer - normalization layers in encoder and decoder,0.4811159670352936
translation,39,163,model,layer - normalization layers,can make,training,layer - normalization layers can make training,0.6286560893058777
translation,39,163,model,training,has,faster and more stable,training has faster and more stable,0.579295814037323
translation,39,163,model,model,n't prune,layer - normalization layers,model n't prune layer - normalization layers,0.6922911405563354
translation,39,170,results,all the datasets,has,weight pruning method,all the datasets has weight pruning method,0.5416771173477173
translation,39,170,results,weight pruning method,has,outperforms,weight pruning method has outperforms,0.6279707551002502
translation,39,170,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,39,170,results,results,In,all the datasets,results In all the datasets,0.504176139831543
translation,39,172,results,contrast capacity - fixed methods,ca n't handle,large domain divergence,contrast capacity - fixed methods ca n't handle large domain divergence,0.6812732815742493
translation,39,172,results,contrast capacity - fixed methods,still suffer,catastrophic forgetting,contrast capacity - fixed methods still suffer catastrophic forgetting,0.6976832747459412
translation,39,172,results,results,has,contrast capacity - fixed methods,results has contrast capacity - fixed methods,0.4892783761024475
translation,39,178,results,our method,superior to,all tasks,our method superior to all tasks,0.7066643834114075
translation,39,178,results,our method,in,all tasks,our method in all tasks,0.49646008014678955
translation,39,178,results,our method,on,more different domains,our method on more different domains,0.5341711044311523
translation,39,178,results,results,has,our method,results has our method,0.5589964985847473
translation,39,184,results,both of our methods,immune to,large domain divergence,both of our methods immune to large domain divergence,0.6377138495445251
translation,39,184,results,results,has,both of our methods,results has both of our methods,0.508056640625
translation,39,185,results,knowledge distillation,bring,modest improvements,knowledge distillation bring modest improvements,0.6529735922813416
translation,39,185,results,modest improvements,on,general domain,modest improvements on general domain,0.5317243337631226
translation,39,185,results,results,has,knowledge distillation,results has knowledge distillation,0.5724591016769409
translation,39,186,results,weight pruning method,is,more effective,weight pruning method is more effective,0.5338679552078247
translation,39,186,results,neuron pruning method,has,weight pruning method,neuron pruning method has weight pruning method,0.5556780695915222
translation,39,186,results,results,Compared with,neuron pruning method,results Compared with neuron pruning method,0.673785388469696
translation,39,196,results,our method,get,significant improvements,our method get significant improvements,0.5400559902191162
translation,39,196,results,significant improvements,on,all the domains,significant improvements on all the domains,0.50677490234375
translation,39,196,results,results,shows that,our method,results shows that our method,0.6864691376686096
translation,39,205,results,ewc,at,all the operating points,ewc at all the operating points,0.5650678873062134
translation,39,205,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,39,205,results,outperforms,has,ewc,outperforms has ewc,0.6053317785263062
translation,39,205,results,results,shows that,our method,results shows that our method,0.6864691376686096
translation,39,206,results,our neuron-pruning method,achieve,comparable results,our neuron-pruning method achieve comparable results,0.5790715217590332
translation,39,206,results,comparable results,as,mll,comparable results as mll,0.6107890009880066
translation,39,206,results,be - 1,has,our neuron-pruning method,be - 1 has our neuron-pruning method,0.6151295900344849
translation,39,206,results,results,has,be - 1,results has be - 1,0.6323134303092957
translation,39,206,results,results,has,our neuron-pruning method,results has our neuron-pruning method,0.5146262645721436
translation,40,74,experimental-setup,batch size,as,1024 tokens,batch size as 1024 tokens,0.523074209690094
translation,40,75,experimental-setup,training stage,adopt,"adam optimizer ( ? 1 = 0.9 , ? 2 = 0.98 )","training stage adopt adam optimizer ( ? 1 = 0.9 , ? 2 = 0.98 )",0.6224147081375122
translation,40,75,experimental-setup,"adam optimizer ( ? 1 = 0.9 , ? 2 = 0.98 )",using,inverse sqrt learning rate schedule,"adam optimizer ( ? 1 = 0.9 , ? 2 = 0.98 ) using inverse sqrt learning rate schedule",0.6461917161941528
translation,40,75,experimental-setup,inverse sqrt learning rate schedule,with,learning rate,inverse sqrt learning rate schedule with learning rate,0.6491345763206482
translation,40,75,experimental-setup,inverse sqrt learning rate schedule,with,4000 warming - up steps,inverse sqrt learning rate schedule with 4000 warming - up steps,0.6561887264251709
translation,40,75,experimental-setup,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,40,75,experimental-setup,learning rate,of,4000 warming - up steps,learning rate of 4000 warming - up steps,0.5649771690368652
translation,40,75,experimental-setup,experimental setup,In,training stage,experimental setup In training stage,0.5299758315086365
translation,40,76,experimental-setup,experimental setup,set,number of sampled start positions s = 8,experimental setup set number of sampled start positions s = 8,0.6381188631057739
translation,40,80,experimental-setup,dropout,set as,0.3,dropout set as 0.3,0.5599774718284607
translation,40,80,experimental-setup,weight decay,as,0.0001,weight decay as 0.0001,0.5233073234558105
translation,40,80,experimental-setup,0.0001,to prevent,overfitting,0.0001 to prevent overfitting,0.5865966081619263
translation,40,80,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,40,80,experimental-setup,experimental setup,has,weight decay,experimental setup has weight decay,0.4727212190628052
translation,40,82,experimental-setup,gradient,for,16 iterations,gradient for 16 iterations,0.5693570971488953
translation,40,82,experimental-setup,16 iterations,to simulate,128 - gpu environment,16 iterations to simulate 128 - gpu environment,0.6460747718811035
translation,40,82,experimental-setup,experimental setup,accumulate,gradient,experimental setup accumulate gradient,0.5255685448646545
translation,40,79,experiments,iwslt14 de? en translation task,use,transformer_small setting,iwslt14 de? en translation task use transformer_small setting,0.6248131394386292
translation,40,79,experiments,transformer_small setting,with,embedding size,transformer_small setting with embedding size,0.6784994006156921
translation,40,79,experiments,transformer_small setting,with,ffn size,transformer_small setting with ffn size,0.6702278256416321
translation,40,79,experiments,embedding size,as,512,embedding size as 512,0.5785781741142273
translation,40,79,experiments,ffn size,as,1024,ffn size as 1024,0.6017284989356995
translation,40,81,experiments,wmt14 en? de translation task,use,transformer_big setting,wmt14 en? de translation task use transformer_big setting,0.6198819279670715
translation,40,81,experiments,transformer_big setting,with,embedding size,transformer_big setting with embedding size,0.6580837965011597
translation,40,81,experiments,transformer_big setting,with,ffn size,transformer_big setting with ffn size,0.6506293416023254
translation,40,81,experiments,embedding size,as,1024,embedding size as 1024,0.5829789042472839
translation,40,81,experiments,ffn size,as,4096,ffn size as 4096,0.5785781145095825
translation,40,5,model,novel method,breaks up,limitation of these decoding orders,novel method breaks up limitation of these decoding orders,0.7197489738464355
translation,40,5,model,limitation of these decoding orders,called,smart - start decoding,limitation of these decoding orders called smart - start decoding,0.6748493313789368
translation,40,5,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,40,85,results,our soft method,significantly gets,improvement,our soft method significantly gets improvement,0.7047353386878967
translation,40,85,results,improvement,of,+0.98 / +1.71 bleu points,improvement of +0.98 / +1.71 bleu points,0.5509637594223022
translation,40,85,results,+0.98 / +1.71 bleu points,than,strong transformer model,+0.98 / +1.71 bleu points than strong transformer model,0.5600124597549438
translation,40,85,results,iwslt14 de?en,has,our soft method,iwslt14 de?en has our soft method,0.6406664848327637
translation,40,85,results,results,of,iwslt14 de?en,results of iwslt14 de?en,0.6394041776657104
translation,40,86,results,results,For,wmt14 en?de task,results For wmt14 en?de task,0.5229457020759583
translation,40,87,results,sb - nmt model,gets,bleu points,sb - nmt model gets bleu points,0.621863842010498
translation,40,87,results,bleu points,of,29.21,bleu points of 29.21,0.5590999722480774
translation,40,87,results,29.21,decodes,l2r and r2l,29.21 decodes l2r and r2l,0.7654288411140442
translation,40,87,results,results,has,sb - nmt model,results has sb - nmt model,0.5191794633865356
translation,40,88,results,our method,achieves,improvement,our method achieves improvement,0.6563941836357117
translation,40,88,results,improvement,of,+ 0.56 bleu points,improvement of + 0.56 bleu points,0.5249953866004944
translation,40,88,results,+ 0.56 bleu points,over,transformer baseline,+ 0.56 bleu points over transformer baseline,0.6529115438461304
translation,40,88,results,results,has,our method,results has our method,0.5589964985847473
translation,40,95,results,gradually improved performance,by increasing,value of s.,gradually improved performance by increasing value of s.,0.7575700283050537
translation,40,95,results,value of s.,has,soft smart - start method,value of s. has soft smart - start method,0.5341441631317139
translation,40,95,results,soft smart - start method,has,outperforms,soft smart - start method has outperforms,0.6201077699661255
translation,40,95,results,outperforms,has,hard method,outperforms has hard method,0.6072396039962769
translation,40,95,results,results,shows,our hard and soft smart - start methods,results shows our hard and soft smart - start methods,0.6243608593940735
translation,40,96,results,soft method,achieves,higher bleu score,soft method achieves higher bleu score,0.6207670569419861
translation,40,96,results,higher bleu score,when,number of sampled start positions,higher bleu score when number of sampled start positions,0.5730281472206116
translation,40,96,results,number of sampled start positions,equals,7,number of sampled start positions equals 7,0.6594105362892151
translation,40,96,results,results,has,soft method,results has soft method,0.584346354007721
translation,41,4,baselines,cuni -,has,marian - baselines,cuni - has marian - baselines,0.6569437980651855
translation,41,11,baselines,baselines,has,cuni - marian - baselines,baselines has cuni - marian - baselines,0.6488161087036133
translation,41,13,results,cuni - doctransformer,is,third best english ? czech system,cuni - doctransformer is third best english ? czech system,0.6140244603157043
translation,41,18,results,results,has,doctransformer improvements 750 subwords,results has doctransformer improvements 750 subwords,0.5850337147712708
translation,42,81,hyperparameters,baseline text models,trained using,fairseq,baseline text models trained using fairseq,0.7120000720024109
translation,42,81,hyperparameters,hyperparameters,has,baseline text models,hyperparameters has baseline text models,0.4868411719799042
translation,42,82,hyperparameters,7 language pairs,from,mttt ted dataset,7 language pairs from mttt ted dataset,0.5517736673355103
translation,42,82,hyperparameters,7 language pairs,follow,recommended fairseq architecture and optimization param eters,7 language pairs follow recommended fairseq architecture and optimization param eters,0.5337181687355042
translation,42,82,hyperparameters,recommended fairseq architecture and optimization param eters,for,iwslt ' 14 deen,recommended fairseq architecture and optimization param eters for iwslt ' 14 deen,0.6945628523826599
translation,42,82,hyperparameters,hyperparameters,For,7 language pairs,hyperparameters For 7 language pairs,0.5338835120201111
translation,42,82,hyperparameters,hyperparameters,follow,recommended fairseq architecture and optimization param eters,hyperparameters follow recommended fairseq architecture and optimization param eters,0.5923787951469421
translation,42,91,hyperparameters,former base models,with,dropout 0.1,former base models with dropout 0.1,0.5998706221580505
translation,42,91,hyperparameters,former base models,with,learning rate,former base models with learning rate,0.5912013649940491
translation,42,91,hyperparameters,learning rate,has,4e4,learning rate has 4e4,0.5813086628913879
translation,42,92,hyperparameters,batch size,of,40 k tokens,batch size of 40 k tokens,0.5787211060523987
translation,42,92,hyperparameters,fails to improve,for,ten epochs,fails to improve for ten epochs,0.6027874946594238
translation,42,92,hyperparameters,heldout validation,has,fails to improve,heldout validation has fails to improve,0.6149692535400391
translation,42,92,hyperparameters,hyperparameters,train until,heldout validation,hyperparameters train until heldout validation,0.6571078300476074
translation,42,93,hyperparameters,german,use,shared unigram subword vocabulary,german use shared unigram subword vocabulary,0.5857622027397156
translation,42,93,hyperparameters,shared unigram subword vocabulary,of size,10k,shared unigram subword vocabulary of size 10k,0.6981490850448608
translation,42,93,hyperparameters,hyperparameters,For,german,hyperparameters For german,0.5819847583770752
translation,42,6,model,visual text representations,dispense with,finite set of text embeddings,visual text representations dispense with finite set of text embeddings,0.660031795501709
translation,42,6,model,finite set of text embeddings,in favor of,continuous vocabularies,finite set of text embeddings in favor of continuous vocabularies,0.654119610786438
translation,42,6,model,continuous vocabularies,process,visually rendered text,continuous vocabularies process visually rendered text,0.7284488677978516
translation,42,6,model,visually rendered text,with,sliding win dows,visually rendered text with sliding win dows,0.686481237411499
translation,42,43,model,convolutional block,comprises,three pieces,convolutional block comprises three pieces,0.6861710548400879
translation,42,43,model,2d convolution,followed by,2d batch normalization,2d convolution followed by 2d batch normalization,0.6211106181144714
translation,42,43,model,2d convolution,followed by,relu layer,2d convolution followed by relu layer,0.6870709657669067
translation,42,43,model,three pieces,has,2d convolution,three pieces has 2d convolution,0.583798348903656
translation,42,43,model,model,has,convolutional block,model has convolutional block,0.5663192272186279
translation,42,89,results,batch size and subword vo cabularies,for,each language pair,batch size and subword vo cabularies for each language pair,0.603729248046875
translation,42,89,results,sig nificant ( 1 - 15 bleu ) improvements,with,larger batch of 16 k tokens,sig nificant ( 1 - 15 bleu ) improvements with larger batch of 16 k tokens,0.6573929786682129
translation,42,89,results,larger batch of 16 k tokens,over,suggested 4096,larger batch of 16 k tokens over suggested 4096,0.7050264477729797
translation,42,89,results,results,jointly tuned,batch size and subword vo cabularies,results jointly tuned batch size and subword vo cabularies,0.734142005443573
translation,42,89,results,results,found,sig nificant ( 1 - 15 bleu ) improvements,results found sig nificant ( 1 - 15 bleu ) improvements,0.6323702335357666
translation,42,90,results,our baselines,improve,?2 bleu,our baselines improve ?2 bleu,0.643665075302124
translation,42,90,results,?2 bleu,over,previous work,?2 bleu over previous work,0.6331440806388855
translation,42,90,results,?2 bleu,on,mttt dataset,?2 bleu on mttt dataset,0.5521849989891052
translation,42,90,results,results,has,our baselines,results has our baselines,0.5782451033592224
translation,42,109,results,smaller mttt dataset,nearly recover,best results,smaller mttt dataset nearly recover best results,0.7559404373168945
translation,42,109,results,best results,from,most optimal bpe segmentation,best results from most optimal bpe segmentation,0.5432401895523071
translation,42,109,results,most optimal bpe segmentation,without,explicit input segmentation,most optimal bpe segmentation without explicit input segmentation,0.6990293264389038
translation,42,109,results,visual representations,with,sliding window,visual representations with sliding window,0.6397870182991028
translation,42,109,results,results,On,smaller mttt dataset,results On smaller mttt dataset,0.5347989797592163
translation,42,110,results,text baselines,on,mttt,text baselines on mttt,0.5390042662620544
translation,42,118,results,larger data scenarios,see,our best vi sual text models,larger data scenarios see our best vi sual text models,0.6038243770599365
translation,42,118,results,our best vi sual text models,approach,deen ),our best vi sual text models approach deen ),0.6334273815155029
translation,42,118,results,our best vi sual text models,approach,exceed ( zh en ),our best vi sual text models approach exceed ( zh en ),0.571810781955719
translation,42,118,results,exceed ( zh en ),has,textbased baselines,exceed ( zh en ) has textbased baselines,0.6164003014564514
translation,42,118,results,results,On,larger data scenarios,results On larger data scenarios,0.5750831961631775
translation,42,120,results,more data,has,c = 0,more data has c = 0,0.5756362080574036
translation,42,120,results,c = 0,has,slightly outperforms,c = 0 has slightly outperforms,0.6268045902252197
translation,42,120,results,slightly outperforms,has,c = 1 model,slightly outperforms has c = 1 model,0.6081600189208984
translation,42,120,results,results,With,more data,results With more data,0.626509428024292
translation,42,157,results,visual text models,for,both language pairs,visual text models for both language pairs,0.6122678518295288
translation,42,157,results,text models,negatively affected by,induced l33tspeak,text models negatively affected by induced l33tspeak,0.673142671585083
translation,42,157,results,visual text models,for,both language pairs,visual text models for both language pairs,0.6122678518295288
translation,42,157,results,both language pairs,has,significantly outperform,both language pairs has significantly outperform,0.6130121946334839
translation,42,157,results,significantly outperform,has,text models,significantly outperform has text models,0.6031021475791931
translation,42,158,results,up to 30 %,of,tokens,up to 30 % of tokens,0.647976279258728
translation,42,158,results,tokens,containing,l33tspeak mappings,tokens containing l33tspeak mappings,0.6837165951728821
translation,42,158,results,visual text models,for,german and french,visual text models for german and french,0.5841903686523438
translation,42,158,results,visual text models,perform,>5 bleu,visual text models perform >5 bleu,0.5904850959777832
translation,42,158,results,better,than,text models,better than text models,0.5819554924964905
translation,42,158,results,up to 30 %,has,visual text models,up to 30 % has visual text models,0.5925868153572083
translation,42,158,results,tokens,has,visual text models,tokens has visual text models,0.5999967455863953
translation,42,158,results,l33tspeak mappings,has,visual text models,l33tspeak mappings has visual text models,0.5769549012184143
translation,42,158,results,>5 bleu,has,better,>5 bleu has better,0.6021152138710022
translation,42,158,results,results,With,up to 30 %,results With up to 30 %,0.6241170763969421
translation,42,170,results,improvements,of,up to 24 bleu,improvements of up to 24 bleu,0.5637864470481873
translation,42,170,results,up to 24 bleu,on,german- english,up to 24 bleu on german- english,0.5320743918418884
translation,42,170,results,up to 24 bleu,mean that,our visual text model,up to 24 bleu mean that our visual text model,0.6117116808891296
translation,42,170,results,german- english,mean that,our visual text model,german- english mean that our visual text model,0.6255331039428711
translation,42,170,results,our visual text model,achieves,25.9 bleu,our visual text model achieves 25.9 bleu,0.64397794008255
translation,42,170,results,results,has,improvements,results has improvements,0.615561842918396
translation,42,183,results,characterlevel models,are,more robust,characterlevel models are more robust,0.5539463758468628
translation,42,183,results,characterlevel models,in some cases,more robust,characterlevel models in some cases more robust,0.6181670427322388
translation,42,183,results,more robust,than,subwords,more robust than subwords,0.6272163987159729
translation,42,183,results,others ( jaen ),where,visual text model,others ( jaen ) where visual text model,0.6263116002082825
translation,42,183,results,visual text model,has,does best,visual text model has does best,0.6063249111175537
translation,42,183,results,results,see that,characterlevel models,results see that characterlevel models,0.5877687931060791
translation,43,141,results,pretrained methods,have,higher accuracy,pretrained methods have higher accuracy,0.5450356006622314
translation,43,141,results,higher accuracy,than,string - based methods,higher accuracy than string - based methods,0.6059413552284241
translation,43,141,results,results,show,pretrained methods,results show pretrained methods,0.5472914576530457
translation,43,146,results,highest accuracy,achieved by,chrf,highest accuracy achieved by chrf,0.6664921641349792
translation,43,146,results,string - based metrics,has,highest accuracy,string - based metrics has highest accuracy,0.5595718026161194
translation,43,146,results,results,In terms of,string - based metrics,results In terms of string - based metrics,0.6890326142311096
translation,43,149,results,pairs,of,most likely equalquality systems,pairs of most likely equalquality systems,0.6064987182617188
translation,43,149,results,100 % accuracy,for,set of strongly different systems,100 % accuracy for set of strongly different systems,0.6104563474655151
translation,43,149,results,set of strongly different systems,with,alpha level,set of strongly different systems with alpha level,0.6418506503105164
translation,43,149,results,alpha level,of,0.001,alpha level of 0.001,0.5370657444000244
translation,43,149,results,most likely equalquality systems,has,increases,most likely equalquality systems has increases,0.6217163801193237
translation,43,149,results,increases,has,accuracy,increases has accuracy,0.5783241391181946
translation,43,149,results,results,removing,pairs,results removing pairs,0.6382623314857483
translation,43,168,results,gains,for,prism,gains for prism,0.7428240180015564
translation,43,168,results,gains,for,  from english   directions,gains for   from english   directions,0.6551439762115479
translation,43,168,results,gains,for,  from english   directions,gains for   from english   directions,0.6551439762115479
translation,43,168,results,prism,for,  from english   directions,prism for   from english   directions,0.6566464304924011
translation,43,168,results,results,observe,gains,results observe gains,0.5510724186897278
translation,43,168,results,results,for,prism,results for prism,0.47884175181388855
translation,43,176,results,drop,in,accuracy,drop in accuracy,0.5577701330184937
translation,43,176,results,accuracy,for,string - based metrics,accuracy for string - based metrics,0.595695972442627
translation,43,176,results,string - based metrics,for,discussion domain,string - based metrics for discussion domain,0.5863260626792908
translation,43,176,results,results,see,drop,results see drop,0.5868945717811584
translation,43,192,results,paired bootstrap resampling,on,automatic metrics,paired bootstrap resampling on automatic metrics,0.5479487776756287
translation,43,192,results,paired bootstrap resampling,with,alpha level 0.05,paired bootstrap resampling with alpha level 0.05,0.6264672875404358
translation,43,192,results,increases,by,around 10 %,increases by around 10 %,0.6696481108665466
translation,43,192,results,around 10 %,for,all metrics,around 10 % for all metrics,0.6074410676956177
translation,43,192,results,paired bootstrap resampling,has,accuracy,paired bootstrap resampling has accuracy,0.5769898295402527
translation,43,192,results,automatic metrics,has,accuracy,automatic metrics has accuracy,0.5321640968322754
translation,43,192,results,alpha level 0.05,has,accuracy,alpha level 0.05 has accuracy,0.5399213433265686
translation,43,192,results,accuracy,has,increases,accuracy has increases,0.6037072539329529
translation,43,192,results,results,apply,paired bootstrap resampling,results apply paired bootstrap resampling,0.5785055756568909
translation,43,195,results,reliability,in,automatic metric decisions,reliability in automatic metric decisions,0.5161198973655701
translation,43,195,results,statistical significance tests,has,largely increases,statistical significance tests has largely increases,0.5582348108291626
translation,43,195,results,largely increases,has,reliability,largely increases has reliability,0.5942396521568298
translation,43,195,results,results,corroborate,statistical significance tests,results corroborate statistical significance tests,0.6097472906112671
translation,43,195,results,results,using,statistical significance tests,results using statistical significance tests,0.5740064978599548
translation,43,213,results,incremental systems,has,bleu,incremental systems has bleu,0.6109005808830261
translation,43,213,results,bleu,has,wins,bleu has wins,0.6252360939979553
translation,43,213,results,results,when inspecting,incremental systems,results when inspecting incremental systems,0.6780107617378235
translation,44,139,ablation-analysis,switching,from,base transformers ( m1 to m3 ),switching from base transformers ( m1 to m3 ),0.5862652659416199
translation,44,139,ablation-analysis,base transformers ( m1 to m3 ),to,big transformer architecture in model 4 ( m4 ),base transformers ( m1 to m3 ) to big transformer architecture in model 4 ( m4 ),0.570383608341217
translation,44,139,ablation-analysis,base transformers ( m1 to m3 ),led to,decent improvement,base transformers ( m1 to m3 ) led to decent improvement,0.6915906667709351
translation,44,139,ablation-analysis,big transformer architecture in model 4 ( m4 ),led to,decent improvement,big transformer architecture in model 4 ( m4 ) led to decent improvement,0.6932845115661621
translation,44,139,ablation-analysis,ablation analysis,has,switching,ablation analysis has switching,0.5401610732078552
translation,44,49,experimental-setup,trainings,run as,multi-gpu trainings,trainings run as multi-gpu trainings,0.7130387425422668
translation,44,49,experimental-setup,multi-gpu trainings,on,2 or 4 nvidia v100 gpus,multi-gpu trainings on 2 or 4 nvidia v100 gpus,0.49445027112960815
translation,44,49,experimental-setup,2 or 4 nvidia v100 gpus,with,16gb ram,2 or 4 nvidia v100 gpus with 16gb ram,0.584937572479248
translation,44,49,experimental-setup,server,with,8 32gb v100 gpus,server with 8 32gb v100 gpus,0.551819920539856
translation,44,49,experimental-setup,experimental setup,has,trainings,experimental setup has trainings,0.5418388843536377
translation,44,8,experiments,domain constrained task,for,french - german language pair,domain constrained task for french - german language pair,0.5653309226036072
translation,44,8,experiments,best system,by,significant margin,best system by significant margin,0.6058388948440552
translation,44,8,experiments,significant margin,in,bleu,significant margin in bleu,0.5657859444618225
translation,45,90,experimental-setup,marian toolkit,to implement,model,marian toolkit to implement model,0.7054635286331177
translation,45,90,experimental-setup,experimental setup,used,marian toolkit,experimental setup used marian toolkit,0.588976263999939
translation,45,142,experimental-setup,training,stopped after,7 epochs,training stopped after 7 epochs,0.7001830339431763
translation,45,142,experimental-setup,around 2 hours and 20 minutes,on,one a100 gpu,around 2 hours and 20 minutes on one a100 gpu,0.6157950162887573
translation,45,142,experimental-setup,7 epochs,has,around 2 hours and 20 minutes,7 epochs has around 2 hours and 20 minutes,0.5847815871238708
translation,45,142,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,45,162,results,from-scratch system,provides,worse results,from-scratch system provides worse results,0.6617926359176636
translation,45,162,results,worse results,than,mbart50 - based model,worse results than mbart50 - based model,0.5686272382736206
translation,45,162,results,results,has,from-scratch system,results has from-scratch system,0.6274483799934387
translation,45,163,results,mbart50 - based models,for,english ? pashto direction,mbart50 - based models for english ? pashto direction,0.677644670009613
translation,45,163,results,scores,obtained with,non-fine- tuned models,scores obtained with non-fine- tuned models,0.6111559271812439
translation,45,163,results,non-fine- tuned models,for,flores test set,non-fine- tuned models for flores test set,0.5726742744445801
translation,45,163,results,mbart50 - based models,has,scores,mbart50 - based models has scores,0.5677782297134399
translation,45,163,results,english ? pashto direction,has,scores,english ? pashto direction has scores,0.6031458973884583
translation,45,163,results,results,Regarding,mbart50 - based models,results Regarding mbart50 - based models,0.5567415952682495
translation,45,165,results,each successive fine-tuning step,improves,scores,each successive fine-tuning step improves scores,0.6780390739440918
translation,45,165,results,last model,evaluated against,flores devtest set,last model evaluated against flores devtest set,0.7219952344894409
translation,45,165,results,english ? pashto direction,has,each successive fine-tuning step,english ? pashto direction has each successive fine-tuning step,0.6040989756584167
translation,45,165,results,results,In,english ? pashto direction,results In english ? pashto direction,0.5359020233154297
translation,45,166,results,system,resulting from,three -step fine-tuning process,system resulting from three -step fine-tuning process,0.6967940330505371
translation,45,166,results,three -step fine-tuning process,improves,google 's scores,three -step fine-tuning process improves google 's scores,0.7039458155632019
translation,45,166,results,google 's scores,as of,april 2021,google 's scores as of april 2021,0.6460599303245544
translation,45,166,results,results,has,system,results has system,0.5883707404136658
translation,46,19,experimental-setup,embedding size,of,1024,embedding size of 1024,0.6278319954872131
translation,46,19,experimental-setup,unit size,of,4096,unit size of 4096,0.6462011337280273
translation,46,19,experimental-setup,transformer - big architecture,has,6 layers of encoders and decoders,transformer - big architecture has 6 layers of encoders and decoders,0.6057922840118408
translation,46,19,experimental-setup,transformer - big architecture,has,16 heads,transformer - big architecture has 16 heads,0.6234036087989807
translation,46,19,experimental-setup,transformer - big architecture,has,embedding size,transformer - big architecture has embedding size,0.5476336479187012
translation,46,19,experimental-setup,transformer - big architecture,has,unit size,transformer - big architecture has unit size,0.5971236228942871
translation,46,21,experimental-setup,same vocabulary,of,32 k sentencepiece subwords,same vocabulary of 32 k sentencepiece subwords,0.6037331223487854
translation,46,21,experimental-setup,same vocabulary,to allow,ensembling,same vocabulary to allow ensembling,0.6642897129058838
translation,46,21,experimental-setup,experimental setup,trained with,same vocabulary,experimental setup trained with same vocabulary,0.7636191844940186
translation,46,22,experimental-setup,shared vocabulary,between,source and target,shared vocabulary between source and target,0.6331157684326172
translation,46,22,experimental-setup,experimental setup,use,shared vocabulary,experimental setup use shared vocabulary,0.6128562092781067
translation,46,25,experimental-setup,models,with,32gb dynamic batch size,models with 32gb dynamic batch size,0.6559639573097229
translation,46,25,experimental-setup,models,with,optimizer delay,models with optimizer delay,0.6404898166656494
translation,46,25,experimental-setup,optimizer delay,),3,optimizer delay ) 3,0.6712518930435181
translation,46,25,experimental-setup,optimizer delay,of,3,optimizer delay of 3,0.6553518772125244
translation,46,25,experimental-setup,3,with,"adam optimizer ( kingma and ba , 2015 )","3 with adam optimizer ( kingma and ba , 2015 )",0.6167420744895935
translation,46,25,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",under,learning rate,"adam optimizer ( kingma and ba , 2015 ) under learning rate",0.6064422726631165
translation,46,25,experimental-setup,learning rate,of,0.0003,learning rate of 0.0003,0.5981547236442566
translation,46,25,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,46,26,experimental-setup,experimental setup,trained with,marian nmt toolkit,experimental setup trained with marian nmt toolkit,0.7077090740203857
translation,46,92,results,fine-tuning,on,retrieved subsets of data,fine-tuning on retrieved subsets of data,0.5477409958839417
translation,46,92,results,fine-tuning,results in,quality gains,fine-tuning results in quality gains,0.6379666924476624
translation,46,92,results,retrieved subsets of data,results in,quality gains,retrieved subsets of data results in quality gains,0.6850849986076355
translation,46,92,results,results,see that,fine-tuning,results see that fine-tuning,0.6683403253555298
translation,46,125,results,our systems,rank at,top ( tied ),our systems rank at top ( tied ),0.7471420764923096
translation,46,125,results,top ( tied ),among,all the constrained submissions,top ( tied ) among all the constrained submissions,0.6117236018180847
translation,46,125,results,all the constrained submissions,for,both translation directions,all the constrained submissions for both translation directions,0.5971723794937134
translation,47,40,ablation-analysis,rare morphological phenomena,indicating,grammatical relations,rare morphological phenomena indicating grammatical relations,0.561609148979187
translation,47,40,ablation-analysis,ablation analysis,For,"isolating languages ( vi , ml )","ablation analysis For isolating languages ( vi , ml )",0.6223136782646179
translation,47,29,experimental-setup,experimental setup,use,transformer architecture,experimental setup use transformer architecture,0.5702999830245972
translation,47,30,experimental-setup,results ' reliability,run,exhaustive search,results ' reliability run exhaustive search,0.6849119663238525
translation,47,30,experimental-setup,exhaustive search,of,hyperparameters,exhaustive search of hyperparameters,0.5783757567405701
translation,47,30,experimental-setup,hyperparameters,including,batch size,hyperparameters including batch size,0.6511549353599548
translation,47,30,experimental-setup,hyperparameters,including,learning rate,hyperparameters including learning rate,0.627096951007843
translation,47,30,experimental-setup,experimental setup,To ensure,results ' reliability,experimental setup To ensure results ' reliability,0.6857015490531921
translation,47,34,results,char,showing,strong competitiveness,char showing strong competitiveness,0.6972000002861023
translation,47,34,results,outperforms,showing,strong competitiveness,outperforms showing strong competitiveness,0.7183896899223328
translation,47,34,results,other algorithms,in,7 out of 8 languages,other algorithms in 7 out of 8 languages,0.47392159700393677
translation,47,34,results,strong competitiveness,of,char 's ability,strong competitiveness of char 's ability,0.5352338552474976
translation,47,34,results,char 's ability,across,languages,char 's ability across languages,0.6910250186920166
translation,47,34,results,char,has,outperforms,char has outperforms,0.6973593831062317
translation,47,34,results,outperforms,has,other algorithms,outperforms has other algorithms,0.56695556640625
translation,47,34,results,bleu,has,"papineni et al. , 2002 )","bleu has papineni et al. , 2002 )",0.5902101397514343
translation,47,34,results,results,see that,char,results see that char,0.5306164622306824
translation,47,41,results,"two open-vocabulary segmentation algorithms ( char , bpe )",show,comparable performances,"two open-vocabulary segmentation algorithms ( char , bpe ) show comparable performances",0.5923139452934265
translation,47,41,results,results,see that,"two open-vocabulary segmentation algorithms ( char , bpe )","results see that two open-vocabulary segmentation algorithms ( char , bpe )",0.6100621223449707
translation,47,42,results,highly agglutinative languages,such as,finnish and turkish,highly agglutinative languages such as finnish and turkish,0.5867872834205627
translation,47,42,results,char,achieves,better performance,char achieves better performance,0.6970220804214478
translation,47,42,results,highly agglutinative languages,has,char,highly agglutinative languages has char,0.5425261855125427
translation,47,42,results,finnish and turkish,has,char,finnish and turkish has char,0.5984771251678467
translation,47,47,results,char,performs,best,char performs best,0.6812941431999207
translation,47,47,results,best,in,10 out of 14 tests,best in 10 out of 14 tests,0.5408512353897095
translation,47,47,results,results,has,char,results has char,0.4211825728416443
translation,47,48,results,char,surpasses,other algorithms,char surpasses other algorithms,0.5831470489501953
translation,47,48,results,other algorithms,by,at least 5 % accuracy,other algorithms by at least 5 % accuracy,0.5470868945121765
translation,47,48,results,comparative adjectives,has,possessive determiner,comparative adjectives has possessive determiner,0.506700873374939
translation,47,48,results,comparative adjectives,has,char,comparative adjectives has char,0.6126440763473511
translation,47,67,results,performances,of,char and bpe,performances of char and bpe,0.5995153784751892
translation,47,67,results,char and bpe,are,on par,char and bpe are on par,0.6745550632476807
translation,47,67,results,char and bpe,better than,word and morfessor,char and bpe better than word and morfessor,0.7426717877388
translation,47,67,results,results,has,performances,results has performances,0.5711642503738403
translation,47,71,results,char,better than,bpe,char better than bpe,0.7278478145599365
translation,47,71,results,en-fi,has,char,en-fi has char,0.7337402105331421
translation,47,71,results,results,For,en-fi,results For en-fi,0.6372359991073608
translation,47,76,results,corpus size,is,50 k to 200k,corpus size is 50 k to 200k,0.6079745292663574
translation,47,76,results,char,performs,best,char performs best,0.6812941431999207
translation,47,76,results,best,among,four segmentation methods,best among four segmentation methods,0.5919604301452637
translation,47,76,results,corpus size,has,char,corpus size has char,0.5928499698638916
translation,47,76,results,50 k to 200k,has,char,50 k to 200k has char,0.6166619062423706
translation,47,76,results,results,When,corpus size,results When corpus size,0.637296199798584
translation,47,87,results,char,achieved,highest recall rates,char achieved highest recall rates,0.7217808365821838
translation,47,87,results,highest recall rates,of,rare and unknown words,highest recall rates of rare and unknown words,0.5646172761917114
translation,47,87,results,results,see that,char,results see that char,0.5306164622306824
translation,47,101,results,char,surpasses,other algorithms,char surpasses other algorithms,0.5831470489501953
translation,47,101,results,other algorithms,in,almost all settings,other algorithms in almost all settings,0.4878334403038025
translation,47,101,results,other algorithms,except when,finetuning,other algorithms except when finetuning,0.6794964075088501
translation,47,101,results,finetuning,from,medical to others,finetuning from medical to others,0.6041419506072998
translation,47,106,results,char,achieves,better performance,char achieves better performance,0.6970220804214478
translation,47,106,results,better performance,on,oov words,better performance on oov words,0.5320107340812683
translation,47,107,results,performances,of,char and subword - based algorithms,performances of char and subword - based algorithms,0.5712395310401917
translation,47,107,results,performances,of,char,performances of char,0.6459337472915649
translation,47,107,results,char and subword - based algorithms,are,on par,char and subword - based algorithms are on par,0.6238473057746887
translation,47,107,results,on par,on,common words,on par on common words,0.6681869029998779
translation,47,107,results,others,by,large margin,others by large margin,0.5819982886314392
translation,47,107,results,large margin,on,domainspecific words,large margin on domainspecific words,0.5342857837677002
translation,47,107,results,char,has,outperforms,char has outperforms,0.6973593831062317
translation,47,107,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,47,113,results,bpedropout,surpasses,bpe,bpedropout surpasses bpe,0.6699522733688354
translation,47,113,results,bpe,by,large margin,bpe by large margin,0.5736846327781677
translation,47,113,results,char,achieves,best performance,char achieves best performance,0.699524462223053
translation,47,113,results,char,shows,superiority,char shows superiority,0.7234017252922058
translation,47,113,results,best performance,shows,superiority,best performance shows superiority,0.6821750998497009
translation,47,113,results,superiority,of,char,superiority of char,0.6781660318374634
translation,48,80,baselines,our model tag + mask,integrates,constraints,our model tag + mask integrates constraints,0.6783789396286011
translation,48,80,baselines,constraints,during,inference time,constraints during inference time,0.6738966703414917
translation,48,80,baselines,baselines,compare,our model tag + mask,baselines compare our model tag + mask,0.6168729066848755
translation,48,64,experimental-setup,experimental setup,use,moses tokenizer,experimental setup use moses tokenizer,0.6008074879646301
translation,48,70,experimental-setup,2048 - dimensional inner layers,for,fully connected feed -forward network,2048 - dimensional inner layers for fully connected feed -forward network,0.6240358948707581
translation,48,70,experimental-setup,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,48,70,experimental-setup,512dimensional embeddings,has,2048 - dimensional inner layers,512dimensional embeddings has 2048 - dimensional inner layers,0.5455014109611511
translation,48,70,experimental-setup,experimental setup,use,512dimensional embeddings,experimental setup use 512dimensional embeddings,0.5622006058692932
translation,48,71,experimental-setup,maximum of 100 epochs,with,batch size,maximum of 100 epochs with batch size,0.6039434671401978
translation,48,71,experimental-setup,maximum of 100 epochs,with,initial learning rate,maximum of 100 epochs with initial learning rate,0.5874502658843994
translation,48,71,experimental-setup,batch size,of,2000 tokens per iteration,batch size of 2000 tokens per iteration,0.5666831731796265
translation,48,71,experimental-setup,initial learning rate,of,5 ? 10 ?4,initial learning rate of 5 ? 10 ?4,0.6462741494178772
translation,48,71,experimental-setup,experimental setup,trained for,minimum of 50 epochs,experimental setup trained for minimum of 50 epochs,0.7151539325714111
translation,48,71,experimental-setup,experimental setup,trained for,maximum of 100 epochs,experimental setup trained for maximum of 100 epochs,0.7019256353378296
translation,48,71,experimental-setup,experimental setup,trained for,initial learning rate,experimental setup trained for initial learning rate,0.7129256725311279
translation,48,72,experimental-setup,validation set,to compute,stopping criterion,validation set to compute stopping criterion,0.7097781896591187
translation,48,72,experimental-setup,each language pair,has,validation set,each language pair has validation set,0.5596736669540405
translation,48,72,experimental-setup,experimental setup,For,each language pair,experimental setup For each language pair,0.584132730960846
translation,48,73,experimental-setup,beam size,of,5,beam size of 5,0.7073217034339905
translation,48,73,experimental-setup,beam size,during,inference,beam size during inference,0.6705121994018555
translation,48,73,experimental-setup,5,during,inference,5 during inference,0.6310677528381348
translation,48,73,experimental-setup,inference,for,all models,inference for all models,0.596608579158783
translation,48,66,experiments,english -> chinese,rely on,"sentence piece ( kudo and richardson , 2018 )","english -> chinese rely on sentence piece ( kudo and richardson , 2018 )",0.7010373473167419
translation,48,66,experiments,english -> chinese,results in,vocabulary size,english -> chinese results in vocabulary size,0.6124988794326782
translation,48,66,experiments,"sentence piece ( kudo and richardson , 2018 )",for,tokenization,"sentence piece ( kudo and richardson , 2018 ) for tokenization",0.582484781742096
translation,48,66,experiments,vocabulary size,of,52172,vocabulary size of 52172,0.6226093769073486
translation,48,66,experiments,vocabulary size,of,39996,vocabulary size of 39996,0.6100746989250183
translation,48,66,experiments,52172,for,chinese,52172 for chinese,0.609093427658081
translation,48,66,experiments,39996,for,english,39996 for english,0.6401333808898926
translation,48,6,model,method,introduces,two main changes,method introduces two main changes,0.6755315065383911
translation,48,6,model,two main changes,to,standard procedure,two main changes to standard procedure,0.5098804235458374
translation,48,6,model,standard procedure,to handle,terminologies,standard procedure to handle terminologies,0.662314236164093
translation,48,6,model,model,rely on,transformer - based architecture,model rely on transformer - based architecture,0.7462427616119385
translation,48,6,model,model,explore,method,model explore method,0.6369842886924744
translation,48,7,model,training data,to encourage,model,training data to encourage model,0.7434569597244263
translation,48,7,model,model,to learn,copy behavior,model to learn copy behavior,0.6475489139556885
translation,48,7,model,copy behavior,when it encounters,terminology constraint terms,copy behavior when it encounters terminology constraint terms,0.5934646725654602
translation,48,7,model,augmenting,has,training data,augmenting has training data,0.5231660604476929
translation,48,8,model,constraint token masking,ease,copy behavior learning,constraint token masking ease copy behavior learning,0.7349850535392761
translation,48,8,model,constraint token masking,improve,model generalization,constraint token masking improve model generalization,0.6696206331253052
translation,48,22,model,our training data,with,constraints,our training data with constraints,0.6467801928520203
translation,48,22,model,constraints,using,tags,constraints using tags,0.7241973876953125
translation,48,22,model,tags,to distinguish,constraints terms,tags to distinguish constraints terms,0.5992612838745117
translation,48,22,model,constraints terms,from,other tokens,constraints terms from other tokens,0.5029690861701965
translation,48,22,model,other tokens,in,sentences,other tokens in sentences,0.4947642385959625
translation,48,22,model,model,annotate,our training data,model annotate our training data,0.7568129301071167
translation,48,68,model,transformer architecture,with,6 stacked encoders / decoders,transformer architecture with 6 stacked encoders / decoders,0.6316116452217102
translation,48,68,model,transformer architecture,with,8 attention heads,transformer architecture with 8 attention heads,0.6505460739135742
translation,48,68,model,model,use,transformer architecture,model use transformer architecture,0.6436730623245239
translation,48,69,model,source and target embeddings,tied with,softmax layer,source and target embeddings tied with softmax layer,0.6220076680183411
translation,48,69,model,english - french,has,source and target embeddings,english - french has source and target embeddings,0.5738688111305237
translation,48,69,model,model,For,english - french,model For english - french,0.6111358404159546
translation,48,81,results,results,on,english ? french,results on english ? french,0.5470064878463745
translation,48,82,results,significantly improves,over,baselines,significantly improves over baselines,0.7216042280197144
translation,48,82,results,baselines,in terms of,all measures,baselines in terms of all measures,0.6096248626708984
translation,48,82,results,tag + mask approach,has,significantly improves,tag + mask approach has significantly improves,0.6242430806159973
translation,48,82,results,results,observe,tag + mask approach,results observe tag + mask approach,0.621680498123169
translation,49,116,ablation-analysis,back - translation,BLEU score,decreases,back - translation BLEU score decreases,0.6788962483406067
translation,49,116,ablation-analysis,ablation analysis,observe,back - translation,ablation analysis observe back - translation,0.6280137300491333
translation,49,116,ablation-analysis,ablation analysis,using,back - translation,ablation analysis using back - translation,0.6887746453285217
translation,49,166,ablation-analysis,phrases,from,smt training,phrases from smt training,0.6409443020820618
translation,49,166,ablation-analysis,phrases,can help,nmt model,phrases can help nmt model,0.6679195165634155
translation,49,166,ablation-analysis,nmt model,perform,better,nmt model perform better,0.6234737038612366
translation,49,166,ablation-analysis,ablation analysis,observe,phrases,ablation analysis observe phrases,0.6801537275314331
translation,49,21,experiments,phrase table injection ( pti ) experiment,explore,phrases,phrase table injection ( pti ) experiment explore phrases,0.689207136631012
translation,49,21,experiments,phrases,generated during,statistical machine translation ( smt ) model training,phrases generated during statistical machine translation ( smt ) model training,0.6541280746459961
translation,49,21,experiments,statistical machine translation ( smt ) model training,be further utilized in,nmt,statistical machine translation ( smt ) model training be further utilized in nmt,0.5841194987297058
translation,49,90,experiments,pytorch version,to carry out,"pti , combined corpus and back -translation experiments","pytorch version to carry out pti , combined corpus and back -translation experiments",0.6503874659538269
translation,49,90,experiments,"of opennmt ( klein et al. , 2017 )",to carry out,"pti , combined corpus and back -translation experiments","of opennmt ( klein et al. , 2017 ) to carry out pti , combined corpus and back -translation experiments",0.6284440159797668
translation,49,90,experiments,pytorch version,has,"of opennmt ( klein et al. , 2017 )","pytorch version has of opennmt ( klein et al. , 2017 )",0.5155613422393799
translation,49,99,experiments,model,trained for,200k training steps,model trained for 200k training steps,0.7699235081672668
translation,49,99,experiments,model,further fine-tuned for,100k training steps,model further fine-tuned for 100k training steps,0.6723117828369141
translation,49,99,experiments,combined corpus experiment,has,model,combined corpus experiment has model,0.5507466197013855
translation,49,100,experiments,transformer model,from,fairseq library,transformer model from fairseq library,0.5374433994293213
translation,49,100,experiments,pivot language based transfer learning experiments,has,transformer model,pivot language based transfer learning experiments has transformer model,0.5472120642662048
translation,49,126,experiments,multilingual model,use,shared decoder,multilingual model use shared decoder,0.5976740121841431
translation,49,126,experiments,shared decoder,for,both the target languages,shared decoder for both the target languages,0.59074866771698
translation,49,128,experiments,multilingual model,observe that,bleu score increase,multilingual model observe that bleu score increase,0.5420359373092651
translation,49,128,experiments,bleu score increase,on,ilci test set,bleu score increase on ilci test set,0.5211617350578308
translation,49,128,experiments,bleu score increase,more than,wat 2021 test set,bleu score increase more than wat 2021 test set,0.5522081851959229
translation,49,128,experiments,direct pivoting,has,step-wise pivoting,direct pivoting has step-wise pivoting,0.5650990605354309
translation,49,92,hyperparameters,smt model,for,pti,smt model for pti,0.6480227708816528
translation,49,92,hyperparameters,smt model,trained using,moses toolkit,smt model trained using moses toolkit,0.6989375948905945
translation,49,92,hyperparameters,hyperparameters,has,smt model,hyperparameters has smt model,0.5354337096214294
translation,49,101,hyperparameters,adam,with,"betas ( 0.9 , 0.98 )","adam with betas ( 0.9 , 0.98 )",0.5990045070648193
translation,49,101,hyperparameters,hyperparameters,has,optimizer,hyperparameters has optimizer,0.5107399225234985
translation,49,102,hyperparameters,5e - 4,with,inverse square root learning rate scheduler,5e - 4 with inverse square root learning rate scheduler,0.6786665320396423
translation,49,102,hyperparameters,5e - 4,with,4000 warm - up updates,5e - 4 with 4000 warm - up updates,0.6506451368331909
translation,49,102,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,49,103,hyperparameters,label smoothed cross entropy,with,label smoothing,label smoothed cross entropy with label smoothing,0.624054491519928
translation,49,103,hyperparameters,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,49,103,hyperparameters,hyperparameters,has,dropout probability value,hyperparameters has dropout probability value,0.5069120526313782
translation,49,103,hyperparameters,hyperparameters,has,criterion,hyperparameters has criterion,0.5238820314407349
translation,49,104,hyperparameters,models,trained for,400 epochs,models trained for 400 epochs,0.7555566430091858
translation,49,104,hyperparameters,hyperparameters,trained for,400 epochs,hyperparameters trained for 400 epochs,0.6690402626991272
translation,49,104,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,49,9,results,significant improvement trend,in,bleu score,significant improvement trend in bleu score,0.5577380061149597
translation,49,9,results,baseline transformer model,has,significant improvement trend,baseline transformer model has significant improvement trend,0.554476797580719
translation,49,9,results,results,Compared to,baseline transformer model,results Compared to baseline transformer model,0.6877502799034119
translation,49,114,results,bleu score,on,wat 2021 test set,bleu score on wat 2021 test set,0.487662136554718
translation,49,114,results,bleu score,on,ilci test set,bleu score on ilci test set,0.48496687412261963
translation,49,114,results,bleu score,while,bleu score,bleu score while bleu score,0.5183222889900208
translation,49,114,results,bleu score,while,ilci test set,bleu score while ilci test set,0.5285005569458008
translation,49,114,results,bleu score,while,decreases,bleu score while decreases,0.6863393187522888
translation,49,114,results,bleu score,on,ilci test set,bleu score on ilci test set,0.48496687412261963
translation,49,114,results,wat 2021 test set,while,bleu score,wat 2021 test set while bleu score,0.5454467535018921
translation,49,114,results,bleu score,on,ilci test set,bleu score on ilci test set,0.48496687412261963
translation,49,114,results,results,In,pti experiment,results In pti experiment,0.5750253796577454
translation,49,115,results,improvement,of,more than 1.5 bleu score,improvement of more than 1.5 bleu score,0.537095308303833
translation,49,115,results,more than 1.5 bleu score,on,both the test sets,more than 1.5 bleu score on both the test sets,0.5044138431549072
translation,49,115,results,results,For,combined corpus,results For combined corpus,0.5973630547523499
translation,49,119,results,combined corpus methods,give,best results,combined corpus methods give best results,0.6106348633766174
translation,49,120,results,results,of,direct pivoting technique,results of direct pivoting technique,0.49709370732307434
translation,49,120,results,direct pivoting technique,show,improvement,direct pivoting technique show improvement,0.6340612173080444
translation,49,120,results,improvement,of,2.29 bleu score,improvement of 2.29 bleu score,0.5215906500816345
translation,49,120,results,improvement,of,0.42,improvement of 0.42,0.5774509310722351
translation,49,120,results,2.29 bleu score,over,baseline model,2.29 bleu score over baseline model,0.5981348156929016
translation,49,120,results,baseline model,on,ilci test set,baseline model on ilci test set,0.5056218504905701
translation,49,120,results,0.42,on,wat 2021 test set,0.42 on wat 2021 test set,0.5469943881034851
translation,49,120,results,results,of,direct pivoting technique,results of direct pivoting technique,0.49709370732307434
translation,49,120,results,results,has,results,results has results,0.48582205176353455
translation,49,121,results,results,of,step-wise pivoting,results of step-wise pivoting,0.5178598761558533
translation,49,121,results,step-wise pivoting,show,improvement,step-wise pivoting show improvement,0.6048926115036011
translation,49,121,results,improvement,of,1.91 bleu score,improvement of 1.91 bleu score,0.5144397616386414
translation,49,121,results,improvement,of,0.74,improvement of 0.74,0.5643344521522522
translation,49,121,results,1.91 bleu score,over,baseline,1.91 bleu score over baseline,0.5963494777679443
translation,49,121,results,baseline,on,ilci test set,baseline on ilci test set,0.5227463841438293
translation,49,121,results,0.74,on,wat 2021 test set,0.74 on wat 2021 test set,0.5411565899848938
translation,49,121,results,results,of,step-wise pivoting,results of step-wise pivoting,0.5178598761558533
translation,49,121,results,results,has,results,results has results,0.48582205176353455
translation,49,124,results,initialization,of,encoder and decoder,initialization of encoder and decoder,0.6018130779266357
translation,49,124,results,initialization,performs,better,initialization performs better,0.6223989725112915
translation,49,124,results,encoder and decoder,performs,better,encoder and decoder performs better,0.6440306901931763
translation,49,124,results,better,than,random initialization,better than random initialization,0.581092357635498
translation,49,124,results,results,observe,initialization,results observe initialization,0.6177158355712891
translation,49,125,results,multilingual model,on,english - marathi translation task,multilingual model on english - marathi translation task,0.5108702182769775
translation,49,125,results,multilingual model,show,bleu score increase,multilingual model show bleu score increase,0.6413103938102722
translation,49,125,results,multilingual model,show,0.83,multilingual model show 0.83,0.5914943218231201
translation,49,125,results,bleu score increase,of,2.8,bleu score increase of 2.8,0.5789061784744263
translation,49,125,results,bleu score increase,of,0.83,bleu score increase of 0.83,0.5415034890174866
translation,49,125,results,2.8,on,ilci test set,2.8 on ilci test set,0.47427621483802795
translation,49,125,results,0.83,on,wat 2021 test set,0.83 on wat 2021 test set,0.5406711101531982
translation,49,125,results,0.83,over,baseline model,0.83 over baseline model,0.5816560983657837
translation,49,125,results,wat 2021 test set,over,baseline model,wat 2021 test set over baseline model,0.6676748991012573
translation,49,125,results,results,of,multilingual model,results of multilingual model,0.5366551280021667
translation,49,165,results,significantly improve,quality of,english - marathi translations,significantly improve quality of english - marathi translations,0.5781965255737305
translation,49,165,results,english - marathi translations,over,baseline,english - marathi translations over baseline,0.6709226369857788
translation,49,165,results,english - marathi translations,by using,hindi,english - marathi translations by using hindi,0.6307237148284912
translation,49,165,results,baseline,by using,hindi,baseline by using hindi,0.6160809397697449
translation,49,165,results,hindi,as,assisting language,hindi as assisting language,0.5377927422523499
translation,49,165,results,pivot based transfer learning approach,has,significantly improve,pivot based transfer learning approach has significantly improve,0.5878448486328125
translation,49,165,results,results,shown,pivot based transfer learning approach,results shown pivot based transfer learning approach,0.6647732853889465
translation,49,167,results,"one ( english ) to many ( hindi , marathi ) multilingual model",able to improve,english - marathi translations,"one ( english ) to many ( hindi , marathi ) multilingual model able to improve english - marathi translations",0.7109501957893372
translation,49,167,results,english - marathi translations,by leveraging,english -hindi parallel corpus,english - marathi translations by leveraging english -hindi parallel corpus,0.6727513074874878
translation,49,167,results,results,has,"one ( english ) to many ( hindi , marathi ) multilingual model","results has one ( english ) to many ( hindi , marathi ) multilingual model",0.558403491973877
translation,50,6,experiments,en-ha,use,iterative back -translation approach,en-ha use iterative back -translation approach,0.6197870373725891
translation,50,6,experiments,en-ha,investigate,vocabulary embedding mapping,en-ha investigate vocabulary embedding mapping,0.5982159376144409
translation,50,6,experiments,iterative back -translation approach,on top of,pre-trained en - de models,iterative back -translation approach on top of pre-trained en - de models,0.7002521753311157
translation,50,6,experiments,iterative back -translation approach,investigate,vocabulary embedding mapping,iterative back -translation approach investigate vocabulary embedding mapping,0.6017114520072937
translation,50,58,experiments,sentence splitting,on,8- encoder - 4 - decoder variant,sentence splitting on 8- encoder - 4 - decoder variant,0.5765244364738464
translation,50,58,experiments,8- encoder - 4 - decoder variant,for,both languages,8- encoder - 4 - decoder variant for both languages,0.6089721322059631
translation,50,18,hyperparameters,tok-enized,using,"sentencepiece ( kudo and richardson , 2018 )","tok-enized using sentencepiece ( kudo and richardson , 2018 )",0.6602347493171692
translation,50,18,hyperparameters,"sentencepiece ( kudo and richardson , 2018 )",with,32 k shared vocabulary,"sentencepiece ( kudo and richardson , 2018 ) with 32 k shared vocabulary",0.6169540882110596
translation,50,18,hyperparameters,few extra tokens,for,tagged backtranslation,few extra tokens for tagged backtranslation,0.5916451811790466
translation,50,18,hyperparameters,hyperparameters,has,sentences,hyperparameters has sentences,0.5618694424629211
translation,50,19,hyperparameters,hyperparameters,trained following,marian 's transformer - big task preset,hyperparameters trained following marian 's transformer - big task preset,0.7056519389152527
translation,50,41,hyperparameters,en?de,mix,back - translations,en?de mix back - translations,0.7539018988609314
translation,50,41,hyperparameters,en?de,for,de?en,en?de for de?en,0.709898829460144
translation,50,41,hyperparameters,en?de,adopt,"tagged back - translation ( caswell et al. , 2019 )","en?de adopt tagged back - translation ( caswell et al. , 2019 )",0.6731306314468384
translation,50,41,hyperparameters,back - translations,generated using,greedy search,back - translations generated using greedy search,0.668574333190918
translation,50,41,hyperparameters,back - translations,generated using,beam search,back - translations generated using beam search,0.7174793481826782
translation,50,41,hyperparameters,back - translations,generated using,sampling,back - translations generated using sampling,0.7145619988441467
translation,50,41,hyperparameters,de?en,adopt,"tagged back - translation ( caswell et al. , 2019 )","de?en adopt tagged back - translation ( caswell et al. , 2019 )",0.6824945211410522
translation,50,41,hyperparameters,hyperparameters,For,en?de,hyperparameters For en?de,0.6609852313995361
translation,50,41,hyperparameters,hyperparameters,for,de?en,hyperparameters for de?en,0.6365035176277161
translation,50,73,hyperparameters,en?ha model,with,pre-trained en?de transformer - big model,en?ha model with pre-trained en?de transformer - big model,0.669706404209137
translation,50,5,model,en- de systems,in,three stages,en- de systems in three stages,0.6393433213233948
translation,50,5,model,model,build,en- de systems,model build en- de systems,0.6913965940475464
translation,50,12,model,training back,has,translations,training back has translations,0.606299638748169
translation,50,12,model,model,add to,training back,model add to training back,0.620108425617218
translation,50,99,results,outputs,of,mapping approach,outputs of mapping approach,0.594796895980835
translation,50,99,results,mapping approach,to,baseline,mapping approach to baseline,0.5948589444160461
translation,50,99,results,mapping approach,are,qualitatively very similar,mapping approach are qualitatively very similar,0.5775352716445923
translation,50,99,results,baseline,for,ha- en system,baseline for ha- en system,0.6180943846702576
translation,50,99,results,results,has,outputs,results has outputs,0.5481858849525452
translation,51,6,baselines,baselines,has,active learning ( al ),baselines has active learning ( al ),0.5599717497825623
translation,51,145,baselines,clustering,done based on,cosine similarity,clustering done based on cosine similarity,0.7343263626098633
translation,51,145,baselines,cosine similarity,between,last output encodings ( corresponding to sentence length ),cosine similarity between last output encodings ( corresponding to sentence length ),0.6187379360198975
translation,51,145,baselines,last output encodings ( corresponding to sentence length ),from,encoder in m.,last output encodings ( corresponding to sentence length ) from encoder in m.,0.548305094242096
translation,51,145,baselines,cosine,has,clustering,cosine has clustering,0.6093937158584595
translation,51,145,baselines,baselines,has,cosine,baselines has cosine,0.6110017895698547
translation,51,149,baselines,clustering,done based on,cosine similarity,clustering done based on cosine similarity,0.7343263626098633
translation,51,149,baselines,cosine similarity,between,sentence embeddings,cosine similarity between sentence embeddings,0.6072803735733032
translation,51,149,baselines,cosine similarity,obtained from,pre-trained infersent model,cosine similarity obtained from pre-trained infersent model,0.581497073173523
translation,51,149,baselines,infersent,has,clustering,infersent has clustering,0.6495401859283447
translation,51,149,baselines,baselines,has,infersent,baselines has infersent,0.6182729005813599
translation,51,124,experiments,translation task,use,lstm based encoder-decoder architecture,translation task use lstm based encoder-decoder architecture,0.5924144387245178
translation,51,124,experiments,lstm based encoder-decoder architecture,with,bahdanau attention,lstm based encoder-decoder architecture with bahdanau attention,0.6375548839569092
translation,51,136,hyperparameters,initial data splits,used for,training,initial data splits used for training,0.6596821546554565
translation,51,136,hyperparameters,model m,set at,2 %,model m set at 2 %,0.6634989976882935
translation,51,136,hyperparameters,2 %,of,randomly sampled data,2 % of randomly sampled data,0.6268700957298279
translation,51,136,hyperparameters,randomly sampled data,for,sequence tagging,randomly sampled data for sequence tagging,0.6602042317390442
translation,51,136,hyperparameters,training,has,model m,training has model m,0.6055253148078918
translation,51,136,hyperparameters,hyperparameters,has,initial data splits,hyperparameters has initial data splits,0.5160243511199951
translation,51,7,model,al strategies,choose,examples,al strategies choose examples,0.7212077379226685
translation,51,7,model,al strategies,may potentially select,similar examples,al strategies may potentially select similar examples,0.7760704755783081
translation,51,7,model,may not contribute significantly,to,learning process,may not contribute significantly to learning process,0.5708298683166504
translation,51,8,model,active 2 learning ( a 2 l ),actively adapts,deep learning model,active 2 learning ( a 2 l ) actively adapts deep learning model,0.7740846872329712
translation,51,8,model,redundant examples,chosen by,al strategy,redundant examples chosen by al strategy,0.716233491897583
translation,51,9,model,2 l,is,widely applicable,2 l is widely applicable,0.5774321556091309
translation,51,9,model,2 l,in conjunction with,several different al strategies,2 l in conjunction with several different al strategies,0.7000683546066284
translation,51,9,model,model,show,2 l,model show 2 l,0.7163286805152893
translation,51,11,model,active learning,has,al ),active learning has al ),0.6381728649139404
translation,51,20,model,approach,called,a 2 l,approach called a 2 l,0.711087167263031
translation,51,20,model,a 2 l,read as,active-squared learning,a 2 l read as active-squared learning,0.6366479992866516
translation,51,20,model,redundancies,of,existing al strategies,redundancies of existing al strategies,0.596267819404602
translation,51,157,results,all baselines,on,all tasks,all baselines on all tasks,0.49477487802505493
translation,51,157,results,approach,has,consistently outperforms,approach has consistently outperforms,0.629890501499176
translation,51,157,results,consistently outperforms,has,all baselines,consistently outperforms has all baselines,0.5969254374504089
translation,51,160,results,sequence tagging,match,performance,sequence tagging match performance,0.7274140119552612
translation,51,160,results,performance,obtained,training,performance obtained training,0.6555877327919006
translation,51,160,results,performance,by,training,performance by training,0.6188759803771973
translation,51,160,results,training,on,full dataset,training on full dataset,0.5418576598167419
translation,51,160,results,full dataset,using,smaller fraction of the data,full dataset using smaller fraction of the data,0.6775668859481812
translation,51,160,results,results,In,sequence tagging,results In sequence tagging,0.5193037986755371
translation,52,189,ablation-analysis,c-aware beam with tscaling ( t = 4 ),achieves,highest pmi correlation r = 0.740,c-aware beam with tscaling ( t = 4 ) achieves highest pmi correlation r = 0.740,0.6598872542381287
translation,52,189,ablation-analysis,ablation analysis,has,c-aware beam with tscaling ( t = 4 ),ablation analysis has c-aware beam with tscaling ( t = 4 ),0.5780142545700073
translation,52,102,baselines,bayes document reranker,has,"yu et al. , 2020 )","bayes document reranker has yu et al. , 2020 )",0.5544466376304626
translation,52,98,hyperparameters,texts,into,subwords,texts into subwords,0.6232675313949585
translation,52,98,hyperparameters,subwords,using,sentencepiece ( v0.1.81 ),subwords using sentencepiece ( v0.1.81 ),0.6294164657592773
translation,52,98,hyperparameters,sentencepiece ( v0.1.81 ),with,unigram lm,sentencepiece ( v0.1.81 ) with unigram lm,0.5823423862457275
translation,52,98,hyperparameters,hyperparameters,encode,texts,hyperparameters encode texts,0.7274997234344482
translation,52,126,hyperparameters,number of decoder blocks,is,12,number of decoder blocks is 12,0.585191547870636
translation,52,126,hyperparameters,hyperparameters,has,number of decoder blocks,hyperparameters has number of decoder blocks,0.5183616876602173
translation,52,127,hyperparameters,model size,is,768,model size is 768,0.5989303588867188
translation,52,127,hyperparameters,768,with,12 attention heads,768 with 12 attention heads,0.7100145220756531
translation,52,127,hyperparameters,inner layer,of,feedforward networks,inner layer of feedforward networks,0.5652838349342346
translation,52,127,hyperparameters,feedforward networks,has,3072 units,feedforward networks has 3072 units,0.5616061091423035
translation,52,127,hyperparameters,hyperparameters,has,model size,hyperparameters has model size,0.48909950256347656
translation,52,127,hyperparameters,hyperparameters,has,inner layer,hyperparameters has inner layer,0.5019038319587708
translation,52,5,model,simple method,to perform,context - aware decoding,simple method to perform context - aware decoding,0.671347439289093
translation,52,5,model,context - aware decoding,with,any pre-trained sentence - level translation model,context - aware decoding with any pre-trained sentence - level translation model,0.6225591897964478
translation,52,5,model,any pre-trained sentence - level translation model,by using,document - level language model,any pre-trained sentence - level translation model by using document - level language model,0.6100443005561829
translation,52,5,model,model,present,simple method,model present simple method,0.6760703921318054
translation,52,6,model,context - aware decoder,built upon,sentence - level parallel data,context - aware decoder built upon sentence - level parallel data,0.6380774974822998
translation,52,6,model,context - aware decoder,built upon,target - side document - level monolingual data,context - aware decoder built upon target - side document - level monolingual data,0.6159637570381165
translation,52,6,model,model,has,context - aware decoder,model has context - aware decoder,0.5447158217430115
translation,52,15,model,simple yet effective approach,to,context - aware nmt,simple yet effective approach to context - aware nmt,0.5748609304428101
translation,52,15,model,context - aware nmt,using,two primitive components,context - aware nmt using two primitive components,0.6712793707847595
translation,52,15,model,context - aware nmt,using,document- level language model ( lm ),context - aware nmt using document- level language model ( lm ),0.6455550193786621
translation,52,15,model,two primitive components,has,sentence - level nmt model,two primitive components has sentence - level nmt model,0.5176253914833069
translation,52,15,model,model,propose,simple yet effective approach,model propose simple yet effective approach,0.6920967102050781
translation,52,16,model,two components,on,common sentence - level parallel data,two components on common sentence - level parallel data,0.525034487247467
translation,52,16,model,two components,on,documentlevel monolingual data,two components on documentlevel monolingual data,0.5511773228645325
translation,52,16,model,two components,without using,document - level parallel data,two components without using document - level parallel data,0.6991716623306274
translation,52,16,model,model,independently train,two components,model independently train two components,0.711056649684906
translation,52,106,model,docrepair,is,sequence- to-sequence post-editing model,docrepair is sequence- to-sequence post-editing model,0.5871697068214417
translation,52,106,model,model,has,docrepair,model has docrepair,0.6076571941375732
translation,52,107,model,document- level inconsistencies,in,text,document- level inconsistencies in text,0.5442293882369995
translation,52,107,model,model,repairs,document- level inconsistencies,model repairs document- level inconsistencies,0.7541821599006653
translation,52,125,model,architecture,of,document- level lm,architecture of document- level lm,0.5576238036155701
translation,52,125,model,document- level lm,decoder part of,transformer,document- level lm decoder part of transformer,0.7223429083824158
translation,52,125,model,model,has,architecture,model has architecture,0.5575731992721558
translation,52,128,model,position embeddings,to represent,position information,position embeddings to represent position information,0.6684990525245667
translation,52,128,model,model,use,position embeddings,model use position embeddings,0.6356493830680847
translation,52,142,results,bayes docreranker and our c-aware rerank,has,consistently outperformed,bayes docreranker and our c-aware rerank has consistently outperformed,0.592186450958252
translation,52,142,results,consistently outperformed,has,baseline senttransformer,consistently outperformed has baseline senttransformer,0.629751980304718
translation,52,142,results,results,has,bayes docreranker and our c-aware rerank,results has bayes docreranker and our c-aware rerank,0.5675950646400452
translation,52,143,results,althogh bayes docreranker,performed,best,althogh bayes docreranker performed best,0.27427855134010315
translation,52,143,results,best,among,all the models,best among all the models,0.5855794548988342
translation,52,143,results,results,has,althogh bayes docreranker,results has althogh bayes docreranker,0.6079641580581665
translation,52,144,results,back - translation,contribute to,bleu,back - translation contribute to bleu,0.6102185249328613
translation,52,144,results,results,has,back - translation,results has back - translation,0.5638759732246399
translation,52,150,results,all the context-aware models,other than,docrepair,all the context-aware models other than docrepair,0.686757504940033
translation,52,150,results,c-score,has,outperforms,c-score has outperforms,0.6174265146255493
translation,52,150,results,outperforms,has,all the context-aware models,outperforms has all the context-aware models,0.5812892913818359
translation,52,150,results,results,has,c-score,results has c-score,0.5276270508766174
translation,52,151,results,performance,of,c-score,performance of c-score,0.5525521039962769
translation,52,151,results,c-score,achieving,large improvements,c-score achieving large improvements,0.6736964583396912
translation,52,151,results,slightly worse,than,docrepair,slightly worse than docrepair,0.6094810366630554
translation,52,151,results,slightly worse,than,docrepair,slightly worse than docrepair,0.6094810366630554
translation,52,151,results,docrepair,for,deixis,docrepair for deixis,0.6498837471008301
translation,52,151,results,large improvements,for,lex.c,large improvements for lex.c,0.6711026430130005
translation,52,151,results,large improvements,for,ell.vp,large improvements for ell.vp,0.6710329055786133
translation,52,151,results,ell.vp,over,docrepair,ell.vp over docrepair,0.6461406946182251
translation,52,151,results,9.8 points ),over,docrepair,9.8 points ) over docrepair,0.6374257802963257
translation,52,151,results,deixis,has,2.2 points,deixis has 2.2 points,0.5407351851463318
translation,52,151,results,ell.infl,has,4.0 points,ell.infl has 4.0 points,0.6197428703308105
translation,52,151,results,lex.c,has,19.1 points,lex.c has 19.1 points,0.614986777305603
translation,52,151,results,ell.vp,has,9.8 points ),ell.vp has 9.8 points ),0.6329166293144226
translation,52,151,results,results,has,performance,results has performance,0.5972660779953003
translation,52,152,results,d-lm only objectives,achieve,higher scores,d-lm only objectives achieve higher scores,0.6421841382980347
translation,52,152,results,higher scores,than,c-score,higher scores than c-score,0.6061593890190125
translation,52,152,results,c-score,except for,ell.infl,c-score except for ell.infl,0.6421512365341187
translation,52,152,results,results,has,d-lm only objectives,results has d-lm only objectives,0.5339188575744629
translation,52,155,results,advantage,of,c-score,advantage of c-score,0.5834602117538452
translation,52,155,results,c-score,over,senttransformer,c-score over senttransformer,0.6985747814178467
translation,52,155,results,excellent performance,of,d-lm,excellent performance of d-lm,0.6318372488021851
translation,52,155,results,d-lm,in capturing,contexts,d-lm in capturing contexts,0.7374351620674133
translation,52,155,results,contexts,in,translation,contexts in translation,0.5728859901428223
translation,52,155,results,results,has,advantage,results has advantage,0.5624832510948181
translation,53,4,baselines,baselines,has,unsupervised cross-lingual word embedding ( clwe ) methods,baselines has unsupervised cross-lingual word embedding ( clwe ) methods,0.5262943506240845
translation,53,82,baselines,word alignments,in,pseudo parallel corpus,word alignments in pseudo parallel corpus,0.49696069955825806
translation,53,82,baselines,fast align,with,default hyperparameters,fast align with default hyperparameters,0.5781927108764648
translation,53,82,baselines,),with,default hyperparameters,) with default hyperparameters,0.6470220685005188
translation,53,82,baselines,fast align,has,),fast align has ),0.6199233531951904
translation,53,166,experiments,dependency parsing and natural language inference,observe,similar trend,dependency parsing and natural language inference observe similar trend,0.5995880961418152
translation,53,166,experiments,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,53,166,experiments,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,53,63,hyperparameters,plain texts,from,wikipedia dumps,plain texts from wikipedia dumps,0.5342511534690857
translation,53,63,hyperparameters,plain texts,randomly extract,10 m sentences,plain texts randomly extract 10 m sentences,0.7852120995521545
translation,53,63,hyperparameters,10 m sentences,for,each language,10 m sentences for each language,0.6212260127067566
translation,53,63,hyperparameters,hyperparameters,use,plain texts,hyperparameters use plain texts,0.6415252089500427
translation,53,63,hyperparameters,hyperparameters,randomly extract,10 m sentences,hyperparameters randomly extract 10 m sentences,0.7530772089958191
translation,53,73,hyperparameters,100k sentences,randomly sampled from,monolingual data set,100k sentences randomly sampled from monolingual data set,0.6316275596618652
translation,53,83,hyperparameters,joint- training method,adopt,bivec,joint- training method adopt bivec,0.6712735295295715
translation,53,83,hyperparameters,bivec,to train,clwes,bivec to train clwes,0.6633027791976929
translation,53,83,hyperparameters,clwes,with,parameters,clwes with parameters,0.6903113126754761
translation,53,83,hyperparameters,parameters,using,pseudo parallel corpus and the word alignments,parameters using pseudo parallel corpus and the word alignments,0.6126294732093811
translation,53,83,hyperparameters,hyperparameters,For,joint- training method,hyperparameters For joint- training method,0.5838046073913574
translation,53,96,hyperparameters,bli scores,adopt,mean reciprocal rank ( mrr ),bli scores adopt mean reciprocal rank ( mrr ),0.6075526475906372
translation,53,96,hyperparameters,bli scores,adopt,p@1,bli scores adopt p@1,0.6860302686691284
translation,53,96,hyperparameters,hyperparameters,For,bli scores,hyperparameters For bli scores,0.5764695405960083
translation,53,6,model,pseudo-parallel corpus,generated by,unsupervised machine translation model,pseudo-parallel corpus generated by unsupervised machine translation model,0.6094158887863159
translation,53,6,model,unsupervised machine translation model,facilitates,structural similarity,unsupervised machine translation model facilitates structural similarity,0.5300684571266174
translation,53,6,model,structural similarity,of,two embedding spaces,structural similarity of two embedding spaces,0.5442988276481628
translation,53,6,model,quality of clwes,in,unsupervised mapping method,quality of clwes in unsupervised mapping method,0.5294250249862671
translation,53,6,model,model,using,pseudo-parallel corpus,model using pseudo-parallel corpus,0.6467326879501343
translation,53,13,model,pseudo sentences,generated from,unsupervised machine translation ( umt ) system,pseudo sentences generated from unsupervised machine translation ( umt ) system,0.5941298604011536
translation,53,13,model,pseudo sentences,facilitates,structural similarity,pseudo sentences facilitates structural similarity,0.6048094034194946
translation,53,13,model,structural similarity,has,without any additional cross-lingual resources,structural similarity has without any additional cross-lingual resources,0.5576961636543274
translation,53,13,model,model,show,pseudo sentences,model show pseudo sentences,0.6805293560028076
translation,53,101,results,mapping method,with,pseudo data augmentation,mapping method with pseudo data augmentation,0.6378503441810608
translation,53,101,results,mapping method,achieves,better performance,mapping method achieves better performance,0.6673942804336548
translation,53,101,results,better performance,than,other methods,better performance than other methods,0.5502663254737854
translation,53,101,results,all the language pairs,has,mapping method,all the language pairs has mapping method,0.5661192536354065
translation,53,101,results,results,In,all the language pairs,results In all the language pairs,0.49375849962234497
translation,53,102,results,greater amount of data,lead to,better performance,greater amount of data lead to better performance,0.686103343963623
translation,53,102,results,better performance,augmenting,both the source and target corpora,better performance augmenting both the source and target corpora,0.6433728933334351
translation,53,102,results,both the source and target corpora,shows,best performance,both the source and target corpora shows best performance,0.6470591425895691
translation,53,102,results,results,think that,greater amount of data,results think that greater amount of data,0.6259763836860657
translation,53,105,results,mapping methods,especially in,english and japanese pairs,mapping methods especially in english and japanese pairs,0.593837559223175
translation,53,105,results,underperform,has,mapping methods,underperform has mapping methods,0.5830087661743164
translation,53,111,results,better the quality of generated data,better the,performance,better the quality of generated data better the performance,0.7984504699707031
translation,53,111,results,performance,of,bli,performance of bli,0.6469729542732239
translation,53,127,results,parallel extension method,shows,slightly better bli performance,parallel extension method shows slightly better bli performance,0.6527413129806519
translation,53,127,results,slightly better bli performance,than,non-parallel extension,slightly better bli performance than non-parallel extension,0.5851742029190063
translation,53,127,results,results,has,parallel extension method,results has parallel extension method,0.5437940955162048
translation,53,129,results,no significant improvement,between,parallel and non-parallel corpora,no significant improvement between parallel and non-parallel corpora,0.6548239588737488
translation,53,129,results,results,In,eigenvector similarity,results In eigenvector similarity,0.5071014165878296
translation,53,131,results,augmentation,using,pseudo data,augmentation using pseudo data,0.7195405960083008
translation,53,131,results,results,show that,augmentation,results show that augmentation,0.4701533317565918
translation,53,137,results,improve,with,extension,improve with extension,0.6328000426292419
translation,53,137,results,extension,from,same target language,extension from same target language,0.5914311408996582
translation,53,137,results,bli performance and eigenvector similarity,has,improve,bli performance and eigenvector similarity has improve,0.6105016469955444
translation,53,163,results,topic classification,obtains,best results,topic classification obtains best results,0.6084293127059937
translation,53,163,results,our method,obtains,best results,our method obtains best results,0.5706233382225037
translation,53,163,results,best results,in,all language pairs,best results in all language pairs,0.5021550059318542
translation,53,163,results,topic classification,has,our method,topic classification has our method,0.5655404925346375
translation,53,163,results,results,For,topic classification,results For topic classification,0.59909588098526
translation,53,164,results,significant difference,obtained in,student 's t-test,significant difference obtained in student 's t-test,0.5638244152069092
translation,53,164,results,en-fr and en-ja,has,significant difference,en-fr and en-ja has significant difference,0.5681758522987366
translation,53,164,results,results,Especially in,en-fr and en-ja,results Especially in en-fr and en-ja,0.6522417664527893
translation,53,165,results,sentiment analysis,observe,significant improvement,sentiment analysis observe significant improvement,0.6061362028121948
translation,53,165,results,significant improvement,in,en- de,significant improvement in en- de,0.6384198665618896
translation,53,165,results,results,For,sentiment analysis,results For sentiment analysis,0.5833495259284973
translation,53,168,results,clwes,obtained from,our method,clwes obtained from our method,0.6541303396224976
translation,53,168,results,clwes,tend to show,higher performance,clwes tend to show higher performance,0.775418758392334
translation,53,168,results,higher performance,not only in,bli,higher performance not only in bli,0.6398950815200806
translation,53,168,results,higher performance,not only in,downstream tasks,higher performance not only in downstream tasks,0.6320818066596985
translation,53,168,results,higher performance,in,downstream tasks,higher performance in downstream tasks,0.5382009744644165
translation,53,175,results,scores,of,monolingual word embeddings,scores of monolingual word embeddings,0.5391559600830078
translation,53,175,results,monolingual word embeddings,using,french and german pseudo corpus,monolingual word embeddings using french and german pseudo corpus,0.5796507596969604
translation,53,175,results,french and german pseudo corpus,are,maintained or improved,french and german pseudo corpus are maintained or improved,0.5754021406173706
translation,53,175,results,decrease,in,japanese,decrease in japanese,0.5622499585151672
translation,53,175,results,results,has,scores,results has scores,0.5219217538833618
translation,53,181,results,with clwe,using,pseudo data,with clwe using pseudo data,0.7437275052070618
translation,53,181,results,with clwe,using,pseudo data,with clwe using pseudo data,0.7437275052070618
translation,53,181,results,with clwe,without,pseudo data,with clwe without pseudo data,0.7722812294960022
translation,53,181,results,higher bleu score,in,first step,higher bleu score in first step,0.5384308099746704
translation,53,181,results,score,at,further steps,score at further steps,0.5788559317588806
translation,53,181,results,clwe,without,pseudo data,clwe without pseudo data,0.7655459642410278
translation,53,181,results,initialization,has,with clwe,initialization has with clwe,0.6016689538955688
translation,53,181,results,results,observe,initialization,results observe initialization,0.6177158355712891
translation,53,187,results,performance,in,bli and downstream tasks,performance in bli and downstream tasks,0.549504280090332
translation,53,187,results,improves,has,performance,improves has performance,0.5770372748374939
translation,54,123,ablation-analysis,dataset scale,of,language pairs,dataset scale of language pairs,0.5811945199966431
translation,54,123,ablation-analysis,dataset scale,of,language pairs,dataset scale of language pairs,0.5811945199966431
translation,54,123,ablation-analysis,bleu and wr,become,larger,bleu and wr become larger,0.6407154202461243
translation,54,123,ablation-analysis,larger,suggesting that,language pairs,larger suggesting that language pairs,0.7049056887626648
translation,54,123,ablation-analysis,language pairs,benefit more from,lass,language pairs benefit more from lass,0.631249725818634
translation,54,123,ablation-analysis,language pairs,has,increasing,language pairs has increasing,0.626323401927948
translation,54,123,ablation-analysis,ablation analysis,observe,dataset scale,ablation analysis observe dataset scale,0.6339008212089539
translation,54,12,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
translation,54,12,experimental-setup,https,:,/nlp,https : /nlp,0.6072655320167542
translation,54,12,experimental-setup,https,:,-playground,https : -playground,0.687782347202301
translation,54,12,experimental-setup,https,:,lass,https : lass,0.6892513632774353
translation,54,12,experimental-setup,-playground,/,lass,-playground / lass,0.6929536461830139
translation,54,12,experimental-setup,//github.com,has,/nlp,//github.com has /nlp,0.4606422781944275
translation,54,12,experimental-setup,/nlp,has,-playground,/nlp has -playground,0.6573740243911743
translation,54,92,experimental-setup,byte pair encoding ( bpe ),preprocess,multilingual sentences,byte pair encoding ( bpe ) preprocess multilingual sentences,0.7002056241035461
translation,54,92,experimental-setup,multilingual sentences,resulting in,vocabulary size,multilingual sentences resulting in vocabulary size,0.6287344098091125
translation,54,92,experimental-setup,vocabulary size,of,30 k,vocabulary size of 30 k,0.6278605461120605
translation,54,92,experimental-setup,vocabulary size,of,64 k,vocabulary size of 64 k,0.6375458836555481
translation,54,92,experimental-setup,30 k,for,iwslt,30 k for iwslt,0.6542819142341614
translation,54,92,experimental-setup,64 k,for,wmt,64 k for wmt,0.6608196496963501
translation,54,92,experimental-setup,experimental setup,apply,byte pair encoding ( bpe ),experimental setup apply byte pair encoding ( bpe ),0.5851911902427673
translation,54,93,experimental-setup,over-sampling,for,iwslt and wmt,over-sampling for iwslt and wmt,0.6398867964744568
translation,54,93,experimental-setup,iwslt and wmt,to balance,training data distribution,iwslt and wmt to balance training data distribution,0.5734153389930725
translation,54,93,experimental-setup,training data distribution,with,temperature,training data distribution with temperature,0.6354981064796448
translation,54,93,experimental-setup,temperature,of,t = 2 and t = 5,temperature of t = 2 and t = 5,0.6658989787101746
translation,54,93,experimental-setup,experimental setup,apply,over-sampling,experimental setup apply over-sampling,0.6328614950180054
translation,54,102,experiments,iwslt,adopt,smaller transformer,iwslt adopt smaller transformer,0.7420492768287659
translation,54,102,experiments,multilingual baseline,on,all language pairs,multilingual baseline on all language pairs,0.5219277143478394
translation,54,102,experiments,smaller transformer,has,consistently outperforms,smaller transformer has consistently outperforms,0.6285796165466309
translation,54,102,experiments,consistently outperforms,has,multilingual baseline,consistently outperforms has multilingual baseline,0.6013474464416504
translation,54,7,model,lass,to jointly train,single unified multilingual mt model,lass to jointly train single unified multilingual mt model,0.6878798604011536
translation,54,7,model,model,propose,lass,model propose lass,0.5920370817184448
translation,54,8,model,lass,learns,language specific sub-network ( lass ),lass learns language specific sub-network ( lass ),0.6983982920646667
translation,54,8,model,language specific sub-network ( lass ),for,each language pair,language specific sub-network ( lass ) for each language pair,0.6066979765892029
translation,54,8,model,each language pair,to counter,parameter interference,each language pair to counter parameter interference,0.6685699224472046
translation,54,8,model,model,has,lass,model has lass,0.5519300103187561
translation,54,23,model,lass,to dynamically find and learn,language specific subnetwork,lass to dynamically find and learn language specific subnetwork,0.6673454642295837
translation,54,23,model,language specific subnetwork,for,multilingual nmt,language specific subnetwork for multilingual nmt,0.6119092702865601
translation,54,23,model,model,propose,lass,model propose lass,0.5920370817184448
translation,54,24,model,lass,accommodates,one sub-network,lass accommodates one sub-network,0.77667635679245
translation,54,24,model,one sub-network,for,each language pair,one sub-network for each language pair,0.6383290886878967
translation,54,24,model,model,has,lass,model has lass,0.5519300103187561
translation,54,31,model,lass,alleviates,parameter interference,lass alleviates parameter interference,0.7795993089675903
translation,54,31,model,model,has,lass,model has lass,0.5519300103187561
translation,54,52,model,multilingual transformer ( mtransformer ),as,backbone network,multilingual transformer ( mtransformer ) as backbone network,0.5394072532653809
translation,54,52,model,model,adopt,multilingual transformer ( mtransformer ),model adopt multilingual transformer ( mtransformer ),0.7123812437057495
translation,54,53,model,same encoder-decoder architecture,with,layers of multihead attention,same encoder-decoder architecture with layers of multihead attention,0.6576393246650696
translation,54,53,model,same encoder-decoder architecture,with,residual connection,same encoder-decoder architecture with residual connection,0.663193941116333
translation,54,53,model,same encoder-decoder architecture,with,layer normalization,same encoder-decoder architecture with layer normalization,0.6570441722869873
translation,54,53,model,mtransformer,has,same encoder-decoder architecture,mtransformer has same encoder-decoder architecture,0.59438157081604
translation,54,53,model,model,has,mtransformer,model has mtransformer,0.571429431438446
translation,54,188,model,more language specific components,on,top and bot -tom layers,more language specific components on top and bot -tom layers,0.5223399996757507
translation,54,188,model,top and bot -tom layers,rather than,middle ones,top and bot -tom layers rather than middle ones,0.6972715258598328
translation,54,188,model,model,tends to distribute,more language specific components,model tends to distribute more language specific components,0.6911579370498657
translation,54,11,results,lass,boosts,zero-shot translation,lass boosts zero-shot translation,0.7554757595062256
translation,54,11,results,zero-shot translation,average of,8.3 bleu,zero-shot translation average of 8.3 bleu,0.643997073173523
translation,54,11,results,8.3 bleu,on,30 language pairs,8.3 bleu on 30 language pairs,0.5075299143791199
translation,54,11,results,results,has,lass,results has lass,0.42956456542015076
translation,54,32,results,strong generalization performance,at,easy adaptation,strong generalization performance at easy adaptation,0.4723389446735382
translation,54,32,results,strong generalization performance,at,zero-shot translation,strong generalization performance at zero-shot translation,0.49295973777770996
translation,54,32,results,easy adaptation,to,new language pairs,easy adaptation to new language pairs,0.5466799139976501
translation,54,32,results,easy adaptation,to,zero-shot translation,easy adaptation to zero-shot translation,0.528962254524231
translation,54,114,results,performance gains,by,up to 26.5 bleu,performance gains by up to 26.5 bleu,0.5787885785102844
translation,54,114,results,lass,has,even improve,lass has even improve,0.6653499007225037
translation,54,114,results,even improve,has,zero-shot translation,even improve has zero-shot translation,0.5674270391464233
translation,54,114,results,results,observe,lass,results observe lass,0.4747473895549774
translation,54,115,results,results,on,iwslt,results on iwslt,0.5376288294792175
translation,54,117,results,multilingual baseline,on,all language pairs,multilingual baseline on all language pairs,0.5219277143478394
translation,54,117,results,lass,has,consistently outperforms,lass has consistently outperforms,0.5682260394096375
translation,54,117,results,consistently outperforms,has,multilingual baseline,consistently outperforms has multilingual baseline,0.6013474464416504
translation,54,117,results,results,has,lass,results has lass,0.42956456542015076
translation,54,121,results,lass,obtains,consistent gains,lass obtains consistent gains,0.6669884920120239
translation,54,121,results,consistent gains,over,multilingual baseline,consistent gains over multilingual baseline,0.6982503533363342
translation,54,121,results,multilingual baseline,on,wmt,multilingual baseline on wmt,0.5409702658653259
translation,54,121,results,wmt,for,transformer - base and transformer - big,wmt for transformer - base and transformer - big,0.6487719416618347
translation,54,121,results,results,has,lass,results has lass,0.42956456542015076
translation,54,122,results,transformer - base,for,transformer - big,transformer - base for transformer - big,0.6833414435386658
translation,54,122,results,lass,achieves,average improvement,lass achieves average improvement,0.6903820037841797
translation,54,122,results,lass,achieves,0.6 bleu improvement,lass achieves 0.6 bleu improvement,0.6541861891746521
translation,54,122,results,lass,for,transformer - big,lass for transformer - big,0.6816210746765137
translation,54,122,results,lass,obtains,0.6 bleu improvement,lass obtains 0.6 bleu improvement,0.5516701936721802
translation,54,122,results,average improvement,of,1.2 bleu,average improvement of 1.2 bleu,0.5167654752731323
translation,54,122,results,1.2 bleu,on,36 language pairs,1.2 bleu on 36 language pairs,0.4959104359149933
translation,54,122,results,36 language pairs,over,baseline,36 language pairs over baseline,0.685048520565033
translation,54,122,results,lass,obtains,0.6 bleu improvement,lass obtains 0.6 bleu improvement,0.5516701936721802
translation,54,122,results,transformer - base,has,lass,transformer - base has lass,0.6242316961288452
translation,54,122,results,transformer - base,has,lass,transformer - base has lass,0.6242316961288452
translation,54,122,results,transformer - big,has,lass,transformer - big has lass,0.6322482824325562
translation,54,122,results,results,For,transformer - base,results For transformer - base,0.6232364773750305
translation,54,122,results,results,for,transformer - big,results for transformer - big,0.6481248736381531
translation,54,122,results,results,for,lass,results for lass,0.5292773842811584
translation,54,125,results,bleu and wr gains,obtained in,transformer - base,bleu and wr gains obtained in transformer - base,0.7078715562820435
translation,54,125,results,transformer - base,larger than,transformer -large,transformer - base larger than transformer -large,0.7845665216445923
translation,54,128,results,underperforms,by,large margin,underperforms by large margin,0.5930063724517822
translation,54,128,results,random,has,underperforms,random has underperforms,0.6447457671165466
translation,54,128,results,underperforms,has,baseline,underperforms has baseline,0.6125874519348145
translation,54,128,results,results,has,random,results has random,0.5395680665969849
translation,54,132,results,boost,obtaining,performance gains,boost obtaining performance gains,0.6843734383583069
translation,54,132,results,performance,in,zero-shot translation scenario,performance in zero-shot translation scenario,0.5285859107971191
translation,54,132,results,performance,obtaining,performance gains,performance obtaining performance gains,0.685111403465271
translation,54,132,results,performance gains,by,up to 26.5 bleu,performance gains by up to 26.5 bleu,0.5787885785102844
translation,54,132,results,boost,has,performance,boost has performance,0.5791411995887756
translation,54,132,results,results,show,lass,results show lass,0.5044913291931152
translation,54,142,results,multilingual baseline model,along with,training steps,multilingual baseline model along with training steps,0.6220651268959045
translation,54,142,results,lass,has,consistently outperforms,lass has consistently outperforms,0.5682260394096375
translation,54,142,results,consistently outperforms,has,multilingual baseline model,consistently outperforms has multilingual baseline model,0.5634896159172058
translation,54,142,results,results,observe,lass,results observe lass,0.4747473895549774
translation,54,143,results,lass,reaches,bilingual model performance,lass reaches bilingual model performance,0.6817867159843445
translation,54,143,results,bilingual model performance,with,fewer steps,bilingual model performance with fewer steps,0.6159178018569946
translation,54,143,results,results,has,lass,results has lass,0.42956456542015076
translation,54,144,results,degradation,of,other language pairs,degradation of other language pairs,0.6199666261672974
translation,54,144,results,degradation,is,much smoother,degradation is much smoother,0.6087915897369385
translation,54,144,results,other language pairs,is,much smoother,other language pairs is much smoother,0.5755126476287842
translation,54,144,results,much smoother,than,baseline,much smoother than baseline,0.6035212278366089
translation,54,145,results,hardly drops,on,other language pairs,hardly drops on other language pairs,0.554964005947113
translation,54,145,results,dramatically drops,by,large margin,dramatically drops by large margin,0.6550705432891846
translation,54,145,results,bilingual baseline performance,has,lass,bilingual baseline performance has lass,0.5851889252662659
translation,54,145,results,lass,has,hardly drops,lass has hardly drops,0.6467317938804626
translation,54,145,results,multilingual baseline model,has,dramatically drops,multilingual baseline model has dramatically drops,0.5718733072280884
translation,54,145,results,results,When reaching,bilingual baseline performance,results When reaching bilingual baseline performance,0.7041570544242859
translation,54,159,results,lass,obtains,consistent gains,lass obtains consistent gains,0.6669884920120239
translation,54,159,results,consistent gains,over,baselines,consistent gains over baselines,0.7230672240257263
translation,54,159,results,baselines,in,all language pairs,baselines in all language pairs,0.4821646213531494
translation,54,159,results,masks,has,lass,masks has lass,0.6140236258506775
translation,54,160,results,outperforms,by,26.5 bleu,outperforms by 26.5 bleu,0.5743364095687866
translation,54,160,results,baseline,by,26.5 bleu,baseline by 26.5 bleu,0.5140770077705383
translation,54,160,results,baseline,reaching,32 bleu,baseline reaching 32 bleu,0.6442793607711792
translation,54,160,results,fr?zh,has,lass,fr?zh has lass,0.6948916912078857
translation,54,160,results,lass,has,outperforms,lass has outperforms,0.6156740784645081
translation,54,160,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,54,160,results,results,for,fr?zh,results for fr?zh,0.6897287368774414
translation,54,163,results,lass,translating into,right target language,lass translating into right target language,0.6711404919624329
translation,54,163,results,significantly alleviates,translating into,right target language,significantly alleviates translating into right target language,0.679954469203949
translation,54,163,results,lass,has,significantly alleviates,lass has significantly alleviates,0.5892465114593506
translation,54,163,results,significantly alleviates,has,off-target issue,significantly alleviates has off-target issue,0.5853169560432434
translation,54,163,results,results,has,lass,results has lass,0.42956456542015076
translation,55,14,experimental-setup,universal encoder-decoder architecture,shares,parameters,universal encoder-decoder architecture shares parameters,0.6938388347625732
translation,55,14,experimental-setup,parameters,across,all languages,parameters across all languages,0.6722486615180969
translation,55,84,experimental-setup,flores - 101 fine-tuning experiments,training on,48 nvidia p40 gpus,flores - 101 fine-tuning experiments training on 48 nvidia p40 gpus,0.6604259014129639
translation,55,85,experimental-setup,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,55,85,experimental-setup,optimizer,with,"? 1 = 0.9 , ? 2 = 0.98 , and = 10 ?9","optimizer with ? 1 = 0.9 , ? 2 = 0.98 , and = 10 ?9",0.6289250254631042
translation,55,85,experimental-setup,experimental setup,apply,"adam ( kingma and ba , 2015 )","experimental setup apply adam ( kingma and ba , 2015 )",0.5883913636207581
translation,55,86,experimental-setup,label smoothing,to,0.2,label smoothing to 0.2,0.5479223728179932
translation,55,86,experimental-setup,dropout rate,to,0.3,dropout rate to 0.3,0.5253456830978394
translation,55,86,experimental-setup,experimental setup,set,label smoothing,experimental setup set label smoothing,0.5923986434936523
translation,55,86,experimental-setup,experimental setup,set,dropout rate,experimental setup set dropout rate,0.62116938829422
translation,55,87,experimental-setup,initial learning rate,set to,5e - 4,initial learning rate set to 5e - 4,0.7290231585502625
translation,55,87,experimental-setup,5e - 4,varied under,warm - up strategy,5e - 4 varied under warm - up strategy,0.7427377104759216
translation,55,87,experimental-setup,warm - up strategy,with,4000 steps,warm - up strategy with 4000 steps,0.6500464677810669
translation,55,87,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,55,88,experimental-setup,batch size,is,4096 tokens,batch size is 4096 tokens,0.5487428903579712
translation,55,88,experimental-setup,4096 tokens,per,gpu,4096 tokens per gpu,0.6156757473945618
translation,55,88,experimental-setup,training,has,batch size,training has batch size,0.5618287920951843
translation,55,88,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,55,89,experimental-setup,fine- tuning flores - 101,apply,temperature sampling strategy,fine- tuning flores - 101 apply temperature sampling strategy,0.6213037967681885
translation,55,89,experimental-setup,temperature sampling strategy,with,sampling temperature t = 1.5,temperature sampling strategy with sampling temperature t = 1.5,0.6371473670005798
translation,55,89,experimental-setup,experimental setup,For,fine- tuning flores - 101,experimental setup For fine- tuning flores - 101,0.6184626221656799
translation,55,90,experimental-setup,inference,decode,beam search,inference decode beam search,0.741058349609375
translation,55,90,experimental-setup,inference,set,beam size,inference set beam size,0.6990290880203247
translation,55,90,experimental-setup,beam size,to,4,beam size to 4,0.6383150219917297
translation,55,90,experimental-setup,4,for,all language pairs,4 for all language pairs,0.578584611415863
translation,55,90,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,55,16,experiments,"base and deeper transformer ( vaswani et al. , 2017 ) architectures",to get,reliable baselines,"base and deeper transformer ( vaswani et al. , 2017 ) architectures to get reliable baselines",0.6099549531936646
translation,55,16,experiments,"base and deeper transformer ( vaswani et al. , 2017 ) architectures",fine - tune,"pre-training model flores - 101 ( goyal et al. , 2021 )","base and deeper transformer ( vaswani et al. , 2017 ) architectures fine - tune pre-training model flores - 101 ( goyal et al. , 2021 )",0.66953444480896
translation,55,16,experiments,"pre-training model flores - 101 ( goyal et al. , 2021 )",to further improve,baseline system,"pre-training model flores - 101 ( goyal et al. , 2021 ) to further improve baseline system",0.6190285086631775
translation,55,34,model,transformer architecture,implemented in,tentrans,transformer architecture implemented in tentrans,0.7523776888847351
translation,55,34,model,tentrans,has,unified end-to - end multilingual and multi-task training platform,tentrans has unified end-to - end multilingual and multi-task training platform,0.5378625988960266
translation,55,34,model,model,based on,transformer architecture,model based on transformer architecture,0.6872271299362183
translation,55,35,model,model,following,transformer base setup,model following transformer base setup,0.6488604545593262
translation,55,35,model,transformer base setup,to jointly training,all language pairs,transformer base setup to jointly training all language pairs,0.7340704202651978
translation,55,35,model,model,train,model,model train model,0.6881781220436096
translation,55,101,model,gradual fine-tuning ( gradual ft ),enables,model,gradual fine-tuning ( gradual ft ) enables model,0.6689838171005249
translation,55,101,model,effective,enables,model,effective enables model,0.7223498225212097
translation,55,101,model,model,has,gradual fine-tuning ( gradual ft ),model has gradual fine-tuning ( gradual ft ),0.581116259098053
translation,55,7,results,effective,during,adapting domain,effective during adapting domain,0.699361264705658
translation,55,7,results,knowledge distillation,brings,slight performance improvement,knowledge distillation brings slight performance improvement,0.6037293076515198
translation,55,7,results,forward / backtranslation,has,significantly improves,forward / backtranslation has significantly improves,0.5940334796905518
translation,55,7,results,significantly improves,has,translation results,significantly improves has translation results,0.5628255605697632
translation,55,7,results,results,find that,forward / backtranslation,results find that forward / backtranslation,0.6499568819999695
translation,55,53,results,domain classifier recognition accuracy,achieved at,93.97 %,domain classifier recognition accuracy achieved at 93.97 %,0.6253626942634583
translation,55,53,results,same domain test set,has,domain classifier recognition accuracy,same domain test set has domain classifier recognition accuracy,0.5698567032814026
translation,55,53,results,results,At,same domain test set,results At same domain test set,0.5178567171096802
translation,55,96,results,performed better,than,baseline system,performed better than baseline system,0.5754040479660034
translation,55,96,results,baseline system,has,with fine-tuning flores - 101,baseline system has with fine-tuning flores - 101,0.6182203888893127
translation,55,96,results,with fine-tuning flores - 101,has,performed better,with fine-tuning flores - 101 has performed better,0.6380658149719238
translation,55,96,results,baseline system,has,with no pre-training model,baseline system has with no pre-training model,0.5627911686897278
translation,55,96,results,with no pre-training model,has,24.23 vs. 22.25,with no pre-training model has 24.23 vs. 22.25,0.5578671097755432
translation,55,96,results,results,found that,baseline system,results found that baseline system,0.6799426674842834
translation,55,97,results,forward -translation and back-translation ( f&b ),greatly improved,translation performance,forward -translation and back-translation ( f&b ) greatly improved translation performance,0.7004621624946594
translation,55,97,results,translation performance,in,tentrans ( 25.05 vs. 22.25 ) and frameworks,translation performance in tentrans ( 25.05 vs. 22.25 ) and frameworks,0.5367142558097839
translation,55,97,results,results,has,forward -translation and back-translation ( f&b ),results has forward -translation and back-translation ( f&b ),0.5226197242736816
translation,55,98,results,deep transformer,with,24 encoder layers,deep transformer with 24 encoder layers,0.6257071495056152
translation,55,98,results,deep transformer,improves,translation results,deep transformer improves translation results,0.6212819218635559
translation,55,98,results,still not as high,as,fine-tuning flores - 101 systems,still not as high as fine-tuning flores - 101 systems,0.63627028465271
translation,56,113,ablation-analysis,significant ( 15 - 16 % ) decrease,of,correlation,significant ( 15 - 16 % ) decrease of correlation,0.5802479982376099
translation,56,113,ablation-analysis,correlation,between,contextualized and decontextualized versions of wmd,correlation between contextualized and decontextualized versions of wmd,0.5980238914489746
translation,56,113,ablation-analysis,contextualized and decontextualized versions of wmd,all cases of,zh-en language pair,contextualized and decontextualized versions of wmd all cases of zh-en language pair,0.6788269877433777
translation,56,113,ablation-analysis,ablation analysis,observe,significant ( 15 - 16 % ) decrease,ablation analysis observe significant ( 15 - 16 % ) decrease,0.611717164516449
translation,56,114,ablation-analysis,differs,in,en-de pair,differs in en-de pair,0.6909269094467163
translation,56,114,ablation-analysis,en-de pair,for,reference - based case,en-de pair for reference - based case,0.6448971033096313
translation,56,114,ablation-analysis,correlation,of,decontextualized version of wmd,correlation of decontextualized version of wmd,0.5808084011077881
translation,56,114,ablation-analysis,decontextualized version of wmd,superior by,7 %,decontextualized version of wmd superior by 7 %,0.7198206782341003
translation,56,114,ablation-analysis,en-de pair,has,correlation,en-de pair has correlation,0.6300754547119141
translation,56,114,ablation-analysis,reference - based case,has,correlation,reference - based case has correlation,0.5770885348320007
translation,56,127,ablation-analysis,ensemble of source- based metrics,shows,significant drops,ensemble of source- based metrics shows significant drops,0.6751376390457153
translation,56,127,ablation-analysis,significant drops,in,zh-en pair,significant drops in zh-en pair,0.6022056937217712
translation,56,127,ablation-analysis,zh-en pair,after removing,prism,zh-en pair after removing prism,0.7337608933448792
translation,56,127,ablation-analysis,zh-en pair,after removing,wmd - contextual,zh-en pair after removing wmd - contextual,0.7296450138092041
translation,56,127,ablation-analysis,ablation analysis,has,ensemble of source- based metrics,ablation analysis has ensemble of source- based metrics,0.5258082747459412
translation,56,150,ablation-analysis,wmd,by,8 %,wmd by 8 %,0.6141219139099121
translation,56,150,ablation-analysis,ablation analysis,has,decontextualization,ablation analysis has decontextualization,0.5260251760482788
translation,56,154,baselines,better results,than,bleu,better results than bleu,0.5607202649116516
translation,56,154,baselines,better results,than,meteor,better results than meteor,0.6042547821998596
translation,56,65,model,term frequencies,discounted by,inverse document frequencies,term frequencies discounted by inverse document frequencies,0.703518807888031
translation,56,65,model,term frequencies,as,weights,term frequencies as weights,0.5382217764854431
translation,56,65,model,term frequencies,construct,term similarity matrix,term frequencies construct term similarity matrix,0.6827729940414429
translation,56,65,model,weights,in,bow vectors,weights in bow vectors,0.4979060888290405
translation,56,65,model,weights,in,nfx smart weighting scheme,weights in nfx smart weighting scheme,0.5209473967552185
translation,56,65,model,term similarity matrix,in,decreasing order,term similarity matrix in decreasing order,0.4934743642807007
translation,56,65,model,decreasing order,of,inverse document frequencies,decreasing order of inverse document frequencies,0.6142997145652771
translation,56,65,model,model,use,term frequencies,model use term frequencies,0.705474317073822
translation,56,65,model,model,construct,term similarity matrix,model construct term similarity matrix,0.7043492197990417
translation,56,151,results,wmd - decontextualized and wmd - decontextualized - tfidf,reached,considerable improvement,wmd - decontextualized and wmd - decontextualized - tfidf reached considerable improvement,0.7346339821815491
translation,56,151,results,considerable improvement,of,16 - 18 %,considerable improvement of 16 - 18 %,0.599213182926178
translation,56,151,results,considerable improvement,compared to,wmd and wmd - tfidf,considerable improvement compared to wmd and wmd - tfidf,0.6781363487243652
translation,56,151,results,16 - 18 %,compared to,wmd and wmd - tfidf,16 - 18 % compared to wmd and wmd - tfidf,0.7076635956764221
translation,56,151,results,wmd and wmd - tfidf,using,fasttext embeddings,wmd and wmd - tfidf using fasttext embeddings,0.6624361872673035
translation,56,151,results,results,has,wmd - decontextualized and wmd - decontextualized - tfidf,results has wmd - decontextualized and wmd - decontextualized - tfidf,0.5327149629592896
translation,57,29,experimental-setup,opennmt toolkit,version of,byte pair encoding ( bpe ),opennmt toolkit version of byte pair encoding ( bpe ),0.6134448647499084
translation,57,29,experimental-setup,opennmt toolkit,for,subword segmentation,opennmt toolkit for subword segmentation,0.5828605890274048
translation,57,29,experimental-setup,byte pair encoding ( bpe ),for,subword segmentation,byte pair encoding ( bpe ) for subword segmentation,0.6037676930427551
translation,57,29,experimental-setup,experimental setup,use,opennmt toolkit,experimental setup use opennmt toolkit,0.6049007177352905
translation,57,29,experimental-setup,experimental setup,for,subword segmentation,experimental setup for subword segmentation,0.5389828681945801
translation,57,32,experimental-setup,iterative back -translation,for,english -russian system,iterative back -translation for english -russian system,0.6363121271133423
translation,57,32,experimental-setup,experimental setup,use,iterative back -translation,experimental setup use iterative back -translation,0.5778825283050537
translation,57,38,experimental-setup,both final models,trained for,approximately 1.2 m steps,both final models trained for approximately 1.2 m steps,0.740942120552063
translation,57,38,experimental-setup,approximately 1.2 m steps,on,two rtx 2080 gpus,approximately 1.2 m steps on two rtx 2080 gpus,0.5127039551734924
translation,57,38,experimental-setup,experimental setup,has,both final models,experimental setup has both final models,0.5551761984825134
translation,57,63,experimental-setup,source side,of,training data,source side of training data,0.5775406956672668
translation,57,63,experimental-setup,training data,with,language tokens,training data with language tokens,0.5733910202980042
translation,57,63,experimental-setup,experimental setup,tag,source side,experimental setup tag source side,0.6234674453735352
translation,57,64,experimental-setup,approximately 1 m steps,on,two rtx 2080 gpus,approximately 1 m steps on two rtx 2080 gpus,0.5226133465766907
translation,57,64,experimental-setup,experimental setup,trained for,approximately 1 m steps,experimental setup trained for approximately 1 m steps,0.6783860921859741
translation,57,20,model,technology promt smart neural dictionary ( smartnd ),aimed at handling,terminology translation,technology promt smart neural dictionary ( smartnd ) aimed at handling terminology translation,0.6934239864349365
translation,57,33,model,intermediate models,trained using,"sentencepiece ( kudo and richardson , 2018 )","intermediate models trained using sentencepiece ( kudo and richardson , 2018 )",0.7184474468231201
translation,57,33,model,"sentencepiece ( kudo and richardson , 2018 )",for,subword segmentation,"sentencepiece ( kudo and richardson , 2018 ) for subword segmentation",0.585929274559021
translation,57,33,model,model,has,intermediate models,model has intermediate models,0.5788748264312744
translation,57,125,results,baseline,according to,terminology - related metrics,baseline according to terminology - related metrics,0.6423816084861755
translation,57,125,results,see that both our approaches,has,clearly outperform,see that both our approaches has clearly outperform,0.5781267285346985
translation,57,125,results,clearly outperform,has,baseline,clearly outperform has baseline,0.6021165251731873
translation,57,130,results,minor decrease,in,exact match scores,minor decrease in exact match scores,0.5355746746063232
translation,57,130,results,exact match scores,for,tuned models,exact match scores for tuned models,0.602688729763031
translation,57,130,results,tuned models,with,smartnd technology,tuned models with smartnd technology,0.7156552672386169
translation,57,130,results,results,observe,minor decrease,results observe minor decrease,0.6672415733337402
translation,57,131,results,ranked last,on,test set,ranked last on test set,0.5686671137809753
translation,57,131,results,test set,according to,exact match and window overlap metrics,test set according to exact match and window overlap metrics,0.6266082525253296
translation,57,131,results,results,has,final english -russian tuned system,results has final english -russian tuned system,0.5734091997146606
translation,57,149,results,better baseline models,receive,better scores,better baseline models receive better scores,0.5871946215629578
translation,57,149,results,better scores,according to,all metrics,better scores according to all metrics,0.5993119478225708
translation,57,151,results,generally higher scores,on,english -russian direction,generally higher scores on english -russian direction,0.4908147156238556
translation,57,151,results,english -russian direction,compared to,english - french direction,english -russian direction compared to english - french direction,0.6964983940124512
translation,57,151,results,results,obtain,generally higher scores,results obtain generally higher scores,0.5613706707954407
translation,58,54,experimental-setup,"transformer ( vaswani et al. , 2017 )",follow,implementation of bert,"transformer ( vaswani et al. , 2017 ) follow implementation of bert",0.6067981123924255
translation,58,54,experimental-setup,experimental setup,use,"transformer ( vaswani et al. , 2017 )","experimental setup use transformer ( vaswani et al. , 2017 )",0.5476985573768616
translation,58,57,experimental-setup,dropout,with,probability 0.1,dropout with probability 0.1,0.6266955733299255
translation,58,57,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,58,58,experimental-setup,lazyadam,as,optimizer,lazyadam as optimizer,0.5499048829078674
translation,58,58,experimental-setup,experimental setup,use,lazyadam,experimental setup use lazyadam,0.5820938944816589
translation,58,59,experimental-setup,learning rate,follows,inverse square root decay,learning rate follows inverse square root decay,0.6902352571487427
translation,58,59,experimental-setup,warms up,for,"16,000 steps","warms up for 16,000 steps",0.6438076496124268
translation,58,59,experimental-setup,learning rate,has,warms up,learning rate has warms up,0.5685316920280457
translation,58,59,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,58,60,experimental-setup,peak learning rate,is,5 ? 10 ?4,peak learning rate is 5 ? 10 ?4,0.6184989213943481
translation,58,60,experimental-setup,peak learning rate,is,1 ? 10 ?4,peak learning rate is 1 ? 10 ?4,0.6077502369880676
translation,58,60,experimental-setup,5 ? 10 ?4,for,parent translation models,5 ? 10 ?4 for parent translation models,0.6606162190437317
translation,58,60,experimental-setup,5 ? 10 ?4,for,child translation models,5 ? 10 ?4 for child translation models,0.6558771729469299
translation,58,60,experimental-setup,1 ? 10 ?4,for,child translation models,1 ? 10 ?4 for child translation models,0.6453253626823425
translation,58,60,experimental-setup,experimental setup,has,peak learning rate,experimental setup has peak learning rate,0.5030112862586975
translation,58,61,experimental-setup,early stopping,occurs when,validation bleu,early stopping occurs when validation bleu,0.5877267718315125
translation,58,61,experimental-setup,does not improve,for,10 checkpoints,does not improve for 10 checkpoints,0.6448949575424194
translation,58,61,experimental-setup,validation bleu,has,does not improve,validation bleu has does not improve,0.5963712334632874
translation,58,61,experimental-setup,experimental setup,has,early stopping,experimental setup has early stopping,0.5249444842338562
translation,58,62,experimental-setup,checkpoint frequency,to,"2,000 updates","checkpoint frequency to 2,000 updates",0.573722243309021
translation,58,62,experimental-setup,checkpoint frequency,to,"1,000 updates","checkpoint frequency to 1,000 updates",0.5720921158790588
translation,58,62,experimental-setup,"2,000 updates",for,parent translation models,"2,000 updates for parent translation models",0.6091763377189636
translation,58,62,experimental-setup,"2,000 updates",for,child translation models,"2,000 updates for child translation models",0.6031379699707031
translation,58,62,experimental-setup,"1,000 updates",for,child translation models,"1,000 updates for child translation models",0.6027448773384094
translation,58,62,experimental-setup,experimental setup,set,checkpoint frequency,experimental setup set checkpoint frequency,0.630342960357666
translation,58,63,experimental-setup,batch size,is,"6,144 tokens","batch size is 6,144 tokens",0.5524740815162659
translation,58,63,experimental-setup,batch size,is,8 nvidia v100 gpus,batch size is 8 nvidia v100 gpus,0.5265567302703857
translation,58,63,experimental-setup,"6,144 tokens",per,gpu,"6,144 tokens per gpu",0.6515164375305176
translation,58,63,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,58,65,experimental-setup,selected finetuning,use,stochastic gradient descent,selected finetuning use stochastic gradient descent,0.6024068593978882
translation,58,65,experimental-setup,selected finetuning,use,learning rate,selected finetuning use learning rate,0.6498993635177612
translation,58,65,experimental-setup,stochastic gradient descent,as,optimizer,stochastic gradient descent as optimizer,0.5452165603637695
translation,58,65,experimental-setup,experimental setup,For,selected finetuning,experimental setup For selected finetuning,0.641721248626709
translation,58,66,experimental-setup,checkpoint,every,100 updates,checkpoint every 100 updates,0.7390131950378418
translation,58,9,model,model,describe,noahnmt system,model describe noahnmt system,0.6520317792892456
translation,58,48,model,bpe codes and vocabularies,learned on,each language 's monolingual data,bpe codes and vocabularies learned on each language 's monolingual data,0.7210069894790649
translation,58,48,model,bpe codes and vocabularies,to,segment,bpe codes and vocabularies to segment,0.607086181640625
translation,58,48,model,segment,has,parallel data,segment has parallel data,0.6033567786216736
translation,58,48,model,model,has,bpe codes and vocabularies,model has bpe codes and vocabularies,0.5773306488990784
translation,58,85,results,transformer big,brings,consistent improvements,transformer big brings consistent improvements,0.6759092211723328
translation,58,85,results,results,show,transformer big,results show transformer big,0.7230536937713623
translation,58,92,results,iterative back -translation,brings,larger improvements,iterative back -translation brings larger improvements,0.621919572353363
translation,58,92,results,larger improvements,for,chv?ru and hsb?de,larger improvements for chv?ru and hsb?de,0.6674185991287231
translation,58,92,results,larger improvements,for,de?hsb,larger improvements for de?hsb,0.685043454170227
translation,58,92,results,larger improvements,than,de?hsb,larger improvements than de?hsb,0.6619222164154053
translation,58,92,results,chv?ru and hsb?de,than,ru?chv,chv?ru and hsb?de than ru?chv,0.6393435001373291
translation,58,92,results,chv?ru and hsb?de,than,de?hsb,chv?ru and hsb?de than de?hsb,0.6589050889015198
translation,58,99,results,ensemble,gives,bleu improvements,ensemble gives bleu improvements,0.5944923162460327
translation,58,99,results,bleu improvements,of,about 0.8,bleu improvements of about 0.8,0.5763080716133118
translation,58,99,results,results,demonstrate,ensemble,results demonstrate ensemble,0.5950490832328796
translation,59,118,experimental-setup,en?fr and cs? de,pre-tokenize,data,en?fr and cs? de pre-tokenize data,0.7574048638343811
translation,59,118,experimental-setup,data,using,moses toolkit,data using moses toolkit,0.702335000038147
translation,59,118,experimental-setup,experimental setup,For,en?fr and cs? de,experimental setup For en?fr and cs? de,0.6242565512657166
translation,59,118,experimental-setup,experimental setup,pre-tokenize,data,experimental setup pre-tokenize data,0.7044857740402222
translation,59,122,experimental-setup,data,using,jieba tokenizer,data using jieba tokenizer,0.6610561013221741
translation,59,122,experimental-setup,subword - nmt,to train,bpe,subword - nmt to train bpe,0.7037268280982971
translation,59,122,experimental-setup,subword - nmt,build,separated vocabularies,subword - nmt build separated vocabularies,0.7016866207122803
translation,59,122,experimental-setup,bpe,on,combined chinese and english corpus,bpe on combined chinese and english corpus,0.4957441985607147
translation,59,122,experimental-setup,bpe,build,separated vocabularies,bpe build separated vocabularies,0.6939101815223694
translation,59,122,experimental-setup,experimental setup,pre-tokenize,data,experimental setup pre-tokenize data,0.7044857740402222
translation,59,122,experimental-setup,experimental setup,use,subword - nmt,experimental setup use subword - nmt,0.5956558585166931
translation,59,123,experimental-setup,final vocabulary size,is,44 k,final vocabulary size is 44 k,0.5791821479797363
translation,59,123,experimental-setup,final vocabulary size,is,32 k,final vocabulary size is 32 k,0.5672162771224976
translation,59,123,experimental-setup,44 k,for,chinese,44 k for chinese,0.6993288397789001
translation,59,123,experimental-setup,32 k,for,english,32 k for english,0.6870679259300232
translation,59,123,experimental-setup,experimental setup,has,final vocabulary size,experimental setup has final vocabulary size,0.543472945690155
translation,59,64,experiments,general- domain language model,trained with,samples,general- domain language model trained with samples,0.7483448386192322
translation,59,64,experiments,chosen randomly,from,en- fr parallel corpus,chosen randomly from en- fr parallel corpus,0.5912723541259766
translation,59,64,experiments,samples,has,chosen randomly,samples has chosen randomly,0.6069512367248535
translation,59,65,experiments,"kenlm ( heafield , 2011 )",to train,5 - gram language models,"kenlm ( heafield , 2011 ) to train 5 - gram language models",0.6361975073814392
translation,59,65,experiments,5 - gram language models,with,modified kneser - ney smoothing,5 - gram language models with modified kneser - ney smoothing,0.59483802318573
translation,59,119,experiments,sentencepiece,to learn,joint byte pair encoding ( bpe ),sentencepiece to learn joint byte pair encoding ( bpe ),0.5826969146728516
translation,59,119,experiments,joint byte pair encoding ( bpe ),with,vocabulary size,joint byte pair encoding ( bpe ) with vocabulary size,0.6122303605079651
translation,59,119,experiments,joint byte pair encoding ( bpe ),with,32 k ( cs? de ),joint byte pair encoding ( bpe ) with 32 k ( cs? de ),0.6216978430747986
translation,59,119,experiments,vocabulary size,has,40 k ( en? fr ),vocabulary size has 40 k ( en? fr ),0.5858851075172424
translation,59,143,experiments,english ? korean back-translation,yields,performance gains,english ? korean back-translation yields performance gains,0.7296735048294067
translation,59,143,experiments,performance gains,across,all metrics,performance gains across all metrics,0.6627905368804932
translation,59,143,experiments,considerable improvements,particularly in,bleu and 1 - term,considerable improvements particularly in bleu and 1 - term,0.6308709383010864
translation,59,162,experiments,our submission,ranks,1st,our submission ranks 1st,0.6846868395805359
translation,59,162,experiments,our submission,ranks,1st- 2nd,our submission ranks 1st- 2nd,0.7119578719139099
translation,59,162,experiments,1st,in terms of,comet,1st in terms of comet,0.7485454678535461
translation,59,162,experiments,1st- 2nd,for,exact match accuracy,1st- 2nd for exact match accuracy,0.6110102534294128
translation,59,162,experiments,exact match accuracy,out of,2 submissions,exact match accuracy out of 2 submissions,0.5855857133865356
translation,59,162,experiments,cs? de,has,our submission,cs? de has our submission,0.6280018091201782
translation,59,70,model,model,trained with,solely the parallel data,model trained with solely the parallel data,0.7544041872024536
translation,59,72,model,na?ve approach,to leverage,term dictionary,na?ve approach to leverage term dictionary,0.6874006986618042
translation,59,72,model,term dictionary,including,provided terms,term dictionary including provided terms,0.6537467241287231
translation,59,72,model,term dictionary,including,additional parallel data,term dictionary including additional parallel data,0.7445202469825745
translation,59,72,model,provided terms,as,additional parallel data,provided terms as additional parallel data,0.550138533115387
translation,59,72,model,additional parallel data,to train,model,additional parallel data to train model,0.6941781044006348
translation,59,72,model,model,take,na?ve approach,model take na?ve approach,0.7245132327079773
translation,59,134,results,tla model,shows,deteriorated performance,tla model shows deteriorated performance,0.6712980270385742
translation,59,134,results,deteriorated performance,on,all other metrics,deteriorated performance on all other metrics,0.4771604537963867
translation,59,134,results,all other metrics,compared to,baseline,all other metrics compared to baseline,0.6341859698295593
translation,59,134,results,tla model,has,improves,tla model has improves,0.6037803888320923
translation,59,134,results,improves,has,exact match accuracy,improves has exact match accuracy,0.5673139691352844
translation,59,134,results,results,has,tla model,results has tla model,0.5083680748939514
translation,59,136,results,eta fine- tuned model,recovers,performance loss,eta fine- tuned model recovers performance loss,0.6572430729866028
translation,59,136,results,eta fine- tuned model,recovers,boosts,eta fine- tuned model recovers boosts,0.7451440095901489
translation,59,136,results,boosts,compared to,baseline and the tla model,boosts compared to baseline and the tla model,0.6656529307365417
translation,59,136,results,1 - term score,compared to,baseline and the tla model,1 - term score compared to baseline and the tla model,0.6222581267356873
translation,59,136,results,same testannotation condition,has,eta fine- tuned model,same testannotation condition has eta fine- tuned model,0.5822381377220154
translation,59,136,results,boosts,has,bleu score,boosts has bleu score,0.5505126118659973
translation,59,136,results,results,under,same testannotation condition,results under same testannotation condition,0.6235445141792297
translation,59,137,results,tla + eta,fine- tune,outperforms,tla + eta fine- tune outperforms,0.737959623336792
translation,59,137,results,baseline,by,"0.33 points , 4.65 % , and 0.24 %","baseline by 0.33 points , 4.65 % , and 0.24 %",0.5444211959838867
translation,59,137,results,"0.33 points , 4.65 % , and 0.24 %",on,"bleu , exact match , and 1 - term","0.33 points , 4.65 % , and 0.24 % on bleu , exact match , and 1 - term",0.49497655034065247
translation,59,137,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,59,137,results,results,has,tla + eta,results has tla + eta,0.51488196849823
translation,59,139,results,indistinguishable,from,original tla + eta fine-tune,indistinguishable from original tla + eta fine-tune,0.5779057145118713
translation,59,139,results,fine-tuned,with,data,fine-tuned with data,0.6635838747024536
translation,59,139,results,data,from,bi-text and mono-text,data from bi-text and mono-text,0.5914372205734253
translation,59,139,results,results,has,results,results has results,0.48582205176353455
translation,59,142,results,term pairs explicitly,to,training,term pairs explicitly to training,0.556195080280304
translation,59,142,results,term consistency,compared to,baseline,term consistency compared to baseline,0.6593016386032104
translation,59,142,results,+ 2.29 % 1 - term ),compared to,baseline,+ 2.29 % 1 - term ) compared to baseline,0.6797553300857544
translation,59,142,results,general translation performance,has,+ 0.73 bleu ),general translation performance has + 0.73 bleu ),0.5481925010681152
translation,59,142,results,term consistency,has,+ 2.29 % 1 - term ),term consistency has + 2.29 % 1 - term ),0.5625516176223755
translation,59,145,results,ensemble model,demonstrates,best performance,ensemble model demonstrates best performance,0.6520289182662964
translation,59,145,results,best performance,across,all metrics,best performance across all metrics,0.6618552207946777
translation,59,145,results,best performance,raising,bleu score,best performance raising bleu score,0.5674679279327393
translation,59,145,results,best performance,raising,exact match accuracy,best performance raising exact match accuracy,0.6617689728736877
translation,59,145,results,best performance,raising,window overlap,best performance raising window overlap,0.6873059868812561
translation,59,145,results,best performance,raising,1 - term,best performance raising 1 - term,0.7026840448379517
translation,59,145,results,bleu score,by,2.52 points,bleu score by 2.52 points,0.5470373630523682
translation,59,145,results,exact match accuracy,by,4.2 %,exact match accuracy by 4.2 %,0.5487668514251709
translation,59,145,results,window overlap,by,0.43 % and 0.54 %,window overlap by 0.43 % and 0.54 %,0.5816679000854492
translation,59,145,results,0.43 % and 0.54 %,for,windows 2 and 3,0.43 % and 0.54 % for windows 2 and 3,0.6437826156616211
translation,59,145,results,1 - term,by,4.88 points,1 - term by 4.88 points,0.5552306175231934
translation,59,145,results,results,has,ensemble model,results has ensemble model,0.5478004217147827
translation,59,147,results,explicit model,does not bring,significant gains,explicit model does not bring significant gains,0.6820465326309204
translation,59,147,results,significant gains,compared to,baseline model,significant gains compared to baseline model,0.6718330383300781
translation,59,147,results,results,discover,explicit model,results discover explicit model,0.5899574756622314
translation,59,150,results,our ensemble model,improves upon,baseline model,our ensemble model improves upon baseline model,0.6991525888442993
translation,59,150,results,baseline model,by,1.5 bleu points,baseline model by 1.5 bleu points,0.5290997624397278
translation,59,150,results,baseline model,by,1.6 % exact match accuracy,baseline model by 1.6 % exact match accuracy,0.5225699543952942
translation,59,150,results,baseline model,by,1.84,baseline model by 1.84,0.4992884397506714
translation,59,150,results,baseline model,by,1.1 points,baseline model by 1.1 points,0.56876540184021
translation,59,150,results,% and 1.74 % window overlap,for,window sizes,% and 1.74 % window overlap for window sizes,0.6184366345405579
translation,59,150,results,1.1 points,in,1 - term,1.1 points in 1 - term,0.5174545645713806
translation,59,150,results,1.84,has,% and 1.74 % window overlap,1.84 has % and 1.74 % window overlap,0.585125744342804
translation,59,150,results,window sizes,has,2 and 3,window sizes has 2 and 3,0.5997914671897888
translation,59,150,results,results,has,our ensemble model,results has our ensemble model,0.5463175177574158
translation,59,159,results,our system,performs,roughly comparable,our system performs roughly comparable,0.6080232262611389
translation,59,159,results,roughly comparable,to,best system,roughly comparable to best system,0.5439717173576355
translation,59,159,results,roughly comparable,ranking,4- 6th,roughly comparable ranking 4- 6th,0.7605607509613037
translation,59,159,results,exact match accuracy,has,our system,exact match accuracy has our system,0.5468887090682983
translation,59,159,results,results,According to,exact match accuracy,results According to exact match accuracy,0.6652347445487976
translation,59,160,results,en?zh,ranks,8th,en?zh ranks 8th,0.6942863464355469
translation,59,160,results,our system,ranks,8th,our system ranks 8th,0.7190789580345154
translation,59,160,results,8th,in,both metrics,8th in both metrics,0.5642194747924805
translation,59,160,results,en?zh,has,our system,en?zh has our system,0.6606005430221558
translation,59,160,results,results,For,en?zh,results For en?zh,0.685762882232666
translation,60,83,ablation-analysis,fine-tuned checkpoint roberta - large-xnli,further improves,performance,fine-tuned checkpoint roberta - large-xnli further improves performance,0.7541645765304565
translation,60,83,ablation-analysis,ablation analysis,using,fine-tuned checkpoint roberta - large-xnli,ablation analysis using fine-tuned checkpoint roberta - large-xnli,0.6854448318481445
translation,60,87,ablation-analysis,drops significantly,without,pre-training phase,drops significantly without pre-training phase,0.7809720635414124
translation,60,87,ablation-analysis,performance,has,drops significantly,performance has drops significantly,0.611855685710907
translation,60,87,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,60,97,ablation-analysis,experimental results,show,source-side information,experimental results show source-side information,0.6590868830680847
translation,60,97,ablation-analysis,does not help much,for,model training,does not help much for model training,0.5940774083137512
translation,60,97,ablation-analysis,low-resource scenarios,has,experimental results,low-resource scenarios has experimental results,0.5504574775695801
translation,60,97,ablation-analysis,source-side information,has,does not help much,source-side information has does not help much,0.5916151404380798
translation,60,97,ablation-analysis,ablation analysis,for,low-resource scenarios,ablation analysis for low-resource scenarios,0.6222215294837952
translation,60,101,ablation-analysis,scores,from,multiple models,scores from multiple models,0.6301947236061096
translation,60,101,ablation-analysis,scores,lead to,significant performance drop,scores lead to significant performance drop,0.6661808490753174
translation,60,101,ablation-analysis,multiple models,lead to,significant performance drop,multiple models lead to significant performance drop,0.7046745419502258
translation,60,101,ablation-analysis,ablation analysis,directly averaging,scores,ablation analysis directly averaging scores,0.7206153869628906
translation,60,63,experimental-setup,over-fitting,apply,dropout ratio,over-fitting apply dropout ratio,0.6471707224845886
translation,60,63,experimental-setup,dropout ratio,as,0.1,dropout ratio as 0.1,0.5334866046905518
translation,60,63,experimental-setup,experimental setup,To avoid,over-fitting,experimental setup To avoid over-fitting,0.6566965579986572
translation,60,64,experimental-setup,pretraining experiments,with,8 nvidia v100 gpus,pretraining experiments with 8 nvidia v100 gpus,0.5779669880867004
translation,60,64,experimental-setup,experimental setup,conduct,pretraining experiments,experimental setup conduct pretraining experiments,0.6674953103065491
translation,60,72,experimental-setup,fine-tuning,set,training steps and warm - up steps,fine-tuning set training steps and warm - up steps,0.6572840809822083
translation,60,72,experimental-setup,training steps and warm - up steps,as,20 k and 2k,training steps and warm - up steps as 20 k and 2k,0.586872398853302
translation,60,72,experimental-setup,experimental setup,During,fine-tuning,experimental setup During fine-tuning,0.7118422985076904
translation,60,74,experimental-setup,fine-tuning experiment,determine,batch size,fine-tuning experiment determine batch size,0.62738436460495
translation,60,74,experimental-setup,batch size,as,16,batch size as 16,0.5711418390274048
translation,60,74,experimental-setup,experimental setup,For,fine-tuning experiment,experimental setup For fine-tuning experiment,0.6124395728111267
translation,60,16,model,better consistency,between,model predictions and human assessments,better consistency between model predictions and human assessments,0.6582655310630798
translation,60,78,results,our robleurt,achieves,best performance,our robleurt achieves best performance,0.7151897549629211
translation,60,78,results,our robleurt,achieves,competitive results,our robleurt achieves competitive results,0.6718166470527649
translation,60,78,results,our robleurt,achieves,competitive results,our robleurt achieves competitive results,0.6718166470527649
translation,60,78,results,best performance,on,cs/de/ja/ru/zh/iu/pl/ta-to-en settings,best performance on cs/de/ja/ru/zh/iu/pl/ta-to-en settings,0.6070489883422852
translation,60,78,results,competitive results,on,km-to-en and ps-to-en,competitive results on km-to-en and ps-to-en,0.5693317651748657
translation,60,78,results,baselines,has,our robleurt,baselines has our robleurt,0.6225699782371521
translation,60,78,results,results,comparing to,baselines,results comparing to baselines,0.6772487163543701
translation,60,82,results,robertalarge,instead of,roberta - base model,robertalarge instead of roberta - base model,0.6859956383705139
translation,60,82,results,robertalarge,base of,robleurt - src model,robertalarge base of robleurt - src model,0.7394367456436157
translation,60,82,results,robertalarge,gives,better performance,robertalarge gives better performance,0.6622597575187683
translation,60,82,results,robleurt - src model,gives,better performance,robleurt - src model gives better performance,0.6340292096138
translation,60,82,results,results,using,robertalarge,results using robertalarge,0.6490080952644348
translation,60,96,results,accepting source,as,extra input,accepting source as extra input,0.516477108001709
translation,60,96,results,extra input,has,significantly improves,extra input has significantly improves,0.6188135743141174
translation,60,96,results,significantly improves,has,correlation scores,significantly improves has correlation scores,0.5758115649223328
translation,60,102,results,our model,takes models over,robleurt - nosrc and robleurt - src settings,our model takes models over robleurt - nosrc and robleurt - src settings,0.7878591418266296
translation,60,102,results,significant performance gain,across,all language pairs,significant performance gain across all language pairs,0.6857697367668152
translation,60,102,results,results,has,our model,results has our model,0.5871725678443909
translation,61,6,baselines,two main classes of transfer- based systems,to study,relatedness,two main classes of transfer- based systems to study relatedness,0.6812697052955627
translation,61,6,baselines,benefit,has,translation performance,benefit has translation performance,0.5737770795822144
translation,61,6,baselines,baselines,build,two main classes of transfer- based systems,baselines build two main classes of transfer- based systems,0.6665408611297607
translation,61,76,hyperparameters,pretrained mt models,released by,helsinki- nlp,pretrained mt models released by helsinki- nlp,0.6363211870193481
translation,61,76,hyperparameters,pretrained mt models,trained on,opus,pretrained mt models trained on opus,0.7402525544166565
translation,61,76,hyperparameters,hyperparameters,has,pretrained mt models,hyperparameters has pretrained mt models,0.4821981191635132
translation,61,79,hyperparameters,tokenization method,is,"sen-tencepiece ( kudo and richardson , 2018 )","tokenization method is sen-tencepiece ( kudo and richardson , 2018 )",0.523984432220459
translation,61,79,hyperparameters,"sen-tencepiece ( kudo and richardson , 2018 )",produces,vocabulary,"sen-tencepiece ( kudo and richardson , 2018 ) produces vocabulary",0.6325464844703674
translation,61,79,hyperparameters,vocabulary,of,size,vocabulary of size,0.6203036308288574
translation,61,79,hyperparameters,"49 , 621 , 21 , 528 and 55 , 255",for,"ca-es , ca-it , and ca-en models","49 , 621 , 21 , 528 and 55 , 255 for ca-es , ca-it , and ca-en models",0.5958839654922485
translation,61,79,hyperparameters,size,has,"49 , 621 , 21 , 528 and 55 , 255","size has 49 , 621 , 21 , 528 and 55 , 255",0.5937046408653259
translation,61,79,hyperparameters,hyperparameters,has,tokenization method,hyperparameters has tokenization method,0.5220303535461426
translation,61,88,hyperparameters,batch size,set to,25,batch size set to 25,0.7264925837516785
translation,61,88,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,61,89,hyperparameters,pre-trained models,fine-tuned for,30,pre-trained models fine-tuned for 30,0.7194423675537109
translation,61,89,hyperparameters,30,",",000 steps,"30 , 000 steps",0.6273990869522095
translation,61,89,hyperparameters,000 steps,on,opus bitext,000 steps on opus bitext,0.587118923664093
translation,61,89,hyperparameters,hyperparameters,has,pre-trained models,hyperparameters has pre-trained models,0.5199089646339417
translation,61,77,model,model,Transformerbased implemented in,marian - nmt framework,model Transformerbased implemented in marian - nmt framework,0.7433698177337646
translation,61,78,model,each layer,has,eight attention heads,each layer has eight attention heads,0.5964767932891846
translation,61,44,results,uzbek-english model,obtains,10.7 bleu score,uzbek-english model obtains 10.7 bleu score,0.5391751527786255
translation,61,44,results,uzbek-english model,improves to,15.0,uzbek-english model improves to 15.0,0.6838338971138
translation,61,44,results,10.7 bleu score,without,parent model,10.7 bleu score without parent model,0.661679744720459
translation,61,44,results,15.0,with,french - english parent model,15.0 with french - english parent model,0.601729154586792
translation,61,44,results,results,has,uzbek-english model,results has uzbek-english model,0.5259255766868591
translation,61,45,results,16.4 bleu score,without,parent model,16.4 bleu score without parent model,0.6688661575317383
translation,61,45,results,31.0,with,french - english parent model,31.0 with french - english parent model,0.573701024055481
translation,61,45,results,spanish - english model,has,16.4 bleu score,spanish - english model has 16.4 bleu score,0.5389701724052429
translation,61,45,results,results,has,spanish - english model,results has spanish - english model,0.5291000604629517
translation,61,46,results,transfer learning,contributes,4.3 and 14.6 bleu points gain,transfer learning contributes 4.3 and 14.6 bleu points gain,0.6056126356124878
translation,61,46,results,results,show,transfer learning,results show transfer learning,0.58036208152771
translation,61,46,results,results,applying,transfer learning,results applying transfer learning,0.6175622344017029
translation,61,104,results,contrastive system,across,all metrics,contrastive system across all metrics,0.6760600805282593
translation,61,104,results,primary system,has,outperforms,primary system has outperforms,0.6309627294540405
translation,61,104,results,outperforms,has,contrastive system,outperforms has contrastive system,0.6339156031608582
translation,61,107,results,baseline and fine-tuned models,show,effectiveness,baseline and fine-tuned models show effectiveness,0.5767974853515625
translation,61,107,results,effectiveness,of,transfer learning,effectiveness of transfer learning,0.5822449326515198
translation,61,107,results,transfer learning,for,related and unrelated language pairs,transfer learning for related and unrelated language pairs,0.5712159276008606
translation,61,107,results,results,show,effectiveness,results show effectiveness,0.588668704032898
translation,61,107,results,results,has,baseline and fine-tuned models,results has baseline and fine-tuned models,0.518247663974762
translation,62,162,ablation-analysis,prior sampling distribution,affect,overall performance,prior sampling distribution affect overall performance,0.6585898995399475
translation,62,162,ablation-analysis,ablation analysis,observe,prior sampling distribution,ablation analysis observe prior sampling distribution,0.61910480260849
translation,62,7,model,multiuat,dynamically adjusts,training data usage,multiuat dynamically adjusts training data usage,0.8273554444313049
translation,62,7,model,training data usage,based on,model 's uncertainty,training data usage based on model 's uncertainty,0.6555009484291077
translation,62,7,model,model 's uncertainty,small set of,trusted clean data,model 's uncertainty small set of trusted clean data,0.7487720251083374
translation,62,7,model,trusted clean data,for,multi-corpus machine translation,trusted clean data for multi-corpus machine translation,0.5693046450614929
translation,62,7,model,approach,has,multiuat,approach has multiuat,0.6944312453269958
translation,62,7,model,model,propose,approach,model propose approach,0.6953083872795105
translation,62,25,model,approach mul - tiuat,leverages,model uncertainty,approach mul - tiuat leverages model uncertainty,0.7425631284713745
translation,62,25,model,model uncertainty,as,reward,model uncertainty as reward,0.4936905801296234
translation,62,25,model,reward,to dynamically adjust,sampling probability distribution,reward to dynamically adjust sampling probability distribution,0.6820011734962463
translation,62,25,model,sampling probability distribution,over,multiple corpora,sampling probability distribution over multiple corpora,0.6728339791297913
translation,62,25,model,model,propose,approach mul - tiuat,model propose approach mul - tiuat,0.6858481168746948
translation,62,29,results,multilingual nmt,improve,overall performance,multilingual nmt improve overall performance,0.6203579306602478
translation,62,29,results,overall performance,from,+ 0.83 bleu score,overall performance from + 0.83 bleu score,0.45512160658836365
translation,62,29,results,overall performance,comparing with,best baseline,overall performance comparing with best baseline,0.632207453250885
translation,62,29,results,+ 0.83 bleu score,to,+ 1.52 bleu score,+ 0.83 bleu score to + 1.52 bleu score,0.5109442472457886
translation,62,29,results,results,In,multilingual nmt,results In multilingual nmt,0.5239394903182983
translation,62,30,results,our approach,improves,in-domain overall performance,our approach improves in-domain overall performance,0.6731002926826477
translation,62,30,results,our approach,achieves,second best out -ofdomain overall performance,our approach achieves second best out -ofdomain overall performance,0.6628749966621399
translation,62,30,results,in-domain overall performance,by,+ 0.58 bleu score,in-domain overall performance by + 0.58 bleu score,0.5235319137573242
translation,62,30,results,in-domain overall performance,comparing with,best baseline,in-domain overall performance comparing with best baseline,0.6453270316123962
translation,62,30,results,multi-domain nmt,has,our approach,multi-domain nmt has our approach,0.5793436169624329
translation,62,30,results,results,In,multi-domain nmt,results In multi-domain nmt,0.4883989691734314
translation,62,127,results,multiuat,with,various uncertainty measures,multiuat with various uncertainty measures,0.6940435171127319
translation,62,127,results,multiuat,reaches,best performance,multiuat reaches best performance,0.7880285978317261
translation,62,127,results,various uncertainty measures,reaches,best performance,various uncertainty measures reaches best performance,0.7001979947090149
translation,62,127,results,best performance,in,all four settings,best performance in all four settings,0.4937005341053009
translation,62,127,results,results,has,multiuat,results has multiuat,0.6106908917427063
translation,62,128,results,multiuat,more favorable to,hrls,multiuat more favorable to hrls,0.7397242188453674
translation,62,128,results,results,observe that,multiuat,results observe that multiuat,0.6279457807540894
translation,62,129,results,multi-domain nmt multiuat,achieves,second best performance,multi-domain nmt multiuat achieves second best performance,0.6967359781265259
translation,62,129,results,all its baselines,on,in-domain evaluation,all its baselines on in-domain evaluation,0.4880414605140686
translation,62,129,results,second best performance,on,out-ofdomain evaluation,second best performance on out-ofdomain evaluation,0.5059595108032227
translation,62,129,results,multi-domain nmt multiuat,has,outperforms,multi-domain nmt multiuat has outperforms,0.605739176273346
translation,62,129,results,outperforms,has,all its baselines,outperforms has all its baselines,0.5860883593559265
translation,62,129,results,results,has,multi-domain nmt multiuat,results has multi-domain nmt multiuat,0.5350372791290283
translation,62,130,results,optimal balance,on,in-domain evaluation,optimal balance on in-domain evaluation,0.5697835683822632
translation,62,130,results,one with exptp,achieves,second best performance,one with exptp achieves second best performance,0.6910290718078613
translation,62,130,results,second best performance,on,out-of- domain evaluation,second best performance on out-of- domain evaluation,0.519966721534729
translation,62,138,results,uncertainty measures,deliver,different results,uncertainty measures deliver different results,0.573294997215271
translation,62,138,results,results,has,uncertainty measures,results has uncertainty measures,0.5171882510185242
translation,62,141,results,multiuat,with,uncertainty measure,multiuat with uncertainty measure,0.6853594779968262
translation,62,141,results,multiuat,with,other uncertainty measures,multiuat with other uncertainty measures,0.6803085207939148
translation,62,141,results,multiuat,performs,substantially worse,multiuat performs substantially worse,0.690414547920227
translation,62,141,results,uncertainty measure,of,vartp,uncertainty measure of vartp,0.6320375800132751
translation,62,141,results,uncertainty measure,performs,substantially worse,uncertainty measure performs substantially worse,0.5902386903762817
translation,62,141,results,substantially worse,than,other uncertainty measures,substantially worse than other uncertainty measures,0.5930994749069214
translation,62,141,results,other uncertainty measures,in,multi-domain nmt,other uncertainty measures in multi-domain nmt,0.5254669189453125
translation,62,141,results,results,has,multiuat,results has multiuat,0.6106908917427063
translation,62,142,results,entropy - based uncertainty measures,are,more robust,entropy - based uncertainty measures are more robust,0.5922040343284607
translation,62,142,results,entropy - based uncertainty measures,deliver,relatively stable improvements,entropy - based uncertainty measures deliver relatively stable improvements,0.5594733953475952
translation,62,142,results,more robust,to,change of datasets,more robust to change of datasets,0.5995709896087646
translation,62,143,results,multiuat,with,uncertainty measures,multiuat with uncertainty measures,0.6841675043106079
translation,62,143,results,multiuat,demonstrate,better out -ofdomain generalization,multiuat demonstrate better out -ofdomain generalization,0.640416145324707
translation,62,143,results,better out -ofdomain generalization,in,multi-domain nmt,better out -ofdomain generalization in multi-domain nmt,0.5409156084060669
translation,62,143,results,better out -ofdomain generalization,compared with,baselines,better out -ofdomain generalization compared with baselines,0.647625207901001
translation,62,143,results,results,find out,multiuat,results find out multiuat,0.6377032995223999
translation,62,144,results,multiuat,with,entropy - based uncertainty measures,multiuat with entropy - based uncertainty measures,0.6250827312469482
translation,62,144,results,multiuat,demonstrates,better robustness,multiuat demonstrates better robustness,0.6977088451385498
translation,62,144,results,better robustness,against,change of datasets,better robustness against change of datasets,0.6562457084655762
translation,62,158,results,knowledge,learned from,wmt,knowledge learned from wmt,0.7420592308044434
translation,62,158,results,knowledge,generalized to,other domains,knowledge generalized to other domains,0.707878053188324
translation,62,158,results,knowledge,learned from,tanzil,knowledge learned from tanzil,0.7213923335075378
translation,62,158,results,wmt,generalized to,other domains,wmt generalized to other domains,0.6533981561660767
translation,62,158,results,knowledge,learned from,tanzil,knowledge learned from tanzil,0.7213923335075378
translation,62,158,results,tanzil,almost not beneficial to,other domains,tanzil almost not beneficial to other domains,0.7905057668685913
translation,62,158,results,results,observe that,knowledge,results observe that knowledge,0.6051970720291138
translation,62,163,results,overall results,on,in - domain and out -of- domain evaluation,overall results on in - domain and out -of- domain evaluation,0.5305703282356262
translation,62,163,results,negatively correlated,with,prior,negatively correlated with prior,0.6698875427246094
translation,62,163,results,multidds -s and multiuat,has,overall results,multidds -s and multiuat has overall results,0.5989782810211182
translation,62,163,results,results,For,multidds -s and multiuat,results For multidds -s and multiuat,0.6217576861381531
translation,63,129,experimental-setup,monolingual documents,prepare,training instances,monolingual documents prepare training instances,0.514792263507843
translation,63,129,experimental-setup,monolingual documents,set,gap sentence ratio,monolingual documents set gap sentence ratio,0.6469836831092834
translation,63,129,experimental-setup,training instances,for,document- level restoration task,training instances for document- level restoration task,0.5511463284492493
translation,63,129,experimental-setup,gap sentence ratio,to,20 %,gap sentence ratio to 20 %,0.620413064956665
translation,63,129,experimental-setup,experimental setup,Upon,monolingual documents,experimental setup Upon monolingual documents,0.6027421951293945
translation,63,129,experimental-setup,experimental setup,set,gap sentence ratio,experimental setup set gap sentence ratio,0.654779314994812
translation,63,148,experimental-setup,"opennmt ( klein et al. , 2017 )",as,implementation of transformer,"opennmt ( klein et al. , 2017 ) as implementation of transformer",0.5307030081748962
translation,63,148,experimental-setup,"opennmt ( klein et al. , 2017 )",implement,our models,"opennmt ( klein et al. , 2017 ) implement our models",0.6468244194984436
translation,63,148,experimental-setup,experimental setup,use,"opennmt ( klein et al. , 2017 )","experimental setup use opennmt ( klein et al. , 2017 )",0.5606618523597717
translation,63,148,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,63,150,experimental-setup,numbers of layers,in,"context encoder , sentence encoder and decoder","numbers of layers in context encoder , sentence encoder and decoder",0.5013272762298584
translation,63,150,experimental-setup,numbers of layers,set to,6,numbers of layers set to 6,0.708488941192627
translation,63,150,experimental-setup,translation models,has,numbers of layers,translation models has numbers of layers,0.5459570288658142
translation,63,151,experimental-setup,the filter size,set to,512 and 2048,the filter size set to 512 and 2048,0.7265321612358093
translation,63,151,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,63,151,experimental-setup,experimental setup,has,the filter size,experimental setup has the filter size,0.5199849605560303
translation,63,152,experimental-setup,number of heads,in,multi-head attention,number of heads in multi-head attention,0.5287878513336182
translation,63,152,experimental-setup,multi-head attention,is,8,multi-head attention is 8,0.5655282139778137
translation,63,152,experimental-setup,dropout rate,is,0.1,dropout rate is 0.1,0.53923100233078
translation,63,152,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,63,152,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,63,153,experimental-setup,pre-training,train,models,pre-training train models,0.7537966370582581
translation,63,153,experimental-setup,models,for,500k steps,models for 500k steps,0.6680067181587219
translation,63,153,experimental-setup,models,on,four v100 gpus,models on four v100 gpus,0.5362474322319031
translation,63,153,experimental-setup,500k steps,on,four v100 gpus,500k steps on four v100 gpus,0.5097510814666748
translation,63,153,experimental-setup,four v100 gpus,with,batch - size 8192,four v100 gpus with batch - size 8192,0.6221417784690857
translation,63,153,experimental-setup,experimental setup,In,pre-training,experimental setup In pre-training,0.5103303790092468
translation,63,154,experimental-setup,"adam ( kingma and ba , 2015 )",with,"?1 = 0.9 , ?2 = 0.98","adam ( kingma and ba , 2015 ) with ?1 = 0.9 , ?2 = 0.98",0.6447789072990417
translation,63,154,experimental-setup,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,63,154,experimental-setup,"adam ( kingma and ba , 2015 )",with,warm - up step,"adam ( kingma and ba , 2015 ) with warm - up step",0.6462300419807434
translation,63,154,experimental-setup,"?1 = 0.9 , ?2 = 0.98",for,optimization,"?1 = 0.9 , ?2 = 0.98 for optimization",0.6374711990356445
translation,63,154,experimental-setup,learning rate,as,1,learning rate as 1,0.5723053812980652
translation,63,154,experimental-setup,warm - up step,as,16k,warm - up step as 16k,0.5424612164497375
translation,63,154,experimental-setup,experimental setup,use,"adam ( kingma and ba , 2015 )","experimental setup use adam ( kingma and ba , 2015 )",0.579606831073761
translation,63,155,experimental-setup,fine-tuning,fine - tune,models,fine-tuning fine - tune models,0.816606879234314
translation,63,155,experimental-setup,models,for,200k steps,models for 200k steps,0.6781901121139526
translation,63,155,experimental-setup,models,on,single v100 gpu,models on single v100 gpu,0.5629827976226807
translation,63,155,experimental-setup,single v100 gpu,with,batch - size 8192,single v100 gpu with batch - size 8192,0.6460972428321838
translation,63,155,experimental-setup,single v100 gpu,with,learning rate,single v100 gpu with learning rate,0.6291253566741943
translation,63,155,experimental-setup,single v100 gpu,with,warm - up step,single v100 gpu with warm - up step,0.6635419130325317
translation,63,155,experimental-setup,learning rate,has,0.3,learning rate has 0.3,0.5459375381469727
translation,63,155,experimental-setup,warm - up step,has,4k,warm - up step has 4k,0.5963621139526367
translation,63,155,experimental-setup,experimental setup,In,fine-tuning,experimental setup In fine-tuning,0.5374264717102051
translation,63,156,experimental-setup,beam size,to,5,beam size to 5,0.6587702035903931
translation,63,5,model,context - aware nmt,by taking the advantage of,availability of both large-scale sentence - level parallel dataset,context - aware nmt by taking the advantage of availability of both large-scale sentence - level parallel dataset,0.691866397857666
translation,63,5,model,model,to improve,context - aware nmt,model to improve context - aware nmt,0.6926159858703613
translation,63,6,model,model,propose,two pre-training tasks,model propose two pre-training tasks,0.6651126146316528
translation,63,7,model,one,translate,sentence,one translate sentence,0.7422573566436768
translation,63,7,model,sentence,from,source language,sentence from source language,0.5127726793289185
translation,63,7,model,source language,to,target language,source language to target language,0.555213212966919
translation,63,7,model,target language,on,sentencelevel parallel dataset,target language on sentencelevel parallel dataset,0.49751466512680054
translation,63,7,model,other,translate,document,other translate document,0.6472327709197998
translation,63,7,model,document,from,deliberately noised,document from deliberately noised,0.6131950616836548
translation,63,7,model,deliberately noised,to,original,deliberately noised to original,0.5818891525268555
translation,63,7,model,original,on,monolingual documents,original on monolingual documents,0.5115593075752258
translation,63,7,model,model,has,one,model has one,0.5633423924446106
translation,63,8,model,jointly and simultaneously learned,via,same model,jointly and simultaneously learned via same model,0.6685951948165894
translation,63,8,model,same model,thereafter fine-tuned on,scalelimited parallel documents,same model thereafter fine-tuned on scalelimited parallel documents,0.7518890500068665
translation,63,8,model,scalelimited parallel documents,from,sentencelevel and document- level perspectives,scalelimited parallel documents from sentencelevel and document- level perspectives,0.5782846808433533
translation,63,8,model,model,has,two pre-training tasks,model has two pre-training tasks,0.5477957129478455
translation,63,18,model,corpus bottleneck,for,context - aware nmt,corpus bottleneck for context - aware nmt,0.603667140007019
translation,63,18,model,corpus bottleneck,by leveraging,largescale sentence - level parallel dataset,corpus bottleneck by leveraging largescale sentence - level parallel dataset,0.6833388209342957
translation,63,18,model,context - aware nmt,by leveraging,monolingual documents,context - aware nmt by leveraging monolingual documents,0.6966814994812012
translation,63,18,model,model,break,corpus bottleneck,model break corpus bottleneck,0.7522119283676147
translation,63,19,model,former,to boost,performance,former to boost performance,0.6689343452453613
translation,63,19,model,performance,of,translation models,performance of translation models,0.596619725227356
translation,63,19,model,latter,to enhance,context encoders ' capability,latter to enhance context encoders ' capability,0.7019074559211731
translation,63,19,model,context encoders ' capability,of capturing,useful context information,context encoders ' capability of capturing useful context information,0.6949273943901062
translation,63,19,model,model,use,former,model use former,0.7230746150016785
translation,63,19,model,model,employ,latter,model employ latter,0.6826038360595703
translation,63,25,model,model,to,sentence - level parallel dataset,model to sentence - level parallel dataset,0.47513917088508606
translation,63,25,model,model,effectively and simultaneously model,sentence - level parallel dataset,model effectively and simultaneously model sentence - level parallel dataset,0.6251837611198425
translation,63,25,model,model,propose,novel cross - task pre-training approach,model propose novel cross - task pre-training approach,0.643205463886261
translation,63,163,results,sentences,as,translation units,sentences as translation units,0.5359516143798828
translation,63,163,results,all translation tasks,with,improvement,all translation tasks with improvement,0.6335532665252686
translation,63,163,results,improvement,of,averaged 1.36 bleu and 1.72 meteor,improvement of averaged 1.36 bleu and 1.72 meteor,0.534472644329071
translation,63,163,results,sentences,has,our models,sentences has our models,0.6201881766319275
translation,63,163,results,translation units,has,our models,translation units has our models,0.5970032215118408
translation,63,163,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,63,163,results,outperform,has,sentence - level transformer baseline,outperform has sentence - level transformer baseline,0.5915812253952026
translation,63,163,results,results,use,sentences,results use sentences,0.5792677998542786
translation,63,164,results,documents,as,translation units,documents as translation units,0.5492788553237915
translation,63,164,results,our models,achieve,further improvement,our models achieve further improvement,0.640580952167511
translation,63,164,results,further improvement,by modeling,document- level context,further improvement by modeling document- level context,0.746918261051178
translation,63,164,results,documents,has,our models,documents has our models,0.6208313703536987
translation,63,164,results,translation units,has,our models,translation units has our models,0.5970032215118408
translation,63,164,results,results,use,documents,results use documents,0.6602299213409424
translation,63,165,results,our approach,surpasses,all context - aware baselines,our approach surpasses all context - aware baselines,0.5857501029968262
translation,63,165,results,our approach,achieves,state- ofthe - art,our approach achieves state- ofthe - art,0.6003237962722778
translation,63,165,results,all context - aware baselines,on,zh-en and en - de ( ted ) tasks,all context - aware baselines on zh-en and en - de ( ted ) tasks,0.5251685380935669
translation,63,165,results,results,shows that,our approach,results shows that our approach,0.7036905288696289
translation,63,174,results,monolingual documents,helps,translation,monolingual documents helps translation,0.6210254430770874
translation,63,174,results,translation,for,contextaware models,translation for contextaware models,0.6023569703102112
translation,63,174,results,results,Using,sentence - level parallel dataset,results Using sentence - level parallel dataset,0.6080155372619629
translation,63,175,results,transformer baselines,fail to achieve,higher performance,transformer baselines fail to achieve higher performance,0.7352495193481445
translation,63,175,results,higher performance,with,monolingual documents,higher performance with monolingual documents,0.6609752178192139
translation,63,175,results,sentence - level parallel dataset,has,transformer baselines,sentence - level parallel dataset has transformer baselines,0.5493430495262146
translation,63,175,results,results,in the presence of,sentence - level parallel dataset,results in the presence of sentence - level parallel dataset,0.6509235501289368
translation,63,176,results,our models,achieve,highest performance,our models achieve highest performance,0.6473178267478943
translation,63,176,results,highest performance,by leveraging,two resources,highest performance by leveraging two resources,0.6667722463607788
translation,63,176,results,results,has,our models,results has our models,0.5733726620674133
translation,63,186,results,documents,as,input units,documents as input units,0.5577029585838318
translation,63,186,results,input units,in,inferring,input units in inferring,0.5407448410987854
translation,63,186,results,joint fine -tuning strategy,provides,no advantage,joint fine -tuning strategy provides no advantage,0.6167296767234802
translation,63,186,results,documents,has,joint fine -tuning strategy,documents has joint fine -tuning strategy,0.5901776552200317
translation,63,186,results,inferring,has,joint fine -tuning strategy,inferring has joint fine -tuning strategy,0.4985259175300598
translation,63,186,results,results,use,documents,results use documents,0.6602299213409424
translation,63,187,results,input units,are,sentences,input units are sentences,0.6154186129570007
translation,63,187,results,outperforms,one not including,sentence - level translation,outperforms one not including sentence - level translation,0.6687140464782715
translation,63,187,results,sentence - level translation,in,fine-tuning,sentence - level translation in fine-tuning,0.4559805989265442
translation,63,187,results,input units,has,joint fine-tuning strategy,input units has joint fine-tuning strategy,0.5340439081192017
translation,63,187,results,joint fine-tuning strategy,has,outperforms,joint fine-tuning strategy has outperforms,0.6007563471794128
translation,63,187,results,results,when,input units,results when input units,0.6608744859695435
translation,63,195,results,performance,of,pronoun translations,performance of pronoun translations,0.589710533618927
translation,63,195,results,our proposed approach,has,well improve,our proposed approach has well improve,0.6179080009460449
translation,63,195,results,well improve,has,performance,well improve has performance,0.5835150480270386
translation,63,195,results,results,observe that,our proposed approach,results observe that our proposed approach,0.609019935131073
translation,63,199,results,best performance,when,ratio,best performance when ratio,0.7010159492492676
translation,63,199,results,ratio,set as,20 %,ratio set as 20 %,0.6948860287666321
translation,64,185,ablation-analysis,gfst f em,reduces,gap,gfst f em reduces gap,0.7038912773132324
translation,64,185,ablation-analysis,gap,between,recall,gap between recall,0.7280121445655823
translation,64,185,ablation-analysis,recall,for,feminine and masculine inputs,recall for feminine and masculine inputs,0.6361566185951233
translation,64,185,ablation-analysis,?r,by,up to 19.8 points,?r by up to 19.8 points,0.6419806480407715
translation,64,185,ablation-analysis,up to 19.8 points,with respect to,baseline,up to 19.8 points with respect to baseline,0.7194417119026184
translation,64,185,ablation-analysis,ablation analysis,has,gfst f em,ablation analysis has gfst f em,0.566028356552124
translation,64,27,baselines,gender -filtered self-training ( gfst ),consists of,self-training,gender -filtered self-training ( gfst ) consists of self-training,0.6138201951980591
translation,64,27,baselines,nmt model,using,genderbalanced monolingual data,nmt model using genderbalanced monolingual data,0.650123119354248
translation,64,27,baselines,genderbalanced monolingual data,filtered to reduce,error propagation,genderbalanced monolingual data filtered to reduce error propagation,0.6847378015518188
translation,64,27,baselines,self-training,has,nmt model,self-training has nmt model,0.5650472044944763
translation,64,120,experimental-setup,"transformers ( vaswani et al. , 2017 )",implemented in,"fairseq-py ( ott et al. , 2019 )","transformers ( vaswani et al. , 2017 ) implemented in fairseq-py ( ott et al. , 2019 )",0.671504557132721
translation,64,6,model,gender-filtered self-training ( gfst ),to improve,gender translation accuracy,gender-filtered self-training ( gfst ) to improve gender translation accuracy,0.6472794413566589
translation,64,6,model,gender translation accuracy,on,unambiguously gendered inputs,gender translation accuracy on unambiguously gendered inputs,0.5304827690124512
translation,64,6,model,model,propose,gender-filtered self-training ( gfst ),model propose gender-filtered self-training ( gfst ),0.6602786183357239
translation,64,26,model,data augmentationbased method,to address,sample bias,data augmentationbased method to address sample bias,0.5987270474433899
translation,64,26,model,sample bias,has,using only source - language monolingual data,sample bias has using only source - language monolingual data,0.517059326171875
translation,64,26,model,model,proposes,data augmentationbased method,model proposes data augmentationbased method,0.7094112634658813
translation,64,28,model,model,has,framework,model has framework,0.5441871285438538
translation,64,33,model,gender-filtered selftraining ( gfst ),for improving,gender translation,gender-filtered selftraining ( gfst ) for improving gender translation,0.677629292011261
translation,64,33,model,model,propose,gender-filtered selftraining ( gfst ),model propose gender-filtered selftraining ( gfst ),0.6583411693572998
translation,64,130,results,outperforms,uses,hard- debiasing,outperforms uses hard- debiasing,0.6442441940307617
translation,64,130,results,baseline model,uses,hard- debiasing,baseline model uses hard- debiasing,0.6343981623649597
translation,64,130,results,hard- debiasing,on,accuracy and ?r,hard- debiasing on accuracy and ?r,0.6106741428375244
translation,64,130,results,accuracy and ?r,for,all language pairs,accuracy and ?r for all language pairs,0.6134240627288818
translation,64,130,results,gfst model,has,outperforms,gfst model has outperforms,0.6309694051742554
translation,64,130,results,outperforms,has,baseline model,outperforms has baseline model,0.5998825430870056
translation,64,130,results,hard- debiasing,has,),hard- debiasing has ),0.6429609060287476
translation,64,130,results,results,has,gfst model,results has gfst model,0.5115732550621033
translation,64,138,results,en-de 12 and en-it,see,large increase,en-de 12 and en-it see large increase,0.6644343733787537
translation,64,138,results,en-de 12 and en-it,for,en-ru,en-de 12 and en-it for en-ru,0.735728919506073
translation,64,138,results,large increase,in,gender translation accuracy,large increase in gender translation accuracy,0.5147279500961304
translation,64,138,results,gender translation accuracy,for,our proposed gfst model,gender translation accuracy for our proposed gfst model,0.5607630014419556
translation,64,138,results,gender translation accuracy,for,en-ru,gender translation accuracy for en-ru,0.602635383605957
translation,64,138,results,our proposed gfst model,compared to,baseline,our proposed gfst model compared to baseline,0.6528629660606384
translation,64,138,results,our proposed gfst model,compared to,baseline,our proposed gfst model compared to baseline,0.6528629660606384
translation,64,138,results,no significant difference,between,baseline,no significant difference between baseline,0.6495862603187561
translation,64,138,results,no significant difference,between,proposed model,no significant difference between proposed model,0.6661750078201294
translation,64,138,results,results,For,en-de 12 and en-it,results For en-de 12 and en-it,0.6435006260871887
translation,64,138,results,results,for,en-ru,results for en-ru,0.656744658946991
translation,64,151,results,gfst,increases,both accuracy,gfst increases both accuracy,0.7496284246444702
translation,64,151,results,gfst,increases,? acc,gfst increases ? acc,0.7397618889808655
translation,64,151,results,? acc,for,feminine and masculine data,? acc for feminine and masculine data,0.6815675497055054
translation,64,151,results,en-it,has,gfst,en-it has gfst,0.7165096998214722
translation,64,151,results,results,For,en-it,results For en-it,0.7495054006576538
translation,64,156,results,general trend,of,small improvements,general trend of small improvements,0.5627731084823608
translation,64,156,results,small improvements,from,selftraining,small improvements from selftraining,0.5476892590522766
translation,64,156,results,results,observe,general trend,results observe general trend,0.6171284317970276
translation,64,162,results,"en-de , en-he , and en-it",has,gfst,"en-de , en-he , and en-it has gfst",0.5951354503631592
translation,64,162,results,gfst,has,significantly improves,gfst has significantly improves,0.6149670481681824
translation,64,162,results,significantly improves,has,overall quality,significantly improves has overall quality,0.5857366323471069
translation,64,162,results,results,For,"en-de , en-he , and en-it","results For en-de , en-he , and en-it",0.6052344441413879
translation,64,163,results,no significant difference,in,overall quality,no significant difference in overall quality,0.4382276237010956
translation,64,163,results,overall quality,between,two models,overall quality between two models,0.6711034774780273
translation,64,163,results,results,For,en-fr and en-ru,results For en-fr and en-ru,0.6037787199020386
translation,64,170,results,no significant drop,in,quality,no significant drop in quality,0.5435695648193359
translation,64,170,results,quality,between,baseline and the fine-tuned models,quality between baseline and the fine-tuned models,0.5748695731163025
translation,64,170,results,en-ru,loses,1.1 bleu,en-ru loses 1.1 bleu,0.6934607028961182
translation,64,170,results,results,For,four of the five language pairs,results For four of the five language pairs,0.5310750603675842
translation,64,172,results,retraining,on,gfst data,retraining on gfst data,0.5722271203994751
translation,64,172,results,gfst data,has,outperforms,gfst data has outperforms,0.6426411867141724
translation,64,172,results,outperforms,has,fine-tuning,outperforms has fine-tuning,0.5970712304115295
translation,64,173,results,finetuning,on,gfst data,finetuning on gfst data,0.5701744556427002
translation,64,173,results,gender accuracy,over,baseline,gender accuracy over baseline,0.6610105037689209
translation,64,173,results,gfst data,has,consistently improves,gfst data has consistently improves,0.6147127747535706
translation,64,173,results,consistently improves,has,gender accuracy,consistently improves has gender accuracy,0.598333477973938
translation,64,173,results,results,has,finetuning,results has finetuning,0.5693389773368835
translation,64,186,results,distribution,between,masculine and feminine genders,distribution between masculine and feminine genders,0.6561050415039062
translation,64,186,results,gfst m sc,has,increases ? r,gfst m sc has increases ? r,0.641722559928894
translation,64,186,results,results,has,gfst m sc,results has gfst m sc,0.5476592183113098
translation,64,187,results,gfst f em,yields,similar accuracy,gfst f em yields similar accuracy,0.7132013440132141
translation,64,187,results,baseline,for,all five language pairs,baseline for all five language pairs,0.5950354337692261
translation,64,187,results,similar accuracy,to,original gfst model,similar accuracy to original gfst model,0.541702389717102
translation,64,187,results,gender accuracy,has,gfst f em,gender accuracy has gfst f em,0.5758282542228699
translation,64,187,results,gfst f em,has,outperforms,gfst f em has outperforms,0.6507731080055237
translation,64,187,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,64,187,results,results,On,gender accuracy,results On gender accuracy,0.486023485660553
translation,64,188,results,gfst m sc,performs,very closely,gfst m sc performs very closely,0.6295768022537231
translation,64,188,results,very closely,to,baseline,very closely to baseline,0.5885152220726013
translation,64,188,results,results,has,gfst m sc,results has gfst m sc,0.5476592183113098
translation,64,190,results,original gfst model,trained on,masculine and feminine additional data,original gfst model trained on masculine and feminine additional data,0.7113913297653198
translation,64,190,results,gfst f em,in,accuracy,gfst f em in accuracy,0.5459276437759399
translation,64,190,results,original gfst model,has,outperforms,original gfst model has outperforms,0.6124801635742188
translation,64,190,results,outperforms,has,gfst f em,outperforms has gfst f em,0.5891385078430176
translation,64,190,results,results,has,original gfst model,results has original gfst model,0.5052280426025391
translation,65,107,baselines,baseline systems,used,multilingual transformer - based predictor - estimator approach,baseline systems used multilingual transformer - based predictor - estimator approach,0.6009220480918884
translation,65,107,baselines,sentence-level baseline systems,used,multilingual transformer - based predictor - estimator approach,sentence-level baseline systems used multilingual transformer - based predictor - estimator approach,0.5791016221046448
translation,65,107,baselines,baseline systems,has,sentence-level baseline systems,baseline systems has sentence-level baseline systems,0.5385322570800781
translation,65,107,baselines,baselines,has,baseline systems,baselines has baseline systems,0.6008633971214294
translation,65,108,baselines,concatenated train portions,of,data,concatenated train portions of data,0.6038122773170471
translation,65,108,baselines,concatenated train portions,of,data,concatenated train portions of data,0.6038122773170471
translation,65,108,baselines,concatenated train portions,of,data,concatenated train portions of data,0.6038122773170471
translation,65,108,baselines,data,for,training,data for training,0.6230091452598572
translation,65,108,baselines,concatenated development portions,of,data,concatenated development portions of data,0.5924034714698792
translation,65,108,baselines,concatenated development portions,for,validation / early - stopping,concatenated development portions for validation / early - stopping,0.6427484154701233
translation,65,108,baselines,baselines,implemented in,"openkiwi ( kepler et al. , 2019 )","baselines implemented in openkiwi ( kepler et al. , 2019 )",0.6390981674194336
translation,65,112,baselines,word - level baseline systems,used,same architecture,word - level baseline systems used same architecture,0.6110556721687317
translation,65,112,baselines,jointly,has,word- level ok / bad tags,jointly has word- level ok / bad tags,0.5902972221374512
translation,65,112,baselines,baselines,has,word - level baseline systems,baselines has word - level baseline systems,0.5511149168014526
translation,65,113,baselines,baselines,has,catastrophic error baseline system,baselines has catastrophic error baseline system,0.6108063459396362
translation,65,114,baselines,baseline model,for,task 3,baseline model for task 3,0.6236816048622131
translation,65,114,baselines,baseline model,follows,monotransquest architecture,baseline model follows monotransquest architecture,0.6661035418510437
translation,65,114,baselines,task 3,follows,monotransquest architecture,task 3 follows monotransquest architecture,0.676580011844635
translation,65,114,baselines,monotransquest architecture,for,sentence - level classification,monotransquest architecture for sentence - level classification,0.5869669914245605
translation,65,114,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,65,127,baselines,ensbrt,propose,system,ensbrt propose system,0.683610200881958
translation,65,127,baselines,system,is,ensemble of multilingual bert ( mbert ) based regression models,system is ensemble of multilingual bert ( mbert ) based regression models,0.5830366015434265
translation,65,127,baselines,ensemble of multilingual bert ( mbert ) based regression models,generated by,fine-tuning,ensemble of multilingual bert ( mbert ) based regression models generated by fine-tuning,0.6632089614868164
translation,65,127,baselines,fine-tuning,on,different input settings,fine-tuning on different input settings,0.5346648097038269
translation,65,127,baselines,baselines,has,ensbrt,baselines has ensbrt,0.5965970754623413
translation,65,145,baselines,two pre-trained monolingual encoders,first produce,monolingual representations,two pre-trained monolingual encoders first produce monolingual representations,0.6223851442337036
translation,65,145,baselines,two pre-trained monolingual encoders,exchanges,information,two pre-trained monolingual encoders exchanges information,0.685922384262085
translation,65,145,baselines,monolingual representations,of,two input data,monolingual representations of two input data,0.5375798940658569
translation,65,145,baselines,information,of,representations,information of representations,0.6039035320281982
translation,65,145,baselines,information,through,two additional cross attention networks,information through two additional cross attention networks,0.6576011180877686
translation,65,145,baselines,representations,through,two additional cross attention networks,representations through two additional cross attention networks,0.6486119627952576
translation,65,145,baselines,baselines,has,jhu - microsoft postech ( t2 ),baselines has jhu - microsoft postech ( t2 ),0.5869839787483215
translation,65,152,baselines,first method,based on,soft alignment,first method based on soft alignment,0.6664066910743713
translation,65,152,baselines,soft alignment,of,multilingual contextual embeddings,soft alignment of multilingual contextual embeddings,0.4928893744945526
translation,65,152,baselines,soft alignment,generated by,pre-trained mbert or xlm -r,soft alignment generated by pre-trained mbert or xlm -r,0.6555663347244263
translation,65,152,baselines,baselines,has,first method,baselines has first method,0.5801792740821838
translation,65,213,baselines,predictor-estimator framework,with,xlm - roberta encoders,predictor-estimator framework with xlm - roberta encoders,0.6595816612243652
translation,65,213,baselines,predictor-estimator framework,with,task -specific classifiers,predictor-estimator framework with task -specific classifiers,0.6313341856002808
translation,65,213,baselines,xlm - roberta encoders,for,predictor,xlm - roberta encoders for predictor,0.669245719909668
translation,65,213,baselines,task -specific classifiers,for,estimator,task -specific classifiers for estimator,0.6208934783935547
translation,65,217,baselines,ensembling approaches,to,boost,ensembling approaches to boost,0.5515901446342468
translation,65,217,baselines,boost,has,performance,boost has performance,0.5791411995887756
translation,65,217,baselines,baselines,has,ensembling approaches,baselines has ensembling approaches,0.5624403357505798
translation,65,106,results,instances,with,not labels,instances with not labels,0.5979786515235901
translation,65,106,results,instances,is,83 %,instances is 83 %,0.6224470734596252
translation,65,106,results,instances,is,84 %,instances is 84 %,0.6243200898170471
translation,65,106,results,not labels,in,training set,not labels in training set,0.47597387433052063
translation,65,106,results,83 %,for,en-cs,83 % for en-cs,0.7071712017059326
translation,65,106,results,72 %,for,en-de,72 % for en-de,0.6788201928138733
translation,65,106,results,91 %,or,en-ja,91 % or en-ja,0.7092886567115784
translation,65,106,results,84 %,for,en- zh,84 % for en- zh,0.6785768866539001
translation,65,168,results,three top performing systems,perform,very closely,three top performing systems perform very closely,0.589125394821167
translation,65,168,results,very closely,on,average,very closely on average,0.5821478366851807
translation,65,168,results,results,has,three top performing systems,results has three top performing systems,0.564353883266449
translation,65,183,results,performance,of,all except four systems,performance of all except four systems,0.597875714302063
translation,65,183,results,all except four systems,is,substantially better,all except four systems is substantially better,0.5885201096534729
translation,65,183,results,substantially better,than,baseline system,substantially better than baseline system,0.5791109204292297
translation,65,183,results,baseline system,for,all languages,baseline system for all languages,0.554994523525238
translation,65,183,results,results,has,performance,results has performance,0.5972660779953003
translation,65,186,results,performance,comparable to,average non-zero-shot language pairs,performance comparable to average non-zero-shot language pairs,0.6075171828269958
translation,65,186,results,average non-zero-shot language pairs,except for,en- ja language pair,average non-zero-shot language pairs except for en- ja language pair,0.6237934231758118
translation,65,186,results,zero-shot languages,has,performance,zero-shot languages has performance,0.5535215735435486
translation,65,186,results,results,On,zero-shot languages,results On zero-shot languages,0.5135098695755005
translation,65,212,results,outperform,with,hw - tsc ranking,outperform with hw - tsc ranking,0.6797431111335754
translation,65,212,results,monolingual approaches,in,individual language pairs,monolingual approaches in individual language pairs,0.5177968740463257
translation,65,212,results,monolingual approaches,with,hw - tsc ranking,monolingual approaches with hw - tsc ranking,0.6583054065704346
translation,65,212,results,hw - tsc ranking,for,"majority of the ' supervised ' language pairs ( en- de , en-zh , ne-en , si-en and ru-en )","hw - tsc ranking for majority of the ' supervised ' language pairs ( en- de , en-zh , ne-en , si-en and ru-en )",0.6093345880508423
translation,65,212,results,first,for,"majority of the ' supervised ' language pairs ( en- de , en-zh , ne-en , si-en and ru-en )","first for majority of the ' supervised ' language pairs ( en- de , en-zh , ne-en , si-en and ru-en )",0.6157007217407227
translation,65,212,results,ist - unbabel,leading,"majority of the zero-shot language pairs ( en- cs , en-ja , ps-en )","ist - unbabel leading majority of the zero-shot language pairs ( en- cs , en-ja , ps-en )",0.7133476138114929
translation,65,212,results,multilingual system submissions,has,outperform,multilingual system submissions has outperform,0.618561863899231
translation,65,212,results,outperform,has,monolingual approaches,outperform has monolingual approaches,0.5774466395378113
translation,65,212,results,hw - tsc ranking,has,first,hw - tsc ranking has first,0.6213100552558899
translation,65,212,results,results,has,multilingual system submissions,results has multilingual system submissions,0.5251086950302124
translation,65,238,results,km - en words,trained only for,target predictions,km - en words trained only for target predictions,0.7356953620910645
translation,65,238,results,target predictions,of,word- level task,target predictions of word- level task,0.5341285467147827
translation,65,238,results,target predictions,obtained,best performance,target predictions obtained best performance,0.64892578125
translation,65,238,results,best performance,in,en-de.,best performance in en-de.,0.5895557999610901
translation,65,238,results,results,has,km - en words,results has km - en words,0.5548961162567139
translation,65,239,results,jhu - microsoft,seemed to obtain,competitive performance,jhu - microsoft seemed to obtain competitive performance,0.7095385193824768
translation,65,239,results,competitive performance,for,et- en and ro - en tasks,competitive performance for et- en and ro - en tasks,0.5846583843231201
translation,65,239,results,results,has,jhu - microsoft,results has jhu - microsoft,0.5617745518684387
translation,65,246,results,hw - tsc,consistently among,top systems,hw - tsc consistently among top systems,0.7089477181434631
translation,65,246,results,results,has,hw - tsc,results has hw - tsc,0.5272117853164673
translation,65,252,results,perform better,than,baseline,perform better than baseline,0.6006702184677124
translation,65,252,results,baseline,for,all language pairs,baseline for all language pairs,0.5758873224258423
translation,65,252,results,all described systems,has,perform better,all described systems has perform better,0.6341732740402222
translation,65,252,results,results,has,all described systems,results has all described systems,0.5425713658332825
translation,65,254,results,nict kyoto,ranked among,top systems,nict kyoto ranked among top systems,0.7326564788818359
translation,65,254,results,top systems,for,all language pairs,top systems for all language pairs,0.580249011516571
translation,65,254,results,best performers,has,nict kyoto,best performers has nict kyoto,0.6007711291313171
translation,65,254,results,results,has,best performers,results has best performers,0.5442720651626587
translation,65,255,results,significantly outperform,has,all other systems,significantly outperform has all other systems,0.5886744856834412
translation,65,255,results,results,for,en- de,results for en- de,0.7156954407691956
translation,65,289,results,results,has,performance,results has performance,0.5972660779953003
translation,65,299,results,mcc,are,promising,mcc are promising,0.6444193720817566
translation,65,299,results,promising,especially for,en-cs and en- de,promising especially for en-cs and en- de,0.7032352089881897
translation,65,300,results,models,achieve,high f1 score,models achieve high f1 score,0.6367220878601074
translation,65,300,results,high f1 score,at,detecting errors,high f1 score at detecting errors,0.5342389345169067
translation,65,300,results,high f1 score,around,0.9 or higher,high f1 score around 0.9 or higher,0.6120632290840149
translation,65,300,results,0.9 or higher,for,all language pairs,0.9 or higher for all language pairs,0.5694824457168579
translation,65,378,results,superlearner model,improves,results,superlearner model improves results,0.6146064400672913
translation,65,378,results,results,over,non-mixture results,results over non-mixture results,0.6746920347213745
translation,65,378,results,results,has,superlearner model,results has superlearner model,0.5432092547416687
translation,66,218,ablation-analysis,all the translation context types,except for,zerocontext,all the translation context types except for zerocontext,0.62703537940979
translation,66,218,ablation-analysis,performance,drops,slowly,performance drops slowly,0.7791904807090759
translation,66,218,ablation-analysis,slowly,when,percentage of noise tokens,slowly when percentage of noise tokens,0.6764428019523621
translation,66,218,ablation-analysis,all the translation context types,has,performance,all the translation context types has performance,0.5524988174438477
translation,66,218,ablation-analysis,zerocontext,has,performance,zerocontext has performance,0.607210636138916
translation,66,218,ablation-analysis,percentage of noise tokens,has,increases,percentage of noise tokens has increases,0.6419641971588135
translation,66,218,ablation-analysis,ablation analysis,For,all the translation context types,ablation analysis For all the translation context types,0.5616616606712341
translation,66,11,baselines,effective method,for,gwlan,effective method for gwlan,0.6547759175300598
translation,66,11,baselines,effective method,compare it with,several strong baselines,effective method compare it with several strong baselines,0.6440966725349426
translation,66,172,baselines,trans -pe,train,vanilla nmt model,trans -pe train vanilla nmt model,0.6756777763366699
translation,66,172,baselines,vanilla nmt model,using,transformer - base model,vanilla nmt model using transformer - base model,0.682749330997467
translation,66,172,baselines,baselines,has,trans -pe,baselines has trans -pe,0.600827693939209
translation,66,175,baselines,trans - npe,train,nmt model,trans - npe train nmt model,0.7033507823944092
translation,66,175,baselines,nmt model,based on,transformer,nmt model based on transformer,0.6788463592529297
translation,66,175,baselines,nmt model,without,position encoding,nmt model without position encoding,0.693665087223053
translation,66,175,baselines,position encoding,on,target side,position encoding on target side,0.5389631986618042
translation,66,175,baselines,baselines,has,trans - npe,baselines has trans - npe,0.5705989003181458
translation,66,27,experiments,general word -level autocompletion ( gwlan ) task,construct,benchmark,general word -level autocompletion ( gwlan ) task construct benchmark,0.6807118058204651
translation,66,27,experiments,benchmark,with,automatic evaluation,benchmark with automatic evaluation,0.612750232219696
translation,66,173,model,inference process,use,context,inference process use context,0.6443948745727539
translation,66,173,model,inference process,return,most possible words,inference process return most possible words,0.7458398938179016
translation,66,173,model,context,on,left hand side,context on left hand side,0.5741024017333984
translation,66,173,model,left hand side,of,human input,left hand side of human input,0.581683337688446
translation,66,173,model,left hand side,of,human input,left hand side of human input,0.581683337688446
translation,66,173,model,human input,as,model input,human input as model input,0.5281829833984375
translation,66,173,model,most possible words,based on,probability,most possible words based on probability,0.6767144799232483
translation,66,173,model,probability,of,valid words,probability of valid words,0.5950766801834106
translation,66,173,model,valid words,selected out by,human input,valid words selected out by human input,0.6765637397766113
translation,66,173,model,model,During,inference process,model During inference process,0.6829817891120911
translation,66,179,results,our methods wpm - sep and wpm - joint,has,significantly outperform,our methods wpm - sep and wpm - joint has significantly outperform,0.566825807094574
translation,66,179,results,significantly outperform,has,three baseline methods,significantly outperform has three baseline methods,0.5637732744216919
translation,66,180,results,wpm - joint method,uses,joint training strategy,wpm - joint method uses joint training strategy,0.6080415844917297
translation,66,180,results,wpm - joint method,achieves,better overall performance,wpm - joint method achieves better overall performance,0.6699852347373962
translation,66,180,results,joint training strategy,to optimize,single model,joint training strategy to optimize single model,0.7341218590736389
translation,66,180,results,better overall performance,than,wpm - sep,better overall performance than wpm - sep,0.5873318314552307
translation,66,180,results,better overall performance,trains,four models,better overall performance trains four models,0.7382979989051819
translation,66,180,results,four models,for,different translation contexts,four models for different translation contexts,0.5982549786567688
translation,66,180,results,results,show,wpm - joint method,results show wpm - joint method,0.6211519837379456
translation,66,183,results,constraint,of,position,constraint of position,0.5992302298545837
translation,66,183,results,constraint,by removing,position encoding,constraint by removing position encoding,0.7496874928474426
translation,66,183,results,position,by removing,position encoding,position by removing position encoding,0.7364908456802368
translation,66,183,results,accuracy,of,model,accuracy of model,0.628264307975769
translation,66,183,results,model,has,improves,model has improves,0.6412354707717896
translation,66,183,results,results,use,same model,results use same model,0.6908242702484131
translation,66,184,results,transtable method,capable of leveraging,zero-context,transtable method capable of leveraging zero-context,0.7451282739639282
translation,66,184,results,transtable method,achieves,good results,transtable method achieves good results,0.6089971661567688
translation,66,184,results,good results,on,chinese - english task,good results on chinese - english task,0.5093234181404114
translation,66,184,results,chinese - english task,target language is,english,chinese - english task target language is english,0.8003339171409607
translation,66,184,results,results,has,interesting finding,results has interesting finding,0.566483199596405
translation,66,191,results,joint training,Compared with,wpm - sep,joint training Compared with wpm - sep,0.6887090802192688
translation,66,191,results,results,has,joint training,results has joint training,0.5075324177742004
translation,66,192,results,wpm - joint,yields,better performances,wpm - joint yields better performances,0.7165312170982361
translation,66,192,results,better performances,than,wpm - sep,better performances than wpm - sep,0.5993663668632507
translation,66,197,results,performances,of,two methods,performances of two methods,0.6100510358810425
translation,66,197,results,two methods,on,prefix and suffix translation contexts,two methods on prefix and suffix translation contexts,0.5140867829322815
translation,66,197,results,prefix and suffix translation contexts,are,nearly the same,prefix and suffix translation contexts are nearly the same,0.5752455592155457
translation,66,197,results,results,has,performances,results has performances,0.5711642503738403
translation,66,199,results,results,on,four translation contexts,results on four translation contexts,0.49954351782798767
translation,66,199,results,performances,on,bi-context,performances on bi-context,0.5718700289726257
translation,66,199,results,bi-context,better than,prefix and suffix,bi-context better than prefix and suffix,0.6857340931892395
translation,66,199,results,bi-context,better than,prefix and suffix,bi-context better than prefix and suffix,0.6857340931892395
translation,66,199,results,prefix and suffix,better than,zero-context,prefix and suffix better than zero-context,0.7224994897842407
translation,66,199,results,results,has,performances,results has performances,0.5711642503738403
translation,66,199,results,four translation contexts,has,performances,four translation contexts has performances,0.5458654761314392
translation,66,199,results,results,among,results,results among results,0.5670046210289001
translation,66,199,results,results,on,four translation contexts,results on four translation contexts,0.49954351782798767
translation,66,203,results,statistical results,shows that,averaged distances,statistical results shows that averaged distances,0.6605081558227539
translation,66,203,results,averaged distances,in,original sentence,averaged distances in original sentence,0.5098380446434021
translation,66,203,results,averaged distances,between,prediction words and translation contexts,averaged distances between prediction words and translation contexts,0.5989152193069458
translation,66,203,results,prediction words and translation contexts,are,various,prediction words and translation contexts are various,0.542317807674408
translation,66,203,results,prediction words and translation contexts,for,different translation contexts,prediction words and translation contexts for different translation contexts,0.5650449991226196
translation,66,203,results,various,for,different translation contexts,various for different translation contexts,0.5981444716453552
translation,66,203,results,different translation contexts,which are,"7.4 , 6.5 , 14.1 , and 3.2","different translation contexts which are 7.4 , 6.5 , 14.1 , and 3.2",0.6439640522003174
translation,66,203,results,"7.4 , 6.5 , 14.1 , and 3.2",for,"prefix , suffix , zero-context , and bi-context","7.4 , 6.5 , 14.1 , and 3.2 for prefix , suffix , zero-context , and bi-context",0.5684092044830322
translation,66,203,results,results,has,statistical results,results has statistical results,0.46913573145866394
translation,66,204,results,desired words,much closer to,context,desired words much closer to context,0.638631284236908
translation,66,204,results,trans - pe,achieve,better performances,trans - pe achieve better performances,0.642625093460083
translation,66,204,results,desired words,has,trans - pe,desired words has trans - pe,0.6449489593505859
translation,66,205,results,trans - pe,achieve,more than 80 accuracy scores,trans - pe achieve more than 80 accuracy scores,0.6531946659088135
translation,66,205,results,more than 80 accuracy scores,when,prediction word,more than 80 accuracy scores when prediction word,0.6765710115432739
translation,66,205,results,prediction word,is,next word,prediction word is next word,0.5685622096061707
translation,66,205,results,next word,of,given prefix,next word of given prefix,0.5913729667663574
translation,66,205,results,performance,has,drops,performance has drops,0.5993483662605286
translation,66,205,results,drops,has,significantly,drops has significantly,0.6735619306564331
translation,66,205,results,results,has,trans - pe,results has trans - pe,0.5752933621406555
translation,66,206,results,trans - npe,removes,position information,trans - npe removes position information,0.7125389575958252
translation,66,206,results,trans - npe,achieves,better overall performances,trans - npe achieves better overall performances,0.6701763272285461
translation,66,206,results,position information,of,target words,position information of target words,0.5644758939743042
translation,66,206,results,better overall performances,compared with,trans - pe,better overall performances compared with trans - pe,0.6809558868408203
translation,66,206,results,results,find that,trans - npe,results find that trans - npe,0.660527229309082
translation,66,207,results,performance,of,transtable,performance of transtable,0.674700140953064
translation,66,207,results,performance,position of,prediction words,performance position of prediction words,0.7054864168167114
translation,66,207,results,transtable,position of,prediction words,transtable position of prediction words,0.7529526948928833
translation,66,207,results,low variances,on,both tasks,low variances on both tasks,0.5083311796188354
translation,66,210,results,overall accu- racy,on,en? zh task,overall accu- racy on en? zh task,0.594535231590271
translation,66,210,results,overall accu- racy,is,much lower,overall accu- racy is much lower,0.595514714717865
translation,66,210,results,zh?en task,has,overall accu- racy,zh?en task has overall accu- racy,0.5733240842819214
translation,66,210,results,results,on,zh?en task,results on zh?en task,0.5814772248268127
translation,66,219,results,80 % words,in,context,80 % words in context,0.523495078086853
translation,66,219,results,performance,of,wpm - joint,performance of wpm - joint,0.6128203272819519
translation,66,219,results,outperforms,case of,zero-context,outperforms case of zero-context,0.6654720306396484
translation,66,219,results,outperforms,shows that,our wpm - joint method,outperforms shows that our wpm - joint method,0.7091893553733826
translation,66,219,results,our wpm - joint method,is,noise tolerant,our wpm - joint method is noise tolerant,0.6115844249725342
translation,66,219,results,80 % words,has,performance,80 % words has performance,0.6055319309234619
translation,66,219,results,wpm - joint,has,outperforms,wpm - joint has outperforms,0.6484185457229614
translation,67,122,experimental-setup,"nltk 3 ( bird et al. , 2009 ) implementation",of,ibm model,"nltk 3 ( bird et al. , 2009 ) implementation of ibm model",0.594343364238739
translation,67,122,experimental-setup,public implementation,of,vecmap,public implementation of vecmap,0.5657658576965332
translation,67,122,experimental-setup,experimental setup,use,"nltk 3 ( bird et al. , 2009 ) implementation","experimental setup use nltk 3 ( bird et al. , 2009 ) implementation",0.6134151220321655
translation,67,124,experimental-setup,minimum confidence threshold,set to,0.1,minimum confidence threshold set to 0.1,0.7086342573165894
translation,67,125,experimental-setup,final translations,for,test set,final translations for test set,0.6051133275032043
translation,67,125,experimental-setup,nearest neighbor,in,target - side mapped space,nearest neighbor in target - side mapped space,0.5467893481254578
translation,67,125,experimental-setup,nearest neighbor,to mitigate,hubness problem,nearest neighbor to mitigate hubness problem,0.6276530027389526
translation,67,125,experimental-setup,target - side mapped space,of,source word,target - side mapped space of source word,0.5659136176109314
translation,67,125,experimental-setup,experimental setup,has,final translations,experimental setup has final translations,0.5363599061965942
translation,67,150,experimental-setup,"maximum of 3,000 seeds",from,ibm model,"maximum of 3,000 seeds from ibm model",0.5500792264938354
translation,67,150,experimental-setup,experimental setup,induce,"maximum of 3,000 seeds","experimental setup induce maximum of 3,000 seeds",0.6673442721366882
translation,67,143,results,state - of- the - art results,in,"english - german , english - finnish , and english - spanish pairs","state - of- the - art results in english - german , english - finnish , and english - spanish pairs",0.4992874264717102
translation,67,143,results,results,achieve,state - of- the - art results,results achieve state - of- the - art results,0.5771380066871643
translation,67,147,results,parallel lines,of,bitext,parallel lines of bitext,0.6398472189903259
translation,67,147,results,performance,of,previous literature,performance of previous literature,0.5981407761573792
translation,67,147,results,performance,achieving,state - of - the - art performance,performance achieving state - of - the - art performance,0.639649510383606
translation,67,147,results,en-es,has,our model,en-es has our model,0.6466245055198669
translation,67,147,results,exceeds,has,performance,exceeds has performance,0.6372079253196716
translation,67,147,results,results,For,en-es,results For en-es,0.7515546679496765
translation,67,164,results,best performance,occurs,our combination method,best performance occurs our combination method,0.6451048851013184
translation,67,164,results,best performance,using,our combination method,best performance using our combination method,0.6424242258071899
translation,67,164,results,our combination method,of keeping,high - frequency translation pairs,our combination method of keeping high - frequency translation pairs,0.5626014471054077
translation,67,164,results,our combination method,of keeping,lower -frequency translation pairs,our combination method of keeping lower -frequency translation pairs,0.565427839756012
translation,67,164,results,high - frequency translation pairs,from,ibm model 2,high - frequency translation pairs from ibm model 2,0.5695888996124268
translation,67,164,results,high - frequency translation pairs,from,vecmap,high - frequency translation pairs from vecmap,0.5573580265045166
translation,67,164,results,high - frequency translation pairs,from,vecmap,high - frequency translation pairs from vecmap,0.5573580265045166
translation,67,164,results,number of induced seeds,has,constant,number of induced seeds has constant,0.573663055896759
translation,67,164,results,number of induced seeds,has,best performance,number of induced seeds has best performance,0.5535491704940796
translation,67,164,results,constant,has,best performance,constant has best performance,0.5911808013916016
translation,67,164,results,results,when holding,number of induced seeds,results when holding number of induced seeds,0.6715548038482666
translation,67,168,results,"of inducing 10,000 pairs",from,vecmap,"of inducing 10,000 pairs from vecmap",0.5899469256401062
translation,67,168,results,"of inducing 10,000 pairs",improves,performance,"of inducing 10,000 pairs improves performance",0.6650142073631287
translation,67,168,results,performance,over,"initial 3,000 seeds","performance over initial 3,000 seeds",0.6883412003517151
translation,67,168,results,"initial 3,000 seeds",across,all tested conditions,"initial 3,000 seeds across all tested conditions",0.6964012384414673
translation,67,168,results,secondary step,has,"of inducing 10,000 pairs","secondary step has of inducing 10,000 pairs",0.5902472138404846
translation,67,168,results,results,observe,secondary step,results observe secondary step,0.6126585602760315
translation,68,151,ablation-analysis,step 3,is,more effective,step 3 is more effective,0.5832704901695251
translation,68,151,ablation-analysis,step 3,using,only step 2,step 3 using only step 2,0.7263403534889221
translation,68,151,ablation-analysis,more effective,than,step 2,more effective than step 2,0.5490907430648804
translation,68,151,ablation-analysis,only step 2,n't help,ape,only step 2 n't help ape,0.6824910640716553
translation,68,151,ablation-analysis,ablation analysis,using,only step 2,ablation analysis using only step 2,0.6958718299865723
translation,68,146,experimental-setup,our system learn,with,junczys - dowmunt and grundkiewicz ( 2016 ),our system learn with junczys - dowmunt and grundkiewicz ( 2016 ),0.6598454713821411
translation,68,146,experimental-setup,our system learn,with,google translate,our system learn with google translate,0.6589868664741516
translation,68,146,experimental-setup,google translate,as,mt_ext,google translate as mt_ext,0.5451493263244629
translation,68,166,experimental-setup,batch size,to,256,batch size to 256,0.5870686173439026
translation,68,166,experimental-setup,256,for,step 2 and step 3,256 for step 2 and step 3,0.6918507218360901
translation,68,166,experimental-setup,step 2 and step 3,in,cts,step 2 and step 3 in cts,0.5658558011054993
translation,68,166,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,68,167,experimental-setup,initial learning rate,to,1e - 4,initial learning rate to 1e - 4,0.5612927675247192
translation,68,167,experimental-setup,1e - 4,using,"scheduler in fairseq ( ng et al. , 2019 )","1e - 4 using scheduler in fairseq ( ng et al. , 2019 )",0.6642809510231018
translation,68,167,experimental-setup,experimental setup,set,initial learning rate,experimental setup set initial learning rate,0.634415328502655
translation,68,169,experimental-setup,our models,using,"adamw ( loshchilov and hutter , 2019 ) optimizer","our models using adamw ( loshchilov and hutter , 2019 ) optimizer",0.6295393705368042
translation,68,169,experimental-setup,16 tesla a100 gpus,for,cts,16 tesla a100 gpus for cts,0.563005805015564
translation,68,169,experimental-setup,tesla v100 gpu,for,mls,tesla v100 gpu for mls,0.5899019837379456
translation,68,169,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,68,145,experiments,news -translation data,with,translated sentences,news -translation data with translated sentences,0.5997794270515442
translation,68,145,experiments,quality estimation nmt model,as,mt,quality estimation nmt model as mt,0.5072826743125916
translation,68,11,model,ape corpus,with,limited data,ape corpus with limited data,0.6326602697372437
translation,68,11,model,ape corpus,add,some related subtasks,ape corpus add some related subtasks,0.6096571683883667
translation,68,11,model,some related subtasks,to learn,unified representation,some related subtasks to learn unified representation,0.6032862067222595
translation,68,11,model,model,To fine- tune,ape corpus,model To fine- tune ape corpus,0.682500958442688
translation,68,59,model,cts,aims at,stepby-step learning,cts aims at stepby-step learning,0.5214289426803589
translation,68,59,model,model,has,cts,model has cts,0.618923544883728
translation,68,155,results,mls with dwa,has,better performance,mls with dwa has better performance,0.5827717781066895
translation,68,155,results,mls with dwa,has,better performance,mls with dwa has better performance,0.5827717781066895
translation,68,155,results,results,observe that,mls with dwa,results observe that mls with dwa,0.6169995665550232
translation,68,160,results,one using all the subtasks,performs,best,one using all the subtasks performs best,0.5994259715080261
translation,68,160,results,best,among,all the combinations,best among all the combinations,0.6037680506706238
translation,68,160,results,results,has,one using all the subtasks,results has one using all the subtasks,0.5806824564933777
translation,68,162,results,methods,on,wmt21 test dataset,methods on wmt21 test dataset,0.46838653087615967
translation,68,162,results,results,on,wmt21 test dataset,results on wmt21 test dataset,0.5174552798271179
translation,68,163,results,baseline scores,of,18.05 and 71.07,baseline scores of 18.05 and 71.07,0.5092107653617859
translation,68,163,results,18.05 and 71.07,higher than,development dataset,18.05 and 71.07 higher than development dataset,0.714385449886322
translation,68,163,results,development dataset,with,19.06 and 68.79,development dataset with 19.06 and 68.79,0.6180839538574219
translation,68,163,results,19.06 and 68.79,in terms of,ter and bleu,19.06 and 68.79 in terms of ter and bleu,0.7537655234336853
translation,68,163,results,test dataset,has,baseline scores,test dataset has baseline scores,0.5495261549949646
translation,68,163,results,results,has,test dataset,results has test dataset,0.5815747380256653
translation,68,164,results,our proposed methods,showed,effectiveness,our proposed methods showed effectiveness,0.6915929913520813
translation,68,168,results,average runtime,of,one epoch,average runtime of one epoch,0.5500122308731079
translation,68,168,results,one epoch,for,each approach,one epoch for each approach,0.6737498641014099
translation,68,168,results,one epoch,was,360 minutes,one epoch was 360 minutes,0.638887345790863
translation,68,168,results,one epoch,was,90 minutes,one epoch was 90 minutes,0.6371122598648071
translation,68,168,results,one epoch,was,40 seconds,one epoch was 40 seconds,0.6244000196456909
translation,68,168,results,one epoch,about,360 minutes,one epoch about 360 minutes,0.6366066932678223
translation,68,168,results,one epoch,about,90 minutes,one epoch about 90 minutes,0.6389872431755066
translation,68,168,results,one epoch,about,40 seconds,one epoch about 40 seconds,0.6115561127662659
translation,68,168,results,360 minutes,for,step 2,360 minutes for step 2,0.6500416398048401
translation,68,168,results,360 minutes,for,step 3,360 minutes for step 3,0.6613232493400574
translation,68,168,results,90 minutes,for,step 3,90 minutes for step 3,0.6562457084655762
translation,68,168,results,40 seconds,for,mls,40 seconds for mls,0.740339457988739
translation,68,168,results,results,has,average runtime,results has average runtime,0.5302354693412781
translation,69,340,ablation-analysis,ref-b,has,all but one metric ( tgt - regemt ),ref-b has all but one metric ( tgt - regemt ),0.6023306846618652
translation,69,340,ablation-analysis,all but one metric ( tgt - regemt ),has,greatly improve,all but one metric ( tgt - regemt ) has greatly improve,0.5764490962028503
translation,69,340,ablation-analysis,greatly improve,has,correlation score,greatly improve has correlation score,0.5048120617866516
translation,69,340,ablation-analysis,ablation analysis,By switching to,ref-b,ablation analysis By switching to ref-b,0.6975523233413696
translation,69,127,baselines,"bertscore bertscore ( zhang et al. , 2020 )",leverages,contextual embeddings,"bertscore bertscore ( zhang et al. , 2020 ) leverages contextual embeddings",0.6824015378952026
translation,69,127,baselines,contextual embeddings,from,pre-trained transformers,contextual embeddings from pre-trained transformers,0.5392407178878784
translation,69,127,baselines,contextual embeddings,to create,soft-alignments,contextual embeddings to create soft-alignments,0.586112916469574
translation,69,127,baselines,words,in,candidate and reference sentences,words in candidate and reference sentences,0.4910324513912201
translation,69,127,baselines,baselines,has,"bertscore bertscore ( zhang et al. , 2020 )","baselines has bertscore bertscore ( zhang et al. , 2020 )",0.5363239049911499
translation,69,141,baselines,openkiwi -mqm,is,multitask model,openkiwi -mqm is multitask model,0.5669374465942383
translation,69,141,baselines,multitask model,estimates,sentence - level mqm score,multitask model estimates sentence - level mqm score,0.61363285779953
translation,69,141,baselines,sentence - level mqm score,along with,word- level ok / bad tags,sentence - level mqm score along with word- level ok / bad tags,0.5649884939193726
translation,69,141,baselines,baselines,has,openkiwi -mqm,baselines has openkiwi -mqm,0.5264594554901123
translation,69,147,baselines,- 2,is,"bilingual , reference-less version","- 2 is bilingual , reference-less version",0.5812253355979919
translation,69,147,baselines,"bilingual , reference-less version",for,mt quality estimation,"bilingual , reference-less version for mt quality estimation",0.5661332607269287
translation,69,161,baselines,only source length and target length,of,given texts,only source length and target length of given texts,0.550999104976654
translation,69,161,baselines,baselines,has,regemt - baseline,baselines has regemt - baseline,0.6158676743507385
translation,69,213,experiments,comet - mqm _2021 and yisi - 1,show,highest correlation,comet - mqm _2021 and yisi - 1 show highest correlation,0.6261773109436035
translation,69,213,experiments,highest correlation,with,human ratings,highest correlation with human ratings,0.6359306573867798
translation,69,213,experiments,human ratings,on,ted domain,human ratings on ted domain,0.5372086763381958
translation,69,36,results,da,ranks,human translations,da ranks human translations,0.7868914604187012
translation,69,36,results,human translations,below,many mt systems,human translations below many mt systems,0.5903644561767578
translation,69,37,results,majority of automatic metrics,correlate better with,mqm,majority of automatic metrics correlate better with mqm,0.6862308382987976
translation,69,37,results,mqm,than,da scores from wmt,mqm than da scores from wmt,0.540903627872467
translation,69,37,results,results,has,majority of automatic metrics,results has majority of automatic metrics,0.5557446479797363
translation,69,41,results,all metrics,of,winning cluster,all metrics of winning cluster,0.5712074041366577
translation,69,41,results,winning cluster,on,news domain,winning cluster on news domain,0.5539771318435669
translation,69,41,results,winning cluster,show,lower correlation,winning cluster show lower correlation,0.6450262069702148
translation,69,41,results,lower correlation,with,human ratings,lower correlation with human ratings,0.6577242612838745
translation,69,41,results,results,has,all metrics,results has all metrics,0.5129380822181702
translation,69,47,results,top performances,across,different language pairs,top performances across different language pairs,0.6670299768447876
translation,69,47,results,three embeddingbased metrics,emerge,distinctly better,three embeddingbased metrics emerge distinctly better,0.6071429252624512
translation,69,47,results,comet -mqm _2021,emerge,distinctly better,comet -mqm _2021 emerge distinctly better,0.700917661190033
translation,69,47,results,distinctly better,than,others,distinctly better than others,0.6222361922264099
translation,69,47,results,top performances,has,three embeddingbased metrics,top performances has three embeddingbased metrics,0.5101040601730347
translation,69,47,results,results,counting,top performances,results counting top performances,0.7254058122634888
translation,69,48,results,reference - free metrics,are,relatively good,reference - free metrics are relatively good,0.5468307733535767
translation,69,48,results,relatively good,at rating,human translations,relatively good at rating human translations,0.7370622754096985
translation,69,48,results,under-perform,at,segment-level,under-perform at segment-level,0.549492359161377
translation,69,48,results,results,has,reference - free metrics,results has reference - free metrics,0.51796555519104
translation,69,113,results,reference translations,ranked,first,reference translations ranked first,0.7559710144996643
translation,69,113,results,all mt systems,except,ref-b,all mt systems except ref-b,0.7195509672164917
translation,69,113,results,all mt systems,except,ref-a,all mt systems except ref-a,0.7293526530265808
translation,69,113,results,ref-b,for,zh?en ted talks,ref-b for zh?en ted talks,0.7050926685333252
translation,69,113,results,ref-b,for,en?de newstest 2021,ref-b for en?de newstest 2021,0.7016015648841858
translation,69,113,results,ref-b,for,en?de newstest 2021,ref-b for en?de newstest 2021,0.7016015648841858
translation,69,113,results,ref-a,for,en?de newstest 2021,ref-a for en?de newstest 2021,0.7042161822319031
translation,69,209,results,reference - based metric c-specpn,are,winners,reference - based metric c-specpn are winners,0.5967119932174683
translation,69,209,results,results,has,qe metric comet -qe-mqm_2021,results has qe metric comet -qe-mqm_2021,0.6383447647094727
translation,69,210,results,embedding - based metrics,rely on,fine-tuning,embedding - based metrics rely on fine-tuning,0.730863630771637
translation,69,210,results,fine-tuning,are,much better,fine-tuning are much better,0.6209394335746765
translation,69,210,results,much better,in rating,human translation,much better in rating human translation,0.7019971013069153
translation,69,210,results,human translation,higher than,mt output,human translation higher than mt output,0.6453514695167542
translation,69,210,results,results,has,embedding - based metrics,results has embedding - based metrics,0.5319677591323853
translation,69,218,results,heat map,of,newstest2021,heat map of newstest2021,0.6065739989280701
translation,69,218,results,heat map,observe that,top performing metrics,heat map observe that top performing metrics,0.5501049757003784
translation,69,218,results,newstest2021,without,human translations,newstest2021 without human translations,0.7330167293548584
translation,69,218,results,top performing metrics,are,not significantly different,top performing metrics are not significantly different,0.5656008720397949
translation,69,218,results,results,looking at,heat map,results looking at heat map,0.654194712638855
translation,69,220,results,significantly better,than,all other metrics,significantly better than all other metrics,0.5336126089096069
translation,69,220,results,results,has,top 2 performing metrics,results has top 2 performing metrics,0.5340171456336975
translation,69,221,results,embedding - based metrics,fine- tuned on,previous years ' human ratings,embedding - based metrics fine- tuned on previous years ' human ratings,0.7382587790489197
translation,69,221,results,previous years ' human ratings,rate,human translations,previous years ' human ratings rate human translations,0.7425683736801147
translation,69,221,results,much better,than,all the other metrics,much better than all the other metrics,0.5525954961776733
translation,69,221,results,human translations,has,much better,human translations has much better,0.5760405659675598
translation,69,221,results,results,highlights,embedding - based metrics,results highlights embedding - based metrics,0.6526970863342285
translation,69,258,results,lexical metrics,observe that,sent - bleu,lexical metrics observe that sent - bleu,0.5222459435462952
translation,69,258,results,lexical metrics,observe that,chrf and cushlepor ( lm ),lexical metrics observe that chrf and cushlepor ( lm ),0.5615969300270081
translation,69,258,results,sent - bleu,not sensitive to,tokenized text,sent - bleu not sensitive to tokenized text,0.7128263711929321
translation,69,258,results,chrf and cushlepor ( lm ),not sensitive to,tokenized text,chrf and cushlepor ( lm ) not sensitive to tokenized text,0.7673653960227966
translation,69,258,results,sent - bleu,has,chrf and cushlepor ( lm ),sent - bleu has chrf and cushlepor ( lm ),0.6224044561386108
translation,69,258,results,results,Regarding,lexical metrics,results Regarding lexical metrics,0.5671178698539734
translation,69,278,results,embedding - based metrics,such as,bleurt - 20 and comet - mqm _2021,embedding - based metrics such as bleurt - 20 and comet - mqm _2021,0.5729686617851257
translation,69,278,results,embedding - based metrics,seem to be,less sensitive,embedding - based metrics seem to be less sensitive,0.6356404423713684
translation,69,278,results,less sensitive,to,subordination,less sensitive to subordination,0.6058372259140015
translation,69,278,results,less sensitive,to,named entities and terminology,less sensitive to named entities and terminology,0.5516230463981628
translation,69,278,results,less sensitive,to,punctuation,less sensitive to punctuation,0.5742650032043457
translation,69,278,results,results,observe,embedding - based metrics,results observe embedding - based metrics,0.5668376684188843
translation,69,279,results,clear performance difference,between,referencefree and reference - based metrics,clear performance difference between referencefree and reference - based metrics,0.644624650478363
translation,69,279,results,results,observe,clear performance difference,results observe clear performance difference,0.643268346786499
translation,69,326,results,"surface- level baselines - bleu , ter , and chrf",join,winners,"surface- level baselines - bleu , ter , and chrf join winners",0.6420165300369263
translation,69,326,results,winners,exclusively at,system level,winners exclusively at system level,0.6504622101783752
translation,69,326,results,results,has,"surface- level baselines - bleu , ter , and chrf","results has surface- level baselines - bleu , ter , and chrf",0.48602190613746643
translation,69,385,results,mqm,correlates better with,raw da scores,mqm correlates better with raw da scores,0.7070918679237366
translation,69,385,results,raw da scores,than with,z-normalized scores,raw da scores than with z-normalized scores,0.6427764892578125
translation,69,385,results,pearson correlations,are,0.508,pearson correlations are 0.508,0.5481610894203186
translation,69,385,results,english ? german and english ? russian,has,mqm,english ? german and english ? russian has mqm,0.6258740425109863
translation,69,385,results,results,For,english ? german and english ? russian,results For english ? german and english ? russian,0.5897141098976135
translation,69,391,results,da scores,perform,better,da scores perform better,0.636566698551178
translation,69,391,results,better,when judging,human output,better when judging human output,0.6981303691864014
translation,69,391,results,better,ranking,"7th , 3rd , and 3rd","better ranking 7th , 3rd , and 3rd",0.6853022575378418
translation,69,391,results,"7th , 3rd , and 3rd",for,"english ? german , english ? russian , and chinese ? english","7th , 3rd , and 3rd for english ? german , english ? russian , and chinese ? english",0.5587812066078186
translation,69,391,results,results,has,da scores,results has da scores,0.5040613412857056
translation,70,15,experimental-setup,containers,in,three different hardware environments,containers in three different hardware environments,0.5508928894996643
translation,70,15,experimental-setup,experimental setup,run,containers,experimental setup run containers,0.6288857460021973
translation,70,32,experimental-setup,gpu,is,nvidia a100,gpu is nvidia a100,0.5242924094200134
translation,70,32,experimental-setup,nvidia a100,from,oracle cloud bm.gpu4.8 instance,nvidia a100 from oracle cloud bm.gpu4.8 instance,0.5090565085411072
translation,70,32,experimental-setup,experimental setup,has,gpu,experimental setup has gpu,0.5304327607154846
translation,70,33,experimental-setup,instance,limited,docker,instance limited docker,0.6720370054244995
translation,70,33,experimental-setup,eight gpus,limited,docker,eight gpus limited docker,0.7146610617637634
translation,70,33,experimental-setup,instance,has,eight gpus,instance has eight gpus,0.558227002620697
translation,70,33,experimental-setup,experimental setup,limited,docker,experimental setup limited docker,0.6705884337425232
translation,70,33,experimental-setup,experimental setup,has,instance,experimental setup has instance,0.5284186005592346
translation,70,34,experimental-setup,amd epyc 7542 cpu,with,all cores allowed,amd epyc 7542 cpu with all cores allowed,0.5893061757087708
translation,70,34,experimental-setup,gpu machine,has,amd epyc 7542 cpu,gpu machine has amd epyc 7542 cpu,0.5208289623260498
translation,70,34,experimental-setup,experimental setup,has,gpu machine,experimental setup has gpu machine,0.5184701681137085
translation,71,114,ablation-analysis,over 3 bleu,to,performance,over 3 bleu to performance,0.544486403465271
translation,71,114,ablation-analysis,ablation analysis,addition of,vocabulary transfer,ablation analysis addition of vocabulary transfer,0.6221023797988892
translation,71,124,ablation-analysis,unmt,helps narrow,gap,unmt helps narrow gap,0.7974420785903931
translation,71,124,ablation-analysis,gap,in,performance,gap in performance,0.5397233963012695
translation,71,124,ablation-analysis,difference,of,3 bleu,difference of 3 bleu,0.6032428741455078
translation,71,124,ablation-analysis,3 bleu,shows that,unsupervised training,3 bleu shows that unsupervised training,0.5240504741668701
translation,71,124,ablation-analysis,unsupervised training,stand to benefit from,better vocabulary initialization,unsupervised training stand to benefit from better vocabulary initialization,0.6882902979850769
translation,71,124,ablation-analysis,unmt,has,difference,unmt has difference,0.6268360018730164
translation,71,48,baselines,"bpe ( sennrich et al. , 2016 )",for,all languages,"bpe ( sennrich et al. , 2016 ) for all languages",0.5622208118438721
translation,71,48,baselines,"bpe ( sennrich et al. , 2016 )",using,fastbpe,"bpe ( sennrich et al. , 2016 ) using fastbpe",0.6130789518356323
translation,71,62,baselines,baselines,has,denoising auto-encoding ( ae ),baselines has denoising auto-encoding ( ae ),0.5461089611053467
translation,71,47,experimental-setup,tokenized,using,moses toolkit,tokenized using moses toolkit,0.673223078250885
translation,71,47,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,71,74,experimental-setup,vocabulary,train,two word embeddings,vocabulary train two word embeddings,0.6584345102310181
translation,71,74,experimental-setup,two word embeddings,on,dsb and hsb data,two word embeddings on dsb and hsb data,0.524416983127594
translation,71,74,experimental-setup,dsb and hsb data,using,fasttext,dsb and hsb data using fasttext,0.7134382724761963
translation,71,74,experimental-setup,fasttext,has,"bojanowski et al. , 2017 )","fasttext has bojanowski et al. , 2017 )",0.5786325335502625
translation,71,74,experimental-setup,experimental setup,align,vocabulary,experimental setup align vocabulary,0.6456170082092285
translation,71,85,experimental-setup,training,done on,nvidia v100 32gb gpu,training done on nvidia v100 32gb gpu,0.630165696144104
translation,71,86,experimental-setup,training step,of,model,training step of model,0.6078580617904663
translation,71,86,experimental-setup,model,limited to,24 hours,model limited to 24 hours,0.7523965835571289
translation,71,86,experimental-setup,model,using,mpc data,model using mpc data,0.6871353387832642
translation,71,86,experimental-setup,first step,trained for,2 days,first step trained for 2 days,0.7962519526481628
translation,71,86,experimental-setup,experimental setup,has,training step,experimental setup has training step,0.5325546264648438
translation,71,106,experiments,vocabulary transfer,see that,model,vocabulary transfer see that model,0.6038230061531067
translation,71,106,experiments,vocabulary transfer,see that,model,vocabulary transfer see that model,0.6038230061531067
translation,71,106,experiments,initialization,than,model,initialization than model,0.5598967671394348
translation,71,5,model,transformer encoder-decoder architecture,make,three changes,transformer encoder-decoder architecture make three changes,0.6932380795478821
translation,71,7,results,novel method,for initializing,vocabulary,novel method for initializing vocabulary,0.7723993062973022
translation,71,7,results,novel method,achieving,improvements,novel method achieving improvements,0.7015816569328308
translation,71,7,results,vocabulary,of,unseen language,vocabulary of unseen language,0.5986533164978027
translation,71,7,results,improvements,of,3.2 bleu,improvements of 3.2 bleu,0.4956565797328949
translation,71,7,results,improvements,of,4.0 bleu,improvements of 4.0 bleu,0.5441602468490601
translation,71,7,results,3.2 bleu,for,de?dsb,3.2 bleu for de?dsb,0.600354790687561
translation,71,7,results,3.2 bleu,for,dsb ?de,3.2 bleu for dsb ?de,0.6373885869979858
translation,71,7,results,3.2 bleu,for,dsb ?de,3.2 bleu for dsb ?de,0.6373885869979858
translation,71,7,results,4.0 bleu,for,dsb ?de,4.0 bleu for dsb ?de,0.6619874238967896
translation,71,7,results,results,introduce,novel method,results introduce novel method,0.6340789794921875
translation,71,94,results,mt,using,synthetic data,mt using synthetic data,0.6722526550292969
translation,71,94,results,synthetic data,obtained from,offline bt,synthetic data obtained from offline bt,0.602716863155365
translation,71,94,results,offline bt,scores,best,offline bt scores best,0.7129777073860168
translation,71,94,results,best,for,de ? dsb,best for de ? dsb,0.68067866563797
translation,71,94,results,results,5 th step of,mt,results 5 th step of mt,0.5327959656715393
translation,71,103,results,equal,compared to,our model,equal compared to our model,0.7400429844856262
translation,71,103,results,equal,compared to,model,equal compared to model,0.7326033115386963
translation,71,103,results,our model,has,without vocabulary transfer,our model has without vocabulary transfer,0.5614928007125854
translation,71,103,results,model,has,with vocabulary transfer,model has with vocabulary transfer,0.6033770442008972
translation,71,108,results,performance,of,our model,performance of our model,0.5847885608673096
translation,71,108,results,our model,on,auxiliary languages,our model on auxiliary languages,0.5126621723175049
translation,71,108,results,auxiliary languages,is,better,auxiliary languages is better,0.6049678325653076
translation,71,108,results,better,focus on,learning,better focus on learning,0.7430114150047302
translation,71,108,results,learning,has,one language pair at a time,learning has one language pair at a time,0.5625208020210266
translation,71,108,results,results,shows that,performance,results shows that performance,0.6973981261253357
translation,71,115,results,both transfer methods,are,competitive,both transfer methods are competitive,0.6412312984466553
translation,71,115,results,competitive,with,each other,competitive with each other,0.6909433007240295
translation,71,115,results,each other,in terms of,improvement,each other in terms of improvement,0.6817161440849304
translation,71,115,results,improvement,to,performance,improvement to performance,0.5461621880531311
translation,71,115,results,results,see,both transfer methods,results see both transfer methods,0.5084538459777832
translation,71,122,results,vocabulary transfer,see,improvement,vocabulary transfer see improvement,0.609929084777832
translation,71,122,results,improvement,of,16 - 18 bleu,improvement of 16 - 18 bleu,0.594169557094574
translation,71,122,results,16 - 18 bleu,with,levenshtein version,16 - 18 bleu with levenshtein version,0.6469874978065491
translation,71,122,results,levenshtein version,performing,best,levenshtein version performing best,0.6011470556259155
translation,71,122,results,results,with,vocabulary transfer,results with vocabulary transfer,0.5505952835083008
translation,71,131,results,performance,of,model,performance of model,0.6080846190452576
translation,71,131,results,model,using,only offline bt,model using only offline bt,0.7177726030349731
translation,71,131,results,model,using,only online bt,model using only online bt,0.7005210518836975
translation,71,131,results,lower quality translations,compared to,only online bt,lower quality translations compared to only online bt,0.6790726184844971
translation,71,132,results,offline bt,with,online bt,offline bt with online bt,0.6453956365585327
translation,71,132,results,offline bt,makes up,difference,offline bt makes up difference,0.7044832110404968
translation,71,132,results,online bt,makes up,difference,online bt makes up difference,0.6977401375770569
translation,71,132,results,difference,in,performance,difference in performance,0.548568844795227
translation,71,132,results,performance,into,de,performance into de,0.725172221660614
translation,71,132,results,much worse,into,dsb,much worse into dsb,0.6066104173660278
translation,71,132,results,results,following,offline bt,results following offline bt,0.6532941460609436
translation,72,188,ablation-analysis,first finding,penalizing,translations,first finding penalizing translations,0.7330451607704163
translation,72,188,ablation-analysis,translations,incorrectly render,function words,translations incorrectly render function words,0.5656251907348633
translation,72,188,ablation-analysis,function words,seems to be,most difficult,function words seems to be most difficult,0.6196865439414978
translation,72,188,ablation-analysis,most difficult,for,bertscore,most difficult for bertscore,0.6594938635826111
translation,72,188,ablation-analysis,ablation analysis,With respect to,first finding,ablation analysis With respect to first finding,0.7154462933540344
translation,72,132,experiments,pe 2 rr,Using,pe 2 rr dataset,pe 2 rr Using pe 2 rr dataset,0.6851056218147278
translation,72,138,results,average bertscore,assigned to,pairs,average bertscore assigned to pairs,0.6888273358345032
translation,72,138,results,average bertscore,assigned to,correct translations pairs,average bertscore assigned to correct translations pairs,0.6314016580581665
translation,72,138,results,average bertscore,assigned to,correct translations pairs,average bertscore assigned to correct translations pairs,0.6314016580581665
translation,72,138,results,pairs,of,correct translations,pairs of correct translations,0.5910782217979431
translation,72,138,results,pairs,of,correct translations pairs,pairs of correct translations pairs,0.6020248532295227
translation,72,138,results,0.587,much lower than,tq - autotest,0.587 much lower than tq - autotest,0.6058781147003174
translation,72,138,results,average bertscore,assigned to,correct translations pairs,average bertscore assigned to correct translations pairs,0.6314016580581665
translation,72,138,results,correct translations pairs,was,0.815,correct translations pairs was 0.815,0.5973477363586426
translation,72,138,results,results,find that,average bertscore,results find that average bertscore,0.6580998301506042
translation,72,152,results,outperforms,achieves,accuracies,outperforms achieves accuracies,0.7450122833251953
translation,72,152,results,below 50 %,in,all categories,below 50 % in all categories,0.5317978858947754
translation,72,152,results,all categories,in,easy setting,all categories in easy setting,0.5237944722175598
translation,72,152,results,below 13 %,in,hard setting,below 13 % in hard setting,0.5694629549980164
translation,72,152,results,once more,has,bertscore,once more has bertscore,0.7068563103675842
translation,72,152,results,bertscore,has,outperforms,bertscore has outperforms,0.6507144570350647
translation,72,152,results,outperforms,has,bleu,outperforms has bleu,0.6163198947906494
translation,72,152,results,accuracies,has,below 50 %,accuracies has below 50 %,0.5675437450408936
translation,72,152,results,results,find that,once more,results find that once more,0.6850836277008057
translation,72,152,results,results,find that,bertscore,results find that bertscore,0.6118735671043396
translation,72,185,results,performance,of,bertscore,performance of bertscore,0.5843965411186218
translation,72,185,results,vary,based on,linguistic phenomena,vary based on linguistic phenomena,0.6788637042045593
translation,72,185,results,results,find that,performance,results find that performance,0.6555676460266113
translation,72,192,results,bertscore,more easily detect,bad translations,bertscore more easily detect bad translations,0.7062427401542664
translation,72,192,results,bad translations,when,less lexical overlap,bad translations when less lexical overlap,0.5976566672325134
translation,72,193,results,easy problem setting,where,good and bad candidate translations,easy problem setting where good and bad candidate translations,0.5815738439559937
translation,72,193,results,easy problem setting,both,good and bad candidate translations,easy problem setting both good and bad candidate translations,0.6129032373428345
translation,72,193,results,bertscore,easily distinguished,good translation,bertscore easily distinguished good translation,0.7412188649177551
translation,72,193,results,good translation,from,bad,good translation from bad,0.5503189563751221
translation,72,193,results,easy problem setting,has,bertscore,easy problem setting has bertscore,0.6047121286392212
translation,72,193,results,results,In,easy problem setting,results In easy problem setting,0.5162662863731384
translation,72,194,results,hard problem setting,where,bad translation,hard problem setting where bad translation,0.5799328088760376
translation,72,194,results,bad translation,has,high lexical overlap,bad translation has high lexical overlap,0.5280941128730774
translation,72,194,results,bad translation,has,stylistically similar,bad translation has stylistically similar,0.5483465790748596
translation,72,194,results,hard problem setting,has,bertscore,hard problem setting has bertscore,0.6085891723632812
translation,72,194,results,bad translation,has,high lexical overlap,bad translation has high lexical overlap,0.5280941128730774
translation,72,194,results,bad translation,has,bertscore,bad translation has bertscore,0.5863355994224548
translation,72,194,results,bertscore,has,struggled,bertscore has struggled,0.682017982006073
translation,72,194,results,results,in,hard problem setting,results in hard problem setting,0.5191967487335205
translation,72,197,results,bertscore,able to fulfill,three of the conditions,bertscore able to fulfill three of the conditions,0.7516558170318604
translation,72,197,results,gec scenario,has,bertscore,gec scenario has bertscore,0.5990261435508728
translation,72,197,results,results,In,gec scenario,results In gec scenario,0.5759168863296509
translation,73,39,ablation-analysis,telu-ku-175 m_hpo,improve,scores,telu-ku-175 m_hpo improve scores,0.7090332508087158
translation,73,39,ablation-analysis,scores,by,1.08 - 1.41,scores by 1.08 - 1.41,0.5638554096221924
translation,73,39,ablation-analysis,1.08 - 1.41,over,baseline,1.08 - 1.41 over baseline,0.6094271540641785
translation,73,39,ablation-analysis,ablation analysis,has,telu-ku-175 m_hpo,ablation analysis has telu-ku-175 m_hpo,0.6245129704475403
translation,73,114,ablation-analysis,lr,is,most important hyperparameter,lr is most important hyperparameter,0.5164225101470947
translation,73,114,ablation-analysis,bs,is,least important,bs is least important,0.5982041954994202
translation,73,114,ablation-analysis,ablation analysis,shows,lr,ablation analysis shows lr,0.6545792818069458
translation,73,114,ablation-analysis,ablation analysis,shows,bs,ablation analysis shows bs,0.6789777874946594
translation,73,72,experimental-setup,all the training and evaluation datasets,by,sentencepiece tokenizer,all the training and evaluation datasets by sentencepiece tokenizer,0.5449875593185425
translation,73,72,experimental-setup,experimental setup,tokenized,all the training and evaluation datasets,experimental setup tokenized all the training and evaluation datasets,0.7405183911323547
translation,73,82,experimental-setup,30 iterations,of,random searches,30 iterations of random searches,0.5810704827308655
translation,73,82,experimental-setup,random searches,for,two epochs,random searches for two epochs,0.635833740234375
translation,73,82,experimental-setup,experimental setup,run,30 iterations,experimental setup run 30 iterations,0.7652910947799683
translation,73,4,experiments,of largescale multilingual machine translation,for,five southeast asian languages,of largescale multilingual machine translation for five southeast asian languages,0.5469146966934204
translation,73,6,experiments,two models,with,average bleu scores,two models with average bleu scores,0.609819769859314
translation,73,6,experiments,average bleu scores,of,12.46 and 13.19,average bleu scores of 12.46 and 13.19,0.5518883466720581
translation,73,97,hyperparameters,hpo,using,random search,hpo using random search,0.6877615451812744
translation,73,97,hyperparameters,random search,for,30 iterations,random search for 30 iterations,0.5859952569007874
translation,73,97,hyperparameters,30 iterations,using,10 %,30 iterations using 10 %,0.7337858080863953
translation,73,97,hyperparameters,10 %,of,dataset,10 % of dataset,0.6420595049858093
translation,73,97,hyperparameters,hyperparameters,run,hpo,hyperparameters run hpo,0.7375826239585876
translation,73,7,results,our models,show,improvement,our models show improvement,0.7110897898674011
translation,73,7,results,improvement,in,most language pairs,improvement in most language pairs,0.5007990002632141
translation,73,7,results,improvement,after optimizing,hyperparameters,improvement after optimizing hyperparameters,0.7478238940238953
translation,73,7,results,results,has,our models,results has our models,0.5733726620674133
translation,73,38,results,telu - ku - 175 m,able to,improves,telu - ku - 175 m able to improves,0.6619316339492798
translation,73,38,results,average bleu scores,by,0.35- 0.59,average bleu scores by 0.35- 0.59,0.5856900811195374
translation,73,38,results,0.35- 0.59,over,m2m -100 175 m,0.35- 0.59 over m2m -100 175 m,0.650867760181427
translation,73,38,results,improves,has,average bleu scores,improves has average bleu scores,0.5531614422798157
translation,73,38,results,results,has,telu - ku - 175 m,results has telu - ku - 175 m,0.5955978631973267
translation,73,90,results,our models,improved,bleu scores,our models improved bleu scores,0.6453638672828674
translation,73,90,results,bleu scores,on,16 - 17 language pairs,bleu scores on 16 - 17 language pairs,0.48162367939949036
translation,73,90,results,results,has,our models,results has our models,0.5733726620674133
translation,73,91,results,improvement,in,each language pair,improvement in each language pair,0.5467103123664856
translation,73,91,results,telu - ku - 175 m and telu-ku-175 m_hpo,improved,bleu scores,telu - ku - 175 m and telu-ku-175 m_hpo improved bleu scores,0.6586250066757202
translation,73,91,results,bleu scores,by,0.04- 5.39 and 0.01 - 12.22,bleu scores by 0.04- 5.39 and 0.01 - 12.22,0.5830036401748657
translation,73,91,results,improvement,has,telu - ku - 175 m and telu-ku-175 m_hpo,improvement has telu - ku - 175 m and telu-ku-175 m_hpo,0.6734707355499268
translation,73,91,results,each language pair,has,telu - ku - 175 m and telu-ku-175 m_hpo,each language pair has telu - ku - 175 m and telu-ku-175 m_hpo,0.6650228500366211
translation,73,91,results,results,In terms of,improvement,results In terms of improvement,0.7343214154243469
translation,73,103,results,fine-tuning,of,10 % dataset,fine-tuning of 10 % dataset,0.5819481015205383
translation,73,103,results,10 % dataset,lead to,best results,10 % dataset lead to best results,0.6884665489196777
translation,73,103,results,best results,for,all language pairs translation,best results for all language pairs translation,0.5781992673873901
translation,73,103,results,best results,compared to,manual hyperparameter tuning ( telu - ku - 175 m ),best results compared to manual hyperparameter tuning ( telu - ku - 175 m ),0.6388974189758301
translation,73,103,results,fine-tuning,using,100 % dataset ( telu - ku -175 m_hpo ),fine-tuning using 100 % dataset ( telu - ku -175 m_hpo ),0.675757110118866
translation,73,103,results,results,found that,fine-tuning,results found that fine-tuning,0.6532807350158691
translation,73,104,results,best hpo,using,10 % of datasets,best hpo using 10 % of datasets,0.6587585806846619
translation,73,104,results,best hpo,resulted in,average bleu,best hpo resulted in average bleu,0.5663092732429504
translation,73,104,results,average bleu,of,12.75,average bleu of 12.75,0.5565547347068787
translation,73,104,results,average bleu,of,12.33,average bleu of 12.33,0.5511289238929749
translation,73,104,results,average bleu,of,12.39,average bleu of 12.39,0.5435371994972229
translation,73,104,results,12.75,for,dev,12.75 for dev,0.6642541289329529
translation,73,104,results,12.33,for,devtest,12.33 for devtest,0.6330385804176331
translation,73,104,results,12.39,for,test datasets,12.39 for test datasets,0.5487484335899353
translation,73,104,results,results,has,best hpo,results has best hpo,0.6304123401641846
translation,74,7,ablation-analysis,significant factor,handling,datascarce domains,significant factor handling datascarce domains,0.6534503698348999
translation,74,7,ablation-analysis,ablation analysis,assume,domain-general knowledge,ablation analysis assume domain-general knowledge,0.6799011826515198
translation,74,160,ablation-analysis,unsupervised and supervised models,initialized by,xlm,unsupervised and supervised models initialized by xlm,0.7105448842048645
translation,74,160,ablation-analysis,worst performance,in,all the cases,worst performance in all the cases,0.5288406610488892
translation,74,190,ablation-analysis,additional losses,i.e.,crossdomain and aggregated meta-train losses,additional losses i.e. crossdomain and aggregated meta-train losses,0.63946133852005
translation,74,190,ablation-analysis,additional losses,beneficial in,boosting up,additional losses beneficial in boosting up,0.7010341882705688
translation,74,190,ablation-analysis,ability,for finding,optimal initialization point,ability for finding optimal initialization point,0.6975576877593994
translation,74,190,ablation-analysis,optimal initialization point,when training,model,optimal initialization point when training model,0.7461546063423157
translation,74,190,ablation-analysis,model,with,out-domain datasets,model with out-domain datasets,0.6072914600372314
translation,74,190,ablation-analysis,boosting up,has,ability,boosting up has ability,0.6276708841323853
translation,74,190,ablation-analysis,ablation analysis,indicate,additional losses,ablation analysis indicate additional losses,0.5901736617088318
translation,74,216,ablation-analysis,result,in,both directions of translation ( en ? de ),result in both directions of translation ( en ? de ),0.5549865365028381
translation,74,216,ablation-analysis,cross-domain and aggregated meta-train losses,has,significantly enhances,cross-domain and aggregated meta-train losses has significantly enhances,0.5424879789352417
translation,74,216,ablation-analysis,significantly enhances,has,result,significantly enhances has result,0.6048168540000916
translation,74,216,ablation-analysis,ablation analysis,combining,cross-domain and aggregated meta-train losses,ablation analysis combining cross-domain and aggregated meta-train losses,0.7125600576400757
translation,74,149,experimental-setup,"transformer ( vaswani et al. , 2017 )",initialized by,masked language model,"transformer ( vaswani et al. , 2017 ) initialized by masked language model",0.6433190107345581
translation,74,149,experimental-setup,masked language model,from,"xlm ( conneau and lample , 2019 )","masked language model from xlm ( conneau and lample , 2019 )",0.5894497036933899
translation,74,149,experimental-setup,masked language model,using,our out-domain datasets,masked language model using our out-domain datasets,0.6025963425636292
translation,74,231,experimental-setup,byte-pair encoding ( bpe ),to build,shared sub-word vocabulary,byte-pair encoding ( bpe ) to build shared sub-word vocabulary,0.6630785465240479
translation,74,231,experimental-setup,shared sub-word vocabulary,using,fastbpe,shared sub-word vocabulary using fastbpe,0.6787766218185425
translation,74,231,experimental-setup,fastbpe,with,"60,000 bpe codes","fastbpe with 60,000 bpe codes",0.6711006164550781
translation,74,231,experimental-setup,experimental setup,use,byte-pair encoding ( bpe ),experimental setup use byte-pair encoding ( bpe ),0.6121708154678345
translation,74,233,experimental-setup,of the models,using,pytorch library,of the models using pytorch library,0.6412211656570435
translation,74,233,experimental-setup,four nvidia v100 gpus,for,pretraining and finetuning,four nvidia v100 gpus for pretraining and finetuning,0.580474317073822
translation,74,233,experimental-setup,experimental setup,implement,of the models,experimental setup implement of the models,0.5987076163291931
translation,74,236,experimental-setup,learning rate,set to,10 ?4,learning rate set to 10 ?4,0.7256936430931091
translation,74,236,experimental-setup,10 ?4,optimized within,range of 10 ?2 to 10 ?5,10 ?4 optimized within range of 10 ?2 to 10 ?5,0.722714900970459
translation,74,236,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,74,237,experimental-setup,number of tokens per batch,set as,"1,120","number of tokens per batch set as 1,120",0.6081487536430359
translation,74,237,experimental-setup,dropout rate,set as,0.1,dropout rate set as 0.1,0.568564772605896
translation,74,237,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,74,239,experimental-setup,pretraining stage,follow,same stopping criterion,pretraining stage follow same stopping criterion,0.6311103701591492
translation,74,239,experimental-setup,experimental setup,In,pretraining stage,experimental setup In pretraining stage,0.5232921838760376
translation,74,164,experiments,proposed approaches,i.e.,metaumt and metagumt ),proposed approaches i.e. metaumt and metagumt ),0.717596173286438
translation,74,164,experiments,other two finetuning models,i.e.,transfer learning,other two finetuning models i.e. transfer learning,0.627891480922699
translation,74,164,experiments,other two finetuning models,i.e.,mixed finetuned ones,other two finetuning models i.e. mixed finetuned ones,0.6553605794906616
translation,74,173,experiments,transfer learning approach,shows,worst performance,transfer learning approach shows worst performance,0.6577728986740112
translation,74,173,experiments,worst performance,except for,unadapted model,worst performance except for unadapted model,0.6301468014717102
translation,74,235,experiments,adam optimizer ( kingma and ba ),for,pretraining stage,adam optimizer ( kingma and ba ) for pretraining stage,0.5842508673667908
translation,74,235,experiments,adam optimizer ( kingma and ba ),for,finetuning stage,adam optimizer ( kingma and ba ) for finetuning stage,0.5982087254524231
translation,74,235,experiments,"adam warmup optimizer ( vaswani et al. , 2017 )",for,finetuning stage,"adam warmup optimizer ( vaswani et al. , 2017 ) for finetuning stage",0.5466601848602295
translation,74,238,experiments,meta-learning approaches,set,learning rates,meta-learning approaches set learning rates,0.6406095623970032
translation,74,238,experiments,learning rates,of,alpha and beta,learning rates of alpha and beta,0.6307828426361084
translation,74,238,experiments,alpha and beta,commonly as,0.0001,alpha and beta commonly as 0.0001,0.589292049407959
translation,74,8,model,metalearning algorithm,utilizes,knowledge,metalearning algorithm utilizes knowledge,0.6368159055709839
translation,74,8,model,knowledge,learned from,high- resource domains,knowledge learned from high- resource domains,0.7022207379341125
translation,74,8,model,knowledge,to boost,performance,knowledge to boost performance,0.6564064025878906
translation,74,8,model,performance,of,low-resource unmt,performance of low-resource unmt,0.6019353270530701
translation,74,8,model,model,extend,metalearning algorithm,model extend metalearning algorithm,0.7100741267204285
translation,74,27,model,new meta-learning approach,for,unmt,new meta-learning approach for unmt,0.6573906540870667
translation,74,27,model,metaumt,for,low-resource domains,metaumt for low-resource domains,0.5939651727676392
translation,74,27,model,low-resource domains,by defining,each task,low-resource domains by defining each task,0.6196671724319458
translation,74,27,model,model,introduce,new meta-learning approach,model introduce new meta-learning approach,0.6703822016716003
translation,74,28,model,objective,of,metaumt,objective of metaumt,0.6079663634300232
translation,74,28,model,metaumt,find,optimal initialization,metaumt find optimal initialization,0.578955352306366
translation,74,28,model,optimal initialization,for,model parameters,optimal initialization for model parameters,0.6043172478675842
translation,74,28,model,model,has,objective,model has objective,0.5278581380844116
translation,74,36,model,improved meta-learning approach,called,metagumt,improved meta-learning approach called metagumt,0.6462692618370056
translation,74,36,model,metagumt,for,lowresource unmt,metagumt for lowresource unmt,0.6433013677597046
translation,74,36,model,lowresource unmt,by explicitly infusing,common knowledge,lowresource unmt by explicitly infusing common knowledge,0.6848741769790649
translation,74,36,model,common knowledge,across,multiple source domains,common knowledge across multiple source domains,0.7036641836166382
translation,74,36,model,generalizable knowledge,from,one particular domain,generalizable knowledge from one particular domain,0.5708582401275635
translation,74,36,model,model,propose,improved meta-learning approach,model propose improved meta-learning approach,0.6847308874130249
translation,74,150,model,model,consist of,6 layers,model consist of 6 layers,0.7235221862792969
translation,74,198,model,cross-domain loss,encourages,model,cross-domain loss encourages model,0.6533412933349609
translation,74,198,model,model,to have,generalization capability,model to have generalization capability,0.6189361810684204
translation,74,198,model,generalization capability,after adapting to,low-resource target domain,generalization capability after adapting to low-resource target domain,0.7027267813682556
translation,74,198,model,model,has,cross-domain loss,model has cross-domain loss,0.5450014472007751
translation,74,214,model,aggregated meta-train loss,allows,model,aggregated meta-train loss allows model,0.5959593653678894
translation,74,214,model,model,to utilize,high- resource domain knowledge,model to utilize high- resource domain knowledge,0.7142906785011292
translation,74,214,model,high- resource domain knowledge,in,finetuning stage,high- resource domain knowledge in finetuning stage,0.48923808336257935
translation,74,214,model,model,to utilize,high- resource domain knowledge,model to utilize high- resource domain knowledge,0.7142906785011292
translation,74,214,model,model,has,aggregated meta-train loss,model has aggregated meta-train loss,0.5263923406600952
translation,74,224,model,model,proposes,novel meta-learning approach,model proposes novel meta-learning approach,0.7368582487106323
translation,74,225,model,improved method,called,metagumt,improved method called metagumt,0.6651564240455627
translation,74,225,model,improved method,enhances,cross-domain generalization,improved method enhances cross-domain generalization,0.6477721333503723
translation,74,225,model,improved method,maintains,high- resource domain knowledge,improved method maintains high- resource domain knowledge,0.6078441143035889
translation,74,225,model,model,introduce,improved method,model introduce improved method,0.63707035779953
translation,74,232,model,shared sub-word vocabulary,constructed from,out-domain datasets,shared sub-word vocabulary constructed from out-domain datasets,0.6773213744163513
translation,74,232,model,shared sub-word vocabulary,split,words,shared sub-word vocabulary split words,0.7248230576515198
translation,74,232,model,words,into,sub-word units,words into sub-word units,0.6342647075653076
translation,74,232,model,sub-word units,for,in-domain dataset,sub-word units for in-domain dataset,0.5831560492515564
translation,74,42,results,enhanced method,shows,fast convergence,enhanced method shows fast convergence,0.5904280543327332
translation,74,42,results,fast convergence,on,pre-training,fast convergence on pre-training,0.5864251852035522
translation,74,42,results,fast convergence,on,finetuning,fast convergence on finetuning,0.5926097631454468
translation,74,42,results,finetuning,i.e.,adapting to a target domain,finetuning i.e. adapting to a target domain,0.6510509252548218
translation,74,42,results,results,empirically demonstrate,enhanced method,results empirically demonstrate enhanced method,0.7045342922210693
translation,74,43,results,model,trained with,metagumt,model trained with metagumt,0.7315871715545654
translation,74,43,results,all baseline models,including,metaumt,all baseline models including metaumt,0.6819276213645935
translation,74,43,results,metagumt,has,consistently outperforms,metagumt has consistently outperforms,0.627556562423706
translation,74,43,results,consistently outperforms,has,all baseline models,consistently outperforms has all baseline models,0.5746190547943115
translation,74,43,results,results,has,model,results has model,0.5339115858078003
translation,74,162,results,performance,of,unadapted model,performance of unadapted model,0.5973585247993469
translation,74,162,results,unadapted model,is,far behind,unadapted model is far behind,0.5524342656135559
translation,74,162,results,far behind,compared to,other models,far behind compared to other models,0.7381032109260559
translation,74,162,results,other models,such as,mixed finetuned model,other models such as mixed finetuned model,0.6345216035842896
translation,74,162,results,other models,such as,transfer learning model,other models such as transfer learning model,0.6255081295967102
translation,74,162,results,other models,such as,metaumt,other models such as metaumt,0.6473801732063293
translation,74,162,results,other models,such as,metagumt,other models such as metagumt,0.6424041390419006
translation,74,165,results,our methods,exhibit,leading performances,our methods exhibit leading performances,0.5873328447341919
translation,74,165,results,our methods,consistently achieve,improvements,our methods consistently achieve improvements,0.6714138984680176
translation,74,165,results,leading performances,both directions of,translation ( en ? de ),leading performances both directions of translation ( en ? de ),0.689119815826416
translation,74,165,results,improvements,of,2 - 3 bleu score,improvements of 2 - 3 bleu score,0.552757740020752
translation,74,165,results,results,has,our methods,results has our methods,0.5312396883964539
translation,74,166,results,metagumt,consistently obtains,better bleu scores,metagumt consistently obtains better bleu scores,0.6311002373695374
translation,74,166,results,converges faster,than,metaumt,converges faster than metaumt,0.620574951171875
translation,74,166,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,171,results,metaumt,demonstrates,better performance,metaumt demonstrates better performance,0.6622803211212158
translation,74,171,results,better performance,than,other methods,better performance than other methods,0.5502663254737854
translation,74,171,results,other methods,in,various settings,other methods in various settings,0.4931941330432892
translation,74,171,results,transfer learning model,has,metaumt,transfer learning model has metaumt,0.5802168846130371
translation,74,171,results,results,Compared to,transfer learning model,results Compared to transfer learning model,0.6812009811401367
translation,74,172,results,metagumt,exhibits,even better performances,metagumt exhibits even better performances,0.6509578227996826
translation,74,172,results,even better performances,in,all settings,even better performances in all settings,0.5209444761276245
translation,74,172,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,182,results,quickly converge,in,low-resource domain,quickly converge in low-resource domain,0.5379778742790222
translation,74,182,results,quickly converge,improving,performances,quickly converge improving performances,0.650244414806366
translation,74,182,results,performances,over,transfer learning method,performances over transfer learning method,0.6800808906555176
translation,74,182,results,transfer learning method,in,various low-resource settings,transfer learning method in various low-resource settings,0.5352463722229004
translation,74,182,results,meta-learning - based methods,has,quickly converge,meta-learning - based methods has quickly converge,0.5245906710624695
translation,74,182,results,results,has,meta-learning - based methods,results has meta-learning - based methods,0.500697135925293
translation,74,188,results,metaumt and metagumt,has,not only converge,metaumt and metagumt has not only converge,0.6192768216133118
translation,74,188,results,not only converge,has,quickly,not only converge has quickly,0.6217535138130188
translation,74,188,results,outperform,has,other baseline methods,outperform has other baseline methods,0.5510974526405334
translation,74,189,results,metagumt,effective in achieving,optimized initialization,metagumt effective in achieving optimized initialization,0.5984370112419128
translation,74,189,results,optimized initialization,at,earlier iteration,optimized initialization at earlier iteration,0.5431916117668152
translation,74,189,results,metaumt,has,metagumt,metaumt has metagumt,0.6653937697410583
translation,74,189,results,results,compared to,metaumt,results compared to metaumt,0.688607394695282
translation,74,197,results,metagumt,achieves,superior performances,metagumt achieves superior performances,0.6719554662704468
translation,74,197,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,200,results,metagumt,has,outperforms,metagumt has outperforms,0.6668294668197632
translation,74,200,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,74,200,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,201,results,our model,has,domain generalization ability,our model has domain generalization ability,0.5553138256072998
translation,74,201,results,domain generalization ability,after,finetuning stage,domain generalization ability after finetuning stage,0.6326126456260681
translation,74,201,results,finetuning stage,due to,cross-domain loss,finetuning stage due to cross-domain loss,0.6925148367881775
translation,74,201,results,cross-domain loss,in,meta-test phase,cross-domain loss in meta-test phase,0.5524322390556335
translation,74,201,results,our model,has,domain generalization ability,our model has domain generalization ability,0.5553138256072998
translation,74,201,results,results,seen that,our model,results seen that our model,0.734937310218811
translation,74,207,results,outperforms,in,all unbalanced data cases,outperforms in all unbalanced data cases,0.5351523160934448
translation,74,207,results,metagumt,has,outperforms,metagumt has outperforms,0.6668294668197632
translation,74,207,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,212,results,effectiveness,of,crossdomain and aggregated meta-train losses,effectiveness of crossdomain and aggregated meta-train losses,0.5738545656204224
translation,74,212,results,crossdomain and aggregated meta-train losses,compared to,metaumt,crossdomain and aggregated meta-train losses compared to metaumt,0.5869952440261841
translation,74,212,results,metaumt,incorporating,cross-domain loss,metaumt incorporating cross-domain loss,0.7552183866500854
translation,74,212,results,cross-domain loss,improves,average bleu score,cross-domain loss improves average bleu score,0.6286981105804443
translation,74,212,results,average bleu score,by,0.21,average bleu score by 0.21,0.5548059940338135
translation,74,212,results,results,empirically show,effectiveness,results empirically show effectiveness,0.6011829376220703
translation,74,212,results,results,compared to,metaumt,results compared to metaumt,0.688607394695282
translation,74,215,results,average bleu score,by,0.37,average bleu score by 0.37,0.551983654499054
translation,74,215,results,0.37,from,metaumt,0.37 from metaumt,0.5318748950958252
translation,74,220,results,metagumt,has,consistently outperforms,metagumt has consistently outperforms,0.627556562423706
translation,74,220,results,consistently outperforms,has,"transfer , the mixedfinetune , and metaumt approaches","consistently outperforms has transfer , the mixedfinetune , and metaumt approaches",0.5681412220001221
translation,74,220,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,74,221,results,size,of,source domains,size of source domains,0.5888599157333374
translation,74,221,results,performance gap,between,ours and the transferring based models,performance gap between ours and the transferring based models,0.6205999851226807
translation,74,221,results,ours and the transferring based models,i.e.,transferring and mixed - finetune models,ours and the transferring based models i.e. transferring and mixed - finetune models,0.6608977913856506
translation,74,221,results,source domains,has,increases,source domains has increases,0.6209582090377808
translation,74,281,results,metagumt,shows,remarkable performances,metagumt shows remarkable performances,0.6707706451416016
translation,74,281,results,remarkable performances,over,metaumt,remarkable performances over metaumt,0.6831855177879333
translation,74,281,results,metaumt,in,all domains,metaumt in all domains,0.5269259810447693
translation,74,281,results,results,has,metagumt,results has metagumt,0.5252384543418884
translation,75,52,baselines,bilingual baseline,trained on,each language pair,bilingual baseline trained on each language pair,0.7092077136039734
translation,75,52,baselines,baselines,has,bilingual baseline,baselines has bilingual baseline,0.60870760679245
translation,75,61,experiments,multilingual models,with,"adam ( kingma and ba , 2015 ) ( ? 1 = 0.9 , ? 2 = 0.98 )","multilingual models with adam ( kingma and ba , 2015 ) ( ? 1 = 0.9 , ? 2 = 0.98 )",0.5951090455055237
translation,75,60,hyperparameters,transformer big architecture,as,backbone model,transformer big architecture as backbone model,0.563636302947998
translation,75,60,hyperparameters,6 layers,with,embedding size,6 layers with embedding size,0.6053335070610046
translation,75,60,hyperparameters,6 layers,with,dropout,6 layers with dropout,0.6230697631835938
translation,75,60,hyperparameters,6 layers,with,feed - forward network size,6 layers with feed - forward network size,0.6321370601654053
translation,75,60,hyperparameters,6 layers,with,16 attention heads,6 layers with 16 attention heads,0.6301254630088806
translation,75,60,hyperparameters,embedding size,of,1024,embedding size of 1024,0.6278319954872131
translation,75,60,hyperparameters,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,75,60,hyperparameters,feed - forward network size,of,4096,feed - forward network size of 4096,0.606183648109436
translation,75,60,hyperparameters,hyperparameters,adopt,transformer big architecture,hyperparameters adopt transformer big architecture,0.6489921808242798
translation,75,62,hyperparameters,learning rate,set as,5e - 4,learning rate set as 5e - 4,0.6365476846694946
translation,75,62,hyperparameters,5e - 4,with,warm - up step,5e - 4 with warm - up step,0.6724618673324585
translation,75,62,hyperparameters,warm - up step,of,"4,000","warm - up step of 4,000",0.5997728705406189
translation,75,62,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,75,63,hyperparameters,label smoothing cross-entropy,with,smoothing ratio,label smoothing cross-entropy with smoothing ratio,0.6123787760734558
translation,75,63,hyperparameters,smoothing ratio,of,0.1,smoothing ratio of 0.1,0.6298884749412537
translation,75,63,hyperparameters,hyperparameters,trained with,label smoothing cross-entropy,hyperparameters trained with label smoothing cross-entropy,0.675887942314148
translation,75,64,hyperparameters,batch size,is,"5,120 tokens","batch size is 5,120 tokens",0.5587164163589478
translation,75,64,hyperparameters,parameters,updated,16 iterations,parameters updated 16 iterations,0.6259520053863525
translation,75,64,hyperparameters,16 iterations,to simulate,128 - gpu environment,16 iterations to simulate 128 - gpu environment,0.6460747718811035
translation,75,64,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,75,64,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,75,6,model,novel agreement - based method,to encourage,multilingual agreement,novel agreement - based method to encourage multilingual agreement,0.6466504335403442
translation,75,6,model,multilingual agreement,among,different translation directions,multilingual agreement among different translation directions,0.5334900617599487
translation,75,6,model,model,propose,novel agreement - based method,model propose novel agreement - based method,0.6832628846168518
translation,75,16,model,novel agreementbased method,explicitly models,shared semantic space,novel agreementbased method explicitly models shared semantic space,0.7473059892654419
translation,75,16,model,novel agreementbased method,encourages,agreement,novel agreementbased method encourages agreement,0.639055073261261
translation,75,16,model,shared semantic space,for,multiple languages,shared semantic space for multiple languages,0.5983554124832153
translation,75,16,model,model,propose,novel agreementbased method,model propose novel agreementbased method,0.6862769722938538
translation,75,17,model,multilingual translation,with,agreement term,multilingual translation with agreement term,0.5595107674598694
translation,75,17,model,model,to produce,source sentence,model to produce source sentence,0.6987951993942261
translation,75,17,model,source sentence,with,multiple languages,source sentence with multiple languages,0.5927649736404419
translation,75,17,model,multiple languages,into,target sentence,multiple languages into target sentence,0.5488090515136719
translation,75,67,results,our method,further improves,one-to - many and one-to- many + pseudo consistently,our method further improves one-to - many and one-to- many + pseudo consistently,0.7516460418701172
translation,75,67,results,results,has,our method,results has our method,0.5589964985847473
translation,75,68,results,pseudo and codeswitched data,brings,more improvements,pseudo and codeswitched data brings more improvements,0.6585989594459534
translation,75,68,results,more improvements,to,"low-resource languages ( et , ro , hi , tr , and gu )","more improvements to low-resource languages ( et , ro , hi , tr , and gu )",0.5510799884796143
translation,75,68,results,"low-resource languages ( et , ro , hi , tr , and gu )",than,"high- resource languages ( fr , cs , de , fi , and lv )","low-resource languages ( et , ro , hi , tr , and gu ) than high- resource languages ( fr , cs , de , fi , and lv )",0.549450695514679
translation,75,68,results,results,Using,pseudo and codeswitched data,results Using pseudo and codeswitched data,0.6512225270271301
translation,75,70,results,results,on,x?en,results on x?en,0.6094858050346375
translation,75,71,results,bilingual nmt,by,+ 4.2 bleu points,bilingual nmt by + 4.2 bleu points,0.5961260199546814
translation,75,71,results,many - to- one,has,outperforms,many - to- one has outperforms,0.6058424711227417
translation,75,71,results,outperforms,has,bilingual nmt,outperforms has bilingual nmt,0.5775583386421204
translation,75,71,results,results,has,many - to- one,results has many - to- one,0.5522086024284363
translation,75,72,results,parallel data,with,pseudo data,parallel data with pseudo data,0.618312656879425
translation,75,72,results,pseudo data,leading to,improvement,pseudo data leading to improvement,0.7623088955879211
translation,75,72,results,improvement,of,+ 1.9 bleu points,improvement of + 1.9 bleu points,0.5263528227806091
translation,75,72,results,+ 1.9 bleu points,over,manyto - one,+ 1.9 bleu points over manyto - one,0.6477963328361511
translation,75,72,results,results,combine,parallel data,results combine parallel data,0.653410792350769
translation,75,73,results,manyto-one + pseudo,by,large gain,manyto-one + pseudo by large gain,0.6064318418502808
translation,75,73,results,large gain,of,+ 0.5 bleu points on average,large gain of + 0.5 bleu points on average,0.534164309501648
translation,75,73,results,outperforms,has,manyto-one + pseudo,outperforms has manyto-one + pseudo,0.6160368919372559
translation,75,73,results,results,has,our method,results has our method,0.5589964985847473
translation,75,82,results,phraselevel substitution,works,better,phraselevel substitution works better,0.6224835515022278
translation,76,8,baselines,xlm,to initialize,encoder,xlm to initialize encoder,0.7706772089004517
translation,76,8,baselines,xlm,randomly initialize,shallow decoder,xlm randomly initialize shallow decoder,0.7814139127731323
translation,76,56,experimental-setup,default fairseq parameters,for,finetuning,default fairseq parameters for finetuning,0.6000045537948608
translation,76,56,experimental-setup,finetuning,has,adam optimizer,finetuning has adam optimizer,0.5648756623268127
translation,76,57,experimental-setup,polynomial decay learning rate schedule,starts from,5e? 04,polynomial decay learning rate schedule starts from 5e? 04,0.6945933699607849
translation,76,57,experimental-setup,polynomial decay learning rate schedule,gradually decays to,0,polynomial decay learning rate schedule gradually decays to 0,0.697928786277771
translation,76,57,experimental-setup,warmed up,to,over 1000 updates,warmed up to over 1000 updates,0.5756941437721252
translation,76,57,experimental-setup,0,has,over around 60 k updates,0 has over around 60 k updates,0.6119453310966492
translation,76,57,experimental-setup,experimental setup,has,polynomial decay learning rate schedule,experimental setup has polynomial decay learning rate schedule,0.49997469782829285
translation,76,58,experimental-setup,model,fine-tuned for,2 days,model fine-tuned for 2 days,0.7961452603340149
translation,76,58,experimental-setup,2 days,on,4 nvidia v100 gpus,2 days on 4 nvidia v100 gpus,0.5262125730514526
translation,76,58,experimental-setup,experimental setup,fine-tuned for,2 days,experimental setup fine-tuned for 2 days,0.685332179069519
translation,76,58,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,76,59,experimental-setup,final learning rate,around,3e?04,final learning rate around 3e?04,0.6948599219322205
translation,76,59,experimental-setup,3e?04,with,batch size,3e?04 with batch size,0.6554487347602844
translation,76,59,experimental-setup,batch size,of,3,batch size of 3,0.6878668069839478
translation,76,59,experimental-setup,batch size,of,072 sentences,batch size of 072 sentences,0.6109607219696045
translation,76,59,experimental-setup,3,",",072 sentences,"3 , 072 sentences",0.6912413835525513
translation,76,59,experimental-setup,experimental setup,has,final learning rate,experimental setup has final learning rate,0.5062005519866943
translation,76,60,experimental-setup,inference,use,beam search generation algorithm,inference use beam search generation algorithm,0.6070883870124817
translation,76,60,experimental-setup,beam search generation algorithm,with,beam size,beam search generation algorithm with beam size,0.6110588312149048
translation,76,60,experimental-setup,beam size,of,5,beam size of 5,0.7073217034339905
translation,76,60,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,76,13,experiments,translation,in,four romance languages,translation in four romance languages,0.5095816850662231
translation,76,7,model,pre-trained language model,namely,xlm - roberta,pre-trained language model namely xlm - roberta,0.7130345106124878
translation,76,7,model,pre-trained language model,later finetuned with,parallel data,pre-trained language model later finetuned with parallel data,0.6924545764923096
translation,76,7,model,parallel data,obtained mostly from,opus,parallel data obtained mostly from opus,0.652462899684906
translation,76,7,model,model,based on,pre-trained language model,model based on pre-trained language model,0.5668734908103943
translation,76,67,results,our system,ranked,5th in average,our system ranked 5th in average,0.7336186170578003
translation,76,67,results,our system,ranked,3rd,our system ranked 3rd,0.737127959728241
translation,76,67,results,our system,ranked,4th,our system ranked 4th,0.6945348381996155
translation,76,67,results,our system,ranked,6th,our system ranked 6th,0.6774703860282898
translation,76,67,results,3rd,in,catalan - to - occitan direction,3rd in catalan - to - occitan direction,0.5851883292198181
translation,76,67,results,4th,in,catalanto -romanian direction,4th in catalanto -romanian direction,0.5965735912322998
translation,76,67,results,6th,in,catalan - to -italian direction,6th in catalan - to -italian direction,0.5637542009353638
translation,76,77,results,average results,of,our system,average results of our system,0.5712524056434631
translation,76,77,results,average results,above,both baselines,average results above both baselines,0.690154492855072
translation,76,77,results,our system,are,both baselines,our system are both baselines,0.5524036288261414
translation,76,77,results,our system,above,both baselines,our system above both baselines,0.675873875617981
translation,77,72,baselines,different back-translation methods,explore,three types,different back-translation methods explore three types,0.59975266456604
translation,77,72,baselines,different back-translation methods,explore,unconstrained sampling,different back-translation methods explore unconstrained sampling,0.6348972320556641
translation,77,72,baselines,beam search,beam size of,five,beam search beam size of five,0.82415771484375
translation,77,72,baselines,sampling,constrained to,most 10 likely words,sampling constrained to most 10 likely words,0.729465901851654
translation,77,72,baselines,baselines,has,different back-translation methods,baselines has different back-translation methods,0.5132770538330078
translation,77,51,experimental-setup,pretrained trans_big,from,m2m_100,pretrained trans_big from m2m_100,0.6007179021835327
translation,77,51,experimental-setup,pretrained trans_big,finetuned on,parallel corpus,pretrained trans_big finetuned on parallel corpus,0.7193845510482788
translation,77,51,experimental-setup,parallel corpus,to generate,high-quality synthetic sentences,parallel corpus to generate high-quality synthetic sentences,0.6364875435829163
translation,77,51,experimental-setup,experimental setup,has,pretrained trans_big,experimental setup has pretrained trans_big,0.5861173868179321
translation,77,53,experimental-setup,adam optimizer,with,"? 1 = 0.90 , ? 2 = 0.98","adam optimizer with ? 1 = 0.90 , ? 2 = 0.98",0.6233240962028503
translation,77,53,experimental-setup,adam optimizer,with,weight- decay,adam optimizer with weight- decay,0.612930417060852
translation,77,53,experimental-setup,weight- decay,of,0.0001,weight- decay of 0.0001,0.5494739413261414
translation,77,53,experimental-setup,label smoothed crossentropy criterion,with,label smoothing,label smoothed crossentropy criterion with label smoothing,0.6078460812568665
translation,77,53,experimental-setup,label smoothed crossentropy criterion,with,initial learning rate,label smoothed crossentropy criterion with initial learning rate,0.5876561999320984
translation,77,53,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,77,53,experimental-setup,initial learning rate,of,0.0003,initial learning rate of 0.0003,0.5776545405387878
translation,77,53,experimental-setup,0.0003,with,inverse square root lr-scheduler,0.0003 with inverse square root lr-scheduler,0.6185843348503113
translation,77,53,experimental-setup,warmup updates,of,"2,500 steps","warmup updates of 2,500 steps",0.5521311163902283
translation,77,6,experiments,different backtranslation methods,from,bilingual translation,different backtranslation methods from bilingual translation,0.5131141543388367
translation,77,92,experiments,m2m_100,offers,multiple pretrained models,m2m_100 offers multiple pretrained models,0.6486461758613586
translation,77,92,experiments,multiple pretrained models,with,128k vocabularies,multiple pretrained models with 128k vocabularies,0.5810027718544006
translation,77,92,experiments,sizes,are,"418m , 1.2b and 12b","sizes are 418m , 1.2b and 12b",0.5910535454750061
translation,77,9,results,smaller size of vocabularies,perform,better,smaller size of vocabularies perform better,0.6142084002494812
translation,77,9,results,extensive monolingual english data,offers,modest improvement,extensive monolingual english data offers modest improvement,0.6554303765296936
translation,77,9,results,results,has,smaller size of vocabularies,results has smaller size of vocabularies,0.5712388753890991
translation,77,67,results,128k vocabulary,has,outperforms,128k vocabulary has outperforms,0.6008495688438416
translation,77,67,results,outperforms,has,256k vocabulary,outperforms has 256k vocabulary,0.611967921257019
translation,77,67,results,outperforms,has,23.14 vs 21.65 spbleu,outperforms has 23.14 vs 21.65 spbleu,0.6395420432090759
translation,77,68,results,pretrained trans_small,with,256k vocabulary,pretrained trans_small with 256k vocabulary,0.618190586566925
translation,77,68,results,0.58 score improvement,compared to,128k trans_small,0.58 score improvement compared to 128k trans_small,0.6319732666015625
translation,77,68,results,pretrained trans_small,has,0.58 score improvement,pretrained trans_small has 0.58 score improvement,0.5532155632972717
translation,77,68,results,results,finetune,pretrained trans_small,results finetune pretrained trans_small,0.6849043965339661
translation,77,83,results,unconstrained sampling,offers,best performance,unconstrained sampling offers best performance,0.6819630265235901
translation,77,83,results,best performance,among,three methods,best performance among three methods,0.6048279404640198
translation,77,83,results,constrained sampling method,gives,best score,constrained sampling method gives best score,0.6230742931365967
translation,77,84,results,beam search,is,worst,beam search is worst,0.6378242373466492
translation,77,84,results,results,has,beam search,results has beam search,0.5500618815422058
translation,77,112,results,our submissions,achieve,second place,our submissions achieve second place,0.5540445446968079
translation,77,112,results,second place,for,both small tasks,second place for both small tasks,0.6216487288475037
translation,77,112,results,results,has,our submissions,results has our submissions,0.5741304159164429
translation,78,115,ablation-analysis,significant loss,of,1.7 bleu,significant loss of 1.7 bleu,0.5572440028190613
translation,78,115,ablation-analysis,1.7 bleu,between,end-to - end st,1.7 bleu between end-to - end st,0.5879836082458496
translation,78,115,ablation-analysis,1.7 bleu,between,cascade st,1.7 bleu between cascade st,0.6146565079689026
translation,78,73,experimental-setup,embedding dimension,was,256,embedding dimension was 256,0.631061315536499
translation,78,73,experimental-setup,hidden units,in,feedforward layer,hidden units in feedforward layer,0.5005497932434082
translation,78,73,experimental-setup,feedforward layer,was,"2,048","feedforward layer was 2,048",0.5875722169876099
translation,78,73,experimental-setup,experimental setup,size of,hidden units,experimental setup size of hidden units,0.6839568018913269
translation,78,73,experimental-setup,experimental setup,has,embedding dimension,experimental setup has embedding dimension,0.5115625262260437
translation,78,74,experimental-setup,attention head,for,self-attention and cross-attention,attention head for self-attention and cross-attention,0.5990802645683289
translation,78,74,experimental-setup,self-attention and cross-attention,set to,4,self-attention and cross-attention set to 4,0.6332303881645203
translation,78,74,experimental-setup,experimental setup,has,attention head,experimental setup has attention head,0.5106930136680603
translation,78,75,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.98","adam optimizer with ? 1 = 0.9 , ? 2 = 0.98",0.6221903562545776
translation,78,75,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,78,76,experimental-setup,mt models,with,global batch size,mt models with global batch size,0.6238497495651245
translation,78,76,experimental-setup,global batch size,of,"25,000 tokens","global batch size of 25,000 tokens",0.5902595520019531
translation,78,76,experimental-setup,experimental setup,trained,mt models,experimental setup trained mt models,0.6967321038246155
translation,78,85,experimental-setup,asr models,trained with,"120,000 frames per batch","asr models trained with 120,000 frames per batch",0.7458364963531494
translation,78,85,experimental-setup,batch size,for,st,batch size for st,0.6682195067405701
translation,78,85,experimental-setup,st,was,"80,000 frames","st was 80,000 frames",0.6709550023078918
translation,78,85,experimental-setup,experimental setup,has,asr models,experimental setup has asr models,0.5218565464019775
translation,78,88,experimental-setup,label smoothing,of,value 0.1,label smoothing of value 0.1,0.5899170637130737
translation,78,88,experimental-setup,value 0.1,for,training,value 0.1 for training,0.6498188376426697
translation,78,88,experimental-setup,training,has,all three tasks,training has all three tasks,0.5538803935050964
translation,78,88,experimental-setup,experimental setup,applied,label smoothing,experimental setup applied label smoothing,0.6266146898269653
translation,78,89,experimental-setup,st model,initialized by,asr encoder,st model initialized by asr encoder,0.6876342296600342
translation,78,89,experimental-setup,experimental setup,encoder of,st model,experimental setup encoder of st model,0.764992356300354
translation,78,2,experiments,neurst,has,neural speech translation toolkit,neurst has neural speech translation toolkit,0.5318304896354675
translation,78,4,experiments,neurst,is,open-source toolkit,neurst is open-source toolkit,0.570888102054596
translation,78,4,experiments,open-source toolkit,for,neural speech translation,open-source toolkit for neural speech translation,0.5900025963783264
translation,78,28,model,end-to - end asr and nmt,for,cascade systems,end-to - end asr and nmt for cascade systems,0.6342289447784424
translation,78,28,model,model,present,neurst,model present neurst,0.7314603924751282
translation,78,72,model,mt,in,cascade systems,mt in cascade systems,0.5990544557571411
translation,78,72,model,6 layers,for,encoder and decoders,6 layers for encoder and decoders,0.6001328825950623
translation,78,72,model,model,for,mt,model for mt,0.714251697063446
translation,78,72,model,model,included,6 layers,model included 6 layers,0.6833710074424744
translation,78,98,results,our transformer - based st model,applies,asr pre-training,our transformer - based st model applies asr pre-training,0.5683627128601074
translation,78,98,results,our transformer - based st model,applies,specaugment,our transformer - based st model applies specaugment,0.6297791600227356
translation,78,98,results,our transformer - based st model,achieves,superior results,our transformer - based st model achieves superior results,0.6775146126747131
translation,78,98,results,superior results,versus,recent works,superior results versus recent works,0.6410664916038513
translation,78,98,results,recent works,about,knowledge distillation,recent works about knowledge distillation,0.6249167919158936
translation,78,98,results,recent works,about,"curriculum pre-training ( wang et al. , 2020 c )","recent works about curriculum pre-training ( wang et al. , 2020 c )",0.5909290909767151
translation,78,98,results,recent works,about,"lut ( dong et al. , 2021 )","recent works about lut ( dong et al. , 2021 )",0.5736280083656311
translation,78,98,results,results,has,our transformer - based st model,results has our transformer - based st model,0.5064998269081116
translation,78,100,results,cascade baseline,slightly worse than,espnet - st,cascade baseline slightly worse than espnet - st,0.6799845695495605
translation,78,100,results,cascade baseline,slightly worse than,- 0.2 bleu,cascade baseline slightly worse than - 0.2 bleu,0.6759710311889648
translation,78,100,results,espnet - st,has,- 0.2 bleu,espnet - st has - 0.2 bleu,0.5909929275512695
translation,78,100,results,results,has,cascade baseline,results has cascade baseline,0.6128959059715271
translation,78,101,results,end-to- end st model,exceeds,cascade system,end-to- end st model exceeds cascade system,0.6296883821487427
translation,78,101,results,cascade system,by,0.4?0.5 bleu,cascade system by 0.4?0.5 bleu,0.5929903388023376
translation,78,101,results,results,find that,end-to- end st model,results find that end-to- end st model,0.630341649055481
translation,78,104,results,results,of,end-to - end st model,results of end-to - end st model,0.5644199252128601
translation,78,104,results,end-to - end st model,are,competitive,end-to - end st model are competitive,0.5523154139518738
translation,78,104,results,competitive,with,fairseq - st,competitive with fairseq - st,0.7039948105812073
translation,78,104,results,competitive,with,espnet - st,competitive with espnet - st,0.7179207801818848
translation,78,104,results,results,of,end-to - end st model,results of end-to - end st model,0.5644199252128601
translation,78,104,results,results,has,results,results has results,0.48582205176353455
translation,79,110,ablation-analysis,ensemble,leads to,0.2 bleu increase,ensemble leads to 0.2 bleu increase,0.6287890672683716
translation,79,110,ablation-analysis,ensemble,leads to,0.1 bleu increase,ensemble leads to 0.1 bleu increase,0.6319390535354614
translation,79,110,ablation-analysis,0.2 bleu increase,on,en2zh direction,0.2 bleu increase on en2zh direction,0.5757637023925781
translation,79,110,ablation-analysis,0.1 bleu increase,on,opposite direction,0.1 bleu increase on opposite direction,0.5627943277359009
translation,79,110,ablation-analysis,ablation analysis,has,ensemble,ablation analysis has ensemble,0.5272953510284424
translation,79,121,ablation-analysis,baseline model,two rounds of,ftst data augmentation,baseline model two rounds of ftst data augmentation,0.6883062720298767
translation,79,121,ablation-analysis,ftst data augmentation,contribute to,1.4 bleu increase,ftst data augmentation contribute to 1.4 bleu increase,0.637505054473877
translation,79,121,ablation-analysis,1.4 bleu increase,on,each directions,1.4 bleu increase on each directions,0.5272780060768127
translation,79,121,ablation-analysis,ablation analysis,Comparing with,baseline model,ablation analysis Comparing with baseline model,0.6797842979431152
translation,79,124,ablation-analysis,ensemble,leads to,0.1 bleu increase,ensemble leads to 0.1 bleu increase,0.6319390535354614
translation,79,124,ablation-analysis,ensemble,leads to,0.3 bleu increase,ensemble leads to 0.3 bleu increase,0.6293063163757324
translation,79,124,ablation-analysis,0.1 bleu increase,on,en2de direction,0.1 bleu increase on en2de direction,0.5621716380119324
translation,79,124,ablation-analysis,0.3 bleu increase,on,opposite direction,0.3 bleu increase on opposite direction,0.5492006540298462
translation,79,124,ablation-analysis,ablation analysis,has,ensemble,ablation analysis has ensemble,0.5272953510284424
translation,79,135,ablation-analysis,iterative ftst data augmentation,contribute to,2.8 bleu and 1.7 bleu increases,iterative ftst data augmentation contribute to 2.8 bleu and 1.7 bleu increases,0.6372539401054382
translation,79,135,ablation-analysis,2.8 bleu and 1.7 bleu increases,on,en2 ja and ja2en directions,2.8 bleu and 1.7 bleu increases on en2 ja and ja2en directions,0.6031856536865234
translation,79,135,ablation-analysis,baseline model,has,iterative ftst data augmentation,baseline model has iterative ftst data augmentation,0.5361388921737671
translation,79,135,ablation-analysis,ablation analysis,Comparing with,baseline model,ablation analysis Comparing with baseline model,0.6797842979431152
translation,79,158,ablation-analysis,second- round ftst data augmentation,get,slight increase,second- round ftst data augmentation get slight increase,0.5838356018066406
translation,79,158,ablation-analysis,slight increase,on,en-ha task,slight increase on en-ha task,0.5979202389717102
translation,79,158,ablation-analysis,0.2 bleu,on,en2ha direction,0.2 bleu on en2ha direction,0.5565675497055054
translation,79,158,ablation-analysis,0.9,on,ha2en direction,0.9 on ha2en direction,0.5758463144302368
translation,79,158,ablation-analysis,en-ha task,has,0.2 bleu,en-ha task has 0.2 bleu,0.583010196685791
translation,79,158,ablation-analysis,ablation analysis,conduct,second- round ftst data augmentation,ablation analysis conduct second- round ftst data augmentation,0.7250799536705017
translation,79,100,baselines,ftst data augmentation,to further enhance,model performance,ftst data augmentation to further enhance model performance,0.6815202236175537
translation,79,14,experimental-setup,each language pair,perform,multi-step data cleansing,each language pair perform multi-step data cleansing,0.6002692580223083
translation,79,14,experimental-setup,each language pair,keep,high-quality subset,each language pair keep high-quality subset,0.6197555065155029
translation,79,14,experimental-setup,multi-step data cleansing,on,provided dataset,multi-step data cleansing on provided dataset,0.5421130657196045
translation,79,14,experimental-setup,experimental setup,For,each language pair,experimental setup For each language pair,0.584132730960846
translation,79,81,experimental-setup,size,of,each batch,size of each batch,0.6566802859306335
translation,79,81,experimental-setup,size,of,parameter update frequency,size of parameter update frequency,0.5662137866020203
translation,79,81,experimental-setup,each batch,set as,2048,each batch set as 2048,0.670348048210144
translation,79,81,experimental-setup,parameter update frequency,as,32,parameter update frequency as 32,0.5660668611526489
translation,79,81,experimental-setup,learning rate,as,5e - 4,learning rate as 5e - 4,0.5762070417404175
translation,79,81,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,79,81,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,79,82,experimental-setup,number of warmup steps,is,4000,number of warmup steps is 4000,0.5959648489952087
translation,79,82,experimental-setup,model,saved,1000 steps,model saved 1000 steps,0.7116293907165527
translation,79,82,experimental-setup,experimental setup,has,number of warmup steps,experimental setup has number of warmup steps,0.5219058394432068
translation,79,82,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,79,84,experimental-setup,dropout,has,rate,dropout has rate,0.5043756365776062
translation,79,84,experimental-setup,experimental setup,adopt,dropout,experimental setup adopt dropout,0.5873570442199707
translation,79,117,experimental-setup,vocabulary,with,32 k words,vocabulary with 32 k words,0.6504590511322021
translation,79,117,experimental-setup,experimental setup,has,source and target side,experimental setup has source and target side,0.516616702079773
translation,79,128,experimental-setup,dropout rate,set to,0.1,dropout rate set to 0.1,0.6670177578926086
translation,79,128,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,79,156,experimental-setup,sampling ratios,according to,monolingual data size,sampling ratios according to monolingual data size,0.6456190943717957
translation,79,156,experimental-setup,monolingual data size,of,each languages,monolingual data size of each languages,0.5779784917831421
translation,79,156,experimental-setup,experimental setup,adjust,sampling ratios,experimental setup adjust sampling ratios,0.6630357503890991
translation,79,104,experiments,data distillation,on,source sentences,data distillation on source sentences,0.504136860370636
translation,79,104,experiments,data distillation,mix,generated data,data distillation mix generated data,0.7618556022644043
translation,79,104,experiments,data distillation,add,noises,data distillation add noises,0.6993162631988525
translation,79,104,experiments,source sentences,from,wmt 2017 and 2018 news test sets,source sentences from wmt 2017 and 2018 news test sets,0.5260201096534729
translation,79,104,experiments,generated data,with,original data,generated data with original data,0.52790766954422
translation,79,104,experiments,noises,to,target side,noises to target side,0.5794536471366882
translation,79,108,experiments,data distillation,on,source sentences,data distillation on source sentences,0.504136860370636
translation,79,108,experiments,data distillation,add,noises,data distillation add noises,0.6993162631988525
translation,79,108,experiments,source sentences,from,wmt 2017 wmt - 2018,source sentences from wmt 2017 wmt - 2018,0.5787869095802307
translation,79,108,experiments,source sentences,from,wmt 2017 - 2018 test sets,source sentences from wmt 2017 - 2018 test sets,0.5439103245735168
translation,79,108,experiments,generated data,with,wmt 2017 - 2018 test sets,generated data with wmt 2017 - 2018 test sets,0.6168410778045654
translation,79,108,experiments,noises,to,target side,noises to target side,0.5794536471366882
translation,79,108,experiments,wmt 2017 wmt - 2018,has,generated data,wmt 2017 wmt - 2018 has generated data,0.5823960304260254
translation,79,111,experiments,model,with,wmt 2019 and 2020 test sets,model with wmt 2019 and 2020 test sets,0.6364623308181763
translation,79,114,experiments,de / en,For,en- de task,de / en For en- de task,0.6729064583778381
translation,79,114,experiments,de / en,adopt,deep 36 - 5 big model,de / en adopt deep 36 - 5 big model,0.6909856200218201
translation,79,114,experiments,de / en,adopt,deep 25 - 6 large model,de / en adopt deep 25 - 6 large model,0.7124770879745483
translation,79,114,experiments,en- de task,adopt,deep 36 - 5 big model,en- de task adopt deep 36 - 5 big model,0.6711704730987549
translation,79,114,experiments,en- de task,adopt,deep 25 - 6 large model,en- de task adopt deep 25 - 6 large model,0.6627851724624634
translation,79,115,experiments,moses,for,english and german word segmentation,moses for english and german word segmentation,0.5675695538520813
translation,79,122,experiments,data distillation,on,source sentences,data distillation on source sentences,0.504136860370636
translation,79,122,experiments,data distillation,mix,generated data,data distillation mix generated data,0.7618556022644043
translation,79,122,experiments,source sentences,from,wmt 2020 news test sets,source sentences from wmt 2020 news test sets,0.5256255865097046
translation,79,122,experiments,generated data,with,wmt 2018 and wmt 2019 test sets,generated data with wmt 2018 and wmt 2019 test sets,0.6211186051368713
translation,79,122,experiments,generated data,after adding,noises,generated data after adding noises,0.719211995601654
translation,79,122,experiments,noises,to,target side,noises to target side,0.5794536471366882
translation,79,136,experiments,data distillation,on,source sentences,data distillation on source sentences,0.504136860370636
translation,79,136,experiments,data distillation,mix,generated data,data distillation mix generated data,0.7618556022644043
translation,79,136,experiments,source sentences,from,wmt 2020 news test sets,source sentences from wmt 2020 news test sets,0.5256255865097046
translation,79,136,experiments,generated data,with,wmt 2020 dev set,generated data with wmt 2020 dev set,0.6373620629310608
translation,79,136,experiments,generated data,after adding,noises,generated data after adding noises,0.719211995601654
translation,79,136,experiments,noises,to,target side,noises to target side,0.5794536471366882
translation,79,149,experiments,all multilingual models,gain,huge improvements,all multilingual models gain huge improvements,0.6866831183433533
translation,79,149,experiments,huge improvements,comparing with,bilingual baseline model,huge improvements comparing with bilingual baseline model,0.7221812009811401
translation,79,149,experiments,eight language directions,has,all multilingual models,eight language directions has all multilingual models,0.5755503177642822
translation,79,85,model,marian,used for,decoding,marian used for decoding,0.7340736985206604
translation,79,85,model,decoding,during,inference,decoding during inference,0.7040165662765503
translation,79,85,model,model,has,marian,model has marian,0.6589898467063904
translation,79,116,model,training data,segmented by,shared sentencepiece model,training data segmented by shared sentencepiece model,0.7598341107368469
translation,79,116,model,model,has,training data,model has training data,0.5542957186698914
translation,79,129,model,training data,segmented by,shared sen-tencepiece model,training data segmented by shared sen-tencepiece model,0.7911969423294067
translation,79,129,model,model,has,training data,model has training data,0.5542957186698914
translation,79,103,results,ftst,leads to,6.0 bleu increase,ftst leads to 6.0 bleu increase,0.6687250733375549
translation,79,103,results,ftst,leads to,5.9 bleu,ftst leads to 5.9 bleu,0.6522719860076904
translation,79,103,results,6.0 bleu increase,on,en2zh direction,6.0 bleu increase on en2zh direction,0.5829370617866516
translation,79,103,results,increase,on,opposite direction,increase on opposite direction,0.6084026098251343
translation,79,103,results,baseline model,has,ftst,baseline model has ftst,0.5547705888748169
translation,79,103,results,5.9 bleu,has,increase,5.9 bleu has increase,0.5829756259918213
translation,79,103,results,results,Comparing with,baseline model,results Comparing with baseline model,0.6776164174079895
translation,79,105,results,model,using,mixed data,model using mixed data,0.7208935022354126
translation,79,105,results,model,achieve,1.1 bleu and 2.0 bleu increases,model achieve 1.1 bleu and 2.0 bleu increases,0.6765019297599792
translation,79,105,results,1.1 bleu and 2.0 bleu increases,on,en2zh and zh2en directions,1.1 bleu and 2.0 bleu increases on en2zh and zh2en directions,0.5831192135810852
translation,79,105,results,results,finetune,model,results finetune model,0.6180015206336975
translation,79,109,results,model,using,mixed data,model using mixed data,0.7208935022354126
translation,79,109,results,model,achieve,0.3 bleu and 0.4 bleu increases,model achieve 0.3 bleu and 0.4 bleu increases,0.6733459234237671
translation,79,109,results,0.3 bleu and 0.4 bleu increases,on,en2zh and zh2en directions,0.3 bleu and 0.4 bleu increases on en2zh and zh2en directions,0.5821877717971802
translation,79,109,results,results,finetune,model,results finetune model,0.6180015206336975
translation,79,112,results,our models,achieve,35.1 bleu,our models achieve 35.1 bleu,0.5700978636741638
translation,79,112,results,our models,achieve,28.9 bleu,our models achieve 28.9 bleu,0.5875965356826782
translation,79,112,results,35.1 bleu,on,en2zh direction,35.1 bleu on en2zh direction,0.5947099328041077
translation,79,112,results,35.1 bleu,on,zh2en direction,35.1 bleu on zh2en direction,0.5852534770965576
translation,79,112,results,28.9 bleu,on,zh2en direction,28.9 bleu on zh2en direction,0.5818645358085632
translation,79,112,results,results,has,our models,results has our models,0.5733726620674133
translation,79,123,results,model,using,mixed data,model using mixed data,0.7208935022354126
translation,79,123,results,model,achieve,3.7 bleu and 2.0 bleu,model achieve 3.7 bleu and 2.0 bleu,0.6283133029937744
translation,79,123,results,increases,on,en2de and de2en directions,increases on en2de and de2en directions,0.6056967973709106
translation,79,123,results,3.7 bleu and 2.0 bleu,has,increases,3.7 bleu and 2.0 bleu has increases,0.5852078795433044
translation,79,123,results,results,fine- tune,model,results fine- tune model,0.6657537817955017
translation,79,137,results,model,using,mixed data,model using mixed data,0.7208935022354126
translation,79,137,results,model,achieve,3.7 bleu and 2.2 bleu increases,model achieve 3.7 bleu and 2.2 bleu increases,0.6599818468093872
translation,79,137,results,3.7 bleu and 2.2 bleu increases,on,en2 ja and ja2en directions,3.7 bleu and 2.2 bleu increases on en2 ja and ja2en directions,0.5872311592102051
translation,79,137,results,results,fine- tune,model,results fine- tune model,0.6657537817955017
translation,79,138,results,four models,on,each direction,four models on each direction,0.5953052043914795
translation,79,138,results,0.9 bleu increase,on,en2 ja direction,0.9 bleu increase on en2 ja direction,0.5884491801261902
translation,79,138,results,0.9 bleu increase,on,opposite direction,0.9 bleu increase on opposite direction,0.548775315284729
translation,79,138,results,0.9 bleu increase,on,opposite direction,0.9 bleu increase on opposite direction,0.548775315284729
translation,79,138,results,1.0 bleu increase,on,opposite direction,1.0 bleu increase on opposite direction,0.5533878207206726
translation,79,138,results,results,train,four models,results train four models,0.6042022705078125
translation,79,140,results,our submitted models,achieve,45.4 bleu,our submitted models achieve 45.4 bleu,0.5729057192802429
translation,79,140,results,our submitted models,achieve,26.5 bleu,our submitted models achieve 26.5 bleu,0.5820846557617188
translation,79,140,results,45.4 bleu,on,en2 ja direction,45.4 bleu on en2 ja direction,0.6332693696022034
translation,79,140,results,45.4 bleu,on,j2en direction,45.4 bleu on j2en direction,0.6030890345573425
translation,79,140,results,26.5 bleu,on,j2en direction,26.5 bleu on j2en direction,0.6004895567893982
translation,79,140,results,results,has,our submitted models,results has our submitted models,0.5533792972564697
translation,79,150,results,en - ha,achieves,greatest improvements,en - ha achieves greatest improvements,0.6915594935417175
translation,79,150,results,12.1 bleu,on,en2ha direction,12.1 bleu on en2ha direction,0.5311497449874878
translation,79,150,results,11.20,on,ha2en direction,11.20 on ha2en direction,0.5818844437599182
translation,79,150,results,greatest improvements,has,12.1 bleu,greatest improvements has 12.1 bleu,0.5222037434577942
translation,79,150,results,greatest improvements,has,11.20,greatest improvements has 11.20,0.5627344846725464
translation,79,150,results,results,has,en - ha,results has en - ha,0.6047756671905518
translation,79,151,results,bn - hi,achieves,slightest improvements,bn - hi achieves slightest improvements,0.6792122721672058
translation,79,151,results,0.4 bleu,on,bn2hi direction,0.4 bleu on bn2hi direction,0.5904691815376282
translation,79,151,results,1.78,on,hi2 bn direction,1.78 on hi2 bn direction,0.5615831017494202
translation,79,151,results,slightest improvements,has,0.4 bleu,slightest improvements has 0.4 bleu,0.5488414764404297
translation,79,151,results,results,has,bn - hi,results has bn - hi,0.5425770282745361
translation,79,152,results,fewer,has,bilingual data,fewer has bilingual data,0.5643125176429749
translation,79,152,results,fewer,has,greater impact,fewer has greater impact,0.5894019603729248
translation,79,152,results,bilingual data,has,greater impact,bilingual data has greater impact,0.5900534987449646
translation,79,152,results,greater impact,has,multilingual model,greater impact has multilingual model,0.6146429181098938
translation,79,152,results,results,demonstrate,fewer,results demonstrate fewer,0.5915444493293762
translation,79,152,results,results,demonstrate,bilingual data,results demonstrate bilingual data,0.52122563123703
translation,79,153,results,other extremely low-resource scenarios,improvement gained by,multilingual model,other extremely low-resource scenarios improvement gained by multilingual model,0.7645213007926941
translation,79,153,results,multilingual model,for,en-ha,multilingual model for en-ha,0.6169188022613525
translation,79,153,results,results,In,other extremely low-resource scenarios,results In other extremely low-resource scenarios,0.5347681641578674
translation,79,157,results,data augmentation strategy,achieves,improvements,data augmentation strategy achieves improvements,0.6784936785697937
translation,79,157,results,improvements,on,all eight language directions,improvements on all eight language directions,0.5145354866981506
translation,79,157,results,improvements,from,1.1 bleu,improvements from 1.1 bleu,0.48396191000938416
translation,79,157,results,1.1 bleu,to,4.4 bleu increase,1.1 bleu to 4.4 bleu increase,0.5173490643501282
translation,79,157,results,results,has,data augmentation strategy,results has data augmentation strategy,0.5212938785552979
translation,80,88,ablation-analysis,embeddings,from,byte- based models,embeddings from byte- based models,0.6008443236351013
translation,80,88,ablation-analysis,embeddings,decreases,performance,embeddings decreases performance,0.7167856693267822
translation,80,88,ablation-analysis,performance,by,average of 0.45 bleu,performance by average of 0.45 bleu,0.577883243560791
translation,80,88,ablation-analysis,average of 0.45 bleu,when generating,english,average of 0.45 bleu when generating english,0.6035541892051697
translation,80,88,ablation-analysis,ablation analysis,removing,embeddings,ablation analysis removing embeddings,0.8107144832611084
translation,80,47,baselines,baselines,train,standard transformer encoder-decoder models,baselines train standard transformer encoder-decoder models,0.69638991355896
translation,80,48,experimental-setup,subword tokenization,apply,moses tokenizer,subword tokenization apply moses tokenizer,0.575832188129425
translation,80,48,experimental-setup,moses tokenizer,followed by,bpe,moses tokenizer followed by bpe,0.6759865283966064
translation,80,48,experimental-setup,experimental setup,For,subword tokenization,experimental setup For subword tokenization,0.5451502203941345
translation,80,51,experimental-setup,"fairseq ( ott et al. , 2019 ) implementation",of,transformer encoder-decoder model,"fairseq ( ott et al. , 2019 ) implementation of transformer encoder-decoder model",0.5461200475692749
translation,80,52,experimental-setup,preprocessing,use,"10,000 merging steps","preprocessing use 10,000 merging steps",0.6634491682052612
translation,80,52,experimental-setup,"10,000 merging steps",when building,bpe vocabulary,"10,000 merging steps when building bpe vocabulary",0.705062747001648
translation,80,52,experimental-setup,bpe vocabulary,for,every language pair,bpe vocabulary for every language pair,0.6159576177597046
translation,80,52,experimental-setup,experimental setup,During,preprocessing,experimental setup During preprocessing,0.6557283997535706
translation,80,54,experimental-setup,hidden dimension,of,512,hidden dimension of 512,0.655615508556366
translation,80,54,experimental-setup,hidden dimension,of,1024,hidden dimension of 1024,0.6431778073310852
translation,80,54,experimental-setup,feed-forward dimension,of,1024,feed-forward dimension of 1024,0.6366336345672607
translation,80,55,experimental-setup,"adam ( kingma and ba , 2014 )",using,inverse square root learning rate scheduler,"adam ( kingma and ba , 2014 ) using inverse square root learning rate scheduler",0.6462193727493286
translation,80,55,experimental-setup,"adam ( kingma and ba , 2014 )",using,label smoothing,"adam ( kingma and ba , 2014 ) using label smoothing",0.6538621783256531
translation,80,55,experimental-setup,inverse square root learning rate scheduler,with,4000 warmup steps,inverse square root learning rate scheduler with 4000 warmup steps,0.6395441889762878
translation,80,55,experimental-setup,inverse square root learning rate scheduler,with,peak learn - ing rate,inverse square root learning rate scheduler with peak learn - ing rate,0.627767026424408
translation,80,55,experimental-setup,peak learn - ing rate,of,5 ? 10 ?4,peak learn - ing rate of 5 ? 10 ?4,0.6352611184120178
translation,80,55,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,80,55,experimental-setup,experimental setup,optimize with,"adam ( kingma and ba , 2014 )","experimental setup optimize with adam ( kingma and ba , 2014 )",0.6846347451210022
translation,80,56,experimental-setup,each model,for,50 k steps,each model for 50 k steps,0.6163421869277954
translation,80,56,experimental-setup,each model,average,top 5 checkpoints,each model average top 5 checkpoints,0.8088918328285217
translation,80,56,experimental-setup,top 5 checkpoints,according to,validation loss,top 5 checkpoints according to validation loss,0.6583024859428406
translation,80,56,experimental-setup,experimental setup,train,each model,experimental setup train each model,0.6811989545822144
translation,80,57,experimental-setup,dropout ( 0.2 or 0.3 ),on,validation set,dropout ( 0.2 or 0.3 ) on validation set,0.5369499921798706
translation,80,57,experimental-setup,experimental setup,tune,dropout ( 0.2 or 0.3 ),experimental setup tune dropout ( 0.2 or 0.3 ),0.6688789129257202
translation,80,58,experimental-setup,batch size,according to,maximum,batch size according to maximum,0.6723238229751587
translation,80,58,experimental-setup,maximum,of,"64,000 bytes per batch","maximum of 64,000 bytes per batch",0.594611406326294
translation,80,58,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,80,58,experimental-setup,experimental setup,controls for,number of batches per epoch,experimental setup controls for number of batches per epoch,0.6271343231201172
translation,80,41,experiments,byte-tokenized embeddingless models,for,machine translation,byte-tokenized embeddingless models for machine translation,0.5604658126831055
translation,80,41,experiments,byte-tokenized embeddingless models,compare them to,"standard byte , character , and subword - based models","byte-tokenized embeddingless models compare them to standard byte , character , and subword - based models",0.6228716969490051
translation,80,41,experiments,"standard byte , character , and subword - based models",on,diverse set of languages,"standard byte , character , and subword - based models on diverse set of languages",0.45765939354896545
translation,80,13,model,dense trainable embedding matrix,with,fixed one- hot encoding,dense trainable embedding matrix with fixed one- hot encoding,0.603663444519043
translation,80,13,model,fixed one- hot encoding,of,vocabulary,fixed one- hot encoding of vocabulary,0.5647150278091431
translation,80,13,model,fixed one- hot encoding,as,first and last layers,fixed one- hot encoding as first and last layers,0.5522212982177734
translation,80,13,model,vocabulary,as,first and last layers,vocabulary as first and last layers,0.5569620728492737
translation,80,13,model,first and last layers,of,standard transformer model,first and last layers of standard transformer model,0.5670012831687927
translation,80,13,model,model,replace,dense trainable embedding matrix,model replace dense trainable embedding matrix,0.622741162776947
translation,80,67,results,difference,between,embeddingless model and the best embeddingbased model,difference between embeddingless model and the best embeddingbased model,0.6412091255187988
translation,80,67,results,embeddingless model and the best embeddingbased model,under,1 bleu,embeddingless model and the best embeddingbased model under 1 bleu,0.6023939847946167
translation,80,67,results,vietnamese,has,difference,vietnamese has difference,0.6130735278129578
translation,80,67,results,results,Except for,vietnamese,results Except for vietnamese,0.6559291481971741
translation,80,68,results,models without embeddings,consistently achieve,higher bleu scores,models without embeddings consistently achieve higher bleu scores,0.5793003439903259
translation,80,68,results,models without embeddings,consistently achieve,equal score,models without embeddings consistently achieve equal score,0.6065941452980042
translation,80,68,results,models without embeddings,with,boost,models without embeddings with boost,0.6737068891525269
translation,80,68,results,higher bleu scores,in,19 of 20 cases,higher bleu scores in 19 of 20 cases,0.5352258682250977
translation,80,68,results,higher bleu scores,with,boost,higher bleu scores with boost,0.4423927366733551
translation,80,68,results,equal score,for,ru-en,equal score for ru-en,0.7255191802978516
translation,80,68,results,boost,of,0.5 bleu,boost of 0.5 bleu,0.5733103156089783
translation,80,68,results,boost,about,0.5 bleu,boost about 0.5 bleu,0.5747706890106201
translation,80,68,results,most controlled setting,has,models without embeddings,most controlled setting has models without embeddings,0.6221520900726318
translation,80,69,results,models,based on,character embeddings,models based on character embeddings,0.5802940130233765
translation,80,69,results,embeddingless byte-to- byte approach,yields,higher bleu scores,embeddingless byte-to- byte approach yields higher bleu scores,0.706329345703125
translation,80,69,results,higher bleu scores,in,17 out of 20 cases,higher bleu scores in 17 out of 20 cases,0.5328195095062256
translation,80,69,results,models,has,embeddingless byte-to- byte approach,models has embeddingless byte-to- byte approach,0.613736093044281
translation,80,69,results,character embeddings,has,embeddingless byte-to- byte approach,character embeddings has embeddingless byte-to- byte approach,0.5752735733985901
translation,80,69,results,results,compared to,models,results compared to models,0.6620222926139832
translation,80,81,results,decoder-side token dropout,improves,performance,decoder-side token dropout improves performance,0.6278927326202393
translation,80,81,results,performance,of,all models,performance of all models,0.5688430666923523
translation,80,81,results,larger impact,on,byte-based models,larger impact on byte-based models,0.5740159153938293
translation,80,81,results,larger impact,on,embeddingless models,larger impact on embeddingless models,0.6133571267127991
translation,80,81,results,results,shows,decoder-side token dropout,results shows decoder-side token dropout,0.5967223644256592
translation,80,85,results,translating,from,original english text,translating from original english text,0.6141003370285034
translation,80,85,results,original english text,to,foreign language,original english text to foreign language,0.539580762386322
translation,80,85,results,different models,perform,roughly on par,different models perform roughly on par,0.5766453146934509
translation,80,85,results,roughly on par,with,single tokenization method,roughly on par with single tokenization method,0.6636883616447449
translation,80,85,results,single tokenization method,dominating,others,single tokenization method dominating others,0.7533257603645325
translation,80,85,results,translating,has,different models,translating has different models,0.5627057552337646
translation,80,85,results,original english text,has,different models,original english text has different models,0.52640700340271
translation,80,85,results,results,when,translating,results when translating,0.5490593314170837
translation,80,86,results,byte-level models,achieve,almost identical results,byte-level models achieve almost identical results,0.5961305499076843
translation,80,86,results,byte-level models,has,with and without embeddings,byte-level models has with and without embeddings,0.5797156691551208
translation,80,86,results,results,has,byte-level models,results has byte-level models,0.5171900391578674
translation,80,87,results,translating,in,opposite direction,translating in opposite direction,0.59764564037323
translation,80,87,results,consistently outperform,with,average gap,consistently outperform with average gap,0.6938272714614868
translation,80,87,results,average gap,of,0.76 bleu,average gap of 0.76 bleu,0.5465162992477417
translation,80,87,results,0.76 bleu,from,next best model,0.76 bleu from next best model,0.4867630898952484
translation,80,87,results,translating,has,subword models,translating has subword models,0.5354785323143005
translation,80,87,results,opposite direction,has,subword models,opposite direction has subword models,0.5827773809432983
translation,80,87,results,subword models,has,consistently outperform,subword models has consistently outperform,0.5939847826957703
translation,80,87,results,consistently outperform,has,other methods,consistently outperform has other methods,0.53681480884552
translation,81,78,ablation-analysis,bit,allows,transformer,bit allows transformer,0.7305987477302551
translation,81,78,ablation-analysis,transformer,to learn,better attention matrices,transformer to learn better attention matrices,0.6564154028892517
translation,81,78,ablation-analysis,better attention matrices,improving,alignment performance,better attention matrices improving alignment performance,0.7007423043251038
translation,81,78,ablation-analysis,alignment performance,has,24.3 vs. 27.1 ),alignment performance has 24.3 vs. 27.1 ),0.540920078754425
translation,81,78,ablation-analysis,ablation analysis,summarizes,bit,ablation analysis summarizes bit,0.6380867958068848
translation,81,26,model,bit,complement,existing data manipulation strategies,bit complement existing data manipulation strategies,0.7073297500610352
translation,81,26,model,model,show,bit,model show bit,0.7361757755279541
translation,81,86,model,pretraining strategy,for,nmt,pretraining strategy for nmt,0.6193010807037354
translation,81,86,model,nmt,with,parallel data,nmt with parallel data,0.663621723651886
translation,81,86,model,model,propose,pretraining strategy,model propose pretraining strategy,0.6715624332427979
translation,81,57,results,bit,achieves,significant improvements,bit achieves significant improvements,0.7075076699256897
translation,81,57,results,significant improvements,over,strong baseline transformer,significant improvements over strong baseline transformer,0.6954558491706848
translation,81,57,results,significant improvements,under,significance test p < 0.01,significant improvements under significance test p < 0.01,0.5737218260765076
translation,81,57,results,strong baseline transformer,in,7 out of 10 directions,strong baseline transformer in 7 out of 10 directions,0.5086762309074402
translation,81,57,results,rest of 3 directions,show,promising performance,rest of 3 directions show promising performance,0.6340409517288208
translation,81,57,results,promising performance,under,significance test p < 0.05,promising performance under significance test p < 0.05,0.613162636756897
translation,81,57,results,results,show,bit,results show bit,0.6054355502128601
translation,81,61,results,our method,significantly and incrementally improves,translation quality,our method significantly and incrementally improves translation quality,0.6045467853546143
translation,81,61,results,translation quality,in,all cases,translation quality in all cases,0.5280786752700806
translation,81,61,results,baselines,has,our method,baselines has our method,0.5691280364990234
translation,81,61,results,results,compared with,baselines,results compared with baselines,0.6914018392562866
translation,81,73,results,results,superiority of,bit,results superiority of bit,0.7125298976898193
translation,81,74,results,bit,improves,alignment quality,bit improves alignment quality,0.7214553356170654
translation,81,74,results,alignment quality,encourages,self-attention,alignment quality encourages self-attention,0.6346038579940796
translation,81,74,results,self-attention,to learn,bilingual agreement,self-attention to learn bilingual agreement,0.6271481513977051
translation,81,74,results,results,has,bit,results has bit,0.516340970993042
translation,81,83,results,our   bit,making,bit-equipped bt,our   bit making bit-equipped bt,0.7212270498275757
translation,81,83,results,initial base model,by,+ 1.0 bleu,initial base model by + 1.0 bleu,0.5848371982574463
translation,81,83,results,initial base model,averaged,+ 1.0 bleu,initial base model averaged + 1.0 bleu,0.7262594103813171
translation,81,83,results,more effective,compared to,vanilla bt,more effective compared to vanilla bt,0.707801103591919
translation,81,83,results,our   bit,has,significantly improves,our   bit has significantly improves,0.6332746744155884
translation,81,83,results,significantly improves,has,initial base model,significantly improves has initial base model,0.6150792241096497
translation,81,83,results,bit-equipped bt,has,more effective,bit-equipped bt has more effective,0.5710936188697815
translation,81,83,results,vanilla bt,has,+ 2.8 bleu,vanilla bt has + 2.8 bleu,0.6157493591308594
translation,81,83,results,results,has,our   bit,results has our   bit,0.6299411058425903
translation,82,160,ablation-analysis,rule- based inflection,helps,all models,rule- based inflection helps all models,0.5636582374572754
translation,82,160,ablation-analysis,all models,leverage,lemma constraints,all models leverage lemma constraints,0.7704980969429016
translation,82,160,ablation-analysis,lemma constraints,has,more accurately,lemma constraints has more accurately,0.5795693397521973
translation,82,160,ablation-analysis,ablation analysis,Adding,rule- based inflection,ablation analysis Adding rule- based inflection,0.6952123045921326
translation,82,115,baselines,preprocessing,apply,normalization,preprocessing apply normalization,0.6566134691238403
translation,82,115,baselines,preprocessing,apply,tokenization,preprocessing apply tokenization,0.6511582136154175
translation,82,115,baselines,preprocessing,apply,true-casing,preprocessing apply true-casing,0.6873741149902344
translation,82,115,baselines,preprocessing,apply,bpe,preprocessing apply bpe,0.7291591167449951
translation,82,115,baselines,preprocessing,apply,"sennrich et al. , 2016 )","preprocessing apply sennrich et al. , 2016 )",0.6144146919250488
translation,82,115,baselines,normalization,has,tokenization,normalization has tokenization,0.5487873554229736
translation,82,115,baselines,bpe,has,"sennrich et al. , 2016 )","bpe has sennrich et al. , 2016 )",0.5594024062156677
translation,82,115,baselines,baselines,For,preprocessing,baselines For preprocessing,0.6370158195495605
translation,82,116,baselines,auto-regressive ( ar ) baseline,without integrating,terminology constraints,auto-regressive ( ar ) baseline without integrating terminology constraints,0.7623370885848999
translation,82,117,baselines,ar,with,constrained decoding ( cd ),ar with constrained decoding ( cd ),0.686303436756134
translation,82,117,baselines,ar,with,target lemma annotation ( tla ),ar with target lemma annotation ( tla ),0.639585018157959
translation,82,117,baselines,constrained decoding ( cd ),to incorporate,hard constraints,constrained decoding ( cd ) to incorporate hard constraints,0.6389245986938477
translation,82,117,baselines,target lemma annotation ( tla ),integrates,lemma constraints,target lemma annotation ( tla ) integrates lemma constraints,0.6454933285713196
translation,82,117,baselines,target lemma annotation ( tla ),as,additional input stream,target lemma annotation ( tla ) as additional input stream,0.49286726117134094
translation,82,117,baselines,lemma constraints,as,additional input stream,lemma constraints as additional input stream,0.476102352142334
translation,82,117,baselines,additional input stream,on,source side,additional input stream on source side,0.5582982301712036
translation,82,117,baselines,baselines,has,ar,baselines has ar,0.6460148096084595
translation,82,118,baselines,nar with constraints ( nar + c ),integrates,constraints,nar with constraints ( nar + c ) integrates constraints,0.6579453349113464
translation,82,118,baselines,constraints,as,initial sequence,constraints as initial sequence,0.5065140128135681
translation,82,118,baselines,initial sequence,in EDITOR,without explicit inflection,initial sequence in EDITOR without explicit inflection,0.6816451549530029
translation,82,118,baselines,baselines,has,nar with constraints ( nar + c ),baselines has nar with constraints ( nar + c ),0.5616175532341003
translation,82,8,experiments,linguistically motivated rule-based and data-driven neuralbased inflection modules,design,english - german health and english - lithuanian news test suites,linguistically motivated rule-based and data-driven neuralbased inflection modules design english - german health and english - lithuanian news test suites,0.5619705319404602
translation,82,121,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2015 ) with initial learning rate",0.5927683115005493
translation,82,121,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,effective batch sizes,"adam optimizer ( kingma and ba , 2015 ) with effective batch sizes",0.6098859906196594
translation,82,121,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,64 k tokens,"adam optimizer ( kingma and ba , 2015 ) with 64 k tokens",0.6358909010887146
translation,82,121,hyperparameters,initial learning rate,of,0.0005,initial learning rate of 0.0005,0.5819736123085022
translation,82,121,hyperparameters,effective batch sizes,of,32 k tokens,effective batch sizes of 32 k tokens,0.5856842994689941
translation,82,121,hyperparameters,effective batch sizes,of,64 k tokens,effective batch sizes of 64 k tokens,0.5821151733398438
translation,82,121,hyperparameters,32 k tokens,for,ar models,32 k tokens for ar models,0.5740808248519897
translation,82,121,hyperparameters,64 k tokens,for,nar models,64 k tokens for nar models,0.5819405913352966
translation,82,121,hyperparameters,nar models,for,maximum 300k steps,nar models for maximum 300k steps,0.6070282459259033
translation,82,123,hyperparameters,best checkpoint,based on,validation perplexity,best checkpoint based on validation perplexity,0.6182332038879395
translation,82,123,hyperparameters,hyperparameters,select,best checkpoint,hyperparameters select best checkpoint,0.6854963302612305
translation,82,139,hyperparameters,training,initialize,encoder parameters,training initialize encoder parameters,0.770551323890686
translation,82,139,hyperparameters,training,train it using,adam optimizer,training train it using adam optimizer,0.7390755414962769
translation,82,139,hyperparameters,encoder parameters,using,nar baseline encoder,encoder parameters using nar baseline encoder,0.6518402099609375
translation,82,139,hyperparameters,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,82,139,hyperparameters,batch size,of,32 k tokens,batch size of 32 k tokens,0.5888497233390808
translation,82,139,hyperparameters,32 k tokens,for,maximum 200k steps,32 k tokens for maximum 200k steps,0.5859997272491455
translation,82,139,hyperparameters,hyperparameters,For,training,hyperparameters For training,0.5661544799804688
translation,82,139,hyperparameters,hyperparameters,train it using,adam optimizer,hyperparameters train it using adam optimizer,0.6452181339263916
translation,82,6,model,modular framework,for incorporating,lemma constraints,modular framework for incorporating lemma constraints,0.6919596195220947
translation,82,6,model,lemma constraints,in,neural mt ( nmt ),lemma constraints in neural mt ( nmt ),0.5404930710792542
translation,82,6,model,lemma constraints,in which,diverse types of nmt models,lemma constraints in which diverse types of nmt models,0.6129565238952637
translation,82,6,model,neural mt ( nmt ),in which,linguistic knowledge,neural mt ( nmt ) in which linguistic knowledge,0.6082286238670349
translation,82,6,model,model,introduce,modular framework,model introduce modular framework,0.6911276578903198
translation,82,7,model,novel cross-lingual inflection module,inflects,target lemma constraints,novel cross-lingual inflection module inflects target lemma constraints,0.7482991218566895
translation,82,7,model,target lemma constraints,based on,source context,target lemma constraints based on source context,0.6050910949707031
translation,82,18,model,modular framework,for inflecting,terminology constraints,modular framework for inflecting terminology constraints,0.6367529630661011
translation,82,18,model,terminology constraints,in,nmt,terminology constraints in nmt,0.5359609127044678
translation,82,18,model,model,introduce,modular framework,model introduce modular framework,0.6911276578903198
translation,82,19,model,cross-lingual inflection module,predicts,inflected form,cross-lingual inflection module predicts inflected form,0.7027381658554077
translation,82,19,model,inflected form,of,each lemma constraint,inflected form of each lemma constraint,0.5717381238937378
translation,82,19,model,inflected form,based on,source context only,inflected form based on source context only,0.5784234404563904
translation,82,19,model,model,relies on,cross-lingual inflection module,model relies on cross-lingual inflection module,0.7281020283699036
translation,82,49,model,inflected forms,can be,inferred,inflected forms can be inferred,0.6361159086227417
translation,82,49,model,inferred,based only on,source context,inferred based only on source context,0.6292181015014648
translation,82,49,model,fluent translation,by,nmt models,fluent translation by nmt models,0.5612317323684692
translation,82,49,model,model,assume that,inflected forms,model assume that inflected forms,0.6208131909370422
translation,82,124,model,nar,trained via,sequence - level knowledge distillation,nar trained via sequence - level knowledge distillation,0.723943829536438
translation,82,124,model,model,has,nar,model has nar,0.5746459364891052
translation,82,146,results,rule- based inflection module,achieves,higher inflection accuracy,rule- based inflection module achieves higher inflection accuracy,0.6700787544250488
translation,82,146,results,higher inflection accuracy,than,neural - based module,higher inflection accuracy than neural - based module,0.5908073782920837
translation,82,146,results,higher inflection accuracy,than,neuralbased module,higher inflection accuracy than neuralbased module,0.5766782164573669
translation,82,146,results,higher inflection accuracy,than,rule- based module,higher inflection accuracy than rule- based module,0.5871044993400574
translation,82,146,results,neural - based module,on,both test suites,neural - based module on both test suites,0.5076029896736145
translation,82,146,results,neuralbased module,obtains,81.2 % accuracy,neuralbased module obtains 81.2 % accuracy,0.5607438087463379
translation,82,146,results,neuralbased module,obtains,15.4 % accuracy,neuralbased module obtains 15.4 % accuracy,0.5764689445495605
translation,82,146,results,neuralbased module,obtains,77.4 % accuracy,neuralbased module obtains 77.4 % accuracy,0.5652852654457092
translation,82,146,results,81.2 % accuracy,on,en- de health set,81.2 % accuracy on en- de health set,0.5760450959205627
translation,82,146,results,15.4 % accuracy,on,en- lt news set,15.4 % accuracy on en- lt news set,0.5421661138534546
translation,82,146,results,rule- based module,achieves,87.6 % accuracy,rule- based module achieves 87.6 % accuracy,0.6198809146881104
translation,82,146,results,rule- based module,achieves,77.4 % accuracy,rule- based module achieves 77.4 % accuracy,0.6141969561576843
translation,82,146,results,87.6 % accuracy,on,en-de,87.6 % accuracy on en-de,0.6411721110343933
translation,82,146,results,77.4 % accuracy,on,en-lt,77.4 % accuracy on en-lt,0.6284309029579163
translation,82,146,results,results,has,rule- based inflection module,results has rule- based inflection module,0.5316702723503113
translation,82,147,results,rule- based module,achieves,close accuracy,rule- based module achieves close accuracy,0.6849496364593506
translation,82,147,results,rule- based module,achieves,higher accuracy,rule- based module achieves higher accuracy,0.677177369594574
translation,82,147,results,close accuracy,to,tla,close accuracy to tla,0.5954744815826416
translation,82,147,results,tla,on,en-de,tla on en-de,0.765008270740509
translation,82,147,results,higher accuracy,on,en-lt,higher accuracy on en-lt,0.6438316106796265
translation,82,147,results,results,has,rule- based module,results has rule- based module,0.5632171630859375
translation,82,153,results,final training perplexity,remains at,5.7,final training perplexity remains at 5.7,0.551723062992096
translation,82,153,results,5.1,on,en-de,5.1 on en-de,0.6398155093193054
translation,82,153,results,5.7,on,en-lt,5.7 on en-lt,0.6739763021469116
translation,82,153,results,results,has,final training perplexity,results has final training perplexity,0.5413395166397095
translation,82,156,results,impact,of,rule-based and neural - based inflection modules,impact of rule-based and neural - based inflection modules,0.60759037733078
translation,82,156,results,rule-based and neural - based inflection modules,on top of,range of ar and nar baselines,rule-based and neural - based inflection modules on top of range of ar and nar baselines,0.6561420559883118
translation,82,156,results,results,shows,impact,results shows impact,0.6381149888038635
translation,82,157,results,nar baselines without constraints,achieves,competitive bleu,nar baselines without constraints achieves competitive bleu,0.6791501045227051
translation,82,157,results,nar baselines without constraints,achieves,slightly lower bleu,nar baselines without constraints achieves slightly lower bleu,0.6558492183685303
translation,82,157,results,competitive bleu,to,ar baseline,competitive bleu to ar baseline,0.5207229852676392
translation,82,157,results,ar baseline,on,en-lt,ar baseline on en-lt,0.7607232332229614
translation,82,157,results,slightly lower bleu,on,en-de,slightly lower bleu on en-de,0.6866332292556763
translation,82,157,results,results,has,nar baselines without constraints,results has nar baselines without constraints,0.5765367746353149
translation,82,159,results,nar + c without inflection,obtains,lower term usage,nar + c without inflection obtains lower term usage,0.5959151983261108
translation,82,159,results,nar + c without inflection,obtains,close or lower bleu,nar + c without inflection obtains close or lower bleu,0.6041236519813538
translation,82,159,results,close or lower bleu,than,ar with tla,close or lower bleu than ar with tla,0.6548704504966736
translation,82,161,results,en- de,significantly improves,term usage accuracy,en- de significantly improves term usage accuracy,0.750806450843811
translation,82,161,results,term usage accuracy,of,ar,term usage accuracy of ar,0.5760681629180908
translation,82,161,results,term usage accuracy,of,nar + c models,term usage accuracy of nar + c models,0.5634251832962036
translation,82,161,results,ar,with,cd,ar with cd,0.7650318741798401
translation,82,161,results,cd,by,+ 4.7 %,cd by + 4.7 %,0.5912426710128784
translation,82,161,results,nar + c models,by,+ 5.1 %,nar + c models by + 5.1 %,0.6066406965255737
translation,82,161,results,results,On,en- de,results On en- de,0.7025283575057983
translation,82,162,results,significantly improves,both,lemma usage rate and term usage accuracy,significantly improves both lemma usage rate and term usage accuracy,0.6460674405097961
translation,82,162,results,lemma usage rate and term usage accuracy,of,ar,lemma usage rate and term usage accuracy of ar,0.6633131504058838
translation,82,162,results,ar,with,cd,ar with cd,0.7650318741798401
translation,82,162,results,ar,with,nar +c,ar with nar +c,0.7063115239143372
translation,82,162,results,+ 3.2 %,on,lemma usage,+ 3.2 % on lemma usage,0.5724784135818481
translation,82,162,results,+ 10.7 %,on,term usage,+ 10.7 % on term usage,0.537926971912384
translation,82,162,results,+ 3.5 %,on,lemma usage,+ 3.5 % on lemma usage,0.5675308108329773
translation,82,162,results,+ 11.0 %,on,term usage,+ 11.0 % on term usage,0.5385518074035645
translation,82,162,results,cd,has,+ 3.2 %,cd has + 3.2 %,0.597036600112915
translation,82,162,results,nar +c,has,+ 3.5 %,nar +c has + 3.5 %,0.6081371903419495
translation,82,162,results,results,On,en-lt,results On en-lt,0.6671424508094788
translation,82,163,results,term accuracy,of,en-lt ar,term accuracy of en-lt ar,0.6935993432998657
translation,82,163,results,en-lt ar,with,tla,en-lt ar with tla,0.6939067840576172
translation,82,163,results,results,improves,term accuracy,results improves term accuracy,0.7040626406669617
translation,82,164,results,constraints,require,inflection,constraints require inflection,0.6796680092811584
translation,82,164,results,rule- based modules,improves by,4.4- 8.3 %,rule- based modules improves by 4.4- 8.3 %,0.6678293347358704
translation,82,164,results,rule- based modules,improves by,38.6-46.5 %,rule- based modules improves by 38.6-46.5 %,0.6655681133270264
translation,82,164,results,rule- based modules,improves by,38.7- 48.1 %,rule- based modules improves by 38.7- 48.1 %,0.6627174019813538
translation,82,164,results,4.4- 8.3 %,on,tla,4.4- 8.3 % on tla,0.5831393003463745
translation,82,164,results,38.6-46.5 %,on,cd,38.6-46.5 % on cd,0.592775285243988
translation,82,164,results,38.7- 48.1 %,on,nar +c,38.7- 48.1 % on nar +c,0.5779589414596558
translation,82,164,results,constraints,has,rule- based modules,constraints has rule- based modules,0.5845351219177246
translation,82,164,results,inflection,has,rule- based modules,inflection has rule- based modules,0.5813284516334534
translation,82,165,results,rule- based modules,has,outperform,rule- based modules has outperform,0.6093100309371948
translation,82,165,results,outperform,has,neural - based ones,outperform has neural - based ones,0.588371753692627
translation,82,170,results,rule- based inflection,combined with,nar +c,rule- based inflection combined with nar +c,0.6839901208877563
translation,82,170,results,rule- based inflection,achieves,close lemma and term usage rates ( ? ? 2 % ),rule- based inflection achieves close lemma and term usage rates ( ? ? 2 % ),0.6573531031608582
translation,82,170,results,rule- based inflection,achieves,+ 7.8 % higher term usage accuracy,rule- based inflection achieves + 7.8 % higher term usage accuracy,0.6280543804168701
translation,82,170,results,close lemma and term usage rates ( ? ? 2 % ),to,tla,close lemma and term usage rates ( ? ? 2 % ) to tla,0.6075557470321655
translation,82,170,results,tla,on,en-de,tla on en-de,0.765008270740509
translation,82,170,results,+ 7.8 % higher term usage accuracy,on,en-lt,+ 7.8 % higher term usage accuracy on en-lt,0.5882910490036011
translation,82,170,results,results,has,rule- based inflection,results has rule- based inflection,0.5205034613609314
translation,82,171,results,largest improvements,on,constraints,largest improvements on constraints,0.5316799879074097
translation,82,171,results,constraints,that require,inflection,constraints that require inflection,0.7039322257041931
translation,82,171,results,+ 20.5 %,on,lemma usage,+ 20.5 % on lemma usage,0.5728074908256531
translation,82,171,results,+ 10.6 %,on,term usage,+ 10.6 % on term usage,0.5407775044441223
translation,82,171,results,en-lt,has,largest improvements,en-lt has largest improvements,0.6057530641555786
translation,82,171,results,inflection,has,+ 20.5 %,inflection has + 20.5 %,0.5776395797729492
translation,82,171,results,results,On,en-lt,results On en-lt,0.6671424508094788
translation,82,172,results,constraints,preserves,translation quality,constraints preserves translation quality,0.7141090035438538
translation,82,172,results,no significant difference,in,bleu,no significant difference in bleu,0.6217076182365417
translation,82,172,results,results,Incorporating,constraints,results Incorporating constraints,0.6529518365859985
translation,82,173,results,benefits,of,integrating,benefits of integrating,0.5924785733222961
translation,82,173,results,linguistic knowledge,via,rule- based inflection,linguistic knowledge via rule- based inflection,0.5595976114273071
translation,82,173,results,rule- based inflection,over,purely data-driven approaches,rule- based inflection over purely data-driven approaches,0.6541078686714172
translation,82,173,results,integrating,has,linguistic knowledge,integrating has linguistic knowledge,0.54617840051651
translation,82,173,results,results,show,benefits,results show benefits,0.6036745309829712
translation,82,174,results,nar + c,with,rule- based inflection,nar + c with rule- based inflection,0.626905620098114
translation,82,174,results,whole nmt model,to incorporate,new lemma terms,whole nmt model to incorporate new lemma terms,0.7174879908561707
translation,82,191,results,tla,to,copy constraint terms,tla to copy constraint terms,0.5386740565299988
translation,82,191,results,tla,adding,rule- based inflection module,tla adding rule- based inflection module,0.6786298751831055
translation,82,191,results,copy constraint terms,infrequent in,training data,copy constraint terms infrequent in training data,0.6838522553443909
translation,82,191,results,rule- based inflection module,helps,tla,rule- based inflection module helps tla,0.605802059173584
translation,82,191,results,tla,has,inflect,tla has inflect,0.5397285223007202
translation,82,191,results,inflect,has,term,inflect has term,0.589510440826416
translation,82,191,results,inflect,has,correctly,inflect has correctly,0.6465544700622559
translation,82,191,results,term,has,correctly,term has correctly,0.6598871946334839
translation,82,191,results,results,has,tla,results has tla,0.5378400683403015
translation,82,192,results,inflection module,improves,translation,inflection module improves translation,0.687212347984314
translation,82,192,results,translation,of,context,translation of context,0.5880863666534424
translation,82,192,results,context,around,constraint terms,context around constraint terms,0.613383948802948
translation,82,192,results,nar +c models,has,inflection module,nar +c models has inflection module,0.5753918290138245
translation,82,192,results,results,In,nar +c models,results In nar +c models,0.49908989667892456
translation,83,164,ablation-analysis,non-explicit mas,to,system,non-explicit mas to system,0.588051974773407
translation,83,164,ablation-analysis,non-explicit mas,on average,user,non-explicit mas on average user,0.655097484588623
translation,83,164,ablation-analysis,user,reduces,his effort,user reduces his effort,0.6823834776878357
translation,83,164,ablation-analysis,his effort,by,27.45 %,his effort by 27.45 %,0.556977391242981
translation,83,164,ablation-analysis,non-explicit mas,has,user,non-explicit mas has user,0.6008222699165344
translation,83,164,ablation-analysis,system,has,user,system has user,0.5850356817245483
translation,83,164,ablation-analysis,ablation analysis,adding,non-explicit mas,ablation analysis adding non-explicit mas,0.7114703059196472
translation,83,166,ablation-analysis,reduction,is,55.9 %,reduction is 55.9 %,0.5676099061965942
translation,83,166,ablation-analysis,interactive - explicit mas,has,reduction,interactive - explicit mas has reduction,0.573859453201294
translation,83,166,ablation-analysis,ablation analysis,take account of,interactive - explicit mas,ablation analysis take account of interactive - explicit mas,0.6620029211044312
translation,83,183,ablation-analysis,umar results,see that,smt models,umar results see that smt models,0.638485312461853
translation,83,183,ablation-analysis,umar results,in,smt models,umar results in smt models,0.5414118766784668
translation,83,183,ablation-analysis,percentage of umar,goes from,6 % to 12 %,percentage of umar goes from 6 % to 12 %,0.5848131775856018
translation,83,183,ablation-analysis,6 % to 12 %,causing,lower wsr relative improvement,6 % to 12 % causing lower wsr relative improvement,0.7129085659980774
translation,83,183,ablation-analysis,smt models,has,percentage of umar,smt models has percentage of umar,0.5802404284477234
translation,83,183,ablation-analysis,ablation analysis,From,umar results,ablation analysis From umar results,0.5324376821517944
translation,83,154,baselines,encoder-decoder architecture,with,attention model,encoder-decoder architecture with attention model,0.6023164987564087
translation,83,154,baselines,baselines,has,rnn - based nmt system,baselines has rnn - based nmt system,0.5433911085128784
translation,83,151,hyperparameters,"adam ( kingma and ba , 2017 )",as,learning algorithm,"adam ( kingma and ba , 2017 ) as learning algorithm",0.5325255990028381
translation,83,151,hyperparameters,"adam ( kingma and ba , 2017 )",with,learning rate,"adam ( kingma and ba , 2017 ) with learning rate",0.5964986085891724
translation,83,151,hyperparameters,learning rate,of,0.0002,learning rate of 0.0002,0.5917128324508667
translation,83,152,hyperparameters,l 2 norm,of,gradient,l 2 norm of gradient,0.5590420961380005
translation,83,152,hyperparameters,gradient,to,5,gradient to 5,0.6626070141792297
translation,83,152,hyperparameters,hyperparameters,clipped,l 2 norm,hyperparameters clipped l 2 norm,0.6337196230888367
translation,83,153,hyperparameters,batch size,set to,30,batch size set to 30,0.7558931112289429
translation,83,153,hyperparameters,beam size,to,6,beam size to 6,0.6420853137969971
translation,83,153,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,83,153,hyperparameters,hyperparameters,has,beam size,hyperparameters has beam size,0.516274631023407
translation,83,155,hyperparameters,dimensions,of,"encoder , decoder , attention model","dimensions of encoder , decoder , attention model",0.5705806612968445
translation,83,155,hyperparameters,dimensions,of,word embeddings,dimensions of word embeddings,0.49344462156295776
translation,83,155,hyperparameters,dimensions,set to,512,dimensions set to 512,0.7065618634223938
translation,83,155,hyperparameters,word embeddings,set to,512,word embeddings set to 512,0.6421719789505005
translation,83,155,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,83,157,hyperparameters,"transformer ( vaswani et al. , 2017 )",used,word embedding,"transformer ( vaswani et al. , 2017 ) used word embedding",0.5403851270675659
translation,83,157,hyperparameters,"transformer ( vaswani et al. , 2017 )",used,dimension size,"transformer ( vaswani et al. , 2017 ) used dimension size",0.6089656949043274
translation,83,157,hyperparameters,dimension size,of,512,dimension size of 512,0.6308591365814209
translation,83,157,hyperparameters,hyperparameters,has,"transformer ( vaswani et al. , 2017 )","hyperparameters has transformer ( vaswani et al. , 2017 )",0.4982716739177704
translation,83,158,hyperparameters,hidden and output dimensions,of,feed -forward layers,hidden and output dimensions of feed -forward layers,0.5469029545783997
translation,83,158,hyperparameters,hidden and output dimensions,set to,2048 and 512,hidden and output dimensions set to 2048 and 512,0.6880975365638733
translation,83,158,hyperparameters,feed -forward layers,set to,2048 and 512,feed -forward layers set to 2048 and 512,0.7005046010017395
translation,83,158,hyperparameters,hyperparameters,has,hidden and output dimensions,hyperparameters has hidden and output dimensions,0.5003905296325684
translation,83,6,model,bandit feedback,as,principal,bandit feedback as principal,0.5940280556678772
translation,83,6,model,model,uses,bandit feedback,model uses bandit feedback,0.6108888387680054
translation,83,156,model,single hidden layer,of,encoder and the decoder,single hidden layer of encoder and the decoder,0.582801103591919
translation,83,156,model,model,used,single hidden layer,model used single hidden layer,0.6376118659973145
translation,83,159,model,multi-head attention layer,stacked,6 layers,multi-head attention layer stacked 6 layers,0.6882931590080261
translation,83,159,model,6 layers,of,encoder and decoder,6 layers of encoder and decoder,0.5862308740615845
translation,83,159,model,multi-head attention layer,has,8 heads,multi-head attention layer has 8 heads,0.5797160267829895
translation,83,159,model,model,has,multi-head attention layer,model has multi-head attention layer,0.5072199702262878
translation,83,182,results,smt models,get,wsr relative improvement,smt models get wsr relative improvement,0.5387080907821655
translation,83,182,results,wsr relative improvement,around,24 %,wsr relative improvement around 24 %,0.6644653677940369
translation,83,182,results,wsr relative improvement,around,57 %,wsr relative improvement around 57 %,0.6454861164093018
translation,83,182,results,nmt models,obtained,relative improvement,nmt models obtained relative improvement,0.6190099716186523
translation,83,182,results,relative improvement,around,57 %,relative improvement around 57 %,0.6849165558815002
translation,83,184,results,nmt model,maintains,percentage of umar,nmt model maintains percentage of umar,0.6772531270980835
translation,83,184,results,percentage of umar,around,35 %,percentage of umar around 35 %,0.685408890247345
translation,83,184,results,results,has,nmt model,results has nmt model,0.5247517228126526
translation,84,6,model,mbert based procedure,whose,core learnable component,mbert based procedure whose core learnable component,0.6476137638092041
translation,84,6,model,core learnable component,is,ternary sequence labeling model,core learnable component is ternary sequence labeling model,0.5289352536201477
translation,84,6,model,ternary sequence labeling model,trained with,limited code-mixed corpus,ternary sequence labeling model trained with limited code-mixed corpus,0.7052112817764282
translation,84,6,model,model,present,mbert based procedure,model present mbert based procedure,0.6911194324493408
translation,84,14,model,"mbert ( devlin et al. , 2019 ) based procedure",for converting,non,"mbert ( devlin et al. , 2019 ) based procedure for converting non",0.760381281375885
translation,84,14,model,"mbert ( devlin et al. , 2019 ) based procedure",for converting,-cm parallel data,"mbert ( devlin et al. , 2019 ) based procedure for converting -cm parallel data",0.7120905518531799
translation,84,14,model,-cm parallel data,to,cm parallel data,-cm parallel data to cm parallel data,0.6010075807571411
translation,84,14,model,non,has,-cm parallel data,non has -cm parallel data,0.6154558658599854
translation,84,14,model,model,present,"mbert ( devlin et al. , 2019 ) based procedure","model present mbert ( devlin et al. , 2019 ) based procedure",0.6741901636123657
translation,84,15,model,core learnable component,of,our procedure,core learnable component of our procedure,0.5505380630493164
translation,84,15,model,core learnable component,requires,fine-tuning mbert,core learnable component requires fine-tuning mbert,0.6800294518470764
translation,84,15,model,fine-tuning mbert,for,three - way sequence labeling task,fine-tuning mbert for three - way sequence labeling task,0.5514860153198242
translation,84,15,model,model,has,core learnable component,model has core learnable component,0.5544705986976624
translation,84,17,model,existing back - translation method,of using,monolingual target data,existing back - translation method of using monolingual target data,0.637991189956665
translation,84,17,model,model,extend,existing back - translation method,model extend existing back - translation method,0.7242732644081116
translation,84,19,results,43.9,to,46.4,43.9 to 46.4,0.5969300866127014
translation,84,19,results,our data augmentation strategy,has,translation bleu,our data augmentation strategy has translation bleu,0.5588967800140381
translation,84,20,results,sentences,that,more heavily code-mixed,sentences that more heavily code-mixed,0.6413351893424988
translation,84,20,results,sentences,on,adversarial test set,sentences on adversarial test set,0.5132840871810913
translation,84,20,results,accuracy,on,adversarial test set,accuracy on adversarial test set,0.5037341117858887
translation,84,20,results,increases,by,5.8 bleu points,increases by 5.8 bleu points,0.617606520652771
translation,84,20,results,adversarial test set,baseline provides,poor accuracy,adversarial test set baseline provides poor accuracy,0.770427942276001
translation,84,20,results,poor accuracy,show,5.4 point bleu increase,poor accuracy show 5.4 point bleu increase,0.6414100527763367
translation,84,20,results,more heavily code-mixed,has,accuracy,more heavily code-mixed has accuracy,0.5768072009086609
translation,84,20,results,accuracy,has,increases,accuracy has increases,0.6037072539329529
translation,84,20,results,results,On,sentences,results On sentences,0.5138509273529053
translation,84,21,results,performance,for,code-switched test sets,performance for code-switched test sets,0.6406785249710083
translation,84,21,results,performance,while maintaining,state of the art performance,performance while maintaining state of the art performance,0.5912021994590759
translation,84,21,results,code-switched test sets,while maintaining,state of the art performance,code-switched test sets while maintaining state of the art performance,0.6248854398727417
translation,84,21,results,state of the art performance,on,non-code-switched inputs,state of the art performance on non-code-switched inputs,0.5281800031661987
translation,84,21,results,our data augmentation strategy,has,improves,our data augmentation strategy has improves,0.6251908540725708
translation,84,21,results,improves,has,performance,improves has performance,0.5770372748374939
translation,84,21,results,results,show that,our data augmentation strategy,results show that our data augmentation strategy,0.5354664921760559
translation,85,81,baselines,tokenization method,based on,bpe and sentencepiece,tokenization method based on bpe and sentencepiece,0.6676011085510254
translation,85,5,experimental-setup,parallel corpora,provided by,wmt 2021 organizers,parallel corpora provided by wmt 2021 organizers,0.611515998840332
translation,85,5,experimental-setup,parallel corpora,for,training,parallel corpora for training,0.5854470729827881
translation,85,5,experimental-setup,wmt 2021 organizers,for,training,wmt 2021 organizers for training,0.6336933970451355
translation,85,5,experimental-setup,development and test data,from,wmt 2020,development and test data from wmt 2020,0.6089276075363159
translation,85,5,experimental-setup,experimental setup,use,parallel corpora,experimental setup use parallel corpora,0.5628107190132141
translation,85,5,experimental-setup,experimental setup,use,development and test data,experimental setup use development and test data,0.6325545310974121
translation,85,82,experimental-setup,"sentencepiece ( kudo and richardson , 2018 )",to train,sentencepiece models,"sentencepiece ( kudo and richardson , 2018 ) to train sentencepiece models",0.6621640920639038
translation,85,82,experimental-setup,sentencepiece models,for,japanese and english,sentencepiece models for japanese and english,0.5750610828399658
translation,85,82,experimental-setup,japanese and english,with,"32,000","japanese and english with 32,000",0.5738445520401001
translation,85,82,experimental-setup,"32,000",as,vocabulary size,"32,000 as vocabulary size",0.5309593677520752
translation,85,82,experimental-setup,experimental setup,used,"sentencepiece ( kudo and richardson , 2018 )","experimental setup used sentencepiece ( kudo and richardson , 2018 )",0.6295887231826782
translation,85,4,model,politeness - and - formality - aware model,implementing,tagger,politeness - and - formality - aware model implementing tagger,0.6823626756668091
translation,85,4,model,politeness - and - formality - aware model,implementing,back - translation,politeness - and - formality - aware model implementing back - translation,0.6953455209732056
translation,85,4,model,politeness - and - formality - aware model,implementing,model ensembling,politeness - and - formality - aware model implementing model ensembling,0.7036550641059875
translation,85,4,model,politeness - and - formality - aware model,implementing,n-best reranking,politeness - and - formality - aware model implementing n-best reranking,0.6536118984222412
translation,85,155,results,training,with,normalized data,training with normalized data,0.6712796688079834
translation,85,155,results,normalized data,by,orthographic conversion,normalized data by orthographic conversion,0.6121529936790466
translation,85,155,results,normalized data,does not improve,models,normalized data does not improve models,0.6429903507232666
translation,85,155,results,models,over,baseline,models over baseline,0.736150860786438
translation,85,155,results,results,seen from,training,results seen from training,0.6174346804618835
translation,85,156,results,models,trained on,normalized data,models trained on normalized data,0.7717868685722351
translation,85,156,results,normalized data,have,similar performances,normalized data have similar performances,0.5676640868186951
translation,85,156,results,results,has,models,results has models,0.5335168838500977
translation,85,166,results,back - translated data,improved,results,back - translated data improved results,0.6963561773300171
translation,85,166,results,results,Using,back - translated data,results Using back - translated data,0.6073518991470337
translation,85,167,results,back - translation,improves,translation quality,back - translation improves translation quality,0.7035064101219177
translation,85,167,results,back - translation,for,languages with low resources,back - translation for languages with low resources,0.6004775166511536
translation,85,167,results,results,reinforce,previous findings,results reinforce previous findings,0.5701952576637268
translation,85,167,results,results,for,languages with low resources,results for languages with low resources,0.6140642762184143
translation,86,228,baselines,xlm,jointly learns,mlm,xlm jointly learns mlm,0.6765874028205872
translation,86,228,baselines,xlm,jointly learns,translation language modeling ( tlm ) task,xlm jointly learns translation language modeling ( tlm ) task,0.7301469445228577
translation,86,228,baselines,baselines,has,xlm,baselines has xlm,0.6134690642356873
translation,86,233,baselines,"infoxlm ( chi et al. , 2021a )",proposes,cross-lingual contrastive learning,"infoxlm ( chi et al. , 2021a ) proposes cross-lingual contrastive learning",0.6251293420791626
translation,86,233,baselines,baselines,has,"infoxlm ( chi et al. , 2021a )","baselines has infoxlm ( chi et al. , 2021a )",0.5591859221458435
translation,86,234,baselines,"xlm - align ( chi et al. , 2021 b )",leverages,token - level alignments,"xlm - align ( chi et al. , 2021 b ) leverages token - level alignments",0.6894983649253845
translation,86,234,baselines,token - level alignments,implied in,translation pairs,token - level alignments implied in translation pairs,0.6684333086013794
translation,86,234,baselines,translation pairs,to improve,cross-lingual transfer,translation pairs to improve cross-lingual transfer,0.6638557314872742
translation,86,234,baselines,baselines,has,"xlm - align ( chi et al. , 2021 b )","baselines has xlm - align ( chi et al. , 2021 b )",0.5417271852493286
translation,86,235,baselines,"xnlg ( chi et al. , 2020 )",introduces,cross-lingual transfer,"xnlg ( chi et al. , 2020 ) introduces cross-lingual transfer",0.5952456593513489
translation,86,235,baselines,"xnlg ( chi et al. , 2020 )",achieves,zero-shot cross-lingual transfer,"xnlg ( chi et al. , 2020 ) achieves zero-shot cross-lingual transfer",0.6453301310539246
translation,86,235,baselines,cross-lingual transfer,for,nlg tasks,cross-lingual transfer for nlg tasks,0.5804398655891418
translation,86,235,baselines,zero-shot cross-lingual transfer,for,question generation,zero-shot cross-lingual transfer for question generation,0.5494059920310974
translation,86,235,baselines,zero-shot cross-lingual transfer,for,abstractive summarization,zero-shot cross-lingual transfer for abstractive summarization,0.5096139311790466
translation,86,235,baselines,baselines,has,"xnlg ( chi et al. , 2020 )","baselines has xnlg ( chi et al. , 2020 )",0.551448404788971
translation,86,236,baselines,"veco ( luo et al. , 2020 )",pretrains,variable cross-lingual pre-training model,"veco ( luo et al. , 2020 ) pretrains variable cross-lingual pre-training model",0.7568232417106628
translation,86,236,baselines,variable cross-lingual pre-training model,that learns,unified language representations,variable cross-lingual pre-training model that learns unified language representations,0.6175715327262878
translation,86,236,baselines,unified language representations,for,nlu and nlg,unified language representations for nlu and nlg,0.5995234251022339
translation,86,236,baselines,baselines,has,"veco ( luo et al. , 2020 )","baselines has veco ( luo et al. , 2020 )",0.5434543490409851
translation,86,136,experimental-setup,vocabulary,provided by,"xlm -r ( conneau et al. , 2020 )","vocabulary provided by xlm -r ( conneau et al. , 2020 )",0.6972225904464722
translation,86,136,experimental-setup,vocabulary,extend it with,100 unique mask tokens,vocabulary extend it with 100 unique mask tokens,0.6730871200561523
translation,86,136,experimental-setup,100 unique mask tokens,for,span corruption tasks,100 unique mask tokens for span corruption tasks,0.5829775929450989
translation,86,136,experimental-setup,experimental setup,use,vocabulary,experimental setup use vocabulary,0.6008449792861938
translation,86,137,experimental-setup,mt6,for,0.5 m steps,mt6 for 0.5 m steps,0.6520594954490662
translation,86,137,experimental-setup,mt6,with,batches,mt6 with batches,0.7543810606002808
translation,86,137,experimental-setup,0.5 m steps,with,batches,0.5 m steps with batches,0.6795742511749268
translation,86,137,experimental-setup,batches,of,256 length - 512 input sequences,batches of 256 length - 512 input sequences,0.6114507913589478
translation,86,137,experimental-setup,experimental setup,pretrain,mt6,experimental setup pretrain mt6,0.751421332359314
translation,86,138,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,linear learning rate scheduler,"adam optimizer ( kingma and ba , 2015 ) with linear learning rate scheduler",0.6043406128883362
translation,86,138,experimental-setup,experimental setup,optimized by,"adam optimizer ( kingma and ba , 2015 )","experimental setup optimized by adam optimizer ( kingma and ba , 2015 )",0.7139168381690979
translation,86,139,experimental-setup,pre-training procedure,takes,about 2.5 days,pre-training procedure takes about 2.5 days,0.5929591655731201
translation,86,139,experimental-setup,about 2.5 days,on,nvidia dgx -2 station,about 2.5 days on nvidia dgx -2 station,0.5476138591766357
translation,86,139,experimental-setup,experimental setup,has,pre-training procedure,experimental setup has pre-training procedure,0.49479395151138306
translation,86,4,experiments,multilingual t5,pretrains,sequence - to-sequence model,multilingual t5 pretrains sequence - to-sequence model,0.7256273031234741
translation,86,4,experiments,sequence - to-sequence model,on,massive monolingual texts,sequence - to-sequence model on massive monolingual texts,0.5164031386375427
translation,86,4,experiments,multilingual t5,has,mt5,multilingual t5 has mt5,0.656731128692627
translation,86,6,experiments,three cross-lingual,has,text - to - text pre-training,three cross-lingual has text - to - text pre-training,0.5266302824020386
translation,86,135,experiments,"small- size transformer model ( xue et al. , 2020 )",with,"d model = 512 , d ff = 1 , 024 , 6 attention heads","small- size transformer model ( xue et al. , 2020 ) with d model = 512 , d ff = 1 , 024 , 6 attention heads",0.6177029013633728
translation,86,135,experiments,"small- size transformer model ( xue et al. , 2020 )",with,8 layers,"small- size transformer model ( xue et al. , 2020 ) with 8 layers",0.609518826007843
translation,86,135,experiments,8 layers,for,encoder and the decoder,8 layers for encoder and the decoder,0.6318029165267944
translation,86,135,experiments,8 layers,both,encoder and the decoder,8 layers both encoder and the decoder,0.6593562364578247
translation,86,182,experiments,"wikilingua ( ladhak et al. , 2020 ) dataset",containing,passage -summary pairs,"wikilingua ( ladhak et al. , 2020 ) dataset containing passage -summary pairs",0.6046454310417175
translation,86,182,experiments,passage -summary pairs,in,four language pairs,passage -summary pairs in four language pairs,0.550142228603363
translation,86,174,hyperparameters,models,for,20 epochs,models for 20 epochs,0.6000266671180725
translation,86,174,hyperparameters,models,with,batch size,models with batch size,0.6254024505615234
translation,86,174,hyperparameters,models,with,learning rate,models with learning rate,0.593966007232666
translation,86,174,hyperparameters,20 epochs,with,batch size,20 epochs with batch size,0.6002897024154663
translation,86,174,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,86,174,hyperparameters,learning rate,of,0.00001,learning rate of 0.00001,0.5829243659973145
translation,86,174,hyperparameters,hyperparameters,fine - tune,models,hyperparameters fine - tune models,0.7283819317817688
translation,86,175,hyperparameters,decoding,use,greedy decoding,decoding use greedy decoding,0.650183379650116
translation,86,175,hyperparameters,greedy decoding,for,all evaluated models,greedy decoding for all evaluated models,0.5847185254096985
translation,86,175,hyperparameters,hyperparameters,During,decoding,hyperparameters During decoding,0.6597015857696533
translation,86,183,hyperparameters,models,for,100k steps,models for 100k steps,0.6734154224395752
translation,86,183,hyperparameters,models,with,batch size,models with batch size,0.6254024505615234
translation,86,183,hyperparameters,models,with,learning rate,models with learning rate,0.593966007232666
translation,86,183,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,86,183,hyperparameters,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,86,183,hyperparameters,hyperparameters,fine- tune,models,hyperparameters fine- tune models,0.7283819317817688
translation,86,184,hyperparameters,greedy decoding,for,all evaluated models,greedy decoding for all evaluated models,0.5847185254096985
translation,86,184,hyperparameters,hyperparameters,use,greedy decoding,hyperparameters use greedy decoding,0.6568648219108582
translation,86,7,model,partially nonautoregressive objective,for,text - to - text pretraining,partially nonautoregressive objective for text - to - text pretraining,0.5767223834991455
translation,86,7,model,model,propose,partially nonautoregressive objective,model propose partially nonautoregressive objective,0.6616098284721375
translation,86,25,model,pnat objective,divides,target sequence,pnat objective divides target sequence,0.6808828711509705
translation,86,25,model,pnat objective,constrains,predictions,pnat objective constrains predictions,0.7906801700592041
translation,86,25,model,target sequence,into,several groups,target sequence into several groups,0.560175359249115
translation,86,25,model,predictions,conditioned on,source tokens,predictions conditioned on source tokens,0.6803894639015198
translation,86,25,model,predictions,conditioned on,target tokens,predictions conditioned on target tokens,0.6769822239875793
translation,86,25,model,target tokens,from,same group,target tokens from same group,0.5776240825653076
translation,86,25,model,model,has,pnat objective,model has pnat objective,0.5633045434951782
translation,86,20,results,mt6,differs from,mt5,mt6 differs from mt5,0.6504943370819092
translation,86,20,results,mt6,in terms of,training objective,mt6 in terms of training objective,0.7062402367591858
translation,86,20,results,mt5,in terms of,pre-training tasks,mt5 in terms of pre-training tasks,0.6872327327728271
translation,86,20,results,mt5,in terms of,training objective,mt5 in terms of training objective,0.7088319063186646
translation,86,20,results,results,has,mt6,results has mt6,0.5808969736099243
translation,86,27,results,mt6 model,yields,substantially better performance,mt6 model yields substantially better performance,0.7419887781143188
translation,86,27,results,substantially better performance,than,mt5,substantially better performance than mt5,0.6213440895080566
translation,86,27,results,mt5,on,eight benchmarks,mt5 on eight benchmarks,0.5130239725112915
translation,86,27,results,results,has,mt6 model,results has mt6 model,0.5582664012908936
translation,86,150,results,mt6,achieves,best performance,mt6 achieves best performance,0.7481249570846558
translation,86,150,results,best performance,on,xtreme,best performance on xtreme,0.591960608959198
translation,86,150,results,best performance,improving,average score,best performance improving average score,0.6664391756057739
translation,86,150,results,average score,from,45.0 to 50.4,average score from 45.0 to 50.4,0.5216705799102783
translation,86,150,results,results,observe,mt6,results observe mt6,0.6161753535270691
translation,86,151,results,model,with,machine translation task,model with machine translation task,0.5811685919761658
translation,86,151,results,model,performs,even worse,model performs even worse,0.637363612651825
translation,86,151,results,even worse,than,mt5,even worse than mt5,0.6670826077461243
translation,86,151,results,results,pre-training,model,results pre-training model,0.6919490694999695
translation,86,153,results,nmt pretrained model,shows,poor results,nmt pretrained model shows poor results,0.6543101668357849
translation,86,153,results,poor results,on,other four tasks,poor results on other four tasks,0.510581374168396
translation,86,153,results,other four tasks,where,all target languages,other four tasks where all target languages,0.5650966167449951
translation,86,153,results,results,has,nmt pretrained model,results has nmt pretrained model,0.5190770626068115
translation,86,158,results,pnat,achieves,best overall performance,pnat achieves best overall performance,0.7144791483879089
translation,86,158,results,best overall performance,on,xtreme benchmark,best overall performance on xtreme benchmark,0.524528443813324
translation,86,158,results,best overall performance,with,substantial gains,best overall performance with substantial gains,0.6231640577316284
translation,86,158,results,substantial gains,over,models,substantial gains over models,0.7820459604263306
translation,86,158,results,models,trained on,monolingual data only,models trained on monolingual data only,0.7740543484687805
translation,86,160,results,translation data,to,text - to - text pre-training,translation data to text - to - text pre-training,0.5457952618598938
translation,86,160,results,performance,on,end tasks,performance on end tasks,0.5581551790237427
translation,86,160,results,end tasks,of,cross-lingual understanding,end tasks of cross-lingual understanding,0.5516234636306763
translation,86,160,results,improve,has,performance,improve has performance,0.5578044652938843
translation,86,160,results,results,demonstrates,translation data,results demonstrates translation data,0.6318418383598328
translation,86,160,results,results,introducing,translation data,results introducing translation data,0.6298835873603821
translation,86,161,results,pnat,provides,consistent gains,pnat provides consistent gains,0.6925980448722839
translation,86,161,results,consistent gains,over,sc and sc + tsc,consistent gains over sc and sc + tsc,0.7264427542686462
translation,86,161,results,results,has,pnat,results has pnat,0.5536153316497803
translation,86,162,results,sc + pnat,obtains,comparable results,sc + pnat obtains comparable results,0.622162401676178
translation,86,162,results,comparable results,to,sc + mt,comparable results to sc + mt,0.5696417689323425
translation,86,162,results,sc + mt,without,parallel data,sc + mt without parallel data,0.7302740812301636
translation,86,162,results,results,has,sc + pnat,results has sc + pnat,0.5430055856704712
translation,86,163,results,sc + tsc,brings,noticeable improvements,sc + tsc brings noticeable improvements,0.652026355266571
translation,86,163,results,noticeable improvements,on,question answering tasks,noticeable improvements on question answering tasks,0.48368003964424133
translation,86,164,results,sc +mt,shows,competitive results,sc +mt shows competitive results,0.7162038087844849
translation,86,164,results,competitive results,on,xnli,competitive results on xnli,0.5487611293792725
translation,86,164,results,results,on,other tasks,results on other tasks,0.5096456408500671
translation,86,164,results,other tasks,are,relatively low,other tasks are relatively low,0.5331282019615173
translation,86,164,results,results,on,other tasks,results on other tasks,0.5096456408500671
translation,86,164,results,results,has,sc +mt,results has sc +mt,0.5349162817001343
translation,86,177,results,mt5,on,all the three target languages,mt5 on all the three target languages,0.542585015296936
translation,86,177,results,mt6,has,consistently outperforms,mt6 has consistently outperforms,0.6337208151817322
translation,86,177,results,consistently outperforms,has,mt5,consistently outperforms has mt5,0.6293134093284607
translation,86,177,results,results,observe,mt6,results observe mt6,0.6161753535270691
translation,86,178,results,"xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models",with,800m parameters,"xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models with 800m parameters",0.5876389145851135
translation,86,178,results,our mt6 model,achieves,similar performance,our mt6 model achieves similar performance,0.7024112939834595
translation,86,178,results,similar performance,with,only 300m parameters,similar performance with only 300m parameters,0.6899648308753967
translation,86,178,results,"xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models",has,our mt6 model,"xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models has our mt6 model",0.5694955587387085
translation,86,178,results,results,Comparing with,"xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models","results Comparing with xlm ( conneau and lample , 2019 ) and xnlg ( chi et al. , 2020 ) models",0.6069787740707397
translation,86,179,results,mt6,shows,more improvements,mt6 shows more improvements,0.7343661785125732
translation,86,179,results,more improvements,over,mt5,more improvements over mt5,0.7077822089195251
translation,86,179,results,fewer training data,has,mt6,fewer training data has mt6,0.6258888244628906
translation,86,194,results,mt6,consistently reduces,transfer gap,mt6 consistently reduces transfer gap,0.7871190309524536
translation,86,194,results,transfer gap,across,all the five tasks,transfer gap across all the five tasks,0.6702443361282349
translation,86,194,results,results,has,mt6,results has mt6,0.5808969736099243
translation,86,211,results,mt6,achieves,lower aer scores,mt6 achieves lower aer scores,0.6771281361579895
translation,86,211,results,lower aer scores,than,mt5,lower aer scores than mt5,0.6190721988677979
translation,86,211,results,three language pairs,has,mt6,three language pairs has mt6,0.6310526728630066
translation,86,211,results,results,Among,three language pairs,results Among three language pairs,0.5417184829711914
translation,86,220,results,mt6,achieves,best results,mt6 achieves best results,0.7146538496017456
translation,86,220,results,best results,with,noise density,best results with noise density,0.6548851728439331
translation,86,220,results,noise density,of,0.5,noise density of 0.5,0.6496789455413818
translation,86,220,results,results,observe,mt6,results observe mt6,0.6161753535270691
translation,86,221,results,tsc task,prefers,higher noise density,tsc task prefers higher noise density,0.752994954586029
translation,86,221,results,results,indicate,tsc task,results indicate tsc task,0.46109092235565186
translation,87,20,baselines,shared vocabulary,used for,better performance,shared vocabulary used for better performance,0.6722139120101929
translation,87,20,baselines,baselines,has,shared vocabulary,baselines has shared vocabulary,0.569073498249054
translation,87,86,results,my best submitted systems,achieve,35.9 and 32.2 bleu,my best submitted systems achieve 35.9 and 32.2 bleu,0.5782914161682129
translation,87,86,results,35.9 and 32.2 bleu,for,english to chinese and chinese to english directions,35.9 and 32.2 bleu for english to chinese and chinese to english directions,0.6223892569541931
translation,87,86,results,wmt 2021 test set,has,my best submitted systems,wmt 2021 test set has my best submitted systems,0.5842782855033875
translation,87,86,results,results,On,wmt 2021 test set,results On wmt 2021 test set,0.5232433080673218
translation,88,112,ablation-analysis,iwslt and europarl,removing,residual connections,iwslt and europarl removing residual connections,0.7218632698059082
translation,88,112,ablation-analysis,zero-shot translation,while retaining,performance,zero-shot translation while retaining performance,0.6086792945861816
translation,88,112,ablation-analysis,performance,on,supervised directions,performance on supervised directions,0.5492165088653564
translation,88,112,ablation-analysis,substantially improves,has,zero-shot translation,substantially improves has zero-shot translation,0.5418304204940796
translation,88,112,ablation-analysis,ablation analysis,On,iwslt and europarl,ablation analysis On iwslt and europarl,0.5662939548492432
translation,88,119,ablation-analysis,first observation,modification in,residual connections,first observation modification in residual connections,0.6852661967277527
translation,88,119,ablation-analysis,residual connections,essential for,zero-shot performance,residual connections essential for zero-shot performance,0.698483407497406
translation,88,119,ablation-analysis,ablation analysis,has,first observation,ablation analysis has first observation,0.5556557774543762
translation,88,126,ablation-analysis,score,of,2.3,score of 2.3,0.5880022644996643
translation,88,126,ablation-analysis,2.3,indicates,outputs are still far from being useful,2.3 indicates outputs are still far from being useful,0.6295444965362549
translation,88,126,ablation-analysis,pmindia,has,score,pmindia has score,0.5995561480522156
translation,88,126,ablation-analysis,improve,has,zeroshot performance,improve has zeroshot performance,0.5622016191482544
translation,88,126,ablation-analysis,zeroshot performance,has,score,zeroshot performance has score,0.5787566900253296
translation,88,126,ablation-analysis,ablation analysis,challenging case of,pmindia,ablation analysis challenging case of pmindia,0.7463570833206177
translation,88,126,ablation-analysis,ablation analysis,removing,residual,ablation analysis removing residual,0.7760905027389526
translation,88,128,ablation-analysis,not effective,when combined with,residual removal,not effective when combined with residual removal,0.718413770198822
translation,88,137,ablation-analysis,loses,from,11.3,loses from 11.3,0.5715376138687134
translation,88,137,ablation-analysis,11.3,has,to 8.2 points,11.3 has to 8.2 points,0.5459277033805847
translation,88,137,ablation-analysis,ablation analysis,has,baseline,ablation analysis has baseline,0.5216511487960815
translation,88,139,ablation-analysis,effect of additional regularization,hypothesized,variational dropout,effect of additional regularization hypothesized variational dropout,0.5748154520988464
translation,88,139,ablation-analysis,variational dropout,helps reduce,position -specific representation,variational dropout helps reduce position -specific representation,0.6478458046913147
translation,88,139,ablation-analysis,ablation analysis,hypothesized,variational dropout,ablation analysis hypothesized variational dropout,0.5953511595726013
translation,88,139,ablation-analysis,ablation analysis,has,effect of additional regularization,ablation analysis has effect of additional regularization,0.5460063815116882
translation,88,143,ablation-analysis,pmindia,combining,our model,pmindia combining our model,0.7215660810470581
translation,88,143,ablation-analysis,pmindia,achieving,reasonable zero-shot performance,pmindia achieving reasonable zero-shot performance,0.6936479210853577
translation,88,143,ablation-analysis,dropout,achieving,reasonable zero-shot performance,dropout achieving reasonable zero-shot performance,0.7021037936210632
translation,88,143,ablation-analysis,increase,has,from 2.4 to 14.3 points,increase has from 2.4 to 14.3 points,0.554720401763916
translation,88,143,ablation-analysis,ablation analysis,On,pmindia,ablation analysis On pmindia,0.6102709770202637
translation,88,179,ablation-analysis,drops sharply,at,third layer,drops sharply at third layer,0.5833154320716858
translation,88,179,ablation-analysis,third layer,where,residual connection,third layer where residual connection,0.6448314785957336
translation,88,179,ablation-analysis,accuracy,has,drops sharply,accuracy has drops sharply,0.6107286214828491
translation,88,195,ablation-analysis,model outputs,are,much more similar,model outputs are much more similar,0.6096932291984558
translation,88,195,ablation-analysis,much more similar,after,transition layer,much more similar after transition layer,0.6809822916984558
translation,88,195,ablation-analysis,much more similar,shown by,sharp increase,much more similar shown by sharp increase,0.6925033330917358
translation,88,195,ablation-analysis,sharp increase,at,layer 3,sharp increase at layer 3,0.6031707525253296
translation,88,217,ablation-analysis,residual connections,in,first encoder layer,residual connections in first encoder layer,0.5054839253425598
translation,88,217,ablation-analysis,residual connections,degrades,zero-shot performance,residual connections degrades zero-shot performance,0.7600571513175964
translation,88,217,ablation-analysis,zero-shot performance,by,2.8 bleu,zero-shot performance by 2.8 bleu,0.5252468585968018
translation,88,217,ablation-analysis,2.8 bleu,on average,iwslt,2.8 bleu on average iwslt,0.658854603767395
translation,88,217,ablation-analysis,2.8 bleu,on,iwslt,2.8 bleu on iwslt,0.5555058121681213
translation,88,217,ablation-analysis,ablation analysis,Removing,residual connections,ablation analysis Removing residual connections,0.7478570342063904
translation,88,218,ablation-analysis,residual connections,in,top encoder layers,residual connections in top encoder layers,0.46620839834213257
translation,88,218,ablation-analysis,residual connections,slows down,convergence,residual connections slows down convergence,0.7481850385665894
translation,88,218,ablation-analysis,top encoder layers,has,fourth or fifth layer,top encoder layers has fourth or fifth layer,0.5533799529075623
translation,88,218,ablation-analysis,ablation analysis,leaving out,residual connections,ablation analysis leaving out residual connections,0.7108713984489441
translation,88,225,ablation-analysis,residual connection,in,middle layer,residual connection in middle layer,0.5202087163925171
translation,88,225,ablation-analysis,residual connection,is,effective,residual connection is effective,0.5722793936729431
translation,88,225,ablation-analysis,supervised,has,29.1,supervised has 29.1,0.5488824844360352
translation,88,225,ablation-analysis,zero-shot,has,17.1,zero-shot has 17.1,0.5693920850753784
translation,88,225,ablation-analysis,ablation analysis,removing,residual connection,ablation analysis removing residual connection,0.7654191255569458
translation,88,90,experimental-setup,classification task,is,lightweight,classification task is lightweight,0.5712869167327881
translation,88,90,experimental-setup,convergence,is,fast,convergence is fast,0.6018749475479126
translation,88,90,experimental-setup,fast,reduce,warmup steps,fast reduce warmup steps,0.7073317766189575
translation,88,90,experimental-setup,warmup steps,to,400,warmup steps to 400,0.6212415099143982
translation,88,90,experimental-setup,warmup steps,while keeping,learning rate schedule,warmup steps while keeping learning rate schedule,0.6671420931816101
translation,88,90,experimental-setup,learning rate schedule,has,unchanged,learning rate schedule has unchanged,0.5808391571044922
translation,88,90,experimental-setup,experimental setup,reduce,warmup steps,experimental setup reduce warmup steps,0.6481090784072876
translation,88,103,experimental-setup,indian languages,use,indicnlp library,indian languages use indicnlp library,0.6226248145103455
translation,88,103,experimental-setup,indian languages,use,"sentencepiece ( kudo and richardson , 2018 )","indian languages use sentencepiece ( kudo and richardson , 2018 )",0.5881544351577759
translation,88,103,experimental-setup,"sentencepiece ( kudo and richardson , 2018 )",for,tokenization and bpe,"sentencepiece ( kudo and richardson , 2018 ) for tokenization and bpe",0.6068534851074219
translation,88,103,experimental-setup,experimental setup,For,indian languages,experimental setup For indian languages,0.5712319612503052
translation,88,104,experimental-setup,40 k merge operations,use,tokens,40 k merge operations use tokens,0.6477781534194946
translation,88,104,experimental-setup,tokens,with,minimum frequency,tokens with minimum frequency,0.6500731110572815
translation,88,104,experimental-setup,minimum frequency,of,50,minimum frequency of 50,0.6702456474304199
translation,88,104,experimental-setup,50,in,training set,50 in training set,0.546126663684845
translation,88,104,experimental-setup,experimental setup,choose,40 k merge operations,experimental setup choose 40 k merge operations,0.6705156564712524
translation,88,102,experiments,languages,with,latin script,languages with latin script,0.5466778874397278
translation,88,102,experiments,languages,apply,moses tokenizer and truecaser,languages apply moses tokenizer and truecaser,0.5927498936653137
translation,88,102,experiments,languages,learn,byte pair encoding ( bpe ),languages learn byte pair encoding ( bpe ),0.6411191821098328
translation,88,102,experiments,byte pair encoding ( bpe ),using,subword - nmt,byte pair encoding ( bpe ) using subword - nmt,0.6908520460128784
translation,88,36,model,easy integration,of,new languages,easy integration of new languages,0.5719891786575317
translation,88,36,model,easy integration,enables,zero-shot translation,easy integration enables zero-shot translation,0.7156693339347839
translation,88,36,model,zero-shot translation,between,new language,zero-shot translation between new language,0.619098424911499
translation,88,36,model,zero-shot translation,between,all other languages previously trained on,zero-shot translation between all other languages previously trained on,0.6191125512123108
translation,88,89,model,classifier,is,linear projection,classifier is linear projection,0.5729526281356812
translation,88,89,model,linear projection,from,encoder hidden dimension,linear projection from encoder hidden dimension,0.535890519618988
translation,88,89,model,linear projection,followed by,softmax activation,linear projection followed by softmax activation,0.661870539188385
translation,88,89,model,encoder hidden dimension,to,number of classes,encoder hidden dimension to number of classes,0.5041639804840088
translation,88,89,model,model,has,classifier,model has classifier,0.6052614450454712
translation,88,92,model,self-attention layer,in,middle encoder layer,self-attention layer in middle encoder layer,0.5018895268440247
translation,88,92,model,model,modify,residual connections,model modify residual connections,0.6967032551765442
translation,88,113,results,our approach,improved further,additional regularization,our approach improved further additional regularization,0.7813845276832581
translation,88,113,results,pmindia,has,our approach,pmindia has our approach,0.633297860622406
translation,88,113,results,results,On,pmindia,results On pmindia,0.5631994009017944
translation,88,114,results,pivoting,via,english,pivoting via english,0.7138230204582214
translation,88,114,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,88,114,results,outperforms,has,pivoting,outperforms has pivoting,0.606286346912384
translation,88,114,results,results,has,our approach,results has our approach,0.6050099730491638
translation,88,118,results,our approach,has,substantially improves,our approach has substantially improves,0.5827473998069763
translation,88,118,results,substantially improves,has,zero-shot translation quality,substantially improves has zero-shot translation quality,0.5255774259567261
translation,88,118,results,results,has,our approach,results has our approach,0.6050099730491638
translation,88,120,results,6.9 and up to 18.5 bleu points,over,baseline,6.9 and up to 18.5 bleu points over baseline,0.5905889868736267
translation,88,120,results,baseline,on,iwslt and europarl,baseline on iwslt and europarl,0.5516648292541504
translation,88,120,results,results,gain,6.9 and up to 18.5 bleu points,results gain 6.9 and up to 18.5 bleu points,0.6604639291763306
translation,88,122,results,our proposed models,consistent in generating,required target languages,our proposed models consistent in generating required target languages,0.7463910579681396
translation,88,122,results,our proposed models,show,competitive performance,our proposed models show competitive performance,0.6690486073493958
translation,88,122,results,required target languages,in,zero-shot conditions,required target languages in zero-shot conditions,0.5495926737785339
translation,88,122,results,competitive performance,to,pivoting,competitive performance to pivoting,0.5863190293312073
translation,88,122,results,pivoting,via,english,pivoting via english,0.7138230204582214
translation,88,122,results,results,has,our proposed models,results has our proposed models,0.5751316547393799
translation,88,124,results,pivoting,when,translating,pivoting when translating,0.7078098654747009
translation,88,124,results,translating,between,languages,translating between languages,0.635346531867981
translation,88,124,results,languages,from,same families,languages from same families,0.5639427900314331
translation,88,124,results,europarl,has,zero-shot,europarl has zero-shot,0.6344879865646362
translation,88,124,results,zero-shot,has,outperforms,zero-shot has outperforms,0.6327565908432007
translation,88,124,results,outperforms,has,pivoting,outperforms has pivoting,0.606286346912384
translation,88,124,results,results,on,europarl,results on europarl,0.5534603595733643
translation,88,135,results,obverse,improved,supervised translation performance,obverse improved supervised translation performance,0.68031907081604
translation,88,135,results,supervised translation performance,by around 1.5 points,all three model configurations,supervised translation performance by around 1.5 points all three model configurations,0.6400173902511597
translation,88,135,results,results,With,non-overlapping data,results With non-overlapping data,0.5972055196762085
translation,88,136,results,increases,from,26.1,increases from 26.1,0.5917074680328369
translation,88,136,results,to 26.7 points,with,our model ( residual ),to 26.7 points with our model ( residual ),0.6331392526626587
translation,88,136,results,26.1,has,to 26.7 points,26.1 has to 26.7 points,0.548456072807312
translation,88,136,results,results,has,zero-shot score,results has zero-shot score,0.5694122910499573
translation,88,141,results,vari-ational dropout,improves,zero-shot performance,vari-ational dropout improves zero-shot performance,0.6702892780303955
translation,88,141,results,zero-shot performance,over,baseline,zero-shot performance over baseline,0.6881088614463806
translation,88,141,results,zero-shot performance,not as strongly as,residual removal,zero-shot performance not as strongly as residual removal,0.6513721346855164
translation,88,141,results,results,has,vari-ational dropout,results has vari-ational dropout,0.46007853746414185
translation,88,142,results,no additive gain,by combining,both techniques,no additive gain by combining both techniques,0.7093626260757446
translation,88,142,results,results,On,iwslt and europarl,results On iwslt and europarl,0.5182715058326721
translation,88,189,results,our model,consistently achieves,higher svcca scores,our model consistently achieves higher svcca scores,0.7526949048042297
translation,88,189,results,our model,consistently achieves,lower classification accuracy,our model consistently achieves lower classification accuracy,0.7591279149055481
translation,88,189,results,lower classification accuracy,than,baseline,lower classification accuracy than baseline,0.5575196743011475
translation,88,189,results,results,has,our model,results has our model,0.5871725678443909
translation,88,219,results,number of training epochs,comes with,loss,number of training epochs comes with loss,0.6006660461425781
translation,88,219,results,loss,of,0.4 bleu,loss of 0.4 bleu,0.5674915909767151
translation,88,219,results,0.4 bleu,on,supervised directions,0.4 bleu on supervised directions,0.4989345669746399
translation,88,219,results,results,keeping,number of training epochs,results keeping number of training epochs,0.6413679718971252
translation,88,224,results,baseline,suffers from,off-target zero-shot translation,baseline suffers from off-target zero-shot translation,0.714881181716919
translation,88,224,results,average bleu scores,on,supervised directions,average bleu scores on supervised directions,0.5050896406173706
translation,88,224,results,off-target zero-shot translation,has,average bleu scores,off-target zero-shot translation has average bleu scores,0.5304048657417297
translation,88,224,results,zeroshot,has,4.8,zeroshot has 4.8,0.5931608080863953
translation,88,224,results,results,has,baseline,results has baseline,0.5317873954772949
translation,89,104,ablation-analysis,both of the two features,contribute,significantly,both of the two features contribute significantly,0.6382443308830261
translation,89,104,ablation-analysis,significantly,to,excellent performance,significantly to excellent performance,0.5543686747550964
translation,89,104,ablation-analysis,excellent performance,of,our model,excellent performance of our model,0.5769716501235962
translation,89,104,ablation-analysis,distance feature,is,more important,distance feature is more important,0.5544303059577942
translation,89,104,ablation-analysis,ablation analysis,obvious that,both of the two features,ablation analysis obvious that both of the two features,0.18354132771492004
translation,89,69,experiments,sacre-bleu,to measure,all results,sacre-bleu to measure all results,0.7103874087333679
translation,89,69,experiments,all results,with,case-sensitive detokenized bleu,all results with case-sensitive detokenized bleu,0.6258422136306763
translation,89,69,experiments,dev set,to train,meta-k network,dev set to train meta-k network,0.7368383407592773
translation,89,69,experiments,meta-k network,for,5 k steps,meta-k network for 5 k steps,0.6617025136947632
translation,89,70,hyperparameters,"adam ( kingma and ba , 2015 )",to optimize,our model,"adam ( kingma and ba , 2015 ) to optimize our model",0.6800381541252136
translation,89,70,hyperparameters,learning rate,set to,3e - 4,learning rate set to 3e - 4,0.725951611995697
translation,89,70,hyperparameters,batch size,set to,32 sentences,batch size set to 32 sentences,0.6700612902641296
translation,89,70,hyperparameters,our model,has,learning rate,our model has learning rate,0.5267466306686401
translation,89,6,model,adaptive knn - mt,to dynamically determine,number of k,adaptive knn - mt to dynamically determine number of k,0.7421385049819946
translation,89,6,model,number of k,for,each target token,number of k for each target token,0.6480019688606262
translation,89,6,model,model,propose,adaptive knn - mt,model propose adaptive knn - mt,0.6635887622833252
translation,89,7,model,light- weight meta-k network,can be,efficiently trained,light- weight meta-k network can be efficiently trained,0.6484227180480957
translation,89,7,model,model,introducing,light- weight meta-k network,model introducing light- weight meta-k network,0.7006280422210693
translation,89,19,model,adaptive knn - mt,determines,choice of k,adaptive knn - mt determines choice of k,0.6830092668533325
translation,89,19,model,choice of k,regarding,each target token,choice of k regarding each target token,0.6096169948577881
translation,89,19,model,each target token,has,adaptively,each target token has adaptively,0.6312246322631836
translation,89,19,model,model,propose,adaptive knn - mt,model propose adaptive knn - mt,0.6635887622833252
translation,89,20,model,set of possible k,smaller than,upper bound k,set of possible k smaller than upper bound k,0.7476086616516113
translation,89,21,model,retrieval results,of,current target token,retrieval results of current target token,0.5523263216018677
translation,89,21,model,retrieval results,propose,light- weight meta-k network,retrieval results propose light- weight meta-k network,0.6308481693267822
translation,89,21,model,light- weight meta-k network,to estimate,importance,light- weight meta-k network to estimate importance,0.7229715585708618
translation,89,21,model,importance,of,all possible k-nearest neighbor results,importance of all possible k-nearest neighbor results,0.6169499158859253
translation,89,21,model,importance,based on,aggregated,importance based on aggregated,0.6682837009429932
translation,89,21,model,aggregated,to obtain,final decision,aggregated to obtain final decision,0.6340129375457764
translation,89,21,model,final decision,of,model,final decision of model,0.5914625525474548
translation,89,21,model,model,given,retrieval results,model given retrieval results,0.7118563055992126
translation,89,107,model,adaptive knn - mt model,to dynamically determine,utilization,adaptive knn - mt model to dynamically determine utilization,0.7146115899085999
translation,89,107,model,utilization,of,retrieved neighbors,utilization of retrieved neighbors,0.612087607383728
translation,89,107,model,retrieved neighbors,for,each target token,retrieved neighbors for each target token,0.5895312428474426
translation,89,107,model,retrieved neighbors,by introducing,light- weight meta-k network,retrieved neighbors by introducing light- weight meta-k network,0.6721317172050476
translation,89,107,model,model,propose,adaptive knn - mt model,model propose adaptive knn - mt model,0.63808673620224
translation,89,24,results,our approach,achieve,1.44?2.97 bleu score improvements,our approach achieve 1.44?2.97 bleu score improvements,0.5513911247253418
translation,89,24,results,1.44?2.97 bleu score improvements,over,vanilla knn - mt,1.44?2.97 bleu score improvements over vanilla knn - mt,0.5920103192329407
translation,89,24,results,vanilla knn - mt,on average,k ? 4,vanilla knn - mt on average k ? 4,0.7121402025222778
translation,89,24,results,vanilla knn - mt,when,k ? 4,vanilla knn - mt when k ? 4,0.6600928902626038
translation,89,24,results,four domains,has,our approach,four domains has our approach,0.5914753675460815
translation,89,24,results,results,Across,four domains,results Across four domains,0.6057833433151245
translation,89,74,results,significantly outperforms,illustrating,benefits,significantly outperforms illustrating benefits,0.7210195660591125
translation,89,74,results,vanilla knn - mt,on,all domains,vanilla knn - mt on all domains,0.5405408143997192
translation,89,74,results,proposed adaptive knn - mt,has,significantly outperforms,proposed adaptive knn - mt has significantly outperforms,0.594505786895752
translation,89,74,results,significantly outperforms,has,vanilla knn - mt,significantly outperforms has vanilla knn - mt,0.6007412672042847
translation,89,74,results,results,observe,proposed adaptive knn - mt,results observe proposed adaptive knn - mt,0.6210475564002991
translation,89,75,results,performance,of,vanilla model,performance of vanilla model,0.6015594005584717
translation,89,75,results,vanilla model,sensitive to,choice of k,vanilla model sensitive to choice of k,0.7345559597015381
translation,89,75,results,our proposed model,is,more robust,our proposed model is more robust,0.5901052355766296
translation,89,75,results,more robust,with,smaller variance,more robust with smaller variance,0.6380019187927246
translation,89,76,results,our model,achieves,better results,our model achieves better results,0.6506028175354004
translation,89,76,results,better results,when choosing,larger number of neighbors,better results when choosing larger number of neighbors,0.6390578150749207
translation,89,76,results,vanilla model,suffers from,performance degradation,vanilla model suffers from performance degradation,0.739816427230835
translation,89,76,results,performance degradation,when,k = 32,performance degradation when k = 32,0.6837234497070312
translation,89,76,results,results,has,our model,results has our model,0.5871725678443909
translation,89,83,results,meta-k network,trained on,it domain,meta-k network trained on it domain,0.7239140868186951
translation,89,83,results,meta-k network,achieves,comparable performance,meta-k network achieves comparable performance,0.6757509112358093
translation,89,83,results,comparable performance,on,all other domains,comparable performance on all other domains,0.5088497996330261
translation,89,83,results,meta-k network,with,in- domain dataset,meta-k network with in- domain dataset,0.6271650195121765
translation,89,105,results,our model,could outperforms,vanilla knn - mt,our model could outperforms vanilla knn - mt,0.7056868672370911
translation,89,105,results,our model,with,hidden size,our model with hidden size,0.6499472856521606
translation,89,105,results,vanilla knn - mt,with,only 100 training sentences,vanilla knn - mt with only 100 training sentences,0.5698800086975098
translation,89,105,results,vanilla knn - mt,with,hidden size,vanilla knn - mt with hidden size,0.6593986749649048
translation,89,105,results,hidden size,of,8,hidden size of 8,0.7005906105041504
translation,89,105,results,results,has,our model,results has our model,0.5871725678443909
translation,90,56,baselines,recurrent neural networks ( rnn ),with,crossattention models,recurrent neural networks ( rnn ) with crossattention models,0.6292734742164612
translation,90,56,baselines,recurrent neural networks ( rnn ),Supports,distributed training,recurrent neural networks ( rnn ) Supports distributed training,0.6711609959602356
translation,90,56,baselines,crossattention models,Supports,distributed training,crossattention models Supports distributed training,0.637016773223877
translation,90,56,baselines,distributed training,on,multi-node multi-gpus,distributed training on multi-node multi-gpus,0.5670148134231567
translation,90,56,baselines,distributed training,on,gradient accumulation,distributed training on gradient accumulation,0.5548057556152344
translation,90,56,baselines,distributed training,on,float16 operations,distributed training on float16 operations,0.5629079341888428
translation,90,58,baselines,beam decoding,with,length normalization,beam decoding with length normalization,0.6341477036476135
translation,90,58,baselines,beam decoding,with,checkpoint averaging,beam decoding with checkpoint averaging,0.6512055397033691
translation,90,58,baselines,length normalization,has,"wu et al. , 2016","length normalization has wu et al. , 2016",0.4924986660480499
translation,90,80,experimental-setup,best performing model,trained with,effective batch size,best performing model trained with effective batch size,0.7371366024017334
translation,90,80,experimental-setup,effective batch size,of about,"720,000 tokens","effective batch size of about 720,000 tokens",0.6134940981864929
translation,90,80,experimental-setup,effective batch size,per,optimizer step,effective batch size per optimizer step,0.6237870454788208
translation,90,80,experimental-setup,experimental setup,has,best performing model,experimental setup has best performing model,0.5432727336883545
translation,90,81,experimental-setup,big batches,achieved by,mixed - precision distributed training,big batches achieved by mixed - precision distributed training,0.6607933640480042
translation,90,81,experimental-setup,mixed - precision distributed training,on,8 nvidia a100 gpus,mixed - precision distributed training on 8 nvidia a100 gpus,0.5028389096260071
translation,90,81,experimental-setup,8 nvidia a100 gpus,with,gradient accumulation,8 nvidia a100 gpus with gradient accumulation,0.6021519899368286
translation,90,81,experimental-setup,gradient accumulation,of,5 minibatches,gradient accumulation of 5 minibatches,0.5799694657325745
translation,90,81,experimental-setup,5 minibatches,having,"maximum of 18,000 tokens","5 minibatches having maximum of 18,000 tokens",0.6359730958938599
translation,90,81,experimental-setup,experimental setup,has,big batches,experimental setup has big batches,0.5742996335029602
translation,90,82,experimental-setup,adam optimizer,with,8000 warm - up steps,adam optimizer with 8000 warm - up steps,0.6069247722625732
translation,90,82,experimental-setup,8000 warm - up steps,followed by,decaying learning rate,8000 warm - up steps followed by decaying learning rate,0.6926671266555786
translation,90,82,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,90,21,model,mtdata,helps to easily obtain,parallel datasets,mtdata helps to easily obtain parallel datasets,0.6713914275169373
translation,90,21,model,vocabulary manager and storage layer,for transforming,sentences,vocabulary manager and storage layer for transforming sentences,0.7393348813056946
translation,90,21,model,sentences,to,integer sequences,sentences to integer sequences,0.5460302829742432
translation,90,21,model,feature - rich pytorch - backed nmt toolkit,supports,reproducible experiments,feature - rich pytorch - backed nmt toolkit supports reproducible experiments,0.6966381669044495
translation,90,21,model,nl - codec,has,vocabulary manager and storage layer,nl - codec has vocabulary manager and storage layer,0.5681509971618652
translation,90,21,model,rtg,has,feature - rich pytorch - backed nmt toolkit,rtg has feature - rich pytorch - backed nmt toolkit,0.5411294102668762
translation,90,57,model,integrated tensorboard,visualizing,training and validation losses,integrated tensorboard visualizing training and validation losses,0.6927940249443054
translation,90,57,model,model,has,integrated tensorboard,model has integrated tensorboard,0.6115918159484863
translation,90,22,results,massive bitext dataset,with,more than 9 billion tokens per side,massive bitext dataset with more than 9 billion tokens per side,0.6144158840179443
translation,90,22,results,single multilingual nmt model,capable of,translating,single multilingual nmt model capable of translating,0.6826208829879761
translation,90,22,results,500 source languages,to,english,500 source languages to english,0.5289768576622009
translation,90,22,results,translating,has,500 source languages,translating has 500 source languages,0.5659458637237549
translation,90,22,results,results,training,single multilingual nmt model,results training single multilingual nmt model,0.6551586389541626
translation,91,75,ablation-analysis,modification rate,is,40 %,modification rate is 40 %,0.5754517316818237
translation,91,75,ablation-analysis,ablation analysis,find that,modification rate,ablation analysis find that modification rate,0.5820926427841187
translation,91,180,ablation-analysis,cast,help denoise,concatenated context,cast help denoise concatenated context,0.7302545309066772
translation,91,180,ablation-analysis,concatenated context,for,h2h + cxt,concatenated context for h2h + cxt,0.6196631789207458
translation,91,180,ablation-analysis,ablation analysis,noticeable,cast,ablation analysis noticeable cast,0.6526824235916138
translation,91,143,baselines,two state - of- the - art nmt models,including,m2m -100,two state - of- the - art nmt models including m2m -100,0.6564445495605469
translation,91,143,baselines,two state - of- the - art nmt models,including,"mbart-50m2 m ( tang et al. , 2020 )","two state - of- the - art nmt models including mbart-50m2 m ( tang et al. , 2020 )",0.6398259401321411
translation,91,143,baselines,baselines,choose,two state - of- the - art nmt models,baselines choose two state - of- the - art nmt models,0.6684669256210327
translation,91,144,baselines,transformer sequence - to-sequence architecture,to capture,features,transformer sequence - to-sequence architecture to capture features,0.6997299194335938
translation,91,144,baselines,transformer sequence - to-sequence architecture,generate,translation,transformer sequence - to-sequence architecture generate translation,0.6713098883628845
translation,91,144,baselines,features,from,source language input,features from source language input,0.5379583239555359
translation,91,149,baselines,fine-tuned,by concatenating,target header and its context,fine-tuned by concatenating target header and its context,0.6569650769233704
translation,91,149,baselines,target header and its context,as,input,target header and its context as input,0.508465051651001
translation,91,149,baselines,h2h+cxt,has,nmt,h2h+cxt has nmt,0.6575505137443542
translation,91,149,baselines,baselines,has,h2h+cxt,baselines has h2h+cxt,0.5581964254379272
translation,91,150,baselines,nmt models,with,two extra transformers layers,nmt models with two extra transformers layers,0.67503422498703
translation,91,150,baselines,two extra transformers layers,at the end of,encoder,two extra transformers layers at the end of encoder,0.7480307817459106
translation,91,150,baselines,h2h+cxt + extl,has,nmt models,h2h+cxt + extl has nmt models,0.6354418396949768
translation,91,150,baselines,baselines,has,h2h+cxt + extl,baselines has h2h+cxt + extl,0.5478701591491699
translation,91,155,experimental-setup,sacrebleu tokenizer,for,chinese,sacrebleu tokenizer for chinese,0.5701097846031189
translation,91,155,experimental-setup,kytea,for,japanese,kytea for japanese,0.6972242593765259
translation,91,155,experimental-setup,moses tokenizer,for,rest of the languages,moses tokenizer for rest of the languages,0.6023920774459839
translation,91,155,experimental-setup,experimental setup,use,sacrebleu tokenizer,experimental setup use sacrebleu tokenizer,0.6024990081787109
translation,91,155,experimental-setup,experimental setup,use,kytea,experimental setup use kytea,0.6125651597976685
translation,91,155,experimental-setup,experimental setup,use,moses tokenizer,experimental setup use moses tokenizer,0.6008074879646301
translation,91,7,experiments,first parallel dataset,for,schema translation,first parallel dataset for schema translation,0.6017231345176697
translation,91,7,experiments,"3,158 tables",with,"11,979 headers","3,158 tables with 11,979 headers",0.6088276505470276
translation,91,7,experiments,"11,979 headers",written in,6 different languages,"11,979 headers written in 6 different languages",0.6961789131164551
translation,91,7,experiments,6 different languages,including,"chinese , french , german , spanish , and japanese","6 different languages including chinese , french , german , spanish , and japanese",0.6367689371109009
translation,91,145,experiments,m2m -100,directly trained on,large-scaled translation data,m2m -100 directly trained on large-scaled translation data,0.7586134076118469
translation,91,145,experiments,mbart -50 m2 m,firstly pre-trained with,multilingual denoising pretraining   objective,mbart -50 m2 m firstly pre-trained with multilingual denoising pretraining   objective,0.7809233665466309
translation,91,145,experiments,mbart -50 m2 m,fine-tuned in,machine -translation task,mbart -50 m2 m fine-tuned in machine -translation task,0.6567564606666565
translation,91,151,experiments,phrasebased statistical machine translation ( pb - smt ) schema translation model,with,moses,phrasebased statistical machine translation ( pb - smt ) schema translation model with moses,0.637244462966919
translation,91,151,experiments,moses,has,"koehn et al. , 2007 )","moses has koehn et al. , 2007 )",0.5827906727790833
translation,91,158,hyperparameters,all of our nmt models,for,4 epochs,all of our nmt models for 4 epochs,0.5959556698799133
translation,91,158,hyperparameters,all of our nmt models,with,batch size,all of our nmt models with batch size,0.6172536611557007
translation,91,158,hyperparameters,all of our nmt models,with,warmup rate,all of our nmt models with warmup rate,0.625110387802124
translation,91,158,hyperparameters,batch size,of,4,batch size of 4,0.6922571659088135
translation,91,158,hyperparameters,warmup rate,of,0.2,warmup rate of 0.2,0.6263343095779419
translation,91,158,hyperparameters,hyperparameters,fine- tune,all of our nmt models,hyperparameters fine- tune all of our nmt models,0.6880389451980591
translation,91,159,hyperparameters,over-fitting,set,early stopping patience,over-fitting set early stopping patience,0.6420483589172363
translation,91,159,hyperparameters,early stopping patience,on,validation set,early stopping patience on validation set,0.569423496723175
translation,91,159,hyperparameters,early stopping patience,as,2,early stopping patience as 2,0.5713545680046082
translation,91,159,hyperparameters,validation set,as,2,validation set as 2,0.5664315223693848
translation,91,159,hyperparameters,hyperparameters,To avoid,over-fitting,hyperparameters To avoid over-fitting,0.6307908296585083
translation,91,160,hyperparameters,context construction,randomly select,5 cell values,context construction randomly select 5 cell values,0.661334216594696
translation,91,160,hyperparameters,5 cell values,for,each target column,5 cell values for each target column,0.6315663456916809
translation,91,160,hyperparameters,hyperparameters,In,context construction,hyperparameters In context construction,0.45726698637008667
translation,91,161,hyperparameters,adam optimizer,with,"?1=0.9 , ?2=0.99 and ? = 1e - 8","adam optimizer with ?1=0.9 , ?2=0.99 and ? = 1e - 8",0.6094391942024231
translation,91,161,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,91,162,hyperparameters,number of relation - aware layers,as,2,number of relation - aware layers as 2,0.5428049564361572
translation,91,162,hyperparameters,learning rate,of,decoder and the relational aware layers,learning rate of decoder and the relational aware layers,0.568253219127655
translation,91,162,hyperparameters,learning rate,of,transformer encoder,learning rate of transformer encoder,0.5725480318069458
translation,91,162,hyperparameters,learning rate,of,transformer encoder,learning rate of transformer encoder,0.5725480318069458
translation,91,162,hyperparameters,decoder and the relational aware layers,as,3e - 5,decoder and the relational aware layers as 3e - 5,0.553193986415863
translation,91,162,hyperparameters,learning rate,of,transformer encoder,learning rate of transformer encoder,0.5725480318069458
translation,91,162,hyperparameters,transformer encoder,to,4 times and 8 times smaller,transformer encoder to 4 times and 8 times smaller,0.5811050534248352
translation,91,162,hyperparameters,4 times and 8 times smaller,for,m2m - 100 and mbart - 50 m2 m,4 times and 8 times smaller for m2m - 100 and mbart - 50 m2 m,0.6785030961036682
translation,91,162,hyperparameters,hyperparameters,set,number of relation - aware layers,hyperparameters set number of relation - aware layers,0.6006399989128113
translation,91,162,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,91,162,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,91,162,hyperparameters,hyperparameters,decrease,learning rate,hyperparameters decrease learning rate,0.662178099155426
translation,91,8,model,first schema translation model,called,cast,first schema translation model called cast,0.6748542785644531
translation,91,8,model,first schema translation model,is,header - to - header neural machine translation model,first schema translation model is header - to - header neural machine translation model,0.5242155194282532
translation,91,8,model,header - to - header neural machine translation model,augmented with,schema context,header - to - header neural machine translation model augmented with schema context,0.6638381481170654
translation,91,8,model,model,propose,first schema translation model,model propose first schema translation model,0.6402179002761841
translation,91,9,model,target header and its context,as,directed graph,target header and its context as directed graph,0.543831467628479
translation,91,9,model,directed graph,to represent,entity types and relations,directed graph to represent entity types and relations,0.641366720199585
translation,91,9,model,model,model,target header and its context,model model target header and its context,0.7415054440498352
translation,91,10,model,cast,encodes,graph,cast encodes graph,0.7877628803253174
translation,91,10,model,cast,uses,another transformer,cast uses another transformer,0.676668643951416
translation,91,10,model,graph,with,relational - aware transformer,graph with relational - aware transformer,0.6371097564697266
translation,91,10,model,another transformer,to decode,header,another transformer to decode header,0.7489120960235596
translation,91,10,model,header,in,target language,header in target language,0.5642715692520142
translation,91,10,model,model,uses,another transformer,model uses another transformer,0.5899554491043091
translation,91,10,model,model,has,cast,model has cast,0.5541477799415588
translation,91,51,model,context aware schema translation ( cast ) model,is,header - to - header neural machine translation model,context aware schema translation ( cast ) model is header - to - header neural machine translation model,0.5275601148605347
translation,91,51,model,header - to - header neural machine translation model,augmented with,table context,header - to - header neural machine translation model augmented with table context,0.669666051864624
translation,91,51,model,model,propose,context aware schema translation ( cast ) model,model propose context aware schema translation ( cast ) model,0.6337121725082397
translation,91,52,model,target header and its context,as,directed graph,target header and its context as directed graph,0.543831467628479
translation,91,52,model,directed graph,to represent,entity types and structural relations,directed graph to represent entity types and structural relations,0.6474979519844055
translation,91,52,model,model,model,target header and its context,model model target header and its context,0.7415054440498352
translation,91,53,model,cast,encodes,graph,cast encodes graph,0.7877628803253174
translation,91,53,model,cast,uses,another transformer,cast uses another transformer,0.676668643951416
translation,91,53,model,graph,with,relational - aware transformer,graph with relational - aware transformer,0.6371097564697266
translation,91,53,model,another transformer,to decode,header,another transformer to decode header,0.7489120960235596
translation,91,53,model,header,in,target language,header in target language,0.5642715692520142
translation,91,53,model,model,uses,another transformer,model uses another transformer,0.5899554491043091
translation,91,53,model,model,has,cast,model has cast,0.5541477799415588
translation,91,55,model,structure relationships,make,transformer encoder,structure relationships make transformer encoder,0.6732461452484131
translation,91,55,model,structure relationships,learn,contextualized representation,structure relationships learn contextualized representation,0.6456478834152222
translation,91,55,model,transformer encoder,capture,structural information,transformer encoder capture structural information,0.7652153968811035
translation,91,55,model,contextualized representation,for,target header,contextualized representation for target header,0.6060484647750854
translation,91,55,model,target header,from,context,target header from context,0.5437011122703552
translation,91,55,model,entity types,differentiate,target header,entity types differentiate target header,0.639012336730957
translation,91,55,model,target header,from,context,target header from context,0.5437011122703552
translation,91,55,model,model,has,structure relationships,model has structure relationships,0.5365652441978455
translation,91,54,results,advantages,of,our approach,advantages of our approach,0.5397433042526245
translation,91,54,results,results,has,advantages,results has advantages,0.5523803234100342
translation,91,166,results,overall performances,of,two nmt models,overall performances of two nmt models,0.5352908372879028
translation,91,166,results,two nmt models,across,five target languages,two nmt models across five target languages,0.6897693872451782
translation,91,166,results,two nmt models,show,similar trends,two nmt models show similar trends,0.6411028504371643
translation,91,166,results,results,has,overall performances,results has overall performances,0.5650156140327454
translation,91,167,results,base,trained only on,plain text,base trained only on plain text,0.7492373585700989
translation,91,167,results,h2h,gains,significant improvement,h2h gains significant improvement,0.8206657767295837
translation,91,167,results,base,has,h2h,base has h2h,0.6060125231742859
translation,91,167,results,results,compared with,base,results compared with base,0.6511810421943665
translation,91,170,results,h2h +cxt,based on,m2m - 100,h2h +cxt based on m2m - 100,0.6880005598068237
translation,91,170,results,h2h +cxt,comparing with,h2h,h2h +cxt comparing with h2h,0.6924107670783997
translation,91,170,results,"2.1 , 0.6 , and 1.6 points of improvement",in,"en-zh , en-de , and en- ja settings","2.1 , 0.6 , and 1.6 points of improvement in en-zh , en-de , and en- ja settings",0.5376344919204712
translation,91,170,results,results,Taking,h2h +cxt,results Taking h2h +cxt,0.6871296167373657
translation,91,171,results,h2h +cxt,based on,mbart - 50 m2 m,h2h +cxt based on mbart - 50 m2 m,0.67059725522995
translation,91,171,results,concatenation,of,context,concatenation of context,0.5818781852722168
translation,91,171,results,concatenation,boosts,bleu score,concatenation boosts bleu score,0.6233250498771667
translation,91,171,results,context,boosts,bleu score,context boosts bleu score,0.6361729502677917
translation,91,171,results,bleu score,for,translating,bleu score for translating,0.635550856590271
translation,91,171,results,schema,from,en to zh and es,schema from en to zh and es,0.6472395658493042
translation,91,171,results,en to zh and es,by,1.5 and 1.2,en to zh and es by 1.5 and 1.2,0.5983871221542358
translation,91,171,results,h2h +cxt,has,concatenation,h2h +cxt has concatenation,0.624315083026886
translation,91,171,results,mbart - 50 m2 m,has,concatenation,mbart - 50 m2 m has concatenation,0.6010576486587524
translation,91,171,results,translating,has,schema,translating has schema,0.6124303340911865
translation,91,171,results,results,In terms of,h2h +cxt,results In terms of h2h +cxt,0.7339993715286255
translation,91,173,results,context,does not help,improve,context does not help improve,0.6960328221321106
translation,91,173,results,performance,of,h2h + cxt,performance of h2h + cxt,0.6488358378410339
translation,91,173,results,h2h + cxt,based on,mbart -50 m2 m and m2m100,h2h + cxt based on mbart -50 m2 m and m2m100,0.6717478632926941
translation,91,173,results,improve,has,performance,improve has performance,0.5578044652938843
translation,91,173,results,results,concatenating,context,results concatenating context,0.5939378142356873
translation,91,178,results,h2h,by,"2.6 , 1.4 , 0.3 , 1.8 , and 1.9 bleu","h2h by 2.6 , 1.4 , 0.3 , 1.8 , and 1.9 bleu",0.6009132266044617
translation,91,178,results,"2.6 , 1.4 , 0.3 , 1.8 , and 1.9 bleu",in,"en-zh , en-es , en-fr , en-de , and en-ja","2.6 , 1.4 , 0.3 , 1.8 , and 1.9 bleu in en-zh , en-es , en-fr , en-de , and en-ja",0.561966061592102
translation,91,179,results,"1.6 , 2.7 , 1.9 , 0.9 , 0.2 improvements",of,bleu points,"1.6 , 2.7 , 1.9 , 0.9 , 0.2 improvements of bleu points",0.5413110852241516
translation,91,179,results,bleu points,over,h2h,bleu points over h2h,0.7402203679084778
translation,91,179,results,h2h,in,translating schema,h2h in translating schema,0.5692058205604553
translation,91,179,results,translating schema,from,en to 5 target languages,translating schema from en to 5 target languages,0.5744580626487732
translation,91,183,results,wilcoxon signed - rank tests,between,cast and h2h +cxt,wilcoxon signed - rank tests between cast and h2h +cxt,0.6429115533828735
translation,91,183,results,improvement,are,significant,improvement are significant,0.5896084308624268
translation,91,184,results,rest of the languages,achieves,comparable results,rest of the languages achieves comparable results,0.6142925024032593
translation,91,184,results,results,For,rest of the languages,results For rest of the languages,0.5986253023147583
translation,91,191,results,h2h,achieves,14.84 % improvement,h2h achieves 14.84 % improvement,0.6539732813835144
translation,91,191,results,14.84 % improvement,over,base,14.84 % improvement over base,0.6510508060455322
translation,91,191,results,performance,further boosted by,3.11 %,performance further boosted by 3.11 %,0.7168344855308533
translation,91,191,results,3.11 %,when,context,3.11 % when context,0.6547486782073975
translation,91,191,results,human evaluation,has,h2h,human evaluation has h2h,0.5848200917243958
translation,91,192,results,cast,obtains,2.3 % improvement,cast obtains 2.3 % improvement,0.6048943996429443
translation,91,192,results,2.3 % improvement,over,h2h + cxt,2.3 % improvement over h2h + cxt,0.6714922189712524
translation,91,192,results,relationaware structure,has,cast,relationaware structure has cast,0.6007943153381348
translation,91,198,results,models,based on,m2m - 100,models based on m2m - 100,0.6758679747581482
translation,91,198,results,m2m - 100,in the setting of,en-de and en- fr,m2m - 100 in the setting of en-de and en- fr,0.7135719656944275
translation,91,198,results,erasing entity types,decreases,performance,erasing entity types decreases performance,0.6784301400184631
translation,91,198,results,performance,of,schema translation,performance of schema translation,0.6195012927055359
translation,92,164,ablation-analysis,mrt,causes,more prominent changes,mrt causes more prominent changes,0.7313196659088135
translation,92,164,ablation-analysis,more prominent changes,in,contributions,more prominent changes in contributions,0.5834094285964966
translation,92,164,ablation-analysis,ablation analysis,has,mrt,ablation analysis has mrt,0.5150133371353149
translation,92,167,ablation-analysis,word dropout,changes in,amount of contributions,word dropout changes in amount of contributions,0.6040172576904297
translation,92,167,ablation-analysis,amount of contributions,are,less noticeable,amount of contributions are less noticeable,0.5804154276847839
translation,92,167,ablation-analysis,target - side word dropout,makes,model,target - side word dropout makes model,0.5503835082054138
translation,92,167,ablation-analysis,more confident,in,choice of relevant source tokens,more confident in choice of relevant source tokens,0.49539613723754883
translation,92,167,ablation-analysis,model,has,more confident,model has more confident,0.5641335248947144
translation,92,167,ablation-analysis,ablation analysis,For,word dropout,ablation analysis For word dropout,0.5624082684516907
translation,92,169,ablation-analysis,all models,has,mrt model,all models has mrt model,0.5642605423927307
translation,92,169,ablation-analysis,mrt model,has,highest influence of source,mrt model has highest influence of source,0.5636200904846191
translation,92,169,ablation-analysis,ablation analysis,see that,all models,ablation analysis see that all models,0.6249008178710938
translation,92,169,ablation-analysis,ablation analysis,among,all models,ablation analysis among all models,0.5863156914710999
translation,92,173,ablation-analysis,word dropout,increase,influence of source,word dropout increase influence of source,0.7437354326248169
translation,92,173,ablation-analysis,both its variants,increase,influence of source,both its variants increase influence of source,0.7431206107139587
translation,92,173,ablation-analysis,both its variants,to,much lesser extent,both its variants to much lesser extent,0.5481358170509338
translation,92,173,ablation-analysis,word dropout,has,both its variants,word dropout has both its variants,0.5982159972190857
translation,92,173,ablation-analysis,ablation analysis,using,word dropout,ablation analysis using word dropout,0.6278266906738281
translation,92,174,ablation-analysis,targetside word dropout,slightly reduces,exposure bias,targetside word dropout slightly reduces exposure bias,0.7140241265296936
translation,92,174,ablation-analysis,exposure bias,leads to,larger increase,exposure bias leads to larger increase,0.6309519410133362
translation,92,174,ablation-analysis,larger increase,of,source influence,larger increase of source influence,0.5870837569236755
translation,92,187,ablation-analysis,decreases,with,more data,decreases with more data,0.6947280764579773
translation,92,187,ablation-analysis,each generation step,has,entropy of source contributions,each generation step has entropy of source contributions,0.5466902852058411
translation,92,187,ablation-analysis,entropy of source contributions,has,decreases,entropy of source contributions has decreases,0.5919374823570251
translation,92,187,ablation-analysis,ablation analysis,at,each generation step,ablation analysis at each generation step,0.532567024230957
translation,92,233,ablation-analysis,models,trained with,more data,models trained with more data,0.7239445447921753
translation,92,233,ablation-analysis,more data,rely on,source information,more data rely on source information,0.5552886128425598
translation,92,233,ablation-analysis,source information,more and have,more sharp token contributions,source information more and have more sharp token contributions,0.668379545211792
translation,92,233,ablation-analysis,training process,is,non-monotonic,training process is non-monotonic,0.5722964406013489
translation,92,233,ablation-analysis,non-monotonic,with,several distinct stages,non-monotonic with several distinct stages,0.6439913511276245
translation,92,246,experimental-setup,translations,sampled using,standard random sampling,translations sampled using standard random sampling,0.7436017394065857
translation,92,246,experimental-setup,standard random sampling,without,temperature,standard random sampling without temperature,0.715251088142395
translation,92,246,experimental-setup,experimental setup,has,translations,experimental setup has translations,0.4867457449436188
translation,92,8,model,lrp,to,transformer,lrp to transformer,0.5630626082420349
translation,92,8,model,nmt models,explicitly evaluates,source and target relative contributions,nmt models explicitly evaluates source and target relative contributions,0.7888804078102112
translation,92,8,model,source and target relative contributions,to,generation process,source and target relative contributions to generation process,0.566750705242157
translation,92,8,model,model,extend,lrp,model extend lrp,0.7657697796821594
translation,92,24,model,one of the lrp variants,to,transformer,one of the lrp variants to transformer,0.601116418838501
translation,92,24,model,of nmt models,explicitly evaluates,source and target relative contributions,of nmt models explicitly evaluates source and target relative contributions,0.7706339359283447
translation,92,24,model,source and target relative contributions,to,generation process,source and target relative contributions to generation process,0.566750705242157
translation,92,24,model,first analysis,has,of nmt models,first analysis has of nmt models,0.5289825201034546
translation,92,24,model,model,extend,one of the lrp variants,model extend one of the lrp variants,0.6711301207542419
translation,93,99,ablation-analysis,residual connection,at,11 - th ( penultimate ) encoder layer,residual connection at 11 - th ( penultimate ) encoder layer,0.5387635231018066
translation,93,99,ablation-analysis,11 - th ( penultimate ) encoder layer,selected on,validation dataset,11 - th ( penultimate ) encoder layer selected on validation dataset,0.5520818829536438
translation,93,99,ablation-analysis,ablation analysis,remove,residual connection,ablation analysis remove residual connection,0.7021493315696716
translation,93,158,ablation-analysis,twostage and bigdec,improve,zeroshot translation performance,twostage and bigdec improve zeroshot translation performance,0.6428962349891663
translation,93,158,ablation-analysis,zeroshot translation performance,by,0.8 and 0.4 average bleu,zeroshot translation performance by 0.8 and 0.4 average bleu,0.5436437129974365
translation,93,161,ablation-analysis,ablation analysis,without using,bigdec,ablation analysis without using bigdec,0.7637602090835571
translation,93,192,ablation-analysis,vocabulary overlapping ( even character overlapping ),between,hindi and korean,vocabulary overlapping ( even character overlapping ) between hindi and korean,0.6443435549736023
translation,93,192,ablation-analysis,hindi and korean,is,low,hindi and korean is low,0.5838853120803833
translation,93,192,ablation-analysis,ablation analysis,notice,vocabulary overlapping ( even character overlapping ),ablation analysis notice vocabulary overlapping ( even character overlapping ),0.6870598196983337
translation,93,118,baselines,five conventional methods,to apply,pretrained transformer encoder,five conventional methods to apply pretrained transformer encoder,0.6741594076156616
translation,93,46,experimental-setup,xlm -r,jointly trained on,100 languages,xlm -r jointly trained on 100 languages,0.7281163334846497
translation,93,46,experimental-setup,xlm -r,as,off-the-shelf mpe,xlm -r as off-the-shelf mpe,0.5661839246749878
translation,93,46,experimental-setup,experimental setup,utilize,xlm -r,experimental setup utilize xlm -r,0.621094286441803
translation,93,94,experimental-setup,experimental setup,implemented on,"fairseq toolkit ( ott et al. , 2019 )","experimental setup implemented on fairseq toolkit ( ott et al. , 2019 )",0.6586320996284485
translation,93,95,experimental-setup,transformer encoder,as,xlm -r base model,transformer encoder as xlm -r base model,0.550243079662323
translation,93,95,experimental-setup,same size,as,xlm -r base model,same size as xlm -r base model,0.6099523901939392
translation,93,95,experimental-setup,transformer encoder,has,same size,transformer encoder has same size,0.5940713286399841
translation,93,95,experimental-setup,experimental setup,set,transformer encoder,experimental setup set transformer encoder,0.6235834956169128
translation,93,98,experimental-setup,encoderdecoder attention modules,are,randomly initialized,encoderdecoder attention modules are randomly initialized,0.5564420819282532
translation,93,98,experimental-setup,experimental setup,has,encoderdecoder attention modules,experimental setup has encoderdecoder attention modules,0.511799693107605
translation,93,106,experimental-setup,adam optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adam optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6311911940574646
translation,93,106,experimental-setup,? 1 = 0.9 and ? 2 = 0.98,used for,training,? 1 = 0.9 and ? 2 = 0.98 used for training,0.6548073887825012
translation,93,106,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,93,107,experimental-setup,label smoothing,with,value 0.1,label smoothing with value 0.1,0.6159650683403015
translation,93,108,experimental-setup,learning rate,is,0.0005,learning rate is 0.0005,0.5783849954605103
translation,93,108,experimental-setup,warmup step,is,4000,warmup step is 4000,0.5803221464157104
translation,93,108,experimental-setup,4000,at,first stage,4000 at first stage,0.6218866109848022
translation,93,108,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,93,108,experimental-setup,experimental setup,has,warmup step,experimental setup has warmup step,0.5123323798179626
translation,93,109,experimental-setup,second stage,set,learning rate,second stage set learning rate,0.6530042290687561
translation,93,109,experimental-setup,second stage,do not use,warmup,second stage do not use warmup,0.6699499487876892
translation,93,109,experimental-setup,learning rate,as,0.0001,learning rate as 0.0001,0.5501636862754822
translation,93,109,experimental-setup,learning rate,do not use,warmup,learning rate do not use warmup,0.6623435020446777
translation,93,109,experimental-setup,experimental setup,For,second stage,experimental setup For second stage,0.6018936634063721
translation,93,110,experimental-setup,drop-out probabilities,set to,0.3,drop-out probabilities set to 0.3,0.6683433055877686
translation,93,110,experimental-setup,experimental setup,has,drop-out probabilities,experimental setup has drop-out probabilities,0.5350755453109741
translation,93,111,experimental-setup,batch size,set as,4096 tokens,batch size set as 4096 tokens,0.6064427495002747
translation,93,111,experimental-setup,4096 tokens,per,gpu,4096 tokens per gpu,0.6156757473945618
translation,93,112,experimental-setup,maximum updates number,is,200k,maximum updates number is 200k,0.5961334109306335
translation,93,112,experimental-setup,maximum updates number,is,30 k,maximum updates number is 30 k,0.6069597005844116
translation,93,112,experimental-setup,200k,for,first stage,200k for first stage,0.6644513010978699
translation,93,112,experimental-setup,30 k,for,second stage,30 k for second stage,0.6828945875167847
translation,93,112,experimental-setup,experimental setup,has,maximum updates number,experimental setup has maximum updates number,0.558646023273468
translation,93,113,experimental-setup,beam search,do not tune,length penalty,beam search do not tune length penalty,0.7300676703453064
translation,93,113,experimental-setup,beam size,is,5,beam size is 5,0.657706618309021
translation,93,113,experimental-setup,beam search,has,beam size,beam search has beam size,0.5626558065414429
translation,93,113,experimental-setup,experimental setup,use,beam search,experimental setup use beam search,0.5703415870666504
translation,93,121,experimental-setup,encoder,with,same size of xlm -r base,encoder with same size of xlm -r base,0.6448238492012024
translation,93,121,experimental-setup,decoder,uses,size of basedec,decoder uses size of basedec,0.6549596190452576
translation,93,121,experimental-setup,experimental setup,has,encoder,experimental setup has encoder,0.5011709332466125
translation,93,122,experimental-setup,model parameters,are,randomly initialized,model parameters are randomly initialized,0.5826751589775085
translation,93,122,experimental-setup,experimental setup,has,model parameters,experimental setup has model parameters,0.4974170923233032
translation,93,124,experimental-setup,encoder,initialized with,xlm -r,encoder initialized with xlm -r,0.7710121870040894
translation,93,124,experimental-setup,experimental setup,has,encoder,experimental setup has encoder,0.5011709332466125
translation,93,172,hyperparameters,criss model,initialized with,mbart model,criss model initialized with mbart model,0.7405720353126526
translation,93,172,hyperparameters,criss model,iteratively finetuned on,1.8 billion sentences,criss model iteratively finetuned on 1.8 billion sentences,0.7034159898757935
translation,93,172,hyperparameters,1.8 billion sentences,covering,90 language pairs,1.8 billion sentences covering 90 language pairs,0.7526546716690063
translation,93,172,hyperparameters,hyperparameters,has,criss model,hyperparameters has criss model,0.5485879778862
translation,93,8,model,sixt,has,simple yet effective model,sixt has simple yet effective model,0.5883651375770569
translation,93,8,model,model,propose,sixt,model propose sixt,0.6939990520477295
translation,93,9,model,sixt,leverages,mpe,sixt leverages mpe,0.7774431109428406
translation,93,9,model,sixt,gets,further improvement,sixt gets further improvement,0.6792694330215454
translation,93,9,model,mpe,with,two -stage training schedule,mpe with two -stage training schedule,0.6232655644416809
translation,93,9,model,further improvement,with,position disentangled encoder,further improvement with position disentangled encoder,0.6680626273155212
translation,93,9,model,further improvement,with,capacity - enhanced decoder,further improvement with capacity - enhanced decoder,0.6588454246520996
translation,93,9,model,model,gets,further improvement,model gets further improvement,0.6229769587516785
translation,93,9,model,model,has,sixt,model has sixt,0.6093483567237854
translation,93,29,model,simple cross-lingual ( x ) transfer nmt model ( sixt ),can directly translates,languages unseen,simple cross-lingual ( x ) transfer nmt model ( sixt ) can directly translates languages unseen,0.68521648645401
translation,93,29,model,languages unseen,during,supervised training,languages unseen during supervised training,0.6586496829986572
translation,93,29,model,model,propose,simple cross-lingual ( x ) transfer nmt model ( sixt ),model propose simple cross-lingual ( x ) transfer nmt model ( sixt ),0.6321243047714233
translation,93,30,model,encoder and decoder embeddings,of,sixt,encoder and decoder embeddings of sixt,0.6216176748275757
translation,93,30,model,encoder and decoder embeddings,propose,two -stage training schedule,encoder and decoder embeddings propose two -stage training schedule,0.6434949040412903
translation,93,30,model,sixt,with,xlm -r,sixt with xlm -r,0.7181324362754822
translation,93,30,model,two -stage training schedule,trades off between,supervised performance and transferability,two -stage training schedule trades off between supervised performance and transferability,0.7858712673187256
translation,93,30,model,model,initialize,encoder and decoder embeddings,model initialize encoder and decoder embeddings,0.7433080673217773
translation,93,31,model,first stage,train,decoder layers,first stage train decoder layers,0.6770829558372498
translation,93,31,model,first stage,at,second stage,first stage at second stage,0.599266767501831
translation,93,31,model,jointly optimized,except,encoder embedding,jointly optimized except encoder embedding,0.6187914609909058
translation,93,31,model,second stage,has,all model parameters,second stage has all model parameters,0.5140737295150757
translation,93,31,model,model,At,first stage,model At first stage,0.6030095815658569
translation,93,31,model,model,at,second stage,model at second stage,0.5982692837715149
translation,93,32,model,model,introducing,position disentangled encoder,model introducing position disentangled encoder,0.689197301864624
translation,93,33,model,position disentangled encoder,enhances,cross-lingual transferability,position disentangled encoder enhances cross-lingual transferability,0.6345194578170776
translation,93,33,model,position disentangled encoder,making,encoder outputs,position disentangled encoder making encoder outputs,0.5925422310829163
translation,93,33,model,cross-lingual transferability,by removing,residual connection,cross-lingual transferability by removing residual connection,0.6785579323768616
translation,93,33,model,residual connection,in,one of the encoder layers,residual connection in one of the encoder layers,0.5040161609649658
translation,93,33,model,encoder outputs,has,more language - agnostic,encoder outputs has more language - agnostic,0.5667116045951843
translation,93,33,model,model,has,position disentangled encoder,model has position disentangled encoder,0.5531147718429565
translation,93,34,model,capacity - enhanced decoder,leverages,bigger decoder,capacity - enhanced decoder leverages bigger decoder,0.682611346244812
translation,93,34,model,bigger decoder,than,vanilla transformer base model,bigger decoder than vanilla transformer base model,0.5363311767578125
translation,93,34,model,model,has,capacity - enhanced decoder,model has capacity - enhanced decoder,0.5605373382568359
translation,93,75,model,new model,can further improve,zero-shot translations,new model can further improve zero-shot translations,0.6902518272399902
translation,93,75,model,model,propose,new model,model propose new model,0.6939241886138916
translation,93,76,model,capacity - enhanced decoder,enhancing,cross-lingual transferability,capacity - enhanced decoder enhancing cross-lingual transferability,0.6791448593139648
translation,93,76,model,cross-lingual transferability,of,encoder,cross-lingual transferability of encoder,0.5726842880249023
translation,93,76,model,cross-lingual transferability,fully utilizing,labelled data,cross-lingual transferability fully utilizing labelled data,0.6232897639274597
translation,93,76,model,model,consists of,position disentangled encoder,model consists of position disentangled encoder,0.631644070148468
translation,93,76,model,model,consists of,capacity - enhanced decoder,model consists of capacity - enhanced decoder,0.6578548550605774
translation,93,119,model,pretrained encoders,replaced with,xlm - r base,pretrained encoders replaced with xlm - r base,0.7108960151672363
translation,93,119,model,xlm - r base,for,fair comparison,xlm - r base for fair comparison,0.6144945025444031
translation,93,119,model,model,has,pretrained encoders,model has pretrained encoders,0.5461461544036865
translation,93,178,model,residual connection,after,self-attention sublayer,residual connection after self-attention sublayer,0.6465104222297668
translation,93,178,model,self-attention sublayer,of,23 - th ( penultimate ) encoder layer,self-attention sublayer of 23 - th ( penultimate ) encoder layer,0.5506423711776733
translation,93,178,model,model,remove,residual connection,model remove residual connection,0.7163585424423218
translation,93,10,results,average improvement,of,7.1 bleu,average improvement of 7.1 bleu,0.5142501592636108
translation,93,10,results,7.1 bleu,on,zero-shot any - to - english test sets,7.1 bleu on zero-shot any - to - english test sets,0.4247565269470215
translation,93,10,results,zero-shot any - to - english test sets,across,14 source languages,zero-shot any - to - english test sets across 14 source languages,0.6416676044464111
translation,93,10,results,sixt,has,significantly outperforms,sixt has significantly outperforms,0.6120162010192871
translation,93,10,results,significantly outperforms,has,mbart,significantly outperforms has mbart,0.6151867508888245
translation,93,10,results,significantly outperforms,has,pretrained multilingual encoderdecoder model,significantly outperforms has pretrained multilingual encoderdecoder model,0.5357635021209717
translation,93,10,results,mbart,has,pretrained multilingual encoderdecoder model,mbart has pretrained multilingual encoderdecoder model,0.5141280889511108
translation,93,36,results,mbart,with,average improvement,mbart with average improvement,0.6314396262168884
translation,93,36,results,average improvement,of,7.1 bleu,average improvement of 7.1 bleu,0.5142501592636108
translation,93,36,results,7.1 bleu,on,zeroshot any - to - english translation,7.1 bleu on zeroshot any - to - english translation,0.4662584066390991
translation,93,36,results,zeroshot any - to - english translation,across,14 source languages,zeroshot any - to - english translation across 14 source languages,0.6839126944541931
translation,93,36,results,sixt,has,significantly outperforms,sixt has significantly outperforms,0.6120162010192871
translation,93,36,results,significantly outperforms,has,mbart,significantly outperforms has mbart,0.6151867508888245
translation,93,36,results,results,has,sixt,results has sixt,0.5306736826896667
translation,93,37,results,sixt model,gets,better performance,sixt model gets better performance,0.6468694806098938
translation,93,37,results,better performance,on,15 any - to - english test sets,better performance on 15 any - to - english test sets,0.5064634084701538
translation,93,37,results,15 any - to - english test sets,than,criss and m2m - 100,15 any - to - english test sets than criss and m2m - 100,0.5742435455322266
translation,93,37,results,much less training computation cost,has,sixt model,much less training computation cost has sixt model,0.5674675107002258
translation,93,37,results,training data,has,sixt model,training data has sixt model,0.5841576457023621
translation,93,37,results,results,with,much less training computation cost,results with much less training computation cost,0.6126365661621094
translation,93,65,results,best,to initialize,encoder embedding,best to initialize encoder embedding,0.7065051198005676
translation,93,65,results,best,to initialize,the encoder layers and the decoder embedding,best to initialize the encoder layers and the decoder embedding,0.7158907651901245
translation,93,65,results,best,keep,parameters,best keep parameters,0.6087931394577026
translation,93,65,results,best,randomly initializing,decoder layers,best randomly initializing decoder layers,0.7366346120834351
translation,93,65,results,encoder embedding,with,xlm -r,encoder embedding with xlm -r,0.6427218914031982
translation,93,65,results,encoder embedding,keep,parameters,encoder embedding keep parameters,0.5710524916648865
translation,93,65,results,the encoder layers and the decoder embedding,with,xlm -r,the encoder layers and the decoder embedding with xlm -r,0.6496941447257996
translation,93,65,results,parameters,has,frozen,parameters has frozen,0.5850687623023987
translation,93,78,results,representations,from,xlm -r initialized encoder,representations from xlm -r initialized encoder,0.5549386739730835
translation,93,78,results,xlm -r initialized encoder,have,strong positional correspondence,xlm -r initialized encoder have strong positional correspondence,0.5280397534370422
translation,93,78,results,strong positional correspondence,to,source sentence,strong positional correspondence to source sentence,0.5296446084976196
translation,93,78,results,results,has,representations,results has representations,0.5003946423530579
translation,93,143,results,vanilla transformer model,with,basedec and bigdec,vanilla transformer model with basedec and bigdec,0.6806081533432007
translation,93,143,results,basedec and bigdec,obtains,bleu score,basedec and bigdec obtains bleu score,0.5778008103370667
translation,93,143,results,bleu score,of,23.5 and 22.9,bleu score of 23.5 and 22.9,0.5655198097229004
translation,93,143,results,23.5 and 22.9,on,de- en test set,23.5 and 22.9 on de- en test set,0.5818073749542236
translation,93,143,results,results,has,vanilla transformer model,results has vanilla transformer model,0.535121738910675
translation,93,144,results,big decoder,improves,performance,big decoder improves performance,0.7511217594146729
translation,93,144,results,big decoder,fails to improve,vanilla transformer,big decoder fails to improve vanilla transformer,0.7488457560539246
translation,93,144,results,performance,of,sixt,performance of sixt,0.6490932106971741
translation,93,144,results,results,has,big decoder,results has big decoder,0.589695930480957
translation,93,146,results,performance,of,proposed sixt,performance of proposed sixt,0.6546759009361267
translation,93,146,results,proposed sixt,comparing with,baselines,proposed sixt comparing with baselines,0.7612487077713013
translation,93,146,results,results,illustrates,performance,results illustrates performance,0.725039005279541
translation,93,147,results,sixt,gets,18.3 average bleu,sixt gets 18.3 average bleu,0.5891985893249512
translation,93,147,results,sixt,improves over,best baseline,sixt improves over best baseline,0.7718614339828491
translation,93,147,results,best baseline,by,5.4 average bleu,best baseline by 5.4 average bleu,0.530359148979187
translation,93,147,results,results,has,sixt,results has sixt,0.5306736826896667
translation,93,148,results,sixt,obtains,better transferring scores,sixt obtains better transferring scores,0.6577961444854736
translation,93,148,results,all language pairs,has,sixt,all language pairs has sixt,0.6744881868362427
translation,93,148,results,results,For,all language pairs,results For all language pairs,0.5478318929672241
translation,93,149,results,vanilla transformer,can,hardly transfer,vanilla transformer can hardly transfer,0.6425467133522034
translation,93,149,results,do not well transfer,to,distant languages,do not well transfer to distant languages,0.5989649295806885
translation,93,149,results,vanilla transformer,has,hardly transfer,vanilla transformer has hardly transfer,0.6022512316703796
translation,93,149,results,other baselines,has,do not well transfer,other baselines has do not well transfer,0.5944041013717651
translation,93,149,results,results,has,vanilla transformer,results has vanilla transformer,0.5522176623344421
translation,93,150,results,sixt,achieves,best result,sixt achieves best result,0.7232756614685059
translation,93,150,results,best result,on,de- en test set,best result on de- en test set,0.5856597423553467
translation,93,157,results,sixt,obtains,best zero-shot translation results,sixt obtains best zero-shot translation results,0.5890482664108276
translation,93,157,results,results,has,sixt,results has sixt,0.5306736826896667
translation,93,159,results,significant improvement,of,2.6 average bleu,significant improvement of 2.6 average bleu,0.5343089699745178
translation,93,162,results,improves,with,twostage and bigdec,improves with twostage and bigdec,0.6755599975585938
translation,93,162,results,degrades,with,resdrop,degrades with resdrop,0.7113434076309204
translation,93,162,results,supervised task ( de -en ),has,improves,supervised task ( de -en ) has improves,0.5990354418754578
translation,93,162,results,results,observe,supervised task ( de -en ),results observe supervised task ( de -en ),0.6352392435073853
translation,93,164,results,resdrop,improves,zero-shot translation,resdrop improves zero-shot translation,0.6823388338088989
translation,93,164,results,resdrop,has,degrades,resdrop has degrades,0.609606146812439
translation,93,164,results,degrades,has,supervised performance,degrades has supervised performance,0.5647076964378357
translation,93,180,results,sixt large model,significantly better than,mbart,sixt large model significantly better than mbart,0.7218080759048462
translation,93,180,results,sixt large model,slightly better than,criss and m2m - 100,sixt large model slightly better than criss and m2m - 100,0.711737871170044
translation,93,181,results,averaged bleu,across,all languages,averaged bleu across all languages,0.6622110605239868
translation,93,181,results,all languages,is,"7.1 , 0.5 and 1.4","all languages is 7.1 , 0.5 and 1.4",0.5787507891654968
translation,93,181,results,higher,than,mbart,higher than mbart,0.6537418365478516
translation,93,181,results,higher,than,criss and m2m - 100,higher than criss and m2m - 100,0.6008086800575256
translation,93,181,results,"7.1 , 0.5 and 1.4",has,higher,"7.1 , 0.5 and 1.4 has higher",0.5228055119514465
translation,93,181,results,results,has,averaged bleu,results has averaged bleu,0.5671790242195129
translation,93,183,results,performance gain,over,mbart,performance gain over mbart,0.6836863160133362
translation,93,183,results,mbart,shows,proper fine-tuning strategy,mbart shows proper fine-tuning strategy,0.6466508507728577
translation,93,183,results,mbart,shows,pretrained multilingual encoder,mbart shows pretrained multilingual encoder,0.6204861998558044
translation,93,183,results,mbart,with,proper fine-tuning strategy,mbart with proper fine-tuning strategy,0.6573438048362732
translation,93,183,results,better cross-lingual transfer ability,on,nmt tasks,better cross-lingual transfer ability on nmt tasks,0.5033618211746216
translation,93,183,results,proper fine-tuning strategy,has,pretrained multilingual encoder,proper fine-tuning strategy has pretrained multilingual encoder,0.5209674835205078
translation,93,183,results,pretrained multilingual encoder,has,better cross-lingual transfer ability,pretrained multilingual encoder has better cross-lingual transfer ability,0.5167471170425415
translation,93,183,results,results,has,performance gain,results has performance gain,0.5905830264091492
translation,93,184,results,sixt model,transfers,well,sixt model transfers well,0.754520058631897
translation,93,184,results,sixt model,gets,2.2 more average bleu,sixt model gets 2.2 more average bleu,0.5811800360679626
translation,93,184,results,1.2b m2m - 100 model,larger than,our model ( 737 m parameters ),1.2b m2m - 100 model larger than our model ( 737 m parameters ),0.7210121154785156
translation,93,184,results,1.2b m2m - 100 model,gets,2.2 more average bleu,1.2b m2m - 100 model gets 2.2 more average bleu,0.5694087743759155
translation,93,184,results,2.2 more average bleu,than,sixt,2.2 more average bleu than sixt,0.551030158996582
translation,93,184,results,large-scale german- english parallel data,has,sixt model,large-scale german- english parallel data has sixt model,0.5409097671508789
translation,93,184,results,well,has,1.2b m2m - 100 model,well has 1.2b m2m - 100 model,0.5877392292022705
translation,93,184,results,results,with,large-scale german- english parallel data,results with large-scale german- english parallel data,0.531506359577179
translation,93,193,results,sixt,obtains,promising results,sixt obtains promising results,0.6570011377334595
translation,93,193,results,promising results,on,ne-en and si- en translation,promising results on ne-en and si- en translation,0.5844792723655701
translation,93,193,results,bleu score,of,16.7 and 9.6,bleu score of 16.7 and 9.6,0.5779904723167419
translation,93,193,results,3.5 million hi- en sentence pairs,has,sixt,3.5 million hi- en sentence pairs has sixt,0.6144537329673767
translation,93,194,results,vanilla transformer,supervised with,flores training set,vanilla transformer supervised with flores training set,0.6540253758430481
translation,93,194,results,vanilla transformer,receives,14.5 and 7.2 bleu score,vanilla transformer receives 14.5 and 7.2 bleu score,0.644923985004425
translation,93,194,results,14.5 and 7.2 bleu score,on,same test sets,14.5 and 7.2 bleu score on same test sets,0.5296769738197327
translation,93,194,results,results,has,vanilla transformer,results has vanilla transformer,0.5522176623344421
translation,93,200,results,performance,of,sixt,performance of sixt,0.6490932106971741
translation,93,200,results,sixt,lower than,vanilla transformer,sixt lower than vanilla transformer,0.757713794708252
translation,93,200,results,sixt,gets,better performance,sixt gets better performance,0.6535929441452026
translation,93,200,results,vanilla transformer,when,more than 20 m parallel sentences,vanilla transformer when more than 20 m parallel sentences,0.655709981918335
translation,93,200,results,more than 20 m parallel sentences,are,available,more than 20 m parallel sentences are available,0.5676146149635315
translation,93,200,results,better performance,with,fewer parallel sentences,better performance with fewer parallel sentences,0.625459611415863
translation,93,200,results,results,has,performance,results has performance,0.5972660779953003
translation,94,119,ablation-analysis,in - domain,only for,wmt - 15,in - domain only for wmt - 15,0.6727579832077026
translation,94,119,ablation-analysis,general- domain data,has,hurts,general- domain data has hurts,0.5438144207000732
translation,94,119,ablation-analysis,hurts,has,correlation significantly,hurts has correlation significantly,0.620883047580719
translation,94,119,ablation-analysis,in - domain,has,helps,in - domain has helps,0.6383794546127319
translation,94,119,ablation-analysis,helps,has,significantly,helps has significantly,0.6237192749977112
translation,94,119,ablation-analysis,ablation analysis,has,general- domain data,ablation analysis has general- domain data,0.5295295715332031
translation,94,144,ablation-analysis,language ( gu ),for which,have no training data,language ( gu ) for which have no training data,0.6457003951072693
translation,94,144,ablation-analysis,have no training data,improves,average correlation,have no training data improves average correlation,0.7206869721412659
translation,94,144,ablation-analysis,average correlation,has,substantially,average correlation has substantially,0.6077061295509338
translation,94,144,ablation-analysis,ablation analysis,Removing,language ( gu ),ablation analysis Removing language ( gu ),0.7205816507339478
translation,94,100,experimental-setup,adafactor optimization,with,learning rate,adafactor optimization with learning rate,0.5971643328666687
translation,94,100,experimental-setup,adafactor optimization,with,batch size,adafactor optimization with batch size,0.6065630316734314
translation,94,100,experimental-setup,learning rate,of,1.0,learning rate of 1.0,0.6061668395996094
translation,94,100,experimental-setup,batch size,of,?8000 samples,batch size of ?8000 samples,0.6151459217071533
translation,94,100,experimental-setup,experimental setup,use,adafactor optimization,experimental setup use adafactor optimization,0.6165528297424316
translation,94,101,experimental-setup,shared vocabulary,comprises,64 k subwords,shared vocabulary comprises 64 k subwords,0.6824796199798584
translation,94,101,experimental-setup,experimental setup,has,shared vocabulary,experimental setup has shared vocabulary,0.5633739233016968
translation,94,109,results,slightly better results,for,source-side tagging ( prism-src2xx ),slightly better results for source-side tagging ( prism-src2xx ),0.5976760983467102
translation,94,109,results,slightly better results,on average,match,slightly better results on average match,0.7659469842910767
translation,94,109,results,results,achieve,slightly better results,results achieve slightly better results,0.5577283501625061
translation,94,117,results,prism -39 and wmt - 15,yielding,relatively small but statistically significant average gain,prism -39 and wmt - 15 yielding relatively small but statistically significant average gain,0.6321483254432678
translation,94,117,results,improves further,yielding,relatively small but statistically significant average gain,improves further yielding relatively small but statistically significant average gain,0.6706726551055908
translation,94,117,results,relatively small but statistically significant average gain,over,pure prism - 39,relatively small but statistically significant average gain over pure prism - 39,0.7085840702056885
translation,94,117,results,relatively small but statistically significant average gain,cost of,lower performance,relatively small but statistically significant average gain cost of lower performance,0.6801890134811401
translation,94,117,results,lower performance,for,en-xx language pairs,lower performance for en-xx language pairs,0.6059482097625732
translation,94,117,results,prism -39 and wmt - 15,has,improves further,prism -39 and wmt - 15 has improves further,0.6265057325363159
translation,94,117,results,results,Combining,prism -39 and wmt - 15,results Combining prism -39 and wmt - 15,0.6054502129554749
translation,94,120,results,monolingual data,tends to help,"lower - resource languages ( gu , kk , lt )","monolingual data tends to help lower - resource languages ( gu , kk , lt )",0.6389679908752441
translation,94,120,results,particularly large gain,for,xx-en,particularly large gain for xx-en,0.7377846837043762
translation,94,120,results,xx-en,with,wmt - 15 + wmt - 15 - mono,xx-en with wmt - 15 + wmt - 15 - mono,0.6873639225959778
translation,94,120,results,results,has,monolingual data,results has monolingual data,0.4745085835456848
translation,94,125,results,bilingual model,performs,comparably,bilingual model performs comparably,0.6095356941223145
translation,94,125,results,comparably,to,multilingual model,comparably to multilingual model,0.5971636772155762
translation,94,125,results,"medium and high resource languages ( de , ru , and zh )",has,bilingual model,"medium and high resource languages ( de , ru , and zh ) has bilingual model",0.5465068221092224
translation,94,125,results,results,for,"medium and high resource languages ( de , ru , and zh )","results for medium and high resource languages ( de , ru , and zh )",0.5899450778961182
translation,94,126,results,multilingual model,is,significantly better,multilingual model is significantly better,0.5642231702804565
translation,94,126,results,low resource language   lt,has,multilingual model,low resource language   lt has multilingual model,0.5299348831176758
translation,94,126,results,results,for,low resource language   lt,results for low resource language   lt,0.6124099493026733
translation,94,137,results,mc - dropout or subwords ( sp - norm ),leads to,significant gains,mc - dropout or subwords ( sp - norm ) leads to significant gains,0.6716679930686951
translation,94,137,results,significant gains,in,some cases,significant gains in some cases,0.580452024936676
translation,94,137,results,significant gains,with,slight overall increase,significant gains with slight overall increase,0.6648035049438477
translation,94,137,results,slight overall increase,over,mean,slight overall increase over mean,0.7473488450050354
translation,94,137,results,mean,for,sp - norm,mean for sp - norm,0.6380663514137268
translation,94,137,results,results,Regularizing with,mc - dropout or subwords ( sp - norm ),results Regularizing with mc - dropout or subwords ( sp - norm ),0.6767365336418152
translation,94,147,results,better performance,for,lower - resource ( < 1 m parallel segments ) language pairs,better performance for lower - resource ( < 1 m parallel segments ) language pairs,0.6202132105827332
translation,94,147,results,lower - resource ( < 1 m parallel segments ) language pairs,than,higher - resource pairs,lower - resource ( < 1 m parallel segments ) language pairs than higher - resource pairs,0.5904645919799805
translation,94,147,results,higher - resource pairs,with respect to,prism - 39 corpora,higher - resource pairs with respect to prism - 39 corpora,0.6975618600845337
translation,94,147,results,poor average performance,on,pairs ( en-gu / gu-en ),poor average performance on pairs ( en-gu / gu-en ),0.5582685470581055
translation,94,147,results,results,achieve,better performance,results achieve better performance,0.6580345034599304
translation,94,154,results,general pattern,across,all groupings,general pattern across all groupings,0.7548624277114868
translation,94,154,results,all groupings,is,prism,all groupings is prism,0.6365766525268555
translation,94,154,results,prism,is,more decisive,prism is more decisive,0.6074748039245605
translation,94,154,results,prism,makes,more significant decisions,prism makes more significant decisions,0.7540351152420044
translation,94,154,results,more significant decisions,than,bleu,more significant decisions than bleu,0.6451826095581055
translation,94,154,results,more significant decisions,leading to,higher rates,more significant decisions leading to higher rates,0.6780927181243896
translation,94,154,results,higher rates,of,correct and incorrect rankings,higher rates of correct and incorrect rankings,0.5713062286376953
translation,94,154,results,higher rates,both,correct and incorrect rankings,higher rates both correct and incorrect rankings,0.6714940667152405
translation,94,154,results,results,has,general pattern,results has general pattern,0.5542104840278625
translation,94,155,results,885 system pairs,considered,significantly different,885 system pairs considered significantly different,0.6842866539955139
translation,94,155,results,languages ),considered,significantly different,languages ) considered significantly different,0.698058545589447
translation,94,155,results,significantly different,according to,human judgment,significantly different according to human judgment,0.6772940754890442
translation,94,155,results,prism,correctly ranks,88 %,prism correctly ranks 88 %,0.7671014070510864
translation,94,155,results,88 %,with,significantly different scores,88 % with significantly different scores,0.6241985559463501
translation,94,155,results,88 %,compared to,87 %,88 % compared to 87 %,0.6777751445770264
translation,94,155,results,87 %,for,bleu,87 % for bleu,0.6181690096855164
translation,94,155,results,885 system pairs,has,prism,885 system pairs has prism,0.6170139312744141
translation,94,155,results,human judgment,has,prism,human judgment has prism,0.5459397435188293
translation,94,155,results,results,Among,885 system pairs,results Among 885 system pairs,0.5974867939949036
translation,95,135,ablation-analysis,hypothesis,that,document graphs,hypothesis that document graphs,0.6495845317840576
translation,95,135,ablation-analysis,document graphs,are,beneficial,document graphs are beneficial,0.605606734752655
translation,95,135,ablation-analysis,beneficial,for,modeling and leveraging,beneficial for modeling and leveraging,0.6528067588806152
translation,95,135,ablation-analysis,modeling and leveraging,has,context,modeling and leveraging has context,0.5507656335830688
translation,95,135,ablation-analysis,ablation analysis,verifies,hypothesis,ablation analysis verifies hypothesis,0.7346845269203186
translation,95,143,ablation-analysis,sentence embedding,show,influence,sentence embedding show influence,0.5692480206489563
translation,95,143,ablation-analysis,ablation analysis,has,sentence embedding,ablation analysis has sentence embedding,0.5389416813850403
translation,95,193,ablation-analysis,target graphs,to achieve,best performance,target graphs to achieve best performance,0.6503413915634155
translation,95,193,ablation-analysis,ablation analysis,has,target graphs,ablation analysis has target graphs,0.5237445831298828
translation,95,194,ablation-analysis,both types of relations,make,significant contributions,both types of relations make significant contributions,0.6092277765274048
translation,95,194,ablation-analysis,both types of relations,has,inter and intra,both types of relations has inter and intra,0.6057342290878296
translation,95,194,ablation-analysis,ablation analysis,has,both types of relations,ablation analysis has both types of relations,0.5915579199790955
translation,95,196,ablation-analysis,inter relations,contribute,more,inter relations contribute more,0.7246921062469482
translation,95,196,ablation-analysis,more,on,all tasks,more on all tasks,0.5565716624259949
translation,95,196,ablation-analysis,all tasks,except,ell. task,all tasks except ell. task,0.7089375257492065
translation,95,196,ablation-analysis,intra relations,has,inter relations,intra relations has inter relations,0.5997243523597717
translation,95,196,ablation-analysis,ablation analysis,compared to,intra relations,ablation analysis compared to intra relations,0.6765632033348083
translation,95,126,baselines,several document- level baselines,on,transformer architecture,several document- level baselines on transformer architecture,0.5044981837272644
translation,95,126,baselines,additional encoder,to learn,context representations,additional encoder to learn context representations,0.6030625104904175
translation,95,126,baselines,context representations,integrated by,cross-attention mechanisms,context representations integrated by cross-attention mechanisms,0.6949694752693176
translation,95,126,baselines,baselines,reimplement,several document- level baselines,baselines reimplement several document- level baselines,0.6743361353874207
translation,95,127,baselines,hierarchical attention mechanism,with,two levels ( word and sentence ) of abstraction,hierarchical attention mechanism with two levels ( word and sentence ) of abstraction,0.6053251624107361
translation,95,127,baselines,context information,from,source and target documents,context information from source and target documents,0.5108806490898132
translation,95,127,baselines,context information,both,source and target documents,context information both source and target documents,0.6120649576187134
translation,95,128,baselines,hm - gdc,learns,representations,hm - gdc learns representations,0.6727836728096008
translation,95,128,baselines,representations,with,global context,representations with global context,0.6135731339454651
translation,95,128,baselines,global context,using,hierarchical attention mechanism,global context using hierarchical attention mechanism,0.6524173021316528
translation,95,128,baselines,baselines,has,hm - gdc,baselines has hm - gdc,0.5774094462394714
translation,95,172,baselines,lex,focuses on,translation consistency,lex focuses on translation consistency,0.7466650009155273
translation,95,172,baselines,translation consistency,of,reiterative phrases,translation consistency of reiterative phrases,0.5528403520584106
translation,95,172,baselines,baselines,has,lex,baselines has lex,0.6266828179359436
translation,95,119,experimental-setup,data,tokenized and segmented into,subword units,data tokenized and segmented into subword units,0.7743343114852905
translation,95,119,experimental-setup,subword units,using,byte-pair encoding,subword units using byte-pair encoding,0.6815019845962524
translation,95,119,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,95,120,experimental-setup,32 k merge steps,for,each language,32 k merge steps for each language,0.599717915058136
translation,95,120,experimental-setup,each language,on,"en- fr , en-ru , en- de tasks","each language on en- fr , en-ru , en- de tasks",0.5416741371154785
translation,95,120,experimental-setup,30 k,for,zh- en task,30 k for zh- en task,0.6480932831764221
translation,95,120,experimental-setup,experimental setup,apply,32 k merge steps,experimental setup apply 32 k merge steps,0.6425134539604187
translation,95,125,experimental-setup,document graph encoder,to,2,document graph encoder to 2,0.5632721185684204
translation,95,125,experimental-setup,experimental setup,layers of,document graph encoder,experimental setup layers of document graph encoder,0.6842105984687805
translation,95,182,experiments,lex.,has,coref. and cohe,lex. has coref. and cohe,0.6647005081176758
translation,95,9,model,both source and target graphs,into,conventional transformer architecture,both source and target graphs into conventional transformer architecture,0.5972214937210083
translation,95,9,model,conventional transformer architecture,with,graph convolutional networks,conventional transformer architecture with graph convolutional networks,0.6512898802757263
translation,95,9,model,model,incorporate,both source and target graphs,model incorporate both source and target graphs,0.6896823048591614
translation,95,94,model,hyb-integration,adding,context - attn,hyb-integration adding context - attn,0.7153515815734863
translation,95,94,model,context - attn,inside,each encoder layer,context - attn inside each encoder layer,0.6878483295440674
translation,95,94,model,model,has,hyb-integration,model has hyb-integration,0.6012529134750366
translation,95,95,model,post - and pre-integration,aggregating,after and before the encoder,post - and pre-integration aggregating after and before the encoder,0.7043061852455139
translation,95,95,model,model,has,post - and pre-integration,model has post - and pre-integration,0.5896492004394531
translation,95,134,results,systems with document graphs,achieve,best performance,systems with document graphs achieve best performance,0.6103707551956177
translation,95,134,results,best performance,among,all context - aware systems,best performance among all context - aware systems,0.5792112350463867
translation,95,134,results,best performance,on,all language pairs,best performance on all language pairs,0.5126841068267822
translation,95,134,results,all context - aware systems,on,all language pairs,all context - aware systems on all language pairs,0.5275520086288452
translation,95,134,results,results,find,systems with document graphs,results find systems with document graphs,0.5588096380233765
translation,95,136,results,translation quality,in terms of,bleu,translation quality in terms of bleu,0.6314836144447327
translation,95,136,results,translation quality,shows,positive effect,translation quality shows positive effect,0.6382757425308228
translation,95,136,results,bleu,gets,slightly improved,bleu gets slightly improved,0.627549409866333
translation,95,136,results,positive effect,of,target context,positive effect of target context,0.5479181408882141
translation,95,136,results,target graphs,has,translation quality,target graphs has translation quality,0.5236393213272095
translation,95,151,results,our model,getting,better,our model getting better,0.716551661491394
translation,95,151,results,better,while,more context,better while more context,0.6509223580360413
translation,95,151,results,results,has,our model,results has our model,0.5871725678443909
translation,95,153,results,selective,achieves,lower performance,selective achieves lower performance,0.7229872345924377
translation,95,153,results,lower performance,than,our model,lower performance than our model,0.5672698020935059
translation,95,153,results,gap,becomes,larger,gap becomes larger,0.7101464867591858
translation,95,153,results,larger,when on,longer context,larger when on longer context,0.6276381611824036
translation,95,153,results,results,has,selective,results has selective,0.4494645893573761
translation,95,156,results,models,considering,global context ( selective and our ),models considering global context ( selective and our ),0.7010859847068787
translation,95,156,results,global context ( selective and our ),achieve,better results,global context ( selective and our ) achieve better results,0.5995973348617554
translation,95,156,results,better results,than,han,better results than han,0.6066626310348511
translation,95,156,results,results,found,models,results found models,0.6269660592079163
translation,95,157,results,consistently better,than,selective,consistently better than selective,0.6087579727172852
translation,95,157,results,consistently better,especially on,shorter and longer documents,consistently better especially on shorter and longer documents,0.64319908618927
translation,95,167,results,pre method,is,more robust,pre method is more robust,0.5956811904907227
translation,95,167,results,more robust,to,parsing accuracy,more robust to parsing accuracy,0.5124295949935913
translation,95,167,results,results,has,pre method,results has pre method,0.5916682481765747
translation,95,179,results,performance,on,discourse phenomena,performance on discourse phenomena,0.5639654397964478
translation,95,179,results,discourse phenomena,over,context-agnostic base model,discourse phenomena over context-agnostic base model,0.6224440336227417
translation,95,179,results,all the context- aware models,has,comprehensively improve,all the context- aware models has comprehensively improve,0.5403974652290344
translation,95,179,results,comprehensively improve,has,performance,comprehensively improve has performance,0.5312461256980896
translation,95,179,results,results,has,all the context- aware models,results has all the context- aware models,0.5370990633964539
translation,95,181,results,our model,achieves,best accuracy,our model achieves best accuracy,0.6548623442649841
translation,95,181,results,best accuracy,on,all tasks,best accuracy on all tasks,0.5107018947601318
translation,95,181,results,prior contextaware models,has,our model,prior contextaware models has our model,0.522473156452179
translation,95,181,results,results,Compared to,prior contextaware models,results Compared to prior contextaware models,0.6439435482025146
translation,95,183,results,outperforms,over,two points,outperforms over two points,0.7098223567008972
translation,95,183,results,others,over,two points,others over two points,0.7246538400650024
translation,95,183,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,95,183,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,95,183,results,results,has,our model,results has our model,0.5871725678443909
translation,95,192,results,our model,with,"only source graphs ( i.e. , w/ o tgt -g )","our model with only source graphs ( i.e. , w/ o tgt -g )",0.6179618239402771
translation,95,192,results,our model,is,consistently better,our model is consistently better,0.5893471837043762
translation,95,192,results,consistently better,than,base model,consistently better than base model,0.6042589545249939
translation,95,192,results,base model,on,all tasks,base model on all tasks,0.5371084809303284
translation,95,192,results,results,found that,our model,results found that our model,0.7129281759262085
translation,95,195,results,combination,brings,significant improvement,combination brings significant improvement,0.6244816184043884
translation,95,195,results,significant improvement,verifying,complementary,significant improvement verifying complementary,0.7961894273757935
translation,95,195,results,results,has,combination,results has combination,0.5242655873298645
translation,95,202,results,han model,translating,?  ,han model translating ?  ,0.7189298272132874
translation,95,202,results,han model,into,migel,han model into migel,0.5834038257598877
translation,95,202,results,our system,translating,?  ,our system translating ?  ,0.7638013362884521
translation,95,202,results,our system,into,migel,our system into migel,0.6430859565734863
translation,95,202,results,?  ,into,migel,?   into migel,0.6002470254898071
translation,95,202,results,more effective capability,of handling,consistency,more effective capability of handling consistency,0.7067123055458069
translation,95,202,results,consistency,in,long-distance context,consistency in long-distance context,0.5224173069000244
translation,95,202,results,han model,has,our system,han model has our system,0.6278722286224365
translation,95,202,results,results,Compared with,han model,results Compared with han model,0.6880877614021301
translation,96,82,ablation-analysis,encoder layers,perform,word translation,encoder layers perform word translation,0.5859384536743164
translation,96,82,ablation-analysis,translation,starts at,embedding layer,translation starts at embedding layer,0.6944262981414795
translation,96,82,ablation-analysis,embedding layer,with,unexpectedly high accuracy,embedding layer with unexpectedly high accuracy,0.6431331038475037
translation,96,83,ablation-analysis,improvements,brought about by,different layers,improvements brought about by different layers,0.7189821004867554
translation,96,83,ablation-analysis,different layers,are,relatively similar,different layers are relatively similar,0.5965784788131714
translation,96,83,ablation-analysis,stacking,has,of encoder layers,stacking has of encoder layers,0.5909759998321533
translation,96,83,ablation-analysis,stacking,has,word translation accuracy,stacking has word translation accuracy,0.5689365267753601
translation,96,83,ablation-analysis,of encoder layers,has,word translation accuracy,of encoder layers has word translation accuracy,0.49221089482307434
translation,96,83,ablation-analysis,word translation accuracy,has,improves,word translation accuracy has improves,0.6309917569160461
translation,96,83,ablation-analysis,ablation analysis,With,stacking,ablation analysis With stacking,0.643512487411499
translation,96,121,ablation-analysis,acceleration,of,trading decoder layers,acceleration of trading decoder layers,0.5756362080574036
translation,96,121,ablation-analysis,trading decoder layers,for,encoder layers,trading decoder layers for encoder layers,0.6162357926368713
translation,96,121,ablation-analysis,encoder layers,in,training,encoder layers in training,0.5204901099205017
translation,96,121,ablation-analysis,small,in,decoding,small in decoding,0.6012976765632629
translation,96,121,ablation-analysis,decoding,is,significant,decoding is significant,0.637800395488739
translation,96,121,ablation-analysis,ablation analysis,shows,acceleration,ablation analysis shows acceleration,0.6143938899040222
translation,96,125,ablation-analysis,biggest relative jump,in,translation quality,biggest relative jump in translation quality,0.486209511756897
translation,96,125,ablation-analysis,5 to 4,has,decoder layers,5 to 4 has decoder layers,0.5851835012435913
translation,96,125,ablation-analysis,ablation analysis,going from,5 to 4,ablation analysis going from 5 to 4,0.5932112336158752
translation,96,60,hyperparameters,parameters,initialized under,"lipschitz con- straint ( xu et al. , 2020 )","parameters initialized under lipschitz con- straint ( xu et al. , 2020 )",0.7300390005111694
translation,96,60,hyperparameters,"lipschitz con- straint ( xu et al. , 2020 )",to ensure,convergence,"lipschitz con- straint ( xu et al. , 2020 ) to ensure convergence",0.7087857127189636
translation,96,60,hyperparameters,convergence,of,deep encoders,convergence of deep encoders,0.5966482758522034
translation,96,60,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,96,62,hyperparameters,joint byte-pair encoding ( bpe ),with,32 k merge operations,joint byte-pair encoding ( bpe ) with 32 k merge operations,0.6271583437919617
translation,96,62,hyperparameters,hyperparameters,applied,joint byte-pair encoding ( bpe ),hyperparameters applied joint byte-pair encoding ( bpe ),0.6767284870147705
translation,96,65,hyperparameters,number of warm - up steps,set to,8 k,number of warm - up steps set to 8 k,0.7177543640136719
translation,96,65,hyperparameters,hyperparameters,has,number of warm - up steps,hyperparameters has number of warm - up steps,0.5215746164321899
translation,96,67,hyperparameters,model,trained for,100k training steps,model trained for 100k training steps,0.7500414252281189
translation,96,67,hyperparameters,100k training steps,with,around 25 k target tokens,100k training steps with around 25 k target tokens,0.6362006664276123
translation,96,67,hyperparameters,around 25 k target tokens,in,each batch,around 25 k target tokens in each batch,0.5557751655578613
translation,96,67,hyperparameters,hyperparameters,trained for,100k training steps,hyperparameters trained for 100k training steps,0.6601889133453369
translation,96,67,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,96,21,model,probing approaches,measure,word translation accuracy,probing approaches measure word translation accuracy,0.64472496509552
translation,96,21,model,word translation accuracy,of,output representations,word translation accuracy of output representations,0.5649430751800537
translation,96,21,model,output representations,of,individual transformer layers,output representations of individual transformer layers,0.5590354800224304
translation,96,21,model,model,adopt,probing approaches,model adopt probing approaches,0.709713339805603
translation,96,21,model,model,measure,word translation accuracy,model measure word translation accuracy,0.63401198387146
translation,96,59,model,source embedding matrix,weight matrix of,classifier,source embedding matrix weight matrix of classifier,0.777144730091095
translation,96,59,model,model,has,source embedding matrix,model has source embedding matrix,0.526337742805481
translation,96,59,model,model,has,target embedding matrix,model has target embedding matrix,0.5348515510559082
translation,96,61,model,approaches,based on,neutron implementation,approaches based on neutron implementation,0.7293141484260559
translation,96,61,model,neutron implementation,of,transformer translation model,neutron implementation of transformer translation model,0.5554012656211853
translation,96,61,model,model,implemented,approaches,model implemented approaches,0.729866623878479
translation,96,84,results,"shallow decoder layers ( 0 , 1 , 2 and 3 )",perform,significantly worse,"shallow decoder layers ( 0 , 1 , 2 and 3 ) perform significantly worse",0.6188429594039917
translation,96,84,results,significantly worse,compared to,corresponding encoder layers,significantly worse compared to corresponding encoder layers,0.6527891159057617
translation,96,84,results,improvements,brought about by,different decoder layers,improvements brought about by different decoder layers,0.6593366265296936
translation,96,84,results,different decoder layers,are,quite different,different decoder layers are quite different,0.5779951810836792
translation,96,84,results,decoder layers,has,"shallow decoder layers ( 0 , 1 , 2 and 3 )","decoder layers has shallow decoder layers ( 0 , 1 , 2 and 3 )",0.566592812538147
translation,96,84,results,results,analyzing,decoder layers,results analyzing decoder layers,0.6330638527870178
translation,96,91,results,both the source embedding layer and the last encoder layer,resulted in,same accuracy,both the source embedding layer and the last encoder layer resulted in same accuracy,0.6442838907241821
translation,96,91,results,same accuracy,of,23.66,same accuracy of 23.66,0.5596083998680115
translation,96,91,results,results,has,both the source embedding layer and the last encoder layer,results has both the source embedding layer and the last encoder layer,0.552204430103302
translation,96,122,results,transformer,with,10 encoder layers and 2 decoder layers,transformer with 10 encoder layers and 2 decoder layers,0.6711861491203308
translation,96,122,results,10 encoder layers and 2 decoder layers,is,2.32 times as fast,10 encoder layers and 2 decoder layers is 2.32 times as fast,0.5105700492858887
translation,96,122,results,10 encoder layers and 2 decoder layers,achieving,slightly higher bleu,10 encoder layers and 2 decoder layers achieving slightly higher bleu,0.577238142490387
translation,96,122,results,2.32 times as fast,as,6 - layer transformer,2.32 times as fast as 6 - layer transformer,0.597726583480835
translation,96,122,results,results,has,transformer,results has transformer,0.4226538836956024
translation,96,123,results,more than 12 encoder layers,with,shallow decoder,more than 12 encoder layers with shallow decoder,0.6193810701370239
translation,96,123,results,18 - 4 model,brings about,+ 1.42 bleu improvements,18 - 4 model brings about + 1.42 bleu improvements,0.599190890789032
translation,96,123,results,+ 1.42 bleu improvements,over,strong baseline,+ 1.42 bleu improvements over strong baseline,0.5801310539245605
translation,96,123,results,1.38 times as fast,in,decoding,1.38 times as fast in decoding,0.5298516750335693
translation,96,123,results,results,Can we use,more than 12 encoder layers,results Can we use more than 12 encoder layers,0.6399913430213928
translation,96,124,results,18 - 4 model,to,8 - 4 model,18 - 4 model to 8 - 4 model,0.6221718788146973
translation,96,124,results,time cost,for using,10 more encoder layers,time cost for using 10 more encoder layers,0.652151882648468
translation,96,124,results,10 more encoder layers,increases,1 second,10 more encoder layers increases 1 second,0.6911630630493164
translation,96,124,results,1 second,for translating,test set,1 second for translating test set,0.6705933213233948
translation,96,124,results,18 - 4 model,has,time cost,18 - 4 model has time cost,0.5391177535057068
translation,96,124,results,8 - 4 model,has,time cost,8 - 4 model has time cost,0.553445041179657
translation,96,124,results,results,Comparing,18 - 4 model,results Comparing 18 - 4 model,0.6905580163002014
translation,96,126,results,more than 18 encoder layers,still bring,improvements,more than 18 encoder layers still bring improvements,0.6831928491592407
translation,96,126,results,gains,are,relatively small,gains are relatively small,0.5736674070358276
translation,96,128,results,more encoder layers,with,fewer but sufficient decoder layers,more encoder layers with fewer but sufficient decoder layers,0.6292716264724731
translation,96,128,results,more encoder layers,good choice in,distribution of encoder and decoder layers ( 18 - 4 ),more encoder layers good choice in distribution of encoder and decoder layers ( 18 - 4 ),0.7631446719169617
translation,96,128,results,decoding speed,with,small gains,decoding speed with small gains,0.6448042988777161
translation,96,128,results,small gains,in,translation quality,small gains in translation quality,0.5258239507675171
translation,96,128,results,significantly boost,has,decoding speed,significantly boost has decoding speed,0.5686250329017639
translation,96,128,results,results,show,more encoder layers,results show more encoder layers,0.6337175965309143
translation,96,128,results,results,using,more encoder layers,results using more encoder layers,0.6458418369293213
translation,96,128,results,results,good choice in,distribution of encoder and decoder layers ( 18 - 4 ),results good choice in distribution of encoder and decoder layers ( 18 - 4 ),0.7602181434631348
translation,97,11,baselines,data cleaning,with,"opusfilter ( aulamo et al. , 2020 )","data cleaning with opusfilter ( aulamo et al. , 2020 )",0.5882338881492615
translation,97,11,baselines,data cleaning,with,iterative training,data cleaning with iterative training,0.6108664274215698
translation,97,11,baselines,transfer learning,has,"aji et al. , 2020 ; zoph et al. , 2016 )","transfer learning has aji et al. , 2020 ; zoph et al. , 2016 )",0.5479309558868408
translation,97,11,baselines,back - translation,has,"sennrich et al. , 2016 a","back - translation has sennrich et al. , 2016 a",0.5439990162849426
translation,97,12,experimental-setup,nmt models,trained with,"fairseq ( ott et al. , 2019 )","nmt models trained with fairseq ( ott et al. , 2019 )",0.7302265167236328
translation,97,12,experimental-setup,first iteration,of,back -translation,first iteration of back -translation,0.5863063931465149
translation,97,12,experimental-setup,back -translation,generated with,"moses ( koehn et al. , 2007 )","back -translation generated with moses ( koehn et al. , 2007 )",0.6568037271499634
translation,97,12,experimental-setup,experimental setup,has,nmt models,experimental setup has nmt models,0.5188751220703125
translation,97,33,experimental-setup,nmt models,follow,base transformer architecture,nmt models follow base transformer architecture,0.5568185448646545
translation,97,33,experimental-setup,nmt models,using,relu,nmt models using relu,0.6863664388656616
translation,97,33,experimental-setup,nmt models,using,"adam ( kingma and ba , 2015 )","nmt models using adam ( kingma and ba , 2015 )",0.6573023796081543
translation,97,33,experimental-setup,relu,as,activation function,relu as activation function,0.5622032880783081
translation,97,33,experimental-setup,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,97,34,experimental-setup,inverse square root learning rate scheduling,with,peak value,inverse square root learning rate scheduling with peak value,0.6634523868560791
translation,97,34,experimental-setup,peak value,of,1e ?3,peak value of 1e ?3,0.622124969959259
translation,97,34,experimental-setup,experimental setup,set,inverse square root learning rate scheduling,experimental setup set inverse square root learning rate scheduling,0.6437078714370728
translation,97,35,experimental-setup,learning rate warmup stage,for,4000 updates,learning rate warmup stage for 4000 updates,0.5936200022697449
translation,97,35,experimental-setup,4000 updates,with,initial learning rate,4000 updates with initial learning rate,0.6193731427192688
translation,97,35,experimental-setup,initial learning rate,of,1e ?7,initial learning rate of 1e ?7,0.622451901435852
translation,97,35,experimental-setup,experimental setup,used,learning rate warmup stage,experimental setup used learning rate warmup stage,0.5874466300010681
translation,97,36,experimental-setup,dropout probability,set to,0.2,dropout probability set to 0.2,0.7079029083251953
translation,97,36,experimental-setup,attention dropout probability,set to,0.1,attention dropout probability set to 0.1,0.6897047758102417
translation,97,36,experimental-setup,experimental setup,has,dropout probability,experimental setup has dropout probability,0.5022250413894653
translation,97,36,experimental-setup,experimental setup,has,attention dropout probability,experimental setup has attention dropout probability,0.4772140681743622
translation,97,37,experimental-setup,label smoothing,with,value,label smoothing with value,0.6401965022087097
translation,97,37,experimental-setup,value,of,0.1,value of 0.1,0.5788514018058777
translation,97,40,experimental-setup,pb - smt models,trained with,default settings,pb - smt models trained with default settings,0.739699125289917
translation,97,40,experimental-setup,default settings,with,"moses ( koehn et al. , 2007 ) toolkit","default settings with moses ( koehn et al. , 2007 ) toolkit",0.5730658173561096
translation,97,40,experimental-setup,experimental setup,has,pb - smt models,experimental setup has pb - smt models,0.5453530550003052
translation,97,41,experimental-setup,5,-,gram operation sequence model,5 - gram operation sequence model,0.5891268253326416
translation,97,41,experimental-setup,5,has,gram operation sequence model,5 has gram operation sequence model,0.49055275321006775
translation,97,41,experimental-setup,experimental setup,trained,5,experimental setup trained 5,0.6172112226486206
translation,97,42,experimental-setup,language models,are,5 - gram models,language models are 5 - gram models,0.502548098564148
translation,97,42,experimental-setup,language models,binarized with,kenlm,language models binarized with kenlm,0.7496738433837891
translation,97,42,experimental-setup,experimental setup,has,language models,experimental setup has language models,0.4941139817237854
translation,97,31,model,transfer learning,from,high- resource language pair ( german- english ),transfer learning from high- resource language pair ( german- english ),0.5515236258506775
translation,97,31,model,transfer learning,from,iterative training,transfer learning from iterative training,0.5874980092048645
translation,97,31,model,transfer learning,from,back - translation,transfer learning from back - translation,0.5723715424537659
translation,97,31,model,model,combine,transfer learning,model combine transfer learning,0.7133166193962097
translation,97,15,results,vanilla nmt baselines,in terms of,"bleu ( pap-ineni et al. , 2002 )","vanilla nmt baselines in terms of bleu ( pap-ineni et al. , 2002 )",0.6316070556640625
translation,97,15,results,final submissions,has,significantly outperform,final submissions has significantly outperform,0.5741362571716309
translation,97,15,results,significantly outperform,has,vanilla nmt baselines,significantly outperform has vanilla nmt baselines,0.585830807685852
translation,97,15,results,results,has,final submissions,results has final submissions,0.5179483294487
translation,97,48,results,pb - smt,performs,significantly better,pb - smt performs significantly better,0.6576249003410339
translation,97,48,results,significantly better,than,nmt,significantly better than nmt,0.5945131182670593
translation,97,48,results,significantly better,achieved,+ 1.8 bleu score,significantly better achieved + 1.8 bleu score,0.6731187701225281
translation,97,48,results,significantly better,achieved,+ 0.7,significantly better achieved + 0.7,0.6725499629974365
translation,97,48,results,+ 1.8 bleu score,on,hausa ? english,+ 1.8 bleu score on hausa ? english,0.49246689677238464
translation,97,48,results,+ 0.7,on,english ? hausa,+ 0.7 on english ? hausa,0.5069747567176819
translation,97,73,results,slight improvements,on,both test sets,slight improvements on both test sets,0.5020260214805603
translation,97,73,results,differences,are,insignificant,differences are insignificant,0.5943230390548706
translation,97,73,results,results,obtained,slight improvements,results obtained slight improvements,0.6780529618263245
translation,98,2,experiments,t4t solution,has,wmt21 similar language task,t4t solution has wmt21 similar language task,0.5522851347923279
translation,98,65,results,bleu scores,for,corpus test data,bleu scores for corpus test data,0.5425562858581543
translation,98,65,results,corpus test data,in,all ntm models ( lstm or transformer ),corpus test data in all ntm models ( lstm or transformer ),0.48529550433158875
translation,98,65,results,all ntm models ( lstm or transformer ),have,better,all ntm models ( lstm or transformer ) have better,0.5715749859809875
translation,98,65,results,all ntm models ( lstm or transformer ),been,better,all ntm models ( lstm or transformer ) been better,0.5722138285636902
translation,98,65,results,syllabic segmentation,has,bleu scores,syllabic segmentation has bleu scores,0.544465184211731
translation,98,65,results,results,Using,syllabic segmentation,results Using syllabic segmentation,0.6219703555107117
translation,99,81,ablation-analysis,filtered 40 % pseudo- refs,achieve,best results,filtered 40 % pseudo- refs achieve best results,0.6214011311531067
translation,99,81,ablation-analysis,best results,except,k = 9,best results except k = 9,0.6697012782096863
translation,99,81,ablation-analysis,ablation analysis,has,filtered 40 % pseudo- refs,ablation analysis has filtered 40 % pseudo- refs,0.5995012521743774
translation,99,82,ablation-analysis,significant reduce,especially for,smaller k,significant reduce especially for smaller k,0.7088258862495422
translation,99,82,ablation-analysis,k-anticipation rate,compared with,original training references,k-anticipation rate compared with original training references,0.6267899870872498
translation,99,82,ablation-analysis,generated pseudo- refs,has,significant reduce,generated pseudo- refs has significant reduce,0.5871564149856567
translation,99,82,ablation-analysis,significant reduce,has,k-anticipation rate,significant reduce has k-anticipation rate,0.5858185291290283
translation,99,82,ablation-analysis,ablation analysis,shows,generated pseudo- refs,ablation analysis shows generated pseudo- refs,0.6711501479148865
translation,99,128,ablation-analysis,pseudo,-,refs,pseudo - refs,0.6985032558441162
translation,99,128,ablation-analysis,substan - tially,improves,translation quality,substan - tially improves translation quality,0.702941358089447
translation,99,128,ablation-analysis,pseudo,has,refs,pseudo has refs,0.6792911291122437
translation,99,128,ablation-analysis,pseudo,has,substan - tially,pseudo has substan - tially,0.615890622138977
translation,99,128,ablation-analysis,refs,has,substan - tially,refs has substan - tially,0.6485397219657898
translation,99,128,ablation-analysis,ablation analysis,Compared with,orig- inal references only,ablation analysis Compared with orig- inal references only,0.6123831272125244
translation,99,75,experimental-setup,"fast align ( dyer et al. , 2013 )",as,word aligner,"fast align ( dyer et al. , 2013 ) as word aligner",0.46077755093574524
translation,99,75,experimental-setup,"fast align ( dyer et al. , 2013 )",train it on,training set,"fast align ( dyer et al. , 2013 ) train it on training set",0.6446726322174072
translation,99,75,experimental-setup,word aligner,Model 2 for,anticipation,word aligner Model 2 for anticipation,0.7703438997268677
translation,99,75,experimental-setup,model 1,for,hallucination,model 1 for hallucination,0.6287489533424377
translation,99,76,experimental-setup,datasets,tokenized with,"bpe ( sennrich et al. , 2016 )","datasets tokenized with bpe ( sennrich et al. , 2016 )",0.7325127124786377
translation,99,76,experimental-setup,experimental setup,has,datasets,experimental setup has datasets,0.47152194380760193
translation,99,7,model,novel method,rewrites,target side,novel method rewrites target side,0.7152765393257141
translation,99,7,model,target side,of,existing fullsentence corpora,target side of existing fullsentence corpora,0.5410701632499695
translation,99,7,model,existing fullsentence corpora,into,simultaneous -style translation,existing fullsentence corpora into simultaneous -style translation,0.5691050291061401
translation,99,7,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,99,20,results,our pseudo-references,lead to,substantial improvements,our pseudo-references lead to substantial improvements,0.6948229074478149
translation,99,20,results,substantial improvements,up to,+ 2.7 bleu,substantial improvements up to + 2.7 bleu,0.5970443487167358
translation,99,20,results,substantial improvements,on,zh!en and ja! en simultaneous translation,substantial improvements on zh!en and ja! en simultaneous translation,0.5595373511314392
translation,99,20,results,results,has,our pseudo-references,results has our pseudo-references,0.54876708984375
translation,100,18,experiments,online learning algorithms,with,theoretical performance guarantees,online learning algorithms with theoretical performance guarantees,0.5622503757476807
translation,100,18,experiments,online learning algorithms,frameworks of,prediction,online learning algorithms frameworks of prediction,0.6573602557182312
translation,100,18,experiments,prediction,has,with expert advice,prediction has with expert advice,0.5859758257865906
translation,100,95,experiments,en-de,was,language pair,en-de was language pair,0.7052984237670898
translation,100,95,experiments,language pair,for which,our approach,language pair for which our approach,0.5418175458908081
translation,100,95,experiments,language pair,appears to be,least successful,language pair appears to be least successful,0.7094850540161133
translation,100,7,model,online learning,given,ensemble of machine translation systems,online learning given ensemble of machine translation systems,0.6207792162895203
translation,100,7,model,dynamically converges,to,best systems,dynamically converges to best systems,0.611009418964386
translation,100,7,model,dynamically converges,by taking advantage of,human feedback available,dynamically converges by taking advantage of human feedback available,0.6729888916015625
translation,100,7,model,best systems,by taking advantage of,human feedback available,best systems by taking advantage of human feedback available,0.6379045248031616
translation,100,7,model,model,novel application of,online learning,model novel application of online learning,0.6156569123268127
translation,100,17,model,"ensemble of competing , independent mt systems",dynamically find,best ones,"ensemble of competing , independent mt systems dynamically find best ones",0.6640135645866394
translation,100,17,model,best ones,while making,most of existing human feedback,best ones while making most of existing human feedback,0.6550374031066895
translation,100,17,model,model,dynamically find,best ones,model dynamically find best ones,0.74210524559021
translation,100,44,model,online learning problem,under,two different frameworks,online learning problem under two different frameworks,0.5816965699195862
translation,100,44,model,prediction,with,expert advice,prediction with expert advice,0.6451883912086487
translation,100,44,model,expert advice,using,ewaf,expert advice using ewaf,0.669398844242096
translation,100,44,model,ewaf,as,learning algorithm,ewaf as learning algorithm,0.5957430005073547
translation,100,44,model,ewaf,as,learning algorithm,ewaf as learning algorithm,0.5957430005073547
translation,100,44,model,ewaf,as,learning algorithm,ewaf as learning algorithm,0.5957430005073547
translation,100,44,model,multi-armed bandits,using,exp3,multi-armed bandits using exp3,0.6820868253707886
translation,100,44,model,exp3,as,learning algorithm,exp3 as learning algorithm,0.6004406809806824
translation,100,19,results,human effort,by immediately incorporating,human feedback,human effort by immediately incorporating human feedback,0.63712078332901
translation,100,19,results,human feedback,to dynamically converge,best systems,human feedback to dynamically converge best systems,0.7163552641868591
translation,100,19,results,wmt'19 news translation test sets,show,approaches,wmt'19 news translation test sets show approaches,0.6158237457275391
translation,100,19,results,reduce,has,human effort,reduce has human effort,0.5549442768096924
translation,100,19,results,results,contribute with,online mt ensemble,results contribute with online mt ensemble,0.6932239532470703
translation,100,98,results,our online approach,converges to,top 3 systems (,our online approach converges to top 3 systems (,0.755452036857605
translation,100,98,results,converges,to,best system,converges to best system,0.5947750210762024
translation,100,98,results,best system,when using,ewaf,best system when using ewaf,0.7079218029975891
translation,100,98,results,ewaf,with,human-comet,ewaf with human-comet,0.7039837837219238
translation,100,98,results,fr-de,has,our online approach,fr-de has our online approach,0.6570804715156555
translation,100,98,results,results,For,fr-de,results For fr-de,0.7180945873260498
translation,100,117,results,our approach,using,ewaf,our approach using ewaf,0.7054611444473267
translation,100,117,results,our approach,using,human-comet,our approach using human-comet,0.7193672060966492
translation,100,117,results,our approach,converges to,best system,our approach converges to best system,0.6888250708580017
translation,100,117,results,our approach,using,human-comet,our approach using human-comet,0.7193672060966492
translation,100,117,results,ewaf,with,human- zero,ewaf with human- zero,0.6949047446250916
translation,100,117,results,subset,within,just 10 iterations,subset within just 10 iterations,0.7070798873901367
translation,100,117,results,of the top 3,within,just 10 iterations,of the top 3 within just 10 iterations,0.6571716666221619
translation,100,117,results,subset,has,of the top 3,subset has of the top 3,0.5820280909538269
translation,100,117,results,results,using,human-comet,results using human-comet,0.6287021636962891
translation,101,9,model,general methodology,for,adversarial testing,general methodology for adversarial testing,0.5745529532432556
translation,101,9,model,adversarial testing,of,qe,adversarial testing of qe,0.587875247001648
translation,101,9,model,qe,for,mt,qe for mt,0.6932433247566223
translation,101,9,model,model,proposing,general methodology,model proposing general methodology,0.6852999925613403
translation,101,19,model,two types of changes,to,high-quality mt outputs,two types of changes to high-quality mt outputs,0.5329166054725647
translation,101,19,model,model,introduce,two types of changes,model introduce two types of changes,0.6446331739425659
translation,101,13,results,results,has,qe,results has qe,0.44085800647735596
translation,101,24,results,sota qe models,fail to properly detect,certain types of maps,sota qe models fail to properly detect certain types of maps,0.7613728046417236
translation,101,24,results,certain types of maps,such,negation omission,certain types of maps such negation omission,0.6337051391601562
translation,101,24,results,results,has,sota qe models,results has sota qe models,0.5677229762077332
translation,101,25,results,probing experiments,on,set of qe models,probing experiments on set of qe models,0.5755394101142883
translation,101,25,results,set of qe models,consistent with,correlation,set of qe models consistent with correlation,0.6716696619987488
translation,101,25,results,correlation,with,human judgements,correlation with human judgements,0.6468774080276489
translation,102,10,ablation-analysis,entire heads and feedforward connections,in,12 - 1 encoder- decoder architecture,entire heads and feedforward connections in 12 - 1 encoder- decoder architecture,0.5261625647544861
translation,102,10,ablation-analysis,entire heads and feedforward connections,gains,additional 51 % speed - up,entire heads and feedforward connections gains additional 51 % speed - up,0.722882866859436
translation,102,10,ablation-analysis,ablation analysis,Pruning,entire heads and feedforward connections,ablation analysis Pruning entire heads and feedforward connections,0.7762371301651001
translation,102,36,ablation-analysis,findings,possible to prune,entire nodes,findings possible to prune entire nodes,0.7390275001525879
translation,102,36,ablation-analysis,entire nodes,from,feedforward layers,entire nodes from feedforward layers,0.535929799079895
translation,102,36,ablation-analysis,entire nodes,resulting in,pareto optimal architectures ( quality vs speed ),entire nodes resulting in pareto optimal architectures ( quality vs speed ),0.6450673937797546
translation,102,36,ablation-analysis,feedforward layers,early during,training,feedforward layers early during training,0.7244749665260315
translation,102,189,ablation-analysis,two -thirds,of,all feedforward parameters,two -thirds of all feedforward parameters,0.6179271340370178
translation,102,189,ablation-analysis,two -thirds,with,- 0.2 bleu,two -thirds with - 0.2 bleu,0.655597448348999
translation,102,189,ablation-analysis,two -thirds,with,+ 34 % speed- up,two -thirds with + 34 % speed- up,0.6905040144920349
translation,102,189,ablation-analysis,ablation analysis,remove,two -thirds,ablation analysis remove two -thirds,0.6721674799919128
translation,102,243,ablation-analysis,head - lasso left attention layers,focusing on removing,connections,head - lasso left attention layers focusing on removing connections,0.6449058055877686
translation,102,243,ablation-analysis,connections,from,feedforward layers,connections from feedforward layers,0.5217405557632446
translation,102,243,ablation-analysis,head - lasso left attention layers,has,almost completely unpruned,head - lasso left attention layers has almost completely unpruned,0.5430641770362854
translation,102,243,ablation-analysis,ablation analysis,has,head - lasso left attention layers,ablation analysis has head - lasso left attention layers,0.5063614249229431
translation,102,124,experimental-setup,shared vocabulary,of,"32,000 subword units","shared vocabulary of 32,000 subword units",0.5851346254348755
translation,102,124,experimental-setup,"32,000 subword units",generated by,senten-cepiece,"32,000 subword units generated by senten-cepiece",0.6961485743522644
translation,102,124,experimental-setup,translate,using,shortlists,translate using shortlists,0.7096545696258545
translation,102,124,experimental-setup,shortlists,of,top 50 words,shortlists of top 50 words,0.5322938561439514
translation,102,130,experimental-setup,dynamic batching,filling,10gb workspace,dynamic batching filling 10gb workspace,0.6560918092727661
translation,102,130,experimental-setup,10gb workspace,on,4,10gb workspace on 4,0.5684331655502319
translation,102,130,experimental-setup,10gb workspace,on,gpus,10gb workspace on gpus,0.49894487857818604
translation,102,130,experimental-setup,10gb workspace,resulting in,"about 71,000 words per batch","10gb workspace resulting in about 71,000 words per batch",0.6782000064849854
translation,102,130,experimental-setup,10gb workspace,resulting in,"about 46,000 words per batch","10gb workspace resulting in about 46,000 words per batch",0.6795514822006226
translation,102,130,experimental-setup,"about 71,000 words per batch",in,  6 - 2tied   student,"about 71,000 words per batch in   6 - 2tied   student",0.5783659815788269
translation,102,130,experimental-setup,"about 71,000 words per batch",in,  6 - 6   student,"about 71,000 words per batch in   6 - 6   student",0.5857739448547363
translation,102,130,experimental-setup,"about 46,000 words per batch",in,  6 - 6   student,"about 46,000 words per batch in   6 - 6   student",0.5845434069633484
translation,102,132,experimental-setup,experimental setup,use,adam optimiser,experimental setup use adam optimiser,0.5980468988418579
translation,102,137,experimental-setup,our models,using,marian nmt toolkit,our models using marian nmt toolkit,0.6578928232192993
translation,102,137,experimental-setup,experimental setup,trained and decoded,our models,experimental setup trained and decoded our models,0.7482398152351379
translation,102,145,experimental-setup,pretrain,for,25 k batches,pretrain for 25 k batches,0.6564862132072449
translation,102,145,experimental-setup,experimental setup,has,pretrain,experimental setup has pretrain,0.5277845859527588
translation,102,146,experimental-setup,regulariser,for,250k batches,regulariser for 250k batches,0.6630145311355591
translation,102,146,experimental-setup,experimental setup,Train with,regulariser,experimental setup Train with regulariser,0.7124441862106323
translation,102,241,experimental-setup,models,pretrained for,50 k updates,models pretrained for 50 k updates,0.7901819348335266
translation,102,241,experimental-setup,models,regularised for,150k,models regularised for 150k,0.7937105298042297
translation,102,241,experimental-setup,models,after,sliced,models after sliced,0.7072232961654663
translation,102,241,experimental-setup,models,trained until,convergence,models trained until convergence,0.7589464783668518
translation,102,241,experimental-setup,150k,after,models,150k after models,0.7357648611068726
translation,102,241,experimental-setup,models,trained until,convergence,models trained until convergence,0.7589464783668518
translation,102,241,experimental-setup,experimental setup,regularised for,150k,experimental setup regularised for 150k,0.7174972891807556
translation,102,241,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,102,283,experimental-setup,speed,measured on,single core cpu,speed measured on single core cpu,0.5831253528594971
translation,102,283,experimental-setup,single core cpu,with,mini-batch,single core cpu with mini-batch,0.6499319672584534
translation,102,283,experimental-setup,mini-batch,of,32,mini-batch of 32,0.6207697987556458
translation,102,283,experimental-setup,experimental setup,has,speed,experimental setup has speed,0.5131431221961975
translation,102,138,experiments,quality and speed,on,1 cpu core,quality and speed on 1 cpu core,0.5327480435371399
translation,102,123,model,8 heads,in,each layer,8 heads in each layer,0.587588906288147
translation,102,123,model,8 heads,replaced by,faster ssru,8 heads replaced by faster ssru,0.7265046238899231
translation,102,123,model,each layer,except for,decoder self-attention,each layer except for decoder self-attention,0.6398647427558899
translation,102,123,model,attention,has,8 heads,attention has 8 heads,0.6433486342430115
translation,102,123,model,model,has,attention,model has attention,0.587727427482605
translation,102,237,model,rowcol - lasso,regularised,individual connections,rowcol - lasso regularised individual connections,0.7700656056404114
translation,102,237,model,rowcol - lasso,removed,entire attention head,rowcol - lasso removed entire attention head,0.7382689118385315
translation,102,237,model,entire attention head,if,at least half of its connections are dead,entire attention head if at least half of its connections are dead,0.6200147867202759
translation,102,237,model,model,has,rowcol - lasso,model has rowcol - lasso,0.560528039932251
translation,102,9,results,6 decoder layers,with,2 tied decoder layers,6 decoder layers with 2 tied decoder layers,0.6290076971054077
translation,102,9,results,pruned model,is,34 % faster,pruned model is 34 % faster,0.5538317561149597
translation,102,9,results,pruned model,with,2 tied decoder layers,pruned model with 2 tied decoder layers,0.6171466112136841
translation,102,9,results,pruned model,is,14 % faster,pruned model is 14 % faster,0.5522508025169373
translation,102,9,results,pruned model,is,14 % faster,pruned model is 14 % faster,0.5522508025169373
translation,102,9,results,6 decoder layers,has,pruned model,6 decoder layers has pruned model,0.5623075366020203
translation,102,9,results,6 decoder layers,has,pruned model,6 decoder layers has pruned model,0.5623075366020203
translation,102,9,results,2 tied decoder layers,has,pruned model,2 tied decoder layers has pruned model,0.5602938532829285
translation,102,9,results,results,With,6 decoder layers,results With 6 decoder layers,0.6045830845832825
translation,102,9,results,results,with,2 tied decoder layers,results with 2 tied decoder layers,0.6173657178878784
translation,102,12,results,our pruned and quantised models,are,1.9- 2.7 ? faster,our pruned and quantised models are 1.9- 2.7 ? faster,0.5779132843017578
translation,102,12,results,1.9- 2.7 ? faster,at,cost,1.9- 2.7 ? faster at cost,0.5323171019554138
translation,102,12,results,0.9- 1.7 bleu,in comparison to,unoptimised baselines,0.9- 1.7 bleu in comparison to unoptimised baselines,0.6281024813652039
translation,102,12,results,wmt 2021 efficiency task,has,our pruned and quantised models,wmt 2021 efficiency task has our pruned and quantised models,0.5849795341491699
translation,102,12,results,cost,has,0.9- 1.7 bleu,cost has 0.9- 1.7 bleu,0.5706537961959839
translation,102,12,results,results,In,wmt 2021 efficiency task,results In wmt 2021 efficiency task,0.42551741003990173
translation,102,37,results,entire heads,results in,even faster models,entire heads results in even faster models,0.63206547498703
translation,102,37,results,results,pruning,entire heads,results pruning entire heads,0.6868512630462646
translation,102,40,results,6 - layered decoder,being,34 % faster,6 - layered decoder being 34 % faster,0.5863056778907776
translation,102,40,results,34 % faster,at,cost,34 % faster at cost,0.5500469207763672
translation,102,40,results,cost,of,0.2 bleu,cost of 0.2 bleu,0.5841479301452637
translation,102,40,results,cost,of,0.3 bleu,cost of 0.3 bleu,0.5757436156272888
translation,102,40,results,12 - 1 encoder- decoder ratio,gaining,additional 51 % speed - up,12 - 1 encoder- decoder ratio gaining additional 51 % speed - up,0.7137134671211243
translation,102,40,results,additional 51 % speed - up,costing,0.3 bleu,additional 51 % speed - up costing 0.3 bleu,0.6722862720489502
translation,102,41,results,pruning,combined with,quantisation,pruning combined with quantisation,0.734894335269928
translation,102,41,results,quantisation,gives,significant speed boost,quantisation gives significant speed boost,0.6180965900421143
translation,102,177,results,same models,achieve,noticeably worse translation quality,same models achieve noticeably worse translation quality,0.5874318480491638
translation,102,177,results,noticeably worse translation quality,trained from,get -go,noticeably worse translation quality trained from get -go,0.7085445523262024
translation,102,177,results,noticeably worse translation quality,in comparison to,careful pruning,noticeably worse translation quality in comparison to careful pruning,0.6074549555778503
translation,102,177,results,results,has,same models,results has same models,0.5820687413215637
translation,102,188,results,regularised models,are,better translation quality,regularised models are better translation quality,0.5420153141021729
translation,102,188,results,regularised models,of,better translation quality,regularised models of better translation quality,0.5470216274261475
translation,102,188,results,better translation quality,than,same architectures,better translation quality than same architectures,0.5787005424499512
translation,102,188,results,results,has,regularised models,results has regularised models,0.5107675194740295
translation,102,200,results,reinitialised models,on,full dataset,reinitialised models on full dataset,0.5549896359443665
translation,102,200,results,reinitialised models,achieve,comparable quality,reinitialised models achieve comparable quality,0.5949565172195435
translation,102,200,results,comparable quality,to,pruned counterparts,comparable quality to pruned counterparts,0.5756553411483765
translation,102,200,results,results,has,reinitialised models,results has reinitialised models,0.5217652916908264
translation,102,211,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,102,211,results,results,In terms of,quality and speed- up,results In terms of quality and speed- up,0.697363018989563
translation,102,220,results,- 0.1 to - 0.3 bleu difference,compared to,same architecture,- 0.1 to - 0.3 bleu difference compared to same architecture,0.6614115834236145
translation,102,220,results,same architecture,trained from,scratch,same architecture trained from scratch,0.7530162334442139
translation,102,220,results,results,has,both pruning methods,results has both pruning methods,0.49647361040115356
translation,102,248,results,our pruned models,are,1.2- 1.7 ? faster,our pruned models are 1.2- 1.7 ? faster,0.592879593372345
translation,102,248,results,1.2- 1.7 ? faster,cost of,0.6- 1.3 bleu,1.2- 1.7 ? faster cost of 0.6- 1.3 bleu,0.6643247604370117
translation,102,248,results,latest testset wmt21,has,our pruned models,latest testset wmt21 has our pruned models,0.6011045575141907
translation,102,248,results,results,Evaluating on,latest testset wmt21,results Evaluating on latest testset wmt21,0.6991573572158813
translation,103,115,ablation-analysis,ran -e and ran - d,are,effective,ran -e and ran - d are effective,0.6468862891197205
translation,103,115,ablation-analysis,ran -e and ran - d,find that,effects,ran -e and ran - d find that effects,0.6463854908943176
translation,103,115,ablation-analysis,effective,find that,effects,effective find that effects,0.6458119750022888
translation,103,115,ablation-analysis,effects,can,accumulated,effects can accumulated,0.6521708369255066
translation,103,115,ablation-analysis,effects,not be,accumulated,effects not be accumulated,0.600302517414093
translation,103,145,ablation-analysis,local windows,to,current position,local windows to current position,0.5245977640151978
translation,103,145,ablation-analysis,weights,of,decoder,weights of decoder,0.6023997068405151
translation,103,145,ablation-analysis,weights,most concentrated in,first token,weights most concentrated in first token,0.6611914038658142
translation,103,145,ablation-analysis,decoder,most concentrated in,first token,decoder most concentrated in first token,0.6922059059143066
translation,103,145,ablation-analysis,first token,of,target sequences,first token of target sequences,0.6046112179756165
translation,103,145,ablation-analysis,local windows,has,weights,local windows has weights,0.5888363718986511
translation,103,145,ablation-analysis,ablation analysis,except attending,local windows,ablation analysis except attending local windows,0.7363404631614685
translation,103,152,ablation-analysis,attention similarity,high especially in,decoder,attention similarity high especially in decoder,0.6773349046707153
translation,103,152,ablation-analysis,ran - all,has,attention similarity,ran - all has attention similarity,0.5850846767425537
translation,103,157,ablation-analysis,encoder positional embeddings,leads to,catastrophic performance degradation,encoder positional embeddings leads to catastrophic performance degradation,0.6427496671676636
translation,103,157,ablation-analysis,catastrophic performance degradation,over,14 points,catastrophic performance degradation over 14 points,0.7181743383407593
translation,103,157,ablation-analysis,ablation analysis,Removing,encoder positional embeddings,ablation analysis Removing encoder positional embeddings,0.7187159061431885
translation,103,159,ablation-analysis,affected marginally,by removing,decoder position embeddings,affected marginally by removing decoder position embeddings,0.7672656178474426
translation,103,159,ablation-analysis,ablation analysis,has,transf,ablation analysis has transf,0.49273890256881714
translation,103,164,ablation-analysis,fixed initial attention matrices,lead to,significant performance degradation,fixed initial attention matrices lead to significant performance degradation,0.6745360493659973
translation,103,164,ablation-analysis,significant performance degradation,has,- 0.1 ? - 0.2 bleu ),significant performance degradation has - 0.1 ? - 0.2 bleu ),0.5608357787132263
translation,103,164,ablation-analysis,ablation analysis,find that,fixed initial attention matrices,ablation analysis find that fixed initial attention matrices,0.6661195755004883
translation,103,166,ablation-analysis,layer normalization and residual connection,leads to,performance drop,layer normalization and residual connection leads to performance drop,0.6762721538543701
translation,103,166,ablation-analysis,ablation analysis,removing,layer normalization and residual connection,ablation analysis removing layer normalization and residual connection,0.7266016006469727
translation,103,78,baselines,standard transformer,has,transf for short ),standard transformer has transf for short ),0.5931966304779053
translation,103,94,experimental-setup,joint vocabulary,of,40 k tokens,joint vocabulary of 40 k tokens,0.5634180903434753
translation,103,94,experimental-setup,joint vocabulary,of,32 k tokens,joint vocabulary of 32 k tokens,0.563021719455719
translation,103,94,experimental-setup,joint vocabulary,of,32 k tokens,joint vocabulary of 32 k tokens,0.563021719455719
translation,103,94,experimental-setup,40 k tokens,for,"en- de , en - fr language pairs","40 k tokens for en- de , en - fr language pairs",0.6366397738456726
translation,103,94,experimental-setup,32 k tokens,for,others,32 k tokens for others,0.6663542985916138
translation,103,94,experimental-setup,separate vocabulary,of,32 k tokens,separate vocabulary of 32 k tokens,0.5889261960983276
translation,103,94,experimental-setup,32 k tokens,for,zh-en,32 k tokens for zh-en,0.6509467959403992
translation,103,94,experimental-setup,experimental setup,use,joint vocabulary,experimental setup use joint vocabulary,0.6285953521728516
translation,103,94,experimental-setup,experimental setup,use,separate vocabulary,experimental setup use separate vocabulary,0.6368352770805359
translation,103,95,experimental-setup,standard base implementation,consists of,6 - layer encoder,standard base implementation consists of 6 - layer encoder,0.6195279359817505
translation,103,95,experimental-setup,standard base implementation,consists of,6 - layer decoder,standard base implementation consists of 6 - layer decoder,0.6234704852104187
translation,103,95,experimental-setup,of transformer,consists of,6 - layer encoder,of transformer consists of 6 - layer encoder,0.6937368512153625
translation,103,95,experimental-setup,standard base implementation,has,of transformer,standard base implementation has of transformer,0.5836915373802185
translation,103,95,experimental-setup,experimental setup,use,standard base implementation,experimental setup use standard base implementation,0.6047086119651794
translation,103,96,experimental-setup,"2,048 hidden units",in,ffn sub-layers,"2,048 hidden units in ffn sub-layers",0.499615877866745
translation,103,96,experimental-setup,experimental setup,set,d k =d v = 512,experimental setup set d k =d v = 512,0.6940962076187134
translation,103,96,experimental-setup,experimental setup,use,"2,048 hidden units","experimental setup use 2,048 hidden units",0.5792829990386963
translation,103,97,experimental-setup,residual dropout,is,0.1,residual dropout is 0.1,0.5665555000305176
translation,103,97,experimental-setup,experimental setup,has,residual dropout,experimental setup has residual dropout,0.5163313746452332
translation,103,98,experimental-setup,rans,set,dropout,rans set dropout,0.7247830629348755
translation,103,98,experimental-setup,of attention,as,0.2,of attention as 0.2,0.4793875217437744
translation,103,98,experimental-setup,of attention,to avoid,over-fitting,of attention to avoid over-fitting,0.6629563570022583
translation,103,98,experimental-setup,over-fitting,except on,en- fr.,over-fitting except on en- fr.,0.7095858454704285
translation,103,98,experimental-setup,dropout,has,of attention,dropout has of attention,0.6107827425003052
translation,103,100,experimental-setup,150k steps,except,wmt14 ( 250 k steps ),150k steps except wmt14 ( 250 k steps ),0.6516848802566528
translation,103,101,experimental-setup,training,performed using,8 x v100 gpus,training performed using 8 x v100 gpus,0.615960955619812
translation,103,101,experimental-setup,8 x v100 gpus,for,all language pairs,8 x v100 gpus for all language pairs,0.5694183111190796
translation,103,101,experimental-setup,all language pairs,except,en-tr,all language pairs except en-tr,0.6999046206474304
translation,103,101,experimental-setup,all language pairs,except,zh-en,all language pairs except zh-en,0.6779855489730835
translation,103,101,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,103,102,experimental-setup,decoding,use,beam width,decoding use beam width,0.6661527156829834
translation,103,102,experimental-setup,decoding,use,length penalty,decoding use length penalty,0.6408089399337769
translation,103,102,experimental-setup,decoding,use,length penalty,decoding use length penalty,0.6408089399337769
translation,103,102,experimental-setup,beam width,of,4,beam width of 4,0.7052165865898132
translation,103,102,experimental-setup,length penalty,of,0.6,length penalty of 0.6,0.5868892073631287
translation,103,102,experimental-setup,length penalty,of,1.0,length penalty of 1.0,0.572574257850647
translation,103,102,experimental-setup,length penalty,of,1.0,length penalty of 1.0,0.572574257850647
translation,103,102,experimental-setup,0.6,for,wmt tasks,0.6 for wmt tasks,0.5793153047561646
translation,103,102,experimental-setup,length penalty,of,1.0,length penalty of 1.0,0.572574257850647
translation,103,102,experimental-setup,1.0,for,zh-en task,1.0 for zh-en task,0.6476368308067322
translation,103,102,experimental-setup,experimental setup,When,decoding,experimental setup When decoding,0.6526182889938354
translation,103,176,experimental-setup,dialog generation,segment,all dialog,dialog generation segment all dialog,0.7329251170158386
translation,103,176,experimental-setup,dialog generation,train,small transformer,dialog generation train small transformer,0.71764075756073
translation,103,176,experimental-setup,all dialog,with,bert tokenizer,all dialog with bert tokenizer,0.654532253742218
translation,103,176,experimental-setup,all dialog,train,small transformer,all dialog train small transformer,0.7526925206184387
translation,103,176,experimental-setup,small transformer,for,20 k steps,small transformer for 20 k steps,0.6416231393814087
translation,103,176,experimental-setup,experimental setup,For,dialog generation,experimental setup For dialog generation,0.515299916267395
translation,103,175,experiments,300k steps,on,2 gpus,300k steps on 2 gpus,0.5114872455596924
translation,103,175,experiments,2 gpus,with,batch size,2 gpus with batch size,0.5867742300033569
translation,103,175,experiments,batch size,of,128 sentences,batch size of 128 sentences,0.582125723361969
translation,103,5,model,novel substitute mechanism,for,self-attention,novel substitute mechanism for self-attention,0.5846378207206726
translation,103,5,model,model,propose,novel substitute mechanism,model propose novel substitute mechanism,0.6805750727653503
translation,103,6,model,ran,directly learns,attention weights,ran directly learns attention weights,0.7449758052825928
translation,103,6,model,ran,further improves,capacity,ran further improves capacity,0.7532410621643066
translation,103,6,model,attention weights,without,token - to -token interaction,attention weights without token - to -token interaction,0.7028823494911194
translation,103,6,model,capacity,by,layer - to - layer interaction,capacity by layer - to - layer interaction,0.6126176118850708
translation,103,6,model,model,has,ran,model has ran,0.6570456027984619
translation,103,22,model,model,propose,novel attention mechanism,model propose novel attention mechanism,0.6772408485412598
translation,103,23,model,ran,starts with,unnormalized initial attention matrix,ran starts with unnormalized initial attention matrix,0.7587962746620178
translation,103,23,model,unnormalized initial attention matrix,for,each head,unnormalized initial attention matrix for each head,0.5785038471221924
translation,103,23,model,unnormalized initial attention matrix,randomly initialized and trained together with,other model parameters,unnormalized initial attention matrix randomly initialized and trained together with other model parameters,0.7285038232803345
translation,103,23,model,model,has,ran,model has ran,0.6570456027984619
translation,103,24,model,recurrent transition module,takes,initial attention matrices,recurrent transition module takes initial attention matrices,0.5969698429107666
translation,103,24,model,recurrent transition module,refines,layer -wise interaction,recurrent transition module refines layer -wise interaction,0.7160828709602356
translation,103,24,model,initial attention matrices,as,input,initial attention matrices as input,0.5159159302711487
translation,103,24,model,layer -wise interaction,between,adjacent layers,layer -wise interaction between adjacent layers,0.7002946734428406
translation,103,24,model,model,introduce,recurrent transition module,model introduce recurrent transition module,0.66997891664505
translation,103,51,model,ran,consists of,set of global initial attention matrices,ran consists of set of global initial attention matrices,0.6737245321273804
translation,103,51,model,ran,consists of,recurrent transition module,ran consists of recurrent transition module,0.7168024182319641
translation,103,51,model,model,has,ran,model has ran,0.6570456027984619
translation,103,7,results,ran models,are,competitive,ran models are competitive,0.6540158987045288
translation,103,7,results,transformer counterpart,in,certain scenarios,transformer counterpart in certain scenarios,0.5564292669296265
translation,103,7,results,outperform,has,transformer counterpart,outperform has transformer counterpart,0.615100622177124
translation,103,106,results,transf,on,all datasets,transf on all datasets,0.5760054588317871
translation,103,106,results,our ran models,consistently yield,competitive or even better results,our ran models consistently yield competitive or even better results,0.7298393249511719
translation,103,106,results,competitive or even better results,against,transf,competitive or even better results against transf,0.653386652469635
translation,103,106,results,competitive or even better results,on,all datasets,competitive or even better results on all datasets,0.4853612184524536
translation,103,106,results,transf,on,all datasets,transf on all datasets,0.5760054588317871
translation,103,106,results,transf,has,our ran models,transf has our ran models,0.6684854030609131
translation,103,106,results,results,Compared with,transf,results Compared with transf,0.5701215863227844
translation,103,107,results,more average bleu / sacrebleu,achieved by,"ran -e , ran -d and ran - all","more average bleu / sacrebleu achieved by ran -e , ran -d and ran - all",0.6639501452445984
translation,103,107,results,"0.13/0.16 , 0.48/0.44 and 0.16/0.22",has,more average bleu / sacrebleu,"0.13/0.16 , 0.48/0.44 and 0.16/0.22 has more average bleu / sacrebleu",0.5934012532234192
translation,103,110,results,biggest performance gain,comes from,low resource translation task tr?en,biggest performance gain comes from low resource translation task tr?en,0.6725314855575562
translation,103,110,results,low resource translation task tr?en,where,ran -d,low resource translation task tr?en where ran -d,0.6488240361213684
translation,103,110,results,outperform,by,0.97/1.0 bleu / sacrebleu points,outperform by 0.97/1.0 bleu / sacrebleu points,0.6083222031593323
translation,103,110,results,transf,by,0.97/1.0 bleu / sacrebleu points,transf by 0.97/1.0 bleu / sacrebleu points,0.6040836572647095
translation,103,110,results,ran -d,has,outperform,ran -d has outperform,0.6836810111999512
translation,103,110,results,outperform,has,transf,outperform has transf,0.6625919342041016
translation,103,110,results,results,has,biggest performance gain,results has biggest performance gain,0.5826330780982971
translation,103,118,results,performance,over,transf,performance over transf,0.7441555857658386
translation,103,118,results,our model,bridges,performance gap,our model bridges performance gap,0.6435248255729675
translation,103,118,results,performance gap,between,transformer,performance gap between transformer,0.6572087407112122
translation,103,118,results,performance gap,between,models,performance gap between models,0.6494503617286682
translation,103,118,results,two methods,has,significantly decrease,two methods has significantly decrease,0.5516585111618042
translation,103,118,results,significantly decrease,has,performance,significantly decrease has performance,0.5606427788734436
translation,103,123,results,decoding,by,up to 23.6 %,decoding by up to 23.6 %,0.619659423828125
translation,103,123,results,batch size,of,100,batch size of 100,0.6914901733398438
translation,103,123,results,ran - all,has,speedups,ran - all has speedups,0.5876652598381042
translation,103,123,results,speedups,has,decoding,speedups has decoding,0.5896772742271423
translation,103,123,results,results,see that,ran - all,results see that ran - all,0.6267233490943909
translation,103,124,results,beam size,shows,consistent improvement,beam size shows consistent improvement,0.7322629690170288
translation,103,124,results,ran - all,shows,consistent improvement,ran - all shows consistent improvement,0.7175887823104858
translation,103,124,results,consistent improvement,about,1.2x,consistent improvement about 1.2x,0.6250798106193542
translation,103,124,results,beam size,has,ran - all,beam size has ran - all,0.6292664408683777
translation,103,124,results,results,In terms of,beam size,results In terms of beam size,0.711293637752533
translation,103,160,results,ran,to,decoder,ran to decoder,0.6169179677963257
translation,103,160,results,ran,obtain,even better performance,ran obtain even better performance,0.6394344568252563
translation,103,160,results,even better performance,than,transf,even better performance than transf,0.6414793133735657
translation,103,160,results,results,After applying,ran,results After applying ran,0.5703859329223633
translation,103,169,results,rans,yield,better performance,rans yield better performance,0.7328763604164124
translation,103,169,results,rans,yield,not good,rans yield not good,0.7082310914993286
translation,103,169,results,better performance,on,short and medium-length sentences,better performance on short and medium-length sentences,0.5190218091011047
translation,103,169,results,not good,at processing,long sentences,not good at processing long sentences,0.665137767791748
translation,103,169,results,results,observe,rans,results observe rans,0.47205156087875366
translation,103,170,results,ran -e,performs,worse,ran -e performs worse,0.7534507513046265
translation,103,170,results,worse,on,long source sentences,worse on long source sentences,0.5321509838104248
translation,103,170,results,long source sentences,than,transf,long source sentences than transf,0.5670452117919922
translation,103,170,results,results,has,ran -e,results has ran -e,0.5930079817771912
translation,104,165,ablation-analysis,two methods,has,only a few indicators,two methods has only a few indicators,0.533687949180603
translation,104,165,ablation-analysis,ablation analysis,combine,two methods,ablation analysis combine two methods,0.6641188859939575
translation,104,172,ablation-analysis,ablation analysis,has,model transfer and translation - based model,ablation analysis has model transfer and translation - based model,0.5257769227027893
translation,104,37,baselines,pgn - adapter pgn - adapter,used for,merging,pgn - adapter pgn - adapter used for merging,0.6568834781646729
translation,104,37,baselines,different languages,to,same space,different languages to same space,0.5961073040962219
translation,104,37,baselines,same space,by,adapter,same space by adapter,0.5977712273597717
translation,104,37,baselines,pgn methods,with,bert,pgn methods with bert,0.6521063446998596
translation,104,37,baselines,merging,has,different languages,merging has different languages,0.592033326625824
translation,104,37,baselines,baselines,has,pgn - adapter pgn - adapter,baselines has pgn - adapter pgn - adapter,0.5883362889289856
translation,104,142,baselines,pgn - adapter,incorporates,richer language information,pgn - adapter incorporates richer language information,0.742165744304657
translation,104,142,baselines,pgn - adapter model,integrates,pgn method,pgn - adapter model integrates pgn method,0.6923397779464722
translation,104,142,baselines,pgn - adapter model,incorporates,richer language information,pgn - adapter model incorporates richer language information,0.7067885398864746
translation,104,142,baselines,pgn method,in,adapter,pgn method in adapter,0.5747489929199219
translation,104,142,baselines,richer language information,from,external given language type,richer language information from external given language type,0.5374234914779663
translation,104,142,baselines,pgn - adapter,has,pgn - adapter model,pgn - adapter has pgn - adapter model,0.5964804887771606
translation,104,142,baselines,adapter,has,pgn - adapter model,adapter has pgn - adapter model,0.5916040539741516
translation,104,142,baselines,baselines,has,pgn - adapter,baselines has pgn - adapter,0.6064591407775879
translation,104,104,experimental-setup,experimental setup,use,pre-trained multilingual bert - base ( cased version ),experimental setup use pre-trained multilingual bert - base ( cased version ),0.5676572322845459
translation,104,124,experimental-setup,implementation,of,models,implementation of models,0.5811580419540405
translation,104,124,experimental-setup,models,are,pytorch,models are pytorch,0.6364953517913818
translation,104,124,experimental-setup,models,are,gpu device,models are gpu device,0.5390753149986267
translation,104,124,experimental-setup,pytorch,with,version 1.4,pytorch with version 1.4,0.6004320383071899
translation,104,124,experimental-setup,gpu device,with,v100 ( 32g ),gpu device with v100 ( 32g ),0.6129342317581177
translation,104,124,experimental-setup,experimental setup,has,implementation,experimental setup has implementation,0.5296319723129272
translation,104,126,experimental-setup,output hidden size,of,bilstm,output hidden size of bilstm,0.559516191482544
translation,104,126,experimental-setup,output hidden size,of,bilstm,output hidden size of bilstm,0.559516191482544
translation,104,126,experimental-setup,output hidden size,of,bilstm,output hidden size of bilstm,0.559516191482544
translation,104,126,experimental-setup,bilstm,to,200,bilstm to 200,0.6685654520988464
translation,104,126,experimental-setup,layer number,of,bilstm,layer number of bilstm,0.5539517998695374
translation,104,126,experimental-setup,bilstm,to,3,bilstm to 3,0.6775710582733154
translation,104,126,experimental-setup,experimental setup,set,output hidden size,experimental setup set output hidden size,0.683332085609436
translation,104,126,experimental-setup,experimental setup,set,layer number,experimental setup set layer number,0.6334757804870605
translation,104,127,experimental-setup,overfitting,set,dropout rate,overfitting set dropout rate,0.6095472574234009
translation,104,127,experimental-setup,dropout rate,to,0.33,dropout rate to 0.33,0.5135328769683838
translation,104,127,experimental-setup,experimental setup,To prevent,overfitting,experimental setup To prevent overfitting,0.6143673658370972
translation,104,128,experimental-setup,pgn - adapter,set,language embedding size,pgn - adapter set language embedding size,0.6460554003715515
translation,104,128,experimental-setup,pgn - adapter,set,adapter size,pgn - adapter set adapter size,0.6571248769760132
translation,104,128,experimental-setup,pgn - adapter,set,language embedding dropout rate,pgn - adapter set language embedding dropout rate,0.5928080081939697
translation,104,128,experimental-setup,pgn - adapter,in,adapter size,pgn - adapter in adapter size,0.5042494535446167
translation,104,128,experimental-setup,language embedding size,in,"[ 16 , 32 , 50 , 64 ]","language embedding size in [ 16 , 32 , 50 , 64 ]",0.49791887402534485
translation,104,128,experimental-setup,language embedding size,in,adapter size,language embedding size in adapter size,0.4955461025238037
translation,104,128,experimental-setup,language embedding size,in,"[ 128 , 256 , 512 ]","language embedding size in [ 128 , 256 , 512 ]",0.517017126083374
translation,104,128,experimental-setup,language embedding size,in,language embedding dropout rate,language embedding size in language embedding dropout rate,0.4565081000328064
translation,104,128,experimental-setup,language embedding size,in,0.1,language embedding size in 0.1,0.530998945236206
translation,104,128,experimental-setup,language embedding size,in,"[ 128 , 256 , 512 ]","language embedding size in [ 128 , 256 , 512 ]",0.517017126083374
translation,104,128,experimental-setup,language embedding size,in,0.1,language embedding size in 0.1,0.530998945236206
translation,104,128,experimental-setup,adapter size,in,"[ 128 , 256 , 512 ]","adapter size in [ 128 , 256 , 512 ]",0.5399026870727539
translation,104,128,experimental-setup,adapter size,in,language embedding dropout rate,adapter size in language embedding dropout rate,0.4732695519924164
translation,104,128,experimental-setup,adapter size,in,0.1,adapter size in 0.1,0.538637101650238
translation,104,128,experimental-setup,language embedding dropout rate,is,0.1,language embedding dropout rate is 0.1,0.52064049243927
translation,104,129,experimental-setup,online training,to learn,model parameters,online training to learn model parameters,0.6020264029502869
translation,104,129,experimental-setup,online training,use,adam optimizer,online training use adam optimizer,0.5968154072761536
translation,104,129,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,104,129,experimental-setup,learning rate,has,0.002,learning rate has 0.002,0.5364621877670288
translation,104,129,experimental-setup,experimental setup,exploit,online training,experimental setup exploit online training,0.6995442509651184
translation,104,130,experimental-setup,adapter module learning rate,is,5e - 6,adapter module learning rate is 5e - 6,0.5683832764625549
translation,104,130,experimental-setup,5e - 6,by,adamw optimizer,5e - 6 by adamw optimizer,0.5983667373657227
translation,104,130,experimental-setup,experimental setup,has,adapter module learning rate,experimental setup has adapter module learning rate,0.4927818477153778
translation,104,131,experimental-setup,mini-batch size,set to,32,mini-batch size set to 32,0.7183046340942383
translation,104,131,experimental-setup,parameters,of,model,parameters of model,0.6297455430030823
translation,104,131,experimental-setup,parameters,updated,4 mini-batches,parameters updated 4 mini-batches,0.6299358010292053
translation,104,131,experimental-setup,experimental setup,has,mini-batch size,experimental setup has mini-batch size,0.5507700443267822
translation,104,131,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,104,132,experimental-setup,gradient clipping,by,max norm 1.0,gradient clipping by max norm 1.0,0.564137876033783
translation,104,25,experiments,contextual parameter generator networks ( pgn ),in,multilingual bert with adapter,contextual parameter generator networks ( pgn ) in multilingual bert with adapter,0.5357456803321838
translation,104,179,experiments,automatic alignment setting ( with human translation ),performs,worst,automatic alignment setting ( with human translation ) performs worst,0.6259188652038574
translation,104,179,experiments,worst,among,three configurations,worst among three configurations,0.6450075507164001
translation,104,27,results,unsupervised cross-lingual transfer,from,english corpus,unsupervised cross-lingual transfer from english corpus,0.4990730285644531
translation,104,27,results,translation - based method,better than,model transfer,translation - based method better than model transfer,0.731208324432373
translation,104,27,results,combination,leads to,further improvements,combination leads to further improvements,0.707901656627655
translation,104,27,results,unsupervised cross-lingual transfer,has,translation - based method,unsupervised cross-lingual transfer has translation - based method,0.5465815663337708
translation,104,27,results,results,observe,unsupervised cross-lingual transfer,results observe unsupervised cross-lingual transfer,0.5446438193321228
translation,104,27,results,results,for,unsupervised cross-lingual transfer,results for unsupervised cross-lingual transfer,0.5510483384132385
translation,104,28,results,scale,of,portuguese corpus,scale of portuguese corpus,0.5921828746795654
translation,104,28,results,scale,is,much smaller,scale is much smaller,0.6165682077407837
translation,104,28,results,portuguese corpus,adding it into,multilingual transfer,portuguese corpus adding it into multilingual transfer,0.6268354654312134
translation,104,28,results,outperforms,has,bilingual counterpart,outperforms has bilingual counterpart,0.5993136167526245
translation,104,154,results,results,of,english - chinese transfer,results of english - chinese transfer,0.509979248046875
translation,104,155,results,almost all experiments,using,pgn - adapter model,almost all experiments using pgn - adapter model,0.6818887591362
translation,104,155,results,pgn - adapter model,achieve,best results,pgn - adapter model achieve best results,0.6401330232620239
translation,104,155,results,best results,in,each group,best results in each group,0.5076841711997986
translation,104,155,results,manual translation and annotation,are,available,manual translation and annotation are available,0.5557010769844055
translation,104,155,results,model,combining,all the datasets,model combining all the datasets,0.7065151333808899
translation,104,155,results,model,performs,best,model performs best,0.6411250829696655
translation,104,155,results,all the datasets,performs,best,all the datasets performs best,0.6060807108879089
translation,104,155,results,automatic translation,from,english corpus,automatic translation from english corpus,0.49492955207824707
translation,104,155,results,latter,has,outperforms,latter has outperforms,0.6338182687759399
translation,104,156,results,translationbased method,in favor of,model transfer,translationbased method in favor of model transfer,0.668778657913208
translation,104,156,results,english to chinese,has,translationbased method,english to chinese has translationbased method,0.57070392370224
translation,104,156,results,results,from,english to chinese,results from english to chinese,0.518278181552887
translation,104,166,results,model transfer,benefits from,second source language,model transfer benefits from second source language,0.6285087466239929
translation,104,166,results,model transfer,in,corpus translation part,model transfer in corpus translation part,0.5038226842880249
translation,104,166,results,model transfer,performance is,noticeably decreasing,model transfer performance is noticeably decreasing,0.7003324627876282
translation,104,166,results,noticeably decreasing,low quality of,machine - translated data,noticeably decreasing low quality of machine - translated data,0.7121375799179077
translation,104,171,results,m odelt rans,makes,worse score,m odelt rans makes worse score,0.6082249283790588
translation,104,171,results,m odelt rans,adding,corpust rans,m odelt rans adding corpust rans,0.6696524620056152
translation,104,171,results,worse score,in,long span,worse score in long span,0.5024818181991577
translation,104,171,results,corpust rans,obtains,similar performance,corpust rans obtains similar performance,0.6408482789993286
translation,104,171,results,similar performance,with,humant rans,similar performance with humant rans,0.6960187554359436
translation,104,171,results,results,see,m odelt rans,results see m odelt rans,0.5191138386726379
translation,104,173,results,best performances,achieved for,middle - length spans,best performances achieved for middle - length spans,0.6846816539764404
translation,104,173,results,target,has,best performances,target has best performances,0.5844727754592896
translation,104,173,results,results,For,target,results For target,0.5463796854019165
translation,105,148,results,translation,with,lexicon,translation with lexicon,0.6481634378433228
translation,105,148,results,translation,with,lexicon,translation with lexicon,0.6481634378433228
translation,105,148,results,translation quality,when,context of the input sentences,translation quality when context of the input sentences,0.591616690158844
translation,105,148,results,translation quality,when,relevant inflected forms,translation quality when relevant inflected forms,0.5930476784706116
translation,105,148,results,translation quality,when,relevant inflected forms,translation quality when relevant inflected forms,0.5930476784706116
translation,105,148,results,context of the input sentences,matches,context,context of the input sentences matches context,0.7192739844322205
translation,105,148,results,context of the input sentences,matches,of the lexicon,context of the input sentences matches of the lexicon,0.7232873439788818
translation,105,148,results,relevant inflected forms,are,lexicon,relevant inflected forms are lexicon,0.5715640187263489
translation,105,148,results,relevant inflected forms,present in,lexicon,relevant inflected forms present in lexicon,0.6726236343383789
translation,105,148,results,context,has,of the lexicon,context has of the lexicon,0.552462100982666
translation,105,148,results,results,has,bleu,results has bleu,0.5385738611221313
translation,105,149,results,translation,without,constraints,translation without constraints,0.7189927697181702
translation,105,149,results,translation,in situations,output,translation in situations output,0.7081465721130371
translation,105,149,results,output,marked as,warning,output marked as warning,0.7002419233322144
translation,105,149,results,warning,resulted in,very slight decrease,warning resulted in very slight decrease,0.648406445980072
translation,105,149,results,very slight decrease,in,bleu score,very slight decrease in bleu score,0.5199991464614868
translation,105,149,results,results,Reverting to,translation,results Reverting to translation,0.5345534086227417
translation,105,151,results,manual evaluation results,indicate,our method,manual evaluation results indicate our method,0.5471528768539429
translation,105,151,results,our method,is,very effective,our method is very effective,0.5524094700813293
translation,105,151,results,very effective,in selecting,correct inflected form,very effective in selecting correct inflected form,0.6838529109954834
translation,105,151,results,correct inflected form,of,constraint,correct inflected form of constraint,0.5683777928352356
translation,105,151,results,correct inflected form,in,domain-specific scenario,correct inflected form in domain-specific scenario,0.515681266784668
translation,105,151,results,constraint,in,domain-specific scenario,constraint in domain-specific scenario,0.5126902461051941
translation,105,151,results,results,has,manual evaluation results,results has manual evaluation results,0.5366913676261902
translation,106,102,experimental-setup,low-resource pairs,use,256 - dimensional embeddings,low-resource pairs use 256 - dimensional embeddings,0.6177574396133423
translation,106,102,experimental-setup,low-resource pairs,use,hidden layers,low-resource pairs use hidden layers,0.6246062517166138
translation,106,102,experimental-setup,experimental setup,for,low-resource pairs,experimental setup for low-resource pairs,0.587684690952301
translation,106,103,experimental-setup,midresource pairs,use,512 - dimensonal embeddings,midresource pairs use 512 - dimensonal embeddings,0.6259644627571106
translation,106,103,experimental-setup,midresource pairs,use,hidden layers,midresource pairs use hidden layers,0.5825100541114807
translation,106,103,experimental-setup,experimental setup,for,midresource pairs,experimental setup for midresource pairs,0.5907355546951294
translation,106,104,experimental-setup,high- resource pairs,use,same 512 - dimensonal embedding and hidden layer sizes,high- resource pairs use same 512 - dimensonal embedding and hidden layer sizes,0.6548107862472534
translation,106,104,experimental-setup,same 512 - dimensonal embedding and hidden layer sizes,for,encoder,same 512 - dimensonal embedding and hidden layer sizes for encoder,0.6108105778694153
translation,106,104,experimental-setup,both dimensions,increased to,1024,both dimensions increased to 1024,0.7333666086196899
translation,106,104,experimental-setup,decoder,has,both dimensions,decoder has both dimensions,0.6245055794715881
translation,106,105,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",over,cross-entropy loss,"adam optimizer ( kingma and ba , 2015 ) over cross-entropy loss",0.6172227263450623
translation,106,105,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,maximum learning rate,"adam optimizer ( kingma and ba , 2015 ) with maximum learning rate",0.6005186438560486
translation,106,105,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,minimum,"adam optimizer ( kingma and ba , 2015 ) with minimum",0.6370671391487122
translation,106,105,experimental-setup,cross-entropy loss,with,maximum learning rate,cross-entropy loss with maximum learning rate,0.5744307041168213
translation,106,105,experimental-setup,cross-entropy loss,with,minimum,cross-entropy loss with minimum,0.576273500919342
translation,106,105,experimental-setup,maximum learning rate,of,3 * 10 ?4,maximum learning rate of 3 * 10 ?4,0.6227807402610779
translation,106,105,experimental-setup,warms up,for,first 4800 training steps,warms up for first 4800 training steps,0.6114107370376587
translation,106,105,experimental-setup,decays,after reaching,maximum,decays after reaching maximum,0.7296074032783508
translation,106,105,experimental-setup,experimental setup,trained with,"adam optimizer ( kingma and ba , 2015 )","experimental setup trained with adam optimizer ( kingma and ba , 2015 )",0.7436712384223938
translation,106,106,experimental-setup,training batch size,of,4096,training batch size of 4096,0.5812935829162598
translation,106,107,experimental-setup,perplexity,as,early stopping metric,perplexity as early stopping metric,0.5216950178146362
translation,106,107,experimental-setup,early stopping metric,with,patience,early stopping metric with patience,0.6227810978889465
translation,106,107,experimental-setup,patience,of,5 epochs,patience of 5 epochs,0.6565783619880676
translation,106,107,experimental-setup,experimental setup,use,perplexity,experimental setup use perplexity,0.6135760545730591
translation,106,108,experimental-setup,"dropout ( srivastava et al. , 2014 ) probability",of,0.3,"dropout ( srivastava et al. , 2014 ) probability of 0.3",0.5690189599990845
translation,106,108,experimental-setup,0.3,in,both the encoder and the decoder,0.3 in both the encoder and the decoder,0.5304275751113892
translation,106,108,experimental-setup,experimental setup,set,"dropout ( srivastava et al. , 2014 ) probability","experimental setup set dropout ( srivastava et al. , 2014 ) probability",0.6082283854484558
translation,106,109,experimental-setup,byte pair encoding ( bpe ),with,joint vocabulary size,byte pair encoding ( bpe ) with joint vocabulary size,0.6139854192733765
translation,106,109,experimental-setup,joint vocabulary size,of,4 k and 32 k,joint vocabulary size of 4 k and 32 k,0.632066011428833
translation,106,109,experimental-setup,4 k and 32 k,for,low - and mid / high-resource scenarios,4 k and 32 k for low - and mid / high-resource scenarios,0.6439260840415955
translation,106,109,experimental-setup,experimental setup,apply,byte pair encoding ( bpe ),experimental setup apply byte pair encoding ( bpe ),0.5851911902427673
translation,106,110,experimental-setup,apex,to speed up,training,apex to speed up training,0.7515910863876343
translation,106,111,experimental-setup,experimental setup,trained on,preemptible gpus,experimental setup trained on preemptible gpus,0.6705458760261536
translation,106,149,results,both models,perform,relatively modestly,both models perform relatively modestly,0.6027308106422424
translation,106,149,results,relatively modestly,on,bible and ted talks,relatively modestly on bible and ted talks,0.5809147357940674
translation,106,149,results,bible and ted talks,with,en-tr model,bible and ted talks with en-tr model,0.7024446725845337
translation,106,149,results,en-tr model,slightly better than,ru-tr,en-tr model slightly better than ru-tr,0.6520889401435852
translation,106,160,results,language pairs,with,more than one script,language pairs with more than one script,0.6286415457725525
translation,106,160,results,consistently underperform,both in,automatic and human evaluations,consistently underperform both in automatic and human evaluations,0.6153239607810974
translation,106,160,results,ones where both the source and target language,use,same script,ones where both the source and target language use same script,0.6562028527259827
translation,106,160,results,more than one script,has,consistently underperform,more than one script has consistently underperform,0.5766888856887817
translation,106,160,results,results,has,language pairs,results has language pairs,0.5183674693107605
translation,106,164,results,human evaluations to bleu,Using,direct assessment ( da ) surveys,human evaluations to bleu Using direct assessment ( da ) surveys,0.6960726380348206
translation,106,164,results,human evaluations to bleu,obtain,average scores,human evaluations to bleu obtain average scores,0.5285625457763672
translation,106,164,results,average scores,of,adequacy and fluency,average scores of adequacy and fluency,0.5442733764648438
translation,106,164,results,adequacy and fluency,for,almost all baseline models,adequacy and fluency for almost all baseline models,0.5731196999549866
translation,106,164,results,results,Comparing,human evaluations to bleu,results Comparing human evaluations to bleu,0.6564004421234131
translation,106,175,results,chrf 's correlation,to,adequacy scores,chrf 's correlation to adequacy scores,0.5494420528411865
translation,106,175,results,adequacy scores,is,about the same,adequacy scores is about the same,0.5856778025627136
translation,106,175,results,results,has,chrf 's correlation,results has chrf 's correlation,0.5112237930297852
translation,107,152,ablation-analysis,pegasus ' contextual embedding,on top of,all this,pegasus ' contextual embedding on top of all this,0.6700425744056702
translation,107,152,ablation-analysis,pegasus ' contextual embedding,results in,boost,pegasus ' contextual embedding results in boost,0.6557416319847107
translation,107,152,ablation-analysis,boost,of,.49,boost of .49,0.6559300422668457
translation,107,152,ablation-analysis,ablation analysis,adding,pegasus ' contextual embedding,ablation analysis adding pegasus ' contextual embedding,0.6866867542266846
translation,107,185,ablation-analysis,news commentary v9,with,v15,news commentary v9 with v15,0.6702031493186951
translation,107,185,ablation-analysis,ablation analysis,replace,news commentary v9,ablation analysis replace news commentary v9,0.6587616801261902
translation,107,193,ablation-analysis,document augmentation,helps,documentlevel model,document augmentation helps documentlevel model,0.6113200187683105
translation,107,193,ablation-analysis,documentlevel model,more than,sentence - level model,documentlevel model more than sentence - level model,0.5947472453117371
translation,107,91,baselines,wmt +,replaces,news commentary v9,wmt + replaces news commentary v9,0.722736656665802
translation,107,91,baselines,news commentary v9,with,newer news commentary v15 dataset,news commentary v9 with newer news commentary v15 dataset,0.6988151669502258
translation,107,92,baselines,second augmentation experiment,denoted,wmt + +,second augmentation experiment denoted wmt + +,0.6748305559158325
translation,107,92,baselines,second augmentation experiment,builds on,first,second augmentation experiment builds on first,0.5900288224220276
translation,107,92,baselines,first,by additionally incorporating,tilde rapid 2019 corpus,first by additionally incorporating tilde rapid 2019 corpus,0.6763691902160645
translation,107,92,baselines,baselines,has,second augmentation experiment,baselines has second augmentation experiment,0.5811979174613953
translation,107,178,experiments,bert - fused 8 and multi-context,has,struggle,bert - fused 8 and multi-context has struggle,0.6401776671409607
translation,107,34,hyperparameters,bertfused,use,pretrained masked language models,bertfused use pretrained masked language models,0.5867384076118469
translation,107,34,hyperparameters,pretrained masked language models,to generate,external embed-dings,pretrained masked language models to generate external embed-dings,0.6828015446662903
translation,107,34,hyperparameters,hyperparameters,Like,bertfused,hyperparameters Like bertfused,0.6288252472877502
translation,107,4,model,architecture,for adapting,sentence - level sequence - to-sequence transformer,architecture for adapting sentence - level sequence - to-sequence transformer,0.7077267169952393
translation,107,4,model,sentence - level sequence - to-sequence transformer,by incorporating,multiple pretrained document context signals,sentence - level sequence - to-sequence transformer by incorporating multiple pretrained document context signals,0.6722348928451538
translation,107,4,model,model,propose,architecture,model propose architecture,0.697865903377533
translation,107,33,model,sentence - level model,with,additional source embeddings,sentence - level model with additional source embeddings,0.5720748901367188
translation,107,33,model,sentence - level model,to,document - level model,sentence - level model to document - level model,0.5219014286994934
translation,107,33,model,document - level model,by providing,contextual embeddings,document - level model by providing contextual embeddings,0.604613721370697
translation,107,33,model,model,enhance,sentence - level model,model enhance sentence - level model,0.6351766586303711
translation,107,33,model,model,convert,sentence - level model,model convert sentence - level model,0.6844598650932312
translation,107,183,model,"additional p s ( 3p,3n ) attention block",to,multi-source model,"additional p s ( 3p,3n ) attention block to multi-source model",0.5319518446922302
translation,107,183,model,"additional p s ( 3p,3n ) attention block",train it with,document data,"additional p s ( 3p,3n ) attention block train it with document data",0.6605340838432312
translation,107,183,model,model,add,"additional p s ( 3p,3n ) attention block","model add additional p s ( 3p,3n ) attention block",0.6537108421325684
translation,107,6,results,best multicontext model,has,consistently outperforms,best multicontext model has consistently outperforms,0.6054258346557617
translation,107,6,results,consistently outperforms,has,best existing context - aware transformers,consistently outperforms has best existing context - aware transformers,0.5874577164649963
translation,107,6,results,results,has,best multicontext model,results has best multicontext model,0.5803434252738953
translation,107,23,results,multiple kinds of source language context,improves,performance,multiple kinds of source language context improves performance,0.6632018089294434
translation,107,23,results,performance,of,document translation,performance of document translation,0.6068668961524963
translation,107,23,results,document translation,over,existing contextual representations,document translation over existing contextual representations,0.6414313912391663
translation,107,122,results,multi-embedding model,has,outperforms,multi-embedding model has outperforms,0.6019461750984192
translation,107,122,results,outperforms,has,all the single embedding models,outperforms has all the single embedding models,0.5691657066345215
translation,107,122,results,results,see that,multi-embedding model,results see that multi-embedding model,0.681071400642395
translation,107,124,results,performance,beyond using,source -side context alone,performance beyond using source -side context alone,0.6681381464004517
translation,107,124,results,target - side context,has,does not improve,target - side context has does not improve,0.5903146266937256
translation,107,124,results,does not improve,has,performance,does not improve has performance,0.5884436368942261
translation,107,124,results,results,incorporating,target - side context,results incorporating target - side context,0.6089839339256287
translation,107,141,results,works better,than,only using source context,works better than only using source context,0.5540685653686523
translation,107,141,results,leveraging,has,both source and target context,leveraging has both source and target context,0.5185749530792236
translation,107,141,results,leveraging,has,works better,leveraging has works better,0.6066956520080566
translation,107,141,results,both source and target context,has,works better,both source and target context has works better,0.5974435806274414
translation,107,149,results,improvement,comes from,stronger sentencelevel model,improvement comes from stronger sentencelevel model,0.6229487061500549
translation,107,149,results,stronger sentencelevel model,produced by adding,bert 's encoding,stronger sentencelevel model produced by adding bert 's encoding,0.7302619218826294
translation,107,149,results,bert 's encoding,of,source sentence,bert 's encoding of source sentence,0.5704966187477112
translation,107,149,results,bert 's encoding,has,full 2.25 bleu improvement,bert 's encoding has full 2.25 bleu improvement,0.5455754995346069
translation,107,149,results,results,much of,improvement,results much of improvement,0.6855869293212891
translation,107,151,results,previous sentence,gives,0.44 bleu,previous sentence gives 0.44 bleu,0.5907225012779236
translation,107,153,results,2.45 bleu,to,source embedding enrichment,2.45 bleu to source embedding enrichment,0.5145959258079529
translation,107,153,results,2.45 bleu,to,1.59,2.45 bleu to 1.59,0.5521113276481628
translation,107,153,results,1.59,to,contextual representations,1.59 to contextual representations,0.5282578468322754
translation,107,171,results,fair comparison,finetune,baseline transformer model,fair comparison finetune baseline transformer model,0.7020168304443359
translation,107,171,results,baseline transformer model,on,new data,baseline transformer model on new data,0.5492871999740601
translation,107,171,results,baseline transformer model,improves,performance,baseline transformer model improves performance,0.6932340264320374
translation,107,171,results,performance,by,1.61 bleu,performance by 1.61 bleu,0.5286818146705627
translation,107,171,results,results,To ensure,fair comparison,results To ensure fair comparison,0.6878238320350647
translation,107,173,results,average improvement,of,sentence - level models,average improvement of sentence - level models,0.541894793510437
translation,107,173,results,sentence - level models,is,1.58,sentence - level models is 1.58,0.511648416519165
translation,107,173,results,1.58,versus,1.98,1.58 versus 1.98,0.6149616241455078
translation,107,173,results,1.98,experienced by,document models,1.98 experienced by document models,0.6822705268859863
translation,107,173,results,results,has,average improvement,results has average improvement,0.5582453012466431
translation,108,106,ablation-analysis,encoder adapters,boost,performance,encoder adapters boost performance,0.7614668011665344
translation,108,106,ablation-analysis,performance,in,all directions,performance in all directions,0.5121933817863464
translation,108,106,ablation-analysis,small models,has,encoder adapters,small models has encoder adapters,0.5703763961791992
translation,108,106,ablation-analysis,performance,has,0.3 - 0.4 bleu on average,performance has 0.3 - 0.4 bleu on average,0.5577653646469116
translation,108,106,ablation-analysis,ablation analysis,For,small models,ablation analysis For small models,0.5974486470222473
translation,108,117,ablation-analysis,adapters,to,backbone decoder,adapters to backbone decoder,0.5559076070785522
translation,108,117,ablation-analysis,adapters,to,both encoder and decoder,adapters to both encoder and decoder,0.5685684084892273
translation,108,117,ablation-analysis,adapters,boosts,performance,adapters boosts performance,0.8308290839195251
translation,108,117,ablation-analysis,both encoder and decoder,boosts,performance,both encoder and decoder boosts performance,0.6897401809692383
translation,108,117,ablation-analysis,ablation analysis,Adding,adapters,ablation analysis Adding adapters,0.7430647611618042
translation,108,64,experimental-setup,implementation,based on,fairseq s2t toolkit,implementation based on fairseq s2t toolkit,0.6289113163948059
translation,108,64,experimental-setup,experimental setup,has,implementation,experimental setup has implementation,0.5296319723129272
translation,108,66,experimental-setup,same encoder,with,12 layers,same encoder with 12 layers,0.6778873205184937
translation,108,67,experimental-setup,6 layers,except for,transfer learning scenario,6 layers except for transfer learning scenario,0.6117668747901917
translation,108,67,experimental-setup,mbart decoder,for,initialization,mbart decoder for initialization,0.5324198007583618
translation,108,67,experimental-setup,decoder,has,6 layers,decoder has 6 layers,0.5910302996635437
translation,108,67,experimental-setup,experimental setup,has,decoder,experimental setup has decoder,0.5172298550605774
translation,108,68,experimental-setup,8 k and 10k unigram vocabulary,),bilingual and multilingual models,8 k and 10k unigram vocabulary ) bilingual and multilingual models,0.5780757069587708
translation,108,68,experimental-setup,8 k and 10k unigram vocabulary,for,bilingual and multilingual models,8 k and 10k unigram vocabulary for bilingual and multilingual models,0.5986681580543518
translation,108,68,experimental-setup,experimental setup,used,8 k and 10k unigram vocabulary,experimental setup used 8 k and 10k unigram vocabulary,0.5743988156318665
translation,108,69,experimental-setup,speech features,are,80 - dimensional log mel filter - bank,speech features are 80 - dimensional log mel filter - bank,0.5838589072227478
translation,108,69,experimental-setup,experimental setup,has,speech features,experimental setup has speech features,0.5198966860771179
translation,108,70,experimental-setup,utterances,more than,3000 frames,utterances more than 3000 frames,0.6192132830619812
translation,108,70,experimental-setup,experimental setup,has,utterances,experimental setup has utterances,0.5139247179031372
translation,108,71,experimental-setup,"specaugment ( park et al. , 2019 )",with,lib-rispeech basic ( lb ) policy,"specaugment ( park et al. , 2019 ) with lib-rispeech basic ( lb ) policy",0.6325693726539612
translation,108,71,experimental-setup,lib-rispeech basic ( lb ) policy,for,data augmentation,lib-rispeech basic ( lb ) policy for data augmentation,0.6121187806129456
translation,108,71,experimental-setup,experimental setup,used,"specaugment ( park et al. , 2019 )","experimental setup used specaugment ( park et al. , 2019 )",0.5945714712142944
translation,108,72,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,108,72,experimental-setup,linearly increased,for,first 10 k steps,linearly increased for first 10 k steps,0.6609993577003479
translation,108,72,experimental-setup,first 10 k steps,to,value ? max,first 10 k steps to value ? max,0.5869320631027222
translation,108,72,experimental-setup,learning rate,has,linearly increased,learning rate has linearly increased,0.576456606388092
translation,108,72,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,108,73,experimental-setup,? max,set to,2e ?3,? max set to 2e ?3,0.6971598863601685
translation,108,65,experiments,small transformer model,with,dimension d = 256,small transformer model with dimension d = 256,0.6524359583854675
translation,108,65,experiments,medium one,where,d = 512,medium one where d = 512,0.640205442905426
translation,108,65,experiments,two architectures,has,small transformer model,two architectures has small transformer model,0.5146987438201904
translation,108,87,experiments,medium model ( d = 512 ),performs,better,medium model ( d = 512 ) performs better,0.6309782266616821
translation,108,87,experiments,better,in,multilingual setting,better in multilingual setting,0.5583570599555969
translation,108,87,experiments,better,than,bilingual one,better than bilingual one,0.6279206275939941
translation,108,87,experiments,multilingual setting,than,bilingual one,multilingual setting than bilingual one,0.6089528799057007
translation,108,85,results,small architecture ( d = 256 ),has,bilingual models,small architecture ( d = 256 ) has bilingual models,0.5864248275756836
translation,108,85,results,bilingual models,has,slightly outperform,bilingual models has slightly outperform,0.5919387340545654
translation,108,85,results,slightly outperform,has,multilingual counterpart,slightly outperform has multilingual counterpart,0.5943824648857117
translation,108,85,results,results,For,small architecture ( d = 256 ),results For small architecture ( d = 256 ),0.6067267060279846
translation,108,86,results,performance,of,each language pair,performance of each language pair,0.5808615684509277
translation,108,86,results,performance,of,higher - resource pairs,performance of higher - resource pairs,0.6238135099411011
translation,108,86,results,multilingual model,able to improve,results,multilingual model able to improve results,0.6693877577781677
translation,108,86,results,results,for,"4 out of 8 pairs ( de , nl , pt , ru )","results for 4 out of 8 pairs ( de , nl , pt , ru )",0.5890048146247864
translation,108,86,results,higher - resource pairs,such as,"es , fr , it , and ro","higher - resource pairs such as es , fr , it , and ro",0.6949942111968994
translation,108,86,results,performance,has,multilingual model,performance has multilingual model,0.5502727031707764
translation,108,86,results,each language pair,has,multilingual model,each language pair has multilingual model,0.5668819546699524
translation,108,86,results,joint multilingual training,has,slightly hurts,joint multilingual training has slightly hurts,0.5787239670753479
translation,108,86,results,slightly hurts,has,performance,slightly hurts has performance,0.5844202637672424
translation,108,86,results,results,Looking further into,performance,results Looking further into performance,0.6733388304710388
translation,108,89,results,fine-tuning,recover,lost performance,fine-tuning recover lost performance,0.691621720790863
translation,108,89,results,both recipes,yield,improvements,both recipes yield improvements,0.7376821637153625
translation,108,89,results,improvements,over,multilingual baseline,improvements over multilingual baseline,0.6653763651847839
translation,108,89,results,lost performance,of,higher - resource directions,lost performance of higher - resource directions,0.5835888385772705
translation,108,89,results,higher - resource directions,compared to,bilingual baseline,higher - resource directions compared to bilingual baseline,0.644486129283905
translation,108,89,results,bilingual baseline,for,small model ( d = 256 ),bilingual baseline for small model ( d = 256 ),0.6164447069168091
translation,108,89,results,fine-tuning,has,both recipes,fine-tuning has both recipes,0.5987188220024109
translation,108,89,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,108,90,results,scores,in,all directions,scores in all directions,0.5383617877960205
translation,108,90,results,results,of,best fine-tuning experiment,results of best fine-tuning experiment,0.558368980884552
translation,108,90,results,medium one ( d = 512 ),has,one adapter tuning,medium one ( d = 512 ) has one adapter tuning,0.6228921413421631
translation,108,90,results,slightly improve,has,scores,slightly improve has scores,0.5613951086997986
translation,108,90,results,approach,has,results,approach has results,0.5711806416511536
translation,108,90,results,results,For,medium one ( d = 512 ),results For medium one ( d = 512 ),0.6299903988838196
translation,108,92,results,small models,show,adapter-tuning,small models show adapter-tuning,0.6860585808753967
translation,108,92,results,adapter-tuning,achieved,best performance,adapter-tuning achieved best performance,0.7146487236022949
translation,108,92,results,best performance,producing,clear improvements,best performance producing clear improvements,0.6784597039222717
translation,108,92,results,clear improvements,over,baseline,clear improvements over baseline,0.7202840447425842
translation,108,92,results,clear improvements,especially for,low-resource languages,clear improvements especially for low-resource languages,0.644774854183197
translation,108,92,results,baseline,especially for,low-resource languages,baseline especially for low-resource languages,0.6038416624069214
translation,108,92,results,+ 1.1 bleu,on average,"nl , ro , de , pt","+ 1.1 bleu on average nl , ro , de , pt",0.6880570650100708
translation,108,92,results,+ 1.1 bleu,on average,competitive,+ 1.1 bleu on average competitive,0.7051572203636169
translation,108,92,results,+ 1.1 bleu,on,"nl , ro , de , pt","+ 1.1 bleu on nl , ro , de , pt",0.612813413143158
translation,108,92,results,+ 0.3 bleu,on,"es , fr , ru , it","+ 0.3 bleu on es , fr , ru , it",0.623832643032074
translation,108,92,results,full fine-tuning,+ 0.9 and,0.4 bleu,full fine-tuning + 0.9 and 0.4 bleu,0.6984698176383972
translation,108,92,results,low-resource languages,has,+ 1.1 bleu,low-resource languages has + 1.1 bleu,0.5426129102706909
translation,108,92,results,results,on,small models,results on small models,0.5390697717666626
translation,108,93,results,improvement,is,smaller,improvement is smaller,0.626675009727478
translation,108,93,results,+ 0.4 bleu,on average,lower - resource pairs,+ 0.4 bleu on average lower - resource pairs,0.701565146446228
translation,108,93,results,+ 0.4 bleu,on average,+ 0.1,+ 0.4 bleu on average + 0.1,0.6862229108810425
translation,108,93,results,+ 0.4 bleu,on,lower - resource pairs,+ 0.4 bleu on lower - resource pairs,0.5537758469581604
translation,108,93,results,+ 0.1,on,higherresource ones,+ 0.1 on higherresource ones,0.5680647492408752
translation,108,93,results,larger models,has,improvement,larger models has improvement,0.6014271974563599
translation,108,93,results,smaller,has,+ 0.4 bleu,smaller has + 0.4 bleu,0.5969433188438416
translation,108,93,results,results,For,larger models,results For larger models,0.62461256980896
translation,108,108,results,bottleneck dimension,has,slightly improves,bottleneck dimension has slightly improves,0.6000492572784424
translation,108,108,results,slightly improves,has,performance,slightly improves has performance,0.5808963179588318
translation,108,108,results,results,increasing,bottleneck dimension,results increasing bottleneck dimension,0.6977360248565674
translation,108,109,results,fine-tuning,remains,best option,fine-tuning remains best option,0.6571990847587585
translation,108,109,results,fine-tuning,leads to,much larger model sizes,fine-tuning leads to much larger model sizes,0.6619452834129333
translation,108,109,results,best option,to optimize,models,best option to optimize models,0.6827104091644287
translation,108,109,results,models,in,most cases,models in most cases,0.6046708822250366
translation,108,109,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,109,5,model,position embeddings,of,existing language models,position embeddings of existing language models,0.5170004963874817
translation,109,5,model,effect,on,self-attention,effect on self-attention,0.5686129331588745
translation,109,5,model,model,analyze,position embeddings,model analyze position embeddings,0.6330106854438782
translation,109,7,model,translation - invariant self-attention ( tisa ),accounts for,relative position,translation - invariant self-attention ( tisa ) accounts for relative position,0.6745991110801697
translation,109,7,model,relative position,between,tokens,relative position between tokens,0.6841332316398621
translation,109,7,model,relative position,without needing,conventional position embeddings,relative position without needing conventional position embeddings,0.7819453477859497
translation,109,7,model,interpretable fashion,without needing,conventional position embeddings,interpretable fashion without needing conventional position embeddings,0.7617377638816833
translation,109,7,model,model,propose,translation - invariant self-attention ( tisa ),model propose translation - invariant self-attention ( tisa ),0.6616451144218445
translation,109,100,results,relative error reductions,between,0.4 and 6.5 %,relative error reductions between 0.4 and 6.5 %,0.6268758177757263
translation,109,100,results,0.4 and 6.5 %,when combining,tisa and conventional position embeddings,0.4 and 6.5 % when combining tisa and conventional position embeddings,0.7646709084510803
translation,109,100,results,results,show,relative error reductions,results show relative error reductions,0.6535245776176453
translation,109,105,results,tisa alone,are,not as strong,tisa alone are not as strong,0.6074132919311523
translation,109,105,results,results,for,tisa alone,results for tisa alone,0.6179015636444092
translation,110,120,ablation-analysis,laser,provides,strongest filtering,laser provides strongest filtering,0.6215221881866455
translation,110,120,ablation-analysis,laser,provides,translation results,laser provides translation results,0.5653541684150696
translation,110,120,ablation-analysis,translation results,beating,other two,translation results beating other two,0.6261292099952698
translation,110,120,ablation-analysis,other two,by,0.3 to 0.9 bleu points,other two by 0.3 to 0.9 bleu points,0.557932436466217
translation,110,120,ablation-analysis,ablation analysis,has,laser,ablation analysis has laser,0.5301617980003357
translation,110,90,baselines,baselines,has,feature- based,baselines has feature- based,0.582370936870575
translation,110,18,hyperparameters,nmt training,score,sentences,nmt training score sentences,0.7349411845207214
translation,110,18,hyperparameters,nmt training,create,batches,nmt training create batches,0.6441392302513123
translation,110,18,hyperparameters,batches,using,random weight vectors,batches using random weight vectors,0.7035216093063354
translation,110,18,hyperparameters,hyperparameters,During,nmt training,hyperparameters During nmt training,0.6305584907531738
translation,110,100,hyperparameters,feed -forward network,to tune,weights,feed -forward network to tune weights,0.7506256103515625
translation,110,100,hyperparameters,feed -forward network,trained using,standard sgd,feed -forward network trained using standard sgd,0.6927050352096558
translation,110,100,hyperparameters,standard sgd,using,learning rate,standard sgd using learning rate,0.5969317555427551
translation,110,100,hyperparameters,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,110,100,hyperparameters,weights,has,two 512 - dimensional layers,weights has two 512 - dimensional layers,0.5962526798248291
translation,110,100,hyperparameters,hyperparameters,has,feed -forward network,hyperparameters has feed -forward network,0.5712249875068665
translation,110,101,hyperparameters,grid search,for,weights,grid search for weights,0.6482213735580444
translation,110,101,hyperparameters,weights,done on,"range [?2.5 , 2.5 ]","weights done on range [?2.5 , 2.5 ]",0.6627840995788574
translation,110,101,hyperparameters,"range [?2.5 , 2.5 ]",with,5000 points,"range [?2.5 , 2.5 ] with 5000 points",0.6040924787521362
translation,110,101,hyperparameters,5000 points,has,uniformly distributed,5000 points has uniformly distributed,0.5393290519714355
translation,110,101,hyperparameters,hyperparameters,has,grid search,hyperparameters has grid search,0.5450132489204407
translation,110,102,hyperparameters,number of samples,used for,reward normalization,number of samples used for reward normalization,0.6529654860496521
translation,110,102,hyperparameters,reward normalization,was,3,reward normalization was 3,0.572777509689331
translation,110,102,hyperparameters,window,for computing,delta-perplexity reward,window for computing delta-perplexity reward,0.685343861579895
translation,110,102,hyperparameters,delta-perplexity reward,set to,3,delta-perplexity reward set to 3,0.6694551110267639
translation,110,102,hyperparameters,hyperparameters,has,number of samples,hyperparameters has number of samples,0.5498680472373962
translation,110,102,hyperparameters,hyperparameters,has,window,hyperparameters has window,0.5617576241493225
translation,110,7,model,alternative approach,learns,weights,alternative approach learns weights,0.6379249691963196
translation,110,7,model,weights,for,multiple sentence - level features,weights for multiple sentence - level features,0.5970621705055237
translation,110,7,model,model,presents,alternative approach,model presents alternative approach,0.7076226472854614
translation,110,121,results,"dual cross-entropy ( junczys - dowmunt , 2018 )",is,strongest feature,"dual cross-entropy ( junczys - dowmunt , 2018 ) is strongest feature",0.5203429460525513
translation,110,121,results,"dual cross-entropy ( junczys - dowmunt , 2018 )",matches,performance,"dual cross-entropy ( junczys - dowmunt , 2018 ) matches performance",0.7448078989982605
translation,110,121,results,performance,of,laser,performance of laser,0.6420518159866333
translation,110,121,results,results,Of,five features,results Of five features,0.5798571109771729
translation,110,122,results,source or target language model scores,in,isolation,source or target language model scores in isolation,0.5421189665794373
translation,110,122,results,source or target language model scores,leads to,weakest translation performance,source or target language model scores leads to weakest translation performance,0.628162145614624
translation,110,122,results,isolation,leads to,weakest translation performance,isolation leads to weakest translation performance,0.62367182970047
translation,110,122,results,ibm model 1 scores,perform,only slightly better,ibm model 1 scores perform only slightly better,0.6062396168708801
translation,110,122,results,results,Using,source or target language model scores,results Using source or target language model scores,0.6151552200317383
translation,110,123,results,simple sentence length ratio feature,beats,all other features,simple sentence length ratio feature beats all other features,0.6710871458053589
translation,110,123,results,all other features,except,dual cross-entropy,all other features except dual cross-entropy,0.6332674026489258
translation,110,123,results,dual cross-entropy,by,1.4 to 1.6 bleu points,dual cross-entropy by 1.4 to 1.6 bleu points,0.5606213212013245
translation,110,123,results,results,has,simple sentence length ratio feature,results has simple sentence length ratio feature,0.5333294868469238
translation,110,130,results,bleu scores,for,weight - based approach,bleu scores for weight - based approach,0.6212878227233887
translation,110,130,results,weight - based approach,as,data,weight - based approach as data,0.6208083629608154
translation,110,130,results,data,from,more candidate runs,data from more candidate runs,0.5756022930145264
translation,110,130,results,more candidate runs,added to,tuning stage,more candidate runs added to tuning stage,0.6396659016609192
translation,110,130,results,more candidate runs,filtering,data set,more candidate runs filtering data set,0.720890462398529
translation,110,130,results,tuning stage,for learning,weights,tuning stage for learning weights,0.7198905944824219
translation,110,130,results,tuning stage,filtering,data set,tuning stage filtering data set,0.7923770546913147
translation,110,130,results,results,improvement in,bleu scores,results improvement in bleu scores,0.6245699524879456
translation,110,131,results,performance,of,final nmt system,performance of final nmt system,0.5974586606025696
translation,110,131,results,steadily improves,as,more data,steadily improves as more data,0.5557456016540527
translation,110,131,results,more data,from,more systems,more data from more systems,0.6043444871902466
translation,110,131,results,more data,added,converges,more data added converges,0.7043188810348511
translation,110,131,results,final nmt system,has,steadily improves,final nmt system has steadily improves,0.6113988757133484
translation,110,131,results,results,has,performance,results has performance,0.5972660779953003
translation,110,132,results,strongest result,achieved with,14 candidate runs,strongest result achieved with 14 candidate runs,0.6906111836433411
translation,110,132,results,14 candidate runs,for,weight - based approach,14 candidate runs for weight - based approach,0.6259944438934326
translation,110,132,results,14 candidate runs,for,"10 , 15 and 20 m setting","14 candidate runs for 10 , 15 and 20 m setting",0.6332988739013672
translation,110,132,results,results,has,strongest result,results has strongest result,0.6075684428215027
translation,110,133,results,uniform weight baseline,by,1.5 to 2 bleu points,uniform weight baseline by 1.5 to 2 bleu points,0.5531261563301086
translation,110,133,results,strongest single feature ( laser ) baseline,by,1 bleu point,strongest single feature ( laser ) baseline by 1 bleu point,0.5360960364341736
translation,110,134,results,feature based approach,performed,slightly better,feature based approach performed slightly better,0.25000855326652527
translation,110,134,results,feature based approach,beat,strongest single feature baseline ( laser ),feature based approach beat strongest single feature baseline ( laser ),0.7383025884628296
translation,110,134,results,slightly better,with,15 candidate runs,slightly better with 15 candidate runs,0.6298350691795349
translation,110,134,results,strongest single feature baseline ( laser ),by,1.3 bleu points,strongest single feature baseline ( laser ) by 1.3 bleu points,0.5211938619613647
translation,110,134,results,results,has,feature based approach,results has feature based approach,0.6019285917282104
translation,110,149,results,filtering,with,transferred weights,filtering with transferred weights,0.6948192119598389
translation,110,149,results,transferred weights,beats,simpler single feature baselines,transferred weights beats simpler single feature baselines,0.7388595342636108
translation,110,149,results,fails,to beat,strongest one,fails to beat strongest one,0.6885899305343628
translation,111,41,hyperparameters,embeddings,have,dimensionality,embeddings have dimensionality,0.5199446082115173
translation,111,41,hyperparameters,dimensionality,of,512,dimensionality of 512,0.6377550363540649
translation,111,41,hyperparameters,512,to match,our marian transformer - base system configuration,512 to match our marian transformer - base system configuration,0.7503501772880554
translation,111,41,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,111,72,hyperparameters,network,trained for,10 epochs,network trained for 10 epochs,0.8011189699172974
translation,111,72,hyperparameters,10 epochs,using,batch size,10 epochs using batch size,0.6121646761894226
translation,111,72,hyperparameters,10 epochs,using,effective batch size,10 epochs using effective batch size,0.6211394667625427
translation,111,72,hyperparameters,10 epochs,using,"lazy adam ( kingma and ba , 2015 ) optimizer","10 epochs using lazy adam ( kingma and ba , 2015 ) optimizer",0.6263272762298584
translation,111,72,hyperparameters,batch size,of,3124,batch size of 3124,0.6604384183883667
translation,111,72,hyperparameters,effective batch size,of,49984,effective batch size of 49984,0.6271243691444397
translation,111,72,hyperparameters,49984,using,"lazy adam ( kingma and ba , 2015 ) optimizer","49984 using lazy adam ( kingma and ba , 2015 ) optimizer",0.6551915407180786
translation,111,72,hyperparameters,"lazy adam ( kingma and ba , 2015 ) optimizer",with,"beta1=0.9 , beta2=0.998","lazy adam ( kingma and ba , 2015 ) optimizer with beta1=0.9 , beta2=0.998",0.5976596474647522
translation,111,72,hyperparameters,"lazy adam ( kingma and ba , 2015 ) optimizer",with,learning rate,"lazy adam ( kingma and ba , 2015 ) optimizer with learning rate",0.6076026558876038
translation,111,72,hyperparameters,learning rate,has,2.0,learning rate has 2.0,0.5442604422569275
translation,111,72,hyperparameters,hyperparameters,has,network,hyperparameters has network,0.5504763126373291
translation,111,95,results,onmt- asr system,tuned with,newstest 2014 - 2017,onmt- asr system tuned with newstest 2014 - 2017,0.7792912125587463
translation,111,95,results,two epochs,of,large data set,two epochs of large data set,0.5800842046737671
translation,111,95,results,newstest 2014 - 2017,+ 2.5 BLEU better than,baseline onmtlarge system,newstest 2014 - 2017 + 2.5 BLEU better than baseline onmtlarge system,0.8157942891120911
translation,111,95,results,baseline onmtlarge system,trained with,10 epochs,baseline onmtlarge system trained with 10 epochs,0.6942505240440369
translation,111,95,results,system,tuned with,newstest 2014- 2017,system tuned with newstest 2014- 2017,0.7901259064674377
translation,111,95,results,results,has,baseline onmt-large system,results has baseline onmt-large system,0.5818040370941162
translation,111,101,results,all three tuning sets,provided,significant improvements,all three tuning sets provided significant improvements,0.6503081321716309
translation,111,101,results,significant improvements,over,baseline systems,significant improvements over baseline systems,0.6389209628105164
translation,111,101,results,significant improvements,in the range of,+ 3.5 bleu,significant improvements in the range of + 3.5 bleu,0.6178805828094482
translation,111,101,results,+ 3.5 bleu,has,on test 2021,+ 3.5 bleu has on test 2021,0.5838159918785095
translation,112,132,ablation-analysis,rhe,lifts,lpea,rhe lifts lpea,0.7520126700401306
translation,112,132,ablation-analysis,substantially outperforms,by,+0.96 bleu points,substantially outperforms by +0.96 bleu points,0.5842162370681763
translation,112,132,ablation-analysis,substantially outperforms,by,+ 0.81 bleu points,substantially outperforms by + 0.81 bleu points,0.5870855450630188
translation,112,132,ablation-analysis,transformer,by,+ 1.0 bleu points,transformer by + 1.0 bleu points,0.611702561378479
translation,112,132,ablation-analysis,transformer,by,+0.96 bleu points,transformer by +0.96 bleu points,0.6033318042755127
translation,112,132,ablation-analysis,+ 1.0 bleu points,on,en?de ( wmt16 ),+ 1.0 bleu points on en?de ( wmt16 ),0.5857018828392029
translation,112,132,ablation-analysis,+ 1.0 bleu points,on,en?de ( wmt14 ),+ 1.0 bleu points on en?de ( wmt14 ),0.5842268466949463
translation,112,132,ablation-analysis,+ 1.0 bleu points,on,en?cs ( wmt16 ),+ 1.0 bleu points on en?cs ( wmt16 ),0.5922589898109436
translation,112,132,ablation-analysis,+ 1.0 bleu points,on,en?de ( wmt14 ),+ 1.0 bleu points on en?de ( wmt14 ),0.5842268466949463
translation,112,132,ablation-analysis,+ 1.0 bleu points,on,en?cs ( wmt16 ),+ 1.0 bleu points on en?cs ( wmt16 ),0.5922589898109436
translation,112,132,ablation-analysis,+0.96 bleu points,on,en?de ( wmt14 ),+0.96 bleu points on en?de ( wmt14 ),0.5738484263420105
translation,112,132,ablation-analysis,+ 0.81 bleu points,on,en?cs ( wmt16 ),+ 0.81 bleu points on en?cs ( wmt16 ),0.592717170715332
translation,112,132,ablation-analysis,lpea,has,substantially outperforms,lpea has substantially outperforms,0.6201305389404297
translation,112,132,ablation-analysis,substantially outperforms,has,transformer,substantially outperforms has transformer,0.6150175333023071
translation,112,132,ablation-analysis,ablation analysis,has,rhe,ablation analysis has rhe,0.5262873768806458
translation,112,141,ablation-analysis,rhe applicability,shows,rhe,rhe applicability shows rhe,0.6411694884300232
translation,112,141,ablation-analysis,rhe applicability,lifts,rel_pos and localness,rhe applicability lifts rel_pos and localness,0.7867194414138794
translation,112,141,ablation-analysis,rhe,lifts,rel_pos and localness,rhe lifts rel_pos and localness,0.7864183783531189
translation,112,141,ablation-analysis,rel_pos and localness,by,+ 0.28 and + 0.20 bleu point,rel_pos and localness by + 0.28 and + 0.20 bleu point,0.5518783926963806
translation,112,141,ablation-analysis,ablation analysis,has,rhe applicability,ablation analysis has rhe applicability,0.5445020198822021
translation,112,143,ablation-analysis,redundant heads,shows,rhe,redundant heads shows rhe,0.6849797368049622
translation,112,143,ablation-analysis,rhe,precisely identify,redundant heads,rhe precisely identify redundant heads,0.7564581036567688
translation,112,143,ablation-analysis,0.1 bleu point,after pruning,identified redundant heads,0.1 bleu point after pruning identified redundant heads,0.7933149337768555
translation,112,143,ablation-analysis,rhe -enabled transformer,has,drops,rhe -enabled transformer has drops,0.6351205706596375
translation,112,143,ablation-analysis,drops,has,0.1 bleu point,drops has 0.1 bleu point,0.5981506705284119
translation,112,143,ablation-analysis,ablation analysis,By pruning,redundant heads,ablation analysis By pruning redundant heads,0.8059396147727966
translation,112,123,experimental-setup,byte-pair encoding ( bpe ) toolkit,used with,32 k merge operations,byte-pair encoding ( bpe ) toolkit used with 32 k merge operations,0.6619012355804443
translation,112,123,experimental-setup,experimental setup,has,byte-pair encoding ( bpe ) toolkit,experimental setup has byte-pair encoding ( bpe ) toolkit,0.5305835008621216
translation,112,121,experiments,berkeley neural parser ( ? ),to generate,constituency trees,berkeley neural parser ( ? ) to generate constituency trees,0.6693903803825378
translation,112,121,experiments,constituency trees,for,english,constituency trees for english,0.6260005831718445
translation,112,121,experiments,constituency trees,for,english,constituency trees for english,0.6260005831718445
translation,112,121,experiments,open-source tool spacy,to parse,dependency trees,open-source tool spacy to parse dependency trees,0.70709627866745
translation,112,121,experiments,dependency trees,for,english,dependency trees for english,0.5881515741348267
translation,112,8,model,syntactic enhancement,in,machine translation,syntactic enhancement in machine translation,0.4442993700504303
translation,112,8,model,model,has,two novel syntax - enhanced attention ( sea ) mechanisms,model has two novel syntax - enhanced attention ( sea ) mechanisms,0.5450437664985657
translation,112,35,model,two novel syntax - enhanced attention ( sea ) mechanisms,for,machine translation,two novel syntax - enhanced attention ( sea ) mechanisms for machine translation,0.5577080249786377
translation,112,35,model,dependency - enhanced attention,to use,dependent matrix,dependency - enhanced attention to use dependent matrix,0.7172198295593262
translation,112,35,model,dependent matrix,as,mask,dependent matrix as mask,0.5933036804199219
translation,112,35,model,mask,to model,intensive attention,mask to model intensive attention,0.7455514073371887
translation,112,35,model,intensive attention,between,dependent elements and filter elements,intensive attention between dependent elements and filter elements,0.6801283359527588
translation,112,35,model,dependent elements and filter elements,without,direct dependent relations,dependent elements and filter elements without direct dependent relations,0.7215481996536255
translation,112,35,model,local- phrase - enhanced attention,to incorporate,distinct and learnable relative local- phrasal position matrix,local- phrase - enhanced attention to incorporate distinct and learnable relative local- phrasal position matrix,0.6711428761482239
translation,112,35,model,distinct and learnable relative local- phrasal position matrix,as,bias,distinct and learnable relative local- phrasal position matrix as bias,0.5144617557525635
translation,112,35,model,constituency tree,under,rules,constituency tree under rules,0.640053391456604
translation,112,35,model,rules,of,local- phrase,rules of local- phrase,0.5785929560661316
translation,112,35,model,model,propose,two novel syntax - enhanced attention ( sea ) mechanisms,model propose two novel syntax - enhanced attention ( sea ) mechanisms,0.6596978306770325
translation,112,37,model,distinct syntactic layer information,for,each word,distinct syntactic layer information for each word,0.6076894402503967
translation,112,37,model,each word,in,constituency tree,each word in constituency tree,0.5183789730072021
translation,112,37,model,relative phrasal position,to reflect,syntactic relations,relative phrasal position to reflect syntactic relations,0.5675318837165833
translation,112,37,model,syntactic relations,between,elements,syntactic relations between elements,0.6154211759567261
translation,112,38,model,novel phrase type local - phrase,to only extract,syntactically related words,novel phrase type local - phrase to only extract syntactically related words,0.6910295486450195
translation,112,38,model,syntactically related words,as,phrase,syntactically related words as phrase,0.5236217975616455
translation,112,38,model,syntactically related words,by leveraging,constituency tree,syntactically related words by leveraging constituency tree,0.6967713236808777
translation,112,38,model,model,define,novel phrase type local - phrase,model define novel phrase type local - phrase,0.604080080986023
translation,112,39,model,dynamic and lightweight redundant heads enlivening,for,multi-head san,dynamic and lightweight redundant heads enlivening for multi-head san,0.582669198513031
translation,112,39,model,rhe ) strat-egy,for,multi-head san,rhe ) strat-egy for multi-head san,0.7291329503059387
translation,112,39,model,dynamic and lightweight redundant heads enlivening,has,rhe ) strat-egy,dynamic and lightweight redundant heads enlivening has rhe ) strat-egy,0.5947210788726807
translation,112,39,model,model,propose,dynamic and lightweight redundant heads enlivening,model propose dynamic and lightweight redundant heads enlivening,0.6770113110542297
translation,112,171,model,dynamic redundant heads enlivening ( rhe ) mechanism,to identify and enliven,each redundant head,dynamic redundant heads enlivening ( rhe ) mechanism to identify and enliven each redundant head,0.6419698596000671
translation,112,171,model,each redundant head,toward,full potential,each redundant head toward full potential,0.6570889949798584
translation,112,171,model,model,introducing,dynamic redundant heads enlivening ( rhe ) mechanism,model introducing dynamic redundant heads enlivening ( rhe ) mechanism,0.7444362044334412
translation,112,179,model,model,has,two syntax - enhanced attention ( sea ) mechanisms,model has two syntax - enhanced attention ( sea ) mechanisms,0.5603866577148438
translation,112,180,model,dea,disables,attention,dea disables attention,0.7252340912818909
translation,112,180,model,attention,between,elements,attention between elements,0.6673683524131775
translation,112,180,model,attention,by leveraging,dependency mask,attention by leveraging dependency mask,0.7454788684844971
translation,112,180,model,elements,without,dependencies,elements without dependencies,0.7403922080993652
translation,112,180,model,lpea,precisely regulates,self-attention distribution,lpea precisely regulates self-attention distribution,0.684643566608429
translation,112,180,model,self-attention distribution,by,distinct and learnable local - phrase bias,self-attention distribution by distinct and learnable local - phrase bias,0.5470725297927856
translation,112,180,model,model,has,dea,model has dea,0.6518775224685669
translation,112,130,results,dea and lpea mechanisms,across,all small and large language pairs,dea and lpea mechanisms across all small and large language pairs,0.7223444581031799
translation,112,130,results,rhe approach,has,significantly lifts,rhe approach has significantly lifts,0.6086974740028381
translation,112,130,results,results,has,rhe approach,results has rhe approach,0.5391551852226257
translation,112,135,results,rel_pos and localness,make improvement,transformer,rel_pos and localness make improvement transformer,0.7167496681213379
translation,112,135,results,rel_pos and localness,consistently beat,standard transformer,rel_pos and localness consistently beat standard transformer,0.7481202483177185
translation,112,135,results,strategies,of enhancing,san,strategies of enhancing san,0.69439297914505
translation,112,135,results,strategies,of enhancing,"our dea , dea + rhe , lpea and lpea + rheenabled transformers","strategies of enhancing our dea , dea + rhe , lpea and lpea + rheenabled transformers",0.7000462412834167
translation,112,135,results,results,both,rel_pos and localness,results both rel_pos and localness,0.6222841739654541
translation,113,149,ablation-analysis,task adapters,are,ineffective,task adapters are ineffective,0.5828707814216614
translation,113,149,ablation-analysis,en?zz direction,has,two baselines mbart - ft,en?zz direction has two baselines mbart - ft,0.6000393033027649
translation,113,149,ablation-analysis,ablation analysis,For,en?zz direction,ablation analysis For en?zz direction,0.5961848497390747
translation,113,158,ablation-analysis,backtranslation,decreased,performance,backtranslation decreased performance,0.6847882866859436
translation,113,158,ablation-analysis,ur,has,backtranslation,ur has backtranslation,0.6236187219619751
translation,113,158,ablation-analysis,ablation analysis,for,ur,ablation analysis for ur,0.6643289923667908
translation,113,158,ablation-analysis,ablation analysis,for,backtranslation,ablation analysis for backtranslation,0.6074179410934448
translation,113,160,ablation-analysis,denoising adapters,without,back -translation,denoising adapters without back -translation,0.7337003350257874
translation,113,160,ablation-analysis,denoising adapters,provide,superior unsupervised translation quality,denoising adapters provide superior unsupervised translation quality,0.6028368473052979
translation,113,160,ablation-analysis,superior unsupervised translation quality,compared to,baselines,superior unsupervised translation quality compared to baselines,0.6611093878746033
translation,113,160,ablation-analysis,baselines,even after,backtranslation,baselines even after backtranslation,0.6302405595779419
translation,113,160,ablation-analysis,ablation analysis,has,denoising adapters,ablation analysis has denoising adapters,0.558385968208313
translation,113,14,baselines,unsupervised neural machine translation,has,unmt ),unsupervised neural machine translation has unmt ),0.6354224681854248
translation,113,14,baselines,baselines,has,unsupervised neural machine translation,baselines has unsupervised neural machine translation,0.5426958799362183
translation,113,25,experiments,monolingual denoising adapters,with,multilingual transfer learning,monolingual denoising adapters with multilingual transfer learning,0.638927698135376
translation,113,25,experiments,multilingual transfer learning,on,auxiliary parallel data,multilingual transfer learning on auxiliary parallel data,0.49769631028175354
translation,113,31,experiments,denoising adapters,for,17 diverse unsupervised languages,denoising adapters for 17 diverse unsupervised languages,0.5977177023887634
translation,113,31,experiments,17 diverse unsupervised languages,together with,20 auxiliary languages,17 diverse unsupervised languages together with 20 auxiliary languages,0.6447206735610962
translation,113,6,model,adapter layers,with,denoising objective,adapter layers with denoising objective,0.6117445230484009
translation,113,6,model,denoising adapters,has,adapter layers,denoising adapters has adapter layers,0.5673498511314392
translation,113,24,model,2 - step approach,based on,denoising adapters,2 - step approach based on denoising adapters,0.698133647441864
translation,113,24,model,denoising adapters,that enable,modular multilingual unsupervised nmt,denoising adapters that enable modular multilingual unsupervised nmt,0.6848951578140259
translation,113,24,model,modular multilingual unsupervised nmt,without,backtranslation,modular multilingual unsupervised nmt without backtranslation,0.6268917322158813
translation,113,24,model,model,propose,2 - step approach,model propose 2 - step approach,0.6642842292785645
translation,113,7,results,resulting translations,are,on - par,resulting translations are on - par,0.6418001055717468
translation,113,7,results,on - par,with,back - translating,on - par with back - translating,0.6865461468696594
translation,113,7,results,back - translating,measured by,bleu,back - translating measured by bleu,0.686265230178833
translation,113,145,results,two baselines mbart - ft and task adapters,are,quite decent,two baselines mbart - ft and task adapters are quite decent,0.5526757836341858
translation,113,145,results,nmt,using,auxiliary parallel data,nmt using auxiliary parallel data,0.649194061756134
translation,113,145,results,auxiliary parallel data,provide,good multilingual unsupervised nmt performance,auxiliary parallel data provide good multilingual unsupervised nmt performance,0.5920684933662415
translation,113,145,results,zz?en,has,two baselines mbart - ft and task adapters,zz?en has two baselines mbart - ft and task adapters,0.6268261075019836
translation,113,145,results,results,For,zz?en,results For zz?en,0.6511057019233704
translation,113,146,results,task-specific mt adapters,better mitigate,catastrophic forgetting,task-specific mt adapters better mitigate catastrophic forgetting,0.7499223351478577
translation,113,146,results,task-specific mt adapters,ensuring,model,task-specific mt adapters ensuring model,0.7580709457397461
translation,113,146,results,model,to benefit more from,multilingual fine-tuning,model to benefit more from multilingual fine-tuning,0.6676532626152039
translation,113,146,results,multilingual fine-tuning,results in,+ 5.4 bleu,multilingual fine-tuning results in + 5.4 bleu,0.5525010824203491
translation,113,146,results,+ 5.4 bleu,compared to,standard finetuning,+ 5.4 bleu compared to standard finetuning,0.606956958770752
translation,113,146,results,two baselines,has,task-specific mt adapters,two baselines has task-specific mt adapters,0.5507513880729675
translation,113,146,results,results,Among,two baselines,results Among two baselines,0.577792227268219
translation,113,147,results,denoising adapters,are,superior,denoising adapters are superior,0.6108118891716003
translation,113,147,results,denoising adapters,result in,+ 8.6 and + 3.2 bleu,denoising adapters result in + 8.6 and + 3.2 bleu,0.6826432347297668
translation,113,147,results,superior,for,all languages,superior for all languages,0.6465883851051331
translation,113,147,results,all languages,compared to,mbart - ft,all languages compared to mbart - ft,0.6291915774345398
translation,113,147,results,all languages,compared to,task adapters,all languages compared to task adapters,0.6400212049484253
translation,113,147,results,approach,has,outperforms,approach has outperforms,0.6689831614494324
translation,113,147,results,outperforms,has,two mbart baselines,outperforms has two mbart baselines,0.5985164642333984
translation,113,147,results,results,has,approach,results has approach,0.5518386363983154
translation,113,148,results,better,than,supervised bilingual models,better than supervised bilingual models,0.5271220207214355
translation,113,148,results,supervised bilingual models,for,most languages,supervised bilingual models for most languages,0.5486646294593811
translation,113,154,results,supervised bilingual models,trained with,less than 50 k parallel sentences,supervised bilingual models trained with less than 50 k parallel sentences,0.7188175916671753
translation,113,154,results,denoising adapters,has,outperform,denoising adapters has outperform,0.6238235235214233
translation,113,154,results,outperform,has,supervised bilingual models,outperform has supervised bilingual models,0.5397542119026184
translation,113,154,results,results,has,denoising adapters,results has denoising adapters,0.5808817744255066
translation,113,157,results,overall impact,of,backtranslation,overall impact of backtranslation,0.5667697191238403
translation,113,157,results,backtranslation,is,very limited,backtranslation is very limited,0.5499631762504578
translation,113,157,results,very limited,for,all models,very limited for all models,0.6429721713066101
translation,113,157,results,all models,including,our approach,all models including our approach,0.727676272392273
translation,113,157,results,zz?en,has,overall impact,zz?en has overall impact,0.6218043565750122
translation,113,157,results,results,For,zz?en,results For zz?en,0.6511057019233704
translation,113,161,results,back -translation,significantly increased,translation results,back -translation significantly increased translation results,0.718329668045044
translation,113,161,results,"+ 15.0 , +16.2 and + 3.0 bleu",for,mbart -ft,"+ 15.0 , +16.2 and + 3.0 bleu for mbart -ft",0.6341451406478882
translation,113,161,results,"+ 15.0 , +16.2 and + 3.0 bleu",for,task adapters,"+ 15.0 , +16.2 and + 3.0 bleu for task adapters",0.6076557040214539
translation,113,161,results,"+ 15.0 , +16.2 and + 3.0 bleu",for,de-noising adapters,"+ 15.0 , +16.2 and + 3.0 bleu for de-noising adapters",0.6374886631965637
translation,113,161,results,en?zz,has,back -translation,en?zz has back -translation,0.6072878241539001
translation,113,161,results,translation results,has,"+ 15.0 , +16.2 and + 3.0 bleu","translation results has + 15.0 , +16.2 and + 3.0 bleu",0.5528737902641296
translation,113,161,results,results,For,en?zz,results For en?zz,0.6312502026557922
translation,113,163,results,outperforms,in,all languages,outperforms in all languages,0.5285770893096924
translation,113,163,results,baselines,in,all languages,baselines in all languages,0.481365829706192
translation,113,163,results,approach,has,outperforms,approach has outperforms,0.6689831614494324
translation,113,163,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,113,163,results,results,has,approach,results has approach,0.5518386363983154
translation,113,164,results,denoising adapters,are,competitive,denoising adapters are competitive,0.6343527436256409
translation,113,164,results,denoising adapters,still,competitive,denoising adapters still competitive,0.7432200908660889
translation,113,164,results,without back - translation,are,competitive,without back - translation are competitive,0.624638020992279
translation,113,164,results,competitive,with,mbart baselines,competitive with mbart baselines,0.6765620708465576
translation,113,164,results,denoising adapters,has,without back - translation,denoising adapters has without back - translation,0.5984348654747009
translation,113,164,results,results,has,denoising adapters,results has denoising adapters,0.5808817744255066
translation,113,177,results,denoising adapters,offer,efficient way,denoising adapters offer efficient way,0.7109991908073425
translation,113,177,results,efficient way,to extend,mbart,efficient way to extend mbart,0.7366560697555542
translation,113,177,results,mbart,to,new languages,mbart to new languages,0.6198813319206238
translation,113,177,results,results,confirm,denoising adapters,results confirm denoising adapters,0.5652068853378296
translation,113,178,results,unsupervised translation quality,for,missing languages,unsupervised translation quality for missing languages,0.5442817807197571
translation,113,182,results,majority of languages,increase in,data size,majority of languages increase in data size,0.663486123085022
translation,113,182,results,performance improvement,increase in,data size,performance improvement increase in data size,0.6467598080635071
translation,113,182,results,majority of languages,has,performance improvement,majority of languages has performance improvement,0.5261760950088501
translation,113,182,results,results,for,majority of languages,results for majority of languages,0.5517786741256714
translation,113,187,results,both directions,multilingual fine-tuning of,mbart ( mbart - ft ),both directions multilingual fine-tuning of mbart ( mbart - ft ),0.6476295590400696
translation,113,187,results,best,on,av-erage,best on av-erage,0.6267004013061523
translation,113,187,results,results,for,both directions,results for both directions,0.5957531929016113
translation,113,197,results,fine-tuning,seems to penalize,performance,fine-tuning seems to penalize performance,0.7759911417961121
translation,113,197,results,only the decoder 's cross-attention,seems to penalize,performance,only the decoder 's cross-attention seems to penalize performance,0.7447538375854492
translation,113,197,results,fine-tuning,has,only the decoder 's cross-attention,fine-tuning has only the decoder 's cross-attention,0.5488896369934082
translation,113,210,results,effectiveness,of,denoising adapters,effectiveness of denoising adapters,0.6048496961593628
translation,113,210,results,denoising adapters,for,low resource languages,denoising adapters for low resource languages,0.5519419312477112
translation,113,210,results,denoising adapters,compared to,strong baseline,denoising adapters compared to strong baseline,0.6445762515068054
translation,113,210,results,denoising adapters,has,significantly outperform,denoising adapters has significantly outperform,0.6110221743583679
translation,113,210,results,significantly outperform,has,mbart - ft,significantly outperform has mbart - ft,0.5758652687072754
translation,114,50,baselines,generated target translation,by,beam search,generated target translation by beam search,0.5869318842887878
translation,114,50,baselines,beam search,with,beam 5,beam search with beam 5,0.7028772234916687
translation,114,50,baselines,selected a word randomly,from,whole distribution,selected a word randomly from whole distribution,0.5821053981781006
translation,114,50,baselines,selected a word randomly,increases,diversity,selected a word randomly increases diversity,0.6713659763336182
translation,114,50,baselines,whole distribution,in,each step,whole distribution in each step,0.583683431148529
translation,114,50,baselines,diversity,of,pseudo corpus,diversity of pseudo corpus,0.5936648845672607
translation,114,50,baselines,pseudo corpus,with,low precision,pseudo corpus with low precision,0.6177409291267395
translation,114,50,baselines,beam search,has,generated target translation,beam search has generated target translation,0.6019286513328552
translation,114,50,baselines,baselines,has,beam search,baselines has beam search,0.5592350959777832
translation,114,68,experimental-setup,adam optimizer,during,training,adam optimizer during training,0.6991541385650635
translation,114,68,experimental-setup,learning rate,was,"5e - 4 , ? 1 = 0.9 , ? 2 = 0.98","learning rate was 5e - 4 , ? 1 = 0.9 , ? 2 = 0.98",0.5742576718330383
translation,114,68,experimental-setup,weight decay,was,0.0001,weight decay was 0.0001,0.5655953884124756
translation,114,68,experimental-setup,label smoothing,was,0.1,label smoothing was 0.1,0.5624558925628662
translation,114,68,experimental-setup,adam optimizer,has,learning rate,adam optimizer has learning rate,0.5073457360267639
translation,114,68,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,114,69,experimental-setup,warmed up,over,"8,000 steps","warmed up over 8,000 steps",0.6795142292976379
translation,114,69,experimental-setup,warmed up,for,pre-normalize architectures transformer - big ffn - 8192 model,warmed up for pre-normalize architectures transformer - big ffn - 8192 model,0.63129061460495
translation,114,69,experimental-setup,learning rate,has,warmed up,learning rate has warmed up,0.5760711431503296
translation,114,69,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,114,93,experimental-setup,"three models ( 3e3d , 4e4d , 6e4d )",trained under,different architectures,"three models ( 3e3d , 4e4d , 6e4d ) trained under different architectures",0.7484796643257141
translation,114,93,experimental-setup,different architectures,on,single 2080 ti gpu,different architectures on single 2080 ti gpu,0.5216258764266968
translation,114,93,experimental-setup,experimental setup,has,"three models ( 3e3d , 4e4d , 6e4d )","experimental setup has three models ( 3e3d , 4e4d , 6e4d )",0.5472880005836487
translation,114,125,experimental-setup,learning rate,for,training,learning rate for training,0.6674456000328064
translation,114,125,experimental-setup,learning rate,for,warmup steps,learning rate for warmup steps,0.6408109664916992
translation,114,125,experimental-setup,training,is,0.001,training is 0.001,0.5673884749412537
translation,114,125,experimental-setup,warmup steps,are,4000,warmup steps are 4000,0.6234406232833862
translation,114,125,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,114,125,experimental-setup,experimental setup,has,warmup steps,experimental setup has warmup steps,0.498988538980484
translation,114,131,experimental-setup,word embedding,was,256,word embedding was 256,0.576023519039154
translation,114,131,experimental-setup,feed -forward network,was,1024,feed -forward network was 1024,0.6625145077705383
translation,114,131,experimental-setup,multi-head,was,4,multi-head was 4,0.6533229351043701
translation,114,131,experimental-setup,encoder and decoder layer,was,4,encoder and decoder layer was 4,0.5689562559127808
translation,114,17,experiments,"knowledge distillation ( freitag et al. , 2017 )",to leverage,source-side monolingual data,"knowledge distillation ( freitag et al. , 2017 ) to leverage source-side monolingual data",0.6515480875968933
translation,114,67,experiments,"chinese ? english , german ? english and french ? german tasks",implemented,transformer - big ffn - 8192,"chinese ? english , german ? english and french ? german tasks implemented transformer - big ffn - 8192",0.6635465025901794
translation,114,67,experiments,transformer - big ffn - 8192,based on,"fairseq ( ott et al. , 2019 )","transformer - big ffn - 8192 based on fairseq ( ott et al. , 2019 )",0.6362135410308838
translation,114,67,experiments,transformer - big ffn - 8192,as,baseline model,transformer - big ffn - 8192 as baseline model,0.49841609597206116
translation,114,67,experiments,"fairseq ( ott et al. , 2019 )",as,baseline model,"fairseq ( ott et al. , 2019 ) as baseline model",0.4599868655204773
translation,114,78,experiments,transformer - big ffn -8192 model,for,chinese ? english,transformer - big ffn -8192 model for chinese ? english,0.5995290279388428
translation,114,84,experiments,submissions,consisted of,three transformer - big ffn - 8192 models,submissions consisted of three transformer - big ffn - 8192 models,0.6637930274009705
translation,114,84,experiments,three transformer - big ffn - 8192 models,using,beam search,three transformer - big ffn - 8192 models using beam search,0.6691232919692993
translation,114,84,experiments,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,114,84,experiments,beam search,set,lenpen 2.0,beam search set lenpen 2.0,0.6670495867729187
translation,114,84,experiments,beam size,of,5,beam size of 5,0.7073217034339905
translation,114,92,experiments,marian trained transformer - small 8 model,for,hausa ? english baseline,marian trained transformer - small 8 model for hausa ? english baseline,0.5843753218650818
translation,114,92,experiments,marian trained transformer - small 8 model,with,learning rate,marian trained transformer - small 8 model with learning rate,0.6463188529014587
translation,114,92,experiments,marian trained transformer - small 8 model,with,warmup steps,marian trained transformer - small 8 model with warmup steps,0.6394838094711304
translation,114,92,experiments,learning rate,ranging from,0.0008 to 0.001,learning rate ranging from 0.0008 to 0.001,0.6172681450843811
translation,114,92,experiments,warmup steps,fixing at,"48,000","warmup steps fixing at 48,000",0.7162705063819885
translation,114,85,results,results,shows,translation quality,results shows translation quality,0.5547988414764404
translation,114,100,results,results,shows,translation quality,results shows translation quality,0.5547988414764404
translation,115,116,baselines,rl,implementation of,gu et al . ( 2017 ),rl implementation of gu et al . ( 2017 ),0.6108352541923523
translation,115,116,baselines,rl,implementation of,policy gradient method,rl implementation of policy gradient method,0.6889303922653198
translation,115,116,baselines,rl,with,policy gradient method,rl with policy gradient method,0.631176769733429
translation,115,116,baselines,gu et al . ( 2017 ),with,policy gradient method,gu et al . ( 2017 ) with policy gradient method,0.6016441583633423
translation,115,116,baselines,baselines,has,rl,baselines has rl,0.633094847202301
translation,115,125,baselines,gsimt - possion,is,proposed generative model,gsimt - possion is proposed generative model,0.6090987324714661
translation,115,125,baselines,proposed generative model,with,possion prior,proposed generative model with possion prior,0.6545453667640686
translation,115,125,baselines,baselines,has,gsimt - possion,baselines has gsimt - possion,0.6184534430503845
translation,115,104,experimental-setup,encoder and decoder,as,consecutive nmt model,encoder and decoder as consecutive nmt model,0.5851995348930359
translation,115,104,experimental-setup,consecutive nmt model,for,10 epochs,consecutive nmt model for 10 epochs,0.5892064571380615
translation,115,104,experimental-setup,experimental setup,pretrain,encoder and decoder,experimental setup pretrain encoder and decoder,0.7373231649398804
translation,115,105,experimental-setup,256 batch size and 1e - 4 learning rate,for training,generative models,256 batch size and 1e - 4 learning rate for training generative models,0.7116507887840271
translation,115,105,experimental-setup,experimental setup,freeze,transformers parameters,experimental setup freeze transformers parameters,0.6254076957702637
translation,115,106,experimental-setup,each epoch,takes,around 40 minutes,each epoch takes around 40 minutes,0.6919443011283875
translation,115,106,experimental-setup,around 40 minutes,with,"adam ( kingma and ba , 2014 )","around 40 minutes with adam ( kingma and ba , 2014 )",0.66121906042099
translation,115,106,experimental-setup,"adam ( kingma and ba , 2014 )",on,single v100 gpu,"adam ( kingma and ba , 2014 ) on single v100 gpu",0.4998531937599182
translation,115,106,experimental-setup,"pytorch ( paszke et al. , 2019 ) platform",has,each epoch,"pytorch ( paszke et al. , 2019 ) platform has each epoch",0.5659976005554199
translation,115,106,experimental-setup,experimental setup,On,"pytorch ( paszke et al. , 2019 ) platform","experimental setup On pytorch ( paszke et al. , 2019 ) platform",0.4884192943572998
translation,115,4,model,model,propose,generative framework,model propose generative framework,0.6945322155952454
translation,115,8,model,re-parameterised poisson prior,to regularise,policies,re-parameterised poisson prior to regularise policies,0.7218084335327148
translation,115,8,model,model,to explicitly balance,translation quality and latency,model to explicitly balance translation quality and latency,0.694226086139679
translation,115,8,model,model,has,re-parameterised poisson prior,model has re-parameterised poisson prior,0.5517719984054565
translation,115,17,model,generative framework,with,latent variable,generative framework with latent variable,0.6359797120094299
translation,115,17,model,latent variable,dynamically decides between,actions,latent variable dynamically decides between actions,0.7310667634010315
translation,115,17,model,actions,of,read or translate,actions of read or translate,0.575663149356842
translation,115,17,model,actions,enabling,formulation,actions enabling formulation,0.7219824194908142
translation,115,17,model,read or translate,at,every time step,read or translate at every time step,0.527676522731781
translation,115,17,model,formulation,of,simt,formulation of simt,0.6403379440307617
translation,115,17,model,simt,as,structural sequence - to-sequence learning task,simt as structural sequence - to-sequence learning task,0.5353125333786011
translation,115,17,model,model,propose,generative framework,model propose generative framework,0.6945322155952454
translation,115,22,model,generative simultaneous machine translation model ( gsimt ),integrates out,all the hypothesises,generative simultaneous machine translation model ( gsimt ) integrates out all the hypothesises,0.7242742776870728
translation,115,22,model,all the hypothesises,by,dynamic programming algorithm,all the hypothesises by dynamic programming algorithm,0.59512859582901
translation,115,22,model,dynamic programming algorithm,with the help of,introduced latent variable,dynamic programming algorithm with the help of introduced latent variable,0.6961040496826172
translation,115,27,model,neural transducer framework,introduce,re-parameterised poisson distribution,neural transducer framework introduce re-parameterised poisson distribution,0.5936954617500305
translation,115,27,model,re-parameterised poisson distribution,to regularise,latency,re-parameterised poisson distribution to regularise latency,0.7606306076049805
translation,115,27,model,neural transducer framework,has,to modern transformer - based translation models,neural transducer framework has to modern transformer - based translation models,0.5439803600311279
translation,115,27,model,model,extend,neural transducer framework,model extend neural transducer framework,0.7150195240974426
translation,115,27,model,model,introduce,re-parameterised poisson distribution,model introduce re-parameterised poisson distribution,0.612639844417572
translation,115,33,results,our proposed model,achieves,best performance,our proposed model achieves best performance,0.704833447933197
translation,115,33,results,best performance,on,bleu scores,best performance on bleu scores,0.46872422099113464
translation,115,33,results,best performance,on,average lagging ( al ),best performance on average lagging ( al ),0.5373949408531189
translation,115,33,results,number of strong baseline models,has,our proposed model,number of strong baseline models has our proposed model,0.5230783224105835
translation,115,33,results,results,Compared to,number of strong baseline models,results Compared to number of strong baseline models,0.6660488843917847
translation,115,115,results,models,on,wmt15 de ?en dataset,models on wmt15 de ?en dataset,0.574979305267334
translation,115,132,results,al,is,very low,al is very low,0.6610904335975647
translation,115,132,results,gsimt - poisson model,maintains,high performance,gsimt - poisson model maintains high performance,0.681114137172699
translation,115,132,results,high performance,on,bleu scores,high performance on bleu scores,0.507616400718689
translation,115,132,results,al,has,gsimt - poisson model,al has gsimt - poisson model,0.6142203211784363
translation,115,132,results,very low,has,gsimt - poisson model,very low has gsimt - poisson model,0.5940101742744446
translation,115,132,results,results,Especially when,al,results Especially when al,0.7295658588409424
translation,115,133,results,performance,of,gsimt - possion - t5,performance of gsimt - possion - t5,0.6493508219718933
translation,115,133,results,performance,of,gsimt - poisson model,performance of gsimt - poisson model,0.5985231399536133
translation,115,133,results,gsimt - possion - t5,is,very similar,gsimt - possion - t5 is very similar,0.6153583526611328
translation,115,133,results,very similar,to,gsimt - poisson model,very similar to gsimt - poisson model,0.5865723490715027
translation,115,133,results,gsimt - poisson model,that updates,all the possible translation paths,gsimt - poisson model that updates all the possible translation paths,0.7254841327667236
translation,115,133,results,all the possible translation paths,instead of,top 5,all the possible translation paths instead of top 5,0.6402048468589783
translation,115,133,results,results,has,performance,results has performance,0.5972660779953003
translation,115,138,results,gsimt - poisson model,maintains,best performance,gsimt - poisson model maintains best performance,0.6901258230209351
translation,115,138,results,all three language pairs,has,gsimt - poisson model,all three language pairs has gsimt - poisson model,0.5884442925453186
translation,115,138,results,results,For,all three language pairs,results For all three language pairs,0.5279620289802551
translation,115,139,results,perform better,than,"state - of- the - art multimodal simt model dec - od ( caglayan et al. , 2020 )","perform better than state - of- the - art multimodal simt model dec - od ( caglayan et al. , 2020 )",0.5206676125526428
translation,115,139,results,visual features,has,gsimt models,visual features has gsimt models,0.5395482182502747
translation,115,139,results,gsimt models,has,perform better,gsimt models has perform better,0.5917763113975525
translation,115,146,results,wait -k,both,gsimt - possion - t5 and gsimt - poisson,wait -k both gsimt - possion - t5 and gsimt - poisson,0.7258285880088806
translation,115,146,results,gsimt - possion - t5 and gsimt - poisson,have,stronger generalisation ability,gsimt - possion - t5 and gsimt - poisson have stronger generalisation ability,0.5773016810417175
translation,115,146,results,stronger generalisation ability,in,test-only setup,stronger generalisation ability in test-only setup,0.5364218354225159
translation,115,146,results,results,Compared to,wait -k,results Compared to wait -k,0.6661258935928345
translation,115,153,results,poisson prior,over,buffer size,poisson prior over buffer size,0.7208408117294312
translation,115,153,results,poisson prior,fills in,gap,poisson prior fills in gap,0.6995534300804138
translation,115,153,results,gap,between,simultaneous mt and structural sequenceto-sequence learning,gap between simultaneous mt and structural sequenceto-sequence learning,0.6435492634773254
translation,115,153,results,results,introduction of,poisson prior,results introduction of poisson prior,0.6672852039337158
translation,116,193,ablation-analysis,multilinguality,of,parallel data,multilinguality of parallel data,0.5652818083763123
translation,116,193,ablation-analysis,multilinguality,is,crucial,multilinguality is crucial,0.6377207636833191
translation,116,193,ablation-analysis,ablation analysis,increasing,multilinguality,ablation analysis increasing multilinguality,0.7119392156600952
translation,116,135,experimental-setup,experimental setup,coded and tested in,tensorflow,experimental setup coded and tested in tensorflow,0.6413552761077881
translation,116,136,experimental-setup,trans-,as the basis of,translation models,trans- as the basis of translation models,0.684420645236969
translation,116,136,experimental-setup,former architecture,as the basis of,translation models,former architecture as the basis of translation models,0.6157334446907043
translation,116,136,experimental-setup,trans-,has,former architecture,trans- has former architecture,0.6178579926490784
translation,116,136,experimental-setup,experimental setup,use,trans-,experimental setup use trans-,0.5519439578056335
translation,116,137,experimental-setup,6 - layer encoder and decoder architecture,with,hidden size,6 - layer encoder and decoder architecture with hidden size,0.6150850057601929
translation,116,137,experimental-setup,6 - layer encoder and decoder architecture,with,8192 feedforward filter size,6 - layer encoder and decoder architecture with 8192 feedforward filter size,0.6244959235191345
translation,116,137,experimental-setup,hidden size,of,1024,hidden size of 1024,0.6436027884483337
translation,116,137,experimental-setup,experimental setup,use,6 - layer encoder and decoder architecture,experimental setup use 6 - layer encoder and decoder architecture,0.5934216976165771
translation,116,144,experimental-setup,first stage,use,adam optimizer,first stage use adam optimizer,0.6332687735557556
translation,116,144,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,116,144,experimental-setup,adam optimizer,with,weight decay,adam optimizer with weight decay,0.6082910299301147
translation,116,144,experimental-setup,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,116,144,experimental-setup,learning rate,of,0.0002,learning rate of 0.0002,0.5917128324508667
translation,116,144,experimental-setup,weight decay,of,0.2,weight decay of 0.2,0.6245629191398621
translation,116,144,experimental-setup,batch size,of,2048 examples,batch size of 2048 examples,0.5542460680007935
translation,116,144,experimental-setup,experimental setup,For,first stage,experimental setup For first stage,0.6107559204101562
translation,116,145,experimental-setup,learning rate schedule,consisting of,linear warmup,learning rate schedule consisting of linear warmup,0.7261075377464294
translation,116,145,experimental-setup,linear warmup,of,4000 steps,linear warmup of 4000 steps,0.5726643800735474
translation,116,145,experimental-setup,linear warmup,to,value 0.0002,linear warmup to value 0.0002,0.5329169631004333
translation,116,145,experimental-setup,4000 steps,to,value 0.0002,4000 steps to value 0.0002,0.5919365882873535
translation,116,145,experimental-setup,value 0.0002,followed by,linear decay,value 0.0002 followed by linear decay,0.5988783240318298
translation,116,145,experimental-setup,linear decay,for,1.2 million steps,linear decay for 1.2 million steps,0.6433269381523132
translation,116,146,experimental-setup,single dataset,to draw,whole batch,single dataset to draw whole batch,0.6107193231582642
translation,116,146,experimental-setup,equal probability,choose,monolingual or parallel,equal probability choose monolingual or parallel,0.6964321732521057
translation,116,151,experimental-setup,final phase,bucket,sequences,final phase bucket sequences,0.7502639889717102
translation,116,151,experimental-setup,final phase,group them up into,batches,final phase group them up into batches,0.6952484250068665
translation,116,151,experimental-setup,sequences,by,sequence length,sequences by sequence length,0.516106367111206
translation,116,151,experimental-setup,sequences,group them up into,batches,sequences group them up into batches,0.6639578938484192
translation,116,151,experimental-setup,batches,of,at most 2000 tokens,batches of at most 2000 tokens,0.6396684050559998
translation,116,151,experimental-setup,experimental setup,For,final phase,experimental setup For final phase,0.6013529300689697
translation,116,152,experimental-setup,model,with,8 nvidia v100 gpus,model with 8 nvidia v100 gpus,0.5951939821243286
translation,116,152,experimental-setup,model,training,synchronously,model training synchronously,0.7307947874069214
translation,116,152,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,116,153,experimental-setup,adamax optimizer,cut,learning rate,adamax optimizer cut learning rate,0.6348060965538025
translation,116,153,experimental-setup,learning rate,by,four once more,learning rate by four once more,0.6005687713623047
translation,116,153,experimental-setup,experimental setup,use,adamax optimizer,experimental setup use adamax optimizer,0.6156953573226929
translation,116,153,experimental-setup,experimental setup,cut,learning rate,experimental setup cut learning rate,0.6175236105918884
translation,116,7,model,single model,for,"5 lowresource languages ( gujarati , kazakh , nepali , sinhala , and turkish )","single model for 5 lowresource languages ( gujarati , kazakh , nepali , sinhala , and turkish )",0.58575838804245
translation,116,7,model,"5 lowresource languages ( gujarati , kazakh , nepali , sinhala , and turkish )",to and from,english directions,"5 lowresource languages ( gujarati , kazakh , nepali , sinhala , and turkish ) to and from english directions",0.609145998954773
translation,116,7,model,english directions,leverages,monolingual and auxiliary parallel data,english directions leverages monolingual and auxiliary parallel data,0.7077276110649109
translation,116,7,model,monolingual and auxiliary parallel data,from,other high- resource language pairs,monolingual and auxiliary parallel data from other high- resource language pairs,0.5430683493614197
translation,116,7,model,monolingual and auxiliary parallel data,via,three - stage training scheme,monolingual and auxiliary parallel data via three - stage training scheme,0.6124702095985413
translation,116,7,model,model,present,single model,model present single model,0.7257543206214905
translation,116,28,model,pre-training,propose,intermediate training stage,pre-training propose intermediate training stage,0.642984926700592
translation,116,28,model,intermediate training stage,leverages,offline back - translation,intermediate training stage leverages offline back - translation,0.6652919054031372
translation,116,28,model,offline back - translation,to generate,synthetic data,offline back - translation to generate synthetic data,0.6884423494338989
translation,116,28,model,synthetic data,from,x?en direction,synthetic data from x?en direction,0.5497105717658997
translation,116,28,model,synthetic data,to boost,en?x accuracy,synthetic data to boost en?x accuracy,0.6817167401313782
translation,116,28,model,model,after,pre-training,model after pre-training,0.7353293299674988
translation,116,8,results,all current state - of - the - art unsupervised baselines,achieving,gains,all current state - of - the - art unsupervised baselines achieving gains,0.6354426741600037
translation,116,8,results,gains,of,up to 14.4 bleu,gains of up to 14.4 bleu,0.5588586926460266
translation,116,8,results,outperform,has,all current state - of - the - art unsupervised baselines,outperform has all current state - of - the - art unsupervised baselines,0.5356280207633972
translation,116,8,results,results,has,outperform,results has outperform,0.642206609249115
translation,116,9,results,strong supervised baselines,for,various language pairs,strong supervised baselines for various language pairs,0.5420293211936951
translation,116,9,results,performance,of,current state - of - the - art supervised model,performance of current state - of - the - art supervised model,0.5473671555519104
translation,116,9,results,current state - of - the - art supervised model,for,ne?en,current state - of - the - art supervised model for ne?en,0.6579714417457581
translation,116,9,results,outperform,has,strong supervised baselines,outperform has strong supervised baselines,0.5568539500236511
translation,116,9,results,match,has,performance,match has performance,0.6163668632507324
translation,116,9,results,results,has,outperform,results has outperform,0.642206609249115
translation,116,29,results,outperforms,including,current state - of - the - art supervised model,outperforms including current state - of - the - art supervised model,0.6161017417907715
translation,116,29,results,variety of supervised and unsupervised baselines,including,current state - of - the - art supervised model,variety of supervised and unsupervised baselines including current state - of - the - art supervised model,0.6469763517379761
translation,116,29,results,current state - of - the - art supervised model,for,ne?en language pair,current state - of - the - art supervised model for ne?en language pair,0.5985510945320129
translation,116,29,results,approach,has,outperforms,approach has outperforms,0.6689831614494324
translation,116,29,results,outperforms,has,variety of supervised and unsupervised baselines,outperforms has variety of supervised and unsupervised baselines,0.5882590413093567
translation,116,177,results,first stage,of,training,first stage of training,0.604648768901825
translation,116,177,results,first stage,obtain,competitive bleu scores,first stage obtain competitive bleu scores,0.5895284414291382
translation,116,177,results,training,obtain,competitive bleu scores,training obtain competitive bleu scores,0.5518607497215271
translation,116,177,results,competitive bleu scores,for,x ? en translation directions,competitive bleu scores for x ? en translation directions,0.587009847164154
translation,116,177,results,competitive bleu scores,outperforming,all unsupervised models,competitive bleu scores outperforming all unsupervised models,0.6841102838516235
translation,116,177,results,all unsupervised models,as,mbart,all unsupervised models as mbart,0.5106659531593323
translation,116,177,results,mbart,for,language pairs kk?en and gu? en,mbart for language pairs kk?en and gu? en,0.6469665765762329
translation,116,177,results,results,After,first stage,results After first stage,0.6850811243057251
translation,116,178,results,second stage of training,see that,en?x language pairs,second stage of training see that en?x language pairs,0.615161657333374
translation,116,178,results,en?x language pairs,observe,large gains,en?x language pairs observe large gains,0.6011190414428711
translation,116,178,results,x?en directions,has,improve,x?en directions has improve,0.6381809115409851
translation,116,178,results,results,Upon completion of,second stage of training,results Upon completion of second stage of training,0.6962129473686218
translation,116,179,results,training,further improves,results,training further improves results,0.7017625570297241
translation,116,179,results,results,in,some language pairs,results in some language pairs,0.5222939848899841
translation,116,179,results,increase,of,+ 0.44 bleu,increase of + 0.44 bleu,0.5954757928848267
translation,116,179,results,results,final round of,training,results final round of training,0.6993798613548279
translation,116,180,results,supervised baselines,on,many of the language pairs,supervised baselines on many of the language pairs,0.47973939776420593
translation,116,180,results,baselines,has,our approach,baselines has our approach,0.5912060737609863
translation,116,180,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,116,180,results,outperforms,has,supervised baselines,outperforms has supervised baselines,0.591803252696991
translation,116,181,results,supervised mbart,on,six out of ten translation directions,supervised mbart on six out of ten translation directions,0.5209982395172119
translation,116,181,results,outperforms,has,supervised mbart,outperforms has supervised mbart,0.5950528383255005
translation,116,181,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,116,182,results,outperform,has,our own multilingual mt baseline,outperform has our own multilingual mt baseline,0.5371756553649902
translation,116,182,results,results,has,outperform,results has outperform,0.642206609249115
translation,116,194,results,more languages,for,monolingual data,more languages for monolingual data,0.5665197372436523
translation,116,194,results,more languages,potentially,harm,more languages potentially harm,0.6896494626998901
translation,116,194,results,more languages,in the presence of,multiple auxiliary language pairs,more languages in the presence of multiple auxiliary language pairs,0.6298657059669495
translation,116,194,results,multiple auxiliary language pairs,with,supervised data,multiple auxiliary language pairs with supervised data,0.5721493363380432
translation,116,194,results,harm,has,performance,harm has performance,0.6014720797538757
translation,116,194,results,supervised data,has,degradation,supervised data has degradation,0.5604426860809326
translation,116,194,results,degradation,has,vanishes,degradation has vanishes,0.6390323042869568
translation,116,194,results,results,Using,more languages,results Using more languages,0.668667197227478
translation,116,201,results,baseline,shows,inferior performance,baseline shows inferior performance,0.6962482333183289
translation,116,201,results,without synthetic parallel data,shows,inferior performance,without synthetic parallel data shows inferior performance,0.6694231033325195
translation,116,201,results,inferior performance,across,all language pairs,inferior performance across all language pairs,0.6924991607666016
translation,116,201,results,inferior performance,compared to,our approach,inferior performance compared to our approach,0.6715863943099976
translation,116,201,results,our approach,leveraging,synthetic parallel data,our approach leveraging synthetic parallel data,0.6784343719482422
translation,116,201,results,baseline,has,without synthetic parallel data,baseline has without synthetic parallel data,0.5682666897773743
translation,116,201,results,results,suggest,baseline,results suggest baseline,0.602773129940033
translation,116,207,results,results,see,two trends,results see two trends,0.6084599494934082
translation,116,208,results,worse,than,multilinguality,worse than multilinguality,0.6303645372390747
translation,116,208,results,multilinguality,inclusion of,cross-translation,multilinguality inclusion of cross-translation,0.6140059232711792
translation,116,208,results,improves,has,performance,improves has performance,0.5770372748374939
translation,116,208,results,results,increasing,multilinguality,results increasing multilinguality,0.6108011603355408
translation,116,215,results,outperform,has,competing unsupervised models,outperform has competing unsupervised models,0.5443388223648071
translation,117,40,ablation-analysis,modality reduction,in,alignart,modality reduction in alignart,0.5756389498710632
translation,117,40,ablation-analysis,modality reduction,addresses,token repetition issue,modality reduction addresses token repetition issue,0.6141205430030823
translation,117,40,ablation-analysis,token repetition issue,even without,sequence - level knowledge distillation,token repetition issue even without sequence - level knowledge distillation,0.7316747903823853
translation,117,40,ablation-analysis,ablation analysis,observe,modality reduction,ablation analysis observe modality reduction,0.5790461301803589
translation,117,163,ablation-analysis,accuracy,between,raw and distilled datasets,accuracy between raw and distilled datasets,0.6078355312347412
translation,117,163,ablation-analysis,raw and distilled datasets,shows,kd,raw and distilled datasets shows kd,0.6879174709320068
translation,117,163,ablation-analysis,multi-modality,of,each component,multi-modality of each component,0.6044809818267822
translation,117,163,ablation-analysis,kd,has,significantly decreases,kd has significantly decreases,0.6179191470146179
translation,117,163,ablation-analysis,significantly decreases,has,multi-modality,significantly decreases has multi-modality,0.6066940426826477
translation,117,165,ablation-analysis,alignment,have,accuracy bottlenecks,alignment have accuracy bottlenecks,0.497094988822937
translation,117,165,ablation-analysis,alignart,using,fast align and giza ++,alignart using fast align and giza ++,0.652206301689148
translation,117,165,ablation-analysis,fast align and giza ++,have,accuracy bottlenecks,fast align and giza ++ have accuracy bottlenecks,0.543499767780304
translation,117,165,ablation-analysis,accuracy bottlenecks,in,permutation and duplication predictors,accuracy bottlenecks in permutation and duplication predictors,0.5125240087509155
translation,117,165,ablation-analysis,alignment,has,alignart,alignment has alignart,0.5845854878425598
translation,117,165,ablation-analysis,ablation analysis,has,alignment,ablation analysis has alignment,0.5203499794006348
translation,117,196,ablation-analysis,deep encoder,assists,alignment estimation,deep encoder assists alignment estimation,0.5801780223846436
translation,117,196,ablation-analysis,shallow decoder,with,aligned inputs,shallow decoder with aligned inputs,0.6674183011054993
translation,117,196,ablation-analysis,lower impact,on,performance degeneration,lower impact on performance degeneration,0.596402645111084
translation,117,196,ablation-analysis,aligned inputs,has,lower impact,aligned inputs has lower impact,0.5722399950027466
translation,117,196,ablation-analysis,ablation analysis,indicate,deep encoder,ablation analysis indicate deep encoder,0.5877662897109985
translation,117,196,ablation-analysis,ablation analysis,indicate,shallow decoder,ablation analysis indicate shallow decoder,0.5887740850448608
translation,117,201,ablation-analysis,filtering ratio,up to,20 %,filtering ratio up to 20 %,0.655559778213501
translation,117,201,ablation-analysis,20 %,preserves,performance,20 % preserves performance,0.7716395258903503
translation,117,201,ablation-analysis,performance,thanks to,noise filtering capability,performance thanks to noise filtering capability,0.45295900106430054
translation,117,201,ablation-analysis,ablation analysis,increasing,filtering ratio,ablation analysis increasing filtering ratio,0.7447075843811035
translation,117,144,experimental-setup,d model /d hidden,to,512/2048,d model /d hidden to 512/2048,0.6220571398735046
translation,117,144,experimental-setup,dropout rate,to,0.3,dropout rate to 0.3,0.5253456830978394
translation,117,144,experimental-setup,experimental setup,set,d model /d hidden,experimental setup set d model /d hidden,0.69719398021698
translation,117,144,experimental-setup,experimental setup,set,dropout rate,experimental setup set dropout rate,0.62116938829422
translation,117,145,experimental-setup,number of heads,in,multi-head attention modules,number of heads in multi-head attention modules,0.5286773443222046
translation,117,145,experimental-setup,multi-head attention modules,is,8,multi-head attention modules is 8,0.5557761192321777
translation,117,145,experimental-setup,8,except for,last attention module,8 except for last attention module,0.5748101472854614
translation,117,145,experimental-setup,last attention module,of,permutation predictor,last attention module of permutation predictor,0.5869146585464478
translation,117,145,experimental-setup,permutation predictor,is,1,permutation predictor is 1,0.618503212928772
translation,117,145,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,117,146,experimental-setup,batch size,to,64 k tokens,batch size to 64 k tokens,0.559291422367096
translation,117,146,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,117,147,experimental-setup,50 k steps,on,en-de / en- ro datasets,50 k steps on en-de / en- ro datasets,0.6495426297187805
translation,117,147,experimental-setup,300k /,has,50 k steps,300k / has 50 k steps,0.6017493009567261
translation,117,150,experimental-setup,learning rate scheduling,starting from,10 ?7,learning rate scheduling starting from 10 ?7,0.6633201837539673
translation,117,150,experimental-setup,learning rate scheduling,warms up to,5e - 4,learning rate scheduling warms up to 5e - 4,0.6896962523460388
translation,117,150,experimental-setup,5e - 4,in,10 k steps,5e - 4 in 10 k steps,0.5806383490562439
translation,117,150,experimental-setup,experimental setup,has,learning rate scheduling,experimental setup has learning rate scheduling,0.5023586750030518
translation,117,151,experimental-setup,label smoothing technique,with,ls = 0.1,label smoothing technique with ls = 0.1,0.6196907758712769
translation,117,151,experimental-setup,ls = 0.1,for,target token distribution,ls = 0.1 for target token distribution,0.6091130971908569
translation,117,151,experimental-setup,experimental setup,use,label smoothing technique,experimental setup use label smoothing technique,0.5972716212272644
translation,117,31,experiments,alignart,divides,machine translation task,alignart divides machine translation task,0.6529144048690796
translation,117,31,experiments,machine translation task,into,alignment estimation,machine translation task into alignment estimation,0.5190055966377258
translation,117,31,experiments,machine translation task,into,non-autoregressive translation,machine translation task into non-autoregressive translation,0.5107351541519165
translation,117,31,experiments,non-autoregressive translation,under,given alignments,non-autoregressive translation under given alignments,0.6391956210136414
translation,117,149,experiments,optimization,use,"adam optimizer ( kingma and ba , 2015 )","optimization use adam optimizer ( kingma and ba , 2015 )",0.6157730221748352
translation,117,149,experiments,"adam optimizer ( kingma and ba , 2015 )",with,"? = ( 0.9 , 0.98 ) and = 10 ?8","adam optimizer ( kingma and ba , 2015 ) with ? = ( 0.9 , 0.98 ) and = 10 ?8",0.633030116558075
translation,117,152,experiments,translation latency,measured on,nvidia tesla v100 gpu,translation latency measured on nvidia tesla v100 gpu,0.6274996995925903
translation,117,159,experiments,non-iterative nart,for,explicit modality reduction,non-iterative nart for explicit modality reduction,0.6054901480674744
translation,117,159,experiments,alignart,shows,best performance,alignart shows best performance,0.7060884237289429
translation,117,159,experiments,best performance,on,en?de,best performance on en?de,0.6581732034683228
translation,117,159,experiments,best performance,on,ro?en,best performance on ro?en,0.6473133563995361
translation,117,159,experiments,non-iterative nart,has,alignart,non-iterative nart has alignart,0.5729031562805176
translation,117,159,experiments,explicit modality reduction,has,alignart,explicit modality reduction has alignart,0.6047444343566895
translation,117,6,model,alignart,leverages,full alignment information,alignart leverages full alignment information,0.6792232990264893
translation,117,6,model,full alignment information,to explicitly reduce,modality of the target distribution,full alignment information to explicitly reduce modality of the target distribution,0.6889132261276245
translation,117,6,model,model,introduce,alignart,model introduce alignart,0.6782151460647583
translation,117,7,model,alignart,divides,machine translation task,alignart divides machine translation task,0.6529144048690796
translation,117,7,model,machine translation task,into,alignment estimation,machine translation task into alignment estimation,0.5190055966377258
translation,117,7,model,machine translation task,into,translation,machine translation task into translation,0.5297567248344421
translation,117,7,model,translation,with,aligned decoder inputs,translation with aligned decoder inputs,0.6538194417953491
translation,117,7,model,model,has,alignart,model has alignart,0.61158287525177
translation,117,8,model,alignment estimation problem,propose,novel alignment decomposition method,alignment estimation problem propose novel alignment decomposition method,0.6149694919586182
translation,117,8,model,model,To alleviate,alignment estimation problem,model To alleviate alignment estimation problem,0.6843101382255554
translation,117,30,model,noniterative nart model,mitigates,multimodality problem,noniterative nart model mitigates multimodality problem,0.7329162955284119
translation,117,30,model,multimodality problem,by utilizing,complete information,multimodality problem by utilizing complete information,0.6634252071380615
translation,117,30,model,complete information,in,word alignments,complete information in word alignments,0.47466933727264404
translation,117,30,model,alignart,has,noniterative nart model,alignart has noniterative nart model,0.5770589709281921
translation,117,30,model,model,propose,alignart,model propose alignart,0.7027339339256287
translation,117,33,model,module,called,aligner,module called aligner,0.6995338797569275
translation,117,33,model,module,augmented to,"nat ( gu et al. , 2018 )","module augmented to nat ( gu et al. , 2018 )",0.7255207896232605
translation,117,33,model,aligner,augmented to,"nat ( gu et al. , 2018 )","aligner augmented to nat ( gu et al. , 2018 )",0.6950138807296753
translation,117,33,model,"nat ( gu et al. , 2018 )",estimates,alignments,"nat ( gu et al. , 2018 ) estimates alignments",0.5776132941246033
translation,117,33,model,alignments,to generate,aligned decoder inputs,alignments to generate aligned decoder inputs,0.6504054665565491
translation,117,33,model,alignart,has,module,alignart has module,0.5999457836151123
translation,117,33,model,model,In,alignart,model In alignart,0.6087539196014404
translation,117,36,model,alignment decomposition,factorizes,alignment process,alignment decomposition factorizes alignment process,0.7623786926269531
translation,117,36,model,alignment process,into,three subprocesses,alignment process into three subprocesses,0.5962676405906677
translation,117,36,model,model,propose,alignment decomposition,model propose alignment decomposition,0.6523897647857666
translation,117,118,model,deep-shallow architecture,improves,inference speed,deep-shallow architecture improves inference speed,0.6631603837013245
translation,117,118,model,model,has,deep-shallow architecture,model has deep-shallow architecture,0.5397098064422607
translation,117,218,model,alignment decomposition method,for,effective alignment estimation,alignment decomposition method for effective alignment estimation,0.5628539323806763
translation,117,218,model,effective alignment estimation,in,nart,effective alignment estimation in nart,0.5352599024772644
translation,117,218,model,model,propose,alignment decomposition method,model propose alignment decomposition method,0.6588872671127319
translation,117,220,model,full alignment information,to directly reduce,degree of the multimodality,full alignment information to directly reduce degree of the multimodality,0.6200106143951416
translation,117,220,model,full alignment information,propose,alignment decomposition method,full alignment information propose alignment decomposition method,0.6318744421005249
translation,117,220,model,degree of the multimodality,in,non-iterative nart,degree of the multimodality in non-iterative nart,0.5541377067565918
translation,117,220,model,alignment decomposition method,for,alignment estimation,alignment decomposition method for alignment estimation,0.5814133286476135
translation,117,220,model,model,leverage,full alignment information,model leverage full alignment information,0.7210555672645569
translation,117,220,model,model,propose,alignment decomposition method,model propose alignment decomposition method,0.6588872671127319
translation,117,9,results,explicit modality reduction,on,wmt14 en?de and wmt16 ro?en,explicit modality reduction on wmt14 en?de and wmt16 ro?en,0.5864429473876953
translation,117,9,results,alignart,has,outperforms,alignart has outperforms,0.6359605193138123
translation,117,9,results,outperforms,has,previous non-iterative nart models,outperforms has previous non-iterative nart models,0.5824230313301086
translation,117,10,results,alignart,achieves,bleu scores,alignart achieves bleu scores,0.6945581436157227
translation,117,10,results,bleu scores,comparable to,state - of- the - art connectionist temporal classification,bleu scores comparable to state - of- the - art connectionist temporal classification,0.5823385119438171
translation,117,10,results,models,on,wmt14 en?de,models on wmt14 en?de,0.6441816091537476
translation,117,10,results,results,has,alignart,results has alignart,0.525576114654541
translation,117,11,results,alignart,effectively addresses,token repetition problem,alignart effectively addresses token repetition problem,0.6941983699798584
translation,117,11,results,token repetition problem,even without,sequence - level knowledge distillation,token repetition problem even without sequence - level knowledge distillation,0.6991639137268066
translation,117,11,results,results,observe,alignart,results observe alignart,0.5482930541038513
translation,117,38,results,previous non-iterative nart models,of,explicit modality reduction,previous non-iterative nart models of explicit modality reduction,0.5169339179992676
translation,117,38,results,explicit modality reduction,on,wmt14 en?de and wmt16 ro?en,explicit modality reduction on wmt14 en?de and wmt16 ro?en,0.5864429473876953
translation,117,38,results,alignart,has,outperforms,alignart has outperforms,0.6359605193138123
translation,117,38,results,outperforms,has,previous non-iterative nart models,outperforms has previous non-iterative nart models,0.5824230313301086
translation,117,38,results,results,show,alignart,results show alignart,0.589796245098114
translation,117,39,results,alignart,achieves,performance,alignart achieves performance,0.7080808877944946
translation,117,39,results,performance,comparable to,recent stateof - the- art non-iterative nart model,performance comparable to recent stateof - the- art non-iterative nart model,0.6254350543022156
translation,117,39,results,recent stateof - the- art non-iterative nart model,on,wmt14 en?de,recent stateof - the- art non-iterative nart model on wmt14 en?de,0.6166175007820129
translation,117,39,results,results,has,alignart,results has alignart,0.525576114654541
translation,117,158,results,alignart,shows,performance,alignart shows performance,0.7112216949462891
translation,117,158,results,baselines,on,en?de and ro?en,baselines on en?de and ro?en,0.6208930611610413
translation,117,158,results,performance,similar to,glat,performance similar to glat,0.6510321497917175
translation,117,158,results,glat,on,de?en,glat on de?en,0.7080215811729431
translation,117,158,results,alignart,has,outperforms,alignart has outperforms,0.6359605193138123
translation,117,158,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,117,158,results,results,has,alignart,results has alignart,0.525576114654541
translation,117,164,results,alig-nart,shows,marginally reduced accuracy,alig-nart shows marginally reduced accuracy,0.6979851126670837
translation,117,164,results,alig-nart,shows,high prediction accuracy,alig-nart shows high prediction accuracy,0.6771216988563538
translation,117,164,results,marginally reduced accuracy,on,raw dataset,marginally reduced accuracy on raw dataset,0.557906985282898
translation,117,164,results,high prediction accuracy,in,each component,high prediction accuracy in each component,0.5033893585205078
translation,117,164,results,high prediction accuracy,resulting in,increased bleu scores,high prediction accuracy resulting in increased bleu scores,0.6207204461097717
translation,117,164,results,each component,on,distillation set,each component on distillation set,0.5835142135620117
translation,117,164,results,kd,has,alig-nart,kd has alig-nart,0.7003473043441772
translation,117,164,results,results,After,kd,results After kd,0.5709056258201599
translation,117,176,results,exception,in,giza ++,exception in giza ++,0.5710713267326355
translation,117,176,results,exception,where,many - toone mapping,exception where many - toone mapping,0.6798790693283081
translation,117,176,results,giza ++,where,many - toone mapping,giza ++ where many - toone mapping,0.645523190498352
translation,117,176,results,many - toone mapping,resulting in,performance,many - toone mapping resulting in performance,0.6600597500801086
translation,117,176,results,performance,without,grouping predictor,performance without grouping predictor,0.7662627100944519
translation,117,176,results,many - toone mapping,has,does not exist,many - toone mapping has does not exist,0.6144353151321411
translation,117,180,results,wmt14 en-de.,For,en?de,wmt14 en-de. For en?de,0.6713969111442566
translation,117,180,results,wmt14 en-de.,For,alignart,wmt14 en-de. For alignart,0.6451001167297363
translation,117,180,results,alignart,using,fast,alignart using fast,0.7153498530387878
translation,117,180,results,alignart,deep-shallow NAT with,kd,alignart deep-shallow NAT with kd,0.7916686534881592
translation,117,180,results,align,without,kd,align without kd,0.7235463857650757
translation,117,180,results,higher bleu scores,than,previous models,higher bleu scores than previous models,0.5328927636146545
translation,117,180,results,en?de,has,alignart,en?de has alignart,0.6519001722335815
translation,117,180,results,fast,has,align,fast has align,0.6121845841407776
translation,117,180,results,kd,has,higher bleu scores,kd has higher bleu scores,0.5760666131973267
translation,117,180,results,previous models,has,without kd,previous models has without kd,0.6217362284660339
translation,117,180,results,results,For,en?de,results For en?de,0.7657796144485474
translation,117,193,results,alignart,without,cross attention module,alignart without cross attention module,0.7149865031242371
translation,117,193,results,smaller impact,on,bleu score,smaller impact on bleu score,0.567879855632782
translation,117,193,results,smaller impact,than,deep-shallow nat,smaller impact than deep-shallow nat,0.604546308517456
translation,117,193,results,bleu score,than,deep-shallow nat,bleu score than deep-shallow nat,0.4893554449081421
translation,117,193,results,cross attention module,has,smaller impact,cross attention module has smaller impact,0.5917044281959534
translation,117,193,results,results,has,alignart,results has alignart,0.525576114654541
translation,117,200,results,filtering out,of,5 %,filtering out of 5 %,0.6402924060821533
translation,117,200,results,5 %,improves,bleu score,5 % improves bleu score,0.6727932691574097
translation,117,200,results,of the samples,improves,bleu score,of the samples improves bleu score,0.6738553643226624
translation,117,200,results,bleu score,in,both the directions,bleu score in both the directions,0.5375703573226929
translation,117,200,results,5 %,has,of the samples,5 % has of the samples,0.6157119870185852
translation,117,200,results,results,observe,filtering out,results observe filtering out,0.6180892586708069
translation,117,221,results,alignart with giza ++,shows,performance,alignart with giza ++ shows performance,0.685121476650238
translation,117,221,results,performance,comparable to,recent ctcbased implicit dependency modeling approach,performance comparable to recent ctcbased implicit dependency modeling approach,0.6074020862579346
translation,117,221,results,performance,comparable to,modality reduction capability,performance comparable to modality reduction capability,0.6742560267448425
translation,117,221,results,recent ctcbased implicit dependency modeling approach,on,wmt14 en- de,recent ctcbased implicit dependency modeling approach on wmt14 en- de,0.6439117193222046
translation,117,221,results,results,has,alignart with giza ++,results has alignart with giza ++,0.637945294380188
translation,118,117,baselines,encoder,with,asr task,encoder with asr task,0.6488997340202332
translation,118,117,baselines,second,leverage,sequence - level knowledge distillation,second leverage sequence - level knowledge distillation,0.7166897654533386
translation,118,117,baselines,sequence - level knowledge distillation,with,"text machine translation model ( ren et al. , 2020 )","sequence - level knowledge distillation with text machine translation model ( ren et al. , 2020 )",0.5841926336288452
translation,118,123,baselines,fixed pre-decision step size,of,320 ms,fixed pre-decision step size of 320 ms,0.596964418888092
translation,118,125,baselines,two inference methods,used for,wait -k,two inference methods used for wait -k,0.7379298806190491
translation,118,125,baselines,conventional beam search,on,target tail,conventional beam search on target tail,0.5724243521690369
translation,118,125,baselines,speculative beam search ( sbs ),with,beam size,speculative beam search ( sbs ) with beam size,0.6386357545852661
translation,118,125,baselines,beam size,of,5,beam size of 5,0.7073217034339905
translation,118,125,baselines,baselines,has,two inference methods,baselines has two inference methods,0.5245853662490845
translation,118,113,experiments,relative positional attention,for,speech encoder,relative positional attention for speech encoder,0.6077843904495239
translation,118,113,experiments,"shaw et al. , 2018 )",for,speech encoder,"shaw et al. , 2018 ) for speech encoder",0.610958456993103
translation,118,113,experiments,cosine positional embedding,for,decoder,cosine positional embedding for decoder,0.6277427077293396
translation,118,113,experiments,relative positional attention,has,"shaw et al. , 2018 )","relative positional attention has shaw et al. , 2018 )",0.525745689868927
translation,118,124,experiments,sequence - level knowledge distillation,with,text translation model,sequence - level knowledge distillation with text translation model,0.5772760510444641
translation,118,5,model,model,jointly optimize,policy and translation models,model jointly optimize policy and translation models,0.7363331913948059
translation,118,6,model,all possible read - write simultaneous translation action paths,adapt,"online automatic speech recognition ( asr ) model , rnn -t","all possible read - write simultaneous translation action paths adapt online automatic speech recognition ( asr ) model , rnn -t",0.7785112857818604
translation,118,6,model,model,To effectively consider,all possible read - write simultaneous translation action paths,model To effectively consider all possible read - write simultaneous translation action paths,0.6821900010108948
translation,118,6,model,model,adapt,"online automatic speech recognition ( asr ) model , rnn -t","model adapt online automatic speech recognition ( asr ) model , rnn -t",0.8032791614532471
translation,118,7,model,caat,introduce,novel latency loss,caat introduce novel latency loss,0.6377525329589844
translation,118,7,model,novel latency loss,optimized by,forward - backward algorithm,novel latency loss optimized by forward - backward algorithm,0.7514827251434326
translation,118,7,model,model,To make,caat,model To make caat,0.6740193367004395
translation,118,7,model,model,introduce,novel latency loss,model introduce novel latency loss,0.6716660261154175
translation,118,8,model,caat,with,transformer,caat with transformer,0.7324861884117126
translation,118,8,model,model,implement,caat,model implement caat,0.7535547018051147
translation,118,14,model,policy and translation model jointly,by expanding,target sequences,policy and translation model jointly by expanding target sequences,0.6495897769927979
translation,118,14,model,target sequences,with,blank symbols,target sequences with blank symbols,0.6459617018699646
translation,118,14,model,blank symbols,for,read actions,blank symbols for read actions,0.6457667350769043
translation,118,14,model,model,optimize,policy and translation model jointly,model optimize policy and translation model jointly,0.7719947099685669
translation,118,19,model,cross attention mechanism,from,target history representation,cross attention mechanism from target history representation,0.518101692199707
translation,118,19,model,cross attention mechanism,viewed as,rnn -t,cross attention mechanism viewed as rnn -t,0.5568467378616333
translation,118,19,model,target history representation,in,attention - based encoder-decoder,target history representation in attention - based encoder-decoder,0.4904066026210785
translation,118,19,model,rnn -t,with,joiner,rnn -t with joiner,0.722754180431366
translation,118,19,model,joiner,being augmented by,cross attention mechanism,joiner being augmented by cross attention mechanism,0.8039422631263733
translation,118,19,model,cross attention mechanism,resulting in,cross attention augmented transducer ( caat ),cross attention mechanism resulting in cross attention augmented transducer ( caat ),0.7062185406684875
translation,118,19,model,model,separate,cross attention mechanism,model separate cross attention mechanism,0.6738102436065674
translation,118,182,model,different beam sizes,introduced for,intra-decision ( b 1 ) and inter-decision ( b 2 ) pruning,different beam sizes introduced for intra-decision ( b 1 ) and inter-decision ( b 2 ) pruning,0.6760238409042358
translation,118,182,model,timely determination,of,outputs,timely determination of outputs,0.6310709714889526
translation,118,182,model,model,has,different beam sizes,model has different beam sizes,0.5532578229904175
translation,118,129,results,offline models,of,caat and wait -k,offline models of caat and wait -k,0.6147376894950867
translation,118,129,results,offline models,of,caat,offline models of caat,0.6067366600036621
translation,118,129,results,caat and wait -k,obtain,similar bleu,caat and wait -k obtain similar bleu,0.5927165150642395
translation,118,129,results,similar bleu,suggesting that,adapted architecture,similar bleu suggesting that adapted architecture,0.650302529335022
translation,118,129,results,adapted architecture,of,caat,adapted architecture of caat,0.6227906346321106
translation,118,129,results,adapted architecture,performs,comparably,adapted architecture performs comparably,0.619125247001648
translation,118,129,results,comparably,with,conventional transformer,comparably with conventional transformer,0.6990782022476196
translation,118,129,results,results,has,offline models,results has offline models,0.5481967329978943
translation,118,130,results,sbs,produce,lower latency,sbs produce lower latency,0.6626179218292236
translation,118,130,results,same wait step k,has,sbs,same wait step k has sbs,0.6304933428764343
translation,118,130,results,results,With,same wait step k,results With same wait step k,0.6911676526069641
translation,118,138,results,performance,of,translation model,performance of translation model,0.5954796671867371
translation,118,138,results,translation model,with,unidirectional encoder,translation model with unidirectional encoder,0.6152279376983643
translation,118,138,results,translation model,drops,2 - 3 bleu points,translation model drops 2 - 3 bleu points,0.7146353125572205
translation,118,138,results,2 - 3 bleu points,compared to,full-sentence encoder,2 - 3 bleu points compared to full-sentence encoder,0.6148650646209717
translation,118,138,results,gradually narrowed,introduction of,right context,gradually narrowed introduction of right context,0.6983798146247864
translation,118,138,results,increase,introduction of,right context,increase introduction of right context,0.663569986820221
translation,118,144,results,beam search,in,one decision step,beam search in one decision step,0.5538755655288696
translation,118,144,results,beam search,brings,improvement,beam search brings improvement,0.6527250409126282
translation,118,144,results,one decision step,brings,improvement,one decision step brings improvement,0.6575303673744202
translation,118,144,results,improvement,of,about 0.7 bleu,improvement of about 0.7 bleu,0.5524185299873352
translation,118,144,results,about 0.7 bleu,over,greedy search,about 0.7 bleu over greedy search,0.6508784890174866
translation,118,144,results,results,find that,beam search,results find that beam search,0.7029019594192505
translation,119,163,ablation-analysis,cmwt dataset,decrease,bleu scores,cmwt dataset decrease bleu scores,0.6304507255554199
translation,119,163,ablation-analysis,bleu scores,has,- 0.6 bleu,bleu scores has - 0.6 bleu,0.5780942440032959
translation,119,163,ablation-analysis,ablation analysis,has,cmwt dataset,ablation analysis has cmwt dataset,0.5084866881370544
translation,119,196,ablation-analysis,bt and ft,further improve,bleu score,bt and ft further improve bleu score,0.6460663080215454
translation,119,196,ablation-analysis,bleu score,by,1.3 bleu scores,bleu score by 1.3 bleu scores,0.5408342480659485
translation,119,196,ablation-analysis,ablation analysis,After adding,bt and ft,ablation analysis After adding bt and ft,0.6476587057113647
translation,119,202,ablation-analysis,rule- based post-processing procedure,on,punctuation,rule- based post-processing procedure on punctuation,0.5286605358123779
translation,119,202,ablation-analysis,punctuation,improve,bleu score,punctuation improve bleu score,0.6478549838066101
translation,119,202,ablation-analysis,bleu score,on,development set,bleu score on development set,0.5230231285095215
translation,119,202,ablation-analysis,bleu score,by,0.5 point,bleu score by 0.5 point,0.5607974529266357
translation,119,194,baselines,baseline model,trained on,bilingual data and r2l data,baseline model trained on bilingual data and r2l data,0.7302066087722778
translation,119,194,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,119,29,experimental-setup,uncertaintybased sampling,to select,content-relevant and high-quality data,uncertaintybased sampling to select content-relevant and high-quality data,0.7077166438102722
translation,119,29,experimental-setup,"jiao et al. , 2021 )",to select,content-relevant and high-quality data,"jiao et al. , 2021 ) to select content-relevant and high-quality data",0.6985176801681519
translation,119,29,experimental-setup,content-relevant and high-quality data,from,parallel and monolingual corpora,content-relevant and high-quality data from parallel and monolingual corpora,0.5621772408485413
translation,119,29,experimental-setup,language coverage bias,has,data rejuvenation,language coverage bias has data rejuvenation,0.47969529032707214
translation,119,29,experimental-setup,uncertaintybased sampling,has,"jiao et al. , 2021 )","uncertaintybased sampling has jiao et al. , 2021 )",0.5576198697090149
translation,119,29,experimental-setup,experimental setup,apply,language coverage bias,experimental setup apply language coverage bias,0.6183443069458008
translation,119,46,experimental-setup,8?16 nvidia v100 gpus,have,32 gb memory,8?16 nvidia v100 gpus have 32 gb memory,0.5318493843078613
translation,119,47,experimental-setup,adam optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adam optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6311911940574646
translation,119,47,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,119,48,experimental-setup,gradient accumulation,used,high gpu memory consumption,gradient accumulation used high gpu memory consumption,0.5770444273948669
translation,119,48,experimental-setup,gradient accumulation,due to,high gpu memory consumption,gradient accumulation due to high gpu memory consumption,0.65511155128479
translation,119,48,experimental-setup,experimental setup,has,gradient accumulation,experimental setup has gradient accumulation,0.4636458456516266
translation,119,50,experimental-setup,training process,conduct,training,training process conduct training,0.7803307175636292
translation,119,50,experimental-setup,training,with,half precision floating point ( fp16 ),training with half precision floating point ( fp16 ),0.6454789638519287
translation,119,50,experimental-setup,experimental setup,To speed up,training process,experimental setup To speed up training process,0.7369841933250427
translation,119,51,experimental-setup,max learning rate,to,0.0007,max learning rate to 0.0007,0.5573468208312988
translation,119,51,experimental-setup,warmup-steps,to,16000,warmup-steps to 16000,0.6294600963592529
translation,119,51,experimental-setup,experimental setup,set,max learning rate,experimental setup set max learning rate,0.6232877373695374
translation,119,51,experimental-setup,experimental setup,set,warmup-steps,experimental setup set warmup-steps,0.6490351557731628
translation,119,52,experimental-setup,dropout probabilities,set to,0.3,dropout probabilities set to 0.3,0.6935458183288574
translation,119,52,experimental-setup,experimental setup,has,dropout probabilities,experimental setup has dropout probabilities,0.508775532245636
translation,119,58,experimental-setup,batch size,is,5120,batch size is 5120,0.5932954549789429
translation,119,58,experimental-setup,5120,with,16 gpus,5120 with 16 gpus,0.5973559617996216
translation,119,58,experimental-setup,update-freq,is,1,update-freq is 1,0.6646876931190491
translation,119,58,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,119,58,experimental-setup,experimental setup,has,update-freq,experimental setup has update-freq,0.599852979183197
translation,119,59,experimental-setup,models,with,400k updates,models with 400k updates,0.6877122521400452
translation,119,59,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,119,61,experimental-setup,empirically designed,based on,transformer - big models,empirically designed based on transformer - big models,0.6596265435218811
translation,119,61,experimental-setup,empirically designed,with,24 encoder layers,empirically designed with 24 encoder layers,0.6296231746673584
translation,119,61,experimental-setup,experimental setup,has,large model,experimental setup has large model,0.5580027103424072
translation,119,62,experimental-setup,batch size,is,4096,batch size is 4096,0.6176068782806396
translation,119,62,experimental-setup,4096,with,8,4096 with 8,0.6624259352684021
translation,119,62,experimental-setup,4096,with,gpus,4096 with gpus,0.577505886554718
translation,119,62,experimental-setup,update-freq,is,4,update-freq is 4,0.687116801738739
translation,119,62,experimental-setup,8,has,gpus,8 has gpus,0.5410045981407166
translation,119,62,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,119,63,experimental-setup,models,with,400k updates,models with 400k updates,0.6877122521400452
translation,119,63,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,119,68,experimental-setup,number of encoder and decoder layers,are,20 and 6,number of encoder and decoder layers are 20 and 6,0.5936603546142578
translation,119,68,experimental-setup,experimental setup,has,number of encoder and decoder layers,experimental setup has number of encoder and decoder layers,0.5076038837432861
translation,119,69,experimental-setup,number of head,is,16,number of head is 16,0.5265998244285583
translation,119,69,experimental-setup,experimental setup,has,number of head,experimental setup has number of head,0.5165041089057922
translation,119,72,experimental-setup,large - ffn models,set,batch size,large - ffn models set batch size,0.668073832988739
translation,119,72,experimental-setup,large - ffn models,set,update-freq,large - ffn models set update-freq,0.6672699451446533
translation,119,72,experimental-setup,batch size,to,8192 toknes per gpu,batch size to 8192 toknes per gpu,0.5251535773277283
translation,119,72,experimental-setup,update-freq,set to,8,update-freq set to 8,0.7375726699829102
translation,119,72,experimental-setup,parameter,set to,8,parameter set to 8,0.7713853120803833
translation,119,72,experimental-setup,update-freq,has,parameter,update-freq has parameter,0.5927608013153076
translation,119,72,experimental-setup,experimental setup,In training,large - ffn models,experimental setup In training large - ffn models,0.7739437818527222
translation,119,73,experimental-setup,8 gpus,for,about 3 days,8 gpus for about 3 days,0.5410762429237366
translation,119,73,experimental-setup,experimental setup,trained on,8 gpus,experimental setup trained on 8 gpus,0.6363903880119324
translation,119,77,experimental-setup,raw data,applied,series of opensource / in- house scripts,raw data applied series of opensource / in- house scripts,0.7103198170661926
translation,119,77,experimental-setup,series of opensource / in- house scripts,including,non-character filter,series of opensource / in- house scripts including non-character filter,0.7022116780281067
translation,119,77,experimental-setup,series of opensource / in- house scripts,including,punctuation normalization,series of opensource / in- house scripts including punctuation normalization,0.6787199378013611
translation,119,77,experimental-setup,series of opensource / in- house scripts,including,tokenization / segmentation,series of opensource / in- house scripts including tokenization / segmentation,0.7053738236427307
translation,119,77,experimental-setup,experimental setup,To process,raw data,experimental setup To process raw data,0.6958515644073486
translation,119,8,experiments,uncertainty - based sampling approaches,to select,content-relevant and highquality data,uncertainty - based sampling approaches to select content-relevant and highquality data,0.7226870059967041
translation,119,8,experiments,content-relevant and highquality data,from,large parallel and monolingual corpora,content-relevant and highquality data from large parallel and monolingual corpora,0.5229565501213074
translation,119,8,experiments,language coverage bias,has,data rejuvenation,language coverage bias has data rejuvenation,0.47969529032707214
translation,119,12,experiments,constrained chinese ? english system,achieves,33.4 case-sensitive bleu score,constrained chinese ? english system achieves 33.4 case-sensitive bleu score,0.6225928068161011
translation,119,12,experiments,highest,among,all submissions,highest among all submissions,0.5426722764968872
translation,119,13,experiments,german ? english system,ranked at,second place,german ? english system ranked at second place,0.6963188052177429
translation,119,49,experiments,models,with,regular batch training,models with regular batch training,0.6115850806236267
translation,119,49,experiments,significantly outperformed,has,models,significantly outperformed has models,0.6238552927970886
translation,119,7,model,different data augmentation methods,including,backtranslation,different data augmentation methods including backtranslation,0.6939918994903564
translation,119,7,model,different data augmentation methods,including,forward - translation,different data augmentation methods including forward - translation,0.6905118227005005
translation,119,7,model,different data augmentation methods,including,right- toleft training,different data augmentation methods including right- toleft training,0.6755901575088501
translation,119,7,model,model,combine,different data augmentation methods,model combine different data augmentation methods,0.72818922996521
translation,119,20,model,mixed attention strategy,by combining,relative position,mixed attention strategy by combining relative position,0.6935596466064453
translation,119,20,model,relative position,with,original one,relative position with original one,0.6777704358100891
translation,119,20,model,representations,of,relative positions,representations of relative positions,0.5899862051010132
translation,119,20,model,model,proposed,mixed attention strategy,model proposed mixed attention strategy,0.7390032410621643
translation,119,23,model,data augmentation,adapt,backtranslation ( bt ),data augmentation adapt backtranslation ( bt ),0.8260038495063782
translation,119,23,model,data augmentation,adapt,forwardtranslation ( ft ),data augmentation adapt forwardtranslation ( ft ),0.8187601566314697
translation,119,23,model,data augmentation,adapt,rightto-left ( r2l ) techniques,data augmentation adapt rightto-left ( r2l ) techniques,0.8103727698326111
translation,119,23,model,rightto-left ( r2l ) techniques,to generate,large-scale synthetic training data,rightto-left ( r2l ) techniques to generate large-scale synthetic training data,0.6781683564186096
translation,119,23,model,model,In terms of,data augmentation,model In terms of data augmentation,0.6946539282798767
translation,119,23,model,model,adapt,backtranslation ( bt ),model adapt backtranslation ( bt ),0.8436747193336487
translation,119,24,model,noise,to,synthetic source sentence,noise to synthetic source sentence,0.5291985273361206
translation,119,24,model,synthetic source sentence,to take advantage of,large-scale monolingual text,synthetic source sentence to take advantage of large-scale monolingual text,0.5942777395248413
translation,119,25,model,tagged bt mechanism,to help,model,tagged bt mechanism to help model,0.5946183204650879
translation,119,25,model,model,better distinguish,originality of data,model better distinguish originality of data,0.6789804697036743
translation,119,27,model,domain-specific knowledge,introduced,approaches,domain-specific knowledge introduced approaches,0.6586363315582275
translation,119,27,model,approaches,at,data and model levels,approaches at data and model levels,0.5616372227668762
translation,119,27,model,model,To enhance,domain-specific knowledge,model To enhance domain-specific knowledge,0.6876211166381836
translation,119,33,model,greedy search ensemble algorithm,to select,best combinations,greedy search ensemble algorithm to select best combinations,0.6881924271583557
translation,119,33,model,best combinations,from,single models,best combinations from single models,0.5527336001396179
translation,119,33,model,model,has,greedy search ensemble algorithm,model has greedy search ensemble algorithm,0.5455187559127808
translation,119,34,model,multi-model & multi-iteration transductive ensemble ( m 2 te ) method,based on,translation results,multi-model & multi-iteration transductive ensemble ( m 2 te ) method based on translation results,0.6098458170890808
translation,119,34,model,translation results,of,ensemble models,translation results of ensemble models,0.5051080584526062
translation,119,34,model,model,propose,multi-model & multi-iteration transductive ensemble ( m 2 te ) method,model propose multi-model & multi-iteration transductive ensemble ( m 2 te ) method,0.6599109172821045
translation,119,70,model,mixed attention strategy,where,"random attention ( zeng et al. , 2021 )","mixed attention strategy where random attention ( zeng et al. , 2021 )",0.5613759756088257
translation,119,70,model,"random attention ( zeng et al. , 2021 )",combined with,original attention,"random attention ( zeng et al. , 2021 ) combined with original attention",0.6297545433044434
translation,119,70,model,model,use,mixed attention strategy,model use mixed attention strategy,0.6968529224395752
translation,119,127,model,source-side sentences,in,parallel and monolingual corpora,source-side sentences in parallel and monolingual corpora,0.49973171949386597
translation,119,127,model,translated pseudo corpus,to improve,l2r model,translated pseudo corpus to improve l2r model,0.6348904967308044
translation,119,127,model,model,translate,source-side sentences,model translate source-side sentences,0.7199836373329163
translation,119,127,model,model,use,translated pseudo corpus,model use translated pseudo corpus,0.5899966359138489
translation,119,189,model,two fine- grained domain-specific models,to translate,covid -19 and government report texts,two fine- grained domain-specific models to translate covid -19 and government report texts,0.6718083620071411
translation,119,189,model,model,employed,two fine- grained domain-specific models,model employed two fine- grained domain-specific models,0.6740473508834839
translation,119,134,results,significantly improve,around,+ 1 bleu point,significantly improve around + 1 bleu point,0.6743098497390747
translation,119,134,results,baseline model,around,+ 1 bleu point,baseline model around + 1 bleu point,0.6249223947525024
translation,119,134,results,significantly improve,has,baseline model,significantly improve has baseline model,0.5804319977760315
translation,119,134,results,results,individually using,ft and r2l,results individually using ft and r2l,0.6528818011283875
translation,119,162,results,source-original data,is,more effective,source-original data is more effective,0.5316053032875061
translation,119,162,results,source-original data,combining,non-source-original one,source-original data combining non-source-original one,0.6640399694442749
translation,119,162,results,more effective,combining,non-source-original one,more effective combining non-source-original one,0.7327291369438171
translation,119,162,results,non-source-original one,into,finetuning dataset,non-source-original one into finetuning dataset,0.5487796664237976
translation,119,162,results,finetuning dataset,has,35.5 vs. 35.7 bleu,finetuning dataset has 35.5 vs. 35.7 bleu,0.5690485835075378
translation,119,162,results,results,has,source-original data,results has source-original data,0.48593783378601074
translation,119,164,results,data reduction (   + ds   ) and expansion (   + da   ) methods,not further improve,performance,data reduction (   + ds   ) and expansion (   + da   ) methods not further improve performance,0.7340763211250305
translation,119,164,results,performance,of,baseline model,performance of baseline model,0.5962602496147156
translation,119,164,results,baseline model,has,- 0.3 and + 0.1 bleu,baseline model has - 0.3 and + 0.1 bleu,0.5341909527778625
translation,119,164,results,results,has,data reduction (   + ds   ) and expansion (   + da   ) methods,results has data reduction (   + ds   ) and expansion (   + da   ) methods,0.4925980269908905
translation,119,184,results,significantly improve,about,1 bleu score,significantly improve about 1 bleu score,0.6095134019851685
translation,119,184,results,baseline,by,1 bleu score,baseline by 1 bleu score,0.5431125164031982
translation,119,184,results,baseline,about,1 bleu score,baseline about 1 bleu score,0.5815378427505493
translation,119,184,results,significantly improve,has,baseline,significantly improve has baseline,0.5968747735023499
translation,119,184,results,results,found that,r2l method,results found that r2l method,0.6909400224685669
translation,119,187,results,best single model,by,0.5? 2.0 bleu scores,best single model by 0.5? 2.0 bleu scores,0.5174397826194763
translation,119,187,results,simple ensembled model,has,outperform,simple ensembled model has outperform,0.6212729215621948
translation,119,187,results,outperform,has,best single model,outperform has best single model,0.6290338039398193
translation,119,187,results,results,has,simple ensembled model,results has simple ensembled model,0.5524794459342957
translation,119,188,results,transductive ensemble,to,each group of models,transductive ensemble to each group of models,0.5388543605804443
translation,119,188,results,performance,achieves,36.8 bleu,performance achieves 36.8 bleu,0.663173496723175
translation,119,188,results,36.8 bleu,on,chinese ? english task,36.8 bleu on chinese ? english task,0.49120697379112244
translation,119,188,results,transductive ensemble,has,performance,transductive ensemble has performance,0.5568045973777771
translation,119,188,results,results,apply,transductive ensemble,results apply transductive ensemble,0.5592429637908936
translation,119,191,results,single models,applied,te,single models applied te,0.7525728940963745
translation,119,191,results,single models,applied,cannot bring further improvement,single models applied cannot bring further improvement,0.7289289236068726
translation,119,191,results,cannot bring further improvement,to,ensemble results,cannot bring further improvement to ensemble results,0.5667669773101807
translation,119,191,results,te,has,cannot bring further improvement,te has cannot bring further improvement,0.6457348465919495
translation,119,191,results,results,find that,single models,results find that single models,0.6295116543769836
translation,119,201,results,best single model,by,1.5 bleu scores,best single model by 1.5 bleu scores,0.517620325088501
translation,119,201,results,ensemble models,has,outperform,ensemble models has outperform,0.6127061247825623
translation,119,201,results,outperform,has,best single model,outperform has best single model,0.6290338039398193
translation,119,201,results,results,has,ensemble models,results has ensemble models,0.5177116394042969
translation,119,205,results,our primary systems,achieve,first and the second bleu scores,our primary systems achieve first and the second bleu scores,0.6151534914970398
translation,119,205,results,first and the second bleu scores,on,chinese ? english and german ? english,first and the second bleu scores on chinese ? english and german ? english,0.511652946472168
translation,120,60,ablation-analysis,embedding layer,with,contextualized embeddings,embedding layer with contextualized embeddings,0.6152772903442383
translation,120,60,ablation-analysis,got - tbert,boosts,bleu scores,got - tbert boosts bleu scores,0.7403410077095032
translation,120,60,ablation-analysis,bleu scores,of,de?en,bleu scores of de?en,0.6189606189727783
translation,120,60,ablation-analysis,de?en,from,33.56 to 36.32,de?en from 33.56 to 36.32,0.5982006788253784
translation,120,60,ablation-analysis,roberta,strengthens,en? de translation,roberta strengthens en? de translation,0.7267893552780151
translation,120,60,ablation-analysis,en? de translation,from,27.3 to 28.74,en? de translation from 27.3 to 28.74,0.585743248462677
translation,120,60,ablation-analysis,embedding layer,has,got - tbert,embedding layer has got - tbert,0.6244268417358398
translation,120,60,ablation-analysis,contextualized embeddings,has,got - tbert,contextualized embeddings has got - tbert,0.6499499678611755
translation,120,60,ablation-analysis,ablation analysis,replacing,embedding layer,ablation analysis replacing embedding layer,0.6425791382789612
translation,120,39,baselines,"multilingual bert ( devlin et al. , 2019 )",pre-trained on,104 highest - resource languages,"multilingual bert ( devlin et al. , 2019 ) pre-trained on 104 highest - resource languages",0.7340324521064758
translation,120,39,baselines,104 highest - resource languages,in,wikipedia,104 highest - resource languages in wikipedia,0.5449974536895752
translation,120,39,baselines,mbert ( cased ),has,"multilingual bert ( devlin et al. , 2019 )","mbert ( cased ) has multilingual bert ( devlin et al. , 2019 )",0.5957120656967163
translation,120,39,baselines,baselines,has,mbert ( cased ),baselines has mbert ( cased ),0.5675243735313416
translation,120,40,baselines,"transformer - based ( vaswani et al. , 2017 ) masked language model",trained on,100 languages,"transformer - based ( vaswani et al. , 2017 ) masked language model trained on 100 languages",0.6763659715652466
translation,120,40,baselines,"transformer - based ( vaswani et al. , 2017 ) masked language model",using,more than two terabytes,"transformer - based ( vaswani et al. , 2017 ) masked language model using more than two terabytes",0.6096605062484741
translation,120,40,baselines,more than two terabytes,of,filtered commoncrawl data,more than two terabytes of filtered commoncrawl data,0.5952891707420349
translation,120,40,baselines,xlm -r,has,base,xlm -r has base,0.6590090394020081
translation,120,40,baselines,xlm -r,has,"transformer - based ( vaswani et al. , 2017 ) masked language model","xlm -r has transformer - based ( vaswani et al. , 2017 ) masked language model",0.5463593006134033
translation,120,40,baselines,base,has,"transformer - based ( vaswani et al. , 2017 ) masked language model","base has transformer - based ( vaswani et al. , 2017 ) masked language model",0.529155969619751
translation,120,40,baselines,outperforms,has,mbert,outperforms has mbert,0.6495157480239868
translation,120,40,baselines,baselines,has,xlm -r,baselines has xlm -r,0.5721707940101624
translation,120,46,experimental-setup,six-layer transformer architecture,with,4 attention heads,six-layer transformer architecture with 4 attention heads,0.6616511940956116
translation,120,46,experimental-setup,transformer_iwslt_de_en,has,six-layer transformer architecture,transformer_iwslt_de_en has six-layer transformer architecture,0.562447726726532
translation,120,46,experimental-setup,ffn dimension size,has,1024,ffn dimension size has 1024,0.607930600643158
translation,120,46,experimental-setup,experimental setup,has,model configuration,experimental setup has model configuration,0.5052096247673035
translation,120,47,experimental-setup,embedding dimension,of,768,embedding dimension of 768,0.6489987373352051
translation,120,47,experimental-setup,dimension,of,pre-trained language models,dimension of pre-trained language models,0.572956383228302
translation,120,47,experimental-setup,experimental setup,use,embedding dimension,experimental setup use embedding dimension,0.6082180738449097
translation,120,80,experimental-setup,unified 52 k vocabulary,using,wordpiece tokenizer,unified 52 k vocabulary using wordpiece tokenizer,0.6585986614227295
translation,120,80,experimental-setup,wordpiece tokenizer,with,67gb english,wordpiece tokenizer with 67gb english,0.6075183153152466
translation,120,80,experimental-setup,wordpiece tokenizer,with,67gb german texts,wordpiece tokenizer with 67gb german texts,0.5763351917266846
translation,120,80,experimental-setup,experimental setup,train,unified 52 k vocabulary,experimental setup train unified 52 k vocabulary,0.6768193244934082
translation,120,81,experimental-setup,bibert en - de,trained on,tpu v3 - 8,bibert en - de trained on tpu v3 - 8,0.6837866306304932
translation,120,81,experimental-setup,tpu v3 - 8,for,four weeks,tpu v3 - 8 for four weeks,0.6728391647338867
translation,120,81,experimental-setup,experimental setup,has,bibert en - de,experimental setup has bibert en - de,0.6271040439605713
translation,120,6,model,output ( contextualized embeddings ),of,tailored and suitable bilingual pre-trained language model ( dubbed bibert ),output ( contextualized embeddings ) of tailored and suitable bilingual pre-trained language model ( dubbed bibert ),0.5254300236701965
translation,120,6,model,tailored and suitable bilingual pre-trained language model ( dubbed bibert ),input of,nmt encoder,tailored and suitable bilingual pre-trained language model ( dubbed bibert ) input of nmt encoder,0.7461447715759277
translation,120,6,model,nmt encoder,achieves,state - of - the - art translation performance,nmt encoder achieves state - of - the - art translation performance,0.6493109464645386
translation,120,6,model,model,using,output ( contextualized embeddings ),model using output ( contextualized embeddings ),0.6432027220726013
translation,120,7,model,concept of dual-directional translation model,to ensure,sufficient utilization,concept of dual-directional translation model to ensure sufficient utilization,0.7053000330924988
translation,120,7,model,sufficient utilization,of,contextualized embeddings,sufficient utilization of contextualized embeddings,0.5988486409187317
translation,120,7,model,model,propose,stochastic layer selection approach,model propose stochastic layer selection approach,0.7040184140205383
translation,120,17,model,dual-directional training,that yield,further improvements,dual-directional training that yield further improvements,0.5833268761634827
translation,120,17,model,two further refinements,has,stochastic layer selection,two further refinements has stochastic layer selection,0.5612354278564453
translation,120,17,model,model,introduce,two further refinements,model introduce two further refinements,0.6446065902709961
translation,120,16,results,output,of,pre-trained language model,output of pre-trained language model,0.5339604616165161
translation,120,16,results,results,using,output,results using output,0.649109423160553
translation,120,59,results,embedding layer,of,mt encoder,embedding layer of mt encoder,0.5670170187950134
translation,120,59,results,mt encoder,is,randomly initialized,mt encoder is randomly initialized,0.5545514822006226
translation,120,59,results,similar bleu scores,for,all baselines,similar bleu scores for all baselines,0.5610488057136536
translation,120,59,results,all baselines,from,english - to - german ( around 27.6 ),all baselines from english - to - german ( around 27.6 ),0.5030978322029114
translation,120,59,results,results,When,embedding layer,results When embedding layer,0.6654741168022156
translation,120,61,results,mbert and xlm -r,provide,modest improvement,mbert and xlm -r provide modest improvement,0.6200697422027588
translation,120,61,results,mbert and xlm -r,provide,degenerate,mbert and xlm -r provide degenerate,0.5785365104675293
translation,120,61,results,modest improvement,in,de?en translation,modest improvement in de?en translation,0.602683961391449
translation,120,61,results,performance,of,en? de translation,performance of en? de translation,0.6090145707130432
translation,120,61,results,degenerate,has,performance,degenerate has performance,0.6086838841438293
translation,120,61,results,results,has,mbert and xlm -r,results has mbert and xlm -r,0.5176823735237122
translation,120,63,results,deterioration,caused by,mbert and xlm -r,deterioration caused by mbert and xlm -r,0.7376357913017273
translation,120,63,results,mbert and xlm -r,on,en?de,mbert and xlm -r on en?de,0.6824268102645874
translation,120,63,results,mbert and xlm -r,over,randomly initialized baselines,mbert and xlm -r over randomly initialized baselines,0.694589376449585
translation,120,63,results,en?de,over,randomly initialized baselines,en?de over randomly initialized baselines,0.7006340026855469
translation,120,63,results,comparatively small gains,versus,monolingual models,comparatively small gains versus monolingual models,0.6845089793205261
translation,120,63,results,results,note,deterioration,results note deterioration,0.5310981869697571
translation,120,67,results,large monolingual models,has,roberta and gottbert,large monolingual models has roberta and gottbert,0.5251545906066895
translation,120,67,results,roberta and gottbert,has,significantly beat,roberta and gottbert has significantly beat,0.587253987789154
translation,120,67,results,significantly beat,has,randomized baseline,significantly beat has randomized baseline,0.5982975363731384
translation,120,67,results,significantly beat,has,multilingual models,significantly beat has multilingual models,0.5919398665428162
translation,120,67,results,results,has,large monolingual models,results has large monolingual models,0.5641886591911316
translation,120,90,results,performance,of,translation model,performance of translation model,0.5954796671867371
translation,120,90,results,translation model,with,bibert en - de,translation model with bibert en - de,0.6668860912322998
translation,120,90,results,translation model,is,robust,translation model is robust,0.5296381115913391
translation,120,90,results,translation model,basically unaffected by,vocabulary size,translation model basically unaffected by vocabulary size,0.6925384402275085
translation,120,90,results,robust,for,de?en,robust for de?en,0.7558297514915466
translation,120,90,results,results,notice that,performance,results notice that performance,0.6314935088157654
translation,120,91,results,results,has,bibert performance bibert en - de results,results has bibert performance bibert en - de results,0.6236751675605774
translation,120,92,results,our bilingual model,help,transformer model,our bilingual model help transformer model,0.6849619150161743
translation,120,92,results,transformer model,achieve,29.65 score,transformer model achieve 29.65 score,0.6674137711524963
translation,120,92,results,29.65 score,for,en? dea gain,29.65 score for en? dea gain,0.6565396189689636
translation,120,92,results,en? dea gain,of,2.12,en? dea gain of 2.12,0.6155233383178711
translation,120,92,results,2.12,over,baseline,2.12 over baseline,0.6173767447471619
translation,120,93,results,our model,achieves,37.58 score,our model achieves 37.58 score,0.6391919255256653
translation,120,93,results,37.58 score,with,gain,37.58 score with gain,0.5924043655395508
translation,120,93,results,gain,of,4.06,gain of 4.06,0.606256365776062
translation,120,93,results,de?en,has,our model,de?en has our model,0.6954885721206665
translation,120,93,results,results,For,de?en,results For de?en,0.7311700582504272
translation,120,103,results,straightforward method,of simply using,final layer of bibert en - de,straightforward method of simply using final layer of bibert en - de,0.7605311274528503
translation,120,103,results,final layer of bibert en - de,has,outperforms,final layer of bibert en - de has outperforms,0.626539409160614
translation,120,103,results,results,has,straightforward method,results has straightforward method,0.555322527885437
translation,120,104,results,model,only uses,monolingual gottbert,model only uses monolingual gottbert,0.7060990929603577
translation,120,104,results,model,achieves,competitive result,model achieves competitive result,0.6970983147621155
translation,120,104,results,competitive result,compared with,previous state - of - the - art approach,competitive result compared with previous state - of - the - art approach,0.6651965379714966
translation,120,104,results,36.32 ),compared with,previous state - of - the - art approach,36.32 ) compared with previous state - of - the - art approach,0.6610407829284668
translation,120,104,results,competitive result,has,36.32 ),competitive result has 36.32 ),0.5704541206359863
translation,120,104,results,previous state - of - the - art approach,has,36.88 ),previous state - of - the - art approach has 36.88 ),0.5357873439788818
translation,120,129,results,translation model,gets,highest score,translation model gets highest score,0.5936678051948547
translation,120,129,results,translation model,when,stochastic layer selection,translation model when stochastic layer selection,0.6113150119781494
translation,120,129,results,37.94,for,de?en,37.94 for de?en,0.6779018044471741
translation,120,129,results,30.04,for,en? de,30.04 for en? de,0.6960258483886719
translation,120,129,results,stochastic layer selection,uses,8 layers,stochastic layer selection uses 8 layers,0.6013131737709045
translation,120,129,results,highest score,has,37.94,highest score has 37.94,0.5145483016967773
translation,120,152,results,one - way model,by obtaining,gain,one - way model by obtaining gain,0.6622028946876526
translation,120,152,results,gain,of,0.52,gain of 0.52,0.5937933325767517
translation,120,152,results,gain,of,0.72,gain of 0.72,0.5892857909202576
translation,120,152,results,0.52,in,en?de,0.52 in en?de,0.6475062370300293
translation,120,152,results,0.72,in,de?en,0.72 in de?en,0.6426718235015869
translation,120,152,results,dual-directional model,has,substantially outperforms,dual-directional model has substantially outperforms,0.6004007458686829
translation,120,152,results,substantially outperforms,has,one - way model,substantially outperforms has one - way model,0.5851525664329529
translation,120,152,results,results,has,dual-directional model,results has dual-directional model,0.521513819694519
translation,120,153,results,fine-tuning,on,in-domain data,fine-tuning on in-domain data,0.5680773258209229
translation,120,153,results,fine-tuning,improves,bleu,fine-tuning improves bleu,0.6998633146286011
translation,120,153,results,in-domain data,improves,bleu,in-domain data improves bleu,0.6765575408935547
translation,120,153,results,bleu,from,29.89 to 30.33,bleu from 29.89 to 30.33,0.5198203921318054
translation,120,153,results,bleu,from,37.97 to 38.12,bleu from 37.97 to 38.12,0.5252370834350586
translation,120,153,results,bleu,from,37.97 to 38.12,bleu from 37.97 to 38.12,0.5252370834350586
translation,120,153,results,29.89 to 30.33,in,en?de,29.89 to 30.33 in en?de,0.658945620059967
translation,120,153,results,29.89 to 30.33,in,de?en,29.89 to 30.33 in de?en,0.6433666944503784
translation,120,153,results,37.97 to 38.12,in,de?en,37.97 to 38.12 in de?en,0.636515200138092
translation,120,153,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,120,154,results,positive results,indicated by,dual-directional model and finetuning approach,positive results indicated by dual-directional model and finetuning approach,0.7297316789627075
translation,120,154,results,dual-directional model and finetuning approach,show,effectiveness,dual-directional model and finetuning approach show effectiveness,0.592309296131134
translation,120,154,results,effectiveness,in helping,translation,effectiveness in helping translation,0.6839236617088318
translation,120,154,results,results,has,positive results,results has positive results,0.5880638360977173
translation,120,164,results,our model,achieves,state - of- the - art bleu,our model achieves state - of- the - art bleu,0.6265609860420227
translation,120,164,results,state - of- the - art bleu,on,( 30.91 ) and de?en ( 34.94 ),state - of- the - art bleu on ( 30.91 ) and de?en ( 34.94 ),0.5189597606658936
translation,120,164,results,bibert en - de contextualized embeddings,has,our model,bibert en - de contextualized embeddings has our model,0.5940378904342651
translation,120,164,results,stochastic layer selection,has,our model,stochastic layer selection has our model,0.5637241005897522
translation,120,164,results,results,With,bibert en - de contextualized embeddings,results With bibert en - de contextualized embeddings,0.6274397969245911
translation,120,165,results,dual-directional translation training,does not show,same strong effectiveness,dual-directional translation training does not show same strong effectiveness,0.6895782351493835
translation,120,165,results,results,has,dual-directional translation training,results has dual-directional translation training,0.4692264795303345
translation,121,67,experimental-setup,"bpe ( sennrich et al. , 2016 )",with,32 k merge operations,"bpe ( sennrich et al. , 2016 ) with 32 k merge operations",0.5921318531036377
translation,121,67,experimental-setup,32 k merge operations,for,both language pairs,32 k merge operations for both language pairs,0.6048426628112793
translation,121,67,experimental-setup,experimental setup,applied,"bpe ( sennrich et al. , 2016 )","experimental setup applied bpe ( sennrich et al. , 2016 )",0.6810933351516724
translation,121,69,experiments,"transformer ( vaswani et al. , 2017 )",as,baseline autoregressive translation model,"transformer ( vaswani et al. , 2017 ) as baseline autoregressive translation model",0.4972434639930725
translation,121,69,experiments,"transformer ( vaswani et al. , 2017 )",as,baseline non-autoregressive model,"transformer ( vaswani et al. , 2017 ) as baseline non-autoregressive model",0.5263973474502563
translation,121,69,experiments,"transformer ( vaswani et al. , 2017 )",as,baseline non-autoregressive model,"transformer ( vaswani et al. , 2017 ) as baseline non-autoregressive model",0.5263973474502563
translation,121,69,experiments,"mask - predict ( ghazvininejad et al. , 2019 )",as,baseline non-autoregressive model,"mask - predict ( ghazvininejad et al. , 2019 ) as baseline non-autoregressive model",0.5108537077903748
translation,121,84,experiments,multi - task nat,shows,significantly better performance,multi - task nat shows significantly better performance,0.6726598739624023
translation,121,84,experiments,sen- tence length ( selen ) prediction task and tree depth ( trdep ) task,has,multi - task nat,sen- tence length ( selen ) prediction task and tree depth ( trdep ) task has multi - task nat,0.5501683354377747
translation,121,86,experiments,mask - predict,by,0.5,mask - predict by 0.5,0.5878522396087646
translation,121,86,experiments,outperforms,has,mask - predict,outperforms has mask - predict,0.6295508146286011
translation,121,7,model,multi-task learning,to transfer,at knowledge,multi-task learning to transfer at knowledge,0.6963352560997009
translation,121,7,model,at knowledge,to,nat models,at knowledge to nat models,0.6944530606269836
translation,121,7,model,at knowledge,through,encoder sharing,at knowledge through encoder sharing,0.733571469783783
translation,121,7,model,nat models,through,encoder sharing,nat models through encoder sharing,0.6689928770065308
translation,121,7,model,model,adopt,multi-task learning,model adopt multi-task learning,0.6219040155410767
translation,121,8,model,at model,as,auxiliary task,at model as auxiliary task,0.5481569766998291
translation,121,8,model,auxiliary task,to enhance,nat model performance,auxiliary task to enhance nat model performance,0.7034972310066223
translation,121,8,model,model,take,at model,model take at model,0.7114900946617126
translation,121,24,model,linguistic differences,adopt,multi-task learning framework,linguistic differences adopt multi-task learning framework,0.6174389719963074
translation,121,24,model,multi-task learning framework,with,"shared encoder ( i.e. , multi - task nat )","multi-task learning framework with shared encoder ( i.e. , multi - task nat )",0.5745702981948853
translation,121,24,model,"shared encoder ( i.e. , multi - task nat )",to transfer,at model knowledge,"shared encoder ( i.e. , multi - task nat ) to transfer at model knowledge",0.6807687878608704
translation,121,24,model,at model knowledge,into,nat model,at model knowledge into nat model,0.5900301337242126
translation,121,24,model,model,by leveraging,linguistic differences,model by leveraging linguistic differences,0.7040500640869141
translation,121,24,model,model,adopt,multi-task learning framework,model adopt multi-task learning framework,0.5980992913246155
translation,121,25,model,additional at task,as,auxiliary task,additional at task as auxiliary task,0.5302079319953918
translation,121,25,model,auxiliary task,of which,encoder parameters,auxiliary task of which encoder parameters,0.5365265607833862
translation,121,25,model,encoder parameters,are shared with,nat task,encoder parameters are shared with nat task,0.655668318271637
translation,121,25,model,nat task,while,parameters,nat task while parameters,0.5419825911521912
translation,121,25,model,model,employ,additional at task,model employ additional at task,0.62142413854599
translation,121,70,model,transformer decoder,into,mask - predict,transformer decoder into mask - predict,0.6589727997779846
translation,121,70,model,mask - predict,to implement,proposed multi - task nat model,mask - predict to implement proposed multi - task nat model,0.7198499441146851
translation,121,70,model,model,integrate,transformer decoder,model integrate transformer decoder,0.7503910064697266
translation,121,95,model,novel multitask learning approach,for,nat model,novel multitask learning approach for nat model,0.6008743047714233
translation,121,95,model,model,presented,novel multitask learning approach,model presented novel multitask learning approach,0.6673708558082581
translation,121,29,results,proposed multi - task nat,achieves,significant improvements,proposed multi - task nat achieves significant improvements,0.7003465890884399
translation,121,29,results,significant improvements,on,wmt14 english ? german and wmt16 english ? romanian datasets,significant improvements on wmt14 english ? german and wmt16 english ? romanian datasets,0.4863869249820709
translation,121,29,results,knowledge distillation,has,proposed multi - task nat,knowledge distillation has proposed multi - task nat,0.5668690800666809
translation,121,29,results,results,with,knowledge distillation,results with knowledge distillation,0.6361663937568665
translation,121,83,results,our multi - task nat,could learn,"better surface , syntactic , and semantic information","our multi - task nat could learn better surface , syntactic , and semantic information",0.5970821976661682
translation,121,83,results,"better surface , syntactic , and semantic information",than,transformer and mask - predict baseline models,"better surface , syntactic , and semantic information than transformer and mask - predict baseline models",0.5347505211830139
translation,121,83,results,results,has,our multi - task nat,results has our multi - task nat,0.5435357689857483
translation,121,85,results,our model,demonstrates,better or on - par performance,our model demonstrates better or on - par performance,0.6586344838142395
translation,121,85,results,better or on - par performance,compared to,nat model,better or on - par performance compared to nat model,0.7040449976921082
translation,121,85,results,other tasks,has,our model,other tasks has our model,0.5603934526443481
translation,121,85,results,results,On,other tasks,results On other tasks,0.5096456408500671
translation,122,188,ablation-analysis,iterative - bt,helps improve,hinted bt,iterative - bt helps improve hinted bt,0.697232723236084
translation,122,188,ablation-analysis,hi?en,has,iterative - bt,hi?en has iterative - bt,0.6881651878356934
translation,122,188,ablation-analysis,hinted bt,has,significantly,hinted bt has significantly,0.6856868267059326
translation,122,188,ablation-analysis,ablation analysis,except for,hi?en,ablation analysis except for hi?en,0.7314029932022095
translation,122,211,ablation-analysis,bleu,of,"hi?en , gu?en and ta?en","bleu of hi?en , gu?en and ta?en",0.627556324005127
translation,122,211,ablation-analysis,"hi?en , gu?en and ta?en",drops to,"30.6 , 16.8 and 15.9","hi?en , gu?en and ta?en drops to 30.6 , 16.8 and 15.9",0.6808410286903381
translation,122,211,ablation-analysis,ablation analysis,observe,bleu,ablation analysis observe bleu,0.6078376173973083
translation,122,212,ablation-analysis,performance,of,quality binning,performance of quality binning,0.6261104345321655
translation,122,212,ablation-analysis,quality binning,to almost match,tagged - bt,quality binning to almost match tagged - bt,0.7527915835380554
translation,122,212,ablation-analysis,random bin assignment,has,degrades,random bin assignment has degrades,0.6140231490135193
translation,122,212,ablation-analysis,degrades,has,performance,degrades has performance,0.5834147930145264
translation,122,212,ablation-analysis,ablation analysis,see that,random bin assignment,ablation analysis see that random bin assignment,0.6132321357727051
translation,122,219,ablation-analysis,quality tags,helps improve,word- level f1,quality tags helps improve word- level f1,0.6485880017280579
translation,122,219,ablation-analysis,ablation analysis,adding,quality tags,ablation analysis adding quality tags,0.7047125697135925
translation,122,245,ablation-analysis,quality tagging,increases as,low-resourcedness,quality tagging increases as low-resourcedness,0.5662550926208496
translation,122,245,ablation-analysis,low-resourcedness,has,increases,low-resourcedness has increases,0.5967823266983032
translation,122,121,baselines,bitext - model,trained only on,bitext data,bitext - model trained only on bitext data,0.7675775289535522
translation,122,121,baselines,baselines,has,bitext - model,baselines has bitext - model,0.5599634051322937
translation,122,126,baselines,bitext + labse topk - bt - model,trained on,bitext data and topk best quality bt pairs,bitext + labse topk - bt - model trained on bitext data and topk best quality bt pairs,0.7252799272537231
translation,122,126,baselines,baselines,has,bitext + labse topk - bt - model,baselines has bitext + labse topk - bt - model,0.4911237359046936
translation,122,112,experiments,all experiments,on,tpus,all experiments on tpus,0.6247759461402893
translation,122,112,experiments,all experiments,train,models,all experiments train models,0.6909366846084595
translation,122,112,experiments,models,for,300k steps,models for 300k steps,0.6795890927314758
translation,122,108,hyperparameters,hyperparameters,train,standard transformer encoder-decoder models,hyperparameters train standard transformer encoder-decoder models,0.65392005443573
translation,122,109,hyperparameters,dimension,of,transformer layers,dimension of transformer layers,0.6308514475822449
translation,122,109,hyperparameters,dimension,of,token embeddings and positional embeddings,dimension of token embeddings and positional embeddings,0.5663421750068665
translation,122,109,hyperparameters,token embeddings and positional embeddings,is,1024,token embeddings and positional embeddings is 1024,0.533511757850647
translation,122,109,hyperparameters,feedforward layer dimension,is,8192,feedforward layer dimension is 8192,0.5617526173591614
translation,122,109,hyperparameters,number of attention heads,is,16,number of attention heads is 16,0.5761368274688721
translation,122,109,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,122,109,hyperparameters,hyperparameters,has,feedforward layer dimension,hyperparameters has feedforward layer dimension,0.4954427182674408
translation,122,111,hyperparameters,training,use,adafactor optimizer,training use adafactor optimizer,0.622465193271637
translation,122,111,hyperparameters,adafactor optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adafactor optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6458317637443542
translation,122,111,hyperparameters,learning rate,varied with,warmup,learning rate varied with warmup,0.7664052844047546
translation,122,111,hyperparameters,warmup,for,"40,000 steps","warmup for 40,000 steps",0.6350551843643188
translation,122,111,hyperparameters,warmup,followed by,decay,warmup followed by decay,0.6008811593055725
translation,122,111,hyperparameters,hyperparameters,For,training,hyperparameters For training,0.5661544799804688
translation,122,113,hyperparameters,batch size,of,3 k,batch size of 3 k,0.6685543060302734
translation,122,113,hyperparameters,3 k,across,all models,3 k across all models,0.6572650074958801
translation,122,113,hyperparameters,source and target,using,wordpiece tokenization,source and target using wordpiece tokenization,0.677520751953125
translation,122,113,hyperparameters,hyperparameters,tokenize,source and target,hyperparameters tokenize source and target,0.7683283090591431
translation,122,5,model,effectiveness,of,available bt data,effectiveness of available bt data,0.5649157166481018
translation,122,5,model,available bt data,introduce,hintedbt -a family of techniques,available bt data introduce hintedbt -a family of techniques,0.6218681931495667
translation,122,5,model,hintedbt -a family of techniques,provides,hints ( through tags ),hintedbt -a family of techniques provides hints ( through tags ),0.6510305404663086
translation,122,5,model,hints ( through tags ),to,encoder and decoder,hints ( through tags ) to encoder and decoder,0.568018913269043
translation,122,5,model,model,To improve,effectiveness,model To improve effectiveness,0.7391270399093628
translation,122,6,model,novel method,of using,high and low quality bt data,novel method of using high and low quality bt data,0.6459227204322815
translation,122,6,model,encoder ),to,model,encoder ) to model,0.5662573575973511
translation,122,6,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,122,29,model,family of techniques,provide,hints,family of techniques provide hints,0.6520423293113708
translation,122,29,model,hints,to,model,hints to model,0.5844553709030151
translation,122,29,model,model,to make,limited bt data even more effective,model to make limited bt data even more effective,0.6467553973197937
translation,122,29,model,hintedbt,has,family of techniques,hintedbt has family of techniques,0.6223861575126648
translation,122,29,model,model,propose,hintedbt,model propose hintedbt,0.7059018611907959
translation,122,110,model,6 layers,in,both encoder and decoder,6 layers in both encoder and decoder,0.5432308912277222
translation,122,110,model,both encoder and decoder,for,hi?en models,both encoder and decoder for hi?en models,0.6163565516471863
translation,122,110,model,4 layers,for,gu?en and ta?en models,4 layers for gu?en and ta?en models,0.5935711860656738
translation,122,110,model,model,use,6 layers,model use 6 layers,0.6963042616844177
translation,122,110,model,model,use,4 layers,model use 4 layers,0.6994897723197937
translation,122,133,results,ta?en bitext data,is,much poorer in quality,ta?en bitext data is much poorer in quality,0.5818860530853271
translation,122,133,results,much poorer in quality,compared to,other two pairs,much poorer in quality compared to other two pairs,0.6846265196800232
translation,122,133,results,results,see that,ta?en bitext data,results see that ta?en bitext data,0.6491952538490295
translation,122,135,results,performance,for,"gu?en , ta ?en","performance for gu?en , ta ?en",0.737185537815094
translation,122,135,results,performance,for,not for hi?en,performance for not for hi?en,0.7343422770500183
translation,122,135,results,improve,has,performance,improve has performance,0.5578044652938843
translation,122,135,results,results,see that,iterative back -translation,results see that iterative back -translation,0.6558260917663574
translation,122,136,results,comparison,between,full - bt,comparison between full - bt,0.6811453700065613
translation,122,136,results,high quality bt data,instead of using,all the bt data,high quality bt data instead of using all the bt data,0.6391353607177734
translation,122,136,results,high quality bt data,proves,beneficial,high quality bt data proves beneficial,0.6987320780754089
translation,122,136,results,beneficial,for,all 3 language pairs,beneficial for all 3 language pairs,0.682296097278595
translation,122,136,results,full - bt,has,and topk - bt,full - bt has and topk - bt,0.6161860227584839
translation,122,136,results,results,has,comparison,results has comparison,0.5800483226776123
translation,122,158,results,quality tagging,able to produce,better results,quality tagging able to produce better results,0.6532531976699829
translation,122,158,results,better results,than,iterative - bt,better results than iterative - bt,0.6326354742050171
translation,122,158,results,better results,with,far lesser computational costs,better results with far lesser computational costs,0.6188927292823792
translation,122,158,results,results,has,quality tagging,results has quality tagging,0.5341585874557495
translation,122,159,results,tagged - bt and topk - bt,for,hi?en and gu?en,tagged - bt and topk - bt for hi?en and gu?en,0.6772969365119934
translation,122,159,results,quality tagging,has,outperforms,quality tagging has outperforms,0.5999499559402466
translation,122,159,results,outperforms,has,tagged - bt and topk - bt,outperforms has tagged - bt and topk - bt,0.6122438907623291
translation,122,159,results,results,has,quality tagging,results has quality tagging,0.5341585874557495
translation,122,162,results,quality tagging,provides,best performance,quality tagging provides best performance,0.6427050828933716
translation,122,162,results,best performance,across,all previous baselines,best performance across all previous baselines,0.6524367928504944
translation,122,162,results,results,see,quality tagging,results see quality tagging,0.5837759971618652
translation,122,166,results,translit tagging,improves,performance,translit tagging improves performance,0.6468157172203064
translation,122,166,results,performance,of,all three languagepairs,performance of all three languagepairs,0.5043762922286987
translation,122,166,results,all three languagepairs,over,baseline,all three languagepairs over baseline,0.6991156339645386
translation,122,166,results,results,has,translit tagging,results has translit tagging,0.5616921186447144
translation,122,170,results,all 3 language pairs,has,combination of these 2 methods,all 3 language pairs has combination of these 2 methods,0.5596800446510315
translation,122,170,results,combination of these 2 methods,has,outperforms,combination of these 2 methods has outperforms,0.5736980438232422
translation,122,170,results,outperforms,has,both methods,outperforms has both methods,0.5823636054992676
translation,122,170,results,results,see that,all 3 language pairs,results see that all 3 language pairs,0.567188024520874
translation,122,170,results,results,for,all 3 language pairs,results for all 3 language pairs,0.5415144562721252
translation,122,171,results,hi?en,gives,overall best results,hi?en gives overall best results,0.660851001739502
translation,122,171,results,overall best results,of,31.6,overall best results of 31.6,0.5613948702812195
translation,122,171,results,multilingual sota,for,hi?en,multilingual sota for hi?en,0.6862055063247681
translation,122,171,results,outperforms,has,bilingual sota,outperforms has bilingual sota,0.609990119934082
translation,122,171,results,results,For,hi?en,results For hi?en,0.6983155012130737
translation,122,172,results,combination,produces,+ 1.9,combination produces + 1.9,0.6197324395179749
translation,122,172,results,+ 1.9,over,already strong topk - bt baseline,+ 1.9 over already strong topk - bt baseline,0.6294351816177368
translation,122,172,results,gu?en,has,combination,gu?en has combination,0.6873570680618286
translation,122,172,results,results,For,gu?en,results For gu?en,0.6938836574554443
translation,122,183,results,iterative version,performs,even better,iterative version performs even better,0.612696647644043
translation,122,183,results,even better,with,gu?en and ta?en,even better with gu?en and ta?en,0.7331916093826294
translation,122,183,results,gu?en and ta?en,getting,bleu scores,gu?en and ta?en getting bleu scores,0.6499945521354675
translation,122,183,results,bleu scores,of,20.0 and 17.2,bleu scores of 20.0 and 17.2,0.5658261775970459
translation,122,185,results,performance,when,iterative - bt,performance when iterative - bt,0.6815251708030701
translation,122,185,results,iterative - bt,combined with,quality tagging,iterative - bt combined with quality tagging,0.6953093409538269
translation,122,185,results,iterative - bt,combined with,translit tagging,iterative - bt combined with translit tagging,0.6927660703659058
translation,122,186,results,helps,for,gu?en,helps for gu?en,0.7570986747741699
translation,122,186,results,helps,giving,further boost,helps giving further boost,0.7651227712631226
translation,122,186,results,gu?en,giving,further boost,gu?en giving further boost,0.776386022567749
translation,122,186,results,further boost,in,performance,further boost in performance,0.5527536869049072
translation,122,186,results,further boost,to get,final bleu score,further boost to get final bleu score,0.5991151928901672
translation,122,186,results,performance,of,+ 0.8,performance of + 0.8,0.6148234605789185
translation,122,186,results,performance,to get,final bleu score,performance to get final bleu score,0.5308911800384521
translation,122,186,results,final bleu score,of,20.8,final bleu score of 20.8,0.5291226506233215
translation,122,218,results,models,based on,translit tags,models based on translit tags,0.6672573089599609
translation,122,218,results,models,have,equal / better f1 scores,models have equal / better f1 scores,0.5663058757781982
translation,122,218,results,equal / better f1 scores,than,full - bt model,equal / better f1 scores than full - bt model,0.5387623906135559
translation,122,218,results,full - bt model,across,all languages,full - bt model across all languages,0.6883396506309509
translation,122,218,results,results,see that,models,results see that models,0.6295062303543091
translation,122,244,results,full bt,under,all scenarios,full bt under all scenarios,0.6869073510169983
translation,122,244,results,quality binning,has,outperforms,quality binning has outperforms,0.6184250116348267
translation,122,244,results,outperforms,has,full bt,outperforms has full bt,0.6195517778396606
translation,122,244,results,results,show,quality binning,results show quality binning,0.6103546023368835
translation,122,254,results,weaker correlation,to,human judgement of similarity,weaker correlation to human judgement of similarity,0.5435275435447693
translation,122,254,results,human judgement of similarity,compared to,labse,human judgement of similarity compared to labse,0.7056725025177002
translation,122,254,results,bot - jaccard,has,weaker correlation,bot - jaccard has weaker correlation,0.5847131013870239
translation,122,254,results,results,has,bot - jaccard,results has bot - jaccard,0.5418704152107239
translation,122,257,results,all our experiments,with,bot - jaccard,all our experiments with bot - jaccard,0.6249635219573975
translation,122,257,results,results,repeat,all our experiments,results repeat all our experiments,0.6548157930374146
translation,122,258,results,bleu increases,of,0.4,bleu increases of 0.4,0.6339500546455383
translation,122,258,results,bleu increases,of,2.8,bleu increases of 2.8,0.6249524354934692
translation,122,258,results,bleu increases,of,1.4,bleu increases of 1.4,0.6166751980781555
translation,122,258,results,0.4,for,hi?en,0.4 for hi?en,0.6708645820617676
translation,122,258,results,2.8,for,gu ?en,2.8 for gu ?en,0.685089647769928
translation,122,258,results,gu ?en,using,quality tagging,gu ?en using quality tagging,0.7330896854400635
translation,122,258,results,1.4,for,ta ?en,1.4 for ta ?en,0.6729499101638794
translation,122,258,results,ta ?en,using,topk - bt,ta ?en using topk - bt,0.7147096395492554
translation,122,259,results,larger monolingual corpus help ?,analyze,providing more bt data,larger monolingual corpus help ? analyze providing more bt data,0.5817827582359314
translation,122,259,results,providing more bt data,helps,model,providing more bt data helps model,0.6684045791625977
translation,122,259,results,results,Does,larger monolingual corpus help ?,results Does larger monolingual corpus help ?,0.3682038486003876
translation,122,259,results,results,analyze,providing more bt data,results analyze providing more bt data,0.6262034773826599
translation,122,262,results,quality tagging,improves BLEU,32.0,quality tagging improves BLEU 32.0,0.7269206047058105
translation,122,262,results,hi?en,has,quality tagging,hi?en has quality tagging,0.6343507170677185
translation,122,262,results,results,For,hi?en,results For hi?en,0.6983155012130737
translation,122,263,results,quality + translit tagging,delivers,performances,quality + translit tagging delivers performances,0.6878844499588013
translation,122,263,results,performances,of,"18.2 and 16.1 , +0.3 and + 0.1","performances of 18.2 and 16.1 , +0.3 and + 0.1",0.5909219980239868
translation,122,263,results,"18.2 and 16.1 , +0.3 and + 0.1",from,previous best experiments,"18.2 and 16.1 , +0.3 and + 0.1 from previous best experiments",0.5334262251853943
translation,122,263,results,gu?en and ta?en,has,quality + translit tagging,gu?en and ta?en has quality + translit tagging,0.645971953868866
translation,122,263,results,results,For,gu?en and ta?en,results For gu?en and ta?en,0.6529747843742371
translation,122,264,results,hintedbt,benefit from,more data,hintedbt benefit from more data,0.6818808317184448
translation,122,264,results,hintedbt,increase in,performance,hintedbt increase in performance,0.6893889307975769
translation,123,152,ablation-analysis,human and   ref   data,results in,cluster changes,human and   ref   data results in cluster changes,0.6890936493873596
translation,123,152,ablation-analysis,cluster changes,to,almost half ( 12/25 ),cluster changes to almost half ( 12/25 ),0.5513519048690796
translation,123,152,ablation-analysis,cluster changes,to,less than 5 % ( 1/21 ),cluster changes to less than 5 % ( 1/21 ),0.5586135983467102
translation,123,152,ablation-analysis,almost half ( 12/25 ),of,sr + dc rankings,almost half ( 12/25 ) of sr + dc rankings,0.5668274760246277
translation,123,152,ablation-analysis,less than 5 % ( 1/21 ),of,sr - dc rankings,less than 5 % ( 1/21 ) of sr - dc rankings,0.5737653970718384
translation,123,152,ablation-analysis,ablation analysis,Removing,human and   ref   data,ablation analysis Removing human and   ref   data,0.7090583443641663
translation,123,13,results,changes in the collection of human judgments,resulted in,rankings,changes in the collection of human judgments resulted in rankings,0.655256450176239
translation,123,13,results,less robust,effects of,outliers,less robust effects of outliers,0.6398674249649048
translation,123,13,results,less robust,effects of,overall annotation task composition,less robust effects of overall annotation task composition,0.6269105076789856
translation,123,13,results,outliers,has,high - or lowperforming systems,outliers has high - or lowperforming systems,0.6168823838233948
translation,123,13,results,results,examine,changes in the collection of human judgments,results examine changes in the collection of human judgments,0.5890652537345886
translation,124,127,ablation-analysis,3 model nmt ensemble,boosts,per formance,3 model nmt ensemble boosts per formance,0.6125685572624207
translation,124,127,ablation-analysis,ablation analysis,has,3 model nmt ensemble,ablation analysis has 3 model nmt ensemble,0.5217297077178955
translation,124,115,baselines,chren nmt model,uses,dropout = 0.3,chren nmt model uses dropout = 0.3,0.566414475440979
translation,124,115,baselines,chren nmt model,uses,dropout =0.5,chren nmt model uses dropout =0.5,0.5654006600379944
translation,124,115,baselines,enchr nmt model,uses,dropout =0.5,enchr nmt model uses dropout =0.5,0.5911942720413208
translation,124,115,baselines,enchr nmt model,uses,minimum word frequency=0,enchr nmt model uses minimum word frequency=0,0.630498468875885
translation,124,117,baselines,supervised qe model,with,xgboost,supervised qe model with xgboost,0.6219320893287659
translation,124,113,experimental-setup,our nmt models,via,opennmt,our nmt models via opennmt,0.6388018727302551
translation,124,113,experimental-setup,experimental setup,develop,our nmt models,experimental setup develop our nmt models,0.5773146152496338
translation,124,114,experimental-setup,chren and enchr nmt models,use,2layer lstm encoder and decoder,chren and enchr nmt models use 2layer lstm encoder and decoder,0.562162458896637
translation,124,114,experimental-setup,chren and enchr nmt models,use,hidden size,chren and enchr nmt models use hidden size,0.5949362516403198
translation,124,114,experimental-setup,chren and enchr nmt models,use,dynamic batching,chren and enchr nmt models use dynamic batching,0.5952984690666199
translation,124,114,experimental-setup,hidden size,=,1024,hidden size = 1024,0.687937319278717
translation,124,114,experimental-setup,label smoothing,equals to,0.2,label smoothing equals to 0.2,0.6644464731216431
translation,124,114,experimental-setup,"szegedy et al. , 2016 )",equals to,0.2,"szegedy et al. , 2016 ) equals to 0.2",0.6257919669151306
translation,124,114,experimental-setup,dynamic batching,with,1000 tokens,dynamic batching with 1000 tokens,0.6652318835258484
translation,124,114,experimental-setup,label smoothing,has,"szegedy et al. , 2016 )","label smoothing has szegedy et al. , 2016 )",0.532588005065918
translation,124,114,experimental-setup,experimental setup,For,chren and enchr nmt models,experimental setup For chren and enchr nmt models,0.6218974590301514
translation,124,116,experimental-setup,nmt model,with,"three random seeds ( 7 , 77 , 777 )","nmt model with three random seeds ( 7 , 77 , 777 )",0.6307741403579712
translation,124,116,experimental-setup,nmt model,use,3 model ensemble,nmt model use 3 model ensemble,0.6247692704200745
translation,124,116,experimental-setup,nmt model,use,beam search ( beam size=5 ),nmt model use beam search ( beam size=5 ),0.6579059362411499
translation,124,116,experimental-setup,3 model ensemble,as,final translation model,3 model ensemble as final translation model,0.5201463103294373
translation,124,116,experimental-setup,beam search ( beam size=5 ),to gener ate,translations,beam search ( beam size=5 ) to gener ate translations,0.7732300758361816
translation,124,116,experimental-setup,experimental setup,train,nmt model,experimental setup train nmt model,0.6855294704437256
translation,124,22,experiments,quality estimation ( qe ),for,smt and nmt,quality estimation ( qe ) for smt and nmt,0.6364992260932922
translation,124,52,experiments,phrasebased smt model,via,"moses ( koehn et al. , 2007 )","phrasebased smt model via moses ( koehn et al. , 2007 )",0.5826218724250793
translation,124,52,experiments,"moses ( koehn et al. , 2007 )",train,"3 gram kenlm ( heafield et al. , 2013 )","moses ( koehn et al. , 2007 ) train 3 gram kenlm ( heafield et al. , 2013 )",0.6716388463973999
translation,124,52,experiments,word alignment,by,"giza ++ ( och and ney , 2003 )","word alignment by giza ++ ( och and ney , 2003 )",0.520543098449707
translation,124,4,model,translation,between,english and an endangered language cherokee,translation between english and an endangered language cherokee,0.6620845198631287
translation,124,4,model,chrentranslate,has,online ma chine translation,chrentranslate has online ma chine translation,0.6113002896308899
translation,124,55,model,model,implement,global attentional model,model implement global attentional model,0.648831844329834
translation,124,111,model,training and tuning,run it as,server process,training and tuning run it as server process,0.6621685028076172
translation,124,111,model,model,After,training and tuning,model After training and tuning,0.6402184367179871
translation,124,7,results,216 pieces,of,expert feedback,216 pieces of expert feedback,0.5999556183815002
translation,124,7,results,216 pieces,find that,nmt,216 pieces find that nmt,0.7243496775627136
translation,124,7,results,results,analyzing,216 pieces,results analyzing 216 pieces,0.5991811156272888
translation,124,126,results,translation per formance,on,1 k development set,translation per formance on 1 k development set,0.546847939491272
translation,124,126,results,translation per formance,are,significantly better,translation per formance are significantly better,0.5667577981948853
translation,124,126,results,significantly better,than,singlemodel,significantly better than singlemodel,0.5996078848838806
translation,124,126,results,results,shows,translation per formance,results shows translation per formance,0.6558929681777954
translation,125,26,ablation-analysis,analysis,of,relevance,analysis of relevance,0.5894381999969482
translation,125,26,ablation-analysis,relevance,of,encoder and decoder representations,relevance of encoder and decoder representations,0.5993165969848633
translation,125,26,ablation-analysis,encoder and decoder representations,in,nmt system output,encoder and decoder representations in nmt system output,0.5104076266288757
translation,125,26,ablation-analysis,mtl da,increases,contribution,mtl da increases contribution,0.5738401412963867
translation,125,26,ablation-analysis,contribution,of,source tokens,contribution of source tokens,0.54561847448349
translation,125,26,ablation-analysis,source tokens,decisions made by,nmt system,source tokens decisions made by nmt system,0.6788935661315918
translation,125,153,ablation-analysis,no consistent differences,in,source influence,no consistent differences in source influence,0.5094227194786072
translation,125,153,ablation-analysis,source influence,between,reverse and replace auxiliary tasks,source influence between reverse and replace auxiliary tasks,0.652830183506012
translation,125,153,ablation-analysis,ablation analysis,has,no consistent differences,ablation analysis has no consistent differences,0.5269870758056641
translation,125,154,ablation-analysis,systems,combining,multiple auxiliary tasks,systems combining multiple auxiliary tasks,0.7595247626304626
translation,125,154,ablation-analysis,highest source influence,confirming,complementarity,highest source influence confirming complementarity,0.6626676321029663
translation,125,154,ablation-analysis,ablation analysis,has,systems,ablation analysis has systems,0.5545880198478699
translation,125,35,experimental-setup,harmful interferences,by,out-ofdistribution target data,harmful interferences by out-ofdistribution target data,0.6117845773696899
translation,125,35,experimental-setup,harmful interferences,add,task -specific artificial token,harmful interferences add task -specific artificial token,0.6465926766395569
translation,125,35,experimental-setup,task -specific artificial token,to,source sentence,task -specific artificial token to source sentence,0.531745433807373
translation,125,35,experimental-setup,task -specific artificial token,to constrain,kind of output to be produced,task -specific artificial token to constrain kind of output to be produced,0.6550063490867615
translation,125,35,experimental-setup,experimental setup,to avoid,harmful interferences,experimental setup to avoid harmful interferences,0.6602320671081543
translation,125,35,experimental-setup,experimental setup,add,task -specific artificial token,experimental setup add task -specific artificial token,0.5872605443000793
translation,125,36,experimental-setup,synthetic corpus,of,same size,synthetic corpus of same size,0.5986477136611938
translation,125,36,experimental-setup,same size,to,original training data,same size to original training data,0.5693435072898865
translation,125,45,experimental-setup,token,has,random target words,token has random target words,0.5825403332710266
translation,125,45,experimental-setup,experimental setup,has,token,experimental setup has token,0.5333447456359863
translation,125,86,experimental-setup,neural model,is,transformerbased model,neural model is transformerbased model,0.5807703733444214
translation,125,86,experimental-setup,amount of warmup_steps,set to,"8,000","amount of warmup_steps set to 8,000",0.7037039995193481
translation,125,86,experimental-setup,experimental setup,has,neural model,experimental setup has neural model,0.5172881484031677
translation,125,87,experimental-setup,experiments,carried out on,single gpu,experiments carried out on single gpu,0.6657339930534363
translation,125,87,experimental-setup,single gpu,with,mini-batches,single gpu with mini-batches,0.6744456887245178
translation,125,87,experimental-setup,mini-batches,made of,"4,000 tokens","mini-batches made of 4,000 tokens",0.6808872818946838
translation,125,87,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,125,90,experimental-setup,raml and switchout,integrated into,fairseq,raml and switchout integrated into fairseq,0.7261545062065125
translation,125,90,experimental-setup,fairseq,has,sampling function,fairseq has sampling function,0.5813142657279968
translation,125,90,experimental-setup,experimental setup,For,raml and switchout,experimental setup For raml and switchout,0.6161318421363831
translation,125,102,experiments,auxiliary tasks,replace,best performing ones,auxiliary tasks replace best performing ones,0.607305109500885
translation,125,102,experiments,auxiliary tasks,are,best performing ones,auxiliary tasks are best performing ones,0.5661087036132812
translation,125,102,experiments,translation,into,target language,translation into target language,0.5506883263587952
translation,125,102,experiments,random replacement,of,target words,random replacement of target words,0.6190019249916077
translation,125,102,experiments,random replacement,of,source words,random replacement of source words,0.6076508164405823
translation,125,102,experiments,source words,aligned with,best performing ones,source words aligned with best performing ones,0.6817591786384583
translation,125,102,experiments,auxiliary tasks,has,reverse,auxiliary tasks has reverse,0.603325366973877
translation,125,102,experiments,reverse,has,translation,reverse has translation,0.5851869583129883
translation,125,6,model,multi-task da approach,generate,new sentence pairs,multi-task da approach generate new sentence pairs,0.6190693974494934
translation,125,6,model,new sentence pairs,with,transformations,new sentence pairs with transformations,0.643402099609375
translation,125,6,model,model,present,multi-task da approach,model present multi-task da approach,0.647477388381958
translation,125,7,model,augmented sentences,used as,auxiliary tasks,augmented sentences used as auxiliary tasks,0.5631794333457947
translation,125,7,model,auxiliary tasks,in,multi-task framework,auxiliary tasks in multi-task framework,0.5594115853309631
translation,125,7,model,new contexts,where,target prefix,new contexts where target prefix,0.634579598903656
translation,125,7,model,target prefix,is,not informative enough,target prefix is not informative enough,0.5851376056671143
translation,125,7,model,not informative enough,to predict,next word,not informative enough to predict next word,0.720509946346283
translation,125,7,model,training,has,augmented sentences,training has augmented sentences,0.6074764132499695
translation,125,7,model,model,During,training,model During training,0.714866042137146
translation,125,14,model,model,has,data augmentation ( da ),model has data augmentation ( da ),0.5842759013175964
translation,125,18,model,completely different framework,for,da,completely different framework for da,0.6565263867378235
translation,125,18,model,da,generate,additional parallel sentences,da generate additional parallel sentences,0.676391065120697
translation,125,18,model,model,propose,completely different framework,model propose completely different framework,0.71286541223526
translation,125,19,model,set of simple da strategies,to produce,synthetic target sentences,set of simple da strategies to produce synthetic target sentences,0.6800750494003296
translation,125,19,model,synthetic target sentences,aimed at,strengthening,synthetic target sentences aimed at strengthening,0.6058876514434814
translation,125,19,model,strengthening,has,encoder,strengthening has encoder,0.6020193696022034
translation,125,19,model,model,propose,set of simple da strategies,model propose set of simple da strategies,0.6579289436340332
translation,125,23,model,proposed framework,has,multi-task learning data augmentation ( mtl da ),proposed framework has multi-task learning data augmentation ( mtl da ),0.5669867396354675
translation,125,23,model,model,call,proposed framework,model call proposed framework,0.6191386580467224
translation,125,10,results,systems,trained with,our approach,systems trained with our approach,0.6928895711898804
translation,125,10,results,our approach,rely more on,source tokens,our approach rely more on source tokens,0.6995923519134521
translation,125,10,results,our approach,suffer,less hallucinations,our approach suffer less hallucinations,0.7079250812530518
translation,125,10,results,more robust,against,domain shift,more robust against domain shift,0.6798477172851562
translation,125,10,results,results,has,systems,results has systems,0.5238543152809143
translation,125,101,results,baseline system,in,all language pairs and translation directions,baseline system in all language pairs and translation directions,0.45531749725341797
translation,125,101,results,our mtl da approach,has,consistently outperforms,our mtl da approach has consistently outperforms,0.6098226308822632
translation,125,101,results,consistently outperforms,has,baseline system,consistently outperforms has baseline system,0.6086269617080688
translation,125,101,results,results,show,our mtl da approach,results show our mtl da approach,0.6323554515838623
translation,125,111,results,three best auxiliary tasks,improves,performance,three best auxiliary tasks improves performance,0.6756351590156555
translation,125,111,results,three best auxiliary tasks,achieving,best results,three best auxiliary tasks achieving best results,0.6241501569747925
translation,125,111,results,best results,in,all translation tasks,best results in all translation tasks,0.4710151255130768
translation,125,111,results,all translation tasks,with,bleu scores,all translation tasks with bleu scores,0.537741482257843
translation,125,111,results,bleu scores,between,1.1 and 1.9 points,bleu scores between 1.1 and 1.9 points,0.5913100838661194
translation,125,111,results,1.1 and 1.9 points,over,baseline,1.1 and 1.9 points over baseline,0.6159643530845642
translation,125,111,results,results,using,three best auxiliary tasks,results using three best auxiliary tasks,0.6416435837745667
translation,125,114,results,mtl da approach,with,"raml , switchout and their combination ( switchout + raml )","mtl da approach with raml , switchout and their combination ( switchout + raml )",0.6598756909370422
translation,125,114,results,mtl da approach,shows,our approach,mtl da approach shows our approach,0.6571164131164551
translation,125,114,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,125,124,results,mtl da,has,outperforms,mtl da has outperforms,0.6463238596916199
translation,125,124,results,outperforms,has,baseline system,outperforms has baseline system,0.610763430595398
translation,125,124,results,results,has,mtl da,results has mtl da,0.5566063523292542
translation,125,150,results,utility,of,mtl da,utility of mtl da,0.6093897819519043
translation,125,150,results,auxiliary tasks,increase,source influence,auxiliary tasks increase source influence,0.7256104946136475
translation,125,150,results,source influence,in,all translation tasks,source influence in all translation tasks,0.46839138865470886
translation,125,150,results,utility,has,baseline system,utility has baseline system,0.5546680688858032
translation,125,150,results,mtl da,has,baseline system,mtl da has baseline system,0.5673280954360962
translation,125,150,results,source,has,smallest influence,source has smallest influence,0.5148831605911255
translation,125,150,results,results,confirm,utility,results confirm utility,0.4961046874523163
translation,126,7,baselines,first nmt system,is,transformer based 6 layer encoder- decoder model,first nmt system is transformer based 6 layer encoder- decoder model,0.5669851899147034
translation,126,7,baselines,transformer based 6 layer encoder- decoder model,trained for,100000 training steps,transformer based 6 layer encoder- decoder model trained for 100000 training steps,0.7241711020469666
translation,126,7,baselines,model,for,bidirectional translation,model for bidirectional translation,0.6310003995895386
translation,126,7,baselines,baselines,has,first nmt system,baselines has first nmt system,0.5783165097236633
translation,126,8,baselines,two unidirectional translation models,addition of utilizing,byte pair encoding ( bpe ),two unidirectional translation models addition of utilizing byte pair encoding ( bpe ),0.677812933921814
translation,126,8,baselines,byte pair encoding ( bpe ),for,subword tokenization,byte pair encoding ( bpe ) for subword tokenization,0.6041125655174255
translation,126,8,baselines,subword tokenization,through,pre-trained multibpemb model,subword tokenization through pre-trained multibpemb model,0.6079393625259399
translation,126,5,experimental-setup,opennmt - py toolkit,to create,quick prototypes,opennmt - py toolkit to create quick prototypes,0.6610528230667114
translation,126,5,experimental-setup,quick prototypes,of,systems,quick prototypes of systems,0.6377289295196533
translation,126,5,experimental-setup,training datasets,containing,parallel corpus,training datasets containing parallel corpus,0.6220188140869141
translation,126,5,experimental-setup,experimental setup,has,opennmt - py toolkit,experimental setup has opennmt - py toolkit,0.5293297171592712
translation,126,6,experimental-setup,dgx station,with,4 - v100 gpus,dgx station with 4 - v100 gpus,0.6540051698684692
translation,126,6,experimental-setup,experimental setup,trained on,dgx station,experimental setup trained on dgx station,0.7014820575714111
translation,126,39,experimental-setup,dgx station,with,4 - v100 gpus,dgx station with 4 - v100 gpus,0.6540051698684692
translation,126,39,experimental-setup,4 - v100 gpus,to train,models,4 - v100 gpus to train models,0.736415684223175
translation,126,39,experimental-setup,experimental setup,has,dgx station,experimental setup has dgx station,0.5409588813781738
translation,127,64,ablation-analysis,ensembles,from,multiple checkpoints,ensembles from multiple checkpoints,0.6390969753265381
translation,127,64,ablation-analysis,multiple checkpoints,of,single experimental run,multiple checkpoints of single experimental run,0.6311342716217041
translation,127,64,ablation-analysis,single experimental run,are,not helpful,single experimental run are not helpful,0.582085907459259
translation,127,64,ablation-analysis,ablation analysis,notice that,ensembles,ablation analysis notice that ensembles,0.6294447779655457
translation,127,35,baselines,five different methods,for,data synthesis,five different methods for data synthesis,0.4879111647605896
translation,127,41,baselines,baselines,has,src-rt- ft,baselines has src-rt- ft,0.5723139047622681
translation,127,86,experimental-setup,levt - qe model,implemented based on,fairseq,levt - qe model implemented based on fairseq,0.7048811912536621
translation,127,4,experiments,jhu - microsoft joint submission,for,wmt 2021 quality estimation shared task,jhu - microsoft joint submission for wmt 2021 quality estimation shared task,0.5941440463066101
translation,127,87,experiments,adam optimizer,with,linear warmup,adam optimizer with linear warmup,0.5890266299247742
translation,127,87,experiments,adam optimizer,with,inverse-sqrt scheduler,adam optimizer with inverse-sqrt scheduler,0.6236183047294617
translation,127,23,model,encoder and decoder,of,levt model,encoder and decoder of levt model,0.5945726633071899
translation,127,23,model,levt model,with those from,"massively pre-trained multilingual nmt model ( m2m - 100 , fan et al. , 2020 )","levt model with those from massively pre-trained multilingual nmt model ( m2m - 100 , fan et al. , 2020 )",0.5670145750045776
translation,127,23,model,model,initialize,encoder and decoder,model initialize encoder and decoder,0.7690961360931396
translation,127,24,model,levt translation model,perform,two -stage finetuning process,levt translation model perform two -stage finetuning process,0.5724625587463379
translation,127,24,model,two -stage finetuning process,to adapt,model,two -stage finetuning process to adapt model,0.6681880354881287
translation,127,24,model,model,from,translation prediction,model from translation prediction,0.5686708092689514
translation,127,24,model,model,using,automatically - generated pseudopost-editing triplets,model using automatically - generated pseudopost-editing triplets,0.6692021489143372
translation,127,24,model,model,using,human post-editing triplets,model using human post-editing triplets,0.6550498604774475
translation,127,24,model,translation prediction,to,quality label prediction,translation prediction to quality label prediction,0.4896903932094574
translation,127,24,model,model,Starting from,levt translation model,model Starting from levt translation model,0.6228222250938416
translation,127,8,results,system,is,top-ranking system,system is top-ranking system,0.5868473649024963
translation,127,8,results,top-ranking system,on,mt mcc metric,top-ranking system on mt mcc metric,0.5321348309516907
translation,127,8,results,mt mcc metric,for,english - german language pair,mt mcc metric for english - german language pair,0.5774583220481873
translation,127,8,results,results,has,system,results has system,0.5883707404136658
translation,127,95,results,outperform,based upon,pre-trained xlm - roberta - base encoder,outperform based upon pre-trained xlm - roberta - base encoder,0.6284204125404358
translation,127,95,results,openkiwi baseline,based upon,pre-trained xlm - roberta - base encoder,openkiwi baseline based upon pre-trained xlm - roberta - base encoder,0.5629948377609253
translation,127,95,results,all language pairs,has,our systems,all language pairs has our systems,0.5883690118789673
translation,127,95,results,outperform,has,openkiwi baseline,outperform has openkiwi baseline,0.5866416096687317
translation,127,95,results,results,In,all language pairs,results In all language pairs,0.4663567543029785
translation,127,96,results,language pairs,with,large amount of available parallel data,language pairs with large amount of available parallel data,0.616880476474762
translation,127,96,results,benefit,of,levt,benefit of levt,0.6326835751533508
translation,127,96,results,most significant,on,language pairs,most significant on language pairs,0.5208141803741455
translation,127,96,results,language pairs,with,large amount of available parallel data,language pairs with large amount of available parallel data,0.616880476474762
translation,127,96,results,language pairs,has,benefit,language pairs has benefit,0.5949132442474365
translation,127,96,results,results,Among,language pairs,results Among language pairs,0.5433920621871948
translation,127,102,results,our nelder - mead ensemble,improves,result,our nelder - mead ensemble improves result,0.7108939290046692
translation,127,102,results,result,by,small but steady margin,result by small but steady margin,0.6490653157234192
translation,127,102,results,results,has,our nelder - mead ensemble,results has our nelder - mead ensemble,0.5190606713294983
translation,127,110,results,result,is,mixed,result is mixed,0.613936185836792
translation,127,110,results,mixed,with,mvppe,mixed with mvppe,0.7349879741668701
translation,127,110,results,performance,for,both language pairs,performance for both language pairs,0.5678526163101196
translation,127,110,results,src-mt1 - mt2,being helpful for,et-en language pair,src-mt1 - mt2 being helpful for et-en language pair,0.6721839308738708
translation,127,110,results,mvppe,has,failing to improve,mvppe has failing to improve,0.6179085373878479
translation,127,110,results,failing to improve,has,performance,failing to improve has performance,0.6044135093688965
translation,127,110,results,results,is,mixed,results is mixed,0.5619810223579407
translation,127,110,results,results,with,mvppe,results with mvppe,0.5958026051521301
translation,127,110,results,results,has,result,results has result,0.5303357243537903
translation,127,112,results,mvppe synthetic data,seems to,significantly improve,mvppe synthetic data seems to significantly improve,0.7331661581993103
translation,127,112,results,f1 score,of,ok label,f1 score of ok label,0.5752792954444885
translation,127,112,results,significantly improve,has,f1 score,significantly improve has f1 score,0.5450616478919983
translation,127,112,results,results,notice,mvppe synthetic data,results notice mvppe synthetic data,0.6733348965644836
translation,127,127,results,highest f1 scores,achieve,gaps,highest f1 scores achieve gaps,0.6764423847198486
translation,127,127,results,highest f1 scores,for,gaps,highest f1 scores for gaps,0.6467466354370117
translation,127,127,results,gaps,rarely higher than,0.8,gaps rarely higher than 0.8,0.5923757553100586
translation,128,27,ablation-analysis,user study,shows,three features,user study shows three features,0.6371628642082214
translation,128,27,ablation-analysis,three features,provided by,intellicat,three features provided by intellicat,0.6919040083885193
translation,128,27,ablation-analysis,postediting time ( 19.2 % ),led to,52.6 % reduction,postediting time ( 19.2 % ) led to 52.6 % reduction,0.6747345328330994
translation,128,27,ablation-analysis,52.6 % reduction,in,translation time,52.6 % reduction in translation time,0.4854694604873657
translation,128,27,ablation-analysis,translation time,compared to,translating from scratch,translation time compared to translating from scratch,0.678823709487915
translation,128,27,ablation-analysis,three features,has,significantly reduce,three features has significantly reduce,0.5604472160339355
translation,128,27,ablation-analysis,intellicat,has,significantly reduce,intellicat has significantly reduce,0.6160958409309387
translation,128,27,ablation-analysis,significantly reduce,has,postediting time ( 19.2 % ),significantly reduce has postediting time ( 19.2 % ),0.565569281578064
translation,128,27,ablation-analysis,ablation analysis,has,user study,ablation analysis has user study,0.5376353859901428
translation,128,51,experimental-setup,nmt model,based on,"transformer ( vaswani et al. , 2017 )","nmt model based on transformer ( vaswani et al. , 2017 )",0.6715218424797058
translation,128,51,experimental-setup,nmt model,using,"opennmt - py ( klein et al. , 2017 )","nmt model using opennmt - py ( klein et al. , 2017 )",0.6132246255874634
translation,128,51,experimental-setup,experimental setup,build,nmt model,experimental setup build nmt model,0.6914107799530029
translation,128,63,experimental-setup,"xlm -roberta ( conneau et al. , 2020 )",with,few additional parameters,"xlm -roberta ( conneau et al. , 2020 ) with few additional parameters",0.6141909956932068
translation,128,63,experimental-setup,few additional parameters,to jointly train,sentence - level and word-level qes,few additional parameters to jointly train sentence - level and word-level qes,0.627970278263092
translation,128,63,experimental-setup,experimental setup,fine- tune,"xlm -roberta ( conneau et al. , 2020 )","experimental setup fine- tune xlm -roberta ( conneau et al. , 2020 )",0.6847253441810608
translation,128,104,experimental-setup,qe and translation suggestion models,trained using,two tesla v100 gpus,qe and translation suggestion models trained using two tesla v100 gpus,0.6868312358856201
translation,128,103,experiments,two suggestion models,fine-tuned with,modified tlm objective,two suggestion models fine-tuned with modified tlm objective,0.792902946472168
translation,128,4,model,neural models,streamline,post-editing process,neural models streamline post-editing process,0.7453959584236145
translation,128,4,model,post-editing process,on,machine translation output,post-editing process on machine translation output,0.5235927104949951
translation,128,4,model,intellicat,has,interactive translation interface,intellicat has interactive translation interface,0.5712081789970398
translation,128,5,model,two quality estimation ( qe ) models,at,different granularities,two quality estimation ( qe ) models at different granularities,0.5361654162406921
translation,128,5,model,two quality estimation ( qe ) models,to predict,quality,two quality estimation ( qe ) models to predict quality,0.7207714319229126
translation,128,5,model,two quality estimation ( qe ) models,to locate,parts of the machinetranslated sentence,two quality estimation ( qe ) models to locate parts of the machinetranslated sentence,0.6877740621566772
translation,128,5,model,quality,of,each machine - translated sentence,quality of each machine - translated sentence,0.5686199069023132
translation,128,5,model,quality,to locate,parts of the machinetranslated sentence,quality to locate parts of the machinetranslated sentence,0.698140025138855
translation,128,5,model,wordlevel qe,to locate,parts of the machinetranslated sentence,wordlevel qe to locate parts of the machinetranslated sentence,0.6621770858764648
translation,128,5,model,parts of the machinetranslated sentence,need,correction,parts of the machinetranslated sentence need correction,0.6382901072502136
translation,128,5,model,model,leverage,two quality estimation ( qe ) models,model leverage two quality estimation ( qe ) models,0.7334434390068054
translation,128,6,model,novel translation suggestion model,conditioned on both,left and right contexts,novel translation suggestion model conditioned on both left and right contexts,0.7889338135719299
translation,128,6,model,specific words or phrases,for,correction,specific words or phrases for correction,0.6166214346885681
translation,128,6,model,model,introduce,novel translation suggestion model,model introduce novel translation suggestion model,0.583458423614502
translation,128,16,model,hybrid cat interface,designed to provide,pe - level efficiency,hybrid cat interface designed to provide pe - level efficiency,0.6699889302253723
translation,128,16,model,pe - level efficiency,while retaining,advantages of itp,pe - level efficiency while retaining advantages of itp,0.665117621421814
translation,128,16,model,intellicat,has,hybrid cat interface,intellicat has hybrid cat interface,0.6192005276679993
translation,128,16,model,model,introduce,intellicat,model introduce intellicat,0.6913560628890991
translation,128,23,model,suggestion model,conditioned on,left and right contexts,suggestion model conditioned on left and right contexts,0.7650506496429443
translation,128,23,model,left and right contexts,based on,"xlm - roberta ( conneau et al. , 2020 )","left and right contexts based on xlm - roberta ( conneau et al. , 2020 )",0.6742354035377502
translation,128,23,model,novel words and phrases,has,suggestion model,novel words and phrases has suggestion model,0.5827614068984985
translation,128,23,model,model,introduce,novel words and phrases,model introduce novel words and phrases,0.6871525049209595
translation,128,144,model,intelligent mt post-editing interface,for,document translation,intelligent mt post-editing interface for document translation,0.5433465242385864
translation,128,144,model,intellicat,has,intelligent mt post-editing interface,intellicat has intelligent mt post-editing interface,0.5661398768424988
translation,128,144,model,model,introduce,intellicat,model introduce intellicat,0.6913560628890991
translation,128,145,model,three neural networkbased features,to assist,post-editing,three neural networkbased features to assist post-editing,0.6754263043403625
translation,128,145,model,alternative translation suggestions,for,words or phrases,alternative translation suggestions for words or phrases,0.5936190485954285
translation,128,145,model,automatic formatting,of,translated document,automatic formatting of translated document,0.5873983502388
translation,128,145,model,translated document,based on,word alignments,translated document based on word alignments,0.6070300340652466
translation,128,107,results,post-edited,based on,qe prediction,post-edited based on qe prediction,0.6910718083381653
translation,128,107,results,qe prediction,with,top - 1 suggestion,qe prediction with top - 1 suggestion,0.6373194456100464
translation,128,107,results,ter and bleu,improved over,baseline,ter and bleu improved over baseline,0.7034775018692017
translation,128,107,results,baseline,by,?2.33 and + 1.56,baseline by ?2.33 and + 1.56,0.5593679547309875
translation,128,107,results,qe prediction,has,ter and bleu,qe prediction has ter and bleu,0.5764854550361633
translation,128,107,results,top - 1 suggestion,has,ter and bleu,top - 1 suggestion has ter and bleu,0.5797463655471802
translation,128,109,results,top - 5 suggestions,are,ter and bleu,top - 5 suggestions are ter and bleu,0.5974254012107849
translation,128,109,results,top - 5 suggestions,provided,ter and bleu,top - 5 suggestions provided ter and bleu,0.606367290019989
translation,128,109,results,ter and bleu,improved by,?6.01 and + 6.15,ter and bleu improved by ?6.01 and + 6.15,0.7051935791969299
translation,128,109,results,ter and bleu,improved by,?14.07 and + 20.13,ter and bleu improved by ?14.07 and + 20.13,0.6907193064689636
translation,128,109,results,?6.01 and + 6.15,for,qe prediction condition,?6.01 and + 6.15 for qe prediction condition,0.679373025894165
translation,128,109,results,?6.01 and + 6.15,for,oracle qe condition,?6.01 and + 6.15 for oracle qe condition,0.6913260221481323
translation,128,109,results,?14.07 and + 20.13,for,oracle qe condition,?14.07 and + 20.13 for oracle qe condition,0.6754891276359558
translation,128,109,results,top - 5 suggestions,has,ter and bleu,top - 5 suggestions has ter and bleu,0.5674594640731812
translation,128,109,results,results,When,top - 5 suggestions,results When top - 5 suggestions,0.6446503400802612
translation,128,111,results,significantly outperforms,showing that,fine-tuning xlm -r,significantly outperforms showing that fine-tuning xlm -r,0.6638716459274292
translation,128,111,results,xlm -r,in,all experimental settings,xlm -r in all experimental settings,0.5014845728874207
translation,128,111,results,xlm -r,showing that,fine-tuning xlm -r,xlm -r showing that fine-tuning xlm -r,0.6652513146400452
translation,128,111,results,fine-tuning xlm -r,with,modified tlm objective,fine-tuning xlm -r with modified tlm objective,0.6381747126579285
translation,128,111,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,128,111,results,significantly outperforms,has,xlm -r,significantly outperforms has xlm -r,0.5825835466384888
translation,128,111,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,129,29,experimental-setup,sentencepiece,to produce,subword units,sentencepiece to produce subword units,0.7041879296302795
translation,129,29,experimental-setup,experimental setup,has,sentencepiece,experimental setup has sentencepiece,0.5422448515892029
translation,129,43,experimental-setup,"transformer ( vaswani et al. , 2017 ) architecture",for,all bilingual models,"transformer ( vaswani et al. , 2017 ) architecture for all bilingual models",0.5597884058952332
translation,129,43,experimental-setup,experimental setup,use,"transformer ( vaswani et al. , 2017 ) architecture","experimental setup use transformer ( vaswani et al. , 2017 ) architecture",0.5739732980728149
translation,129,61,experimental-setup,transformer architecture,implemented in,fairseq,transformer architecture implemented in fairseq,0.7165786027908325
translation,129,61,experimental-setup,fairseq,has,"ott et al. , 2019 )","fairseq has ott et al. , 2019 )",0.6141664981842041
translation,129,61,experimental-setup,experimental setup,use,transformer architecture,experimental setup use transformer architecture,0.5702999830245972
translation,129,62,experimental-setup,nmt and mnmt systems,use,transformer - big architecture,nmt and mnmt systems use transformer - big architecture,0.6297403573989868
translation,129,62,experimental-setup,experimental setup,For training,nmt and mnmt systems,experimental setup For training nmt and mnmt systems,0.748661994934082
translation,129,63,experimental-setup,optimization,follow,default settings,optimization follow default settings,0.6631185412406921
translation,129,63,experimental-setup,optimization,used,adam optimizer,optimization used adam optimizer,0.559220016002655
translation,129,63,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,129,63,experimental-setup,learning rate,of,0.0003,learning rate of 0.0003,0.5981547236442566
translation,129,63,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,129,63,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,129,64,experimental-setup,overfitting,applied,dropout,overfitting applied dropout,0.6830777525901794
translation,129,64,experimental-setup,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,129,64,experimental-setup,0.3,on,all layers,0.3 on all layers,0.5387910604476929
translation,129,64,experimental-setup,experimental setup,To prevent,overfitting,experimental setup To prevent overfitting,0.6143673658370972
translation,129,65,experimental-setup,beam search,of size,5,beam search of size 5,0.7482876181602478
translation,129,65,experimental-setup,beam search,to balance,decoding time and accuracy,beam search to balance decoding time and accuracy,0.5986949801445007
translation,129,65,experimental-setup,decoding time and accuracy,of,search,decoding time and accuracy of search,0.6100500822067261
translation,129,65,experimental-setup,inference,has,beam search,inference has beam search,0.5723796486854553
translation,129,65,experimental-setup,experimental setup,time of,inference,experimental setup time of inference,0.6351554989814758
translation,129,66,experimental-setup,number of warm - up steps,set to,4000,number of warm - up steps set to 4000,0.7289392352104187
translation,129,66,experimental-setup,vocabulary size,is,133k,vocabulary size is 133k,0.5855304598808289
translation,129,66,experimental-setup,experimental setup,has,number of warm - up steps,experimental setup has number of warm - up steps,0.5233396887779236
translation,129,66,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,129,67,experimental-setup,length penalty factor,of,1.7,length penalty factor of 1.7,0.5805371999740601
translation,129,67,experimental-setup,length penalty factor,to maintain,balance,length penalty factor to maintain balance,0.6552155613899231
translation,129,67,experimental-setup,balance,between,long and short sentences,balance between long and short sentences,0.6655496954917908
translation,129,67,experimental-setup,experimental setup,set,length penalty factor,experimental setup set length penalty factor,0.5889873504638672
translation,129,68,experimental-setup,batch size,set to,128,batch size set to 128,0.7535710334777832
translation,129,68,experimental-setup,128,during,decoding,128 during decoding,0.7111367583274841
translation,129,68,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,129,69,experimental-setup,our models,for,approximately 3 weeks,our models for approximately 3 weeks,0.6351732015609741
translation,129,69,experimental-setup,approximately 3 weeks,on,one machine,approximately 3 weeks on one machine,0.5596316456794739
translation,129,69,experimental-setup,one machine,with,8 nvidia gtx 2080 ti 11gb gpus,one machine with 8 nvidia gtx 2080 ti 11gb gpus,0.5583564043045044
translation,129,69,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,129,78,experimental-setup,bilingual models,using,standard transformer - big architecture,bilingual models using standard transformer - big architecture,0.6348649263381958
translation,129,78,experimental-setup,standard transformer - big architecture,for,6 languages,standard transformer - big architecture for 6 languages,0.5965172648429871
translation,129,78,experimental-setup,6 languages,in,30 directions,6 languages in 30 directions,0.5399587154388428
translation,129,78,experimental-setup,experimental setup,trained,bilingual models,experimental setup trained bilingual models,0.7112495303153992
translation,129,76,experiments,flores 101_mm100_615 m,tested on,devtest datasets,flores 101_mm100_615 m tested on devtest datasets,0.7465752363204956
translation,129,39,model,single multilingual model,translate,all language pairs,single multilingual model translate all language pairs,0.6976460814476013
translation,129,39,model,model,trained,single multilingual model,model trained single multilingual model,0.7253170013427734
translation,129,45,model,data,into,subword units,data into subword units,0.612906813621521
translation,129,45,model,subword units,using,sentencepiece,subword units using sentencepiece,0.6785262823104858
translation,129,45,model,sentencepiece,jointly learned for,all languages,sentencepiece jointly learned for all languages,0.6964952945709229
translation,129,45,model,model,segment,data,model segment data,0.7589826583862305
translation,129,94,results,sr-en language pair,produced,best results,sr-en language pair produced best results,0.6611705422401428
translation,129,94,results,best results,in terms of,spbleu score,best results in terms of spbleu score,0.6812323927879333
translation,129,94,results,hu-hr language pair,scored,lowest,hu-hr language pair scored lowest,0.7331334352493286
translation,129,94,results,results,see from,sr-en language pair,results see from sr-en language pair,0.568414032459259
translation,130,59,experiments,mt task,use,12 stacked transformer layers,mt task use 12 stacked transformer layers,0.6582022905349731
translation,130,59,experiments,mt task,use,6 stacked transformer layer,mt task use 6 stacked transformer layer,0.6560087203979492
translation,130,59,experiments,12 stacked transformer layers,with size,2048,12 stacked transformer layers with size 2048,0.7797143459320068
translation,130,59,experiments,2048,as,encoder,2048 as encoder,0.6124551296234131
translation,130,59,experiments,2048,as,decoder,2048 as decoder,0.6158087849617004
translation,130,59,experiments,2048,as,decoder,2048 as decoder,0.6158087849617004
translation,130,59,experiments,6 stacked transformer layer,with size 2048,decoder,6 stacked transformer layer with size 2048 decoder,0.7537438273429871
translation,130,81,experiments,joint vocabulary training vanilla e2e st model,uses,separate vocabularies,joint vocabulary training vanilla e2e st model uses separate vocabularies,0.5663642287254333
translation,130,81,experiments,separate vocabularies,for,source and target languages,separate vocabularies for source and target languages,0.5784323811531067
translation,130,55,hyperparameters,input and target texts,employ,newly proposed subword regularisation method,input and target texts employ newly proposed subword regularisation method,0.511100709438324
translation,130,55,hyperparameters,newly proposed subword regularisation method,to build,vocabulary,newly proposed subword regularisation method to build vocabulary,0.6808613538742065
translation,130,55,hyperparameters,vocabulary,with,size,vocabulary with size,0.6556599140167236
translation,130,55,hyperparameters,size,of,8000,size of 8000,0.6586341261863708
translation,130,55,hyperparameters,hyperparameters,For,input and target texts,hyperparameters For input and target texts,0.5332984328269958
translation,130,55,hyperparameters,hyperparameters,both,input and target texts,hyperparameters both input and target texts,0.6070728898048401
translation,130,58,hyperparameters,6 stacked transformer layers,size,512,6 stacked transformer layers size 512,0.7102683782577515
translation,130,58,hyperparameters,512,as,decoder,512 as decoder,0.6094554662704468
translation,130,58,hyperparameters,hyperparameters,use,6 stacked transformer layers,hyperparameters use 6 stacked transformer layers,0.5970174074172974
translation,130,61,hyperparameters,maximum number of tokens,in,each batch,maximum number of tokens in each batch,0.5225655436515808
translation,130,61,hyperparameters,maximum number of tokens,set to,40000,maximum number of tokens set to 40000,0.7193883657455444
translation,130,61,hyperparameters,hyperparameters,has,maximum number of tokens,hyperparameters has maximum number of tokens,0.49987244606018066
translation,130,19,model,mutual - learning paradigm,regards,st and mt models,mutual - learning paradigm regards st and mt models,0.6308789253234863
translation,130,19,model,st and mt models,as,peers,st and mt models as peers,0.6062827706336975
translation,130,19,model,peers,learn,collaboratively,peers learn collaboratively,0.69854336977005
translation,130,19,model,collaboratively,aiming to,iteratively learn,collaboratively aiming to iteratively learn,0.6357278227806091
translation,130,19,model,knowledge,between,two models,knowledge between two models,0.7042422890663147
translation,130,20,model,model,has,mutual - learning,model has mutual - learning,0.594639241695404
translation,130,9,results,performance,of,mt model,performance of mt model,0.6049423813819885
translation,130,24,results,mutuallearning method,by integrating,cyclical annealing schedule,mutuallearning method by integrating cyclical annealing schedule,0.6294588446617126
translation,130,24,results,cyclical annealing schedule,alleviates,kl vanishing problem,cyclical annealing schedule alleviates kl vanishing problem,0.6192906498908997
translation,130,24,results,results,improve,mutuallearning method,results improve mutuallearning method,0.6023554801940918
translation,130,67,results,mutual - learning - based st model,provides,competitive results,mutual - learning - based st model provides competitive results,0.6038509011268616
translation,130,67,results,competitive results,comparing to,cascaded model,competitive results comparing to cascaded model,0.7482876777648926
translation,130,67,results,results,has,mutual - learning - based st model,results has mutual - learning - based st model,0.5161113142967224
translation,130,68,results,our model,achieves,0.6 and 0.5 bleu score improvement,our model achieves 0.6 and 0.5 bleu score improvement,0.6371724009513855
translation,130,68,results,0.6 and 0.5 bleu score improvement,in,en-fr and en- es datasets,0.6 and 0.5 bleu score improvement in en-fr and en- es datasets,0.5216377973556519
translation,130,68,results,results,has,our model,results has our model,0.5871725678443909
translation,130,77,results,proposed mutual - learning approach,outperforms,one way knowledge distillation strategy ( e2e + kd ),proposed mutual - learning approach outperforms one way knowledge distillation strategy ( e2e + kd ),0.7286608815193176
translation,130,77,results,one way knowledge distillation strategy ( e2e + kd ),by,1.0 and 0.6 bleu score,one way knowledge distillation strategy ( e2e + kd ) by 1.0 and 0.6 bleu score,0.5354326963424683
translation,130,77,results,1.0 and 0.6 bleu score,on,en-fr and en- es datasets,1.0 and 0.6 bleu score on en-fr and en- es datasets,0.5247053503990173
translation,130,80,results,our mutual - learning strategy,more effective way of,joint learning,our mutual - learning strategy more effective way of joint learning,0.6222269535064697
translation,130,80,results,"0.7 , 0.3 bleu score increase",over,mtl,"0.7 , 0.3 bleu score increase over mtl",0.6445135474205017
translation,130,80,results,mtl,in,st task,mtl in st task,0.49855440855026245
translation,130,80,results,results,show,our mutual - learning strategy,results show our mutual - learning strategy,0.6229948997497559
translation,130,87,results,mutual - learning,improves,mt model 's performance,mutual - learning improves mt model 's performance,0.5974387526512146
translation,130,87,results,results,conclude that,mutual - learning,results conclude that mutual - learning,0.6200563907623291
translation,130,88,results,our system,gains,0.7 and 0.3 bleu score,our system gains 0.7 and 0.3 bleu score,0.6924387216567993
translation,130,88,results,0.7 and 0.3 bleu score,in,en-fr and en- es datasets,0.7 and 0.3 bleu score in en-fr and en- es datasets,0.5297964811325073
translation,130,88,results,0.7 and 0.3 bleu score,compared to,independently trained mt system,0.7 and 0.3 bleu score compared to independently trained mt system,0.6136885285377502
translation,130,88,results,results,has,our system,results has our system,0.5954442024230957
translation,130,89,results,our system,exceeds,typical mtl approach,our system exceeds typical mtl approach,0.6327630281448364
translation,130,89,results,typical mtl approach,by,0.2 and 0.4 bleu score,typical mtl approach by 0.2 and 0.4 bleu score,0.5255329608917236
translation,130,89,results,0.2 and 0.4 bleu score,in,mt task,0.2 and 0.4 bleu score in mt task,0.5270532965660095
translation,130,89,results,results,has,our system,results has our system,0.5954442024230957
translation,131,37,experiments,morph + bpe based subword segmentation,has,embedding size,morph + bpe based subword segmentation has embedding size,0.5540840029716492
translation,131,37,experiments,embedding size,has,512,embedding size has 512,0.6119686365127563
translation,131,38,hyperparameters,transformer,for,encoder and decoder,transformer for encoder and decoder,0.635772168636322
translation,131,38,hyperparameters,rnn_size,has,512,rnn_size has 512,0.6426093578338623
translation,131,38,hyperparameters,heads 4 encoder - decoder layers,has,2,heads 4 encoder - decoder layers has 2,0.5918335914611816
translation,131,38,hyperparameters,label smoothing,has,1.0,label smoothing has 1.0,0.5428792834281921
translation,131,38,hyperparameters,dropout,has,0.30,dropout has 0.30,0.5667999982833862
translation,131,38,hyperparameters,optimizer,has,adam,optimizer has adam,0.5559503436088562
translation,131,38,hyperparameters,beam size,has,4 ( train ) and 10 ( test ),beam size has 4 ( train ) and 10 ( test ),0.586389422416687
translation,131,38,hyperparameters,training steps,has,20k,training steps has 20k,0.6000010967254639
translation,131,38,hyperparameters,hyperparameters,has,transformer,hyperparameters has transformer,0.517048180103302
translation,131,45,results,transformer network based mt models,can be,improved,transformer network based mt models can be improved,0.6985415816307068
translation,131,45,results,improved,with,morph based segmentation,improved with morph based segmentation,0.6579366326332092
translation,131,45,results,morph based segmentation,along with,byte pair encoding,morph based segmentation along with byte pair encoding,0.6122859716415405
translation,131,45,results,byte pair encoding,for,morph rich languages,byte pair encoding for morph rich languages,0.577631413936615
translation,131,45,results,low resource settings,has,transformer network based mt models,low resource settings has transformer network based mt models,0.5720870494842529
translation,131,45,results,results,show,low resource settings,results show low resource settings,0.6067590117454529
translation,131,45,results,results,for,low resource settings,results for low resource settings,0.5844101309776306
translation,131,46,results,multilingual machine translation problem,along with,monolingual data,multilingual machine translation problem along with monolingual data,0.5678418278694153
translation,131,46,results,multilingual machine translation problem,improves,quality of mt models,multilingual machine translation problem improves quality of mt models,0.5812230110168457
translation,131,46,results,results,forming it as,multilingual machine translation problem,results forming it as multilingual machine translation problem,0.5828024744987488
translation,132,117,ablation-analysis,noisy word-translations,lead to,very low bleu scores,noisy word-translations lead to very low bleu scores,0.5898392796516418
translation,132,117,ablation-analysis,ablation analysis,Without ( M) DAE pre-training,noisy word-translations,ablation analysis Without ( M) DAE pre-training noisy word-translations,0.749553382396698
translation,132,160,ablation-analysis,models,almost always produce,best performing model,models almost always produce best performing model,0.6708775758743286
translation,132,28,baselines,umt - enhanced ssnmt,jointly learns,mt,umt - enhanced ssnmt jointly learns mt,0.7392206788063049
translation,132,28,baselines,umt - enhanced ssnmt,extracting,similar sentences,umt - enhanced ssnmt extracting similar sentences,0.7167630791664124
translation,132,28,baselines,similar sentences,for,training,similar sentences for training,0.5867654085159302
translation,132,28,baselines,training,from,comparable corpora,training from comparable corpora,0.5824403166770935
translation,132,28,baselines,comparable corpora,in,loop on -line,comparable corpora in loop on -line,0.5539977550506592
translation,132,28,baselines,baselines,has,umt - enhanced ssnmt,baselines has umt - enhanced ssnmt,0.5883321762084961
translation,132,41,baselines,baselines,has,word-translation ( wt ),baselines has word-translation ( wt ),0.5455885529518127
translation,132,65,hyperparameters,word-embedding - based initialization,learn,cbow word embeddings,word-embedding - based initialization learn cbow word embeddings,0.6327946186065674
translation,132,65,hyperparameters,cbow word embeddings,using,"word2vec ( mikolov et al. , 2013 )","cbow word embeddings using word2vec ( mikolov et al. , 2013 )",0.5359134078025818
translation,132,65,hyperparameters,cbow word embeddings,projected into,common multilingual space,cbow word embeddings projected into common multilingual space,0.6713846921920776
translation,132,65,hyperparameters,"word2vec ( mikolov et al. , 2013 )",projected into,common multilingual space,"word2vec ( mikolov et al. , 2013 ) projected into common multilingual space",0.6271259188652039
translation,132,65,hyperparameters,common multilingual space,via,"vecmap ( artetxe et al. , 2017 )","common multilingual space via vecmap ( artetxe et al. , 2017 )",0.636900782585144
translation,132,65,hyperparameters,"vecmap ( artetxe et al. , 2017 )",to attain,bilingual embeddings,"vecmap ( artetxe et al. , 2017 ) to attain bilingual embeddings",0.5457385778427124
translation,132,65,hyperparameters,bilingual embeddings,between,"en -{ af , ... , yo}","bilingual embeddings between en -{ af , ... , yo}",0.6177898645401001
translation,132,65,hyperparameters,hyperparameters,For,word-embedding - based initialization,hyperparameters For word-embedding - based initialization,0.541510820388794
translation,132,70,hyperparameters,"bart - style noise ( ? = 3.5 , p = 0.35 )",for,word sequence masking,"bart - style noise ( ? = 3.5 , p = 0.35 ) for word sequence masking",0.5480865836143494
translation,132,70,hyperparameters,hyperparameters,use,"bart - style noise ( ? = 3.5 , p = 0.35 )","hyperparameters use bart - style noise ( ? = 3.5 , p = 0.35 )",0.5773830413818359
translation,132,83,hyperparameters,ssnmt,is,transformer base,ssnmt is transformer base,0.6092394590377808
translation,132,83,hyperparameters,transformer base,with,default parameters,transformer base with default parameters,0.5935497879981995
translation,132,83,hyperparameters,hyperparameters,implementation of,ssnmt,hyperparameters implementation of ssnmt,0.6424988508224487
translation,132,84,hyperparameters,batch size,of,50 sentences,batch size of 50 sentences,0.5830963253974915
translation,132,84,hyperparameters,maximum sequence length,of,100 tokens,maximum sequence length of 100 tokens,0.5846397280693054
translation,132,16,model,ssnmt,on,different data sizes,ssnmt on different data sizes,0.5801113247871399
translation,132,16,model,different data sizes,ranging from,very low-resource ( ? 66 k non-parallel sentences ),different data sizes ranging from very low-resource ( ? 66 k non-parallel sentences ),0.6424311995506287
translation,132,16,model,different data sizes,ranging from,high- resource ( ? 20 m sentences ),different data sizes ranging from high- resource ( ? 20 m sentences ),0.6733712553977966
translation,132,71,model,one random mask insertion,has,per sequence,one random mask insertion has per sequence,0.6174763441085815
translation,132,71,model,model,add,one random mask insertion,model add one random mask insertion,0.6761806011199951
translation,132,7,results,umt techniques,into,ssnmt,umt techniques into ssnmt,0.6917511820793152
translation,132,7,results,umt techniques,into,ssnmt and umt,umt techniques into ssnmt and umt,0.6440939903259277
translation,132,7,results,ssnmt and umt,on,all tested language pairs,ssnmt and umt on all tested language pairs,0.5496988296508789
translation,132,7,results,ssnmt and umt,with,improvements,ssnmt and umt with improvements,0.6941167712211609
translation,132,7,results,improvements,of,+ 51.5,improvements of + 51.5,0.5829483866691589
translation,132,7,results,improvements,of,statistical umt and hybrid umt,improvements of statistical umt and hybrid umt,0.6139405965805054
translation,132,7,results,improvements,up to,"+ 4.3 bleu , + 50.8 bleu","improvements up to + 4.3 bleu , + 50.8 bleu",0.576030969619751
translation,132,7,results,improvements,up to,+ 51.5,improvements up to + 51.5,0.650258481502533
translation,132,7,results,improvements,up to,statistical umt and hybrid umt,improvements up to statistical umt and hybrid umt,0.61595219373703
translation,132,7,results,+ 51.5,over,ssnmt,+ 51.5 over ssnmt,0.6172568798065186
translation,132,7,results,statistical umt and hybrid umt,on,afrikaans to english,statistical umt and hybrid umt on afrikaans to english,0.5573719143867493
translation,132,7,results,ssnmt,has,significantly outperforms,ssnmt has significantly outperforms,0.6169436573982239
translation,132,7,results,significantly outperforms,has,ssnmt and umt,significantly outperforms has ssnmt and umt,0.5757811665534973
translation,132,7,results,results,show,umt techniques,results show umt techniques,0.6398722529411316
translation,132,7,results,results,including,umt techniques,results including umt techniques,0.6740823984146118
translation,132,89,results,translation quality,measured by,bleu,translation quality measured by bleu,0.6559703350067139
translation,132,89,results,bleu,is,very low,bleu is very low,0.5863638520240784
translation,132,89,results,very low,in,lowresource setting,very low in lowresource setting,0.54645174741745
translation,132,89,results,results,shows that,translation quality,results shows that translation quality,0.5445561408996582
translation,132,90,results,bleu,close to,zero,bleu close to zero,0.7188472747802734
translation,132,90,results,zero,with,base ( b ) and b +bt models,zero with base ( b ) and b +bt models,0.6707764267921448
translation,132,90,results,only 4 k comparable articles,has,bleu,only 4 k comparable articles has bleu,0.5670458078384399
translation,132,91,results,wt,applied to,rejected back - translated pairs,wt applied to rejected back - translated pairs,0.7245145440101624
translation,132,91,results,bleus,of,3.38 11 ( en2 f r ),bleus of 3.38 11 ( en2 f r ),0.5914685130119324
translation,132,91,results,bleus,of,3.58 ( f r2en ),bleus of 3.58 ( f r2en ),0.5539210438728333
translation,132,91,results,results,when,wt,results when wt,0.645835280418396
translation,132,113,results,difference,between,no ( none ) and word-embedding ( we ) initialization,difference between no ( none ) and word-embedding ( we ) initialization,0.6379470825195312
translation,132,113,results,no ( none ) and word-embedding ( we ) initialization,is,barely significant,no ( none ) and word-embedding ( we ) initialization is barely significant,0.5794867873191833
translation,132,113,results,barely significant,across,all language pairs and techniques,barely significant across all language pairs and techniques,0.7307342290878296
translation,132,113,results,results,has,difference,results has difference,0.5636705756187439
translation,132,114,results,all language pairs,except,en-af,all language pairs except en-af,0.6815385222434998
translation,132,114,results,mdae initialization,tends to be,best choice,mdae initialization tends to be best choice,0.6231489181518555
translation,132,114,results,best choice,with,major gains,best choice with major gains,0.6479672789573669
translation,132,114,results,major gains,of,"+ 4.2 bleu ( yo2en , b+bt )","major gains of + 4.2 bleu ( yo2en , b+bt )",0.569970965385437
translation,132,114,results,major gains,of,"+ 5.3 bleu ( kn2en , b +bt )","major gains of + 5.3 bleu ( kn2en , b +bt )",0.5478348135948181
translation,132,114,results,"+ 5.3 bleu ( kn2en , b +bt )",over,we -initialized counterparts,"+ 5.3 bleu ( kn2en , b +bt ) over we -initialized counterparts",0.6383312344551086
translation,132,114,results,all language pairs,has,mdae initialization,all language pairs has mdae initialization,0.55177903175354
translation,132,114,results,en-af,has,mdae initialization,en-af has mdae initialization,0.5724965929985046
translation,132,114,results,results,For,all language pairs,results For all language pairs,0.5478318929672241
translation,132,116,results,noisy inputs,resulting in,big improvement,noisy inputs resulting in big improvement,0.663543164730072
translation,132,116,results,big improvement,in,translation performance,big improvement in translation performance,0.49160951375961304
translation,132,116,results,big improvement,on,en-af and en-sw b+bt + wt models,big improvement on en-af and en-sw b+bt + wt models,0.553515613079071
translation,132,116,results,en-af and en-sw b+bt + wt models,in comparison to,we -initialized counterparts,en-af and en-sw b+bt + wt models in comparison to we -initialized counterparts,0.6998670101165771
translation,132,116,results,results,By performing,m ) dae,results By performing m ) dae,0.6912214756011963
translation,132,118,results,adding the + n data augmentation technique,lets,model,adding the + n data augmentation technique lets model,0.7216210961341858
translation,132,118,results,model,learn from,noisy word-translations,model learn from noisy word-translations,0.6275254487991333
translation,132,118,results,noisy word-translations,with,improved results,noisy word-translations with improved results,0.6068636178970337
translation,132,118,results,results,Adding,additional denoising task,results Adding additional denoising task,0.6705467700958252
translation,132,119,results,we initialization,performs,best,we initialization performs best,0.6596128344535828
translation,132,119,results,best,with,bleu scores,best with bleu scores,0.5567317008972168
translation,132,119,results,bleu scores,of,52.2 ( af 2en ) and 51.2 ( en2 af ),bleu scores of 52.2 ( af 2en ) and 51.2 ( en2 af ),0.5521405339241028
translation,132,119,results,en-af only,has,we initialization,en-af only has we initialization,0.6640484929084778
translation,132,119,results,results,For,en-af only,results For en-af only,0.6536070108413696
translation,132,124,results,method,translating into,low-resource languages,method translating into low-resource languages,0.612606406211853
translation,132,124,results,beneficial,translating into,low-resource languages,beneficial translating into low-resource languages,0.7058610320091248
translation,132,124,results,low-resource languages,with,en2kn,low-resource languages with en2kn,0.6254149079322815
translation,132,124,results,en2kn,reaching,bleu 3.3,en2kn reaching bleu 3.3,0.675198495388031
translation,132,124,results,results,has,method,results has method,0.49327942728996277
translation,132,125,results,b+bt +wt,seems to be,best data augmentation technique,b+bt +wt seems to be best data augmentation technique,0.6559287905693054
translation,132,125,results,best data augmentation technique,when,amount of data,best data augmentation technique when amount of data,0.6466801762580872
translation,132,125,results,amount of data,is,very small,amount of data is very small,0.5497183203697205
translation,132,125,results,gains,of,+ 2.4 bleu,gains of + 2.4 bleu,0.5502070188522339
translation,132,125,results,+ 2.4 bleu,on,en2 yo,+ 2.4 bleu on en2 yo,0.6696037650108337
translation,132,125,results,en2 yo,over,baseline b,en2 yo over baseline b,0.6574084758758545
translation,132,125,results,results,has,b+bt +wt,results has b+bt +wt,0.5503409504890442
translation,132,128,results,+ bt,tends to be,best choice,+ bt tends to be best choice,0.7192782759666443
translation,132,128,results,+ bt,with,top bleus,+ bt with top bleus,0.7298736572265625
translation,132,128,results,best choice,with,top bleus,best choice with top bleus,0.6657626628875732
translation,132,128,results,top bleus,on,en-sw,top bleus on en-sw,0.7585669755935669
translation,132,128,results,en-sw,of,"7.4 ( en2sw , mdae ) and 7.9 ( sw2en , mdae )","en-sw of 7.4 ( en2sw , mdae ) and 7.9 ( sw2en , mdae )",0.606153130531311
translation,132,128,results,"languages with more data available ( en - { af , kn , my , ne , sw} )",has,+ bt,"languages with more data available ( en - { af , kn , my , ne , sw} ) has + bt",0.6122943758964539
translation,132,128,results,results,On,"languages with more data available ( en - { af , kn , my , ne , sw} )","results On languages with more data available ( en - { af , kn , my , ne , sw} )",0.5041489601135254
translation,132,136,results,beneficial,in,my2en direction,beneficial in my2en direction,0.5232264399528503
translation,132,136,results,no effect,on,en2my,no effect on en2my,0.5909843444824219
translation,132,136,results,en-my,has,mdae approach,en-my has mdae approach,0.6333232522010803
translation,132,136,results,results,For,en-my,results For en-my,0.6658051609992981
translation,132,154,results,multilingual ss - nmt training,in combination with,back - translation ( b+ bt ),multilingual ss - nmt training in combination with back - translation ( b+ bt ),0.6561746597290039
translation,132,154,results,back - translation ( b+ bt ),leads to,top results,back - translation ( b+ bt ) leads to top results,0.6136032342910767
translation,132,154,results,top results,for,low-resource similar and distant language combinations,top results for low-resource similar and distant language combinations,0.5897226333618164
translation,132,155,results,multilingual setup,is,less beneficial,multilingual setup is less beneficial,0.5696729421615601
translation,132,155,results,en-af only,has,multilingual setup,en-af only has multilingual setup,0.59095299243927
translation,132,155,results,results,For,en-af only,results For en-af only,0.6536070108413696
translation,132,159,results,bilingual finetuning,improves,multilingual model,bilingual finetuning improves multilingual model,0.6221297979354858
translation,132,159,results,multilingual model,with,major increase,multilingual model with major increase,0.6513940691947937
translation,132,159,results,major increase,of,+ 4.2 bleu,major increase of + 4.2 bleu,0.5542780160903931
translation,132,159,results,+ 4.2 bleu,for,en-sw,+ 4.2 bleu for en-sw,0.6818987131118774
translation,132,159,results,+ 4.2 bleu,resulting in,bleu score,+ 4.2 bleu resulting in bleu score,0.6174679398536682
translation,132,159,results,bleu score,of,11.6,bleu score of 11.6,0.5459384322166443
translation,132,159,results,results,has,bilingual finetuning,results has bilingual finetuning,0.5315796136856079
translation,132,161,results,results,has,comparison to other nmt architectures,results has comparison to other nmt architectures,0.5610795021057129
translation,132,163,results,outperforms,both,ssnmt baseline and umt models,outperforms both ssnmt baseline and umt models,0.641590416431427
translation,132,163,results,all languages,has,ssnmt with data augmentation,all languages has ssnmt with data augmentation,0.61798095703125
translation,132,163,results,ssnmt with data augmentation,has,outperforms,ssnmt with data augmentation has outperforms,0.609862208366394
translation,132,163,results,results,Over,all languages,results Over all languages,0.5966923832893372
translation,132,167,results,supervised model,trained on,laser extractions,supervised model trained on laser extractions,0.7600468397140503
translation,132,167,results,en-af,has,our model,en-af has our model,0.6328913569450378
translation,132,167,results,results,For,en-af,results For en-af,0.6529962420463562
translation,132,168,results,statistically significantly outperforms,has,supervised laser model,statistically significantly outperforms has supervised laser model,0.6021112203598022
translation,132,175,results,supervised nmt model,trained on,laser extractions,supervised nmt model trained on laser extractions,0.7489028573036194
translation,132,175,results,laser extractions,outperform,current tss,laser extractions outperform current tss,0.6948000192642212
translation,132,175,results,current tss,with,gain in bleu,current tss with gain in bleu,0.702393651008606
translation,132,175,results,gain in bleu,of,+ 16.9 (,gain in bleu of + 16.9 (,0.5342403054237366
translation,132,175,results,en2 af only,has,our best configuration,en2 af only has our best configuration,0.613641083240509
translation,132,175,results,results,For,en2 af only,results For en2 af only,0.6461752653121948
translation,132,176,results,online sentence pair extraction,with,umt,online sentence pair extraction with umt,0.6537849307060242
translation,132,176,results,significantly outperforms,has,ssnmt baseline and unsupervised mt models,significantly outperforms has ssnmt baseline and unsupervised mt models,0.567622184753418
translation,132,177,results,supervised nmt systems,trained on,laser extractions,supervised nmt systems trained on laser extractions,0.7451193928718567
translation,132,177,results,outperform,has,supervised nmt systems,outperform has supervised nmt systems,0.5765441060066223
translation,133,49,experimental-setup,berttokenizer,with,bert - base-uncased,berttokenizer with bert - base-uncased,0.6897441744804382
translation,133,49,experimental-setup,berttokenizer,fine- tune,bert model,berttokenizer fine- tune bert model,0.6673012971878052
translation,133,49,experimental-setup,bert - base-uncased,as,our vocabulary file,bert - base-uncased as our vocabulary file,0.5841667652130127
translation,133,49,experimental-setup,bert model,using,one nvidia v100 gpu,bert model using one nvidia v100 gpu,0.6501073837280273
translation,133,49,experimental-setup,one nvidia v100 gpu,to classify,tweets,one nvidia v100 gpu to classify tweets,0.7159005999565125
translation,133,49,experimental-setup,tweets,into,positive or negative labels,tweets into positive or negative labels,0.5882132053375244
translation,133,49,experimental-setup,positive or negative labels,for,one epoch,positive or negative labels for one epoch,0.6115948557853699
translation,133,49,experimental-setup,one epoch,using,adam optimizer,one epoch using adam optimizer,0.6654866933822632
translation,133,49,experimental-setup,adam optimizer,with,weight decay,adam optimizer with weight decay,0.6082910299301147
translation,133,49,experimental-setup,linear learning rate schedule,with,warmup,linear learning rate schedule with warmup,0.661418080329895
translation,133,49,experimental-setup,experimental setup,fine- tune,bert model,experimental setup fine- tune bert model,0.6584931015968323
translation,133,50,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,133,50,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,133,50,experimental-setup,epsilon value,of,1e - 8,epsilon value of 1e - 8,0.6226520538330078
translation,133,50,experimental-setup,1e - 8,for,adam,1e - 8 for adam,0.6924672722816467
translation,133,67,experimental-setup,n = 10 best candidate translations,using,beam size,n = 10 best candidate translations using beam size,0.6169068813323975
translation,133,67,experimental-setup,beam size,of,10,beam size of 10,0.7020634412765503
translation,133,67,experimental-setup,10,at,decoding time,10 at decoding time,0.5544804930686951
translation,133,7,experiments,monolingual sentiment classifiers,in,english and spanish,monolingual sentiment classifiers in english and spanish,0.5191532969474792
translation,133,53,experiments,spanish sentiment classifier,fine- tune,xlm - roberta large,spanish sentiment classifier fine- tune xlm - roberta large,0.7152012586593628
translation,133,53,experiments,multilingual language model,shown to,significantly outperform,multilingual language model shown to significantly outperform,0.7416068911552429
translation,133,53,experiments,spanish sentiment classifier,has,multilingual language model,spanish sentiment classifier has multilingual language model,0.5223953127861023
translation,133,53,experiments,xlm - roberta large,has,multilingual language model,xlm - roberta large has multilingual language model,0.5477641820907593
translation,133,53,experiments,significantly outperform,has,multilingual bert ( mbert ),significantly outperform has multilingual bert ( mbert ),0.6257534623146057
translation,133,6,model,decoder-side approach,incorporates,automatic sentiment scoring,decoder-side approach incorporates automatic sentiment scoring,0.7119295597076416
translation,133,6,model,automatic sentiment scoring,into,mt candidate selection process,automatic sentiment scoring into mt candidate selection process,0.5607868432998657
translation,133,52,results,accuracy,of,85.2 %,accuracy of 85.2 %,0.5544253587722778
translation,133,52,results,85.2 %,on,english test set,85.2 % on english test set,0.501976728439331
translation,133,52,results,results,achieve,accuracy,results achieve accuracy,0.6312925815582275
translation,133,96,results,all ' subset,of,data,all ' subset of data,0.6554388999938965
translation,133,96,results,all ' subset,see,+ 0.12 gain,all ' subset see + 0.12 gain,0.6577878594398499
translation,133,96,results,+ 0.12 gain,for,our modified pipeline,+ 0.12 gain for our modified pipeline,0.6071177124977112
translation,133,96,results,our modified pipeline,over,baseline,our modified pipeline over baseline,0.7027510404586792
translation,133,96,results,+ 0.11 reduction,in,sentiment divergence,+ 0.11 reduction in sentiment divergence,0.4931607246398926
translation,133,96,results,results,In,all ' subset,results In all ' subset,0.56474769115448
translation,133,97,results,differences,see,+ 0.80 gain,differences see + 0.80 gain,0.6225914359092712
translation,133,97,results,more pronounced,see,+ 0.80 gain,more pronounced see + 0.80 gain,0.603693962097168
translation,133,97,results,more pronounced,see,+ 0.35 reduction,more pronounced see + 0.35 reduction,0.5877395272254944
translation,133,97,results,+ 0.80 gain,over,baseline,+ 0.80 gain over baseline,0.6644439697265625
translation,133,97,results,baseline,for,accuracy,baseline for accuracy,0.5950701832771301
translation,133,97,results,+ 0.35 reduction,in,sentiment divergence,+ 0.35 reduction in sentiment divergence,0.4885869324207306
translation,133,97,results,idiomatic subset,has,differences,idiomatic subset has differences,0.5539959669113159
translation,133,97,results,results,On,idiomatic subset,results On idiomatic subset,0.5774714350700378
translation,133,98,results,our pipeline,lags behind,google translate,our pipeline lags behind google translate,0.738816499710083
translation,133,98,results,google translate,in,all metrics,google translate in all metrics,0.49024325609207153
translation,133,98,results,google translate,for,english - spanish,google translate for english - spanish,0.6199142336845398
translation,133,98,results,all metrics,for,english - spanish,all metrics for english - spanish,0.5811597108840942
translation,133,98,results,results,has,our pipeline,results has our pipeline,0.5645846724510193
translation,133,113,results,our method,causes,decrease,our method causes decrease,0.6737093925476074
translation,133,113,results,decrease,in,bleu score,decrease in bleu score,0.5675590634346008
translation,133,113,results,bleu score,on,tatoeba test data,bleu score on tatoeba test data,0.520124614238739
translation,133,113,results,tatoeba test data,for,both languages,tatoeba test data for both languages,0.5881670117378235
translation,133,113,results,method,improves over,baseline,method improves over baseline,0.6499777436256409
translation,133,113,results,baseline,for,spanish tweets,baseline for spanish tweets,0.5991281270980835
translation,133,113,results,results,In terms,automatic mt evaluation,results In terms automatic mt evaluation,0.5622375011444092
translation,133,115,results,our method,in terms of,bleu score,our method in terms of bleu score,0.5863426327705383
translation,133,115,results,bleu score,on,tatoeba and the tweets,bleu score on tatoeba and the tweets,0.5510458946228027
translation,133,115,results,google translate,has,outperforms,google translate has outperforms,0.6274974942207336
translation,133,115,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,133,115,results,results,has,google translate,results has google translate,0.5633153915405273
translation,133,125,results,multilingual model,achieves,accuracy,multilingual model achieves accuracy,0.6646100282669067
translation,133,125,results,accuracy,of,83.8 %,accuracy of 83.8 %,0.5594848394393921
translation,133,125,results,83.8 %,comparable to,accuracy score,83.8 % comparable to accuracy score,0.6854809522628784
translation,133,125,results,accuracy score,achieved using,bert monolingual model,accuracy score achieved using bert monolingual model,0.6492788791656494
translation,133,125,results,english test data,has,multilingual model,english test data has multilingual model,0.566575825214386
translation,133,125,results,results,On,english test data,results On english test data,0.5180987119674683
translation,133,126,results,multilingual model,achieves,somewhat lower score,multilingual model achieves somewhat lower score,0.6536962985992432
translation,133,126,results,somewhat lower score,of,73.6 %,somewhat lower score of 73.6 %,0.479207307100296
translation,133,126,results,spanish test set,has,multilingual model,spanish test set has multilingual model,0.5602477788925171
translation,133,126,results,results,On,spanish test set,results On spanish test set,0.5522697567939758
translation,133,135,results,outperforms,in,accuracy and sentiment divergence,outperforms in accuracy and sentiment divergence,0.5419843196868896
translation,133,135,results,baseline,in,accuracy and sentiment divergence,baseline in accuracy and sentiment divergence,0.4888113737106323
translation,133,135,results,accuracy and sentiment divergence,on,every subset,accuracy and sentiment divergence on every subset,0.5668966174125671
translation,133,135,results,every subset,of,en-id data,every subset of en-id data,0.658859372138977
translation,133,135,results,comparable or better,than,google translate,comparable or better than google translate,0.631841242313385
translation,133,135,results,google translate,on,  all   and idiomatic subsets,google translate on   all   and idiomatic subsets,0.5669012069702148
translation,133,135,results,modified model,has,outperforms,modified model has outperforms,0.6382901072502136
translation,133,135,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,133,135,results,results,observe,modified model,results observe modified model,0.6044613122940063
translation,133,136,results,  all   subset,see,reductions,  all   subset see reductions,0.6030312776565552
translation,133,136,results,  all   subset,see,respective reductions,  all   subset see respective reductions,0.6105356812477112
translation,133,136,results,reductions,of,+ 0.33 and + 0.12,reductions of + 0.33 and + 0.12,0.6184279918670654
translation,133,136,results,reductions,of,+ 0.70 and + 0.36,reductions of + 0.70 and + 0.36,0.6075199246406555
translation,133,136,results,reductions,of,+ 0.70 and + 0.36,reductions of + 0.70 and + 0.36,0.6075199246406555
translation,133,136,results,+ 0.33 and + 0.12,over,baseline,+ 0.33 and + 0.12 over baseline,0.6552090048789978
translation,133,136,results,+ 0.33 and + 0.12,for,accuracy and sentiment divergence,+ 0.33 and + 0.12 for accuracy and sentiment divergence,0.6078227162361145
translation,133,136,results,baseline,for,accuracy and sentiment divergence,baseline for accuracy and sentiment divergence,0.5559439659118652
translation,133,136,results,idiomatic subset,see,respective reductions,idiomatic subset see respective reductions,0.5288866758346558
translation,133,136,results,respective reductions,of,+ 0.70 and + 0.36,respective reductions of + 0.70 and + 0.36,0.5987499952316284
translation,133,136,results,results,on,  all   subset,results on   all   subset,0.5966230034828186
translation,133,136,results,results,on,idiomatic subset,results on idiomatic subset,0.5774714350700378
translation,133,163,results,our method,improves over,baseline,our method improves over baseline,0.6977164149284363
translation,133,163,results,baseline,for,spanish tweets,baseline for spanish tweets,0.5991281270980835
translation,133,163,results,automatic mt evaluation,has,our method,automatic mt evaluation has our method,0.5167457461357117
translation,133,163,results,results,In terms of,automatic mt evaluation,results In terms of automatic mt evaluation,0.5779052972793579
translation,133,165,results,baseline and our pipeline,in terms of,bleu score,baseline and our pipeline in terms of bleu score,0.6186825037002563
translation,133,165,results,bleu score,on,tatoeba,bleu score on tatoeba,0.5179083347320557
translation,133,165,results,bleu score,on,tweets,bleu score on tweets,0.5141093730926514
translation,133,165,results,tatoeba,for,both languages,tatoeba for both languages,0.6213878989219666
translation,133,165,results,google translate,has,outperforms,google translate has outperforms,0.6274974942207336
translation,133,165,results,outperforms,has,baseline and our pipeline,outperforms has baseline and our pipeline,0.5767178535461426
translation,134,90,experimental-setup,experimental setup,trained on,two nvidia tesla p100 gpus,experimental setup trained on two nvidia tesla p100 gpus,0.691334068775177
translation,134,2,experiments,wmt21,has,news translation and biomedical translation,wmt21 has news translation and biomedical translation,0.6273329854011536
translation,134,4,experiments,dmath systems,used for,wmt 2021 news translation,dmath systems used for wmt 2021 news translation,0.6642724275588989
translation,134,4,experiments,dmath systems,used for,biomedical translation,dmath systems used for biomedical translation,0.722554087638855
translation,134,104,model,three lightweight systems,used,sub-subword features,three lightweight systems used sub-subword features,0.5646806359291077
translation,134,104,model,sub-subword features,to build,embeddings,sub-subword features to build embeddings,0.6176990270614624
translation,134,104,model,model,built and submitted,three lightweight systems,model built and submitted three lightweight systems,0.6825892329216003
translation,134,84,results,sub-subword features ( + sswf ),improve,results,sub-subword features ( + sswf ) improve results,0.6419093608856201
translation,134,84,results,results,of,corresponding - sswf models,results of corresponding - sswf models,0.5412136912345886
translation,134,84,results,corresponding - sswf models,under,low-resource settings,corresponding - sswf models under low-resource settings,0.690549910068512
translation,134,84,results,results,show,sub-subword features ( + sswf ),results show sub-subword features ( + sswf ),0.5963506102561951
translation,134,85,results,+ sswf system,did not achieve,better bleu scores,+ sswf system did not achieve better bleu scores,0.6215560436248779
translation,134,85,results,+ sswf system,achieved,better chrf2,+ sswf system achieved better chrf2,0.7168306708335876
translation,134,85,results,better bleu scores,than,corresponding - sswf system,better bleu scores than corresponding - sswf system,0.5448877215385437
translation,134,85,results,hausa ? english,has,+ sswf system,hausa ? english has + sswf system,0.6397296190261841
translation,134,85,results,results,In the case of,hausa ? english,results In the case of hausa ? english,0.6596838235855103
translation,134,88,results,the english ? basque biomedical abstract translation,did,not improve,the english ? basque biomedical abstract translation did not improve,0.6420808434486389
translation,134,88,results,not improve,when using,backtranslation data,not improve when using backtranslation data,0.7479270100593567
translation,134,88,results,results,has,the english ? basque biomedical abstract translation,results has the english ? basque biomedical abstract translation,0.5810322761535645
translation,135,213,ablation-analysis,one lookahead approach,important to improve,performance,one lookahead approach important to improve performance,0.6632298827171326
translation,135,213,ablation-analysis,ablation analysis,has,one lookahead approach,ablation analysis has one lookahead approach,0.5721919536590576
translation,135,6,model,prereordering,propose,couple of simple decision rules,prereordering propose couple of simple decision rules,0.6532962918281555
translation,135,6,model,model,propose,couple of simple decision rules,model propose couple of simple decision rules,0.6455002427101135
translation,135,37,model,words,of,source sentence,words of source sentence,0.5608823299407959
translation,135,37,model,sov language,in,full-sentence statistical machine translation,sov language in full-sentence statistical machine translation,0.48407092690467834
translation,135,37,model,model,Head Finalization reorders,words,model Head Finalization reorders words,0.7229739427566528
translation,135,180,results,outperformed,in,wide range of al,outperformed in wide range of al,0.6575323939323425
translation,135,180,results,baselines,in,wide range of al,baselines in wide range of al,0.5787604451179504
translation,135,180,results,our proposed method,has,outperformed,our proposed method has outperformed,0.6112541556358337
translation,135,180,results,outperformed,has,baselines,outperformed has baselines,0.6378934979438782
translation,135,180,results,results,has,our proposed method,results has our proposed method,0.5673112869262695
translation,135,186,results,results,has,fixed - size segmentation,results has fixed - size segmentation,0.5562122464179993
translation,135,208,results,lstm - based iclp,better in,precision,lstm - based iclp better in precision,0.6545277237892151
translation,135,208,results,results,has,lstm - based iclp,results has lstm - based iclp,0.5528244972229004
translation,136,123,ablation-analysis,additional features,are,useful,additional features are useful,0.5897279381752014
translation,136,123,ablation-analysis,additional features,depends on,language pair,additional features depends on language pair,0.7325554490089417
translation,136,123,ablation-analysis,some of them,are,useful,some of them are useful,0.5957565307617188
translation,136,123,ablation-analysis,some of them,depends on,language pair,some of them depends on language pair,0.7787175178527832
translation,136,123,ablation-analysis,additional features,has,some of them,additional features has some of them,0.5698873996734619
translation,136,22,baselines,two approaches,to take,additional features,two approaches to take additional features,0.6247832179069519
translation,136,22,baselines,additional features,into,account,additional features into account,0.6417204737663269
translation,136,22,baselines,additional features,at,token and hidden state levels,additional features at token and hidden state levels,0.5078235864639282
translation,136,70,experimental-setup,pre-trained xlm - roberta models,released by,"huggingface 's model repository ( wolf et al. , 2020 )","pre-trained xlm - roberta models released by huggingface 's model repository ( wolf et al. , 2020 )",0.6944591403007507
translation,136,70,experimental-setup,experimental setup,use,pre-trained xlm - roberta models,experimental setup use pre-trained xlm - roberta models,0.5941734910011292
translation,136,37,experiments,en- de dataset,is,least imbalanced,en- de dataset is least imbalanced,0.5611311197280884
translation,136,37,experiments,least imbalanced,compared to,other three language pairs,least imbalanced compared to other three language pairs,0.6704805493354797
translation,136,37,experiments,other three language pairs,where,proportion of err label,other three language pairs where proportion of err label,0.5811454057693481
translation,136,37,experiments,proportion of err label,in,en- de training set,proportion of err label in en- de training set,0.5422102212905884
translation,136,37,experiments,proportion of err label,reaches,27.9 %,proportion of err label reaches 27.9 %,0.6796252131462097
translation,136,5,model,cross-lingual pre-trained representations,in,sequence classification model,cross-lingual pre-trained representations in sequence classification model,0.5073261857032776
translation,136,6,model,base classifier,adding,weighted sampler,base classifier adding weighted sampler,0.7110379934310913
translation,136,6,model,base classifier,introducing,feature engineering,base classifier introducing feature engineering,0.7311545610427856
translation,136,6,model,weighted sampler,to deal with,imbalanced data,weighted sampler to deal with imbalanced data,0.6895966529846191
translation,136,6,model,feature engineering,where,features,feature engineering where features,0.630053699016571
translation,136,6,model,feature engineering,where,named -entities and sentiment,feature engineering where named -entities and sentiment,0.6007822751998901
translation,136,6,model,features,related to,toxicity,features related to toxicity,0.6619805097579956
translation,136,6,model,features,related to,named -entities and sentiment,features related to named -entities and sentiment,0.6919680237770081
translation,136,6,model,features,extracted using,existing tools,features extracted using existing tools,0.6864760518074036
translation,136,6,model,named -entities and sentiment,extracted using,existing tools,named -entities and sentiment extracted using existing tools,0.687162458896637
translation,136,6,model,model,improve,base classifier,model improve base classifier,0.7041918635368347
translation,136,7,model,models,with,one type of feature at a time,models with one type of feature at a time,0.6646304130554199
translation,136,7,model,models,with,ensemble,models with ensemble,0.6626445651054382
translation,136,7,model,improve,over,base classifier,improve over base classifier,0.685978353023529
translation,136,7,model,base classifier,on,development set,base classifier on development set,0.5682821273803711
translation,136,7,model,model,train,models,model train models,0.6628757119178772
translation,136,7,model,model,train,ensemble,model train ensemble,0.7246121764183044
translation,136,69,model,output,of,transformer encoder,output of transformer encoder,0.5951633453369141
translation,136,69,model,transformer encoder,fed into,classification head,transformer encoder fed into classification head,0.6946651935577393
translation,136,69,model,classification head,where,cross-entropy,classification head where cross-entropy,0.6232235431671143
translation,136,69,model,cross-entropy,adopted as,loss function,cross-entropy adopted as loss function,0.601344883441925
translation,136,69,model,model,has,output,model has output,0.5534584522247314
translation,136,150,model,baseline monotransquest architecture,by exploring,feature engineering,baseline monotransquest architecture by exploring feature engineering,0.6830736994743347
translation,136,150,model,baseline monotransquest architecture,by exploring,model ensembling,baseline monotransquest architecture by exploring model ensembling,0.6654601097106934
translation,136,150,model,weighted sampling,to deal with,imbalanced datasets,weighted sampling to deal with imbalanced datasets,0.6536788940429688
translation,136,150,model,model,extends,baseline monotransquest architecture,model extends baseline monotransquest architecture,0.678450882434845
translation,136,120,results,en- de,could achieve,highest mcc score,en- de could achieve highest mcc score,0.7120901346206665
translation,136,120,results,highest mcc score,among,four language pairs,highest mcc score among four language pairs,0.6007154583930969
translation,136,120,results,more balanced,compared to,other three language pairs,more balanced compared to other three language pairs,0.6672215461730957
translation,136,121,results,en - ja,has,lowest mcc score,en - ja has lowest mcc score,0.614326536655426
translation,136,121,results,results,has,en - ja,results has en - ja,0.6189123392105103
translation,136,122,results,weighted sampler,to deal with,imbalanced data,weighted sampler to deal with imbalanced data,0.6895966529846191
translation,136,122,results,weighted sampler,improves,models ' performance,weighted sampler improves models ' performance,0.6400441527366638
translation,136,122,results,models ' performance,in,most cases,models ' performance in most cases,0.5110005140304565
translation,136,122,results,results,adding,weighted sampler,results adding weighted sampler,0.6856573820114136
translation,136,130,results,test set,use,ensembling,test set use ensembling,0.6735889911651611
translation,136,130,results,ensembling,to produce,final results,ensembling to produce final results,0.7282657027244568
translation,136,130,results,results,on,test set,results on test set,0.582119882106781
translation,136,132,results,not all these models,lead to,improvements,not all these models lead to improvements,0.7571316957473755
translation,136,132,results,improvements,over,base ( no features ) model,improvements over base ( no features ) model,0.6681909561157227
translation,136,132,results,improvements,adding,some features,improvements adding some features,0.7570748329162598
translation,136,132,results,performance,for,some languages,performance for some languages,0.5835084319114685
translation,136,132,results,some features,has,decreases,some features has decreases,0.6090416312217712
translation,136,132,results,decreases,has,performance,decreases has performance,0.5981842875480652
translation,136,132,results,results,has,not all these models,results has not all these models,0.546786904335022
translation,136,134,results,all models,leads to,lower score,all models leads to lower score,0.6236947178840637
translation,136,134,results,lower score,than,ensembling,lower score than ensembling,0.6319162249565125
translation,136,134,results,ensembling,has,all models,ensembling has all models,0.5904148817062378
translation,136,134,results,ensembling,has,best few models,ensembling has best few models,0.5979762673377991
translation,136,134,results,results,found,ensembling,results found ensembling,0.6359959840774536
translation,136,135,results,ensembling models,with,bet-ter performance,ensembling models with bet-ter performance,0.6285504102706909
translation,136,135,results,bet-ter performance,than,base model,bet-ter performance than base model,0.5339329242706299
translation,136,135,results,bet-ter performance,improves,results,bet-ter performance improves results,0.6806584596633911
translation,136,135,results,results,of,all languages,results of all languages,0.5202380418777466
translation,136,135,results,all languages,except,en- de,all languages except en- de,0.6894197463989258
translation,136,152,results,multiple random seeds,show,our feature engineering and ensembling,multiple random seeds show our feature engineering and ensembling,0.6418664455413818
translation,136,152,results,our feature engineering and ensembling,lead to,large improvements,our feature engineering and ensembling lead to large improvements,0.6755903363227844
translation,136,152,results,large improvements,over,baseline,large improvements over baseline,0.7241522669792175
translation,136,152,results,results,averaged over,multiple random seeds,results averaged over multiple random seeds,0.699221134185791
translation,137,120,experimental-setup,each embedding and linear layer,generates,vector,each embedding and linear layer generates vector,0.6696045398712158
translation,137,120,experimental-setup,vector,of dimension,512,vector of dimension 512,0.6895625591278076
translation,137,120,experimental-setup,experimental setup,has,each embedding and linear layer,experimental setup has each embedding and linear layer,0.5485266447067261
translation,137,121,experimental-setup,our agent,using,"adam ( kingma and ba , 2014 ) optimizer","our agent using adam ( kingma and ba , 2014 ) optimizer",0.6504722237586975
translation,137,121,experimental-setup,experimental setup,train,our agent,experimental setup train our agent,0.6552521586418152
translation,137,122,experimental-setup,initial learning rate,set to,0.0008,initial learning rate set to 0.0008,0.6828367710113525
translation,137,122,experimental-setup,initial learning rate,use,fixed learning rate scheduler,initial learning rate use fixed learning rate scheduler,0.6636332273483276
translation,137,122,experimental-setup,fixed learning rate scheduler,with,shrink factor,fixed learning rate scheduler with shrink factor,0.6413509249687195
translation,137,122,experimental-setup,shrink factor,of,0.95,shrink factor of 0.95,0.5962815880775452
translation,137,122,experimental-setup,experimental setup,use,fixed learning rate scheduler,experimental setup use fixed learning rate scheduler,0.6151311993598938
translation,137,122,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,137,100,experiments,wmt15,choose,bpe merge operations,wmt15 choose bpe merge operations,0.7118405699729919
translation,137,100,experiments,bpe merge operations,achieve,joint bpe vocabulary,bpe merge operations achieve joint bpe vocabulary,0.6584175229072571
translation,137,100,experiments,joint bpe vocabulary,of size,32 k types,joint bpe vocabulary of size 32 k types,0.7399185299873352
translation,137,5,model,novel supervised learning approach,for training,agent,novel supervised learning approach for training agent,0.7116678953170776
translation,137,5,model,each target token,by comparing,simultaneous translations,each target token by comparing simultaneous translations,0.6922411322593689
translation,137,5,model,simultaneous translations,against,full-sentence translations,simultaneous translations against full-sentence translations,0.5955934524536133
translation,137,5,model,model,propose,novel supervised learning approach,model propose novel supervised learning approach,0.716925323009491
translation,137,6,model,oracle sequences,train,supervised model,oracle sequences train supervised model,0.7088200449943542
translation,137,6,model,supervised model,for,action generation,supervised model for action generation,0.5704570412635803
translation,137,6,model,action generation,at,inference time,action generation at inference time,0.5318573713302612
translation,137,6,model,model,has,oracle sequences,model has oracle sequences,0.6279503107070923
translation,137,7,model,alternative,to,current heuristic methods,alternative to current heuristic methods,0.4978440999984741
translation,137,7,model,current heuristic methods,in,simultaneous translation,current heuristic methods in simultaneous translation,0.5091426372528076
translation,137,19,model,oracle agent,using,fully trained neural machine translation model,oracle agent using fully trained neural machine translation model,0.6294938921928406
translation,137,19,model,model,create,oracle agent,model create oracle agent,0.6733355522155762
translation,137,116,model,implementation,to perform,simultaneous translation,implementation to perform simultaneous translation,0.6573260426521301
translation,137,116,model,our agent,trained to produce,read and write actions,our agent trained to produce read and write actions,0.7483190894126892
translation,137,116,model,read and write actions,with,base nmt system and decoder,read and write actions with base nmt system and decoder,0.6775302290916443
translation,137,116,model,model,augment,implementation,model augment implementation,0.6932713389396667
translation,137,116,model,model,incorporate,our agent,model incorporate our agent,0.7038763165473938
translation,137,123,results,our agent,trained to produce,optimal segments,our agent trained to produce optimal segments,0.7433748245239258
translation,137,123,results,optimal segments,obtains,average accuracy,optimal segments obtains average accuracy,0.612686812877655
translation,137,123,results,average accuracy,of,92.3 %,average accuracy of 92.3 %,0.5397956967353821
translation,137,123,results,average accuracy,of,83 %,average accuracy of 83 %,0.5956829786300659
translation,137,123,results,92.3 %,on,training data,92.3 % on training data,0.5249475836753845
translation,137,123,results,83 %,on,dev set,83 % on dev set,0.6026825308799744
translation,137,123,results,4 experiments,on,iwslt datasets,4 experiments on iwslt datasets,0.4533838927745819
translation,137,123,results,50 epochs,has,our agent,50 epochs has our agent,0.5686803460121155
translation,137,123,results,results,training for,50 epochs,results training for 50 epochs,0.7004858255386353
translation,137,144,results,eval- wait - 5 policy,in terms of,translation quality and latency,eval- wait - 5 policy in terms of translation quality and latency,0.6615860462188721
translation,137,144,results,translation quality and latency,on,de ? en language direction,translation quality and latency on de ? en language direction,0.5858868956565857
translation,137,144,results,offline and multi-path settings,has,our oracle policy,offline and multi-path settings has our oracle policy,0.5609491467475891
translation,137,144,results,our oracle policy,has,outperforms,our oracle policy has outperforms,0.6371139287948608
translation,137,144,results,outperforms,has,eval- wait - 5 policy,outperforms has eval- wait - 5 policy,0.6041082143783569
translation,137,144,results,results,On,offline and multi-path settings,results On offline and multi-path settings,0.5362257361412048
translation,137,146,results,performance,of,eval-wait - 5 policy,performance of eval-wait - 5 policy,0.5838091969490051
translation,137,146,results,eval-wait - 5 policy,replace,offline interpreter,eval-wait - 5 policy replace offline interpreter,0.6048685312271118
translation,137,146,results,improves,replace,offline interpreter,improves replace offline interpreter,0.6364819407463074
translation,137,146,results,offline interpreter,with,multi-path model,offline interpreter with multi-path model,0.6585898399353027
translation,137,146,results,eval-wait - 5 policy,has,improves,eval-wait - 5 policy has improves,0.619904637336731
translation,137,146,results,results,has,performance,results has performance,0.5972660779953003
translation,137,147,results,does not outperform,combined with,our oracle policy,does not outperform combined with our oracle policy,0.6324486136436462
translation,137,147,results,eval- wait - 5,combined with,our oracle policy,eval- wait - 5 combined with our oracle policy,0.6601784229278564
translation,137,147,results,multi-path model,has,does not outperform,multi-path model has does not outperform,0.616201639175415
translation,137,147,results,does not outperform,has,eval- wait - 5,does not outperform has eval- wait - 5,0.6527711153030396
translation,137,147,results,results,has,multi-path model,results has multi-path model,0.5372996926307678
translation,137,158,results,oracle policy,in,both language pairs,oracle policy in both language pairs,0.5186740159988403
translation,137,158,results,oracle policy,has,outperforms,oracle policy has outperforms,0.6206334829330444
translation,137,158,results,outperforms,has,oracle policy,outperforms has oracle policy,0.6072412133216858
translation,137,158,results,results,has,oracle policy,results has oracle policy,0.5526455044746399
translation,138,64,ablation-analysis,head - lasso left attention layers,focusing on removing,connections,head - lasso left attention layers focusing on removing connections,0.6449058055877686
translation,138,64,ablation-analysis,connections,from,feedforward layers,connections from feedforward layers,0.5217405557632446
translation,138,64,ablation-analysis,head - lasso left attention layers,has,almost completely unpruned,head - lasso left attention layers has almost completely unpruned,0.5430641770362854
translation,138,64,ablation-analysis,ablation analysis,has,head - lasso left attention layers,ablation analysis has head - lasso left attention layers,0.5063614249229431
translation,138,95,ablation-analysis,4 - bit models,find that,doubling,4 - bit models find that doubling,0.6943541765213013
translation,138,95,ablation-analysis,doubling,has,warm - up duration,doubling has warm - up duration,0.5713085532188416
translation,138,95,ablation-analysis,ablation analysis,for,4 - bit models,ablation analysis for 4 - bit models,0.6244020462036133
translation,138,5,baselines,simpler simple recurrent unit ( ssru ) decoder,with,one or two layers,simpler simple recurrent unit ( ssru ) decoder with one or two layers,0.6067818999290466
translation,138,5,baselines,simpler simple recurrent unit ( ssru ) decoder,with,lexical shortlists,simpler simple recurrent unit ( ssru ) decoder with lexical shortlists,0.6385570764541626
translation,138,5,baselines,simpler simple recurrent unit ( ssru ) decoder,with,smaller numerical formats,simpler simple recurrent unit ( ssru ) decoder with smaller numerical formats,0.622518002986908
translation,138,5,baselines,simpler simple recurrent unit ( ssru ) decoder,with,pruning,simpler simple recurrent unit ( ssru ) decoder with pruning,0.6443352103233337
translation,138,5,baselines,several efficiency strategies,has,simpler simple recurrent unit ( ssru ) decoder,several efficiency strategies has simpler simple recurrent unit ( ssru ) decoder,0.5532016754150391
translation,138,5,baselines,knowledge distillation,has,simpler simple recurrent unit ( ssru ) decoder,knowledge distillation has simpler simple recurrent unit ( ssru ) decoder,0.5841172933578491
translation,138,6,experimental-setup,cpu track,used,quantized 8 - bit models,cpu track used quantized 8 - bit models,0.6038479208946228
translation,138,6,experimental-setup,experimental setup,For,cpu track,experimental setup For cpu track,0.5859474539756775
translation,138,7,experimental-setup,gpu track,experimented with,fp16 and 8 - bit integers,gpu track experimented with fp16 and 8 - bit integers,0.6810649037361145
translation,138,7,experimental-setup,fp16 and 8 - bit integers,in,tensorcores,fp16 and 8 - bit integers in tensorcores,0.5507277846336365
translation,138,7,experimental-setup,experimental setup,For,gpu track,experimental setup For gpu track,0.5691519975662231
translation,138,28,experimental-setup,ubuntu 20.04 based images,for,our systems,ubuntu 20.04 based images for our systems,0.5520204901695251
translation,138,28,experimental-setup,ubuntu 20.04 based images,with,standard ubuntu,ubuntu 20.04 based images with standard ubuntu,0.5564451813697815
translation,138,28,experimental-setup,ubuntu 20.04 based images,with,nvidia 's ubuntubased cuda - 11.4 docker,ubuntu 20.04 based images with nvidia 's ubuntubased cuda - 11.4 docker,0.6127994060516357
translation,138,28,experimental-setup,standard ubuntu,for,cpu - only systems,standard ubuntu for cpu - only systems,0.5342553853988647
translation,138,28,experimental-setup,nvidia 's ubuntubased cuda - 11.4 docker,for,gpu - capable systems,nvidia 's ubuntubased cuda - 11.4 docker for gpu - capable systems,0.5757116079330444
translation,138,28,experimental-setup,experimental setup,used,ubuntu 20.04 based images,experimental setup used ubuntu 20.04 based images,0.5810844898223877
translation,138,28,experimental-setup,experimental setup,with,nvidia 's ubuntubased cuda - 11.4 docker,experimental setup with nvidia 's ubuntubased cuda - 11.4 docker,0.6067959070205688
translation,138,29,experimental-setup,docker images,created using,multistage builds,docker images created using multistage builds,0.7247186899185181
translation,138,29,experimental-setup,multistage builds,with,model disk size,multistage builds with model disk size,0.6072777509689331
translation,138,29,experimental-setup,model disk size,reduced by,compression,model disk size reduced by compression,0.6918579936027527
translation,138,29,experimental-setup,compression,with,xzip,compression with xzip,0.675852358341217
translation,138,29,experimental-setup,experimental setup,has,docker images,experimental setup has docker images,0.5329047441482544
translation,138,32,experimental-setup,transformerbig models,using,shared 32 k sentencepiece,transformerbig models using shared 32 k sentencepiece,0.656646192073822
translation,138,32,experimental-setup,"richardson , 2018 ) vocabulary",built in,three stages,"richardson , 2018 ) vocabulary built in three stages",0.6653771996498108
translation,138,32,experimental-setup,experimental setup,trained,transformerbig models,experimental setup trained transformerbig models,0.6525363922119141
translation,138,35,experimental-setup,synthesized parallel data,using,handcrafted rules,synthesized parallel data using handcrafted rules,0.6705350875854492
translation,138,35,experimental-setup,bottom 5 %,according to,cross-entropy per word,bottom 5 % according to cross-entropy per word,0.6249433755874634
translation,138,35,experimental-setup,cross-entropy per word,on,generated side,cross-entropy per word on generated side,0.5615999698638916
translation,138,35,experimental-setup,generated side,using,kenlm,generated side using kenlm,0.7481998205184937
translation,138,35,experimental-setup,experimental setup,filtered,synthesized parallel data,experimental setup filtered synthesized parallel data,0.7155723571777344
translation,138,61,experimental-setup,pruning,with,? = 0.5,pruning with ? = 0.5,0.6873593926429749
translation,138,61,experimental-setup,experimental setup,control,pruning,experimental setup control pruning,0.6957346200942993
translation,138,62,experimental-setup,models,pretrained for,50 k updates,models pretrained for 50 k updates,0.7901819348335266
translation,138,62,experimental-setup,models,regularised for,150k,models regularised for 150k,0.7937105298042297
translation,138,62,experimental-setup,models,after,sliced,models after sliced,0.7072232961654663
translation,138,62,experimental-setup,models,trained until,convergence,models trained until convergence,0.7589464783668518
translation,138,62,experimental-setup,150k,after,models,150k after models,0.7357648611068726
translation,138,62,experimental-setup,models,trained until,convergence,models trained until convergence,0.7589464783668518
translation,138,62,experimental-setup,experimental setup,regularised for,150k,experimental setup regularised for 150k,0.7174972891807556
translation,138,62,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,138,117,experimental-setup,process,bound to,9 cores,process bound to 9 cores,0.7889628410339355
translation,138,117,experimental-setup,9 cores,assigned,sequentially,9 cores assigned sequentially,0.6455270051956177
translation,138,117,experimental-setup,memory domain,corresponding to,socket,memory domain corresponding to socket,0.578837513923645
translation,138,117,experimental-setup,experimental setup,has,process,experimental setup has process,0.5221959948539734
translation,138,4,experiments,gpu hardware,with,throughput and latency conditions,gpu hardware with throughput and latency conditions,0.6613291501998901
translation,138,34,experiments,sequence - level knowledge distillation,to synthesize,"forward , backward , and backward - forward translations","sequence - level knowledge distillation to synthesize forward , backward , and backward - forward translations",0.6111056804656982
translation,138,34,experiments,"forward , backward , and backward - forward translations",using,teachers,"forward , backward , and backward - forward translations using teachers",0.6548546552658081
translation,138,57,model,feedforward and attention layers,in,encoder,feedforward and attention layers in encoder,0.531714916229248
translation,138,57,model,model,prune,feedforward and attention layers,model prune feedforward and attention layers,0.6497801542282104
translation,138,58,model,rowcol - lasso,regularised,individual connections ( rows and columns ),rowcol - lasso regularised individual connections ( rows and columns ),0.7576518058776855
translation,138,58,model,rowcol - lasso,removed,entire attention head,rowcol - lasso removed entire attention head,0.7382689118385315
translation,138,58,model,entire attention head,if,at least half of its connections are dead,entire attention head if at least half of its connections are dead,0.6200147867202759
translation,138,58,model,model,has,rowcol - lasso,model has rowcol - lasso,0.560528039932251
translation,138,70,results,our pruned models,are,1.2- 1.7 ? faster,our pruned models are 1.2- 1.7 ? faster,0.592879593372345
translation,138,70,results,1.2- 1.7 ? faster,cost of,0.6- 1.3 bleu,1.2- 1.7 ? faster cost of 0.6- 1.3 bleu,0.6643247604370117
translation,138,70,results,latest testset wmt21,has,our pruned models,latest testset wmt21 has our pruned models,0.6011045575141907
translation,138,70,results,results,Evaluating on,latest testset wmt21,results Evaluating on latest testset wmt21,0.6991573572158813
translation,138,94,results,learning rate,to,0.0001,learning rate to 0.0001,0.5543551445007324
translation,138,94,results,0.0001,yields,better model quality,0.0001 yields better model quality,0.6373838782310486
translation,138,94,results,results,lowering,learning rate,results lowering learning rate,0.6732544302940369
translation,138,96,results,8 - bit quantization models,aim for,speed improvement,8 - bit quantization models aim for speed improvement,0.5753011703491211
translation,138,96,results,results,has,8 - bit quantization models,results has 8 - bit quantization models,0.5360779166221619
translation,138,98,results,8 - bit inference,achieves,significant speedup,8 - bit inference achieves significant speedup,0.692884087562561
translation,139,175,ablation-analysis,monolingual tm,boost,nmt performance,monolingual tm boost nmt performance,0.6525298953056335
translation,139,175,ablation-analysis,end-to - end learning,of,retriever model,end-to - end learning of retriever model,0.5611857771873474
translation,139,175,ablation-analysis,retriever model,key for,substantial performance improvement,retriever model key for substantial performance improvement,0.6969459056854248
translation,139,175,ablation-analysis,ablation analysis,confirms,monolingual tm,ablation analysis confirms monolingual tm,0.6177029609680176
translation,139,175,ablation-analysis,ablation analysis,confirms,end-to - end learning,ablation analysis confirms end-to - end learning,0.648879885673523
translation,139,196,ablation-analysis,training pairs,are,very scarce,training pairs are very scarce,0.5834314227104187
translation,139,196,ablation-analysis,training pairs,has,small size of tm,training pairs has small size of tm,0.5500952005386353
translation,139,196,ablation-analysis,small size of tm,has,hurts,small size of tm has hurts,0.5701212882995605
translation,139,196,ablation-analysis,hurts,has,model performance,hurts has model performance,0.5826900005340576
translation,139,196,ablation-analysis,ablation analysis,when,training pairs,ablation analysis when training pairs,0.6271902918815613
translation,139,165,baselines,tm - augmented nmt,using,source similarity search,tm - augmented nmt using source similarity search,0.6922403573989868
translation,139,165,baselines,baselines,has,tm - augmented nmt,baselines has tm - augmented nmt,0.5605872273445129
translation,139,201,experiments,target-to-source transformer base model,using,bilingual pairs,target-to-source transformer base model using bilingual pairs,0.6422748565673828
translation,139,201,experiments,target-to-source transformer base model,use,resultant model,target-to-source transformer base model use resultant model,0.6133915185928345
translation,139,201,experiments,resultant model,to translate,monolingual sentences,resultant model to translate monolingual sentences,0.6494989395141602
translation,139,201,experiments,monolingual sentences,to obtain,additional synthetic parallel data,monolingual sentences to obtain additional synthetic parallel data,0.5431426763534546
translation,139,143,hyperparameters,our model,using,transformer blocks,our model using transformer blocks,0.704778790473938
translation,139,143,hyperparameters,hyperparameters,build,our model,hyperparameters build our model,0.6788299083709717
translation,139,144,hyperparameters,number of transformer blocks,is,3,number of transformer blocks is 3,0.596407413482666
translation,139,144,hyperparameters,number of transformer blocks,is,4,number of transformer blocks is 4,0.5763769745826721
translation,139,144,hyperparameters,number of transformer blocks,is,6,number of transformer blocks is 6,0.5795153379440308
translation,139,144,hyperparameters,3,for,retrieval model,3 for retrieval model,0.5733938813209534
translation,139,144,hyperparameters,4,for,memory encoder,4 for memory encoder,0.5618284940719604
translation,139,144,hyperparameters,memory encoder,in,translation model,memory encoder in translation model,0.5008237957954407
translation,139,144,hyperparameters,memory encoder,in,translation model,memory encoder in translation model,0.5008237957954407
translation,139,144,hyperparameters,6,for,encoder-decoder architecture,6 for encoder-decoder architecture,0.5752732753753662
translation,139,144,hyperparameters,encoder-decoder architecture,in,translation model,encoder-decoder architecture in translation model,0.49893927574157715
translation,139,144,hyperparameters,hyperparameters,has,number of transformer blocks,hyperparameters has number of transformer blocks,0.5303295850753784
translation,139,147,hyperparameters,learning rate schedule,has,dropout,learning rate schedule has dropout,0.5373743772506714
translation,139,147,hyperparameters,hyperparameters,follow,learning rate schedule,hyperparameters follow learning rate schedule,0.575567364692688
translation,139,5,model,new framework,uses,monolingual memory,new framework uses monolingual memory,0.548677384853363
translation,139,5,model,new framework,performs,learnable memory retrieval,new framework performs learnable memory retrieval,0.5585789680480957
translation,139,5,model,learnable memory retrieval,in,crosslingual manner,learnable memory retrieval in crosslingual manner,0.5029879808425903
translation,139,5,model,model,propose,new framework,model propose new framework,0.7318839430809021
translation,139,8,model,memory retriever and nmt model,jointly optimized for,ultimate translation goal,memory retriever and nmt model jointly optimized for ultimate translation goal,0.7420772314071655
translation,139,8,model,model,has,memory retriever and nmt model,model has memory retriever and nmt model,0.5546122193336487
translation,139,29,model,nmt models,with,monolingual tm,nmt models with monolingual tm,0.6357263326644897
translation,139,29,model,nmt models,with,learnable crosslingual memory retriever,nmt models with learnable crosslingual memory retriever,0.6073859930038452
translation,139,29,model,model,augment,nmt models,model augment nmt models,0.703606903553009
translation,139,30,model,corresponding targetside translations,in,latent vector space,corresponding targetside translations in latent vector space,0.5248824954032898
translation,139,30,model,latent vector space,using,simple dual-encoder framework,latent vector space using simple dual-encoder framework,0.6529890298843384
translation,139,30,model,score function,for,retrieval,score function for retrieval,0.6768076419830322
translation,139,30,model,model,align,source-side sentences,model align source-side sentences,0.6721262335777283
translation,139,32,model,memory retriever,selects,highest -scored memories,memory retriever selects highest -scored memories,0.6565239429473877
translation,139,32,model,highest -scored memories,from,large collection of monolingual sentences ( tm ),highest -scored memories from large collection of monolingual sentences ( tm ),0.5233298540115356
translation,139,32,model,each translation,has,memory retriever,each translation has memory retriever,0.5841935873031616
translation,139,32,model,model,Before running,each translation,model Before running each translation,0.7177265286445618
translation,139,33,model,memory retriever,with,differentiable neural networks,memory retriever with differentiable neural networks,0.5763518214225769
translation,139,33,model,model,design,memory retriever,model design memory retriever,0.6086124777793884
translation,139,39,model,retrieval model,using,two cross-alignment tasks,retrieval model using two cross-alignment tasks,0.6130928993225098
translation,139,39,model,warm-start,has,retrieval model,warm-start has retrieval model,0.57452791929245
translation,139,83,model,retrieval model,using,simple dual-encoder framework,retrieval model using simple dual-encoder framework,0.657192587852478
translation,139,83,model,model,implement,retrieval model,model implement retrieval model,0.6857341527938843
translation,139,40,results,our model,leads to,significant improvements,our model leads to significant improvements,0.6602242588996887
translation,139,40,results,significant improvements,over,non,significant improvements over non,0.7678322792053223
translation,139,40,results,significant improvements,over,-tm baseline nmt model,significant improvements over -tm baseline nmt model,0.7018398642539978
translation,139,40,results,non,has,-tm baseline nmt model,non has -tm baseline nmt model,0.5603824257850647
translation,139,40,results,outperforming,has,strong tm - augmented baselines,outperforming has strong tm - augmented baselines,0.5526521801948547
translation,139,42,results,translation quality,in,low-resource scenarios,translation quality in low-resource scenarios,0.5390908122062683
translation,139,42,results,translation quality,by utilizing,extra monolingual tm,translation quality by utilizing extra monolingual tm,0.6472146511077881
translation,139,42,results,extra monolingual tm,not present in,training pairs,extra monolingual tm not present in training pairs,0.6827993988990784
translation,139,42,results,substantially boost,has,translation quality,substantially boost has translation quality,0.545266330242157
translation,139,42,results,results,has,our model,results has our model,0.5871725678443909
translation,139,174,results,our full model,trained with,asynchronous index refresh,our full model trained with asynchronous index refresh,0.6747639179229736
translation,139,174,results,our full model,delivers,best performance,our full model delivers best performance,0.6413450241088867
translation,139,174,results,best performance,on,test sets,best performance on test sets,0.5485513806343079
translation,139,174,results,best performance,across,all four translation tasks,best performance across all four translation tasks,0.6249731779098511
translation,139,174,results,best performance,outperforming,non,best performance outperforming non,0.7536447048187256
translation,139,174,results,best performance,outperforming,-tm baseline,best performance outperforming -tm baseline,0.7466961741447449
translation,139,174,results,test sets,across,all four translation tasks,test sets across all four translation tasks,0.6178867816925049
translation,139,174,results,non,by,3.26 bleu points,non by 3.26 bleu points,0.5859435200691223
translation,139,174,results,asynchronous index refresh,has,model # 5 ),asynchronous index refresh has model # 5 ),0.6123625636100769
translation,139,174,results,non,has,-tm baseline,non has -tm baseline,0.5670727491378784
translation,139,174,results,-tm baseline,has,1 ),-tm baseline has 1 ),0.6336727142333984
translation,139,176,results,pre-trained fixed crosslingual retriever,gives,moderate test performance,pre-trained fixed crosslingual retriever gives moderate test performance,0.6076084971427917
translation,139,176,results,e src and fixing e tgt,boosts,performance,e src and fixing e tgt boosts performance,0.7562617063522339
translation,139,176,results,significantly,boosts,performance,significantly boosts performance,0.747631311416626
translation,139,176,results,fine - tuning,has,e src and fixing e tgt,fine - tuning has e src and fixing e tgt,0.6057420372962952
translation,139,176,results,e src and fixing e tgt,has,significantly,e src and fixing e tgt has significantly,0.6366453766822815
translation,139,176,results,results,using,pre-trained fixed crosslingual retriever,results using pre-trained fixed crosslingual retriever,0.6045690774917603
translation,139,183,results,our results,has,significantly outperform,our results has significantly outperform,0.573015570640564
translation,139,183,results,significantly outperform,has,previous arts,significantly outperform has previous arts,0.5550567507743835
translation,139,183,results,results,see that,our results,results see that our results,0.625874936580658
translation,139,184,results,best model,surpasses,"best reported model ( xia et al. , 2019 )","best model surpasses best reported model ( xia et al. , 2019 )",0.5979576706886292
translation,139,184,results,"best reported model ( xia et al. , 2019 )",by,1.69 bleu points,"best reported model ( xia et al. , 2019 ) by 1.69 bleu points",0.5205367207527161
translation,139,184,results,"best reported model ( xia et al. , 2019 )",up to,2.9 bleu points ( de?en ),"best reported model ( xia et al. , 2019 ) up to 2.9 bleu points ( de?en )",0.6412151455879211
translation,139,184,results,results,has,best model,results has best model,0.5634682774543762
translation,139,186,results,our translation model,using,traditional similarity search ( model # 2 ),our translation model using traditional similarity search ( model # 2 ),0.6564193367958069
translation,139,186,results,outperforms,has,best previously reported results,outperforms has best previously reported results,0.6046322584152222
translation,139,194,results,all available monolingual data ( 4/4 ),has,translation quality,all available monolingual data ( 4/4 ) has translation quality,0.5422083735466003
translation,139,194,results,results,using,all available monolingual data ( 4/4 ),results using all available monolingual data ( 4/4 ),0.6053492426872253
translation,139,195,results,performance,of,models without retraining,performance of models without retraining,0.5701755881309509
translation,139,195,results,results,has,performance,results has performance,0.5972660779953003
translation,139,202,results,our method,performs,better,our method performs better,0.6336022615432739
translation,139,202,results,our method,performs,worse,our method performs worse,0.6674834489822388
translation,139,202,results,our method,performs,worse,our method performs worse,0.6674834489822388
translation,139,202,results,better,than,bt,better than bt,0.6626626253128052
translation,139,202,results,bt,with,2/4 bilingual pairs,bt with 2/4 bilingual pairs,0.6853981018066406
translation,139,202,results,worse,with,1/4 bilingual pairs,worse with 1/4 bilingual pairs,0.6743485927581787
translation,139,202,results,results,has,our method,results has our method,0.5589964985847473
translation,139,209,results,tm - augmented model,obtains,higher bleu scores,tm - augmented model obtains higher bleu scores,0.5819083452224731
translation,139,209,results,tm - augmented model,obtains,slightly lower scores,tm - augmented model obtains slightly lower scores,0.6230958700180054
translation,139,209,results,higher bleu scores,in,domains,higher bleu scores in domains,0.5267487168312073
translation,139,209,results,domains,with,less data,domains with less data,0.6580134034156799
translation,139,209,results,slightly lower scores,in,other domains,slightly lower scores in other domains,0.4357064962387085
translation,139,209,results,bilingual data,has,tm - augmented model,bilingual data has tm - augmented model,0.5601553916931152
translation,139,209,results,results,when only using,bilingual data,results when only using bilingual data,0.5880817174911499
translation,139,210,results,tm,to,domain-specific tm,tm to domain-specific tm,0.5481863021850586
translation,139,210,results,significantly boosted,in,all domains,significantly boosted in all domains,0.5171666145324707
translation,139,210,results,significantly boosted,improving,non,significantly boosted improving non,0.7967649698257446
translation,139,210,results,significantly boosted,improving,-tm baseline,significantly boosted improving -tm baseline,0.7511293292045593
translation,139,210,results,-tm baseline,average of,1.85 bleu points,-tm baseline average of 1.85 bleu points,0.6677892208099365
translation,139,210,results,improvements,as large as,2.57 bleu points,improvements as large as 2.57 bleu points,0.4947516918182373
translation,139,210,results,improvements,as large as,2.51 bleu point,improvements as large as 2.51 bleu point,0.5011743903160095
translation,139,210,results,2.57 bleu points,on,law,2.57 bleu points on law,0.5376724004745483
translation,139,210,results,2.51 bleu point,on,medical,2.51 bleu point on medical,0.5627838969230652
translation,139,210,results,tm,has,translation quality,tm has translation quality,0.5340485572814941
translation,139,210,results,domain-specific tm,has,translation quality,domain-specific tm has translation quality,0.5205642580986023
translation,139,210,results,non,has,-tm baseline,non has -tm baseline,0.5670727491378784
translation,139,210,results,results,switch,tm,results switch tm,0.5093266367912292
translation,140,30,baselines,xlm15,simulating,lowresource scenario,xlm15 simulating lowresource scenario,0.7112002372741699
translation,140,30,baselines,baselines,based on,"multilingual pre-trained language models mbert ( devlin et al. , 2019 )","baselines based on multilingual pre-trained language models mbert ( devlin et al. , 2019 )",0.5510455369949341
translation,140,174,baselines,best auxiliary setups,are,aux-mlm,best auxiliary setups are aux-mlm,0.5659566521644592
translation,140,174,baselines,aux-mlm,followed by,aux - ud,aux-mlm followed by aux - ud,0.6548386216163635
translation,140,174,baselines,baselines,has,best auxiliary setups,baselines has best auxiliary setups,0.5256587862968445
translation,140,93,experimental-setup,"machamp v0.2 ( van der goot et al. , 2021 )",has,"allennlpbased ( gardner et al. , 2018 ) multi-task learning toolkit","machamp v0.2 ( van der goot et al. , 2021 ) has allennlpbased ( gardner et al. , 2018 ) multi-task learning toolkit",0.5158535242080688
translation,140,93,experimental-setup,experimental setup,implemented in,"machamp v0.2 ( van der goot et al. , 2021 )","experimental setup implemented in machamp v0.2 ( van der goot et al. , 2021 )",0.6776785850524902
translation,140,110,experimental-setup,sentences,encoded using,bytepair encoding ( bpe ),sentences encoded using bytepair encoding ( bpe ),0.7687714695930481
translation,140,110,experimental-setup,bytepair encoding ( bpe ),with,shared vocabulary,bytepair encoding ( bpe ) with shared vocabulary,0.6155250072479248
translation,140,110,experimental-setup,shared vocabulary,of,"32,000 tokens","shared vocabulary of 32,000 tokens",0.5790947079658508
translation,140,110,experimental-setup,experimental setup,has,sentences,experimental setup has sentences,0.5204469561576843
translation,140,203,experimental-setup,experiments,executed on,single v100 nvidia gpu,experiments executed on single v100 nvidia gpu,0.7069841623306274
translation,140,203,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,arabic ( ar ),new cross-lingual slu evaluation dataset covering arabic ( ar ),0.7651923894882202
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,chinese ( zh ),new cross-lingual slu evaluation dataset covering chinese ( zh ),0.7332170009613037
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,danish ( da ),new cross-lingual slu evaluation dataset covering danish ( da ),0.750190258026123
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,dutch ( nl ),new cross-lingual slu evaluation dataset covering dutch ( nl ),0.7374564409255981
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,english ( en ),new cross-lingual slu evaluation dataset covering english ( en ),0.7525264024734497
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,german ( de ),new cross-lingual slu evaluation dataset covering german ( de ),0.7323552966117859
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,indonesian ( id ),new cross-lingual slu evaluation dataset covering indonesian ( id ),0.7610960602760315
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,italian ( it ),new cross-lingual slu evaluation dataset covering italian ( it ),0.7303294539451599
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,japanese ( ja ),new cross-lingual slu evaluation dataset covering japanese ( ja ),0.7362867593765259
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,kazakh ( kk ),new cross-lingual slu evaluation dataset covering kazakh ( kk ),0.7587815523147583
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,serbian ( sr ),new cross-lingual slu evaluation dataset covering serbian ( sr ),0.7483928799629211
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,turkish ( tr ),new cross-lingual slu evaluation dataset covering turkish ( tr ),0.7519484162330627
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,austro-bavarian german dialect,new cross-lingual slu evaluation dataset covering austro-bavarian german dialect,0.7120038866996765
translation,140,26,model,new cross-lingual slu evaluation dataset,covering,south tyrolean ( de-st ),new cross-lingual slu evaluation dataset covering south tyrolean ( de-st ),0.7859144806861877
translation,140,26,model,xsid,has,new cross-lingual slu evaluation dataset,xsid has new cross-lingual slu evaluation dataset,0.5312960743904114
translation,140,26,model,model,provide,xsid,model provide xsid,0.6192222833633423
translation,140,129,model,linguistic knowledge,from,english to the target language,linguistic knowledge from english to the target language,0.5322270393371582
translation,140,129,model,linguistic knowledge,together with,target task,linguistic knowledge together with target task,0.6273085474967957
translation,140,129,model,linguistic knowledge,implement,nmt decoder,linguistic knowledge implement nmt decoder,0.6547458171844482
translation,140,129,model,english to the target language,together with,target task,english to the target language together with target task,0.6175562739372253
translation,140,129,model,nmt decoder,based on,shared encoder,nmt decoder based on shared encoder,0.6158915162086487
translation,140,129,model,model,To jointly learn to transfer,linguistic knowledge,model To jointly learn to transfer linguistic knowledge,0.7455284595489502
translation,140,130,model,sequenceto-sequence model,with,recurrent neural network decoder,sequenceto-sequence model with recurrent neural network decoder,0.6608254313468933
translation,140,130,model,recurrent neural network decoder,suits,auto-regressive nature,recurrent neural network decoder suits auto-regressive nature,0.6623191237449646
translation,140,130,model,auto-regressive nature,of,machine translation tasks,auto-regressive nature of machine translation tasks,0.5353002548217773
translation,140,130,model,attention mechanism,to avoid,compressing,attention mechanism to avoid compressing,0.6332595348358154
translation,140,130,model,whole source sentence,into,fixed - length vector,whole source sentence into fixed - length vector,0.5496091842651367
translation,140,130,model,compressing,has,whole source sentence,compressing has whole source sentence,0.5777503848075867
translation,140,130,model,model,use,sequenceto-sequence model,model use sequenceto-sequence model,0.6677314639091492
translation,140,19,results,evaluation data,for,13 languages,evaluation data for 13 languages,0.584884524345398
translation,140,19,results,13 languages,from,six language families,13 languages from six language families,0.5781181454658508
translation,140,19,results,results,has,xsid,results has xsid,0.5162312984466553
translation,140,165,results,auxiliary tasks,beneficial for,majority of the languages,auxiliary tasks beneficial for majority of the languages,0.6790693402290344
translation,140,165,results,best performing multi-task model ( aux-mlm ),achieves,+ 1.3,best performing multi-task model ( aux-mlm ) achieves + 1.3,0.6565094590187073
translation,140,165,results,best performing multi-task model ( aux-mlm ),achieves,+ 7.7,best performing multi-task model ( aux-mlm ) achieves + 7.7,0.6492186188697815
translation,140,165,results,best performing multi-task model ( aux-mlm ),achieves,improvements,best performing multi-task model ( aux-mlm ) achieves improvements,0.6757767796516418
translation,140,165,results,+ 1.3,for,mbert,+ 1.3 for mbert,0.6755747199058533
translation,140,165,results,+ 7.7,for,xlm15,+ 7.7 for xlm15,0.6555529832839966
translation,140,165,results,improvements,over,baseline,improvements over baseline,0.7402786016464233
translation,140,165,results,slot filling,has,auxiliary tasks,slot filling has auxiliary tasks,0.5919739007949829
translation,140,165,results,results,For,slot filling,results For slot filling,0.6317754983901978
translation,140,166,results,mbert and xlm15,are,significant performance drops,mbert and xlm15 are significant performance drops,0.6192604303359985
translation,140,166,results,significant performance drops,for,languages,significant performance drops for languages,0.6682958006858826
translation,140,166,results,results,comparing,mbert and xlm15,results comparing mbert and xlm15,0.6306746602058411
translation,140,168,results,other languages,involved in,pre-training,other languages involved in pre-training,0.6481836438179016
translation,140,168,results,aux -mlm and aux - ud,beat,baseline model,aux -mlm and aux - ud beat baseline model,0.6960368752479553
translation,140,168,results,results,For,other languages,results For other languages,0.593324601650238
translation,140,172,results,main findings,confirmed on,test data,main findings confirmed on test data,0.6701815724372864
translation,140,172,results,test data,evaluate,multiatis ++.,test data evaluate multiatis ++.,0.6981164813041687
translation,140,172,results,results,confirmed on,test data,results confirmed on test data,0.6875929236412048
translation,140,172,results,results,has,main findings,results has main findings,0.3973226845264435
translation,140,173,results,nmt-transfer model,perform,superior,nmt-transfer model perform superior,0.6602609157562256
translation,140,173,results,superior,on,intents,superior on intents,0.5564413070678711
translation,140,173,results,performance,on,slots,performance on slots,0.6104799509048462
translation,140,173,results,slots,is,worse,slots is worse,0.6247275471687317
translation,140,173,results,results,has,nmt-transfer model,results has nmt-transfer model,0.5188594460487366
translation,140,175,results,most significant gains,with,auxiliary tasks,most significant gains with auxiliary tasks,0.6079608201980591
translation,140,175,results,auxiliary tasks,obtained for,languages not included in pre-training,auxiliary tasks obtained for languages not included in pre-training,0.6408666968345642
translation,140,175,results,languages not included in pre-training,has,),languages not included in pre-training has ),0.5850763916969299
translation,140,175,results,results,has,most significant gains,results has most significant gains,0.538836658000946
translation,140,178,results,results,on,multiatis ++,results on multiatis ++,0.5334581732749939
translation,140,178,results,multiatis ++,are,lower,multiatis ++ are lower,0.6382431983947754
translation,140,178,results,lower,compared to,xu et al . ( 2020 ),lower compared to xu et al . ( 2020 ),0.661549985408783
translation,140,178,results,results,on,multiatis ++,results on multiatis ++,0.5334581732749939
translation,140,180,results,effect of language distance,plot,performance increase,effect of language distance plot performance increase,0.7110444903373718
translation,140,180,results,performance increase,over,baseline,performance increase over baseline,0.6863471865653992
translation,140,180,results,baseline,for,each auxiliary task,baseline for each auxiliary task,0.5938675403594971
translation,140,180,results,each auxiliary task,with respect to,language distance,each auxiliary task with respect to language distance,0.6984534859657288
translation,140,180,results,language distance,when using,mbert,language distance when using mbert,0.6590180397033691
translation,140,180,results,results,plot,performance increase,results plot performance increase,0.7850990295410156
translation,140,180,results,results,has,effect of language distance,results has effect of language distance,0.5348763465881348
translation,140,181,results,aux-mlm,is,most promising auxiliary model,aux-mlm is most promising auxiliary model,0.5685306191444397
translation,140,181,results,languages,with,large distance to english,languages with large distance to english,0.6135226488113403
translation,140,181,results,results,confirm,aux-mlm,results confirm aux-mlm,0.5759278535842896
translation,140,183,results,aux - ud,as well as,aux -mlm,aux - ud as well as aux -mlm,0.6776331663131714
translation,140,183,results,aux -mlm,are,beneficial,aux -mlm are beneficial,0.6713811159133911
translation,140,183,results,close languages,has,aux - ud,close languages has aux - ud,0.6456181406974792
translation,140,183,results,results,for,close languages,results for close languages,0.6350272297859192
translation,140,184,results,aux - ud model,performs,better,aux - ud model performs better,0.6365188956260681
translation,140,184,results,better,for,more distant languages,better for more distant languages,0.6351838111877441
translation,140,184,results,results,has,aux - ud model,results has aux - ud model,0.5518773198127747
translation,140,196,results,other large difference,for,aux - nmt,other large difference for aux - nmt,0.6576661467552185
translation,140,196,results,aux - nmt,with,xlm15,aux - nmt with xlm15,0.6697511076927185
translation,140,196,results,aux - nmt,makes,more errors,aux - nmt makes more errors,0.6969141960144043
translation,140,196,results,results,has,other large difference,results has other large difference,0.5658978223800659
translation,140,201,results,only datasets,from,languages,only datasets from languages,0.561814546585083
translation,140,201,results,languages,included in,"pre-trained language model ( i.e. , mbert )","languages included in pre-trained language model ( i.e. , mbert )",0.6376869678497314
translation,140,201,results,language distance and auxiliary task performance,are,competitive predictors,language distance and auxiliary task performance are competitive predictors,0.536848247051239
translation,140,201,results,new languages,considered ( XLM15 ),auxiliary task performance,new languages considered ( XLM15 ) auxiliary task performance,0.7442794442176819
translation,140,201,results,only datasets,has,auxiliary task performance,only datasets has auxiliary task performance,0.5372518301010132
translation,140,201,results,results,when using,only datasets,results when using only datasets,0.7202431559562683
translation,141,91,ablation-analysis,re - lm,when,translating out of en,re - lm when translating out of en,0.7432559728622437
translation,141,91,ablation-analysis,step,of,mlm,step of mlm,0.646341860294342
translation,141,91,ablation-analysis,mlm,with,pretrained embeddings,mlm with pretrained embeddings,0.5984920859336853
translation,141,91,ablation-analysis,ablation analysis,for,re - lm,ablation analysis for re - lm,0.6144252419471741
translation,141,111,ablation-analysis,lexical alignment,is,more beneficial,lexical alignment is more beneficial,0.5626545548439026
translation,141,111,ablation-analysis,more beneficial,for,en-mk,more beneficial for en-mk,0.6655815243721008
translation,141,111,ablation-analysis,ablation analysis,observe,lexical alignment,ablation analysis observe lexical alignment,0.5949280261993408
translation,141,31,experimental-setup,mapped embeddings,of,two languages,mapped embeddings of two languages,0.5852789282798767
translation,141,31,experimental-setup,mapped embeddings,to initialize,embedding layer,mapped embeddings to initialize embedding layer,0.6856055855751038
translation,141,31,experimental-setup,embedding layer,of,transformer - based encoder,embedding layer of transformer - based encoder,0.5750296711921692
translation,141,31,experimental-setup,experimental setup,use,mapped embeddings,experimental setup use mapped embeddings,0.6030057072639465
translation,141,33,experimental-setup,token embedding layer,of,mlm transformer encoder,token embedding layer of mlm transformer encoder,0.5476843118667603
translation,141,33,experimental-setup,mlm transformer encoder,with,pretrained vecmap embeddings,mlm transformer encoder with pretrained vecmap embeddings,0.5756610035896301
translation,141,33,experimental-setup,pretrained vecmap embeddings,provide,informative mapping,pretrained vecmap embeddings provide informative mapping,0.548748254776001
translation,141,33,experimental-setup,informative mapping,i.e.,cross-lingual lexical representations,informative mapping i.e. cross-lingual lexical representations,0.6175272464752197
translation,141,33,experimental-setup,experimental setup,initialize,token embedding layer,experimental setup initialize token embedding layer,0.7469886541366577
translation,141,60,experimental-setup,xlm,on,two languages of interest,xlm on two languages of interest,0.5253139734268188
translation,141,60,experimental-setup,two languages of interest,with,masked language modeling objective,two languages of interest with masked language modeling objective,0.5104295015335083
translation,141,60,experimental-setup,experimental setup,train,xlm,experimental setup train xlm,0.6708739995956421
translation,141,74,experimental-setup,monolingual data and validation / test sets,using,"moses ( koehn et al. , 2006 )","monolingual data and validation / test sets using moses ( koehn et al. , 2006 )",0.5801519751548767
translation,141,74,experimental-setup,experimental setup,tokenize,monolingual data and validation / test sets,experimental setup tokenize monolingual data and validation / test sets,0.6776127219200134
translation,141,75,experimental-setup,"xlm ( lample and conneau , 2019 )",use,bpe splitting,"xlm ( lample and conneau , 2019 ) use bpe splitting",0.6554771661758423
translation,141,75,experimental-setup,bpe splitting,with,32 k operations,bpe splitting with 32 k operations,0.6628048419952393
translation,141,75,experimental-setup,32 k operations,jointly learned on,both languages,32 k operations jointly learned on both languages,0.6895943880081177
translation,141,75,experimental-setup,experimental setup,For,"xlm ( lample and conneau , 2019 )","experimental setup For xlm ( lample and conneau , 2019 )",0.5909788608551025
translation,141,79,experimental-setup,transformer architecture,for,both the baselines and unmt models,transformer architecture for both the baselines and unmt models,0.6163138747215271
translation,141,79,experimental-setup,experimental setup,use,transformer architecture,experimental setup use transformer architecture,0.5702999830245972
translation,141,80,experimental-setup,encoder transformer,used for,masked language modeling,encoder transformer used for masked language modeling,0.6342094540596008
translation,141,80,experimental-setup,embedding and model size,is,1024,embedding and model size is 1024,0.5709574818611145
translation,141,80,experimental-setup,number of attention heads,is,8,number of attention heads is 8,0.5953323841094971
translation,141,80,experimental-setup,encoder transformer,has,embedding and model size,encoder transformer has embedding and model size,0.5523287653923035
translation,141,80,experimental-setup,encoder transformer,has,number of attention heads,encoder transformer has number of attention heads,0.541466236114502
translation,141,80,experimental-setup,masked language modeling,has,embedding and model size,masked language modeling has embedding and model size,0.5560168623924255
translation,141,80,experimental-setup,masked language modeling,has,number of attention heads,masked language modeling has number of attention heads,0.5317162275314331
translation,141,80,experimental-setup,experimental setup,For,encoder transformer,experimental setup For encoder transformer,0.5554301142692566
translation,141,82,experimental-setup,learning rate,set to,10 ?4,learning rate set to 10 ?4,0.7256936430931091
translation,141,82,experimental-setup,10 ?4,for,xlm and unmt,10 ?4 for xlm and unmt,0.7016069293022156
translation,141,82,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,141,83,experimental-setup,models,on,8 nvidia gtx 11 gb gpus,models on 8 nvidia gtx 11 gb gpus,0.5208441019058228
translation,141,83,experimental-setup,experimental setup,train,models,experimental setup train models,0.607352077960968
translation,141,85,experimental-setup,per-gpu batch size,is,32,per-gpu batch size is 32,0.5582557320594788
translation,141,85,experimental-setup,per-gpu batch size,is,26,per-gpu batch size is 26,0.5722641348838806
translation,141,85,experimental-setup,32,during,xlm,32 during xlm,0.5683676600456238
translation,141,85,experimental-setup,26,during,unmt,26 during unmt,0.6988252997398376
translation,141,85,experimental-setup,experimental setup,has,per-gpu batch size,experimental setup has per-gpu batch size,0.5276788473129272
translation,141,86,experimental-setup,experimental setup,built on,publicly available xlm and re - lm codebases,experimental setup built on publicly available xlm and re - lm codebases,0.6727101802825928
translation,141,87,experimental-setup,final translations,with,beam search,final translations with beam search,0.6618642807006836
translation,141,87,experimental-setup,beam search,has,of size 5,beam search has of size 5,0.636243999004364
translation,141,87,experimental-setup,experimental setup,generate,final translations,experimental setup generate final translations,0.6361768245697021
translation,141,28,experiments,subword monolingual embeddings,with,fasttext,subword monolingual embeddings with fasttext,0.5981263518333435
translation,141,28,experiments,fasttext,has,"bojanowski et al. , 2017 )","fasttext has bojanowski et al. , 2017 )",0.5786325335502625
translation,141,34,experiments,model,on,data,model on data,0.5742186307907104
translation,141,34,experiments,model,using,masked language modeling,model using masked language modeling,0.6794360280036926
translation,141,34,experiments,data,from,both languages,data from both languages,0.579334557056427
translation,141,34,experiments,data,using,masked language modeling,data using masked language modeling,0.67437744140625
translation,141,53,experiments,approach,to,re,approach to re,0.7901792526245117
translation,141,53,experiments,approach,to,"-lm ( chronopoulou et al. , 2020 )","approach to -lm ( chronopoulou et al. , 2020 )",0.579384446144104
translation,141,53,experiments,re,has,"-lm ( chronopoulou et al. , 2020 )","re has -lm ( chronopoulou et al. , 2020 )",0.5847124457359314
translation,141,7,model,bilingual masked language model pretraining,with,lexical - level information,bilingual masked language model pretraining with lexical - level information,0.605475127696991
translation,141,7,model,lexical - level information,by using,type-level cross-lingual subword embeddings,lexical - level information by using type-level cross-lingual subword embeddings,0.6010501980781555
translation,141,7,model,model,enhance,bilingual masked language model pretraining,model enhance bilingual masked language model pretraining,0.6425492167472839
translation,141,14,model,new method,to enhance,embedding alignment,new method to enhance embedding alignment,0.6809545755386353
translation,141,14,model,embedding alignment,of,bilingual language model,embedding alignment of bilingual language model,0.5011072754859924
translation,141,14,model,embedding alignment,entitled,lexically aligned mlm,embedding alignment entitled lexically aligned mlm,0.6413442492485046
translation,141,14,model,initialization,for,unmt,initialization for unmt,0.6098797917366028
translation,141,14,model,model,propose,new method,model propose new method,0.675626814365387
translation,141,15,model,type-level embeddings,separately for,two languages of interest,type-level embeddings separately for two languages of interest,0.6403893232345581
translation,141,15,model,model,learn,type-level embeddings,model learn type-level embeddings,0.6707781553268433
translation,141,16,model,monolingual embeddings,to,common space,monolingual embeddings to common space,0.5287925004959106
translation,141,16,model,monolingual embeddings,use them to initialize,embedding layer,monolingual embeddings use them to initialize embedding layer,0.6211308240890503
translation,141,16,model,embedding layer,of,mlm,embedding layer of mlm,0.5617559552192688
translation,141,16,model,model,map,monolingual embeddings,model map monolingual embeddings,0.6460438966751099
translation,141,29,model,monolingual embeddings,of,two languages,monolingual embeddings of two languages,0.5419799089431763
translation,141,29,model,two languages,to,shared space,two languages to shared space,0.5670856237411499
translation,141,29,model,"vecmap ( artetxe et al. , 2018a )",with,identical tokens,"vecmap ( artetxe et al. , 2018a ) with identical tokens",0.6010613441467285
translation,141,29,model,identical tokens,occurring in,both languages,identical tokens occurring in both languages,0.671160876750946
translation,141,29,model,model,map,monolingual embeddings,model map monolingual embeddings,0.6460438966751099
translation,141,39,model,model,for,nmt,model for nmt,0.643406867980957
translation,141,39,model,model,using,denoising auto-encoding,model using denoising auto-encoding,0.6725783348083496
translation,141,39,model,model,using,back - translation,model using back - translation,0.6984596252441406
translation,141,39,model,nmt,in,unsupervised way,nmt in unsupervised way,0.5207637548446655
translation,141,39,model,denoising auto-encoding,has,"vincent et al. , 2008 )","denoising auto-encoding has vincent et al. , 2008 )",0.5348656177520752
translation,141,39,model,model,train,model,model train model,0.6881781220436096
translation,141,39,model,model,train,nmt,model train nmt,0.7622680068016052
translation,141,81,model,nmt model,is,6 - layer encoder / decoder transformer,nmt model is 6 - layer encoder / decoder transformer,0.5449842214584351
translation,141,81,model,encoder transformer,has,6 layers,encoder transformer has 6 layers,0.5982091426849365
translation,141,81,model,model,has,encoder transformer,model has encoder transformer,0.5399833917617798
translation,141,126,model,lexical knowledge,of,pretrained cross-lingual embeddings,lexical knowledge of pretrained cross-lingual embeddings,0.48625117540359497
translation,141,20,results,outperform,has,unmt baseline,outperform has unmt baseline,0.5924097299575806
translation,141,20,results,results,has,outperform,results has outperform,0.642206609249115
translation,141,89,results,two pretraining approaches,effect of,cross-lingual lexical alignment,two pretraining approaches effect of cross-lingual lexical alignment,0.612959623336792
translation,141,89,results,cross-lingual lexical alignment,is,more evident,cross-lingual lexical alignment is more evident,0.5448768734931946
translation,141,89,results,more evident,for,en-mk,more evident for en-mk,0.743274450302124
translation,141,92,results,our method,provides,better alignment,our method provides better alignment,0.6065989136695862
translation,141,92,results,better alignment,of,lexical - level representations,better alignment of lexical - level representations,0.570311427116394
translation,141,92,results,better alignment,thanks to,transferred vecmap embeddings,better alignment thanks to transferred vecmap embeddings,0.457044392824173
translation,141,92,results,lexical - level representations,of,mlm,lexical - level representations of mlm,0.5894848704338074
translation,141,92,results,results,has,our method,results has our method,0.5589964985847473
translation,141,95,results,two pretraining models,for,unmt,two pretraining models for unmt,0.6479094624519348
translation,141,95,results,two pretraining models,providing for,highest bleu and chrf1 scores,two pretraining models providing for highest bleu and chrf1 scores,0.6006062626838684
translation,141,95,results,highest bleu and chrf1 scores,on,all translation directions,highest bleu and chrf1 scores on all translation directions,0.4918409287929535
translation,141,95,results,consistently outperforms,has,two pretraining models,consistently outperforms has two pretraining models,0.5767244696617126
translation,141,113,results,mlm,with,pretrained embeddings,mlm with pretrained embeddings,0.5984920859336853
translation,141,113,results,pretrained embeddings,has,largely improves,pretrained embeddings has largely improves,0.5507872104644775
translation,141,113,results,largely improves,has,performance,largely improves has performance,0.5754314661026001
translation,141,113,results,results,initializing,mlm,results initializing mlm,0.7099152207374573
translation,141,114,results,en-sq,effect of,our approach,en-sq effect of our approach,0.6267877221107483
translation,141,114,results,our approach,is,smaller yet consistent,our approach is smaller yet consistent,0.6174619197845459
translation,141,114,results,results,In,en-sq,results In en-sq,0.533678412437439
translation,141,116,results,lexical - level information,captured by,pretrained mlms,lexical - level information captured by pretrained mlms,0.693629264831543
translation,141,120,results,biggest performance gains,obtained when,proposed approach,biggest performance gains obtained when proposed approach,0.6603214740753174
translation,141,120,results,up to + 10.4 ),obtained when,proposed approach,up to + 10.4 ) obtained when proposed approach,0.6516023874282837
translation,141,120,results,proposed approach,applied to,xlm,proposed approach applied to xlm,0.7520365118980408
translation,141,120,results,biggest performance gains,has,up to + 10.4 ),biggest performance gains has up to + 10.4 ),0.5926625728607178
translation,141,120,results,results,has,biggest performance gains,results has biggest performance gains,0.5816678404808044
translation,141,130,results,finetuning approach,provides,higher performance,finetuning approach provides higher performance,0.6462520360946655
translation,141,130,results,higher performance,both in,en-mk and en-sq,higher performance both in en-mk and en-sq,0.7096022963523865
translation,141,130,results,results,has,finetuning approach,results has finetuning approach,0.5838472247123718
translation,142,103,experimental-setup,relu activation function,used in,all encoder and decoder layers,relu activation function used in all encoder and decoder layers,0.6126957535743713
translation,142,103,experimental-setup,experimental setup,has,relu activation function,experimental setup has relu activation function,0.518997073173523
translation,142,104,experimental-setup,"adam ( kingma and ba , 2015 )",set up with,maximum learning rate,"adam ( kingma and ba , 2015 ) set up with maximum learning rate",0.6911188364028931
translation,142,104,experimental-setup,"adam ( kingma and ba , 2015 )",set up with,inverse square root decay schedule,"adam ( kingma and ba , 2015 ) set up with inverse square root decay schedule",0.7336220145225525
translation,142,104,experimental-setup,"adam ( kingma and ba , 2015 )",set up with,4000 warmup updates,"adam ( kingma and ba , 2015 ) set up with 4000 warmup updates",0.7401185631752014
translation,142,104,experimental-setup,maximum learning rate,of,0.0005,maximum learning rate of 0.0005,0.584716260433197
translation,142,104,experimental-setup,experimental setup,optimize with,"adam ( kingma and ba , 2015 )","experimental setup optimize with adam ( kingma and ba , 2015 )",0.686191737651825
translation,142,106,experimental-setup,batch size,of,4096 tokens,batch size of 4096 tokens,0.5803205966949463
translation,142,106,experimental-setup,batch size,on,4 v100 gpus,batch size on 4 v100 gpus,0.5080792903900146
translation,142,106,experimental-setup,4096 tokens,on,4 v100 gpus,4096 tokens on 4 v100 gpus,0.507770836353302
translation,142,106,experimental-setup,4 v100 gpus,for,300k updates,4 v100 gpus for 300k updates,0.5504045486450195
translation,142,5,experiments,biomedical translation,developed,resource -heavy systems,biomedical translation developed resource -heavy systems,0.6269183158874512
translation,142,5,experiments,resource -heavy systems,for,english - french language pair,resource -heavy systems for english - french language pair,0.6128581762313843
translation,142,5,experiments,resource -heavy systems,using,out -ofdomain and in-domain corpora,resource -heavy systems using out -ofdomain and in-domain corpora,0.6398108005523682
translation,142,13,experiments,biomedical translation tasks,developed,resource -heavy systems,biomedical translation tasks developed resource -heavy systems,0.6126599311828613
translation,142,13,experiments,resource -heavy systems,for,english - french language pair,resource -heavy systems for english - french language pair,0.6128581762313843
translation,142,13,experiments,resource -heavy systems,using,diversity of out - of- domain and in-domain corpora,resource -heavy systems using diversity of out - of- domain and in-domain corpora,0.6335263848304749
translation,142,41,results,appreciable gains,both from,backtranslation,appreciable gains both from backtranslation,0.6878148317337036
translation,142,41,results,appreciable gains,both from,document structure processing,appreciable gains both from document structure processing,0.6599015593528748
translation,142,120,results,good improvement,in,both directions,good improvement in both directions,0.4891221821308136
translation,142,120,results,both directions,amounting to,4.2 and 4.8 bleu points,both directions amounting to 4.2 and 4.8 bleu points,0.6744944453239441
translation,142,120,results,4.2 and 4.8 bleu points,adding,around 1 m sentences,4.2 and 4.8 bleu points adding around 1 m sentences,0.6669543385505676
translation,142,120,results,around 1 m sentences,of,additional cochrane and taus corpora,around 1 m sentences of additional cochrane and taus corpora,0.5742104053497314
translation,142,120,results,around 1 m sentences,to,already available 3.4 m sentences,around 1 m sentences to already available 3.4 m sentences,0.5436555743217468
translation,142,120,results,results,see,good improvement,results see good improvement,0.5930106043815613
translation,142,145,results,baseline ( tf1 ),using,all available ( in domain + out-of- domain ) data,baseline ( tf1 ) using all available ( in domain + out-of- domain ) data,0.649790346622467
translation,142,145,results,all available ( in domain + out-of- domain ) data,tagged with,our 3 level scheme,all available ( in domain + out-of- domain ) data tagged with our 3 level scheme,0.6643960475921631
translation,142,145,results,all available ( in domain + out-of- domain ) data,yielded,bleu score,all available ( in domain + out-of- domain ) data yielded bleu score,0.6056690812110901
translation,142,145,results,bleu score,of,32.1,bleu score of 32.1,0.5467178225517273
translation,142,145,results,results,has,baseline ( tf1 ),results has baseline ( tf1 ),0.5441856384277344
translation,142,146,results,fine-tuning,with,all in- domain data ( tf2 ),fine-tuning with all in- domain data ( tf2 ),0.6697342395782471
translation,142,146,results,fine-tuning,gives,improvement,fine-tuning gives improvement,0.6033868789672852
translation,142,146,results,improvement,of,3.6 bleu points,improvement of 3.6 bleu points,0.5171341896057129
translation,142,146,results,3.6 bleu points,does not improve,further,3.6 bleu points does not improve further,0.6200954914093018
translation,142,146,results,further,when,fine-tuning,further when fine-tuning,0.6899899244308472
translation,142,146,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,143,74,ablation-analysis,ensembling,of,three best-scoring systems,ensembling of three best-scoring systems,0.5533442497253418
translation,143,74,ablation-analysis,ensembling,has,negligible effect,ensembling has negligible effect,0.6108826398849487
translation,143,74,ablation-analysis,three best-scoring systems,has,negligible effect,three best-scoring systems has negligible effect,0.548796534538269
translation,143,74,ablation-analysis,ablation analysis,has,ensembling,ablation analysis has ensembling,0.5626808404922485
translation,143,24,experimental-setup,transformer architecture,implemented in,"fairseq ( ott et al. , 2019 )","transformer architecture implemented in fairseq ( ott et al. , 2019 )",0.6838585138320923
translation,143,24,experimental-setup,transformer architecture,has,),transformer architecture has ),0.6620265245437622
translation,143,25,experimental-setup,data,segmented using,"bpe ( sennrich et al. , 2016 b )","data segmented using bpe ( sennrich et al. , 2016 b )",0.6778470873832703
translation,143,25,experimental-setup,"bpe ( sennrich et al. , 2016 b )",with,16 k merge operations,"bpe ( sennrich et al. , 2016 b ) with 16 k merge operations",0.6044855117797852
translation,143,25,experimental-setup,youtokentome,without,previous explicit tokenization,youtokentome without previous explicit tokenization,0.7065396308898926
translation,143,25,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,143,29,experimental-setup,most default hyperparameters,from,predefined architectures,most default hyperparameters from predefined architectures,0.47856947779655457
translation,143,29,experimental-setup,predefined architectures,in,fairseq,predefined architectures in fairseq,0.5479247570037842
translation,143,29,experimental-setup,experimental setup,keep,most default hyperparameters,experimental setup keep most default hyperparameters,0.5606384873390198
translation,143,30,experimental-setup,batch size,is,6 k tokens,batch size is 6 k tokens,0.5770304799079895
translation,143,30,experimental-setup,batch size,is,2 k tokens,batch size is 2 k tokens,0.5599954128265381
translation,143,30,experimental-setup,6 k tokens,for,base models,6 k tokens for base models,0.541462242603302
translation,143,30,experimental-setup,2 k tokens,for,big models,2 k tokens for big models,0.5866983532905579
translation,143,30,experimental-setup,big models,on,single gpu,big models on single gpu,0.5486680269241333
translation,143,30,experimental-setup,dropout,on,standard value,dropout on standard value,0.5231038928031921
translation,143,30,experimental-setup,standard value,of,0.1,standard value of 0.1,0.5552041530609131
translation,143,30,experimental-setup,experimental setup,keep,dropout,experimental setup keep dropout,0.5432248115539551
translation,143,30,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,143,31,experimental-setup,weights,of,supervised low-resource models,weights of supervised low-resource models,0.5652802586555481
translation,143,31,experimental-setup,supervised low-resource models,without restarting,optimizer,supervised low-resource models without restarting optimizer,0.7947070002555847
translation,143,88,hyperparameters,authentic and synthetic parallel training data,in,one-to- one ratio,authentic and synthetic parallel training data in one-to- one ratio,0.5350359082221985
translation,143,88,hyperparameters,hyperparameters,mix,authentic and synthetic parallel training data,hyperparameters mix authentic and synthetic parallel training data,0.6911622285842896
translation,143,14,results,our system,reaches,approximately the same translation quality,our system reaches approximately the same translation quality,0.6637982130050659
translation,143,15,results,upper sorbian systems,ranked,third out of six systems,upper sorbian systems ranked third out of six systems,0.7077531218528748
translation,143,15,results,third out of six systems,in,official ranking,third out of six systems in official ranking,0.5349611639976501
translation,143,15,results,results,has,upper sorbian systems,results has upper sorbian systems,0.5863348245620728
translation,143,17,results,lower sorbian systems,ranked,second ( de?dsb ),lower sorbian systems ranked second ( de?dsb ),0.7168108224868774
translation,143,17,results,lower sorbian systems,ranked,third ( dbs?de ),lower sorbian systems ranked third ( dbs?de ),0.7178349494934082
translation,143,17,results,third ( dbs?de ),out of,teams,third ( dbs?de ) out of teams,0.6782490015029907
translation,143,17,results,results,has,lower sorbian systems,results has lower sorbian systems,0.5852447152137756
translation,143,61,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,143,96,results,surprisingly small effect,on,translation quality,surprisingly small effect on translation quality,0.49826499819755554
translation,143,96,results,other language pairs,has,back - translation,other language pairs has back - translation,0.5976746082305908
translation,143,96,results,back - translation,has,surprisingly small effect,back - translation has surprisingly small effect,0.5823013186454773
translation,143,96,results,results,Compared to,other language pairs,results Compared to other language pairs,0.644063413143158
translation,143,99,results,character - level systems,are,2 bleu points,character - level systems are 2 bleu points,0.5660144090652466
translation,143,99,results,character - level systems,on average,2 bleu points,character - level systems on average 2 bleu points,0.6646783947944641
translation,143,99,results,worse,than,subword counterparts,worse than subword counterparts,0.6228634119033813
translation,143,99,results,2 bleu points,has,worse,2 bleu points has worse,0.5971184968948364
translation,143,99,results,results,has,character - level systems,results has character - level systems,0.554998517036438
translation,143,100,results,character - level models,seem to have,much larger gains,character - level models seem to have much larger gains,0.6066654920578003
translation,143,100,results,much larger gains,from,model ensembling,much larger gains from model ensembling,0.6060070395469666
translation,143,100,results,model ensembling,than,subword - based models,model ensembling than subword - based models,0.549617350101471
translation,143,100,results,results,has,character - level models,results has character - level models,0.46649184823036194
translation,143,101,results,character - level models,is,statistically indistinguishable,character - level models is statistically indistinguishable,0.541595995426178
translation,143,101,results,statistically indistinguishable,from,best subword - based models,statistically indistinguishable from best subword - based models,0.5879259705543518
translation,144,172,ablation-analysis,further reduced,with,npd,further reduced with npd,0.706533670425415
translation,144,172,ablation-analysis,glat,has,significantly reduces,glat has significantly reduces,0.5802083611488342
translation,144,172,ablation-analysis,significantly reduces,has,occurrence of repetition,significantly reduces has occurrence of repetition,0.6257783770561218
translation,144,172,ablation-analysis,ablation analysis,show,glat,ablation analysis show glat,0.651718020439148
translation,144,186,ablation-analysis,sampling strategy,first offers,relatively easy generation problems,sampling strategy first offers relatively easy generation problems,0.7765191793441772
translation,144,186,ablation-analysis,sampling strategy,turns,harder,sampling strategy turns harder,0.7231901288032532
translation,144,186,ablation-analysis,harder,benefits,final performance,harder benefits final performance,0.6405791640281677
translation,144,186,ablation-analysis,ablation analysis,has,sampling strategy,ablation analysis has sampling strategy,0.5053806304931641
translation,144,211,ablation-analysis,adaptive glanc-ing sampling approach,contributes,most,adaptive glanc-ing sampling approach contributes most,0.6614992618560791
translation,144,211,ablation-analysis,adaptive glanc-ing sampling approach,use of,representations,adaptive glanc-ing sampling approach use of representations,0.6898467540740967
translation,144,211,ablation-analysis,most,to,final improvement,most to final improvement,0.5713803172111511
translation,144,211,ablation-analysis,representations,from,encoder,representations from encoder,0.605746328830719
translation,144,211,ablation-analysis,representations,helps,bit,representations helps bit,0.7009439468383789
translation,144,71,baselines,glat,differs from,vanilla nat,glat differs from vanilla nat,0.6696358919143677
translation,144,71,baselines,vanilla nat,explicitly encourages,word interdependency,vanilla nat explicitly encourages word interdependency,0.7493472695350647
translation,144,71,baselines,word interdependency,via training with,glancing language model ( glm ),word interdependency via training with glancing language model ( glm ),0.7227467894554138
translation,144,71,baselines,baselines,has,glat,baselines has glat,0.5501830577850342
translation,144,113,baselines,two more complex methods,to better decide,output lengths,two more complex methods to better decide output lengths,0.7129367589950562
translation,144,124,experimental-setup,transformer,with,base setting,transformer with base setting,0.6592757701873779
translation,144,124,experimental-setup,teacher,for,knowledge distillation,teacher for knowledge distillation,0.6505269408226013
translation,144,124,experimental-setup,experimental setup,employ,transformer,experimental setup employ transformer,0.6101129055023193
translation,144,135,experimental-setup,iwslt16,use,5 layers,iwslt16 use 5 layers,0.5813120007514954
translation,144,135,experimental-setup,iwslt16,set,model size d model,iwslt16 set model size d model,0.6607871055603027
translation,144,135,experimental-setup,5 layers,for,encoder and decoder,5 layers for encoder and decoder,0.59730464220047
translation,144,135,experimental-setup,model size d model,to,256,model size d model to 256,0.5961064100265503
translation,144,135,experimental-setup,experimental setup,For,iwslt16,experimental setup For iwslt16,0.5908893942832947
translation,144,136,experimental-setup,nvidia v100 gpus,train,model,nvidia v100 gpus train model,0.7329397201538086
translation,144,136,experimental-setup,model,with,batches,model with batches,0.6916288733482361
translation,144,136,experimental-setup,batches,of,64k/8 k tokens,batches of 64k/8 k tokens,0.6476152539253235
translation,144,136,experimental-setup,64k/8 k tokens,for,wmt / iwslt,64k/8 k tokens for wmt / iwslt,0.6671422719955444
translation,144,136,experimental-setup,experimental setup,Using,nvidia v100 gpus,experimental setup Using nvidia v100 gpus,0.6036123633384705
translation,144,137,experimental-setup,dropout rate,to,0.1,dropout rate to 0.1,0.5268309712409973
translation,144,137,experimental-setup,adam optimizer,with,"? = ( 0.9 , 0.999 )","adam optimizer with ? = ( 0.9 , 0.999 )",0.6042236089706421
translation,144,137,experimental-setup,experimental setup,set,dropout rate,experimental setup set dropout rate,0.62116938829422
translation,144,137,experimental-setup,experimental setup,set,adam optimizer,experimental setup set adam optimizer,0.629711925983429
translation,144,137,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,144,138,experimental-setup,learning rate,warms up to,5e ? 4,learning rate warms up to 5e ? 4,0.7247487306594849
translation,144,138,experimental-setup,5e ? 4,in,4 k steps,5e ? 4 in 4 k steps,0.5983043909072876
translation,144,138,experimental-setup,gradually decays,according to,inverse square root schedule,gradually decays according to inverse square root schedule,0.7425791025161743
translation,144,138,experimental-setup,wmt datasets,has,learning rate,wmt datasets has learning rate,0.5042259097099304
translation,144,138,experimental-setup,experimental setup,For,wmt datasets,experimental setup For wmt datasets,0.5112258195877075
translation,144,139,experimental-setup,iwslt16 de - en,adopt,linear annealing,iwslt16 de - en adopt linear annealing,0.6598542332649231
translation,144,139,experimental-setup,linear annealing,has,from 3e ? 4 to 1e ? 5 ),linear annealing has from 3e ? 4 to 1e ? 5 ),0.6037508249282837
translation,144,140,experimental-setup,linear annealing,from,0.5 to 0.3,linear annealing from 0.5 to 0.3,0.5516790747642517
translation,144,140,experimental-setup,0.5 to 0.3,for,wmt datasets,0.5 to 0.3 for wmt datasets,0.6374996304512024
translation,144,140,experimental-setup,fixed value,of,0.5,fixed value of 0.5,0.6232510805130005
translation,144,140,experimental-setup,0.5,for,iwslt16,0.5 for iwslt16,0.6491934657096863
translation,144,143,experiments,average latency per sentence,on,single nvidia 1080ti gpu,average latency per sentence on single nvidia 1080ti gpu,0.5269522666931152
translation,144,6,model,glancing language model ( glm ),for,single - pass parallel generation models,glancing language model ( glm ) for single - pass parallel generation models,0.6086083650588989
translation,144,6,model,model,propose,glancing language model ( glm ),model propose glancing language model ( glm ),0.6580740809440613
translation,144,7,model,glm,develop,glancing transformer ( glat ),glm develop glancing transformer ( glat ),0.657453715801239
translation,144,7,model,glancing transformer ( glat ),for,machine translation,glancing transformer ( glat ) for machine translation,0.601109504699707
translation,144,7,model,model,With,glm,model With glm,0.6556454300880432
translation,144,27,model,new method,to train,probabilistic sequence model,new method to train probabilistic sequence model,0.6740046143531799
translation,144,27,model,glancing language model ( glm ),has,new method,glancing language model ( glm ) has new method,0.58878093957901
translation,144,27,model,model,propose,glancing language model ( glm ),model propose glancing language model ( glm ),0.6580740809440613
translation,144,28,model,glm,develop,glancing transformer ( glat ),glm develop glancing transformer ( glat ),0.657453715801239
translation,144,28,model,glancing transformer ( glat ),for,neural machine translation,glancing transformer ( glat ) for neural machine translation,0.6035352945327759
translation,144,28,model,model,Based on,glm,model Based on glm,0.6795371770858765
translation,144,28,model,model,develop,glancing transformer ( glat ),model develop glancing transformer ( glat ),0.6449459195137024
translation,144,35,model,glm,proposes,adaptive glancing sampling strategy,glm proposes adaptive glancing sampling strategy,0.6853971481323242
translation,144,35,model,adaptive glancing sampling strategy,enables,glat,adaptive glancing sampling strategy enables glat,0.6994127631187439
translation,144,35,model,glat,to generate,sentences,glat to generate sentences,0.7326636910438538
translation,144,35,model,sentences,in,one-iteration way,sentences in one-iteration way,0.5994821190834045
translation,144,35,model,sentences,working by,gradual training,sentences working by gradual training,0.7156240344047546
translation,144,35,model,gradual training,instead of,iterative inference,gradual training instead of iterative inference,0.6335581541061401
translation,144,35,model,model,has,glm,model has glm,0.5774986147880554
translation,144,146,model,explicit word interdependency modeling,for,decoder,explicit word interdependency modeling for decoder,0.6343324780464172
translation,144,146,model,simultaneous generation,of,whole sequences,simultaneous generation of whole sequences,0.6275110244750977
translation,144,146,model,model,gradually learns,simultaneous generation,model gradually learns simultaneous generation,0.7454010844230652
translation,144,177,model,inference algorithm,in,mask - predict,inference algorithm in mask - predict,0.583174467086792
translation,144,177,model,mask - predict,for,multiple -iteration decoding,mask - predict for multiple -iteration decoding,0.6333056092262268
translation,144,177,model,model,adopt,inference algorithm,model adopt inference algorithm,0.7069262862205505
translation,144,8,results,glat,able to generate,high-quality translation,glat able to generate high-quality translation,0.7043169736862183
translation,144,8,results,high-quality translation,with,8 ? - 15 ? speedup,high-quality translation with 8 ? - 15 ? speedup,0.6430176496505737
translation,144,8,results,single - pass parallel decoding,has,glat,single - pass parallel decoding has glat,0.5877127051353455
translation,144,8,results,results,With,single - pass parallel decoding,results With single - pass parallel decoding,0.5549707412719727
translation,144,43,results,glat,obtains,significant improvements,glat obtains significant improvements,0.6163613796234131
translation,144,43,results,significant improvements,on,standard benchmarks,significant improvements on standard benchmarks,0.4973541498184204
translation,144,43,results,standard benchmarks,compared to,vanilla nat,standard benchmarks compared to vanilla nat,0.603325366973877
translation,144,43,results,significant improvements,has,about 5 bleu,significant improvements has about 5 bleu,0.5685545802116394
translation,144,43,results,results,show,glat,results show glat,0.6396870017051697
translation,144,147,results,our method,completely maintains,inference efficiency advantage,our method completely maintains inference efficiency advantage,0.7256692051887512
translation,144,147,results,inference efficiency advantage,of,fully non-autoregressive models,inference efficiency advantage of fully non-autoregressive models,0.5808774828910828
translation,144,147,results,iterative decoding,has,our method,iterative decoding has our method,0.5895796418190002
translation,144,148,results,glat,is,highly effective,glat is highly effective,0.5802838206291199
translation,144,148,results,baselines,has,glat,baselines has glat,0.5501830577850342
translation,144,149,results,glat,obtains,significant improvements ( about 5 bleu ),glat obtains significant improvements ( about 5 bleu ),0.5868512988090515
translation,144,149,results,significant improvements ( about 5 bleu ),on,en -de / de-en,significant improvements ( about 5 bleu ) on en -de / de-en,0.5920308828353882
translation,144,149,results,vanilla nat - base models,has,glat,vanilla nat - base models has glat,0.5966768860816956
translation,144,149,results,results,Compared with,vanilla nat - base models,results Compared with vanilla nat - base models,0.6730291843414307
translation,144,150,results,other fully non-autoregressive models,with,substantial margin,other fully non-autoregressive models with substantial margin,0.6346036195755005
translation,144,150,results,outperforms,has,other fully non-autoregressive models,outperforms has other fully non-autoregressive models,0.5883957743644714
translation,144,150,results,substantial margin,has,almost + 2 bleu points on average,substantial margin has almost + 2 bleu points on average,0.578305721282959
translation,144,151,results,results,are,even very close,results are even very close,0.6033062934875488
translation,144,151,results,even very close,to,at model,even very close to at model,0.6357402205467224
translation,144,151,results,even very close,shows,great potential,even very close shows great potential,0.6807345747947693
translation,144,151,results,at model,shows,great potential,at model shows great potential,0.6986120939254761
translation,144,151,results,results,are,even very close,results are even very close,0.6033062934875488
translation,144,151,results,results,has,results,results has results,0.48582205176353455
translation,144,168,results,glat,outperforms,autoregressive transformer,glat outperforms autoregressive transformer,0.658065140247345
translation,144,168,results,autoregressive transformer,when,source input length,autoregressive transformer when source input length,0.6644124388694763
translation,144,168,results,source input length,is,smaller than 20,source input length is smaller than 20,0.5886474251747131
translation,144,168,results,results,find that,glat,results find that glat,0.6725408434867859
translation,144,175,results,glat,Achieves,strong results,glat Achieves strong results,0.6765879988670349
translation,144,175,results,strong results,without,multiple iterations,strong results without multiple iterations,0.7330520749092102
translation,144,175,results,results,has,glat,results has glat,0.5337822437286377
translation,144,179,results,glat,achieve,decent performances,glat achieve decent performances,0.710528552532196
translation,144,179,results,decent performances,with,only one decoding iteration,decent performances with only one decoding iteration,0.6275415420532227
translation,144,179,results,further iterations,obtain,minor improvements,further iterations obtain minor improvements,0.5521677136421204
translation,144,179,results,minor improvements,of,0.2?0.3 bleu,minor improvements of 0.2?0.3 bleu,0.5501347184181213
translation,144,179,results,results,find that,glat,results find that glat,0.6725408434867859
translation,144,184,results,outperforms,with,big margins,outperforms with big margins,0.6919472217559814
translation,144,184,results,baseline models,with,big margins,baseline models with big margins,0.6263797879219055
translation,144,184,results,our adaptive approach,has,outperforms,our adaptive approach has outperforms,0.633310854434967
translation,144,184,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,144,185,results,sampling schedule,affects,generation performance,sampling schedule affects generation performance,0.7450557947158813
translation,144,185,results,generation performance,of,our nat model,generation performance of our nat model,0.5606790781021118
translation,144,187,results,glat,achieves,remarkable results,glat achieves remarkable results,0.676108717918396
translation,144,187,results,simplest constant ratio,has,glat,simplest constant ratio has glat,0.5614482164382935
translation,144,188,results,baseline ? = 0.0,by,2.5 bleu points,baseline ? = 0.0 by 2.5 bleu points,0.5681491494178772
translation,144,188,results,outperforms,has,baseline ? = 0.0,outperforms has baseline ? = 0.0,0.5893813967704773
translation,144,190,results,works better,than,constant one,works better than constant one,0.6409794092178345
translation,144,190,results,our proposed adaptive approaches,achieve,best results,our proposed adaptive approaches achieve best results,0.6538918018341064
translation,144,190,results,flexible decreasing ratio method,has,works better,flexible decreasing ratio method has works better,0.5957647562026978
translation,144,190,results,results,has,flexible decreasing ratio method,results has flexible decreasing ratio method,0.5365368127822876
translation,144,205,results,glat,with,hamming distance,glat with hamming distance,0.6403255462646484
translation,144,205,results,glat,with,levenshtein distance,glat with levenshtein distance,0.6313621997833252
translation,144,205,results,glat,with,levenshtein distance,glat with levenshtein distance,0.6313621997833252
translation,144,205,results,hamming distance,outperforms,glat,hamming distance outperforms glat,0.7502361536026001
translation,144,205,results,glat,with,levenshtein distance,glat with levenshtein distance,0.6313621997833252
translation,144,205,results,levenshtein distance,by about,0.7 bleu,levenshtein distance by about 0.7 bleu,0.5937637090682983
translation,144,205,results,levenshtein distance,by about,0.9 bleu,levenshtein distance by about 0.9 bleu,0.5898588299751282
translation,144,205,results,0.9 bleu,on,wmt14 en - de and de -en,0.9 bleu on wmt14 en - de and de -en,0.600602388381958
translation,144,205,results,target length reranking,has,glat,target length reranking has glat,0.5735039114952087
translation,144,210,results,glancing sampling strategy,outperforms,uniform sampling strategy,glancing sampling strategy outperforms uniform sampling strategy,0.7183841466903687
translation,144,210,results,glancing sampling strategy,feeding,representations,glancing sampling strategy feeding representations,0.7569329142570496
translation,144,210,results,uniform sampling strategy,by,5?6 bleu points,uniform sampling strategy by 5?6 bleu points,0.5847545862197876
translation,144,210,results,representations,from,encoder,representations from encoder,0.605746328830719
translation,144,210,results,representations,improve,strong baseline,representations improve strong baseline,0.6550709009170532
translation,144,210,results,encoder,as,decoder input,encoder as decoder input,0.573307454586029
translation,144,210,results,strong baseline,by,0.2?0.3 bleu points,strong baseline by 0.2?0.3 bleu points,0.531026303768158
translation,144,210,results,0.2?0.3 bleu points,after adopting,glancing sampling,0.2?0.3 bleu points after adopting glancing sampling,0.7145392298698425
translation,144,210,results,results,show,glancing sampling strategy,results show glancing sampling strategy,0.5504553318023682
translation,144,210,results,results,feeding,representations,results feeding representations,0.6638321280479431
translation,146,64,baselines,medline corpus,for,domain adaptation,medline corpus for domain adaptation,0.5624547600746155
translation,146,64,baselines,significant increase,in,bleu score,significant increase in bleu score,0.5593109726905823
translation,146,64,baselines,bleu score,of,+ 3.56,bleu score of + 3.56,0.5346491932868958
translation,146,64,baselines,+ 3.56,for,fr ?en,+ 3.56 for fr ?en,0.6769484281539917
translation,146,18,experiments,effect,on,en translation,effect on en translation,0.6103248000144958
translation,146,18,experiments,en translation,using,multilingual,en translation using multilingual,0.6453122496604919
translation,146,18,experiments,in biomedical domain,using,multilingual,in biomedical domain using multilingual,0.7171338200569153
translation,146,18,experiments,en translation,has,in biomedical domain,en translation has in biomedical domain,0.6014467477798462
translation,146,52,hyperparameters,batch size,of,4 k words,batch size of 4 k words,0.6039134860038757
translation,146,52,hyperparameters,adam optimizer,used in,all experiments,adam optimizer used in all experiments,0.6175627708435059
translation,146,52,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,146,65,results,increase,of,+ 0.26 bleu,increase of + 0.26 bleu,0.5923150777816772
translation,146,65,results,increase,of,+ 0.28 bleu,increase of + 0.28 bleu,0.5924684405326843
translation,146,65,results,+ 0.26 bleu,for,de?en,+ 0.26 bleu for de?en,0.6762279272079468
translation,146,65,results,+ 0.26 bleu,for,es ?en,+ 0.26 bleu for es ?en,0.6827571988105774
translation,146,65,results,+ 0.26 bleu,for,es ?en,+ 0.26 bleu for es ?en,0.6827571988105774
translation,146,65,results,+ 0.28 bleu,for,es ?en,+ 0.28 bleu for es ?en,0.6815862059593201
translation,146,65,results,ir corpus,for,fr,ir corpus for fr,0.673611581325531
translation,146,65,results,results,has,increase,results has increase,0.5239861011505127
translation,147,161,experimental-setup,cased multilingual bert embeddings,with,768 dimensions,cased multilingual bert embeddings with 768 dimensions,0.642341673374176
translation,147,161,experimental-setup,cased multilingual bert embeddings,with,12 layers,cased multilingual bert embeddings with 12 layers,0.5979540348052979
translation,147,161,experimental-setup,cased multilingual bert embeddings,with,12 attention heads,cased multilingual bert embeddings with 12 attention heads,0.6194217801094055
translation,147,161,experimental-setup,cased multilingual bert embeddings,with,179m parameters,cased multilingual bert embeddings with 179m parameters,0.6218571066856384
translation,147,161,experimental-setup,experimental setup,use,cased multilingual bert embeddings,experimental setup use cased multilingual bert embeddings,0.5438725352287292
translation,147,150,model,babalign,leverages,translation infor-mation,babalign leverages translation infor-mation,0.7203571796417236
translation,147,150,model,babalign,post-process,alignment,babalign post-process alignment,0.7654638886451721
translation,147,150,model,translation infor-mation,from,babelnet,translation infor-mation from babelnet,0.577149510383606
translation,147,150,model,translation infor-mation,to create,synthetic training data,translation infor-mation to create synthetic training data,0.6144621968269348
translation,147,150,model,alignment,produced using,base unsupervised alignment method,alignment produced using base unsupervised alignment method,0.6438381671905518
translation,147,150,model,model,has,babalign,model has babalign,0.6192402839660645
translation,147,168,results,both methods,based on,intersecting sets,both methods based on intersecting sets,0.6473715901374817
translation,147,168,results,intersecting sets,of,multi-synsets,intersecting sets of multi-synsets,0.5430602431297302
translation,147,168,results,intersecting sets,of,cval,intersecting sets of cval,0.5758845806121826
translation,147,168,results,intersecting sets,of,csub,intersecting sets of csub,0.5950185060501099
translation,147,168,results,baseline translation identity method,has,ident,baseline translation identity method has ident,0.5611840486526489
translation,147,168,results,outperforming,has,both methods,outperforming has both methods,0.5658589601516724
translation,147,168,results,results,has,baseline translation identity method,results has baseline translation identity method,0.5196060538291931
translation,147,171,results,best performing method,is,sub,best performing method is sub,0.6070154309272766
translation,147,171,results,best performing method,shows,improvement,best performing method shows improvement,0.693098783493042
translation,147,171,results,improvement,when combining,all three languages of translation,improvement when combining all three languages of translation,0.7614673972129822
translation,147,171,results,results,has,best performing method,results has best performing method,0.5347415208816528
translation,147,184,results,sub method,achieves,best performance,sub method achieves best performance,0.710878312587738
translation,147,184,results,best performance,with,combination of all three languages,best performance with combination of all three languages,0.6289672255516052
translation,147,185,results,ident method,performs,surprisingly well,ident method performs surprisingly well,0.6430858969688416
translation,147,185,results,outperforming,has,more complex cval and csub methods,outperforming has more complex cval and csub methods,0.5459654331207275
translation,147,185,results,results,has,ident method,results has ident method,0.5620156526565552
translation,147,186,results,russian,yields,substantially better performance,russian yields substantially better performance,0.7272315621376038
translation,147,186,results,substantially better performance,compared to,french or italian,substantially better performance compared to french or italian,0.6745820641517639
translation,147,186,results,french or italian,across,all four methods,french or italian across all four methods,0.6638596057891846
translation,147,186,results,italian,yields,better performance,italian yields better performance,0.722079873085022
translation,147,186,results,better performance,than,french,better performance than french,0.5954239964485168
translation,147,189,results,multiple languages of translation,results in,substantial reduction,multiple languages of translation results in substantial reduction,0.6360341906547546
translation,147,189,results,substantial reduction,in,false positives,substantial reduction in false positives,0.5382099747657776
translation,147,189,results,substantial reduction,increase in,false negatives,substantial reduction increase in false negatives,0.7169556021690369
translation,147,189,results,results,using,multiple languages of translation,results using multiple languages of translation,0.6127967834472656
translation,148,42,baselines,margin- based ratio score ( mrs ),measure,distance between sentence embeddings,margin- based ratio score ( mrs ) measure distance between sentence embeddings,0.6106942892074585
translation,148,42,baselines,baselines,has,margin- based ratio score ( mrs ),baselines has margin- based ratio score ( mrs ),0.5324827432632446
translation,148,152,baselines,phrases,with,high overlap,phrases with high overlap,0.6562097072601318
translation,148,152,baselines,high overlap,with,test data,high overlap with test data,0.6590989828109741
translation,148,152,baselines,random phrase selection,has,ngf,random phrase selection has ngf,0.603395938873291
translation,148,114,experimental-setup,nmt model,use,6 - layer 512 - unit transformer,nmt model use 6 - layer 512 - unit transformer,0.6349915266036987
translation,148,114,experimental-setup,nmt model,use,subword vocabulary,nmt model use subword vocabulary,0.5511935353279114
translation,148,114,experimental-setup,nmt model,use,subword vocabulary,nmt model use subword vocabulary,0.5511935353279114
translation,148,114,experimental-setup,6 - layer 512 - unit transformer,implemented in,fairseq,6 - layer 512 - unit transformer implemented in fairseq,0.7323578000068665
translation,148,114,experimental-setup,subword vocabulary,of,"50,000","subword vocabulary of 50,000",0.5909575819969177
translation,148,114,experimental-setup,"50,000",for,both languages,"50,000 for both languages",0.5673672556877136
translation,148,114,experimental-setup,both languages,constructed by,byte pair encoding,both languages constructed by byte pair encoding,0.6323615908622742
translation,148,114,experimental-setup,experimental setup,use,subword vocabulary,experimental setup use subword vocabulary,0.5861636996269226
translation,148,115,experimental-setup,base model,with,adam,base model with adam,0.6847034096717834
translation,148,115,experimental-setup,base model,decay,learning rate,base model decay learning rate,0.7707033157348633
translation,148,115,experimental-setup,adam,for,10 epochs,adam for 10 epochs,0.5618956685066223
translation,148,115,experimental-setup,adam,with,4 k warmup steps,adam with 4 k warmup steps,0.6251125335693359
translation,148,115,experimental-setup,adam,with,peak learning rate,adam with peak learning rate,0.6482047438621521
translation,148,115,experimental-setup,10 epochs,with,4 k warmup steps,10 epochs with 4 k warmup steps,0.6317790150642395
translation,148,115,experimental-setup,peak learning rate,of,1e - 3,peak learning rate of 1e - 3,0.6436079144477844
translation,148,115,experimental-setup,learning rate,based on,inverse square root,learning rate based on inverse square root,0.6292312145233154
translation,148,115,experimental-setup,inverse square root,of,number of update steps,inverse square root of number of update steps,0.6221041083335876
translation,148,115,experimental-setup,experimental setup,train,base model,experimental setup train base model,0.6683900356292725
translation,148,116,experimental-setup,active learning,set,annotation budgets,active learning set annotation budgets,0.6449952721595764
translation,148,116,experimental-setup,annotation budgets,by,number of words translated,annotation budgets by number of words translated,0.5345826745033264
translation,148,116,experimental-setup,annotation budgets,from,2.5 k words,annotation budgets from 2.5 k words,0.5575993657112122
translation,148,116,experimental-setup,2.5 k words,has,up to 40 k words,2.5 k words has up to 40 k words,0.6093612909317017
translation,148,116,experimental-setup,experimental setup,For,active learning,experimental setup For active learning,0.5610997080802917
translation,148,184,experimental-setup,nmt model,use,6 - layer 512 - unit transformer,nmt model use 6 - layer 512 - unit transformer,0.6349915266036987
translation,148,184,experimental-setup,nmt model,use,subword vocabulary,nmt model use subword vocabulary,0.5511935353279114
translation,148,184,experimental-setup,nmt model,use,subword vocabulary,nmt model use subword vocabulary,0.5511935353279114
translation,148,184,experimental-setup,6 - layer 512 - unit transformer,implemented in,fairseq,6 - layer 512 - unit transformer implemented in fairseq,0.7323578000068665
translation,148,184,experimental-setup,subword vocabulary,of,"5,000","subword vocabulary of 5,000",0.5944480299949646
translation,148,184,experimental-setup,"5,000",for,both languages,"5,000 for both languages",0.5950424075126648
translation,148,184,experimental-setup,both languages,constructed by,byte pair encoding,both languages constructed by byte pair encoding,0.6323615908622742
translation,148,184,experimental-setup,experimental setup,use,subword vocabulary,experimental setup use subword vocabulary,0.5861636996269226
translation,148,187,experimental-setup,base model,with,adam,base model with adam,0.6847034096717834
translation,148,187,experimental-setup,base model,decay,learning rate,base model decay learning rate,0.7707033157348633
translation,148,187,experimental-setup,adam,for,10 epochs,adam for 10 epochs,0.5618956685066223
translation,148,187,experimental-setup,adam,with,4 k warmup steps,adam with 4 k warmup steps,0.6251125335693359
translation,148,187,experimental-setup,adam,with,peak learning rate,adam with peak learning rate,0.6482047438621521
translation,148,187,experimental-setup,10 epochs,with,4 k warmup steps,10 epochs with 4 k warmup steps,0.6317790150642395
translation,148,187,experimental-setup,peak learning rate,of,1e - 3,peak learning rate of 1e - 3,0.6436079144477844
translation,148,187,experimental-setup,learning rate,based on,inverse square root,learning rate based on inverse square root,0.6292312145233154
translation,148,187,experimental-setup,inverse square root,of,number of update steps,inverse square root of number of update steps,0.6221041083335876
translation,148,187,experimental-setup,experimental setup,train,base model,experimental setup train base model,0.6683900356292725
translation,148,189,experimental-setup,training / inference time,train,each model,training / inference time train each model,0.6794430017471313
translation,148,189,experimental-setup,each model,on,one nvidia rtx 2080 ti gpu,each model on one nvidia rtx 2080 ti gpu,0.5347486734390259
translation,148,189,experimental-setup,experimental setup,has,training / inference time,experimental setup has training / inference time,0.5034182667732239
translation,148,5,model,active learning setting,spend,given budget,active learning setting spend given budget,0.6926171779632568
translation,148,5,model,given budget,on,translating in - domain data,given budget on translating in - domain data,0.607897162437439
translation,148,5,model,pre-trained out - of- domain nmt model,on,newly translated data,pre-trained out - of- domain nmt model on newly translated data,0.5169893503189087
translation,148,5,model,model,in,active learning setting,model in active learning setting,0.5502428412437439
translation,148,20,model,method,for incorporating,phrase - based active learning,method for incorporating phrase - based active learning,0.6553388237953186
translation,148,20,model,phrase - based active learning,into,nmt,phrase - based active learning into nmt,0.600420355796814
translation,148,21,model,sentence - based and phrase - based selection strategies,propose,hybrid strategy,sentence - based and phrase - based selection strategies propose hybrid strategy,0.6112442016601562
translation,148,21,model,hybrid strategy,combines,both methods,hybrid strategy combines both methods,0.7348427176475525
translation,148,21,model,model,describe,sentence - based and phrase - based selection strategies,model describe sentence - based and phrase - based selection strategies,0.6967576742172241
translation,148,21,model,model,propose,hybrid strategy,model propose hybrid strategy,0.6838043332099915
translation,148,77,model,simple yet novel hybrid selection strategy,leverages,benefits,simple yet novel hybrid selection strategy leverages benefits,0.7544209957122803
translation,148,77,model,benefits,of,sentence - based and phrase - based selection strategies,benefits of sentence - based and phrase - based selection strategies,0.589909553527832
translation,148,77,model,model,propose,simple yet novel hybrid selection strategy,model propose simple yet novel hybrid selection strategy,0.6925266981124878
translation,148,84,model,strategy,of,mixed fine-tuning,strategy of mixed fine-tuning,0.6055113077163696
translation,148,84,model,pretrained out - of- domain model,on,in - domain data,pretrained out - of- domain model on in - domain data,0.5269594192504883
translation,148,84,model,pretrained out - of- domain model,on,certain amount of out - of- domain data,pretrained out - of- domain model on certain amount of out - of- domain data,0.5074241161346436
translation,148,84,model,pretrained out - of- domain model,to prevent,overfitting,pretrained out - of- domain model to prevent overfitting,0.5936658978462219
translation,148,84,model,certain amount of out - of- domain data,to prevent,overfitting,certain amount of out - of- domain data to prevent overfitting,0.6018094420433044
translation,148,84,model,overfitting,to,relatively small in - domain data,overfitting to relatively small in - domain data,0.5409345030784607
translation,148,84,model,model,adapt,strategy,model adapt strategy,0.7497732639312744
translation,148,124,results,significantly improves,with,small annotation budget,significantly improves with small annotation budget,0.6355758905410767
translation,148,124,results,translation accuracy,of,in-domain words,translation accuracy of in-domain words,0.561775803565979
translation,148,124,results,translation accuracy,with,small annotation budget,translation accuracy with small annotation budget,0.5829671621322632
translation,148,124,results,ngf - smp,has,significantly improves,ngf - smp has significantly improves,0.6031970977783203
translation,148,124,results,significantly improves,has,translation accuracy,significantly improves has translation accuracy,0.5682457089424133
translation,148,124,results,results,has,ngf - smp,results has ngf - smp,0.5149660110473633
translation,148,126,results,hybrid selection strategy,of,ngf - smp and mrs,hybrid selection strategy of ngf - smp and mrs,0.5876205563545227
translation,148,126,results,even higher accuracy,when,budget,even higher accuracy when budget,0.7161957621574402
translation,148,126,results,budget,greater than,40 k annotated words,budget greater than 40 k annotated words,0.6286661028862
translation,148,126,results,results,find that,hybrid selection strategy,results find that hybrid selection strategy,0.664824366569519
translation,148,138,results,phrase - based selection methods,has,ngf - smp,phrase - based selection methods has ngf - smp,0.6077079176902771
translation,148,138,results,ngf - smp,has,significantly outperforms,ngf - smp has significantly outperforms,0.6067166924476624
translation,148,138,results,significantly outperforms,has,random phrase selection strategy,significantly outperforms has random phrase selection strategy,0.5978579521179199
translation,148,138,results,results,For,phrase - based selection methods,results For phrase - based selection methods,0.5583693981170654
translation,148,139,results,sentence selection methods,when,annotation budget,sentence selection methods when annotation budget,0.6165637969970703
translation,148,139,results,sentence selection methods,for,adaption to the medicine domain,sentence selection methods for adaption to the medicine domain,0.5827305316925049
translation,148,139,results,annotation budget,is,small ( less than 20 k words ),annotation budget is small ( less than 20 k words ),0.5254366993904114
translation,148,139,results,small ( less than 20 k words ),for,adaption to the medicine domain,small ( less than 20 k words ) for adaption to the medicine domain,0.6123780012130737
translation,148,139,results,even outperforms,has,sentence selection methods,even outperforms has sentence selection methods,0.5843287110328674
translation,148,140,results,annotation budget,to,40 k annotated words,annotation budget to 40 k annotated words,0.5153642892837524
translation,148,140,results,sentence selection strategies,outperform,phrase selection strategies,sentence selection strategies outperform phrase selection strategies,0.674888551235199
translation,148,140,results,annotation budget,has,sentence selection strategies,annotation budget has sentence selection strategies,0.5438704490661621
translation,148,140,results,40 k annotated words,has,sentence selection strategies,40 k annotated words has sentence selection strategies,0.5680068731307983
translation,148,140,results,results,increase,annotation budget,results increase annotation budget,0.6565763354301453
translation,148,142,results,nmt models,trained with,phrasal translations,nmt models trained with phrasal translations,0.7121177315711975
translation,148,142,results,sentence translations,when adapting to,it domain,sentence translations when adapting to it domain,0.636348307132721
translation,148,142,results,results,find that,nmt models,results find that nmt models,0.5912908911705017
translation,148,145,results,hybrid selection,of,ngf - smp and mrs strategies,hybrid selection of ngf - smp and mrs strategies,0.5863573551177979
translation,148,145,results,outperforms,improving,best phrase selection strategy ngf - smp,outperforms improving best phrase selection strategy ngf - smp,0.6630237102508545
translation,148,145,results,individual selection strategies,over,every budget,individual selection strategies over every budget,0.6906203627586365
translation,148,145,results,individual selection strategies,improving,best phrase selection strategy ngf - smp,individual selection strategies improving best phrase selection strategy ngf - smp,0.6727296113967896
translation,148,145,results,individual selection strategies,improving,best sentence selection strategy,individual selection strategies improving best sentence selection strategy,0.6661502718925476
translation,148,145,results,best phrase selection strategy ngf - smp,by,0.49 average bleu points,best phrase selection strategy ngf - smp by 0.49 average bleu points,0.5307700634002686
translation,148,145,results,best phrase selection strategy ngf - smp,by,1.11 average bleu points,best phrase selection strategy ngf - smp by 1.11 average bleu points,0.5222598910331726
translation,148,145,results,best sentence selection strategy,by,1.11 average bleu points,best sentence selection strategy by 1.11 average bleu points,0.5340889096260071
translation,148,145,results,mrs,by,1.11 average bleu points,mrs by 1.11 average bleu points,0.5854054093360901
translation,148,145,results,1.11 average bleu points,in,medicine domain,1.11 average bleu points in medicine domain,0.45964381098747253
translation,148,145,results,ngf - smp and mrs strategies,has,outperforms,ngf - smp and mrs strategies has outperforms,0.632892370223999
translation,148,145,results,outperforms,has,individual selection strategies,outperforms has individual selection strategies,0.6089670062065125
translation,148,145,results,best sentence selection strategy,has,mrs,best sentence selection strategy has mrs,0.6025722622871399
translation,148,145,results,results,has,hybrid selection,results has hybrid selection,0.5554856061935425
translation,148,147,results,adaptation scenario,requires,longer context,adaptation scenario requires longer context,0.6454878449440002
translation,148,147,results,best phrase - based strategy ngf - smp,by,1.2 average bleu points,best phrase - based strategy ngf - smp by 1.2 average bleu points,0.5195227265357971
translation,148,147,results,mrs,by,0.15 bleu points,mrs by 0.15 bleu points,0.589770495891571
translation,148,147,results,adaptation scenario,has,hybrid strategy,adaptation scenario has hybrid strategy,0.5940061807632446
translation,148,147,results,longer context,has,hybrid strategy,longer context has hybrid strategy,0.6184400320053101
translation,148,147,results,hybrid strategy,has,significantly outperforms,hybrid strategy has significantly outperforms,0.6237283945083618
translation,148,147,results,significantly outperforms,has,best phrase - based strategy ngf - smp,significantly outperforms has best phrase - based strategy ngf - smp,0.6052528023719788
translation,148,147,results,best sentence selection strategy,has,mrs,best sentence selection strategy has mrs,0.6025722622871399
translation,148,147,results,results,For,adaptation scenario,results For adaptation scenario,0.6196637153625488
translation,148,148,results,sentence and phrase selection strategies,in,domain adaptation setting,sentence and phrase selection strategies in domain adaptation setting,0.4820830821990967
translation,148,148,results,results,has,hybrid selection strategy,results has hybrid selection strategy,0.5557212233543396
translation,148,153,results,sentence selection strategies,cover,fewer phrases,sentence selection strategies cover fewer phrases,0.7113261818885803
translation,148,153,results,fewer phrases,in,test data,fewer phrases in test data,0.4851354956626892
translation,148,153,results,fewer phrases,than,phrase selection strategies,fewer phrases than phrase selection strategies,0.5662205219268799
translation,148,153,results,results,observe,sentence selection strategies,results observe sentence selection strategies,0.5323863625526428
translation,148,161,results,randomly sampled sentence pairs,from,out-of- domain data,randomly sampled sentence pairs from out-of- domain data,0.5586937069892883
translation,148,161,results,randomly sampled sentence pairs,helps,hurts,randomly sampled sentence pairs helps hurts,0.6721494793891907
translation,148,161,results,annotation budget,less than,5 k annotated words,annotation budget less than 5 k annotated words,0.5912086367607117
translation,148,161,results,hurts,increase,budget,hurts increase budget,0.7303619980812073
translation,148,163,results,mixed fine-tuning,on,synthetic data,mixed fine-tuning on synthetic data,0.5496460199356079
translation,148,163,results,mixed fine-tuning,performs,slightly worse,mixed fine-tuning performs slightly worse,0.6043967604637146
translation,148,163,results,synthetic data,by switching,phrases,synthetic data by switching phrases,0.7063718438148499
translation,148,163,results,slightly worse,than,mixed fine - tuning,slightly worse than mixed fine - tuning,0.6095422506332397
translation,148,163,results,mixed fine - tuning,on,real retrieved data,mixed fine - tuning on real retrieved data,0.578626811504364
translation,148,163,results,annotation budget,is,small,annotation budget is small,0.5505384206771851
translation,148,163,results,outperforms,has,fine-tuning,outperforms has fine-tuning,0.5970712304115295
translation,148,163,results,outperforms,has,without any out-ofdomain data,outperforms has without any out-ofdomain data,0.6077761650085449
translation,148,163,results,fine-tuning,has,without any out-ofdomain data,fine-tuning has without any out-ofdomain data,0.5731395483016968
translation,148,163,results,results,has,mixed fine-tuning,results has mixed fine-tuning,0.5761734247207642
translation,148,164,results,synthetic data,by switching,phrase and real retrieved data,synthetic data by switching phrase and real retrieved data,0.7810145616531372
translation,148,164,results,synthetic data,improves,translation performance,synthetic data improves translation performance,0.6488739848136902
translation,148,164,results,phrase and real retrieved data,for,mixed fine-tuning,phrase and real retrieved data for mixed fine-tuning,0.6056101322174072
translation,148,164,results,translation performance,over,training,translation performance over training,0.6778820753097534
translation,148,164,results,training,only on,synthetic data,training only on synthetic data,0.6024898886680603
translation,148,164,results,results,Combining,synthetic data,results Combining synthetic data,0.6544250249862671
translation,148,165,results,contextualized method,performs,worst,contextualized method performs worst,0.6159524917602539
translation,148,165,results,worst,among,all mixed fine-tuning methods,worst among all mixed fine-tuning methods,0.6086573004722595
translation,148,165,results,results,has,contextualized method,results has contextualized method,0.5081028342247009
translation,150,218,ablation-analysis,hurts,has,bleu scores,hurts has bleu scores,0.618149995803833
translation,150,218,ablation-analysis,ablation analysis,find that,smaller and larger queue size,ablation analysis find that smaller and larger queue size,0.6507337689399719
translation,150,160,baselines,word - kd,is,standard method,word - kd is standard method,0.5914474129676819
translation,150,160,baselines,word - kd,distills,knowledge equally,word - kd distills knowledge equally,0.7335799932479858
translation,150,160,baselines,standard method,distills,knowledge equally,standard method distills knowledge equally,0.7584132552146912
translation,150,160,baselines,knowledge equally,for,each word,knowledge equally for each word,0.6460971832275391
translation,150,160,baselines,baselines,has,word - kd,baselines has word - kd,0.5871209502220154
translation,150,144,experimental-setup,transformer ( base ),contains,six stacked encoder layers,transformer ( base ) contains six stacked encoder layers,0.6366050839424133
translation,150,144,experimental-setup,transformer ( base ),contains,six stacked decoder layers,transformer ( base ) contains six stacked decoder layers,0.6297764778137207
translation,150,144,experimental-setup,six stacked decoder layers,as,teacher model,six stacked decoder layers as teacher model,0.5152349472045898
translation,150,144,experimental-setup,six stacked decoder layers,as,student model,six stacked decoder layers as student model,0.5285730957984924
translation,150,144,experimental-setup,experimental setup,use,transformer ( base ),experimental setup use transformer ( base ),0.5861861705780029
translation,150,146,experimental-setup,training processing,use,adam optimizer,training processing use adam optimizer,0.647388219833374
translation,150,146,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.98","adam optimizer with ? 1 = 0.9 , ? 2 = 0.98",0.6221903562545776
translation,150,146,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,150,146,experimental-setup,adam optimizer,with,dropout,adam optimizer with dropout,0.5948058366775513
translation,150,146,experimental-setup,learning rate,is,7e - 4,learning rate is 7e - 4,0.5948405265808105
translation,150,146,experimental-setup,dropout,is,0.1,dropout is 0.1,0.5574049949645996
translation,150,146,experimental-setup,experimental setup,In,training processing,experimental setup In training processing,0.5167000889778137
translation,150,147,experimental-setup,experiments,conducted using,4 nvidia p40 gpus,experiments conducted using 4 nvidia p40 gpus,0.6194345951080322
translation,150,147,experimental-setup,4 nvidia p40 gpus,where,batch size,4 nvidia p40 gpus where batch size,0.6218196749687195
translation,150,150,experimental-setup,two hyper-parameters,i.e.,distil rate r%,two hyper-parameters i.e. distil rate r%,0.7064165472984314
translation,150,150,experimental-setup,two hyper-parameters,i.e.,global queue size q size,two hyper-parameters i.e. global queue size q size,0.6499438285827637
translation,150,155,experimental-setup,q size,=,30 k,q size = 30 k,0.7087787389755249
translation,150,155,experimental-setup,30 k,for,wmt '14 en- de,30 k for wmt '14 en- de,0.7144187688827515
translation,150,155,experimental-setup,experimental setup,set,q size,experimental setup set q size,0.681222677230835
translation,150,134,experiments,wmt '14 en- de task,use,4.5 m,wmt '14 en- de task use 4.5 m,0.5990740060806274
translation,150,134,experiments,4.5 m,is,tokenized and split,4.5 m is tokenized and split,0.6094391345977783
translation,150,134,experiments,tokenized and split,using,byte pair encoded ( bpe ),tokenized and split using byte pair encoded ( bpe ),0.7382562756538391
translation,150,134,experiments,byte pair encoded ( bpe ),with,32 k merge operations,byte pair encoded ( bpe ) with 32 k merge operations,0.6462257504463196
translation,150,134,experiments,byte pair encoded ( bpe ),with,shared vocabulary,byte pair encoded ( bpe ) with shared vocabulary,0.6357477903366089
translation,150,134,experiments,shared vocabulary,for,english and german,shared vocabulary for english and german,0.6083959937095642
translation,150,134,experiments,4.5 m,has,preprocessed data,4.5 m has preprocessed data,0.5768086314201355
translation,150,156,experiments,larger dataset wmt '19 zh-en,enlarge,q size,larger dataset wmt '19 zh-en enlarge q size,0.7434829473495483
translation,150,156,experiments,larger dataset wmt '19 zh-en,keep,word rate,larger dataset wmt '19 zh-en keep word rate,0.588462233543396
translation,150,156,experiments,q size,from,30 k to 50 k,q size from 30 k to 50 k,0.6262487173080444
translation,150,156,experiments,word rate,has,unchanged,word rate has unchanged,0.6098760366439819
translation,150,10,model,two simple yet effective strategies,i.e.,batch - level and global - level selections,two simple yet effective strategies i.e. batch - level and global - level selections,0.7291682362556458
translation,150,10,model,two simple yet effective strategies,to pick,suitable samples,two simple yet effective strategies to pick suitable samples,0.7057451009750366
translation,150,10,model,suitable samples,for,distillation,suitable samples for distillation,0.6377483606338501
translation,150,10,model,model,propose,two simple yet effective strategies,model propose two simple yet effective strategies,0.6839460134506226
translation,150,33,model,batch - level selection strategy,chooses,words,batch - level selection strategy chooses words,0.6991565823554993
translation,150,33,model,words,with,higher word ce,words with higher word ce,0.7083742022514343
translation,150,33,model,higher word ce,within,current batch 's distribution,higher word ce within current batch 's distribution,0.6678202748298645
translation,150,33,model,model,propose,batch - level selection strategy,model propose batch - level selection strategy,0.6563183069229126
translation,150,34,model,local ( batch ) distribution,to,global distribution,local ( batch ) distribution to global distribution,0.5613734722137451
translation,150,34,model,local ( batch ) distribution,use,global - level fifo queue,local ( batch ) distribution use global - level fifo queue,0.6101279258728027
translation,150,34,model,global distribution,use,global - level fifo queue,global distribution use global - level fifo queue,0.6287347078323364
translation,150,34,model,global - level fifo queue,to approximate,optimal global selection strategy,global - level fifo queue to approximate optimal global selection strategy,0.7635497450828552
translation,150,34,model,optimal global selection strategy,caches,word ce distributions,optimal global selection strategy caches word ce distributions,0.7511626482009888
translation,150,34,model,word ce distributions,across,several steps,word ce distributions across several steps,0.7429623603820801
translation,150,34,model,model,to step forward,local ( batch ) distribution,model to step forward local ( batch ) distribution,0.6785293817520142
translation,150,34,model,model,use,global - level fifo queue,model use global - level fifo queue,0.6418638825416565
translation,150,39,model,model,propose,two selective strategies,model propose two selective strategies,0.6401810646057129
translation,150,145,model,deep transformers,with,twelve encoder layers,deep transformers with twelve encoder layers,0.6611449122428894
translation,150,145,model,deep transformers,with,six decoder layers,deep transformers with six decoder layers,0.63065105676651
translation,150,145,model,model,use,deep transformers,model use deep transformers,0.6652324199676514
translation,150,148,model,gradient,of,parameters,gradient of parameters,0.5826181173324585
translation,150,148,model,gradient,update,two steps,gradient update two steps,0.8019248247146606
translation,150,148,model,model,accumulate,gradient,model accumulate gradient,0.6768090128898621
translation,150,28,results,model,with,half of the knowledge,model with half of the knowledge,0.6513793468475342
translation,150,28,results,better performance,than,model,better performance than model,0.5706859230995178
translation,150,28,results,model,using,all distill knowledge,model using all distill knowledge,0.704626739025116
translation,150,28,results,some partitions,has,model,some partitions has model,0.6387836933135986
translation,150,28,results,model 's word cross-entropy,has,model,model 's word cross-entropy has model,0.5798999667167664
translation,150,28,results,results,with,some partitions,results with some partitions,0.6776633858680725
translation,150,149,results,average runtimes,are,3 gpu days,average runtimes are 3 gpu days,0.5603790283203125
translation,150,149,results,3 gpu days,for,all experiments,3 gpu days for all experiments,0.5418451428413391
translation,150,149,results,results,has,average runtimes,results has average runtimes,0.5246347188949585
translation,150,172,results,our re-implemented word-level and the sequencelevel distillation,show,similar improvements,our re-implemented word-level and the sequencelevel distillation show similar improvements,0.6370351910591125
translation,150,172,results,similar improvements,with,bleu scores,similar improvements with bleu scores,0.6189613938331604
translation,150,172,results,bleu scores,up from,27.29 to 28.14 and 28.15,bleu scores up from 27.29 to 28.14 and 28.15,0.6717166304588318
translation,150,172,results,transformer ( base ),has,our re-implemented word-level and the sequencelevel distillation,transformer ( base ) has our re-implemented word-level and the sequencelevel distillation,0.6073750257492065
translation,150,172,results,results,compared with,transformer ( base ),results compared with transformer ( base ),0.6786475777626038
translation,150,173,results,our batch - level selective approach,extends,improvement,our batch - level selective approach extends improvement,0.7313978672027588
translation,150,173,results,improvement,to,28.42,improvement to 28.42,0.5599537491798401
translation,150,173,results,improvement,proving,selective strategy 's effectiveness,improvement proving selective strategy 's effectiveness,0.7039563655853271
translation,150,173,results,methods,has,our batch - level selective approach,methods has our batch - level selective approach,0.5615052580833435
translation,150,174,results,global - level distillation,achieves,28.57 bleu score,global - level distillation achieves 28.57 bleu score,0.6144853830337524
translation,150,174,results,outperforms,has,all previous methods,outperforms has all previous methods,0.573925256729126
translation,150,174,results,results,has,global - level distillation,results has global - level distillation,0.5282900333404541
translation,150,175,results,translation quality,over,all others methods,translation quality over all others methods,0.5824947357177734
translation,150,175,results,all others methods,including,word -kd,all others methods including word -kd,0.643582284450531
translation,150,175,results,our strategy,has,significantly improves,our strategy has significantly improves,0.6144673824310303
translation,150,175,results,significantly improves,has,translation quality,significantly improves has translation quality,0.5744650959968567
translation,150,176,results,our methods,show,comparable / better performance,our methods show comparable / better performance,0.6004085540771484
translation,150,176,results,our methods,even surpass,transformer ( big ),our methods even surpass transformer ( big ),0.7418829798698425
translation,150,176,results,comparable / better performance,than,other existing nmt systems,comparable / better performance than other existing nmt systems,0.584314227104187
translation,150,176,results,transformer ( big ),with,much fewer parameters,transformer ( big ) with much fewer parameters,0.6479347348213196
translation,150,176,results,results,has,our methods,results has our methods,0.5312396883964539
translation,150,204,results,transformer ( base ),with,+ 0.89,transformer ( base ) with + 0.89,0.652881383895874
translation,150,204,results,significantly outperforms,has,transformer ( base ),significantly outperforms has transformer ( base ),0.6372320055961609
translation,150,204,results,results,has,our method,results has our method,0.5589964985847473
translation,150,205,results,consistently improves,with,+ 0.41 bleu points,consistently improves with + 0.41 bleu points,0.6099292635917664
translation,150,205,results,word - kd,has,our approach,word - kd has our approach,0.5839638710021973
translation,150,205,results,our approach,has,consistently improves,our approach has consistently improves,0.598388135433197
translation,150,205,results,results,Compared with,word - kd,results Compared with word - kd,0.6451668739318848
translation,150,206,results,seq - kd,with,our methods,seq - kd with our methods,0.651014506816864
translation,150,206,results,seq - kd,extends,improvement,seq - kd extends improvement,0.771354079246521
translation,150,206,results,improvement,of,bleu score,improvement of bleu score,0.5634691119194031
translation,150,206,results,bleu score,from,27.27 to 27.61,bleu score from 27.27 to 27.61,0.5212950110435486
translation,150,206,results,results,find that,seq - kd,results find that seq - kd,0.66207355260849
translation,150,212,results,deep transformer ( 12 + 6 ) and word - kd,achieved,strong performance,deep transformer ( 12 + 6 ) and word - kd achieved strong performance,0.679614782333374
translation,150,212,results,strong performance,with,up to 28.90 bleu points,strong performance with up to 28.90 bleu points,0.6261294484138489
translation,150,212,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,150,212,results,outperforms,has,baselines ( 29.12 bleu ),outperforms has baselines ( 29.12 bleu ),0.5707733631134033
translation,150,219,results,30 k and 50 k,of,queue size,30 k and 50 k of queue size,0.6605945229530334
translation,150,219,results,queue size,are,best,queue size are best,0.5877248644828796
translation,150,219,results,best,for,wmt '14 en-de and wmt '19 zh-en,best for wmt '14 en-de and wmt '19 zh-en,0.6340806484222412
translation,150,219,results,results,has,30 k and 50 k,results has 30 k and 50 k,0.5622016191482544
translation,151,60,baselines,text -only baseline,use,variant of the transformer,text -only baseline use variant of the transformer,0.6100562810897827
translation,151,60,baselines,variant of the transformer,has,4 encoder layers,variant of the transformer has 4 encoder layers,0.6002851724624634
translation,151,60,baselines,variant of the transformer,has,4 decoder layers,variant of the transformer has 4 decoder layers,0.592689573764801
translation,151,60,baselines,variant of the transformer,has,4 attention heads,variant of the transformer has 4 attention heads,0.6156060099601746
translation,151,60,baselines,4 attention heads,in,each layer,4 attention heads in each layer,0.5660417675971985
translation,151,60,baselines,baselines,For,text -only baseline,baselines For text -only baseline,0.5872690677642822
translation,151,61,experimental-setup,dimensions,of,input / output layers and inner feed -forward layers,dimensions of input / output layers and inner feed -forward layers,0.6000141501426697
translation,151,61,experimental-setup,input / output layers and inner feed -forward layers,reduced to,128 and 256,input / output layers and inner feed -forward layers reduced to 128 and 256,0.6950244903564453
translation,151,61,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,151,75,experimental-setup,maximum number of tokens,in,mini-batch,maximum number of tokens in mini-batch,0.5345529913902283
translation,151,75,experimental-setup,mini-batch,is,4096,mini-batch is 4096,0.6275564432144165
translation,151,75,experimental-setup,experimental setup,has,maximum number of tokens,experimental setup has maximum number of tokens,0.5387575626373291
translation,151,76,experimental-setup,warms up,from,1e ? 7,warms up from 1e ? 7,0.631604790687561
translation,151,76,experimental-setup,1e ? 7,to,0.005,1e ? 7 to 0.005,0.5517010688781738
translation,151,76,experimental-setup,0.005,in,2000 steps,0.005 in 2000 steps,0.5677893161773682
translation,151,76,experimental-setup,decays,based on,inverse square root,decays based on inverse square root,0.7393701076507568
translation,151,76,experimental-setup,inverse square root,of,update number,inverse square root of update number,0.6249436140060425
translation,151,76,experimental-setup,learning rate,has,warms up,learning rate has warms up,0.5685316920280457
translation,151,76,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,151,77,experimental-setup,"dropout ( srivastava et al. , 2014 )",of,0.3,"dropout ( srivastava et al. , 2014 ) of 0.3",0.5741616487503052
translation,151,77,experimental-setup,label - smoothing,of,0.1,label - smoothing of 0.1,0.601080060005188
translation,151,77,experimental-setup,experimental setup,has,"dropout ( srivastava et al. , 2014 )","experimental setup has dropout ( srivastava et al. , 2014 )",0.4943450689315796
translation,151,78,experimental-setup,last ten checkpoints,averaged for,inference,last ten checkpoints averaged for inference,0.79239821434021
translation,151,78,experimental-setup,experimental setup,trained with,earlystopping ( patience = 10 ),experimental setup trained with earlystopping ( patience = 10 ),0.7436354160308838
translation,151,79,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,151,80,experimental-setup,"toolkit fairseq ( ott et al. , 2019 )",for,our implementation,"toolkit fairseq ( ott et al. , 2019 ) for our implementation",0.5050716996192932
translation,151,80,experimental-setup,experimental setup,use,"toolkit fairseq ( ott et al. , 2019 )","experimental setup use toolkit fairseq ( ott et al. , 2019 )",0.5491004586219788
translation,151,27,model,two methods,to necessitate,visual context - back - translation,two methods to necessitate visual context - back - translation,0.6576981544494629
translation,151,27,model,two methods,to necessitate,word dropout,two methods to necessitate word dropout,0.6565003991127014
translation,151,27,model,visual context - back - translation,from,gender-neutral language ( e.g. turkish ),visual context - back - translation from gender-neutral language ( e.g. turkish ),0.5244109034538269
translation,151,27,model,word dropout,in,source sentence,word dropout in source sentence,0.4878862202167511
translation,151,27,model,model,propose,two methods,model propose two methods,0.663135290145874
translation,151,95,results,our mmt models,provide,little to no improvement,our mmt models provide little to no improvement,0.6136394143104553
translation,151,95,results,little to no improvement,over,text-only transformer,little to no improvement over text-only transformer,0.6969388723373413
translation,151,95,results,results,found,our mmt models,results found our mmt models,0.5710002779960632
translation,151,106,results,same multimodal models,on,ambiguous captions dataset,same multimodal models on ambiguous captions dataset,0.4979667663574219
translation,151,106,results,same multimodal models,results in,substantial improvements,same multimodal models results in substantial improvements,0.6265902519226074
translation,151,106,results,substantial improvements,in terms of,bleu scores,substantial improvements in terms of bleu scores,0.6662662625312805
translation,151,106,results,substantial improvements,in terms of,gender accuracy,substantial improvements in terms of gender accuracy,0.728096067905426
translation,151,106,results,gender accuracy,compared to,our text-only baseline,gender accuracy compared to our text-only baseline,0.5818221569061279
translation,151,106,results,results,Training,same multimodal models,results Training same multimodal models,0.7516911029815674
translation,151,110,results,both the gated fusion and concatenation model,behave,similarly,both the gated fusion and concatenation model behave similarly,0.6545788645744324
translation,151,110,results,results,find that,both the gated fusion and concatenation model,results find that both the gated fusion and concatenation model,0.6098232865333557
translation,151,112,results,word dropout,tends to increase,image awareness,word dropout tends to increase image awareness,0.7314255833625793
translation,151,112,results,image awareness,for,concatenation model,image awareness for concatenation model,0.637087345123291
translation,151,112,results,results,found,word dropout,results found word dropout,0.6222503781318665
translation,151,116,results,some deterioration,of,bleu and gender accuracy,some deterioration of bleu and gender accuracy,0.5850836038589478
translation,151,116,results,bleu and gender accuracy,compared to,model,bleu and gender accuracy compared to model,0.6018232703208923
translation,151,116,results,model,trained without,word dropout,model trained without word dropout,0.7346643805503845
translation,151,116,results,results,observe,some deterioration,results observe some deterioration,0.649042546749115
translation,152,51,results,all of our system outputs,score,highest,all of our system outputs score highest,0.6810576319694519
translation,152,51,results,other systems,has,all of our system outputs,other systems has all of our system outputs,0.5835747718811035
translation,152,51,results,results,In comparison with,other systems,results In comparison with other systems,0.6718507409095764
translation,152,62,results,basictok model,fared a bit,better,basictok model fared a bit better,0.705410361289978
translation,152,62,results,better,than,model,better than model,0.5832422375679016
translation,152,62,results,model,trained on,morf segmented dataset,model trained on morf segmented dataset,0.7455635666847229
translation,152,62,results,results,see,basictok model,results see basictok model,0.6054063439369202
translation,153,28,experimental-setup,cross-lingual word embeddings,to extract,parallel sentences,cross-lingual word embeddings to extract parallel sentences,0.639777660369873
translation,153,28,experimental-setup,parallel sentences,from,wikipedia,parallel sentences from wikipedia,0.600089967250824
translation,153,28,experimental-setup,experimental setup,make use of,cross-lingual word embeddings,experimental setup make use of cross-lingual word embeddings,0.6025736927986145
translation,153,45,experimental-setup,cross-lingual word embeddings,to extract,parallel sentences,cross-lingual word embeddings to extract parallel sentences,0.639777660369873
translation,153,45,experimental-setup,parallel sentences,from,wikipedia,parallel sentences from wikipedia,0.600089967250824
translation,153,45,experimental-setup,experimental setup,make use of,cross-lingual word embeddings,experimental setup make use of cross-lingual word embeddings,0.6025736927986145
translation,153,133,experimental-setup,standard sequence - to-sequence transformer - based translation model,with,"six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture","standard sequence - to-sequence transformer - based translation model with six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture",0.5730839371681213
translation,153,133,experimental-setup,"six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture",from,"huggingface ( wolf et al. , 2019 )","six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture from huggingface ( wolf et al. , 2019 )",0.5336699485778809
translation,153,133,experimental-setup,"six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture",with,"shared sentencepiece ( kudo and richardson , 2018 ) vocabulary","six-layer bert - based ( devlin et al. , 2019 ) encoder-decoder architecture with shared sentencepiece ( kudo and richardson , 2018 ) vocabulary",0.5918349027633667
translation,153,133,experimental-setup,"pytorch ( paszke et al. , 2019 )",with,"shared sentencepiece ( kudo and richardson , 2018 ) vocabulary","pytorch ( paszke et al. , 2019 ) with shared sentencepiece ( kudo and richardson , 2018 ) vocabulary",0.6175567507743835
translation,153,133,experimental-setup,experimental setup,use,standard sequence - to-sequence transformer - based translation model,experimental setup use standard sequence - to-sequence transformer - based translation model,0.5865788459777832
translation,153,133,experimental-setup,experimental setup,use,"pytorch ( paszke et al. , 2019 )","experimental setup use pytorch ( paszke et al. , 2019 )",0.568523108959198
translation,153,189,experimental-setup,four models,on,3 -,four models on 3 -,0.6091527342796326
translation,153,189,experimental-setup,four models,on,tuples,four models on tuples,0.5991156101226807
translation,153,189,experimental-setup,3 -,via,single nvidia geforce rtx 2080 ti,3 - via single nvidia geforce rtx 2080 ti,0.7120184302330017
translation,153,189,experimental-setup,tuples,of,languages,tuples of languages,0.6080416440963745
translation,153,189,experimental-setup,single nvidia geforce rtx 2080 ti,with,11 gb,single nvidia geforce rtx 2080 ti with 11 gb,0.6201843619346619
translation,153,189,experimental-setup,11 gb,of,memory,11 gb of memory,0.5739430785179138
translation,153,189,experimental-setup,3 -,has,tuples,3 - has tuples,0.6385043859481812
translation,153,189,experimental-setup,experimental setup,pretrain,four models,experimental setup pretrain four models,0.7675658464431763
translation,153,190,experimental-setup,batches,of,4 k words,batches of 4 k words,0.6540217995643616
translation,153,190,experimental-setup,batches,run,pretraining,batches run pretraining,0.7731013298034668
translation,153,190,experimental-setup,pretraining,for,two million iterations,pretraining for two million iterations,0.5912750959396362
translation,153,190,experimental-setup,gradients,for,8 steps,gradients for 8 steps,0.6440924406051636
translation,153,190,experimental-setup,experimental setup,create,batches,experimental setup create batches,0.5935625433921814
translation,153,190,experimental-setup,experimental setup,accumulate,gradients,experimental setup accumulate gradients,0.5029471516609192
translation,153,191,experimental-setup,apex library,to use,fp - 16 tensors,apex library to use fp - 16 tensors,0.6923522353172302
translation,153,191,experimental-setup,experimental setup,use,apex library,experimental setup use apex library,0.6171454787254333
translation,153,192,experimental-setup,four weeks,in,single gpu,four weeks in single gpu,0.5581614375114441
translation,153,193,experimental-setup,adam optimizer,with,inverse square root,adam optimizer with inverse square root,0.6287417411804199
translation,153,193,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,153,193,experimental-setup,adam optimizer,with,4000 warm - up steps,adam optimizer with 4000 warm - up steps,0.6018317341804504
translation,153,193,experimental-setup,adam optimizer,with,dropout probability,adam optimizer with dropout probability,0.602870762348175
translation,153,193,experimental-setup,learning rate,of,10 ?4,learning rate of 10 ?4,0.6496500372886658
translation,153,193,experimental-setup,dropout probability,of,0.1,dropout probability of 0.1,0.5893268585205078
translation,153,193,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,153,47,experiments,our translation models,to generate,high-quality translations,our translation models to generate high-quality translations,0.6666606068611145
translation,153,47,experiments,our translation models,train,cross-lingual image captioning model,our translation models train cross-lingual image captioning model,0.6217970252037048
translation,153,47,experiments,high-quality translations,of,"ms - coco ( chen et al. , 2015 ) and flickr ( hodosh et al. , 2013 ) datasets","high-quality translations of ms - coco ( chen et al. , 2015 ) and flickr ( hodosh et al. , 2013 ) datasets",0.49986162781715393
translation,153,47,experiments,cross-lingual image captioning model,in,multi-task pipeline,cross-lingual image captioning model in multi-task pipeline,0.47796890139579773
translation,153,47,experiments,cross-lingual image captioning model,paired with,machine translation,cross-lingual image captioning model paired with machine translation,0.5624421834945679
translation,153,47,experiments,multi-task pipeline,paired with,machine translation,multi-task pipeline paired with machine translation,0.6167050004005432
translation,153,47,experiments,machine translation,in which,model,machine translation in which model,0.5530120730400085
translation,153,47,experiments,model,initialized by,parameters,model initialized by parameters,0.6332444548606873
translation,153,47,experiments,parameters,from,our translation model,parameters from our translation model,0.5364473462104797
translation,153,4,model,wikipedia,for,neural machine translation,wikipedia for neural machine translation,0.5415629744529724
translation,153,4,model,direct supervision,from,external parallel data,direct supervision from external parallel data,0.59232497215271
translation,153,4,model,direct supervision,from,supervised models,direct supervision from supervised models,0.5705211162567139
translation,153,27,model,seed bilingual dictionary,small collection of,"first sentence pairs , titles and captions","seed bilingual dictionary small collection of first sentence pairs , titles and captions",0.6991411447525024
translation,153,57,model,novel modification,to,annotation projection method,novel modification to annotation projection method,0.4958687126636505
translation,153,57,model,annotation projection method,to be able to leverage,our translation models,annotation projection method to be able to leverage our translation models,0.6163360476493835
translation,153,57,model,model,propose,novel modification,model propose novel modification,0.7071748971939087
translation,153,9,results,captioning results,on,arabic,captioning results on arabic,0.5565959215164185
translation,153,9,results,captioning results,are,slightly better,captioning results are slightly better,0.5321011543273926
translation,153,9,results,arabic,are,slightly better,arabic are slightly better,0.597781777381897
translation,153,9,results,slightly better,than,supervised model,slightly better than supervised model,0.5661569237709045
translation,153,9,results,results,has,captioning results,results has captioning results,0.5752630829811096
translation,153,33,results,arabic captioning,show,bleu score,arabic captioning show bleu score,0.5654797554016113
translation,153,33,results,bleu score,of,5.72,bleu score of 5.72,0.5293395519256592
translation,153,33,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,33,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,33,results,5.72,that is,slightly better,5.72 that is slightly better,0.59462970495224
translation,153,33,results,slightly better,than,supervised captioning model,slightly better than supervised captioning model,0.5422723293304443
translation,153,33,results,supervised captioning model,with,bleu score,supervised captioning model with bleu score,0.5526286363601685
translation,153,33,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,33,results,results,on,arabic captioning,results on arabic captioning,0.5174159407615662
translation,153,46,results,strong unsupervised translation models,for,low-resource languages,strong unsupervised translation models for low-resource languages,0.5591458082199097
translation,153,46,results,strong unsupervised translation models,improve,bleu score,strong unsupervised translation models improve bleu score,0.6136524081230164
translation,153,46,results,bleu score,of,english ? gujarati,bleu score of english ? gujarati,0.5530129671096802
translation,153,46,results,bleu score,of,english ? kazakh,bleu score of english ? kazakh,0.5471658706665039
translation,153,46,results,english ? gujarati,from,0.6 to 15.2,english ? gujarati from 0.6 to 15.2,0.4601280689239502
translation,153,46,results,english ? kazakh,from,0.8 to 12.1,english ? kazakh from 0.8 to 12.1,0.47140491008758545
translation,153,48,results,arabic captioning,show,bleu score,arabic captioning show bleu score,0.5654797554016113
translation,153,48,results,bleu score,of,5.72,bleu score of 5.72,0.5293395519256592
translation,153,48,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,48,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,48,results,5.72,that is,slightly better,5.72 that is slightly better,0.59462970495224
translation,153,48,results,slightly better,than,supervised captioning model,slightly better than supervised captioning model,0.5422723293304443
translation,153,48,results,supervised captioning model,with,bleu score,supervised captioning model with bleu score,0.5526286363601685
translation,153,48,results,bleu score,of,5.22,bleu score of 5.22,0.5367008447647095
translation,153,48,results,results,on,arabic captioning,results on arabic captioning,0.5174159407615662
translation,153,58,results,dependency parsing,performs,better,dependency parsing performs better,0.6462128162384033
translation,153,58,results,better,than,previous work,better than previous work,0.5636769533157349
translation,153,58,results,results,on,dependency parsing,results on dependency parsing,0.5065709948539734
translation,153,210,results,two percent increase,in,performance,two percent increase in performance,0.5703703165054321
translation,153,210,results,performance,by using,projected tags,performance by using projected tags,0.636395275592804
translation,153,210,results,results,observe,two percent increase,results observe two percent increase,0.6406599283218384
translation,153,217,results,strong supervised models,boosted by,backtranslation,strong supervised models boosted by backtranslation,0.6983031630516052
translation,153,217,results,outperform,has,strong supervised models,outperform has strong supervised models,0.5636610984802246
translation,153,217,results,results,In,low-resource settings,results In low-resource settings,0.5232512354850769
translation,153,218,results,our arabic models,achieve,very high performance,our arabic models achieve very high performance,0.6079424023628235
translation,153,218,results,high- resource settings,has,our arabic models,high- resource settings has our arabic models,0.5720542669296265
translation,153,218,results,results,In,high- resource settings,results In high- resource settings,0.5221796035766602
translation,153,223,results,unsupervised translation,performs,very poorly,unsupervised translation performs very poorly,0.5661149621009827
translation,153,223,results,very poorly,compared to,our approach,very poorly compared to our approach,0.6494796872138977
translation,153,223,results,our approach,in,all directions,our approach in all directions,0.5333301424980164
translation,153,223,results,results,has,unsupervised translation,results has unsupervised translation,0.556451678276062
translation,153,228,results,our model,with,translation model,our model with translation model,0.6497141122817993
translation,153,228,results,our model,with,translation,our model with translation,0.6785386800765991
translation,153,228,results,our model,with,translation,our model with translation,0.6785386800765991
translation,153,228,results,multitask it,with,translation,multitask it with translation,0.6560190916061401
translation,153,228,results,multitask it,achieve,much higher performance,multitask it achieve much higher performance,0.6719176769256592
translation,153,228,results,results,initialize,our model,results initialize our model,0.7216920256614685
translation,153,228,results,results,initialize,multitask it,results initialize multitask it,0.7073836922645569
translation,153,229,results,english output,on,test data,english output on test data,0.5328658819198608
translation,153,229,results,test data,to,arabic,test data to arabic,0.6235077977180481
translation,153,229,results,results,translating,english output,results translating english output,0.7473900318145752
translation,153,231,results,fails,to perform,well,fails to perform well,0.6765221953392029
translation,153,231,results,supervised translation,has,fails,supervised translation has fails,0.5966922044754028
translation,153,231,results,results,see that,supervised translation,results see that supervised translation,0.6475921869277954
translation,153,233,results,google translate,is,strong machine translation system,google translate is strong machine translation system,0.530453085899353
translation,153,233,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,153,233,results,outperforms,has,google translate,outperforms has google translate,0.6156994700431824
translation,153,233,results,results,see that,our model,results see that our model,0.6820751428604126
translation,153,234,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,153,234,results,outperforms,has,supervised captioning,outperforms has supervised captioning,0.5703104734420776
translation,153,263,results,our model,performs,very high,our model performs very high,0.6165520548820496
translation,153,263,results,very high,in,romanian,very high in romanian,0.5328359603881836
translation,153,263,results,very high,with,uas,very high with uas,0.7066913843154907
translation,153,263,results,uas,of,74,uas of 74,0.6831387877464294
translation,153,263,results,results,see that,our model,results see that our model,0.6820751428604126
translation,153,264,results,arabic,performs,even better,arabic performs even better,0.6223978996276855
translation,153,264,results,even better,than using,gold -standard parallel data,even better than using gold -standard parallel data,0.6794183850288391
translation,153,264,results,arabic,has,outperforms,arabic has outperforms,0.5812028050422668
translation,153,264,results,outperforms,has,previous work,outperforms has previous work,0.6127970814704895
translation,153,266,results,part- of-speech tags,achieve,roughly 2 percent absolute improvement,part- of-speech tags achieve roughly 2 percent absolute improvement,0.6172201633453369
translation,153,267,results,our final results,on,kazakh,our final results on kazakh,0.59001624584198
translation,153,267,results,our final results,are,significantly higher,our final results are significantly higher,0.6064920425415039
translation,153,267,results,kazakh,are,significantly higher,kazakh are significantly higher,0.5827047228813171
translation,153,267,results,significantly higher,than,gold -standard parallel text ( 7 k sentences ),significantly higher than gold -standard parallel text ( 7 k sentences ),0.5728616118431091
translation,153,267,results,results,has,our final results,results has our final results,0.5594894886016846
translation,154,98,ablation-analysis,additional positive effect,on,bleu improvement,additional positive effect on bleu improvement,0.5518547892570496
translation,154,102,ablation-analysis,backward seqkd,still,beneficial,backward seqkd still beneficial,0.7048006057739258
translation,154,102,ablation-analysis,beneficial,on top of,forward seqkd,beneficial on top of forward seqkd,0.7611706256866455
translation,154,102,ablation-analysis,ablation analysis,has,backward seqkd,ablation analysis has backward seqkd,0.5457858443260193
translation,154,103,ablation-analysis,target translations,by concatenating,d st and d fwd st ( 2 ref training ),target translations by concatenating d st and d fwd st ( 2 ref training ),0.7201588749885559
translation,154,103,ablation-analysis,ablation analysis,augmented,target translations,ablation analysis augmented target translations,0.7576502561569214
translation,154,146,ablation-analysis,translation performance,of,forward seqkd,translation performance of forward seqkd,0.6002064347267151
translation,154,146,ablation-analysis,gains,of,bidirectional seqkd,gains of bidirectional seqkd,0.6451698541641235
translation,154,146,ablation-analysis,conformer encoder,has,significantly boosted,conformer encoder has significantly boosted,0.6109472513198853
translation,154,146,ablation-analysis,significantly boosted,has,translation performance,significantly boosted has translation performance,0.5916069149971008
translation,154,146,ablation-analysis,ablation analysis,found,conformer encoder,ablation analysis found conformer encoder,0.6327180862426758
translation,154,78,experimental-setup,"transformer ( vaswani et al. , 2017 ) architecture",having,12 encoder layers,"transformer ( vaswani et al. , 2017 ) architecture having 12 encoder layers",0.5840646624565125
translation,154,78,experimental-setup,"transformer ( vaswani et al. , 2017 ) architecture",having,six decoder layers,"transformer ( vaswani et al. , 2017 ) architecture having six decoder layers",0.5967052578926086
translation,154,78,experimental-setup,12 encoder layers,following,two cnn blocks,12 encoder layers following two cnn blocks,0.6182945966720581
translation,154,78,experimental-setup,12 encoder layers,following,six decoder layers,12 encoder layers following six decoder layers,0.6045883893966675
translation,154,78,experimental-setup,six decoder layers,for,asr and e2e -st tasks,six decoder layers for asr and e2e -st tasks,0.5838938355445862
translation,154,78,experimental-setup,experimental setup,used,"transformer ( vaswani et al. , 2017 ) architecture","experimental setup used transformer ( vaswani et al. , 2017 ) architecture",0.5828211903572083
translation,154,80,experimental-setup,our models,with,"espnet - st toolkit ( inaguma et al. , 2020 )","our models with espnet - st toolkit ( inaguma et al. , 2020 )",0.5860408544540405
translation,154,80,experimental-setup,experimental setup,built,our models,experimental setup built our models,0.6641656756401062
translation,154,83,experimental-setup,encoder parameters,of,e2e-st model,encoder parameters of e2e-st model,0.5587738156318665
translation,154,83,experimental-setup,experimental setup,initialized,encoder parameters,experimental setup initialized encoder parameters,0.6846902370452881
translation,154,87,experimental-setup,ar models,used,beam width,ar models used beam width,0.6509134769439697
translation,154,87,experimental-setup,beam width,of,4,beam width of 4,0.7052165865898132
translation,154,87,experimental-setup,experimental setup,For,ar models,experimental setup For ar models,0.5993684530258179
translation,154,6,experiments,sequencelevel knowledge distillation ( seqkd ),from,external text - based nmt models,sequencelevel knowledge distillation ( seqkd ) from external text - based nmt models,0.5639042258262634
translation,154,7,model,source language information,propose,backward seqkd,source language information propose backward seqkd,0.635343611240387
translation,154,7,model,backward seqkd,from,target- to-source backward nmt model,backward seqkd from target- to-source backward nmt model,0.5910951495170593
translation,154,7,model,seqkd,from,target- to-source backward nmt model,seqkd from target- to-source backward nmt model,0.5797629952430725
translation,154,7,model,backward seqkd,has,seqkd,backward seqkd has seqkd,0.6212906241416931
translation,154,8,model,bilingual e2e -st model,to predict,paraphrased transcriptions,bilingual e2e -st model to predict paraphrased transcriptions,0.686396062374115
translation,154,8,model,paraphrased transcriptions,as,auxiliary task,paraphrased transcriptions as auxiliary task,0.5081819295883179
translation,154,8,model,auxiliary task,with,single decoder,auxiliary task with single decoder,0.6739648580551147
translation,154,8,model,model,train,bilingual e2e -st model,model train bilingual e2e -st model,0.7113617658615112
translation,154,9,model,paraphrases,generated from,translations,paraphrases generated from translations,0.621687650680542
translation,154,9,model,translations,in,bitext,translations in bitext,0.5505459904670715
translation,154,9,model,model,has,paraphrases,model has paraphrases,0.591317892074585
translation,154,10,model,bidirectional seqkd,in which,seqkd,bidirectional seqkd in which seqkd,0.6143122911453247
translation,154,10,model,seqkd,from,forward and backward nmt models,seqkd from forward and backward nmt models,0.5798007845878601
translation,154,10,model,model,propose,bidirectional seqkd,model propose bidirectional seqkd,0.6932181715965271
translation,154,21,model,seqkd,from,text - based nmt models,seqkd from text - based nmt models,0.6021917462348938
translation,154,21,model,seqkd,to improve,performance,seqkd to improve performance,0.7146300673484802
translation,154,21,model,performance,of,bilingual e2e -st model,performance of bilingual e2e -st model,0.5486961007118225
translation,154,21,model,model,focus on,seqkd,model focus on seqkd,0.690275251865387
translation,154,22,model,source language information,propose,backward se- qkd,source language information propose backward se- qkd,0.6320591568946838
translation,154,22,model,backward se- qkd,targets,paraphrased source transcriptions,backward se- qkd targets paraphrased source transcriptions,0.6121905446052551
translation,154,22,model,paraphrased source transcriptions,generated from,target - to-source backward nmt model,paraphrased source transcriptions generated from target - to-source backward nmt model,0.61733078956604
translation,154,22,model,model,to fully leverage,source language information,model to fully leverage source language information,0.6898527145385742
translation,154,23,model,single st decoder,trained to predict,source and target language text,single st decoder trained to predict source and target language text,0.7595657110214233
translation,154,23,model,source and target language text,as in,multilingual setting,source and target language text as in multilingual setting,0.6232571601867676
translation,154,23,model,model,has,single st decoder,model has single st decoder,0.524931788444519
translation,154,25,model,two nmt models,in,both language directions,two nmt models in both language directions,0.48998764157295227
translation,154,25,model,model,propose,bidirectional seqkd,model propose bidirectional seqkd,0.6932181715965271
translation,154,60,model,bidirectional seqkd,to,autoregressive ( ar ) and non-autoregressive ( nar ) e2e -st models,bidirectional seqkd to autoregressive ( ar ) and non-autoregressive ( nar ) e2e -st models,0.5459907054901123
translation,154,60,model,model,apply,bidirectional seqkd,model apply bidirectional seqkd,0.6678290963172913
translation,154,79,model,mt models,used,six encoder layers,mt models used six encoder layers,0.6092132925987244
translation,154,79,model,model,For,mt models,model For mt models,0.6745392680168152
translation,154,94,results,st decoder,with,forward mt decoder,st decoder with forward mt decoder,0.6575939059257507
translation,154,94,results,forward mt decoder,improved,baseline performance,forward mt decoder improved baseline performance,0.6886256337165833
translation,154,94,results,pre-training,has,st decoder,pre-training has st decoder,0.5859588980674744
translation,154,95,results,joint asr,showed,marginal improvement,joint asr showed marginal improvement,0.7151436805725098
translation,154,95,results,marginal improvement,on,en- de,marginal improvement on en- de,0.6753474473953247
translation,154,95,results,degraded performance,on,en- fr ( a3 ),degraded performance on en- fr ( a3 ),0.5971056222915649
translation,154,95,results,results,has,joint asr,results has joint asr,0.5600928664207458
translation,154,97,results,backward seqkd,showed,small but consistent improvements,backward seqkd showed small but consistent improvements,0.6903772950172424
translation,154,97,results,backward seqkd,as effective as,mt pre-training,backward seqkd as effective as mt pre-training,0.638793408870697
translation,154,97,results,small but consistent improvements,in,both language directions,small but consistent improvements in both language directions,0.5251912474632263
translation,154,97,results,results,has,backward seqkd,results has backward seqkd,0.5909221768379211
translation,154,99,results,forward seqkd,has,significantly improved,forward seqkd has significantly improved,0.5988048911094666
translation,154,99,results,significantly improved,has,performance,significantly improved has performance,0.5962094068527222
translation,154,99,results,results,has,forward seqkd,results has forward seqkd,0.5803638100624084
translation,154,101,results,more effective,than,backward seqkd,more effective than backward seqkd,0.606846034526825
translation,154,101,results,results,has,forward seqkd,results has forward seqkd,0.5803638100624084
translation,154,104,results,2 ref training and backward se-qkd,showed,best result,2 ref training and backward se-qkd showed best result,0.6371341347694397
translation,154,104,results,best result,has,),best result has ),0.6196829676628113
translation,154,104,results,results,combination of,2 ref training and backward se-qkd,results combination of 2 ref training and backward se-qkd,0.6057736277580261
translation,154,110,results,joint asr,improved,performance,joint asr improved performance,0.661926805973053
translation,154,110,results,performance,on,all nar models,performance on all nar models,0.5052897334098816
translation,154,110,results,all nar models,except for,en- fr,all nar models except for en- fr,0.6871496438980103
translation,154,110,results,en- fr,with,number of iterations t = 10,en- fr with number of iterations t = 10,0.6945123672485352
translation,154,110,results,results,has,joint asr,results has joint asr,0.5600928664207458
translation,154,111,results,bidirectional seqkd,with,d bidir st,bidirectional seqkd with d bidir st,0.7006744742393494
translation,154,111,results,d bidir st,improved,performance,d bidir st improved performance,0.7736121416091919
translation,154,111,results,results,has,bidirectional seqkd,results has bidirectional seqkd,0.5415188074111938
translation,154,145,results,forward seqkd,in,both language directions,forward seqkd in both language directions,0.5249401926994324
translation,154,145,results,bidirectional se-qkd,has,always outperformed,bidirectional se-qkd has always outperformed,0.6130572557449341
translation,154,145,results,always outperformed,has,forward seqkd,always outperformed has forward seqkd,0.6247693300247192
translation,154,145,results,results,confirmed,bidirectional se-qkd,results confirmed bidirectional se-qkd,0.5591087937355042
translation,155,92,experiments,nmt models,with,"sockeye 2 ( domhan et al. , 2020 )","nmt models with sockeye 2 ( domhan et al. , 2020 )",0.5935977697372437
translation,155,90,hyperparameters,"sentencepiece ( kudo , 2018 )",with,subword regularization,"sentencepiece ( kudo , 2018 ) with subword regularization",0.6043296456336975
translation,155,90,hyperparameters,subword regularization,as,only preprocessing step,subword regularization as only preprocessing step,0.4854295551776886
translation,155,90,hyperparameters,hyperparameters,use,"sentencepiece ( kudo , 2018 )","hyperparameters use sentencepiece ( kudo , 2018 )",0.581449568271637
translation,155,97,hyperparameters,beam search,use,beam size,beam search use beam size,0.6815485954284668
translation,155,97,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
translation,155,97,hyperparameters,hyperparameters,For,beam search,hyperparameters For beam search,0.6096653342247009
translation,155,119,results,beam search,underestimates,true length of translations,beam search underestimates true length of translations,0.7647510170936584
translation,155,119,results,mean length of translations,has,beam search,mean length of translations has beam search,0.573603093624115
translation,155,139,results,mbr,approaches but does not outperform,beam search,mbr approaches but does not outperform beam search,0.6802369356155396
translation,155,139,results,beam search,on,our in - domain data,beam search on our in - domain data,0.5900458097457886
translation,155,139,results,number of samples,has,grows,number of samples has grows,0.5946211814880371
translation,155,139,results,number of samples,has,mbr,number of samples has mbr,0.5869655609130859
translation,155,139,results,grows,has,mbr,grows has mbr,0.6302852034568787
translation,155,139,results,results,as,number of samples,results as number of samples,0.5728999972343445
translation,155,143,results,beam search,on,2 out of 4 unknown test domains,beam search on 2 out of 4 unknown test domains,0.5397182703018188
translation,155,143,results,outperforms,has,beam search,outperforms has beam search,0.6323174238204956
translation,155,148,results,higher domain robustness,than,beam search,higher domain robustness than beam search,0.5939055681228638
translation,155,148,results,mbr decoding,has,higher domain robustness,mbr decoding has higher domain robustness,0.5395144820213318
translation,155,148,results,results,find that,mbr decoding,results find that mbr decoding,0.621071457862854
translation,155,155,results,mbr,assigns,much lower utility,mbr assigns much lower utility,0.6339468359947205
translation,155,155,results,much lower utility,to,copy hypotheses,much lower utility to copy hypotheses,0.5765962600708008
translation,155,155,results,copy hypotheses,than to,all hypotheses taken together,copy hypotheses than to all hypotheses taken together,0.6102558970451355
translation,155,155,results,results,show,mbr,results show mbr,0.6652852296829224
translation,156,30,baselines,direct approach,relies on,single neural network,direct approach relies on single neural network,0.7546579837799072
translation,156,30,baselines,single neural network,maps,audio,single neural network maps audio,0.7422882318496704
translation,156,30,baselines,audio,into,target language text,audio into target language text,0.5774396657943726
translation,156,30,baselines,audio,bypassing,intermediate symbolic representation,audio bypassing intermediate symbolic representation,0.6856438517570496
translation,156,126,hyperparameters,label - smoothed cross entropy,with,adam,label - smoothed cross entropy with adam,0.6389853358268738
translation,156,126,hyperparameters,", 2016 )",with,adam,", 2016 ) with adam",0.6530265808105469
translation,156,126,hyperparameters,adam,with,learning rate,adam with learning rate,0.6478732824325562
translation,156,126,hyperparameters,linearly increases,for,"8,000 updates","linearly increases for 8,000 updates",0.6361255049705505
translation,156,126,hyperparameters,"8,000 updates",up to,0.0005,"8,000 updates up to 0.0005",0.6603631377220154
translation,156,126,hyperparameters,decays,with,inverse square root policy,decays with inverse square root policy,0.7195477485656738
translation,156,126,hyperparameters,label - smoothed cross entropy,has,", 2016 )","label - smoothed cross entropy has , 2016 )",0.5445864796638489
translation,156,126,hyperparameters,hyperparameters,optimized on,label - smoothed cross entropy,hyperparameters optimized on label - smoothed cross entropy,0.665756106376648
translation,156,127,hyperparameters,batch,composed of,4 mini-batches,batch composed of 4 mini-batches,0.7366037964820862
translation,156,127,hyperparameters,4 mini-batches,made of,3072 tokens,4 mini-batches made of 3072 tokens,0.6633532643318176
translation,156,127,hyperparameters,hyperparameters,has,batch,hyperparameters has batch,0.5509300827980042
translation,156,128,hyperparameters,dropout,set to,0.3,dropout set to 0.3,0.6373292803764343
translation,156,128,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,156,129,hyperparameters,"200,000 updates",average,last 10 checkpoints,"200,000 updates average last 10 checkpoints",0.7521510124206543
translation,156,129,hyperparameters,hyperparameters,train for,"200,000 updates","hyperparameters train for 200,000 updates",0.7173264622688293
translation,156,130,hyperparameters,source and target languages,share,"bpe ( sennrich et al. , 2016 ) vocabulary","source and target languages share bpe ( sennrich et al. , 2016 ) vocabulary",0.6345701217651367
translation,156,130,hyperparameters,"bpe ( sennrich et al. , 2016 ) vocabulary",of,32 k sub-words,"bpe ( sennrich et al. , 2016 ) vocabulary of 32 k sub-words",0.5588077306747437
translation,156,130,hyperparameters,hyperparameters,has,source and target languages,hyperparameters has source and target languages,0.4973228871822357
translation,156,7,model,- of - the - art st systems,in translating,nes and terminology,- of - the - art st systems in translating nes and terminology,0.6288553476333618
translation,156,7,model,european parliament speeches,annotated with,nes and terminology,european parliament speeches annotated with nes and terminology,0.7237228155136108
translation,156,7,model,model,release,neuroparl - st,model release neuroparl - st,0.733866274356842
translation,156,35,model,cascade system,integrates,"competitive transformer - based ( vaswani et al. , 2017 ) asr","cascade system integrates competitive transformer - based ( vaswani et al. , 2017 ) asr",0.67460697889328
translation,156,35,model,cascade system,integrates,mt components,cascade system integrates mt components,0.7342022061347961
translation,156,35,model,mt components,built from,large training corpora,mt components built from large training corpora,0.6223238110542297
translation,156,35,model,model,has,cascade system,model has cascade system,0.605209469795227
translation,156,90,results,transcribing nes,more difficult than,transcribing terms,transcribing nes more difficult than transcribing terms,0.776498019695282
translation,156,90,results,transcribing terms,has,vs 92.4,transcribing terms has vs 92.4,0.5404927730560303
translation,156,90,results,results,In terms of,accuracy,results In terms of accuracy,0.7184101343154907
translation,156,95,results,ne and term translation quality,notice,nes,ne and term translation quality notice nes,0.6821685433387756
translation,156,95,results,nes,are,harder to handle,nes are harder to handle,0.608991265296936
translation,156,95,results,harder to handle,compared to,terminology,harder to handle compared to terminology,0.6364372372627258
translation,156,95,results,results,Analyzing,ne and term translation quality,results Analyzing ne and term translation quality,0.5322328805923462
translation,156,102,results,large drops,in,all languages,large drops in all languages,0.5394489765167236
translation,156,102,results,translation quality,has,- 13.2 bleu on average,translation quality has - 13.2 bleu on average,0.5533202886581421
translation,156,102,results,ne / term accuracy,has,- 12.8/-6.0 ),ne / term accuracy has - 12.8/-6.0 ),0.5661101341247559
translation,156,103,results,bleu scores,are,on par,bleu scores are on par,0.572224497795105
translation,156,103,results,on par,for,en-es and en-it,on par for en-es and en-it,0.7903041839599609
translation,156,103,results,direct one,is,significantly better,direct one is significantly better,0.5821191668510437
translation,156,103,results,significantly better,for,en-fr,significantly better for en-fr,0.6934424042701721
translation,156,103,results,cascade and direct models,has,bleu scores,cascade and direct models has bleu scores,0.5249658226966858
translation,156,103,results,results,Comparing,cascade and direct models,results Comparing cascade and direct models,0.6191633343696594
translation,156,105,results,mt model,of,cascade,mt model of cascade,0.6195850372314453
translation,156,105,results,mt model,trained on,massive corpora,mt model trained on massive corpora,0.6772564053535461
translation,156,105,results,mt model,tends to produce,translations,mt model tends to produce translations,0.7488171458244324
translation,156,105,results,massive corpora,including,europarl,massive corpora including europarl,0.6588823795318604
translation,156,105,results,massive corpora,including,europarl - st references,massive corpora including europarl - st references,0.6585835814476013
translation,156,105,results,massive corpora,tends to produce,translations,massive corpora tends to produce translations,0.6939905881881714
translation,156,105,results,translations,shorter than,europarl - st references,translations shorter than europarl - st references,0.7012869119644165
translation,156,105,results,similar in length,to,transcripts,similar in length to transcripts,0.586458146572113
translation,156,105,results,results,has,mt model,results has mt model,0.5513349771499634
translation,156,106,results,europarl -st,among,training corpora,europarl -st among training corpora,0.5668495297431946
translation,156,106,results,direct model,produces,outputs,direct model produces outputs,0.6570888757705688
translation,156,106,results,more similar in length,to,references,more similar in length to references,0.5036681890487671
translation,156,106,results,more similar in length,resulting in,2.8 bleu gain,more similar in length resulting in 2.8 bleu gain,0.642794668674469
translation,156,106,results,europarl -st,has,direct model,europarl -st has direct model,0.6001882553100586
translation,156,106,results,training corpora,has,direct model,training corpora has direct model,0.5640884637832642
translation,156,106,results,outputs,has,more similar in length,outputs has more similar in length,0.5957397818565369
translation,156,106,results,results,Having,europarl -st,results Having europarl -st,0.5622634887695312
translation,156,107,results,trend,is,clear and coherent,trend is clear and coherent,0.6210483908653259
translation,156,107,results,clear and coherent,on,all languages,clear and coherent on all languages,0.5364295244216919
translation,156,107,results,direct,on,terminology ( + 3.5 on average,direct on terminology ( + 3.5 on average,0.5125411152839661
translation,156,107,results,edge ( + 0.5 ),in handling,nes,edge ( + 0.5 ) in handling nes,0.7823403477668762
translation,156,107,results,ne and term translation quality,has,trend,ne and term translation quality has trend,0.5568848252296448
translation,156,107,results,ne and term translation quality,has,cascade,ne and term translation quality has cascade,0.5662872791290283
translation,156,107,results,all languages,has,cascade,all languages has cascade,0.5859917998313904
translation,156,107,results,cascade,has,outperforms,cascade has outperforms,0.5958935022354126
translation,156,107,results,outperforms,has,direct,outperforms has direct,0.6686431169509888
translation,156,107,results,outperforms,has,direct,outperforms has direct,0.6686431169509888
translation,156,107,results,outperforms,has,edge ( + 0.5 ),outperforms has edge ( + 0.5 ),0.6289616823196411
translation,156,107,results,direct,has,edge ( + 0.5 ),direct has edge ( + 0.5 ),0.5413460731506348
translation,156,107,results,results,In terms of,ne and term translation quality,results In terms of ne and term translation quality,0.643390417098999
translation,156,111,results,ne types,has,two st systems,ne types has two st systems,0.6339206695556641
translation,156,111,results,results,Looking at,ne types,results Looking at ne types,0.5601463913917542
translation,157,80,experimental-setup,data set,preprocessed with,two -level tokenization,data set preprocessed with two -level tokenization,0.7380257248878479
translation,157,80,experimental-setup,standard tokenization ( spacy ),segments,data,standard tokenization ( spacy ) segments data,0.7743424773216248
translation,157,80,experimental-setup,data,into,words,data into words,0.5323816537857056
translation,157,80,experimental-setup,"bpe tokenization ( sentencepiece ( kudo , 2018 )",into,sub-words,"bpe tokenization ( sentencepiece ( kudo , 2018 ) into sub-words",0.5374255180358887
translation,157,80,experimental-setup,experimental setup,has,data set,experimental setup has data set,0.5154800415039062
translation,157,81,experimental-setup,32,",",000,"32 , 000",0.7067277431488037
translation,157,81,experimental-setup,vocabulary size,has,32,vocabulary size has 32,0.6219449639320374
translation,157,81,experimental-setup,maximum sentence length,has,50,maximum sentence length has 50,0.6109430193901062
translation,157,81,experimental-setup,maximum output length,has,100,maximum output length has 100,0.6064174175262451
translation,157,81,experimental-setup,normalization,has,tokens,normalization has tokens,0.6184198260307312
translation,157,81,experimental-setup,encoder embedding dimension,has,512,encoder embedding dimension has 512,0.5845873951911926
translation,157,81,experimental-setup,decoder embedding dimension,has,512,decoder embedding dimension has 512,0.5888720750808716
translation,157,81,experimental-setup,hidden size,has,512,hidden size has 512,0.6254094243049622
translation,157,92,experimental-setup,vocabulary size,has,"31,000","vocabulary size has 31,000",0.5889498591423035
translation,157,92,experimental-setup,learning optimizer,has,lazyadam,learning optimizer has lazyadam,0.5454347133636475
translation,158,18,ablation-analysis,student model 's efficiency,by removing,unimportant components,student model 's efficiency by removing unimportant components,0.7246378064155579
translation,158,18,ablation-analysis,unimportant components,including,ffn sub-layers,unimportant components including ffn sub-layers,0.6765768527984619
translation,158,18,ablation-analysis,unimportant components,including,multi-head mechanism,unimportant components including multi-head mechanism,0.7263690829277039
translation,158,18,ablation-analysis,ablation analysis,improve,student model 's efficiency,ablation analysis improve student model 's efficiency,0.6698154211044312
translation,158,88,ablation-analysis,multi-head mechanism,in,student models,multi-head mechanism in student models,0.5709284543991089
translation,158,88,ablation-analysis,throughput,without,performance loss,throughput without performance loss,0.6913666129112244
translation,158,88,ablation-analysis,student models,has,significantly improves,student models has significantly improves,0.6348589658737183
translation,158,88,ablation-analysis,significantly improves,has,throughput,significantly improves has throughput,0.5810167789459229
translation,158,88,ablation-analysis,ablation analysis,removing,multi-head mechanism,ablation analysis removing multi-head mechanism,0.7095347046852112
translation,158,147,ablation-analysis,gpu system,with,student - 6 - 1 - 512 model,gpu system with student - 6 - 1 - 512 model,0.6688074469566345
translation,158,147,ablation-analysis,gpu system,reduces,translation time,gpu system reduces translation time,0.656147301197052
translation,158,147,ablation-analysis,translation time,by,onefour,translation time by onefour,0.6133891344070435
translation,158,147,ablation-analysis,onefour,with,only six encoder layers,onefour with only six encoder layers,0.6436732411384583
translation,158,147,ablation-analysis,ablation analysis,reduce,number of encoder layers,ablation analysis reduce number of encoder layers,0.6686577796936035
translation,158,147,ablation-analysis,ablation analysis,reduce,gpu system,ablation analysis reduce gpu system,0.7016226053237915
translation,158,30,experimental-setup,number of encoder layers,is,40,number of encoder layers is 40,0.5798505544662476
translation,158,30,experimental-setup,experimental setup,has,number of encoder layers,experimental setup has number of encoder layers,0.513801097869873
translation,158,31,experimental-setup,relative position representation ( rpr ),to further improve,teacher models,relative position representation ( rpr ) to further improve teacher models,0.6953496932983398
translation,158,31,experimental-setup,relative position representation ( rpr ),set,key 's relative length,relative position representation ( rpr ) set key 's relative length,0.6443685293197632
translation,158,31,experimental-setup,key 's relative length,to,8,key 's relative length to 8,0.5911737084388733
translation,158,31,experimental-setup,experimental setup,adopt,relative position representation ( rpr ),experimental setup adopt relative position representation ( rpr ),0.6300537586212158
translation,158,54,experimental-setup,data,tokenized with,moses tokenizer,data tokenized with moses tokenizer,0.8274663090705872
translation,158,54,experimental-setup,data,tokenized with,32 k merge operations,data tokenized with 32 k merge operations,0.7673945426940918
translation,158,54,experimental-setup,encoded ( bpe ),with,32 k merge operations,encoded ( bpe ) with 32 k merge operations,0.6464998722076416
translation,158,54,experimental-setup,32 k merge operations,using,shared vocabulary,32 k merge operations using shared vocabulary,0.6480233669281006
translation,158,54,experimental-setup,moses tokenizer,has,encoded ( bpe ),moses tokenizer has encoded ( bpe ),0.6188028454780579
translation,158,54,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,158,59,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.997 and = 10 ?8","adam optimizer with ? 1 = 0.9 , ? 2 = 0.997 and = 10 ?8",0.6263731718063354
translation,158,59,experimental-setup,adam optimizer,with,gradient accumulation,adam optimizer with gradient accumulation,0.6063017249107361
translation,158,59,experimental-setup,gradient accumulation,due to,high gpu memory footprints,gradient accumulation due to high gpu memory footprints,0.6341981291770935
translation,158,59,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,158,60,experimental-setup,8 titan v gpus,for,up to 11 epochs,8 titan v gpus for up to 11 epochs,0.5586233735084534
translation,158,61,experimental-setup,decayed,based on,inverse square root,decayed based on inverse square root,0.7068706750869751
translation,158,61,experimental-setup,inverse square root,of,update number,inverse square root of update number,0.6249436140060425
translation,158,61,experimental-setup,update number,after,"1,6000 warm - up steps","update number after 1,6000 warm - up steps",0.6922006011009216
translation,158,61,experimental-setup,maximum learning rate,is,0.002,maximum learning rate is 0.002,0.5424801707267761
translation,158,61,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,158,61,experimental-setup,experimental setup,has,maximum learning rate,experimental setup has maximum learning rate,0.5034512877464294
translation,158,69,experimental-setup,"cutoff method ( shen et al. , 2020 )",to boost,our student models,"cutoff method ( shen et al. , 2020 ) to boost our student models",0.7134435772895813
translation,158,69,experimental-setup,"cutoff method ( shen et al. , 2020 )",train,student model,"cutoff method ( shen et al. , 2020 ) train student model",0.6938203573226929
translation,158,69,experimental-setup,student model,with,21 epochs,student model with 21 epochs,0.6989525556564331
translation,158,69,experimental-setup,experimental setup,use,"cutoff method ( shen et al. , 2020 )","experimental setup use cutoff method ( shen et al. , 2020 )",0.6118091344833374
translation,158,69,experimental-setup,experimental setup,train,student model,experimental setup train student model,0.68052738904953
translation,158,102,experimental-setup,all the weight matrices,to avoid,repeated operation,all the weight matrices to avoid repeated operation,0.6661946773529053
translation,158,102,experimental-setup,repeated operation,during,inference,repeated operation during inference,0.719081699848175
translation,158,102,experimental-setup,experimental setup,pre-quantize and pre-pack,all the weight matrices,experimental setup pre-quantize and pre-pack all the weight matrices,0.7318066358566284
translation,158,130,experimental-setup,one mkl thread,for,each process,one mkl thread for each process,0.6074426174163818
translation,158,130,experimental-setup,one mkl thread,set,sbatch / wbatch,one mkl thread set sbatch / wbatch,0.6553933024406433
translation,158,130,experimental-setup,sbatch / wbatch,to,128/2048,sbatch / wbatch to 128/2048,0.5608011484146118
translation,158,130,experimental-setup,experimental setup,use,one mkl thread,experimental setup use one mkl thread,0.6171197891235352
translation,158,130,experimental-setup,experimental setup,set,sbatch / wbatch,experimental setup set sbatch / wbatch,0.6865981817245483
translation,158,135,experimental-setup,fastbpe,to speed,bpe process,fastbpe to speed bpe process,0.6870272159576416
translation,158,135,experimental-setup,experimental setup,use,fastbpe,experimental setup use fastbpe,0.6201204061508179
translation,158,143,experimental-setup,gpu systems,compiled with,cuda,gpu systems compiled with cuda,0.6495974063873291
translation,158,143,experimental-setup,gpu track submissions,has,gpu systems,gpu track submissions has gpu systems,0.5432780981063843
translation,158,143,experimental-setup,experimental setup,For,gpu track submissions,experimental setup For gpu track submissions,0.560010552406311
translation,158,144,experimental-setup,number of our decoder attention head,to,1,number of our decoder attention head to 1,0.5654239654541016
translation,158,144,experimental-setup,1,for,gpu systems,1 for gpu systems,0.5311182737350464
translation,158,144,experimental-setup,experimental setup,set,num-ber of decoder layers,experimental setup set num-ber of decoder layers,0.6116962432861328
translation,158,144,experimental-setup,experimental setup,set,number of our decoder attention head,experimental setup set number of our decoder attention head,0.6130136847496033
translation,158,144,experimental-setup,experimental setup,for,gpu systems,experimental setup for gpu systems,0.574461042881012
translation,158,150,experimental-setup,experimental setup,compiled in,11.2.1 - devel-centos7 docker image,experimental setup compiled in 11.2.1 - devel-centos7 docker image,0.6278833746910095
translation,158,153,experimental-setup,cpu track submissions,use,test machine,cpu track submissions use test machine,0.6562803387641907
translation,158,153,experimental-setup,experimental setup,For,cpu track submissions,experimental setup For cpu track submissions,0.577043890953064
translation,158,154,experimental-setup,cpu version,compiled with,mkl static library,cpu version compiled with mkl static library,0.6622455716133118
translation,158,154,experimental-setup,executable file,is,23 mib,executable file is 23 mib,0.5997529029846191
translation,158,154,experimental-setup,experimental setup,has,cpu version,experimental setup has cpu version,0.5239957571029663
translation,158,155,experimental-setup,8 bit matrix multiplication,with,packing,8 bit matrix multiplication with packing,0.6760114431381226
translation,158,155,experimental-setup,packing,to speed,matrix multiplication,packing to speed matrix multiplication,0.6438983082771301
translation,158,155,experimental-setup,matrix multiplication,in,network,matrix multiplication in network,0.5541373491287231
translation,158,155,experimental-setup,experimental setup,use,8 bit matrix multiplication,experimental setup use 8 bit matrix multiplication,0.595488965511322
translation,158,157,experimental-setup,cpu docker images,use,base- centos7 docker image,cpu docker images use base- centos7 docker image,0.6865993738174438
translation,158,157,experimental-setup,base- centos7 docker image,to deploy,cpu mt systems,base- centos7 docker image to deploy cpu mt systems,0.7423691153526306
translation,158,157,experimental-setup,experimental setup,For,cpu docker images,experimental setup For cpu docker images,0.5911514163017273
translation,158,78,experiments,gpu - based decoding,mainly explore,dynamic batching,gpu - based decoding mainly explore dynamic batching,0.6892672777175903
translation,158,78,experiments,gpu - based decoding,mainly explore,fp16 inference,gpu - based decoding mainly explore fp16 inference,0.6932406425476074
translation,158,129,experiments,number of processes,managed by,parallel tool,number of processes managed by parallel tool,0.6584987640380859
translation,158,129,experiments,parallel tool,is,more efficient and accurate,parallel tool is more efficient and accurate,0.5399936437606812
translation,158,129,experiments,cpu systems,has,number of processes,cpu systems has number of processes,0.5724438428878784
translation,158,132,experiments,our systems,insensitive to,beam size,our systems insensitive to beam size,0.7119625806808472
translation,158,156,experiments,student -3 - 1- 512 and student - 6 - 1- 512 models,in,cpu systems,student -3 - 1- 512 and student - 6 - 1- 512 models in cpu systems,0.5466652512550354
translation,158,156,experiments,student -3 - 1- 512 and student - 6 - 1- 512 models,achieve,31.5 and 32.8 bleu,student -3 - 1- 512 and student - 6 - 1- 512 models achieve 31.5 and 32.8 bleu,0.6172803640365601
translation,158,156,experiments,31.5 and 32.8 bleu,on,newstest20,31.5 and 32.8 bleu on newstest20,0.5581977963447571
translation,158,13,model,model,investigates,efficient transformers architectures and optimizations,model investigates efficient transformers architectures and optimizations,0.6330563426017761
translation,158,14,model,deep encoder and shallow decoder transformer models,optimize them for,gpus and cpus,deep encoder and shallow decoder transformer models optimize them for gpus and cpus,0.7042787075042725
translation,158,14,model,model,study,deep encoder and shallow decoder transformer models,model study deep encoder and shallow decoder transformer models,0.5492790341377258
translation,158,19,model,model- agnostic optimizations,including,graph optimization,model- agnostic optimizations including graph optimization,0.6612767577171326
translation,158,19,model,model- agnostic optimizations,including,dynamic batching,model- agnostic optimizations including dynamic batching,0.7076095342636108
translation,158,19,model,model- agnostic optimizations,including,parallel pre / postprocessing,model- agnostic optimizations including parallel pre / postprocessing,0.72855544090271
translation,158,19,model,model- agnostic optimizations,including,8 - bit matrix multiplication,model- agnostic optimizations including 8 - bit matrix multiplication,0.7027513980865479
translation,158,19,model,model- agnostic optimizations,including,16 - bit computation,model- agnostic optimizations including 16 - bit computation,0.6954730153083801
translation,158,19,model,8 - bit matrix multiplication,on,cpus,8 - bit matrix multiplication on cpus,0.5014346241950989
translation,158,19,model,16 - bit computation,on,gpus,16 - bit computation on gpus,0.5010012984275818
translation,158,55,model,decoding,remove,bpe separators,decoding remove bpe separators,0.6673131585121155
translation,158,55,model,decoding,detokenize,all tokens,decoding detokenize all tokens,0.7257184982299805
translation,158,55,model,all tokens,with,moses detokenizer,all tokens with moses detokenizer,0.6366695165634155
translation,158,55,model,model,After,decoding,model After decoding,0.7522495985031128
translation,158,58,model,source-side and target -side embeddings,with,decoder output weights,source-side and target -side embeddings with decoder output weights,0.6103329658508301
translation,158,58,model,model,share,source-side and target -side embeddings,model share source-side and target -side embeddings,0.6213755011558533
translation,158,81,model,dynamic batching scheme,maximizes,number of sentences,dynamic batching scheme maximizes number of sentences,0.730452835559845
translation,158,81,model,number of sentences,in,batch,number of sentences in batch,0.5485188364982605
translation,158,81,model,batch,while limiting,number of tokens,batch while limiting number of tokens,0.6679596304893494
translation,158,81,model,model,implement,dynamic batching scheme,model implement dynamic batching scheme,0.6417921185493469
translation,158,116,model,memory allocation or movement,with,efficient memory pool,memory allocation or movement with efficient memory pool,0.6160673499107361
translation,158,116,model,model,reduce,memory allocation or movement,model reduce memory allocation or movement,0.7514132857322693
translation,158,121,model,decoding,on,cpus,decoding on cpus,0.5451861619949341
translation,158,121,model,model,to accelerate,pre-processing,model to accelerate pre-processing,0.696997880935669
translation,158,121,model,model,to accelerate,post-processing,model to accelerate post-processing,0.7195050120353699
translation,158,6,results,translation efficiency,with,graph optimization,translation efficiency with graph optimization,0.6022850871086121
translation,158,6,results,translation efficiency,with,low precision,translation efficiency with low precision,0.6012730598449707
translation,158,6,results,translation efficiency,with,dynamic batching,translation efficiency with dynamic batching,0.6373509168624878
translation,158,6,results,translation efficiency,with,parallel pre / postprocessing,translation efficiency with parallel pre / postprocessing,0.6394588947296143
translation,158,6,results,results,improve,translation efficiency,results improve translation efficiency,0.587385356426239
translation,158,17,results,shallow decoder ( 1 layer ),gives,reasonable improvements,shallow decoder ( 1 layer ) gives reasonable improvements,0.5892722010612488
translation,158,17,results,reasonable improvements,in,speed,reasonable improvements in speed,0.5266779065132141
translation,158,17,results,reasonable improvements,while maintaining,high translation quality,reasonable improvements while maintaining high translation quality,0.6091887950897217
translation,158,17,results,results,find,shallow decoder ( 1 layer ),results find shallow decoder ( 1 layer ),0.5652672648429871
translation,158,17,results,results,using,deep encoder ( 6 layers ),results using deep encoder ( 6 layers ),0.6234637498855591
translation,158,17,results,results,using,shallow decoder ( 1 layer ),results using shallow decoder ( 1 layer ),0.6283701658248901
translation,158,47,results,transformer student model,with,one decoder layer and one decoder attention head,transformer student model with one decoder layer and one decoder attention head,0.623299241065979
translation,158,47,results,one decoder layer and one decoder attention head,can achieve,similar translation quality,one decoder layer and one decoder attention head can achieve similar translation quality,0.6778267025947571
translation,158,47,results,similar translation quality,to,baseline,similar translation quality to baseline,0.5044426918029785
translation,158,47,results,results,shows,transformer student model,results shows transformer student model,0.7022812962532043
translation,158,66,results,final ensemble teacher model,achieve,bleu score,final ensemble teacher model achieve bleu score,0.5699335932731628
translation,158,66,results,bleu score,of,33.4,bleu score of 33.4,0.5521994829177856
translation,158,66,results,33.4,on,newstest20,33.4 on newstest20,0.5603683590888977
translation,158,66,results,results,has,final ensemble teacher model,results has final ensemble teacher model,0.5601562261581421
translation,158,71,results,our student model,yields,significant speedup ( 2 ?- 2.6 ? ),our student model yields significant speedup ( 2 ?- 2.6 ? ),0.6999782919883728
translation,158,71,results,significant speedup ( 2 ?- 2.6 ? ),with,modest sacrifice,significant speedup ( 2 ?- 2.6 ? ) with modest sacrifice,0.6212102770805359
translation,158,71,results,modest sacrifice,in terms of,bleu,modest sacrifice in terms of bleu,0.7371826171875
translation,158,71,results,bleu,has,0.2-0.9 on newstes t 20,bleu has 0.2-0.9 on newstes t 20,0.6068487763404846
translation,158,71,results,results,has,our student model,results has our student model,0.5864686965942383
translation,158,138,results,significantly speed up,without losing,bleu,significantly speed up without losing bleu,0.7297314405441284
translation,158,138,results,our system,without losing,bleu,our system without losing bleu,0.6089183688163757
translation,158,138,results,our optimizations,has,significantly speed up,our optimizations has significantly speed up,0.5644806027412415
translation,158,138,results,significantly speed up,has,our system,significantly speed up has our system,0.586053729057312
translation,158,138,results,results,show,our optimizations,results show our optimizations,0.6537056565284729
translation,158,139,results,interesting,about,bleu,interesting about bleu,0.6068273782730103
translation,158,139,results,bleu,achieve,additional improvements,bleu achieve additional improvements,0.6215541362762451
translation,158,139,results,additional improvements,of,0.4/0.1 bleu points,additional improvements of 0.4/0.1 bleu points,0.5363128185272217
translation,158,139,results,0.4/0.1 bleu points,on,gpu / cpu,0.4/0.1 bleu points on gpu / cpu,0.5294116735458374
translation,158,139,results,gpu / cpu,through,decoding optimizations,gpu / cpu through decoding optimizations,0.6802722811698914
translation,158,140,results,other models,after,decoding optimizations,other models after decoding optimizations,0.7169054746627808
translation,158,140,results,results,measure,other models,results measure other models,0.6939221620559692
translation,158,145,results,speedup,of,more than 6 ?,speedup of more than 6 ?,0.6533933281898499
translation,158,145,results,more than 6 ?,on,gpu system,more than 6 ? on gpu system,0.5648669600486755
translation,158,145,results,gpu system,created by,student - 12 - 1- 512 model,gpu system created by student - 12 - 1- 512 model,0.6397325992584229
translation,158,145,results,slight decrease,of,only 0.2 bleu,slight decrease of only 0.2 bleu,0.579677402973175
translation,158,145,results,only 0.2 bleu,on,newstest20,only 0.2 bleu on newstest20,0.5304341316223145
translation,158,145,results,only 0.2 bleu,compared to,deep ensemble model,only 0.2 bleu compared to deep ensemble model,0.5936411619186401
translation,158,145,results,results,see,speedup,results see speedup,0.6613780856132507
translation,158,148,results,fastest gpu system,consists of,three encoder layers,fastest gpu system consists of three encoder layers,0.6024645566940308
translation,158,148,results,fastest gpu system,consists of,one decoder layer,fastest gpu system consists of one decoder layer,0.6043833494186401
translation,158,148,results,fastest gpu system,consists of,1.6 ? speedup,fastest gpu system consists of 1.6 ? speedup,0.5692961812019348
translation,158,148,results,fastest gpu system,achieves,31.5 bleu,fastest gpu system achieves 31.5 bleu,0.630416750907898
translation,158,148,results,fastest gpu system,achieves,1.6 ? speedup,fastest gpu system achieves 1.6 ? speedup,0.6207998991012573
translation,158,148,results,31.5 bleu,on,newstest20,31.5 bleu on newstest20,0.5321090221405029
translation,158,148,results,newstest20,with,gpu,newstest20 with gpu,0.6658316254615784
translation,158,148,results,1.6 ? speedup,compared to,base-gpu -system,1.6 ? speedup compared to base-gpu -system,0.6452219486236572
translation,158,148,results,results,has,fastest gpu system,results has fastest gpu system,0.5447758436203003
translation,158,149,results,student - 6 - 1 - 0 model,to create,gpu system,student - 6 - 1 - 0 model to create gpu system,0.7002915740013123
translation,158,149,results,student - 6 - 1 - 0 model,can achieve,1.3 ? speedup,student - 6 - 1 - 0 model can achieve 1.3 ? speedup,0.7210254073143005
translation,158,149,results,1.3 ? speedup,compared to,base-gpu -system,1.3 ? speedup compared to base-gpu -system,0.6508620381355286
translation,158,149,results,results,employ,student - 6 - 1 - 0 model,results employ student - 6 - 1 - 0 model,0.6038039326667786
translation,158,161,results,our systems,for,gpu - throughput track,our systems for gpu - throughput track,0.6089964509010315
translation,158,161,results,gpu - throughput track,are,fastest overall submissions,gpu - throughput track are fastest overall submissions,0.5393058657646179
translation,158,161,results,results,has,our systems,results has our systems,0.5982377529144287
translation,158,162,results,student -3 - 1- 512 system,translate about,250 thousand words,student -3 - 1- 512 system translate about 250 thousand words,0.7461028099060059
translation,158,162,results,student -3 - 1- 512 system,achieve,25.5 bleu,student -3 - 1- 512 system achieve 25.5 bleu,0.6136878728866577
translation,158,162,results,25.5 bleu,on,newstest21,25.5 bleu on newstest21,0.5260546803474426
translation,158,162,results,250 thousand words,has,per second,250 thousand words has per second,0.5720564126968384
translation,158,162,results,results,has,student -3 - 1- 512 system,results has student -3 - 1- 512 system,0.6121556162834167
translation,158,164,results,cpu track,has,our system,cpu track has our system,0.582483172416687
translation,158,164,results,results,In,cpu track,results In cpu track,0.5560439229011536
translation,158,165,results,fastest cpu system,created by,student - 3 - 1- 512 model,fastest cpu system created by student - 3 - 1- 512 model,0.6293951869010925
translation,158,165,results,student - 3 - 1- 512 model,translate about,48 thousand words,student - 3 - 1- 512 model translate about 48 thousand words,0.7479926943778992
translation,158,165,results,48 thousand words,via,36 cpu cores,48 thousand words via 36 cpu cores,0.5500329732894897
translation,158,165,results,results,has,fastest cpu system,results has fastest cpu system,0.5401608347892761
translation,158,166,results,number of encoder layers,for,student model,number of encoder layers for student model,0.5887097120285034
translation,158,166,results,number of encoder layers,achieves,lower bleu scores,number of encoder layers achieves lower bleu scores,0.6436555981636047
translation,158,166,results,lower bleu scores,at,similar speed,lower bleu scores at similar speed,0.5214892625808716
translation,158,166,results,similar speed,for,our cpu systems,similar speed for our cpu systems,0.6484869718551636
translation,158,166,results,results,reducing,number of encoder layers,results reducing number of encoder layers,0.6510323286056519
translation,158,169,results,gpu system,with,student - 3 - 1- 512 model,gpu system with student - 3 - 1- 512 model,0.6698610186576843
translation,158,169,results,student - 3 - 1- 512 model,can translate,300m words per dollar,student - 3 - 1- 512 model can translate 300m words per dollar,0.7186225652694702
translation,158,169,results,300m words per dollar,with,acceptable quality,300m words per dollar with acceptable quality,0.6163304448127747
translation,158,169,results,results,has,gpu system,results has gpu system,0.5543575882911682
translation,159,207,ablation-analysis,postprocessing translated output,to use,standardized punctuation,postprocessing translated output to use standardized punctuation,0.6405303478240967
translation,159,207,ablation-analysis,standardized punctuation,in,each language,standardized punctuation in each language,0.4975496232509613
translation,159,207,ablation-analysis,standardized punctuation,is,very important,standardized punctuation is very important,0.5172398090362549
translation,159,207,ablation-analysis,very important,for,bleu scores,very important for bleu scores,0.5718077421188354
translation,159,207,ablation-analysis,bleu scores,when,translating out of english,bleu scores when translating out of english,0.567547619342804
translation,159,207,ablation-analysis,ablation analysis,observe,postprocessing translated output,ablation analysis observe postprocessing translated output,0.6166304349899292
translation,159,52,experimental-setup,spm model,with,temperature upsampling,spm model with temperature upsampling,0.6081591844558716
translation,159,52,experimental-setup,experimental setup,train,spm model,experimental setup train spm model,0.6567703485488892
translation,159,53,experimental-setup,bilingual models,used,vocabulary size,bilingual models used vocabulary size,0.6363193392753601
translation,159,53,experimental-setup,bilingual models,used,000,bilingual models used 000,0.6114900708198547
translation,159,53,experimental-setup,bilingual models,for,multilingual models,bilingual models for multilingual models,0.6300851106643677
translation,159,53,experimental-setup,vocabulary size,of,32,vocabulary size of 32,0.6482106447219849
translation,159,53,experimental-setup,vocabulary size,of,000,vocabulary size of 000,0.6598989963531494
translation,159,53,experimental-setup,32,",",000,"32 , 000",0.7067277431488037
translation,159,53,experimental-setup,multilingual models,used,128,multilingual models used 128,0.4893075227737427
translation,159,53,experimental-setup,128,",",000,"128 , 000",0.6902894973754883
translation,159,53,experimental-setup,experimental setup,For,bilingual models,experimental setup For bilingual models,0.6017057299613953
translation,159,53,experimental-setup,experimental setup,For,multilingual models,experimental setup For multilingual models,0.5589572191238403
translation,159,53,experimental-setup,experimental setup,for,multilingual models,experimental setup for multilingual models,0.5589572191238403
translation,159,63,experimental-setup,our models,using,fairseq,our models using fairseq,0.7241167426109314
translation,159,63,experimental-setup,fairseq,on,32 volta 32gb gpus,fairseq on 32 volta 32gb gpus,0.4910776913166046
translation,159,63,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,159,64,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,159,64,experimental-setup,learning rate,tune,dropout rate,learning rate tune dropout rate,0.7032228112220764
translation,159,64,experimental-setup,0.001,with,adam optimizer,0.001 with adam optimizer,0.5981147885322571
translation,159,64,experimental-setup,batch size,of,"768,000 tokens","batch size of 768,000 tokens",0.6008805632591248
translation,159,64,experimental-setup,dropout rate,for,each language direction,dropout rate for each language direction,0.5407366752624512
translation,159,64,experimental-setup,each language direction,has,independently,each language direction has independently,0.6048305630683899
translation,159,64,experimental-setup,experimental setup,tune,dropout rate,experimental setup tune dropout rate,0.6992032527923584
translation,159,91,experimental-setup,sparsely gated mixture - of- expert ( moe ) models,for,english to many and many to english,sparsely gated mixture - of- expert ( moe ) models for english to many and many to english,0.5917426347732544
translation,159,91,experimental-setup,),for,english to many and many to english,) for english to many and many to english,0.7161802053451538
translation,159,91,experimental-setup,sparsely gated mixture - of- expert ( moe ) models,has,),sparsely gated mixture - of- expert ( moe ) models has ),0.5497040152549744
translation,159,91,experimental-setup,experimental setup,train,sparsely gated mixture - of- expert ( moe ) models,experimental setup train sparsely gated mixture - of- expert ( moe ) models,0.6106234192848206
translation,159,96,experimental-setup,gate loss term,to balance,expert assignment,gate loss term to balance expert assignment,0.610009491443634
translation,159,96,experimental-setup,expert assignment,across,tokens,expert assignment across tokens,0.7061480283737183
translation,159,96,experimental-setup,expert assignment,with,gate loss weight,expert assignment with gate loss weight,0.6161141395568848
translation,159,96,experimental-setup,gate loss weight,of,0.01,gate loss weight of 0.01,0.6300911903381348
translation,159,96,experimental-setup,experimental setup,add,gate loss term,experimental setup add gate loss term,0.5763062238693237
translation,159,97,experimental-setup,expert capacity factor,of,2.0,expert capacity factor of 2.0,0.5790126919746399
translation,159,97,experimental-setup,experimental setup,use,expert capacity factor,experimental setup use expert capacity factor,0.6366469860076904
translation,159,98,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,159,98,experimental-setup,learning rate,of,1.5 million tokens,learning rate of 1.5 million tokens,0.5461888909339905
translation,159,98,experimental-setup,0.001,with,adam optimizer,0.001 with adam optimizer,0.5981147885322571
translation,159,98,experimental-setup,0.001,with,4000 warmup updates,0.001 with 4000 warmup updates,0.6206242442131042
translation,159,98,experimental-setup,0.001,with,4000 warmup updates,0.001 with 4000 warmup updates,0.6206242442131042
translation,159,98,experimental-setup,adam optimizer,with,4000 warmup updates,adam optimizer with 4000 warmup updates,0.6240659356117249
translation,159,98,experimental-setup,batch size,of,1 million tokens,batch size of 1 million tokens,0.5801458358764648
translation,159,98,experimental-setup,batch size,of,1.5 million tokens,batch size of 1.5 million tokens,0.5675547122955322
translation,159,98,experimental-setup,moe model,with,64 experts,moe model with 64 experts,0.6623899340629578
translation,159,98,experimental-setup,moe model,with,128 experts,moe model with 128 experts,0.6614684462547302
translation,159,98,experimental-setup,moe model,with,128 experts,moe model with 128 experts,0.6614684462547302
translation,159,98,experimental-setup,moe model,with,128 experts,moe model with 128 experts,0.6614684462547302
translation,159,98,experimental-setup,1 million tokens,has,moe model,1 million tokens has moe model,0.5551671981811523
translation,159,98,experimental-setup,1.5 million tokens,has,moe model,1.5 million tokens has moe model,0.5637848973274231
translation,159,113,experimental-setup,experimental setup,tune,length penalty,experimental setup tune length penalty,0.6841393709182739
translation,159,114,experimental-setup,search bounds,for,weights and the length penalty,search bounds for weights and the length penalty,0.6059351563453674
translation,159,114,experimental-setup,weights and the length penalty,are,"[ 0 ,2 ]","weights and the length penalty are [ 0 ,2 ]",0.5624685287475586
translation,159,114,experimental-setup,experimental setup,has,search bounds,experimental setup has search bounds,0.5185214281082153
translation,159,50,experiments,multilingual vocabulary,first learn,multilingual subword tokenizer,multilingual vocabulary first learn multilingual subword tokenizer,0.6148303151130676
translation,159,50,experiments,multilingual subword tokenizer,on,combined training data,multilingual subword tokenizer on combined training data,0.50899738073349
translation,159,50,experiments,combined training data,across,all languages,combined training data across all languages,0.7223359942436218
translation,159,51,experiments,"sentence -piece ( kudo and richardson , 2018 )",which learns,subword units,"sentence -piece ( kudo and richardson , 2018 ) which learns subword units",0.6521880626678467
translation,159,51,experiments,subword units,from,untokenized text,subword units from untokenized text,0.5612314343452454
translation,159,119,experiments,hausa and icelandic,trained,smaller language models,hausa and icelandic trained smaller language models,0.6450887322425842
translation,159,119,experiments,smaller language models,with,6 decoder layers,smaller language models with 6 decoder layers,0.6032689213752747
translation,159,79,model,model,improve,bilingual models,model improve bilingual models,0.684532105922699
translation,159,95,model,transformer architecture,with,feed forward block,transformer architecture with feed forward block,0.6350567936897278
translation,159,95,model,feed forward block,in,every alternate transformer layer,feed forward block in every alternate transformer layer,0.5446474552154541
translation,159,95,model,feed forward block,replaced with,sparsely gated mixture - of - experts layer,feed forward block replaced with sparsely gated mixture - of - experts layer,0.7175619602203369
translation,159,95,model,sparsely gated mixture - of - experts layer,with,top - 2 gating,sparsely gated mixture - of - experts layer with top - 2 gating,0.6330972909927368
translation,159,95,model,top - 2 gating,in,encoder and decoder,top - 2 gating in encoder and decoder,0.5469352006912231
translation,159,95,model,model,use,transformer architecture,model use transformer architecture,0.6436730623245239
translation,159,10,results,our multilingual system,improved,translation quality,our multilingual system improved translation quality,0.690729558467865
translation,159,10,results,translation quality,on,all language directions,translation quality on all language directions,0.4889509379863739
translation,159,10,results,translation quality,with,average improvement,translation quality with average improvement,0.605439305305481
translation,159,10,results,average improvement,of,2.0 bleu,average improvement of 2.0 bleu,0.5381183624267578
translation,159,27,results,our final multilingual submission,improves,translation quality,our final multilingual submission improves translation quality,0.6769037842750549
translation,159,27,results,translation quality,on average,+ 2.0,translation quality on average + 2.0,0.6807308793067932
translation,159,27,results,translation quality,compared to,wmt2020 winning models,translation quality compared to wmt2020 winning models,0.5905969142913818
translation,159,27,results,results,has,our final multilingual submission,results has our final multilingual submission,0.5693297982215881
translation,159,74,results,downsampling high resource languages,works better than,upsampling,downsampling high resource languages works better than upsampling,0.7773405313491821
translation,159,74,results,upsampling,has,low resource languages,upsampling has low resource languages,0.528449535369873
translation,159,74,results,results,find that,downsampling high resource languages,results find that downsampling high resource languages,0.6315677165985107
translation,159,138,results,all directions,using,specialized bilingual vocabulary,all directions using specialized bilingual vocabulary,0.6580933928489685
translation,159,138,results,specialized bilingual vocabulary,is,superior,specialized bilingual vocabulary is superior,0.5833539962768555
translation,159,138,results,specialized bilingual vocabulary,usually,superior,specialized bilingual vocabulary usually superior,0.7952672839164734
translation,159,138,results,results,across,all directions,results across all directions,0.6884357929229736
translation,159,139,results,some directions,such as,en-is and en-ja,some directions such as en-is and en-ja,0.6162197589874268
translation,159,139,results,some directions,such as,no difference,some directions such as no difference,0.6025406122207642
translation,159,139,results,some directions,has,no difference,some directions has no difference,0.5345715880393982
translation,159,139,results,en-is and en-ja,has,no difference,en-is and en-ja has no difference,0.6092687249183655
translation,159,139,results,results,for,some directions,results for some directions,0.6403129696846008
translation,159,142,results,dense multilingual models,are,fairly competitive,dense multilingual models are fairly competitive,0.5510526299476624
translation,159,142,results,fairly competitive,with,dense bilingual models,fairly competitive with dense bilingual models,0.6415924429893494
translation,159,143,results,multilingual models,benefit greatly from,additional model capacity,multilingual models benefit greatly from additional model capacity,0.5746232867240906
translation,159,143,results,results,find,multilingual models,results find multilingual models,0.49575552344322205
translation,159,144,results,scaling dense models,increasing,dense model size,scaling dense models increasing dense model size,0.7168261408805847
translation,159,144,results,dense model size,from,12/,dense model size from 12/,0.5838181376457214
translation,159,144,results,dense model size,from,12 to 24 / 24,dense model size from 12 to 24 / 24,0.6110914945602417
translation,159,144,results,dense model size,correspond to,significant improvement,dense model size correspond to significant improvement,0.5929738879203796
translation,159,144,results,significant improvement,for,many to english,significant improvement for many to english,0.65583735704422
translation,159,144,results,12/,has,12 to 24 / 24,12/ has 12 to 24 / 24,0.6338003277778625
translation,159,146,results,slightly decline or no improvement,in,performance,slightly decline or no improvement in performance,0.5285826325416565
translation,159,146,results,performance,of,moe models,performance of moe models,0.6235775947570801
translation,159,146,results,number of experts,from,64 to 128,number of experts from 64 to 128,0.5602700710296631
translation,159,148,results,training convergence,per expert,slower,training convergence per expert slower,0.7131658792495728
translation,159,148,results,training convergence,is,slower,training convergence is slower,0.560088038444519
translation,159,148,results,slower,as,each expert,slower as each expert,0.621802806854248
translation,159,148,results,each expert,exposed to,fewer tokens,each expert exposed to fewer tokens,0.6478269696235657
translation,159,148,results,fewer tokens,during,training,fewer tokens during training,0.6941133141517639
translation,159,148,results,64 experts,has,training convergence,64 experts has training convergence,0.5705726742744446
translation,159,148,results,results,Compared to,64 experts,results Compared to 64 experts,0.6499506831169128
translation,159,149,results,finetuning,on,in- domain data,finetuning on in- domain data,0.5761865377426147
translation,159,149,results,finetuning,observe,significant improvement,finetuning observe significant improvement,0.6513886451721191
translation,159,149,results,in- domain data,observe,significant improvement,in- domain data observe significant improvement,0.6326360702514648
translation,159,149,results,significant improvement,in,performance,significant improvement in performance,0.5336322784423828
translation,159,149,results,performance,across,board,performance across board,0.6779528856277466
translation,159,149,results,results,After,finetuning,results After finetuning,0.7041951417922974
translation,159,150,results,larger improvement,from,finetuning,larger improvement from finetuning,0.5548955202102661
translation,159,150,results,finetuning,in,moe models,finetuning in moe models,0.5480327010154724
translation,159,150,results,finetuning,compared to,associated cs-en,finetuning compared to associated cs-en,0.6257920861244202
translation,159,151,results,moe model,with,128 experts,moe model with 128 experts,0.6614684462547302
translation,159,151,results,moe model,gives,best results,moe model gives best results,0.6220594048500061
translation,159,151,results,best results,for,all but two directions,best results for all but two directions,0.6035275459289551
translation,159,151,results,results,has,moe model,results has moe model,0.5395684838294983
translation,159,153,results,results,has,effects of in-domain finetuning finetuning,results has effects of in-domain finetuning finetuning,0.5529811382293701
translation,159,155,results,multilingual systems,benefit more from,in- domain finetuning,multilingual systems benefit more from in- domain finetuning,0.636225163936615
translation,159,155,results,results,has,multilingual systems,results has multilingual systems,0.5800584554672241
translation,159,161,results,multilingual finetuning,almost always better than,bilingual finetuning,multilingual finetuning almost always better than bilingual finetuning,0.7422550916671753
translation,159,161,results,results,find that,multilingual finetuning,results find that multilingual finetuning,0.5919845104217529
translation,159,179,results,zh-en and cs-en,after,in- domain finetuning,zh-en and cs-en after in- domain finetuning,0.6668036580085754
translation,159,179,results,system,trained with,backtranslation,system trained with backtranslation,0.7499886751174927
translation,159,179,results,system,trained with,backtranslation,system trained with backtranslation,0.7499886751174927
translation,159,179,results,system,trained without,backtranslation,system trained without backtranslation,0.7633993625640869
translation,159,179,results,backtranslation,has,stronger performance,backtranslation has stronger performance,0.5573943257331848
translation,159,179,results,stronger performance,compared to,system,stronger performance compared to system,0.6936638355255127
translation,159,179,results,system,trained without,backtranslation,system trained without backtranslation,0.7633993625640869
translation,159,179,results,zh-en and cs-en,has,system,zh-en and cs-en has system,0.6101354360580444
translation,159,179,results,in- domain finetuning,has,system,in- domain finetuning has system,0.5672450661659241
translation,159,179,results,backtranslation,has,stronger performance,backtranslation has stronger performance,0.5573943257331848
translation,159,179,results,stronger performance,has,+ 0.4 bleu ),stronger performance has + 0.4 bleu ),0.5728114247322083
translation,159,179,results,results,For,zh-en and cs-en,results For zh-en and cs-en,0.5992031693458557
translation,159,184,results,improves,has,performance,improves has performance,0.5770372748374939
translation,159,189,results,our bilingual baselines,have,high bleu scores,our bilingual baselines have high bleu scores,0.5141842365264893
translation,159,189,results,high bleu scores,particularly for,ru-en,high bleu scores particularly for ru-en,0.6535211205482483
translation,159,189,results,results,find that,our bilingual baselines,results find that our bilingual baselines,0.6228119730949402
translation,159,190,results,en-ha and ha-en,significantly lower than,20 bleu,en-ha and ha-en significantly lower than 20 bleu,0.6940445303916931
translation,159,195,results,impact,of,in-domain finetuning,impact of in-domain finetuning,0.6115130186080933
translation,159,195,results,impact,find,almost 3 bleu improvement,impact find almost 3 bleu improvement,0.6101513504981995
translation,159,195,results,in-domain finetuning,find,almost 3 bleu improvement,in-domain finetuning find almost 3 bleu improvement,0.5538697838783264
translation,159,195,results,almost 3 bleu improvement,across,directions,almost 3 bleu improvement across directions,0.6747376918792725
translation,159,195,results,directions,for,translation,directions for translation,0.6841378211975098
translation,159,195,results,0.7 bleu improvement,for,translation out of english,0.7 bleu improvement for translation out of english,0.5482988953590393
translation,159,195,results,translation,has,into english,translation has into english,0.6206303834915161
translation,159,195,results,results,evaluate,impact,results evaluate impact,0.5994877219200134
translation,159,196,results,finetuning,is,almost universally helpful,finetuning is almost universally helpful,0.5197808742523193
translation,159,196,results,all language directions,has,finetuning,all language directions has finetuning,0.5744608044624329
translation,159,196,results,results,Across,all language directions,results Across all language directions,0.6007784008979797
translation,159,198,results,multilingual models,have,stronger performance,multilingual models have stronger performance,0.5218345522880554
translation,159,198,results,stronger performance,in,every direction,stronger performance in every direction,0.5236155986785889
translation,159,198,results,bilingual models,has,multilingual models,bilingual models has multilingual models,0.6037116050720215
translation,159,198,results,results,Compared to,bilingual models,results Compared to bilingual models,0.5978880524635315
translation,159,199,results,multilingual models,benefit much more from,scaling model size,multilingual models benefit much more from scaling model size,0.6072892546653748
translation,159,199,results,our largest architecture ( moe - 128 24/ 24 ),has,best performance,our largest architecture ( moe - 128 24/ 24 ) has best performance,0.5612704157829285
translation,159,199,results,results,has,multilingual models,results has multilingual models,0.5098084211349487
translation,159,201,results,ensembling,on,average,ensembling on average,0.5556021332740784
translation,159,201,results,ensembling,on,specific directions,ensembling on specific directions,0.5881540179252625
translation,159,201,results,ensembling,is,fairly minor,ensembling is fairly minor,0.6294293999671936
translation,159,201,results,average,is,fairly minor,average is fairly minor,0.6022882461547852
translation,159,201,results,specific directions,see,large improvements,specific directions see large improvements,0.596928060054779
translation,159,205,results,results,On average,performance,results On average performance,0.7087794542312622
translation,160,21,baselines,multilingual training strategy,to make use of,english ? x parallel resources,multilingual training strategy to make use of english ? x parallel resources,0.6506956219673157
translation,160,72,experimental-setup,models,based on,"fairseq ( ott et al. , 2019 )","models based on fairseq ( ott et al. , 2019 )",0.6563156247138977
translation,160,72,experimental-setup,experimental setup,implementation of,models,experimental setup implementation of models,0.7030203938484192
translation,160,73,experimental-setup,models,carried out on,2 nvidia 3090 gpus,models carried out on 2 nvidia 3090 gpus,0.6479194164276123
translation,160,73,experimental-setup,24 gb,of,memory,24 gb of memory,0.5464227199554443
translation,160,73,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,160,75,experimental-setup,adam optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adam optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6311911940574646
translation,160,75,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,160,76,experimental-setup,batch size,set to,4096 tokens,batch size set to 4096 tokens,0.6636665463447571
translation,160,76,experimental-setup,  update-freq   parameter,in,fairseq,  update-freq   parameter in fairseq,0.5690706968307495
translation,160,76,experimental-setup,fairseq,set to,2,fairseq set to 2,0.7472038269042969
translation,160,76,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,160,76,experimental-setup,experimental setup,has,  update-freq   parameter,experimental setup has   update-freq   parameter,0.5401818156242371
translation,160,77,experimental-setup,batch size,is,2048,batch size is 2048,0.6247645020484924
translation,160,77,experimental-setup,updatefreq,is,4,updatefreq is 4,0.6305714249610901
translation,160,77,experimental-setup,pre-trained language models settings ( i.e.,has,batch size,pre-trained language models settings ( i.e. has batch size,0.5146148800849915
translation,160,77,experimental-setup,mbart and mrasp ),has,batch size,mbart and mrasp ) has batch size,0.583622932434082
translation,160,78,experimental-setup,initial learning rate,set to,5e ?4,initial learning rate set to 5e ?4,0.7368857264518738
translation,160,78,experimental-setup,initial learning rate,set to,3e ?5,initial learning rate set to 3e ?5,0.7401319742202759
translation,160,78,experimental-setup,5e ?4,for,training,5e ?4 for training,0.694389283657074
translation,160,78,experimental-setup,3e ?5,for,fine-tuning,3e ?5 for fine-tuning,0.6421145796775818
translation,160,78,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,160,79,experimental-setup,learning scheduler,is,inverse_sqrt,learning scheduler is inverse_sqrt,0.6201564073562622
translation,160,79,experimental-setup,dropout probabilities,set to,0.1,dropout probabilities set to 0.1,0.6957004070281982
translation,160,79,experimental-setup,experimental setup,has,learning scheduler,experimental setup has learning scheduler,0.5443568825721741
translation,160,79,experimental-setup,experimental setup,has,dropout probabilities,experimental setup has dropout probabilities,0.508775532245636
translation,160,35,experiments,multilingual architecture,has,multilingual neural machine translation ( mnmt ),multilingual architecture has multilingual neural machine translation ( mnmt ),0.5981168150901794
translation,160,20,model,two pipelines,for combining,english related resources,two pipelines for combining english related resources,0.7698466777801514
translation,160,20,model,two pipelines,incorporates,english- to - chinese translator,two pipelines incorporates english- to - chinese translator,0.6738793253898621
translation,160,20,model,english- to - chinese translator,into,direct translation process,english- to - chinese translator into direct translation process,0.5602929592132568
translation,160,20,model,english- to - chinese translator,to normalize,translation,english- to - chinese translator to normalize translation,0.6794859170913696
translation,160,20,model,direct translation process,to normalize,translation,direct translation process to normalize translation,0.6612092852592468
translation,160,20,model,translation,of,rare words,translation of rare words,0.5273352861404419
translation,160,31,model,each encoder layer,consists of,selfattention mechanism,each encoder layer consists of selfattention mechanism,0.6653157472610474
translation,160,31,model,each encoder layer,consists of,feed-forward network,each encoder layer consists of feed-forward network,0.6620169878005981
translation,160,31,model,model,has,each encoder layer,model has each encoder layer,0.545050859451294
translation,160,32,model,decoder layer,consists of,masked selfattention layer,decoder layer consists of masked selfattention layer,0.6609236598014832
translation,160,32,model,decoder layer,consists of,cross self-attention layer,decoder layer consists of cross self-attention layer,0.6640830636024475
translation,160,32,model,decoder layer,consists of,feed-forward network layer,decoder layer consists of feed-forward network layer,0.6623289585113525
translation,160,32,model,model,has,decoder layer,model has decoder layer,0.5389419794082642
translation,160,33,model,layer normalization,for,enhancement,layer normalization for enhancement,0.5824864506721497
translation,160,69,results,fine-tuning,on,mrasp,fine-tuning on mrasp,0.5384777784347534
translation,160,69,results,fine-tuning,achieves,anticipated improvements,fine-tuning achieves anticipated improvements,0.6728567481040955
translation,160,69,results,mrasp,with,filtered parallel data,mrasp with filtered parallel data,0.6670756936073303
translation,160,97,results,two multilingual translation strategies,effectively improve,performance,two multilingual translation strategies effectively improve performance,0.6027213335037231
translation,160,97,results,performance,of,baseline model,performance of baseline model,0.5962602496147156
translation,160,97,results,performance,especially for,rare words,performance especially for rare words,0.6166459918022156
translation,160,97,results,baseline model,especially for,rare words,baseline model especially for rare words,0.5951508283615112
translation,160,97,results,results,has,two multilingual translation strategies,results has two multilingual translation strategies,0.49788445234298706
translation,161,6,model,novel difficulty - aware mt evaluation metric,expanding,evaluation dimension,novel difficulty - aware mt evaluation metric expanding evaluation dimension,0.6398504376411438
translation,161,6,model,evaluation dimension,by taking,translation difficulty,evaluation dimension by taking translation difficulty,0.5679371356964111
translation,161,6,model,translation difficulty,into,consideration,translation difficulty into consideration,0.6161040663719177
translation,161,6,model,model,propose,novel difficulty - aware mt evaluation metric,model propose novel difficulty - aware mt evaluation metric,0.5980955362319946
translation,161,22,model,translation difficulty,into,account,translation difficulty into account,0.5881466269493103
translation,161,22,model,translation difficulty,in,mt evaluation,translation difficulty in mt evaluation,0.47710368037223816
translation,161,22,model,effectiveness,on,"representative mt metric bertscore ( zhang et al. , 2020 )","effectiveness on representative mt metric bertscore ( zhang et al. , 2020 )",0.4915805160999298
translation,161,22,model,model,take,translation difficulty,model take translation difficulty,0.6301165223121643
translation,161,22,model,model,test,effectiveness,model test effectiveness,0.7536994814872742
translation,161,23,model,difficulty,across,systems,difficulty across systems,0.7497367262840271
translation,161,23,model,difficulty,exploited as,weight,difficulty exploited as weight,0.6489441394805908
translation,161,23,model,systems,help of,pairwise similarity,systems help of pairwise similarity,0.6584341526031494
translation,161,23,model,weight,in,final score function,weight in final score function,0.516972541809082
translation,161,23,model,model,has,difficulty,model has difficulty,0.5539286136627197
translation,162,62,ablation-analysis,lay-erdrop,degrades,translation quality,lay-erdrop degrades translation quality,0.7309386134147644
translation,162,62,ablation-analysis,translation quality,has,substantially,translation quality has substantially,0.5633801817893982
translation,162,62,ablation-analysis,ablation analysis,has,lay-erdrop,ablation analysis has lay-erdrop,0.5662603974342346
translation,162,10,experiments,fine-tuning,has,flo-res101_mm100 model,fine-tuning has flo-res101_mm100 model,0.6063501834869385
translation,162,42,experiments,two different learning rates,on,smaller dataset ( bible-uedin ),two different learning rates on smaller dataset ( bible-uedin ),0.560520589351654
translation,162,42,experiments,two different learning rates,has,3e ?5 and 3e ?7,two different learning rates has 3e ?5 and 3e ?7,0.6116279363632202
translation,162,11,hyperparameters,different fine-tuning configurations,minimize,computational and data resources,different fine-tuning configurations minimize computational and data resources,0.7144034504890442
translation,162,11,hyperparameters,goal,minimize,computational and data resources,goal minimize computational and data resources,0.6963983178138733
translation,162,11,hyperparameters,hyperparameters,consider,different fine-tuning configurations,hyperparameters consider different fine-tuning configurations,0.6245080232620239
translation,162,14,hyperparameters,layerdrop,have,regularization effect,layerdrop have regularization effect,0.5451027750968933
translation,162,14,hyperparameters,model size,for,inference,model size for inference,0.6392310261726379
translation,162,14,hyperparameters,structure dropout technique,has,layerdrop,structure dropout technique has layerdrop,0.5574136972427368
translation,162,14,hyperparameters,hyperparameters,adopt,structure dropout technique,hyperparameters adopt structure dropout technique,0.6024029850959778
translation,162,73,hyperparameters,fine-tuning,done for,10 epochs,fine-tuning done for 10 epochs,0.6274095177650452
translation,162,73,hyperparameters,hyperparameters,has,fine-tuning,hyperparameters has fine-tuning,0.5253989100456238
translation,162,13,model,selectively dropping layers,during,fine-tuning,selectively dropping layers during fine-tuning,0.6585161089897156
translation,162,13,model,selectively dropping layers,to reduce,computational cost,selectively dropping layers to reduce computational cost,0.646719217300415
translation,162,13,model,fine-tuning,to reduce,computational cost,fine-tuning to reduce computational cost,0.6705209612846375
translation,162,13,model,computational cost,of working with,transformer model,computational cost of working with transformer model,0.665066659450531
translation,162,13,model,transformer model,with,millions of parameters,transformer model with millions of parameters,0.6241827607154846
translation,162,13,model,model,consider,selectively dropping layers,model consider selectively dropping layers,0.7237374186515808
translation,162,72,model,relu,as,activation function,relu as activation function,0.5622032880783081
translation,162,72,model,all the original layers,in,encoder and decoder,all the original layers in encoder and decoder,0.4931257665157318
translation,162,12,results,impact,of,finetuning,impact of finetuning,0.6185661554336548
translation,162,12,results,finetuning,on,datasets,finetuning on datasets,0.5490745902061462
translation,162,12,results,finetuning,with,smaller dataset,finetuning with smaller dataset,0.6586666703224182
translation,162,12,results,finetuning,gives,better performance,finetuning gives better performance,0.6474220752716064
translation,162,12,results,smaller dataset,gives,better performance,smaller dataset gives better performance,0.6269421577453613
translation,162,12,results,better performance,for,some language pairs,better performance for some language pairs,0.6025217771530151
translation,162,12,results,results,consider,impact,results consider impact,0.6656209826469421
translation,162,43,results,learning rate,from,3e ?5 to 3e ?7,learning rate from 3e ?5 to 3e ?7,0.6039913892745972
translation,162,43,results,learning rate,boosts,bleu score,learning rate boosts bleu score,0.5876664519309998
translation,162,43,results,3e ?5 to 3e ?7,boosts,bleu score,3e ?5 to 3e ?7 boosts bleu score,0.7320253849029541
translation,162,43,results,bleu score,of,bible - uedin,bleu score of bible - uedin,0.5573461651802063
translation,162,43,results,fine- tuned model,to,15.10,fine- tuned model to 15.10,0.5328412055969238
translation,162,43,results,results,Changing,learning rate,results Changing learning rate,0.7179730534553528
translation,162,56,results,higher bleu scores,across,board,higher bleu scores across board,0.5980991721153259
translation,162,60,results,bleu scores,obtained with,fine-tuning configurations,bleu scores obtained with fine-tuning configurations,0.5863476991653442
translation,162,60,results,fine-tuning configurations,vary in,activation function used,fine-tuning configurations vary in activation function used,0.7343234419822693
translation,162,60,results,fine-tuning configurations,in the use of,layerdrop technique,fine-tuning configurations in the use of layerdrop technique,0.6986232995986938
translation,162,60,results,layerdrop technique,for reducing,model size,layerdrop technique for reducing model size,0.6882575750350952
translation,162,67,results,some translations,from,tamil,some translations from tamil,0.5866402387619019
translation,162,67,results,some translations,show,improvements,some translations show improvements,0.6793166399002075
translation,162,67,results,1.3,on,tam-eng,1.3 on tam-eng,0.5070763230323792
translation,162,67,results,1.31,on,tgl- tam,1.31 on tgl- tam,0.5678129196166992
translation,162,67,results,improvements,has,1.3,improvements has 1.3,0.5450006127357483
translation,162,67,results,results,has,some translations,results has some translations,0.5794283151626587
translation,163,151,ablation-analysis,averaged bleu score,of,rnnsearch,averaged bleu score of rnnsearch,0.5591856241226196
translation,163,151,ablation-analysis,rnnsearch,raised by,3.1 %,rnnsearch raised by 3.1 %,0.6938926577568054
translation,163,151,ablation-analysis,transformer,increased by,1.05 %,transformer increased by 1.05 %,0.7050709128379822
translation,163,151,ablation-analysis,ablation analysis,has,averaged bleu score,ablation analysis has averaged bleu score,0.5590227246284485
translation,163,176,ablation-analysis,gaps,reduced relatively by,80.7 %,gaps reduced relatively by 80.7 %,0.7064999341964722
translation,163,176,ablation-analysis,80.7 %,for,rnnsearch,80.7 % for rnnsearch,0.6272915601730347
translation,163,176,ablation-analysis,75.8 %,for,transformer,75.8 % for transformer,0.6428167223930359
translation,163,176,ablation-analysis,evaluator,has,gaps,evaluator has gaps,0.5567588210105896
translation,163,176,ablation-analysis,ablation analysis,Using,evaluator,ablation analysis Using evaluator,0.6114338636398315
translation,163,146,baselines,deliberation networks,adopts,second decoder,deliberation networks adopts second decoder,0.630281388759613
translation,163,146,baselines,second decoder,to polish,raw sequence,second decoder to polish raw sequence,0.6775675415992737
translation,163,146,baselines,raw sequence,produced by,first- pass decoder,raw sequence produced by first- pass decoder,0.6570172309875488
translation,163,146,baselines,abd -nmt,uses,backward decoder,abd -nmt uses backward decoder,0.5777361989021301
translation,163,146,baselines,abd -nmt,uses,forward decoder,abd -nmt uses forward decoder,0.5762814879417419
translation,163,146,baselines,backward decoder,to generate,translation,backward decoder to generate translation,0.7226245999336243
translation,163,146,baselines,forward decoder,to refine,attention mechanism,forward decoder to refine attention mechanism,0.7485381364822388
translation,163,146,baselines,adaptive multi-pass decoder,utilizes,rl,adaptive multi-pass decoder utilizes rl,0.6115372180938721
translation,163,146,baselines,rl,to model,iterative rewriting process,rl to model iterative rewriting process,0.6765639781951904
translation,163,135,experimental-setup,dimensions,of,word embeddings and hidden layers,dimensions of word embeddings and hidden layers,0.528904139995575
translation,163,135,experimental-setup,word embeddings and hidden layers,are both,600,word embeddings and hidden layers are both 600,0.6112365126609802
translation,163,135,experimental-setup,rnnsearch,has,dimensions,rnnsearch has dimensions,0.5958821773529053
translation,163,135,experimental-setup,experimental setup,For,rnnsearch,experimental setup For rnnsearch,0.6107689142227173
translation,163,137,experimental-setup,dropout rate,set to,0.2,dropout rate set to 0.2,0.6595621705055237
translation,163,137,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,163,138,experimental-setup,transformer,follow,setting of transformer - base,transformer follow setting of transformer - base,0.5950077772140503
translation,163,139,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,163,139,experimental-setup,maximum number of training tokens,at,every step,maximum number of training tokens at every step,0.532028079032898
translation,163,140,experimental-setup,"adam ( kingma and ba , 2014 )",for,optimization,"adam ( kingma and ba , 2014 ) for optimization",0.6054565906524658
translation,163,140,experimental-setup,experimental setup,use,"adam ( kingma and ba , 2014 )","experimental setup use adam ( kingma and ba , 2014 )",0.5799989700317383
translation,163,141,experimental-setup,proposed models,run on,nvidia tesla v100 gpus,proposed models run on nvidia tesla v100 gpus,0.737666130065918
translation,163,142,experimental-setup,maximum number of rewriting iterations k,is,6,maximum number of rewriting iterations k is 6,0.6026642322540283
translation,163,142,experimental-setup,termination threshold,is,0.05,termination threshold is 0.05,0.5640403032302856
translation,163,142,experimental-setup,rewriter - evaluator,has,maximum number of rewriting iterations k,rewriter - evaluator has maximum number of rewriting iterations k,0.5880341529846191
translation,163,142,experimental-setup,experimental setup,For,rewriter - evaluator,experimental setup For rewriter - evaluator,0.5829428434371948
translation,163,9,hyperparameters,termination,of,multi-pass process,termination of multi-pass process,0.6084153652191162
translation,163,9,hyperparameters,multi-pass process,determined by,score,multi-pass process determined by score,0.7026691436767578
translation,163,9,hyperparameters,score,of,translation quality,score of translation quality,0.5525178909301758
translation,163,9,hyperparameters,translation quality,estimated by,evaluator,translation quality estimated by evaluator,0.6259654760360718
translation,163,9,hyperparameters,hyperparameters,has,termination,hyperparameters has termination,0.5135675668716431
translation,163,6,model,novel architecture,has,of rewriter -evaluator,novel architecture has of rewriter -evaluator,0.5623640418052673
translation,163,6,model,model,introduce,novel architecture,model introduce novel architecture,0.6750567555427551
translation,163,7,model,model,has,translating a source sentence,model has translating a source sentence,0.5512099862098694
translation,163,10,model,prioritized gradient descent ( pgd ),to jointly and efficiently train,rewriter and the evaluator,prioritized gradient descent ( pgd ) to jointly and efficiently train rewriter and the evaluator,0.6895835399627686
translation,163,10,model,model,propose,prioritized gradient descent ( pgd ),model propose prioritized gradient descent ( pgd ),0.6442039608955383
translation,163,15,model,encoder,firstly converts it into,hidden representations,encoder firstly converts it into hidden representations,0.6589145660400391
translation,163,15,model,hidden representations,conditioned by,decoder,hidden representations conditioned by decoder,0.7967066764831543
translation,163,15,model,decoder,to produce,target sentence,decoder to produce target sentence,0.6961068511009216
translation,163,15,model,source sentence,has,encoder,source sentence has encoder,0.5405246615409851
translation,163,15,model,model,Given,source sentence,model Given source sentence,0.6706663966178894
translation,163,24,model,architecture,contains,rewriter,architecture contains rewriter,0.6283564567565918
translation,163,24,model,architecture,contains,evaluator,architecture contains evaluator,0.591697633266449
translation,163,24,model,model,has,architecture,model has architecture,0.5575731992721558
translation,163,29,model,prioritized gradient descent ( pgd ),facilitates,training,prioritized gradient descent ( pgd ) facilitates training,0.6335538625717163
translation,163,29,model,training,has,rewriter and the evaluator,training has rewriter and the evaluator,0.5894501209259033
translation,163,29,model,model,propose,prioritized gradient descent ( pgd ),model propose prioritized gradient descent ( pgd ),0.6442039608955383
translation,163,30,model,pgd,uses,priority queue,pgd uses priority queue,0.6132762432098389
translation,163,30,model,priority queue,to store,previous translation cases,priority queue to store previous translation cases,0.7033344507217407
translation,163,30,model,model,has,pgd,model has pgd,0.6358145475387573
translation,163,136,model,encoder,has,3 layers,encoder has 3 layers,0.5921659469604492
translation,163,136,model,decoder,has,2 layers,decoder has 2 layers,0.5892198085784912
translation,163,136,model,model,has,encoder,model has encoder,0.5940273404121399
translation,163,44,results,evaluator,helps,our models,evaluator helps our models,0.6411864757537842
translation,163,44,results,our models,achieve,performances,our models achieve performances,0.6665565967559814
translation,163,44,results,close,to,oracle,close to oracle,0.5702014565467834
translation,163,44,results,performances,has,close,performances has close,0.5692800879478455
translation,163,44,results,outperforming,has,methods of fixing the number of rewriting turns,outperforming has methods of fixing the number of rewriting turns,0.5835681557655334
translation,163,44,results,results,indicate,evaluator,results indicate evaluator,0.6041275262832642
translation,163,45,results,averaged performances,using,fixed number of rewriting iterations,averaged performances using fixed number of rewriting iterations,0.6194087862968445
translation,163,45,results,performance gaps,to,oracle,performance gaps to oracle,0.532680869102478
translation,163,45,results,performance gaps,reduced by,80.7 %,performance gaps reduced by 80.7 %,0.6899963021278381
translation,163,45,results,performance gaps,reduced by,75.8 %,performance gaps reduced by 75.8 %,0.6876492500305176
translation,163,45,results,80.7 %,in the case of,rnnsearch,80.7 % in the case of rnnsearch,0.664788544178009
translation,163,45,results,75.8 %,case of,transformer,75.8 % case of transformer,0.736568808555603
translation,163,45,results,averaged performances,has,performance gaps,averaged performances has performance gaps,0.5578622221946716
translation,163,45,results,fixed number of rewriting iterations,has,performance gaps,fixed number of rewriting iterations has performance gaps,0.5415796041488647
translation,163,45,results,results,Compared against,averaged performances,results Compared against averaged performances,0.6657706499099731
translation,163,46,results,evaluator,trained with,pgd,evaluator trained with pgd,0.736808717250824
translation,163,46,results,pgd,is,significantly more accurate,pgd is significantly more accurate,0.5534236431121826
translation,163,46,results,significantly more accurate,in determining,optimal number of rewriting turns,significantly more accurate in determining optimal number of rewriting turns,0.6575357913970947
translation,163,46,results,results,find,evaluator,results find evaluator,0.6034157872200012
translation,163,150,results,translation quality,of,nmt models,translation quality of nmt models,0.5653546452522278
translation,163,150,results,rewriter - evaluator,has,significantly improves,rewriter - evaluator has significantly improves,0.6033264994621277
translation,163,150,results,significantly improves,has,translation quality,significantly improves has translation quality,0.5744650959968567
translation,163,150,results,results,has,rewriter - evaluator,results has rewriter - evaluator,0.5441291332244873
translation,163,152,results,proposed architecture,has,notably outperforms,proposed architecture has notably outperforms,0.6081645488739014
translation,163,152,results,notably outperforms,has,prior multi-pass decoding methods,notably outperforms has prior multi-pass decoding methods,0.5698258876800537
translation,163,152,results,results,has,proposed architecture,results has proposed architecture,0.6126819849014282
translation,163,153,results,performance,of,rnnsearch w/ rewriter -evaluator,performance of rnnsearch w/ rewriter -evaluator,0.6049394011497498
translation,163,153,results,rnnsearch w/ rewriter -evaluator,surpasses,deliberation network,rnnsearch w/ rewriter -evaluator surpasses deliberation network,0.6552039384841919
translation,163,153,results,rnnsearch w/ rewriter -evaluator,surpasses,abd - nmt,rnnsearch w/ rewriter -evaluator surpasses abd - nmt,0.6621434688568115
translation,163,153,results,rnnsearch w/ rewriter -evaluator,surpasses,adaptive multi-pass decoder,rnnsearch w/ rewriter -evaluator surpasses adaptive multi-pass decoder,0.6825711727142334
translation,163,153,results,deliberation network,by,2.46 %,deliberation network by 2.46 %,0.5627948045730591
translation,163,153,results,abd - nmt,by,2.06 %,abd - nmt by 2.06 %,0.5809059739112854
translation,163,153,results,adaptive multi-pass decoder,by,1.72 %,adaptive multi-pass decoder by 1.72 %,0.54975426197052
translation,163,153,results,results,has,performance,results has performance,0.5972660779953003
translation,163,154,results,rewriter - evaluator,superior to,other alternative methods,rewriter - evaluator superior to other alternative methods,0.7218479514122009
translation,163,154,results,results,validate,rewriter - evaluator,results validate rewriter - evaluator,0.5954089164733887
translation,163,155,results,proposed architecture,improve,transformer backbone,proposed architecture improve transformer backbone,0.6293165683746338
translation,163,155,results,transformer backbone,by,1.05 %,transformer backbone by 1.05 %,0.5712669491767883
translation,163,155,results,1.05 %,has,on average,1.05 % has on average,0.5777087211608887
translation,163,155,results,results,has,proposed architecture,results has proposed architecture,0.6126819849014282
translation,163,161,results,new architecture,improve,bleu scores,new architecture improve bleu scores,0.6545050740242004
translation,163,161,results,bleu scores,on,rnnsearch and transformer backbones,bleu scores on rnnsearch and transformer backbones,0.5308380126953125
translation,163,161,results,results,see that,new architecture,results see that new architecture,0.6852574944496155
translation,163,162,results,improvements,on,rnnsearch backbone,improvements on rnnsearch backbone,0.5344396233558655
translation,163,162,results,rnnsearch backbone,are,2.13 %,rnnsearch backbone are 2.13 %,0.5483280420303345
translation,163,162,results,rnnsearch backbone,are,2.24 %,rnnsearch backbone are 2.24 %,0.5499079823493958
translation,163,162,results,2.13 %,on,wmt '14,2.13 % on wmt '14,0.604428768157959
translation,163,162,results,2.24 %,on,wmt '18,2.24 % on wmt '18,0.5983130931854248
translation,163,162,results,results,has,improvements,results has improvements,0.615561842918396
translation,163,163,results,scores,raised by,1.38 %,scores raised by 1.38 %,0.7064542770385742
translation,163,163,results,scores,raised by,1.43 %,scores raised by 1.43 %,0.7047628164291382
translation,163,163,results,1.38 %,on,wmt '14,1.38 % on wmt '14,0.5911322236061096
translation,163,163,results,1.38 %,on,wmt '18,1.38 % on wmt '18,0.5875684022903442
translation,163,163,results,1.43 %,on,wmt '18,1.43 % on wmt '18,0.5898929238319397
translation,163,163,results,adaptive multi-pass decoder,by,1.31 % and 1.32 %,adaptive multi-pass decoder by 1.31 % and 1.32 %,0.5658501982688904
translation,163,163,results,transformer backbone,has,scores,transformer backbone has scores,0.6052297353744507
translation,163,163,results,rnnsearch w/ rewriter -evaluator,has,outperforms,rnnsearch w/ rewriter -evaluator has outperforms,0.6342878937721252
translation,163,163,results,outperforms,has,adaptive multi-pass decoder,outperforms has adaptive multi-pass decoder,0.6027543544769287
translation,163,163,results,results,On,transformer backbone,results On transformer backbone,0.5497404336929321
translation,163,164,results,proposed architecture,on,rnnsearch backbone,proposed architecture on rnnsearch backbone,0.5723727345466614
translation,163,164,results,rnnsearch backbone,surpasses,transformer,rnnsearch backbone surpasses transformer,0.6309033036231995
translation,163,164,results,results,has,proposed architecture,results has proposed architecture,0.6126819849014282
translation,163,174,results,evaluator,with,27.86 % bleu score,evaluator with 27.86 % bleu score,0.5905326008796692
translation,163,174,results,evaluator,with,28.91 bleu score,evaluator with 28.91 bleu score,0.5772352814674377
translation,163,174,results,evaluator,with,much better,evaluator with much better,0.5981441140174866
translation,163,174,results,evaluator,is,much better,evaluator is much better,0.5390295386314392
translation,163,174,results,much better,than,strategies,much better than strategies,0.63801109790802
translation,163,174,results,strategies,of using,fixed number of rewriting turns,strategies of using fixed number of rewriting turns,0.7198901772499084
translation,163,174,results,results,show,evaluator,results show evaluator,0.6773472428321838
translation,163,175,results,gaps,between,oracle and the averaged performance,gaps between oracle and the averaged performance,0.59401935338974
translation,163,175,results,oracle and the averaged performance,by,rnnsearch and transformer,oracle and the averaged performance by rnnsearch and transformer,0.6007153391838074
translation,163,175,results,rnnsearch and transformer,with,fixed iterations,rnnsearch and transformer with fixed iterations,0.6891685128211975
translation,163,175,results,fixed iterations,are,1.92 % and 1.90 %,fixed iterations are 1.92 % and 1.90 %,0.5642474293708801
translation,163,175,results,results,has,gaps,results has gaps,0.5407499074935913
translation,163,184,results,rnnsearch w/ rewriter -evaluator,surpasses,adaptive multi-pass decoder,rnnsearch w/ rewriter -evaluator surpasses adaptive multi-pass decoder,0.6825711727142334
translation,163,184,results,adaptive multi-pass decoder,by,40.96 %,adaptive multi-pass decoder by 40.96 %,0.5465756058692932
translation,163,184,results,adaptive multi-pass decoder,by,10.35 %,adaptive multi-pass decoder by 10.35 %,0.5594674348831177
translation,163,184,results,40.96 %,on,wmt '14,40.96 % on wmt '14,0.5795990228652954
translation,163,184,results,40.96 %,on,wmt '18,40.96 % on wmt '18,0.5774648785591125
translation,163,184,results,10.35 %,on,wmt '18,10.35 % on wmt '18,0.5910174250602722
translation,163,184,results,results,has,rnnsearch w/ rewriter -evaluator,results has rnnsearch w/ rewriter -evaluator,0.596861720085144
translation,164,92,ablation-analysis,model compression,causes,performance degradation,model compression causes performance degradation,0.6748815774917603
translation,164,92,ablation-analysis,performance degradation,in,all language pairs,performance degradation in all language pairs,0.5314577221870422
translation,164,92,ablation-analysis,all language pairs,with respect to,all three performance metrics,all language pairs with respect to all three performance metrics,0.6040327548980713
translation,164,92,ablation-analysis,ablation analysis,has,model compression,ablation analysis has model compression,0.5278114676475525
translation,164,93,ablation-analysis,amount of degradation,in terms of,pearson 's correlation coefficient,amount of degradation in terms of pearson 's correlation coefficient,0.6978777050971985
translation,164,93,ablation-analysis,zero-shot,than in,supervised settings,zero-shot than in supervised settings,0.6153750419616699
translation,164,93,ablation-analysis,ablation analysis,has,amount of degradation,ablation analysis has amount of degradation,0.5638588070869446
translation,164,65,experimental-setup,our model,in,data parallelism,our model in data parallelism,0.4941577911376953
translation,164,65,experimental-setup,our model,optimized with,"adam ( kingma and ba , 2015 )","our model optimized with adam ( kingma and ba , 2015 )",0.7361123561859131
translation,164,65,experimental-setup,data parallelism,on,multiple nvidia tesla v100 gpus,data parallelism on multiple nvidia tesla v100 gpus,0.49679699540138245
translation,164,65,experimental-setup,multiple nvidia tesla v100 gpus,for,3 epochs,multiple nvidia tesla v100 gpus for 3 epochs,0.5970450043678284
translation,164,65,experimental-setup,3 epochs,with,batch size,3 epochs with batch size,0.6305070519447327
translation,164,65,experimental-setup,batch size,of,8,batch size of 8,0.6920418739318848
translation,164,65,experimental-setup,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,164,65,experimental-setup,learning rate,of,7e ?6,learning rate of 7e ?6,0.6316951513290405
translation,164,65,experimental-setup,dropout,with,0.15,dropout with 0.15,0.6097959876060486
translation,164,65,experimental-setup,0.15,applied to,activation function,0.15 applied to activation function,0.6489551663398743
translation,164,5,experiments,multilingual quality estimation system,combination of,pretrained language models,multilingual quality estimation system combination of pretrained language models,0.5205046534538269
translation,164,5,experiments,multilingual quality estimation system,combination of,multitask learning architectures,multilingual quality estimation system combination of multitask learning architectures,0.48732706904411316
translation,164,22,experiments,system,is,single multilingual sentence - level qe model,system is single multilingual sentence - level qe model,0.5313059687614441
translation,164,22,experiments,single multilingual sentence - level qe model,performs,very strongly,single multilingual sentence - level qe model performs very strongly,0.5751654505729675
translation,164,22,experiments,very strongly,in,multilingual and individual language pair settings,very strongly in multilingual and individual language pair settings,0.528854489326477
translation,164,6,model,iterative training pipeline,based on,finetuning,iterative training pipeline based on finetuning,0.652744472026825
translation,164,6,model,iterative training pipeline,pretraining with,large amounts of in-domain synthetic data,iterative training pipeline pretraining with large amounts of in-domain synthetic data,0.7140976786613464
translation,164,6,model,finetuning,with,gold ( labeled ) data,finetuning with gold ( labeled ) data,0.6489002704620361
translation,164,6,model,model,propose,iterative training pipeline,model propose iterative training pipeline,0.6773771047592163
translation,164,7,model,our system,via,knowledge distillation,our system via knowledge distillation,0.6971944570541382
translation,164,7,model,knowledge distillation,to reduce,parameters,knowledge distillation to reduce parameters,0.6958263516426086
translation,164,7,model,knowledge distillation,maintain,strong performance,knowledge distillation maintain strong performance,0.6547626256942749
translation,164,7,model,model,compress,our system,model compress our system,0.6689570546150208
translation,164,19,model,final stage,of,training pipeline,final stage of training pipeline,0.5850891470909119
translation,164,19,model,knowledge distillation,applied from,teacher,knowledge distillation applied from teacher,0.7436237931251526
translation,164,19,model,knowledge distillation,to reduce,model size,knowledge distillation to reduce model size,0.6424989700317383
translation,164,19,model,model size,while maintaining,competitive performance,model size while maintaining competitive performance,0.6844111084938049
translation,164,19,model,final stage,has,knowledge distillation,final stage has knowledge distillation,0.6066079139709473
translation,164,19,model,training pipeline,has,knowledge distillation,training pipeline has knowledge distillation,0.564251720905304
translation,164,19,model,teacher,has,to student model,teacher has to student model,0.5787895321846008
translation,164,19,model,model,During,final stage,model During final stage,0.7390756011009216
translation,164,78,results,base large + mtl +ikt iter=2,to,our large baseline model,base large + mtl +ikt iter=2 to our large baseline model,0.508807897567749
translation,164,78,results,results,Comparing,base large + mtl +ikt iter=2,results Comparing base large + mtl +ikt iter=2,0.6834195852279663
translation,164,80,results,results,has,our final compressed model base large + mtl +ikt iter=2 + kd,results has our final compressed model base large + mtl +ikt iter=2 + kd,0.5858021974563599
translation,164,88,results,ranking,based purely on,performance ( r2 ),ranking based purely on performance ( r2 ),0.6317307949066162
translation,164,88,results,systems,based purely on,performance ( r2 ),systems based purely on performance ( r2 ),0.6552966237068176
translation,164,88,results,base large + mtl + ikt,performs,strongly,base large + mtl + ikt performs strongly,0.6435219645500183
translation,164,88,results,ranking,has,systems,ranking has systems,0.6005805134773254
translation,164,88,results,ranking,has,base large + mtl + ikt,ranking has base large + mtl + ikt,0.5953369140625
translation,164,88,results,systems,has,base large + mtl + ikt,systems has base large + mtl + ikt,0.6009222865104675
translation,164,88,results,performance ( r2 ),has,base large + mtl + ikt,performance ( r2 ) has base large + mtl + ikt,0.570088803768158
translation,164,89,results,systems,ranked based on,performance and size ( r1 ),systems ranked based on performance and size ( r1 ),0.8026024103164673
translation,164,89,results,our compressed model base large + mtl +ikt + kd,ranks,very competitively,our compressed model base large + mtl +ikt + kd ranks very competitively,0.7174710035324097
translation,164,89,results,performance and size ( r1 ),has,our compressed model base large + mtl +ikt + kd,performance and size ( r1 ) has our compressed model base large + mtl +ikt + kd,0.5638907551765442
translation,164,90,results,outperforms,with,great margin,outperforms with great margin,0.724496066570282
translation,164,90,results,organizer 's baseline,in,all language pair settings,organizer 's baseline in all language pair settings,0.48698368668556213
translation,164,90,results,organizer 's baseline,with,great margin,organizer 's baseline with great margin,0.6496394872665405
translation,164,90,results,great margin,using,approximately 5.7 % more parameters,great margin using approximately 5.7 % more parameters,0.6864097714424133
translation,164,90,results,compressed model,has,outperforms,compressed model has outperforms,0.6268587112426758
translation,164,90,results,outperforms,has,organizer 's baseline,outperforms has organizer 's baseline,0.5754541158676147
translation,164,90,results,results,has,compressed model,results has compressed model,0.5399819612503052
translation,164,91,results,our compressed model,is,relatively less competitive,our compressed model is relatively less competitive,0.5617707967758179
translation,164,91,results,relatively less competitive,under,zeroshot,relatively less competitive under zeroshot,0.6979014873504639
translation,164,91,results,relatively less competitive,ranked based on,r1,relatively less competitive ranked based on r1,0.7844969630241394
translation,164,91,results,zeroshot,than in,supervised settings,zeroshot than in supervised settings,0.6431419253349304
translation,164,91,results,results,observe,our compressed model,results observe our compressed model,0.6201274394989014
translation,165,161,ablation-analysis,additional context sentences,led to,drop,additional context sentences led to drop,0.6368685364723206
translation,165,161,ablation-analysis,drop,in,translation quality,drop in translation quality,0.5339705348014832
translation,165,161,ablation-analysis,ablation analysis,using,additional context sentences,ablation analysis using additional context sentences,0.6474327445030212
translation,165,178,ablation-analysis,accuracy,improved,context,accuracy improved context,0.7125859260559082
translation,165,178,ablation-analysis,context,at,"k = 1 , 2 , and 4","context at k = 1 , 2 , and 4",0.6155545711517334
translation,165,178,ablation-analysis,ablation analysis,has,accuracy,ablation analysis has accuracy,0.4860230088233948
translation,165,7,model,wait -k simultaneous document- level nmt,keep,context encoder,wait -k simultaneous document- level nmt keep context encoder,0.5931258201599121
translation,165,7,model,wait -k simultaneous document- level nmt,replace,source sentence encoder and target language decoder,wait -k simultaneous document- level nmt replace source sentence encoder and target language decoder,0.5706071853637695
translation,165,7,model,source sentence encoder and target language decoder,with,wait -k equivalents,source sentence encoder and target language decoder with wait -k equivalents,0.6588603854179382
translation,165,7,model,model,propose,wait -k simultaneous document- level nmt,model propose wait -k simultaneous document- level nmt,0.6701879501342773
translation,165,135,results,snmt models,with,small wait - k's,snmt models with small wait - k's,0.667156994342804
translation,165,135,results,small wait - k's,give,poor translation quality,small wait - k's give poor translation quality,0.6201021075248718
translation,165,135,results,poor translation quality,compared to,full sentence models,poor translation quality compared to full sentence models,0.5964738130569458
translation,165,136,results,value of wait -k,naturally improves,translation quality,value of wait -k naturally improves translation quality,0.7297660708427429
translation,165,136,results,results,Increasing,value of wait -k,results Increasing value of wait -k,0.7041828036308289
translation,165,142,results,non-contextual versus contextual full- sentence models,when,context sentences,non-contextual versus contextual full- sentence models when context sentences,0.6008450388908386
translation,165,142,results,context sentences,used for,full sentence translation,context sentences used for full sentence translation,0.6007142663002014
translation,165,142,results,full sentence translation,for,russian ? english,full sentence translation for russian ? english,0.5502570271492004
translation,165,142,results,quality,for,"up to 1 , 2 , and 3 previous sentences","quality for up to 1 , 2 , and 3 previous sentences",0.5975647568702698
translation,165,142,results,quality,when,"up to 1 , 2 , and 3 previous sentences","quality when up to 1 , 2 , and 3 previous sentences",0.6265258193016052
translation,165,142,results,"up to 1 , 2 , and 3 previous sentences",as,context,"up to 1 , 2 , and 3 previous sentences as context",0.5475979447364807
translation,165,142,results,"up to 1 , 2 , and 3 previous sentences",used,"35.2 , 35.5 , and 35.7","up to 1 , 2 , and 3 previous sentences used 35.2 , 35.5 , and 35.7",0.5810658931732178
translation,165,142,results,context,used,"35.2 , 35.5 , and 35.7","context used 35.2 , 35.5 , and 35.7",0.6179614663124084
translation,165,142,results,context,is,"35.2 , 35.5 , and 35.7","context is 35.2 , 35.5 , and 35.7",0.5875972509384155
translation,165,142,results,results,has,non-contextual versus contextual full- sentence models,results has non-contextual versus contextual full- sentence models,0.5088571310043335
translation,165,143,results,baseline score,of,34.9,baseline score of 34.9,0.4753669500350952
translation,165,143,results,improvements,are,"0.3 , 0.6 , and 0.8 bleu","improvements are 0.3 , 0.6 , and 0.8 bleu",0.5662195086479187
translation,165,143,results,baseline score,has,improvements,baseline score has improvements,0.5423783659934998
translation,165,143,results,34.9,has,improvements,34.9 has improvements,0.5884265899658203
translation,165,144,results,english ? russian,compared to,baseline score,english ? russian compared to baseline score,0.6147302985191345
translation,165,144,results,baseline score,of,26.7,baseline score of 26.7,0.4759807288646698
translation,165,144,results,26.7,using,"up to 1 , 2 , and 3 previous sentences","26.7 using up to 1 , 2 , and 3 previous sentences",0.6472508311271667
translation,165,144,results,"up to 1 , 2 , and 3 previous sentences",as,context,"up to 1 , 2 , and 3 previous sentences as context",0.5475979447364807
translation,165,144,results,"up to 1 , 2 , and 3 previous sentences",lead to,translation quality improvements,"up to 1 , 2 , and 3 previous sentences lead to translation quality improvements",0.6241755485534668
translation,165,144,results,translation quality improvements,of,"0.3 , 0.5 , and 0.5","translation quality improvements of 0.3 , 0.5 , and 0.5",0.6022017598152161
translation,165,144,results,results,for,english ? russian,results for english ? russian,0.6031427383422852
translation,165,148,results,wait -k non-contextual model,against,contextual models,wait -k non-contextual model against contextual models,0.6845630407333374
translation,165,148,results,wait -k non-contextual model,shows,context,wait -k non-contextual model shows context,0.686348021030426
translation,165,148,results,contextual models,using,up to n context sentences,contextual models using up to n context sentences,0.6216821074485779
translation,165,148,results,context,is,helpful,context is helpful,0.6167652010917664
translation,165,148,results,helpful,in,snmt setting,helpful in snmt setting,0.5734366774559021
translation,165,148,results,results,Comparing,wait -k non-contextual model,results Comparing wait -k non-contextual model,0.6441411375999451
translation,165,149,results,up to 3 context sentences,for,wait -k values,up to 3 context sentences for wait -k values,0.643633246421814
translation,165,149,results,wait -k values,of,"1 , 2 , 4 , 6 and 8","wait -k values of 1 , 2 , 4 , 6 and 8",0.6637824773788452
translation,165,149,results,bleu score improvements,over,non-contextual counterparts,bleu score improvements over non-contextual counterparts,0.656390368938446
translation,165,149,results,non-contextual counterparts,are,"0.6 , 0.5 , 0.8 , 0.9 , 1.0","non-contextual counterparts are 0.6 , 0.5 , 0.8 , 0.9 , 1.0",0.5258858799934387
translation,165,149,results,"0.6 , 0.5 , 0.8 , 0.9 , 1.0",for,russian ? english translation,"0.6 , 0.5 , 0.8 , 0.9 , 1.0 for russian ? english translation",0.5642478466033936
translation,165,149,results,up to 3 context sentences,has,bleu score improvements,up to 3 context sentences has bleu score improvements,0.5502735376358032
translation,165,149,results,wait -k values,has,bleu score improvements,wait -k values has bleu score improvements,0.58975750207901
translation,165,149,results,results,When using,up to 3 context sentences,results When using up to 3 context sentences,0.6769933104515076
translation,165,152,results,more source language tokens,impact of,previous sentences,more source language tokens impact of previous sentences,0.5223551392555237
translation,165,152,results,previous sentences,as,context,previous sentences as context,0.4677509665489197
translation,165,152,results,context,seems to be,higher,context seems to be higher,0.7162926197052002
translation,165,154,results,maximum gain,for,snmt models,maximum gain for snmt models,0.6349858641624451
translation,165,154,results,snmt models,using,up to 3 context sentences,snmt models using up to 3 context sentences,0.6258951425552368
translation,165,154,results,up to 3 context sentences,which is,1.0,up to 3 context sentences which is 1.0,0.5824374556541443
translation,165,154,results,up to 3 context sentences,which is,0.6,up to 3 context sentences which is 0.6,0.5984074473381042
translation,165,154,results,1.0,for,russian ? english,1.0 for russian ? english,0.5386919975280762
translation,165,154,results,0.6,for,english ? russian,0.6 for english ? russian,0.5438637733459473
translation,165,154,results,results,note,maximum gain,results note maximum gain,0.6421985626220703
translation,165,155,results,corresponding gains,are,0.8 and 0.5,corresponding gains are 0.8 and 0.5,0.602645754814148
translation,165,155,results,full sentence models,has,corresponding gains,full sentence models has corresponding gains,0.580281138420105
translation,165,155,results,results,Compared to,full sentence models,results Compared to full sentence models,0.604547381401062
translation,165,176,results,bleu,seen that,results,bleu seen that results,0.6439124345779419
translation,165,176,results,results,are,almost the same,results are almost the same,0.5887760519981384
translation,165,176,results,almost the same,between,non-contextual and the contextual models,almost the same between non-contextual and the contextual models,0.6516829133033752
translation,165,176,results,results,In,bleu,results In bleu,0.5303049683570862
translation,165,176,results,results,are,almost the same,results are almost the same,0.5887760519981384
translation,166,49,baselines,in- domain nmt model,choose,in-house nmt system,in- domain nmt model choose in-house nmt system,0.7015467882156372
translation,166,49,baselines,in-house nmt system,trained on,general domain data,in-house nmt system trained on general domain data,0.7606261968612671
translation,166,40,experimental-setup,experimental setup,trained on,one tesla v100 gpu,experimental setup trained on one tesla v100 gpu,0.6856728792190552
translation,166,50,experimental-setup,lazyadam optimizer,used during,training phase,lazyadam optimizer used during training phase,0.6828209757804871
translation,166,50,experimental-setup,lazyadam optimizer,with,warm - up period,lazyadam optimizer with warm - up period,0.649267315864563
translation,166,50,experimental-setup,training phase,with,learning rate,training phase with learning rate,0.6454961895942688
translation,166,50,experimental-setup,training phase,with,warm - up period,training phase with warm - up period,0.6314598321914673
translation,166,50,experimental-setup,learning rate,of,1e ?5,learning rate of 1e ?5,0.6333191394805908
translation,166,50,experimental-setup,warm - up period,of,"16,000 steps","warm - up period of 16,000 steps",0.5718482732772827
translation,166,50,experimental-setup,experimental setup,has,lazyadam optimizer,experimental setup has lazyadam optimizer,0.546436071395874
translation,166,51,experimental-setup,dropout ratio,set to,0.1,dropout ratio set to 0.1,0.6955536007881165
translation,166,51,experimental-setup,batch size,for,training and validation,batch size for training and validation,0.6275224685668945
translation,166,51,experimental-setup,experimental setup,has,dropout ratio,experimental setup has dropout ratio,0.5205348134040833
translation,166,51,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,166,52,experimental-setup,beam search,is,4,beam search is 4,0.637763261795044
translation,166,73,results,finetuning order,generates,significantly different bleu scores,finetuning order generates significantly different bleu scores,0.6106511950492859
translation,166,73,results,significantly different bleu scores,with,strategy 1,significantly different bleu scores with strategy 1,0.6662737727165222
translation,166,73,results,strategy 1,achieving,bleu score + 8.89,strategy 1 achieving bleu score + 8.89,0.5976307988166809
translation,166,73,results,bleu score + 8.89,higher than,strategy 2,bleu score + 8.89 higher than strategy 2,0.6509386897087097
translation,166,73,results,results,observed,finetuning order,results observed finetuning order,0.6464998126029968
translation,166,79,results,finetuning,with,mixture of ood and ind data,finetuning with mixture of ood and ind data,0.6469685435295105
translation,166,79,results,mixture of ood and ind data,generates,minor improvements,mixture of ood and ind data generates minor improvements,0.631744921207428
translation,167,142,ablation-analysis,cnat decoding,with,reference length,cnat decoding with reference length,0.6048084497451782
translation,167,142,ablation-analysis,reference length,has,slightly ( 0.44 bleu ),reference length has slightly ( 0.44 bleu ),0.5450738668441772
translation,167,142,ablation-analysis,ablation analysis,see that,cnat decoding,ablation analysis see that cnat decoding,0.6161327362060547
translation,167,160,ablation-analysis,cnat,with,ar predictor,cnat with ar predictor,0.7167414426803589
translation,167,160,ablation-analysis,ar predictor,degenerates,standard lt model,ar predictor degenerates standard lt model,0.7047983407974243
translation,167,160,ablation-analysis,standard lt model,with,smaller latent space,standard lt model with smaller latent space,0.6141017079353333
translation,167,160,ablation-analysis,gatenet,has,cnat,gatenet has cnat,0.6540074944496155
translation,167,160,ablation-analysis,ablation analysis,Without,gatenet,ablation analysis Without gatenet,0.7669757604598999
translation,167,113,baselines,nat,builds upon,latent variables,nat builds upon latent variables,0.5305765271186829
translation,167,113,baselines,nat,with,extra autoregressive decoding,nat with extra autoregressive decoding,0.6651738286018372
translation,167,113,baselines,nat,with,extra autoregressive decoding,nat with extra autoregressive decoding,0.6651738286018372
translation,167,113,baselines,nat,with,iterative refinement,nat with iterative refinement,0.5888980031013489
translation,167,97,experimental-setup,shared subword embeddings,between,source language and target language,shared subword embeddings between source language and target language,0.6031944751739502
translation,167,97,experimental-setup,shared subword embeddings,between,separated subword embeddings,shared subword embeddings between separated subword embeddings,0.6509594321250916
translation,167,97,experimental-setup,source language and target language,for,wmt datasets,source language and target language for wmt datasets,0.5645779371261597
translation,167,97,experimental-setup,separated subword embeddings,for,iwslt14 dataset,separated subword embeddings for iwslt14 dataset,0.5213599801063538
translation,167,97,experimental-setup,experimental setup,use,shared subword embeddings,experimental setup use shared subword embeddings,0.5517089366912842
translation,167,100,experimental-setup,wmt tasks,use,transformer - base setting,wmt tasks use transformer - base setting,0.6863265633583069
translation,167,100,experimental-setup,transformer - base setting,of,vaswani et al . ( 2017 ),transformer - base setting of vaswani et al . ( 2017 ),0.5409786105155945
translation,167,100,experimental-setup,transformer - base setting,has,d model = 512,transformer - base setting has d model = 512,0.5984537601470947
translation,167,100,experimental-setup,transformer - base setting,has,d hidden = 512,transformer - base setting has d hidden = 512,0.589840829372406
translation,167,100,experimental-setup,transformer - base setting,has,p dropout = 0.3,transformer - base setting has p dropout = 0.3,0.5782301425933838
translation,167,100,experimental-setup,experimental setup,For,wmt tasks,experimental setup For wmt tasks,0.5155990719795227
translation,167,101,experimental-setup,hyperparameter,to,1.0 and 0.999,hyperparameter to 1.0 and 0.999,0.5462655425071716
translation,167,101,experimental-setup,experimental setup,set,hyperparameter,experimental setup set hyperparameter,0.6472918391227722
translation,167,103,experimental-setup,our model,based on,"open-source framework of fairseq ( ott et al. , 2019 )","our model based on open-source framework of fairseq ( ott et al. , 2019 )",0.6535086631774902
translation,167,103,experimental-setup,"open-source framework of fairseq ( ott et al. , 2019 )",has,optimization,"open-source framework of fairseq ( ott et al. , 2019 ) has optimization",0.5163224339485168
translation,167,103,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,167,104,experimental-setup,parameter,with,"adam ( kingma and ba , 2015 )","parameter with adam ( kingma and ba , 2015 )",0.6317201256752014
translation,167,104,experimental-setup,"adam ( kingma and ba , 2015 )",with,"? = ( 0.9 , 0.98 )","adam ( kingma and ba , 2015 ) with ? = ( 0.9 , 0.98 )",0.6335246562957764
translation,167,104,experimental-setup,experimental setup,optimize,parameter,experimental setup optimize parameter,0.7208694219589233
translation,167,105,experimental-setup,inverse square root learning rate scheduling,for,wmt tasks,inverse square root learning rate scheduling for wmt tasks,0.6120022535324097
translation,167,105,experimental-setup,inverse square root learning rate scheduling,from,3 ? 10 ?4 to 1 ? 10 ?5,inverse square root learning rate scheduling from 3 ? 10 ?4 to 1 ? 10 ?5,0.6071563363075256
translation,167,105,experimental-setup,linear annealing schedule,from,3 ? 10 ?4 to 1 ? 10 ?5,linear annealing schedule from 3 ? 10 ?4 to 1 ? 10 ?5,0.5621403455734253
translation,167,105,experimental-setup,3 ? 10 ?4 to 1 ? 10 ?5,for,iwslt14 task,3 ? 10 ?4 to 1 ? 10 ?5 for iwslt14 task,0.6700152158737183
translation,167,105,experimental-setup,experimental setup,use,inverse square root learning rate scheduling,experimental setup use inverse square root learning rate scheduling,0.6092888116836548
translation,167,105,experimental-setup,experimental setup,use,linear annealing schedule,experimental setup use linear annealing schedule,0.5853396058082581
translation,167,106,experimental-setup,mini-batch,consists of,2048 tokens,mini-batch consists of 2048 tokens,0.6574684381484985
translation,167,106,experimental-setup,mini-batch,consists of,32 k tokens,mini-batch consists of 32 k tokens,0.6433551907539368
translation,167,106,experimental-setup,2048 tokens,for,iwslt14,2048 tokens for iwslt14,0.6735619306564331
translation,167,106,experimental-setup,32 k tokens,for,wmt tasks,32 k tokens for wmt tasks,0.5636110901832581
translation,167,106,experimental-setup,experimental setup,has,mini-batch,experimental setup has mini-batch,0.5513284206390381
translation,167,8,model,cnat,learns,implicitly categorical codes,cnat learns implicitly categorical codes,0.6658589243888855
translation,167,8,model,implicitly categorical codes,as,latent variables,implicitly categorical codes as latent variables,0.5226574540138245
translation,167,8,model,latent variables,into,non-autoregressive decoding,latent variables into non-autoregressive decoding,0.5551664233207703
translation,167,8,model,model,propose,cnat,model propose cnat,0.7042653560638428
translation,167,9,model,interaction,among,categorical codes,interaction among categorical codes,0.5791633129119873
translation,167,9,model,interaction,remedies,missing dependencies,interaction remedies missing dependencies,0.727120041847229
translation,167,9,model,categorical codes,remedies,missing dependencies,categorical codes remedies missing dependencies,0.7296283841133118
translation,167,9,model,model,has,interaction,model has interaction,0.5850335955619812
translation,167,26,model,codes,in,unsupervised way,codes in unsupervised way,0.5634079575538635
translation,167,26,model,codes,use,each latent code,codes use each latent code,0.6865751147270203
translation,167,26,model,each latent code,to represent,fuzzy target category,each latent code to represent fuzzy target category,0.7083168029785156
translation,167,26,model,fuzzy target category,instead of,chunk,fuzzy target category instead of chunk,0.6543689370155334
translation,167,26,model,model,To learn,codes,model To learn codes,0.6299514770507812
translation,167,27,model,vector quantization,to discretize,target language,vector quantization to discretize target language,0.6131078004837036
translation,167,27,model,target language,to,latent space,target language to latent space,0.5409214496612549
translation,167,27,model,latent space,with,smaller number ( less than 128 ),latent space with smaller number ( less than 128 ),0.6019323468208313
translation,167,27,model,smaller number ( less than 128 ),of,latent variables,smaller number ( less than 128 ) of latent variables,0.5508917570114136
translation,167,27,model,smaller number ( less than 128 ),serve as,fuzzy word-class information,smaller number ( less than 128 ) serve as fuzzy word-class information,0.6156778335571289
translation,167,27,model,fuzzy word-class information,has,each target language word,fuzzy word-class information has each target language word,0.5539615750312805
translation,167,27,model,model,employ,vector quantization,model employ vector quantization,0.6115483641624451
translation,167,28,model,latent variables,with,conditional random fields,latent variables with conditional random fields,0.5606238842010498
translation,167,28,model,conditional random fields,has,crf,conditional random fields has crf,0.5664066672325134
translation,167,28,model,model,model,latent variables,model model latent variables,0.8459678888320923
translation,167,29,model,gated neural network,to form,decoder inputs,gated neural network to form decoder inputs,0.6018356084823608
translation,167,29,model,model,propose using,gated neural network,model propose using gated neural network,0.7334938049316406
translation,167,74,model,structural prediction module,instead of,standard autoregressive transformer,structural prediction module instead of standard autoregressive transformer,0.6346842646598816
translation,167,74,model,structural prediction module,to model,latent sequence,structural prediction module to model latent sequence,0.6914424300193787
translation,167,74,model,model,employ,structural prediction module,model employ structural prediction module,0.6134536266326904
translation,167,108,model,sequence -level knowledge distillation,to alleviate,multi-modality problem,sequence -level knowledge distillation to alleviate multi-modality problem,0.6347811818122864
translation,167,108,model,model,has,sequence -level knowledge distillation,model has sequence -level knowledge distillation,0.5716792941093445
translation,167,31,results,wmt14 and iwslt14,show,cnat,wmt14 and iwslt14 show cnat,0.6668321490287781
translation,167,31,results,cnat,achieves,new state - of - theart performance,cnat achieves new state - of - theart performance,0.676825225353241
translation,167,31,results,new state - of - theart performance,without,knowledge distillation,new state - of - theart performance without knowledge distillation,0.7148914337158203
translation,167,31,results,results,on,wmt14 and iwslt14,results on wmt14 and iwslt14,0.5191235542297363
translation,167,32,results,cnat,comparable to,current state - of - the - art iterative - based model,cnat comparable to current state - of - the - art iterative - based model,0.6538533568382263
translation,167,32,results,cnat,keeping,competitive decoding speedup,cnat keeping competitive decoding speedup,0.6373366713523865
translation,167,32,results,sequence-level knowledge distillation and reranking techniques,has,cnat,sequence-level knowledge distillation and reranking techniques has cnat,0.6499038338661194
translation,167,32,results,results,With,sequence-level knowledge distillation and reranking techniques,results With sequence-level knowledge distillation and reranking techniques,0.6147767901420593
translation,167,120,results,cnat,with,nat models,cnat with nat models,0.6472974419593811
translation,167,120,results,nat models,without using,advanced techniques,nat models without using advanced techniques,0.6873818039894104
translation,167,120,results,nat - dcrf,around,0.20 bleu,nat - dcrf around 0.20 bleu,0.6737522482872009
translation,167,120,results,0.20 bleu,in,en - de,0.20 bleu in en - de,0.6304543018341064
translation,167,120,results,results,compare,cnat,results compare cnat,0.6249629855155945
translation,167,126,results,cnat,achieves,better result,cnat achieves better result,0.6928337812423706
translation,167,126,results,better result,than,cmlm,better result than cmlm,0.6278223991394043
translation,167,126,results,better result,than,ir - nat,better result than ir - nat,0.6100801825523376
translation,167,126,results,cmlm,with,four iterations,cmlm with four iterations,0.6964075565338135
translation,167,126,results,ir - nat,with,ten iterations,ir - nat with ten iterations,0.7524122595787048
translation,167,126,results,results,see that,cnat,results see that cnat,0.6712899208068848
translation,167,134,results,outperforms,in,bleu,outperforms in bleu,0.5735594034194946
translation,167,134,results,outperforms,in,speed - up,outperforms in speed - up,0.5718624591827393
translation,167,134,results,our baselines,in,bleu,our baselines in bleu,0.51111900806427
translation,167,134,results,our baselines,in,speed - up,our baselines in speed - up,0.5452604293823242
translation,167,134,results,bleu,if,speed - up,bleu if speed - up,0.6514855623245239
translation,167,134,results,bleu,if,speed - up,bleu if speed - up,0.6514855623245239
translation,167,134,results,speed - up,if,bleu,speed - up if bleu,0.644989550113678
translation,167,134,results,cnat,has,outperforms,cnat has outperforms,0.6619529724121094
translation,167,134,results,outperforms,has,our baselines,outperforms has our baselines,0.6048160195350647
translation,167,134,results,results,has,cnat,results has cnat,0.5657079219818115
translation,167,141,results,z ref,as,latent sequence,z ref as latent sequence,0.5343813896179199
translation,167,141,results,model,achieves,surprisingly good performance,model achieves surprisingly good performance,0.6926600933074951
translation,167,141,results,z ref,has,model,z ref has model,0.5879243016242981
translation,167,141,results,latent sequence,has,model,latent sequence has model,0.5826120972633362
translation,167,141,results,results,With,z ref,results With z ref,0.6399682760238647
translation,167,157,results,crf - based predictor,surpasses,transformer predictor,crf - based predictor surpasses transformer predictor,0.6780765652656555
translation,167,157,results,3.5 bleu,with,gatenet,3.5 bleu with gatenet,0.6181840300559998
translation,167,157,results,gap,enlarges,5.3 bleu,gap enlarges 5.3 bleu,0.6597966551780701
translation,167,157,results,transformer predictor,has,3.5 bleu,transformer predictor has 3.5 bleu,0.5722205638885498
translation,167,157,results,transformer predictor,has,gap,transformer predictor has gap,0.5629046559333801
translation,167,157,results,results,see that,crf - based predictor,results see that crf - based predictor,0.6371232867240906
translation,167,158,results,crf,better than,transformer,crf better than transformer,0.7525316476821899
translation,167,158,results,transformer,to model,dependencies,transformer to model dependencies,0.7272002696990967
translation,167,158,results,dependencies,among,latent variables,dependencies among latent variables,0.5974878072738647
translation,167,158,results,dependencies,when,number of categories,dependencies when number of categories,0.6330239176750183
translation,167,158,results,latent variables,on,machine translation,latent variables on machine translation,0.5102861523628235
translation,167,158,results,machine translation,when,number of categories,machine translation when number of categories,0.5555306077003479
translation,167,158,results,number of categories,is,small,number of categories is small,0.5919758081436157
translation,167,162,results,outperforms,with,large margin ( around 4.0 bleu ),outperforms with large margin ( around 4.0 bleu ),0.6501865386962891
translation,167,162,results,nat baseline,with,large margin ( around 4.0 bleu ),nat baseline with large margin ( around 4.0 bleu ),0.6376803517341614
translation,167,162,results,outperforms,has,nat baseline,outperforms has nat baseline,0.5975924134254456
translation,167,170,results,  w/ pos tags  ,achieves,higher v- measure score,  w/ pos tags   achieves higher v- measure score,0.6598383188247681
translation,168,140,ablation-analysis,language -family -specific multilingual fine-tuning ( poetic langfamily ),helps,some of the languages,language -family -specific multilingual fine-tuning ( poetic langfamily ) helps some of the languages,0.5913714170455933
translation,168,140,ablation-analysis,some of the languages,compared to,mul- tilingual fine-tuning,some of the languages compared to mul- tilingual fine-tuning,0.6351904273033142
translation,168,140,ablation-analysis,mul- tilingual fine-tuning,on,all languages,mul- tilingual fine-tuning on all languages,0.5689965486526489
translation,168,140,ablation-analysis,all languages,has,poetic all ),all languages has poetic all ),0.6144130229949951
translation,168,97,baselines,fine-tuned mbart50,on,non-poetic data,fine-tuned mbart50 on non-poetic data,0.5747907757759094
translation,168,97,baselines,non-poetic data,from,opus100,non-poetic data from opus100,0.5953876376152039
translation,168,97,baselines,non-poetic data,for,respective languages,non-poetic data for respective languages,0.6002450585365295
translation,168,97,baselines,non-poetic bi ( opus ),has,fine-tuned mbart50,non-poetic bi ( opus ) has fine-tuned mbart50,0.616486132144928
translation,168,97,baselines,respective languages,has,bilingually,respective languages has bilingually,0.6157082319259644
translation,168,97,baselines,baselines,has,non-poetic bi ( opus ),baselines has non-poetic bi ( opus ),0.5754385590553284
translation,168,98,baselines,mbart - large-50-many-to-one model,implemented in,huggingface package,mbart - large-50-many-to-one model implemented in huggingface package,0.7268717288970947
translation,168,98,baselines,non-poetic multi ( ml50 ),has,mbart - large-50-many-to-one model,non-poetic multi ( ml50 ) has mbart - large-50-many-to-one model,0.6056343913078308
translation,168,98,baselines,baselines,has,non-poetic multi ( ml50 ),baselines has non-poetic multi ( ml50 ),0.5452695488929749
translation,168,101,baselines,fine - tuned mbart50,on,poetic data,fine - tuned mbart50 on poetic data,0.5718567967414856
translation,168,101,baselines,multilingually fine-tuned mbartlarge -50 - many -to-one,on,all poetic data combined,multilingually fine-tuned mbartlarge -50 - many -to-one on all poetic data combined,0.515739917755127
translation,168,101,baselines,poetic,has,fine - tuned mbart50,poetic has fine - tuned mbart50,0.6586257815361023
translation,168,101,baselines,fine - tuned mbart50,has,bilingually,fine - tuned mbart50 has bilingually,0.5902106761932373
translation,168,101,baselines,poetic all,has,multilingually fine-tuned mbartlarge -50 - many -to-one,poetic all has multilingually fine-tuned mbartlarge -50 - many -to-one,0.6051204204559326
translation,168,101,baselines,baselines,has,poetic,baselines has poetic,0.5968658328056335
translation,168,85,experimental-setup,fine- tuning,on,poetic data,fine- tuning on poetic data,0.5524193048477173
translation,168,85,experimental-setup,fine- tuning,use,mbart- large - 50 checkpoint,fine- tuning use mbart- large - 50 checkpoint,0.6697445511817932
translation,168,85,experimental-setup,poetic data,use,mbart- large - 50 checkpoint,poetic data use mbart- large - 50 checkpoint,0.6606501340866089
translation,168,85,experimental-setup,mbart- large - 50 checkpoint,from,"wolf et al. , 2020 )","mbart- large - 50 checkpoint from wolf et al. , 2020 )",0.5723932981491089
translation,168,85,experimental-setup,best checkpoint,based on,eval - bleu scores,best checkpoint based on eval - bleu scores,0.5999919772148132
translation,168,85,experimental-setup,experimental setup,For,fine- tuning,experimental setup For fine- tuning,0.6026358008384705
translation,168,86,experimental-setup,bilingual fine-tuning,on,non-poetic data,bilingual fine-tuning on non-poetic data,0.5647814869880676
translation,168,86,experimental-setup,bilingual fine-tuning,fine - tune,model,bilingual fine-tuning fine - tune model,0.763495683670044
translation,168,86,experimental-setup,model,for,3 epochs,model for 3 epochs,0.6671459078788757
translation,168,86,experimental-setup,experimental setup,For,bilingual fine-tuning,experimental setup For bilingual fine-tuning,0.585931658744812
translation,168,88,experimental-setup,multilingual fine-tuning,for,3 epochs,multilingual fine-tuning for 3 epochs,0.5703585743904114
translation,168,88,experimental-setup,3 epochs,for,poetic / non-poetic data,3 epochs for poetic / non-poetic data,0.5626093745231628
translation,168,88,experimental-setup,experimental setup,perform,multilingual fine-tuning,experimental setup perform multilingual fine-tuning,0.5392953753471375
translation,168,90,experimental-setup,( 2 - 4 ) nvidia a100 gpus,for,fine-tuning,( 2 - 4 ) nvidia a100 gpus for fine-tuning,0.6360605359077454
translation,168,90,experimental-setup,fine-tuning,has,pretrained checkpoints,fine-tuning has pretrained checkpoints,0.5827398300170898
translation,168,91,experimental-setup,fine- tuning mbart,on,non-poetic data,fine- tuning mbart on non-poetic data,0.5788912177085876
translation,168,91,experimental-setup,fine- tuning mbart,set,gradient_accumulation_steps,fine- tuning mbart set gradient_accumulation_steps,0.7151640057563782
translation,168,91,experimental-setup,fine- tuning mbart,set,batch size,fine- tuning mbart set batch size,0.6455985307693481
translation,168,91,experimental-setup,fine- tuning mbart,set,gra-dient_accumulation_steps,fine- tuning mbart set gra-dient_accumulation_steps,0.7333132028579712
translation,168,91,experimental-setup,fine- tuning mbart,for,poetic fine - tuning,fine- tuning mbart for poetic fine - tuning,0.6733381748199463
translation,168,91,experimental-setup,gradient_accumulation_steps,to,10,gradient_accumulation_steps to 10,0.6211486458778381
translation,168,91,experimental-setup,batch size,to,8,batch size to 8,0.6451488733291626
translation,168,91,experimental-setup,poetic fine - tuning,vary,batch size,poetic fine - tuning vary batch size,0.7376214861869812
translation,168,91,experimental-setup,poetic fine - tuning,set,gra-dient_accumulation_steps,poetic fine - tuning set gra-dient_accumulation_steps,0.7455388307571411
translation,168,91,experimental-setup,batch size,between,24 and 32,batch size between 24 and 32,0.6754686832427979
translation,168,91,experimental-setup,gra-dient_accumulation_steps,to,1,gra-dient_accumulation_steps to 1,0.6457852125167847
translation,168,91,experimental-setup,experimental setup,For,fine- tuning mbart,experimental setup For fine- tuning mbart,0.620622992515564
translation,168,91,experimental-setup,experimental setup,for,poetic fine - tuning,experimental setup for poetic fine - tuning,0.6115871071815491
translation,168,41,experiments,multilingual parallel corpus,consisting of,"more than 190,000 lines of poetry","multilingual parallel corpus consisting of more than 190,000 lines of poetry",0.6788309812545776
translation,168,41,experiments,"more than 190,000 lines of poetry",spanning,over six languages,"more than 190,000 lines of poetry spanning over six languages",0.6425338983535767
translation,168,87,experiments,multilingual fine-tuning,use,mbart- large - 50 - manyto-one- mmt,multilingual fine-tuning use mbart- large - 50 - manyto-one- mmt,0.5986308455467224
translation,168,49,results,multilingual fine-tuning,on,poetic data,multilingual fine-tuning on poetic data,0.5278515815734863
translation,168,49,results,bilingual fine-tuning,on,poetic data,bilingual fine-tuning on poetic data,0.564177393913269
translation,168,49,results,poetic data,has,outperforms,poetic data has outperforms,0.6526716947555542
translation,168,49,results,outperforms,has,bilingual fine-tuning,outperforms has bilingual fine-tuning,0.5369388461112976
translation,168,49,results,results,has,multilingual fine-tuning,results has multilingual fine-tuning,0.5445360541343689
translation,168,51,results,multilingual fine-tuning,on,languages belonging to the same language family,multilingual fine-tuning on languages belonging to the same language family,0.5175831913948059
translation,168,51,results,multilingual fine-tuning,leads to,improvement,multilingual fine-tuning leads to improvement,0.5986530780792236
translation,168,51,results,improvement,over,fine -tuning,improvement over fine -tuning,0.7059425711631775
translation,168,51,results,fine -tuning,on,all languages,fine -tuning on all languages,0.5088685154914856
translation,168,51,results,results,show,multilingual fine-tuning,results show multilingual fine-tuning,0.5725226998329163
translation,168,77,results,mbart initialization,leads to,significant gains,mbart initialization leads to significant gains,0.7009337544441223
translation,168,77,results,significant gains,across,low / medium - resource pairs ( < 10 m bi-text pairs ),significant gains across low / medium - resource pairs ( < 10 m bi-text pairs ),0.6926018595695496
translation,168,77,results,supervised sentence - level mt,has,mbart initialization,supervised sentence - level mt has mbart initialization,0.47706839442253113
translation,168,77,results,significant gains,has,up to 12 bleu points,significant gains has up to 12 bleu points,0.584752082824707
translation,168,77,results,results,For,supervised sentence - level mt,results For supervised sentence - level mt,0.5858060121536255
translation,168,130,results,multilingual fine-tuning,on,poetic data ( poetic all and poetic langfamily ),multilingual fine-tuning on poetic data ( poetic all and poetic langfamily ),0.5441567897796631
translation,168,130,results,mutilingual fine-tuning,on,"non-poetic data ( non -poetic multi ( ml50 , opus ) )","mutilingual fine-tuning on non-poetic data ( non -poetic multi ( ml50 , opus ) )",0.5687411427497864
translation,168,130,results,mutilingual fine-tuning,for,all languages,mutilingual fine-tuning for all languages,0.6180092096328735
translation,168,130,results,"non-poetic data ( non -poetic multi ( ml50 , opus ) )",for,all languages,"non-poetic data ( non -poetic multi ( ml50 , opus ) ) for all languages",0.5798380374908447
translation,168,130,results,multilingual fine-tuning,has,outperforms,multilingual fine-tuning has outperforms,0.605641782283783
translation,168,130,results,poetic data ( poetic all and poetic langfamily ),has,outperforms,poetic data ( poetic all and poetic langfamily ) has outperforms,0.617805004119873
translation,168,130,results,outperforms,has,mutilingual fine-tuning,outperforms has mutilingual fine-tuning,0.6118903160095215
translation,168,131,results,poetic-all and non-poetic multi,see,at least 2.5 point improvement,poetic-all and non-poetic multi see at least 2.5 point improvement,0.6285216808319092
translation,168,131,results,poetic-all and non-poetic multi,see,1 point improvement,poetic-all and non-poetic multi see 1 point improvement,0.6203365325927734
translation,168,131,results,at least 2.5 point improvement,in,bleu scores,at least 2.5 point improvement in bleu scores,0.517996072769165
translation,168,131,results,at least 2.5 point improvement,as,1 point improvement,at least 2.5 point improvement as 1 point improvement,0.4847949743270874
translation,168,131,results,1 point improvement,in,bertscore,1 point improvement in bertscore,0.5616973638534546
translation,168,131,results,bertscore,in,translation,bertscore in translation,0.5787289142608643
translation,168,131,results,translation,of,every language pair,translation of every language pair,0.6156539916992188
translation,168,131,results,results,Between,poetic-all and non-poetic multi,results Between poetic-all and non-poetic multi,0.5922127962112427
translation,168,132,results,recently developed metric comet,see that,best models,recently developed metric comet see that best models,0.6570767164230347
translation,168,132,results,best models,are,multilingually fine-tuned poetic models,best models are multilingually fine-tuned poetic models,0.5633667707443237
translation,168,132,results,results,For,recently developed metric comet,results For recently developed metric comet,0.6396515965461731
translation,168,143,results,our initial experiments,show that,fine-tuning,our initial experiments show that fine-tuning,0.5423126220703125
translation,168,143,results,fine-tuning,languages from,same language family,fine-tuning languages from same language family,0.731556236743927
translation,168,144,results,multilingual fine- tuned model,on,poetic data ( poetic all ),multilingual fine- tuned model on poetic data ( poetic all ),0.5350447297096252
translation,168,144,results,multilingual fine- tuned model,is,consistently better,multilingual fine- tuned model is consistently better,0.529857873916626
translation,168,144,results,poetic data ( poetic all ),across,all languages,poetic data ( poetic all ) across all languages,0.7009050846099854
translation,168,144,results,consistently better,than,bilingual fine- tuned model,consistently better than bilingual fine- tuned model,0.5616325736045837
translation,168,144,results,bilingual fine- tuned model,on,poetic data ( poetic ),bilingual fine- tuned model on poetic data ( poetic ),0.5598552227020264
translation,168,144,results,bilingual fine- tuned model,across,all languages,bilingual fine- tuned model across all languages,0.6846860647201538
translation,168,144,results,poetic data ( poetic ),across,all languages,poetic data ( poetic ) across all languages,0.7004618644714355
translation,168,144,results,results,notice,multilingual fine- tuned model,results notice multilingual fine- tuned model,0.6690261960029602
translation,168,150,results,other two multilingual models,fine-tuned on,non-poetic data,other two multilingual models fine-tuned on non-poetic data,0.7396948933601379
translation,168,150,results,multilingually fine-tuned poetic model,has,outperforms,multilingually fine-tuned poetic model has outperforms,0.6128490567207336
translation,168,150,results,outperforms,has,other two multilingual models,outperforms has other two multilingual models,0.5506925582885742
translation,168,150,results,results,shows that,multilingually fine-tuned poetic model,results shows that multilingually fine-tuned poetic model,0.610410749912262
translation,168,161,results,performance,across,all languages,performance across all languages,0.674177348613739
translation,168,161,results,performance,on,bleu and bertscore metrics,performance on bleu and bertscore metrics,0.5036957263946533
translation,168,161,results,bleu and bertscore metrics,evaluated on,multi( opus ) model,bleu and bertscore metrics evaluated on multi( opus ) model,0.7180153131484985
translation,168,161,results,style transfer experiments,has,decrease,style transfer experiments has decrease,0.542999804019928
translation,168,161,results,decrease,has,performance,decrease has performance,0.577602744102478
translation,169,8,ablation-analysis,impact of each metric,on,downstream task,impact of each metric on downstream task,0.5744466781616211
translation,169,8,ablation-analysis,downstream task,indicates,higher performance,downstream task indicates higher performance,0.7063343524932861
translation,169,8,ablation-analysis,higher performance,for,token oriented metrics,higher performance for token oriented metrics,0.624703049659729
translation,169,8,ablation-analysis,ablation analysis,deeper look at,impact of each metric,ablation analysis deeper look at impact of each metric,0.6446273922920227
translation,169,4,experiments,wmt,has,quality estimation ( qe ) critical error detection shared task,wmt has quality estimation ( qe ) critical error detection shared task,0.5529485940933228
translation,169,58,hyperparameters,model pretraining,on,synthetic data,model pretraining on synthetic data,0.5052957534790039
translation,169,58,hyperparameters,synthetic data,conducted for,one epoch,synthetic data conducted for one epoch,0.6906144022941589
translation,169,58,hyperparameters,hyperparameters,Training Procedure,model pretraining,hyperparameters Training Procedure model pretraining,0.6995393633842468
translation,169,60,hyperparameters,adamw optimizer,used with,"? 1 = 0.9 , ? 2 = 0.999 and = 1 ? 10 ?6","adamw optimizer used with ? 1 = 0.9 , ? 2 = 0.999 and = 1 ? 10 ?6",0.667661726474762
translation,169,60,hyperparameters,adamw optimizer,used with,weight decay,adamw optimizer used with weight decay,0.6276413202285767
translation,169,60,hyperparameters,weight decay,set to,0,weight decay set to 0,0.7196193933486938
translation,169,60,hyperparameters,hyperparameters,has,adamw optimizer,hyperparameters has adamw optimizer,0.4970969557762146
translation,169,60,hyperparameters,hyperparameters,has,weight decay,hyperparameters has weight decay,0.4915081858634949
translation,169,61,hyperparameters,linear learning rate warmup,used during,first 50 k updates,linear learning rate warmup used during first 50 k updates,0.6291428804397583
translation,169,61,hyperparameters,first 50 k updates,to reach,maximum value,first 50 k updates to reach maximum value,0.6657543778419495
translation,169,61,hyperparameters,maximum value,of,5 ? 10 ?6,maximum value of 5 ? 10 ?6,0.6286332607269287
translation,169,61,hyperparameters,hyperparameters,has,linear learning rate warmup,hyperparameters has linear learning rate warmup,0.49465465545654297
translation,169,62,hyperparameters,dropout rates,set to,0.1,dropout rates set to 0.1,0.6557121872901917
translation,169,62,hyperparameters,0.1,for,both the embeddings and the transformer blocks ( feedforward and attention layers ),0.1 for both the embeddings and the transformer blocks ( feedforward and attention layers ),0.5620985627174377
translation,169,62,hyperparameters,hyperparameters,has,dropout rates,hyperparameters has dropout rates,0.49439549446105957
translation,169,6,model,xlm -r checkpoint,perform,continued training,xlm -r checkpoint perform continued training,0.6377255320549011
translation,169,6,model,continued training,by modifying,learning objective,continued training by modifying learning objective,0.6818752884864807
translation,169,6,model,learning objective,switching from,masked language modeling,learning objective switching from masked language modeling,0.661052942276001
translation,169,6,model,masked language modeling,to,qe oriented signals,masked language modeling to qe oriented signals,0.5168946981430054
translation,169,6,model,masked language modeling,before,finetuning,masked language modeling before finetuning,0.630884051322937
translation,169,6,model,masked language modeling,before,ensembling,masked language modeling before ensembling,0.6596488356590271
translation,169,6,model,ensembling,has,models,ensembling has models,0.6020753979682922
translation,169,6,model,model,Starting from,xlm -r checkpoint,model Starting from xlm -r checkpoint,0.7262857556343079
translation,169,18,model,model,on,translation quality scores,model on translation quality scores,0.49658602476119995
translation,169,18,model,translation quality scores,computed with,automatic metrics,translation quality scores computed with automatic metrics,0.586223304271698
translation,169,18,model,model,pretrain,model,model pretrain model,0.6890357136726379
translation,169,7,results,test set,in terms of,correlation coefficient and f-score,test set in terms of correlation coefficient and f-score,0.7282595038414001
translation,169,7,results,test set,show,automatic metrics and synthetic data,test set show automatic metrics and synthetic data,0.6187930107116699
translation,169,7,results,correlation coefficient and f-score,show,automatic metrics and synthetic data,correlation coefficient and f-score show automatic metrics and synthetic data,0.6088599562644958
translation,169,7,results,automatic metrics and synthetic data,perform,well,automatic metrics and synthetic data perform well,0.6080212593078613
translation,169,7,results,well,for,pretraining,well for pretraining,0.6678944826126099
translation,169,7,results,results,on,test set,results on test set,0.582119882106781
translation,169,73,results,our ensembles,reach,highest performance,our ensembles reach highest performance,0.7147151827812195
translation,169,73,results,highest performance,according to,correlation score and f-measure,highest performance according to correlation score and f-measure,0.6587631106376648
translation,169,73,results,all our submissions,has,outperform,all our submissions has outperform,0.6051041483879089
translation,169,73,results,outperform,has,official baseline,outperform has official baseline,0.6099780797958374
translation,169,73,results,results,has,all our submissions,results has all our submissions,0.5594949722290039
translation,169,76,results,other shared task participants,in terms of,mcc and f1 scores,other shared task participants in terms of mcc and f1 scores,0.6287259459495544
translation,169,76,results,mcc and f1 scores,shows,our submissions,mcc and f1 scores shows our submissions,0.6828042268753052
translation,169,76,results,our submissions,ranked,first,our submissions ranked first,0.691051185131073
translation,169,76,results,our submissions,ranked,third,our submissions ranked third,0.7187755107879639
translation,169,76,results,our submissions,ranked,sixth,our submissions ranked sixth,0.6998251080513
translation,169,76,results,first,for,english - czech and english - german,first for english - czech and english - german,0.62645423412323
translation,169,76,results,third,for,english - chinese,third for english - chinese,0.6444228291511536
translation,169,76,results,sixth,for,english - japanese,sixth for english - japanese,0.648687481880188
translation,169,89,results,pretrained xlm -r,with,masked lm,pretrained xlm -r with masked lm,0.6560016870498657
translation,169,89,results,qe pretraining,on,synthetic data,qe pretraining on synthetic data,0.5259239077568054
translation,169,89,results,synthetic data,leads to,best results,synthetic data leads to best results,0.6773732304573059
translation,169,89,results,best results,on,four language pairs,best results on four language pairs,0.5382960438728333
translation,169,89,results,qe pretraining,performs,better,qe pretraining performs better,0.6651656031608582
translation,169,89,results,better,than,no checkpoint configuration,better than no checkpoint configuration,0.6187981367111206
translation,169,96,results,sentence - level metrics,during,pretraining,sentence - level metrics during pretraining,0.6207201480865479
translation,169,96,results,sentence - level metrics,not leading to,best downstream performance,sentence - level metrics not leading to best downstream performance,0.7064047455787659
translation,169,96,results,best downstream performance,compared to using,wordlevel metrics,best downstream performance compared to using wordlevel metrics,0.6525573134422302
translation,169,96,results,best downstream performance,combining,sentence and wordlevel quality indicators,best downstream performance combining sentence and wordlevel quality indicators,0.6913734674453735
translation,169,97,results,three sentencelevel metrics,has,ter and bleu,three sentencelevel metrics has ter and bleu,0.5862697958946228
translation,169,97,results,pretraining,has,ter and bleu,pretraining has ter and bleu,0.6041477918624878
translation,169,97,results,ter and bleu,has,outperform,ter and bleu has outperform,0.6341856718063354
translation,169,97,results,outperform,has,chrf,outperform has chrf,0.6428782343864441
translation,169,97,results,results,From,three sentencelevel metrics,results From three sentencelevel metrics,0.4949604868888855
translation,169,98,results,english - german and english - chinese,using,wordlevel metrics,english - german and english - chinese using wordlevel metrics,0.655830979347229
translation,169,98,results,wordlevel metrics,has,outperforms,wordlevel metrics has outperforms,0.6294698119163513
translation,169,98,results,outperforms,has,combination of all metrics,outperforms has combination of all metrics,0.5915457010269165
translation,169,98,results,results,For,english - german and english - chinese,results For english - german and english - chinese,0.5711372494697571
translation,170,8,model,prototype,supporting,midair hand gestures,prototype supporting midair hand gestures,0.5241450667381287
translation,170,8,model,midair hand gestures,for,cursor placement,midair hand gestures for cursor placement,0.6105016469955444
translation,170,8,model,midair hand gestures,for,text selection,midair hand gestures for text selection,0.5960211157798767
translation,170,8,model,midair hand gestures,for,deletion,midair hand gestures for deletion,0.6196562051773071
translation,170,8,model,midair hand gestures,for,reordering,midair hand gestures for reordering,0.6274139285087585
translation,170,8,model,model,develop,prototype,model develop prototype,0.6870610117912292
translation,171,183,baselines,our model,trained via,pretraining - then - finetuning approach,our model trained via pretraining - then - finetuning approach,0.7374586462974548
translation,171,183,baselines,model,trained via,pretraining - then - finetuning approach,model trained via pretraining - then - finetuning approach,0.7500295042991638
translation,171,136,hyperparameters,size,of,embedding dimension,size of embedding dimension,0.5521069169044495
translation,171,136,hyperparameters,size,of,positionwise feed -forward layers,size of positionwise feed -forward layers,0.6144965887069702
translation,171,136,hyperparameters,dimension,of,positionwise feed -forward layers,dimension of positionwise feed -forward layers,0.596585214138031
translation,171,136,hyperparameters,positionwise feed -forward layers,as,"768 , and 3,072","positionwise feed -forward layers as 768 , and 3,072",0.554146409034729
translation,171,136,hyperparameters,positionwise feed -forward layers,"256 ,","768 , and 3,072","positionwise feed -forward layers 256 , 768 , and 3,072",0.6574927568435669
translation,171,136,hyperparameters,embedding dimension,has,d emb,embedding dimension has d emb,0.5958759784698486
translation,171,136,hyperparameters,hidden vector dimension,has,d model,hidden vector dimension has d model,0.5716758966445923
translation,171,136,hyperparameters,hyperparameters,set,size,hyperparameters set size,0.6779580116271973
translation,171,136,hyperparameters,hyperparameters,set,dimension,hyperparameters set dimension,0.6708630919456482
translation,171,179,hyperparameters,sentences,using,beam search method,sentences using beam search method,0.696707546710968
translation,171,179,hyperparameters,beam search method,with,length normalization,beam search method with length normalization,0.6179832220077515
translation,171,179,hyperparameters,hyperparameters,generate,sentences,hyperparameters generate sentences,0.6634888052940369
translation,171,269,hyperparameters,number of topics,as,20,number of topics as 20,0.5127291679382324
translation,171,269,hyperparameters,hyperparameters,set,number of topics,hyperparameters set number of topics,0.6251891851425171
translation,171,22,model,topic modeling,to,translated historical records,topic modeling to translated historical records,0.538962721824646
translation,171,22,model,translated historical records,to efficiently discover,important historical events,translated historical records to efficiently discover important historical events,0.6741461753845215
translation,171,22,model,important historical events,over,last hundreds of years,important historical events over last hundreds of years,0.6091451644897461
translation,171,22,model,model,apply,topic modeling,model apply topic modeling,0.6139765381813049
translation,171,137,model,"shared encoder , the translation decoder , and the restoration encoder",consist of,"12 , 12 , and 6 layers","shared encoder , the translation decoder , and the restoration encoder consist of 12 , 12 , and 6 layers",0.6075014472007751
translation,171,137,model,model,has,"shared encoder , the translation decoder , and the restoration encoder","model has shared encoder , the translation decoder , and the restoration encoder",0.5137872695922852
translation,171,138,model,12 attention heads,for,each multi-head attention layer,12 attention heads for each multi-head attention layer,0.5966426134109497
translation,171,225,model,novel approach,to,translate and restore,novel approach to translate and restore,0.6296545267105103
translation,171,225,model,multi-task learning task,based on,self-attention mechanism,multi-task learning task based on self-attention mechanism,0.62245112657547
translation,171,225,model,translate and restore,has,historical records,translate and restore has historical records,0.5553282499313354
translation,171,225,model,historical records,has,of the joseon dynasty,historical records has of the joseon dynasty,0.43896666169166565
translation,171,225,model,model,proposed,novel approach,model proposed novel approach,0.7607510089874268
translation,171,156,results,top - 10 accuracy,of,our proposed model,top - 10 accuracy of our proposed model,0.5662732124328613
translation,171,156,results,top - 10 accuracy,is,almost 89 %,top - 10 accuracy is almost 89 %,0.564686119556427
translation,171,156,results,our proposed model,is,almost 89 %,our proposed model is almost 89 %,0.5752628445625305
translation,171,156,results,almost 89 %,indicates,high performance,almost 89 % indicates high performance,0.7115858197212219
translation,171,156,results,high performance,of,our model,high performance of our model,0.5855887532234192
translation,171,156,results,results,has,top - 10 accuracy,results has top - 10 accuracy,0.5597742795944214
translation,171,157,results,baseline model,trained without,multi-task learning,baseline model trained without multi-task learning,0.7657060027122498
translation,171,157,results,baseline model,performs,slightly better,baseline model performs slightly better,0.5986572504043579
translation,171,157,results,slightly better,than,one with multitask learning,slightly better than one with multitask learning,0.5576590299606323
translation,171,157,results,results,has,baseline model,results has baseline model,0.5361924171447754
translation,171,159,results,slightly lower,than,baseline model,slightly lower than baseline model,0.5552552938461304
translation,171,159,results,benefits,of,multi-task learning approach,benefits of multi-task learning approach,0.5738663673400879
translation,171,159,results,multi-task learning approach,significantly manifested in,nmt task,multi-task learning approach significantly manifested in nmt task,0.6349841952323914
translation,171,160,results,our model,shows,acceptable performances,our model shows acceptable performances,0.6703835725784302
translation,171,160,results,acceptable performances,on,restoration and the translation tasks,acceptable performances on restoration and the translation tasks,0.5465137958526611
translation,171,160,results,acceptable performances,both,restoration and the translation tasks,acceptable performances both restoration and the translation tasks,0.6528264880180359
translation,171,160,results,our model,learns,purpose of our research well,our model learns purpose of our research well,0.6803470849990845
translation,171,160,results,purpose of our research well,via,multi-task learning,purpose of our research well via multi-task learning,0.6194449663162231
translation,171,160,results,results,via,multi-task learning,results via multi-task learning,0.5312705039978027
translation,171,160,results,results,has,our model,results has our model,0.5871725678443909
translation,171,181,results,results,obtained with,beam size,results obtained with beam size,0.6635313034057617
translation,171,181,results,beam size,of,3,beam size of 3,0.6917811036109924
translation,171,181,results,3,are,slightly better,3 are slightly better,0.6188271641731262
translation,171,181,results,slightly better,than,greedy search method,slightly better than greedy search method,0.5817762017250061
translation,171,181,results,results,obtained with,beam size,results obtained with beam size,0.6635313034057617
translation,171,181,results,results,has,results,results has results,0.48582205176353455
translation,171,182,results,bleu score,of,our model,bleu score of our model,0.527224063873291
translation,171,182,results,our model,obtained as,0.5410,our model obtained as 0.5410,0.6157165765762329
translation,171,182,results,0.5410,indicates that,our model,0.5410 indicates that our model,0.6611624956130981
translation,171,182,results,our model,performs,reasonably well,our model performs reasonably well,0.6186372637748718
translation,171,182,results,reasonably well,compared to,other recent models,reasonably well compared to other recent models,0.651328980922699
translation,171,182,results,other recent models,trained in,other languages,other recent models trained in other languages,0.7444151639938354
translation,171,182,results,results,has,bleu score,results has bleu score,0.5436024069786072
translation,171,184,results,bleu score,is,0.3755,bleu score is 0.3755,0.5640238523483276
translation,171,184,results,0.3755,is,5.9 % higher,0.3755 is 5.9 % higher,0.5943448543548584
translation,171,184,results,0.3755,is,28.7 % lower,0.3755 is 28.7 % lower,0.5696500539779663
translation,171,184,results,5.9 % higher,than,model,5.9 % higher than model,0.5437468886375427
translation,171,184,results,28.7 % lower,than,our multi-task learning approach,28.7 % lower than our multi-task learning approach,0.5359959602355957
translation,171,186,results,multi-task learning,fully utilizes,paired and unpaired data,multi-task learning fully utilizes paired and unpaired data,0.7127678394317627
translation,171,186,results,paired and unpaired data,for,translation task,paired and unpaired data for translation task,0.6129346489906311
translation,171,186,results,paired and unpaired data,compared to,pretraining - thenfinetuning approach,paired and unpaired data compared to pretraining - thenfinetuning approach,0.6939575672149658
translation,172,156,ablation-analysis,vq - semface,under,encoder pre-training,vq - semface under encoder pre-training,0.6456374526023865
translation,172,156,ablation-analysis,most influential auxiliary loss,is,diversity loss l d,most influential auxiliary loss is diversity loss l d,0.5643478631973267
translation,172,156,ablation-analysis,diversity loss l d,contributes,4.33 bleu scores,diversity loss l d contributes 4.33 bleu scores,0.5881021022796631
translation,172,156,ablation-analysis,4.33 bleu scores,in,final results,4.33 bleu scores in final results,0.48006653785705566
translation,172,156,ablation-analysis,vq - semface,has,most influential auxiliary loss,vq - semface has most influential auxiliary loss,0.5237374901771545
translation,172,156,ablation-analysis,ablation analysis,for,vq - semface,ablation analysis for vq - semface,0.6380702257156372
translation,172,158,ablation-analysis,l mse and l2,have,sim-ilar effect,l mse and l2 have sim-ilar effect,0.5752482414245605
translation,172,158,ablation-analysis,sim-ilar effect,stabilizes,training,sim-ilar effect stabilizes training,0.6718958616256714
translation,172,158,ablation-analysis,about 1 bleu score,in,final result,about 1 bleu score in final result,0.5006572604179382
translation,172,158,ablation-analysis,ablation analysis,has,l mse and l2,ablation analysis has l mse and l2,0.5333366394042969
translation,172,25,baselines,two types of semantic interfaces,namely,cl - semface,two types of semantic interfaces namely cl - semface,0.7008050680160522
translation,172,25,baselines,two types of semantic interfaces,namely,vq - semface,two types of semantic interfaces namely vq - semface,0.7009145617485046
translation,172,46,baselines,vq - semface,automatically learns,context - aware vector quantized ( vq ) embedding space,vq - semface automatically learns context - aware vector quantized ( vq ) embedding space,0.7087075710296631
translation,172,46,baselines,context - aware vector quantized ( vq ) embedding space,as,interface,context - aware vector quantized ( vq ) embedding space as interface,0.5714379549026489
translation,172,46,baselines,interface,during,pre-training,interface during pre-training,0.7046284079551697
translation,172,46,baselines,baselines,has,third,baselines has third,0.6360859870910645
translation,172,111,baselines,"xlm ( lample and conneau , 2019 )",pre-trains,transformer encoder,"xlm ( lample and conneau , 2019 ) pre-trains transformer encoder",0.769840657711029
translation,172,111,baselines,transformer encoder,with,mlm or clm loss,transformer encoder with mlm or clm loss,0.6319743990898132
translation,172,111,baselines,transformer encoder,initializes,encoder and the decoder,transformer encoder initializes encoder and the decoder,0.6478384733200073
translation,172,111,baselines,encoder and the decoder,with,pre-trained model,encoder and the decoder with pre-trained model,0.6554431319236755
translation,172,113,baselines,mbart,pre-trains,whole sequenceto-sequence architecture,mbart pre-trains whole sequenceto-sequence architecture,0.7261821031570435
translation,172,113,baselines,whole sequenceto-sequence architecture,with,denoising autoencoder loss,whole sequenceto-sequence architecture with denoising autoencoder loss,0.6088876128196716
translation,172,113,baselines,denoising autoencoder loss,on,multilingual corpus,denoising autoencoder loss on multilingual corpus,0.5097305774688721
translation,172,53,hyperparameters,monolingual corpora,of,two languages,monolingual corpora of two languages,0.5372956395149231
translation,172,53,hyperparameters,monolingual corpora,learn,joint bpe,monolingual corpora learn joint bpe,0.6311619877815247
translation,172,53,hyperparameters,cross-lingual bpe embeddings,with,"vecmap ( artetxe et al. , 2018 )","cross-lingual bpe embeddings with vecmap ( artetxe et al. , 2018 )",0.562263548374176
translation,172,53,hyperparameters,hyperparameters,concatenate,monolingual corpora,hyperparameters concatenate monolingual corpora,0.6441658139228821
translation,172,112,hyperparameters,parameters,of,cross-attention module,parameters of cross-attention module,0.5624302625656128
translation,172,112,hyperparameters,cross-attention module,are,randomly initialized,cross-attention module are randomly initialized,0.5370144844055176
translation,172,112,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,172,123,hyperparameters,both encoder and decoder,use,6 - layer transformers,both encoder and decoder use 6 - layer transformers,0.638118326663971
translation,172,123,hyperparameters,both encoder and decoder,use,dropout rate,both encoder and decoder use dropout rate,0.6412663459777832
translation,172,123,hyperparameters,6 - layer transformers,with,embedding and hidden dimensions,6 - layer transformers with embedding and hidden dimensions,0.6263701319694519
translation,172,123,hyperparameters,6 - layer transformers,with,dropout rate,6 - layer transformers with dropout rate,0.6377367377281189
translation,172,123,hyperparameters,embedding and hidden dimensions,of,"1024 , 8 attention heads","embedding and hidden dimensions of 1024 , 8 attention heads",0.5733141899108887
translation,172,123,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,172,123,hyperparameters,hyperparameters,For,both encoder and decoder,hyperparameters For both encoder and decoder,0.573896586894989
translation,172,124,hyperparameters,maximum sequence length,is,256,maximum sequence length is 256,0.5634257197380066
translation,172,124,hyperparameters,batch size,is,128,batch size is 128,0.6402363181114197
translation,172,124,hyperparameters,hyperparameters,has,maximum sequence length,hyperparameters has maximum sequence length,0.5047650337219238
translation,172,124,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,172,125,hyperparameters,adam optimizer,for,pre-training,adam optimizer for pre-training,0.5926895141601562
translation,172,125,hyperparameters,adam optimizer,for,fine-tuning,adam optimizer for fine-tuning,0.5911228060722351
translation,172,125,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,172,126,hyperparameters,learning rate,is,0.0001,learning rate is 0.0001,0.5682897567749023
translation,172,126,hyperparameters,pre-training,has,learning rate,pre-training has learning rate,0.5576084852218628
translation,172,126,hyperparameters,hyperparameters,During,pre-training,hyperparameters During pre-training,0.659151017665863
translation,172,127,hyperparameters,learning rate,is,0.0001,learning rate is 0.0001,0.5682897567749023
translation,172,127,hyperparameters,0.0001,with,"4,000 warm - up steps","0.0001 with 4,000 warm - up steps",0.5864596366882324
translation,172,127,hyperparameters,decayed,based on,inverse square root,decayed based on inverse square root,0.7068706750869751
translation,172,127,hyperparameters,inverse square root,of,update number,inverse square root of update number,0.6249436140060425
translation,172,127,hyperparameters,mt fine-tuning,has,learning rate,mt fine-tuning has learning rate,0.47863855957984924
translation,172,127,hyperparameters,hyperparameters,During,mt fine-tuning,hyperparameters During mt fine-tuning,0.6452347636222839
translation,172,6,model,better pre-training method,for,nmt,better pre-training method for nmt,0.6200835704803467
translation,172,6,model,semantic interface ( semface ),between,pre-trained encoder and the pre-trained decoder,semantic interface ( semface ) between pre-trained encoder and the pre-trained decoder,0.631793737411499
translation,172,6,model,model,propose,better pre-training method,model propose better pre-training method,0.6903908848762512
translation,172,7,model,two types of semantic interfaces,including,cl - semface,two types of semantic interfaces including cl - semface,0.6740104556083679
translation,172,7,model,cl - semface,regards,cross-lingual embeddings,cl - semface regards cross-lingual embeddings,0.6338741183280945
translation,172,7,model,cross-lingual embeddings,as,interface,cross-lingual embeddings as interface,0.5533100366592407
translation,172,7,model,vq - semface,employs,vector quantized embeddings,vq - semface employs vector quantized embeddings,0.5771810412406921
translation,172,7,model,vector quantized embeddings,to constrain,encoder outputs and decoder inputs,vector quantized embeddings to constrain encoder outputs and decoder inputs,0.6834250688552856
translation,172,7,model,encoder outputs and decoder inputs,in,same language - independent space,encoder outputs and decoder inputs in same language - independent space,0.48932600021362305
translation,172,7,model,model,propose,two types of semantic interfaces,model propose two types of semantic interfaces,0.663769543170929
translation,172,26,model,former,takes,"trained unsupervised cross-lingual embeddings ( artetxe et al. , 2018 )","former takes trained unsupervised cross-lingual embeddings ( artetxe et al. , 2018 )",0.5873008966445923
translation,172,26,model,"trained unsupervised cross-lingual embeddings ( artetxe et al. , 2018 )",as,interface,"trained unsupervised cross-lingual embeddings ( artetxe et al. , 2018 ) as interface",0.47953417897224426
translation,172,26,model,interface,for,encoder and decoder pretraining,interface for encoder and decoder pretraining,0.6168519258499146
translation,172,26,model,model,has,former,model has former,0.6011738181114197
translation,172,27,model,language - independent vector quantized ( vq ) embeddings ( semantic unites ),as,interface,language - independent vector quantized ( vq ) embeddings ( semantic unites ) as interface,0.5200010538101196
translation,172,27,model,interface,to map,encoder outputs and decoder inputs,interface to map encoder outputs and decoder inputs,0.6897774338722229
translation,172,27,model,encoder outputs and decoder inputs,into,shared vq space,encoder outputs and decoder inputs into shared vq space,0.5770445466041565
translation,172,38,model,encoder,pre-trained to map,input,encoder pre-trained to map input,0.7685427069664001
translation,172,38,model,input,from,monolingual semantic space,input from monolingual semantic space,0.541222333908081
translation,172,38,model,monolingual semantic space,into,interface,monolingual semantic space into interface,0.5570921301841736
translation,172,38,model,monolingual semantic space,into,interface,monolingual semantic space into interface,0.5570921301841736
translation,172,38,model,decoder,pre-trained to use,content,decoder pre-trained to use content,0.7421746253967285
translation,172,38,model,content,from,interface,content from interface,0.6091923713684082
translation,172,38,model,content,via,cross attention module,content via cross attention module,0.7049906849861145
translation,172,38,model,cross attention module,to finish,decoding,cross attention module to finish decoding,0.6233600378036499
translation,172,38,model,model,has,encoder,model has encoder,0.5940273404121399
translation,172,38,model,model,has,decoder,model has decoder,0.6226420402526855
translation,172,39,model,parameters,of,encoder and the decoder,parameters of encoder and the decoder,0.6080167889595032
translation,172,39,model,model,has,parameters,model has parameters,0.49046650528907776
translation,172,47,model,the decoder,to generate or leverage,language - independent information,the decoder to generate or leverage language - independent information,0.7124860286712646
translation,172,47,model,model,define,language - independent interface,model define language - independent interface,0.6673058867454529
translation,172,52,model,cl - semface,uses,cross-lingual embedding space,cl - semface uses cross-lingual embedding space,0.5889499187469482
translation,172,52,model,- semface,uses,cross-lingual embedding space,- semface uses cross-lingual embedding space,0.6114577054977417
translation,172,52,model,cross-lingual embedding space,as,interface,cross-lingual embedding space as interface,0.5877951383590698
translation,172,52,model,interface,between,encoder and the decoder,interface between encoder and the decoder,0.6917325258255005
translation,172,52,model,encoder and the decoder,during,pre-training,encoder and the decoder during pre-training,0.6777149438858032
translation,172,52,model,cl - semface,has,- semface,cl - semface has - semface,0.6840790510177612
translation,172,52,model,model,has,cl - semface,model has cl - semface,0.6368258595466614
translation,172,67,model,context-dependent semantic units freely,propose,another interface type,context-dependent semantic units freely propose another interface type,0.6240077614784241
translation,172,67,model,another interface type,has,vector quantized embeddings,another interface type has vector quantized embeddings,0.5992301106452942
translation,172,67,model,model,to learn,context-dependent semantic units freely,model to learn context-dependent semantic units freely,0.6359671354293823
translation,172,67,model,model,propose,another interface type,model propose another interface type,0.687286913394928
translation,172,28,results,supervised and unsupervised translation tasks,demonstrate,semface,supervised and unsupervised translation tasks demonstrate semface,0.5944075584411621
translation,172,28,results,semface,effectively connects,pre-trained encoder and decoder,semface effectively connects pre-trained encoder and decoder,0.7209715843200684
translation,172,28,results,semface,achieves,significant improvement,semface achieves significant improvement,0.7048438191413879
translation,172,28,results,significant improvement,by,3.7 and 1.5 bleu points,significant improvement by 3.7 and 1.5 bleu points,0.5620036721229553
translation,172,32,results,proposed cl - semface and vq - semface,lead to,significant improvements,proposed cl - semface and vq - semface lead to significant improvements,0.652769923210144
translation,172,32,results,significant improvements,of,3.38 and 3.76 blue points,significant improvements of 3.38 and 3.76 blue points,0.5582583546638489
translation,172,32,results,3.38 and 3.76 blue points,on,low-resource language pairs,3.38 and 3.76 blue points on low-resource language pairs,0.5536891222000122
translation,172,32,results,results,has,proposed cl - semface and vq - semface,results has proposed cl - semface and vq - semface,0.5870599746704102
translation,172,137,results,significantly outperform,with,average improvement,significantly outperform with average improvement,0.6978784203529358
translation,172,137,results,non-pre-training transformer,with,average improvement,non-pre-training transformer with average improvement,0.6481949090957642
translation,172,137,results,average improvement,of,over 3 bleu scores,average improvement of over 3 bleu scores,0.5335116386413574
translation,172,137,results,our proposed methods cl - semface and vq - semface,has,significantly outperform,our proposed methods cl - semface and vq - semface has significantly outperform,0.5343937873840332
translation,172,137,results,significantly outperform,has,non-pre-training transformer,significantly outperform has non-pre-training transformer,0.6204675436019897
translation,172,138,results,outperform,by,0.8 to 1.2 bleu scores,outperform by 0.8 to 1.2 bleu scores,0.5841682553291321
translation,172,138,results,strong baseline mbart,has,our methods,strong baseline mbart has our methods,0.5311663150787354
translation,172,138,results,results,Compared with,strong baseline mbart,results Compared with strong baseline mbart,0.716532826423645
translation,172,139,results,vq - semface,better than,cl - semface,vq - semface better than cl - semface,0.7501276731491089
translation,172,139,results,most translation directions,has,vq - semface,most translation directions has vq - semface,0.5424399375915527
translation,172,139,results,results,For,most translation directions,results For most translation directions,0.5774847865104675
translation,172,145,results,baseline xlm,over,1 bleu score,baseline xlm over 1 bleu score,0.622433602809906
translation,172,145,results,our proposed methods,has,significantly outperform,our proposed methods has significantly outperform,0.5861532092094421
translation,172,145,results,significantly outperform,has,baseline xlm,significantly outperform has baseline xlm,0.5757752656936646
translation,172,146,results,mbart,obtain,improvement,mbart obtain improvement,0.6085729598999023
translation,172,146,results,improvement,of,nearly 0.9 bleu score,improvement of nearly 0.9 bleu score,0.549415111541748
translation,172,146,results,nearly 0.9 bleu score,has,cl - semface ),nearly 0.9 bleu score has cl - semface ),0.6032027006149292
translation,172,147,results,performance,of,cl - semface,performance of cl - semface,0.6822507977485657
translation,172,147,results,cl - semface,better than,vq - semface,cl - semface better than vq - semface,0.7474362850189209
translation,172,159,results,two losses,is,comparable,two losses is comparable,0.5615201592445374
translation,172,159,results,comparable,with,mlm,comparable with mlm,0.7333300113677979
translation,172,159,results,decoder pre-training,has,performance,decoder pre-training has performance,0.5586243867874146
translation,172,159,results,mlm,has,slightly better,mlm has slightly better,0.6193743348121643
translation,172,159,results,results,For,decoder pre-training,results For decoder pre-training,0.5850468277931213
translation,172,165,results,number of parallel training data,less than,10 6.7 ? 5 m,number of parallel training data less than 10 6.7 ? 5 m,0.6668114066123962
translation,172,165,results,model,with,pre-training,model with pre-training,0.6503974199295044
translation,172,165,results,non-pre-training model,by,3 to 5 bleu scores,non-pre-training model by 3 to 5 bleu scores,0.57987380027771
translation,172,165,results,non-pre-training model,about,3 to 5 bleu scores,non-pre-training model about 3 to 5 bleu scores,0.5761853456497192
translation,172,165,results,number of parallel training data,has,model,number of parallel training data has model,0.5410009026527405
translation,172,165,results,pre-training,has,significantly outperforms,pre-training has significantly outperforms,0.6116904616355896
translation,172,165,results,significantly outperforms,has,non-pre-training model,significantly outperforms has non-pre-training model,0.6009405851364136
translation,173,22,baselines,light - weight model,based on,comet framework,light - weight model based on comet framework,0.6957051753997803
translation,173,22,baselines,light - weight model,replaces,original xlm -r large encoder,light - weight model replaces original xlm -r large encoder,0.6914036273956299
translation,173,22,baselines,original xlm -r large encoder,with,"minilmv2 ( wang et al. , 2020 )","original xlm -r large encoder with minilmv2 ( wang et al. , 2020 )",0.5441149473190308
translation,173,22,baselines,cometinho,has,light - weight model,cometinho has light - weight model,0.5368607640266418
translation,173,22,baselines,baselines,present,cometinho,baselines present cometinho,0.7227975130081177
translation,173,151,experiments,contrastive inference times,tested using,2.3 ghz intel core i5,contrastive inference times tested using 2.3 ghz intel core i5,0.6380003690719604
translation,173,151,experiments,contrastive inference times,tested using,nvidia t4,contrastive inference times tested using nvidia t4,0.679568350315094
translation,173,151,experiments,2.3 ghz intel core i5,for,cpu,2.3 ghz intel core i5 for cpu,0.4794623553752899
translation,173,151,experiments,nvidia t4,for,gpu,nvidia t4 for gpu,0.6042346358299255
translation,173,4,model,ist,to,wmt 2021 metrics shared task,ist to wmt 2021 metrics shared task,0.5625895261764526
translation,173,4,model,model,present,joint contribution,model present joint contribution,0.6716759204864502
translation,173,8,model,a light- weight comet model,that is,19x faster,a light- weight comet model that is 19x faster,0.6560115218162537
translation,173,8,model,19x faster,on,cpu,19x faster on cpu,0.5117469429969788
translation,173,8,model,cpu,than,original model,cpu than original model,0.5586573481559753
translation,173,8,model,cometinho,has,a light- weight comet model,cometinho has a light- weight comet model,0.5224980115890503
translation,173,8,model,model,present,cometinho,model present cometinho,0.7051509022712708
translation,173,7,results,referencefree comet models,becoming,competitive,referencefree comet models becoming competitive,0.6839191913604736
translation,173,7,results,competitive,with,reference - based models,competitive with reference - based models,0.6466148495674133
translation,173,7,results,outperforming,has,best comet model,outperforming has best comet model,0.6240172982215881
translation,173,7,results,outperforming,has,from 2020,outperforming has from 2020,0.6215044856071472
translation,173,7,results,best comet model,has,from 2020,best comet model has from 2020,0.5926249027252197
translation,173,24,results,qe as a metric,show that,reference - free evaluation models,qe as a metric show that reference - free evaluation models,0.510184109210968
translation,173,24,results,reference - free evaluation models,reach,surprisingly high correlations,reference - free evaluation models reach surprisingly high correlations,0.7044902443885803
translation,173,24,results,surprisingly high correlations,with,human judgements,surprisingly high correlations with human judgements,0.6219187378883362
translation,173,24,results,competitive,with,corresponding referencebased models,competitive with corresponding referencebased models,0.6672043800354004
translation,173,24,results,results,For,qe as a metric,results For qe as a metric,0.6141601800918579
translation,173,101,results,fine-tuning,on,mqm data,fine-tuning on mqm data,0.5604891777038574
translation,173,101,results,fine-tuning,gave,boost,fine-tuning gave boost,0.6292290091514587
translation,173,101,results,boost,in,performance,boost in performance,0.5569588541984558
translation,173,101,results,reference - free metrics,has,fine-tuning,reference - free metrics has fine-tuning,0.5381745100021362
translation,173,101,results,results,For,reference - free metrics,results For reference - free metrics,0.545449435710907
translation,173,104,results,openkiwi model,has,competitive correlations,openkiwi model has competitive correlations,0.5590944290161133
translation,173,104,results,competitive correlations,when looking to,other trainable metrics,competitive correlations when looking to other trainable metrics,0.6047012209892273
translation,173,104,results,competitive correlations,when looking to,comet models,competitive correlations when looking to comet models,0.6268178820610046
translation,173,104,results,competitive correlations,to,comet models,competitive correlations to comet models,0.5772181153297424
translation,173,104,results,comet models,not fine-tuned on,mqm development data,comet models not fine-tuned on mqm development data,0.7379897236824036
translation,173,104,results,openkiwi model,has,competitive correlations,openkiwi model has competitive correlations,0.5590944290161133
translation,173,104,results,results,has,openkiwi model,results has openkiwi model,0.5528624653816223
translation,173,125,results,comet and bleurt,achieved,highest correlations,comet and bleurt achieved highest correlations,0.7050158381462097
translation,173,125,results,comet and bleurt,some of,highest correlations,comet and bleurt some of highest correlations,0.6290416121482849
translation,173,125,results,comet and bleurt,shared,podium,comet and bleurt shared podium,0.6720850467681885
translation,173,125,results,highest correlations,with,human judgements,highest correlations with human judgements,0.6211099624633789
translation,173,125,results,podium,with,"prism ( thompson and post , 2020 )","podium with prism ( thompson and post , 2020 )",0.6663744449615479
translation,173,125,results,results,In,2020,results In 2020,0.5361903309822083
translation,174,90,baselines,plain convattention ),whose,encoder,plain convattention ) whose encoder,0.6374370455741882
translation,174,90,baselines,stack of convattention layers,without,vanilla transformer - encoder layers,stack of convattention layers without vanilla transformer - encoder layers,0.661677896976471
translation,174,68,hyperparameters,compression factor,chosen among,"4 , 8 , and 16","compression factor chosen among 4 , 8 , and 16",0.7350690960884094
translation,174,68,hyperparameters,),chosen among,"4 , 8 , and 16",") chosen among 4 , 8 , and 16",0.7387292385101318
translation,174,68,hyperparameters,compression factor,has,),compression factor has ),0.5740014910697937
translation,174,68,hyperparameters,hyperparameters,has,compression factor,hyperparameters has compression factor,0.5110269784927368
translation,174,69,hyperparameters,hyperparameters,has,kernel size,hyperparameters has kernel size,0.4991050958633423
translation,174,8,model,information,only at,higher level,information only at higher level,0.6405340433120728
translation,174,8,model,higher level,according to,more informed linguistic criteria,higher level according to more informed linguistic criteria,0.6724096536636353
translation,174,8,model,model,propose,speechformer,model propose speechformer,0.691890299320221
translation,174,27,model,initial fixed compression,propose,speechformer,initial fixed compression propose speechformer,0.682739794254303
translation,174,27,model,first transformer - based architecture,processes,full audio content,first transformer - based architecture processes full audio content,0.6990232467651367
translation,174,27,model,full audio content,maintaining,original dimensions,full audio content maintaining original dimensions,0.7371193766593933
translation,174,27,model,original dimensions,of,input sequence,original dimensions of input sequence,0.5858758091926575
translation,174,27,model,speechformer,has,first transformer - based architecture,speechformer has first transformer - based architecture,0.602061927318573
translation,174,27,model,model,To avoid,initial fixed compression,model To avoid initial fixed compression,0.6481323838233948
translation,174,28,model,reduced,by means of,convolutional layers,reduced by means of convolutional layers,0.6643530130386353
translation,174,28,model,model,introduce,novel attention layer,model introduce novel attention layer,0.633532702922821
translation,174,29,model,high- level representation,of,input sequence,high- level representation of input sequence,0.56348717212677
translation,174,29,model,input sequence,in,linguistically informed way,input sequence in linguistically informed way,0.5161775350570679
translation,174,29,model,model,aggregate,high- level representation,model aggregate high- level representation,0.7294058203697205
translation,174,70,results,compression factor,of,4,compression factor of 4,0.6784912943840027
translation,174,70,results,kernel size,of,8,kernel size of 8,0.6693401336669922
translation,174,70,results,better performance,compared to,other combinations,better performance compared to other combinations,0.680971086025238
translation,174,70,results,results,combination of,compression factor,results combination of compression factor,0.6461288928985596
translation,174,87,results,ctc compression,to,baseline model,ctc compression to baseline model,0.5290286540985107
translation,174,87,results,ctc compression,bring,benefits,ctc compression bring benefits,0.6681227087974548
translation,174,87,results,baseline model,bring,benefits,baseline model bring benefits,0.6263490319252014
translation,174,87,results,results,addition of,ctc compression,results addition of ctc compression,0.6431161761283875
translation,174,89,results,speechformer,results in,statistically significant improvements,speechformer results in statistically significant improvements,0.6685305237770081
translation,174,89,results,statistically significant improvements,over,baseline,statistically significant improvements over baseline,0.7283158302307129
translation,174,89,results,statistically significant improvements,with,bleu gains,statistically significant improvements with bleu gains,0.6424031257629395
translation,174,89,results,baseline,in,all language directions,baseline in all language directions,0.4880555272102356
translation,174,89,results,bleu gains,ranging from,0.5,bleu gains ranging from 0.5,0.6072359681129456
translation,174,89,results,bleu gains,ranging from,0.8,bleu gains ranging from 0.8,0.6095216274261475
translation,174,89,results,0.5,for,en-nl ),0.5 for en-nl ),0.6667407751083374
translation,174,89,results,results,has,speechformer,results has speechformer,0.5292982459068298
translation,174,91,results,drop,in,performance,drop in performance,0.5523719787597656
translation,174,91,results,performance,with respect to,speechformer,performance with respect to speechformer,0.6869455575942993
translation,174,91,results,speechformer,varies between,0.4 and 0.8 bleu,speechformer varies between 0.4 and 0.8 bleu,0.700504720211029
translation,174,91,results,0.4 and 0.8 bleu,on,all language pairs,0.4 and 0.8 bleu on all language pairs,0.5534207820892334
translation,174,91,results,results,has,drop,results has drop,0.5150626301765442
translation,175,53,experimental-setup,experimental setup,use,"transformers ( vaswani et al. , 2017 )","experimental setup use transformers ( vaswani et al. , 2017 )",0.5475263595581055
translation,175,54,experimental-setup,shared bpe model,of,64 k tokens,shared bpe model of 64 k tokens,0.5871636867523193
translation,175,54,experimental-setup,shared bpe model,with,bytelevel fallback,shared bpe model with bytelevel fallback,0.648446798324585
translation,175,54,experimental-setup,bytelevel fallback,using,sentencepiece 6 library,bytelevel fallback using sentencepiece 6 library,0.6644161939620972
translation,175,54,experimental-setup,experimental setup,use,transformer big configuration,experimental setup use transformer big configuration,0.5856959223747253
translation,175,54,experimental-setup,experimental setup,use,shared bpe model,experimental setup use shared bpe model,0.6126198768615723
translation,175,61,experimental-setup,"mass objective ( song et al. , 2019 )",on,monolingual data,"mass objective ( song et al. , 2019 ) on monolingual data",0.5148016214370728
translation,175,61,experimental-setup,cross-entropy,on,parallel data,cross-entropy on parallel data,0.5652287006378174
translation,175,61,experimental-setup,experimental setup,apply,"mass objective ( song et al. , 2019 )","experimental setup apply mass objective ( song et al. , 2019 )",0.598253607749939
translation,175,62,experimental-setup,"adam( kingma and ba , 2015 ) optimizer",with,initial learning rate,"adam( kingma and ba , 2015 ) optimizer with initial learning rate",0.5993229150772095
translation,175,62,experimental-setup,"adam( kingma and ba , 2015 ) optimizer",coupled with,linear warmup,"adam( kingma and ba , 2015 ) optimizer coupled with linear warmup",0.6881570816040039
translation,175,62,experimental-setup,initial learning rate,of,4e - 4,initial learning rate of 4e - 4,0.6349340677261353
translation,175,62,experimental-setup,initial learning rate,coupled with,linear warmup,initial learning rate coupled with linear warmup,0.6938965916633606
translation,175,62,experimental-setup,linear warmup,followed by,linear decay,linear warmup followed by linear decay,0.6578307747840881
translation,175,62,experimental-setup,linear decay,to,0,linear decay to 0,0.623953104019165
translation,175,62,experimental-setup,experimental setup,used,"adam( kingma and ba , 2015 ) optimizer","experimental setup used adam( kingma and ba , 2015 ) optimizer",0.6132118105888367
translation,175,63,experimental-setup,initial warmup,took,1 k steps,initial warmup took 1 k steps,0.644005298614502
translation,175,63,experimental-setup,total training time,was,500k steps,total training time was 500k steps,0.57322758436203
translation,175,63,experimental-setup,experimental setup,has,initial warmup,experimental setup has initial warmup,0.5037030577659607
translation,175,63,experimental-setup,experimental setup,has,total training time,experimental setup has total training time,0.5265299677848816
translation,175,64,experimental-setup,weight decay,with,hyperparameter,weight decay with hyperparameter,0.5617867112159729
translation,175,64,experimental-setup,hyperparameter,of,0.2,hyperparameter of 0.2,0.5771277546882629
translation,175,66,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,175,66,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,175,66,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,175,66,experimental-setup,length penalty,of,?   0.6,length penalty of ?   0.6,0.5653038024902344
translation,175,66,experimental-setup,?   0.6,for,decoding,?   0.6 for decoding,0.6773292422294617
translation,175,88,experimental-setup,learning rate,to,5e - 5,learning rate to 5e - 5,0.6061331629753113
translation,175,88,experimental-setup,learning rate,keep,fixed,learning rate keep fixed,0.6742234230041504
translation,175,88,experimental-setup,experimental setup,reset,adam optimizer 's stored accumulators,experimental setup reset adam optimizer 's stored accumulators,0.7508041262626648
translation,175,52,experiments,our experiments,in,"jax ( bradbury et al. , 2018 )","our experiments in jax ( bradbury et al. , 2018 )",0.525227427482605
translation,175,52,experiments,our experiments,using,neural network library flax,our experiments using neural network library flax,0.6485921740531921
translation,175,52,experiments,"jax ( bradbury et al. , 2018 )",using,neural network library flax,"jax ( bradbury et al. , 2018 ) using neural network library flax",0.6426458954811096
translation,175,4,model,straightforward vocabulary adaptation scheme,to extend,language capacity,straightforward vocabulary adaptation scheme to extend language capacity,0.6520623564720154
translation,175,4,model,language capacity,of,multilingual machine translation models,language capacity of multilingual machine translation models,0.49520814418792725
translation,175,4,model,model,propose,straightforward vocabulary adaptation scheme,model propose straightforward vocabulary adaptation scheme,0.6224001049995422
translation,175,21,model,simple adaptation scheme,that allows,our translation model,simple adaptation scheme that allows our translation model,0.6564165949821472
translation,175,21,model,our translation model,to attain,competitive performance,our translation model to attain competitive performance,0.5490002632141113
translation,175,21,model,competitive performance,with,strong bilingual and multilingual baselines,competitive performance with strong bilingual and multilingual baselines,0.6636436581611633
translation,175,21,model,model,propose,simple adaptation scheme,model propose simple adaptation scheme,0.657178521156311
translation,175,93,results,our models,adapted with,parallel data,our models adapted with parallel data,0.6235368251800537
translation,175,93,results,parallel data,are,competitive,parallel data are competitive,0.6168572306632996
translation,175,93,results,competitive,with,oracle models,competitive with oracle models,0.5957225561141968
translation,175,93,results,results,has,our models,results has our models,0.5733726620674133
translation,175,94,results,languages,share,scripts,languages share scripts,0.7121076583862305
translation,175,94,results,languages,attain,strong performance,languages attain strong performance,0.6280040740966797
translation,175,94,results,scripts,with,original ones ( kazakh and polish ),scripts with original ones ( kazakh and polish ),0.6361388564109802
translation,175,94,results,strong performance,leveraging,monolingual data alone,strong performance leveraging monolingual data alone,0.7023596167564392
translation,175,94,results,results,For,languages,results For languages,0.53122878074646
translation,175,99,results,all the models ' performance,on,original language pairs,all the models ' performance on original language pairs,0.4796714186668396
translation,175,99,results,deviate only slightly,from,oracle model,deviate only slightly from oracle model,0.5963406562805176
translation,175,99,results,some of the degradation,from,vocabulary substitution,some of the degradation from vocabulary substitution,0.6007084846496582
translation,175,99,results,results,has,all the models ' performance,results has all the models ' performance,0.5422489047050476
translation,175,104,results,adapted models,able to outperform,mbart,adapted models able to outperform mbart,0.8020262718200684
translation,175,104,results,mbart,in,both directions,mbart in both directions,0.5525261163711548
translation,175,104,results,results,has,adapted models,results has adapted models,0.5248281955718994
translation,176,85,ablation-analysis,algorithmic choices,resulting in,highest speed - up,algorithmic choices resulting in highest speed - up,0.6667376160621643
translation,176,85,ablation-analysis,highest speed - up,result in,1.5 % and 4 % relative drop,highest speed - up result in 1.5 % and 4 % relative drop,0.7017729878425598
translation,176,85,ablation-analysis,1.5 % and 4 % relative drop,in,bleu,1.5 % and 4 % relative drop in bleu,0.5631544589996338
translation,176,85,ablation-analysis,1.5 % and 4 % relative drop,compared to,baseline model,1.5 % and 4 % relative drop compared to baseline model,0.7042520642280579
translation,176,85,ablation-analysis,bleu,for,en-es and en-de,bleu for en-es and en-de,0.7802219390869141
translation,176,85,ablation-analysis,ablation analysis,has,algorithmic choices,ablation analysis has algorithmic choices,0.4855681359767914
translation,176,66,baselines,baseline,is,standard large transformer,baseline is standard large transformer,0.5564383268356323
translation,176,66,baselines,standard large transformer,with,"( 6 , 6 ) encoder -decoder layer configuration","standard large transformer with ( 6 , 6 ) encoder -decoder layer configuration",0.6529203057289124
translation,176,66,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,176,68,baselines,smaller shallow decoder ( ssd ) models,without increasing,"encoder depth { ( 6 , 4 ) , ( 6 , 2 ) , ( 6 , 1 ) }","smaller shallow decoder ( ssd ) models without increasing encoder depth { ( 6 , 4 ) , ( 6 , 2 ) , ( 6 , 1 ) }",0.6930148601531982
translation,176,95,experiments,anti-stereotypical ( anti ) column,observe,below-chance accuracies,anti-stereotypical ( anti ) column observe below-chance accuracies,0.6419409513473511
translation,176,95,experiments,below-chance accuracies,of,only 44.2 % and 39.7 %,below-chance accuracies of only 44.2 % and 39.7 %,0.5174911618232727
translation,176,95,experiments,only 44.2 % and 39.7 %,for,two language directions,only 44.2 % and 39.7 % for two language directions,0.6162281632423401
translation,176,67,hyperparameters,sd models,use,"encoder-decoder layer configurations { ( 8 , 4 ) , ( 10 , 2 ) , ( 11 , 1 ) }","sd models use encoder-decoder layer configurations { ( 8 , 4 ) , ( 10 , 2 ) , ( 11 , 1 ) }",0.6215589642524719
translation,176,73,hyperparameters,byte-pair encoding ( bpe ),with,32 k merge operations,byte-pair encoding ( bpe ) with 32 k merge operations,0.6226884722709656
translation,176,73,hyperparameters,32 k merge operations,to,data,32 k merge operations to data,0.5692311525344849
translation,176,73,hyperparameters,hyperparameters,apply,byte-pair encoding ( bpe ),hyperparameters apply byte-pair encoding ( bpe ),0.6113237738609314
translation,176,87,results,accuracies,are,relatively high ( 80.9 to 77.7 ),accuracies are relatively high ( 80.9 to 77.7 ),0.5169650912284851
translation,176,87,results,relatively high ( 80.9 to 77.7 ),for,all the models,relatively high ( 80.9 to 77.7 ) for all the models,0.6165102124214172
translation,176,87,results,results,has,accuracies,results has accuracies,0.5684004426002502
translation,176,101,results,model choices,find that,aans,model choices find that aans,0.642134964466095
translation,176,101,results,aans,deliver,moderate speed-ups,aans deliver moderate speed-ups,0.6362492442131042
translation,176,101,results,aans,deliver,minimal bleu reduction,aans deliver minimal bleu reduction,0.5810139179229736
translation,176,101,results,minimal bleu reduction,compared to,baseline,minimal bleu reduction compared to baseline,0.6265473365783691
translation,176,101,results,results,In terms of,model choices,results In terms of model choices,0.6655982732772827
translation,176,101,results,results,find that,aans,results find that aans,0.6434046626091003
translation,176,103,results,greedy decoding,with,baseline model,greedy decoding with baseline model,0.6312618255615234
translation,176,103,results,smallest degradation,in terms of,gender-bias,smallest degradation in terms of gender-bias,0.666374146938324
translation,176,103,results,baseline model,has,smallest degradation,baseline model has smallest degradation,0.5385542511940002
translation,176,103,results,results,has,greedy decoding,results has greedy decoding,0.5355871319770813
translation,177,127,ablation-analysis,cross,with,mbart,cross with mbart,0.771181046962738
translation,177,127,ablation-analysis,all performances,rise,significantly,all performances rise significantly,0.6602820754051208
translation,177,127,ablation-analysis,cross,has,all performances,cross has all performances,0.6024099588394165
translation,177,127,ablation-analysis,mbart,has,all performances,mbart has all performances,0.5805931687355042
translation,177,127,ablation-analysis,ablation analysis,replace,cross,ablation analysis replace cross,0.6541152596473694
translation,177,162,ablation-analysis,mbart,pushes up,system 's performance further,mbart pushes up system 's performance further,0.6330552697181702
translation,177,162,ablation-analysis,system 's performance further,with,mbart st,system 's performance further with mbart st,0.7250972390174866
translation,177,162,ablation-analysis,system 's performance further,with,mbart mt,system 's performance further with mbart mt,0.6968775987625122
translation,177,162,ablation-analysis,mbart st,achieving,77.0,mbart st achieving 77.0,0.6338836550712585
translation,177,162,ablation-analysis,mbart mt,has,79.9,mbart mt has 79.9,0.5873839855194092
translation,177,162,ablation-analysis,ablation analysis,usage of,mbart,ablation analysis usage of mbart,0.7526014447212219
translation,177,7,model,semantic parsing,towards,multiple formalisms,semantic parsing towards multiple formalisms,0.6270619034767151
translation,177,7,model,multiple formalisms,as,multilingual neural machine translation ( mnmt ),multiple formalisms as multilingual neural machine translation ( mnmt ),0.5312291383743286
translation,177,7,model,many - to - many seq2seq architecture,trained with,mnmt objective,many - to - many seq2seq architecture trained with mnmt objective,0.7407858967781067
translation,177,7,model,sgl,has,many - to - many seq2seq architecture,sgl has many - to - many seq2seq architecture,0.5645480751991272
translation,177,7,model,model,reframe,semantic parsing,model reframe semantic parsing,0.656078040599823
translation,177,7,model,model,propose,sgl,model propose sgl,0.7146718502044678
translation,177,21,model,competitively scale,across,formalisms,competitively scale across formalisms,0.7352766394615173
translation,177,21,model,competitively scale,across,languages,competitively scale across languages,0.7459729909896851
translation,177,21,model,graph languages ( sgl ),has,many - to - many seq2seq architecture,graph languages ( sgl ) has many - to - many seq2seq architecture,0.5575423240661621
translation,177,21,model,model,propose,graph languages ( sgl ),model propose graph languages ( sgl ),0.6342896223068237
translation,177,21,model,model,Speak,graph languages ( sgl ),model Speak graph languages ( sgl ),0.7671545743942261
translation,177,23,model,seq2seq model,with,multilingual neural machine translation ( mnmt ) objective,seq2seq model with multilingual neural machine translation ( mnmt ) objective,0.6198020577430725
translation,177,23,model,seq2seq model,given,input text,seq2seq model given input text,0.7078053951263428
translation,177,23,model,model,train,seq2seq model,model train seq2seq model,0.7313029766082764
translation,177,128,results,all its alternatives,except for,spring and spring bart,all its alternatives except for spring and spring bart,0.6534256935119629
translation,177,128,results,mbart st,has,outperforms,mbart st has outperforms,0.6500133275985718
translation,177,128,results,outperforms,has,all its alternatives,outperforms has all its alternatives,0.596706748008728
translation,177,128,results,results,has,mbart st,results has mbart st,0.5583699345588684
translation,177,129,results,mbart mt and mbart f t mt,push,performances further up,mbart mt and mbart f t mt push performances further up,0.795464038848877
translation,177,129,results,performances further up,showing,mt data,performances further up showing mt data,0.78983473777771
translation,177,129,results,mt data,are,beneficial,mt data are beneficial,0.6409130096435547
translation,177,129,results,results,has,mbart mt and mbart f t mt,results has mbart mt and mbart f t mt,0.5675970911979675
translation,177,138,results,cross f t mt,attains,performances,cross f t mt attains performances,0.7649886608123779
translation,177,138,results,results,has,most interesting aspect,results has most interesting aspect,0.5313474535942078
translation,177,144,results,zh ? amr parsing,with,additional decoding machinery,zh ? amr parsing with additional decoding machinery,0.6382767558097839
translation,177,144,results,performances,would be,significantly lower,performances would be significantly lower,0.6720677018165588
translation,177,144,results,significantly lower,with,mbart st,significantly lower with mbart st,0.6616138815879822
translation,177,144,results,zh ? amr parsing,has,performances,zh ? amr parsing has performances,0.5809749960899353
translation,177,144,results,additional decoding machinery,has,performances,additional decoding machinery has performances,0.5660951137542725
translation,177,144,results,results,perform,zh ? amr parsing,results perform zh ? amr parsing,0.6464382410049438
translation,177,147,results,mbart mt,pushes,bar,mbart mt pushes bar,0.7481489181518555
translation,177,147,results,bar,with,performances,bar with performances,0.729053258895874
translation,177,147,results,performances,on,"german , spanish and italian","performances on german , spanish and italian",0.5638695955276489
translation,177,147,results,bar,has,further up,bar has further up,0.6968791484832764
translation,177,147,results,only roughly 10 points behind,has,english counterparts,only roughly 10 points behind has english counterparts,0.5871074795722961
translation,177,147,results,results,has,mbart mt,results has mbart mt,0.5534012317657471
translation,177,148,results,parallel data,beneficial for,cross-lingual amr,parallel data beneficial for cross-lingual amr,0.7193005084991455
translation,177,148,results,mbart mt,has,significantly outperforms,mbart mt has significantly outperforms,0.6290367245674133
translation,177,148,results,significantly outperforms,has,mbart st,significantly outperforms has mbart st,0.6097680926322937
translation,177,148,results,results,has,mbart mt,results has mbart mt,0.5534012317657471
translation,177,149,results,mbart f t mt,yield,improvement,mbart f t mt yield improvement,0.7697237133979797
translation,177,149,results,performances slightly drop,on,average,performances slightly drop on average,0.5497453808784485
translation,177,160,results,ucca,benefits from,transfer learning,ucca benefits from transfer learning,0.6843047738075256
translation,177,160,results,transfer learning,to,even greater extent,transfer learning to even greater extent,0.5909313559532166
translation,177,160,results,even greater extent,than,amr parsing,even greater extent than amr parsing,0.6028406023979187
translation,177,160,results,our multilingual framework,has,ucca,our multilingual framework has ucca,0.6268381476402283
translation,178,77,experimental-setup,preprocessing stage,use,sacremoses,preprocessing stage use sacremoses,0.6868789792060852
translation,178,77,experimental-setup,preprocessing stage,use,byte pair encoding ( bpe ),preprocessing stage use byte pair encoding ( bpe ),0.6378412246704102
translation,178,77,experimental-setup,preprocessing stage,apply,byte pair encoding ( bpe ),preprocessing stage apply byte pair encoding ( bpe ),0.6368374228477478
translation,178,77,experimental-setup,sacremoses,for,tokenization,sacremoses for tokenization,0.6647910475730896
translation,178,77,experimental-setup,sacremoses,apply,byte pair encoding ( bpe ),sacremoses apply byte pair encoding ( bpe ),0.6279610991477966
translation,178,77,experimental-setup,byte pair encoding ( bpe ),with,joint vocabulary size,byte pair encoding ( bpe ) with joint vocabulary size,0.6139854192733765
translation,178,77,experimental-setup,joint vocabulary size,of,4 k and 32 k,joint vocabulary size of 4 k and 32 k,0.632066011428833
translation,178,77,experimental-setup,experimental setup,In,preprocessing stage,experimental setup In preprocessing stage,0.5430477261543274
translation,178,77,experimental-setup,experimental setup,apply,byte pair encoding ( bpe ),experimental setup apply byte pair encoding ( bpe ),0.5851911902427673
translation,178,79,experimental-setup,learning rate,of,3 * 10 ?4,learning rate of 3 * 10 ?4,0.6264562010765076
translation,178,79,experimental-setup,learning rate,applied along with,dropout rate,learning rate applied along with dropout rate,0.6317488551139832
translation,178,79,experimental-setup,3 * 10 ?4,applied along with,dropout rate,3 * 10 ?4 applied along with dropout rate,0.6585260033607483
translation,178,79,experimental-setup,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,178,79,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,178,80,experimental-setup,batch size,of,4096 bpe tokens,batch size of 4096 bpe tokens,0.6085251569747925
translation,178,80,experimental-setup,4096 bpe tokens,with,8 accumulations,4096 bpe tokens with 8 accumulations,0.6996233463287354
translation,178,80,experimental-setup,8 accumulations,to simulate,training,8 accumulations to simulate training,0.7279406785964966
translation,178,80,experimental-setup,training,on,8 gpu machines,training on 8 gpu machines,0.49766820669174194
translation,178,81,experimental-setup,mod- els,except,english -turkish and turkish - english,mod- els except english -turkish and turkish - english,0.7158890962600708
translation,178,81,experimental-setup,mod- els,trained on,google colab 's,mod- els trained on google colab 's,0.8030426502227783
translation,178,81,experimental-setup,google colab 's,has,freely availably preemptible gpus,google colab 's has freely availably preemptible gpus,0.5977533459663391
translation,178,81,experimental-setup,experimental setup,has,mod- els,experimental setup has mod- els,0.5213797688484192
translation,178,94,experimental-setup,model,using,transformer architecture,model using transformer architecture,0.6854490637779236
translation,178,94,experimental-setup,transformer architecture,in,transformer - base configuration,transformer architecture in transformer - base configuration,0.5536741614341736
translation,178,94,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,178,95,experimental-setup,"fairseq ( ott et al. , 2019 ) implementation",with,6 layers,"fairseq ( ott et al. , 2019 ) implementation with 6 layers",0.6002900004386902
translation,178,95,experimental-setup,6 layers,has,both in the encoder and decoder,6 layers has both in the encoder and decoder,0.5768308639526367
translation,178,95,experimental-setup,experimental setup,transformer wmt en de version from,"fairseq ( ott et al. , 2019 ) implementation","experimental setup transformer wmt en de version from fairseq ( ott et al. , 2019 ) implementation",0.5762863755226135
translation,178,97,experimental-setup,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,178,97,experimental-setup,learning rate,of,5 * 10 ?4,learning rate of 5 * 10 ?4,0.6313912272453308
translation,178,97,experimental-setup,warm - up updates,of,40k,warm - up updates of 40k,0.6199275851249695
translation,178,97,experimental-setup,experimental setup,apply,dropout rate,experimental setup apply dropout rate,0.5810881853103638
translation,178,97,experimental-setup,experimental setup,apply,learning rate,experimental setup apply learning rate,0.5722567439079285
translation,178,97,experimental-setup,experimental setup,apply,warm - up updates,experimental setup apply warm - up updates,0.6199184060096741
translation,178,98,experimental-setup,effective batch size,is,"16,384 bpe tokens","effective batch size is 16,384 bpe tokens",0.5870352983474731
translation,178,98,experimental-setup,experimental setup,has,effective batch size,experimental setup has effective batch size,0.5394346117973328
translation,178,99,experimental-setup,4 nvidia v100 gpu machines,for,a little over 1 million steps,4 nvidia v100 gpu machines for a little over 1 million steps,0.5816283822059631
translation,178,99,experimental-setup,4 nvidia v100 gpu machines,takes,about 36 - 48 hours,4 nvidia v100 gpu machines takes about 36 - 48 hours,0.6699317693710327
translation,178,99,experimental-setup,a little over 1 million steps,takes,about 36 - 48 hours,a little over 1 million steps takes about 36 - 48 hours,0.6857609152793884
translation,178,99,experimental-setup,experimental setup,trained using,4 nvidia v100 gpu machines,experimental setup trained using 4 nvidia v100 gpu machines,0.6780646443367004
translation,178,200,experiments,first largescale mnmt model,for,turkic language family,first largescale mnmt model for turkic language family,0.5504333972930908
translation,178,200,experiments,turkic language family,consists of,many underexplored languages,turkic language family consists of many underexplored languages,0.5969384908676147
translation,178,107,results,first obvious trend,is,dominance,first obvious trend is dominance,0.5972023010253906
translation,178,107,results,dominance,of,bilingual baselines,dominance of bilingual baselines,0.5745185613632202
translation,178,107,results,dominance,as,overperform,dominance as overperform,0.5475134253501892
translation,178,107,results,bilingual baselines,on,in- domain test sets,bilingual baselines on in- domain test sets,0.512139618396759
translation,178,107,results,mnmt model,in,most of the high - to mid-resource language pairs,mnmt model in most of the high - to mid-resource language pairs,0.5429486036300659
translation,178,107,results,overperform,has,mnmt model,overperform has mnmt model,0.5834726691246033
translation,178,107,results,results,has,first obvious trend,results has first obvious trend,0.5575199127197266
translation,178,108,results,results,become,more comparable,results become more comparable,0.5765172839164734
translation,178,108,results,results,become,even better,results become even better,0.6303598880767822
translation,178,108,results,more comparable,in terms of,bleu,more comparable in terms of bleu,0.6660516262054443
translation,178,108,results,even better,for,mnmt,even better for mnmt,0.6648008227348328
translation,178,108,results,mnmt,evaluated in,chrf,mnmt evaluated in chrf,0.6852612495422363
translation,178,108,results,train size,has,decreases,train size has decreases,0.614372730255127
translation,178,108,results,train size,has,results,train size has results,0.5714870691299438
translation,178,108,results,decreases,has,results,decreases has results,0.6527493596076965
translation,178,108,results,results,become,more comparable,results become more comparable,0.5765172839164734
translation,178,109,results,domain shift,with,x - wmt set,domain shift with x - wmt set,0.6620280742645264
translation,178,109,results,mnmt,results in,gains,mnmt results in gains,0.6905210614204407
translation,178,109,results,gains,across,almost all pairs,gains across almost all pairs,0.7327028512954712
translation,178,109,results,domain shift,has,mnmt,domain shift has mnmt,0.5941352248191833
translation,178,109,results,x - wmt set,has,mnmt,x - wmt set has mnmt,0.6398493051528931
translation,178,196,results,performance,of,models,performance of models,0.598082423210144
translation,178,196,results,performance,improves,steadily,performance improves steadily,0.8061590790748596
translation,178,196,results,models,improves,steadily,models improves steadily,0.7494080066680908
translation,178,196,results,steadily,across,all resource types,steadily across all resource types,0.7653477787971497
translation,178,196,results,low-resource cases,experiencing,gains,low-resource cases experiencing gains,0.769859790802002
translation,178,196,results,gains,up to,44 bleu points,gains up to 44 bleu points,0.6124728918075562
translation,178,196,results,gains,up to,0.4 chrf,gains up to 0.4 chrf,0.669877290725708
translation,178,196,results,gains,from,bilingual baselines,gains from bilingual baselines,0.5892011523246765
translation,178,196,results,bilingual baselines,in,in-domain evaluation,bilingual baselines in in-domain evaluation,0.49663519859313965
translation,179,122,baselines,reimplemented,on top of,transformer,reimplemented on top of transformer,0.7207998037338257
translation,179,122,baselines,baselines,has,"tf -p ( zhang et al. , 2018 )","baselines has tf -p ( zhang et al. , 2018 )",0.5068976283073425
translation,179,119,experimental-setup,experimental setup,employ,byte pair encoding ( bpe ),experimental setup employ byte pair encoding ( bpe ),0.5358485579490662
translation,179,109,hyperparameters,"adam ( kingma and ba , 2014 )",with,default settings,"adam ( kingma and ba , 2014 ) with default settings",0.6117965579032898
translation,179,109,hyperparameters,default settings,as,learning rate schema,default settings as learning rate schema,0.5396941304206848
translation,179,109,hyperparameters,hyperparameters,employ,"adam ( kingma and ba , 2014 )","hyperparameters employ adam ( kingma and ba , 2014 )",0.5281162261962891
translation,179,6,model,fast and accurate approach,to,tm - based nmt,fast and accurate approach to tm - based nmt,0.5854319334030151
translation,179,6,model,tm - based nmt,within,transformer framework,tm - based nmt within transformer framework,0.6278460025787354
translation,179,6,model,single bilingual sentence,as,tm,single bilingual sentence as tm,0.5962204933166504
translation,179,6,model,model,propose,fast and accurate approach,model propose fast and accurate approach,0.6851382851600647
translation,179,19,model,fast and accurate approach,for,tm - based nmt,fast and accurate approach for tm - based nmt,0.6414889097213745
translation,179,19,model,model,present,fast and accurate approach,model present fast and accurate approach,0.6672181487083435
translation,179,20,model,model,design,light- weight tm - based nmt model,model design light- weight tm - based nmt model,0.5871673822402954
translation,179,23,model,novel training criterion,for optimizing,parameters,novel training criterion for optimizing parameters,0.723586916923523
translation,179,23,model,parameters,of,our model,parameters of our model,0.5934821963310242
translation,179,23,model,parameters,inspired by,multiple - task learning,parameters inspired by multiple - task learning,0.6679487824440002
translation,179,23,model,model,propose,novel training criterion,model propose novel training criterion,0.7005195021629333
translation,179,24,model,loss function,includes,two terms,loss function includes two terms,0.6045947074890137
translation,179,24,model,loss function,includes,second term,loss function includes second term,0.5650469660758972
translation,179,24,model,first term,induced by,bilingual corpus,first term induced by bilingual corpus,0.6724429130554199
translation,179,24,model,first term,induced by,bilingual corpus,first term induced by bilingual corpus,0.6724429130554199
translation,179,24,model,bilingual corpus,with,tm,bilingual corpus with tm,0.6664010882377625
translation,179,24,model,bilingual corpus,with,without any tm,bilingual corpus with without any tm,0.616705060005188
translation,179,24,model,second term,induced by,bilingual corpus,second term induced by bilingual corpus,0.6723293662071228
translation,179,24,model,two terms,has,first term,two terms has first term,0.588694155216217
translation,179,24,model,bilingual corpus,has,without any tm,bilingual corpus has without any tm,0.5875517725944519
translation,179,24,model,model,has,loss function,model has loss function,0.5092374086380005
translation,179,5,results,tmbased,has,machine translation ( nmt ),tmbased has machine translation ( nmt ),0.5668033361434937
translation,179,137,results,our models,achieve,substantial improvements,our models achieve substantial improvements,0.622541069984436
translation,179,137,results,substantial improvements,over,transformer ( tf ),substantial improvements over transformer ( tf ),0.6944518685340881
translation,179,137,results,transformer ( tf ),which does not use,any tm,transformer ( tf ) which does not use any tm,0.7563730478286743
translation,179,137,results,results,see that,our models,results see that our models,0.6678143739700317
translation,179,138,results,tf - sa,performs,better,tf - sa performs better,0.6611425876617432
translation,179,138,results,tf - sa,thanks to,fine-grained alignment information,tf - sa thanks to fine-grained alignment information,0.4448734223842621
translation,179,138,results,better,than,tf -s,better than tf -s,0.6720711588859558
translation,179,138,results,better,than,tf - ss,better than tf - ss,0.6498866677284241
translation,179,138,results,better,thanks to,fine-grained alignment information,better thanks to fine-grained alignment information,0.48052310943603516
translation,179,138,results,tf - ss,thanks to,fine-grained alignment information,tf - ss thanks to fine-grained alignment information,0.4520922303199768
translation,179,138,results,fine-grained alignment information,encoded in,tm,fine-grained alignment information encoded in tm,0.7543318867683411
translation,179,138,results,results,has,tf - sa,results has tf - sa,0.5124512314796448
translation,179,139,results,outperforms,by,at least 1.0 bleu point,outperforms by at least 1.0 bleu point,0.5877140164375305
translation,179,139,results,all tm - based baselines,by,at least 1.0 bleu point,all tm - based baselines by at least 1.0 bleu point,0.5374495983123779
translation,179,139,results,tf - sa,has,outperforms,tf - sa has outperforms,0.6197617053985596
translation,179,139,results,outperforms,has,all tm - based baselines,outperforms has all tm - based baselines,0.5862048268318176
translation,179,139,results,results,has,tf - sa,results has tf - sa,0.5124512314796448
translation,179,142,results,gains,of,our models,gains of our models,0.6432349681854248
translation,179,142,results,our models,over,tf baseline,our models over tf baseline,0.7301347255706787
translation,179,142,results,our models,mainly from,sentences whose tms,our models mainly from sentences whose tms,0.6525259613990784
translation,179,142,results,sentences whose tms,with,relatively high similarity,sentences whose tms with relatively high similarity,0.6459226608276367
translation,179,142,results,results,find that,gains,results find that gains,0.6088190078735352
translation,179,148,results,tf - sa,delivers,gains,tf - sa delivers gains,0.747191309928894
translation,179,148,results,tf - sa,delivers,gains,tf - sa delivers gains,0.747191309928894
translation,179,148,results,gains,of,1.2 bleu points,gains of 1.2 bleu points,0.5539582371711731
translation,179,148,results,gains,of,5.7 bleu points,gains of 5.7 bleu points,0.5549466609954834
translation,179,148,results,gains,of,5.7 bleu points,gains of 5.7 bleu points,0.5549466609954834
translation,179,148,results,1.2 bleu points,over,standard training,1.2 bleu points over standard training,0.5806001424789429
translation,179,148,results,gains,of,5.7 bleu points,gains of 5.7 bleu points,0.5549466609954834
translation,179,148,results,5.7 bleu points,over,strong tf baseline,5.7 bleu points over strong tf baseline,0.6044346690177917
translation,179,148,results,strong tf baseline,on,entire test set,strong tf baseline on entire test set,0.5232409238815308
translation,179,148,results,joint training,has,tf - sa,joint training has tf - sa,0.5905296206474304
translation,179,148,results,results,With the help of,joint training,results With the help of joint training,0.6173416972160339
translation,180,117,ablation-analysis,large-scale pre-training,effectively accomplish,model transferring,large-scale pre-training effectively accomplish model transferring,0.6415855288505554
translation,180,117,ablation-analysis,large-scale pre-training,advance,performance,large-scale pre-training advance performance,0.6893735527992249
translation,180,117,ablation-analysis,performance,of,doc- mt,performance of doc- mt,0.6167398691177368
translation,180,117,ablation-analysis,ablation analysis,confirm,large-scale pre-training,ablation analysis confirm large-scale pre-training,0.5523184537887573
translation,180,125,ablation-analysis,parallel sentence translation ( pst ) task,results in,largest performance degradation,parallel sentence translation ( pst ) task results in largest performance degradation,0.5950802564620972
translation,180,125,ablation-analysis,ablation analysis,removal of,parallel sentence translation ( pst ) task,ablation analysis removal of parallel sentence translation ( pst ) task,0.6372193098068237
translation,180,135,ablation-analysis,original lm task and newly introduced nmt task,based on,importance of parameters,original lm task and newly introduced nmt task based on importance of parameters,0.6635949611663818
translation,180,135,ablation-analysis,overfitting,of,pre-trained model,overfitting of pre-trained model,0.5587568879127502
translation,180,135,ablation-analysis,pre-trained model,on,limited downstream data,pre-trained model on limited downstream data,0.5440924763679504
translation,180,135,ablation-analysis,original lm task and newly introduced nmt task,has,overfitting,original lm task and newly introduced nmt task has overfitting,0.5268570184707642
translation,180,135,ablation-analysis,importance of parameters,has,overfitting,importance of parameters has overfitting,0.534587562084198
translation,180,135,ablation-analysis,ablation analysis,weighing,original lm task and newly introduced nmt task,ablation analysis weighing original lm task and newly introduced nmt task,0.7359746694564819
translation,180,111,baselines,flat- transformer,apply,bert,flat- transformer apply bert,0.635876476764679
translation,180,111,baselines,bert,as,initialization,bert as initialization,0.5462874174118042
translation,180,111,baselines,initialization,of,encoder,initialization of encoder,0.5531216859817505
translation,180,111,baselines,baselines,has,flat- transformer,baselines has flat- transformer,0.6159510016441345
translation,180,112,baselines,parallel sentence translationbased pre-training,with,"mbart ( liu et al. , 2020 ) initialization","parallel sentence translationbased pre-training with mbart ( liu et al. , 2020 ) initialization",0.5765708684921265
translation,180,96,experimental-setup,transformer,consisting of,12 encoder and 12 decoder layers,transformer consisting of 12 encoder and 12 decoder layers,0.750984251499176
translation,180,96,experimental-setup,12 encoder and 12 decoder layers,with,1024 hidden size,12 encoder and 12 decoder layers with 1024 hidden size,0.5835582613945007
translation,180,96,experimental-setup,1024 hidden size,on,16 heads,1024 hidden size on 16 heads,0.5444967746734619
translation,180,96,experimental-setup,experimental setup,train,transformer,experimental setup train transformer,0.6441354751586914
translation,180,97,experimental-setup,public mbart.cc25,released by,liu et al . ( 2020 ),public mbart.cc25 released by liu et al . ( 2020 ),0.7302601337432861
translation,180,97,experimental-setup,public mbart.cc25,as,initialization,public mbart.cc25 as initialization,0.48366862535476685
translation,180,97,experimental-setup,liu et al . ( 2020 ),as,initialization,liu et al . ( 2020 ) as initialization,0.5848804116249084
translation,180,97,experimental-setup,experimental setup,adopt,public mbart.cc25,experimental setup adopt public mbart.cc25,0.6625588536262512
translation,180,101,experimental-setup,sentence piece model,to tokenize,all data,sentence piece model to tokenize all data,0.7520283460617065
translation,180,101,experimental-setup,experimental setup,use,sentence piece model,experimental setup use sentence piece model,0.5748251676559448
translation,180,102,experimental-setup,gradient accumulation,to simulate,batch size,gradient accumulation to simulate batch size,0.7000914812088013
translation,180,102,experimental-setup,batch size,of,128 k tokens,batch size of 128 k tokens,0.5851271152496338
translation,180,102,experimental-setup,experimental setup,has,gradient accumulation,experimental setup has gradient accumulation,0.4636458456516266
translation,180,104,experimental-setup,learning rate and dropout,set to,3e?5 and 0.1,learning rate and dropout set to 3e?5 and 0.1,0.7067628502845764
translation,180,104,experimental-setup,experimental setup,has,learning rate and dropout,experimental setup has learning rate and dropout,0.49277547001838684
translation,180,98,experiments,pre-training data,consists of,ted,pre-training data consists of ted,0.7128801941871643
translation,180,98,experiments,pre-training data,consists of,europarl,pre-training data consists of europarl,0.6231664419174194
translation,180,98,experiments,pre-training data,consists of,news commentary,pre-training data consists of news commentary,0.640559196472168
translation,180,98,experiments,pre-training data,consists of,rapid corpus,pre-training data consists of rapid corpus,0.6421546339988708
translation,180,98,experiments,cst task,has,pre-training data,cst task has pre-training data,0.5359058380126953
translation,180,100,experiments,pst task,sample,bilingual sentences,pst task sample bilingual sentences,0.6338700652122498
translation,180,100,experiments,bilingual sentences,in,newscrawl utill 2018,bilingual sentences in newscrawl utill 2018,0.5152269005775452
translation,180,103,experiments,adam optimizer,with,linear learning rate decay,adam optimizer with linear learning rate decay,0.5811677575111389
translation,180,129,experiments,cross-sentence dependency,within,document,cross-sentence dependency within document,0.5907596349716187
translation,180,129,experiments,cross-sentence dependency,is,better captured,cross-sentence dependency is better captured,0.5179563164710999
translation,180,129,experiments,target doc-level language modeling,has,cross-sentence dependency,target doc-level language modeling has cross-sentence dependency,0.500544548034668
translation,180,6,model,simple yet effective context- interactive pre-training approach,benefiting from,external largescale corpora,simple yet effective context- interactive pre-training approach benefiting from external largescale corpora,0.6341188549995422
translation,180,6,model,targets,benefiting from,external largescale corpora,targets benefiting from external largescale corpora,0.68227618932724
translation,180,6,model,simple yet effective context- interactive pre-training approach,has,targets,simple yet effective context- interactive pre-training approach has targets,0.5447918772697449
translation,180,6,model,model,propose,simple yet effective context- interactive pre-training approach,model propose simple yet effective context- interactive pre-training approach,0.6900681853294373
translation,180,20,model,simple yet effective context-interactive pre-training approach,for,doc- mt,simple yet effective context-interactive pre-training approach for doc- mt,0.6509870290756226
translation,180,20,model,model,propose,simple yet effective context-interactive pre-training approach,model propose simple yet effective context-interactive pre-training approach,0.6900681853294373
translation,180,26,model,parallel sentence translation,to alleviate,lack of doc-level bilingual corpora,parallel sentence translation to alleviate lack of doc-level bilingual corpora,0.6090244054794312
translation,180,26,model,model,introduce,parallel sentence translation,model introduce parallel sentence translation,0.5801219344139099
translation,180,27,model,catastrophic forgetting,of,pretrained model,catastrophic forgetting of pretrained model,0.5312484502792358
translation,180,27,model,pretrained model,in,downstream fine-tuning,pretrained model in downstream fine-tuning,0.512016236782074
translation,180,27,model,elastic weight consolidation ( ewc ) regularization,to further enhance,model performance,elastic weight consolidation ( ewc ) regularization to further enhance model performance,0.6898146271705627
translation,180,27,model,catastrophic forgetting,has,elastic weight consolidation ( ewc ) regularization,catastrophic forgetting has elastic weight consolidation ( ewc ) regularization,0.5563738346099854
translation,180,27,model,downstream fine-tuning,has,elastic weight consolidation ( ewc ) regularization,downstream fine-tuning has elastic weight consolidation ( ewc ) regularization,0.5549312233924866
translation,180,27,model,model,to avoid,catastrophic forgetting,model to avoid catastrophic forgetting,0.6848009824752808
translation,180,87,model,model,introduce,elastic weight consolidation ( ewc ) regularization,model introduce elastic weight consolidation ( ewc ) regularization,0.6418072581291199
translation,180,128,model,whole source sentence,in,input,whole source sentence in input,0.5012216567993164
translation,180,128,model,whole source sentence,via,cst,whole source sentence via cst,0.6764672994613647
translation,180,128,model,input,via,cst,input via cst,0.745280921459198
translation,180,128,model,model,more effectively extract and utilize,valuable information,model more effectively extract and utilize valuable information,0.7157325744628906
translation,180,128,model,valuable information,from,extra contexts,valuable information from extra contexts,0.563178300857544
translation,180,128,model,whole source sentence,has,model,whole source sentence has model,0.5712658762931824
translation,180,128,model,cst,has,model,cst has model,0.5935795307159424
translation,180,132,model,catastrophic forgetting,of,pre-trained models,catastrophic forgetting of pre-trained models,0.5568807125091553
translation,180,132,model,catastrophic forgetting,introduce,ewc regularization,catastrophic forgetting introduce ewc regularization,0.6123623847961426
translation,180,132,model,pre-trained models,introduce,ewc regularization,pre-trained models introduce ewc regularization,0.6049575209617615
translation,180,132,model,ewc regularization,to force,model,ewc regularization to force model,0.6602602601051331
translation,180,132,model,model,to remember,original language modeling task,model to remember original language modeling task,0.5569015145301819
translation,180,132,model,model,To avoid,catastrophic forgetting,model To avoid catastrophic forgetting,0.6848009824752808
translation,180,132,model,model,introduce,ewc regularization,model introduce ewc regularization,0.6439771056175232
translation,180,132,model,model,to remember,original language modeling task,model to remember original language modeling task,0.5569015145301819
translation,180,118,results,significant performance gain,for,our approach,significant performance gain for our approach,0.5995497703552246
translation,180,118,results,significant performance gain,compared to,baselines,significant performance gain compared to baselines,0.6702569127082825
translation,180,118,results,our approach,compared to,baselines,our approach compared to baselines,0.6564284563064575
translation,180,118,results,results,observe,significant performance gain,results observe significant performance gain,0.6437168121337891
translation,180,119,results,mbart initialized model,with,pst,mbart initialized model with pst,0.6396936178207397
translation,180,119,results,pst,by,0.72 bleu,pst by 0.72 bleu,0.597385048866272
translation,180,120,results,more effective knowledge,from,external large-scale corpora,more effective knowledge from external large-scale corpora,0.584097683429718
translation,180,120,results,more effective knowledge,leading to,better translation quality,more effective knowledge leading to better translation quality,0.6576173901557922
translation,180,126,results,parallel sentences,used for,pst,parallel sentences used for pst,0.6881176233291626
translation,180,126,results,far exceeds,for,other two tasks,far exceeds for other two tasks,0.6119894981384277
translation,180,126,results,far exceeds,bringing,significant performance gains,far exceeds bringing significant performance gains,0.7064436674118042
translation,180,126,results,pst,has,far exceeds,pst has far exceeds,0.6417003870010376
translation,180,126,results,results,scale of,parallel sentences,results scale of parallel sentences,0.6442447900772095
translation,180,133,results,approach,with or without,ewc regularization,approach with or without ewc regularization,0.6456583142280579
translation,180,133,results,approach,demonstrating,effectiveness,approach demonstrating effectiveness,0.7176311612129211
translation,180,133,results,effectiveness,in improving,model performance,effectiveness in improving model performance,0.6972563862800598
translation,180,134,results,ewc regularization,achieve,consistent improvements,ewc regularization achieve consistent improvements,0.6310917139053345
translation,180,134,results,consistent improvements,on,various datasets,consistent improvements on various datasets,0.5083755850791931
translation,180,134,results,consistent improvements,increasing,average bleu score,consistent improvements increasing average bleu score,0.6271312832832336
translation,180,134,results,average bleu score,from,29.,average bleu score from 29.,0.5321353077888489
translation,180,134,results,29.,has,24 to 29.54,29. has 24 to 29.54,0.5314540266990662
translation,180,134,results,results,show,ewc regularization,results show ewc regularization,0.6034232378005981
translation,181,169,ablation-analysis,reordered sets,are,refined,reordered sets are refined,0.6166083216667175
translation,181,169,ablation-analysis,reordered sets,has,monotonicity,reordered sets has monotonicity,0.5633652210235596
translation,181,169,ablation-analysis,refined,has,monotonicity,refined has monotonicity,0.6110528111457825
translation,181,169,ablation-analysis,monotonicity,has,marginally decreases,monotonicity has marginally decreases,0.6157664060592651
translation,181,169,ablation-analysis,ablation analysis,After,reordered sets,ablation analysis After reordered sets,0.7042900919914246
translation,181,7,experiments,widely used wait -k simultaneous translation model,on,reorderedand - refined corpus,widely used wait -k simultaneous translation model on reorderedand - refined corpus,0.4745541512966156
translation,181,6,model,algorithm,to reorder and refine,target side,algorithm to reorder and refine target side,0.7314593195915222
translation,181,6,model,target side,of,full sentence translation corpus,target side of full sentence translation corpus,0.5214087963104248
translation,181,6,model,model,propose,algorithm,model propose algorithm,0.729954719543457
translation,181,28,model,paraphrasing method,to generate,monotonic parallel corpus,paraphrasing method to generate monotonic parallel corpus,0.6434023976325989
translation,181,28,model,monotonic parallel corpus,to allow,monotonic interpretation strategy,monotonic parallel corpus to allow monotonic interpretation strategy,0.6769934296607971
translation,181,28,model,monotonic interpretation strategy,in,simulmt,monotonic interpretation strategy in simulmt,0.5746659636497498
translation,181,28,model,model,propose,paraphrasing method,model propose paraphrasing method,0.6066479682922363
translation,181,155,results,models,trained on,monotonically reordered - and - refined corpora,models trained on monotonically reordered - and - refined corpora,0.7393836975097656
translation,181,155,results,monotonically reordered - and - refined corpora,show,higher bleu scores,monotonically reordered - and - refined corpora show higher bleu scores,0.6242315173149109
translation,181,155,results,monotonically reordered - and - refined corpora,show,monotonicity,monotonically reordered - and - refined corpora show monotonicity,0.6095297932624817
translation,181,155,results,results,observe,models,results observe models,0.5709750056266785
translation,181,180,results,about 8 % bleu improvement,over,offline,about 8 % bleu improvement over offline,0.6534697413444519
translation,181,180,results,offline,in,enko,offline in enko,0.7035453915596008
translation,181,180,results,about 3 % improvement,in,enzh,about 3 % improvement in enzh,0.616449236869812
translation,181,180,results,about 2 % improvement,in,deen,about 2 % improvement in deen,0.6363835334777832
translation,182,119,baselines,mma,has,monotonic multi-head attention ( mma ),mma has monotonic multi-head attention ( mma ),0.5902001857757568
translation,182,119,baselines,baselines,has,mma,baselines has mma,0.5706968903541565
translation,182,6,model,universal simt model,with,mixture - of- experts wait -k policy,universal simt model with mixture - of- experts wait -k policy,0.651699960231781
translation,182,6,model,universal simt model,to achieve,best translation quality,universal simt model to achieve best translation quality,0.6350386142730713
translation,182,6,model,mixture - of- experts wait -k policy,to achieve,best translation quality,mixture - of- experts wait -k policy to achieve best translation quality,0.6262542009353638
translation,182,6,model,best translation quality,under,arbitrary latency,best translation quality under arbitrary latency,0.5912737846374512
translation,182,6,model,model,propose,universal simt model,model propose universal simt model,0.6558905839920044
translation,182,7,model,multi-head attention,to accomplish,mixture of experts,multi-head attention to accomplish mixture of experts,0.5818382501602173
translation,182,7,model,mixture of experts,where,each head,mixture of experts where each head,0.5877295732498169
translation,182,7,model,each head,treated as,wait -k expert,each head treated as wait -k expert,0.7587976455688477
translation,182,7,model,wait -k expert,with,waiting words number,wait -k expert with waiting words number,0.681011974811554
translation,182,7,model,wait -k expert,with,source inputs,wait -k expert with source inputs,0.615882933139801
translation,182,7,model,wait -k expert,given,source inputs,wait -k expert given source inputs,0.7387809753417969
translation,182,7,model,weights,of,experts,weights of experts,0.6175459623336792
translation,182,7,model,source inputs,has,weights,source inputs has weights,0.5550843477249146
translation,182,21,model,self -adapt,to,different latency,self -adapt to different latency,0.5934414267539978
translation,182,21,model,model,propose,universal simultaneous machine translation model,model propose universal simultaneous machine translation model,0.6421069502830505
translation,182,22,model,mixture - of- experts wait -k policy ( moe wait -k policy ),for,simt,mixture - of- experts wait -k policy ( moe wait -k policy ) for simt,0.6241673827171326
translation,182,22,model,simt,where,each expert,simt where each expert,0.6711214780807495
translation,182,22,model,each expert,employs,wait -k policy,each expert employs wait -k policy,0.6031407117843628
translation,182,22,model,wait -k policy,with,own number of waiting source words,wait -k policy with own number of waiting source words,0.6750277876853943
translation,182,22,model,model,propose,mixture - of- experts wait -k policy ( moe wait -k policy ),model propose mixture - of- experts wait -k policy ( moe wait -k policy ),0.6769465804100037
translation,182,25,model,outputs,of,different heads ( aka experts ),outputs of different heads ( aka experts ),0.5840290784835815
translation,182,25,model,different heads ( aka experts ),combined with,different weights,different heads ( aka experts ) combined with different weights,0.6703462600708008
translation,182,25,model,dynamically adjusted,to achieve,best translation,dynamically adjusted to achieve best translation,0.6806958317756653
translation,182,25,model,best translation,under,different latency,best translation under different latency,0.6070249676704407
translation,182,25,model,model,has,outputs,model has outputs,0.5564337968826294
translation,182,205,model,mixture - of-experts waitk policy,to develop,universal simt,mixture - of-experts waitk policy to develop universal simt,0.6365249156951904
translation,182,205,model,universal simt,can perform,high quality simt,universal simt can perform high quality simt,0.722193717956543
translation,182,205,model,high quality simt,under,arbitrary latency,high quality simt under arbitrary latency,0.6247918605804443
translation,182,205,model,model,propose,mixture - of-experts waitk policy,model propose mixture - of-experts waitk policy,0.6525149345397949
translation,182,145,results,outperforms,under,all latency,outperforms under all latency,0.626106858253479
translation,182,145,results,previous methods,under,all latency,previous methods under all latency,0.6209725141525269
translation,182,145,results,moe wait - k + ft,has,outperforms,moe wait - k + ft has outperforms,0.647431492805481
translation,182,145,results,outperforms,has,previous methods,outperforms has previous methods,0.5876122713088989
translation,182,145,results,results,has,moe wait - k + ft,results has moe wait - k + ft,0.5626639127731323
translation,182,146,results,our method,improves,performance,our method improves performance,0.667780339717865
translation,182,146,results,performance,of,simt,performance of simt,0.6355416774749756
translation,182,146,results,performance,of,full-sentence mt,performance of full-sentence mt,0.5477781891822815
translation,182,146,results,performance,of,full-sentence mt,performance of full-sentence mt,0.5477781891822815
translation,182,146,results,simt,much closer to,offline model,simt much closer to offline model,0.6376863718032837
translation,182,146,results,simt,almost reaches,performance,simt almost reaches performance,0.7748875021934509
translation,182,146,results,performance,of,full-sentence mt,performance of full-sentence mt,0.5477781891822815
translation,182,146,results,full-sentence mt,when,lagging 9 tokens,full-sentence mt when lagging 9 tokens,0.6010569334030151
translation,182,146,results,results,has,our method,results has our method,0.5589964985847473
translation,182,147,results,standard wait - k,improves,2.11 bleu,standard wait - k improves 2.11 bleu,0.6931573748588562
translation,182,147,results,standard wait - k,improves,2.33 bleu,standard wait - k improves 2.33 bleu,0.6824437975883484
translation,182,147,results,standard wait - k,improves,2.56 bleu,standard wait - k improves 2.56 bleu,0.6768744587898254
translation,182,147,results,our method,improves,0.60 bleu,our method improves 0.60 bleu,0.6159526109695435
translation,182,147,results,our method,improves,2.11 bleu,our method improves 2.11 bleu,0.5939249992370605
translation,182,147,results,our method,improves,2.33 bleu,our method improves 2.33 bleu,0.586943507194519
translation,182,147,results,our method,improves,2.56 bleu,our method improves 2.56 bleu,0.5854805111885071
translation,182,147,results,0.60 bleu,on,en-vi,0.60 bleu on en-vi,0.657672107219696
translation,182,147,results,2.11 bleu,on,en-ro,2.11 bleu on en-ro,0.6733101010322571
translation,182,147,results,2.33 bleu,on,de-en( base ),2.33 bleu on de-en( base ),0.5524778366088867
translation,182,147,results,2.33 bleu,on,de-en ( big ),2.33 bleu on de-en ( big ),0.5680153965950012
translation,182,147,results,2.56 bleu,on,de-en ( big ),2.56 bleu on de-en ( big ),0.5705189108848572
translation,182,147,results,standard wait - k,has,our method,standard wait - k has our method,0.5692629814147949
translation,182,147,results,results,Compared with,standard wait - k,results Compared with standard wait - k,0.6627988219261169
translation,182,170,results,outperforms,improving,3.90 bleu,outperforms improving 3.90 bleu,0.6366688013076782
translation,182,170,results,standard wait -k,on,all difficulty levels,standard wait -k on all difficulty levels,0.5134220719337463
translation,182,170,results,standard wait -k,improving,3.90 bleu,standard wait -k improving 3.90 bleu,0.6249313354492188
translation,182,170,results,3.90 bleu,on,hard set,3.90 bleu on hard set,0.48670390248298645
translation,182,170,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,182,170,results,outperforms,has,standard wait -k,outperforms has standard wait -k,0.6299888491630554
translation,182,170,results,results,has,our method,results has our method,0.5589964985847473
translation,182,197,results,different k moe,shows that,our method,different k moe shows that our method,0.6723165512084961
translation,182,197,results,slight impact,on,performance,slight impact on performance,0.5831252336502075
translation,182,197,results,all types of k moe,has,outperform,all types of k moe has outperform,0.65392005443573
translation,182,197,results,outperform,has,baseline,outperform has baseline,0.6223028898239136
translation,182,197,results,results,has,all types of k moe,results has all types of k moe,0.5500736832618713
translation,183,34,baselines,constbtlm,based on,constmlm,constbtlm based on constmlm,0.6365246176719666
translation,183,35,experiments,unmt and language modeling training,prepared,encoder-decoder models and encoder-only models,unmt and language modeling training prepared encoder-decoder models and encoder-only models,0.5392180681228638
translation,183,35,experiments,encoder-decoder models and encoder-only models,for,"constbtlm , btlm , and constmlm approaches","encoder-decoder models and encoder-only models for constbtlm , btlm , and constmlm approaches",0.6168286204338074
translation,184,110,ablation-analysis,drops,by,average of ?4.2 bleu points,drops by average of ?4.2 bleu points,0.6151466965675354
translation,184,110,ablation-analysis,drops,by,average of ?1.0 bleu points,drops by average of ?1.0 bleu points,0.6233400702476501
translation,184,110,ablation-analysis,average of ?4.2 bleu points,between,internal test set and the shared task validation set,average of ?4.2 bleu points between internal test set and the shared task validation set,0.580713152885437
translation,184,110,ablation-analysis,drops,by,average of ?1.0 bleu points,drops by average of ?1.0 bleu points,0.6233400702476501
translation,184,110,ablation-analysis,mt5 _base_ada_ft performance,has,drops,mt5 _base_ada_ft performance has drops,0.6538565158843994
translation,184,110,ablation-analysis,marian_ft_esmb model performance,has,drops,marian_ft_esmb model performance has drops,0.6223340630531311
translation,184,110,ablation-analysis,ablation analysis,has,mt5 _base_ada_ft performance,ablation analysis has mt5 _base_ada_ft performance,0.6059873700141907
translation,184,79,experimental-setup,training data,use,"bpe - dropout ( provilkov et al. , 2020 )","training data use bpe - dropout ( provilkov et al. , 2020 )",0.5529138445854187
translation,184,79,experimental-setup,"bpe - dropout ( provilkov et al. , 2020 )",with,probability,"bpe - dropout ( provilkov et al. , 2020 ) with probability",0.650711715221405
translation,184,79,experimental-setup,probability,of,0.1,probability of 0.1,0.6106660962104797
translation,184,79,experimental-setup,experimental setup,BPE -ing,training data,experimental setup BPE -ing training data,0.7025333642959595
translation,184,86,experimental-setup,mini-batch,to,workspace,mini-batch to workspace,0.5954526662826538
translation,184,86,experimental-setup,mini-batch,set,learning rate,mini-batch set learning rate,0.623399019241333
translation,184,86,experimental-setup,workspace,of,6144 mb,workspace of 6144 mb,0.5880869626998901
translation,184,86,experimental-setup,learning rate,to,0.0003,learning rate to 0.0003,0.5614109635353088
translation,184,86,experimental-setup,0.0003,with,warm - up,0.0003 with warm - up,0.5756367444992065
translation,184,86,experimental-setup,warm - up,increasing,linearly,warm - up increasing linearly,0.7929983735084534
translation,184,86,experimental-setup,warm - up,decaying by,16000 ? no. batches,warm - up decaying by 16000 ? no. batches,0.7246736884117126
translation,184,86,experimental-setup,linearly,for,16000 batches,linearly for 16000 batches,0.6967990398406982
translation,184,86,experimental-setup,experimental setup,fit,mini-batch,experimental setup fit mini-batch,0.676987886428833
translation,184,87,experimental-setup,multiple gpus,using,"adam ( kingma and ba , 2014 )","multiple gpus using adam ( kingma and ba , 2014 )",0.6227456331253052
translation,184,87,experimental-setup,"adam ( kingma and ba , 2014 )",with,synchronous updates,"adam ( kingma and ba , 2014 ) with synchronous updates",0.625138521194458
translation,184,87,experimental-setup,synchronous updates,for,optimization,synchronous updates for optimization,0.6213341951370239
translation,184,87,experimental-setup,synchronous updates,setting,"? 1 = 0.9 , ? 2 = 0.98 and = 1e ? 09","synchronous updates setting ? 1 = 0.9 , ? 2 = 0.98 and = 1e ? 09",0.42201128602027893
translation,184,87,experimental-setup,experimental setup,train on,multiple gpus,experimental setup train on multiple gpus,0.6757235527038574
translation,184,88,experimental-setup,transformer dropout,between,layers,transformer dropout between layers,0.6761347055435181
translation,184,88,experimental-setup,layers,to,0.01,layers to 0.01,0.5881240367889404
translation,184,88,experimental-setup,experimental setup,set,transformer dropout,experimental setup set transformer dropout,0.6230356693267822
translation,184,89,experimental-setup,maximum sentence length,of,200 tokens,maximum sentence length of 200 tokens,0.5668289065361023
translation,184,89,experimental-setup,maximum target length,as,source length factor,maximum target length as source length factor,0.5434369444847107
translation,184,89,experimental-setup,source length factor,of,2,source length factor of 2,0.6546915173530579
translation,184,89,experimental-setup,label smoothing,of,0.01,label smoothing of 0.01,0.5914155840873718
translation,184,89,experimental-setup,experimental setup,use,maximum sentence length,experimental setup use maximum sentence length,0.5972570776939392
translation,184,89,experimental-setup,experimental setup,use,maximum target length,experimental setup use maximum target length,0.5790070295333862
translation,184,89,experimental-setup,experimental setup,use,label smoothing,experimental setup use label smoothing,0.5524656176567078
translation,184,90,experimental-setup,validation,use,beam size,validation use beam size,0.6760420799255371
translation,184,90,experimental-setup,validation,normalize,translation score,validation normalize translation score,0.6482487320899963
translation,184,90,experimental-setup,beam size,of,6,beam size of 6,0.7021368145942688
translation,184,90,experimental-setup,translation score,by,translation_length 0.6,translation score by translation_length 0.6,0.5462260842323303
translation,184,90,experimental-setup,experimental setup,During,validation,experimental setup During validation,0.661569356918335
translation,184,92,experimental-setup,approximately 66 hours,on,four nvidia geforce rtx 3090 gpus,approximately 66 hours on four nvidia geforce rtx 3090 gpus,0.5442292094230652
translation,184,92,experimental-setup,experimental setup,trained for,approximately 66 hours,experimental setup trained for approximately 66 hours,0.7514873743057251
translation,184,97,experimental-setup,approximately 54 hours,on,two nvidia geforce rtx 2080 ti gpus,approximately 54 hours on two nvidia geforce rtx 2080 ti gpus,0.539524257183075
translation,184,97,experimental-setup,experimental setup,trained for,approximately 54 hours,experimental setup trained for approximately 54 hours,0.7503055334091187
translation,184,106,experimental-setup,approximately 46 hours,on,single nvidia a100 sxm4 gpu,approximately 46 hours on single nvidia a100 sxm4 gpu,0.5104684233665466
translation,184,106,experimental-setup,experimental setup,trained for,approximately 46 hours,experimental setup trained for approximately 46 hours,0.7525683045387268
translation,184,9,experiments,swedish,has,sv ),swedish has sv ),0.6160393357276917
translation,184,99,experiments,"mt5 ( xue et al. , 2020 )",to,translation task,"mt5 ( xue et al. , 2020 ) to translation task",0.5230413675308228
translation,184,99,experiments,multilingual pre-trained transformer language model,to,translation task,multilingual pre-trained transformer language model to translation task,0.4831251800060272
translation,184,99,experiments,"mt5 ( xue et al. , 2020 )",has,multilingual pre-trained transformer language model,"mt5 ( xue et al. , 2020 ) has multilingual pre-trained transformer language model",0.5388925671577454
translation,184,7,results,most translation directions,has,our models,most translation directions has our models,0.5780981183052063
translation,184,7,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,184,7,results,outperform,has,other submitted systems,outperform has other submitted systems,0.6116759181022644
translation,184,7,results,results,In,most translation directions,results In most translation directions,0.4849632680416107
translation,184,109,results,internal test set and shared task validation sets,show,our models,internal test set and shared task validation sets show our models,0.6132504343986511
translation,184,109,results,our models,fail to generalize,well,our models fail to generalize well,0.7900200486183167
translation,184,109,results,well,to,shared task domain,well to shared task domain,0.5565537214279175
translation,184,109,results,results,on,internal test set and shared task validation sets,results on internal test set and shared task validation sets,0.5345596671104431
translation,184,113,results,primary mt5 _base_ada system,outperforms,marian model,primary mt5 _base_ada system outperforms marian model,0.734923779964447
translation,184,113,results,marian model,trained from,scratch,marian model trained from scratch,0.79039067029953
translation,184,113,results,marian model,average of,+ 10.7 and + 8.6 bleu points,marian model average of + 10.7 and + 8.6 bleu points,0.7157654762268066
translation,184,113,results,+ 10.7 and + 8.6 bleu points,on,internal and shared task validation sets,+ 10.7 and + 8.6 bleu points on internal and shared task validation sets,0.5241952538490295
translation,184,114,results,further fine-tuned variant mt5 _base_ada_ft,leads to,additional average improvement,further fine-tuned variant mt5 _base_ada_ft leads to additional average improvement,0.6319671869277954
translation,184,114,results,additional average improvement,of,just under + 1 bleu point,additional average improvement of just under + 1 bleu point,0.5286940336227417
translation,184,114,results,just under + 1 bleu point,on,both sets,just under + 1 bleu point on both sets,0.5967313051223755
translation,184,114,results,results,has,further fine-tuned variant mt5 _base_ada_ft,results has further fine-tuned variant mt5 _base_ada_ft,0.637032151222229
translation,184,115,results,marian model,outperformed by,fine-tuned variant marian_ft,marian model outperformed by fine-tuned variant marian_ft,0.7504259943962097
translation,184,115,results,average improvement,of,+ 7.7 bleu points,average improvement of + 7.7 bleu points,0.5188549757003784
translation,184,115,results,average improvement,of,+ 8.5 bleu points,average improvement of + 8.5 bleu points,0.5251764059066772
translation,184,115,results,+ 7.7 bleu points,on,internal test set,+ 7.7 bleu points on internal test set,0.5230264663696289
translation,184,115,results,+ 8.5 bleu points,on,shared task validation set,+ 8.5 bleu points on shared task validation set,0.5087978839874268
translation,184,115,results,results,has,marian model,results has marian model,0.5433902144432068
translation,184,117,results,last 4 checkpoints,of,fine-tuned marian model,last 4 checkpoints of fine-tuned marian model,0.5845863819122314
translation,184,117,results,fine-tuned marian model,for,marian_ft_esmb,fine-tuned marian model for marian_ft_esmb,0.6580116748809814
translation,184,117,results,fine-tuned marian model,boosts,performance,fine-tuned marian model boosts performance,0.6891719698905945
translation,184,117,results,performance,by,+ 0.3 and + 0.6 average bleu,performance by + 0.3 and + 0.6 average bleu,0.5734619498252869
translation,184,117,results,performance,on,internal test set and the shared task validation set,performance on internal test set and the shared task validation set,0.5607479214668274
translation,184,117,results,performance,by,+ 3.7 and + 0.5 average bleu,performance by + 3.7 and + 0.5 average bleu,0.5765633583068848
translation,184,117,results,performance,on,internal test set and the shared task validation set,performance on internal test set and the shared task validation set,0.5607479214668274
translation,184,117,results,+ 0.3 and + 0.6 average bleu,on,internal and shared task validation sets,+ 0.3 and + 0.6 average bleu on internal and shared task validation sets,0.5243832468986511
translation,184,117,results,internal and shared task validation sets,over,marian_ft,internal and shared task validation sets over marian_ft,0.658689558506012
translation,184,117,results,model,by,+ 3.7 and + 0.5 average bleu,model by + 3.7 and + 0.5 average bleu,0.5879642367362976
translation,184,117,results,+ 3.7 and + 0.5 average bleu,on,internal test set and the shared task validation set,+ 3.7 and + 0.5 average bleu on internal test set and the shared task validation set,0.5400896072387695
translation,184,117,results,ensembling,has,last 4 checkpoints,ensembling has last 4 checkpoints,0.6156033873558044
translation,184,117,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,184,118,results,mt5 _base_ada_ft model,as,primary system,mt5 _base_ada_ft model as primary system,0.5736324787139893
translation,184,118,results,primary system,to,shared task,primary system to shared task,0.5678272843360901
translation,184,118,results,system,won in,shared task rankings,system won in shared task rankings,0.651607871055603
translation,184,118,results,results,submitted,mt5 _base_ada_ft model,results submitted mt5 _base_ada_ft model,0.6391621828079224
translation,184,119,results,global automated evaluations,of,shared task,global automated evaluations of shared task,0.5667716264724731
translation,184,119,results,contrastive system,is,best-performing submitted system,contrastive system is best-performing submitted system,0.5791947841644287
translation,184,119,results,outperforming,by,+ 8.5 bleu,outperforming by + 8.5 bleu,0.5840052366256714
translation,184,119,results,outperforming,approximately,+ 8.5 bleu,outperforming approximately + 8.5 bleu,0.6211816668510437
translation,184,119,results,official mt5 baseline,by,+ 8.5 bleu,official mt5 baseline by + 8.5 bleu,0.5399420857429504
translation,184,119,results,official mt5 baseline,approximately,+ 8.5 bleu,official mt5 baseline approximately + 8.5 bleu,0.5569724440574646
translation,184,119,results,global automated evaluations,has,contrastive system,global automated evaluations has contrastive system,0.5894376635551453
translation,184,119,results,shared task,has,contrastive system,shared task has contrastive system,0.6131817102432251
translation,184,119,results,outperforming,has,official mt5 baseline,outperforming has official mt5 baseline,0.5862427949905396
translation,184,119,results,results,In,global automated evaluations,results In global automated evaluations,0.5332716703414917
translation,184,121,results,"m2m-100 ( fan et al. , 2020 ) baseline",has,outperforms,"m2m-100 ( fan et al. , 2020 ) baseline has outperforms",0.5520995259284973
translation,184,121,results,outperforms,has,all submitted systems,outperforms has all submitted systems,0.5815348625183105
translation,184,121,results,results,has,"m2m-100 ( fan et al. , 2020 ) baseline","results has m2m-100 ( fan et al. , 2020 ) baseline",0.5252652168273926
translation,185,87,experimental-setup,base nmt models,learn,30 k byte-pair encoding ( bpe ) units,base nmt models learn 30 k byte-pair encoding ( bpe ) units,0.6621374487876892
translation,185,87,experimental-setup,base nmt models,learn,20k bpe units,base nmt models learn 20k bpe units,0.6911813020706177
translation,185,87,experimental-setup,base nmt models,learn,24 k bpe units,base nmt models learn 24 k bpe units,0.6801573634147644
translation,185,87,experimental-setup,30 k byte-pair encoding ( bpe ) units,for,de-en and en-de,30 k byte-pair encoding ( bpe ) units for de-en and en-de,0.6700683832168579
translation,185,87,experimental-setup,30 k byte-pair encoding ( bpe ) units,using,sentencepiece toolkit,30 k byte-pair encoding ( bpe ) units using sentencepiece toolkit,0.6609807014465332
translation,185,87,experimental-setup,20k bpe units,for,en-ta,20k bpe units for en-ta,0.7000729441642761
translation,185,87,experimental-setup,24 k bpe units,for,ru - en,24 k bpe units for ru - en,0.7115916013717651
translation,185,87,experimental-setup,24 k bpe units,using,sentencepiece toolkit,24 k bpe units using sentencepiece toolkit,0.6462484002113342
translation,185,87,experimental-setup,experimental setup,For,base nmt models,experimental setup For base nmt models,0.5827277302742004
translation,185,90,experimental-setup,mt models,using,bitext data only,mt models using bitext data only,0.6676421761512756
translation,185,90,experimental-setup,experimental setup,use,"transformer ( vaswani et al. , 2017 ) architecture","experimental setup use transformer ( vaswani et al. , 2017 ) architecture",0.5739732980728149
translation,185,90,experimental-setup,experimental setup,train,mt models,experimental setup train mt models,0.6423301696777344
translation,185,103,experimental-setup,weights and the length penalty,tuned on,validation set,weights and the length penalty tuned on validation set,0.7303066253662109
translation,185,103,experimental-setup,validation set,via,random search,validation set via random search,0.7069026231765747
translation,185,103,experimental-setup,experimental setup,has,weights and the length penalty,experimental setup has weights and the length penalty,0.5501005053520203
translation,185,104,experimental-setup,lms,transformers with,16 blocks,lms transformers with 16 blocks,0.7542418837547302
translation,185,104,experimental-setup,lms,transformers with,16 attention heads,lms transformers with 16 attention heads,0.7346101403236389
translation,185,104,experimental-setup,lms,embedding size,1024,lms embedding size 1024,0.7357824444770813
translation,185,104,experimental-setup,experimental setup,has,lms,experimental setup has lms,0.5008388757705688
translation,185,110,experimental-setup,n-best lists,from,bitext and bt,n-best lists from bitext and bt,0.5916820764541626
translation,185,110,experimental-setup,n-best lists,use,only bt data,n-best lists use only bt data,0.6743807196617126
translation,185,110,experimental-setup,bitext and bt,as,training data,bitext and bt as training data,0.5250601768493652
translation,185,110,experimental-setup,rerankers,for,"de-en , en-de and en-ta","rerankers for de-en , en-de and en-ta",0.6811137795448303
translation,185,110,experimental-setup,only bt data,for,ru-en,only bt data for ru-en,0.7414195537567139
translation,185,110,experimental-setup,experimental setup,combine,n-best lists,experimental setup combine n-best lists,0.6267510056495667
translation,185,111,experimental-setup,drnmt,with,batch size,drnmt with batch size,0.663829505443573
translation,185,111,experimental-setup,drnmt,with,early - stop,drnmt with early - stop,0.7012472152709961
translation,185,111,experimental-setup,drnmt,use,"adam ( kingma and ba , 2015 )","drnmt use adam ( kingma and ba , 2015 )",0.6055436134338379
translation,185,111,experimental-setup,early - stop,when,validation performance,early - stop when validation performance,0.6488705277442932
translation,185,111,experimental-setup,does not improve,after,12 k parameter updates,does not improve after 12 k parameter updates,0.6595943570137024
translation,185,111,experimental-setup,batch size,has,512,batch size has 512,0.6082481741905212
translation,185,111,experimental-setup,validation performance,has,does not improve,validation performance has does not improve,0.610643744468689
translation,185,111,experimental-setup,experimental setup,train,drnmt,experimental setup train drnmt,0.6691830158233643
translation,185,112,experimental-setup,hyper-parameters,including,learning rate,hyper-parameters including learning rate,0.6627153754234314
translation,185,112,experimental-setup,hyper-parameters,including,number of warmup steps,hyper-parameters including number of warmup steps,0.6421201229095459
translation,185,112,experimental-setup,hyper-parameters,including,dropout rate,hyper-parameters including dropout rate,0.6596527695655823
translation,185,112,experimental-setup,hyper-parameters,tuned on,validation set,hyper-parameters tuned on validation set,0.7546796202659607
translation,185,112,experimental-setup,experimental setup,has,hyper-parameters,experimental setup has hyper-parameters,0.5121466517448425
translation,185,109,experiments,beam decoding,on,bitext and bt data,beam decoding on bitext and bt data,0.5622174143791199
translation,185,109,experiments,bitext and bt data,using,baseline mt models,bitext and bt data using baseline mt models,0.6304906010627747
translation,185,109,experiments,baseline mt models,to generate,n-best lists,baseline mt models to generate n-best lists,0.5943154096603394
translation,185,109,experiments,n-best lists,with,50 hypotheses,n-best lists with 50 hypotheses,0.6406438946723938
translation,185,131,experiments,en-ta,is,difficult lan- guage pair,en-ta is difficult lan- guage pair,0.6677860021591187
translation,185,131,experiments,difficult lan- guage pair,in which,baseline nmt,difficult lan- guage pair in which baseline nmt,0.5874127149581909
translation,185,131,experiments,baseline nmt,is,weak,baseline nmt is weak,0.5821259617805481
translation,185,4,model,better output hypothesis,within,n-best list or lattice,better output hypothesis within n-best list or lattice,0.6382394433021545
translation,185,4,model,model,has,reranking models,model has reranking models,0.5885222554206848
translation,185,21,model,training large transformer models,using,reranking objective,training large transformer models using reranking objective,0.617971658706665
translation,185,21,model,reranking objective,can further improve,performance,reranking objective can further improve performance,0.6921895146369934
translation,185,21,model,model,explore,training large transformer models,model explore training large transformer models,0.6303878426551819
translation,185,22,model,drnmt,takes as input,entire source sentence,drnmt takes as input entire source sentence,0.6758556962013245
translation,185,22,model,drnmt,takes as input,n-best list,drnmt takes as input n-best list,0.773230254650116
translation,185,22,model,n-best list,of,output hypotheses,n-best list of output hypotheses,0.583881676197052
translation,185,22,model,distribution,of,sentence - level evaluation scores,distribution of sentence - level evaluation scores,0.5484266877174377
translation,185,22,model,model,dubbed,drnmt,model dubbed drnmt,0.8191524744033813
translation,185,28,model,success,of,pre-training,success of pre-training,0.5563589930534363
translation,185,28,model,pre-training,by finetuning,masked language models ( mlm ; devlin et al. 2019 ),pre-training by finetuning masked language models ( mlm ; devlin et al. 2019 ),0.7796550989151001
translation,185,28,model,masked language models ( mlm ; devlin et al. 2019 ),initializes,model,masked language models ( mlm ; devlin et al. 2019 ) initializes model,0.6542699337005615
translation,185,28,model,model,with,features,model with features,0.6380274295806885
translation,185,28,model,features,trained on,much more training data,features trained on much more training data,0.7524219155311584
translation,185,28,model,model,leverage,success,model leverage success,0.7535521388053894
translation,185,29,model,original dataset,with,back - translated data,original dataset with back - translated data,0.5970668792724609
translation,185,29,model,back - translated data,has,bt,back - translated data has bt,0.6194638013839722
translation,185,29,model,model,augment,original dataset,model augment original dataset,0.6781730055809021
translation,185,119,results,all methods,improve over,beam search output,all methods improve over beam search output,0.7051058411598206
translation,185,119,results,beam search output,with,gains,beam search output with gains,0.666617751121521
translation,185,119,results,gains,ranging from,1.0 to 4.1 bleu,gains ranging from 1.0 to 4.1 bleu,0.6388359069824219
translation,185,119,results,results,notice,all methods,results notice all methods,0.6661849617958069
translation,185,125,results,drnmt,performs,on par,drnmt performs on par,0.6564368605613708
translation,185,125,results,drnmt,performs,better ( de-en ),drnmt performs better ( de-en ),0.6766281723976135
translation,185,125,results,better ( de-en ),than,ncd,better ( de-en ) than ncd,0.6316527128219604
translation,185,125,results,on par,has,"en-ta , en-de and ru-en )","on par has en-ta , en-de and ru-en )",0.6443120837211609
translation,185,125,results,results,has,drnmt,results has drnmt,0.6297767758369446
translation,185,127,results,our reranker,works,at least as well,our reranker works at least as well,0.6162525415420532
translation,185,127,results,at least as well,as,ncd,at least as well as ncd,0.675982654094696
translation,185,127,results,results,has,our reranker,results has our reranker,0.5683082938194275
translation,185,128,results,discriminative reranker and ncd,complementary to,each other,discriminative reranker and ncd complementary to each other,0.6769778728485107
translation,185,128,results,gains,between,0.9 bleu ( de-en ) and 0.2 ( en - ta ),gains between 0.9 bleu ( de-en ) and 0.2 ( en - ta ),0.6549769639968872
translation,185,128,results,gains,between,4.1 bleu ( de-en ) and 0.5 ( en - ta ),gains between 4.1 bleu ( de-en ) and 0.5 ( en - ta ),0.6306252479553223
translation,185,128,results,gains,between,4.1 bleu ( de-en ) and 0.5 ( en - ta ),gains between 4.1 bleu ( de-en ) and 0.5 ( en - ta ),0.6306252479553223
translation,185,128,results,0.9 bleu ( de-en ) and 0.2 ( en - ta ),compared to,ncd,0.9 bleu ( de-en ) and 0.2 ( en - ta ) compared to ncd,0.708865225315094
translation,185,128,results,overall gain,between,4.1 bleu ( de-en ) and 0.5 ( en - ta ),overall gain between 4.1 bleu ( de-en ) and 0.5 ( en - ta ),0.5722788572311401
translation,185,128,results,4.1 bleu ( de-en ) and 0.5 ( en - ta ),compared to,beam baseline,4.1 bleu ( de-en ) and 0.5 ( en - ta ) compared to beam baseline,0.6236127018928528
translation,185,128,results,results,has,discriminative reranker and ncd,results has discriminative reranker and ncd,0.5468895435333252
translation,185,129,results,gain,brought by,discriminative reranking,gain brought by discriminative reranking,0.694495439529419
translation,185,129,results,discriminative reranking,comparing,drnmt,discriminative reranking comparing drnmt,0.7017182111740112
translation,185,129,results,results,has,gain,results has gain,0.5428218841552734
translation,185,130,results,discriminative reranking,yields,better translations,discriminative reranking yields better translations,0.6440502405166626
translation,185,130,results,discriminative reranking,with,gains,discriminative reranking with gains,0.6387209296226501
translation,185,130,results,better translations,with,gains,better translations with gains,0.6558917164802551
translation,185,130,results,gains,between,0.2 and 2.3 bleu points,gains between 0.2 and 2.3 bleu points,0.627270519733429
translation,185,151,results,size,of,n-best list,size of n-best list,0.5752169489860535
translation,185,151,results,n-best list,during,test time,n-best list during test time,0.6438881158828735
translation,185,151,results,test time,has,increases,test time has increases,0.60179603099823
translation,185,151,results,all rerankers and ncd,has,improve,all rerankers and ncd has improve,0.6094103455543518
translation,185,151,results,results,As,size,results As size,0.5499626398086548
translation,185,153,results,reranker,trained with,50 hypotheses,reranker trained with 50 hypotheses,0.7829625606536865
translation,185,153,results,reranker,gives,1.4 bleu improvement,reranker gives 1.4 bleu improvement,0.543519914150238
translation,185,153,results,50 hypotheses,gives,1.4 bleu improvement,50 hypotheses gives 1.4 bleu improvement,0.5478918552398682
translation,185,153,results,1.4 bleu improvement,over,beam decoding,1.4 bleu improvement over beam decoding,0.6179351806640625
translation,185,153,results,beam decoding,when,beam size,beam decoding when beam size,0.6841189861297607
translation,185,153,results,beam decoding,when,beam size,beam decoding when beam size,0.6841189861297607
translation,185,153,results,beam size,is,5,beam size is 5,0.657706618309021
translation,185,153,results,5,at,test time,5 at test time,0.4709719717502594
translation,185,153,results,improvement,increases to,3.4 bleu,improvement increases to 3.4 bleu,0.6448771357536316
translation,185,153,results,3.4 bleu,increase,beam size,3.4 bleu increase beam size,0.6295966506004333
translation,185,153,results,results,has,reranker,results has reranker,0.5974451899528503
translation,185,154,results,consistently perform better than or equally well,as,ncd,consistently perform better than or equally well as ncd,0.5805241465568542
translation,185,154,results,ncd,in,all training and testing scenarios,ncd in all training and testing scenarios,0.5502858757972717
translation,185,154,results,drnmt,has,consistently perform better than or equally well,drnmt has consistently perform better than or equally well,0.6136360168457031
translation,185,154,results,results,has,drnmt,results has drnmt,0.6297767758369446
translation,185,164,results,randomly initialized reranker,performs,significantly less well,randomly initialized reranker performs significantly less well,0.5539777278900146
translation,185,164,results,significantly less well,with,decrease,significantly less well with decrease,0.6528201699256897
translation,185,164,results,decrease,of,0.8 bleu,decrease of 0.8 bleu,0.6269804239273071
translation,185,172,results,source sentences,achieves,small gain,source sentences achieves small gain,0.661834180355072
translation,185,172,results,small gain,of,0.2 bleu,small gain of 0.2 bleu,0.5932977795600891
translation,185,178,results,reranker,with,only bitext data,reranker with only bitext data,0.5932154059410095
translation,185,178,results,reranker,deteriorates,model 's performance,reranker deteriorates model 's performance,0.6209782361984253
translation,185,178,results,model 's performance,by,2 bleu points,model 's performance by 2 bleu points,0.5610242486000061
translation,185,178,results,training,has,reranker,training has reranker,0.558047890663147
translation,185,178,results,results,see that,training,results see that training,0.6431185603141785
translation,185,181,results,model,achieves,best validation performance,model achieves best validation performance,0.6577022075653076
translation,185,181,results,best validation performance,after,1.9 passes,best validation performance after 1.9 passes,0.6637566089630127
translation,185,181,results,1.9 passes,over,combination of bitext and bt data,1.9 passes over combination of bitext and bt data,0.601698637008667
translation,185,181,results,results,has,model,results has model,0.5339115858078003
translation,185,192,results,drnmt,gives,1.5 bleu improvement,drnmt gives 1.5 bleu improvement,0.5973690748214722
translation,185,192,results,drnmt,combining,ncd and reranker,drnmt combining ncd and reranker,0.7764762043952942
translation,185,192,results,1.5 bleu improvement,over,beam decoding baseline,1.5 bleu improvement over beam decoding baseline,0.6193039417266846
translation,185,192,results,ncd and reranker,gives,additional gain,ncd and reranker gives additional gain,0.6345608830451965
translation,185,192,results,additional gain,of,0.5 bleu,additional gain of 0.5 bleu,0.5791524052619934
translation,185,192,results,results,see that,drnmt,results see that drnmt,0.7000107169151306
translation,185,192,results,results,combining,ncd and reranker,results combining ncd and reranker,0.6791671514511108
translation,185,212,results,ncd and our discrim-inative reranker,complementary to,each other,ncd and our discrim-inative reranker complementary to each other,0.6971083283424377
translation,185,212,results,sizeable improvements,over,each other,sizeable improvements over each other,0.7301129698753357
translation,185,212,results,sizeable improvements,over,beam baseline,sizeable improvements over beam baseline,0.719659686088562
translation,185,212,results,results,found that,ncd and our discrim-inative reranker,results found that ncd and our discrim-inative reranker,0.6899811625480652
translation,186,7,ablation-analysis,attention weights,assigned to,indispensable tokens,attention weights assigned to indispensable tokens,0.6715459227561951
translation,186,7,ablation-analysis,removal,leads to,dramatic performance decrease,removal leads to dramatic performance decrease,0.7099975943565369
translation,186,7,ablation-analysis,ablation analysis,increase,attention weights,ablation analysis increase attention weights,0.7353107333183289
translation,186,25,ablation-analysis,attention weights,by reallocating,more attention,attention weights by reallocating more attention,0.6722888946533203
translation,186,25,ablation-analysis,more attention,to,informative inputs,more attention to informative inputs,0.5479626655578613
translation,186,25,ablation-analysis,ablation analysis,calibrate,attention weights,ablation analysis calibrate attention weights,0.7211552858352661
translation,186,138,ablation-analysis,annealing learning,is,comparatively more stable,annealing learning is comparatively more stable,0.5512286424636841
translation,186,138,ablation-analysis,comparatively more stable,reduces,impact of acn,comparatively more stable reduces impact of acn,0.6911531686782837
translation,186,138,ablation-analysis,impact of acn,when,mask perturbation model,impact of acn when mask perturbation model,0.6729539632797241
translation,186,138,ablation-analysis,mask perturbation model,is,not well -trained,mask perturbation model is not well -trained,0.59526127576828
translation,186,138,ablation-analysis,not well -trained,at,initial stage,not well -trained at initial stage,0.5318149328231812
translation,186,138,ablation-analysis,ablation analysis,has,annealing learning,ablation analysis has annealing learning,0.5251897573471069
translation,186,172,ablation-analysis,calibrated attention weights,are,more uniform,calibrated attention weights are more uniform,0.5529243350028992
translation,186,172,ablation-analysis,calibrated attention weights,are,more focused,calibrated attention weights are more focused,0.5707818865776062
translation,186,172,ablation-analysis,calibrated attention weights,become,more focused,calibrated attention weights become more focused,0.5905883312225342
translation,186,172,ablation-analysis,more uniform,at,1 - 3 layers,more uniform at 1 - 3 layers,0.5761459469795227
translation,186,172,ablation-analysis,more focused,at,4 - 6 layers,more focused at 4 - 6 layers,0.5754362940788269
translation,186,172,ablation-analysis,attention weights,become,more focused,attention weights become more focused,0.5880405306816101
translation,186,172,ablation-analysis,more focused,for,all layers,more focused for all layers,0.6435045003890991
translation,186,172,ablation-analysis,all layers,except,1- st layer,all layers except 1- st layer,0.6581688523292542
translation,186,172,ablation-analysis,en? de translation,has,calibrated attention weights,en? de translation has calibrated attention weights,0.5797742009162903
translation,186,172,ablation-analysis,ablation analysis,For,en? de translation,ablation analysis For en? de translation,0.6421452164649963
translation,186,116,experimental-setup,described models,with,fairseq 5 toolkit,described models with fairseq 5 toolkit,0.618804931640625
translation,186,116,experimental-setup,experimental setup,implement,described models,experimental setup implement described models,0.6506701707839966
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",hidden size d model =,512,"transformer base ( vaswani et al. , 2017 ) hidden size d model = 512",0.7734886407852173
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",hidden size d model =,6 encoder and decoder layers,"transformer base ( vaswani et al. , 2017 ) hidden size d model = 6 encoder and decoder layers",0.7182709574699402
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",hidden size d model =,8 attention heads,"transformer base ( vaswani et al. , 2017 ) hidden size d model = 8 attention heads",0.7370493412017822
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",hidden size d model =,2048 feed -forward innerlayer dimension,"transformer base ( vaswani et al. , 2017 ) hidden size d model = 2048 feed -forward innerlayer dimension",0.7528250217437744
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",has,6 encoder and decoder layers,"transformer base ( vaswani et al. , 2017 ) has 6 encoder and decoder layers",0.5588074326515198
translation,186,117,experimental-setup,"transformer base ( vaswani et al. , 2017 )",has,8 attention heads,"transformer base ( vaswani et al. , 2017 ) has 8 attention heads",0.5763049721717834
translation,186,117,experimental-setup,experimental setup,experiment with,"transformer base ( vaswani et al. , 2017 )","experimental setup experiment with transformer base ( vaswani et al. , 2017 )",0.6785895824432373
translation,186,118,experimental-setup,dropout rate,of,residual connection,dropout rate of residual connection,0.5791635513305664
translation,186,118,experimental-setup,residual connection,is,0.1,residual connection is 0.1,0.579466700553894
translation,186,118,experimental-setup,0.1,except for,zh?en ( 0.3 ),0.1 except for zh?en ( 0.3 ),0.6355189681053162
translation,186,118,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,186,119,experimental-setup,training,use,label smoothing,training use label smoothing,0.6442112922668457
translation,186,119,experimental-setup,training,employ,"adam ( ? 1 = 0.9 , ? 2 = 0.998 )","training employ adam ( ? 1 = 0.9 , ? 2 = 0.998 )",0.5411302447319031
translation,186,119,experimental-setup,label smoothing,employ,"adam ( ? 1 = 0.9 , ? 2 = 0.998 )","label smoothing employ adam ( ? 1 = 0.9 , ? 2 = 0.998 )",0.5280033349990845
translation,186,119,experimental-setup,"adam ( ? 1 = 0.9 , ? 2 = 0.998 )",for,parameter optimization,"adam ( ? 1 = 0.9 , ? 2 = 0.998 ) for parameter optimization",0.621825098991394
translation,186,119,experimental-setup,"adam ( ? 1 = 0.9 , ? 2 = 0.998 )",with,scheduled learning rate,"adam ( ? 1 = 0.9 , ? 2 = 0.998 ) with scheduled learning rate",0.6014915108680725
translation,186,119,experimental-setup,parameter optimization,with,scheduled learning rate,parameter optimization with scheduled learning rate,0.6319870352745056
translation,186,119,experimental-setup,scheduled learning rate,of,"4,000 warm - up steps","scheduled learning rate of 4,000 warm - up steps",0.5982118844985962
translation,186,119,experimental-setup,label smoothing,has,of value ls = 0.1,label smoothing has of value ls = 0.1,0.5685350298881531
translation,186,119,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,186,119,experimental-setup,experimental setup,employ,"adam ( ? 1 = 0.9 , ? 2 = 0.998 )","experimental setup employ adam ( ? 1 = 0.9 , ? 2 = 0.998 )",0.5265344381332397
translation,186,6,model,attention weights,by introducing,mask perturbation model,attention weights by introducing mask perturbation model,0.7175841331481934
translation,186,6,model,mask perturbation model,automatically evaluates,each input 's contribution,mask perturbation model automatically evaluates each input 's contribution,0.7357592582702637
translation,186,6,model,each input 's contribution,to,model outputs,each input 's contribution to model outputs,0.5092949867248535
translation,186,6,model,model,calibrate,attention weights,model calibrate attention weights,0.7360048294067383
translation,186,12,model,source-side information,by inducing,conditional distribution,source-side information by inducing conditional distribution,0.6943233013153076
translation,186,12,model,conditional distribution,over,inputs,conditional distribution over inputs,0.7126203775405884
translation,186,12,model,model,dynamically encodes,source-side information,model dynamically encodes source-side information,0.7837679982185364
translation,186,26,model,three fusion methods,to incorporate,calibrated attention weights,three fusion methods to incorporate calibrated attention weights,0.6421220302581787
translation,186,26,model,calibrated attention weights,into,original attention weights,calibrated attention weights into original attention weights,0.5656174421310425
translation,186,26,model,model,design,three fusion methods,model design three fusion methods,0.5875487923622131
translation,186,127,results,our method,exhibits,better performance,our method exhibits better performance,0.6234087944030762
translation,186,127,results,better performance,than,above models,better performance than above models,0.6072666645050049
translation,186,127,results,results,has,our method,results has our method,0.5589964985847473
translation,186,128,results,our model,yields,significant gain,our model yields significant gain,0.6945786476135254
translation,186,128,results,significant gain,by looking into,inputs,significant gain by looking into inputs,0.695243239402771
translation,186,128,results,inputs,affect,model 's internal training,inputs affect model 's internal training,0.6839689612388611
translation,186,130,results,significantly outperforms,by,0.96 ( mt02 ),significantly outperforms by 0.96 ( mt02 ),0.6208133101463318
translation,186,130,results,significantly outperforms,by,"0.84 ( mt03 ) , 0.58 ( mt04 ) , 1.02 ( mt05 ) and 0.76 ( mt06 )","significantly outperforms by 0.84 ( mt03 ) , 0.58 ( mt04 ) , 1.02 ( mt05 ) and 0.76 ( mt06 )",0.609149158000946
translation,186,130,results,baseline,by,0.96 ( mt02 ),baseline by 0.96 ( mt02 ),0.5616183876991272
translation,186,130,results,baseline,by,"0.84 ( mt03 ) , 0.58 ( mt04 ) , 1.02 ( mt05 ) and 0.76 ( mt06 )","baseline by 0.84 ( mt03 ) , 0.58 ( mt04 ) , 1.02 ( mt05 ) and 0.76 ( mt06 )",0.5864028930664062
translation,186,130,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,186,130,results,significantly outperforms,has,baseline,significantly outperforms has baseline,0.6019589304924011
translation,186,130,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,186,132,results,our methods,improve,performance,our methods improve performance,0.6793423295021057
translation,186,132,results,performance,over,baseline,performance over baseline,0.6994789242744446
translation,186,132,results,performance,by,0.6 bleu ( fi?en ),performance by 0.6 bleu ( fi?en ),0.5616458058357239
translation,186,132,results,performance,by,0.57,performance by 0.57,0.5677220821380615
translation,186,132,results,performance,by,0.95 bleu ( lv?en ),performance by 0.95 bleu ( lv?en ),0.5330274701118469
translation,186,132,results,baseline,by,0.95 bleu ( lv?en ),baseline by 0.95 bleu ( lv?en ),0.5501025319099426
translation,186,132,results,0.57,has,bleu ( en?lv ),0.57 has bleu ( en?lv ),0.5721119046211243
translation,186,133,results,our methods,achieve,substantial improvement,our methods achieve substantial improvement,0.6141694784164429
translation,186,133,results,substantial improvement,of,1.44 more bleu,substantial improvement of 1.44 more bleu,0.5169143676757812
translation,186,133,results,substantial improvement,of,0.95 bleu,substantial improvement of 0.95 bleu,0.5490614771842957
translation,186,133,results,small-scale wmt16 en?ro,has,our methods,small-scale wmt16 en?ro has our methods,0.5534690022468567
translation,186,133,results,results,For,small-scale wmt16 en?ro,results For small-scale wmt16 en?ro,0.6478062868118286
translation,186,134,results,strong baselines,especially for,small-scale dataset,strong baselines especially for small-scale dataset,0.628915011882782
translation,186,134,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,186,134,results,significantly outperforms,has,strong baselines,significantly outperforms has strong baselines,0.6021574139595032
translation,186,134,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,186,137,results,three fusion methods,has,fixed weighted sum,three fusion methods has fixed weighted sum,0.5749197006225586
translation,186,137,results,fixed weighted sum,has,limited gain,fixed weighted sum has limited gain,0.6084127426147461
translation,186,137,results,results,For,three fusion methods,results For three fusion methods,0.5824973583221436
translation,187,163,ablation-analysis,mpd score,of,wsls,mpd score of wsls,0.5612768530845642
translation,187,163,ablation-analysis,wsls,almost 1.5 higher than,gogr,wsls almost 1.5 higher than gogr,0.6477434039115906
translation,187,163,ablation-analysis,md metric,revealing,rationality,md metric revealing rationality,0.6607376933097839
translation,187,163,ablation-analysis,ablation analysis,has,mpd score,ablation analysis has mpd score,0.5179781317710876
translation,187,171,ablation-analysis,attack effectiveness,is,significant,attack effectiveness is significant,0.585189163684845
translation,187,138,baselines,encoder,consists of,forward and backward rnns,encoder consists of forward and backward rnns,0.6827105283737183
translation,187,138,baselines,forward and backward rnns,each having,1000 hidden units,forward and backward rnns each having 1000 hidden units,0.6784543991088867
translation,187,138,baselines,forward and backward rnns,each having,1000 hidden units,forward and backward rnns each having 1000 hidden units,0.6784543991088867
translation,187,138,baselines,decoder,with,1000 hidden units,decoder with 1000 hidden units,0.6492833495140076
translation,187,138,baselines,rnnsearch,has,encoder,rnnsearch has encoder,0.5809045433998108
translation,187,138,baselines,baselines,has,rnnsearch,baselines has rnnsearch,0.583325207233429
translation,187,156,hyperparameters,crafted,by substituting,20 % words,crafted by substituting 20 % words,0.5453910827636719
translation,187,156,hyperparameters,hyperparameters,has,adversaries,hyperparameters has adversaries,0.5180636048316956
translation,187,5,model,novel method,to craft,nmt adversarial examples,novel method to craft nmt adversarial examples,0.6318133473396301
translation,187,5,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,187,8,model,promising black - box attack method,called,word saliency speedup local search ( wsls ),promising black - box attack method called word saliency speedup local search ( wsls ),0.6592872738838196
translation,187,8,model,effectively attack,has,mainstream nmt architectures,effectively attack has mainstream nmt architectures,0.5185051560401917
translation,187,8,model,model,propose,promising black - box attack method,model propose promising black - box attack method,0.6432521343231201
translation,187,47,model,machine translation adversaries,leveraging,round-trip translation,machine translation adversaries leveraging round-trip translation,0.6720766425132751
translation,187,140,model,transformer,comprises,six layers of transformer,transformer comprises six layers of transformer,0.6338769793510437
translation,187,140,model,six layers of transformer,with,512 hidden units,six layers of transformer with 512 hidden units,0.6041532158851624
translation,187,140,model,model,has,transformer,model has transformer,0.5715572834014893
translation,187,147,results,word-level victim models ( rnns. and transf . ),achieve,average bleu score,word-level victim models ( rnns. and transf . ) achieve average bleu score,0.5809698700904846
translation,187,147,results,average bleu score,of,36.71 and 41.55,average bleu score of 36.71 and 41.55,0.5534260272979736
translation,187,147,results,36.71 and 41.55,for,zh? en translation,36.71 and 41.55 for zh? en translation,0.6691774725914001
translation,187,147,results,results,observe,word-level victim models ( rnns. and transf . ),results observe word-level victim models ( rnns. and transf . ),0.49835100769996643
translation,187,148,results,oracle models,achieve,average bleu score,oracle models achieve average bleu score,0.5425150990486145
translation,187,148,results,oracle models,achieve,bleu score,oracle models achieve bleu score,0.5170213580131531
translation,187,148,results,average bleu score,of,82.9,average bleu score of 82.9,0.5393493175506592
translation,187,148,results,82.9,for,en? zh translation,82.9 for en? zh translation,0.619641125202179
translation,187,148,results,bleu score,of,54.83 and 57.24,bleu score of 54.83 and 57.24,0.5548330545425415
translation,187,148,results,54.83 and 57.24,for,de?en,54.83 and 57.24 for de?en,0.6913244724273682
translation,187,148,results,54.83 and 57.24,for,en translations,54.83 and 57.24 for en translations,0.6755093932151794
translation,187,148,results,backtranslation,has,oracle models,backtranslation has oracle models,0.5599321722984314
translation,187,148,results,results,For,backtranslation,results For backtranslation,0.6003214120864868
translation,187,160,results,gogr and wsls,have,md scores,gogr and wsls have md scores,0.5648046135902405
translation,187,160,results,gogr and wsls,have,attack results,gogr and wsls have attack results,0.5678543448448181
translation,187,160,results,md scores,close to,original reconstruction scores,md scores close to original reconstruction scores,0.6630612015724182
translation,187,160,results,original reconstruction scores,for,rnns.,original reconstruction scores for rnns.,0.6338731050491333
translation,187,160,results,original reconstruction scores,for,transf.,original reconstruction scores for transf.,0.6704882383346558
translation,187,160,results,original reconstruction scores,for,bpe - transf.,original reconstruction scores for bpe - transf.,0.6550803184509277
translation,187,160,results,much better,than,ast - lexical,much better than ast - lexical,0.6556392312049866
translation,187,160,results,much better,than,rogr,much better than rogr,0.6164145469665527
translation,187,162,results,wsls,superior to,gogr,wsls superior to gogr,0.6922897696495056
translation,187,162,results,results,has,wsls,results has wsls,0.5717101693153381
translation,187,167,results,initialization,of,gogr,initialization of gogr,0.570972204208374
translation,187,167,results,initialization,exhibits,significantly better results,initialization exhibits significantly better results,0.6768630743026733
translation,187,167,results,significantly better results,than,rogr,significantly better results than rogr,0.6222891807556152
translation,187,167,results,significantly better results,than,rogr,significantly better results than rogr,0.6222891807556152
translation,187,167,results,faster,than,rogr,faster than rogr,0.5712735652923584
translation,187,167,results,wsls without word saliency speedup,exhibits,slightly higher attack results,wsls without word saliency speedup exhibits slightly higher attack results,0.647506058216095
translation,187,167,results,much longer,than,wsls,much longer than wsls,0.6329039335250854
translation,188,65,baselines,as baselines,for,language directions,as baselines for language directions,0.6753536462783813
translation,188,65,baselines,our own models,has,as baselines,our own models has as baselines,0.5843916535377502
translation,188,65,baselines,baselines,used,our own models,baselines used our own models,0.6739267706871033
translation,188,127,experiments,best performance,obtained by,huawei_agi team,best performance obtained by huawei_agi team,0.6050998568534851
translation,188,127,experiments,huawei_agi team,with,0.4531 and 0.0.4425,huawei_agi team with 0.4531 and 0.0.4425,0.645368218421936
translation,188,127,experiments,en2fr and en2it,has,best performance,en2fr and en2it has best performance,0.585758626461029
translation,188,128,experiments,nvidia nemo team,obtained,best score ( 0.4139 ),nvidia nemo team obtained best score ( 0.4139 ),0.601692259311676
translation,188,128,experiments,best score ( 0.4139 ),for,en2ru,best score ( 0.4139 ) for en2ru,0.6296350359916687
translation,188,129,experiments,highest score,over,all teams and language pairs,highest score over all teams and language pairs,0.619983434677124
translation,188,129,experiments,all teams and language pairs,was,0.5685,all teams and language pairs was 0.5685,0.602003276348114
translation,188,129,experiments,translation into english,has,highest score,translation into english has highest score,0.5756024122238159
translation,188,70,results,en2 fr,for,en2de,en2 fr for en2de,0.7029085159301758
translation,188,70,results,our own model,trained on,biomedical data,our own model trained on biomedical data,0.7533559799194336
translation,188,70,results,our own model,by,almost 3 bleu points,our own model by almost 3 bleu points,0.5531939268112183
translation,188,70,results,outperformed,has,our own model,outperformed has our own model,0.6455883383750916
translation,188,70,results,results,For,en2 fr,results For en2 fr,0.7018494606018066
translation,188,70,results,results,for,en2de,results for en2de,0.5902853012084961
translation,188,115,results,results,not as high as in,medline abstracts task,results not as high as in medline abstracts task,0.6707098484039307
translation,188,115,results,results,above,baseline system,results above baseline system,0.6835640072822571
translation,188,115,results,improved,from,best results,improved from best results,0.5483123064041138
translation,188,115,results,best results,from,2020 challenge,best results from 2020 challenge,0.5674296617507935
translation,188,115,results,results,has,results,results has results,0.48582205176353455
translation,188,120,results,abstracts,for,terminology set,abstracts for terminology set,0.5664762854576111
translation,188,120,results,outperforms,showing,multilingual,outperforms showing multilingual,0.6832391023635864
translation,188,120,results,run2,showing,multilingual,run2 showing multilingual,0.733069121837616
translation,188,120,results,abstracts,has,run1,abstracts has run1,0.6187243461608887
translation,188,120,results,terminology set,has,run1,terminology set has run1,0.6419565677642822
translation,188,120,results,run1,has,outperforms,run1 has outperforms,0.6613432765007019
translation,188,120,results,outperforms,has,run2,outperforms has run2,0.6423908472061157
translation,188,126,results,all the language pairs,for which,huawei_tsc participated,all the language pairs for which huawei_tsc participated,0.6761895418167114
translation,188,126,results,huawei_tsc participated,i.e.,en2de and en2zh,huawei_tsc participated i.e. en2de and en2zh,0.6236870884895325
translation,188,126,results,highest score,namely,0.3259 and 0.4650,highest score namely 0.3259 and 0.4650,0.6842833161354065
translation,188,126,results,results,for,all the language pairs,results for all the language pairs,0.5752717852592468
translation,188,130,results,tmt,obtained,best results,tmt obtained best results,0.6978201866149902
translation,188,130,results,best results,for,three of the language pairs,best results for three of the language pairs,0.5747634768486023
translation,188,130,results,best results,scores,"0.4501 , 0.5382 , and 0.4928","best results scores 0.4501 , 0.5382 , and 0.4928",0.7158277034759521
translation,188,130,results,three of the language pairs,namely,"de2en , es2en , and fr2en","three of the language pairs namely de2en , es2en , and fr2en",0.6828107237815857
translation,188,130,results,results,has,tmt,results has tmt,0.5575854778289795
translation,188,131,results,slightly higher scores,obtained by,huawei_agi team,slightly higher scores obtained by huawei_agi team,0.6119570136070251
translation,188,131,results,it2en and zh2en,has,slightly higher scores,it2en and zh2en has slightly higher scores,0.6123776435852051
translation,188,131,results,slightly higher scores,has,0.4570 and 0.3943,slightly higher scores has 0.4570 and 0.3943,0.5658342242240906
translation,188,131,results,results,For,it2en and zh2en,results For it2en and zh2en,0.5999495387077332
translation,188,278,results,translations,were,good,translations were good,0.6056915521621704
translation,188,278,results,good,containing,sentences,good containing sentences,0.6936125159263611
translation,188,278,results,sentences,with,high - level of fluency,sentences with high - level of fluency,0.6370053887367249
translation,188,278,results,sentences,with,high adequacy,sentences with high adequacy,0.6201021671295166
translation,188,278,results,high adequacy,with respect to,source,high adequacy with respect to source,0.6763001084327698
translation,189,66,ablation-analysis,round-trip translation,based on,"bleu and bert ( devlin et al. , 2019 )","round-trip translation based on bleu and bert ( devlin et al. , 2019 )",0.6420984864234924
translation,189,66,ablation-analysis,"nguyen - son et al. , 2019a )",based on,"bleu and bert ( devlin et al. , 2019 )","nguyen - son et al. , 2019a ) based on bleu and bert ( devlin et al. , 2019 )",0.6799042224884033
translation,189,66,ablation-analysis,improves,by,approximately 10 %,improves by approximately 10 %,0.7052409648895264
translation,189,66,ablation-analysis,round-trip translation,has,"nguyen - son et al. , 2019a )","round-trip translation has nguyen - son et al. , 2019a )",0.5488831996917725
translation,189,66,ablation-analysis,round-trip translation,has,improves,round-trip translation has improves,0.5970962643623352
translation,189,66,ablation-analysis,"bleu and bert ( devlin et al. , 2019 )",has,improves,"bleu and bert ( devlin et al. , 2019 ) has improves",0.5869389176368713
translation,189,66,ablation-analysis,ablation analysis,has,round-trip translation,ablation analysis has round-trip translation,0.5731507539749146
translation,189,87,ablation-analysis,bert and round-trip translation,yield,unstable results,bert and round-trip translation yield unstable results,0.6940739154815674
translation,189,87,ablation-analysis,tsrt,remains,consistent,tsrt remains consistent,0.7300816178321838
translation,189,87,ablation-analysis,top three detectors,has,tsrt,top three detectors has tsrt,0.575113832950592
translation,189,87,ablation-analysis,unstable results,has,tsrt,unstable results has tsrt,0.6166043877601624
translation,189,87,ablation-analysis,ablation analysis,In,top three detectors,ablation analysis In top three detectors,0.5345422625541687
translation,189,73,hyperparameters,hyperparameters,with,recommended values,hyperparameters with recommended values,0.5719033479690552
translation,189,73,hyperparameters,hyperparameters,with,epoch,hyperparameters with epoch,0.6199615597724915
translation,189,73,hyperparameters,recommended values,from,bert,recommended values from bert,0.6121220588684082
translation,189,73,hyperparameters,maximum size,of,128,maximum size of 128,0.6126399040222168
translation,189,73,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,189,73,hyperparameters,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,189,73,hyperparameters,epoch,of,3,epoch of 3,0.6931780576705933
translation,189,73,hyperparameters,bert,has,maximum size,bert has maximum size,0.6365572214126587
translation,189,73,hyperparameters,hyperparameters,optimize,hyperparameters,hyperparameters optimize hyperparameters,0.6864498853683472
translation,189,37,model,novel translation detector,utilizes,text similarity,novel translation detector utilizes text similarity,0.557334840297699
translation,189,37,model,text similarity,with,round-trip translation,text similarity with round-trip translation,0.6455192565917969
translation,189,37,model,model,propose,novel translation detector,model propose novel translation detector,0.6384269595146179
translation,189,9,results,tsrt,achieves,86.9 % accuracy,tsrt achieves 86.9 % accuracy,0.6507461071014404
translation,189,9,results,86.9 % accuracy,in detecting,translated text,86.9 % accuracy in detecting translated text,0.6687712073326111
translation,189,9,results,translated text,from,strange translator,translated text from strange translator,0.5824775695800781
translation,189,9,results,results,has,tsrt,results has tsrt,0.5370588898658752
translation,189,10,results,outperforms,has,existing detectors,outperforms has existing detectors,0.5836828947067261
translation,189,10,results,existing detectors,has,77.9 % ),existing detectors has 77.9 % ),0.5198536515235901
translation,189,10,results,human recognition,has,53.3 % ),human recognition has 53.3 % ),0.563858151435852
translation,189,10,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,189,67,results,tsrt,provides,highest performance,tsrt provides highest performance,0.6698203086853027
translation,189,67,results,results,has,tsrt,results has tsrt,0.5370588898658752
translation,189,69,results,bert,surpasses,round trips,bert surpasses round trips,0.705883264541626
translation,189,69,results,round trips,in,only short length ranges,round trips in only short length ranges,0.4869008958339691
translation,189,69,results,others,in,all ranges,others in all ranges,0.5144826769828796
translation,189,69,results,tsrt,has,outperforms,tsrt has outperforms,0.6513783931732178
translation,189,69,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,189,69,results,results,has,bert,results has bert,0.43097156286239624
translation,189,86,results,decreased,for,all methods,decreased for all methods,0.6537618637084961
translation,190,24,ablation-analysis,round-trip translation,at,test time,round-trip translation at test time,0.5310289263725281
translation,190,24,ablation-analysis,round-trip translation,consistently reduces,fairness gap,round-trip translation consistently reduces fairness gap,0.7996982336044312
translation,190,24,ablation-analysis,round-trip translation,for,our best models,round-trip translation for our best models,0.5780915021896362
translation,190,24,ablation-analysis,our best models,both,training and test data,our best models both training and test data,0.6726540923118591
translation,190,24,ablation-analysis,svms,stacked on,bert representations,svms stacked on bert representations,0.7008928656578064
translation,190,24,ablation-analysis,disappears,when,training and test data,disappears when training and test data,0.6543428301811218
translation,190,24,ablation-analysis,disappears,both,training and test data,disappears both training and test data,0.712655246257782
translation,190,24,ablation-analysis,training and test data,translated into,foreign language and back,training and test data translated into foreign language and back,0.7183794975280762
translation,190,24,ablation-analysis,our best models,has,svms,our best models has svms,0.5638766884803772
translation,190,24,ablation-analysis,our best models,has,effect,our best models has effect,0.613236129283905
translation,190,24,ablation-analysis,effect,has,disappears,effect has disappears,0.6273092031478882
translation,190,24,ablation-analysis,ablation analysis,find that,round-trip translation,ablation analysis find that round-trip translation,0.6642451286315918
translation,190,47,ablation-analysis,significant decrease,in,kl - divergence,significant decrease in kl - divergence,0.5088932514190674
translation,190,47,ablation-analysis,kl - divergence,for,all groups,kl - divergence for all groups,0.6074047684669495
translation,190,47,ablation-analysis,all groups,after,round -trip translating,all groups after round -trip translating,0.7303268313407898
translation,190,49,ablation-analysis,number of unique words,dropped by,36 %,number of unique words dropped by 36 %,0.6814844012260437
translation,190,49,ablation-analysis,36 %,after,round -trip translation,36 % after round -trip translation,0.6829888224601746
translation,190,49,ablation-analysis,ablation analysis,see that,number of unique words,ablation analysis see that number of unique words,0.6057654619216919
translation,190,58,ablation-analysis,test time normalization,with,round trip translation,test time normalization with round trip translation,0.6487547159194946
translation,190,58,ablation-analysis,overall positive effect,on,cross-group generalization,overall positive effect on cross-group generalization,0.511343240737915
translation,190,58,ablation-analysis,fairness gap,with,up to ? 27 %,fairness gap with up to ? 27 %,0.6602929830551147
translation,190,58,ablation-analysis,test time normalization,has,overall positive effect,test time normalization has overall positive effect,0.5662668347358704
translation,190,58,ablation-analysis,round trip translation,has,overall positive effect,round trip translation has overall positive effect,0.5779203772544861
translation,190,58,ablation-analysis,ablation analysis,has,test time normalization,ablation analysis has test time normalization,0.528400182723999
translation,190,61,ablation-analysis,translating the data,reduces,overall accuracy,translating the data reduces overall accuracy,0.7089628577232361
translation,190,61,ablation-analysis,overall accuracy,of,our document classifiers,overall accuracy of our document classifiers,0.5305272340774536
translation,190,61,ablation-analysis,process of round-trip,has,translating the data,process of round-trip has translating the data,0.5429555177688599
translation,190,66,baselines,round -trip translation,to reduce,group disparity,round -trip translation to reduce group disparity,0.7106284499168396
translation,190,66,baselines,group disparity,of,sentiment classifiers,group disparity of sentiment classifiers,0.6175428032875061
translation,190,66,baselines,sentiment classifiers,for,danish,sentiment classifiers for danish,0.6394626498222351
translation,190,52,hyperparameters,two different pretrained language models,namely,"multilingual laser model ( artetxe and schwenk , 2019 )","two different pretrained language models namely multilingual laser model ( artetxe and schwenk , 2019 )",0.6330909729003906
translation,190,52,hyperparameters,two different pretrained language models,namely,"monolingual bert ( devlin et al. , 2019 )","two different pretrained language models namely monolingual bert ( devlin et al. , 2019 )",0.6247427463531494
translation,190,52,hyperparameters,"monolingual bert ( devlin et al. , 2019 )",trained for,danish,"monolingual bert ( devlin et al. , 2019 ) trained for danish",0.7341945767402649
translation,190,52,hyperparameters,hyperparameters,use,two different pretrained language models,hyperparameters use two different pretrained language models,0.5140702724456787
translation,190,52,hyperparameters,hyperparameters,trained for,danish,hyperparameters trained for danish,0.7159761786460876
translation,190,6,results,impact,of,round-trip translation,impact of round-trip translation,0.6108768582344055
translation,190,6,results,round-trip translation,on,demographic parity,round-trip translation on demographic parity,0.5237032771110535
translation,190,6,results,round-trip translation,show,round -trip translation,round-trip translation show round -trip translation,0.6608357429504395
translation,190,6,results,demographic parity,of,sentiment classifiers,demographic parity of sentiment classifiers,0.5852938890457153
translation,190,6,results,classification fairness,at,test time,classification fairness at test time,0.5175735354423523
translation,190,6,results,round -trip translation,has,consistently improves,round -trip translation has consistently improves,0.6124139428138733
translation,190,6,results,consistently improves,has,classification fairness,consistently improves has classification fairness,0.5590733885765076
translation,190,6,results,test time,has,reducing,test time has reducing,0.5751363039016724
translation,190,6,results,reducing,has,up to 47 %,reducing has up to 47 %,0.6123502254486084
translation,190,6,results,results,explore,impact,results explore impact,0.6217379570007324
translation,190,59,results,increases,up to,39 %,increases up to 39 %,0.6934587359428406
translation,190,59,results,increases,up to,47 %,increases up to 47 %,0.6752683520317078
translation,190,59,results,decreases,up to,47 %,decreases up to 47 %,0.6795217394828796
translation,190,77,results,2/4 classifiers,saw,improvements,2/4 classifiers saw improvements,0.6917025446891785
translation,190,77,results,improvements,for,majority groups,improvements for majority groups,0.615971028804779
translation,190,77,results,results,For,2/4 classifiers,results For 2/4 classifiers,0.5625653266906738
translation,191,136,results,baseline,has,outperforms,baseline has outperforms,0.619843065738678
translation,191,136,results,outperforms,has,"traditional mt evaluation metrics ( sentbleu , bleu )","outperforms has traditional mt evaluation metrics ( sentbleu , bleu )",0.5818004012107849
translation,191,140,results,worst results,achieved by,mteqa exact,worst results achieved by mteqa exact,0.6369005441665649
translation,191,140,results,mteqa exact,requires,exact match,mteqa exact requires exact match,0.6737575531005859
translation,191,140,results,results,has,worst results,results has worst results,0.5692225098609924
translation,191,183,results,our metric,performs,close,our metric performs close,0.6309970021247864
translation,191,183,results,close,to,state - of - the - art solutions,close to state - of - the - art solutions,0.5476665496826172
translation,191,183,results,state - of - the - art solutions,for,mqm labels,state - of - the - art solutions for mqm labels,0.5832636952400208
translation,191,183,results,noticeable drop,in,performance,noticeable drop in performance,0.570785641670227
translation,191,183,results,da labels,has,our metric,da labels has our metric,0.609967827796936
translation,191,183,results,results,for,da labels,results for da labels,0.6845227479934692
translation,192,26,baselines,baselines,empirically investigate,strong baselines,baselines empirically investigate strong baselines,0.7650752067565918
translation,192,64,baselines,parallel sentences,within,parallel document pair,parallel sentences within parallel document pair,0.6482537984848022
translation,192,64,baselines,strong neural mt engine google translate,to translate,each english source sentence,strong neural mt engine google translate to translate each english source sentence,0.6605955362319946
translation,192,64,baselines,each english source sentence,into,vietnamese,each english source sentence into vietnamese,0.5613126158714294
translation,192,23,model,high-quality and large-scale vietnamese - english parallel dataset,named,phomt,high-quality and large-scale vietnamese - english parallel dataset named phomt,0.6887942552566528
translation,192,23,model,high-quality and large-scale vietnamese - english parallel dataset,consists of,3.02 m sentence pairs,high-quality and large-scale vietnamese - english parallel dataset consists of 3.02 m sentence pairs,0.5993951559066772
translation,192,23,model,model,present,high-quality and large-scale vietnamese - english parallel dataset,model present high-quality and large-scale vietnamese - english parallel dataset,0.5620477199554443
translation,192,27,results,mbart,obtains,highest scores,mbart obtains highest scores,0.6439154744148254
translation,192,27,results,highest scores,in terms of,automatic and human evaluations,highest scores in terms of automatic and human evaluations,0.612390398979187
translation,192,27,results,automatic and human evaluations,on,both translation directions,automatic and human evaluations on both translation directions,0.4806478023529053
translation,192,27,results,results,find that,mbart,results find that mbart,0.6471192240715027
translation,192,93,results,mbart,achieves,best performance,mbart achieves best performance,0.7119218111038208
translation,192,93,results,best performance,among,all models,best performance among all models,0.607068657875061
translation,192,93,results,results,has,mbart,results has mbart,0.5544499754905701
translation,192,97,results,models,produce,lower bleu scores,models produce lower bleu scores,0.6245743036270142
translation,192,97,results,lower bleu scores,for,short - and medium -length sentences ( i.e. < 20 tokens ),lower bleu scores for short - and medium -length sentences ( i.e. < 20 tokens ),0.5405929088592529
translation,192,97,results,short - and medium -length sentences ( i.e. < 20 tokens ),than for,long sentences,short - and medium -length sentences ( i.e. < 20 tokens ) than for long sentences,0.5783005952835083
translation,192,97,results,results,find that,models,results find that models,0.6197972297668457
translation,192,108,results,transformer - base,trained using,sampled opus 's opensubtitles set,transformer - base trained using sampled opus 's opensubtitles set,0.7771912813186646
translation,192,108,results,transformer - base,produces,significantly lower,transformer - base produces significantly lower,0.6968026757240295
translation,192,108,results,significantly lower,on,our opensubtitles test subset,significantly lower on our opensubtitles test subset,0.5824213624000549
translation,192,108,results,vi-to - en bleu score,on,our opensubtitles test subset,vi-to - en bleu score on our opensubtitles test subset,0.5031382441520691
translation,192,108,results,significantly lower,has,vi-to - en bleu score,significantly lower has vi-to - en bleu score,0.5662091970443726
translation,192,108,results,our opensubtitles training subset,has,29.72 vs. 31.11 ),our opensubtitles training subset has 29.72 vs. 31.11 ),0.5780295133590698
translation,192,108,results,results,find that,transformer - base,results find that transformer - base,0.6588669419288635
translation,192,109,results,transformer - base,trained using,whole phomt 's training set,transformer - base trained using whole phomt 's training set,0.7697233557701111
translation,192,109,results,higher vi-to - en bleu score,at,32.29,higher vi-to - en bleu score at 32.29,0.5048959851264954
translation,192,109,results,32.29,on,our opensubtitles test subset,32.29 on our opensubtitles test subset,0.5521238446235657
translation,192,119,results,mbart,gains,highest human evaluation scores,mbart gains highest human evaluation scores,0.7086672782897949
translation,192,119,results,highest human evaluation scores,demonstrating,qualitative effectiveness,highest human evaluation scores demonstrating qualitative effectiveness,0.6545076966285706
translation,192,119,results,qualitative effectiveness,for,en-to - vi and vi-to - en translation,qualitative effectiveness for en-to - vi and vi-to - en translation,0.6291760206222534
translation,193,96,ablation-analysis,wait -k,adapted on,"source - fb , raw","wait -k adapted on source - fb , raw",0.6554221510887146
translation,193,96,ablation-analysis,wait -k,has,lowest delay,wait -k has lowest delay,0.6369566321372986
translation,193,96,ablation-analysis,"source - fb , raw",has,lowest delay,"source - fb , raw has lowest delay",0.5915980935096741
translation,193,82,baselines,wait -k baselines,trained on,europarl,wait -k baselines trained on europarl,0.7423299551010132
translation,193,82,baselines,wait -k baselines,adapted on,raw,wait -k baselines adapted on raw,0.6527570486068726
translation,193,82,baselines,baselines,compared,wait -k baselines,baselines compared wait -k baselines,0.7653606534004211
translation,193,22,model,translation - tointerpretation ( t2i ) style transfer method,to produce,pseudo-interpretations,translation - tointerpretation ( t2i ) style transfer method to produce pseudo-interpretations,0.669327974319458
translation,193,22,model,pseudo-interpretations,from,abundant offline translations,pseudo-interpretations from abundant offline translations,0.5782422423362732
translation,193,22,model,simple and effective,has,translation - tointerpretation ( t2i ) style transfer method,simple and effective has translation - tointerpretation ( t2i ) style transfer method,0.5154653191566467
translation,193,103,results,our approach,yields,significantly better results,our approach yields significantly better results,0.6919941902160645
translation,193,103,results,significantly better results,on,interpretation asr,significantly better results on interpretation asr,0.5649019479751587
translation,193,103,results,significantly better results,compared to,baselines,significantly better results compared to baselines,0.6907857656478882
translation,193,103,results,results,has,our approach,results has our approach,0.6050099730491638
translation,193,104,results,pre-trained wait -k,by,2.79 bleu score,pre-trained wait -k by 2.79 bleu score,0.5166854858398438
translation,193,104,results,best model,has,outperformed,best model has outperformed,0.6278076171875
translation,193,104,results,outperformed,has,pre-trained wait -k,outperformed has pre-trained wait -k,0.6200559139251709
translation,193,104,results,results,has,best model,results has best model,0.5634682774543762
translation,193,107,results,all t2i models,work,consistently well,all t2i models work consistently well,0.6080825328826904
translation,193,107,results,consistently well,in,supervised and unsupervised settings,consistently well in supervised and unsupervised settings,0.5541911721229553
translation,193,107,results,results,has,all t2i models,results has all t2i models,0.438101589679718
translation,193,108,results,approach,surpasses,seq2seq,approach surpasses seq2seq,0.6182776093482971
translation,193,108,results,seq2seq,by,6.47 points,seq2seq by 6.47 points,0.5562034845352173
translation,193,108,results,6.47 points,on,interpretation asr,6.47 points on interpretation asr,0.565065324306488
translation,193,108,results,results,has,approach,results has approach,0.5518386363983154
translation,194,146,ablation-analysis,translation quality,of,12 - 1 model,translation quality of 12 - 1 model,0.6112061142921448
translation,194,146,ablation-analysis,translation quality,is,not significantly improved,translation quality is not significantly improved,0.5712162852287292
translation,194,146,ablation-analysis,not significantly improved,compared with,9 - 1 model,not significantly improved compared with 9 - 1 model,0.7160604596138
translation,194,146,ablation-analysis,decreases,by,about 25 %,decreases by about 25 %,0.6842700242996216
translation,194,146,ablation-analysis,our small model setting,has,translation quality,our small model setting has translation quality,0.5523324012756348
translation,194,146,ablation-analysis,inference speed,has,decreases,inference speed has decreases,0.5788120031356812
translation,194,146,ablation-analysis,ablation analysis,In,our small model setting,ablation analysis In our small model setting,0.5778235197067261
translation,194,5,baselines,sentence - level teacher -student distillation technique,train,several small - size models,sentence - level teacher -student distillation technique train several small - size models,0.6355103254318237
translation,194,5,baselines,several small - size models,find,balance,several small - size models find balance,0.610784113407135
translation,194,5,baselines,balance,between,efficiency and quality,balance between efficiency and quality,0.6588117480278015
translation,194,17,experimental-setup,"knowledge distillation ( hinton et al. , 2015 )",to train,small models,"knowledge distillation ( hinton et al. , 2015 ) to train small models",0.7111324667930603
translation,194,58,experimental-setup,vocabulary size,set to,"25,000 tokens","vocabulary size set to 25,000 tokens",0.6683485507965088
translation,194,58,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,194,59,experimental-setup,sentencepiece regularization,during,data processing,sentencepiece regularization during data processing,0.6900156736373901
translation,194,59,experimental-setup,experimental setup,employ,sentencepiece regularization,experimental setup employ sentencepiece regularization,0.5239444971084595
translation,194,73,experimental-setup,sampling size,set to,64,sampling size set to 64,0.7104104161262512
translation,194,73,experimental-setup,smoothing parameter,to,0.1,smoothing parameter to 0.1,0.533808171749115
translation,194,73,experimental-setup,0.1,for,subword regularization,0.1 for subword regularization,0.5748982429504395
translation,194,73,experimental-setup,experimental setup,has,sampling size,experimental setup has sampling size,0.5369634628295898
translation,194,74,experimental-setup,8 nvidia tesla v100,for,two days,8 nvidia tesla v100 for two days,0.634060800075531
translation,194,74,experimental-setup,8 nvidia tesla v100,with,batch size,8 nvidia tesla v100 with batch size,0.6269604563713074
translation,194,74,experimental-setup,batch size,of,4096,batch size of 4096,0.6386147141456604
translation,194,74,experimental-setup,experimental setup,trained using,8 nvidia tesla v100,experimental setup trained using 8 nvidia tesla v100,0.6942494511604309
translation,194,89,experimental-setup,cpu optimization,use,bolt v1.3.0,cpu optimization use bolt v1.3.0,0.615231454372406
translation,194,89,experimental-setup,experimental setup,has,inference optimizations,experimental setup has inference optimizations,0.5224403738975525
translation,194,110,experimental-setup,bolt,uses,avx512 vnni instructions,bolt uses avx512 vnni instructions,0.6917470097541809
translation,194,110,experimental-setup,avx512 vnni instructions,to perform,u8s8s32 matrix multiplication,avx512 vnni instructions to perform u8s8s32 matrix multiplication,0.6769701242446899
translation,194,110,experimental-setup,experimental setup,has,bolt,experimental setup has bolt,0.5474623441696167
translation,194,7,experiments,efficient and light - weight library,for,on -device inference,efficient and light - weight library for on -device inference,0.5654325485229492
translation,194,7,experiments,huawei noah 's bolt,has,efficient and light - weight library,huawei noah 's bolt has efficient and light - weight library,0.5830625295639038
translation,194,8,experiments,int8 quantization,submit,four smallsize and efficient translation models,int8 quantization submit four smallsize and efficient translation models,0.5061379075050354
translation,194,8,experiments,four smallsize and efficient translation models,with,high translation quality,four smallsize and efficient translation models with high translation quality,0.5685673952102661
translation,194,8,experiments,high translation quality,for,one cpu core latency track,high translation quality for one cpu core latency track,0.5891740322113037
translation,194,8,experiments,int8 quantization,has,self-defined general matrix multiplication ( gemm ) operator,int8 quantization has self-defined general matrix multiplication ( gemm ) operator,0.5908920168876648
translation,194,16,experiments,four models,of,different sizes,four models of different sizes,0.6177746653556824
translation,194,23,experiments,huawei noah 's bolt,as,inference library,huawei noah 's bolt as inference library,0.581670880317688
translation,194,25,experiments,cpu task,realize,int8 quantization inference,cpu task realize int8 quantization inference,0.7288686037063599
translation,194,25,experiments,cpu task,realize,efficient gemm operator,cpu task realize efficient gemm operator,0.7578281760215759
translation,194,25,experiments,efficient gemm operator,faster than,intel onednn,efficient gemm operator faster than intel onednn,0.8185110092163086
translation,194,71,experiments,distillation experiments,based on,fairseq,distillation experiments based on fairseq,0.6472482681274414
translation,194,71,experiments,traning,has,distillation experiments,traning has distillation experiments,0.565407931804657
translation,194,81,experiments,12 - 1 base configuration model,as,baseline model,12 - 1 base configuration model as baseline model,0.5116832852363586
translation,194,81,experiments,12 - 1 base configuration model,achieves,38.02 bleu,12 - 1 base configuration model achieves 38.02 bleu,0.6661438941955566
translation,194,81,experiments,38.02 bleu,on,2020 test set,38.02 bleu on 2020 test set,0.48671552538871765
translation,194,81,experiments,lower,than,our teacher model,lower than our teacher model,0.6071124076843262
translation,194,81,experiments,1.7 bleu,has,lower,1.7 bleu has lower,0.5672248005867004
translation,194,96,experiments,bolt,supports,most of the nlp and cv models,bolt supports most of the nlp and cv models,0.7009820342063904
translation,194,96,experiments,most of the nlp and cv models,on,x86 and arm cpu,most of the nlp and cv models on x86 and arm cpu,0.5121244192123413
translation,194,96,experiments,inference,on,x86 and arm cpu,inference on x86 and arm cpu,0.5351553559303284
translation,194,96,experiments,x86 and arm cpu,as well as,mali gpu,x86 and arm cpu as well as mali gpu,0.5948288440704346
translation,194,96,experiments,most of the nlp and cv models,has,inference,most of the nlp and cv models has inference,0.5464962720870972
translation,194,154,experiments,inference speed,performed with,intel ( r ) xeon ( r ) gold 6278c cpu @ 2.60 ghz,inference speed performed with intel ( r ) xeon ( r ) gold 6278c cpu @ 2.60 ghz,0.5067093968391418
translation,194,6,model,light - weight rnn,with,ssru layer,light - weight rnn with ssru layer,0.6089973449707031
translation,194,6,model,model,feature,deep encoder,model feature deep encoder,0.7730111479759216
translation,194,6,model,model,feature,shallow decoder,model feature shallow decoder,0.7676874399185181
translation,194,6,model,model,feature,light - weight rnn,model feature light - weight rnn,0.7428310513496399
translation,194,57,model,joint subword segmentation model,from,synthesized parallel data,joint subword segmentation model from synthesized parallel data,0.5489459037780762
translation,194,57,model,synthesized parallel data,using,sentencepiece,synthesized parallel data using sentencepiece,0.7232120037078857
translation,194,57,model,model,build,joint subword segmentation model,model build joint subword segmentation model,0.6274355053901672
translation,194,60,model,sentencepiece,into,training code,sentencepiece into training code,0.5903162360191345
translation,194,60,model,subword segmentation,on,source side,subword segmentation on source side,0.5503748059272766
translation,194,60,model,subword segmentation,via,sampling,subword segmentation via sampling,0.6558851003646851
translation,194,60,model,model,integrate,sentencepiece,model integrate sentencepiece,0.699520468711853
translation,194,72,model,sentencepiece,into,training,sentencepiece into training,0.5941413640975952
translation,194,72,model,model,integrate,sentencepiece,model integrate sentencepiece,0.699520468711853
translation,194,95,model,bolt,perform,fast inference,bolt perform fast inference,0.585345983505249
translation,194,95,model,fast inference,without,third - party dependencies,fast inference without third - party dependencies,0.66299968957901
translation,194,95,model,bolt,has,standalone c ++ runtime,bolt has standalone c ++ runtime,0.5364010334014893
translation,194,95,model,model,has,bolt,model has bolt,0.6136506199836731
translation,194,97,model,assembly - level optimizations,to ensure,computing performance and memory accessing efficiency,assembly - level optimizations to ensure computing performance and memory accessing efficiency,0.6475553512573242
translation,194,97,model,model,apply,assembly - level optimizations,model apply assembly - level optimizations,0.661331295967102
translation,194,38,results,final ensembled model,gained,39.7 bleu,final ensembled model gained 39.7 bleu,0.553247332572937
translation,194,38,results,39.7 bleu,on,wmt 2020 test set,39.7 bleu on wmt 2020 test set,0.5034692287445068
translation,194,38,results,results,has,final ensembled model,results has final ensembled model,0.5638493299484253
translation,194,65,results,ssru models cam,satisfies,translation quality requirements,ssru models cam satisfies translation quality requirements,0.6549075841903687
translation,194,65,results,teacherstudent distillation setting,has,ssru models cam,teacherstudent distillation setting has ssru models cam,0.593113124370575
translation,194,151,results,speed,of,our tiny model,speed of our tiny model,0.6029118299484253
translation,194,151,results,our tiny model,is,30 % faster,our tiny model is 30 % faster,0.5501214265823364
translation,194,151,results,30 % faster,than,our tiny .2 model,30 % faster than our tiny .2 model,0.5549509525299072
translation,194,151,results,our tiny .2 model,by dropping,decoder layer,our tiny .2 model by dropping decoder layer,0.6663632988929749
translation,194,151,results,results,has,speed,results has speed,0.5244524478912354
translation,195,114,experiments,most successful teams,were,nrc - cnrc,most successful teams were nrc - cnrc,0.6339911818504333
translation,195,114,experiments,most successful teams,were,noahnmt,most successful teams were noahnmt,0.6405425071716309
translation,195,114,experiments,nrc - cnrc,best or on par with,best systems,nrc - cnrc best or on par with best systems,0.7557536363601685
translation,195,114,experiments,best systems,in,all sorbian tasks,best systems in all sorbian tasks,0.48672881722450256
translation,195,114,experiments,best systems,in,german - upper sorbian translation,best systems in german - upper sorbian translation,0.4993651211261749
translation,195,114,experiments,best systems,in,german - upper sorbian translation,best systems in german - upper sorbian translation,0.4993651211261749
translation,195,114,experiments,noahnmt,on par with,best systems,noahnmt on par with best systems,0.6874608397483826
translation,195,114,experiments,noahnmt,on par with,second,noahnmt on par with second,0.7288613319396973
translation,195,114,experiments,best,in,chuvash tasks,best in chuvash tasks,0.5560833215713501
translation,195,114,experiments,best systems,in,german - upper sorbian translation,best systems in german - upper sorbian translation,0.4993651211261749
translation,195,114,experiments,second,in,upper sorbian - german direction,second in upper sorbian - german direction,0.513516366481781
translation,195,115,experiments,german- upper sorbian translation,reach,very similar results,german- upper sorbian translation reach very similar results,0.6620405912399292
translation,195,115,experiments,best two system,reach,very similar results,best two system reach very similar results,0.6831607818603516
translation,195,115,experiments,german- upper sorbian translation,has,best two system,german- upper sorbian translation has best two system,0.5720546841621399
translation,195,115,experiments,best two system,has,nrc - cnrc and noahnmt,best two system has nrc - cnrc and noahnmt,0.5786465406417847
translation,195,119,experiments,cl_rug,ranked on par with,nrc - cnrc,cl_rug ranked on par with nrc - cnrc,0.7392711639404297
translation,195,119,experiments,nrc - cnrc,in,translation,nrc - cnrc in translation,0.5902684926986694
translation,195,119,experiments,translation,into,german,translation into german,0.6343783140182495
translation,195,119,experiments,unsupervised german-lower sorbian task,has,cl_rug,unsupervised german-lower sorbian task has cl_rug,0.6099874377250671
translation,195,121,experiments,outperformed,using,larger datasets,outperformed using larger datasets,0.6872965097427368
translation,195,121,experiments,outperformed,using,more advanced transfer learning technique,outperformed using more advanced transfer learning technique,0.7340285181999207
translation,195,121,experiments,munich,using,larger datasets,munich using larger datasets,0.6914019584655762
translation,195,121,experiments,russian - chuvash translation,has,noah,russian - chuvash translation has noah,0.6052201986312866
translation,195,121,experiments,outperformed,has,munich,outperformed has munich,0.6642288565635681
translation,195,117,results,lmu,used,data,lmu used data,0.7192913889884949
translation,195,117,results,lmu,not use,most of the further tricks,lmu not use most of the further tricks,0.7287858724594116
translation,195,117,results,data,of,similar size,data of similar size,0.6420566439628601
translation,195,117,results,similar size,to,nrc - cnrc,similar size to nrc - cnrc,0.6127818822860718
translation,195,117,results,results,has,lmu,results has lmu,0.5861995220184326
translation,196,99,ablation-analysis,ensemble of multiple models of different settings,further improve,performance,ensemble of multiple models of different settings further improve performance,0.67647385597229
translation,196,99,ablation-analysis,performance,of,qemind systems,performance of qemind systems,0.6359131932258606
translation,196,99,ablation-analysis,ablation analysis,has,ensemble of multiple models of different settings,ablation analysis has ensemble of multiple models of different settings,0.5379114151000977
translation,196,88,experimental-setup,model settings,of,"transquest ( ranasinghe et al. , 2020 )","model settings of transquest ( ranasinghe et al. , 2020 )",0.5574475526809692
translation,196,88,experimental-setup,"transquest ( ranasinghe et al. , 2020 )",to fine- tune,qe model,"transquest ( ranasinghe et al. , 2020 ) to fine- tune qe model",0.7022684812545776
translation,196,88,experimental-setup,qe model,based on,xlm - roberta large model,qe model based on xlm - roberta large model,0.6563543677330017
translation,196,88,experimental-setup,qe model,with,classification / regression head,qe model with classification / regression head,0.6693096160888672
translation,196,88,experimental-setup,xlm - roberta large model,with,classification / regression head,xlm - roberta large model with classification / regression head,0.6525022983551025
translation,196,88,experimental-setup,classification / regression head,on,single p100 gpu,classification / regression head on single p100 gpu,0.5679948329925537
translation,196,88,experimental-setup,experimental setup,follow,model settings,experimental setup follow model settings,0.6044647097587585
translation,196,89,experimental-setup,training batch size,set to,8,training batch size set to 8,0.7177265286445618
translation,196,89,experimental-setup,training process,takes,about 2 hours,training process takes about 2 hours,0.6184633374214172
translation,196,89,experimental-setup,about 2 hours,to,convergence,about 2 hours to convergence,0.5956713557243347
translation,196,89,experimental-setup,experimental setup,has,training batch size,experimental setup has training batch size,0.5232828259468079
translation,196,6,experiments,several useful features,to evaluate,uncertainty,several useful features to evaluate uncertainty,0.7271479368209839
translation,196,6,experiments,uncertainty,of,translations,uncertainty of translations,0.6471072435379028
translation,196,6,experiments,uncertainty,named,qemind,uncertainty named qemind,0.7198694944381714
translation,196,6,experiments,translations,to build,qe system,translations to build qe system,0.7119284272193909
translation,196,6,experiments,qe system,named,qemind,qe system named qemind,0.7204645872116089
translation,196,22,model,qe systems,incorporating,all the features,qe systems incorporating all the features,0.7020344138145447
translation,196,22,model,all the features,potentially evaluate,uncertainty,all the features potentially evaluate uncertainty,0.7140277028083801
translation,196,22,model,uncertainty,of,machine translations,uncertainty of machine translations,0.5783326029777527
translation,196,22,model,supervised qe model,based on,transfer learning,supervised qe model based on transfer learning,0.6457915902137756
translation,196,22,model,transfer learning,from,xlm - roberta,transfer learning from xlm - roberta,0.6302367448806763
translation,196,22,model,model,develop,qe systems,model develop qe systems,0.6680428981781006
translation,196,76,model,two data augmentation strategies,applied for,ced task,two data augmentation strategies applied for ced task,0.7014685869216919
translation,196,76,model,model,has,two data augmentation strategies,model has two data augmentation strategies,0.5722300410270691
translation,196,29,results,"two classes , not and err",are,extremely unbalanced,"two classes , not and err are extremely unbalanced",0.6231090426445007
translation,196,29,results,extremely unbalanced,for,all four language pairs,extremely unbalanced for all four language pairs,0.6350142955780029
translation,196,29,results,results,For,ced task,results For ced task,0.6045730710029602
translation,196,98,results,da test sets,of,wmt 2020,da test sets of wmt 2020,0.6190866231918335
translation,196,98,results,da test sets,show,multilingual strategies,da test sets show multilingual strategies,0.6263188123703003
translation,196,98,results,work well,especially for,high- resource language pairs,work well especially for high- resource language pairs,0.6267220377922058
translation,196,98,results,uncertainty features enhanced multilingual model,achieves,highest performance,uncertainty features enhanced multilingual model achieves highest performance,0.6756556630134583
translation,196,98,results,highest performance,among,all single models,highest performance among all single models,0.5930585265159607
translation,196,98,results,multilingual strategies,has,work well,multilingual strategies has work well,0.600849986076355
translation,196,98,results,results,on,da test sets,results on da test sets,0.5431748032569885
translation,196,100,results,best single and ensemble models,for,each language pair,best single and ensemble models for each language pair,0.5257253050804138
translation,196,100,results,best single and ensemble models,produce,predictions,best single and ensemble models produce predictions,0.6295405030250549
translation,196,100,results,predictions,on,newly released blind test sets,predictions on newly released blind test sets,0.54133141040802
translation,196,100,results,newly released blind test sets,of,wmt 2021,newly released blind test sets of wmt 2021,0.5831069946289062
translation,196,100,results,newly released blind test sets,including,4 zero-shot language pairs,newly released blind test sets including 4 zero-shot language pairs,0.6515886187553406
translation,196,100,results,wmt 2021,including,4 zero-shot language pairs,wmt 2021 including 4 zero-shot language pairs,0.6609764099121094
translation,196,100,results,results,pick,best single and ensemble models,results pick best single and ensemble models,0.6826217174530029
translation,196,106,results,strategies,of,uncertainty features ( qemind + unc ),strategies of uncertainty features ( qemind + unc ),0.5902689099311829
translation,196,106,results,strategies,back - translation ( QEMind + BK ) and,uncertainty features ( qemind + unc ),strategies back - translation ( QEMind + BK ) and uncertainty features ( qemind + unc ),0.8548333644866943
translation,196,106,results,strategies,achieve,comparable or better performances,strategies achieve comparable or better performances,0.5898562669754028
translation,196,106,results,qe - mind,has,strategies,qe - mind has strategies,0.5916189551353455
translation,196,106,results,results,compared to,qe - mind,results compared to qe - mind,0.6489133238792419
translation,196,107,results,ensemble of all these models,makes,significant improvement,ensemble of all these models makes significant improvement,0.5775328874588013
translation,196,107,results,results,has,ensemble of all these models,results has ensemble of all these models,0.5489926934242249
translation,197,143,ablation-analysis,model capacity,level of,+ adapter,model capacity level of + adapter,0.42804083228111267
translation,197,143,ablation-analysis,our approach,can achieve,better translation performance,our approach can achieve better translation performance,0.6492510437965393
translation,197,143,ablation-analysis,model capacity,has,our approach,model capacity has our approach,0.60971599817276
translation,197,143,ablation-analysis,+ adapter,has,our approach,+ adapter has our approach,0.6200319528579712
translation,197,143,ablation-analysis,ablation analysis,expand,model capacity,ablation analysis expand model capacity,0.6762970685958862
translation,197,182,ablation-analysis,general neurons,are,erased,general neurons are erased,0.5958759784698486
translation,197,182,ablation-analysis,bleu points,of,all the language pairs,bleu points of all the language pairs,0.5677019357681274
translation,197,182,ablation-analysis,general neurons,has,bleu points,general neurons has bleu points,0.5746886730194092
translation,197,182,ablation-analysis,all the language pairs,has,drop a lot,all the language pairs has drop a lot,0.6261359453201294
translation,197,182,ablation-analysis,drop a lot,has,about 15 to 20 bleu,drop a lot has about 15 to 20 bleu,0.5797080993652344
translation,197,110,baselines,ts,has,"blackwood et al. , 2018 )","ts has blackwood et al. , 2018 )",0.5777498483657837
translation,197,110,baselines,baselines,has,ts,baselines has ts,0.6266400218009949
translation,197,123,baselines,other contrast methods,on,advanced transformer model,other contrast methods on advanced transformer model,0.48555225133895874
translation,197,123,baselines,open-source toolkit fairseq - py,has,"ott et al. , 2019 )","open-source toolkit fairseq - py has ott et al. , 2019 )",0.5033074021339417
translation,197,124,experimental-setup,configurations,of,nmt model,configurations of nmt model,0.6259915232658386
translation,197,124,experimental-setup,6 stacked encoder / decoder layers,with,layer size,6 stacked encoder / decoder layers with layer size,0.6055996417999268
translation,197,124,experimental-setup,layer size,being,512,layer size being 512,0.5901334285736084
translation,197,125,experimental-setup,4 nvidia 2080 ti gpus,allocated with,batch size,4 nvidia 2080 ti gpus allocated with batch size,0.6955808401107788
translation,197,125,experimental-setup,batch size,of,"4,096 tokens","batch size of 4,096 tokens",0.5991408228874207
translation,197,125,experimental-setup,batch size,of,"2,048 tokens","batch size of 2,048 tokens",0.5985899567604065
translation,197,125,experimental-setup,"4,096 tokens",for,one - to - many scenario,"4,096 tokens for one - to - many scenario",0.6374122500419617
translation,197,125,experimental-setup,"2,048 tokens",for,many - to -many scenario,"2,048 tokens for many - to -many scenario",0.6334432363510132
translation,197,125,experimental-setup,experimental setup,trained on,4 nvidia 2080 ti gpus,experimental setup trained on 4 nvidia 2080 ti gpus,0.6837136745452881
translation,197,126,experimental-setup,baseline model,using,"adam optimizer ( kingma and ba , 2015 )","baseline model using adam optimizer ( kingma and ba , 2015 )",0.6122567057609558
translation,197,126,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,"? 1 = 0.9 , ? 2 = 0.98 , and = 10 ?9","adam optimizer ( kingma and ba , 2015 ) with ? 1 = 0.9 , ? 2 = 0.98 , and = 10 ?9",0.6136887073516846
translation,197,126,experimental-setup,experimental setup,train,baseline model,experimental setup train baseline model,0.6665930151939392
translation,197,128,experimental-setup,hyperparameter,controls,proportion of general neurons,hyperparameter controls proportion of general neurons,0.6743280291557312
translation,197,128,experimental-setup,proportion of general neurons,in,each module,proportion of general neurons in each module,0.47690680623054504
translation,197,128,experimental-setup,proportion of general neurons,from,80 % to 95 %,proportion of general neurons from 80 % to 95 %,0.4931158423423767
translation,197,128,experimental-setup,experimental setup,vary,hyperparameter,experimental setup vary hyperparameter,0.6626729965209961
translation,197,131,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,197,131,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,197,131,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,197,131,experimental-setup,length penalty,? =,0.6,length penalty ? = 0.6,0.6168525218963623
translation,197,127,model,proposed models,further trained with,corresponding parameters,proposed models further trained with corresponding parameters,0.6740890145301819
translation,197,127,model,corresponding parameters,initialized by,pre-trained baseline model,corresponding parameters initialized by pre-trained baseline model,0.6967630982398987
translation,197,127,model,model,has,proposed models,model has proposed models,0.5728424191474915
translation,197,136,results,improvements,brought by,+ ts and + adapter methods,improvements brought by + ts and + adapter methods,0.7024005651473999
translation,197,136,results,+ ts and + adapter methods,are,not large,+ ts and + adapter methods are not large,0.5857099890708923
translation,197,136,results,results,see that,improvements,results see that improvements,0.7084582448005676
translation,197,139,results,our method,based on,taylor expansion,our method based on taylor expansion,0.6586768627166748
translation,197,139,results,outperforms,in,datasets,outperforms in datasets,0.5458177924156189
translation,197,139,results,all the baselines,in,datasets,all the baselines in datasets,0.5052388906478882
translation,197,139,results,all language pairs,has,our method,all language pairs has our method,0.5517144203186035
translation,197,139,results,taylor expansion,has,outperforms,taylor expansion has outperforms,0.6070453524589539
translation,197,139,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,197,139,results,results,In,all language pairs,results In all language pairs,0.4663567543029785
translation,197,142,results,multilingual baseline,in,all language pairs,multilingual baseline in all language pairs,0.5113464593887329
translation,197,142,results,outperforms,without,capacity increment,outperforms without capacity increment,0.7898182272911072
translation,197,142,results,other baselines,in,most language pairs,other baselines in most language pairs,0.4630850553512573
translation,197,142,results,other baselines,without,capacity increment,other baselines without capacity increment,0.7170628309249878
translation,197,142,results,our method,has,exceeds,our method has exceeds,0.6335184574127197
translation,197,142,results,exceeds,has,multilingual baseline,exceeds has multilingual baseline,0.6234189867973328
translation,197,142,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,197,144,results,results,of,individual baseline,results of individual baseline,0.5252575278282166
translation,197,144,results,individual baseline,are,worse,individual baseline are worse,0.615614652633667
translation,197,144,results,worse,than,other baselines,worse than other baselines,0.5912690162658691
translation,197,144,results,results,of,individual baseline,results of individual baseline,0.5252575278282166
translation,197,175,results,translation performance,of,two proposed approaches,translation performance of two proposed approaches,0.5513447523117065
translation,197,175,results,translation performance,increases with,increment of k,translation performance increases with increment of k,0.7431022524833679
translation,197,175,results,translation performance,reach,best performance,translation performance reach best performance,0.7085144519805908
translation,197,175,results,best performance,when,k equals 0.7,best performance when k equals 0.7,0.6655903458595276
translation,198,97,ablation-analysis,ffn layer,of,mhplstm - based decoder,ffn layer of mhplstm - based decoder,0.5617421865463257
translation,198,97,ablation-analysis,ffn layer,lead to,further acceleration,ffn layer lead to further acceleration,0.6793033480644226
translation,198,97,ablation-analysis,further acceleration,while performing,competitively,further acceleration while performing competitively,0.7116157412528992
translation,198,97,ablation-analysis,competitively,with,transformer baseline,competitively with transformer baseline,0.6832584142684937
translation,198,97,ablation-analysis,competitively,with,fewer parameters,competitively with fewer parameters,0.670418918132782
translation,198,97,ablation-analysis,ablation analysis,removing,ffn layer,ablation analysis removing ffn layer,0.729507565498352
translation,198,101,ablation-analysis,2 - layer neural network,for,computation,2 - layer neural network for computation,0.5994089841842651
translation,198,101,ablation-analysis,2 - layer neural network,for,gate computation,2 - layer neural network for gate computation,0.6058388352394104
translation,198,101,ablation-analysis,2 - layer neural network,for,gate computation,2 - layer neural network for gate computation,0.6058388352394104
translation,198,101,ablation-analysis,computation,of,hidden states,computation of hidden states,0.5926861763000488
translation,198,101,ablation-analysis,important,for,performance,important for performance,0.6339983344078064
translation,198,101,ablation-analysis,impact,of using,2 - layer neural network,impact of using 2 - layer neural network,0.7341569066047668
translation,198,101,ablation-analysis,2 - layer neural network,for,gate computation,2 - layer neural network for gate computation,0.6058388352394104
translation,198,101,ablation-analysis,gate computation,is,neglectable,gate computation is neglectable,0.5820468664169312
translation,198,101,ablation-analysis,ablation analysis,shows,2 - layer neural network,ablation analysis shows 2 - layer neural network,0.624413013458252
translation,198,101,ablation-analysis,ablation analysis,using,2 - layer neural network,ablation analysis using 2 - layer neural network,0.6738505363464355
translation,198,82,baselines,additionsubtraction twin-gated recurrent ( atr ) network,on,wmt 14 en- de task,additionsubtraction twin-gated recurrent ( atr ) network on wmt 14 en- de task,0.5540799498558044
translation,198,14,experiments,number of lstm parameters,compute,several small hplstms,number of lstm parameters compute several small hplstms,0.7237863540649414
translation,198,14,experiments,several small hplstms,in,parallel,several small hplstms in parallel,0.6072731018066406
translation,198,14,experiments,parallel,like,multi-head attention,parallel like multi-head attention,0.5654613375663757
translation,198,63,hyperparameters,joint byte-pair encoding ( bpe ),with,32 k merging operations,joint byte-pair encoding ( bpe ) with 32 k merging operations,0.624405562877655
translation,198,63,hyperparameters,32 k merging operations,on,all data sets,32 k merging operations on all data sets,0.5204475522041321
translation,198,63,hyperparameters,hyperparameters,applied,joint byte-pair encoding ( bpe ),hyperparameters applied joint byte-pair encoding ( bpe ),0.6767284870147705
translation,198,67,hyperparameters,training steps,for,transformer base and transformer big,training steps for transformer base and transformer big,0.6664333343505859
translation,198,67,hyperparameters,transformer base and transformer big,were,100k and 300k,transformer base and transformer big were 100k and 300k,0.6269716620445251
translation,198,67,hyperparameters,hyperparameters,has,training steps,hyperparameters has training steps,0.5018224120140076
translation,198,69,hyperparameters,embedding dimension and the hidden dimension,of,position - wise feed -forward neural network,embedding dimension and the hidden dimension of position - wise feed -forward neural network,0.5648639798164368
translation,198,69,hyperparameters,position - wise feed -forward neural network,were,512 and 2048,position - wise feed -forward neural network were 512 and 2048,0.6341727375984192
translation,198,69,hyperparameters,corresponding values,for,transformer big model bleu para,corresponding values for transformer big model bleu para,0.6662035584449768
translation,198,69,hyperparameters,transformer base setting,has,embedding dimension and the hidden dimension,transformer base setting has embedding dimension and the hidden dimension,0.5798230171203613
translation,198,69,hyperparameters,transformer base setting,has,corresponding values,transformer base setting has corresponding values,0.5752823948860168
translation,198,69,hyperparameters,hyperparameters,For,transformer base setting,hyperparameters For transformer base setting,0.5624101161956787
translation,198,10,model,sequence-level parallelization,of,lstms,sequence-level parallelization of lstms,0.5818642377853394
translation,198,10,model,sequence-level parallelization,approximate,full lstm context modelling,sequence-level parallelization approximate full lstm context modelling,0.7550381422042847
translation,198,10,model,full lstm context modelling,by computing,hidden states and gates,full lstm context modelling by computing hidden states and gates,0.6756221652030945
translation,198,10,model,hidden states and gates,with,current input,hidden states and gates with current input,0.6413230299949646
translation,198,10,model,simple bag-of-words representation,of,preceding tokens context,simple bag-of-words representation of preceding tokens context,0.5062688589096069
translation,198,10,model,model,To enable,sequence-level parallelization,model To enable sequence-level parallelization,0.6755410432815552
translation,198,11,model,each input step,in,parallel,each input step in parallel,0.5541427135467529
translation,198,11,model,each input step,avoiding,costly sequential linear transformations,each input step avoiding costly sequential linear transformations,0.7465853691101074
translation,198,11,model,efficiently,in,parallel,efficiently in parallel,0.5527854561805725
translation,198,11,model,each input step,has,efficiently,each input step has efficiently,0.58558189868927
translation,198,11,model,model,compute,each input step,model compute each input step,0.755620539188385
translation,198,12,model,outputs,of,each parallel step,outputs of each parallel step,0.6249525547027588
translation,198,12,model,model,connect,outputs,model connect outputs,0.7016196846961975
translation,198,19,model,transformer,trained,efficiently,transformer trained efficiently,0.776294469833374
translation,198,19,model,efficiently,due to,highly parallelized self-attention network,efficiently due to highly parallelized self-attention network,0.6514376997947693
translation,198,19,model,model,has,transformer,model has transformer,0.5715572834014893
translation,198,26,model,all linear transformations of an lstm,at,sequence level,all linear transformations of an lstm at sequence level,0.5396550297737122
translation,198,26,model,model,efficiently parallelize,all linear transformations of an lstm,model efficiently parallelize all linear transformations of an lstm,0.7518606781959534
translation,198,59,model,self-attention layers,of,transformer decoder,self-attention layers of transformer decoder,0.5560768842697144
translation,198,59,model,transformer decoder,with,mhplstm,transformer decoder with mhplstm,0.6551958322525024
translation,198,59,model,model,replace,self-attention layers,model replace self-attention layers,0.6034772396087646
translation,198,83,model,aan,consists of,average layer,aan consists of average layer,0.6642022132873535
translation,198,83,model,aan,consists of,gating layer,aan consists of gating layer,0.6647493243217468
translation,198,83,model,average layer,averages,preceding embeddings,average layer averages preceding embeddings,0.7567808628082275
translation,198,83,model,feed-forward network,to perform,context - aware encoding,feed-forward network to perform context - aware encoding,0.6954674124717712
translation,198,83,model,context - aware encoding,based on,averaged context embedding,context - aware encoding based on averaged context embedding,0.6596981883049011
translation,198,83,model,gating layer,to enhance,expressiveness,gating layer to enhance expressiveness,0.6554729342460632
translation,198,83,model,model,has,aan,model has aan,0.6313980221748352
translation,198,28,results,hplstm model,computes,lstm gates and the hidden state,hplstm model computes lstm gates and the hidden state,0.753385603427887
translation,198,28,results,hplstm model,computes,bag-ofwords representation,hplstm model computes bag-ofwords representation,0.6307557821273804
translation,198,28,results,hplstm model,divide,high-dimensional hplstm computation,hplstm model divide high-dimensional hplstm computation,0.7374086380004883
translation,198,28,results,lstm gates and the hidden state,with,current input embedding,lstm gates and the hidden state with current input embedding,0.616410493850708
translation,198,28,results,lstm gates and the hidden state,with,bag-ofwords representation,lstm gates and the hidden state with bag-ofwords representation,0.6001875400543213
translation,198,28,results,bag-ofwords representation,of,preceding representations,bag-ofwords representation of preceding representations,0.5700730085372925
translation,198,28,results,high-dimensional hplstm computation,into,several lowdimensional hplstm transformations,high-dimensional hplstm computation into several lowdimensional hplstm transformations,0.5628620386123657
translation,198,28,results,several lowdimensional hplstm transformations,namely,multi-head hplstm,several lowdimensional hplstm transformations namely multi-head hplstm,0.6606736779212952
translation,198,28,results,mhplstm decoder,achieve,improved performance,mhplstm decoder achieve improved performance,0.6638527512550354
translation,198,28,results,mhplstm decoder,achieve,self-attention networks and recurrent approaches,mhplstm decoder achieve self-attention networks and recurrent approaches,0.6325358152389526
translation,198,28,results,mhplstm decoder,achieve,significantly faster,mhplstm decoder achieve significantly faster,0.6437440514564514
translation,198,28,results,mhplstm decoder,achieve,decoding,mhplstm decoder achieve decoding,0.6787319779396057
translation,198,28,results,results,present,hplstm model,results present hplstm model,0.62008136510849
translation,198,28,results,results,present,improved performance,results present improved performance,0.7204731702804565
translation,198,28,results,results,present,self-attention networks and recurrent approaches,results present self-attention networks and recurrent approaches,0.598342776298523
translation,198,28,results,results,present,significantly faster,results present significantly faster,0.6602349281311035
translation,198,28,results,results,present,decoding,results present decoding,0.5373268127441406
translation,198,28,results,results,divide,high-dimensional hplstm computation,results divide high-dimensional hplstm computation,0.5981897711753845
translation,198,28,results,results,divide,improved performance,results divide improved performance,0.7213106751441956
translation,198,28,results,results,divide,self-attention networks and recurrent approaches,results divide self-attention networks and recurrent approaches,0.6100094318389893
translation,198,28,results,results,divide,significantly faster,results divide significantly faster,0.6439425945281982
translation,198,28,results,results,divide,decoding,results divide decoding,0.5299158096313477
translation,198,79,results,lstm - based decoder,bring,significant improvements,lstm - based decoder bring significant improvements,0.6254091262817383
translation,198,79,results,significant improvements,over,self-attention decoder,significant improvements over self-attention decoder,0.704086184501648
translation,198,79,results,results,using,lstm - based decoder,results using lstm - based decoder,0.6372978091239929
translation,198,80,results,mh -plstm,improves,+ 0.82 and + 0.77 bleu,mh -plstm improves + 0.82 and + 0.77 bleu,0.6432833075523376
translation,198,80,results,+ 0.82 and + 0.77 bleu,on,en-de and en- fr task,+ 0.82 and + 0.77 bleu on en-de and en- fr task,0.5528563261032104
translation,198,80,results,+ 0.82 and + 0.77 bleu,using,base setting,+ 0.82 and + 0.77 bleu using base setting,0.67316734790802
translation,198,80,results,+ 1.13 and + 0.92,using,big setting,+ 1.13 and + 0.92 using big setting,0.6956053376197815
translation,198,81,results,lstm - based decoder,improve,translation quality,lstm - based decoder improve translation quality,0.6449786424636841
translation,198,81,results,translation quality,with,mhplstm,translation quality with mhplstm,0.6116679906845093
translation,198,81,results,mhplstm,further improving over,ln - lstm,mhplstm further improving over ln - lstm,0.6879014372825623
translation,198,81,results,results,using,lstm - based decoder,results using lstm - based decoder,0.6372978091239929
translation,198,89,results,mhplstm,is,fastest,mhplstm is fastest,0.6322886943817139
translation,198,89,results,mhplstm,not only,fastest,mhplstm not only fastest,0.7135294079780579
translation,198,89,results,mhplstm,leads to,best performance,mhplstm leads to best performance,0.6771858334541321
translation,198,89,results,fastest,in,training and decoding,fastest in training and decoding,0.5090934038162231
translation,198,89,results,fastest,both,training and decoding,fastest both training and decoding,0.6722058057785034
translation,198,89,results,best performance,compared to,baselines,best performance compared to baselines,0.6500247120857239
translation,198,89,results,results,shows,mhplstm,results shows mhplstm,0.5947105288505554
translation,198,90,results,mhplstm,surpasses,ln - lstm,mhplstm surpasses ln - lstm,0.6179535984992981
translation,198,90,results,results,has,mhplstm,results has mhplstm,0.4890182912349701
translation,198,106,results,number of heads,increases,both parameters and time consumption,number of heads increases both parameters and time consumption,0.7299120426177979
translation,198,106,results,both parameters and time consumption,with,small performance gains,both parameters and time consumption with small performance gains,0.6493527293205261
translation,198,106,results,small performance gains,compared to using,8 heads,small performance gains compared to using 8 heads,0.6828014254570007
translation,198,106,results,results,reducing,number of heads,results reducing number of heads,0.6534658074378967
translation,198,112,results,mhplstm,for,encoding,mhplstm for encoding,0.6697043180465698
translation,198,112,results,mhplstm,leads to,significant performance drop,mhplstm leads to significant performance drop,0.6782089471817017
translation,198,112,results,encoding,leads to,significant performance drop,encoding leads to significant performance drop,0.7111369967460632
translation,198,112,results,significant performance drop,with,more parameters,significant performance drop with more parameters,0.6832066774368286
translation,198,112,results,underperforms,has,baseline,underperforms has baseline,0.6125874519348145
translation,198,112,results,slowing down,has,decoding,slowing down has decoding,0.5681196451187134
translation,198,112,results,results,using,mhplstm,results using mhplstm,0.5964895486831665
translation,198,119,results,mhplstm,surpasses,other approaches,mhplstm surpasses other approaches,0.640734076499939
translation,198,119,results,other approaches,in,most length groups,other approaches in most length groups,0.5382786989212036
translation,198,119,results,improvements,of using,mhplstm based - decoder,improvements of using mhplstm based - decoder,0.6846442222595215
translation,198,119,results,mhplstm based - decoder,are,more significant,mhplstm based - decoder are more significant,0.603663980960846
translation,198,119,results,more significant,for,long sentences,more significant for long sentences,0.6212637424468994
translation,198,119,results,long sentences,than,short sentences,long sentences than short sentences,0.5758229494094849
translation,198,119,results,results,shows,mhplstm,results shows mhplstm,0.5947105288505554
translation,198,120,results,all recurrent - based approaches,are,faster,all recurrent - based approaches are faster,0.5855787992477417
translation,198,120,results,faster,than,self-attention decoder,faster than self-attention decoder,0.572502851486206
translation,198,120,results,self-attention decoder,in,all length groups,self-attention decoder in all length groups,0.5346618890762329
translation,198,120,results,mhplstm,achieves,comparable decoding speed,mhplstm achieves comparable decoding speed,0.6868498921394348
translation,198,120,results,comparable decoding speed,as,lstm and atr,comparable decoding speed as lstm and atr,0.5511149168014526
translation,198,120,results,results,shows,all recurrent - based approaches,results shows all recurrent - based approaches,0.6432949900627136
translation,198,120,results,results,shows,mhplstm,results shows mhplstm,0.5947105288505554
translation,198,127,results,outperforms,in,almost all cases,outperforms in almost all cases,0.5612384080886841
translation,198,127,results,baselines,in,almost all cases,baselines in almost all cases,0.5352045893669128
translation,198,127,results,mhplstm,has,outperforms,mhplstm has outperforms,0.6508635878562927
translation,198,127,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,198,127,results,results,shows,mhplstm,results shows mhplstm,0.5947105288505554
translation,198,128,results,self-attention network,performs,best,self-attention network performs best,0.6021853089332581
translation,198,128,results,best,indicating,strong ability,best indicating strong ability,0.6384468674659729
translation,198,128,results,mhplstm,surpasses,other recurrent approaches,mhplstm surpasses other recurrent approaches,0.6456502079963684
translation,198,128,results,distances,has,longer than 15,distances has longer than 15,0.624334454536438
translation,198,128,results,distances,has,self-attention network,distances has self-attention network,0.5704831480979919
translation,198,128,results,longer than 15,has,self-attention network,longer than 15 has self-attention network,0.5862751007080078
translation,198,128,results,results,For,distances,results For distances,0.6031729578971863
translation,199,47,ablation-analysis,early stopping,after,32 checkpoints,early stopping after 32 checkpoints,0.6966045498847961
translation,199,47,ablation-analysis,32 checkpoints,has,without improvement,32 checkpoints has without improvement,0.5957104563713074
translation,199,47,ablation-analysis,ablation analysis,performed,early stopping,ablation analysis performed early stopping,0.2659168243408203
translation,199,37,experimental-setup,bpe vocabularies,of size,"10k , 15 k , 20k , and 25 k merges","bpe vocabularies of size 10k , 15 k , 20k , and 25 k merges",0.7009968757629395
translation,199,37,experimental-setup,"10k , 15 k , 20k , and 25 k merges",using,subword - nmt,"10k , 15 k , 20k , and 25 k merges using subword - nmt",0.6819922924041748
translation,199,37,experimental-setup,experimental setup,build,bpe vocabularies,experimental setup build bpe vocabularies,0.6708649396896362
translation,199,45,experimental-setup,"transformer models ( vaswani et al. , 2017 )",using,"sockeye ( hieber et al. , 2018 ) version 2.3.14","transformer models ( vaswani et al. , 2017 ) using sockeye ( hieber et al. , 2018 ) version 2.3.14",0.6556720733642578
translation,199,45,experimental-setup,"transformer models ( vaswani et al. , 2017 )",using,cuda - 10.1,"transformer models ( vaswani et al. , 2017 ) using cuda - 10.1",0.6350039839744568
translation,199,45,experimental-setup,experimental setup,built,"transformer models ( vaswani et al. , 2017 )","experimental setup built transformer models ( vaswani et al. , 2017 )",0.637416660785675
translation,199,46,experimental-setup,default value,of,6 encoder / decoder layers,default value of 6 encoder / decoder layers,0.5591860413551331
translation,199,46,experimental-setup,default value,of,8 attention heads,default value of 8 attention heads,0.5838844776153564
translation,199,46,experimental-setup,default value,of,"adam ( kingma and ba , 2015 ) optimizer","default value of adam ( kingma and ba , 2015 ) optimizer",0.537006676197052
translation,199,46,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,199,46,experimental-setup,model size,of,512 units,model size of 512 units,0.6543954014778137
translation,199,46,experimental-setup,model size,with,ffn size,model size with ffn size,0.6068729162216187
translation,199,46,experimental-setup,512 units,with,ffn size,512 units with ffn size,0.6730079054832458
translation,199,46,experimental-setup,ffn size,of,2048,ffn size of 2048,0.6194594502449036
translation,199,46,experimental-setup,experimental setup,used,default value,experimental setup used default value,0.6478846669197083
translation,199,46,experimental-setup,experimental setup,used,model size,experimental setup used model size,0.6703687310218811
translation,199,50,experimental-setup,batch size,set to,8192 tokens,batch size set to 8192 tokens,0.6836884021759033
translation,199,50,experimental-setup,maximum sequence length,for,both source and target,maximum sequence length for both source and target,0.6143475770950317
translation,199,50,experimental-setup,both source and target,set to,200 tokens,both source and target set to 200 tokens,0.6760866045951843
translation,199,50,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,199,50,experimental-setup,experimental setup,has,maximum sequence length,experimental setup has maximum sequence length,0.5124795436859131
translation,199,51,experimental-setup,gradient clipping,to,absolute,gradient clipping to absolute,0.5756909251213074
translation,199,51,experimental-setup,gradient clipping,kept,initial learning rate,gradient clipping kept initial learning rate,0.5862320065498352
translation,199,51,experimental-setup,initial learning rate,of,0.0002,initial learning rate of 0.0002,0.5694879293441772
translation,199,51,experimental-setup,experimental setup,used,weight tying,experimental setup used weight tying,0.5412151217460632
translation,199,51,experimental-setup,experimental setup,set,gradient clipping,experimental setup set gradient clipping,0.5943775773048401
translation,199,172,experiments,german-lower sorbian system,scored,29.9 bleu ( 0.599 chrf ),german-lower sorbian system scored 29.9 bleu ( 0.599 chrf ),0.6621887683868408
translation,199,172,experiments,29.9 bleu ( 0.599 chrf ),on,test,29.9 bleu ( 0.599 chrf ) on test,0.5420540571212769
translation,199,172,experiments,de-dsb,has,german-lower sorbian system,de-dsb has german-lower sorbian system,0.5911647081375122
translation,199,6,results,automatic metrics,showed,strong performance,automatic metrics showed strong performance,0.6458337903022766
translation,199,6,results,our systems,showed,strong performance,our systems showed strong performance,0.6909357905387878
translation,199,6,results,automatic metrics,has,our systems,automatic metrics has our systems,0.5966519713401794
translation,199,103,results,random baseline,three variations of,bpe - dropout,random baseline three variations of bpe - dropout,0.7134295105934143
translation,199,103,results,moore-lewis approach,has,outperforms or matches,moore-lewis approach has outperforms or matches,0.648132860660553
translation,199,103,results,outperforms or matches,has,random baseline,outperforms or matches has random baseline,0.5918450355529785
translation,199,103,results,results,find that,moore-lewis approach,results find that moore-lewis approach,0.672240674495697
translation,199,161,results,preliminary automatic metric results,from,shared task organizers,preliminary automatic metric results from shared task organizers,0.5224925875663757
translation,199,161,results,our systems,performed,quite well,our systems performed quite well,0.25615939497947693
translation,199,161,results,preliminary automatic metric results,has,our systems,preliminary automatic metric results has our systems,0.5829935073852539
translation,199,161,results,shared task organizers,has,our systems,shared task organizers has our systems,0.5687922239303589
translation,199,161,results,results,According to,preliminary automatic metric results,results According to preliminary automatic metric results,0.603959321975708
translation,199,164,results,significantly better,than,all four other systems,significantly better than all four other systems,0.5863348245620728
translation,199,164,results,all four other systems,in terms of,bert score,all four other systems in terms of bert score,0.7558382153511047
translation,199,164,results,hsb - de,has,best bert score ( 0.981 ),hsb - de has best bert score ( 0.981 ),0.5639315247535706
translation,199,164,results,second- best bleu score,has,"67.3 , 0.4 bleu behind","second- best bleu score has 67.3 , 0.4 bleu behind",0.5446287393569946
translation,199,164,results,"67.3 , 0.4 bleu behind",has,noahnmt,"67.3 , 0.4 bleu behind has noahnmt",0.5971108078956604
translation,199,164,results,results,has,hsb - de,results has hsb - de,0.5290058851242065
translation,199,167,results,dsb - de system,tied for,best bert score ( 0.953 ),dsb - de system tied for best bert score ( 0.953 ),0.6821432709693909
translation,199,167,results,dsb - de system,in terms of,bleu and chrf,dsb - de system in terms of bleu and chrf,0.7350671887397766
translation,199,167,results,dsb - de system,in terms of,bert score,dsb - de system in terms of bert score,0.7114285230636597
translation,199,167,results,best bert score ( 0.953 ),with,cl_rug,best bert score ( 0.953 ) with cl_rug,0.640335202217102
translation,199,167,results,best bert score ( 0.953 ),in terms of,bleu and chrf,best bert score ( 0.953 ) in terms of bleu and chrf,0.6445326805114746
translation,199,167,results,bleu and chrf,in terms of,bert score,bleu and chrf in terms of bert score,0.6732649207115173
translation,199,167,results,significantly better,than,one other system,significantly better than one other system,0.5870619416236877
translation,199,167,results,significantly better,than,two other systems,significantly better than two other systems,0.5920506715774536
translation,199,167,results,results,has,dsb - de system,results has dsb - de system,0.5733660459518433
translation,199,168,results,both of our systems,has,translating out of german,both of our systems has translating out of german,0.6276023983955383
translation,199,168,results,results,has,both of our systems,results has both of our systems,0.578615128993988
translation,199,169,results,de - hsb system,alongside,noahnmt,de - hsb system alongside noahnmt,0.7099729180335999
translation,199,169,results,significantly better,than,three other systems,significantly better than three other systems,0.5973465442657471
translation,199,169,results,three other systems,in,both automatic metrics,three other systems in both automatic metrics,0.4976186156272888
translation,199,169,results,results,has,de - hsb system,results has de - hsb system,0.5943934321403503
translation,199,170,results,de - dsb system,was,significantly better,de - dsb system was significantly better,0.6632062792778015
translation,199,170,results,de - dsb system,alongside,lmu,de - dsb system alongside lmu,0.6583837866783142
translation,199,170,results,significantly better,than,three other systems,significantly better than three other systems,0.5973465442657471
translation,199,170,results,significantly better,in,automatic metrics,significantly better in automatic metrics,0.5364973545074463
translation,199,170,results,results,has,de - dsb system,results has de - dsb system,0.6001611351966858
translation,200,162,experiments,both uu and ur models,lead to,large number of repeated generations,both uu and ur models lead to large number of repeated generations,0.6275789141654968
translation,200,162,experiments,bt,has,both uu and ur models,bt has both uu and ur models,0.6239030361175537
translation,200,16,model,systematic study,of,different kinds of hallucinations,systematic study of different kinds of hallucinations,0.5842570066452026
translation,200,16,model,different kinds of hallucinations,studying them through,lens,different kinds of hallucinations studying them through lens,0.7455216646194458
translation,200,16,model,different kinds of hallucinations,studying them through,memorization,different kinds of hallucinations studying them through memorization,0.6901041269302368
translation,200,16,model,lens,of,generalization,lens of generalization,0.614111602306366
translation,200,16,model,lens,of,memorization,lens of memorization,0.5900730490684509
translation,200,16,model,lens,of,optimization,lens of optimization,0.5470203757286072
translation,200,16,model,optimization,in,sequence to sequence models,optimization in sequence to sequence models,0.5531517863273621
translation,200,16,model,model,present,systematic study,model present systematic study,0.7001838088035583
translation,200,139,model,ur model,produces,num-ber of repeated outputs ( irs repeats ),ur model produces num-ber of repeated outputs ( irs repeats ),0.6237254738807678
translation,200,139,model,num-ber of repeated outputs ( irs repeats ),from,training corpus,num-ber of repeated outputs ( irs repeats ) from training corpus,0.5143793225288391
translation,200,139,model,irs set,has,ur model,irs set has ur model,0.5944715738296509
translation,200,139,model,model,On,irs set,model On irs set,0.632987916469574
translation,201,78,ablation-analysis,forward translation,enrich,information,forward translation enrich information,0.6874858140945435
translation,201,78,ablation-analysis,information,in,low- resource condition,information in low- resource condition,0.5178300738334656
translation,201,78,ablation-analysis,information,with,improvement,information with improvement,0.6411734819412231
translation,201,78,ablation-analysis,improvement,of,0.1 to 0.65 bleu score,improvement of 0.1 to 0.65 bleu score,0.5436500310897827
translation,201,78,ablation-analysis,ablation analysis,has,forward translation,ablation analysis has forward translation,0.5639491081237793
translation,201,77,experiments,multilingual translation model,gets,im,multilingual translation model gets im,0.5419168472290039
translation,201,77,experiments,im,-,provement,im - provement,0.7216448783874512
translation,201,77,experiments,provement,ranging from,0.02 to 1.16 bleu score,provement ranging from 0.02 to 1.16 bleu score,0.5927920937538147
translation,201,77,experiments,im,has,provement,im has provement,0.6375266313552856
translation,201,76,results,back - translation,is,most ef- fective method,back - translation is most ef- fective method,0.5795472264289856
translation,201,76,results,most ef- fective method,with,improvement,most ef- fective method with improvement,0.6649307608604431
translation,201,76,results,improvement,ranging from,0.03 to 6.07 bleu score,improvement ranging from 0.03 to 6.07 bleu score,0.5686896443367004
translation,201,76,results,0.03 to 6.07 bleu score,in,low-resource condition,0.03 to 6.07 bleu score in low-resource condition,0.5060577392578125
translation,201,76,results,results,has,back - translation,results has back - translation,0.5638759732246399
translation,202,179,ablation-analysis,bleu scores,of,both decoders,bleu scores of both decoders,0.5376405715942383
translation,202,179,ablation-analysis,cross-attention,has,bleu scores,cross-attention has bleu scores,0.5522798299789429
translation,202,179,ablation-analysis,both decoders,has,decrease dramatically,both decoders has decrease dramatically,0.5961651802062988
translation,202,179,ablation-analysis,ablation analysis,without,cross-attention,ablation analysis without cross-attention,0.6934490203857422
translation,202,208,ablation-analysis,decreases dramatically,at,almost the same extent,decreases dramatically at almost the same extent,0.5718792676925659
translation,202,208,ablation-analysis,obvious gain,compared to,transformer,obvious gain compared to transformer,0.6677268147468567
translation,202,208,ablation-analysis,future or past information,has,translation performance,future or past information has translation performance,0.5505159497261047
translation,202,208,ablation-analysis,translation performance,has,decreases dramatically,translation performance has decreases dramatically,0.6147804856300354
translation,202,208,ablation-analysis,ablation analysis,exclude,future or past information,ablation analysis exclude future or past information,0.7088636159896851
translation,202,212,ablation-analysis,knowledge distillation,is,dropped,knowledge distillation is dropped,0.6489781141281128
translation,202,212,ablation-analysis,decline,has,greatly,decline has greatly,0.6054545044898987
translation,202,212,ablation-analysis,ablation analysis,When,knowledge distillation,ablation analysis When knowledge distillation,0.6410521864891052
translation,202,156,baselines,global planning,including,reinforcement - based method ( rl - nmt ),global planning including reinforcement - based method ( rl - nmt ),0.6507385969161987
translation,202,156,baselines,global planning,including,twopass decoding method ( abdnmt ),global planning including twopass decoding method ( abdnmt ),0.6546830534934998
translation,202,156,baselines,global planning,including,twin networks,global planning including twin networks,0.6741437911987305
translation,202,156,baselines,twin networks,match,past and future information ( twinnet ),twin networks match past and future information ( twinnet ),0.7804652452468872
translation,202,156,baselines,nmt model,with,evaluate module,nmt model with evaluate module,0.6273530125617981
translation,202,156,baselines,evaluate module,to evaluate,fluency and faithfulness ( evanmt ),evaluate module to evaluate fluency and faithfulness ( evanmt ),0.7340344190597534
translation,202,5,model,another decoder,called,seer decoder,another decoder called seer decoder,0.6672883629798889
translation,202,5,model,another decoder,into,encoder-decoder framework,another decoder into encoder-decoder framework,0.5668066143989563
translation,202,5,model,encoder-decoder framework,during,training,encoder-decoder framework during training,0.6838286519050598
translation,202,5,model,encoder-decoder framework,involves,future information,encoder-decoder framework involves future information,0.6214669346809387
translation,202,5,model,future information,in,target predictions,future information in target predictions,0.48688313364982605
translation,202,5,model,model,introduce,another decoder,model introduce another decoder,0.6987552046775818
translation,202,29,model,seer decoder,to guide,behaviors,seer decoder to guide behaviors,0.6960023641586304
translation,202,29,model,seer decoder,at test,translation model,seer decoder at test translation model,0.7101202011108398
translation,202,29,model,behaviors,of,conventional decoder,behaviors of conventional decoder,0.6255671381950378
translation,202,29,model,behaviors,of,conventional decoder,behaviors of conventional decoder,0.6255671381950378
translation,202,29,model,inferences,with,conventional decoder,inferences with conventional decoder,0.7143285870552063
translation,202,29,model,inferences,without introducing,extra parameters,inferences without introducing extra parameters,0.7511358857154846
translation,202,29,model,training,has,seer decoder,training has seer decoder,0.5691528916358948
translation,202,29,model,model,During,training,model During training,0.714866042137146
translation,202,157,model,learning mechanisms,transfer,knowledge,learning mechanisms transfer knowledge,0.7042692303657532
translation,202,157,model,knowledge,from,seer decoder,knowledge from seer decoder,0.6159481406211853
translation,202,157,model,knowledge,including,l 2 regularization ( seer +l 2 ),knowledge including l 2 regularization ( seer +l 2 ),0.7032799124717712
translation,202,157,model,knowledge,including,adversarial learning ( seer + al ),knowledge including adversarial learning ( seer + al ),0.6626132130622864
translation,202,157,model,knowledge,including,knowledge distillation,knowledge including knowledge distillation,0.6948522329330444
translation,202,157,model,knowledge distillation,has,our method ),knowledge distillation has our method ),0.606899082660675
translation,202,157,model,model,explore,learning mechanisms,model explore learning mechanisms,0.6584896445274353
translation,202,161,results,twinnet,get,comparable bleu scores,twinnet get comparable bleu scores,0.5535176396369934
translation,202,161,results,comparable bleu scores,with,our method,comparable bleu scores with our method,0.5838059186935425
translation,202,161,results,comparable bleu scores,with,mostly negative difference,comparable bleu scores with mostly negative difference,0.6092344522476196
translation,202,161,results,our method,on,small data sets,our method on small data sets,0.5347951054573059
translation,202,161,results,mostly negative difference,on,big data sets,mostly negative difference on big data sets,0.5506983399391174
translation,202,161,results,results,has,twinnet,results has twinnet,0.5522757172584534
translation,202,162,results,evanmt,achieve,consistent improvements,evanmt achieve consistent improvements,0.6726458668708801
translation,202,162,results,evanmt,achieve,greater improvements,evanmt achieve greater improvements,0.6873620748519897
translation,202,162,results,greater improvements,on,en ? de data set,greater improvements on en ? de data set,0.6024785041809082
translation,202,162,results,results,has,evanmt,results has evanmt,0.5311245322227478
translation,202,163,results,knowledge distillation,show,consistent superiority,knowledge distillation show consistent superiority,0.6346860527992249
translation,202,163,results,consistent superiority,over,l 2 regularization,consistent superiority over l 2 regularization,0.6999151110649109
translation,202,163,results,consistent superiority,over,adversarial learning,consistent superiority over adversarial learning,0.6650993227958679
translation,202,163,results,learning mechanisms,has,knowledge distillation,learning mechanisms has knowledge distillation,0.5534070730209351
translation,202,163,results,results,For,learning mechanisms,results For learning mechanisms,0.5834559798240662
translation,202,164,results,adversarial learning,bring,improvements,adversarial learning bring improvements,0.6686630845069885
translation,202,164,results,improvements,over,transformer,improvements over transformer,0.7688023447990417
translation,202,164,results,transformer,on,all the data sets,transformer on all the data sets,0.6061811447143555
translation,202,164,results,l 2 regularization,acts,unstable,l 2 regularization acts unstable,0.7197287678718567
translation,202,164,results,unstable,on,big data sets,unstable on big data sets,0.5896468162536621
translation,202,164,results,results,has,adversarial learning,results has adversarial learning,0.5380653738975525
translation,202,178,results,seer decoder,make,super large improvements,seer decoder make super large improvements,0.6740462779998779
translation,202,178,results,super large improvements,over,conventional decoder,super large improvements over conventional decoder,0.7132219076156616
translation,202,178,results,super large improvements,on,all the test sets,super large improvements on all the test sets,0.53692227602005
translation,202,192,results,conventional decoder,in,our method,conventional decoder in our method,0.5180636048316956
translation,202,192,results,conventional decoder,can achieve,higher accuracy and recall,conventional decoder can achieve higher accuracy and recall,0.7016085386276245
translation,202,192,results,higher accuracy and recall,compared to,decoder of transformer,higher accuracy and recall compared to decoder of transformer,0.7209472060203552
translation,202,192,results,results,show,conventional decoder,results show conventional decoder,0.6660056710243225
translation,203,36,experimental-setup,"joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 )",to,pre-tokenized corpus,"joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 ) to pre-tokenized corpus",0.4836455285549164
translation,203,36,experimental-setup,"joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 )",with,10 k character limitation,"joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 ) with 10 k character limitation",0.6028650999069214
translation,203,36,experimental-setup,pre-tokenized corpus,with,75 k mergeoperations,pre-tokenized corpus with 75 k mergeoperations,0.624440610408783
translation,203,36,experimental-setup,pre-tokenized corpus,with,10 k character limitation,pre-tokenized corpus with 10 k character limitation,0.630806028842926
translation,203,36,experimental-setup,10 k character limitation,using,open-source toolkit transformers,10 k character limitation using open-source toolkit transformers,0.673117458820343
translation,203,36,experimental-setup,experimental setup,apply,"joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 )","experimental setup apply joint multilingual byte-pair encoding ( bpe , sennrich et al. 2016 )",0.529647171497345
translation,203,65,experimental-setup,bitext data,to maintain,1 - to - 1 ratio,bitext data to maintain 1 - to - 1 ratio,0.6861061453819275
translation,203,65,experimental-setup,1 - to - 1 ratio,of,real to synthetic bitext,1 - to - 1 ratio of real to synthetic bitext,0.6298372149467468
translation,203,65,experimental-setup,real to synthetic bitext,during,training phase,real to synthetic bitext during training phase,0.6982629299163818
translation,203,65,experimental-setup,experimental setup,upsample,bitext data,experimental setup upsample bitext data,0.7613841891288757
translation,203,82,experimental-setup,batch size,of,3584 tokens,batch size of 3584 tokens,0.6033377051353455
translation,203,82,experimental-setup,model parameters,using,adam optimizer,model parameters using adam optimizer,0.6400127410888672
translation,203,82,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,203,82,experimental-setup,learning rate,has,1e - 3 ? 1 = 0.9 and ? 2 = 0,learning rate has 1e - 3 ? 1 = 0.9 and ? 2 = 0,0.541126549243927
translation,203,82,experimental-setup,experimental setup,train with,batch size,experimental setup train with batch size,0.699195384979248
translation,203,83,experimental-setup,label smoothing,with,factor,label smoothing with factor,0.6382258534431458
translation,203,83,experimental-setup,factor,of,0.1,factor of 0.1,0.6363644003868103
translation,203,83,experimental-setup,experimental setup,apply,label smoothing,experimental setup apply label smoothing,0.5487022399902344
translation,203,84,experimental-setup,dropout,set to,0.1,dropout set to 0.1,0.616740882396698
translation,203,84,experimental-setup,attention dropout,set to,0.3,attention dropout set to 0.3,0.6612024903297424
translation,203,84,experimental-setup,training phase,has,dropout,training phase has dropout,0.555915355682373
translation,203,84,experimental-setup,training phase,has,attention dropout,training phase has attention dropout,0.5451079607009888
translation,203,84,experimental-setup,experimental setup,In,training phase,experimental setup In training phase,0.5292714238166809
translation,203,85,experimental-setup,early stopping technique,using,wmt21 triangular mt development set,early stopping technique using wmt21 triangular mt development set,0.6707829236984253
translation,203,85,experimental-setup,experimental setup,apply,early stopping technique,experimental setup apply early stopping technique,0.5975091457366943
translation,203,86,experimental-setup,our models,using,"fairseq 6 ( ott et al. , 2019 )","our models using fairseq 6 ( ott et al. , 2019 )",0.6383406519889832
translation,203,86,experimental-setup,"fairseq 6 ( ott et al. , 2019 )",on,8 nvidia tesla v100 gpus,"fairseq 6 ( ott et al. , 2019 ) on 8 nvidia tesla v100 gpus",0.4942983388900757
translation,203,86,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,203,21,model,existing novel techniques,to integrate them for,triangular mt tasks,existing novel techniques to integrate them for triangular mt tasks,0.7409226894378662
translation,203,21,model,model,explore,existing novel techniques,model explore existing novel techniques,0.7618249654769897
translation,203,81,model,base system,based on,transformer - large,base system based on transformer - large,0.6737105250358582
translation,203,81,model,transformer - large,with,embedding size,transformer - large with embedding size,0.6846246123313904
translation,203,81,model,transformer - large,with,12 encoder and decoder layers,transformer - large with 12 encoder and decoder layers,0.6452115774154663
translation,203,81,model,transformer - large,with,12 attention heads,transformer - large with 12 attention heads,0.6826748847961426
translation,203,81,model,transformer - large,with,shared source and target embedding,transformer - large with shared source and target embedding,0.6600528359413147
translation,203,81,model,transformer - large,with,sinusoidal positional embedding,transformer - large with sinusoidal positional embedding,0.6660494208335876
translation,203,81,model,transformer - large,with,pre-norm,transformer - large with pre-norm,0.7063276171684265
translation,203,81,model,embedding size,of,1024,embedding size of 1024,0.6278319954872131
translation,203,81,model,model,has,base system,model has base system,0.6133134365081787
translation,203,11,results,our final system,provides,12.7 bleu points improvement,our final system provides 12.7 bleu points improvement,0.5673468708992004
translation,203,11,results,12.7 bleu points improvement,over,baseline system,12.7 bleu points improvement over baseline system,0.5898246765136719
translation,203,11,results,12.7 bleu points improvement,on,wmt21 triangular mt development set,12.7 bleu points improvement on wmt21 triangular mt development set,0.4981713891029358
translation,203,11,results,results,has,our final system,results has our final system,0.6009615659713745
translation,203,88,results,about + 12.7 bleu,compared to,organizer 's systems,about + 12.7 bleu compared to organizer 's systems,0.6455663442611694
translation,203,88,results,our final model,has,outperforms,our final model has outperforms,0.6125113368034363
translation,203,88,results,outperforms,has,about + 12.7 bleu,outperforms has about + 12.7 bleu,0.6068791747093201
translation,203,88,results,results,has,our final model,results has our final model,0.5639100670814514
translation,203,90,results,our model,achieved,improvement,our model achieved improvement,0.7070896625518799
translation,203,90,results,improvement,of,about 1 bleu,improvement of about 1 bleu,0.5595924258232117
translation,203,90,results,data filtering process,has,our model,data filtering process has our model,0.5719868540763855
translation,203,90,results,results,Through,data filtering process,results Through data filtering process,0.6816790699958801
translation,203,91,results,our model,obtained,gain,our model obtained gain,0.704319417476654
translation,203,91,results,gain,of,about 1.5 bleu,gain of about 1.5 bleu,0.5485257506370544
translation,203,91,results,russian - to - chinese corpus,has,our model,russian - to - chinese corpus has our model,0.5404364466667175
translation,203,91,results,results,augmenting,russian - to - chinese corpus,results augmenting russian - to - chinese corpus,0.6058472990989685
translation,203,106,results,sampling and noise beam search,are,more effective,sampling and noise beam search are more effective,0.5909788012504578
translation,203,106,results,more effective,than,vanilla beam search,more effective than vanilla beam search,0.5387568473815918
translation,203,106,results,results,observed,sampling and noise beam search,results observed sampling and noise beam search,0.5895780324935913
translation,203,108,results,none of the decoding strategies,demonstrates,superior performance,none of the decoding strategies demonstrates superior performance,0.6240377426147461
translation,203,108,results,results,has,none of the decoding strategies,results has none of the decoding strategies,0.5305953621864319
translation,203,115,results,does not work well,when,large enough data,does not work well when large enough data,0.6429551839828491
translation,203,115,results,large enough data,are,available,large enough data are available,0.5652877688407898
translation,203,115,results,finetuning mbart,has,does not work well,finetuning mbart has does not work well,0.610263466835022
translation,203,115,results,results,has,finetuning mbart,results has finetuning mbart,0.5669302344322205
translation,204,51,hyperparameters,parameters,set to,default values,parameters set to default values,0.6940023303031921
translation,204,51,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,204,6,model,preprocessing step,interacts with,word alignment task,preprocessing step interacts with word alignment task,0.6430506110191345
translation,204,6,model,preprocessing step,propose,several tokenization strategies,preprocessing step propose several tokenization strategies,0.6124177575111389
translation,204,6,model,several tokenization strategies,to obtain,well - segmented parallel corpora,several tokenization strategies to obtain well - segmented parallel corpora,0.5522486567497253
translation,204,6,model,model,propose,several tokenization strategies,model propose several tokenization strategies,0.7018302083015442
translation,204,66,results,proper choice,of,unit size,proper choice of unit size,0.6244543194770813
translation,204,66,results,bpe - based models,able to outperform,word - based counterparts,bpe - based models able to outperform word - based counterparts,0.8195160031318665
translation,204,66,results,gain,of,2 aer points,gain of 2 aer points,0.6541140079498291
translation,204,66,results,gain,about,2 aer points,gain about 2 aer points,0.693169891834259
translation,204,66,results,proper choice,has,bpe - based models,proper choice has bpe - based models,0.5704009532928467
translation,204,66,results,unit size,has,bpe - based models,unit size has bpe - based models,0.5885835289955139
translation,204,73,results,hmm or ibm - 4,notice,bpe - based models,hmm or ibm - 4 notice bpe - based models,0.744517982006073
translation,204,73,results,bpe - based models,less prone to,over- generate,bpe - based models less prone to over- generate,0.7090023159980774
translation,204,73,results,over- generate,has,null links,over- generate has null links,0.6230297088623047
translation,204,73,results,results,Compared with,hmm or ibm - 4,results Compared with hmm or ibm - 4,0.6769211292266846
translation,204,86,results,bpe models,using,agreement level,bpe models using agreement level,0.6719915270805359
translation,204,86,results,agreement level,of,70 %,agreement level of 70 %,0.5517479181289673
translation,204,86,results,70 %,improves,f-score,70 % improves f-score,0.7085101008415222
translation,204,86,results,f-score,by,almost 2 points,f-score by almost 2 points,0.5739554762840271
translation,204,86,results,almost 2 points,for,german ? english and japanese ? english,almost 2 points for german ? english and japanese ? english,0.6153332591056824
translation,204,86,results,results,considering,bpe models,results considering bpe models,0.5932427048683167
translation,204,86,results,results,using,agreement level,results using agreement level,0.6393772959709167
translation,204,100,results,bpe - based models,minimizing,length difference,bpe - based models minimizing length difference,0.6900015473365784
translation,204,100,results,length difference,between,source and target sentence,length difference between source and target sentence,0.5841236710548401
translation,204,100,results,length difference,outperforms,word - based models,length difference outperforms word - based models,0.7507319450378418
translation,204,100,results,word - based models,with,gain,word - based models with gain,0.6838870644569397
translation,204,100,results,gain,of,at least 1 point,gain of at least 1 point,0.6044997572898865
translation,204,100,results,at least 1 point,in,f-score,at least 1 point in f-score,0.5137041807174683
translation,204,100,results,results,For,bpe - based models,results For bpe - based models,0.5750957727432251
translation,204,112,results,sp - m,fails to achieve,better f-scores,sp - m fails to achieve better f-scores,0.7542794942855835
translation,204,112,results,word- based model,for,english - french and english - vietnamese,word- based model for english - french and english - vietnamese,0.629054069519043
translation,204,112,results,better f-scores,than,two global methods,better f-scores than two global methods,0.5595717430114746
translation,204,112,results,bpe - based models,has,sp - m,bpe - based models has sp - m,0.6257826685905457
translation,204,112,results,outperforms,has,word- based model,outperforms has word- based model,0.5896710157394409
translation,204,112,results,results,For,bpe - based models,results For bpe - based models,0.5750957727432251
translation,204,114,results,word- based models,for,"french , japanese and vietnamese","word- based models for french , japanese and vietnamese",0.5919272303581238
translation,204,114,results,outperform,has,baseline,outperform has baseline,0.6223028898239136
translation,204,114,results,outperform,has,word- based models,outperform has word- based models,0.5830860137939453
translation,204,114,results,baseline,has,fixed 16k - 16 k model,baseline has fixed 16k - 16 k model,0.55800861120224
translation,204,115,results,several segmentation samples,for,each sentence pair,several segmentation samples for each sentence pair,0.5858907699584961
translation,204,115,results,several segmentation samples,helps to improve,performance,several segmentation samples helps to improve performance,0.6589047908782959
translation,204,115,results,each sentence pair,in,training data,each sentence pair in training data,0.47451096773147583
translation,204,115,results,performance,resulting in,simple scheme,performance resulting in simple scheme,0.6656216382980347
translation,204,115,results,simple scheme,based only on,length differences,simple scheme based only on length differences,0.6792398691177368
translation,204,115,results,consistently outperforms,has,all other unigram-based methods,consistently outperforms has all other unigram-based methods,0.5675279498100281
translation,204,115,results,results,including,several segmentation samples,results including several segmentation samples,0.6455997228622437
translation,204,117,results,last method ( ssm5 - gs ),does not succeed,improving,last method ( ssm5 - gs ) does not succeed improving,0.6758286952972412
translation,204,117,results,improving,has,ssm5 - 1vp,improving has ssm5 - 1vp,0.5910240411758423
translation,204,117,results,results,has,last method ( ssm5 - gs ),results has last method ( ssm5 - gs ),0.557650089263916
translation,205,18,ablation-analysis,alignment supervision,mitigates,off-target translation issue,alignment supervision mitigates off-target translation issue,0.7210063934326172
translation,205,18,ablation-analysis,off-target translation issue,in,zero-shot case,off-target translation issue in zero-shot case,0.5368477702140808
translation,205,18,ablation-analysis,ablation analysis,show,alignment supervision,ablation analysis show alignment supervision,0.596104621887207
translation,205,85,baselines,alignment methods in transformer - based models,for,highly multilingual english-centric mt setups,alignment methods in transformer - based models for highly multilingual english-centric mt setups,0.5574691295623779
translation,205,113,baselines,baselines,consist of,variant,baselines consist of variant,0.7411088347434998
translation,205,113,baselines,variant,with,1.5- entmax function,variant with 1.5- entmax function,0.6168102622032166
translation,205,113,baselines,1.5- entmax function,on,cross attention heads,1.5- entmax function on cross attention heads,0.5507659912109375
translation,205,113,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,205,51,experiments,many- to - many english -centric scenario,on,concatenation,many- to - many english -centric scenario on concatenation,0.5341771245002747
translation,205,51,experiments,concatenation,of,training data,concatenation of training data,0.6048902869224548
translation,205,51,experiments,training data,having,english,training data having english,0.6559810042381287
translation,205,45,hyperparameters,joint byte-pair encoding ( bpe ) segmentation,with,shared vocabulary size,joint byte-pair encoding ( bpe ) segmentation with shared vocabulary size,0.63556307554245
translation,205,45,hyperparameters,shared vocabulary size,of,32 k symbols,shared vocabulary size of 32 k symbols,0.5917609333992004
translation,205,45,hyperparameters,shared vocabulary size,of,64 k,shared vocabulary size of 64 k,0.6189397573471069
translation,205,45,hyperparameters,32 k symbols,for,ted talks,32 k symbols for ted talks,0.6663581132888794
translation,205,45,hyperparameters,64 k,for,wmt - 2018 and opus - 100,64 k for wmt - 2018 and opus - 100,0.6613228917121887
translation,205,45,hyperparameters,hyperparameters,apply,joint byte-pair encoding ( bpe ) segmentation,hyperparameters apply joint byte-pair encoding ( bpe ) segmentation,0.6053816080093384
translation,205,50,hyperparameters,transformer models,base setting of,vaswani et al . ( 2017 ),transformer models base setting of vaswani et al . ( 2017 ),0.7294661998748779
translation,205,50,hyperparameters,hyperparameters,has,transformer models,hyperparameters has transformer models,0.5544906258583069
translation,205,105,hyperparameters,hyperparameters,use,opennmt - py framework,hyperparameters use opennmt - py framework,0.6276494860649109
translation,205,105,hyperparameters,hyperparameters,use,transformer base model setting,hyperparameters use transformer base model setting,0.6293789744377136
translation,205,106,hyperparameters,6 layers,for,encoder and the decoder,6 layers for encoder and the decoder,0.6234134435653687
translation,205,106,hyperparameters,6 layers,for,512,6 layers for 512,0.5789616703987122
translation,205,106,hyperparameters,6 layers,for,2048,6 layers for 2048,0.6235937476158142
translation,205,106,hyperparameters,512,as,model dimension,512 as model dimension,0.580206573009491
translation,205,106,hyperparameters,2048,as,hidden dimension,2048 as hidden dimension,0.5820262432098389
translation,205,106,hyperparameters,hyperparameters,use,6 layers,hyperparameters use 6 layers,0.5576580762863159
translation,205,107,hyperparameters,0.1,as,dropout,0.1 as dropout,0.5785402059555054
translation,205,107,hyperparameters,0.1,using,adam optimizer,0.1 using adam optimizer,0.6133359670639038
translation,205,107,hyperparameters,0.1,with,learning rate,0.1 with learning rate,0.6034390926361084
translation,205,107,hyperparameters,dropout,for,both residual layers and attention weights,dropout for both residual layers and attention weights,0.6091408729553223
translation,205,107,hyperparameters,adam optimizer,with,?1 = 0.9,adam optimizer with ?1 = 0.9,0.6178155541419983
translation,205,107,hyperparameters,adam optimizer,with,?2 = 0.998,adam optimizer with ?2 = 0.998,0.6130428910255432
translation,205,107,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,205,107,hyperparameters,learning rate,set at,3 and 40 k warmup steps,learning rate set at 3 and 40 k warmup steps,0.5760002136230469
translation,205,107,hyperparameters,hyperparameters,applied,0.1,hyperparameters applied 0.1,0.6600180268287659
translation,205,108,hyperparameters,models,with,three random seeds each,models with three random seeds each,0.6831787824630737
translation,205,108,hyperparameters,models,for,500 k,models for 500 k,0.6595337390899658
translation,205,108,hyperparameters,models,for,500 k,models for 500 k,0.6595337390899658
translation,205,108,hyperparameters,200 k training steps,for,ted talks and wmt - 2018 benchmarks,200 k training steps for ted talks and wmt - 2018 benchmarks,0.5703282952308655
translation,205,108,hyperparameters,steps,for,opus - 100,steps for opus - 100,0.6978616118431091
translation,205,108,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,205,108,hyperparameters,hyperparameters,for,500 k,hyperparameters for 500 k,0.5845397114753723
translation,205,8,model,benefits,of,explicit alignment to language labels,benefits of explicit alignment to language labels,0.5459425449371338
translation,205,8,model,explicit alignment to language labels,in,transformer - based mnmt models,explicit alignment to language labels in transformer - based mnmt models,0.5093449354171753
translation,205,8,model,explicit alignment to language labels,by jointly training,one cross attention head,explicit alignment to language labels by jointly training one cross attention head,0.7707679271697998
translation,205,8,model,transformer - based mnmt models,in,zero-shot context,transformer - based mnmt models in zero-shot context,0.5102154612541199
translation,205,8,model,one cross attention head,with,word alignment supervision,one cross attention head with word alignment supervision,0.6312512159347534
translation,205,8,model,one cross attention head,to stress,focus,one cross attention head to stress focus,0.6899687647819519
translation,205,8,model,focus,on,target language label,focus on target language label,0.5375187397003174
translation,205,8,model,model,investigate,benefits,model investigate benefits,0.6475893259048462
translation,205,17,model,guided alignment,in,johnson et al . ( 2017 ) setting,guided alignment in johnson et al . ( 2017 ) setting,0.49094632267951965
translation,205,17,model,guided alignment,by jointly training,one cross attention head,guided alignment by jointly training one cross attention head,0.7676216959953308
translation,205,17,model,one cross attention head,to explicitly focus,target language label,one cross attention head to explicitly focus target language label,0.7395734190940857
translation,205,17,model,model,role of,guided alignment,model role of guided alignment,0.7066916227340698
translation,205,62,results,5,attaining,best performance,5 attaining best performance,0.6099541783332825
translation,205,62,results,best performance,with,almost 2 bleu points,best performance with almost 2 bleu points,0.6199728846549988
translation,205,62,results,better,than,baseline,better than baseline,0.6157954335212708
translation,205,62,results,almost 2 bleu points,has,better,almost 2 bleu points has better,0.5807422995567322
translation,205,62,results,considerably improves,has,target language identification accuracy ( acc zero ),considerably improves has target language identification accuracy ( acc zero ),0.6042015552520752
translation,205,62,results,results,has,zero-shot results,results has zero-shot results,0.549163818359375
translation,205,64,results,results,on,wmt - 2018 benchmark,results on wmt - 2018 benchmark,0.5061510801315308
translation,205,65,results,bilingual baselines,are,hard to beat,bilingual baselines are hard to beat,0.5491657257080078
translation,205,65,results,highresource scenario,has,bilingual baselines,highresource scenario has bilingual baselines,0.5912309288978577
translation,205,67,results,model,with,alignment supervision ( c ),model with alignment supervision ( c ),0.6321973204612732
translation,205,67,results,alignment supervision ( c ),results in,best system,alignment supervision ( c ) results in best system,0.5992822647094727
translation,205,67,results,best system,with,improvement,best system with improvement,0.6615539789199829
translation,205,67,results,improvement,of,more than 3 bleu points,improvement of more than 3 bleu points,0.546381413936615
translation,205,67,results,more than 3 bleu points,in,zero-shot ( 2020 ),more than 3 bleu points in zero-shot ( 2020 ),0.5393665432929993
translation,205,67,results,results,Enriching,model,results Enriching model,0.5931881070137024
translation,205,74,results,our results,demonstrate,positive effect,our results demonstrate positive effect,0.5881858468055725
translation,205,74,results,positive effect,of,alignment,positive effect of alignment,0.6099439263343811
translation,205,74,results,alignment,on,zero-shot translation,alignment on zero-shot translation,0.5051535964012146
translation,205,74,results,results,demonstrate,positive effect,results demonstrate positive effect,0.6400994062423706
translation,205,75,results,consistent results,across,different benchmarks,consistent results across different benchmarks,0.734442949295044
translation,205,75,results,utility,of,guided alignment,utility of guided alignment,0.6364330649375916
translation,205,75,results,guided alignment,in,highly multilingual mt scenarios,guided alignment in highly multilingual mt scenarios,0.5390183329582214
translation,205,76,results,single cross attention head,with,alignment method,single cross attention head with alignment method,0.6198715567588806
translation,205,76,results,single cross attention head,substantially reduces,instability,single cross attention head substantially reduces instability,0.7044196724891663
translation,205,76,results,instability,between,training runs,instability between training runs,0.6543627977371216
translation,205,76,results,instability,mitigating,off-target translation issue,instability mitigating off-target translation issue,0.6760283708572388
translation,205,76,results,off-target translation issue,in,zero-shot evaluation,off-target translation issue in zero-shot evaluation,0.5162135362625122
translation,205,76,results,results,Supervising,single cross attention head,results Supervising single cross attention head,0.6004061698913574
translation,205,77,results,zero-shot improvements,i.e.,bleu zero and acc zero,zero-shot improvements i.e. bleu zero and acc zero,0.6153576374053955
translation,205,77,results,bleu zero and acc zero,are,large,bleu zero and acc zero are large,0.6044590473175049
translation,205,77,results,large,in,two benchmarks out of three,large in two benchmarks out of three,0.5514242053031921
translation,205,77,results,results,has,zero-shot improvements,results has zero-shot improvements,0.5633229613304138
translation,206,61,experiments,tamil-to - telugu translation system,achieved,4th rank,tamil-to - telugu translation system achieved 4th rank,0.6439364552497864
translation,206,61,experiments,tamil-to - telugu translation system,achieved,6th rank,tamil-to - telugu translation system achieved 6th rank,0.6459237933158875
translation,206,61,experiments,4th rank,with,bleu score,4th rank with bleu score,0.6165662407875061
translation,206,61,experiments,4th rank,with,bleu score,4th rank with bleu score,0.6165662407875061
translation,206,61,experiments,bleu score,of,4.05,bleu score of 4.05,0.5418864488601685
translation,206,61,experiments,bleu score,of,4.05,bleu score of 4.05,0.5418864488601685
translation,206,61,experiments,bleu score,of,4.05,bleu score of 4.05,0.5418864488601685
translation,206,61,experiments,4.05,for,telugu -to - tamil translation,4.05 for telugu -to - tamil translation,0.6149160861968994
translation,206,61,experiments,6th rank,with,bleu score,6th rank with bleu score,0.6074749827384949
translation,206,61,experiments,bleu score,of,4.05,bleu score of 4.05,0.5418864488601685
translation,206,61,experiments,4.05,for,telugu -to - tamil translation,4.05 for telugu -to - tamil translation,0.6149160861968994
translation,207,62,baselines,gru and lstm,with,hidden dimension,gru and lstm with hidden dimension,0.6366522312164307
translation,207,62,baselines,hidden dimension,of,512,hidden dimension of 512,0.655615508556366
translation,207,62,baselines,spatial han,without,middle layer,spatial han without middle layer,0.7251058220863342
translation,207,59,hyperparameters,maximum sentence length,to,40,maximum sentence length to 40,0.586955189704895
translation,207,59,hyperparameters,word embedding size,to,1,word embedding size to 1,0.5835485458374023
translation,207,59,hyperparameters,word embedding size,",",024,"word embedding size , 024",0.627902090549469
translation,207,59,hyperparameters,1,",",024,"1 , 024",0.7358691096305847
translation,207,59,hyperparameters,text encoder and motion encoder,of,both 2 - layer bi-gru,text encoder and motion encoder of both 2 - layer bi-gru,0.5605800747871399
translation,207,59,hyperparameters,both 2 - layer bi-gru,with,hidden dimension,both 2 - layer bi-gru with hidden dimension,0.6422371864318848
translation,207,59,hyperparameters,hidden dimension,of,512,hidden dimension of 512,0.655615508556366
translation,207,60,hyperparameters,spatial han,used,"faster r-cnn ( anderson et al. , 2017 )","spatial han used faster r-cnn ( anderson et al. , 2017 )",0.5016424655914307
translation,207,60,hyperparameters,"faster r-cnn ( anderson et al. , 2017 )",to extract,object - level features,"faster r-cnn ( anderson et al. , 2017 ) to extract object - level features",0.6592387557029724
translation,207,60,hyperparameters,object - level features,as,input,object - level features as input,0.48685798048973083
translation,207,61,hyperparameters,hidden dimensions,of,both object- level and frame-level attention layers,hidden dimensions of both object- level and frame-level attention layers,0.5319929718971252
translation,207,61,hyperparameters,both object- level and frame-level attention layers,were,512,both object- level and frame-level attention layers were 512,0.5411269068717957
translation,207,61,hyperparameters,hyperparameters,has,hidden dimensions,hyperparameters has hidden dimensions,0.5240819454193115
translation,207,63,hyperparameters,2 - layer gru,with,hidden dimension,2 - layer gru with hidden dimension,0.6351699829101562
translation,207,63,hyperparameters,hidden dimension,of,512,hidden dimension of 512,0.655615508556366
translation,207,63,hyperparameters,hyperparameters,has,target decoder,hyperparameters has target decoder,0.5273938775062561
translation,207,64,hyperparameters,training,used,adam optimizer,training used adam optimizer,0.5732697248458862
translation,207,64,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,207,64,hyperparameters,adam optimizer,with,early stop,adam optimizer with early stop,0.6149167418479919
translation,207,64,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,207,64,hyperparameters,early stop,with,patience,early stop with patience,0.6682699918746948
translation,207,64,hyperparameters,patience,to,10 times,patience to 10 times,0.6067209243774414
translation,207,64,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,207,6,model,video-guided machine translation system,by using,spatial and motion representations,video-guided machine translation system by using spatial and motion representations,0.6044657826423645
translation,207,6,model,spatial and motion representations,in,videos,spatial and motion representations in videos,0.5448227524757385
translation,207,6,model,model,propose,video-guided machine translation system,model propose video-guided machine translation system,0.6202552914619446
translation,207,7,model,spatial features,propose,hierarchical attention network,spatial features propose hierarchical attention network,0.5659743547439575
translation,207,7,model,hierarchical attention network,to model,spatial information,hierarchical attention network to model spatial information,0.6842981576919556
translation,207,7,model,spatial information,from,object - level to video-level,spatial information from object - level to video-level,0.5640667080879211
translation,207,7,model,model,For,spatial features,model For spatial features,0.6204952597618103
translation,207,25,model,spatial representations efficiently,propose to use,hierarchical attention network ( han ),spatial representations efficiently propose to use hierarchical attention network ( han ),0.6964014768600464
translation,207,25,model,hierarchical attention network ( han ),to model,spatial information,hierarchical attention network ( han ) to model spatial information,0.7013982534408569
translation,207,25,model,spatial information,from,object - level to video-level,spatial information from object - level to video-level,0.5640667080879211
translation,207,25,model,spatial information,call,spatial han module,spatial information call spatial han module,0.5969192981719971
translation,207,25,model,model,propose to use,hierarchical attention network ( han ),model propose to use hierarchical attention network ( han ),0.723680853843689
translation,207,26,model,better contextual spatial information,add,several kinds of middle layers,better contextual spatial information add several kinds of middle layers,0.6039437651634216
translation,207,26,model,several kinds of middle layers,between,object - to - frame layer and frameto-video layer,several kinds of middle layers between object - to - frame layer and frameto-video layer,0.6262326240539551
translation,207,26,model,object - to - frame layer and frameto-video layer,in,original han,object - to - frame layer and frameto-video layer in original han,0.5529752373695374
translation,207,26,model,model,to obtain,better contextual spatial information,model to obtain better contextual spatial information,0.6081562042236328
translation,207,70,results,proposed system,achieves,35.75 score,proposed system achieves 35.75 score,0.6449085474014282
translation,207,70,results,proposed system,achieves,35.86 score,proposed system achieves 35.86 score,0.6378397345542908
translation,207,70,results,35.75 score,on,validation set,35.75 score on validation set,0.5218814015388489
translation,207,70,results,35.86 score,on,test set,35.86 score on test set,0.5144261121749878
translation,207,70,results,4.76 bleu score improvement,over,"va - tex 's baseline model ( wang et al. , 2019 )","4.76 bleu score improvement over va - tex 's baseline model ( wang et al. , 2019 )",0.5793808698654175
translation,207,70,results,"va - tex 's baseline model ( wang et al. , 2019 )",with,text corpus and action features,"va - tex 's baseline model ( wang et al. , 2019 ) with text corpus and action features",0.5724793076515198
translation,207,70,results,results,has,proposed system,results has proposed system,0.6201663613319397
translation,207,71,results,0.24 bleu score improvement,over,best single model,0.24 bleu score improvement over best single model,0.5795587301254272
translation,207,71,results,our vmt baseline,has,0.24 bleu score improvement,our vmt baseline has 0.24 bleu score improvement,0.5318071246147156
translation,207,76,results,our models,achieve,comparable bleu score results,our models achieve comparable bleu score results,0.5751295685768127
translation,207,76,results,comparable bleu score results,with and without,motion features,comparable bleu score results with and without motion features,0.660934329032898
translation,207,76,results,results,notice,our models,results notice our models,0.7373254895210266
translation,207,80,results,50 random samples,show,our model,50 random samples show our model,0.6316781044006348
translation,207,80,results,12 better translations,than,vmt baseline model,12 better translations than vmt baseline model,0.5267324447631836
translation,207,80,results,12 better translations,than,vmt baseline model,12 better translations than vmt baseline model,0.5267324447631836
translation,207,80,results,vmt baseline model,mainly on,noun sense disambiguation,vmt baseline model mainly on noun sense disambiguation,0.5913808941841125
translation,207,80,results,noun sense disambiguation,where,vmt baseline model,noun sense disambiguation where vmt baseline model,0.5506657958030701
translation,207,80,results,6 better translations,mainly on,verb sense disambiguation and syntax,6 better translations mainly on verb sense disambiguation and syntax,0.5723413825035095
translation,207,80,results,our model,has,12 better translations,our model has 12 better translations,0.5851443409919739
translation,207,80,results,vmt baseline model,has,6 better translations,vmt baseline model has 6 better translations,0.5648037195205688
translation,207,80,results,results,on,50 random samples,results on 50 random samples,0.5086321830749512
translation,208,198,ablation-analysis,drops significantly,without,pre-trained asr encoder,drops significantly without pre-trained asr encoder,0.7478536367416382
translation,208,198,ablation-analysis,pre-trained asr encoder,especially on,must - c corpus,pre-trained asr encoder especially on must - c corpus,0.59681236743927
translation,208,198,ablation-analysis,must - c corpus,that contains,noisy speech,must - c corpus that contains noisy speech,0.5864443182945251
translation,208,198,ablation-analysis,model performance,has,drops significantly,model performance has drops significantly,0.6186867356300354
translation,208,198,ablation-analysis,ablation analysis,has,model performance,ablation analysis has model performance,0.5138536095619202
translation,208,204,ablation-analysis,great effect,on,final performance,great effect on final performance,0.5659485459327698
translation,208,204,ablation-analysis,each pre-trained module,has,great effect,each pre-trained module has great effect,0.5894878506660461
translation,208,204,ablation-analysis,ablation analysis,has,each pre-trained module,ablation analysis has each pre-trained module,0.5664246678352356
translation,208,208,ablation-analysis,straight connection,omits,representation inconsistency issue,straight connection omits representation inconsistency issue,0.6182813048362732
translation,208,208,ablation-analysis,straight connection,results in,lower benefit,straight connection results in lower benefit,0.6013728380203247
translation,208,208,ablation-analysis,lower benefit,of,pre-training,lower benefit of pre-training,0.5499246716499329
translation,208,208,ablation-analysis,ablation analysis,has,straight connection,ablation analysis has straight connection,0.537434995174408
translation,208,149,experimental-setup,tokenized,using,scripts of moses,tokenized using scripts of moses,0.6150015592575073
translation,208,149,experimental-setup,experimental setup,has,text,experimental setup has text,0.5133448839187622
translation,208,150,experimental-setup,", 2016 ) subword segmentation",with,"10,000 merge operations",", 2016 ) subword segmentation with 10,000 merge operations",0.6192575097084045
translation,208,150,experimental-setup,"10,000 merge operations",based on,shared source and target vocabulary,"10,000 merge operations based on shared source and target vocabulary",0.6522741913795471
translation,208,150,experimental-setup,shared source and target vocabulary,for,all datasets,shared source and target vocabulary for all datasets,0.5521236658096313
translation,208,150,experimental-setup,byte-pair encoding,has,", 2016 ) subword segmentation","byte-pair encoding has , 2016 ) subword segmentation",0.5836183428764343
translation,208,150,experimental-setup,experimental setup,learn,byte-pair encoding,experimental setup learn byte-pair encoding,0.6104363203048706
translation,208,153,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.997","adam optimizer with ? 1 = 0.9 , ? 2 = 0.997",0.6222790479660034
translation,208,153,experimental-setup,adam optimizer,adopt,default learning schedule,adam optimizer adopt default learning schedule,0.6620776057243347
translation,208,153,experimental-setup,default learning schedule,in,espnet,default learning schedule in espnet,0.5658468008041382
translation,208,153,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,208,154,experimental-setup,dropout,with,rate,dropout with rate,0.5557408928871155
translation,208,154,experimental-setup,dropout,with,label smoothing ls = 0.1,dropout with label smoothing ls = 0.1,0.5951043963432312
translation,208,154,experimental-setup,rate,of,0.1,rate of 0.1,0.6306067109107971
translation,208,154,experimental-setup,label smoothing ls = 0.1,for,regularization,label smoothing ls = 0.1 for regularization,0.5638765692710876
translation,208,154,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,208,154,experimental-setup,experimental setup,apply,label smoothing ls = 0.1,experimental setup apply label smoothing ls = 0.1,0.5560547113418579
translation,208,155,experimental-setup,input speech features,processed by,two convolutional layers,input speech features processed by two convolutional layers,0.6239937543869019
translation,208,155,experimental-setup,two convolutional layers,stride of,2 ? 2,two convolutional layers stride of 2 ? 2,0.7110639810562134
translation,208,155,experimental-setup,two convolutional layers,downsample,sequence,two convolutional layers downsample sequence,0.7085258364677429
translation,208,155,experimental-setup,sequence,by,factor of 4,sequence by factor of 4,0.6363707184791565
translation,208,155,experimental-setup,computational cost,has,input speech features,computational cost has input speech features,0.49728378653526306
translation,208,155,experimental-setup,experimental setup,For reducing,computational cost,experimental setup For reducing computational cost,0.6127395629882812
translation,208,159,experimental-setup,weight,of,ctc objective,weight of ctc objective,0.5523492097854614
translation,208,159,experimental-setup,ctc objective,for,multitask learning,ctc objective for multitask learning,0.5617177486419678
translation,208,159,experimental-setup,multitask learning,set to,0.3,multitask learning set to 0.3,0.658606767654419
translation,208,159,experimental-setup,0.3,for,all asr and st models,0.3 for all asr and st models,0.6208814382553101
translation,208,159,experimental-setup,experimental setup,has,weight,experimental setup has weight,0.5154320597648621
translation,208,160,experimental-setup,coefficients,set to,0.5,coefficients set to 0.5,0.628846287727356
translation,208,160,experimental-setup,0.5,for,mtkd method,0.5 for mtkd method,0.6518367528915405
translation,208,160,experimental-setup,experimental setup,has,coefficients,experimental setup has coefficients,0.49910613894462585
translation,208,162,experimental-setup,unrestricted setting,use,"superior architecture conformer ( gulati et al. , 2020 )","unrestricted setting use superior architecture conformer ( gulati et al. , 2020 )",0.6189588904380798
translation,208,162,experimental-setup,"superior architecture conformer ( gulati et al. , 2020 )",on,asr and st tasks,"superior architecture conformer ( gulati et al. , 2020 ) on asr and st tasks",0.5282711386680603
translation,208,162,experimental-setup,"superior architecture conformer ( gulati et al. , 2020 )",widen,model,"superior architecture conformer ( gulati et al. , 2020 ) widen model",0.7029986381530762
translation,208,162,experimental-setup,model,by increasing,hidden size,model by increasing hidden size,0.6688989400863647
translation,208,162,experimental-setup,model,by increasing,attention heads,model by increasing attention heads,0.6854121685028076
translation,208,162,experimental-setup,hidden size,to,512,hidden size to 512,0.6113746166229248
translation,208,162,experimental-setup,attention heads,to,8,attention heads to 8,0.6470775604248047
translation,208,162,experimental-setup,experimental setup,For,unrestricted setting,experimental setup For unrestricted setting,0.5381766557693481
translation,208,62,hyperparameters,connectionist temporal classification ( ctc ),as,auxiliary loss,connectionist temporal classification ( ctc ) as auxiliary loss,0.48195862770080566
translation,208,62,hyperparameters,auxiliary loss,on,encoders,auxiliary loss on encoders,0.526282787322998
translation,208,62,hyperparameters,encoders,train,asr and st systems,encoders train asr and st systems,0.6666265726089478
translation,208,7,model,stacked acoustic - and - textual encoding ( sate ) method,for,speech translation,stacked acoustic - and - textual encoding ( sate ) method for speech translation,0.604441225528717
translation,208,7,model,model,propose,stacked acoustic - and - textual encoding ( sate ) method,model propose stacked acoustic - and - textual encoding ( sate ) method,0.658819317817688
translation,208,8,model,later,behaves more like,mt encoder,later behaves more like mt encoder,0.6594286561012268
translation,208,8,model,mt encoder,for,global representation,mt encoder for global representation,0.5890151262283325
translation,208,8,model,global representation,of,input sequence,global representation of input sequence,0.5792403221130371
translation,208,10,model,adaptor module,to alleviate,representation inconsistency,adaptor module to alleviate representation inconsistency,0.6695335507392883
translation,208,10,model,adaptor module,develop,multi-teacher knowledge distillation method,adaptor module develop multi-teacher knowledge distillation method,0.5971012711524963
translation,208,10,model,representation inconsistency,between,pre-trained asr encoder and mt encoder,representation inconsistency between pre-trained asr encoder and mt encoder,0.6168034076690674
translation,208,10,model,multi-teacher knowledge distillation method,to preserve,pre-training knowledge,multi-teacher knowledge distillation method to preserve pre-training knowledge,0.60134357213974
translation,208,10,model,model,develop,adaptor module,model develop adaptor module,0.6644275188446045
translation,208,10,model,model,develop,multi-teacher knowledge distillation method,model develop multi-teacher knowledge distillation method,0.5709820985794067
translation,208,21,model,st encoder,both roles of,acoustic encoding,st encoder both roles of acoustic encoding,0.6496193408966064
translation,208,21,model,st encoder,both roles of,textual encoding,st encoder both roles of textual encoding,0.6775867342948914
translation,208,21,model,model,find that,st encoder,model find that st encoder,0.6374503970146179
translation,208,29,model,stacked acoustic- and - textual encoding ( sate ) method,to cascade,asr encoder and the mt encoder,stacked acoustic- and - textual encoding ( sate ) method to cascade asr encoder and the mt encoder,0.620583176612854
translation,208,29,model,model,propose,stacked acoustic- and - textual encoding ( sate ) method,model propose stacked acoustic- and - textual encoding ( sate ) method,0.658819317817688
translation,208,30,model,sequence of acoustic features,as,usual asr encoder,sequence of acoustic features as usual asr encoder,0.5352602005004883
translation,208,30,model,model,reads and processes,sequence of acoustic features,model reads and processes sequence of acoustic features,0.7130149006843567
translation,208,33,model,multi-teacher knowledge distillation method,to robustly train,st encoder,multi-teacher knowledge distillation method to robustly train st encoder,0.7030491828918457
translation,208,33,model,multi-teacher knowledge distillation method,preserve,pretrained knowledge,multi-teacher knowledge distillation method preserve pretrained knowledge,0.690720796585083
translation,208,33,model,pretrained knowledge,during,fine-tuning,pretrained knowledge during fine-tuning,0.6351954340934753
translation,208,33,model,model,develop,multi-teacher knowledge distillation method,model develop multi-teacher knowledge distillation method,0.5709820985794067
translation,208,156,model,encoder,consists of,12 layers,encoder consists of 12 layers,0.6667727828025818
translation,208,156,model,encoder,consists of,6 layers,encoder consists of 6 layers,0.6748580932617188
translation,208,156,model,12 layers,for,asr and vanilla st models,12 layers for asr and vanilla st models,0.6044102907180786
translation,208,156,model,12 layers,both the,asr and vanilla st models,12 layers both the asr and vanilla st models,0.6281693577766418
translation,208,156,model,12 layers,for,mt model,12 layers for mt model,0.6303580403327942
translation,208,156,model,6 layers,for,mt model,6 layers for mt model,0.6280948519706726
translation,208,156,model,model,has,encoder,model has encoder,0.5940273404121399
translation,208,157,model,encoder of sate,includes,acoustic encoder,encoder of sate includes acoustic encoder,0.6328964829444885
translation,208,157,model,encoder of sate,includes,textual encoder,encoder of sate includes textual encoder,0.6293219327926636
translation,208,157,model,acoustic encoder,of,12 layers,acoustic encoder of 12 layers,0.5549999475479126
translation,208,157,model,model,has,encoder of sate,model has encoder of sate,0.5767364501953125
translation,208,158,model,decoder,consists of,6 layers,decoder consists of 6 layers,0.665356457233429
translation,208,158,model,model,has,decoder,model has decoder,0.6226420402526855
translation,208,161,model,transformer architecture,where,each layer,transformer architecture where each layer,0.6756157279014587
translation,208,161,model,each layer,comprises,256 hidden units,each layer comprises 256 hidden units,0.6395272016525269
translation,208,161,model,each layer,comprises,4 attention heads,each layer comprises 4 attention heads,0.6750666499137878
translation,208,161,model,each layer,comprises,2048 feed -forward size,each layer comprises 2048 feed -forward size,0.6852628588676453
translation,208,36,results,sate,achieves,comparable or even better performance,sate achieves comparable or even better performance,0.6951807737350464
translation,208,36,results,comparable or even better performance,than,cascaded st counterpart,comparable or even better performance than cascaded st counterpart,0.6044604778289795
translation,208,36,results,large-scale asr and mt data,has,sate,large-scale asr and mt data has sate,0.6089801788330078
translation,208,169,results,pretrained asr and mt models,into,sate,pretrained asr and mt models into sate,0.6072853207588196
translation,208,169,results,pretrained asr and mt models,achieves,remarkable improvement,pretrained asr and mt models achieves remarkable improvement,0.6689168810844421
translation,208,169,results,sate,releases,encoding burden,sate releases encoding burden,0.7338018417358398
translation,208,169,results,encoding burden,of,model,encoding burden of model,0.5853680968284607
translation,208,169,results,results,Incorporating,pretrained asr and mt models,results Incorporating pretrained asr and mt models,0.5996708869934082
translation,208,174,results,must -c en-de,Under,unrestricted setting,must -c en-de Under unrestricted setting,0.7195614576339722
translation,208,174,results,large-scale asr and mt data,available,st data,large-scale asr and mt data available st data,0.6551110148429871
translation,208,174,results,must -c en-de,has,large-scale asr and mt data,must -c en-de has large-scale asr and mt data,0.6161105036735535
translation,208,174,results,unrestricted setting,has,large-scale asr and mt data,unrestricted setting has large-scale asr and mt data,0.5062170624732971
translation,208,174,results,results,on,must -c en-de,results on must -c en-de,0.6991147398948669
translation,208,175,results,outperforms,with,huge margin,outperforms with huge margin,0.7083204984664917
translation,208,175,results,vanilla e2e method,with,huge margin,vanilla e2e method with huge margin,0.6475132703781128
translation,208,175,results,huge margin,of,4.5 bleu points,huge margin of 4.5 bleu points,0.5558228492736816
translation,208,175,results,cascaded method,has,outperforms,cascaded method has outperforms,0.6162976622581482
translation,208,175,results,outperforms,has,vanilla e2e method,outperforms has vanilla e2e method,0.5959398746490479
translation,208,175,results,results,leads to,cascaded method,results leads to cascaded method,0.6876054406166077
translation,208,177,results,sate,incorporates,pre-trained models,sate incorporates pre-trained models,0.7166891098022461
translation,208,177,results,sate,achieves,significant improvement,sate achieves significant improvement,0.7221097350120544
translation,208,177,results,pre-trained models,achieves,significant improvement,pre-trained models achieves significant improvement,0.6439815759658813
translation,208,177,results,significant improvement,of,3.7 bleu points,significant improvement of 3.7 bleu points,0.5166134834289551
translation,208,177,results,pre-trained models,has,fully,pre-trained models has fully,0.554740846157074
translation,208,177,results,results,has,sate,results has sate,0.6021808981895447
translation,208,178,results,mtkd and specaugment methods,achieve,comparable performance,mtkd and specaugment methods achieve comparable performance,0.6150667667388916
translation,208,178,results,comparable performance,of,28.1 bleu points,comparable performance of 28.1 bleu points,0.5203924775123596
translation,208,178,results,results,With,mtkd and specaugment methods,results With mtkd and specaugment methods,0.5771625638008118
translation,208,182,results,pre-training,helps,model,pre-training helps model,0.6556944847106934
translation,208,182,results,model,achieve,improvement,model achieve improvement,0.6344038248062134
translation,208,182,results,improvement,of,0.8 bleu points,improvement of 0.8 bleu points,0.5501246452331543
translation,208,182,results,0.8 bleu points,over,cascaded baseline,0.8 bleu points over cascaded baseline,0.6202902793884277
translation,208,182,results,results,has,pre-training,results has pre-training,0.528938889503479
translation,208,183,results,sate without pre-training,achieves,slight improvement,sate without pre-training achieves slight improvement,0.6659994721412659
translation,208,183,results,above methods,achieves,slight improvement,above methods achieves slight improvement,0.6210876703262329
translation,208,183,results,slight improvement,than,mt model,slight improvement than mt model,0.5826120376586914
translation,208,183,results,sate without pre-training,has,outperforms,sate without pre-training has outperforms,0.6241061687469482
translation,208,183,results,outperforms,has,above methods,outperforms has above methods,0.5749467611312866
translation,208,183,results,above methods,has,significantly,above methods has significantly,0.5820100903511047
translation,208,183,results,results,has,sate without pre-training,results has sate without pre-training,0.5865339040756226
translation,208,186,results,librispeech en-fr,Combining,our proposed methods,librispeech en-fr Combining our proposed methods,0.7177679538726807
translation,208,186,results,librispeech en-fr,yields,substantial improvement,librispeech en-fr yields substantial improvement,0.7381844520568848
translation,208,186,results,our proposed methods,yields,substantial improvement,our proposed methods yields substantial improvement,0.6627753376960754
translation,208,186,results,substantial improvement,of,2.0 bleu points,substantial improvement of 2.0 bleu points,0.5287527441978455
translation,208,186,results,2.0 bleu points,over,cascaded baseline,2.0 bleu points over cascaded baseline,0.6216567754745483
translation,208,186,results,results,on,librispeech en-fr,results on librispeech en-fr,0.6376053690910339
translation,208,188,results,cascaded counterpart,by,0.2 bleu points,cascaded counterpart by 0.2 bleu points,0.5869765281677246
translation,208,188,results,0.2 bleu points,on,unrestricted task,0.2 bleu points on unrestricted task,0.49012434482574463
translation,208,188,results,outperform,has,cascaded counterpart,outperform has cascaded counterpart,0.6089780330657959
translation,208,188,results,results,has,outperform,results has outperform,0.642206609249115
translation,208,191,results,vanilla e2e st model,yields,inference speedup,vanilla e2e st model yields inference speedup,0.733856737613678
translation,208,191,results,vanilla e2e st model,demonstrates,low latency,vanilla e2e st model demonstrates low latency,0.6697546243667603
translation,208,191,results,inference speedup,of,1.91,inference speedup of 1.91,0.5576244592666626
translation,208,191,results,inference speedup,than,cascaded counterpart,inference speedup than cascaded counterpart,0.5840172171592712
translation,208,191,results,1.91,than,cascaded counterpart,1.91 than cascaded counterpart,0.569247841835022
translation,208,191,results,results,has,vanilla e2e st model,results has vanilla e2e st model,0.5224434733390808
translation,208,194,results,our method,reaches up to,1.69 ? speedup,our method reaches up to 1.69 ? speedup,0.6791566610336304
translation,208,194,results,performance,of,1.9 bleu points,performance of 1.9 bleu points,0.531447172164917
translation,208,194,results,1.69 ? speedup,than,cascaded baseline,1.69 ? speedup than cascaded baseline,0.5343179702758789
translation,208,194,results,improves,has,performance,improves has performance,0.5770372748374939
translation,208,194,results,results,has,our method,results has our method,0.5589964985847473
translation,208,201,results,effect,of,pretrained mt model,effect of pretrained mt model,0.5886228680610657
translation,208,201,results,pretrained mt model,is,more remarkable,pretrained mt model is more remarkable,0.5409725308418274
translation,208,201,results,more remarkable,on,lib-rispeech corpus,more remarkable on lib-rispeech corpus,0.49881792068481445
translation,208,201,results,more remarkable,due to,modeling burden,more remarkable due to modeling burden,0.6494477391242981
translation,208,201,results,lib-rispeech corpus,due to,modeling burden,lib-rispeech corpus due to modeling burden,0.619153618812561
translation,208,201,results,modeling burden,on,translation,modeling burden on translation,0.595389187335968
translation,208,201,results,results,has,effect,results has effect,0.5185524821281433
translation,208,211,results,mapping method,achieves,slight improvement,mapping method achieves slight improvement,0.6581718921661377
translation,208,211,results,slight improvement,by transforming,acoustic representation,slight improvement by transforming acoustic representation,0.6732490062713623
translation,208,211,results,acoustic representation,to,textual representation,acoustic representation to textual representation,0.49490997195243835
translation,208,211,results,results,has,mapping method,results has mapping method,0.5378661751747131
translation,208,212,results,soft and mapping representation,enriches,information,soft and mapping representation enriches information,0.7399427890777588
translation,208,212,results,soft and mapping representation,avoids,representation inconsistency issue,soft and mapping representation avoids representation inconsistency issue,0.5721865296363831
translation,208,212,results,soft and mapping representation,achieves,best performances,soft and mapping representation achieves best performances,0.6651241779327393
translation,208,212,results,representation inconsistency issue,achieves,best performances,representation inconsistency issue achieves best performances,0.6361098885536194
translation,208,212,results,results,Fusing,soft and mapping representation,results Fusing soft and mapping representation,0.7198757529258728
translation,209,36,ablation-analysis,authentic source training data,has,increases,authentic source training data has increases,0.5657927989959717
translation,209,36,ablation-analysis,increases,has,below 50 %,increases has below 50 %,0.6217751502990723
translation,209,36,ablation-analysis,data,has,increases,data has increases,0.6232753396034241
translation,209,36,ablation-analysis,ablation analysis,has,translation quality,ablation analysis has translation quality,0.5040645599365234
translation,209,62,experimental-setup,"transformer ( vaswani et al. , 2017 ) models",using,"sockeye -1.18.115 ( hieber et al. , 2018 )","transformer ( vaswani et al. , 2017 ) models using sockeye -1.18.115 ( hieber et al. , 2018 )",0.649228572845459
translation,209,62,experimental-setup,"transformer ( vaswani et al. , 2017 ) models",with,8 attention heads,"transformer ( vaswani et al. , 2017 ) models with 8 attention heads",0.6179118752479553
translation,209,62,experimental-setup,"sockeye -1.18.115 ( hieber et al. , 2018 )",with,6 layers,"sockeye -1.18.115 ( hieber et al. , 2018 ) with 6 layers",0.6040459275245667
translation,209,62,experimental-setup,"sockeye -1.18.115 ( hieber et al. , 2018 )",with,8 attention heads,"sockeye -1.18.115 ( hieber et al. , 2018 ) with 8 attention heads",0.6294238567352295
translation,209,62,experimental-setup,network size,of,512 units,network size of 512 units,0.6204918026924133
translation,209,62,experimental-setup,feedforward size,of,2048 units,feedforward size of 2048 units,0.6064908504486084
translation,209,62,experimental-setup,experimental setup,build,"transformer ( vaswani et al. , 2017 ) models","experimental setup build transformer ( vaswani et al. , 2017 ) models",0.645627498626709
translation,209,63,experimental-setup,default gradient clipping type,to,absolute,default gradient clipping type to absolute,0.5013195872306824
translation,209,63,experimental-setup,default gradient clipping type,to,initial learning rate,default gradient clipping type to initial learning rate,0.48522475361824036
translation,209,63,experimental-setup,default gradient clipping type,to,batches,default gradient clipping type to batches,0.5162692070007324
translation,209,63,experimental-setup,default gradient clipping type,used,source - target soft-max weight tying,default gradient clipping type used source - target soft-max weight tying,0.5758363008499146
translation,209,63,experimental-setup,default gradient clipping type,optimizing for,bleu,default gradient clipping type optimizing for bleu,0.6782269477844238
translation,209,63,experimental-setup,initial learning rate,of,0.0002,initial learning rate of 0.0002,0.5694879293441772
translation,209,63,experimental-setup,batches,optimizing for,bleu,batches optimizing for bleu,0.6423942446708679
translation,209,63,experimental-setup,maximum sentence length,of,200 tokens,maximum sentence length of 200 tokens,0.5668289065361023
translation,209,63,experimental-setup,checkpoint intervals,of,4000,checkpoint intervals of 4000,0.6495473384857178
translation,209,63,experimental-setup,early stopping,after,32 checkpoints without improvement,early stopping after 32 checkpoints without improvement,0.6878155469894409
translation,209,63,experimental-setup,source - target soft-max weight tying,has,initial learning rate,source - target soft-max weight tying has initial learning rate,0.5106314420700073
translation,209,63,experimental-setup,experimental setup,changed,default gradient clipping type,experimental setup changed default gradient clipping type,0.6548540592193604
translation,209,63,experimental-setup,experimental setup,changed,early stopping,experimental setup changed early stopping,0.6740573048591614
translation,209,64,experimental-setup,decoding,used,beam size,decoding used beam size,0.6021908521652222
translation,209,64,experimental-setup,beam size,has,5,beam size has 5,0.6528496742248535
translation,209,64,experimental-setup,experimental setup,has,decoding,experimental setup has decoding,0.5178583860397339
translation,209,34,results,q1,find that,systems,q1 find that systems,0.7086180448532104
translation,209,34,results,systems,trained exclusively with,authentic source data,systems trained exclusively with authentic source data,0.6680700182914734
translation,209,34,results,outperform,by,large margin,outperform by large margin,0.6385749578475952
translation,209,34,results,large margin,those trained exclusively with,translationese source data,large margin those trained exclusively with translationese source data,0.6350660920143127
translation,209,35,results,authentic and translationese source,not always produce,significantly better systems,authentic and translationese source not always produce significantly better systems,0.7540366649627686
translation,209,35,results,significantly better systems,compared to using,authentic source,significantly better systems compared to using authentic source,0.7287662029266357
translation,209,35,results,translationese source,in,training and tuning data ( q2 ),translationese source in training and tuning data ( q2 ),0.5333011150360107
translation,209,35,results,tagging,has,translationese source,tagging has translationese source,0.5841778516769409
translation,209,35,results,improve,has,performance,improve has performance,0.5578044652938843
translation,209,35,results,results,Combining,authentic and translationese source,results Combining authentic and translationese source,0.6725174188613892
translation,209,107,results,both translation directions,using,authentic source data,both translation directions using authentic source data,0.6861417293548584
translation,209,107,results,authentic source data,for,training,authentic source data for training,0.6290484666824341
translation,209,107,results,authentic source data,outperforms using,translationese source data,authentic source data outperforms using translationese source data,0.7612661719322205
translation,209,107,results,translationese source data,by,difference,translationese source data by difference,0.5926474928855896
translation,209,107,results,translationese source data,by,difference,translationese source data by difference,0.5926474928855896
translation,209,107,results,difference,of,4.8 bleu,difference of 4.8 bleu,0.551443338394165
translation,209,107,results,difference,of,4.0,difference of 4.0,0.5976230502128601
translation,209,107,results,difference,of,4.0,difference of 4.0,0.5976230502128601
translation,209,107,results,4.8 bleu,in,en - fr direction,4.8 bleu in en - fr direction,0.5884197950363159
translation,209,107,results,difference,of,4.0,difference of 4.0,0.5976230502128601
translation,209,107,results,4.0,in,fr -en direction,4.0 in fr -en direction,0.6555286645889282
translation,209,107,results,results,in,both translation directions,results in both translation directions,0.478345662355423
translation,209,110,results,performance,training with,mixed data,performance training with mixed data,0.7787549495697021
translation,209,110,results,mixed data,is,very comparable,mixed data is very comparable,0.5380593538284302
translation,209,110,results,results,has,performance,results has performance,0.5972660779953003
translation,209,111,results,en - fr direction,in,fr - en direction,en - fr direction in fr - en direction,0.5506666898727417
translation,209,111,results,difference,of,0.2 bleu,difference of 0.2 bleu,0.5655393600463867
translation,209,111,results,difference,of,0.08 bleu,difference of 0.08 bleu,0.5603046417236328
translation,209,111,results,difference,of,0.08 bleu,difference of 0.08 bleu,0.5603046417236328
translation,209,111,results,0.2 bleu,in favor of,mixed training data,0.2 bleu in favor of mixed training data,0.5892598032951355
translation,209,111,results,very small difference,of,0.08 bleu,very small difference of 0.08 bleu,0.5514017939567566
translation,209,111,results,results,In,en - fr direction,results In en - fr direction,0.5559250116348267
translation,209,111,results,results,in,fr - en direction,results in fr - en direction,0.548780620098114
translation,209,112,results,pairwise bootstrap resampling,of,bleu scores,pairwise bootstrap resampling of bleu scores,0.5623014569282532
translation,209,112,results,en - fr mixed system,is,significantly better,en - fr mixed system is significantly better,0.6169826984405518
translation,209,112,results,significantly better,than,authentic only system,significantly better than authentic only system,0.6039167046546936
translation,209,112,results,performing better,in,95.7 %,performing better in 95.7 %,0.5038897395133972
translation,209,112,results,pairwise bootstrap resampling,has,en - fr mixed system,pairwise bootstrap resampling has en - fr mixed system,0.5932927131652832
translation,209,112,results,bleu scores,has,en - fr mixed system,bleu scores has en - fr mixed system,0.5343121886253357
translation,209,112,results,mixed system,has,performing better,mixed system has performing better,0.5997896790504456
translation,209,112,results,results,According to,pairwise bootstrap resampling,results According to pairwise bootstrap resampling,0.6716104745864868
translation,209,113,results,bleu difference,between,mixed and authentic,bleu difference between mixed and authentic,0.6712613105773926
translation,209,113,results,fr -en direction,has,bleu difference,fr -en direction has bleu difference,0.584550142288208
translation,209,113,results,results,In,fr -en direction,results In fr -en direction,0.548780620098114
translation,209,114,results,chrf,shows,small gain,chrf shows small gain,0.6899337768554688
translation,209,114,results,chrf,in,fr - en direction,chrf in fr - en direction,0.553943395614624
translation,209,114,results,small gain,for,mixed training data,small gain for mixed training data,0.6534580588340759
translation,209,114,results,authentic source,has,very small advantage,authentic source has very small advantage,0.5551735162734985
translation,209,114,results,en - fr direction,has,chrf,en - fr direction has chrf,0.5996429920196533
translation,209,114,results,en - fr direction,has,authentic source,en - fr direction has authentic source,0.581177294254303
translation,209,114,results,fr - en direction,has,authentic source,fr - en direction has authentic source,0.5943701863288879
translation,209,114,results,authentic source,has,very small advantage,authentic source has very small advantage,0.5551735162734985
translation,209,114,results,results,In,en - fr direction,results In en - fr direction,0.5559250116348267
translation,209,114,results,results,in,fr - en direction,results in fr - en direction,0.548780620098114
translation,209,114,results,results,has,chrf,results has chrf,0.5315447449684143
translation,209,136,results,tagging,is,stronger,tagging is stronger,0.6333770155906677
translation,209,136,results,stronger,in,fr - en translation direction,stronger in fr - en translation direction,0.5337850451469421
translation,209,136,results,bleu score increase,of,0.6,bleu score increase of 0.6,0.5767158269882202
translation,209,142,results,fr -en direction,see,additional 0.3 bleu improvement,fr -en direction see additional 0.3 bleu improvement,0.545544445514679
translation,209,142,results,additional 0.3 bleu improvement,when using,tagged mixed + mixdev,additional 0.3 bleu improvement when using tagged mixed + mixdev,0.6898210048675537
translation,209,142,results,results,In,fr -en direction,results In fr -en direction,0.548780620098114
translation,209,143,results,corresponding change,in,chrf,corresponding change in chrf,0.6086004972457886
translation,209,143,results,results,see,small 0.1 bleu improvement,results see small 0.1 bleu improvement,0.5436306595802307
translation,209,145,results,fr -en direction,both,tagged mixed and tagged mixed + mixdev systems,fr -en direction both tagged mixed and tagged mixed + mixdev systems,0.7338084578514099
translation,209,145,results,tagged mixed and tagged mixed + mixdev systems,found to be,significantly better,tagged mixed and tagged mixed + mixdev systems found to be significantly better,0.6468897461891174
translation,209,145,results,significantly better,in terms of,bleu,significantly better in terms of bleu,0.6577858328819275
translation,209,145,results,significantly better,according to,pairwise bootstrap resampling,significantly better according to pairwise bootstrap resampling,0.6843299865722656
translation,209,145,results,bleu,than,mixed ( untagged ) system,bleu than mixed ( untagged ) system,0.5606598854064941
translation,209,145,results,results,in,fr -en direction,results in fr -en direction,0.548780620098114
translation,209,146,results,paired bootstrap resampling,finds that,fr -en direction,paired bootstrap resampling finds that fr -en direction,0.7090734839439392
translation,209,146,results,paired bootstrap resampling,in,fr -en direction,paired bootstrap resampling in fr -en direction,0.6022019982337952
translation,209,146,results,fr -en direction,is,significantly better,fr -en direction is significantly better,0.6081181168556213
translation,209,146,results,significantly better,in terms of,bleu,significantly better in terms of bleu,0.6577858328819275
translation,209,146,results,bleu,than,tagged mixed system,bleu than tagged mixed system,0.614495038986206
translation,209,146,results,fr -en direction,has,tagged mixed + mixdev system,fr -en direction has tagged mixed + mixdev system,0.6515450477600098
translation,209,146,results,results,has,paired bootstrap resampling,results has paired bootstrap resampling,0.5224428176879883
translation,209,163,results,translation quality,on,authentic source test data,translation quality on authentic source test data,0.5050584077835083
translation,209,163,results,translation quality,increases as,percentage,translation quality increases as percentage,0.5525602102279663
translation,209,163,results,percentage,of,authentic source training data,percentage of authentic source training data,0.6113319396972656
translation,209,163,results,authentic source training data,has,increases,authentic source training data has increases,0.5657927989959717
translation,210,34,ablation-analysis,fine- tuning,on,new data,fine- tuning on new data,0.55885249376297
translation,210,34,ablation-analysis,strong bias,towards,new words,strong bias towards new words,0.6547183990478516
translation,210,34,ablation-analysis,new words,in,output embedding layer,new words in output embedding layer,0.49237194657325745
translation,210,34,ablation-analysis,new words,of,decoder,new words of decoder,0.6601961255073547
translation,210,34,ablation-analysis,new words,results in,bias,new words results in bias,0.6844567060470581
translation,210,34,ablation-analysis,decoder,results in,bias,decoder results in bias,0.6546676754951477
translation,210,34,ablation-analysis,ablation analysis,when,fine- tuning,ablation analysis when fine- tuning,0.6424018740653992
translation,210,192,ablation-analysis,performance,improved,proposed dynamic knowledge distillation techniques,performance improved proposed dynamic knowledge distillation techniques,0.7215834259986877
translation,210,192,ablation-analysis,performance,when incorporating,knowledge distillation,performance when incorporating knowledge distillation,0.6970376968383789
translation,210,192,ablation-analysis,performance,when incorporating,ewc regularization,performance when incorporating ewc regularization,0.6439827680587769
translation,210,192,ablation-analysis,performance,when incorporating,proposed dynamic knowledge distillation techniques,performance when incorporating proposed dynamic knowledge distillation techniques,0.6889563202857971
translation,210,192,ablation-analysis,proposed dynamic knowledge distillation techniques,into,fine-tuning process,proposed dynamic knowledge distillation techniques into fine-tuning process,0.5709857940673828
translation,210,192,ablation-analysis,proposed dynamic knowledge distillation techniques,shows,learning - without - forgetting strategies,proposed dynamic knowledge distillation techniques shows learning - without - forgetting strategies,0.6241148710250854
translation,210,192,ablation-analysis,learning - without - forgetting strategies,help,model,learning - without - forgetting strategies help model,0.6756300926208496
translation,210,192,ablation-analysis,model,remember,general knowledge,model remember general knowledge,0.7578698992729187
translation,210,192,ablation-analysis,model,benefit,fine-tuning,model benefit fine-tuning,0.7017566561698914
translation,210,192,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,210,211,ablation-analysis,fine-tuning,on,new domain,fine-tuning on new domain,0.5669809579849243
translation,210,211,ablation-analysis,fine-tuning,occurrence of,catastrophic forgetting,fine-tuning occurrence of catastrophic forgetting,0.6501144170761108
translation,210,211,ablation-analysis,new domain,that contains,more training samples,new domain that contains more training samples,0.6263099312782288
translation,210,211,ablation-analysis,catastrophic forgetting,would be,more obvious,catastrophic forgetting would be more obvious,0.709110677242279
translation,210,211,ablation-analysis,our method,gain,more improvements,our method gain more improvements,0.7217697501182556
translation,210,211,ablation-analysis,ablation analysis,show,fine-tuning,ablation analysis show fine-tuning,0.615489661693573
translation,210,211,ablation-analysis,ablation analysis,when,fine-tuning,ablation analysis when fine-tuning,0.6424018740653992
translation,210,227,ablation-analysis,bias correction module,contribute to,improvement,bias correction module contribute to improvement,0.6913273334503174
translation,210,227,ablation-analysis,improvement,of,results,improvement of results,0.6296150088310242
translation,210,227,ablation-analysis,ablation analysis,show,dynamic knowledge distillation,ablation analysis show dynamic knowledge distillation,0.6354026794433594
translation,210,227,ablation-analysis,ablation analysis,show,bias correction module,ablation analysis show bias correction module,0.6369637846946716
translation,210,229,ablation-analysis,result,of,model,result of model,0.5978901386260986
translation,210,229,ablation-analysis,result,drops by,0.9- 2.1 bleu scores,result drops by 0.9- 2.1 bleu scores,0.7064530253410339
translation,210,229,ablation-analysis,model,drops by,0.9- 2.1 bleu scores,model drops by 0.9- 2.1 bleu scores,0.7097377181053162
translation,210,229,ablation-analysis,bias correction module,has,result,bias correction module has result,0.5546373128890991
translation,210,229,ablation-analysis,ablation analysis,After removing,bias correction module,ablation analysis After removing bias correction module,0.7043567895889282
translation,210,236,ablation-analysis,results of ablation study,show,bias correction module,results of ablation study show bias correction module,0.6258209347724915
translation,210,236,ablation-analysis,bias correction module,is,less beneficial,bias correction module is less beneficial,0.5745633244514465
translation,210,236,ablation-analysis,less beneficial,for,model,less beneficial for model,0.641325056552887
translation,210,236,ablation-analysis,removal,of,bias correction module,removal of bias correction module,0.5897897481918335
translation,210,236,ablation-analysis,removal,results in,decrease,removal results in decrease,0.651601254940033
translation,210,236,ablation-analysis,decrease,of,0.1- 0.2 bleu score,decrease of 0.1- 0.2 bleu score,0.5986481308937073
translation,210,236,ablation-analysis,0.1- 0.2 bleu score,to,performance,0.1- 0.2 bleu score to performance,0.5439220666885376
translation,210,236,ablation-analysis,ablation analysis,show,bias correction module,ablation analysis show bias correction module,0.6369637846946716
translation,210,236,ablation-analysis,ablation analysis,has,results of ablation study,ablation analysis has results of ablation study,0.5103171467781067
translation,210,143,experimental-setup,memory size,set to,50,memory size set to 50,0.7458284497261047
translation,210,143,experimental-setup,50,",",000,"50 , 000",0.6518385410308838
translation,210,143,experimental-setup,experimental setup,has,memory size,experimental setup has memory size,0.498731791973114
translation,210,177,experimental-setup,"fairseq toolkit ( ott et al. , 2019 )",to implement,proposed model,"fairseq toolkit ( ott et al. , 2019 ) to implement proposed model",0.6588617563247681
translation,210,177,experimental-setup,experimental setup,use,"fairseq toolkit ( ott et al. , 2019 )","experimental setup use fairseq toolkit ( ott et al. , 2019 )",0.5555488467216492
translation,210,178,experimental-setup,text,into,subword units,text into subword units,0.6192895174026489
translation,210,178,experimental-setup,subword units,by using,subword- nmt toolkit,subword units by using subword- nmt toolkit,0.6662383079528809
translation,210,178,experimental-setup,experimental setup,process,text,experimental setup process text,0.6795642971992493
translation,210,180,experimental-setup,"model 's hidden size , feed - forward hidden size",to,"512 , 2048","model 's hidden size , feed - forward hidden size to 512 , 2048",0.5808099508285522
translation,210,180,experimental-setup,number of layers and the number of heads,to,6 and 8,number of layers and the number of heads to 6 and 8,0.5672407746315002
translation,210,180,experimental-setup,experimental setup,set,"model 's hidden size , feed - forward hidden size","experimental setup set model 's hidden size , feed - forward hidden size",0.6551094651222229
translation,210,180,experimental-setup,experimental setup,set,number of layers and the number of heads,experimental setup set number of layers and the number of heads,0.626612663269043
translation,210,180,experimental-setup,experimental setup,set,number of layers and the number of heads,experimental setup set number of layers and the number of heads,0.626612663269043
translation,210,181,experimental-setup,same configuration,for,all encoders and decoders,same configuration for all encoders and decoders,0.6094653010368347
translation,210,181,experimental-setup,experimental setup,use,same configuration,experimental setup use same configuration,0.628749668598175
translation,210,183,experimental-setup,warm - up learning rate,for,"first 3,000 steps","warm - up learning rate for first 3,000 steps",0.6110014319419861
translation,210,183,experimental-setup,initial warm - up learning rate,set to,1e - 7,initial warm - up learning rate set to 1e - 7,0.7072030305862427
translation,210,183,experimental-setup,warm - up learning rate,has,initial warm - up learning rate,warm - up learning rate has initial warm - up learning rate,0.5626178979873657
translation,210,184,experimental-setup,dropout technique,set,dropout rate,dropout technique set dropout rate,0.6334943771362305
translation,210,184,experimental-setup,dropout rate,to,0.4,dropout rate to 0.4,0.5363330841064453
translation,210,184,experimental-setup,experimental setup,use,dropout technique,experimental setup use dropout technique,0.6161073446273804
translation,210,184,experimental-setup,experimental setup,set,dropout rate,experimental setup set dropout rate,0.62116938829422
translation,210,185,experimental-setup,beam search,for,inference,beam search for inference,0.5969042181968689
translation,210,185,experimental-setup,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,210,7,model,new continual learning framework,for,nmt models,new continual learning framework for nmt models,0.6096462607383728
translation,210,7,model,model,propose,new continual learning framework,model propose new continual learning framework,0.7208439707756042
translation,210,8,model,training,comprised of,multiple stages,training comprised of multiple stages,0.6687576770782471
translation,210,8,model,training,propose,dynamic knowledge distillation technique,training propose dynamic knowledge distillation technique,0.5883393287658691
translation,210,33,model,catastrophic forgetting,in,systematic and principled way,catastrophic forgetting in systematic and principled way,0.5188837051391602
translation,210,33,model,model,propose,dynamic knowledge distillation - based method,model propose dynamic knowledge distillation - based method,0.6312167644500732
translation,210,35,model,model,with,bias-correction module,model with bias-correction module,0.6171781420707703
translation,210,35,model,bias-correction module,normalizes,weights,bias-correction module normalizes weights,0.7859017848968506
translation,210,35,model,weights,in,projection layer,weights in projection layer,0.4914126694202423
translation,210,35,model,model,incorporate,model,model incorporate model,0.6846771836280823
translation,210,35,model,model,with,bias-correction module,model with bias-correction module,0.6171781420707703
translation,210,41,model,catastrophic forgetting,in,systematic way,catastrophic forgetting in systematic way,0.5293707847595215
translation,210,41,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,210,142,model,data,stored in,memory,data stored in memory,0.6987596154212952
translation,210,142,model,new data,fed to,model,new data fed to model,0.7283461689949036
translation,210,142,model,model,at,each stage,model at each stage,0.5569782853126526
translation,210,142,model,model,has,data,model has data,0.5736815929412842
translation,210,175,model,bias correction module,from,proposed model,bias correction module from proposed model,0.5529279708862305
translation,210,175,model,model,removes,bias correction module,model removes bias correction module,0.7084648013114929
translation,210,179,model,"transformer ( vaswani et al. , 2017 )",as,model architecture,"transformer ( vaswani et al. , 2017 ) as model architecture",0.5230960845947266
translation,210,179,model,model,adopt,"transformer ( vaswani et al. , 2017 )","model adopt transformer ( vaswani et al. , 2017 )",0.6552146673202515
translation,210,228,model,bias correction module,is,simple,bias correction module is simple,0.5551814436912537
translation,210,228,model,bias correction module,plays,very important role,bias correction module plays very important role,0.6999531388282776
translation,210,228,model,very important role,in,proposed model,very important role in proposed model,0.543896496295929
translation,210,193,results,improvement,is,less significant,improvement is less significant,0.6013733148574829
translation,210,193,results,less significant,for,ewc - based model,less significant for ewc - based model,0.6514203548431396
translation,210,193,results,results,has,improvement,results has improvement,0.6248279809951782
translation,210,194,results,results,of,our model,results of our model,0.6090980172157288
translation,210,194,results,results,of,our model,results of our model,0.6090980172157288
translation,210,194,results,results,see that,our model,results see that our model,0.6820751428604126
translation,210,194,results,our model,with,knowledge distillation - based and ewc regularization - based methods,our model with knowledge distillation - based and ewc regularization - based methods,0.6182523965835571
translation,210,194,results,our model,see that,our model,our model see that our model,0.6230370998382568
translation,210,194,results,our model,see that,outperforms,our model see that outperforms,0.7485296130180359
translation,210,194,results,outperforms,in,all cases,outperforms in all cases,0.5688782334327698
translation,210,194,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,210,194,results,results,comparing,results,results comparing results,0.5670907497406006
translation,210,194,results,results,comparing,our model,results comparing our model,0.7061519026756287
translation,210,194,results,results,of,our model,results of our model,0.6090980172157288
translation,210,195,results,proposed model,achieves,average improvement,proposed model achieves average improvement,0.6873819828033447
translation,210,195,results,average improvement,of,0.3 and 0.8 bleu scores,average improvement of 0.3 and 0.8 bleu scores,0.5507140159606934
translation,210,195,results,0.3 and 0.8 bleu scores,compared to,knowledge distillation - based and ewc regularization - based methods,0.3 and 0.8 bleu scores compared to knowledge distillation - based and ewc regularization - based methods,0.5905446410179138
translation,210,195,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,210,196,results,learning - without - forgetting strategies,demonstrate,proposed method,learning - without - forgetting strategies demonstrate proposed method,0.5901749730110168
translation,210,196,results,benefit,has,continual training,benefit has continual training,0.5802138447761536
translation,210,196,results,results,confirm,finding,results confirm finding,0.49713781476020813
translation,210,196,results,results,that,learning - without - forgetting strategies,results that learning - without - forgetting strategies,0.6157162189483643
translation,210,207,results,all competitive models,at,any stage,all competitive models at any stage,0.5459249019622803
translation,210,207,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,210,207,results,outperforms,has,all competitive models,outperforms has all competitive models,0.5756316184997559
translation,210,208,results,proposed method,to,fine-tuning,proposed method to fine-tuning,0.5835510492324829
translation,210,208,results,fine-tuning,bring,improvement,fine-tuning bring improvement,0.6573666930198669
translation,210,208,results,improvement,of,3 - 4 bleu scores,improvement of 3 - 4 bleu scores,0.572117030620575
translation,210,208,results,3 - 4 bleu scores,in,general domain,3 - 4 bleu scores in general domain,0.49285900592803955
translation,210,208,results,results,Incorporating,proposed method,results Incorporating proposed method,0.6883881688117981
translation,210,212,results,results,on,general domain,results on general domain,0.5939735770225525
translation,210,212,results,improvement,lower than,ours,improvement lower than ours,0.7642365097999573
translation,210,212,results,improve,has,results,improve has results,0.5343002676963806
translation,210,212,results,results,has,knowledge distillation - based method,results has knowledge distillation - based method,0.5722615718841553
translation,210,217,results,our method,lead to,fewer changes,our method lead to fewer changes,0.7212830185890198
translation,210,217,results,fewer changes,compared to,baseline model ( 0.16 vs. 0.21 ),fewer changes compared to baseline model ( 0.16 vs. 0.21 ),0.6161642074584961
translation,210,217,results,results,find that,our method,results find that our method,0.6377817392349243
translation,210,222,results,our model,performs,slightly better or at least comparable,our model performs slightly better or at least comparable,0.5757204294204712
translation,210,222,results,slightly better or at least comparable,to,model,slightly better or at least comparable to model,0.5836717486381531
translation,210,222,results,model,directly fine -tuned on,new data,model directly fine -tuned on new data,0.7592750191688538
translation,210,222,results,results,show,our model,results show our model,0.6888449192047119
translation,210,230,results,results,has,time-incremental training,results has time-incremental training,0.5468660593032837
translation,210,233,results,competitive models,in,most cases,competitive models in most cases,0.5585175156593323
translation,210,233,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,210,233,results,outperforms,has,competitive models,outperforms has competitive models,0.5938325524330139
translation,210,233,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,210,234,results,improvement,of,0.3- 0.8 bleu scores,improvement of 0.3- 0.8 bleu scores,0.5715348720550537
translation,210,234,results,improvement,of,0 - 0.3 bleu scores,improvement of 0 - 0.3 bleu scores,0.5630469918251038
translation,210,234,results,improvement,of,0.2- 0.5 bleu scores,improvement of 0.2- 0.5 bleu scores,0.5782089829444885
translation,210,234,results,0.3- 0.8 bleu scores,over,finetuned model,0.3- 0.8 bleu scores over finetuned model,0.6315902471542358
translation,210,234,results,0 - 0.3 bleu scores,over,knowledge distillation - based model,0 - 0.3 bleu scores over knowledge distillation - based model,0.6241390705108643
translation,210,234,results,0.2- 0.5 bleu scores,over,ewc regularization - based model,0.2- 0.5 bleu scores over ewc regularization - based model,0.620735764503479
translation,210,235,results,proposed method,for,continual training,proposed method for continual training,0.6557353138923645
translation,210,235,results,proposed method,is,effective,proposed method is effective,0.6211891770362854
translation,210,235,results,continual training,is,effective,continual training is effective,0.5702667832374573
translation,210,235,results,results,show,proposed method,results show proposed method,0.6722329258918762
translation,211,184,baselines,english-centric bilingual baselines,are,transformer big 6 - 6 models,english-centric bilingual baselines are transformer big 6 - 6 models,0.5540823340415955
translation,211,184,baselines,transformer big 6 - 6 models,trained on,unpc,transformer big 6 - 6 models trained on unpc,0.7337708473205566
translation,211,184,baselines,unpc,for,120k steps,unpc for 120k steps,0.672771692276001
translation,211,184,baselines,unpc,with,joint bpe vocabularies,unpc with joint bpe vocabularies,0.6610314249992371
translation,211,184,baselines,joint bpe vocabularies,of size,16 k,joint bpe vocabularies of size 16 k,0.739303708076477
translation,211,184,baselines,baselines,has,english-centric bilingual baselines,baselines has english-centric bilingual baselines,0.5694142580032349
translation,211,183,experimental-setup,chinese data,tokenized with,jieba,chinese data tokenized with jieba,0.7721273899078369
translation,211,183,experimental-setup,experimental setup,has,chinese data,experimental setup has chinese data,0.5357370972633362
translation,211,182,experiments,monolingual bpe models,of size,8k,monolingual bpe models of size 8k,0.6842333674430847
translation,211,4,model,technique,adding,new source or target language,technique adding new source or target language,0.7119033932685852
translation,211,4,model,new source or target language,to,existing multilingual nmt model,new source or target language to existing multilingual nmt model,0.5334503054618835
translation,211,4,model,model,adding,new source or target language,model adding new source or target language,0.7096385955810547
translation,211,161,results,incremental training,is,more data-efficient,incremental training is more data-efficient,0.5590386390686035
translation,211,161,results,incremental training,can achieve,decent performance,incremental training can achieve decent performance,0.6928116679191589
translation,211,161,results,more data-efficient,than,bilingual models,more data-efficient than bilingual models,0.5331429839134216
translation,211,161,results,decent performance,with,tiny amounts of training data,decent performance with tiny amounts of training data,0.6372810006141663
translation,211,161,results,results,observe,incremental training,results observe incremental training,0.5916624665260315
translation,211,163,results,our approach,reaches,higher performance,our approach reaches higher performance,0.6940491199493408
translation,211,163,results,higher performance,in,much fewer updates,higher performance in much fewer updates,0.5367549061775208
translation,211,163,results,much fewer updates,than,alternatives,much fewer updates than alternatives,0.6441816091537476
translation,211,199,results,results,has,multi-parallel model,results has multi-parallel model,0.5502533316612244
translation,211,201,results,results,observe that,our   re-training   baselines ( 43 and 44 ),results observe that our   re-training   baselines ( 43 and 44 ),0.5966336131095886
translation,211,204,results,more parameters ( + 8 % per source language ),match,baseline performance,more parameters ( + 8 % per source language ) match baseline performance,0.7034265995025635
translation,211,204,results,more parameters ( + 8 % per source language ),gives,poor zero-shot performance,more parameters ( + 8 % per source language ) gives poor zero-shot performance,0.5843160152435303
translation,211,204,results,baseline performance,in,all 3 languages ( 47 and 48 ),baseline performance in all 3 languages ( 47 and 48 ),0.4876130521297455
translation,211,204,results,results,Learning,more parameters ( + 8 % per source language ),results Learning more parameters ( + 8 % per source language ),0.6612836718559265
translation,211,215,results,pivot translation,with,two incrementally - trained models ( 47 with 54 ),pivot translation with two incrementally - trained models ( 47 with 54 ),0.635520875453949
translation,211,215,results,excellent results,close to,bilingual baselines,excellent results close to bilingual baselines,0.7241674065589905
translation,211,215,results,results,has,pivot translation,results has pivot translation,0.5043292045593262
translation,212,19,baselines,bertscore,measures,similarity,bertscore measures similarity,0.5754420757293701
translation,212,19,baselines,similarity,of,reference and hypothesis translation,similarity of reference and hypothesis translation,0.6001964807510376
translation,212,19,baselines,similarity,by,cosinesimilarity,similarity by cosinesimilarity,0.5810043811798096
translation,212,19,baselines,reference and hypothesis translation,by,cosinesimilarity,reference and hypothesis translation by cosinesimilarity,0.5459970831871033
translation,212,19,baselines,cosinesimilarity,of,token embeddings,cosinesimilarity of token embeddings,0.5576732158660889
translation,212,19,baselines,token embeddings,for,each token,token embeddings for each token,0.5703377723693848
translation,212,19,baselines,each token,in,reference and hypothesis,each token in reference and hypothesis,0.5389193892478943
translation,212,19,baselines,baselines,has,bertscore,baselines has bertscore,0.5821647047996521
translation,212,9,results,finetuning,on,pseudo-negatives,finetuning on pseudo-negatives,0.5691593885421753
translation,212,9,results,finetuning,achieved,better pearson 's correlation score,finetuning achieved better pearson 's correlation score,0.7134073972702026
translation,212,9,results,pseudo-negatives,using,wmt15 - 17 and wmt18 - 20 metric corpus,pseudo-negatives using wmt15 - 17 and wmt18 - 20 metric corpus,0.6572580337524414
translation,212,9,results,pseudo-negatives,achieved,better pearson 's correlation score,pseudo-negatives achieved better pearson 's correlation score,0.6930692195892334
translation,212,9,results,better pearson 's correlation score,than,one fine-tuned without negative examples,better pearson 's correlation score than one fine-tuned without negative examples,0.5667696595191956
translation,212,9,results,wmt21,has,finetuning,wmt21 has finetuning,0.5949139595031738
translation,212,65,results,results,of,wmt20 mqm segment- level corpus,results of wmt20 mqm segment- level corpus,0.521803617477417
translation,212,66,results,models,trained on,negative examples,models trained on negative examples,0.6924043297767639
translation,212,66,results,negative examples,of,wmt15 - 17 and wmt18 - 20,negative examples of wmt15 - 17 and wmt18 - 20,0.6210052371025085
translation,212,66,results,negative examples,overcame,plain models,negative examples overcame plain models,0.7914119362831116
translation,212,66,results,plain models,in,pearson 's correlation,plain models in pearson 's correlation,0.5054193139076233
translation,213,169,ablation-analysis,performance,of,both models,performance of both models,0.5794581770896912
translation,213,169,ablation-analysis,both models,has,significantly decrease,both models has significantly decrease,0.5668575167655945
translation,213,168,baselines,pdc ( w / o src-view ),that is,removed sourceview disambiguation,pdc ( w / o src-view ) that is removed sourceview disambiguation,0.5948472023010254
translation,213,168,baselines,pdc ( w / o src-view ),that is,removed target - view disambiguation,pdc ( w / o src-view ) that is removed target - view disambiguation,0.5962535738945007
translation,213,168,baselines,pdc ( w / o src-view ),that is,removed target - view disambiguation,pdc ( w / o src-view ) that is removed target - view disambiguation,0.5962535738945007
translation,213,168,baselines,pdc ( w / o tgt - view ),that is,removed target - view disambiguation,pdc ( w / o tgt - view ) that is removed target - view disambiguation,0.5976709127426147
translation,213,134,experimental-setup,our model,on top of,"thumt ( zhang et al. , 2017a ) toolkit","our model on top of thumt ( zhang et al. , 2017a ) toolkit",0.7059546709060669
translation,213,134,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,213,135,experimental-setup,dropout rate,set to be,0.1,dropout rate set to be 0.1,0.6433757543563843
translation,213,135,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,213,136,experimental-setup,size,of,mini-batch,size of mini-batch,0.6188174486160278
translation,213,136,experimental-setup,mini-batch,is,4096,mini-batch is 4096,0.6275564432144165
translation,213,136,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,213,141,experimental-setup,"bpe ( sennrich et al. , 2016 )",with,32 k merge operations,"bpe ( sennrich et al. , 2016 ) with 32 k merge operations",0.5921318531036377
translation,213,141,experimental-setup,experimental setup,apply,"bpe ( sennrich et al. , 2016 )","experimental setup apply bpe ( sennrich et al. , 2016 )",0.5894983410835266
translation,213,5,model,model,introducing,three novel components,model introducing three novel components,0.7255537509918213
translation,213,6,model,pointer,leverages,semantic information,pointer leverages semantic information,0.7440043687820435
translation,213,6,model,semantic information,from,bilingual dictionaries,semantic information from bilingual dictionaries,0.46816471219062805
translation,213,6,model,semantic information,to better locate,source words,semantic information to better locate source words,0.6206709742546082
translation,213,6,model,disambiguator,synthesizes,contextual information,disambiguator synthesizes contextual information,0.6712818741798401
translation,213,6,model,disambiguator,based on,hierarchical copy mechanism,disambiguator based on hierarchical copy mechanism,0.6495221257209778
translation,213,6,model,contextual information,from,source view and the target view,contextual information from source view and the target view,0.556627094745636
translation,213,6,model,specific source word,from,multiple candidates,specific source word from multiple candidates,0.5651381015777588
translation,213,6,model,multiple candidates,in,dictionaries,multiple candidates in dictionaries,0.5305781960487366
translation,213,6,model,copier,systematically connects,pointer and disambiguator,copier systematically connects pointer and disambiguator,0.6700150966644287
translation,213,6,model,pointer and disambiguator,based on,hierarchical copy mechanism,pointer and disambiguator based on hierarchical copy mechanism,0.6379488110542297
translation,213,6,model,model,building,end-to - end architecture,model building end-to - end architecture,0.6792014241218567
translation,213,43,model,copier,couples,pointer and disambiguator,copier couples pointer and disambiguator,0.7228432893753052
translation,213,43,model,pointer and disambiguator,based on,hierarchical copy mechanism,pointer and disambiguator based on hierarchical copy mechanism,0.6379488110542297
translation,213,43,model,hierarchical copy mechanism,seamlessly integrated with,transformer,hierarchical copy mechanism seamlessly integrated with transformer,0.7025877833366394
translation,213,43,model,model,has,copier,model has copier,0.6420501470565796
translation,213,44,model,simple and effective method,to integrate,byte-pair encoding ( bpe ),simple and effective method to integrate byte-pair encoding ( bpe ),0.7265597581863403
translation,213,44,model,byte-pair encoding ( bpe ),with,bilingual dictionaries,byte-pair encoding ( bpe ) with bilingual dictionaries,0.6150568127632141
translation,213,44,model,bilingual dictionaries,in,our architecture,bilingual dictionaries in our architecture,0.4846956431865692
translation,213,44,model,model,design,simple and effective method,model design simple and effective method,0.6046717166900635
translation,213,76,model,pointer,extracts,semantic information,pointer extracts semantic information,0.6445491313934326
translation,213,76,model,semantic information,of,candidates,semantic information of candidates,0.5821921825408936
translation,213,76,model,candidates,with,candidate - wise encoding,candidates with candidate - wise encoding,0.6739729046821594
translation,213,76,model,model,has,pointer,model has pointer,0.59323650598526
translation,213,77,model,candidate representations,of,each source word,candidate representations of each source word,0.5423597693443298
translation,213,77,model,candidate representations,fused and interacted with,source representations,candidate representations fused and interacted with source representations,0.653489351272583
translation,213,77,model,each source word,fused and interacted with,source representations,each source word fused and interacted with source representations,0.6752632260322571
translation,213,77,model,source representations,from,transformer encoder,source representations from transformer encoder,0.5610731244087219
translation,213,77,model,model,has,candidate representations,model has candidate representations,0.5693750381469727
translation,213,111,model,simple and effective strategy,named,selective bpe,simple and effective strategy named selective bpe,0.681936502456665
translation,213,111,model,bpe,on,all source words,bpe on all source words,0.5093859434127808
translation,213,111,model,bpe,on,portion of target words,bpe on portion of target words,0.5026811957359314
translation,213,111,model,model,present,simple and effective strategy,model present simple and effective strategy,0.6677575707435608
translation,213,163,model,pdc variant,named,pdc ( w / o dict- pointer ),pdc variant named pdc ( w / o dict- pointer ),0.7345314025878906
translation,213,163,model,pdc variant,named,pointer,pdc variant named pointer,0.7200937271118164
translation,213,163,model,pdc variant,whose,pointer,pdc variant whose pointer,0.6428909301757812
translation,213,163,model,pointer,locates,source words,pointer locates source words,0.49509984254837036
translation,213,163,model,source words,based on,encoder state ( h ),source words based on encoder state ( h ),0.5964367985725403
translation,213,163,model,encoder state ( h ),of,vanilla transformer,encoder state ( h ) of vanilla transformer,0.5772870779037476
translation,213,163,model,vanilla transformer,instead of,dictionaryenhanced encoder state ( h ),vanilla transformer instead of dictionaryenhanced encoder state ( h ),0.6415747404098511
translation,213,163,model,model,implement,pdc variant,model implement pdc variant,0.6946360468864441
translation,213,7,results,chinese-english and english - japanese benchmarks,demonstrate,pdc,chinese-english and english - japanese benchmarks demonstrate pdc,0.6107537150382996
translation,213,7,results,results,on,chinese-english and english - japanese benchmarks,results on chinese-english and english - japanese benchmarks,0.5030222535133362
translation,213,154,results,pdc,achieves,very competitive results,pdc achieves very competitive results,0.6690454483032227
translation,213,154,results,existing state - of- the - art nmt systems,has,pdc,existing state - of- the - art nmt systems has pdc,0.5923847556114197
translation,213,154,results,results,compared with,existing state - of- the - art nmt systems,results compared with existing state - of- the - art nmt systems,0.6461398601531982
translation,213,155,results,single - copy,has,outperforms,single - copy has outperforms,0.6122167706489563
translation,213,155,results,results,has,single - copy,results has single - copy,0.5112118721008301
translation,213,160,results,single-copy and flat- copy,with,improvements,single-copy and flat- copy with improvements,0.6936680674552917
translation,213,160,results,improvements,of,1.66 and 2.20 average bleu points,improvements of 1.66 and 2.20 average bleu points,0.5648912787437439
translation,213,160,results,pdc,has,substantially outperforms,pdc has substantially outperforms,0.6195732951164246
translation,213,160,results,substantially outperforms,has,single-copy and flat- copy,substantially outperforms has single-copy and flat- copy,0.5851407647132874
translation,213,160,results,results,has,pdc,results has pdc,0.5712554454803467
translation,213,165,results,performance,of,pdc,performance of pdc,0.5986983776092529
translation,213,165,results,performance,of,pdc,performance of pdc,0.5986983776092529
translation,213,165,results,pdc,demonstrates,decrement,pdc demonstrates decrement,0.7047304511070251
translation,213,165,results,decrement,of,nearly 1.0 average bleu score,decrement of nearly 1.0 average bleu score,0.5639100670814514
translation,213,165,results,nearly 1.0 average bleu score,on,test sets,nearly 1.0 average bleu score on test sets,0.5247123837471008
translation,213,165,results,nearly 1.0 average bleu score,compared with,pdc,nearly 1.0 average bleu score compared with pdc,0.6690090894699097
translation,213,165,results,results,has,performance,results has performance,0.5972660779953003
translation,213,190,results,pdc,has,outperforms,pdc has outperforms,0.6540055274963379
translation,213,190,results,results,find that,pdc,results find that pdc,0.6671401858329773
translation,213,193,results,results,on,iwslt and kftt,results on iwslt and kftt,0.5238701701164246
translation,214,8,model,scheduled sampling methods,based on,decoding steps,scheduled sampling methods based on decoding steps,0.7154989242553711
translation,214,8,model,scheduled sampling methods,increasing,selection chance,scheduled sampling methods increasing selection chance,0.7111304402351379
translation,214,8,model,selection chance,of,predicted tokens,selection chance of predicted tokens,0.565045177936554
translation,214,8,model,predicted tokens,growth of,decoding steps,predicted tokens growth of decoding steps,0.7196375131607056
translation,214,8,model,model,propose,scheduled sampling methods,model propose scheduled sampling methods,0.5957064628601074
translation,214,37,model,scheduled sampling,based on,training steps and decoding steps,scheduled sampling based on training steps and decoding steps,0.7210966944694519
translation,214,37,model,model,investigate,scheduled sampling,model investigate scheduled sampling,0.6806543469429016
translation,214,41,results,our approaches,bring,further improvements,our approaches bring further improvements,0.6506150960922241
translation,214,41,results,further improvements,by,"0.58 , 0.62 , and 0.55 bleu points","further improvements by 0.58 , 0.62 , and 0.55 bleu points",0.5684470534324646
translation,214,41,results,"0.58 , 0.62 , and 0.55 bleu points",on,wmt tasks,"0.58 , 0.62 , and 0.55 bleu points on wmt tasks",0.5072007179260254
translation,214,41,results,stronger vanilla scheduled sampling method,has,our approaches,stronger vanilla scheduled sampling method has our approaches,0.5773497819900513
translation,214,41,results,results,comparing with,stronger vanilla scheduled sampling method,results comparing with stronger vanilla scheduled sampling method,0.7591468691825867
translation,215,119,baselines,two vacs models,one on,all - cs ( vacsv1 ),two vacs models one on all - cs ( vacsv1 ),0.7078239321708679
translation,215,119,baselines,two vacs models,other on,opsub-emt,two vacs models other on opsub-emt,0.6449969410896301
translation,215,119,baselines,opsub-emt,followed by,all - cs ( vacsv2 ),opsub-emt followed by all - cs ( vacsv2 ),0.6963253617286682
translation,215,120,baselines,gan - based sequence generation model,to generate,cs sentences,gan - based sequence generation model to generate cs sentences,0.6581469178199768
translation,215,120,baselines,cs sentences,by providing,rnnlm,cs sentences by providing rnnlm,0.6489404439926147
translation,215,120,baselines,rnnlm,as,generator,rnnlm as generator,0.5618219971656799
translation,215,120,baselines,"seqgan ( yu et al. , 2017 )",has,gan - based sequence generation model,"seqgan ( yu et al. , 2017 ) has gan - based sequence generation model",0.5112800598144531
translation,215,121,baselines,two seqgan 5 models,one on,all - cs ( seqganv1 ),two seqgan 5 models one on all - cs ( seqganv1 ),0.6959543228149414
translation,215,121,baselines,two seqgan 5 models,one on,opsub-emt,two seqgan 5 models one on opsub-emt,0.6860396862030029
translation,215,121,baselines,two seqgan 5 models,one on,opsub-emt,two seqgan 5 models one on opsub-emt,0.6860396862030029
translation,215,121,baselines,opsub-emt,followed by,all - cs ( seqganv2 ),opsub-emt followed by all - cs ( seqganv2 ),0.6874134540557861
translation,215,165,baselines,baselines,has,gluecos benchmark,baselines has gluecos benchmark,0.5331908464431763
translation,215,248,baselines,nt - asgd,variant of,averaged stochastic gradient method,nt - asgd variant of averaged stochastic gradient method,0.7671495676040649
translation,215,248,baselines,nt - asgd,to update,weights,nt - asgd to update weights,0.8081623315811157
translation,215,235,experimental-setup,initialisation step,learn,"token embeddings ( mikolov et al. , 2013 )","initialisation step learn token embeddings ( mikolov et al. , 2013 )",0.6005417704582214
translation,215,235,experimental-setup,"token embeddings ( mikolov et al. , 2013 )",on,same corpus,"token embeddings ( mikolov et al. , 2013 ) on same corpus",0.4735754430294037
translation,215,235,experimental-setup,same corpus,using,skipgram,same corpus using skipgram,0.7016638517379761
translation,215,235,experimental-setup,experimental setup,As,initialisation step,experimental setup As initialisation step,0.5110883712768555
translation,215,236,experimental-setup,embedding dimension,set to,256,embedding dimension set to 256,0.7243790626525879
translation,215,236,experimental-setup,encoder-decoder layers,share,lookup tables,encoder-decoder layers share lookup tables,0.6861090660095215
translation,215,236,experimental-setup,experimental setup,has,embedding dimension,experimental setup has embedding dimension,0.5115625262260437
translation,215,237,experimental-setup,adam optimiser,with,learning rate,adam optimiser with learning rate,0.6170838475227356
translation,215,237,experimental-setup,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,215,237,experimental-setup,experimental setup,has,adam optimiser,experimental setup has adam optimiser,0.5539454817771912
translation,215,247,experimental-setup,learning rate,set at,30,learning rate set at 30,0.6339593529701233
translation,215,247,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,215,249,experimental-setup,mix-review decay parameter,set to,0.9,mix-review decay parameter set to 0.9,0.708286464214325
translation,215,249,experimental-setup,experimental setup,has,mix-review decay parameter,experimental setup has mix-review decay parameter,0.520592451095581
translation,215,258,experimental-setup,2 epochs,with,batch size,2 epochs with batch size,0.6229449510574341
translation,215,258,experimental-setup,2 epochs,with,gradient accumulation step,2 epochs with gradient accumulation step,0.6178990602493286
translation,215,258,experimental-setup,batch size,of,4,batch size of 4,0.6922571659088135
translation,215,258,experimental-setup,gradient accumulation step,of,10,gradient accumulation step of 10,0.5921494960784912
translation,215,258,experimental-setup,experimental setup,trained for,2 epochs,experimental setup trained for 2 epochs,0.5991547703742981
translation,215,257,experiments,masked language modeling ( mlm ),select,default parameters,masked language modeling ( mlm ) select default parameters,0.6789361834526062
translation,215,257,experiments,default parameters,for,learning rate ( 5e - 5 ),default parameters for learning rate ( 5e - 5 ),0.6108816862106323
translation,215,257,experiments,default parameters,for,batch masking probability,default parameters for batch masking probability,0.5984516739845276
translation,215,257,experiments,default parameters,for,sequence length,default parameters for sequence length,0.5374993681907654
translation,215,257,experiments,batch masking probability,has,0.15 ),batch masking probability has 0.15 ),0.5902755260467529
translation,215,257,experiments,sequence length,has,512 ),sequence length has 512 ),0.6030787229537964
translation,215,123,hyperparameters,~ 2 m lines,for,each dataset,~ 2 m lines for each dataset,0.6365705132484436
translation,215,123,hyperparameters,hyperparameters,sample,~ 2 m lines,hyperparameters sample ~ 2 m lines,0.7140670418739319
translation,215,140,hyperparameters,three lstm layers,of,1200 hidden units,three lstm layers of 1200 hidden units,0.5534992814064026
translation,215,140,hyperparameters,1200 hidden units,with,weight tying,1200 hidden units with weight tying,0.6392713189125061
translation,215,140,hyperparameters,1200 hidden units,with,300 - dimensional word embeddings,1200 hidden units with 300 - dimensional word embeddings,0.5904204845428467
translation,215,5,model,state - of - the - art neural machine translation model,to generate,hindi-english codeswitched sentences,state - of - the - art neural machine translation model to generate hindi-english codeswitched sentences,0.6418572664260864
translation,215,5,model,hindi-english codeswitched sentences,starting from,monolingual hindi sentences,hindi-english codeswitched sentences starting from monolingual hindi sentences,0.6086236834526062
translation,215,5,model,model,adapt,state - of - the - art neural machine translation model,model adapt state - of - the - art neural machine translation model,0.7046034336090088
translation,215,21,results,synthetic cs text,important role in,improving,synthetic cs text important role in improving,0.6661850810050964
translation,215,21,results,our model 's ability,to capture,cs patterns,our model 's ability to capture cs patterns,0.7454054355621338
translation,215,21,results,improving,has,our model 's ability,improving has our model 's ability,0.560215175151825
translation,215,21,results,results,show,synthetic cs text,results show synthetic cs text,0.6669785976409912
translation,215,31,results,sentences,generated from,our model,sentences generated from our model,0.6790867447853088
translation,215,31,results,sentences,to train,language models,sentences to train language models,0.6709733009338379
translation,215,31,results,sentences,show,significant improvements,sentences show significant improvements,0.6762776970863342
translation,215,31,results,language models,for,hindi-english cs text,language models for hindi-english cs text,0.5689036250114441
translation,215,31,results,significant improvements,in,perplexity,significant improvements in perplexity,0.5617007613182068
translation,215,31,results,perplexity,compared to,other approaches,perplexity compared to other approaches,0.6669290661811829
translation,215,31,results,results,use,sentences,results use sentences,0.5792677998542786
translation,215,130,results,supervised finetuning,with,all - cs,supervised finetuning with all - cs,0.6523157358169556
translation,215,130,results,supervised finetuning,clearly superior to,unsupervised finetuning,supervised finetuning clearly superior to unsupervised finetuning,0.7255822420120239
translation,215,130,results,results,has,supervised finetuning,results has supervised finetuning,0.5604625940322876
translation,215,134,results,bleu,increase,cs data,bleu increase cs data,0.7140312194824219
translation,215,134,results,increases substantially,increase,cs data,increases substantially increase cs data,0.8215491771697998
translation,215,134,results,cs data,1000 lines to,5000 lines,cs data 1000 lines to 5000 lines,0.7957546710968018
translation,215,134,results,trend,of,diminishing returns,trend of diminishing returns,0.6042365431785583
translation,215,134,results,bleu,has,increases substantially,bleu has increases substantially,0.6274228096008301
translation,215,135,results,d 1,as good as,model,d 1 as good as model,0.6756385564804077
translation,215,135,results,model,trained using,4000 lines,model trained using 4000 lines,0.7403793931007385
translation,215,135,results,4000 lines,of,parallel text,4000 lines of parallel text,0.6111308932304382
translation,215,135,results,results,find that,d 1,results find that d 1,0.6616740226745605
translation,215,172,results,plain monolingual pretraining,leads to,performance improvements,plain monolingual pretraining leads to performance improvements,0.6089501976966858
translation,215,172,results,performance improvements,on,both tasks,performance improvements on both tasks,0.4817073345184326
translation,215,172,results,results,has,plain monolingual pretraining,results has plain monolingual pretraining,0.5077507495880127
translation,215,174,results,consistently better scores,than,lex and emt,consistently better scores than lex and emt,0.6181135177612305
translation,215,174,results,synthetic methods,has,tcs ( s ),synthetic methods has tcs ( s ),0.5508111715316772
translation,215,174,results,tcs ( s ),has,consistently better scores,tcs ( s ) has consistently better scores,0.5917100310325623
translation,215,174,results,results,Among,synthetic methods,results Among synthetic methods,0.527790367603302
translation,215,175,results,pretraining,on,real cs text,pretraining on real cs text,0.5649349093437195
translation,215,175,results,real cs text,from,all - cs,real cs text from all - cs,0.5550856590270996
translation,215,175,results,sa,has,tcs ( s ),sa has tcs ( s ),0.5963582396507263
translation,215,175,results,tcs ( s ),has,outperforms,tcs ( s ) has outperforms,0.6586735844612122
translation,215,175,results,outperforms,has,pretraining,outperforms has pretraining,0.6299026012420654
translation,215,175,results,results,For,sa,results For sa,0.5874460935592651
translation,215,197,results,classification accuracy,of,sample is fake or not,classification accuracy of sample is fake or not,0.5927071571350098
translation,215,197,results,classification accuracy,whether,sample is fake or not,classification accuracy whether sample is fake or not,0.709867000579834
translation,215,197,results,classification accuracy,is,lowest,classification accuracy is lowest,0.6008252501487732
translation,215,197,results,sample is fake or not,is,lowest,sample is fake or not is lowest,0.6184695959091187
translation,215,197,results,lowest,outputs from,tcs,lowest outputs from tcs,0.7314902544021606
translation,215,197,results,results,see,classification accuracy,results see classification accuracy,0.5853936672210693
translation,215,213,results,higher diversity score,compared to,lex and emt,higher diversity score compared to lex and emt,0.6976447701454163
translation,215,213,results,results,has,tcs ( s ) and tcs ( u ),results has tcs ( s ) and tcs ( u ),0.5226597189903259
translation,215,295,results,tcs ( lex ) and tcs ( simalign ),perform,comparably,tcs ( lex ) and tcs ( simalign ) perform comparably,0.6124309301376343
translation,215,295,results,comparably,to,tcs ( s ) and tcs ( u ),comparably to tcs ( s ) and tcs ( u ),0.5978385210037231
translation,215,295,results,comparably,on,all metrics,comparably on all metrics,0.538652777671814
translation,215,295,results,tcs ( s ) and tcs ( u ),on,all metrics,tcs ( s ) and tcs ( u ) on all metrics,0.5507283210754395
translation,215,295,results,results,observe,tcs ( lex ) and tcs ( simalign ),results observe tcs ( lex ) and tcs ( simalign ),0.5626424551010132
translation,215,308,results,tcs models,have,lowest accuracy,tcs models have lowest accuracy,0.5568888187408447
translation,215,308,results,lowest accuracy,among,synthetic methods,lowest accuracy among synthetic methods,0.5972859263420105
translation,215,308,results,results,has,tcs models,results has tcs models,0.5017970204353333
translation,215,337,results,sentences,generated using,tcs,sentences generated using tcs,0.7251037955284119
translation,215,337,results,tcs,yield,largest reductions,tcs yield largest reductions,0.7333467602729797
translation,215,337,results,largest reductions,in,test perplexities,largest reductions in test perplexities,0.5153082013130188
translation,215,337,results,results,has,sentences,results has sentences,0.4975784122943878
translation,216,8,results,most of the systems,achieve,good accuracies,most of the systems achieve good accuracies,0.5921530723571777
translation,216,8,results,good accuracies,in,majority of linguistic phenomena,good accuracies in majority of linguistic phenomena,0.480680912733078
translation,216,9,results,two systems,have,significantly better,two systems have significantly better,0.5584466457366943
translation,216,9,results,test suite accuracy,in,macroaverage,test suite accuracy in macroaverage,0.4902295768260956
translation,216,9,results,test suite accuracy,in,online -w,test suite accuracy in online -w,0.5579992532730103
translation,216,9,results,test suite accuracy,in,facebook - ai,test suite accuracy in facebook - ai,0.5279505848884583
translation,216,9,results,test suite accuracy,in,online -w,test suite accuracy in online -w,0.5579992532730103
translation,216,9,results,test suite accuracy,in,online -w,test suite accuracy in online -w,0.5579992532730103
translation,216,9,results,macroaverage,in,every language direction,macroaverage in every language direction,0.5295426845550537
translation,216,9,results,facebook - ai,for,german to english and volctrans,facebook - ai for german to english and volctrans,0.6657673716545105
translation,216,9,results,facebook - ai,for,english to german,facebook - ai for english to german,0.645397424697876
translation,216,9,results,online -w,for,english to german,online -w for english to german,0.6385859847068787
translation,216,9,results,significantly better,has,test suite accuracy,significantly better has test suite accuracy,0.5626497864723206
translation,216,9,results,results,has,two systems,results has two systems,0.5660581588745117
translation,216,77,results,significantly best systems,are,facebookai,significantly best systems are facebookai,0.5933338403701782
translation,216,77,results,significantly best systems,are,online -a.,significantly best systems are online -a.,0.5841764211654663
translation,216,78,results,average accuracies,are,very high,average accuracies are very high,0.570011556148529
translation,216,78,results,very high,with,lowest system ( happyface ),very high with lowest system ( happyface ),0.6312369108200073
translation,216,78,results,lowest system ( happyface ),having,micro-average,lowest system ( happyface ) having micro-average,0.6591963768005371
translation,216,78,results,micro-average,of,72.3 %,micro-average of 72.3 %,0.5689654350280762
translation,216,78,results,results,has,average accuracies,results has average accuracies,0.547120213508606
translation,216,89,results,other categories,such as,composition,other categories such as composition,0.6319642066955566
translation,216,89,results,other categories,such as,subordination,other categories such as subordination,0.6632530093193054
translation,216,89,results,other categories,such as,named entities & terminology,other categories such as named entities & terminology,0.6399878859519958
translation,216,89,results,other categories,reach,average of more than 90 % accuracy,other categories reach average of more than 90 % accuracy,0.6906285285949707
translation,216,89,results,named entities & terminology,reach,average of more than 90 % accuracy,named entities & terminology reach average of more than 90 % accuracy,0.6728299260139465
translation,216,89,results,average of more than 90 % accuracy,in,german to english,average of more than 90 % accuracy in german to english,0.5454544425010681
translation,216,89,results,results,has,other categories,results has other categories,0.5572068691253662
translation,216,148,results,best system,seems to perform,better,best system seems to perform better,0.7172152996063232
translation,216,148,results,better,in,few categories,better in few categories,0.5424481630325317
translation,216,148,results,better,with,most impressive improvements,better with most impressive improvements,0.5695725679397583
translation,216,148,results,most impressive improvements,at,false friends ( + 14 % ),most impressive improvements at false friends ( + 14 % ),0.5127922892570496
translation,216,148,results,most impressive improvements,at,non-verbal agreement ( + 5 % ),most impressive improvements at non-verbal agreement ( + 5 % ),0.4986676871776581
translation,217,18,baselines,pivot-based methods,to augment,training corpus,pivot-based methods to augment training corpus,0.6403825283050537
translation,217,15,experimental-setup,model architecture,adopt,universal encoder-decoder architecture,model architecture adopt universal encoder-decoder architecture,0.6574946045875549
translation,217,15,experimental-setup,universal encoder-decoder architecture,shares,parameters,universal encoder-decoder architecture shares parameters,0.6938388347625732
translation,217,15,experimental-setup,parameters,across,all languages,parameters across all languages,0.6722486615180969
translation,217,37,experimental-setup,joint byte-pair-encoding ( bpe ),with,32 k split operations,joint byte-pair-encoding ( bpe ) with 32 k split operations,0.6275615096092224
translation,217,37,experimental-setup,32 k split operations,for,subword segmentation,32 k split operations for subword segmentation,0.598235011100769
translation,217,37,experimental-setup,experimental setup,To enable,open-vocabulary,experimental setup To enable open-vocabulary,0.6588397026062012
translation,217,92,experimental-setup,preventing overfitting,set,max-tokens,preventing overfitting set max-tokens,0.6099898815155029
translation,217,92,experimental-setup,preventing overfitting,force,fine-tuning,preventing overfitting force fine-tuning,0.6105210781097412
translation,217,92,experimental-setup,max-tokens,to be,2 k,max-tokens to be 2 k,0.611562192440033
translation,217,92,experimental-setup,2 k,with,learning rate,2 k with learning rate,0.6644824147224426
translation,217,92,experimental-setup,learning rate,of,3e - 5,learning rate of 3e - 5,0.6390295624732971
translation,217,92,experimental-setup,fine-tuning,to stop,finishing,fine-tuning to stop finishing,0.7482945322990417
translation,217,92,experimental-setup,finishing,has,first epoch,finishing has first epoch,0.5969390869140625
translation,217,92,experimental-setup,experimental setup,force,fine-tuning,experimental setup force fine-tuning,0.7296295762062073
translation,217,19,experiments,knowledge transfer,explore,pre-trained model,knowledge transfer explore pre-trained model,0.6604863405227661
translation,217,19,experiments,knowledge transfer,explore,the multilingual model,knowledge transfer explore the multilingual model,0.6335592269897461
translation,217,19,experiments,the multilingual model,trained with,low and high resource language pairs,the multilingual model trained with low and high resource language pairs,0.7419774532318115
translation,217,97,hyperparameters,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,217,97,hyperparameters,optimizer,to update,parameters,optimizer to update parameters,0.7501731514930725
translation,217,97,hyperparameters,parameters,with,? 1 = 0.9 and ? 2 = 0.98,parameters with ? 1 = 0.9 and ? 2 = 0.98,0.6410133242607117
translation,217,98,hyperparameters,label smoothing and dropout rate,to,0.1,label smoothing and dropout rate to 0.1,0.5335704684257507
translation,217,98,hyperparameters,hyperparameters,set,label smoothing and dropout rate,hyperparameters set label smoothing and dropout rate,0.5863495469093323
translation,217,99,hyperparameters,initial learning rate,set to,5e - 4,initial learning rate set to 5e - 4,0.7290231585502625
translation,217,99,hyperparameters,5e - 4,varied under,warm - up strategy,5e - 4 varied under warm - up strategy,0.7427377104759216
translation,217,99,hyperparameters,warm - up strategy,with,4000 steps,warm - up strategy with 4000 steps,0.6500464677810669
translation,217,99,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,217,100,hyperparameters,batch size,is,8 k tokens,batch size is 8 k tokens,0.569542407989502
translation,217,100,hyperparameters,8 k tokens,per,gpu,8 k tokens per gpu,0.6348743438720703
translation,217,100,hyperparameters,training stage,has,batch size,training stage has batch size,0.5552055835723877
translation,217,100,hyperparameters,hyperparameters,In,training stage,hyperparameters In training stage,0.46812766790390015
translation,217,68,model,all synthetic parallel data,generated from,back -translation and pivot-based method,all synthetic parallel data generated from back -translation and pivot-based method,0.6613574624061584
translation,217,68,model,back -translation and pivot-based method,with,genuine parallel data,back -translation and pivot-based method with genuine parallel data,0.6131708025932312
translation,217,68,model,genuine parallel data,to jointly train,multilingual model,genuine parallel data to jointly train multilingual model,0.657143771648407
translation,217,68,model,multilingual model,named,combine - all,multilingual model named combine - all,0.7274453639984131
translation,217,68,model,multilingual model,has,from scratch,multilingual model has from scratch,0.5511932969093323
translation,217,68,model,model,combine,all synthetic parallel data,model combine all synthetic parallel data,0.7476075291633606
translation,217,8,results,best-submitted system,average of,43.45 case -sensitive bleu scores,best-submitted system average of 43.45 case -sensitive bleu scores,0.6509162187576294
translation,217,8,results,43.45 case -sensitive bleu scores,across,all low-resource pairs,43.45 case -sensitive bleu scores across all low-resource pairs,0.6131588816642761
translation,217,8,results,test set,has,best-submitted system,test set has best-submitted system,0.5598379969596863
translation,217,8,results,results,On,test set,results On test set,0.582119882106781
translation,217,17,results,low and high resource data,in,multilingual low-resource scenario,low and high resource data in multilingual low-resource scenario,0.5715922117233276
translation,217,17,results,low and high resource data,explore,several approaches,low and high resource data explore several approaches,0.6756472587585449
translation,217,17,results,results,effectively exploit,low and high resource data,results effectively exploit low and high resource data,0.7069947719573975
translation,217,57,results,model,with,separate decoders,model with separate decoders,0.6497703790664673
translation,217,57,results,separate decoders,for,each target language,separate decoders for each target language,0.6112678050994873
translation,217,57,results,universal encoderdecoder model,has,outperforms,universal encoderdecoder model has outperforms,0.5957595705986023
translation,217,57,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,217,57,results,results,has,universal encoderdecoder model,results has universal encoderdecoder model,0.5292876362800598
translation,217,58,results,universal multilingual 1 - to - 3 baseline system,performs,great improvement,universal multilingual 1 - to - 3 baseline system performs great improvement,0.5479164719581604
translation,217,58,results,great improvement,on,low-resource languages,great improvement on low-resource languages,0.511726438999176
translation,217,58,results,bilingual baseline system,has,universal multilingual 1 - to - 3 baseline system,bilingual baseline system has universal multilingual 1 - to - 3 baseline system,0.5680778622627258
translation,217,58,results,results,Compared to,bilingual baseline system,results Compared to bilingual baseline system,0.6783439517021179
translation,217,106,results,back - translation,gives,solid improvement,back - translation gives solid improvement,0.6260734796524048
translation,217,106,results,solid improvement,by,nearly 0.8 bleu,solid improvement by nearly 0.8 bleu,0.5729773044586182
translation,217,106,results,results,has,back - translation,results has back - translation,0.5638759732246399
translation,217,107,results,pivot- based method,offers,1?2 bleu,pivot- based method offers 1?2 bleu,0.6832253932952881
translation,217,107,results,1?2 bleu,in,"catalan? romanian , catalan","1?2 bleu in catalan? romanian , catalan",0.5684778094291687
translation,217,107,results,results,has,pivot- based method,results has pivot- based method,0.5357440710067749
translation,217,108,results,8 - to - 4 multilingual model,jointly with both,high and low resource languages,8 - to - 4 multilingual model jointly with both high and low resource languages,0.6992384195327759
translation,217,108,results,model,shows,absolute improvement,model shows absolute improvement,0.6880618929862976
translation,217,108,results,absolute improvement,in,three task directions,absolute improvement in three task directions,0.505669355392456
translation,217,108,results,three task directions,of,6 bleu,three task directions of 6 bleu,0.5665975213050842
translation,217,108,results,6 bleu,on,average score,6 bleu on average score,0.5210527777671814
translation,217,108,results,8 - to - 4 multilingual model,has,model,8 - to - 4 multilingual model has model,0.5671825408935547
translation,217,108,results,results,train,8 - to - 4 multilingual model,results train 8 - to - 4 multilingual model,0.6463331580162048
translation,217,110,results,m2m - 100 1.2b model,performs,very well,m2m - 100 1.2b model performs very well,0.593848705291748
translation,217,110,results,very well,in,"catalan ? romanian , catalan ? italian directions","very well in catalan ? romanian , catalan ? italian directions",0.519503116607666
translation,217,110,results,very well,without,fine-tuning,very well without fine-tuning,0.7731236219406128
translation,217,110,results,"catalan ? romanian , catalan ? italian directions",without,fine-tuning,"catalan ? romanian , catalan ? italian directions without fine-tuning",0.7105565071105957
translation,217,111,results,outperforms,by about,2.6 bleu,outperforms by about 2.6 bleu,0.645999014377594
translation,217,111,results,multilingual fine-tuning,by about,2.6 bleu,multilingual fine-tuning by about 2.6 bleu,0.5694068074226379
translation,217,111,results,average bilingual fine-tuning,has,outperforms,average bilingual fine-tuning has outperforms,0.6074401140213013
translation,217,111,results,outperforms,has,multilingual fine-tuning,outperforms has multilingual fine-tuning,0.5309233069419861
translation,217,111,results,results,find that,average bilingual fine-tuning,results find that average bilingual fine-tuning,0.6523227691650391
translation,217,112,results,some systems,hold,comparable performance,some systems hold comparable performance,0.6313378810882568
translation,217,112,results,comparable performance,with,m2m - 100 1.2b model,comparable performance with m2m - 100 1.2b model,0.6489582061767578
translation,217,112,results,comparable performance,when training data,abundant,comparable performance when training data abundant,0.7769808173179626
translation,217,112,results,m2m - 100 1.2b model,in,catalan ? romanian and catalan ? italian directions,m2m - 100 1.2b model in catalan ? romanian and catalan ? italian directions,0.5039691925048828
translation,217,112,results,results,observe,some systems,results observe some systems,0.6299979090690613
translation,217,114,results,in - domain fine- tuning,restricted to,in- domain data size,in - domain fine- tuning restricted to in- domain data size,0.7499850392341614
translation,217,114,results,in - domain fine- tuning,obtain,solid improvement,in - domain fine- tuning obtain solid improvement,0.5758662223815918
translation,217,114,results,solid improvement,of,1.5 bleu,solid improvement of 1.5 bleu,0.5614045858383179
translation,217,114,results,1.5 bleu,especially in,catalan ? occitan direction,1.5 bleu especially in catalan ? occitan direction,0.5824555158615112
translation,217,114,results,results,has,in - domain fine- tuning,results has in - domain fine- tuning,0.5446354150772095
translation,217,115,results,m2 m -kd model,yields,greater improvement,m2 m -kd model yields greater improvement,0.72566819190979
translation,217,115,results,greater improvement,get,best bleu,greater improvement get best bleu,0.5787954926490784
translation,217,115,results,best bleu,in,"catalan? occitan , catalan ? romanian directions","best bleu in catalan? occitan , catalan ? romanian directions",0.546046257019043
translation,217,115,results,"catalan? occitan , catalan ? romanian directions",with,"65.18 , 32.85","catalan? occitan , catalan ? romanian directions with 65.18 , 32.85",0.5860302448272705
translation,217,115,results,results,has,m2 m -kd model,results has m2 m -kd model,0.5361509919166565
translation,218,61,experimental-setup,transformer models,using,sockeye 2 toolkit,transformer models using sockeye 2 toolkit,0.6754146814346313
translation,218,61,experimental-setup,transformer models,using,adam optimizer,transformer models using adam optimizer,0.6466599702835083
translation,218,61,experimental-setup,sockeye 2 toolkit,in,big variant,sockeye 2 toolkit in big variant,0.5426966547966003
translation,218,61,experimental-setup,big variant,with,six encoder and decoder layers,big variant with six encoder and decoder layers,0.6607651114463806
translation,218,61,experimental-setup,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,218,61,experimental-setup,adam optimizer,with,linear warmup,adam optimizer with linear warmup,0.5890266299247742
translation,218,61,experimental-setup,initial learning rate,of,0.06325,initial learning rate of 0.06325,0.5809037089347839
translation,218,61,experimental-setup,linear warmup,over,4000 training steps,linear warmup over 4000 training steps,0.6333320736885071
translation,218,61,experimental-setup,experimental setup,train,transformer models,experimental setup train transformer models,0.6242439150810242
translation,218,64,experimental-setup,data,using,sacremoses,data using sacremoses,0.7626569271087646
translation,218,64,experimental-setup,data,using,truecase,data using truecase,0.7386821508407593
translation,218,64,experimental-setup,sacremoses,apply,byte pair encoding ( bpe ),sacremoses apply byte pair encoding ( bpe ),0.6279610991477966
translation,218,64,experimental-setup,byte pair encoding ( bpe ),with,"32,000 merge operations","byte pair encoding ( bpe ) with 32,000 merge operations",0.6287057995796204
translation,218,64,experimental-setup,sacremoses,has,truecase,sacremoses has truecase,0.6123649477958679
translation,218,64,experimental-setup,truecase,has,data,truecase has data,0.5926777720451355
translation,218,64,experimental-setup,experimental setup,tokenize,data,experimental setup tokenize data,0.6903775334358215
translation,218,79,experimental-setup,reduced initial adaptation learning rate,of,2e - 5,reduced initial adaptation learning rate of 2e - 5,0.6282035708427429
translation,218,83,experimental-setup,data sample,during,adaptation,data sample during adaptation,0.6945289373397827
translation,218,83,experimental-setup,training sample,of,increased size,training sample of increased size,0.6105809807777405
translation,218,83,experimental-setup,training sample,for,1:1 train sample and domain data ratio,training sample for 1:1 train sample and domain data ratio,0.6093106865882874
translation,218,83,experimental-setup,increased size,for,1:1 train sample and domain data ratio,increased size for 1:1 train sample and domain data ratio,0.6279900670051575
translation,218,83,experimental-setup,experimental setup,to avoid,overfitting,experimental setup to avoid overfitting,0.6404601335525513
translation,218,83,experimental-setup,experimental setup,upsample,domain-specific adaptation set 20x,experimental setup upsample domain-specific adaptation set 20x,0.7552430629730225
translation,218,6,model,existing system quality,while incorporating,data,existing system quality while incorporating data,0.6755483150482178
translation,218,6,model,data,for,domains,data for domains,0.6298362612724304
translation,218,6,model,model,in,adaptation setting,model in adaptation setting,0.5677229166030884
translation,218,16,model,single nmt system per language pair,performs,well,single nmt system per language pair performs well,0.675835371017456
translation,218,16,model,well,across,many different domains,well across many different domains,0.7507566213607788
translation,218,16,model,model,train,single nmt system per language pair,model train single nmt system per language pair,0.6658798456192017
translation,218,8,results,comparable performance,on,new domains,comparable performance on new domains,0.5570738315582275
translation,218,8,results,mitigated significantly,on,strong wmt baselines,mitigated significantly on strong wmt baselines,0.5684228539466858
translation,218,8,results,comparable performance,has,catastrophic forgetting,comparable performance has catastrophic forgetting,0.6115432977676392
translation,218,8,results,new domains,has,catastrophic forgetting,new domains has catastrophic forgetting,0.5873854756355286
translation,218,8,results,results,At,comparable performance,results At comparable performance,0.5433435440063477
translation,218,25,results,quality trade- off,in,our multi-domain setting,quality trade- off in our multi-domain setting,0.5272956490516663
translation,218,25,results,quality trade- off,to be,unfavourable,quality trade- off to be unfavourable,0.6157130002975464
translation,218,25,results,gains,on,new domains,gains on new domains,0.5792133808135986
translation,218,25,results,new domains,with,ewc,new domains with ewc,0.67730313539505
translation,218,25,results,ewc,are,limited,ewc are limited,0.6493645906448364
translation,218,25,results,most of the generic performance,has,gains,most of the generic performance has gains,0.592873215675354
translation,218,25,results,results,find,quality trade- off,results find quality trade- off,0.6327526569366455
translation,218,67,results,baseline performance,is,42.7 bleu,baseline performance is 42.7 bleu,0.5110275745391846
translation,218,67,results,baseline performance,is,41.8 bleu,baseline performance is 41.8 bleu,0.5206185579299927
translation,218,67,results,42.7 bleu,on,new-stest2019,42.7 bleu on new-stest2019,0.508849561214447
translation,218,67,results,41.8 bleu,on,newstest 2020,41.8 bleu on newstest 2020,0.5286411643028259
translation,218,67,results,newstest 2020,for,our de ?en system,newstest 2020 for our de ?en system,0.7091477513313293
translation,218,67,results,results,has,baseline performance,results has baseline performance,0.5577890276908875
translation,218,68,results,en ? fr system,yields,41.2 bleu,en ? fr system yields 41.2 bleu,0.698576033115387
translation,218,68,results,en ? fr system,yields,39.2 bleu,en ? fr system yields 39.2 bleu,0.697804868221283
translation,218,68,results,41.2 bleu,on,newstest2014,41.2 bleu on newstest2014,0.4985657334327698
translation,218,68,results,39.2 bleu,on,new-stest2015,39.2 bleu on new-stest2015,0.5170624852180481
translation,218,68,results,results,has,en ? fr system,results has en ? fr system,0.6250025629997253
translation,218,91,results,data mixing,with,1:1 ratio,data mixing with 1:1 ratio,0.6776785850524902
translation,218,91,results,data mixing,retaining,substantially higher generic performance,data mixing retaining substantially higher generic performance,0.7576060891151428
translation,218,91,results,1:1 ratio,of,train sample / adaptation data,1:1 ratio of train sample / adaptation data,0.6096234321594238
translation,218,91,results,train sample / adaptation data,allows for,high quality,train sample / adaptation data allows for high quality,0.7372918725013733
translation,218,91,results,high quality,on,adapted domains,high quality on adapted domains,0.526282787322998
translation,218,91,results,high quality,retaining,substantially higher generic performance,high quality retaining substantially higher generic performance,0.7717149257659912
translation,218,91,results,substantially higher generic performance,than,ewc,substantially higher generic performance than ewc,0.6187710165977478
translation,218,91,results,results,has,data mixing,results has data mixing,0.5172175168991089
translation,218,94,results,similar bleu scores,on,adapted domains ( 40.0 vs 40.2 ),similar bleu scores on adapted domains ( 40.0 vs 40.2 ),0.5456770658493042
translation,218,94,results,ewc + data mixing,with,1:1 training sample / domain data ratio,ewc + data mixing with 1:1 training sample / domain data ratio,0.665529727935791
translation,218,94,results,ewc + data mixing,yields,improvement,ewc + data mixing yields improvement,0.7165274024009705
translation,218,94,results,improvement,of,2 bleu,improvement of 2 bleu,0.5942081212997437
translation,218,94,results,2 bleu,on,news,2 bleu on news,0.5139601826667786
translation,218,94,results,2 bleu,over,ewc,2 bleu over ewc,0.6899036765098572
translation,218,94,results,news,over,ewc,news over ewc,0.7172912955284119
translation,218,94,results,ewc,with,?=10 ?5 ( 44.0 vs 42.0 ),ewc with ?=10 ?5 ( 44.0 vs 42.0 ),0.6010967493057251
translation,218,94,results,similar bleu scores,has,ewc + data mixing,similar bleu scores has ewc + data mixing,0.5681299567222595
translation,218,94,results,results,For,similar bleu scores,results For similar bleu scores,0.5764583349227905
translation,218,103,results,training sample 20x,compared to,1x,training sample 20x compared to 1x,0.6953805685043335
translation,218,103,results,upsampling,has,adaptation data,upsampling has adaptation data,0.5706508755683899
translation,218,103,results,results,effect of,upsampling,results effect of upsampling,0.7001635432243347
translation,218,106,results,learning rate ( lr=2e - 4 ),yields,more forgetting,learning rate ( lr=2e - 4 ) yields more forgetting,0.7156131267547607
translation,218,106,results,more forgetting,on,generic sets,more forgetting on generic sets,0.5501648187637329
translation,218,106,results,lr=2e - 6 ),yields,smaller improvements,lr=2e - 6 ) yields smaller improvements,0.7155587077140808
translation,218,106,results,smaller improvements,on,adapted domains,smaller improvements on adapted domains,0.5129876136779785
translation,218,106,results,results,increasing,learning rate ( lr=2e - 4 ),results increasing learning rate ( lr=2e - 4 ),0.6887376308441162
translation,219,77,ablation-analysis,subtree dele -tion mismatches,affect,models ' confidence,subtree dele -tion mismatches affect models ' confidence,0.705732524394989
translation,219,77,ablation-analysis,models ' confidence,than,other types,models ' confidence than other types,0.5559049844741821
translation,219,77,ablation-analysis,less,than,other types,less than other types,0.6212568283081055
translation,219,77,ablation-analysis,confidence,at,inference,confidence at inference,0.6046595573425293
translation,219,77,ablation-analysis,confidence,at,at training time,confidence at at training time,0.5642985105514526
translation,219,77,ablation-analysis,most,at,inference,most at inference,0.6004359722137451
translation,219,77,ablation-analysis,most,at,at training time,most at at training time,0.5822115540504456
translation,219,77,ablation-analysis,models ' confidence,has,less,models ' confidence has less,0.5929145812988281
translation,219,77,ablation-analysis,phrase replacement,has,hurts,phrase replacement has hurts,0.6235691905021667
translation,219,77,ablation-analysis,hurts,has,confidence,hurts has confidence,0.6034995317459106
translation,219,77,ablation-analysis,confidence,has,most,confidence has most,0.622901976108551
translation,219,77,ablation-analysis,ablation analysis,has,subtree dele -tion mismatches,ablation analysis has subtree dele -tion mismatches,0.5164875984191895
translation,219,85,ablation-analysis,nmt,to,divergences,nmt to divergences,0.5842638611793518
translation,219,85,ablation-analysis,increases,percentage of,degenerated outputs,increases percentage of degenerated outputs,0.7542577385902405
translation,219,85,ablation-analysis,divergences,has,increases,divergences has increases,0.6328245997428894
translation,219,85,ablation-analysis,ablation analysis,shows,nmt,ablation analysis shows nmt,0.6488507986068726
translation,219,85,ablation-analysis,ablation analysis,exposing,nmt,ablation analysis exposing nmt,0.8131049275398254
translation,219,89,ablation-analysis,summary synthetic divergences,has,hurt,summary synthetic divergences has hurt,0.5507780909538269
translation,219,89,ablation-analysis,hurt,has,translation quality,hurt has translation quality,0.5526980757713318
translation,219,89,ablation-analysis,ablation analysis,has,summary synthetic divergences,ablation analysis has summary synthetic divergences,0.5312803983688354
translation,219,91,ablation-analysis,lexical substitution,causes,largest degradation,lexical substitution causes largest degradation,0.6694747805595398
translation,219,91,ablation-analysis,largest degradation,in,translation quality,largest degradation in translation quality,0.4840999245643616
translation,219,91,ablation-analysis,largest degradation,increase,number of degenerated beam hypotheses,largest degradation increase number of degenerated beam hypotheses,0.7093873023986816
translation,219,91,ablation-analysis,phrase replacement,increase,number of degenerated beam hypotheses,phrase replacement increase number of degenerated beam hypotheses,0.6834177374839783
translation,219,91,ablation-analysis,phrase replacement,has,hurts,phrase replacement has hurts,0.6235691905021667
translation,219,91,ablation-analysis,hurts,has,models ' confidence,hurts has models ' confidence,0.5668222308158875
translation,219,91,ablation-analysis,models ' confidence,has,most,models ' confidence has most,0.5800663232803345
translation,219,150,ablation-analysis,translation quality,across,board,translation quality across board,0.6638268232345581
translation,219,150,ablation-analysis,translation quality,compared to,equivalents model,translation quality compared to equivalents model,0.6818250417709351
translation,219,150,ablation-analysis,divergent data,has,div - agnostic ),divergent data has div - agnostic ),0.5977059006690979
translation,219,150,ablation-analysis,divergent data,has,hurts,divergent data has hurts,0.5279994606971741
translation,219,150,ablation-analysis,div - agnostic ),has,hurts,div - agnostic ) has hurts,0.645014226436615
translation,219,150,ablation-analysis,hurts,has,translation quality,hurts has translation quality,0.5436042547225952
translation,219,150,ablation-analysis,ablation analysis,Gradually adding,divergent data,ablation analysis Gradually adding divergent data,0.740689754486084
translation,219,152,ablation-analysis,div-factorized,is,most effective mitigation strategy,div-factorized is most effective mitigation strategy,0.5806007385253906
translation,219,152,ablation-analysis,ablation analysis,has,div-factorized,ablation analysis has div-factorized,0.5557302832603455
translation,219,164,ablation-analysis,factorizing divergences,helps mitigate,impact,factorizing divergences helps mitigate impact,0.772045373916626
translation,219,164,ablation-analysis,impact,of,naturally occurring divergences,impact of naturally occurring divergences,0.6164529919624329
translation,219,164,ablation-analysis,naturally occurring divergences,on,uncertainty,naturally occurring divergences on uncertainty,0.557924747467041
translation,219,164,ablation-analysis,ablation analysis,has,factorizing divergences,ablation analysis has factorizing divergences,0.5589714050292969
translation,219,63,hyperparameters,32 k bpes,using,"sentence -piece ( kudo and richardson , 2018 )","32 k bpes using sentence -piece ( kudo and richardson , 2018 )",0.6534296870231628
translation,219,63,hyperparameters,hyperparameters,learn,32 k bpes,hyperparameters learn 32 k bpes,0.661651074886322
translation,219,64,hyperparameters,base transformer architecture,with,embedding size,base transformer architecture with embedding size,0.6289570331573486
translation,219,64,hyperparameters,base transformer architecture,with,transformer hidden size,base transformer architecture with transformer hidden size,0.6544978022575378
translation,219,64,hyperparameters,base transformer architecture,with,8 attention heads,base transformer architecture with 8 attention heads,0.6666164398193359
translation,219,64,hyperparameters,base transformer architecture,with,6 transformer layers,base transformer architecture with 6 transformer layers,0.6657501459121704
translation,219,64,hyperparameters,base transformer architecture,with,dropout,base transformer architecture with dropout,0.6807937622070312
translation,219,64,hyperparameters,embedding size,of,512,embedding size of 512,0.6241155862808228
translation,219,64,hyperparameters,transformer hidden size,of,"2,048","transformer hidden size of 2,048",0.6344348788261414
translation,219,64,hyperparameters,transformer hidden size,of,6 transformer layers,transformer hidden size of 6 transformer layers,0.6064237356185913
translation,219,64,hyperparameters,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,219,64,hyperparameters,hyperparameters,use,base transformer architecture,hyperparameters use base transformer architecture,0.6127553582191467
translation,219,66,hyperparameters,label smoothing,has,0.1 ),label smoothing has 0.1 ),0.5575680136680603
translation,219,66,hyperparameters,hyperparameters,train with,label smoothing,hyperparameters train with label smoothing,0.6912756562232971
translation,219,67,hyperparameters,"adam ( kingma and ba , 2015 )",with,batch size,"adam ( kingma and ba , 2015 ) with batch size",0.6160398721694946
translation,219,67,hyperparameters,"adam ( kingma and ba , 2015 )",checkpoint,models,"adam ( kingma and ba , 2015 ) checkpoint models",0.7168163061141968
translation,219,67,hyperparameters,batch size,of,"4,096 tokens","batch size of 4,096 tokens",0.5991408228874207
translation,219,67,hyperparameters,models,has,"every 1,000 updates","models has every 1,000 updates",0.6029132008552551
translation,219,67,hyperparameters,hyperparameters,optimize with,"adam ( kingma and ba , 2015 )","hyperparameters optimize with adam ( kingma and ba , 2015 )",0.6923909187316895
translation,219,68,hyperparameters,initial learning rate,is,0.0002,initial learning rate is 0.0002,0.5486441254615784
translation,219,68,hyperparameters,initial learning rate,reduced by,30 %,initial learning rate reduced by 30 %,0.7013917565345764
translation,219,68,hyperparameters,30 %,after,4 checkpoints,30 % after 4 checkpoints,0.7102612257003784
translation,219,68,hyperparameters,4 checkpoints,without,validation perplexity improvement,4 checkpoints without validation perplexity improvement,0.6787253022193909
translation,219,68,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,219,132,hyperparameters,size,of,factor embeddings,size of factor embeddings,0.5882676243782043
translation,219,132,hyperparameters,size,of,source token embeddings,size of source token embeddings,0.5093421936035156
translation,219,132,hyperparameters,size,of,target embeddings,size of target embeddings,0.5478686094284058
translation,219,132,hyperparameters,factor embeddings,to,8,factor embeddings to 8,0.551448404788971
translation,219,132,hyperparameters,source token embeddings,to,504,source token embeddings to 504,0.5803665518760681
translation,219,132,hyperparameters,target embeddings,to,514,target embeddings to 514,0.5895352959632874
translation,219,132,hyperparameters,hyperparameters,set,size,hyperparameters set size,0.6779580116271973
translation,219,132,hyperparameters,hyperparameters,set,source token embeddings,hyperparameters set source token embeddings,0.5340270400047302
translation,219,14,model,model,understand and mitigate,impact,model understand and mitigate impact,0.6678885817527771
translation,219,65,model,target embeddings,tied with,output layer weights,target embeddings tied with output layer weights,0.6338428854942322
translation,219,65,model,model,has,target embeddings,model has target embeddings,0.4919103682041168
translation,219,100,model,semantic factors,to inform,nmt,semantic factors to inform nmt,0.6834439635276794
translation,219,100,model,nmt,of,tokens,nmt of tokens,0.656434953212738
translation,219,100,model,meaning differences,in,each sentence pair,meaning differences in each sentence pair,0.5171935558319092
translation,219,100,model,model,use,semantic factors,model use semantic factors,0.705666720867157
translation,219,147,results,model,trained on,equivalents,model trained on equivalents,0.7441251277923584
translation,219,147,results,equivalents,represents,very competitive baseline,equivalents represents very competitive baseline,0.6350557804107666
translation,219,147,results,very competitive baseline,performs,better or statistically comparable,very competitive baseline performs better or statistically comparable,0.5605394244194031
translation,219,147,results,better or statistically comparable,to,all models,better or statistically comparable to all models,0.5348179340362549
translation,219,147,results,results,has,model,results has model,0.5339115858078003
translation,219,149,results,outperforms,across,metrics and translation directions,outperforms across metrics and translation directions,0.6772801876068115
translation,219,149,results,laser,across,metrics and translation directions,laser across metrics and translation directions,0.6784119606018066
translation,219,149,results,equivalents model,has,outperforms,equivalents model has outperforms,0.6619048118591309
translation,219,149,results,outperforms,has,laser,outperforms has laser,0.6555704474449158
translation,219,149,results,results,has,equivalents model,results has equivalents model,0.5302141308784485
translation,219,153,results,models,recover from,degradation,models recover from degradation,0.649914026260376
translation,219,153,results,degradation,caused by,divergences,degradation caused by divergences,0.7081069350242615
translation,219,153,results,segment - level constraints ( div - tagged ),has,models,segment - level constraints ( div - tagged ) has models,0.5620124340057373
translation,219,153,results,divergences,has,div - agnostic,divergences has div - agnostic,0.6070202589035034
translation,219,153,results,results,With,segment - level constraints ( div - tagged ),results With segment - level constraints ( div - tagged ),0.6305829286575317
translation,219,154,results,token - level factors ( div - factorized ),NMT recover from,impact of divergences,token - level factors ( div - factorized ) NMT recover from impact of divergences,0.7188366651535034
translation,219,154,results,impact of divergences,across,data setups,impact of divergences across data setups,0.7468560934066772
translation,219,154,results,div -factorized,improves over,equiv-alents model,div -factorized improves over equiv-alents model,0.7103688716888428
translation,219,154,results,div-agnostic models,perform,comparably,div-agnostic models perform comparably,0.5743103623390198
translation,219,154,results,comparably,to,equiv - alents,comparably to equiv - alents,0.6150961518287659
translation,219,154,results,factorizing divergences,improves on,latter,factorizing divergences improves on latter,0.6875354051589966
translation,219,154,results,latter,by,? + 1 bleu,latter by ? + 1 bleu,0.6615947484970093
translation,219,154,results,results,has,token - level factors ( div - factorized ),results has token - level factors ( div - factorized ),0.49971088767051697
translation,219,167,results,div -factorized models,are,more confident and more accurate,div -factorized models are more confident and more accurate,0.5781576633453369
translation,219,167,results,div -factorized models,on average,more confident and more accurate,div -factorized models on average more confident and more accurate,0.7074558734893799
translation,219,167,results,more confident and more accurate,than,div -agnostic counterparts,more confident and more accurate than div -agnostic counterparts,0.5998603701591492
translation,219,167,results,results,has,div -factorized models,results has div -factorized models,0.4628302752971649
translation,220,89,baselines,first model,is,russian - english model,first model is russian - english model,0.5611192584037781
translation,220,89,baselines,russian - english model,trained on,russian - english parallel corpus,russian - english model trained on russian - english parallel corpus,0.7061008810997009
translation,220,89,baselines,baselines,has,first model,baselines has first model,0.5739482641220093
translation,220,77,experimental-setup,transformer model,from,fairseq library,transformer model from fairseq library,0.5374433994293213
translation,220,78,experimental-setup,adam,with,"betas ( 0.9 , 0.98 )","adam with betas ( 0.9 , 0.98 )",0.5990045070648193
translation,220,78,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,220,79,experimental-setup,inverse square root learning rate scheduler,used with,initial learning rate,inverse square root learning rate scheduler used with initial learning rate,0.6887946724891663
translation,220,79,experimental-setup,inverse square root learning rate scheduler,used with,4000 warm - up updates,inverse square root learning rate scheduler used with 4000 warm - up updates,0.6577902436256409
translation,220,79,experimental-setup,initial learning rate,of,5e - 4,initial learning rate of 5e - 4,0.6266334652900696
translation,220,79,experimental-setup,experimental setup,has,inverse square root learning rate scheduler,experimental setup has inverse square root learning rate scheduler,0.5204663872718811
translation,220,80,experimental-setup,label smoothed cross entropy,with,label smoothing,label smoothed cross entropy with label smoothing,0.624054491519928
translation,220,80,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,220,80,experimental-setup,experimental setup,has,criterion,experimental setup has criterion,0.4970235526561737
translation,220,81,experimental-setup,0.3,for,all layers,0.3 for all layers,0.6081695556640625
translation,220,81,experimental-setup,experimental setup,has,dropout probability value,experimental setup has dropout probability value,0.5379623770713806
translation,220,10,experiments,pivot language,that is,english,pivot language that is english,0.6215572357177734
translation,220,10,experiments,pivot language,to leverage,transfer learning,pivot language to leverage transfer learning,0.6506940126419067
translation,220,10,experiments,transfer learning,to improve,quality of russian - chinese translation,transfer learning to improve quality of russian - chinese translation,0.6281596422195435
translation,220,106,experiments,pivot language - based transfer learning technique,to improve,task of translation between non-english language pair,pivot language - based transfer learning technique to improve task of translation between non-english language pair,0.6778662800788879
translation,220,106,experiments,task of translation between non-english language pair,that is,russian - chinese,task of translation between non-english language pair that is russian - chinese,0.5874096155166626
translation,220,36,model,cascade model,makes use of,resources,cascade model makes use of resources,0.7202062606811523
translation,220,36,model,resources,of,english language,resources of english language,0.4970512390136719
translation,220,36,model,english language,to train,russian - chinese mt system,english language to train russian - chinese mt system,0.6323954463005066
translation,220,36,model,model,has,cascade model,model has cascade model,0.5871158838272095
translation,220,65,model,byte pair encoding ( bpe ),used as,segmentation technique,byte pair encoding ( bpe ) used as segmentation technique,0.6503057479858398
translation,220,65,model,model,has,byte pair encoding ( bpe ),model has byte pair encoding ( bpe ),0.5866608619689941
translation,220,88,model,cascade model,consists of,two nmt models,cascade model consists of two nmt models,0.6747313141822815
translation,220,88,model,two nmt models,has,trained separately,two nmt models has trained separately,0.5807965397834778
translation,220,88,model,model,has,cascade model,model has cascade model,0.5871158838272095
translation,220,91,model,russian sentence,to,chinese,russian sentence to chinese,0.5591766834259033
translation,220,91,model,model,translating,russian sentence,model translating russian sentence,0.7145481109619141
translation,220,11,results,baseline transformer - based neural machine translation system,observe that,pivot language - based transfer learning technique,baseline transformer - based neural machine translation system observe that pivot language - based transfer learning technique,0.566723108291626
translation,220,11,results,pivot language - based transfer learning technique,gives,higher bleu score,pivot language - based transfer learning technique gives higher bleu score,0.5740465521812439
translation,220,11,results,results,Compared to,baseline transformer - based neural machine translation system,results Compared to baseline transformer - based neural machine translation system,0.6223328709602356
translation,220,100,results,direct pivoting model,produced,bleu score,direct pivoting model produced bleu score,0.5722344517707825
translation,220,100,results,bleu score,of,18.8,bleu score of 18.8,0.5503387451171875
translation,220,100,results,bleu score,by,0.6 points,bleu score by 0.6 points,0.565057635307312
translation,220,100,results,18.8,improved,bleu score,18.8 improved bleu score,0.663364589214325
translation,220,100,results,bleu score,by,0.6 points,bleu score by 0.6 points,0.565057635307312
translation,220,100,results,0.6 points,over,baseline model,0.6 points over baseline model,0.6546204686164856
translation,220,100,results,results,has,direct pivoting model,results has direct pivoting model,0.5125729441642761
translation,220,107,results,pivot language - based transfer learning technique,improves,bleu score,pivot language - based transfer learning technique improves bleu score,0.6225681304931641
translation,220,107,results,bleu score,over,baseline model,bleu score over baseline model,0.6059039831161499
translation,220,107,results,results,observe,pivot language - based transfer learning technique,results observe pivot language - based transfer learning technique,0.609011173248291
translation,220,108,results,error propagation,present in,simple cascadebased models,error propagation present in simple cascadebased models,0.676003098487854
translation,220,108,results,results,observe,pivot language - based transfer learning technique,results observe pivot language - based transfer learning technique,0.609011173248291
translation,221,79,ablation-analysis,performance,leading to,unrealistically weak performance,performance leading to unrealistically weak performance,0.6635656356811523
translation,221,79,ablation-analysis,other artifacts,has,hinders,other artifacts has hinders,0.5652734637260437
translation,221,79,ablation-analysis,hinders,has,performance,hinders has performance,0.5852683782577515
translation,221,179,ablation-analysis,scenario,for,english -xhosa,scenario for english -xhosa,0.6559581756591797
translation,221,153,baselines,afrobart,with both,dictionary augmentation,afrobart with both dictionary augmentation,0.694477379322052
translation,221,153,baselines,afrobart,with both,pseudo monolingual data,afrobart with both pseudo monolingual data,0.6932892799377441
translation,221,153,baselines,afrobart,with both,randomly initialized models,afrobart with both randomly initialized models,0.7094578146934509
translation,221,153,baselines,afrobart,with,randomly initialized models,afrobart with randomly initialized models,0.6484540104866028
translation,221,153,baselines,afrobart,with,"cross-lingual transfer models ( neubig and hu , 2018 )","afrobart with cross-lingual transfer models ( neubig and hu , 2018 )",0.6273671388626099
translation,221,153,baselines,afrobart,jointly trained with,larger amount of parallel data ( full afromt data ),afrobart jointly trained with larger amount of parallel data ( full afromt data ),0.7511443495750427
translation,221,153,baselines,"cross-lingual transfer models ( neubig and hu , 2018 )",jointly trained with,larger amount of parallel data ( full afromt data ),"cross-lingual transfer models ( neubig and hu , 2018 ) jointly trained with larger amount of parallel data ( full afromt data )",0.7667486667633057
translation,221,153,baselines,larger amount of parallel data ( full afromt data ),in,related language,larger amount of parallel data ( full afromt data ) in related language,0.4888225197792053
translation,221,123,experimental-setup,afrobart models,utilizing,mbart implementation,afrobart models utilizing mbart implementation,0.6759470701217651
translation,221,123,experimental-setup,mbart implementation,in,fairseq 12 library,mbart implementation in fairseq 12 library,0.4273267686367035
translation,221,123,experimental-setup,experimental setup,to train,afrobart models,experimental setup to train afrobart models,0.6885610818862915
translation,221,124,experimental-setup,data,using,"sentence -piece ( kudo and richardson , 2018 )","data using sentence -piece ( kudo and richardson , 2018 )",0.6835957169532776
translation,221,124,experimental-setup,data,using,80 k subword vocabulary,data using 80 k subword vocabulary,0.6310176849365234
translation,221,124,experimental-setup,data,using,80 k subword vocabulary,data using 80 k subword vocabulary,0.6310176849365234
translation,221,124,experimental-setup,experimental setup,tokenize,data,experimental setup tokenize data,0.6903775334358215
translation,221,125,experimental-setup,transformer - base architecture,of,hidden dimension,transformer - base architecture of hidden dimension,0.5669528245925903
translation,221,125,experimental-setup,transformer - base architecture,of,feedforward size,transformer - base architecture of feedforward size,0.6159631013870239
translation,221,125,experimental-setup,transformer - base architecture,of,6 layers,transformer - base architecture of 6 layers,0.5875028967857361
translation,221,125,experimental-setup,hidden dimension,of,512,hidden dimension of 512,0.655615508556366
translation,221,125,experimental-setup,feedforward size,of,2048,feedforward size of 2048,0.6445165872573853
translation,221,125,experimental-setup,6 layers,for,both the encoder and decoder,6 layers for both the encoder and decoder,0.6211029887199402
translation,221,125,experimental-setup,experimental setup,use,transformer - base architecture,experimental setup use transformer - base architecture,0.5966783165931702
translation,221,126,experimental-setup,maximum sequence length,to be,512,maximum sequence length to be 512,0.5441758036613464
translation,221,126,experimental-setup,maximum sequence length,using,batch size,maximum sequence length using batch size,0.6553272604942322
translation,221,126,experimental-setup,batch size,of,1024,batch size of 1024,0.6398507952690125
translation,221,126,experimental-setup,1024,for,100k iterations,1024 for 100k iterations,0.608605146408081
translation,221,126,experimental-setup,100k iterations,with,32 nvidia v100,100k iterations with 32 nvidia v100,0.5839425921440125
translation,221,126,experimental-setup,experimental setup,set,maximum sequence length,experimental setup set maximum sequence length,0.653292715549469
translation,221,24,experiments,afromt,consists of,translation tasks,afromt consists of translation tasks,0.6500612497329712
translation,221,24,experiments,translation tasks,between,english and 8 african languages,translation tasks between english and 8 african languages,0.618399977684021
translation,221,23,model,machine translation benchmark,for,african languages,machine translation benchmark for african languages,0.4795804023742676
translation,221,23,model,pretraining techniques,to deal with,previously unexplored case,pretraining techniques to deal with previously unexplored case,0.7119306325912476
translation,221,23,model,previously unexplored case,where,size of monolingual data resources,previously unexplored case where size of monolingual data resources,0.5779338479042053
translation,221,23,model,size of monolingual data resources,for,pretraining,size of monolingual data resources for pretraining,0.56646728515625
translation,221,23,model,parallel data resources,for,finetuning,parallel data resources for finetuning,0.5872347950935364
translation,221,27,model,first method,leverages,bilingual dictionaries,first method leverages bilingual dictionaries,0.6763871908187866
translation,221,27,model,bilingual dictionaries,to augment,data,bilingual dictionaries to augment data,0.6671651601791382
translation,221,27,model,data,in,high- resource languages ( hrl ),data in high- resource languages ( hrl ),0.5618135929107666
translation,221,27,model,second method,iteratively creates,pseudo-monolingual data,second method iteratively creates pseudo-monolingual data,0.6572905778884888
translation,221,27,model,pseudo-monolingual data,in,low-resource languages ( lrl ),pseudo-monolingual data in low-resource languages ( lrl ),0.5413515567779541
translation,221,27,model,pseudo-monolingual data,for,pretraining,pseudo-monolingual data for pretraining,0.5681363940238953
translation,221,27,model,low-resource languages ( lrl ),for,pretraining,low-resource languages ( lrl ) for pretraining,0.6211280822753906
translation,221,27,model,model,has,first method,model has first method,0.5816154479980469
translation,221,78,results,afromt model,find that,not filtering the data,afromt model find that not filtering the data,0.7115029096603394
translation,221,78,results,not filtering the data,for,leakage,not filtering the data for leakage,0.6336157321929932
translation,221,78,results,leakage,leads to,misleading results,leakage leads to misleading results,0.7270869016647339
translation,221,78,results,results,Comparing,noisy model,results Comparing noisy model,0.7191247940063477
translation,221,146,results,initializing,with,pretrained afrobart weights,initializing with pretrained afrobart weights,0.5894578695297241
translation,221,146,results,pretrained afrobart weights,results in,performance gains,pretrained afrobart weights results in performance gains,0.6012577414512634
translation,221,146,results,performance gains,of,?1 bleu,performance gains of ?1 bleu,0.5817636251449585
translation,221,146,results,?1 bleu,across,all language pairs,?1 bleu across all language pairs,0.6596880555152893
translation,221,146,results,results,find,initializing,results find initializing,0.5896280407905579
translation,221,147,results,our pretraining data,with,dictionary,our pretraining data with dictionary,0.6295877695083618
translation,221,147,results,our pretraining data,results in,performance gains,our pretraining data results in performance gains,0.6564615368843079
translation,221,147,results,performance gains,across,all pairs,performance gains across all pairs,0.7185596823692322
translation,221,147,results,all pairs,in terms of,chrf,all pairs in terms of chrf,0.7022242546081543
translation,221,147,results,all pairs,in terms of,6/8 pairs,all pairs in terms of 6/8 pairs,0.65864497423172
translation,221,147,results,6/8 pairs,in terms of,bleu,6/8 pairs in terms of bleu,0.6190205812454224
translation,221,147,results,augmenting,has,our pretraining data,augmenting has our pretraining data,0.5162438154220581
translation,221,147,results,results,observe,augmenting,results observe augmenting,0.6237819790840149
translation,221,148,results,results,has,gain,results has gain,0.5428218841552734
translation,221,150,results,further improvements,augmenting with,pseudo monolingual data,further improvements augmenting with pseudo monolingual data,0.747266948223114
translation,221,150,results,results,see,further improvements,results see further improvements,0.6121314764022827
translation,221,154,results,performance,of,other models,performance of other models,0.5805433988571167
translation,221,154,results,other models,with,significant performance increase,other models with significant performance increase,0.5819275379180908
translation,221,154,results,significant performance increase,over,random initialization,significant performance increase over random initialization,0.6728753447532654
translation,221,154,results,random initialization,of,15 + bleu,random initialization of 15 + bleu,0.5545536279678345
translation,221,154,results,15 + bleu,on,english - zulu,15 + bleu on english - zulu,0.5661822557449341
translation,221,154,results,randomly initialized models,trained on,5 x the data,randomly initialized models trained on 5 x the data,0.7656028270721436
translation,221,154,results,almost double,has,performance,almost double has performance,0.6157394051551819
translation,221,154,results,outperforming,has,randomly initialized models,outperforming has randomly initialized models,0.6060921549797058
translation,221,155,results,clt,performs,random,clt performs random,0.6781930327415466
translation,221,155,results,clt,than,random,clt than random,0.660102367401123
translation,221,155,results,random,on,english -xhosa,random on english -xhosa,0.5855826139450073
translation,221,155,results,random,as,data size,random as data size,0.5589038729667664
translation,221,155,results,data size,has,increases,data size has increases,0.6059015393257141
translation,221,155,results,results,notice,clt,results notice clt,0.6912906169891357
translation,221,159,results,afrobart,able to leverage,knowl- edge,afrobart able to leverage knowl- edge,0.7416452169418335
translation,221,159,results,knowl- edge,gained during,training,knowl- edge gained during training,0.7330934405326843
translation,221,159,results,knowl- edge,for,fast adaptation,knowl- edge for fast adaptation,0.6507763266563416
translation,221,159,results,training,for,fast adaptation,training for fast adaptation,0.6519045829772949
translation,221,159,results,fast adaptation,even with,small amounts of data,fast adaptation even with small amounts of data,0.676295816898346
translation,221,177,results,translation accuracy,for,nouns,translation accuracy for nouns,0.6128953099250793
translation,221,177,results,afrobart,has,significantly improves,afrobart has significantly improves,0.6051033735275269
translation,221,177,results,significantly improves,has,translation accuracy,significantly improves has translation accuracy,0.5682457089424133
translation,221,178,results,afrobart,improves,translation accuracy,afrobart improves translation accuracy,0.6724900603294373
translation,221,178,results,translation accuracy,over,ten noun classes,translation accuracy over ten noun classes,0.6267340183258057
translation,221,178,results,ten noun classes,by,1.08 %,ten noun classes by 1.08 %,0.5677310824394226
translation,221,178,results,1.08 %,over,random baseline,1.08 % over random baseline,0.6472107768058777
translation,221,178,results,results,has,afrobart,results has afrobart,0.5542288422584534
translation,221,180,results,noun class accuracy,on,classes,noun class accuracy on classes,0.5032544732093811
translation,221,183,results,accuracy,of,crosslingual transfer baseline,accuracy of crosslingual transfer baseline,0.5175401568412781
translation,221,183,results,accuracy,of,crosslingual transfer baseline,accuracy of crosslingual transfer baseline,0.5175401568412781
translation,221,183,results,improvement,of,16.33 %,improvement of 16.33 %,0.5653408169746399
translation,221,183,results,crosslingual transfer baseline,on,noun classes,crosslingual transfer baseline on noun classes,0.508674144744873
translation,221,183,results,afrobart,has,almost doubles,afrobart has almost doubles,0.641082227230072
translation,221,183,results,almost doubles,has,accuracy,almost doubles has accuracy,0.5844010710716248
translation,221,183,results,accuracy,has,improvement,accuracy has improvement,0.5407899022102356
translation,221,183,results,results,note,afrobart,results note afrobart,0.5828182101249695
translation,222,24,baselines,cmbt ( contextually - mined back - translation ),relies on,contextualized cross-lingual word representations ( ccwrs ),cmbt ( contextually - mined back - translation ) relies on contextualized cross-lingual word representations ( ccwrs ),0.731657087802887
translation,222,24,baselines,contextualized cross-lingual word representations ( ccwrs ),to translate,source language multi-sense words,contextualized cross-lingual word representations ( ccwrs ) to translate source language multi-sense words,0.6338362097740173
translation,222,24,baselines,contextualized cross-lingual word representations ( ccwrs ),find,target language sentences,contextualized cross-lingual word representations ( ccwrs ) find target language sentences,0.5541394352912903
translation,222,24,baselines,target language sentences,containing,translations,target language sentences containing translations,0.6434061527252197
translation,222,24,baselines,translations,of,right senses,translations of right senses,0.6085845828056335
translation,222,123,experimental-setup,"base transformer nmt models ( vaswani et al. , 2017 )",on,gold parallel data,"base transformer nmt models ( vaswani et al. , 2017 ) on gold parallel data",0.5300226807594299
translation,222,123,experimental-setup,gold parallel data,with,early - stopping,gold parallel data with early - stopping,0.6723188757896423
translation,222,123,experimental-setup,early - stopping,based on,validation perplexity,early - stopping based on validation perplexity,0.6199235916137695
translation,222,123,experimental-setup,experimental setup,train,"base transformer nmt models ( vaswani et al. , 2017 )","experimental setup train base transformer nmt models ( vaswani et al. , 2017 )",0.6609375476837158
translation,222,124,experimental-setup,4 nvidia gtx 1080 ti gpus,with,per-gpu batch size,4 nvidia gtx 1080 ti gpus with per-gpu batch size,0.6093093156814575
translation,222,124,experimental-setup,4 nvidia gtx 1080 ti gpus,delaying,stochastic gradient descent updates,4 nvidia gtx 1080 ti gpus delaying stochastic gradient descent updates,0.6102268099784851
translation,222,124,experimental-setup,per-gpu batch size,of,4096 tokens,per-gpu batch size of 4096 tokens,0.5727927684783936
translation,222,124,experimental-setup,stochastic gradient descent updates,with,factor of 2,stochastic gradient descent updates with factor of 2,0.6315324902534485
translation,222,124,experimental-setup,experimental setup,trained on,4 nvidia gtx 1080 ti gpus,experimental setup trained on 4 nvidia gtx 1080 ti gpus,0.6880650520324707
translation,222,126,experimental-setup,dropout and label smoothing,with,value,dropout and label smoothing with value,0.637153148651123
translation,222,126,experimental-setup,value,of,0.1,value of 0.1,0.5788514018058777
translation,222,126,experimental-setup,experimental setup,use,dropout and label smoothing,experimental setup use dropout and label smoothing,0.5440289974212646
translation,222,130,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,222,130,experimental-setup,beam size,of,5,beam size of 5,0.7073217034339905
translation,222,130,experimental-setup,4,for,back - translation,4 for back - translation,0.5310256481170654
translation,222,130,experimental-setup,5,for,translation,5 for translation,0.6360850930213928
translation,222,130,experimental-setup,translation,of,mucow,translation of mucow,0.6631720066070557
translation,222,132,experimental-setup,datasets,tokenized using,moses,datasets tokenized using moses,0.7243212461471558
translation,222,132,experimental-setup,experimental setup,has,datasets,experimental setup has datasets,0.47152194380760193
translation,222,133,experimental-setup,bpe split-ting,with,32 k merge operations,bpe split-ting with 32 k merge operations,0.6465560793876648
translation,222,133,experimental-setup,32 k merge operations,computed jointly on,source and target data,32 k merge operations computed jointly on source and target data,0.6947849988937378
translation,222,134,experimental-setup,word alignment,performed using,"fastalign ( dyer et al. , 2013 )","word alignment performed using fastalign ( dyer et al. , 2013 )",0.5709241628646851
translation,222,134,experimental-setup,experimental setup,has,word alignment,experimental setup has word alignment,0.4930987060070038
translation,222,25,experiments,synthetic parallel corpus,by back - translating,sentences,synthetic parallel corpus by back - translating sentences,0.7532092928886414
translation,222,25,experiments,original multisense words,contained on,source side,original multisense words contained on source side,0.6310622692108154
translation,222,25,experiments,original multisense words,to extend,training corpus,original multisense words to extend training corpus,0.6329421997070312
translation,222,25,experiments,training corpus,for,better sense coverage,training corpus for better sense coverage,0.5455524325370789
translation,222,6,model,model,propose,cmbt,model propose cmbt,0.7157758474349976
translation,222,23,model,method,to mine,addi-tional data,method to mine addi-tional data,0.6849282383918762
translation,222,23,model,addi-tional data,containing,contextual translations,addi-tional data containing contextual translations,0.6649012565612793
translation,222,23,model,contextual translations,of,rare and unseen senses,contextual translations of rare and unseen senses,0.5486238598823547
translation,222,220,model,general framework,effectively exploits,contextdependent cross-lingual correspondence,general framework effectively exploits contextdependent cross-lingual correspondence,0.7501720786094666
translation,222,220,model,contextdependent cross-lingual correspondence,from,pre-trained ccwr,contextdependent cross-lingual correspondence from pre-trained ccwr,0.5686293840408325
translation,222,220,model,pre-trained ccwr,for,mt system,pre-trained ccwr for mt system,0.6348958015441895
translation,222,220,model,model,serves as,general framework,model serves as general framework,0.6599116921424866
translation,222,7,results,ccwrs,capture,word senses,ccwrs capture word senses,0.6966601014137268
translation,222,7,results,word senses,that are,missing or very rare,word senses that are missing or very rare,0.6055127382278442
translation,222,7,results,missing or very rare,in,parallel corpora,missing or very rare in parallel corpora,0.5161404013633728
translation,222,7,results,parallel corpora,used to train,mt,parallel corpora used to train mt,0.6586766839027405
translation,222,157,results,acc@n cmbt word translation,performs,significantly better,acc@n cmbt word translation performs significantly better,0.6084898710250854
translation,222,157,results,significantly better,than,bwes,significantly better than bwes,0.6133155822753906
translation,222,157,results,results,In terms of,acc@n cmbt word translation,results In terms of acc@n cmbt word translation,0.6747440099716187
translation,222,159,results,xlm -r,shows,better wsd performance,xlm -r shows better wsd performance,0.6928557753562927
translation,222,159,results,better wsd performance,train,system acc@1 acc@5 f,better wsd performance train system acc@1 acc@5 f,0.698286235332489
translation,222,159,results,better wsd performance,by relying on,context,better wsd performance by relying on context,0.7002075910568237
translation,222,159,results,context,in,sentences,context in sentences,0.5155997276306152
translation,222,159,results,results,has,xlm -r,results has xlm -r,0.5405601263046265
translation,222,160,results,improved nmt system,using,cmbt,improved nmt system using cmbt,0.7142152786254883
translation,222,160,results,outperforms,in,all setups,outperforms in all setups,0.5236067175865173
translation,222,160,results,baseline system,in,all setups,baseline system in all setups,0.4918909966945648
translation,222,160,results,cmbt,has,outperforms,cmbt has outperforms,0.661577045917511
translation,222,160,results,outperforms,has,baseline system,outperforms has baseline system,0.610763430595398
translation,222,160,results,results,has,improved nmt system,results has improved nmt system,0.5924208164215088
translation,222,162,results,rare senses,in,higher relative frequency range bin,rare senses in higher relative frequency range bin,0.5415216684341431
translation,222,163,results,bwes based mining,helpful for,unseen and very rare senses,bwes based mining helpful for unseen and very rare senses,0.605308473110199
translation,222,163,results,bwes based mining,is,less effective,bwes based mining is less effective,0.6079204678535461
translation,222,163,results,less effective,compared to,cmbt,less effective compared to cmbt,0.7038645148277283
translation,222,163,results,results,has,bwes based mining,results has bwes based mining,0.6030008792877197
translation,222,168,results,effective,when evaluating on,whole mucow test dataset,effective when evaluating on whole mucow test dataset,0.6714403033256531
translation,222,168,results,more prominent,on,lower frequency ranges,more prominent on lower frequency ranges,0.5452975630760193
translation,222,168,results,results,has,cmbt,results has cmbt,0.5495296120643616
translation,222,179,results,all +,achieved,only minor improvements overall ( all ),all + achieved only minor improvements overall ( all ),0.7264541387557983
translation,222,179,results,only minor improvements overall ( all ),on,sample - 10,only minor improvements overall ( all ) on sample - 10,0.5680413842201233
translation,222,179,results,decreased,on,unseen,decreased on unseen,0.6110593676567078
translation,222,179,results,performance,has,decreased,performance has decreased,0.6074588894844055
translation,222,179,results,results,has,all +,results has all +,0.5336442589759827
translation,222,181,results,cmbt +,achieved,improvements,cmbt + achieved improvements,0.716310441493988
translation,222,181,results,improvements,on,both setups,improvements on both setups,0.5369995832443237
translation,222,181,results,improvements,by following,sense distribution,improvements by following sense distribution,0.6683223843574524
translation,222,181,results,sense distribution,of,test set,sense distribution of test set,0.5938447713851929
translation,222,181,results,results,has,cmbt +,results has cmbt +,0.5154315829277039
translation,222,186,results,cmbt,achieves,best scores,cmbt achieves best scores,0.5950106978416443
translation,222,186,results,cmbt,achieves,minor decrease,cmbt achieves minor decrease,0.6404908299446106
translation,222,186,results,best scores,on,full test sets ( all ),best scores on full test sets ( all ),0.5080409049987793
translation,222,186,results,full test sets ( all ),of,unseen and sample - 10 setups,full test sets ( all ) of unseen and sample - 10 setups,0.5492508411407471
translation,222,186,results,minor decrease,on,big,minor decrease on big,0.5831127762794495
translation,222,186,results,results,has,cmbt,results has cmbt,0.5495296120643616
translation,222,221,results,cmbt,brings,significant improvements,cmbt brings significant improvements,0.578449547290802
translation,222,221,results,significant improvements,for,multisense word translation,significant improvements for multisense word translation,0.5864013433456421
translation,222,221,results,multisense word translation,on,english ? german mucow test set,multisense word translation on english ? german mucow test set,0.5124946236610413
translation,222,221,results,results,show,cmbt,results show cmbt,0.622128963470459
translation,223,80,ablation-analysis,first-order dependency,could not provide,further improvements,first-order dependency could not provide further improvements,0.6500707864761353
translation,223,80,ablation-analysis,further improvements,over,zeroorder model,further improvements over zeroorder model,0.7326547503471375
translation,223,80,ablation-analysis,ablation analysis,seen that,first-order dependency,ablation analysis seen that first-order dependency,0.6520376205444336
translation,223,96,baselines,argmax,of,average of the attention heads,argmax of average of the attention heads,0.5483377575874329
translation,223,96,baselines,average of the attention heads,in,fifth and sixth decoder layers,average of the attention heads in fifth and sixth decoder layers,0.5278898477554321
translation,223,97,model,models,trained in,both directions,models trained in both directions,0.754856288433075
translation,223,97,model,both directions,to get,bidirectional alignments,both directions to get bidirectional alignments,0.66343754529953
translation,223,97,model,model,has,models,model has models,0.5616568326950073
translation,223,78,results,direct hmms,achieve,comparable performance,direct hmms achieve comparable performance,0.6194584369659424
translation,223,78,results,comparable performance,to,transformer baselines,comparable performance to transformer baselines,0.5639768242835999
translation,223,78,results,comparable performance,in terms of,bleu scores,comparable performance in terms of bleu scores,0.6636764407157898
translation,223,78,results,baseline systems,in terms of,ter scores,baseline systems in terms of ter scores,0.7115009427070618
translation,223,78,results,outperform,has,baseline systems,outperform has baseline systems,0.5964462757110596
translation,223,78,results,results,show,direct hmms,results show direct hmms,0.6365805268287659
translation,223,79,results,ter metric,to favor,shorter hypotheses,ter metric to favor shorter hypotheses,0.7176237106323242
translation,223,79,results,ter metric,from,length ratio results,ter metric from length ratio results,0.5773568153381348
translation,223,79,results,length ratio results,conclude,improvements,length ratio results conclude improvements,0.6693840622901917
translation,223,79,results,results,has,ter metric,results has ter metric,0.6056488752365112
translation,223,99,results,alignment,generated by,direct hmm,alignment generated by direct hmm,0.605920672416687
translation,223,99,results,significantly better quality,extracted directly from,transformer attention weights,significantly better quality extracted directly from transformer attention weights,0.6479920744895935
translation,223,99,results,alignment,has,significantly better quality,alignment has significantly better quality,0.572428822517395
translation,223,99,results,direct hmm,has,significantly better quality,direct hmm has significantly better quality,0.585207462310791
translation,224,167,ablation-analysis,first observation,sentences with,high uncertainty,first observation sentences with high uncertainty,0.7532398700714111
translation,224,167,ablation-analysis,high uncertainty,are with,"relatively low bleu scores ( i.e. , 31.0 )","high uncertainty are with relatively low bleu scores ( i.e. , 31.0 )",0.5444527864456177
translation,224,167,ablation-analysis,ablation analysis,has,first observation,ablation analysis has first observation,0.5556557774543762
translation,224,69,baselines,"state- of- the- art trans - former ( vaswani et al. , 2017 ) network",consists of,decoder of 6 layers,"state- of- the- art trans - former ( vaswani et al. , 2017 ) network consists of decoder of 6 layers",0.6027998328208923
translation,224,67,experimental-setup,each language pair,applied,byte pair encoding,each language pair applied byte pair encoding,0.7116570472717285
translation,224,67,experimental-setup,byte pair encoding,with,32 k merge operations,byte pair encoding with 32 k merge operations,0.6211714744567871
translation,224,67,experimental-setup,byte pair encoding,has,bpe,byte pair encoding has bpe,0.5673273205757141
translation,224,67,experimental-setup,experimental setup,For,each language pair,experimental setup For each language pair,0.584132730960846
translation,224,70,experimental-setup,open-source toolkit fairseq,to implement,model,open-source toolkit fairseq to implement model,0.688588559627533
translation,224,70,experimental-setup,experimental setup,adopted,open-source toolkit fairseq,experimental setup adopted open-source toolkit fairseq,0.6405008435249329
translation,224,75,experimental-setup,transformer - big model,trained it for,30 k steps,transformer - big model trained it for 30 k steps,0.7298169136047363
translation,224,75,experimental-setup,30 k steps,with,460 k ( 3600 ? 128 ) tokens per batch,30 k steps with 460 k ( 3600 ? 128 ) tokens per batch,0.651505172252655
translation,224,75,experimental-setup,460 k ( 3600 ? 128 ) tokens per batch,with,cosine learning rate schedule,460 k ( 3600 ? 128 ) tokens per batch with cosine learning rate schedule,0.6551710367202759
translation,224,75,experimental-setup,experimental setup,For,transformer - big model,experimental setup For transformer - big model,0.5915344953536987
translation,224,76,experimental-setup,16 nvidia v100 gpus,selected,final model,16 nvidia v100 gpus selected final model,0.6421768069267273
translation,224,76,experimental-setup,final model,by,best perplexity,final model by best perplexity,0.5457448363304138
translation,224,76,experimental-setup,best perplexity,on,validation set,best perplexity on validation set,0.5161999464035034
translation,224,76,experimental-setup,experimental setup,selected,final model,experimental setup selected final model,0.6329764127731323
translation,224,24,experiments,uncertainty,of,monolingual sentences,uncertainty of monolingual sentences,0.5842666625976562
translation,224,24,experiments,monolingual sentences,by using,bilingual dictionary,monolingual sentences by using bilingual dictionary,0.6157636046409607
translation,224,24,experiments,bilingual dictionary,extracted from,authentic parallel data ( ?2.1 ),bilingual dictionary extracted from authentic parallel data ( ?2.1 ),0.5898820161819458
translation,224,175,experiments,low-frequency words,in,bitext,low-frequency words in bitext,0.5182464122772217
translation,224,175,experiments,low-frequency words,are,more difficult to predict,low-frequency words are more difficult to predict,0.5616454482078552
translation,224,175,experiments,bitext,are,more difficult to predict,bitext are more difficult to predict,0.6179003715515137
translation,224,175,experiments,more difficult to predict,than,medium - and high - frequency words,more difficult to predict than medium - and high - frequency words,0.6030756831169128
translation,224,23,model,uncertain monolingual sentences,implicitly hold,difficult patterns,uncertain monolingual sentences implicitly hold difficult patterns,0.6726682782173157
translation,224,23,model,uncertain monolingual sentences,exploit them to boost,self-training performance,uncertain monolingual sentences exploit them to boost self-training performance,0.6612952351570129
translation,224,23,model,model,investigate and identify,uncertain monolingual sentences,model investigate and identify uncertain monolingual sentences,0.6038762927055359
translation,224,150,model,another n-gram language model,at,target side,another n-gram language model at target side,0.47777390480041504
translation,224,150,model,another n-gram language model,to further filter out,data,another n-gram language model to further filter out data,0.7187055349349976
translation,224,150,model,data,with,potentially erroneous target sentences,data with potentially erroneous target sentences,0.610837459564209
translation,224,88,results,self- training,with,8 m synthetic data,self- training with 8 m synthetic data,0.6390355229377747
translation,224,88,results,8 m synthetic data,improve,nmt performance,8 m synthetic data improve nmt performance,0.6704809069633484
translation,224,88,results,nmt performance,by,significant margin,nmt performance by significant margin,0.5746587514877319
translation,224,88,results,bleu score,on,wmt,bleu score on wmt,0.5392301082611084
translation,224,88,results,significant margin,has,36.2,significant margin has 36.2,0.5459582209587097
translation,224,88,results,results,has,self- training,results has self- training,0.5300098061561584
translation,224,89,results,size,of,added monolingual data,size of added monolingual data,0.5373134016990662
translation,224,89,results,added monolingual data,does,bring much more benefit,added monolingual data does bring much more benefit,0.30467143654823303
translation,224,89,results,added monolingual data,not,bring much more benefit,added monolingual data not bring much more benefit,0.6931567192077637
translation,224,89,results,results,Increasing,size,results Increasing size,0.6089740991592407
translation,224,90,results,final performance,achieves,only 36.5 bleu points,final performance achieves only 36.5 bleu points,0.6218575835227966
translation,224,90,results,all the 40m monolingual sentences,has,final performance,all the 40m monolingual sentences has final performance,0.5468615889549255
translation,224,90,results,results,With,all the 40m monolingual sentences,results With all the 40m monolingual sentences,0.5636371374130249
translation,224,132,results,excessively high uncertainty,improves,translation performance,excessively high uncertainty improves translation performance,0.699313759803772
translation,224,132,results,translation performance,from,36.6 to 36.9,translation performance from 36.6 to 36.9,0.5367546677589417
translation,224,141,results,interesting finding,using,pseudo-sentences,interesting finding using pseudo-sentences,0.6454036235809326
translation,224,141,results,outperforms,using,manual translations,outperforms using manual translations,0.618732750415802
translation,224,141,results,pseudo-sentences,has,outperforms,pseudo-sentences has outperforms,0.6393612027168274
translation,224,148,results,srclm,achieves,marginal improvement,srclm achieves marginal improvement,0.676211416721344
translation,224,148,results,marginal improvement,over,randsamp,marginal improvement over randsamp,0.655687153339386
translation,224,149,results,outperforms,by,+ 0.7 bleu point,outperforms by + 0.7 bleu point,0.5868521928787231
translation,224,149,results,baseline randsamp,by,+ 0.7 bleu point,baseline randsamp by + 0.7 bleu point,0.519587516784668
translation,224,149,results,proposed uncsamp approach,has,outperforms,proposed uncsamp approach has outperforms,0.620826244354248
translation,224,149,results,outperforms,has,baseline randsamp,outperforms has baseline randsamp,0.5629758834838867
translation,224,149,results,results,has,proposed uncsamp approach,results has proposed uncsamp approach,0.6174801588058472
translation,224,151,results,20 % sentences,from,sampled 8 m sentences,20 % sentences from sampled 8 m sentences,0.5904582738876343
translation,224,151,results,our uncsamp approach,achieves,further improvement,our uncsamp approach achieves further improvement,0.6692086458206177
translation,224,151,results,further improvement,up to,+ 0.9 bleu point,further improvement up to + 0.9 bleu point,0.6184573769569397
translation,224,151,results,20 % sentences,has,our uncsamp approach,20 % sentences has our uncsamp approach,0.6123165488243103
translation,224,151,results,sampled 8 m sentences,has,our uncsamp approach,sampled 8 m sentences has our uncsamp approach,0.6232550740242004
translation,224,158,results,our transformer - big models,trained on,authentic parallel data,our transformer - big models trained on authentic parallel data,0.7183964252471924
translation,224,158,results,our transformer - big models,achieve,performance competitive,our transformer - big models achieve performance competitive,0.64946049451828
translation,224,158,results,performance competitive,with or even better,submissions to wmt competitions,performance competitive with or even better submissions to wmt competitions,0.7195425033569336
translation,224,158,results,results,has,our transformer - big models,results has our transformer - big models,0.5366196036338806
translation,224,159,results,randsamp,improves,performance,randsamp improves performance,0.7050538659095764
translation,224,159,results,performance,by,+ 2.0 and + 0.9 bleu points,performance by + 2.0 and + 0.9 bleu points,0.5765134692192078
translation,224,159,results,+ 2.0 and + 0.9 bleu points,on,en?de and en? zh tasks,+ 2.0 and + 0.9 bleu points on en?de and en? zh tasks,0.5614299774169922
translation,224,160,results,self-training,achieves,further significant improvement,self-training achieves further significant improvement,0.6113027930259705
translation,224,160,results,further significant improvement,by,+ 1.1 and + 0.6 bleu points,further significant improvement by + 1.1 and + 0.6 bleu points,0.5853219628334045
translation,224,160,results,+ 1.1 and + 0.6 bleu points,over,random sampling strategy,+ 1.1 and + 0.6 bleu points over random sampling strategy,0.6831933259963989
translation,224,160,results,uncertaintybased sampling strategy uncsamp,has,self-training,uncertaintybased sampling strategy uncsamp has self-training,0.5718821883201599
translation,224,160,results,results,With,uncertaintybased sampling strategy uncsamp,results With uncertaintybased sampling strategy uncsamp,0.6452887654304504
translation,224,168,results,our uncsamp approach,improves,translation performance,our uncsamp approach improves translation performance,0.6867980360984802
translation,224,168,results,translation performance,on,all sentences,translation performance on all sentences,0.4790404140949249
translation,224,168,results,results,has,our uncsamp approach,results has our uncsamp approach,0.5755133628845215
translation,224,176,results,monolingual data,by,selftraining,monolingual data by selftraining,0.5675637125968933
translation,224,176,results,monolingual data,improves,prediction performance,monolingual data improves prediction performance,0.6342294812202454
translation,224,176,results,prediction performance,of,low-frequency words,prediction performance of low-frequency words,0.5707505941390991
translation,224,176,results,results,adding,monolingual data,results adding monolingual data,0.5523940324783325
translation,224,177,results,randsamp,on,lowfrequency words,randsamp on lowfrequency words,0.5102736353874207
translation,224,177,results,significantly,on,lowfrequency words,significantly on lowfrequency words,0.5507671236991882
translation,224,177,results,uncsamp approach,has,outperforms,uncsamp approach has outperforms,0.6311938762664795
translation,224,177,results,outperforms,has,randsamp,outperforms has randsamp,0.608092188835144
translation,224,177,results,randsamp,has,significantly,randsamp has significantly,0.6749048233032227
translation,224,177,results,results,has,uncsamp approach,results has uncsamp approach,0.570088267326355
translation,225,7,experimental-setup,open-source high- performance inference toolkit,for,transformer models,open-source high- performance inference toolkit for transformer models,0.6054125428199768
translation,225,7,experimental-setup,experimental setup,release,open-source high- performance inference toolkit,experimental setup release open-source high- performance inference toolkit,0.6582525372505188
translation,225,15,experimental-setup,training phase,trained,variety of smaller compact student models,training phase trained variety of smaller compact student models,0.7579296231269836
translation,225,15,experimental-setup,variety of smaller compact student models,using,common teacher -student training approach,variety of smaller compact student models using common teacher -student training approach,0.6752070784568787
translation,225,15,experimental-setup,common teacher -student training approach,on,opensource multilingual training platform tentrans - py,common teacher -student training approach on opensource multilingual training platform tentrans - py,0.5467550754547119
translation,225,15,experimental-setup,experimental setup,In terms of,training phase,experimental setup In terms of training phase,0.6554638743400574
translation,225,15,experimental-setup,experimental setup,trained,variety of smaller compact student models,experimental setup trained variety of smaller compact student models,0.7183024883270264
translation,225,36,experimental-setup,shallow decoder architecture,of,all student models,shallow decoder architecture of all student models,0.5372971296310425
translation,225,36,experimental-setup,number of decoder layers,set to,1,number of decoder layers set to 1,0.6796901226043701
translation,225,36,experimental-setup,experimental setup,adopt,deep encoder,experimental setup adopt deep encoder,0.6114229559898376
translation,225,36,experimental-setup,experimental setup,adopt,shallow decoder architecture,experimental setup adopt shallow decoder architecture,0.6116988062858582
translation,225,8,model,optimizations,built on top of,inference engine,optimizations built on top of inference engine,0.6891916990280151
translation,225,8,model,inference engine,including,attention caching,inference engine including attention caching,0.6707277297973633
translation,225,8,model,inference engine,including,kernel fusion,inference engine including kernel fusion,0.6461966633796692
translation,225,8,model,inference engine,including,early -stop,inference engine including early -stop,0.6808961033821106
translation,225,8,model,model,has,optimizations,model has optimizations,0.5773434638977051
translation,225,17,model,strategy,for,shared task,strategy for shared task,0.6281225681304932
translation,225,17,model,shared task,includes,attention caching,shared task includes attention caching,0.6567121744155884
translation,225,17,model,shared task,includes,kernel fusion,shared task includes kernel fusion,0.6656469106674194
translation,225,17,model,shared task,includes,early -stop,shared task includes early -stop,0.672065258026123
translation,225,17,model,inference phase,has,strategy,inference phase has strategy,0.5975699424743652
translation,225,17,model,model,For,inference phase,model For inference phase,0.632452130317688
translation,225,53,model,tentrans -decoding,is,open-source highoptimized inference engine,tentrans -decoding is open-source highoptimized inference engine,0.5498108267784119
translation,225,53,model,open-source highoptimized inference engine,for,transformer models,open-source highoptimized inference engine for transformer models,0.6080913543701172
translation,225,53,model,model,has,tentrans -decoding,model has tentrans -decoding,0.5959574580192566
translation,225,57,model,of caching linear projections,in,transformer decoder layers,of caching linear projections in transformer decoder layers,0.5352506041526794
translation,225,57,model,common technique,has,of caching linear projections,common technique has of caching linear projections,0.5598416924476624
translation,225,57,model,model,apply,common technique,model apply common technique,0.7070797085762024
translation,225,58,model,linear transformations,for,keys and values,linear transformations for keys and values,0.6210073828697205
translation,225,58,model,linear transformations,for,each step,linear transformations for each step,0.616599977016449
translation,225,58,model,linear transformations,before,cross-attention layers,linear transformations before cross-attention layers,0.5918892025947571
translation,225,58,model,keys and values,before,cross-attention layers,keys and values before cross-attention layers,0.6104729175567627
translation,225,58,model,each step,of,decoder self-attention layers,each step of decoder self-attention layers,0.5619139671325684
translation,225,58,model,model,cache,linear transformations,model cache linear transformations,0.8028528690338135
translation,225,60,model,kernel launching overhead,enhance,gpu computation efficiency,kernel launching overhead enhance gpu computation efficiency,0.5984525680541992
translation,225,60,model,gpu computation efficiency,implement,many kernel fusion techniques,gpu computation efficiency implement many kernel fusion techniques,0.6548749804496765
translation,225,60,model,many kernel fusion techniques,for,transformer models,many kernel fusion techniques for transformer models,0.5917646288871765
translation,225,60,model,model,To reduce,kernel launching overhead,model To reduce kernel launching overhead,0.5876317024230957
translation,225,62,model,layer normalization,between,two general matrix multiplications ( gemms ),layer normalization between two general matrix multiplications ( gemms ),0.6237599849700928
translation,225,62,model,layer normalization,reorganize,"addbias kernel , residual network","layer normalization reorganize addbias kernel , residual network",0.7205088138580322
translation,225,62,model,layer normalization,reorganize,layernormalization kernel,layer normalization reorganize layernormalization kernel,0.7107259035110474
translation,225,62,model,two general matrix multiplications ( gemms ),reorganize,layernormalization kernel,two general matrix multiplications ( gemms ) reorganize layernormalization kernel,0.7109344601631165
translation,225,62,model,layernormalization kernel,into,single one,layernormalization kernel into single one,0.5766983032226562
translation,225,62,model,model,For,layer normalization,model For layer normalization,0.5932480692863464
translation,225,64,model,feed-forward network layers,of,transformer model,feed-forward network layers of transformer model,0.5620419979095459
translation,225,64,model,addbias kernel and relu kernel,fused into,one,addbias kernel and relu kernel fused into one,0.736049473285675
translation,225,64,model,feed-forward network layers,has,addbias kernel and relu kernel,feed-forward network layers has addbias kernel and relu kernel,0.5549476146697998
translation,225,64,model,transformer model,has,addbias kernel and relu kernel,transformer model has addbias kernel and relu kernel,0.5653952360153198
translation,225,64,model,model,In,feed-forward network layers,model In feed-forward network layers,0.5253589153289795
translation,225,66,model,every encoder or decoder layer,fuse,addbias kernel,every encoder or decoder layer fuse addbias kernel,0.7676272392272949
translation,225,66,model,every encoder or decoder layer,fuse,residual network,every encoder or decoder layer fuse residual network,0.7416074275970459
translation,225,50,results,results,correlate well with,expectation,results correlate well with expectation,0.5432755947113037
translation,225,50,results,expectation,that,more model parameters,expectation that more model parameters,0.6704683899879456
translation,225,50,results,more model parameters,lead to,better performance,more model parameters lead to better performance,0.7028466463088989
translation,225,50,results,results,correlate well with,expectation,results correlate well with expectation,0.5432755947113037
translation,225,50,results,results,has,results,results has results,0.48582205176353455
translation,225,51,results,our distillation student models,show,strong competitiveness,our distillation student models show strong competitiveness,0.714168906211853
translation,225,51,results,strong competitiveness,when,number of parameters,strong competitiveness when number of parameters,0.6452957987785339
translation,225,51,results,number of parameters,is,greatly reduced,number of parameters is greatly reduced,0.5832905769348145
translation,225,51,results,results,has,our distillation student models,results has our distillation student models,0.5644720792770386
translation,225,85,results,tentrans,leads to,2.62x speedup,tentrans leads to 2.62x speedup,0.6322405338287354
translation,225,85,results,- decoding,leads to,2.62x speedup,- decoding leads to 2.62x speedup,0.6386806964874268
translation,225,85,results,2.62x speedup,than,tentrans - py baseline,2.62x speedup than tentrans - py baseline,0.4999583661556244
translation,225,85,results,2.62x speedup,without,inference optimizations,2.62x speedup without inference optimizations,0.6875733137130737
translation,225,85,results,tentrans,has,- decoding,tentrans has - decoding,0.6430904865264893
translation,225,85,results,results,has,tentrans,results has tentrans,0.5669392347335815
translation,225,85,results,results,has,- decoding,results has - decoding,0.5764846205711365
translation,225,86,results,7.23 x speedup,with,no accuracy loss,7.23 x speedup with no accuracy loss,0.6180459260940552
translation,225,86,results,no accuracy loss,over,baseline,no accuracy loss over baseline,0.6835642457008362
translation,226,158,ablation-analysis,african languages,not originally included in,model pretraining,african languages not originally included in model pretraining,0.6226782202720642
translation,226,158,ablation-analysis,reconstruction,together with,backtranslation,reconstruction together with backtranslation,0.6309093832969666
translation,226,158,ablation-analysis,reconstruction,introduces,more noise,reconstruction introduces more noise,0.6542412042617798
translation,226,158,ablation-analysis,more noise,could harm,crosslingual learning,more noise could harm crosslingual learning,0.7406725883483887
translation,226,158,ablation-analysis,african languages,has,reconstruction,african languages has reconstruction,0.542181670665741
translation,226,158,ablation-analysis,model pretraining,has,reconstruction,model pretraining has reconstruction,0.5490744113922119
translation,226,158,ablation-analysis,ablation analysis,For,african languages,ablation analysis For african languages,0.5867794752120972
translation,226,124,baselines,bt&rec,refers to,joint backtranslation and reconstruction,bt&rec refers to joint backtranslation and reconstruction,0.610887348651886
translation,226,124,baselines,bt&rec,while,finetuning,bt&rec while finetuning,0.6747986078262329
translation,226,124,baselines,joint backtranslation and reconstruction,while,finetuning,joint backtranslation and reconstruction while finetuning,0.6084977984428406
translation,226,124,baselines,baselines,has,bt&rec,baselines has bt&rec,0.5999364852905273
translation,226,110,experimental-setup,pretrained mt5 - base model,using,hugging face 's auto-modelforseq2seqlm,pretrained mt5 - base model using hugging face 's auto-modelforseq2seqlm,0.6441847681999207
translation,226,110,experimental-setup,training process,with,"weights &biases ( biewald , 2020 )","training process with weights &biases ( biewald , 2020 )",0.638380229473114
translation,226,110,experimental-setup,experimental setup,initialized,pretrained mt5 - base model,experimental setup initialized pretrained mt5 - base model,0.6807445287704468
translation,226,129,experimental-setup,learning rate,of,5e ?4,learning rate of 5e ?4,0.6386009454727173
translation,226,129,experimental-setup,learning rate,with,gradient accumulation,learning rate with gradient accumulation,0.6354437470436096
translation,226,129,experimental-setup,batch size,of,32 sentences,batch size of 32 sentences,0.5807510614395142
translation,226,129,experimental-setup,batch size,of,256 sentences,batch size of 256 sentences,0.5777139663696289
translation,226,129,experimental-setup,gradient accumulation,up to,early stopping patience,gradient accumulation up to early stopping patience,0.624538242816925
translation,226,129,experimental-setup,gradient accumulation,batch of,256 sentences,gradient accumulation batch of 256 sentences,0.7344293594360352
translation,226,129,experimental-setup,early stopping patience,of,100 evaluation steps,early stopping patience of 100 evaluation steps,0.5839067697525024
translation,226,135,experimental-setup,number of sentences,for,backtranslation,number of sentences for backtranslation,0.5840378403663635
translation,226,135,experimental-setup,backtranslation,to,50,backtranslation to 50,0.5911059379577637
translation,226,135,experimental-setup,experimental setup,reduced,number of sentences,experimental setup reduced number of sentences,0.6176290512084961
translation,226,137,experimental-setup,gradient accumulation,to increase,updates,gradient accumulation to increase updates,0.6471864581108093
translation,226,137,experimental-setup,updates,from,initial batch size,updates from initial batch size,0.5731478333473206
translation,226,137,experimental-setup,initial batch size,of,64 sentences,initial batch size of 64 sentences,0.5547954440116882
translation,226,137,experimental-setup,batch gradient computation size,of,4096 sentences,batch gradient computation size of 4096 sentences,0.5515273809432983
translation,226,138,experimental-setup,pytorch 's dataparallel package,to parallelize,training,pytorch 's dataparallel package to parallelize training,0.7972198724746704
translation,226,138,experimental-setup,training,across,gpus,training across gpus,0.6101345419883728
translation,226,138,experimental-setup,experimental setup,utilized,pytorch 's dataparallel package,experimental setup utilized pytorch 's dataparallel package,0.5934396386146545
translation,226,139,experimental-setup,learning rate ( lr ),of,3e ?6,learning rate ( lr ) of 3e ?6,0.6167272925376892
translation,226,141,experimental-setup,evaluations,made using,spbleu,evaluations made using spbleu,0.7444012761116028
translation,226,141,experimental-setup,spbleu,has,"sentencepiece ( kudo and richardson , 2018 )","spbleu has sentencepiece ( kudo and richardson , 2018 )",0.5746418833732605
translation,226,141,experimental-setup,experimental setup,has,evaluations,experimental setup has evaluations,0.5171922445297241
translation,226,5,experiments,mmtafrica,for,two non-african languages,mmtafrica for two non-african languages,0.635752260684967
translation,226,5,experiments,multilingual translation system,for,six african languages,multilingual translation system for six african languages,0.5488143563270569
translation,226,5,experiments,mmtafrica,has,multilingual translation system,mmtafrica has multilingual translation system,0.5311139822006226
translation,226,82,hyperparameters,mt5 model,pretrained with,modification of the masked language modelling objective,mt5 model pretrained with modification of the masked language modelling objective,0.632878839969635
translation,226,82,hyperparameters,hyperparameters,has,mt5 model,hyperparameters has mt5 model,0.5430538058280945
translation,226,6,model,multilingual translation,introduce,novel backtranslation and reconstruction objective,multilingual translation introduce novel backtranslation and reconstruction objective,0.5860769152641296
translation,226,6,model,novel backtranslation and reconstruction objective,to effectively leverage,monolingual data,novel backtranslation and reconstruction objective to effectively leverage monolingual data,0.6265674233436584
translation,226,6,model,multilingual translation,has,concerning african languages,multilingual translation has concerning african languages,0.5072537064552307
translation,226,6,model,novel backtranslation and reconstruction objective,has,bt&rec,novel backtranslation and reconstruction objective has bt&rec,0.5957397818565369
translation,226,6,model,model,For,multilingual translation,model For multilingual translation,0.5833494663238525
translation,226,54,results,self-supervision,improves,zero-shot translation quality,self-supervision improves zero-shot translation quality,0.6458898186683655
translation,226,54,results,zero-shot translation quality,in,multilingual models,zero-shot translation quality in multilingual models,0.4708293080329895
translation,226,54,results,results,has,self-supervision,results has self-supervision,0.48805171251296997
translation,226,146,results,target language,is,fon,target language is fon,0.6194305419921875
translation,226,146,results,target language,observe,considerable boost,target language observe considerable boost,0.6134685277938843
translation,226,146,results,considerable boost,in,spbleu,considerable boost in spbleu,0.6641599535942078
translation,226,146,results,spbleu,of,bt setting,spbleu of bt setting,0.6285784244537354
translation,226,146,results,significantly outperformed,has,base and bt&rec,significantly outperformed has base and bt&rec,0.615797221660614
translation,226,146,results,results,When,target language,results When target language,0.580933690071106
translation,226,149,results,target language,is,ibo,target language is ibo,0.5958468914031982
translation,226,149,results,bt&rec,gives,best results,bt&rec gives best results,0.6427072286605835
translation,226,149,results,target language,has,bt&rec,target language has bt&rec,0.6078522801399231
translation,226,149,results,ibo,has,bt&rec,ibo has bt&rec,0.6893916726112366
translation,226,149,results,results,when,target language,results when target language,0.580933690071106
translation,226,156,results,african languages,included in,original model pretraining,african languages included in original model pretraining,0.6026387214660645
translation,226,156,results,african languages,using,bt&rec setting,african languages using bt&rec setting,0.6457793712615967
translation,226,156,results,bt&rec setting,for,finetuning,bt&rec setting for finetuning,0.670835018157959
translation,226,156,results,finetuning,produces,best results,finetuning produces best results,0.6501925587654114
translation,226,156,results,results,For,african languages,results For african languages,0.5264517068862915
translation,226,161,results,mmtafrica,compared,mmtafrica,mmtafrica compared mmtafrica,0.7096309065818787
translation,226,161,results,mmtafrica,with,"m2m mmt ( fan et al. , 2020 ) benchmark results","mmtafrica with m2m mmt ( fan et al. , 2020 ) benchmark results",0.6027893424034119
translation,226,161,results,results,compared,mmtafrica,results compared mmtafrica,0.6224391460418701
translation,226,161,results,results,has,mmtafrica,results has mmtafrica,0.5309880971908569
translation,226,162,results,all language pairs,except,swa?eng,all language pairs except swa?eng,0.7223153114318848
translation,226,162,results,all language pairs,report,improvement,all language pairs report improvement,0.6656487584114075
translation,226,162,results,improvement,from,mmtafrica,improvement from mmtafrica,0.5115172266960144
translation,226,162,results,improvement,ranging from,+ 0.58,improvement ranging from + 0.58,0.5875144004821777
translation,226,162,results,spbleu gains,ranging from,+ 0.58,spbleu gains ranging from + 0.58,0.6122923493385315
translation,226,162,results,+ 0.58,in,swa? fra,+ 0.58 in swa? fra,0.588757336139679
translation,226,162,results,+ 19.46,in,fra?xho,+ 19.46 in fra?xho,0.628174901008606
translation,226,162,results,mmtafrica,has,spbleu gains,mmtafrica has spbleu gains,0.6098450422286987
translation,226,162,results,results,On,all language pairs,results On all language pairs,0.4689609706401825
translation,227,144,ablation-analysis,ambiguous coco,demands,more visual contribution,ambiguous coco demands more visual contribution,0.7083996534347534
translation,227,144,ablation-analysis,ablation analysis,For,ambiguous coco,ablation analysis For ambiguous coco,0.6652917861938477
translation,227,120,baselines,baseline,is,conventional text-only transformer,baseline is conventional text-only transformer,0.5302451848983765
translation,227,120,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,227,121,hyperparameters,inner feed -forward layer filter size,set to,2048,inner feed -forward layer filter size set to 2048,0.7049269080162048
translation,227,121,hyperparameters,encoder-decoder,has,6 - layer stacked transformer network,encoder-decoder has 6 - layer stacked transformer network,0.5695673823356628
translation,227,121,hyperparameters,hyperparameters,has,encoder-decoder,hyperparameters has encoder-decoder,0.5513951182365417
translation,227,122,hyperparameters,dropout,set to,p = 0.1,dropout set to p = 0.1,0.7511864304542542
translation,227,122,hyperparameters,dropout,use,adam optimizer,dropout use adam optimizer,0.5638203024864197
translation,227,122,hyperparameters,adam optimizer,to tune,parameter,adam optimizer to tune parameter,0.7087095975875854
translation,227,122,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,227,122,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,227,124,hyperparameters,model,up to,10,model up to 10,0.6873244643211365
translation,227,124,hyperparameters,10,",",000 steps,"10 , 000 steps",0.6261343359947205
translation,227,124,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,227,127,hyperparameters,noise vector 's dimension,is,100,noise vector 's dimension is 100,0.5952325463294983
translation,227,127,hyperparameters,generated visual feature,is,128 ? 128,generated visual feature is 128 ? 128,0.6025587320327759
translation,227,127,hyperparameters,imagination network,has,noise vector 's dimension,imagination network has noise vector 's dimension,0.5041279196739197
translation,227,127,hyperparameters,imagination network,has,generated visual feature,imagination network has generated visual feature,0.5653890371322632
translation,227,127,hyperparameters,hyperparameters,For,imagination network,hyperparameters For imagination network,0.5853120684623718
translation,227,131,hyperparameters,batch size,is,64,batch size is 64,0.6388692259788513
translation,227,131,hyperparameters,learning rate,initialized to be,2e ?4,learning rate initialized to be 2e ?4,0.6392776966094971
translation,227,131,hyperparameters,learning rate,decayed to,half,learning rate decayed to half,0.7188597321510315
translation,227,131,hyperparameters,half,of,previous value,half of previous value,0.6430712938308716
translation,227,131,hyperparameters,previous value,every,100 epochs,previous value every 100 epochs,0.715559184551239
translation,227,131,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,227,131,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,227,133,hyperparameters,margin size,set to,0.1,margin size set to 0.1,0.6870176196098328
translation,227,133,hyperparameters,hyperparameters,has,margin size,hyperparameters has margin size,0.5286725759506226
translation,227,133,hyperparameters,hyperparameters,has,balance weight,hyperparameters has balance weight,0.5299758315086365
translation,227,6,model,novel machine translation method,via,visual imagination,novel machine translation method via visual imagination,0.6169260144233704
translation,227,6,model,imagit,has,novel machine translation method,imagit has novel machine translation method,0.5796510577201843
translation,227,6,model,model,propose,imagit,model propose imagit,0.69065922498703
translation,227,7,model,imagit,learns to generate,visual representation,imagit learns to generate visual representation,0.8179782629013062
translation,227,7,model,imagit,utilizes,source sentence,imagit utilizes source sentence,0.6493982672691345
translation,227,7,model,imagit,utilizes,  imagined representation,imagit utilizes   imagined representation,0.6069498658180237
translation,227,7,model,visual representation,from,source sentence,visual representation from source sentence,0.49305859208106995
translation,227,7,model,visual representation,from,source sentence,visual representation from source sentence,0.49305859208106995
translation,227,7,model,  imagined representation,to produce,target translation,  imagined representation to produce target translation,0.6846925616264343
translation,227,7,model,model,has,imagit,model has imagit,0.6209543943405151
translation,227,24,model,end-to - end machine translation model,embedded in,visual semantics,end-to - end machine translation model embedded in visual semantics,0.6302748918533325
translation,227,24,model,visual semantics,with,generative imagination ( imagit ),visual semantics with generative imagination ( imagit ),0.6442489624023438
translation,227,24,model,model,propose,end-to - end machine translation model,model propose end-to - end machine translation model,0.6413075923919678
translation,227,25,model,imagit,transforms,word representations,imagit transforms word representations,0.8166103959083557
translation,227,25,model,word representations,into,visual features,word representations into visual features,0.5371590256690979
translation,227,25,model,visual features,through,attentive generator,visual features through attentive generator,0.642629861831665
translation,227,25,model,attentive generator,effectively capture,semantics,attentive generator effectively capture semantics,0.6565103530883789
translation,227,25,model,semantics,of,global and local levels,semantics of global and local levels,0.5984421968460083
translation,227,25,model,semantics,both,global and local levels,semantics both global and local levels,0.7223252654075623
translation,227,25,model,generated visual representations,considered as,semantic -equivalent reconstructions,generated visual representations considered as semantic -equivalent reconstructions,0.5973639488220215
translation,227,25,model,semantic -equivalent reconstructions,of,sentences,semantic -equivalent reconstructions of sentences,0.5972609519958496
translation,227,25,model,source language sentence,has,imagit,source language sentence has imagit,0.5953570008277893
translation,227,25,model,model,Given,source language sentence,model Given source language sentence,0.6508367657661438
translation,227,27,model,target language sentence,based on,joint features,target language sentence based on joint features,0.5782287120819092
translation,227,27,model,model,In,final stage,model In final stage,0.5740973353385925
translation,227,28,model,target language translation loss,based on,cross-entropy,target language translation loss based on cross-entropy,0.5695306658744812
translation,227,128,model,upsampling and residual block,in,visual feature transformers,upsampling and residual block in visual feature transformers,0.5241526365280151
translation,227,128,model,upsampling and residual block,consist of,relu activation,upsampling and residual block consist of relu activation,0.661309540271759
translation,227,128,model,visual feature transformers,consist of,3 ? 3 stride 1 convolution,visual feature transformers consist of 3 ? 3 stride 1 convolution,0.6071234941482544
translation,227,128,model,visual feature transformers,consist of,batch normalization,visual feature transformers consist of batch normalization,0.620694100856781
translation,227,128,model,model,has,upsampling and residual block,model has upsampling and residual block,0.555286705493927
translation,227,29,results,proposed imagit model,does not require,images,proposed imagit model does not require images,0.694266140460968
translation,227,29,results,proposed imagit model,leverage,visual information,proposed imagit model leverage visual information,0.7613753080368042
translation,227,29,results,images,as,input,images as input,0.5512511134147644
translation,227,29,results,input,during,inference time,input during inference time,0.6917708516120911
translation,227,29,results,visual information,through,imagination,visual information through imagination,0.6666988134384155
translation,227,123,results,learning rate,increases,linearly,learning rate increases linearly,0.7532610297203064
translation,227,123,results,learning rate,decreases with,step number 's inverse square root,learning rate decreases with step number 's inverse square root,0.7139381170272827
translation,227,123,results,linearly,for,warmup strategy,linearly for warmup strategy,0.7041444778442383
translation,227,123,results,warmup strategy,with,8,warmup strategy with 8,0.7037848234176636
translation,227,123,results,warmup strategy,with,000 steps,warmup strategy with 000 steps,0.6582847237586975
translation,227,123,results,8,",",000 steps,"8 , 000 steps",0.6454721689224243
translation,227,123,results,results,has,learning rate,results has learning rate,0.5624368786811829
translation,227,135,results,results,for,"en-de test2016 , en-de test2017 , en-fr test2016 and en-fr test2017 tasks","results for en-de test2016 , en-de test2017 , en-fr test2016 and en-fr test2017 tasks",0.600971519947052
translation,227,136,results,"our text-only transformer baseline ( vaswani et al. , 2017 )",has,similar results,"our text-only transformer baseline ( vaswani et al. , 2017 ) has similar results",0.5170083045959473
translation,227,136,results,similar results,compared to,most prior mnmt works,similar results compared to most prior mnmt works,0.6760096549987793
translation,227,136,results,"our text-only transformer baseline ( vaswani et al. , 2017 )",has,similar results,"our text-only transformer baseline ( vaswani et al. , 2017 ) has similar results",0.5170083045959473
translation,227,136,results,results,has,"our text-only transformer baseline ( vaswani et al. , 2017 )","results has our text-only transformer baseline ( vaswani et al. , 2017 )",0.5058377385139465
translation,227,138,results,imagit,gains,improvements,imagit gains improvements,0.8560401797294617
translation,227,138,results,improvements,over,text-only transformer baseline,improvements over text-only transformer baseline,0.6522213220596313
translation,227,138,results,text-only transformer baseline,on,four evaluation datasets,text-only transformer baseline on four evaluation datasets,0.475579172372818
translation,227,138,results,results,has,imagit,results has imagit,0.5902113318443298
translation,227,140,results,noticeable margin,in terms of,bleu score,noticeable margin in terms of bleu score,0.6475627422332764
translation,227,140,results,noticeable margin,in terms of,meteor score,noticeable margin in terms of meteor score,0.6842107772827148
translation,227,140,results,results,observe,our approach,results observe our approach,0.6077187061309814
translation,227,141,results,competitive,with,imagit + ground truth,competitive with imagit + ground truth,0.6564992070198059
translation,227,141,results,competitive,is,our translation decoder,competitive is our translation decoder,0.6056216359138489
translation,227,141,results,imagit + ground truth,is,our translation decoder,imagit + ground truth is our translation decoder,0.5124207735061646
translation,227,141,results,our translation decoder,taking,ground truth visual representations,our translation decoder taking ground truth visual representations,0.5909392833709717
translation,227,141,results,ground truth visual representations,instead of,imagined ones,ground truth visual representations instead of imagined ones,0.6068261861801147
translation,227,141,results,results,has,our imagit,results has our imagit,0.6123806238174438
translation,227,143,results,results,for,en-de en-fr am-biguous coco,results for en-de en-fr am-biguous coco,0.7069555521011353
translation,227,145,results,our imagit,benefits from,visual imagination,our imagit benefits from visual imagination,0.6890099048614502
translation,227,145,results,substantially outperforms,on,ambiguous coco,substantially outperforms on ambiguous coco,0.5562155246734619
translation,227,145,results,previous works,on,ambiguous coco,previous works on ambiguous coco,0.5959405303001404
translation,227,145,results,substantially outperforms,has,previous works,substantially outperforms has previous works,0.5692922472953796
translation,227,145,results,results,has,our imagit,results has our imagit,0.6123806238174438
translation,227,167,results,bleu score,of,text-only nmt,bleu score of text-only nmt,0.5211924314498901
translation,227,167,results,text-only nmt,has,decreases,text-only nmt has decreases,0.5997369289398193
translation,227,167,results,decreases,has,1.3,decreases has 1.3,0.5636850595474243
translation,227,167,results,decreases,has,0.5,decreases has 0.5,0.5057767629623413
translation,227,174,results,large bleu score drop,of,text-only,large bleu score drop of text-only,0.5543866157531738
translation,227,174,results,text-only,Transformer,baseline,text-only Transformer baseline,0.7275790572166443
translation,227,174,results,mnmt and imagit,are,relatively smaller,mnmt and imagit are relatively smaller,0.5997575521469116
translation,227,174,results,text-only,has,baseline,text-only has baseline,0.5822601914405823
translation,227,174,results,results,observe,large bleu score drop,results observe large bleu score drop,0.5818338394165039
translation,227,176,results,our imagit model,pretrained with,half ms coco,our imagit model pretrained with half ms coco,0.6508170962333679
translation,227,176,results,half ms coco,gain,0.6 meteor increase,half ms coco gain 0.6 meteor increase,0.7885729670524597
translation,227,176,results,more apparent,when training with,whole ms coco,more apparent when training with whole ms coco,0.7371038794517517
translation,228,21,baselines,simple policy search methods,try and find,best policies,simple policy search methods try and find best policies,0.6580432653427124
translation,228,21,baselines,best policies,using,expensive grid search,best policies using expensive grid search,0.644636869430542
translation,228,21,baselines,best policies,using,pruned - tree search methods,best policies using pruned - tree search methods,0.6780380606651306
translation,228,21,baselines,baselines,explore,simple policy search methods,baselines explore simple policy search methods,0.6375429630279541
translation,228,82,baselines,nmt setup,trained on,nepali-english corpus,nmt setup trained on nepali-english corpus,0.7069377899169922
translation,228,84,baselines,baselines,has,hi-en random baseline,baselines has hi-en random baseline,0.6077936887741089
translation,228,85,baselines,nmt system,trained on,high-resource hindi-english dataset,nmt system trained on high-resource hindi-english dataset,0.7258452773094177
translation,228,85,baselines,high-resource hindi-english dataset,with,nepali-english validation and test sets,high-resource hindi-english dataset with nepali-english validation and test sets,0.6024936437606812
translation,228,85,baselines,baselines,has,nmt system,baselines has nmt system,0.5831279754638672
translation,228,88,baselines,hi-en random baseline,as,starting point,hi-en random baseline as starting point,0.5551087856292725
translation,228,88,baselines,baselines,has,multilingual transformer,baselines has multilingual transformer,0.5726940631866455
translation,228,89,baselines,non-mab search - based curriculum baselines,are,grid search,non-mab search - based curriculum baselines are grid search,0.5867484211921692
translation,228,89,baselines,static curriculum,learned by searching over,space of sampling probabilities,static curriculum learned by searching over space of sampling probabilities,0.7947701811790466
translation,228,89,baselines,space of sampling probabilities,for,bins,space of sampling probabilities for bins,0.6384220123291016
translation,228,89,baselines,non-mab search - based curriculum baselines,has,static curriculum,non-mab search - based curriculum baselines has static curriculum,0.5675067901611328
translation,228,89,baselines,grid search,has,static curriculum,grid search has static curriculum,0.5762282013893127
translation,228,89,baselines,baselines,has,non-mab search - based curriculum baselines,baselines has non-mab search - based curriculum baselines,0.5779368877410889
translation,228,20,experiments,contextual multi-arm bandits,for,our agents,contextual multi-arm bandits for our agents,0.6060822606086731
translation,228,20,experiments,our agents,which learn,multilingual data sampling policies,our agents which learn multilingual data sampling policies,0.7035554051399231
translation,228,20,experiments,multilingual data sampling policies,jointly with,training of the nmt system,multilingual data sampling policies jointly with training of the nmt system,0.5764687061309814
translation,228,65,hyperparameters,grid search,search over,"range [ 0 , 1 ]","grid search search over range [ 0 , 1 ]",0.7263665795326233
translation,228,65,hyperparameters,"range [ 0 , 1 ]",for,sampling,"range [ 0 , 1 ] for sampling",0.6256492137908936
translation,228,65,hyperparameters,sampling,in increments of,0.1,sampling in increments of 0.1,0.6275525689125061
translation,228,65,hyperparameters,hyperparameters,has,grid search,hyperparameters has grid search,0.5450132489204407
translation,228,66,hyperparameters,pruned tree-search,uses,beam width,pruned tree-search uses beam width,0.6042575240135193
translation,228,66,hyperparameters,beam width,of,1,beam width of 1,0.6785910725593567
translation,228,66,hyperparameters,hyperparameters,has,pruned tree-search,hyperparameters has pruned tree-search,0.5544559359550476
translation,228,67,hyperparameters,phase duration,for,tree -search,phase duration for tree -search,0.6684442162513733
translation,228,67,hyperparameters,phase duration,set to,one epoch,phase duration set to one epoch,0.7138528823852539
translation,228,67,hyperparameters,tree -search,set to,one epoch,tree -search set to one epoch,0.7055611610412598
translation,228,67,hyperparameters,one epoch,of,nmt training,one epoch of nmt training,0.587978184223175
translation,228,67,hyperparameters,hyperparameters,has,phase duration,hyperparameters has phase duration,0.5369439721107483
translation,228,68,hyperparameters,5 or 10 concurrent contextual mabs,implemented as,two 256 - dimensional feed forward neural networks,5 or 10 concurrent contextual mabs implemented as two 256 - dimensional feed forward neural networks,0.6337006092071533
translation,228,68,hyperparameters,two 256 - dimensional feed forward neural networks,trained using,rmsprop,two 256 - dimensional feed forward neural networks trained using rmsprop,0.7672680020332336
translation,228,68,hyperparameters,rmsprop,with,learning rate,rmsprop with learning rate,0.6125419735908508
translation,228,68,hyperparameters,rmsprop,with,decay,rmsprop with decay,0.6644090414047241
translation,228,68,hyperparameters,learning rate,of,0.00025,learning rate of 0.00025,0.589148998260498
translation,228,68,hyperparameters,decay,of,0.95,decay of 0.95,0.6119470000267029
translation,228,68,hyperparameters,hyperparameters,use,5 or 10 concurrent contextual mabs,hyperparameters use 5 or 10 concurrent contextual mabs,0.6467825174331665
translation,228,72,hyperparameters,exploration strategy,use,linearly decaying epsilon function,exploration strategy use linearly decaying epsilon function,0.6342772245407104
translation,228,72,hyperparameters,linearly decaying epsilon function,with,decay period,linearly decaying epsilon function with decay period,0.6559873223304749
translation,228,72,hyperparameters,decay period,set to,25 k steps,decay period set to 25 k steps,0.7180442214012146
translation,228,72,hyperparameters,hyperparameters,For,exploration strategy,hyperparameters For exploration strategy,0.5891860723495483
translation,228,73,hyperparameters,decay floor,set to,0.01,decay floor set to 0.01,0.6663039922714233
translation,228,73,hyperparameters,hyperparameters,has,decay floor,hyperparameters has decay floor,0.5488767027854919
translation,228,90,hyperparameters,hyperparameters,has,grid search + continued training,hyperparameters has grid search + continued training,0.5140491724014282
translation,228,101,hyperparameters,once,every,500 updates,once every 500 updates,0.7013517618179321
translation,228,101,hyperparameters,2 epochs,of,nmt training,2 epochs of nmt training,0.5477314591407776
translation,228,101,hyperparameters,500 updates,has,2 epochs,500 updates has 2 epochs,0.5745673775672913
translation,228,101,hyperparameters,hyperparameters,update,bandit policy,hyperparameters update bandit policy,0.7628677487373352
translation,228,5,model,orderings,of,multilingual training data,orderings of multilingual training data,0.5744500160217285
translation,228,5,model,two simple search based curricula,has,orderings,two simple search based curricula has orderings,0.5975455641746521
translation,228,5,model,model,propose,two simple search based curricula,model propose two simple search based curricula,0.7050179243087769
translation,228,6,model,curriculum,for,mnmt,curriculum for mnmt,0.6481717824935913
translation,228,6,model,curriculum,jointly with,training,curriculum jointly with training,0.5824626684188843
translation,228,6,model,training,of,translation system,training of translation system,0.5782989859580994
translation,228,6,model,translation system,using,contextual multi-arm bandits,translation system using contextual multi-arm bandits,0.6735306978225708
translation,228,6,model,mnmt,has,from scratch,mnmt has from scratch,0.6379674673080444
translation,228,6,model,model,attempt to learn,curriculum,model attempt to learn curriculum,0.7478225231170654
translation,228,29,model,multi-lingual training curriculum,uses,multiple multi-arm bandits,multi-lingual training curriculum uses multiple multi-arm bandits,0.605390191078186
translation,228,29,model,multiple multi-arm bandits,as,agents,multiple multi-arm bandits as agents,0.5918358564376831
translation,228,29,model,independent of each other,in,randomly initialized environments ( nmt systems ),independent of each other in randomly initialized environments ( nmt systems ),0.5206296443939209
translation,228,95,results,ne-en and hi-en baselines,are,very weak,ne-en and hi-en baselines are very weak,0.5696143507957458
translation,228,95,results,latter,has,lagging behind,latter has lagging behind,0.5949012041091919
translation,228,95,results,results,see that,ne-en and hi-en baselines,results see that ne-en and hi-en baselines,0.6312154531478882
translation,228,97,results,random baseline,with,combination of the two datasets ( upsampled low-resource ),random baseline with combination of the two datasets ( upsampled low-resource ),0.6131578683853149
translation,228,97,results,random baseline,is,strongest,random baseline is strongest,0.6178903579711914
translation,228,97,results,random baseline,is,continued training baselines,random baseline is continued training baselines,0.6036052703857422
translation,228,97,results,combination of the two datasets ( upsampled low-resource ),is,strongest,combination of the two datasets ( upsampled low-resource ) is strongest,0.5199405550956726
translation,228,97,results,strongest,amongst,fixed baselines,strongest amongst fixed baselines,0.6632115244865417
translation,228,97,results,fixed baselines,has,marginally beating,fixed baselines has marginally beating,0.6117925643920898
translation,228,97,results,marginally beating,has,multi-lingual transformer,marginally beating has multi-lingual transformer,0.5863568186759949
translation,228,97,results,results,has,random baseline,results has random baseline,0.551530659198761
translation,228,98,results,grid search and pruned - tree search baselines,are,close in performance,grid search and pruned - tree search baselines are close in performance,0.5397219657897949
translation,228,98,results,close in performance,to,best fixed baselines,close in performance to best fixed baselines,0.5389760732650757
translation,228,98,results,much stronger results,where,50/50 configuration,much stronger results where 50/50 configuration,0.6106754541397095
translation,228,98,results,50/50 configuration,for,grid search,50/50 configuration for grid search,0.6437986493110657
translation,228,98,results,grid search,provides,best result,grid search provides best result,0.6701527833938599
translation,228,98,results,best result,at,15.1 bleu,best result at 15.1 bleu,0.4821185767650604
translation,228,98,results,tree search,has,slightly behind,tree search has slightly behind,0.5463346838951111
translation,228,103,results,curricula,learned using,contextual mabs,curricula learned using contextual mabs,0.7288641929626465
translation,228,103,results,curricula,able to match,performance,curricula able to match performance,0.6767513155937195
translation,228,103,results,curricula,performs,slightly worse,curricula performs slightly worse,0.6047578454017639
translation,228,103,results,contextual mabs,able to match,performance,contextual mabs able to match performance,0.7241789102554321
translation,228,103,results,performance,of,strongest fixed policy ( ne-hi-en random baseline ),performance of strongest fixed policy ( ne-hi-en random baseline ),0.5560434460639954
translation,228,103,results,slightly worse,than,curriculum,slightly worse than curriculum,0.607367992401123
translation,228,103,results,slightly worse,by about,0.2 bleu points,slightly worse by about 0.2 bleu points,0.62699294090271
translation,228,103,results,( expensive ) grid search,combined with,continued training,( expensive ) grid search combined with continued training,0.7187769412994385
translation,228,103,results,( expensive ) grid search,by about,0.2 bleu points,( expensive ) grid search by about 0.2 bleu points,0.6092736124992371
translation,228,104,results,continuing training,from,models,continuing training from models,0.5930143594741821
translation,228,104,results,mabs,leads to,strongest results,mabs leads to strongest results,0.6707132458686829
translation,228,104,results,results,has,continuing training,results has continuing training,0.5393306612968445
translation,228,105,results,model,trained using,curriculum,model trained using curriculum,0.6798416972160339
translation,228,105,results,curriculum,learned by,strongest mab,curriculum learned by strongest mab,0.7303597331047058
translation,228,105,results,bleu score,of,15.45,bleu score of 15.45,0.5317212343215942
translation,228,105,results,gain,of,0.6,gain of 0.6,0.6281446814537048
translation,228,105,results,gain,on,strongest baseline,gain on strongest baseline,0.5567969083786011
translation,228,105,results,0.6,on,strongest baseline,0.6 on strongest baseline,0.536048948764801
translation,228,105,results,results,using,model,results using model,0.6410605907440186
translation,229,111,baselines,deltalm + zcode,is,best performing model,deltalm + zcode is best performing model,0.5345867276191711
translation,229,111,baselines,best performing model,for,small track,best performing model for small track,0.6135799884796143
translation,229,111,baselines,baselines,has,deltalm + zcode,baselines has deltalm + zcode,0.552740752696991
translation,229,115,baselines,baselines,has,transformer + heuristics vs .,baselines has transformer + heuristics vs .,0.5477885603904724
translation,229,130,baselines,zcode model,is,best performing model,zcode model is best performing model,0.5594595074653625
translation,229,130,baselines,baselines,has,zcode model,baselines has zcode model,0.5565407276153564
translation,229,91,experimental-setup,experimental setup,trained using,"adam ( kingma and ba , 2014 ) optimizer","experimental setup trained using adam ( kingma and ba , 2014 ) optimizer",0.7332207560539246
translation,229,94,experimental-setup,label smoothing factor,of,0.1,label smoothing factor of 0.1,0.6042227149009705
translation,229,94,experimental-setup,experimental setup,set,adam 's ? 2 = 0.998,experimental setup set adam 's ? 2 = 0.998,0.6280936002731323
translation,229,94,experimental-setup,experimental setup,use,label smoothing factor,experimental setup use label smoothing factor,0.6029111742973328
translation,229,95,experimental-setup,batching,accumulate,tokens,batching accumulate tokens,0.6178890466690063
translation,229,95,experimental-setup,tokens,reach,maximum size,tokens reach maximum size,0.7544332146644592
translation,229,95,experimental-setup,maximum size,has,"of approximately 32,000 tokens per batch","maximum size has of approximately 32,000 tokens per batch",0.6008884906768799
translation,229,95,experimental-setup,experimental setup,For,batching,experimental setup For batching,0.6196475625038147
translation,229,96,experimental-setup,base model and the large model,for,"100,000 steps and 300,000 steps","base model and the large model for 100,000 steps and 300,000 steps",0.6421918869018555
translation,229,96,experimental-setup,experimental setup,train,base model and the large model,experimental setup train base model and the large model,0.6572521924972534
translation,229,97,experimental-setup,8 nvidia tesla p100 gpus,in,parallel,8 nvidia tesla p100 gpus in parallel,0.521056056022644
translation,229,97,experimental-setup,8 nvidia tesla p100 gpus,using,"opennmt - py ( klein et al. , 2017 ) toolkit","8 nvidia tesla p100 gpus using opennmt - py ( klein et al. , 2017 ) toolkit",0.5713968276977539
translation,229,97,experimental-setup,parallel,using,"opennmt - py ( klein et al. , 2017 ) toolkit","parallel using opennmt - py ( klein et al. , 2017 ) toolkit",0.5926358699798584
translation,229,97,experimental-setup,experimental setup,trained on,8 nvidia tesla p100 gpus,experimental setup trained on 8 nvidia tesla p100 gpus,0.696139395236969
translation,229,99,experimental-setup,translations,using,model,translations using model,0.7396795749664307
translation,229,99,experimental-setup,translations,use,beam search,translations use beam search,0.6934077739715576
translation,229,99,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,229,99,experimental-setup,beam search,apply,average length penalty,beam search apply average length penalty,0.5945303440093994
translation,229,99,experimental-setup,average length penalty,of,0.6,average length penalty of 0.6,0.5735619068145752
translation,229,99,experimental-setup,experimental setup,To generate,translations,experimental setup To generate translations,0.6580290198326111
translation,229,110,hyperparameters,m2m -100 model,trained on,ccmatrix,m2m -100 model trained on ccmatrix,0.7407984137535095
translation,229,110,hyperparameters,m2m -100 model,trained on,ccaligned,m2m -100 model trained on ccaligned,0.7569369673728943
translation,229,110,hyperparameters,ccaligned,with no further finetuning,contest dataset,ccaligned with no further finetuning contest dataset,0.7120867371559143
translation,229,110,hyperparameters,hyperparameters,has,m2m -100 model,hyperparameters has m2m -100 model,0.525987982749939
translation,229,6,results,final submission model,scored,22.92 average bleu,final submission model scored 22.92 average bleu,0.6674137711524963
translation,229,6,results,final submission model,scored,22.97 average bleu,final submission model scored 22.97 average bleu,0.6681966781616211
translation,229,6,results,final submission model,scored,22.97 average bleu,final submission model scored 22.97 average bleu,0.6681966781616211
translation,229,6,results,22.92 average bleu,on,flores - 101 devtest set,22.92 average bleu on flores - 101 devtest set,0.4943075478076935
translation,229,6,results,22.97 average bleu,on,contest 's hidden test set,22.97 average bleu on contest 's hidden test set,0.514747142791748
translation,229,6,results,22.97 average bleu,ranking,sixth,22.97 average bleu ranking sixth,0.6865419149398804
translation,229,6,results,results,has,final submission model,results has final submission model,0.5767641067504883
translation,229,7,results,model,ranked,first,model ranked first,0.7486059069633484
translation,229,7,results,first,in,indonesian ? javanese,first in indonesian ? javanese,0.5428436994552612
translation,229,118,results,base heuristics,scored,average bleu,base heuristics scored average bleu,0.6849364042282104
translation,229,118,results,average bleu,of,20.78,average bleu of 20.78,0.5425539612770081
translation,229,118,results,20.78,on,all 30 directions,20.78 on all 30 directions,0.5461055636405945
translation,229,118,results,results,has,base heuristics,results has base heuristics,0.6006482839584351
translation,229,119,results,large heuristics,scored,22.92 average bleu,large heuristics scored 22.92 average bleu,0.6559246182441711
translation,229,119,results,22.92 average bleu,on,all 30 directions,22.92 average bleu on all 30 directions,0.499934583902359
translation,229,119,results,22.92 average bleu,is,2.14 bleu points higher,22.92 average bleu is 2.14 bleu points higher,0.5508235692977905
translation,229,119,results,2.14 bleu points higher,than,base model,2.14 bleu points higher than base model,0.536953866481781
translation,229,119,results,results,has,large heuristics,results has large heuristics,0.58407062292099
translation,229,120,results,outperformed,with,base model,outperformed with base model,0.7215778231620789
translation,229,120,results,outperformed,with,large model,outperformed with large model,0.7087153196334839
translation,229,120,results,m2m -100 615 m baseline,with,base model,m2m -100 615 m baseline with base model,0.6487708687782288
translation,229,120,results,m2m -100 615 m baseline,with,large model,m2m -100 615 m baseline with large model,0.6611177325248718
translation,229,120,results,base model,giving,5.32 bleu improvement,base model giving 5.32 bleu improvement,0.61460942029953
translation,229,120,results,base model,giving,7.46 bleu improvement,base model giving 7.46 bleu improvement,0.6068857312202454
translation,229,120,results,large model,giving,7.46 bleu improvement,large model giving 7.46 bleu improvement,0.5888338685035706
translation,229,120,results,both models,has,outperformed,both models has outperformed,0.579180896282196
translation,229,120,results,outperformed,has,m2m -100 615 m baseline,outperformed has m2m -100 615 m baseline,0.6077099442481995
translation,229,120,results,outperformed,has,large model,outperformed has large model,0.6450284123420715
translation,229,120,results,results,has,both models,results has both models,0.5060139894485474
translation,229,121,results,fails to outperform,on,four specific translation directions,fails to outperform on four specific translation directions,0.5173691511154175
translation,229,121,results,base heuristics,has,outperforms,base heuristics has outperforms,0.6395365595817566
translation,229,121,results,base heuristics,has,fails to outperform,base heuristics has fails to outperform,0.6340523362159729
translation,229,121,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,229,126,results,baseline,in,one direction,baseline in one direction,0.552209734916687
translation,229,126,results,baseline,in,one direction,baseline in one direction,0.552209734916687
translation,229,126,results,baseline,in,one direction,baseline in one direction,0.552209734916687
translation,229,126,results,marginally underperformed,against,baseline,marginally underperformed against baseline,0.7165618538856506
translation,229,126,results,baseline,in,one direction,baseline in one direction,0.552209734916687
translation,229,126,results,marginally outperformed,has,baseline,marginally outperformed has baseline,0.6104019284248352
translation,229,126,results,results,has,large heuristics,results has large heuristics,0.58407062292099
translation,229,128,results,m2m -100,is,advantageous,m2m -100 is advantageous,0.6982661485671997
translation,229,128,results,advantageous,in,translation directions,advantageous in translation directions,0.5122358798980713
translation,229,128,results,difference,is,only marginal,difference is only marginal,0.6051915884017944
translation,229,129,results,both our transformer models and the baseline model,significantly outperformed by,deltalm +,both our transformer models and the baseline model significantly outperformed by deltalm +,0.79413241147995
translation,229,129,results,results,has,both our transformer models and the baseline model,results has both our transformer models and the baseline model,0.5235841274261475
translation,229,131,results,outperforms,by,significant 11.02 average bleu,outperforms by significant 11.02 average bleu,0.580714225769043
translation,229,131,results,our best model ( large heuristics ),by,significant 11.02 average bleu,our best model ( large heuristics ) by significant 11.02 average bleu,0.543809711933136
translation,229,131,results,baseline model,by,18.48 average bleu,baseline model by 18.48 average bleu,0.5234449505805969
translation,229,131,results,best model,has,outperforms,best model has outperforms,0.6310157775878906
translation,229,131,results,outperforms,has,our best model ( large heuristics ),outperforms has our best model ( large heuristics ),0.604295015335083
translation,229,131,results,results,has,best model,results has best model,0.5634682774543762
translation,229,133,results,outperforms,in terms of,average performance,outperforms in terms of average performance,0.6820082664489746
translation,229,133,results,our model,in terms of,average performance,our model in terms of average performance,0.6486960053443909
translation,229,133,results,our model,managed to outperform,deltalm + zcode,our model managed to outperform deltalm + zcode,0.7104653120040894
translation,229,133,results,deltalm + zcode,in,one translation direction,deltalm + zcode in one translation direction,0.5100167393684387
translation,229,133,results,deltalm + zcode,scored,23.35 bleu,deltalm + zcode scored 23.35 bleu,0.6626803278923035
translation,229,133,results,zcode,has,outperforms,zcode has outperforms,0.6513155102729797
translation,229,133,results,outperforms,has,our model,outperforms has our model,0.6141566634178162
translation,229,133,results,results,has,zcode,results has zcode,0.5392757058143616
translation,229,150,results,our models,including,other models,our models including other models,0.7287967205047607
translation,229,150,results,other models,on,shared task leaderboard,other models on shared task leaderboard,0.5302419662475586
translation,229,150,results,other models,struggled with,tamil,other models struggled with tamil,0.6840032935142517
translation,229,150,results,results,observe,our models,results observe our models,0.6281133890151978
translation,229,164,results,final submission,for,shared task,final submission for shared task,0.5757275819778442
translation,229,164,results,shared task,was,our large heuristics model,shared task was our large heuristics model,0.5970702767372131
translation,229,164,results,our large heuristics model,performed with,average bleu,our large heuristics model performed with average bleu,0.6404396891593933
translation,229,164,results,average bleu,of,22.97,average bleu of 22.97,0.5412976145744324
translation,229,164,results,22.97,on,shared task 's hidden test set,22.97 on shared task 's hidden test set,0.522426962852478
translation,229,164,results,results,has,final submission,results has final submission,0.5058330297470093
translation,229,166,results,large heuristics,has,unsurprisingly,large heuristics has unsurprisingly,0.574786901473999
translation,229,166,results,outperformed,has,base heuristics,outperformed has base heuristics,0.6404136419296265
translation,229,166,results,base heuristics,has,20.73 average bleu,base heuristics has 20.73 average bleu,0.5522680282592773
translation,229,166,results,results,has,large heuristics,results has large heuristics,0.58407062292099
translation,229,170,results,sixth,in,contest leaderboard,sixth in contest leaderboard,0.548069953918457
translation,229,170,results,sixth,scoring,22.97 bleu,sixth scoring 22.97 bleu,0.6326594352722168
translation,229,170,results,22.97 bleu,on,hidden test set,22.97 bleu on hidden test set,0.5077597498893738
translation,229,170,results,results,rank,sixth,results rank sixth,0.5579054355621338
translation,229,171,results,first place,for,id ? jv translation direction,first place for id ? jv translation direction,0.5964757204055786
translation,229,171,results,first place,beating,all other more complex models,first place beating all other more complex models,0.6128789782524109
translation,229,171,results,results,reached,first place,results reached first place,0.7560150623321533
translation,229,194,results,all other models,in,hidden test set,all other models in hidden test set,0.48283058404922485
translation,229,194,results,all other models,scoring,33.89 average bleu,all other models scoring 33.89 average bleu,0.624407172203064
translation,229,194,results,10.92 improvement,over,our best model,10.92 improvement over our best model,0.608166515827179
translation,229,194,results,shared task 's best performing model,has,still outperforms,shared task 's best performing model has still outperforms,0.6056482195854187
translation,229,194,results,deltalm + zcode,has,still outperforms,deltalm + zcode has still outperforms,0.6304360628128052
translation,229,194,results,still outperforms,has,all other models,still outperforms has all other models,0.5757049918174744
translation,229,194,results,results,has,shared task 's best performing model,results has shared task 's best performing model,0.5617557764053345
translation,229,197,results,large heuristics,ranked,first,large heuristics ranked first,0.6766318678855896
translation,229,197,results,first,in,id ? jv translation direction,first in id ? jv translation direction,0.506977379322052
translation,229,197,results,first,scoring,24.05 bleu,first scoring 24.05 bleu,0.6632208824157715
translation,229,197,results,hidden test set,has,large heuristics,hidden test set has large heuristics,0.6040613055229187
translation,229,197,results,results,On,hidden test set,results On hidden test set,0.5877171754837036
translation,230,163,ablation-analysis,entropy regularization,improves,diversity of samples,entropy regularization improves diversity of samples,0.6683552265167236
translation,230,163,ablation-analysis,oracle 's score,increases by,0.67 bleu points,oracle 's score increases by 0.67 bleu points,0.6649436950683594
translation,230,163,ablation-analysis,ablation analysis,has,entropy regularization,ablation analysis has entropy regularization,0.5149291753768921
translation,230,121,baselines,basenmt +lm,draws,samples,basenmt +lm draws samples,0.7051756978034973
translation,230,121,baselines,basenmt +lm,draws,samples,basenmt +lm draws samples,0.7051756978034973
translation,230,121,baselines,basenmt +lm,uses,log p nmt ( y|x ) + ? log p lm ( y ),basenmt +lm uses log p nmt ( y|x ) + ? log p lm ( y ),0.5712865591049194
translation,230,121,baselines,samples,from,basenmt,samples from basenmt,0.6395344138145447
translation,230,121,baselines,log p nmt ( y|x ) + ? log p lm ( y ),to rank,samples,log p nmt ( y|x ) + ? log p lm ( y ) to rank samples,0.7343484163284302
translation,230,121,baselines,baselines,has,basenmt +lm,baselines has basenmt +lm,0.5836026072502136
translation,230,148,baselines,baselines,has,shared - ebr on all tasks,baselines has shared - ebr on all tasks,0.5677193403244019
translation,230,96,experimental-setup,transformer,as,our basenmt,transformer as our basenmt,0.6599298715591431
translation,230,96,experimental-setup,experimental setup,use,transformer,experimental setup use transformer,0.6054069995880127
translation,230,98,experimental-setup,label smoothing,to regularize,our models,label smoothing to regularize our models,0.7012501955032349
translation,230,98,experimental-setup,dropout,has,weight decay,dropout has weight decay,0.5476875305175781
translation,230,98,experimental-setup,dropout,has,label smoothing,dropout has label smoothing,0.530269205570221
translation,230,98,experimental-setup,experimental setup,use,dropout,experimental setup use dropout,0.5759405493736267
translation,230,99,experimental-setup,experimental setup,use,layer normalization,experimental setup use layer normalization,0.5803878903388977
translation,230,100,experimental-setup,"adam ( kingma and ba , 2015 )",with,"parameters ? 1 = 0.9 , ? 2 = 0.98 , and = 1e ?8","adam ( kingma and ba , 2015 ) with parameters ? 1 = 0.9 , ? 2 = 0.98 , and = 1e ?8",0.6072443723678589
translation,230,100,experimental-setup,experimental setup,optimized using,"adam ( kingma and ba , 2015 )","experimental setup optimized using adam ( kingma and ba , 2015 )",0.6984754204750061
translation,230,101,experimental-setup,our models,on,1 nvidia titanx gpu,our models on 1 nvidia titanx gpu,0.5454491376876831
translation,230,101,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,230,103,experimental-setup,energy network,over,sentences,energy network over sentences,0.6709052324295044
translation,230,103,experimental-setup,energy network,use,"pre-bert ( devlin et al. , 2019 )","energy network use pre-bert ( devlin et al. , 2019 )",0.6103750467300415
translation,230,103,experimental-setup,sentences,of,target language,sentences of target language,0.5530616641044617
translation,230,103,experimental-setup,"pre-bert ( devlin et al. , 2019 )",from,"huggingface ( wolf et al. , 2019 )","pre-bert ( devlin et al. , 2019 ) from huggingface ( wolf et al. , 2019 )",0.5405823588371277
translation,230,103,experimental-setup,"pre-bert ( devlin et al. , 2019 )",as,pretrained language model,"pre-bert ( devlin et al. , 2019 ) as pretrained language model",0.4445815682411194
translation,230,103,experimental-setup,"pre-bert ( devlin et al. , 2019 )",project,hidden state,"pre-bert ( devlin et al. , 2019 ) project hidden state",0.7451427578926086
translation,230,103,experimental-setup,"pre-bert ( devlin et al. , 2019 )",define,energy value,"pre-bert ( devlin et al. , 2019 ) define energy value",0.581684410572052
translation,230,103,experimental-setup,"huggingface ( wolf et al. , 2019 )",as,pretrained language model,"huggingface ( wolf et al. , 2019 ) as pretrained language model",0.4410400390625
translation,230,103,experimental-setup,hidden state,of,bert,hidden state of bert,0.6517144441604614
translation,230,103,experimental-setup,bert,for,each output token,bert for each output token,0.6771979331970215
translation,230,103,experimental-setup,each output token,into,scalar value,each output token into scalar value,0.5796002149581909
translation,230,103,experimental-setup,each output token,into,scalar values,each output token into scalar values,0.6006107926368713
translation,230,103,experimental-setup,energy value,of,target sentence,energy value of target sentence,0.5572861433029175
translation,230,103,experimental-setup,experimental setup,To construct,energy network,experimental setup To construct energy network,0.6530450582504272
translation,230,103,experimental-setup,experimental setup,project,hidden state,experimental setup project hidden state,0.7800898551940918
translation,230,104,experimental-setup,bert - base uncased model,with,12 encoder layers,bert - base uncased model with 12 encoder layers,0.6478108763694763
translation,230,104,experimental-setup,bert - base uncased model,with,768 hidden state dimension,bert - base uncased model with 768 hidden state dimension,0.6393798589706421
translation,230,104,experimental-setup,bert - base uncased model,with,12 attention heads,bert - base uncased model with 12 attention heads,0.6562690734863281
translation,230,104,experimental-setup,bert - base uncased model,with,110m parameters,bert - base uncased model with 110m parameters,0.6490411758422852
translation,230,104,experimental-setup,experimental setup,use,bert - base uncased model,experimental setup use bert - base uncased model,0.5963467359542847
translation,230,105,experimental-setup,projection layer,use,2 - layer mlp,projection layer use 2 - layer mlp,0.6199823617935181
translation,230,105,experimental-setup,2 - layer mlp,with,256 hidden variables,2 - layer mlp with 256 hidden variables,0.609069287776947
translation,230,105,experimental-setup,experimental setup,For,projection layer,experimental setup For projection layer,0.564652144908905
translation,230,107,experimental-setup,margin weight,of,? = 10,margin weight of ? = 10,0.6706042289733887
translation,230,107,experimental-setup,margin weight,of,temperature t = 1000,margin weight of temperature t = 1000,0.646084189414978
translation,230,108,experimental-setup,projection layer,using,l2 regularization,projection layer using l2 regularization,0.5956374406814575
translation,230,108,experimental-setup,experimental setup,regularize,projection layer,experimental setup regularize projection layer,0.7049156427383423
translation,230,109,experimental-setup,"adam ( kingma and ba , 2015 )",with,"parameters ? 1 = 0.9 , ? 2 = 0.98 , and = 1e ?8","adam ( kingma and ba , 2015 ) with parameters ? 1 = 0.9 , ? 2 = 0.98 , and = 1e ?8",0.6072443723678589
translation,230,109,experimental-setup,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,230,109,experimental-setup,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,230,109,experimental-setup,experimental setup,optimized using,"adam ( kingma and ba , 2015 )","experimental setup optimized using adam ( kingma and ba , 2015 )",0.6984754204750061
translation,230,110,experimental-setup,all experiments,on,1 nvidia tesla m40 gpu,all experiments on 1 nvidia tesla m40 gpu,0.5072183012962341
translation,230,110,experimental-setup,experimental setup,run,all experiments,experimental setup run all experiments,0.7501644492149353
translation,230,171,experimental-setup,xy - bert,use,german bert,xy - bert use german bert,0.5703008770942688
translation,230,171,experimental-setup,xy - bert,use,english bert,xy - bert use english bert,0.6392321586608887
translation,230,171,experimental-setup,german bert,for,encoder,german bert for encoder,0.6340189576148987
translation,230,171,experimental-setup,german bert,for,decoder,german bert for decoder,0.6494625210762024
translation,230,171,experimental-setup,german bert,for,decoder,german bert for decoder,0.6494625210762024
translation,230,171,experimental-setup,english bert,for,decoder,english bert for decoder,0.6623355150222778
translation,230,171,experimental-setup,experimental setup,For,xy - bert,experimental setup For xy - bert,0.6155962944030762
translation,230,115,experiments,translation tasks,use,"bert - base , multilingual cased model","translation tasks use bert - base , multilingual cased model",0.5870429873466492
translation,230,115,experiments,"bert - base , multilingual cased model",with,12 encoder layers,"bert - base , multilingual cased model with 12 encoder layers",0.6343669295310974
translation,230,115,experiments,"bert - base , multilingual cased model",with,768 hidden state dimension,"bert - base , multilingual cased model with 768 hidden state dimension",0.6407994627952576
translation,230,115,experiments,"bert - base , multilingual cased model",with,12 attention heads,"bert - base , multilingual cased model with 12 attention heads",0.6647178530693054
translation,230,115,experiments,"bert - base , multilingual cased model",with,110m parameters,"bert - base , multilingual cased model with 110m parameters",0.6602351665496826
translation,230,186,experiments,marginal -ebm and joint-ebm,by maximizing,expected bleu score,marginal -ebm and joint-ebm by maximizing expected bleu score,0.6833016276359558
translation,230,186,experiments,expected bleu score,on,iwslt '14 de-en,expected bleu score on iwslt '14 de-en,0.5778247117996216
translation,230,6,model,autoregressive factorization,provides,tractable likelihood computation,autoregressive factorization provides tractable likelihood computation,0.5578492283821106
translation,230,6,model,autoregressive factorization,provides,efficient sampling,autoregressive factorization provides efficient sampling,0.5856719017028809
translation,230,6,model,model,has,autoregressive factorization,model has autoregressive factorization,0.5414029359817505
translation,230,97,model,transformer architecture,includes,six encoder and six decoder layers,transformer architecture includes six encoder and six decoder layers,0.6180147528648376
translation,230,97,model,embedding dimension and inner-layer dimension,are,"8 , 512 and 4096","embedding dimension and inner-layer dimension are 8 , 512 and 4096",0.584298849105835
translation,230,97,model,number of attention heads,has,embedding dimension and inner-layer dimension,number of attention heads has embedding dimension and inner-layer dimension,0.5479066967964172
translation,230,97,model,model,has,transformer architecture,model has transformer architecture,0.5729222893714905
translation,230,23,results,joint - ebm,works,better,joint - ebm works better,0.6541880965232849
translation,230,23,results,better,than,marginal - ebm,better than marginal - ebm,0.6373052597045898
translation,230,23,results,better,using,marginal - ebm,better using marginal - ebm,0.7356864809989929
translation,230,23,results,correlation,of,source and target sentences,correlation of source and target sentences,0.5862891674041748
translation,230,23,results,results,observe,joint - ebm,results observe joint - ebm,0.5887190103530884
translation,230,135,results,basenmt + sample,achieves,better score,basenmt + sample achieves better score,0.7095444202423096
translation,230,135,results,better score,than,beam decoding,better score than beam decoding,0.5913494229316711
translation,230,135,results,beam decoding,suggesting that,our multinomial sampling,beam decoding suggesting that our multinomial sampling,0.6613004803657532
translation,230,135,results,our multinomial sampling,supports,modes of the distribution,our multinomial sampling supports modes of the distribution,0.6475551128387451
translation,230,135,results,modes of the distribution,defined by,basenmt,modes of the distribution defined by basenmt,0.6287578344345093
translation,230,135,results,results,has,basenmt + sample,results has basenmt + sample,0.5845289826393127
translation,230,140,results,basenmt +mlm,is,consistently better,basenmt +mlm is consistently better,0.5792761445045471
translation,230,140,results,consistently better,than,basenmt +lm,consistently better than basenmt +lm,0.6150434613227844
translation,230,140,results,results,has,basenmt +mlm,results has basenmt +mlm,0.5966557860374451
translation,230,142,results,marginal -ebr,performs,considerably better,marginal -ebr performs considerably better,0.6189191937446594
translation,230,142,results,marginal -ebr,better than,nce - ebr,marginal -ebr better than nce - ebr,0.7582675814628601
translation,230,142,results,considerably better,than,"basenmt + { beam , sample , lm , mlm }","considerably better than basenmt + { beam , sample , lm , mlm }",0.5730269551277161
translation,230,142,results,nce - ebr,on,all tasks,nce - ebr on all tasks,0.5754201412200928
translation,230,142,results,results,has,marginal -ebr,results has marginal -ebr,0.5661916732788086
translation,230,145,results,shared - ebr,improves,lowresource task,shared - ebr improves lowresource task,0.6934334635734558
translation,230,145,results,significant improvement,over,marginal - ebr,significant improvement over marginal - ebr,0.7007749080657959
translation,230,145,results,lowresource task,of,si?en,lowresource task of si?en,0.6329593062400818
translation,230,145,results,lowresource task,by,more than 2 bleu points,lowresource task by more than 2 bleu points,0.5530902147293091
translation,230,145,results,shared - ebr,has,significant improvement,shared - ebr has significant improvement,0.600440263748169
translation,230,145,results,results,has,shared - ebr,results has shared - ebr,0.5324863195419312
translation,230,146,results,more language pairs,in,training,more language pairs in training,0.5131836533546448
translation,230,146,results,more language pairs,improves,performance,more language pairs improves performance,0.7177572250366211
translation,230,146,results,results,using,more language pairs,results using more language pairs,0.663918137550354
translation,230,149,results,performance,of,conditional - ebr,performance of conditional - ebr,0.6346063017845154
translation,230,149,results,conditional - ebr,use of,joint - ebm model,conditional - ebr use of joint - ebm model,0.696810781955719
translation,230,149,results,results,has,performance,results has performance,0.5972660779953003
translation,230,164,results,benefits,less than,0.1 bleu points,benefits less than 0.1 bleu points,0.5913267731666565
translation,230,164,results,0.1 bleu points,from,regularization,0.1 bleu points from regularization,0.519427478313446
translation,230,164,results,conditional - ebr,improves by,0.3 bleu points,conditional - ebr improves by 0.3 bleu points,0.7200884819030762
translation,230,173,results,conditional -ebr,with,xy - bert,conditional -ebr with xy - bert,0.7078248262405396
translation,230,173,results,conditional -ebr,with,joint - bert,conditional -ebr with joint - bert,0.7222768664360046
translation,230,173,results,conditional -ebr,with,joint - bert,conditional -ebr with joint - bert,0.7222768664360046
translation,230,173,results,xy - bert,achieves,38.33 bleu score,xy - bert achieves 38.33 bleu score,0.6410641074180603
translation,230,173,results,xy - bert,achieves,0.75 bleu points higher,xy - bert achieves 0.75 bleu points higher,0.7009037137031555
translation,230,173,results,xy - bert,improves,performance,xy - bert improves performance,0.7663050889968872
translation,230,173,results,38.33 bleu score,is,0.75 bleu points higher,38.33 bleu score is 0.75 bleu points higher,0.5523285865783691
translation,230,173,results,0.75 bleu points higher,than,conditional - ebr,0.75 bleu points higher than conditional - ebr,0.5720340013504028
translation,230,173,results,conditional - ebr,with,joint - bert,conditional - ebr with joint - bert,0.7222768664360046
translation,230,173,results,performance,of,xy - bert,performance of xy - bert,0.6448314189910889
translation,230,173,results,xy - bert,with,mask - predict decoding,xy - bert with mask - predict decoding,0.7110887765884399
translation,230,173,results,mask - predict decoding,by,1.84 bleu points,mask - predict decoding by 1.84 bleu points,0.5381351709365845
translation,230,173,results,results,has,conditional -ebr,results has conditional -ebr,0.5541139841079712
translation,230,188,results,underperform,has,rankbased training,underperform has rankbased training,0.5735832452774048
translation,231,113,ablation-analysis,profit,of,fine-tuning,profit of fine-tuning,0.6161264777183533
translation,231,113,ablation-analysis,transformer,is,significant,transformer is significant,0.6183207035064697
translation,231,113,ablation-analysis,fine-tuning,has,transformer,fine-tuning has transformer,0.5697752833366394
translation,231,113,ablation-analysis,ablation analysis,has,profit,ablation analysis has profit,0.5182805061340332
translation,231,93,experimental-setup,image features,use,"butd ( anderson et al. , 2018 )","image features use butd ( anderson et al. , 2018 )",0.611228346824646
translation,231,93,experimental-setup,"butd ( anderson et al. , 2018 )",to extract,4 groups of features,"butd ( anderson et al. , 2018 ) to extract 4 groups of features",0.7496837973594666
translation,231,93,experimental-setup,4 groups of features,for,each object,4 groups of features for each object,0.6063039898872375
translation,231,93,experimental-setup,each object,including,pooled roi feature vector,each object including pooled roi feature vector,0.6291701793670654
translation,231,93,experimental-setup,each object,including,object class,each object including object class,0.6172706484794617
translation,231,93,experimental-setup,each object,including,object attribute,each object including object attribute,0.621380090713501
translation,231,93,experimental-setup,each object,including,bounding box,each object including bounding box,0.6600815057754517
translation,231,93,experimental-setup,experimental setup,For,image features,experimental setup For image features,0.5651928186416626
translation,231,97,experimental-setup,pre-trained transformer model,provided by,"fairseq ( ott et al. , 2019 )","pre-trained transformer model provided by fairseq ( ott et al. , 2019 )",0.5735567808151245
translation,231,97,experimental-setup,pre-trained transformer model,implemented with,"pytorch ( paszke et al. , 2019 )","pre-trained transformer model implemented with pytorch ( paszke et al. , 2019 )",0.6673005223274231
translation,231,97,experimental-setup,experimental setup,use,pre-trained transformer model,experimental setup use pre-trained transformer model,0.5833292007446289
translation,231,103,experimental-setup,fine- tuning,use,learning - rate,fine- tuning use learning - rate,0.6418606042861938
translation,231,103,experimental-setup,learning - rate,of,1e - 4,learning - rate of 1e - 4,0.6454080939292908
translation,231,103,experimental-setup,1e - 4,with,4000 steps,1e - 4 with 4000 steps,0.6352168917655945
translation,231,103,experimental-setup,4000 steps,of,warm - up,4000 steps of warm - up,0.600881814956665
translation,231,103,experimental-setup,experimental setup,During,fine- tuning,experimental setup During fine- tuning,0.7118422985076904
translation,231,104,experimental-setup,0.3,for,dropout probability,0.3 for dropout probability,0.5959447622299194
translation,231,104,experimental-setup,0.1,for,label smoothing,0.1 for label smoothing,0.5935676097869873
translation,231,104,experimental-setup,"adam ( kingma and ba , 2015 )",used as,optimizer,"adam ( kingma and ba , 2015 ) used as optimizer",0.6165988445281982
translation,231,104,experimental-setup,experimental setup,use,0.3,experimental setup use 0.3,0.5994043946266174
translation,231,104,experimental-setup,experimental setup,use,0.1,experimental setup use 0.1,0.5657556056976318
translation,231,105,experimental-setup,vl - transformer,use,parameter,vl - transformer use parameter,0.6174402236938477
translation,231,105,experimental-setup,parameter,of,fairseq pre-trained transformer,parameter of fairseq pre-trained transformer,0.5572112798690796
translation,231,105,experimental-setup,fairseq pre-trained transformer,to initialize,backbone and text related embeddings,fairseq pre-trained transformer to initialize backbone and text related embeddings,0.707757830619812
translation,231,105,experimental-setup,parameters,initialized,randomly,parameters initialized randomly,0.8213622570037842
translation,231,105,experimental-setup,experimental setup,For,vl - transformer,experimental setup For vl - transformer,0.576058030128479
translation,231,106,experimental-setup,fine-tuned,on,tesla v100 gpu,fine-tuned on tesla v100 gpu,0.5337905287742615
translation,231,106,experimental-setup,tesla v100 gpu,with,fp16 enabled,tesla v100 gpu with fp16 enabled,0.6509868502616882
translation,231,106,experimental-setup,converges,in,less than 20 minutes,converges in less than 20 minutes,0.5900126099586487
translation,231,106,experimental-setup,less than 20 minutes,for,10 epochs,less than 20 minutes for 10 epochs,0.6361541152000427
translation,231,106,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,231,12,experiments,multimodal machine translation ( mmt ),incorporating,image signals,multimodal machine translation ( mmt ) incorporating image signals,0.7089590430259705
translation,231,12,experiments,image signals,into,rnn - based encoder - decoder,image signals into rnn - based encoder - decoder,0.5837716460227966
translation,231,12,experiments,rnn - based encoder - decoder,shows,improvements,rnn - based encoder - decoder shows improvements,0.657924473285675
translation,231,12,experiments,improvements,on,translation quality,improvements on translation quality,0.5269098281860352
translation,231,12,experiments,translation quality,due to,forceful disambiguation,translation quality due to forceful disambiguation,0.6396666169166565
translation,231,98,experiments,en-de model ( transformer - large ),trained on,wmt '19 corpus,en-de model ( transformer - large ) trained on wmt '19 corpus,0.745582103729248
translation,231,98,experiments,en- fr ( transformer - big ) model,trained on,wmt '14 corpus,en- fr ( transformer - big ) model trained on wmt '14 corpus,0.7217434048652649
translation,231,5,model,model,propose,transfer learning solution,model propose transfer learning solution,0.705299437046051
translation,231,13,model,paradigm,of,first pretraining and then fine- tuning,paradigm of first pretraining and then fine- tuning,0.5453060269355774
translation,231,13,model,first pretraining and then fine- tuning,be effectively applied to,mmt,first pretraining and then fine- tuning be effectively applied to mmt,0.6744154691696167
translation,231,100,model,parameters,of,embedding layer,parameters of embedding layer,0.5253254175186157
translation,231,100,model,parameters,of,output projection layer,parameters of output projection layer,0.5327224135398865
translation,231,100,model,parameters,shared for,encoder,parameters shared for encoder,0.6851484179496765
translation,231,100,model,parameters,shared for,the decoder,parameters shared for the decoder,0.719789981842041
translation,231,100,model,embedding layer,as well as,output projection layer,embedding layer as well as output projection layer,0.5924233794212341
translation,231,100,model,output projection layer,shared for,the decoder,output projection layer shared for the decoder,0.6959010362625122
translation,231,100,model,model,has,parameters,model has parameters,0.49046650528907776
translation,231,114,results,already better,than,text-only method,already better than text-only method,0.6000683903694153
translation,231,114,results,vltransformer,has,model,vltransformer has model,0.5690581202507019
translation,231,114,results,model,has,trained without cmm,model has trained without cmm,0.5784875154495239
translation,231,114,results,results,For,vltransformer,results For vltransformer,0.5894449949264526
translation,232,134,ablation-analysis,noun phrases and mistranslations,have,very similar ( high ) influence,noun phrases and mistranslations have very similar ( high ) influence,0.521374523639679
translation,232,134,ablation-analysis,very similar ( high ) influence,on,error perception,very similar ( high ) influence on error perception,0.5486536622047424
translation,232,134,ablation-analysis,error perception,in,all data sets,error perception in all data sets,0.510763943195343
translation,232,134,ablation-analysis,rephrasing,has,ambiguous words,rephrasing has ambiguous words,0.5939453840255737
translation,232,134,ablation-analysis,ablation analysis,has,rephrasing,ablation analysis has rephrasing,0.5483537316322327
translation,232,135,results,results,has,rephrasing,results has rephrasing,0.5632426142692566
translation,233,96,experimental-setup,nmt engines,follow,transformer architecture,nmt engines follow transformer architecture,0.5856661200523376
translation,233,96,experimental-setup,transformer architecture,learned from,source and target corpora,transformer architecture learned from source and target corpora,0.6611687541007996
translation,233,96,experimental-setup,experimental setup,has,nmt engines,experimental setup has nmt engines,0.5588430166244507
translation,233,97,experimental-setup,mt models,using,noam schedule,mt models using noam schedule,0.7142335772514343
translation,233,97,experimental-setup,noam schedule,with,4000 warm - up iterations,noam schedule with 4000 warm - up iterations,0.6576473116874695
translation,233,97,experimental-setup,noam schedule,has,),noam schedule has ),0.672886848449707
translation,233,97,experimental-setup,experimental setup,train,mt models,experimental setup train mt models,0.6423301696777344
translation,233,101,experimental-setup,learning,performed over,8 gpus,learning performed over 8 gpus,0.6771148443222046
translation,233,101,experimental-setup,8 gpus,during,300 k steps,8 gpus during 300 k steps,0.5953641533851624
translation,233,101,experimental-setup,8 gpus,with,batch size,8 gpus with batch size,0.6039891839027405
translation,233,101,experimental-setup,300 k steps,with,batch size,300 k steps with batch size,0.6607450842857361
translation,233,101,experimental-setup,batch size,of,32 k tokens per step,batch size of 32 k tokens per step,0.5776144862174988
translation,233,101,experimental-setup,experimental setup,has,learning,experimental setup has learning,0.5334027409553528
translation,233,102,experimental-setup,training,filtered out,sentences,training filtered out sentences,0.750878095626831
translation,233,102,experimental-setup,sentences,larger than,250 tokens,sentences larger than 250 tokens,0.7082825899124146
translation,233,102,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,233,17,experiments,english - to - french nmt engines,built from,abundant generic ( out - of- domain ) training data,english - to - french nmt engines built from abundant generic ( out - of- domain ) training data,0.6732496023178101
translation,233,129,results,methods,learn to apply,constraints ( app and app + ),methods learn to apply constraints ( app and app + ),0.655797004699707
translation,233,129,results,constraints ( app and app + ),obtain,best performance,constraints ( app and app + ) obtain best performance,0.5682503581047058
translation,233,130,results,gec model,succeeds in,fixing,gec model succeeds in fixing,0.8010661602020264
translation,233,130,results,fixing,has,grammatically incorrect french words,fixing has grammatically incorrect french words,0.5669201016426086
translation,233,130,results,results,has,gec model,results has gec model,0.5339294075965881
translation,234,123,ablation-analysis,cushlepor optimisation framework,proves to be,functional,cushlepor optimisation framework proves to be functional,0.6503488421440125
translation,234,123,ablation-analysis,functional,offering,high performance,functional offering high performance,0.780755877494812
translation,234,123,ablation-analysis,high performance,towards,pretrained lms,high performance towards pretrained lms,0.6963023543357849
translation,234,123,ablation-analysis,much improved agreement,of,cush-lepor to labse scores,much improved agreement of cush-lepor to labse scores,0.6037363409996033
translation,234,123,ablation-analysis,cush-lepor to labse scores,in comparison to,hle - por,cush-lepor to labse scores in comparison to hle - por,0.6728395819664001
translation,234,123,ablation-analysis,ablation analysis,has,cushlepor optimisation framework,ablation analysis has cushlepor optimisation framework,0.5473418831825256
translation,234,2,baselines,baselines,has,cushlepor,baselines has cushlepor,0.6348118782043457
translation,234,69,baselines,labse,built on,bert ( bidirectional encoder representations from transformers ) architecture,labse built on bert ( bidirectional encoder representations from transformers ) architecture,0.7288230657577515
translation,234,69,baselines,labse,trained on,monolingual ( for dictionaries ) and bilingual training data,labse trained on monolingual ( for dictionaries ) and bilingual training data,0.7509753108024597
translation,234,69,baselines,baselines,has,labse,baselines has labse,0.6325457692146301
translation,234,122,baselines,alternative pre-trained language models ( lms ),to boost,performance,alternative pre-trained language models ( lms ) to boost performance,0.665064811706543
translation,234,8,experiments,professional human evaluation data,based on,mqm and psqm framework,professional human evaluation data based on mqm and psqm framework,0.626820981502533
translation,234,8,experiments,mqm and psqm framework,on,english - german and chinese - english language pairs,mqm and psqm framework on english - german and chinese - english language pairs,0.5607934594154358
translation,234,113,experiments,hlepor baseline metric,wins,english-to - russian ted domain language specific ranking,hlepor baseline metric wins english-to - russian ted domain language specific ranking,0.741502046585083
translation,234,7,model,customised hlepor ( cushle - por ),uses,optuna hyper-parameter optimisation framework,customised hlepor ( cushle - por ) uses optuna hyper-parameter optimisation framework,0.599317193031311
translation,234,7,model,optuna hyper-parameter optimisation framework,to fine- tune,hlepor weighting parameters,optuna hyper-parameter optimisation framework to fine- tune hlepor weighting parameters,0.7043615579605103
translation,234,7,model,hlepor weighting parameters,towards,better agreement,hlepor weighting parameters towards better agreement,0.6379432082176208
translation,234,7,model,better agreement,to,pre-trained language models,better agreement to pre-trained language models,0.5358334183692932
translation,234,7,model,model,present,customised hlepor ( cushle - por ),model present customised hlepor ( cushle - por ),0.6702539324760437
translation,234,20,model,advantages,of,high performing automatic metric,advantages of high performing automatic metric,0.5463512539863586
translation,234,20,model,advantages,of,pre-trained language model,advantages of pre-trained language model,0.5308108925819397
translation,234,20,model,mt evaluation metric,from,segment level perspectives,mt evaluation metric from segment level perspectives,0.5300551056861877
translation,234,20,model,model,take,advantages,model take advantages,0.6446319818496704
translation,234,117,model,customised hlepor metric,can be,automatically trained and optimised,customised hlepor metric can be automatically trained and optimised,0.6375927925109863
translation,234,117,model,automatically trained and optimised,using,human labelled mqm scores,automatically trained and optimised using human labelled mqm scores,0.6580371856689453
translation,234,117,model,large scale pretrained language model ( lm ) labse,towards,better agreement,large scale pretrained language model ( lm ) labse towards better agreement,0.6230021119117737
translation,234,117,model,large scale pretrained language model ( lm ) labse,towards,distilled lm performance,large scale pretrained language model ( lm ) labse towards distilled lm performance,0.6446109414100647
translation,234,117,model,better agreement,to,human experts level judgements,better agreement to human experts level judgements,0.5176377296447754
translation,234,117,model,better agreement,to,distilled lm performance,better agreement to distilled lm performance,0.564690351486206
translation,234,117,model,cushlepor,has,customised hlepor metric,cushlepor has customised hlepor metric,0.6438804864883423
translation,234,117,model,model,described,cushlepor,model described cushlepor,0.6472492814064026
translation,234,112,results,cushlepor,wins,english-to - german and chinese- to - english language pairs,cushlepor wins english-to - german and chinese- to - english language pairs,0.6816803812980652
translation,234,112,results,english-to - german and chinese- to - english language pairs,including,ted data condition,english-to - german and chinese- to - english language pairs including ted data condition,0.6704615950584412
translation,234,112,results,language specific ranking,has,cushlepor,language specific ranking has cushlepor,0.5989949107170105
translation,234,112,results,results,in,language specific ranking,results in language specific ranking,0.5182505249977112
translation,234,118,results,cushlepor,towards,human translators ' evaluation scores,cushlepor towards human translators ' evaluation scores,0.6513267159461975
translation,234,118,results,cushlepor,showed,much improved performance,cushlepor showed much improved performance,0.7092214226722717
translation,234,118,results,much improved performance,than,bleu and original hlepor,much improved performance than bleu and original hlepor,0.5376206040382385
translation,234,118,results,results,optimised,cushlepor,results optimised cushlepor,0.7391707301139832
translation,234,124,results,optimised cush-lepor,achieves,better agreement,optimised cush-lepor achieves better agreement,0.6625689268112183
translation,234,124,results,better agreement,towards,professional translator 's evaluation ( psqm ),better agreement towards professional translator 's evaluation ( psqm ),0.6034708023071289
translation,234,124,results,results,has,optimised cush-lepor,results has optimised cush-lepor,0.5959455370903015
translation,235,19,experiments,"two highly specialized domains ( i.e. , law and medicine )",which contain,domain-specific terminologies,"two highly specialized domains ( i.e. , law and medicine ) which contain domain-specific terminologies",0.618852436542511
translation,235,8,model,consistent improvements,on,terminology and sentence - level translation,consistent improvements on terminology and sentence - level translation,0.5121768116950989
translation,235,8,model,terminology and sentence - level translation,for,three domain-specific corpora,terminology and sentence - level translation for three domain-specific corpora,0.5708196759223938
translation,235,8,model,three domain-specific corpora,in,two language pairs,three domain-specific corpora in two language pairs,0.49618130922317505
translation,235,8,model,model,propose,simple and effective,model propose simple and effective,0.6495240926742554
translation,235,18,model,simple yet effective training scheme,to improve,terminology translation,simple yet effective training scheme to improve terminology translation,0.6505333781242371
translation,235,18,model,model,propose,simple yet effective training scheme,model propose simple yet effective training scheme,0.6940471529960632
translation,235,45,model,extra span-level prediction task,in,translation,extra span-level prediction task in translation,0.4648340344429016
translation,235,45,model,model,propose,extra span-level prediction task,model propose extra span-level prediction task,0.6054167151451111
translation,235,21,results,domain-specific setups,where,longer n-gram terms,domain-specific setups where longer n-gram terms,0.5793648362159729
translation,235,21,results,our method,demonstrates,improved performance,our method demonstrates improved performance,0.6756443977355957
translation,235,21,results,improved performance,over,standard maximum likelihood estimation ( mle ) approach,improved performance over standard maximum likelihood estimation ( mle ) approach,0.6674516201019287
translation,235,21,results,standard maximum likelihood estimation ( mle ) approach,in terms of,terminology and sentence - level translation quality,standard maximum likelihood estimation ( mle ) approach in terms of terminology and sentence - level translation quality,0.6595078706741333
translation,235,21,results,domain-specific setups,has,our method,domain-specific setups has our method,0.5454829931259155
translation,235,21,results,longer n-gram terms,has,our method,longer n-gram terms has our method,0.5534340739250183
translation,235,21,results,results,In,domain-specific setups,results In domain-specific setups,0.5241175889968872
translation,235,85,results,our training scheme,shows,consistent improvements,our training scheme shows consistent improvements,0.6692585945129395
translation,235,85,results,consistent improvements,over,standard mle counterparts,consistent improvements over standard mle counterparts,0.7298201322555542
translation,235,85,results,legal domain,has,our training scheme,legal domain has our training scheme,0.562057375907898
translation,235,85,results,legal domain,has,our method,legal domain has our method,0.5738623738288879
translation,235,85,results,results,For,legal domain,results For legal domain,0.5629759430885315
translation,235,91,results,susanto20,shows,better performance,susanto20 shows better performance,0.6874053478240967
translation,235,91,results,better performance,on,several metrics,better performance on several metrics,0.502820611000061
translation,235,91,results,better performance,on,long terminology translation,better performance on long terminology translation,0.5324822664260864
translation,235,91,results,several metrics,on,long terminology translation,several metrics on long terminology translation,0.4963177442550659
translation,235,91,results,decreased,by,about 8 points,decreased by about 8 points,0.6752760410308838
translation,235,91,results,decreased,compared to,no dictionary use,decreased compared to no dictionary use,0.6193981170654297
translation,235,91,results,about 8 points,compared to,no dictionary use,about 8 points compared to no dictionary use,0.6564127206802368
translation,236,204,ablation-analysis,performance,on,out ? in,performance on out ? in,0.6583425402641296
translation,236,204,ablation-analysis,performance,improves on,out?out group,performance improves on out?out group,0.7710247039794922
translation,236,204,ablation-analysis,unfreezing language adapters,has,decreases,unfreezing language adapters has decreases,0.5789771676063538
translation,236,204,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,236,204,ablation-analysis,ablation analysis,has,unfreezing language adapters,ablation analysis has unfreezing language adapters,0.5384761095046997
translation,236,74,baselines,domain adapter dropout,has,dadrop ),domain adapter dropout has dadrop ),0.6259394884109497
translation,236,74,baselines,baselines,has,domain adapter dropout,baselines has domain adapter dropout,0.5511713624000549
translation,236,175,baselines,several ' straightforward ' adapter models,for,subset of in-domain languages,several ' straightforward ' adapter models for subset of in-domain languages,0.6016029119491577
translation,236,175,baselines,several ' straightforward ' adapter models,stacking them in,encoder and decoder,several ' straightforward ' adapter models stacking them in encoder and decoder,0.6275386214256287
translation,236,175,baselines,subset of in-domain languages,on the top of,baseline model,subset of in-domain languages on the top of baseline model,0.6302922964096069
translation,236,175,baselines,domain and language adapters,stacking them in,encoder and decoder,domain and language adapters stacking them in encoder and decoder,0.6018804907798767
translation,236,92,experiments,"fairseq ( ott et al. , 2019 )",for,800k updates,"fairseq ( ott et al. , 2019 ) for 800k updates",0.5653517246246338
translation,236,92,experiments,"fairseq ( ott et al. , 2019 )",with,batch size,"fairseq ( ott et al. , 2019 ) with batch size",0.5839835405349731
translation,236,92,experiments,"fairseq ( ott et al. , 2019 )",with,accumulated gradients,"fairseq ( ott et al. , 2019 ) with accumulated gradients",0.6262068748474121
translation,236,92,experiments,800k updates,with,batch size,800k updates with batch size,0.620797872543335
translation,236,92,experiments,800k updates,with,accumulated gradients,800k updates with accumulated gradients,0.6157031059265137
translation,236,92,experiments,batch size,of,maximum 4000 tokens,batch size of maximum 4000 tokens,0.5794243812561035
translation,236,92,experiments,accumulated gradients,over,64 steps,accumulated gradients over 64 steps,0.6665663719177246
translation,236,95,hyperparameters,bpe model,trained with,temperature - based sampling,bpe model trained with temperature - based sampling,0.7169830203056335
translation,236,95,hyperparameters,temperature - based sampling,with,"t = 5 ( arivazhagan et al. , 2019 )","temperature - based sampling with t = 5 ( arivazhagan et al. , 2019 )",0.5876055955886841
translation,236,95,hyperparameters,hyperparameters,Both,multilingual models,hyperparameters Both multilingual models,0.6152396202087402
translation,236,95,hyperparameters,hyperparameters,has,multilingual models,hyperparameters has multilingual models,0.527941107749939
translation,236,95,hyperparameters,hyperparameters,has,bpe model,hyperparameters has bpe model,0.5317132472991943
translation,236,79,model,model,leverage,english-centric back - translation ( bt ),model leverage english-centric back - translation ( bt ),0.7533073425292969
translation,236,93,model,shared and tied,with,output layer,shared and tied with output layer,0.672592282295227
translation,236,93,model,model,has,source / target embeddings,model has source / target embeddings,0.5235262513160706
translation,236,10,results,best adapter combinations,obtain,improvements,best adapter combinations obtain improvements,0.5727382302284241
translation,236,10,results,improvements,of,3 - 4 bleu,improvements of 3 - 4 bleu,0.5858025550842285
translation,236,10,results,3 - 4 bleu,on average,source languages,3 - 4 bleu on average source languages,0.652183473110199
translation,236,10,results,3 - 4 bleu,for,source languages,3 - 4 bleu for source languages,0.5978994369506836
translation,236,10,results,results,With,best adapter combinations,results With best adapter combinations,0.6386487483978271
translation,236,11,results,target languages,without,in- domain data,target languages without in- domain data,0.6912274956703186
translation,236,11,results,target languages,achieve,similar improvement,target languages achieve similar improvement,0.6250529885292053
translation,236,11,results,similar improvement,by combining,adapters,similar improvement by combining adapters,0.7039101123809814
translation,236,11,results,adapters,with,back - translation,adapters with back - translation,0.6316446661949158
translation,236,11,results,results,For,target languages,results For target languages,0.5530668497085571
translation,236,36,results,cross-lingual transfer,of,domain knowledge,cross-lingual transfer of domain knowledge,0.5235048532485962
translation,236,36,results,cross-lingual transfer,for,adapters,cross-lingual transfer for adapters,0.6265559792518616
translation,236,36,results,results,improve,cross-lingual transfer,results improve cross-lingual transfer,0.5485538244247437
translation,236,73,results,work well,with,back - translation,work well with back - translation,0.6471585035324097
translation,236,73,results,decoder-only adapters,has,work well,decoder-only adapters has work well,0.5761553049087524
translation,236,73,results,results,find empirically that,decoder-only adapters,results find empirically that decoder-only adapters,0.621773898601532
translation,236,150,results,model,with,stacked adapters,model with stacked adapters,0.6684504747390747
translation,236,150,results,finetuning with domain tags,has,outperforms,finetuning with domain tags has outperforms,0.6254091262817383
translation,236,150,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,236,150,results,results,has,finetuning with domain tags,results has finetuning with domain tags,0.5716046690940857
translation,236,152,results,model,with,tags,model with tags,0.7005603313446045
translation,236,152,results,model,is,clearly better,model is clearly better,0.6099300980567932
translation,236,152,results,clearly better,for,all language directions,clearly better for all language directions,0.6294592618942261
translation,236,152,results,it and medical domains,has,model,it and medical domains has model,0.6170479655265808
translation,236,152,results,results,For,it and medical domains,results For it and medical domains,0.5547295212745667
translation,236,153,results,most of the differences,are,not statistically significant,most of the differences are not statistically significant,0.545504093170166
translation,236,153,results,not statistically significant,except for,english -centric language pairs,not statistically significant except for english -centric language pairs,0.6911847591400146
translation,236,153,results,english -centric language pairs,for,ted,english -centric language pairs for ted,0.6736774444580078
translation,236,153,results,adapter model,is,better,adapter model is better,0.5952348113059998
translation,236,153,results,lowest resource domains,has,koran and ted,lowest resource domains has koran and ted,0.6276444792747498
translation,236,153,results,lowest resource domains,has,most of the differences,lowest resource domains has most of the differences,0.554181694984436
translation,236,153,results,koran and ted,has,most of the differences,koran and ted has most of the differences,0.5964726805686951
translation,236,153,results,results,For,lowest resource domains,results For lowest resource domains,0.6142520308494568
translation,236,155,results,stacking domain and language adapters,results in,better performance,stacking domain and language adapters results in better performance,0.5707212686538696
translation,236,155,results,better performance,than,model,better performance than model,0.5706859230995178
translation,236,155,results,same parameter budget,devoted to,language adapters only,same parameter budget devoted to language adapters only,0.6439770460128784
translation,236,155,results,results,has,stacking domain and language adapters,results has stacking domain and language adapters,0.5432712435722351
translation,236,157,results,higher capacity language adapter model,does not perform,as well,higher capacity language adapter model does not perform as well,0.7094199061393738
translation,236,158,results,encoder-only domain adapters,has,outperforms,encoder-only domain adapters has outperforms,0.5831657648086548
translation,236,158,results,outperforms,has,decoder-only domain adapter model,outperforms has decoder-only domain adapter model,0.571478009223938
translation,236,158,results,results,usage of,encoder-only domain adapters,results usage of encoder-only domain adapters,0.6974587440490723
translation,236,173,results,improvements,domain-specific ( rather than,language-specific ) information,improvements domain-specific ( rather than language-specific ) information,0.6424388289451599
translation,236,176,results,good scores,translating into,in- domain languages,good scores translating into in- domain languages,0.6412754058837891
translation,236,176,results,good scores,on par or better than,oracle scores,good scores on par or better than oracle scores,0.7148385047912598
translation,236,176,results,good scores,much higher than,baselines,good scores much higher than baselines,0.6028735637664795
translation,236,186,results,decoder-only model,can,better translate,decoder-only model can better translate,0.58404141664505
translation,236,186,results,better translate,from,out - of- domain languages,better translate from out - of- domain languages,0.6008909344673157
translation,236,186,results,slightly improves,for,translations,slightly improves for translations,0.5735281109809875
translation,236,186,results,translations,into,out - of- domain languages,translations into out - of- domain languages,0.5894005298614502
translation,236,186,results,encoder-only model,has,slightly improves,encoder-only model has slightly improves,0.5730229616165161
translation,236,186,results,results,has,decoder-only model,results has decoder-only model,0.5368703603744507
translation,236,191,results,translation quality,into,out - of- domain languages,translation quality into out - of- domain languages,0.5606496334075928
translation,236,192,results,model,with,tags,model with tags,0.7005603313446045
translation,236,192,results,tags,reaches,competitive results,tags reaches competitive results,0.7369168400764465
translation,236,192,results,results,has,model,results has model,0.5339115858078003
translation,236,193,results,models,with,back - translation data,models with back - translation data,0.6261983513832092
translation,236,193,results,encoder-only adapter ( 19 ) model,on,out - of- domain target languages,encoder-only adapter ( 19 ) model on out - of- domain target languages,0.5090672969818115
translation,236,193,results,strongest results,on,translating,strongest results on translating,0.5398058891296387
translation,236,193,results,models,has,decoder-only adapter ( 20 ) model,models has decoder-only adapter ( 20 ) model,0.5804252028465271
translation,236,193,results,back - translation data,has,decoder-only adapter ( 20 ) model,back - translation data has decoder-only adapter ( 20 ) model,0.5692973732948303
translation,236,193,results,decoder-only adapter ( 20 ) model,has,outperforms,decoder-only adapter ( 20 ) model has outperforms,0.6047337651252747
translation,236,193,results,outperforms,has,encoder-only adapter ( 19 ) model,outperforms has encoder-only adapter ( 19 ) model,0.5963540077209473
translation,236,193,results,results,For,models,results For models,0.6105780005455017
translation,236,195,results,decoder-only bt model,improves over,baseline,decoder-only bt model improves over baseline,0.6997827887535095
translation,236,195,results,),improves over,baseline,) improves over baseline,0.7139260172843933
translation,236,195,results,baseline,for,all the language directions,baseline for all the language directions,0.606200635433197
translation,236,195,results,all the language directions,except for,translation into english,all the language directions except for translation into english,0.690976083278656
translation,236,195,results,decoder-only bt model,has,),decoder-only bt model has ),0.5978346467018127
translation,236,195,results,results,has,decoder-only bt model,results has decoder-only bt model,0.5312868356704712
translation,236,201,results,backtranslation,reaches,very competitive results,backtranslation reaches very competitive results,0.6739251017570496
translation,236,203,results,performance,on,out?out and in?out groups,performance on out?out and in?out groups,0.5661172866821289
translation,236,203,results,performance,on,out ? in group,performance on out ? in group,0.5856243371963501
translation,236,203,results,performance,on,out ? in group,performance on out ? in group,0.5856243371963501
translation,236,203,results,performance,on,out ? in group,performance on out ? in group,0.5856243371963501
translation,236,203,results,back-translation,has,improves,back-translation has improves,0.5933161973953247
translation,236,203,results,improves,has,performance,improves has performance,0.5770372748374939
translation,236,203,results,decreases,has,performance,decreases has performance,0.5981842875480652
translation,236,203,results,results,has,back-translation,results has back-translation,0.5638759732246399
translation,237,76,experimental-setup,mariannmt,to train,transformer - big models,mariannmt to train transformer - big models,0.7501590251922607
translation,237,76,experimental-setup,transformer - big models,with,standard parameters,transformer - big models with standard parameters,0.6103443503379822
translation,237,76,experimental-setup,experimental setup,use,mariannmt,experimental setup use mariannmt,0.5695306658744812
translation,237,88,model,term metric,is,weighted ter,term metric is weighted ter,0.5975101590156555
translation,237,88,model,weighted ter,uses,higher weights,weighted ter uses higher weights,0.690205454826355
translation,237,88,model,higher weights,for,tokens,higher weights for tokens,0.6594313383102417
translation,237,88,model,tokens,part of,term,tokens part of term,0.6268938779830933
translation,237,88,model,term,from,terminology database,term from terminology database,0.5350854992866516
translation,237,88,model,sensitivity,differences in,terminology,sensitivity differences in terminology,0.7023109793663025
translation,237,88,model,model,has,term metric,model has term metric,0.5507696866989136
translation,237,96,results,model,trained with,lemmatized constraints,model trained with lemmatized constraints,0.7087289690971375
translation,237,96,results,model,uses,only one variant,model uses only one variant,0.6743130683898926
translation,237,96,results,only one variant,performs,best,only one variant performs best,0.680602490901947
translation,237,96,results,most metrics,has,model,most metrics has model,0.5722333788871765
translation,237,96,results,results,in,most metrics,results in most metrics,0.46557459235191345
translation,237,97,results,systems,trained with,multiple variants,systems trained with multiple variants,0.7425857782363892
translation,237,97,results,multiple variants,of,target term,multiple variants of target term,0.5700583457946777
translation,237,97,results,multiple variants,show,large degradation,multiple variants show large degradation,0.6250565648078918
translation,237,97,results,large degradation,in,bleu scores,large degradation in bleu scores,0.5376375913619995
translation,237,97,results,results,has,systems,results has systems,0.5238543152809143
translation,238,180,ablation-analysis,our approach,improves,performance,our approach improves performance,0.6848096251487732
translation,238,180,ablation-analysis,performance,of,pronoun translation,performance of pronoun translation,0.5837688446044922
translation,238,180,ablation-analysis,pronoun translation,while exchanging,context information,pronoun translation while exchanging context information,0.6668720841407776
translation,238,180,ablation-analysis,context information,among,linked words (,context information among linked words (,0.5899796485900879
translation,238,180,ablation-analysis,context information,contributes more,consistency constraint loss,context information contributes more consistency constraint loss,0.6471551060676575
translation,238,180,ablation-analysis,ablation analysis,observe,our approach,ablation analysis observe our approach,0.6144513487815857
translation,238,124,experimental-setup,"opennmt ( klein et al. , 2017 )",as,implementation,"opennmt ( klein et al. , 2017 ) as implementation",0.5013309717178345
translation,238,124,experimental-setup,experimental setup,use,"opennmt ( klein et al. , 2017 )","experimental setup use opennmt ( klein et al. , 2017 )",0.5606618523597717
translation,238,125,experimental-setup,number of linked words,with,current word,number of linked words with current word,0.6030973196029663
translation,238,125,experimental-setup,number of linked words,set,k = 6,number of linked words set k = 6,0.6762471795082092
translation,238,125,experimental-setup,experimental setup,For,number of linked words,experimental setup For number of linked words,0.5492621064186096
translation,238,126,experimental-setup,margin size,in,consistency constraint loss,margin size in consistency constraint loss,0.47628456354141235
translation,238,126,experimental-setup,consistency constraint loss,set to,0.2,consistency constraint loss set to 0.2,0.6505270600318909
translation,238,126,experimental-setup,weight,in,joint objective function,weight in joint objective function,0.4616296589374542
translation,238,126,experimental-setup,experimental setup,has,margin size,experimental setup has margin size,0.5582840442657471
translation,238,7,model,word link,for,each source word,word link for each source word,0.6112532019615173
translation,238,7,model,each source word,in,document,each source word in document,0.5021147131919861
translation,238,7,model,model,first obtaining,word link,model first obtaining word link,0.5930421948432922
translation,238,9,model,encoding sentences,within,document,encoding sentences within document,0.6435361504554749
translation,238,9,model,encoding sentences,properly exchange,context information,encoding sentences properly exchange context information,0.7558777928352356
translation,238,9,model,context information,of,words,context information of words,0.559069812297821
translation,238,9,model,model,when,encoding sentences,model when encoding sentences,0.6342039108276367
translation,238,10,model,auxiliary loss function,to better constrain,translations,auxiliary loss function to better constrain translations,0.7124496102333069
translation,238,10,model,translations,should be,consistent,translations should be consistent,0.6736282706260681
translation,238,10,model,model,propose,auxiliary loss function,model propose auxiliary loss function,0.6706376075744629
translation,238,22,model,word link,for,each source word,word link for each source word,0.6112532019615173
translation,238,22,model,word link,tells,positions,word link tells positions,0.6850915551185608
translation,238,22,model,each source word,in,document,each source word in document,0.5021147131919861
translation,238,22,model,model,obtain,word link,model obtain word link,0.5815140604972839
translation,238,23,model,translation consistency,for,words,translation consistency for words,0.5940489172935486
translation,238,23,model,translation consistency,exchange,context information,translation consistency exchange context information,0.7169374227523804
translation,238,23,model,words,within,link,words within link,0.7193520665168762
translation,238,23,model,context information,encoding,sentences,context information encoding sentences,0.7909486889839172
translation,238,23,model,sentences,in,document,sentences in document,0.5417858362197876
translation,238,23,model,model,To encourage,translation consistency,model To encourage translation consistency,0.6992631554603577
translation,238,136,results,exchanging information,via,words,exchanging information via words,0.6993167996406555
translation,238,136,results,words,within,"word links ( i.e. , + word-link","words within word links ( i.e. , + word-link",0.6134196519851685
translation,238,136,results,words,achieves,significant improvement,words achieves significant improvement,0.6892290711402893
translation,238,136,results,significant improvement,in,bleu,significant improvement in bleu,0.5865817070007324
translation,238,136,results,bleu,over,sentence - level ),bleu over sentence - level ),0.6111981272697449
translation,238,136,results,results,has,exchanging information,results has exchanging information,0.5801989436149597
translation,238,139,results,our approach,gains,+ 2.23 and + 2.05 bleu,our approach gains + 2.23 and + 2.05 bleu,0.6812926530838013
translation,238,139,results,+ 2.23 and + 2.05 bleu,on,two domains,+ 2.23 and + 2.05 bleu on two domains,0.5866289734840393
translation,238,139,results,transformer,has,our approach,transformer has our approach,0.6095443964004517
translation,238,139,results,results,Comparing to,transformer,results Comparing to transformer,0.5134370923042297
translation,238,142,results,our approach,achieves,better performance,our approach achieves better performance,0.6741553544998169
translation,238,142,results,better performance,in,bleu,better performance in bleu,0.5420271754264832
translation,238,142,results,performance,in,ltcr,performance in ltcr,0.5598015189170837
translation,238,142,results,greatly improves,has,performance,greatly improves has performance,0.591407299041748
translation,238,146,results,results,has,english - french translation,results has english - french translation,0.5272669196128845
translation,238,149,results,our approach,gains,+ 2.18 bleu,our approach gains + 2.18 bleu,0.6578370928764343
translation,238,149,results,our approach,gains,+ 6.96 % ltcr,our approach gains + 6.96 % ltcr,0.6956294775009155
translation,238,149,results,+ 6.96 % ltcr,over,transformer,+ 6.96 % ltcr over transformer,0.6817245483398438
translation,238,149,results,results,has,our approach,results has our approach,0.6050099730491638
translation,238,168,results,words,in,word lists,words in word lists,0.49945417046546936
translation,238,168,results,words,with,random words,words with random words,0.5908401608467102
translation,238,168,results,words,achieves,+ 0.49 bleu,words achieves + 0.49 bleu,0.6243860721588135
translation,238,168,results,+ 0.49 bleu,over,transformer,+ 0.49 bleu over transformer,0.6705052256584167
translation,238,168,results,results,replacing,words,results replacing words,0.5482410192489624
translation,238,170,results,random linked words,does not bring,ltcr improvement,random linked words does not bring ltcr improvement,0.6426605582237244
translation,238,170,results,ltcr improvement,over,transformer,ltcr improvement over transformer,0.678365170955658
translation,238,170,results,results,using,random linked words,results using random linked words,0.6375877857208252
translation,238,175,results,our approach,achieves,higher ltcr,our approach achieves higher ltcr,0.7141525149345398
translation,238,175,results,higher ltcr,than,transformer,higher ltcr than transformer,0.6109920740127563
translation,238,175,results,higher ltcr,especially for,nouns,higher ltcr especially for nouns,0.7264723777770996
translation,238,175,results,transformer,over,all pos tags,transformer over all pos tags,0.6840857267379761
translation,238,175,results,results,shows that,our approach,results shows that our approach,0.7036905288696289
translation,238,190,results,transformer,in,64 % cases,transformer in 64 % cases,0.6313254833221436
translation,238,190,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,238,190,results,outperforms,has,transformer,outperforms has transformer,0.6521211862564087
translation,238,190,results,results,has,our approach,results has our approach,0.6050099730491638
translation,238,194,results,spe,slightly improves,bleu,spe slightly improves bleu,0.7322728037834167
translation,238,194,results,spe,slightly improves,ltcr,spe slightly improves ltcr,0.6906977891921997
translation,238,194,results,ltcr,over,word- link transformer,ltcr over word- link transformer,0.6601452231407166
translation,238,194,results,+ 0.84 % ),over,word- link transformer,+ 0.84 % ) over word- link transformer,0.6473676562309265
translation,238,194,results,bleu,has,+ 0.49 ),bleu has + 0.49 ),0.6168068051338196
translation,238,194,results,ltcr,has,+ 0.84 % ),ltcr has + 0.84 % ),0.5771304965019226
translation,238,194,results,word- link transformer,has,without spe,word- link transformer has without spe,0.6477320194244385
translation,238,194,results,results,has,spe,results has spe,0.4727642834186554
translation,239,73,ablation-analysis,perplexity,is,small,perplexity is small,0.5910595655441284
translation,239,73,ablation-analysis,small,when adding,additional bible data,small when adding additional bible data,0.759327232837677
translation,239,73,ablation-analysis,additional bible data,to,jw300,additional bible data to jw300,0.5995797514915466
translation,239,73,ablation-analysis,additional bible data,namely,% ( en ),additional bible data namely % ( en ),0.7082844972610474
translation,239,73,ablation-analysis,additional bible data,namely,% ( yo ),additional bible data namely % ( yo ),0.6909687519073486
translation,239,117,ablation-analysis,bleu ( precision ),of,19.0 ( 29.8 ),bleu ( precision ) of 19.0 ( 29.8 ),0.5681805610656738
translation,239,117,ablation-analysis,diacritics,using,our system,diacritics using our system,0.708461344242096
translation,239,117,ablation-analysis,bleu ( precision ),of,87.0 ( 97.1 ),bleu ( precision ) of 87.0 ( 97.1 ),0.5609130263328552
translation,239,117,ablation-analysis,ablation analysis,On,test set,ablation analysis On test set,0.5879389643669128
translation,239,125,ablation-analysis,menyo - 20 k ( c4 ),to,training data,menyo - 20 k ( c4 ) to training data,0.56597900390625
translation,239,125,ablation-analysis,translation quality,increases by,+ 2.8 ( en-yo ) and + 3.2 ( yo-en ),translation quality increases by + 2.8 ( en-yo ) and + 3.2 ( yo-en ),0.7018859386444092
translation,239,125,ablation-analysis,menyo - 20 k ( c4 ),has,translation quality,menyo - 20 k ( c4 ) has translation quality,0.570357084274292
translation,239,125,ablation-analysis,training data,has,translation quality,training data has translation quality,0.5148476958274841
translation,239,125,ablation-analysis,ablation analysis,adding,menyo - 20 k ( c4 ),ablation analysis adding menyo - 20 k ( c4 ),0.7228484749794006
translation,239,92,baselines,fine-tuning,has,massively multilingual model mt5,fine-tuning has massively multilingual model mt5,0.6168467998504639
translation,239,119,baselines,four basic nmt engines,on,different subsets,four basic nmt engines on different subsets,0.54631507396698
translation,239,119,baselines,different subsets,of,training data,different subsets of training data,0.6039957404136658
translation,239,176,baselines,three top performing models,in,en-yo and yo-en,three top performing models in en-yo and yo-en,0.5547147393226624
translation,239,7,experiments,clean orthography,for,yor ?b?- english,clean orthography for yor ?b?- english,0.645180881023407
translation,239,7,experiments,menyo - 20k,has,first multi-domain parallel corpus,menyo - 20k has first multi-domain parallel corpus,0.5444321632385254
translation,239,17,experiments,yor ?b?- english ( yo-en ) language pair,by creating,multi-domain parallel dataset,yor ?b?- english ( yo-en ) language pair by creating multi-domain parallel dataset,0.6766066551208496
translation,239,141,experiments,largely outperformed,even by,our simple jw300 baseline,largely outperformed even by our simple jw300 baseline,0.6817492246627808
translation,239,141,experiments,largely outperformed,by,5 bleu points,largely outperformed by 5 bleu points,0.5787646770477295
translation,239,141,experiments,5 bleu points,in,both translation directions,5 bleu points in both translation directions,0.4995136260986328
translation,239,168,experiments,proverb subset,is,especially difficult,proverb subset is especially difficult,0.5853137373924255
translation,239,168,experiments,proverb subset,shows,lowest translation performance,proverb subset shows lowest translation performance,0.66892409324646
translation,239,168,experiments,especially difficult,in,both directions,especially difficult in both directions,0.5561556220054626
translation,239,168,experiments,lowest translation performance,across,all domains,lowest translation performance across all domains,0.6576152443885803
translation,239,168,experiments,maximum bleu,of,9.04 ( en-yo ) and 8.74 ( yo-en ),maximum bleu of 9.04 ( en-yo ) and 8.74 ( yo-en ),0.6094895601272583
translation,239,170,experiments,ted domain,is,best performing test domain,ted domain is best performing test domain,0.5869288444519043
translation,239,170,experiments,best performing test domain,with,maximum bleu,best performing test domain with maximum bleu,0.6118436455726624
translation,239,170,experiments,maximum bleu,of,16.1 ( en-yo ) and 16.8 ( yo-en ),maximum bleu of 16.1 ( en-yo ) and 16.8 ( yo-en ),0.6181102395057678
translation,239,84,hyperparameters,drop-out,at,0.3,drop-out at 0.3,0.5457861423492432
translation,239,84,hyperparameters,batch size,at,10,batch size at 10,0.6211313009262085
translation,239,84,hyperparameters,batch size,at,", 240 tokens","batch size at , 240 tokens",0.5471739172935486
translation,239,84,hyperparameters,10,has,", 240 tokens","10 has , 240 tokens",0.5970914959907532
translation,239,84,hyperparameters,hyperparameters,set,drop-out,hyperparameters set drop-out,0.6520504355430603
translation,239,84,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,239,85,hyperparameters,optimization,use,"adam ( kingma and ba , 2015 )","optimization use adam ( kingma and ba , 2015 )",0.5987501740455627
translation,239,85,hyperparameters,"adam ( kingma and ba , 2015 )",with,? 1 = 0.9 and ? 2 = 0.98,"adam ( kingma and ba , 2015 ) with ? 1 = 0.9 and ? 2 = 0.98",0.6558489799499512
translation,239,85,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,239,85,hyperparameters,learning rate,of,0.0005,learning rate of 0.0005,0.6050455570220947
translation,239,85,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,239,86,hyperparameters,warmup update,of,4000,warmup update of 4000,0.6262494325637817
translation,239,86,hyperparameters,label smoothed cross-entropy loss function,with,label - smoothing value,label smoothed cross-entropy loss function with label - smoothing value,0.6065837144851685
translation,239,86,hyperparameters,label - smoothing value,of,0.1,label - smoothing value of 0.1,0.5840045809745789
translation,239,86,hyperparameters,learning rate,has,warmup update,learning rate has warmup update,0.5122045278549194
translation,239,86,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,239,107,hyperparameters,joint bpe,with,vocabulary threshold,joint bpe with vocabulary threshold,0.6747830510139465
translation,239,107,hyperparameters,vocabulary threshold,of,20 and 40 k merge operations,vocabulary threshold of 20 and 40 k merge operations,0.6263254284858704
translation,239,107,hyperparameters,hyperparameters,apply,joint bpe,hyperparameters apply joint bpe,0.6083403825759888
translation,239,74,results,both jw300 and bible,close to,ted domain,both jw300 and bible close to ted domain,0.7689037322998047
translation,239,74,results,1st and 2nd lowest perplexities,for,en and yo,1st and 2nd lowest perplexities for en and yo,0.585067868232727
translation,239,74,results,results,has,both jw300 and bible,results has both jw300 and bible,0.5046300292015076
translation,239,75,results,domain-diverse menyo - 20 k corpus,largely decreases,perplexity,domain-diverse menyo - 20 k corpus largely decreases perplexity,0.7320143580436707
translation,239,75,results,perplexity,across,all domains,perplexity across all domains,0.696245551109314
translation,239,75,results,perplexity,with,major decrease,perplexity with major decrease,0.6506114602088928
translation,239,75,results,%,on,it ( yo ),% on it ( yo ),0.6018916964530945
translation,239,75,results,%,on,books ( en ),% on books ( en ),0.533551812171936
translation,239,75,results,results,Adding,domain-diverse menyo - 20 k corpus,results Adding domain-diverse menyo - 20 k corpus,0.6209238171577454
translation,239,76,results,perplexity scores,correlate negatively with,resulting bleu scores,perplexity scores correlate negatively with resulting bleu scores,0.6186784505844116
translation,239,76,results,results,has,perplexity scores,results has perplexity scores,0.5127846002578735
translation,239,129,results,menyo - 20k dataset,to fine- tune,converged jw300 + bible + menyo - 20 k model,menyo - 20k dataset to fine- tune converged jw300 + bible + menyo - 20 k model,0.6649172306060791
translation,239,129,results,bleu,over,jw300 + bible,bleu over jw300 + bible,0.7177693247795105
translation,239,129,results,bleu,for,both translation directions,bleu for both translation directions,0.521539568901062
translation,239,129,results,jw300 + bible,for,both translation directions,jw300 + bible for both translation directions,0.5996580123901367
translation,239,129,results,+ 4.3,for,en-yo,+ 4.3 for en-yo,0.6541216373443604
translation,239,129,results,+ 4.3,for,yo-en,+ 4.3 for yo-en,0.6704615950584412
translation,239,129,results,+ 3.8,for,yo-en,+ 3.8 for yo-en,0.663411557674408
translation,239,129,results,both translation directions,has,+ 4.3,both translation directions has + 4.3,0.5443980693817139
translation,239,129,results,results,use,menyo - 20k dataset,results use menyo - 20k dataset,0.5609375238418579
translation,239,131,results,performance,of,semi-supervised system ( c4 + transfer + bt ),performance of semi-supervised system ( c4 + transfer + bt ),0.5640989542007446
translation,239,132,results,two iterations,of,bt,two iterations of bt,0.6325480341911316
translation,239,132,results,two iterations,obtain,improvement,two iterations obtain improvement,0.5924918055534363
translation,239,132,results,bt,obtain,improvement,bt obtain improvement,0.5766226649284363
translation,239,132,results,improvement,of,+ 3.6 bleu points,improvement of + 3.6 bleu points,0.5173817873001099
translation,239,132,results,+ 3.6 bleu points,on,yo-en,+ 3.6 bleu points on yo-en,0.6354838013648987
translation,239,132,results,results,After,two iterations,results After two iterations,0.710776686668396
translation,239,134,results,fine-tuning mt5,with,menyo - 20 k,fine-tuning mt5 with menyo - 20 k,0.6745850443840027
translation,239,134,results,fine-tuning,only,jw300 + bible system,fine-tuning only jw300 + bible system,0.6990020275115967
translation,239,134,results,jw300 + bible system,on,en-yo,jw300 + bible system on en-yo,0.6055902242660522
translation,239,134,results,fine-tuning,has,jw300 + bible system,fine-tuning has jw300 + bible system,0.6139799356460571
translation,239,134,results,results,has,fine-tuning mt5,results has fine-tuning mt5,0.565925657749176
translation,239,138,results,opus - mt,achieves,bleu,opus - mt achieves bleu,0.726111888885498
translation,239,138,results,bleu,of,5.9,bleu of 5.9,0.6060665845870972
translation,239,138,results,5.9,for,yo-en,5.9 for yo-en,0.655539333820343
translation,239,138,results,results,has,opus - mt,results has opus - mt,0.570850670337677
translation,239,139,results,jw300,largely outperformed by,our nmt model,jw300 largely outperformed by our nmt model,0.7954322099685669
translation,239,139,results,our nmt model,trained on,jw300 only ( bleu + 3.7 ),our nmt model trained on jw300 only ( bleu + 3.7 ),0.7470579743385315
translation,239,144,results,opposite direction,shows,significantly lower performance ( bleu 3.7 ),opposite direction shows significantly lower performance ( bleu 3.7 ),0.6542052030563354
translation,239,144,results,en-yo,shows,significantly lower performance ( bleu 3.7 ),en-yo shows significantly lower performance ( bleu 3.7 ),0.6494970917701721
translation,239,144,results,outperformed,by,our simple jw300 baseline ( bleu + 3.8 ),outperformed by our simple jw300 baseline ( bleu + 3.8 ),0.5444887280464172
translation,239,144,results,opposite direction,has,en-yo,opposite direction has en-yo,0.6337714791297913
translation,239,144,results,results,has,opposite direction,results has opposite direction,0.5950462818145752
translation,239,157,results,both models,perform,equally well,both models perform equally well,0.6005297899246216
translation,239,157,results,equally well,with,bleu 14.0,equally well with bleu 14.0,0.6651626825332642
translation,239,157,results,bleu 14.0,for,diacritized and undiacritized versions,bleu 14.0 for diacritized and undiacritized versions,0.6206831932067871
translation,239,157,results,in- domain data ( jw300 + bible + menyo - 20 k ),has,both models,in- domain data ( jw300 + bible + menyo - 20 k ) has both models,0.5567599534988403
translation,239,158,results,diacritization,not needed,bleu,diacritization not needed bleu,0.6290623545646667
translation,239,158,results,diacritization,fine - tune,model,diacritization fine - tune model,0.6896392703056335
translation,239,158,results,model,with,data,model with data,0.6407009959220886
translation,239,158,results,data,shares,domain,data shares domain,0.7850494980812073
translation,239,158,results,bleu,is,13.2,bleu is 13.2,0.6277104020118713
translation,239,158,results,13.2,for,diacritized version,13.2 for diacritized version,0.5874237418174744
translation,239,158,results,test set,has,+ transfer ),test set has + transfer ),0.6585197448730469
translation,239,158,results,results,has,diacritization,results has diacritization,0.5562979578971863
translation,239,172,results,most bleu results,on line with,lm perplexity results,most bleu results on line with lm perplexity results,0.5648747086524963
translation,239,172,results,results,has,most bleu results,results has most bleu results,0.5453393459320068
translation,239,173,results,only small improvements,of,bleu,only small improvements of bleu,0.5327965021133423
translation,239,173,results,only small improvements,i.e.,+ 0.7 ( yo-en ),only small improvements i.e. + 0.7 ( yo-en ),0.7195607423782349
translation,239,173,results,only small improvements,when adding,menyo - 20 k ( c4 ),only small improvements when adding menyo - 20 k ( c4 ),0.7900070548057556
translation,239,173,results,menyo - 20 k ( c4 ),to,jw300 + bible ( c3 ) training data pool,menyo - 20 k ( c4 ) to jw300 + bible ( c3 ) training data pool,0.5411994457244873
translation,239,174,results,it domain,benefits the most,additional menyo - 20 k data,it domain benefits the most additional menyo - 20 k data,0.6617952585220337
translation,239,174,results,additional menyo - 20 k data,with,major gains,additional menyo - 20 k data with major gains,0.6681911945343018
translation,239,174,results,major gains,of,bleu + 5.5 ( en-yo ) and 4.6 ( yo-en ),major gains of bleu + 5.5 ( en-yo ) and 4.6 ( yo-en ),0.5934545397758484
translation,239,174,results,results,has,it domain,results has it domain,0.5612241625785828
translation,239,187,results,intra-agreement,for,four yor ?b? raters,intra-agreement for four yor ?b? raters,0.6130982041358948
translation,239,187,results,intra-agreement,for,three english raters,intra-agreement for three english raters,0.5407853126525879
translation,239,187,results,intra-agreement,for,three english raters,intra-agreement for three english raters,0.5407853126525879
translation,239,187,results,four yor ?b? raters,are,"0.75 , 0.91 , 0.66 , and 0.87","four yor ?b? raters are 0.75 , 0.91 , 0.66 , and 0.87",0.540327787399292
translation,239,187,results,intra-agreement,for,three english raters,intra-agreement for three english raters,0.5407853126525879
translation,239,187,results,three english raters,across,all evaluation tasks,three english raters across all evaluation tasks,0.5885601043701172
translation,239,187,results,three english raters,are,"0.92 , 0.71 , and 0.81","three english raters are 0.92 , 0.71 , and 0.81",0.5214992165565491
translation,239,187,results,all evaluation tasks,are,"0.92 , 0.71 , and 0.81","all evaluation tasks are 0.92 , 0.71 , and 0.81",0.5384658575057983
translation,239,187,results,results,has,intra-agreement,results has intra-agreement,0.48839130997657776
translation,239,188,results,our evaluators,rated on average,gmnmt,our evaluators rated on average gmnmt,0.7574504017829895
translation,239,188,results,gmnmt,to be,best,gmnmt to be best,0.6132866144180298
translation,239,188,results,best,in terms of,adequacy ( 4.02 out of 5 ),best in terms of adequacy ( 4.02 out of 5 ),0.6833885312080383
translation,239,188,results,yo-en,has,our evaluators,yo-en has our evaluators,0.6040360927581787
translation,239,188,results,results,For,yo-en,results For yo-en,0.6111958026885986
translation,239,190,results,gmnmt,performs,worst,gmnmt performs worst,0.6745718121528625
translation,239,190,results,best,in terms of,adequacy ( 3.69 ),best in terms of adequacy ( 3.69 ),0.685854971408844
translation,239,190,results,adequacy ( 3.69 ),followed by,c4 + transfer + bt,adequacy ( 3.69 ) followed by c4 + transfer + bt,0.6812529563903809
translation,239,190,results,worst,in terms of,fluency and diacritics prediction accuracy,worst in terms of fluency and diacritics prediction accuracy,0.6804914474487305
translation,239,190,results,en-yo,has,gmnmt,en-yo has gmnmt,0.6998422741889954
translation,239,190,results,results,For,en-yo,results For en-yo,0.6344481110572815
translation,239,193,results,c4 + transfer and c4 + transfer + bt,trained on,cleaned corpora,c4 + transfer and c4 + transfer + bt trained on cleaned corpora,0.7886664867401123
translation,239,193,results,high scores,in terms of,fluency,high scores in terms of fluency,0.6824199557304382
translation,239,193,results,near perfect score,in,diacritics prediction accuracy ( 4.91 ? 0.1 ),near perfect score in diacritics prediction accuracy ( 4.91 ? 0.1 ),0.4813744127750397
translation,239,193,results,results,has,c4 + transfer and c4 + transfer + bt,results has c4 + transfer and c4 + transfer + bt,0.5524266362190247
translation,241,6,results,our models,for,catalan -spanish ( 82.79 bleu ) and portuguese -spanish ( 87.11 bleu ),our models for catalan -spanish ( 82.79 bleu ) and portuguese -spanish ( 87.11 bleu ),0.567158043384552
translation,241,6,results,our models,rank,top 1,our models rank top 1,0.7407039403915405
translation,241,6,results,results,has,our models,results has our models,0.5733726620674133
translation,241,24,results,our experiments,show that,tokenized settings,our experiments show that tokenized settings,0.5280876755714417
translation,241,24,results,perform better,than,untokenized settings,perform better than untokenized settings,0.5938501358032227
translation,241,24,results,untokenized settings,for,all language pairs,untokenized settings for all language pairs,0.5751688480377197
translation,241,24,results,tokenized settings,has,perform better,tokenized settings has perform better,0.6006574034690857
translation,241,25,results,model,fine-tuned,pre-trained mt model,model fine-tuned pre-trained mt model,0.6800582408905029
translation,241,25,results,pre-trained mt model,has,higher performance,pre-trained mt model has higher performance,0.5631871223449707
translation,241,25,results,higher performance,than,our baseline model,higher performance than our baseline model,0.5530377626419067
translation,241,25,results,model,trained from,scratch,model trained from scratch,0.7898691892623901
translation,241,25,results,model,for,six epochs,model for six epochs,0.6270445585250854
translation,241,25,results,pre-trained mt model,has,higher performance,pre-trained mt model has higher performance,0.5631871223449707
translation,241,25,results,results,has,model,results has model,0.5339115858078003
translation,241,26,results,our models,for,ca -es and pt - es language pairs,our models for ca -es and pt - es language pairs,0.6281929612159729
translation,241,26,results,ca -es and pt - es language pairs,achieve,top 1 rank,ca -es and pt - es language pairs achieve top 1 rank,0.6020026206970215
translation,241,26,results,top 1 rank,in,offical shared task results,top 1 rank in offical shared task results,0.50272136926651
translation,241,26,results,results,has,our models,results has our models,0.5733726620674133
translation,242,71,ablation-analysis,large-scale in- domain data,makes,our en- ru nmt system,large-scale in- domain data makes our en- ru nmt system,0.6130350232124329
translation,242,71,ablation-analysis,significantly lower,than,state- ofthe - art systems,significantly lower than state- ofthe - art systems,0.565684974193573
translation,242,71,ablation-analysis,our en- ru nmt system,has,significantly lower,our en- ru nmt system has significantly lower,0.5734915733337402
translation,242,71,ablation-analysis,ablation analysis,lack of,large-scale in- domain data,ablation analysis lack of large-scale in- domain data,0.6865251660346985
translation,242,47,experimental-setup,iterative knowledge distillation,performed with,3 big transformer teachers,iterative knowledge distillation performed with 3 big transformer teachers,0.6597188711166382
translation,242,47,experimental-setup,iterative knowledge distillation,performed with,3 iterations,iterative knowledge distillation performed with 3 iterations,0.6669464707374573
translation,242,47,experimental-setup,experimental setup,has,iterative knowledge distillation,experimental setup has iterative knowledge distillation,0.5464694499969482
translation,242,53,experimental-setup,learning rate,set to,0.0007,learning rate set to 0.0007,0.703606128692627
translation,242,53,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,242,54,experimental-setup,600k steps,on,8 tesla v100 gpus,600k steps on 8 tesla v100 gpus,0.483174204826355
translation,242,54,experimental-setup,600k steps,allocated with,batch size,600k steps allocated with batch size,0.7386699318885803
translation,242,54,experimental-setup,batch size,of,8192 tokens,batch size of 8192 tokens,0.5871670246124268
translation,242,55,experimental-setup,pre-train model,adopt,publicly available mbart25 20 model,pre-train model adopt publicly available mbart25 20 model,0.617902934551239
translation,242,55,experimental-setup,pre-train model,fine- tune,mbart25,pre-train model fine- tune mbart25,0.7057101726531982
translation,242,55,experimental-setup,mbart25,on,in-domain data,mbart25 on in-domain data,0.5650221109390259
translation,242,55,experimental-setup,experimental setup,For,pre-train model,experimental setup For pre-train model,0.6119672656059265
translation,242,56,experimental-setup,finetuning phase,minimize,label smoothed cross entropy,finetuning phase minimize label smoothed cross entropy,0.7384609580039978
translation,242,56,experimental-setup,label smoothed cross entropy,with,smoothing factor,label smoothed cross entropy with smoothing factor,0.6301195621490479
translation,242,56,experimental-setup,smoothing factor,of,0.2,smoothing factor of 0.2,0.619792640209198
translation,242,56,experimental-setup,experimental setup,In,finetuning phase,experimental setup In finetuning phase,0.541785717010498
translation,242,57,experimental-setup,"adam ( kingma and ba , 2015 ) optimizer",with,"? 1 = 0.9 , ? 2 = 0.98 , and = 1e?6","adam ( kingma and ba , 2015 ) optimizer with ? 1 = 0.9 , ? 2 = 0.98 , and = 1e?6",0.5935957431793213
translation,242,57,experimental-setup,experimental setup,use,"adam ( kingma and ba , 2015 ) optimizer","experimental setup use adam ( kingma and ba , 2015 ) optimizer",0.6027743816375732
translation,242,58,experimental-setup,learning rate,scheduled to,increase,learning rate scheduled to increase,0.6434381008148193
translation,242,58,experimental-setup,learning rate,scheduled to,decreases,learning rate scheduled to decreases,0.6605379581451416
translation,242,58,experimental-setup,increase,from,0,increase from 0,0.6695120930671692
translation,242,58,experimental-setup,increase,from,0,increase from 0,0.6695120930671692
translation,242,58,experimental-setup,0,to,maximum value,0 to maximum value,0.5739943981170654
translation,242,58,experimental-setup,maximum value,in,warm - up phase,maximum value in warm - up phase,0.543614387512207
translation,242,58,experimental-setup,linearly,to,0,linearly to 0,0.7068044543266296
translation,242,58,experimental-setup,0,in,remaining steps,0 in remaining steps,0.5323295593261719
translation,242,58,experimental-setup,decreases,has,linearly,decreases has linearly,0.6106449961662292
translation,242,58,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,242,59,experimental-setup,dropout rate,is,0.3,dropout rate is 0.3,0.5389049649238586
translation,242,59,experimental-setup,dropout rate,is,0.1,dropout rate is 0.1,0.53923100233078
translation,242,59,experimental-setup,0.3,for,each residual connection,0.3 for each residual connection,0.6345871090888977
translation,242,59,experimental-setup,0.1,for,attention matrices,0.1 for attention matrices,0.6098425984382629
translation,242,59,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,242,65,experimental-setup,in-domain data,as,default setting,in-domain data as default setting,0.5530017018318176
translation,242,65,experimental-setup,in-domain data,apply,setting,in-domain data apply setting,0.608858585357666
translation,242,65,experimental-setup,back-translation,as,default setting,back-translation as default setting,0.5525692701339722
translation,242,65,experimental-setup,back-translation,as,setting,back-translation as setting,0.5893056988716125
translation,242,65,experimental-setup,back-translation,apply,setting,back-translation apply setting,0.615894615650177
translation,242,65,experimental-setup,setting,to,transformer - big and transformer - large models,setting to transformer - big and transformer - large models,0.5784745812416077
translation,242,65,experimental-setup,transformer - big and transformer - large models,on,eight language directions,transformer - big and transformer - large models on eight language directions,0.5381584167480469
translation,242,65,experimental-setup,in-domain data,has,data rejuvenation,in-domain data has data rejuvenation,0.5538742542266846
translation,242,65,experimental-setup,in-domain data,has,back-translation,in-domain data has back-translation,0.5857216715812683
translation,242,65,experimental-setup,experimental setup,adopt,in-domain data,experimental setup adopt in-domain data,0.6233462691307068
translation,242,68,experimental-setup,length penalty,set to,0.6,length penalty set to 0.6,0.682100236415863
translation,242,68,experimental-setup,beam size,set to,4,beam size set to 4,0.7776850461959839
translation,242,68,experimental-setup,model inference,has,length penalty,model inference has length penalty,0.5176724195480347
translation,242,68,experimental-setup,model inference,has,beam size,model inference has beam size,0.5317088961601257
translation,242,68,experimental-setup,experimental setup,For,model inference,experimental setup For model inference,0.548684298992157
translation,242,46,experiments,synthetic bilingual data generation,adopt,iterative knowledge distillation,synthetic bilingual data generation adopt iterative knowledge distillation,0.5921837091445923
translation,242,46,experiments,iterative knowledge distillation,to improve,translation quality,iterative knowledge distillation to improve translation quality,0.6358799338340759
translation,242,52,experiments,student model,on,"combination of the synthetic and real bilingual data ( jiao et al. , 2021 )","student model on combination of the synthetic and real bilingual data ( jiao et al. , 2021 )",0.5211981534957886
translation,242,66,experiments,5 big and 5 large transformer models,with,different random seeds initialization,5 big and 5 large transformer models with different random seeds initialization,0.5846053957939148
translation,242,29,hyperparameters,sequence- to- sequence pre-training,adopt,mbart25,sequence- to- sequence pre-training adopt mbart25,0.6425356864929199
translation,242,29,hyperparameters,mbart25,as,pre-train model,mbart25 as pre-train model,0.5114705562591553
translation,242,29,hyperparameters,12 encoder and decoder layers,with,default size,12 encoder and decoder layers with default size,0.6190682649612427
translation,242,29,hyperparameters,default size,of,hidden state,default size of hidden state,0.5994306802749634
translation,242,29,hyperparameters,hidden state,is,1024,hidden state is 1024,0.6178596019744873
translation,242,29,hyperparameters,hyperparameters,For,sequence- to- sequence pre-training,hyperparameters For sequence- to- sequence pre-training,0.575443685054779
translation,242,13,model,pre-train and fine-tune paradigm,for,biomedical translation task,pre-train and fine-tune paradigm for biomedical translation task,0.5758660435676575
translation,242,13,model,model,apply,pre-train and fine-tune paradigm,model apply pre-train and fine-tune paradigm,0.6539490818977356
translation,242,14,model,pre-train model,trained on,large-scale monolingual data,pre-train model trained on large-scale monolingual data,0.7196468114852905
translation,242,14,model,pre-train model,fine-tuned on,downstream bilingual data,pre-train model fine-tuned on downstream bilingual data,0.74336177110672
translation,242,14,model,large-scale monolingual data,in,self-supervised manner,large-scale monolingual data in self-supervised manner,0.5001307725906372
translation,242,14,model,model,has,pre-train model,model has pre-train model,0.5945674777030945
translation,242,15,model,encoder-decoder pre-trained model mbart,to implement,pre-training strategy,encoder-decoder pre-trained model mbart to implement pre-training strategy,0.71989506483078
translation,242,15,model,model,adopt,encoder-decoder pre-trained model mbart,model adopt encoder-decoder pre-trained model mbart,0.6514963507652283
translation,242,27,model,big and large transformer models,contain,6 - layer and 20 - layer encoders,big and large transformer models contain 6 - layer and 20 - layer encoders,0.5974099040031433
translation,242,27,model,6 - layer and 20 - layer encoders,with,transformer - big setting,6 - layer and 20 - layer encoders with transformer - big setting,0.693060040473938
translation,242,27,model,model,has,big and large transformer models,model has big and large transformer models,0.5372217893600464
translation,242,30,model,denoising objective,on,large-scale monolingual data,denoising objective on large-scale monolingual data,0.48540231585502625
translation,242,30,model,model,pre-trained with,denoising objective,model pre-trained with denoising objective,0.7174056172370911
translation,242,64,model,data rejuvenation,achieve,further improvement,data rejuvenation achieve further improvement,0.6333909630775452
translation,242,64,model,data rejuvenation,has,back-translation,data rejuvenation has back-translation,0.5343337059020996
translation,242,64,model,data rejuvenation,has,model ensemble strategies,data rejuvenation has model ensemble strategies,0.5176211595535278
translation,242,64,model,model,apply,data rejuvenation,model apply data rejuvenation,0.6526869535446167
translation,242,67,model,model ensemble strategy,with,greedy based ensemble,model ensemble strategy with greedy based ensemble,0.6244258880615234
translation,242,67,model,model ensemble strategy,to get,final translation outputs,model ensemble strategy to get final translation outputs,0.585361897945404
translation,242,63,results,in- domain data,improves,baseline transformer - big model,in- domain data improves baseline transformer - big model,0.685405969619751
translation,242,63,results,baseline transformer - big model,with,0.42 bleu point,baseline transformer - big model with 0.42 bleu point,0.5722246766090393
translation,242,63,results,results,has,in- domain data,results has in- domain data,0.5435342192649841
translation,242,70,results,pretraining and back -translation strategies,achieve,strong performance,pretraining and back -translation strategies achieve strong performance,0.6018058657646179
translation,242,70,results,strong performance,on,"de?en , en?fr and es? en translation tasks","strong performance on de?en , en?fr and es? en translation tasks",0.5182483196258545
translation,242,70,results,different transformer architectures,has,pretraining and back -translation strategies,different transformer architectures has pretraining and back -translation strategies,0.5460965633392334
translation,242,70,results,results,utilizing,different transformer architectures,results utilizing different transformer architectures,0.6318778395652771
translation,242,82,results,our final systems,in,german / french / spanish ? english,our final systems in german / french / spanish ? english,0.521324098110199
translation,242,82,results,results,has,our final systems,results has our final systems,0.5989584922790527
translation,243,205,ablation-analysis,contrastive learning,increases,translation diversity,contrastive learning increases translation diversity,0.6596806645393372
translation,243,205,ablation-analysis,ablation analysis,indicate,contrastive learning,ablation analysis indicate contrastive learning,0.5957548022270203
translation,243,146,baselines,transformer - based nmt model,pretrained on,wmt2017 corpus,transformer - based nmt model pretrained on wmt2017 corpus,0.8007946014404297
translation,243,9,experiments,first chinese-english parallel corpus,annotated with,user behavior,first chinese-english parallel corpus annotated with user behavior,0.7206211090087891
translation,243,9,experiments,user behavior,called,udt - corpus,user behavior called udt - corpus,0.6403560638427734
translation,243,135,hyperparameters,byte pair encoding,with,32 k merge operations,byte pair encoding with 32 k merge operations,0.6211714744567871
translation,243,135,hyperparameters,32 k merge operations,to deal with,all sentences,32 k merge operations to deal with all sentences,0.6603020429611206
translation,243,135,hyperparameters,hyperparameters,employ,byte pair encoding,hyperparameters employ byte pair encoding,0.5535168051719666
translation,243,136,hyperparameters,word embedding dimension,set to,512,word embedding dimension set to 512,0.6932660341262817
translation,243,136,hyperparameters,hidden layer dimension,is,2048,hidden layer dimension is 2048,0.5858369469642639
translation,243,136,hyperparameters,layer numbers,of,both encoder and decoder,layer numbers of both encoder and decoder,0.5850305557250977
translation,243,136,hyperparameters,layer numbers,set to,6,layer numbers set to 6,0.7194451093673706
translation,243,136,hyperparameters,both encoder and decoder,set to,6,both encoder and decoder set to 6,0.7182243466377258
translation,243,136,hyperparameters,number of attention heads,set to,8,number of attention heads set to 8,0.7061899900436401
translation,243,137,hyperparameters,4 gpus,for,training,4 gpus for training,0.5195785760879517
translation,243,138,hyperparameters,pre-training stage,employ,adam optimizer,pre-training stage employ adam optimizer,0.5864470601081848
translation,243,138,hyperparameters,adam optimizer,with,? 2 = 0.998,adam optimizer with ? 2 = 0.998,0.6130428910255432
translation,243,138,hyperparameters,hyperparameters,At,pre-training stage,hyperparameters At pre-training stage,0.4681013226509094
translation,243,139,hyperparameters,batch size,of,"16,384 tokens","batch size of 16,384 tokens",0.5854885578155518
translation,243,139,hyperparameters,model,for,"200,000 steps","model for 200,000 steps",0.6762357354164124
translation,243,139,hyperparameters,hyperparameters,use,batch size,hyperparameters use batch size,0.6251612901687622
translation,243,139,hyperparameters,hyperparameters,pre-train,model,hyperparameters pre-train model,0.7566714286804199
translation,243,140,hyperparameters,dropout strategy,with,rate 0.1,dropout strategy with rate 0.1,0.6138949990272522
translation,243,140,hyperparameters,dropout strategy,to enhance,robustness,dropout strategy to enhance robustness,0.726901113986969
translation,243,140,hyperparameters,rate 0.1,to enhance,robustness,rate 0.1 to enhance robustness,0.676772952079773
translation,243,140,hyperparameters,robustness,of,our model,robustness of our model,0.5817158222198486
translation,243,140,hyperparameters,hyperparameters,adopt,dropout strategy,hyperparameters adopt dropout strategy,0.6369616985321045
translation,243,141,hyperparameters,fine- tuning,fine - tune,model,fine- tuning fine - tune model,0.8096703290939331
translation,243,141,hyperparameters,model,keep,other settings,model keep other settings,0.7062657475471497
translation,243,141,hyperparameters,model,reduce,batch size,model reduce batch size,0.7242517471313477
translation,243,141,hyperparameters,model,fine - tune,model,model fine - tune model,0.7036628127098083
translation,243,141,hyperparameters,model,with,early - stopping strategy,model with early - stopping strategy,0.6200785636901855
translation,243,141,hyperparameters,other settings,consistent with,pre-training stage,other settings consistent with pre-training stage,0.6478501558303833
translation,243,141,hyperparameters,batch size,to,2048 tokens,batch size to 2048 tokens,0.5460720062255859
translation,243,141,hyperparameters,model,with,early - stopping strategy,model with early - stopping strategy,0.6200785636901855
translation,243,141,hyperparameters,fine- tuning,has,model,fine- tuning has model,0.5608916282653809
translation,243,141,hyperparameters,hyperparameters,When,fine- tuning,hyperparameters When fine- tuning,0.6249778866767883
translation,243,141,hyperparameters,hyperparameters,reduce,batch size,hyperparameters reduce batch size,0.6488391757011414
translation,243,141,hyperparameters,hyperparameters,fine - tune,model,hyperparameters fine - tune model,0.7390584349632263
translation,243,7,model,novel framework,called,user-driven nmt,novel framework called user-driven nmt,0.6602481603622437
translation,243,7,model,model,introduce,novel framework,model introduce novel framework,0.6889747381210327
translation,243,8,model,user-driven contrastive learning method,to offer,nmt,user-driven contrastive learning method to offer nmt,0.6110482811927795
translation,243,8,model,potential user traits,from,historical inputs,potential user traits from historical inputs,0.5298389196395874
translation,243,8,model,potential user traits,under,zero-shot learning fashion,potential user traits under zero-shot learning fashion,0.6643198132514954
translation,243,8,model,historical inputs,under,zero-shot learning fashion,historical inputs under zero-shot learning fashion,0.6568057537078857
translation,243,26,model,userdriven nmt,that generates,personalized translations,userdriven nmt that generates personalized translations,0.6514034867286682
translation,243,26,model,personalized translations,for,users,personalized translations for users,0.5993013381958008
translation,243,26,model,users,unseen in,training dataset,users unseen in training dataset,0.5455483794212341
translation,243,26,model,model,explore,userdriven nmt,model explore userdriven nmt,0.7111220359802246
translation,243,29,model,novel framework,where,nmt model,novel framework where nmt model,0.6211605072021484
translation,243,29,model,nmt model,equipped with,cache module,nmt model equipped with cache module,0.6691887378692627
translation,243,29,model,cache module,to restore and update,historical inputs,cache module to restore and update historical inputs,0.6734323501586914
translation,243,29,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,243,30,model,regularization framework,based on,contrastive learning,regularization framework based on contrastive learning,0.5868405103683472
translation,243,30,model,divergence,between,translations,divergence between translations,0.7313263416290283
translation,243,30,model,divergence,increasing,diversity,divergence increasing diversity,0.7186671495437622
translation,243,30,model,translations,of,similar users,translations of similar users,0.585456371307373
translation,243,30,model,diversity,on,dissimilar users,diversity on dissimilar users,0.5894927978515625
translation,243,30,model,model,design,regularization framework,model design regularization framework,0.5836323499679565
translation,243,129,model,user-driven nmt model,based on,open-nmt transformer,user-driven nmt model based on open-nmt transformer,0.6799340844154358
translation,243,129,model,user-driven nmt model,adopt,two -stage strategy,user-driven nmt model adopt two -stage strategy,0.6723640561103821
translation,243,129,model,two -stage strategy,pre-train,transformer - based nmt model,two -stage strategy pre-train transformer - based nmt model,0.5598315596580505
translation,243,129,model,transformer - based nmt model,on,wmt2017 chinese - to - english dataset,transformer - based nmt model on wmt2017 chinese - to - english dataset,0.5092858076095581
translation,243,129,model,model,to,our user-driven nmt model,model to our user-driven nmt model,0.5384669899940491
translation,243,129,model,our user-driven nmt model,using,udt - corpus,our user-driven nmt model using udt - corpus,0.632014274597168
translation,243,129,model,model,develop,user-driven nmt model,model develop user-driven nmt model,0.6340076923370361
translation,243,129,model,model,adopt,two -stage strategy,model adopt two -stage strategy,0.684088408946991
translation,243,129,model,model,pre-train,transformer - based nmt model,model pre-train transformer - based nmt model,0.6651849150657654
translation,243,167,results,consistently outperforms,in terms of,two metrics,consistently outperforms in terms of two metrics,0.7065829634666443
translation,243,167,results,all baselines,in terms of,two metrics,all baselines in terms of two metrics,0.6185371279716492
translation,243,167,results,ud - nmt model,has,consistently outperforms,ud - nmt model has consistently outperforms,0.611453115940094
translation,243,167,results,consistently outperforms,has,all baselines,consistently outperforms has all baselines,0.5969254374504089
translation,243,167,results,results,observe,ud - nmt model,results observe ud - nmt model,0.5794486403465271
translation,243,172,results,ud - nmt,exhibits better than,tf - ft + pesu-data,ud - nmt exhibits better than tf - ft + pesu-data,0.7722676992416382
translation,243,172,results,results,has,ud - nmt,results has ud - nmt,0.5527085661888123
translation,243,174,results,better performance,than,tf - ft + user-bias,better performance than tf - ft + user-bias,0.5824647545814514
translation,243,174,results,tf - ft + user-bias,without introducing,extra parameters,tf - ft + user-bias without introducing extra parameters,0.7360033988952637
translation,243,216,results,hint,of,keywords,hint of keywords,0.5674194097518921
translation,243,216,results,keywords,in,historical inputs,keywords in historical inputs,0.452383816242218
translation,243,216,results,our ud - nmt,able to produce,suitable translation,our ud - nmt able to produce suitable translation,0.7081139087677002
translation,243,216,results,suitable translation,consistent with,topic preference,suitable translation consistent with topic preference,0.5956459641456604
translation,243,216,results,hint,has,our ud - nmt,hint has our ud - nmt,0.6806139945983887
translation,243,216,results,keywords,has,our ud - nmt,keywords has our ud - nmt,0.5592433214187622
translation,243,216,results,historical inputs,has,our ud - nmt,historical inputs has our ud - nmt,0.6061903238296509
translation,243,216,results,results,with,hint,results with hint,0.5471135377883911
translation,243,219,results,our model,generate,translations,our model generate translations,0.6860012412071228
translation,243,219,results,translations,more in line with,history inputs,translations more in line with history inputs,0.6256003975868225
translation,243,219,results,history inputs,than,baseline models,history inputs than baseline models,0.5444793701171875
translation,243,219,results,baseline models,in,most cases,baseline models in most cases,0.5435512661933899
translation,243,219,results,results,show,our model,results show our model,0.6888449192047119
translation,244,124,ablation-analysis,effective synchronous constrain,differed between,"syntactically close , i.e. , german-english","effective synchronous constrain differed between syntactically close , i.e. , german-english",0.6877045631408691
translation,244,124,ablation-analysis,ablation analysis,found that,effective synchronous constrain,ablation analysis found that effective synchronous constrain,0.6488169431686401
translation,244,126,ablation-analysis,quality,of,latent phrase structures,quality of latent phrase structures,0.5796400308609009
translation,244,126,ablation-analysis,synchronous constraint,has,hurt,synchronous constraint has hurt,0.6162426471710205
translation,244,126,ablation-analysis,hurt,has,quality,hurt has quality,0.5995983481407166
translation,244,126,ablation-analysis,ablation analysis,show,synchronous constraint,ablation analysis show synchronous constraint,0.6425104737281799
translation,244,142,ablation-analysis,aer,indicates,synchronous constrain,aer indicates synchronous constrain,0.7421488165855408
translation,244,142,ablation-analysis,synchronizing,has,source and target latent phrase structure,synchronizing has source and target latent phrase structure,0.5238240957260132
translation,244,142,ablation-analysis,source and target latent phrase structure,has,decreases,source and target latent phrase structure has decreases,0.574068009853363
translation,244,142,ablation-analysis,decreases,has,aer,decreases has aer,0.6774176955223083
translation,244,142,ablation-analysis,ablation analysis,has,synchronizing,ablation analysis has synchronizing,0.5638697147369385
translation,244,143,ablation-analysis,synchronous constrain,by,rank loss,synchronous constrain by rank loss,0.5526431798934937
translation,244,143,ablation-analysis,rank loss,resulted in,deterioration,rank loss resulted in deterioration,0.6893168091773987
translation,244,143,ablation-analysis,deterioration,in,aer,deterioration in aer,0.6181055307388306
translation,244,143,ablation-analysis,improving,has,translation performance bleu,improving has translation performance bleu,0.5609648823738098
translation,244,143,ablation-analysis,ablation analysis,has,synchronous constrain,ablation analysis has synchronous constrain,0.5749714374542236
translation,244,146,ablation-analysis,synchronous constrain,by,mse,synchronous constrain by mse,0.6007814407348633
translation,244,146,ablation-analysis,synchronous constrain,by,rank loss,synchronous constrain by rank loss,0.5526431798934937
translation,244,146,ablation-analysis,synchronous constrain,by,rank loss,synchronous constrain by rank loss,0.5526431798934937
translation,244,146,ablation-analysis,mse,improvement of,aer,mse improvement of aer,0.6895136833190918
translation,244,146,ablation-analysis,synchronous constrain,by,rank loss,synchronous constrain by rank loss,0.5526431798934937
translation,244,146,ablation-analysis,penultimate layer,has,synchronous constrain,penultimate layer has synchronous constrain,0.6225508451461792
translation,244,146,ablation-analysis,worsened,has,aer,worsened has aer,0.6407555341720581
translation,244,146,ablation-analysis,ablation analysis,In,penultimate layer,ablation analysis In penultimate layer,0.5442580580711365
translation,244,147,ablation-analysis,rank loss,resulted in,significant improvement,rank loss resulted in significant improvement,0.6549608707427979
translation,244,147,ablation-analysis,significant improvement,in,third and fourth layers,significant improvement in third and fourth layers,0.5500550866127014
translation,244,147,ablation-analysis,aer,in,third and fourth layers,aer in third and fourth layers,0.5977011919021606
translation,244,147,ablation-analysis,significant improvement,has,aer,significant improvement has aer,0.6043412089347839
translation,244,147,ablation-analysis,ablation analysis,has,rank loss,ablation analysis has rank loss,0.5196579694747925
translation,244,148,ablation-analysis,synchronous constraints,by,mse,synchronous constraints by mse,0.5996212959289551
translation,244,148,ablation-analysis,rank loss,result in,worse aer,rank loss result in worse aer,0.681808590888977
translation,244,148,ablation-analysis,ablation analysis,In,final layer,ablation analysis In final layer,0.5386143922805786
translation,244,157,ablation-analysis,transformer,with,local attention,transformer with local attention,0.6571990847587585
translation,244,157,ablation-analysis,reduced,by,0.93 bleu point,reduced by 0.93 bleu point,0.5489547848701477
translation,244,157,ablation-analysis,reduced,by,0.61 bleu point,reduced by 0.61 bleu point,0.5489366054534912
translation,244,157,ablation-analysis,0.93 bleu point,in,iwslt '14 german-english,0.93 bleu point in iwslt '14 german-english,0.5100449323654175
translation,244,157,ablation-analysis,0.61 bleu point,in,aspec japanese - english,0.61 bleu point in aspec japanese - english,0.4680297374725342
translation,244,157,ablation-analysis,0.61 bleu point,without,position embedding,0.61 bleu point without position embedding,0.6385114789009094
translation,244,157,ablation-analysis,transformer,has,performance,transformer has performance,0.5373453497886658
translation,244,157,ablation-analysis,local attention,has,performance,local attention has performance,0.5532189011573792
translation,244,157,ablation-analysis,ablation analysis,in,transformer,ablation analysis in transformer,0.5493102073669434
translation,244,93,experimental-setup,transformer iwslt de en align fairseq configuration,for,german- english dataset,transformer iwslt de en align fairseq configuration for german- english dataset,0.5928522944450378
translation,244,93,experimental-setup,transformer align fairseq configuration,for,japanese - english dataset,transformer align fairseq configuration for japanese - english dataset,0.5598596930503845
translation,244,93,experimental-setup,experimental setup,employ,transformer iwslt de en align fairseq configuration,experimental setup employ transformer iwslt de en align fairseq configuration,0.5773330926895142
translation,244,98,experiments,phrase structures,for,mt tasks,phrase structures for mt tasks,0.5538012385368347
translation,244,98,experiments,mt tasks,on,iwslt,mt tasks on iwslt,0.5524161458015442
translation,244,7,model,two methods,to explicitly incorporate,grammatical information,two methods to explicitly incorporate grammatical information,0.7171667218208313
translation,244,7,model,grammatical information,without supervising,annotation,grammatical information without supervising annotation,0.7456929683685303
translation,244,7,model,latent phrase structure,induced in,unsupervised fashion,latent phrase structure induced in unsupervised fashion,0.6379234194755554
translation,244,7,model,unsupervised fashion,from,multi-head attention mechanism,unsupervised fashion from multi-head attention mechanism,0.551954448223114
translation,244,7,model,induced phrase structures,in,encoder and decoder,induced phrase structures in encoder and decoder,0.5152881741523743
translation,244,7,model,synchronized,compatible with,each other,synchronized compatible with each other,0.7055586576461792
translation,244,7,model,each other,using,constraints,each other using constraints,0.6836586594581604
translation,244,7,model,constraints,during,training,constraints during training,0.7287390232086182
translation,244,7,model,model,propose,two methods,model propose two methods,0.663135290145874
translation,244,20,model,approach,to incorporate,phrase structure explicitly into transformer,approach to incorporate phrase structure explicitly into transformer,0.7080626487731934
translation,244,21,model,latent phrase structures,induced in,unsupervised fashion,latent phrase structures induced in unsupervised fashion,0.6204215288162231
translation,244,21,model,unsupervised fashion,for,source and target sides,unsupervised fashion for source and target sides,0.6507470607757568
translation,244,21,model,two induced latent phrase structures,synchronously agreed with,each other,two induced latent phrase structures synchronously agreed with each other,0.7535300254821777
translation,244,21,model,each other,through,attention mechanism,each other through attention mechanism,0.6866697072982788
translation,244,94,model,two mha layers,from,bottom,two mha layers from bottom,0.603177011013031
translation,244,94,model,two mha layers,from,top,two mha layers from top,0.6165396571159363
translation,244,94,model,bottom,to induct,phrase structures,bottom to induct phrase structures,0.6735340356826782
translation,244,94,model,two encoder-decoder mha layers,from,top,two encoder-decoder mha layers from top,0.601661741733551
translation,244,94,model,two encoder-decoder mha layers,to synchronize,encoder and decoder syntactic distances,two encoder-decoder mha layers to synchronize encoder and decoder syntactic distances,0.7475879788398743
translation,244,94,model,model,use,two mha layers,model use two mha layers,0.6752814650535583
translation,244,94,model,model,use,two encoder-decoder mha layers,model use two encoder-decoder mha layers,0.6340193152427673
translation,244,8,results,better performance and explainability,in,two tasks,better performance and explainability in two tasks,0.47096842527389526
translation,244,8,results,better performance and explainability,in,translation,better performance and explainability in translation,0.5241183638572693
translation,244,8,results,better performance and explainability,in,alignment tasks,better performance and explainability in alignment tasks,0.5065072774887085
translation,244,8,results,better performance and explainability,without,extra resources,better performance and explainability without extra resources,0.6998090147972107
translation,244,141,results,model,with,latent phrase structure,model with latent phrase structure,0.5929293036460876
translation,244,141,results,model,show,better translation performance,model show better translation performance,0.6539542078971863
translation,244,141,results,latent phrase structure,show,quality of alignments,latent phrase structure show quality of alignments,0.6051297783851624
translation,244,141,results,transformer,has,model,transformer has model,0.537415623664856
translation,244,141,results,results,Compared with,transformer,results Compared with transformer,0.566961944103241
translation,245,32,ablation-analysis,various factors,exert,salient effects,various factors exert salient effects,0.6865437626838684
translation,245,32,ablation-analysis,salient effects,on,model 's ability,salient effects on model 's ability,0.5319017171859741
translation,245,32,ablation-analysis,model 's ability,of,compositional generalization,model 's ability of compositional generalization,0.5823851823806763
translation,245,32,ablation-analysis,compositional generalization,such as,compound frequency,compositional generalization such as compound frequency,0.5896172523498535
translation,245,32,ablation-analysis,compositional generalization,such as,compound length,compositional generalization such as compound length,0.5921654105186462
translation,245,32,ablation-analysis,compositional generalization,such as,atom co-occurrence,compositional generalization such as atom co-occurrence,0.5740514993667603
translation,245,32,ablation-analysis,compositional generalization,such as,linguistic patterns,compositional generalization such as linguistic patterns,0.5537698268890381
translation,245,32,ablation-analysis,compositional generalization,such as,context complexity,compositional generalization such as context complexity,0.5610276460647583
translation,245,32,ablation-analysis,ablation analysis,observe,various factors,ablation analysis observe various factors,0.5837118625640869
translation,245,178,ablation-analysis,error rate,of,pp compounds,error rate of pp compounds,0.6064144968986511
translation,245,178,ablation-analysis,much higher,than,other two,much higher than other two,0.6819706559181213
translation,245,178,ablation-analysis,error rate,has,37.72 %,error rate has 37.72 %,0.5529891848564148
translation,245,178,ablation-analysis,ablation analysis,observe that,error rate,ablation analysis observe that error rate,0.5562430024147034
translation,245,184,ablation-analysis,translation error rate,increases to,5.00 %,translation error rate increases to 5.00 %,0.6686186194419861
translation,245,184,ablation-analysis,much lower,than,zero-shot compounds,much lower than zero-shot compounds,0.6257910132408142
translation,245,184,ablation-analysis,much lower,with,error rate,much lower with error rate,0.6626020669937134
translation,245,184,ablation-analysis,zero-shot compounds,with,error rate,zero-shot compounds with error rate,0.6629971861839294
translation,245,184,ablation-analysis,error rate,of,27.53 %,error rate of 27.53 %,0.563829779624939
translation,245,184,ablation-analysis,few-shot compounds,has,translation error rate,few-shot compounds has translation error rate,0.5123652815818787
translation,245,184,ablation-analysis,ablation analysis,For,few-shot compounds,ablation analysis For few-shot compounds,0.6341406106948853
translation,245,213,ablation-analysis,mod atom,exerts,salient influence,mod atom exerts salient influence,0.7582007050514221
translation,245,213,ablation-analysis,salient influence,on,translation error rate,salient influence on translation error rate,0.5325415134429932
translation,245,213,ablation-analysis,ablation analysis,has,mod atom,ablation analysis has mod atom,0.5951067209243774
translation,245,214,ablation-analysis,error rate,of,compounds with mod,error rate of compounds with mod,0.6348497867584229
translation,245,214,ablation-analysis,compounds with mod,is,19.78 % higher,compounds with mod is 19.78 % higher,0.5976597666740417
translation,245,214,ablation-analysis,ablation analysis,has,error rate,ablation analysis has error rate,0.5103930234909058
translation,245,215,ablation-analysis,adj,into,compounds,adj into compounds,0.626638650894165
translation,245,215,ablation-analysis,adj,increases,error rate,adj increases error rate,0.6946461796760559
translation,245,215,ablation-analysis,error rate,by,2.66 %,error rate by 2.66 %,0.5644636154174805
translation,245,215,ablation-analysis,ablation analysis,adding,adj,ablation analysis adding adj,0.6837277412414551
translation,245,6,baselines,nmt models,from the perspective of,compositional generalization,nmt models from the perspective of compositional generalization,0.6295823454856873
translation,245,6,baselines,compositional generalization,by building,benchmark dataset,compositional generalization by building benchmark dataset,0.6467720866203308
translation,245,6,baselines,baselines,study,nmt models,baselines study nmt models,0.576885461807251
translation,245,148,experimental-setup,english side,using,moses tokenizer,english side using moses tokenizer,0.6810068488121033
translation,245,148,experimental-setup,experimental setup,tokenize,english side,experimental setup tokenize english side,0.7131171226501465
translation,245,150,experimental-setup,bpe,with,"3,000 merge operations","bpe with 3,000 merge operations",0.6717459559440613
translation,245,150,experimental-setup,bpe,generating,vocabulary,bpe generating vocabulary,0.7672560811042786
translation,245,150,experimental-setup,vocabulary,of,"5,500 subwords","vocabulary of 5,500 subwords",0.5864929556846619
translation,245,150,experimental-setup,experimental setup,employ,bpe,experimental setup employ bpe,0.5994799733161926
translation,245,155,experimental-setup,model parameters,optimized by,"adam ( kingma and ba , 2015 )","model parameters optimized by adam ( kingma and ba , 2015 )",0.6764065027236938
translation,245,155,experimental-setup,"adam ( kingma and ba , 2015 )",with,"1 = 0.1 , 2 = 0.98 and ? = 10 9","adam ( kingma and ba , 2015 ) with 1 = 0.1 , 2 = 0.98 and ? = 10 9",0.6414997577667236
translation,245,155,experimental-setup,experimental setup,has,model parameters,experimental setup has model parameters,0.4974170923233032
translation,245,149,experiments,chinese sentences,segmented by,jieba segmenter,chinese sentences segmented by jieba segmenter,0.7492554783821106
translation,245,153,model,6 - layer decoder,with,hidden size 512,6 - layer decoder with hidden size 512,0.6464024782180786
translation,245,153,model,model,consists of,6 - layer encoder,model consists of 6 - layer encoder,0.6682887077331543
translation,245,153,model,model,consists of,6 - layer decoder,model consists of 6 - layer decoder,0.658859372138977
translation,245,154,model,input and output embeddings,on,target side,input and output embeddings on target side,0.5688717365264893
translation,245,154,model,model,tie,input and output embeddings,model tie input and output embeddings,0.5357275009155273
translation,245,166,results,model,achieves,69.58 bleu score,model achieves 69.58 bleu score,0.6206379532814026
translation,245,166,results,69.58 bleu score,on,random test set,69.58 bleu score on random test set,0.4912881851196289
translation,245,166,results,results,has,model,results has model,0.5339115858078003
translation,245,208,results,error rates,with,positive mean pmi scores,error rates with positive mean pmi scores,0.6135882139205933
translation,245,208,results,positive mean pmi scores,lower than,negative ones,positive mean pmi scores lower than negative ones,0.7013739943504333
translation,245,208,results,all groups,has,error rates,all groups has error rates,0.5435503721237183
translation,245,234,results,increases stably,with,context length,increases stably with context length,0.6689842939376831
translation,245,234,results,context length,as well as,dependency distance,context length as well as dependency distance,0.5361761450767517
translation,245,234,results,translation error rate,has,increases stably,translation error rate has increases stably,0.608839750289917
translation,245,234,results,results,has,translation error rate,results has translation error rate,0.521835207939148
translation,246,4,experiments,wmt21,has,shared news translation,wmt21 has shared news translation,0.6148349046707153
translation,246,45,hyperparameters,64 k joint vocabulary,constructed using,"sentencepiece ( kudo and richardson , 2018 )","64 k joint vocabulary constructed using sentencepiece ( kudo and richardson , 2018 )",0.6484909653663635
translation,246,45,hyperparameters,64 k joint vocabulary,from,subset,64 k joint vocabulary from subset,0.6065809726715088
translation,246,45,hyperparameters,subset,of,monolingual data,subset of monolingual data,0.5652871131896973
translation,246,45,hyperparameters,monolingual data,of,each language,monolingual data of each language,0.5569887757301331
translation,246,45,hyperparameters,hyperparameters,uses,64 k joint vocabulary,hyperparameters uses 64 k joint vocabulary,0.5330842733383179
translation,246,7,model,data,using,back translation,data using back translation,0.7144421339035034
translation,246,7,model,model,augment,data,model augment data,0.6968998908996582
translation,246,8,model,bilingual model,incorporating,back translation,bilingual model incorporating back translation,0.7261335253715515
translation,246,8,model,bilingual model,incorporating,knowledge distillation,bilingual model incorporating knowledge distillation,0.7080562114715576
translation,246,8,model,bilingual model,combine,two models,bilingual model combine two models,0.6905596852302551
translation,246,8,model,two models,using,sequence - to-sequence mapping,two models using sequence - to-sequence mapping,0.681456446647644
translation,246,8,model,model,train,bilingual model,model train bilingual model,0.7171711921691895
translation,246,37,model,transformer base architecture,comprising,6 encoder and 6 decoder layers,transformer base architecture comprising 6 encoder and 6 decoder layers,0.6899023652076721
translation,246,37,model,24 k joint vocabulary,built for,bengali ? hindi,24 k joint vocabulary built for bengali ? hindi,0.7067176699638367
translation,246,37,model,8 k joint vocabulary,built for,english ? hausa,8 k joint vocabulary built for english ? hausa,0.7461556196212769
translation,246,37,model,4 k joint vocabulary,built for,xhosa ? zulu,4 k joint vocabulary built for xhosa ? zulu,0.7237077355384827
translation,246,37,model,4 k joint vocabulary,to learn,subword units,4 k joint vocabulary to learn subword units,0.6015531420707703
translation,246,37,model,xhosa ? zulu,using,sentencepiece,xhosa ? zulu using sentencepiece,0.6651824116706848
translation,246,37,model,sentencepiece,to learn,subword units,sentencepiece to learn subword units,0.6032606959342957
translation,246,37,model,subword units,to tokenize,sentences,subword units to tokenize sentences,0.7115015983581543
translation,246,37,model,model,use,transformer base architecture,model use transformer base architecture,0.6350939273834229
translation,246,46,model,transformer model,has,12 encoder and 6 decoder layers,transformer model has 12 encoder and 6 decoder layers,0.5469233989715576
translation,246,46,model,model,has,transformer model,model has transformer model,0.5662795305252075
translation,246,47,model,multitask objective,used during,training,multitask objective used during training,0.6700664758682251
translation,246,47,model,training,to make use of,monolingual data,training to make use of monolingual data,0.6454936861991882
translation,246,48,model,usual parallel data likelihood,referred to,mt,usual parallel data likelihood referred to mt,0.6365525722503662
translation,246,48,model,usual parallel data likelihood,referred to,masked language model ( mlm ),usual parallel data likelihood referred to masked language model ( mlm ),0.6105408668518066
translation,246,48,model,usual parallel data likelihood,referred to,denoising auto-encoder ( dae ),usual parallel data likelihood referred to denoising auto-encoder ( dae ),0.6099696159362793
translation,246,48,model,usual parallel data likelihood,at,decoder side,usual parallel data likelihood at decoder side,0.5478098392486572
translation,246,48,model,masked language model ( mlm ),at,encoder,masked language model ( mlm ) at encoder,0.5466110706329346
translation,246,48,model,denoising auto-encoder ( dae ),at,decoder side,denoising auto-encoder ( dae ) at decoder side,0.5393859148025513
translation,246,48,model,mt,has,masked language model ( mlm ),mt has masked language model ( mlm ),0.5770477652549744
translation,246,108,model,data,using,back translation,data using back translation,0.7144421339035034
translation,246,108,model,model,augment,data,model augment data,0.6968998908996582
translation,246,95,results,monolingual objective,brings,nice improvements,monolingual objective brings nice improvements,0.5909061431884766
translation,246,95,results,nice improvements,for,all language pairs,nice improvements for all language pairs,0.5817025303840637
translation,246,95,results,results,adding,monolingual objective,results adding monolingual objective,0.6300035715103149
translation,246,96,results,m t + dae and m t + m lm + dae,perform,closely,m t + dae and m t + m lm + dae perform closely,0.640981137752533
translation,246,96,results,closely,for,all language pairs,closely for all language pairs,0.6562016606330872
translation,246,96,results,results,has,m t + dae and m t + m lm + dae,results has m t + dae and m t + m lm + dae,0.5386380553245544
translation,246,98,results,back translation,brings,significant improvement,back translation brings significant improvement,0.6044495701789856
translation,246,98,results,significant improvement,to,all language pairs,significant improvement to all language pairs,0.5514259338378906
translation,246,98,results,results,adding,back translation,results adding back translation,0.6886022686958313
translation,246,99,results,multilingual model,to create,data,multilingual model to create data,0.6854906678199768
translation,246,99,results,data,for,bilingual model,data for bilingual model,0.6321929097175598
translation,246,99,results,excellent results,that outperform,multilingual model,excellent results that outperform multilingual model,0.6645534634590149
translation,246,99,results,results,using,multilingual model,results using multilingual model,0.6254453063011169
translation,246,100,results,ensemble,performs,better,ensemble performs better,0.6546496748924255
translation,246,100,results,better,than,individual models,better than individual models,0.5955447554588318
translation,246,100,results,results,has,ensemble,results has ensemble,0.5487152338027954
translation,247,8,ablation-analysis,static embeddings,freezing,embedding layer weights,static embeddings freezing embedding layer weights,0.656668484210968
translation,247,8,ablation-analysis,static embeddings,freezing,embedding layer weights,static embeddings freezing embedding layer weights,0.656668484210968
translation,247,8,ablation-analysis,static embeddings,lead to,better gains,static embeddings lead to better gains,0.6505720615386963
translation,247,8,ablation-analysis,better gains,compared to updating,embedding layer weights,better gains compared to updating embedding layer weights,0.7423957586288452
translation,247,8,ablation-analysis,embedding layer weights,during,training,embedding layer weights during training,0.6598848700523376
translation,247,8,ablation-analysis,ablation analysis,has,static embeddings,ablation analysis has static embeddings,0.5060378909111023
translation,247,112,ablation-analysis,embedding layer,with,static cross-lingual embedding,embedding layer with static cross-lingual embedding,0.6162163019180298
translation,247,112,ablation-analysis,static cross-lingual embedding,helps,mass - based and dae - based unmt systems,static cross-lingual embedding helps mass - based and dae - based unmt systems,0.5833792090415955
translation,247,112,ablation-analysis,static cross-lingual embedding,both,mass - based and dae - based unmt systems,static cross-lingual embedding both mass - based and dae - based unmt systems,0.6712366342544556
translation,247,112,ablation-analysis,mass - based and dae - based unmt systems,to learn,better translations,mass - based and dae - based unmt systems to learn better translations,0.6593606472015381
translation,247,112,ablation-analysis,ablation analysis,Initialising,embedding layer,ablation analysis Initialising embedding layer,0.6801735162734985
translation,247,21,baselines,two existing unmt approaches,namely,masked sequence- to- sequence ( mass ),two existing unmt approaches namely masked sequence- to- sequence ( mass ),0.6938238739967346
translation,247,21,baselines,two existing unmt approaches,namely,variation of denoising auto-encoder ( dae ) based unmt approach,two existing unmt approaches namely variation of denoising auto-encoder ( dae ) based unmt approach,0.67902010679245
translation,247,21,baselines,variation of denoising auto-encoder ( dae ) based unmt approach,for,english to indoaryan language pairs,variation of denoising auto-encoder ( dae ) based unmt approach for english to indoaryan language pairs,0.6077889800071716
translation,247,21,baselines,english to indoaryan language pairs,has,"i.e. english-hindi , english -bengali , english - gujarati","english to indoaryan language pairs has i.e. english-hindi , english -bengali , english - gujarati",0.5766608119010925
translation,247,21,baselines,baselines,experiment with,two existing unmt approaches,baselines experiment with two existing unmt approaches,0.6884317398071289
translation,247,70,experimental-setup,experimental setup,use,"vecmap ( artetxe et al. , 2018a )","experimental setup use vecmap ( artetxe et al. , 2018a )",0.5525140166282654
translation,247,9,experiments,denoising autoencoder ( dae ) unmt approaches,for,three distant language pairs,denoising autoencoder ( dae ) unmt approaches for three distant language pairs,0.5758113861083984
translation,247,74,experiments,models,using,8 approaches,models using 8 approaches,0.6718989610671997
translation,247,74,experiments,models,using,3 approaches,models using 3 approaches,0.6751347780227661
translation,247,74,experiments,8 approaches,for,all language - pair,8 approaches for all language - pair,0.5740295052528381
translation,247,74,experiments,3 approaches,use,dae,3 approaches use dae,0.5971571803092957
translation,247,74,experiments,3 approaches,use,mass,3 approaches use mass,0.6363950371742249
translation,247,74,experiments,dae,as,lm pretraining,dae as lm pretraining,0.5000022649765015
translation,247,74,experiments,dae,as,lm pretraining,dae as lm pretraining,0.5000022649765015
translation,247,74,experiments,dae,as,lm pretraining,dae as lm pretraining,0.5000022649765015
translation,247,74,experiments,3 approaches,use,mass,3 approaches use mass,0.6363950371742249
translation,247,74,experiments,mass,as,lm pretraining,mass as lm pretraining,0.5208452939987183
translation,247,74,experiments,other two,train,dae and bt simultaneously,other two train dae and bt simultaneously,0.7196761965751648
translation,247,88,experiments,bpe - segmented monolingual corpora,to independently train,embeddings,bpe - segmented monolingual corpora to independently train embeddings,0.683350682258606
translation,247,88,experiments,embeddings,for,each language,embeddings for each language,0.6194770932197571
translation,247,88,experiments,embeddings,using,skip-gram model,embeddings using skip-gram model,0.6299237608909607
translation,247,88,experiments,skip-gram model,of,fasttext,skip-gram model of fasttext,0.5524231195449829
translation,247,95,experiments,adam optimizer,with,beta 1,adam optimizer with beta 1,0.6178165078163147
translation,247,95,experiments,adam optimizer,with,beta 2,adam optimizer with beta 2,0.6091148257255554
translation,247,95,experiments,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,247,95,experiments,beta 1,set to,0.9,beta 1 set to 0.9,0.6736928820610046
translation,247,95,experiments,beta 2,to,0.98,beta 2 to 0.98,0.5161531567573547
translation,247,95,experiments,learning rate,to,0.0001,learning rate to 0.0001,0.5543551445007324
translation,247,86,hyperparameters,bpe segmentation,use,fastbpe,bpe segmentation use fastbpe,0.6060215830802917
translation,247,86,hyperparameters,fastbpe,on,source and target data,fastbpe on source and target data,0.5541489124298096
translation,247,86,hyperparameters,fastbpe,with,number of merge operations,fastbpe with number of merge operations,0.6171213984489441
translation,247,86,hyperparameters,number of merge operations,set to,100k,number of merge operations set to 100k,0.6996808648109436
translation,247,86,hyperparameters,hyperparameters,For,bpe segmentation,hyperparameters For bpe segmentation,0.5883038640022278
translation,247,93,hyperparameters,models,with,6 layer 8- headed transformer encoder-decoder architecture,models with 6 layer 8- headed transformer encoder-decoder architecture,0.6447172164916992
translation,247,93,hyperparameters,6 layer 8- headed transformer encoder-decoder architecture,dimension,1024,6 layer 8- headed transformer encoder-decoder architecture dimension 1024,0.6903694868087769
translation,247,93,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,247,94,hyperparameters,epoch size,of,0.2 m steps,epoch size of 0.2 m steps,0.5993672609329224
translation,247,94,hyperparameters,batch size,of,64 sentences,batch size of 64 sentences,0.5814248919487
translation,247,94,hyperparameters,hyperparameters,trained using,epoch size,hyperparameters trained using epoch size,0.6828019022941589
translation,247,99,hyperparameters,language code,at,encoder side,language code at encoder side,0.517347514629364
translation,247,99,hyperparameters,hyperparameters,drop,language code,hyperparameters drop language code,0.7062629461288452
translation,247,100,hyperparameters,mass pre-training,use,word-mass,mass pre-training use word-mass,0.6375841498374939
translation,247,100,hyperparameters,word-mass,of,0.5,word-mass of 0.5,0.6460742354393005
translation,247,100,hyperparameters,hyperparameters,For,mass pre-training,hyperparameters For mass pre-training,0.5697529315948486
translation,247,19,model,effect of initialising the embedding layer,with,cross-lingual embeddings,effect of initialising the embedding layer with cross-lingual embeddings,0.6297490000724792
translation,247,19,model,cross-lingual embeddings,for training,unmt systems,cross-lingual embeddings for training unmt systems,0.7204360961914062
translation,247,19,model,unmt systems,for,distant languages,unmt systems for distant languages,0.6152558326721191
translation,247,19,model,model,explore,effect of initialising the embedding layer,model explore effect of initialising the embedding layer,0.6573906540870667
translation,247,62,model,random n-grams,in,input,random n-grams in input,0.5182457566261292
translation,247,62,model,random n-grams,are,masked,random n-grams are masked,0.6311243772506714
translation,247,62,model,input,are,masked,input are masked,0.6393423080444336
translation,247,62,model,model,trained to generate,missing n-grams,model trained to generate missing n-grams,0.7512891292572021
translation,247,62,model,missing n-grams,in,pre-training phase,missing n-grams in pre-training phase,0.4995952546596527
translation,247,62,model,"mass ( song et al. , 2019 )",has,random n-grams,"mass ( song et al. , 2019 ) has random n-grams",0.5967693328857422
translation,247,62,model,"mass ( song et al. , 2019 )",has,model,"mass ( song et al. , 2019 ) has model",0.5962177515029907
translation,247,62,model,model,In,"mass ( song et al. , 2019 )","model In mass ( song et al. , 2019 )",0.49630439281463623
translation,247,62,model,model,trained to generate,missing n-grams,model trained to generate missing n-grams,0.7512891292572021
translation,247,7,results,embedding layer,of,unmt models,embedding layer of unmt models,0.5635720491409302
translation,247,7,results,embedding layer,shows,significant improvements,embedding layer shows significant improvements,0.6824573874473572
translation,247,7,results,unmt models,with,cross-lingual embeddings,unmt models with cross-lingual embeddings,0.5899415612220764
translation,247,7,results,significant improvements,in,bleu score,significant improvements in bleu score,0.5336595177650452
translation,247,7,results,bleu score,over,existing approaches,bleu score over existing approaches,0.5877646207809448
translation,247,7,results,existing approaches,with,embeddings randomly initialized,existing approaches with embeddings randomly initialized,0.6707484722137451
translation,247,7,results,results,initialising,embedding layer,results initialising embedding layer,0.7015885710716248
translation,247,20,results,effect,of,static cross-lingual embeddings,effect of static cross-lingual embeddings,0.5644453763961792
translation,247,20,results,static cross-lingual embeddings,v/s,non-static cross-lingual embeddings,static cross-lingual embeddings v/s non-static cross-lingual embeddings,0.6690351366996765
translation,247,20,results,results,explore,effect,results explore effect,0.5829496383666992
translation,247,113,results,freezing,results in,better translation quality,freezing results in better translation quality,0.6138951778411865
translation,247,113,results,cross-lingual embeddings ( static ),during,unmt training,cross-lingual embeddings ( static ) during unmt training,0.6490282416343689
translation,247,113,results,cross-lingual embeddings ( static ),results in,better translation quality,cross-lingual embeddings ( static ) results in better translation quality,0.5513647198677063
translation,247,113,results,better translation quality,compared to,approach,better translation quality compared to approach,0.6422812342643738
translation,247,113,results,approach,where,cross-lingual embeddings are updated ( non-static ),approach where cross-lingual embeddings are updated ( non-static ),0.5764262080192566
translation,247,113,results,freezing,has,cross-lingual embeddings ( static ),freezing has cross-lingual embeddings ( static ),0.6052485108375549
translation,247,113,results,results,suggest,freezing,results suggest freezing,0.5508835315704346
translation,247,113,results,results,suggest,cross-lingual embeddings ( static ),results suggest cross-lingual embeddings ( static ),0.5570465922355652
translation,247,114,results,bleu scores,suggest that,dae objective based models,bleu scores suggest that dae objective based models,0.5928011536598206
translation,247,114,results,dae objective based models,surpass,mass objective based models,dae objective based models surpass mass objective based models,0.7020015120506287
translation,247,114,results,results,has,bleu scores,results has bleu scores,0.5230661034584045
translation,247,128,results,embedding initialised models,observe,better convergence,embedding initialised models observe better convergence,0.6031758189201355
translation,247,128,results,better convergence,for,models,better convergence for models,0.6365259885787964
translation,247,128,results,better convergence,models where,embedding layers are frozen ( static ),better convergence models where embedding layers are frozen ( static ),0.7219125032424927
translation,247,128,results,embedding layers are frozen ( static ),than,models,embedding layers are frozen ( static ) than models,0.5606841444969177
translation,247,128,results,models,where,embedding layers are updated ( non-static ),models where embedding layers are updated ( non-static ),0.6337350010871887
translation,247,128,results,results,Among,embedding initialised models,results Among embedding initialised models,0.5800612568855286
translation,247,129,results,dae - unmt models,converge better than,mass - unmt models,dae - unmt models converge better than mass - unmt models,0.7670057415962219
translation,247,129,results,mass - unmt models,when initialized with,cross-lingual embeddings,mass - unmt models when initialized with cross-lingual embeddings,0.6504738926887512
translation,247,129,results,results,observe,dae - unmt models,results observe dae - unmt models,0.5706899166107178
translation,248,131,ablation-analysis,models,initially trained on,news task data,models initially trained on news task data,0.646097719669342
translation,248,131,ablation-analysis,models,seen,higher amount of biomedical data,models seen higher amount of biomedical data,0.6516228318214417
translation,248,131,ablation-analysis,performance gaps,between,models,performance gaps between models,0.685067892074585
translation,248,131,ablation-analysis,models,seen,higher amount of biomedical data,models seen higher amount of biomedical data,0.6516228318214417
translation,248,131,ablation-analysis,ablation analysis,In -domain finetuning,models,ablation analysis In -domain finetuning models,0.754551351070404
translation,248,47,experimental-setup,"bpe - dropout ( provilkov et al. , 2019 )",of,0.1,"bpe - dropout ( provilkov et al. , 2019 ) of 0.1",0.5739040374755859
translation,248,47,experimental-setup,0.1,for,both language pairs and tasks,0.1 for both language pairs and tasks,0.5896173715591431
translation,248,48,experimental-setup,model generated translations,to replace,quotes,model generated translations to replace quotes,0.7380945086479187
translation,248,48,experimental-setup,quotes,with,german equivalents,quotes with german equivalents,0.6136044859886169
translation,248,48,experimental-setup,experimental setup,post-process,model generated translations,experimental setup post-process model generated translations,0.7404693365097046
translation,248,108,experimental-setup,en ? de models,trained for,up to 450k updates,en ? de models trained for up to 450k updates,0.7751322388648987
translation,248,108,experimental-setup,up to 450k updates,using,adam optimizer,up to 450k updates using adam optimizer,0.6191930174827576
translation,248,108,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.98","adam optimizer with ? 1 = 0.9 , ? 2 = 0.98",0.6221903562545776
translation,248,108,experimental-setup,adam optimizer,with,inverse square root annealing,adam optimizer with inverse square root annealing,0.6133766174316406
translation,248,108,experimental-setup,inverse square root annealing,with,30 k warm - up steps,inverse square root annealing with 30 k warm - up steps,0.6631457209587097
translation,248,108,experimental-setup,", 2017 )",with,30 k warm - up steps,", 2017 ) with 30 k warm - up steps",0.6478860974311829
translation,248,108,experimental-setup,maximum learning rate,of,4e - 4,maximum learning rate of 4e - 4,0.6294503211975098
translation,248,108,experimental-setup,training & optimization,has,en ? de models,training & optimization has en ? de models,0.5947707891464233
translation,248,108,experimental-setup,experimental setup,has,training & optimization,experimental setup has training & optimization,0.5338456630706787
translation,248,109,experimental-setup,? ru models,trained for,up to 150k updates,? ru models trained for up to 150k updates,0.7812270522117615
translation,248,109,experimental-setup,up to 150k updates,with,7 k warmup steps,up to 150k updates with 7 k warmup steps,0.6567481756210327
translation,248,109,experimental-setup,en,has,? ru models,en has ? ru models,0.7111567258834839
translation,248,109,experimental-setup,experimental setup,has,en,experimental setup has en,0.5951910018920898
translation,248,109,experimental-setup,experimental setup,has,? ru models,experimental setup has ? ru models,0.5469332933425903
translation,248,110,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,248,110,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,248,110,experimental-setup,0.1,on,intermediate activations,0.1 on intermediate activations,0.541995108127594
translation,248,110,experimental-setup,0.1,to regularize,our models,0.1 to regularize our models,0.6969349980354309
translation,248,110,experimental-setup,intermediate activations,including,attention scores,intermediate activations including attention scores,0.7256014347076416
translation,248,111,experimental-setup,  large   models,trained on,nvidia dgx - 1 machines,  large   models trained on nvidia dgx - 1 machines,0.6757338047027588
translation,248,111,experimental-setup,nvidia dgx - 1 machines,with,8 32g v100 gpus,nvidia dgx - 1 machines with 8 32g v100 gpus,0.5883045792579651
translation,248,111,experimental-setup,experimental setup,trained on,nvidia dgx - 1 machines,experimental setup trained on nvidia dgx - 1 machines,0.7056642174720764
translation,248,111,experimental-setup,experimental setup,has,  large   models,experimental setup has   large   models,0.5377575159072876
translation,248,112,experimental-setup,batch size,of,16 k tokens,batch size of 16 k tokens,0.591965913772583
translation,248,112,experimental-setup,16 k tokens,per,gpu,16 k tokens per gpu,0.6318978667259216
translation,248,112,experimental-setup,16 k tokens,for,effective batch size,16 k tokens for effective batch size,0.5872660279273987
translation,248,112,experimental-setup,gpu,for,effective batch size,gpu for effective batch size,0.6004321575164795
translation,248,112,experimental-setup,effective batch size,of,128 k tokens,effective batch size of 128 k tokens,0.5921131372451782
translation,248,112,experimental-setup,experimental setup,use,batch size,experimental setup use batch size,0.6168987154960632
translation,248,113,experimental-setup,  xlarge   models,trained on,64 gpus,  xlarge   models trained on 64 gpus,0.6490802764892578
translation,248,113,experimental-setup,64 gpus,split across,4 nvidia dgx - 2 nodes,64 gpus split across 4 nvidia dgx - 2 nodes,0.6548112034797668
translation,248,113,experimental-setup,4 nvidia dgx - 2 nodes,with,16 32g v100 gpus each,4 nvidia dgx - 2 nodes with 16 32g v100 gpus each,0.6265585422515869
translation,248,113,experimental-setup,experimental setup,has,  xlarge   models,experimental setup has   xlarge   models,0.5397321581840515
translation,248,114,experimental-setup,effective batch size,of,256 k tokens,effective batch size of 256 k tokens,0.5949116945266724
translation,248,115,experimental-setup,  xxlarge   models,trained on,256 gpus,  xxlarge   models trained on 256 gpus,0.6477035880088806
translation,248,115,experimental-setup,256 gpus,across,16 dgx - 2 nodes,256 gpus across 16 dgx - 2 nodes,0.6253004670143127
translation,248,115,experimental-setup,256 gpus,with,effective batch size,256 gpus with effective batch size,0.6095598936080933
translation,248,115,experimental-setup,effective batch size,of,512 k tokens,effective batch size of 512 k tokens,0.5871685743331909
translation,248,115,experimental-setup,experimental setup,has,  xxlarge   models,experimental setup has   xxlarge   models,0.5159549713134766
translation,248,5,experiments,our news task submissions,for,english ? german ( en ? de ) and english ? russian ( en ? ru ),our news task submissions for english ? german ( en ? de ) and english ? russian ( en ? ru ),0.6150091290473938
translation,248,5,experiments,our news task submissions,built on top of,baseline transformer - based sequence - to-sequence model,our news task submissions built on top of baseline transformer - based sequence - to-sequence model,0.673046886920929
translation,248,5,experiments,english ? german ( en ? de ) and english ? russian ( en ? ru ),built on top of,baseline transformer - based sequence - to-sequence model,english ? german ( en ? de ) and english ? russian ( en ? ru ) built on top of baseline transformer - based sequence - to-sequence model,0.664890468120575
translation,248,21,experiments,parallel corpora,provided by,wmt,parallel corpora provided by wmt,0.6576473116874695
translation,248,93,experiments,ensemble 4 finetuned models,whose,base configurations,ensemble 4 finetuned models whose base configurations,0.6312160491943359
translation,248,93,experiments,base configurations,trained with,different mixed domain sampling ratios,base configurations trained with different mixed domain sampling ratios,0.7079235911369324
translation,248,93,experiments,en,has,? ru biomedical task,en has ? ru biomedical task,0.6530157923698425
translation,248,6,model,data augmentation,with,backtranslation,data augmentation with backtranslation,0.6050199866294861
translation,248,6,model,data augmentation,with,knowledge distillation,data augmentation with knowledge distillation,0.6200805902481079
translation,248,6,model,knowledge distillation,from,right - to- left factorized models,knowledge distillation from right - to- left factorized models,0.5449984669685364
translation,248,6,model,finetuning,on,test sets,finetuning on test sets,0.5258636474609375
translation,248,6,model,shallow fusion decoding,with,transformer language models,shallow fusion decoding with transformer language models,0.594840407371521
translation,248,8,results,our news system,achieves,sacre- bleu score,our news system achieves sacre- bleu score,0.6722310781478882
translation,248,8,results,sacre- bleu score,of,39.5,sacre- bleu score of 39.5,0.5476680397987366
translation,248,8,results,39.5,on,wmt '20 en ? de test set,39.5 on wmt '20 en ? de test set,0.5736868977546692
translation,248,8,results,outperforming,has,best submission,outperforming has best submission,0.6041030883789062
translation,248,8,results,results,has,our news system,results has our news system,0.5878683924674988
translation,248,9,results,biomedical task ru ? en and en ? ru systems,reach,bleu scores,biomedical task ru ? en and en ? ru systems reach bleu scores,0.6352426409721375
translation,248,9,results,bleu scores,of,43.8 and 40.3,bleu scores of 43.8 and 40.3,0.5647438168525696
translation,248,9,results,43.8 and 40.3,on,wmt '20 biomedical task,43.8 and 40.3 on wmt '20 biomedical task,0.5320801138877869
translation,248,9,results,results,has,biomedical task ru ? en and en ? ru systems,results has biomedical task ru ? en and en ? ru systems,0.5754223465919495
translation,248,16,results,small improvement,in,bleu scores,small improvement in bleu scores,0.5266016125679016
translation,248,16,results,small improvement,with,backtranslation results,small improvement with backtranslation results,0.612009584903717
translation,248,16,results,small improvement,being,mixed,small improvement being mixed,0.682904839515686
translation,248,16,results,bleu scores,with,backtranslation results,bleu scores with backtranslation results,0.5769088268280029
translation,248,16,results,backtranslation results,being,mixed,backtranslation results being mixed,0.6504397988319397
translation,248,18,results,en ? ru biomedical task submission,using,biomedical vocabulary,en ? ru biomedical task submission using biomedical vocabulary,0.6276323795318604
translation,248,18,results,from scratch,using,biomedical vocabulary,from scratch using biomedical vocabulary,0.6657370328903198
translation,248,18,results,similar model improvements,report,sacrebleu score,similar model improvements report sacrebleu score,0.6348792910575867
translation,248,18,results,sacrebleu score,of,40.3,sacrebleu score of 40.3,0.525458037853241
translation,248,18,results,sacrebleu score,of,43.8,sacrebleu score of 43.8,0.524396538734436
translation,248,18,results,40.3,on,en,40.3 on en,0.662365198135376
translation,248,18,results,43.8,on,ru,43.8 on ru,0.6082916855812073
translation,248,18,results,ru,on,wmt '20 biomedical shared task test dataset,ru on wmt '20 biomedical shared task test dataset,0.5051698088645935
translation,248,18,results,en ? ru biomedical task submission,has,from scratch,en ? ru biomedical task submission has from scratch,0.6069310307502747
translation,248,18,results,results,Training,en ? ru biomedical task submission,results Training en ? ru biomedical task submission,0.7291348576545715
translation,248,130,results,base models,on,biomedical domain data,base models on biomedical domain data,0.5217981934547424
translation,248,130,results,base models,improved,bleu scores,base models improved bleu scores,0.6349194049835205
translation,248,130,results,bleu scores,for,all models,bleu scores for all models,0.520829439163208
translation,248,130,results,up-sampling,has,in - domain unsurprisingly,up-sampling has in - domain unsurprisingly,0.5594648718833923
translation,248,130,results,up-sampling,has,finetuning,up-sampling has finetuning,0.5570499897003174
translation,248,130,results,finetuning,has,base models,finetuning has base models,0.5827045440673828
translation,248,130,results,results,found that,up-sampling,results found that up-sampling,0.6924034953117371
translation,248,132,results,noisy channel re-ranking,improved,model performance,noisy channel re-ranking improved model performance,0.7026217579841614
translation,248,132,results,model performance,after,ensembling,model performance after ensembling,0.6928806900978088
translation,248,132,results,ensembling,for,ru ? en direction,ensembling for ru ? en direction,0.6961644887924194
translation,248,132,results,results,has,shallow fusion,results has shallow fusion,0.5817294120788574
translation,248,133,results,both techniques,has,individually improved,both techniques has individually improved,0.5822353363037109
translation,248,133,results,individually improved,has,en ? ru performance,individually improved has en ? ru performance,0.5946096777915955
translation,248,133,results,results,has,both techniques,results has both techniques,0.4939937889575958
translation,248,134,results,ensembling,led to,additional performance boost,ensembling led to additional performance boost,0.6438761949539185
translation,248,134,results,our models,led to,additional performance boost,our models led to additional performance boost,0.6365178227424622
translation,248,134,results,our models,allowed us to reach,maximum en ? ru bleu score,our models allowed us to reach maximum en ? ru bleu score,0.5785018801689148
translation,248,134,results,our models,allowed us to reach,ru ? en bleu score,our models allowed us to reach ru ? en bleu score,0.5631563067436218
translation,248,134,results,maximum en ? ru bleu score,of,40.3,maximum en ? ru bleu score of 40.3,0.5692511200904846
translation,248,134,results,ru ? en bleu score,of,43.8,ru ? en bleu score of 43.8,0.5625377297401428
translation,248,134,results,ensembling,has,our models,ensembling has our models,0.5824901461601257
translation,248,134,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,249,53,baselines,three different model architectures,with,transformer,three different model architectures with transformer,0.6415722370147705
translation,249,34,experimental-setup,english text,with,moses,english text with moses,0.5872012972831726
translation,249,34,experimental-setup,english text,with,jieba,english text with jieba,0.638692319393158
translation,249,34,experimental-setup,english text,with,tokenizer,english text with tokenizer,0.6277088522911072
translation,249,34,experimental-setup,chinese text,with,jieba,chinese text with jieba,0.6060941219329834
translation,249,34,experimental-setup,chinese text,with,tokenizer,chinese text with tokenizer,0.5931165218353271
translation,249,34,experimental-setup,jieba,has,tokenizer,jieba has tokenizer,0.5860561728477478
translation,249,35,experimental-setup,"joint source and target bpe vocab ( sennrich et al. , 2016 )",with,40 k merge operations,"joint source and target bpe vocab ( sennrich et al. , 2016 ) with 40 k merge operations",0.6208051443099976
translation,249,35,experimental-setup,40 k merge operations,using,filtered bilingual dataset,40 k merge operations using filtered bilingual dataset,0.6556933522224426
translation,249,35,experimental-setup,vocabulary,with,size,vocabulary with size,0.6556599140167236
translation,249,35,experimental-setup,size,of,63 k words,size of 63 k words,0.6271932721138
translation,249,35,experimental-setup,experimental setup,create,"joint source and target bpe vocab ( sennrich et al. , 2016 )","experimental setup create joint source and target bpe vocab ( sennrich et al. , 2016 )",0.5370466113090515
translation,249,56,experimental-setup,6 decoder layers,for,all models,6 decoder layers for all models,0.588292121887207
translation,249,57,experimental-setup,experimental setup,implemented with,open-source toolkit fairseq,experimental setup implemented with open-source toolkit fairseq,0.713793933391571
translation,249,87,experimental-setup,4 nvidia p100 - pcie gpus,with,16 gb memory,4 nvidia p100 - pcie gpus with 16 gb memory,0.5767008662223816
translation,249,87,experimental-setup,experimental setup,implemented in,fairseq library,experimental setup implemented in fairseq library,0.7318130731582642
translation,249,88,experimental-setup,"adam algorithm ( kingma and ba , 2015 )",with,? 1 = 0.9 and ? 2 = 0.98,"adam algorithm ( kingma and ba , 2015 ) with ? 1 = 0.9 and ? 2 = 0.98",0.6264166235923767
translation,249,88,experimental-setup,experimental setup,optimized with,"adam algorithm ( kingma and ba , 2015 )","experimental setup optimized with adam algorithm ( kingma and ba , 2015 )",0.7168305516242981
translation,249,89,experimental-setup,max learning rate,to,0.001,max learning rate to 0.001,0.5473223328590393
translation,249,89,experimental-setup,max learning rate,to,0.0007,max learning rate to 0.0007,0.5573468208312988
translation,249,89,experimental-setup,0.001,when training,single model from scratch,0.001 when training single model from scratch,0.6979769468307495
translation,249,89,experimental-setup,0.0007,when,fine-tuning the model,0.0007 when fine-tuning the model,0.6289427876472473
translation,249,89,experimental-setup,experimental setup,set,max learning rate,experimental setup set max learning rate,0.6232877373695374
translation,249,90,experimental-setup,batch size,set to,2048 tokens,batch size set to 2048 tokens,0.6695392727851868
translation,249,90,experimental-setup,2048 tokens,per,gpu,2048 tokens per gpu,0.6224781274795532
translation,249,90,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,249,91,experimental-setup,updatefreq ' parameter,in,fairseq,updatefreq ' parameter in fairseq,0.5332451462745667
translation,249,91,experimental-setup,updatefreq ' parameter,set to,16,updatefreq ' parameter set to 16,0.6745629906654358
translation,249,91,experimental-setup,16,when,training,16 when training,0.682802677154541
translation,249,91,experimental-setup,4,when,finetuning,4 when finetuning,0.592139482498169
translation,249,91,experimental-setup,training,has,single model from scratch,training has single model from scratch,0.5415138006210327
translation,249,91,experimental-setup,training,has,4,training has 4,0.6508060693740845
translation,249,91,experimental-setup,finetuning,has,model,finetuning has model,0.5823925733566284
translation,249,91,experimental-setup,experimental setup,has,updatefreq ' parameter,experimental setup has updatefreq ' parameter,0.5294139981269836
translation,249,92,experimental-setup,"dropout ( gal and ghahramani , 2016 ) probabilities",set to,0.1,"dropout ( gal and ghahramani , 2016 ) probabilities set to 0.1",0.6716454029083252
translation,249,92,experimental-setup,experimental setup,has,"dropout ( gal and ghahramani , 2016 ) probabilities","experimental setup has dropout ( gal and ghahramani , 2016 ) probabilities",0.4990383982658386
translation,249,102,experimental-setup,"fastalign ( dyer et al. , 2013 )",to generate,word alignments,"fastalign ( dyer et al. , 2013 ) to generate word alignments",0.6067700386047363
translation,249,103,experimental-setup,word alignments,extract,phrase table,word alignments extract phrase table,0.7023553252220154
translation,249,103,experimental-setup,phrase table,by using,"moses ( koehn et al. , 2007 )","phrase table by using moses ( koehn et al. , 2007 )",0.6297756433486938
translation,249,103,experimental-setup,"moses ( koehn et al. , 2007 )",with,default settings,"moses ( koehn et al. , 2007 ) with default settings",0.6119683980941772
translation,249,103,experimental-setup,experimental setup,based on,word alignments,experimental setup based on word alignments,0.5610656142234802
translation,249,104,experimental-setup,experimental setup,use,count- based pruning,experimental setup use count- based pruning,0.6357152462005615
translation,249,84,experiments,three transformer models,with,different architectures,three transformer models with different architectures,0.6493335366249084
translation,249,84,experiments,different architectures,including,variants of transformer - big,different architectures including variants of transformer - big,0.6721826195716858
translation,249,84,experiments,different architectures,including,transformer -deep,different architectures including transformer -deep,0.7500283122062683
translation,249,84,experiments,different architectures,including,transformer - large,different architectures including transformer - large,0.7407301664352417
translation,249,65,model,chinese-to- english transformer - deep nmt model,based on,bilingual training dataset,chinese-to- english transformer - deep nmt model based on bilingual training dataset,0.5880837440490723
translation,249,65,model,model,train,chinese-to- english transformer - deep nmt model,model train chinese-to- english transformer - deep nmt model,0.6774799227714539
translation,250,26,ablation-analysis,mmt models ' improvements,over,text-only counterparts,mmt models ' improvements over text-only counterparts,0.7008049488067627
translation,250,26,ablation-analysis,mmt models ' improvements,result from,regularization effect,mmt models ' improvements result from regularization effect,0.6924945116043091
translation,250,26,ablation-analysis,sufficient textual context,has,mmt models ' improvements,sufficient textual context has mmt models ' improvements,0.5659650564193726
translation,250,26,ablation-analysis,ablation analysis,under,sufficient textual context,ablation analysis under sufficient textual context,0.6134768128395081
translation,250,174,ablation-analysis,2 norm,of,model weights,2 norm of model weights,0.5838433504104614
translation,250,174,ablation-analysis,model weights,in,resnet- based gated fusion and noise - based gated fusion,model weights in resnet- based gated fusion and noise - based gated fusion,0.5380667448043823
translation,250,174,ablation-analysis,resnet- based gated fusion and noise - based gated fusion,are,97.7 % and 95.2 %,resnet- based gated fusion and noise - based gated fusion are 97.7 % and 95.2 %,0.5384147763252258
translation,250,174,ablation-analysis,97.7 % and 95.2 %,of,transformer,97.7 % and 95.2 % of transformer,0.5801950097084045
translation,250,174,ablation-analysis,97.7 % and 95.2 %,on,en? de,97.7 % and 95.2 % on en? de,0.6404833793640137
translation,250,174,ablation-analysis,transformer,on,en? de,transformer on en? de,0.6953052878379822
translation,250,174,ablation-analysis,ablation analysis,find that,2 norm,ablation analysis find that 2 norm,0.643621563911438
translation,250,212,ablation-analysis,visual feature selection,critical for,mmt 's performance,visual feature selection critical for mmt 's performance,0.6944667100906372
translation,250,212,ablation-analysis,ablation analysis,find that,visual feature selection,ablation analysis find that visual feature selection,0.6045962572097778
translation,250,177,baselines,baselines,consider,three models,baselines consider three models,0.7083951234817505
translation,250,95,experimental-setup,adam,with,"? 1 = 0.9 , ? 2 = 0.98","adam with ? 1 = 0.9 , ? 2 = 0.98",0.6682090759277344
translation,250,95,experimental-setup,"? 1 = 0.9 , ? 2 = 0.98",for,model optimization,"? 1 = 0.9 , ? 2 = 0.98 for model optimization",0.6249138116836548
translation,250,96,experimental-setup,"warmup phase ( 2,000 steps )",linearly increase,learning rate,"warmup phase ( 2,000 steps ) linearly increase learning rate",0.7550655603408813
translation,250,96,experimental-setup,learning rate,from,10 ?7 to 0.005,learning rate from 10 ?7 to 0.005,0.5380722880363464
translation,250,97,experimental-setup,learning rate,proportional to,number of updates,learning rate proportional to number of updates,0.7312131524085999
translation,250,97,experimental-setup,experimental setup,decay,learning rate,experimental setup decay learning rate,0.7320895791053772
translation,250,98,experimental-setup,training batch,contains,"at most 4,096","training batch contains at most 4,096",0.6492379307746887
translation,250,98,experimental-setup,"at most 4,096",has,source / target tokens,"at most 4,096 has source / target tokens",0.5978878736495972
translation,250,98,experimental-setup,experimental setup,has,training batch,experimental setup has training batch,0.5551625490188599
translation,250,99,experimental-setup,label smoothing weight,to,0.1,label smoothing weight to 0.1,0.5342792272567749
translation,250,99,experimental-setup,experimental setup,set,label smoothing weight,experimental setup set label smoothing weight,0.5950526595115662
translation,250,99,experimental-setup,experimental setup,set,dropout,experimental setup set dropout,0.597813606262207
translation,250,100,experimental-setup,training,if,validation loss,training if validation loss,0.5758872628211975
translation,250,100,experimental-setup,does not improve,for,ten epochs,does not improve for ten epochs,0.6002358198165894
translation,250,100,experimental-setup,early - stop,has,training,early - stop has training,0.5827264189720154
translation,250,100,experimental-setup,validation loss,has,does not improve,validation loss has does not improve,0.6121518015861511
translation,250,101,experimental-setup,last ten checkpoints,for,inference,last ten checkpoints for inference,0.6177573204040527
translation,250,101,experimental-setup,experimental setup,average,last ten checkpoints,experimental setup average last ten checkpoints,0.7184063196182251
translation,250,102,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,250,102,experimental-setup,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,250,102,experimental-setup,experimental setup,perform,beam search,experimental setup perform beam search,0.5451395511627197
translation,250,104,experimental-setup,one single machine,with,two titan p100 gpus,one single machine with two titan p100 gpus,0.5848242044448853
translation,250,104,experimental-setup,experimental setup,trained and evaluated on,one single machine,experimental setup trained and evaluated on one single machine,0.7309132218360901
translation,250,19,model,visual context,in,mmt,visual context in mmt,0.5326204299926758
translation,250,19,model,visual context,by designing,two interpretable models,visual context by designing two interpretable models,0.7411376237869263
translation,250,19,model,model,need for,visual context,model need for visual context,0.5950790643692017
translation,250,27,model,mmt models,leverage,visual context,mmt models leverage visual context,0.6961140632629395
translation,250,27,model,visual context,to help,translation,visual context to help translation,0.5867558717727661
translation,250,27,model,limited textual context,has,mmt models,limited textual context has mmt models,0.5547849535942078
translation,250,27,model,model,under,limited textual context,model under limited textual context,0.613935112953186
translation,250,88,results,pre-trained retriever,achieves,r@1,pre-trained retriever achieves r@1,0.6806076765060425
translation,250,88,results,pre-trained retriever,achieves,r@5,pre-trained retriever achieves r@5,0.7129696011543274
translation,250,88,results,r@1,of,22.8 %,r@1 of 22.8 %,0.5918006896972656
translation,250,88,results,r@1,of,39.6 %,r@1 of 39.6 %,0.5880997776985168
translation,250,88,results,r@1,of,39.6 %,r@1 of 39.6 %,0.5880997776985168
translation,250,88,results,r@5,of,39.6 %,r@5 of 39.6 %,0.5950090289115906
translation,250,88,results,results,has,pre-trained retriever,results has pre-trained retriever,0.5894720554351807
translation,250,121,results,state- of- the-arts ( gmnmt and dccn ),on,en? de translation,state- of- the-arts ( gmnmt and dccn ) on en? de translation,0.6122614145278931
translation,250,121,results,"baseline mmt models ( doubly - att , imagination and uvr - nmt )",has,significantly outperform,"baseline mmt models ( doubly - att , imagination and uvr - nmt ) has significantly outperform",0.5288015007972717
translation,250,121,results,significantly outperform,has,state- of- the-arts ( gmnmt and dccn ),significantly outperform has state- of- the-arts ( gmnmt and dccn ),0.5884615778923035
translation,250,122,results,improvement,of,all these methods,improvement of all these methods,0.5838363170623779
translation,250,122,results,improvement,over,base transformer - tiny model,improvement over base transformer - tiny model,0.6320228576660156
translation,250,122,results,improvement,is,very marginal,improvement is very marginal,0.5709238052368164
translation,250,122,results,all these methods,over,base transformer - tiny model,all these methods over base transformer - tiny model,0.6449063420295715
translation,250,122,results,all these methods,is,very marginal,all these methods is very marginal,0.5882091522216797
translation,250,122,results,results,has,improvement,results has improvement,0.6248279809951782
translation,250,151,results,text-only nmt,utilizing,visual features,text-only nmt utilizing visual features,0.6495604515075684
translation,250,151,results,visual features,lowers,mmt models ' trust,visual features lowers mmt models ' trust,0.682117760181427
translation,250,151,results,mmt models ' trust,in,hidden representations,mmt models ' trust in hidden representations,0.5421987771987915
translation,250,151,results,hidden representations,generated from,textual encoders,hidden representations generated from textual encoders,0.6318929195404053
translation,250,151,results,results,Compared with,text-only nmt,results Compared with text-only nmt,0.6596118807792664
translation,250,152,results,training,continues,textual encoder,training continues textual encoder,0.7024353742599487
translation,250,152,results,textual encoder,learns to represent,source text better,textual encoder learns to represent source text better,0.6470608115196228
translation,250,152,results,textual encoder,importance of,visual context,textual encoder importance of visual context,0.6067965626716614
translation,250,152,results,training,has,textual encoder,training has textual encoder,0.5778765082359314
translation,250,152,results,visual context,has,gradually decreases,visual context has gradually decreases,0.5595767498016357
translation,250,161,results,regularization techniques,achieves,similar gains,regularization techniques achieves similar gains,0.6776239275932312
translation,250,161,results,similar gains,over,text-only baseline,similar gains over text-only baseline,0.6789121627807617
translation,250,161,results,similar gains,as incorporating,multimodal information,similar gains as incorporating multimodal information,0.7113407254219055
translation,250,161,results,results,applying,regularization techniques,results applying regularization techniques,0.6646875143051147
translation,250,179,results,best results,of,text-only transformer model,best results of text-only transformer model,0.5973438024520874
translation,250,179,results,text-only transformer model,with,fine-tuned weight decay,text-only transformer model with fine-tuned weight decay,0.6235052943229675
translation,250,179,results,fine-tuned weight decay,are,comparable or even better,fine-tuned weight decay are comparable or even better,0.5603591799736023
translation,250,179,results,comparable or even better,than,mmt models,comparable or even better than mmt models,0.5814998149871826
translation,250,179,results,comparable or even better,than,gated fusion,comparable or even better than gated fusion,0.6078651547431946
translation,250,179,results,gated fusion,that utilize,visual context,gated fusion that utilize visual context,0.6401059627532959
translation,250,179,results,results,see that,best results,results see that best results,0.6217278838157654
translation,250,190,results,weight decay,bring,marginal improvement,weight decay bring marginal improvement,0.6407092809677124
translation,250,190,results,marginal improvement,over,text-only transformer model,marginal improvement over text-only transformer model,0.6826378107070923
translation,250,191,results,resnet- based models,that utilize,visual context,resnet- based models that utilize visual context,0.583579421043396
translation,250,191,results,visual context,has,significantly improve,visual context has significantly improve,0.5732734203338623
translation,250,191,results,significantly improve,has,translation results,significantly improve has translation results,0.5146471261978149
translation,250,191,results,results,has,resnet- based models,results has resnet- based models,0.5115172863006592
translation,250,197,results,resnet features,observe,significant drop,resnet features observe significant drop,0.5930846929550171
translation,250,197,results,significant drop,in,bleu and me-teor scores,significant drop in bleu and me-teor scores,0.5791091322898865
translation,250,197,results,results,even with,resnet features,results even with resnet features,0.6673393249511719
translation,251,194,ablation-analysis,decoder-side codes,result in,noticeable drop,decoder-side codes result in noticeable drop,0.6909618377685547
translation,251,194,ablation-analysis,noticeable drop,in,performance,noticeable drop in performance,0.570785641670227
translation,251,194,ablation-analysis,performance,in,most directions,performance in most directions,0.5065335631370544
translation,251,194,ablation-analysis,base 12 - 2 architecture,has,decoder-side codes,base 12 - 2 architecture has decoder-side codes,0.5792987942695618
translation,251,194,ablation-analysis,ablation analysis,with,base 12 - 2 architecture,ablation analysis with base 12 - 2 architecture,0.658279538154602
translation,251,142,baselines,big 6 - 6 bilingual baselines,trained with,same hyperparameters,big 6 - 6 bilingual baselines trained with same hyperparameters,0.6964923143386841
translation,251,142,baselines,same hyperparameters,for,120k steps,same hyperparameters for 120k steps,0.614806056022644
translation,251,142,baselines,same hyperparameters,with,joint bpe vocabularies,same hyperparameters with joint bpe vocabularies,0.6156752109527588
translation,251,142,baselines,joint bpe vocabularies,has,of size 16 k,joint bpe vocabularies has of size 16 k,0.6098716855049133
translation,251,142,baselines,baselines,has,big 6 - 6 bilingual baselines,baselines has big 6 - 6 bilingual baselines,0.588682234287262
translation,251,97,experimental-setup,english-centric models,for,120 epochs,english-centric models for 120 epochs,0.6420848369598389
translation,251,97,experimental-setup,120 epochs,has,? 1.8 m updates,120 epochs has ? 1.8 m updates,0.5641404390335083
translation,251,97,experimental-setup,experimental setup,train,english-centric models,experimental setup train english-centric models,0.6366665363311768
translation,251,100,experimental-setup,shared bpe model,with,64 k merge operations,shared bpe model with 64 k merge operations,0.6434639692306519
translation,251,100,experimental-setup,shared bpe model,with,inline casing,shared bpe model with inline casing,0.7150377631187439
translation,251,100,experimental-setup,shared bpe model,with,inline casing,shared bpe model with inline casing,0.7150377631187439
translation,251,100,experimental-setup,inline casing,has,"berard et al. , 2019 )","inline casing has berard et al. , 2019 )",0.5899701714515686
translation,251,100,experimental-setup,experimental setup,create,shared bpe model,experimental setup create shared bpe model,0.5960478186607361
translation,251,140,experimental-setup,english-centric models,for,1 m steps,english-centric models for 1 m steps,0.6224202513694763
translation,251,140,experimental-setup,multi-parallel data,for,200k more steps,multi-parallel data for 200k more steps,0.6079203486442566
translation,251,140,experimental-setup,experimental setup,train,english-centric models,experimental setup train english-centric models,0.6366665363311768
translation,251,207,experimental-setup,language -specific decoders,trained with,homogeneous batches,language -specific decoders trained with homogeneous batches,0.751655101776123
translation,251,207,experimental-setup,homogeneous batches,with respect to,target language,homogeneous batches with respect to target language,0.7119016051292419
translation,251,207,experimental-setup,experimental setup,has,language -specific decoders,experimental setup has language -specific decoders,0.5249189734458923
translation,251,141,experiments,fine-tuned,for,200k steps,fine-tuned for 200k steps,0.6538453102111816
translation,251,141,experiments,200k steps,from,englishcentric models,200k steps from englishcentric models,0.5896921753883362
translation,251,141,experiments,englishcentric models,with,multi-parallel data,englishcentric models with multi-parallel data,0.6118762493133545
translation,251,119,results,"base 12 - 2 models ( 3 , 6 )",perform,as well or better,"base 12 - 2 models ( 3 , 6 ) perform as well or better",0.5986682176589966
translation,251,119,results,as well or better,as,"base 6 - 6 models ( 2 , 5 )","as well or better as base 6 - 6 models ( 2 , 5 )",0.6173340082168579
translation,251,119,results,as well or better,with,1.7 ? speed boost,as well or better with 1.7 ? speed boost,0.6740966439247131
translation,251,119,results,"base 6 - 6 models ( 2 , 5 )",in,all language directions,"base 6 - 6 models ( 2 , 5 ) in all language directions",0.5183599591255188
translation,251,120,results,"multi-parallel fine-tuning ( 5 , 6 )",incurs,no drop,"multi-parallel fine-tuning ( 5 , 6 ) incurs no drop",0.7690821886062622
translation,251,120,results,translation quality,between,non-english languages,translation quality between non-english languages,0.6171494722366333
translation,251,120,results,no drop,in,performance,no drop in performance,0.5421490669250488
translation,251,120,results,performance,in,?en directions,performance in ?en directions,0.5385175943374634
translation,251,120,results,"multi-parallel fine-tuning ( 5 , 6 )",has,significantly increases,"multi-parallel fine-tuning ( 5 , 6 ) has significantly increases",0.5540211200714111
translation,251,120,results,significantly increases,has,translation quality,significantly increases has translation quality,0.5973691940307617
translation,251,120,results,results,has,"multi-parallel fine-tuning ( 5 , 6 )","results has multi-parallel fine-tuning ( 5 , 6 )",0.5542291402816772
translation,251,158,results,transformer big 12 - 2,has,consistently outperforms,transformer big 12 - 2 has consistently outperforms,0.6232250332832336
translation,251,158,results,consistently outperforms,has,big 6 - 6,consistently outperforms has big 6 - 6,0.6288989782333374
translation,251,158,results,multi-parallel training,has,consistently hurts,multi-parallel training has consistently hurts,0.5713242888450623
translation,251,158,results,consistently hurts,has,?en performance,consistently hurts has ?en performance,0.6306809782981873
translation,251,158,results,results,see that,transformer big 12 - 2,results see that transformer big 12 - 2,0.7152895331382751
translation,251,174,results,big 12 - 2 ( 24 ),performs,consistently better,big 12 - 2 ( 24 ) performs consistently better,0.6048405766487122
translation,251,174,results,consistently better,than,big 6 - 6 ( 23 ),consistently better than big 6 - 6 ( 23 ),0.5787182450294495
translation,251,174,results,consistently better,across,all sentence lengths,consistently better across all sentence lengths,0.6794586181640625
translation,251,174,results,results,note,big 12 - 2 ( 24 ),results note big 12 - 2 ( 24 ),0.6261478066444397
translation,251,271,results,big 12 - 2,has,outperforms,big 12 - 2 has outperforms,0.6626315712928772
translation,251,271,results,results,has,big 12 - 2,results has big 12 - 2,0.6363707780838013
translation,252,131,baselines,ca - transformer,with,additional context encoder,ca - transformer with additional context encoder,0.6271291971206665
translation,252,131,baselines,context - aware transformer model ( ca - transformer ),with,additional context encoder,context - aware transformer model ( ca - transformer ) with additional context encoder,0.6158979535102844
translation,252,131,baselines,additional context encoder,to incorporate,document contextual information,additional context encoder to incorporate document contextual information,0.6528618931770325
translation,252,131,baselines,additional context encoder,to incorporate,document contextual information,additional context encoder to incorporate document contextual information,0.6528618931770325
translation,252,131,baselines,document contextual information,into,model,document contextual information into model,0.557849109172821
translation,252,131,baselines,context- aware hierarchical attention networks ( ca - han ),integrate,document contextual information,context- aware hierarchical attention networks ( ca - han ) integrate document contextual information,0.5850258469581604
translation,252,131,baselines,document contextual information,from,source side and target side,document contextual information from source side and target side,0.5427003502845764
translation,252,131,baselines,ca - transformer,has,context - aware transformer model ( ca - transformer ),ca - transformer has context - aware transformer model ( ca - transformer ),0.5798815488815308
translation,252,131,baselines,ca- han,has,context- aware hierarchical attention networks ( ca - han ),ca- han has context- aware hierarchical attention networks ( ca - han ),0.56113201379776
translation,252,131,baselines,cadec,has,two - pass machine translation model,cadec has two - pass machine translation model,0.552952766418457
translation,252,131,baselines,two - pass machine translation model,has,"context - aware decoder , cadec )","two - pass machine translation model has context - aware decoder , cadec )",0.5851887464523315
translation,252,133,experimental-setup,experimental setup,implemented on,open-source toolkit thumt,experimental setup implemented on open-source toolkit thumt,0.728324830532074
translation,252,134,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",applied with,initial learning rate,"adam optimizer ( kingma and ba , 2014 ) applied with initial learning rate",0.6552746891975403
translation,252,134,experimental-setup,initial learning rate,has,0.1,initial learning rate has 0.1,0.5062676668167114
translation,252,134,experimental-setup,experimental setup,has,"adam optimizer ( kingma and ba , 2014 )","experimental setup has adam optimizer ( kingma and ba , 2014 )",0.535233736038208
translation,252,135,experimental-setup,size,of,hidden dimension and feed -forward layer,size of hidden dimension and feed -forward layer,0.5803710222244263
translation,252,135,experimental-setup,size,set to,512 and 2048,size set to 512 and 2048,0.698930561542511
translation,252,135,experimental-setup,hidden dimension and feed -forward layer,set to,512 and 2048,hidden dimension and feed -forward layer set to 512 and 2048,0.7184435725212097
translation,252,135,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,252,137,experimental-setup,dropout,is,0.1,dropout is 0.1,0.5574049949645996
translation,252,137,experimental-setup,batch size,set to,4096,batch size set to 4096,0.7435297966003418
translation,252,137,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,252,137,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,252,138,experimental-setup,beam size,is,4,beam size is 4,0.6488027572631836
translation,252,138,experimental-setup,4,for,inference,4 for inference,0.6272199153900146
translation,252,138,experimental-setup,experimental setup,has,beam size,experimental setup has beam size,0.46948209404945374
translation,252,140,experimental-setup,context window size,set to,3,context window size set to 3,0.7113036513328552
translation,252,140,experimental-setup,experimental setup,has,context window size,experimental setup has context window size,0.5050400495529175
translation,252,7,model,novel multi-hop transformer ( mht ),offers,nmt abilities,novel multi-hop transformer ( mht ) offers nmt abilities,0.7022477984428406
translation,252,7,model,nmt abilities,to explicitly model,human-like draft-editing and reasoning process,nmt abilities to explicitly model human-like draft-editing and reasoning process,0.7045267224311829
translation,252,7,model,model,propose,novel multi-hop transformer ( mht ),model propose novel multi-hop transformer ( mht ),0.6742410063743591
translation,252,8,model,sentence - level translation,as,draft,sentence - level translation as draft,0.5520980358123779
translation,252,8,model,representations,by attending to,multiple antecedent sentences iteratively,representations by attending to multiple antecedent sentences iteratively,0.7209137082099915
translation,252,8,model,model,serves,sentence - level translation,model serves sentence - level translation,0.6029062271118164
translation,252,23,model,document- level nmt,using,novel framework - multi-hop transformer,document- level nmt using novel framework - multi-hop transformer,0.6489161252975464
translation,252,23,model,novel framework - multi-hop transformer,imitates,draft-editing and reasoning process,novel framework - multi-hop transformer imitates draft-editing and reasoning process,0.6600664854049683
translation,252,23,model,model,improve,document- level nmt,model improve document- level nmt,0.6675810813903809
translation,252,24,model,explicit reasoning process,by exploiting,source and target antecedent sentences,explicit reasoning process by exploiting source and target antecedent sentences,0.638117253780365
translation,252,24,model,source and target antecedent sentences,with,concurrently stacked attention layers,source and target antecedent sentences with concurrently stacked attention layers,0.6004717946052551
translation,252,24,model,concurrently stacked attention layers,performing,progressive refinement,concurrently stacked attention layers performing progressive refinement,0.6331650614738464
translation,252,24,model,progressive refinement,on,representations,progressive refinement on representations,0.5547552704811096
translation,252,24,model,representations,of,current sentence and its translation,representations of current sentence and its translation,0.5557671189308167
translation,252,24,model,model,implement,explicit reasoning process,model implement explicit reasoning process,0.6930835843086243
translation,252,25,model,draft,to present,context information,draft to present context information,0.6775592565536499
translation,252,25,model,context information,on,target side,context information on target side,0.5628215074539185
translation,252,25,model,target side,during,training and testing,target side during training and testing,0.6981838941574097
translation,252,25,model,model,leverage,draft,model leverage draft,0.6917060613632202
translation,252,36,model,decoder,applys,multi-head cross attention,decoder applys multi-head cross attention,0.6804924011230469
translation,252,36,model,multi-head cross attention,to capture,information,multi-head cross attention to capture information,0.656008243560791
translation,252,36,model,information,from,encoder,information from encoder,0.6118977069854736
translation,252,59,model,different context sentences,captures,inter-sentence reasoning semantics,different context sentences captures inter-sentence reasoning semantics,0.6664197444915771
translation,252,59,model,inter-sentence reasoning semantics,of,contextual sentences,inter-sentence reasoning semantics of contextual sentences,0.5123980641365051
translation,252,59,model,inter-sentence reasoning semantics,to incrementally refine,representation,inter-sentence reasoning semantics to incrementally refine representation,0.6883515119552612
translation,252,59,model,representation,of,current sentence,representation of current sentence,0.5636553168296814
translation,252,59,model,model,iteratively focuses on,different context sentences,model iteratively focuses on different context sentences,0.7049320936203003
translation,252,61,model,novel method,called,multi-hop transformer,novel method called multi-hop transformer,0.6443212032318115
translation,252,61,model,novel method,models,reasoning process,novel method models reasoning process,0.625847339630127
translation,252,61,model,reasoning process,among,multiple contextual sentences,reasoning process among multiple contextual sentences,0.5572057962417603
translation,252,61,model,multiple contextual sentences,in,source side and target side,multiple contextual sentences in source side and target side,0.5151837468147278
translation,252,61,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,252,62,model,source-side contexts,directly acquired from,document,source-side contexts directly acquired from document,0.7255927920341492
translation,252,62,model,model,has,source-side contexts,model has source-side contexts,0.5329360961914062
translation,252,67,model,representations,for,source-side contexts and target-side drafts,representations for source-side contexts and target-side drafts,0.6404916644096375
translation,252,70,model,information,from,target-side drafts,information from target-side drafts,0.5635886788368225
translation,252,70,model,information,models,translation probability distribution,information models translation probability distribution,0.6980448365211487
translation,252,70,model,model,has,multi-hop decoder,model has multi-hop decoder,0.5503084659576416
translation,252,73,model,stack of six identical layers,consists of,two sub-layers,stack of six identical layers consists of two sub-layers,0.691969096660614
translation,252,136,model,6 layers,with,8 heads,6 layers with 8 heads,0.7101464867591858
translation,252,136,model,8 heads,has,multi-head attention,8 heads has multi-head attention,0.5933308005332947
translation,252,136,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,252,141,model,sentence - level nmt model,trained from,source language,sentence - level nmt model trained from source language,0.6863530278205872
translation,252,141,model,source language,using,corresponding datasets,source language using corresponding datasets,0.6496505737304688
translation,252,141,model,corresponding datasets,without,additional corpus,corresponding datasets without additional corpus,0.675971508026123
translation,252,141,model,source-side sentence encoder,has,sentence - level nmt model,source-side sentence encoder has sentence - level nmt model,0.5033696293830872
translation,252,141,model,model,initialize,source-side sentence encoder,model initialize source-side sentence encoder,0.7126972079277039
translation,252,32,results,significantly improves,has,document-level translation performance,significantly improves has document-level translation performance,0.5729502439498901
translation,252,139,results,translation quality,evaluated by,traditional metric bleu ( papineni,translation quality evaluated by traditional metric bleu ( papineni,0.7122145891189575
translation,252,139,results,traditional metric bleu ( papineni,on,tokenized text,traditional metric bleu ( papineni on tokenized text,0.5052260160446167
translation,252,139,results,results,has,translation quality,results has translation quality,0.46428924798965454
translation,252,150,results,our model mht,obtains,best average results,our model mht obtains best average results,0.6032888889312744
translation,252,150,results,translation quality,in terms of,bleu,translation quality in terms of bleu,0.6314836144447327
translation,252,150,results,best average results,that gain,"0.38 , 0.69 and 1.57 bleu points","best average results that gain 0.38 , 0.69 and 1.57 bleu points",0.6594133973121643
translation,252,150,results,"0.38 , 0.69 and 1.57 bleu points",over,"cadec , ca - transformer and ca - han","0.38 , 0.69 and 1.57 bleu points over cadec , ca - transformer and ca - han",0.6342229843139648
translation,252,150,results,our model mht,has,significantly improves,our model mht has significantly improves,0.6153216361999512
translation,252,150,results,significantly improves,has,translation quality,significantly improves has translation quality,0.5744650959968567
translation,252,150,results,results,has,our model mht,results has our model mht,0.5804296731948853
translation,252,152,results,our translation systems,achieve,new state - of - the - art translation qualities,our translation systems achieve new state - of - the - art translation qualities,0.6207543611526489
translation,252,152,results,language models,has,our translation systems,language models has our translation systems,0.5450845956802368
translation,252,153,results,underlying reasoning semantics,by,multi-hop mechanism,underlying reasoning semantics by multi-hop mechanism,0.5286828279495239
translation,252,153,results,multi-hop mechanism,benefits,neural machine translation,multi-hop mechanism benefits neural machine translation,0.5601275563240051
translation,252,153,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,252,153,results,results,explicitly modeling,underlying reasoning semantics,results explicitly modeling underlying reasoning semantics,0.6952771544456482
translation,252,169,results,mht model reasoning,with,natural sentence order,mht model reasoning with natural sentence order,0.5646050572395325
translation,252,169,results,mht model,with,opposite reasoning direction,mht model with opposite reasoning direction,0.6376304626464844
translation,252,169,results,natural sentence order,has,outperforms,natural sentence order has outperforms,0.6283205151557922
translation,252,169,results,outperforms,has,mht model,outperforms has mht model,0.6041356325149536
translation,252,169,results,results,has,mht model reasoning,results has mht model reasoning,0.5364046096801758
translation,252,176,results,mht model,using,drafts,mht model using drafts,0.6769768595695496
translation,252,176,results,mht model,directly using,target - side context references,mht model directly using target - side context references,0.6364083290100098
translation,252,176,results,drafts,as,contexts,drafts as contexts,0.606675922870636
translation,252,176,results,contexts,outperforms,mht model,contexts outperforms mht model,0.6956586241722107
translation,252,176,results,mht model,directly using,target - side context references,mht model directly using target - side context references,0.6364083290100098
translation,252,176,results,results,see that,mht model,results see that mht model,0.6594671607017517
translation,253,60,experimental-setup,pg and mrt,in,"joey nmt ( kreutzer et al. , 2019 )","pg and mrt in joey nmt ( kreutzer et al. , 2019 )",0.5223961472511292
translation,253,60,experimental-setup,pg and mrt,for,transformers,pg and mrt for transformers,0.7085846066474915
translation,253,60,experimental-setup,"joey nmt ( kreutzer et al. , 2019 )",for,transformers,"joey nmt ( kreutzer et al. , 2019 ) for transformers",0.6441699862480164
translation,253,60,experimental-setup,experimental setup,implement,pg and mrt,experimental setup implement pg and mrt,0.6796581745147705
translation,253,87,results,domain adaptation,via,self-training,domain adaptation via self-training,0.6860056519508362
translation,253,87,results,self-training,does not show,improvements,self-training does not show improvements,0.6826186776161194
translation,253,87,results,improvements,over,baseline,improvements over baseline,0.7402786016464233
translation,253,87,results,results,has,domain adaptation,results has domain adaptation,0.5254684686660767
translation,253,88,results,weak,with,maximum gain,weak with maximum gain,0.7077902555465698
translation,253,88,results,maximum gain,of,0.5 bleu,maximum gain of 0.5 bleu,0.5908833742141724
translation,253,88,results,0.5 bleu,over,baseline,0.5 bleu over baseline,0.6295343637466431
translation,253,88,results,clear advantage,of using,informative rewards,clear advantage of using informative rewards,0.6183884739875793
translation,253,88,results,informative rewards,with,up to + 4.7 bleu,informative rewards with up to + 4.7 bleu,0.619723916053772
translation,253,88,results,informative rewards,with,+ 6.7 bleu,informative rewards with + 6.7 bleu,0.5939357280731201
translation,253,88,results,up to + 4.7 bleu,for,pg,up to + 4.7 bleu for pg,0.6595253348350525
translation,253,88,results,+ 6.7 bleu,for,mrt,+ 6.7 bleu for mrt,0.6274389624595642
translation,253,88,results,effects,has,in - domain,effects has in - domain,0.5966572761535645
translation,253,88,results,results,for,domain adaptation,results for domain adaptation,0.5749132633209229
translation,253,93,results,scaled reward,increases,score,scaled reward increases score,0.7303342223167419
translation,253,93,results,score,by,almost 1 bleu,score by almost 1 bleu,0.582839846611023
translation,253,93,results,average reward baseline,by,almost 2 bleu,average reward baseline by almost 2 bleu,0.5555834770202637
translation,253,93,results,mrt,leads to,gain,mrt leads to gain,0.6980849504470825
translation,253,93,results,gain,of,about 4.5 bleu,gain of about 4.5 bleu,0.5442733764648438
translation,253,93,results,about 4.5 bleu,over,plain pg,about 4.5 bleu over plain pg,0.6758227348327637
translation,253,93,results,results,has,scaled reward,results has scaled reward,0.5721437335014343
translation,253,95,results,improvements,of,rl,improvements of rl,0.6543291807174683
translation,253,95,results,rl,over,baseline,rl over baseline,0.7369728684425354
translation,253,95,results,rl,are,higher,rl are higher,0.6969237923622131
translation,253,95,results,baseline,are,higher,baseline are higher,0.6470133662223816
translation,253,95,results,higher,with,lower beam sizes,higher with lower beam sizes,0.6458706259727478
translation,253,95,results,results,show that,improvements,results show that improvements,0.5038777589797974
translation,253,97,results,bleu reductions,caused by,larger beams,bleu reductions caused by larger beams,0.6691025495529175
translation,253,97,results,larger beams,weaker than for,baseline model,larger beams weaker than for baseline model,0.7097546458244324
translation,253,97,results,rl models,has,bleu reductions,rl models has bleu reductions,0.5446299314498901
translation,253,97,results,results,For,rl models,results For rl models,0.5895084142684937
translation,254,37,ablation-analysis,self attention ( sa ) parameters,in,facilitating,self attention ( sa ) parameters in facilitating,0.536994457244873
translation,254,37,ablation-analysis,facilitating,has,crosslingual transfer ability,facilitating has crosslingual transfer ability,0.557590126991272
translation,254,37,ablation-analysis,ablation analysis,investigate,role,ablation analysis investigate role,0.6854152083396912
translation,254,104,ablation-analysis,interesting finding,applying,lna finetuning,interesting finding applying lna finetuning,0.7023053765296936
translation,254,104,ablation-analysis,lna finetuning,to,decoder,lna finetuning to decoder,0.52958083152771
translation,254,104,ablation-analysis,decoder,crucial for,zero-shot transfer to unseen languages ( ja ),decoder crucial for zero-shot transfer to unseen languages ( ja ),0.7327763438224792
translation,254,104,ablation-analysis,ablation analysis,applying,lna finetuning,ablation analysis applying lna finetuning,0.7201428413391113
translation,254,115,ablation-analysis,lna,to,"both encoder and decoder ( lna - min , lna - e , d )","lna to both encoder and decoder ( lna - min , lna - e , d )",0.5487514138221741
translation,254,115,ablation-analysis,lna,reduces,amount of parameters,lna reduces amount of parameters,0.620123028755188
translation,254,115,ablation-analysis,lna,maintain,strong performance,lna maintain strong performance,0.6797148585319519
translation,254,115,ablation-analysis,amount of parameters,to,train,amount of parameters to train,0.5690069794654846
translation,254,115,ablation-analysis,amount of parameters,to,only 8 ? 20 %,amount of parameters to only 8 ? 20 %,0.5661394596099854
translation,254,115,ablation-analysis,only 8 ? 20 %,of,all parameters,only 8 ? 20 % of all parameters,0.5905042290687561
translation,254,115,ablation-analysis,all parameters,in,pretrained models,all parameters in pretrained models,0.4731898009777069
translation,254,115,ablation-analysis,strong performance,compared to,strong baselines,strong performance compared to strong baselines,0.6648804545402527
translation,254,115,ablation-analysis,strong baselines,such as,asr pt,strong baselines such as asr pt,0.6282688975334167
translation,254,115,ablation-analysis,strong baselines,such as,best cascade models,strong baselines such as best cascade models,0.557266354560852
translation,254,115,ablation-analysis,asr pt,with,multilingual finetuning ( asr pt + multi ),asr pt with multilingual finetuning ( asr pt + multi ),0.6611475944519043
translation,254,115,ablation-analysis,ablation analysis,Applying,lna,ablation analysis Applying lna,0.6902067065238953
translation,254,130,ablation-analysis,finetuning layernorm parameter,important for,training stability,finetuning layernorm parameter important for training stability,0.6983723640441895
translation,254,130,ablation-analysis,training stability,when,finetuning pretrained models,training stability when finetuning pretrained models,0.6286871433258057
translation,254,130,ablation-analysis,finetuning pretrained models,without which,( - ln ) training diverges,finetuning pretrained models without which ( - ln ) training diverges,0.5916096568107605
translation,254,130,ablation-analysis,ablation analysis,find,finetuning layernorm parameter,ablation analysis find finetuning layernorm parameter,0.583207905292511
translation,254,75,baselines,first baseline,trains,sequenceto-sequence model,first baseline trains sequenceto-sequence model,0.7246350049972534
translation,254,75,baselines,sequenceto-sequence model,with,transformer architecture,sequenceto-sequence model with transformer architecture,0.6637372970581055
translation,254,75,baselines,transformer architecture,without,pretraining,transformer architecture without pretraining,0.6743190288543701
translation,254,75,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,254,90,baselines,baselines,of,finetuning,baselines of finetuning,0.6004480123519897
translation,254,90,baselines,baselines,of,finetuning feature extractor,baselines of finetuning feature extractor,0.5657880306243896
translation,254,90,baselines,finetuning,has,entire encoder ( all ),finetuning has entire encoder ( all ),0.594429612159729
translation,254,90,baselines,baselines,compare to,baselines,baselines compare to baselines,0.6858891844749451
translation,254,90,baselines,baselines,of,finetuning,baselines of finetuning,0.6004480123519897
translation,254,54,experimental-setup,encoder,using,opensourced 1 wav2vec 2.0 large architecture,encoder using opensourced 1 wav2vec 2.0 large architecture,0.5814995169639587
translation,254,54,experimental-setup,opensourced 1 wav2vec 2.0 large architecture,pretrained on,unlabelled english-only ( xmef - en ) audio,opensourced 1 wav2vec 2.0 large architecture pretrained on unlabelled english-only ( xmef - en ) audio,0.7620847225189209
translation,254,54,experimental-setup,unlabelled english-only ( xmef - en ) audio,from,"librivox ( baevski et al. , 2020 )","unlabelled english-only ( xmef - en ) audio from librivox ( baevski et al. , 2020 )",0.5113391280174255
translation,254,54,experimental-setup,experimental setup,initialize,encoder,experimental setup initialize encoder,0.6722666025161743
translation,254,58,experimental-setup,decoder,with,opensourced 2 mbart50 models,decoder with opensourced 2 mbart50 models,0.6696630120277405
translation,254,58,experimental-setup,decoder,with,same vocabulary,decoder with same vocabulary,0.6916762590408325
translation,254,58,experimental-setup,experimental setup,initialize,decoder,experimental setup initialize decoder,0.686920702457428
translation,254,59,experimental-setup,mbart50n1 ( 49 languages to english ),for,x - en st directions,mbart50n1 ( 49 languages to english ) for x - en st directions,0.669462263584137
translation,254,59,experimental-setup,mbart501n ( english to 49 languages ),for,translating en-x st directions,mbart501n ( english to 49 languages ) for translating en-x st directions,0.6709191203117371
translation,254,59,experimental-setup,experimental setup,use,mbart50n1 ( 49 languages to english ),experimental setup use mbart50n1 ( 49 languages to english ),0.5816940665245056
translation,254,59,experimental-setup,experimental setup,use,mbart501n ( english to 49 languages ),experimental setup use mbart501n ( english to 49 languages ),0.5906859636306763
translation,254,55,experiments,many - to- one experiments,experiment with,multilingual wav2vec 2.0 ( xmef - x ),many - to- one experiments experiment with multilingual wav2vec 2.0 ( xmef - x ),0.6725524067878723
translation,254,55,experiments,multilingual wav2vec 2.0 ( xmef - x ),pretrained on,raw audio,multilingual wav2vec 2.0 ( xmef - x ) pretrained on raw audio,0.7112241983413696
translation,254,55,experiments,raw audio,from,"53 languages ( conneau et al. , 2020 )","raw audio from 53 languages ( conneau et al. , 2020 )",0.5545086860656738
translation,254,92,experiments,zero-shot crosslingual transfer,evaluate,xmef 's crosslingual transfer performance,zero-shot crosslingual transfer evaluate xmef 's crosslingual transfer performance,0.6729132533073425
translation,254,17,model,transfer learning strategy,by only finetuning,layernorm and attention ( lna ) parameters,transfer learning strategy by only finetuning layernorm and attention ( lna ) parameters,0.8106600046157837
translation,254,17,model,layernorm and attention ( lna ) parameters,of,pretrained models,layernorm and attention ( lna ) parameters of pretrained models,0.5266526341438293
translation,254,17,model,efficient,has,transfer learning strategy,efficient has transfer learning strategy,0.5902383327484131
translation,254,17,model,model,present,efficient,model present efficient,0.759250819683075
translation,254,17,model,model,present,transfer learning strategy,model present transfer learning strategy,0.6464253664016724
translation,254,26,model,"pretrained wav2vec 2.0 ( baevski et al. , 2020 )",as,encoder,"pretrained wav2vec 2.0 ( baevski et al. , 2020 ) as encoder",0.4594420790672302
translation,254,26,model,"pretrained wav2vec 2.0 ( baevski et al. , 2020 )",as,decoder,"pretrained wav2vec 2.0 ( baevski et al. , 2020 ) as decoder",0.46226930618286133
translation,254,26,model,"pretrained wav2vec 2.0 ( baevski et al. , 2020 )",as,decoder,"pretrained wav2vec 2.0 ( baevski et al. , 2020 ) as decoder",0.46226930618286133
translation,254,26,model,encoder,for,acoustic modeling,encoder for acoustic modeling,0.6458675265312195
translation,254,26,model,encoder,for,language modeling,encoder for language modeling,0.6005983948707581
translation,254,26,model,encoder,for,language modeling,encoder for language modeling,0.6005983948707581
translation,254,26,model,pretrained multilingual bart ( mbart ),as,decoder,pretrained multilingual bart ( mbart ) as decoder,0.5220815539360046
translation,254,26,model,decoder,for,language modeling,decoder for language modeling,0.6030555367469788
translation,254,26,model,model,leverages,"pretrained wav2vec 2.0 ( baevski et al. , 2020 )","model leverages pretrained wav2vec 2.0 ( baevski et al. , 2020 )",0.5916169285774231
translation,254,27,model,unlabelled data,via,self-supervised learning,unlabelled data via self-supervised learning,0.6232017874717712
translation,254,27,model,model,pretrained on,unlabelled data,model pretrained on unlabelled data,0.7643691301345825
translation,254,30,model,lightweight adaptor module,in between,encoder and decoder,lightweight adaptor module in between encoder and decoder,0.6291834712028503
translation,254,30,model,model,add,lightweight adaptor module,model add lightweight adaptor module,0.5756849646568298
translation,254,31,model,adaptor module,performs,projection and downsampling,adaptor module performs projection and downsampling,0.6275335550308228
translation,254,31,model,projection and downsampling,to alleviate,length inconsistency,projection and downsampling to alleviate length inconsistency,0.6895162463188171
translation,254,31,model,length inconsistency,between,audio and text sequences,length inconsistency between audio and text sequences,0.6567630767822266
translation,254,31,model,model,has,adaptor module,model has adaptor module,0.5684814453125
translation,254,33,model,lna finetuning,propose,parameter efficient finetuning strategy ( lna ),lna finetuning propose parameter efficient finetuning strategy ( lna ),0.6136270761489868
translation,254,33,model,parameter efficient finetuning strategy ( lna ),of only finetuning,layer normalization ( layernorm ),parameter efficient finetuning strategy ( lna ) of only finetuning layer normalization ( layernorm ),0.7990370392799377
translation,254,33,model,parameter efficient finetuning strategy ( lna ),of only finetuning,multi-head attention ( mha ) parameters,parameter efficient finetuning strategy ( lna ) of only finetuning multi-head attention ( mha ) parameters,0.7832045555114746
translation,254,33,model,model,propose,parameter efficient finetuning strategy ( lna ),model propose parameter efficient finetuning strategy ( lna ),0.6204288005828857
translation,254,33,model,model,has,lna finetuning,model has lna finetuning,0.571414053440094
translation,254,56,model,encoder output,followed by,3 1 - d convolution layers,encoder output followed by 3 1 - d convolution layers,0.6319243311882019
translation,254,56,model,3 1 - d convolution layers,with,stride 2,3 1 - d convolution layers with stride 2,0.604436457157135
translation,254,56,model,stride 2,to achieve,8x down - sampling,stride 2 to achieve 8x down - sampling,0.6395540833473206
translation,254,56,model,8x down - sampling,of,audio encoder outputs,8x down - sampling of audio encoder outputs,0.5772984623908997
translation,254,56,model,model,has,encoder output,model has encoder output,0.5511632561683655
translation,254,67,model,last 12 transformer layers,in,wav2vec encoder,last 12 transformer layers in wav2vec encoder,0.4669642746448517
translation,254,67,model,last 12 transformer layers,replaced with,12 mbart encoder layers,last 12 transformer layers replaced with 12 mbart encoder layers,0.7023639678955078
translation,254,67,model,model,has,last 12 transformer layers,model has last 12 transformer layers,0.5429863333702087
translation,254,131,model,adapting,has,pretrained text decoder,adapting has pretrained text decoder,0.5690075159072876
translation,254,131,model,model,Finetuning,encoder attention ( ea ) parameters,model Finetuning encoder attention ( ea ) parameters,0.6963356733322144
translation,254,86,results,proposed lna - minimalist,generalizes,better,proposed lna - minimalist generalizes better,0.7704762816429138
translation,254,86,results,lower perplexity,on,dev set,lower perplexity on dev set,0.5493868589401245
translation,254,86,results,both low data and high data regimes,has,proposed lna - minimalist,both low data and high data regimes has proposed lna - minimalist,0.6072664260864258
translation,254,86,results,better,has,lower perplexity,better has lower perplexity,0.5947751402854919
translation,254,86,results,substantially improves,has,training efficiency,substantially improves has training efficiency,0.5708115696907043
translation,254,86,results,results,show,both low data and high data regimes,results show both low data and high data regimes,0.6092071533203125
translation,254,86,results,results,in,both low data and high data regimes,results in both low data and high data regimes,0.523529589176178
translation,254,91,results,lna,demonstrates,improved generalization,lna demonstrates improved generalization,0.6704105138778687
translation,254,91,results,improved generalization,than,alternative finetuning approaches,improved generalization than alternative finetuning approaches,0.5705739259719849
translation,254,98,results,"lna finetuning ( lna - e , d )",achieves,better generalization,"lna finetuning ( lna - e , d ) achieves better generalization",0.6732953190803528
translation,254,98,results,better generalization,both for,seen and unseen languages,better generalization both for seen and unseen languages,0.6527998447418213
translation,254,98,results,more parameters,has,"lna finetuning ( lna - e , d )","more parameters has lna finetuning ( lna - e , d )",0.5498288869857788
translation,254,98,results,"lna finetuning ( lna - e , d )",has,trains,"lna finetuning ( lna - e , d ) has trains",0.5958705544471741
translation,254,98,results,trains,has,more than 2 ? faster,trains has more than 2 ? faster,0.6249155402183533
translation,254,103,results,zero-shot transfer capability,for,translating to new languages,zero-shot transfer capability for translating to new languages,0.6247487664222717
translation,254,103,results,zero-shot transfer capability,with,unsupervised translation,zero-shot transfer capability with unsupervised translation,0.6051308512687683
translation,254,103,results,unsupervised translation,for,english - japanese only,unsupervised translation for english - japanese only,0.6269917488098145
translation,254,103,results,1.3 bleu,behind,best supervised result,1.3 bleu behind best supervised result,0.6620137691497803
translation,254,103,results,english - japanese only,has,1.3 bleu,english - japanese only has 1.3 bleu,0.5364833474159241
translation,254,108,results,xmef - en,perform,very well,xmef - en perform very well,0.5982058644294739
translation,254,108,results,xmef - en,even surpassing,best cascade results,xmef - en even surpassing best cascade results,0.714898943901062
translation,254,108,results,very well,on,"romance , germanic and slavic language families","very well on romance , germanic and slavic language families",0.4995582103729248
translation,254,108,results,"romance , germanic and slavic language families",in,high- resource ( ? 100 hours training data ) and low-resource directions ( 7 ? 44 hours training data ),"romance , germanic and slavic language families in high- resource ( ? 100 hours training data ) and low-resource directions ( 7 ? 44 hours training data )",0.5066097378730774
translation,254,108,results,best cascade results,on,8 languages,best cascade results on 8 languages,0.5266284942626953
translation,254,109,results,multilingual model,improves,distant ( from english ) and extremely low resource ( mostly ? 5 hours training data ) languages,multilingual model improves distant ( from english ) and extremely low resource ( mostly ? 5 hours training data ) languages,0.6185621023178101
translation,254,109,results,results,has,multilingual model,results has multilingual model,0.5447034239768982
translation,254,110,results,crosslingual adaptation,from,xmef - en,crosslingual adaptation from xmef - en,0.5779083967208862
translation,254,110,results,crosslingual adaptation,outperforms,finetuning,crosslingual adaptation outperforms finetuning,0.6837074756622314
translation,254,110,results,xmef - en,to,speech input,xmef - en to speech input,0.5846232175827026
translation,254,110,results,speech input,of,other languages,speech input of other languages,0.5566191077232361
translation,254,110,results,"lna -e , d",finetuning,entire encoder,"lna -e , d finetuning entire encoder",0.702414870262146
translation,254,110,results,finetuning,by,0.7 bleu,finetuning by 0.7 bleu,0.5328617095947266
translation,254,110,results,entire model,by,0.7 bleu,entire model by 0.7 bleu,0.5652338266372681
translation,254,110,results,entire encoder,brings,+ 1.2 bleu,entire encoder brings + 1.2 bleu,0.5699621438980103
translation,254,110,results,crosslingual adaptation,has,"lna -e , d","crosslingual adaptation has lna -e , d",0.594729483127594
translation,254,110,results,xmef - en,has,"lna -e , d","xmef - en has lna -e , d",0.6439084410667419
translation,254,110,results,finetuning,has,entire model,finetuning has entire model,0.593836784362793
translation,254,110,results,results,For,crosslingual adaptation,results For crosslingual adaptation,0.5379130244255066
translation,254,111,results,finetuning xmef - x,achieves,best average bleu score,finetuning xmef - x achieves best average bleu score,0.649128794670105
translation,254,111,results,results,has,finetuning xmef - x,results has finetuning xmef - x,0.5572469234466553
translation,254,114,results,performance,applying,lna finetuning,performance applying lna finetuning,0.695807695388794
translation,254,114,results,lna finetuning,to,encoder only ( lna - e ),lna finetuning to encoder only ( lna - e ),0.5545802116394043
translation,254,114,results,lna finetuning,very close to,24.2 vs. 24.5 averaged bleu ),lna finetuning very close to 24.2 vs. 24.5 averaged bleu ),0.7099273800849915
translation,254,114,results,finetuning,has,entire model ( finetune all ),finetuning has entire model ( finetune all ),0.5883510708808899
translation,254,114,results,results,has,performance,results has performance,0.5972660779953003
translation,254,126,results,joint training,improves,results further,joint training improves results further,0.7395902276039124
translation,254,126,results,results further,by,another 0.6 bleu,results further by another 0.6 bleu,0.6131392121315002
translation,254,126,results,our zero-shot results,beats,+ 5.6 bleu ),our zero-shot results beats + 5.6 bleu ),0.6696321368217468
translation,254,126,results,our zero-shot results,beats,supervised many - to - many model,our zero-shot results beats supervised many - to - many model,0.7179707288742065
translation,254,126,results,supervised many - to - many model,on,28 pair-wise,supervised many - to - many model on 28 pair-wise,0.5472398400306702
translation,254,126,results,supervised many - to - many model,on,translation directions,supervised many - to - many model on translation directions,0.518419623374939
translation,254,126,results,one to many case (   en - x   ),has,joint training,one to many case (   en - x   ) has joint training,0.5881214737892151
translation,254,126,results,+ 5.6 bleu ),has,supervised many - to - many model,+ 5.6 bleu ) has supervised many - to - many model,0.5623945593833923
translation,254,126,results,results,In,one to many case (   en - x   ),results In one to many case (   en - x   ),0.5646097660064697
translation,254,132,results,single language pair,find,finetuning,single language pair find finetuning,0.595598042011261
translation,254,132,results,self attention ( + sa ) parameters,in,decoder,self attention ( + sa ) parameters in decoder,0.5005602240562439
translation,254,132,results,single language pair,has,downstream st task ( english - german ),single language pair has downstream st task ( english - german ),0.5534772276878357
translation,254,132,results,finetuning,has,self attention ( + sa ) parameters,finetuning has self attention ( + sa ) parameters,0.5758562088012695
translation,254,132,results,results,adapting to,single language pair,results adapting to single language pair,0.644062340259552
translation,255,90,hyperparameters,model,with,batch size,model with batch size,0.6316491365432739
translation,255,90,hyperparameters,model,accumulate,gradients,model accumulate gradients,0.6512214541435242
translation,255,90,hyperparameters,batch size,of,16,batch size of 16,0.6842944622039795
translation,255,90,hyperparameters,gradients,every,two batches,gradients every two batches,0.7070366144180298
translation,255,90,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,255,91,hyperparameters,learning rate,set to,3e - 4,learning rate set to 3e - 4,0.725951611995697
translation,255,91,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,255,92,hyperparameters,model,trained,20 epochs,model trained 20 epochs,0.7907857298851013
translation,255,92,hyperparameters,model,trained,30 epochs,model trained 30 epochs,0.7883689999580383
translation,255,92,hyperparameters,20 epochs,for,"aope , uabsa , and aste task","20 epochs for aope , uabsa , and aste task",0.5503939390182495
translation,255,92,hyperparameters,30 epochs,for,tasd task,30 epochs for tasd task,0.5256171822547913
translation,255,92,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,255,7,model,various absa tasks,in,unified generative framework,various absa tasks in unified generative framework,0.5045676827430725
translation,255,7,model,model,tackle,various absa tasks,model tackle various absa tasks,0.7015240788459778
translation,255,27,model,unified generative model,seamlessly adapted to,multiple tasks,unified generative model seamlessly adapted to multiple tasks,0.7041706442832947
translation,255,27,model,model,has,unified generative model,model has unified generative model,0.51792973279953
translation,255,28,model,annotation -style and extractionstyle modeling,to transform,original task,annotation -style and extractionstyle modeling to transform original task,0.7197547554969788
translation,255,97,results,our proposed methods,based on either,annotation -style or extraction -style modeling,our proposed methods based on either annotation -style or extraction -style modeling,0.6675086617469788
translation,255,97,results,our proposed methods,establish,new state - of - the - art results,our proposed methods establish new state - of - the - art results,0.5749536156654358
translation,255,97,results,new state - of - the - art results,in,almost all cases,new state - of - the - art results in almost all cases,0.5302949547767639
translation,255,97,results,results,noticeable that,our proposed methods,results noticeable that our proposed methods,0.6735867857933044
translation,255,98,results,rest15 dataset,for,aope task,rest15 dataset for aope task,0.558331310749054
translation,255,98,results,our method,on par with,previous best performance,our method on par with previous best performance,0.6131632924079895
translation,255,98,results,rest15 dataset,has,our method,rest15 dataset has our method,0.583564043045044
translation,255,98,results,aope task,has,our method,aope task has our method,0.5868739485740662
translation,255,99,results,tackling various absa tasks,with,proposed unified generative method,tackling various absa tasks with proposed unified generative method,0.6085982322692871
translation,255,99,results,proposed unified generative method,is,effective solution,proposed unified generative method is effective solution,0.5719950795173645
translation,255,99,results,results,shows,tackling various absa tasks,results shows tackling various absa tasks,0.5940951704978943
translation,255,100,results,our method,performs,especially well,our method performs especially well,0.6311126947402954
translation,255,100,results,especially well,on,aste and tasd tasks,especially well on aste and tasd tasks,0.5899572968482971
translation,255,100,results,previous best models,by,7.6 and 3.7 average f1 scores,previous best models by 7.6 and 3.7 average f1 scores,0.545375645160675
translation,255,100,results,proposed extraction -style method,has,outperforms,proposed extraction -style method has outperforms,0.6110876798629761
translation,255,100,results,outperforms,has,previous best models,outperforms has previous best models,0.601570725440979
translation,255,100,results,results,see that,our method,results see that our method,0.631170928478241
translation,256,183,ablation-analysis,learning rate and dropout rate,are,most significant factors,learning rate and dropout rate are most significant factors,0.5375127792358398
translation,256,183,ablation-analysis,ablation analysis,show,learning rate and dropout rate,ablation analysis show learning rate and dropout rate,0.6251949071884155
translation,256,219,ablation-analysis,auxiliary decoder ( aux ),effectively improves,performance,auxiliary decoder ( aux ) effectively improves performance,0.7379322648048401
translation,256,219,ablation-analysis,performance,by about,0.6 bleu scores,performance by about 0.6 bleu scores,0.59453946352005
translation,256,219,ablation-analysis,ablation analysis,show,auxiliary decoder ( aux ),ablation analysis show auxiliary decoder ( aux ),0.6646219491958618
translation,256,226,ablation-analysis,domain adaptation,on,glat,domain adaptation on glat,0.5619007349014282
translation,256,226,ablation-analysis,domain adaptation,gain,slight improvement,domain adaptation gain slight improvement,0.8013626337051392
translation,256,226,ablation-analysis,glat,gain,slight improvement,glat gain slight improvement,0.8064236044883728
translation,256,226,ablation-analysis,slight improvement,of,about 0.2,slight improvement of about 0.2,0.6095806956291199
translation,256,226,ablation-analysis,ablation analysis,has,domain adaptation,ablation analysis has domain adaptation,0.5336741209030151
translation,256,227,ablation-analysis,additional reranker,with,more diverse features,additional reranker with more diverse features,0.6227510571479797
translation,256,227,ablation-analysis,additional reranker,boost,performance,additional reranker boost performance,0.6956276893615723
translation,256,227,ablation-analysis,performance,by about,0.6,performance by about 0.6,0.6004191637039185
translation,256,18,baselines,parallel translation system,based on,glancing transformer,parallel translation system based on glancing transformer,0.6638781428337097
translation,256,66,experimental-setup,decoders,use,6 layers,decoders use 6 layers,0.6054306626319885
translation,256,66,experimental-setup,decoders,use,2 layers,decoders use 2 layers,0.6096401810646057
translation,256,66,experimental-setup,6 layers,for,original decoder,6 layers for original decoder,0.5839056968688965
translation,256,66,experimental-setup,2 layers,for,auxiliary decoder,2 layers for auxiliary decoder,0.6043517589569092
translation,256,66,experimental-setup,experimental setup,For,decoders,experimental setup For decoders,0.6076251864433289
translation,256,88,experimental-setup,pycld3 1 library,to filter,german ? english sentence pairs,pycld3 1 library to filter german ? english sentence pairs,0.628415048122406
translation,256,88,experimental-setup,german ? english sentence pairs,with,language likelihood,german ? english sentence pairs with language likelihood,0.6213012933731079
translation,256,88,experimental-setup,german ? english sentence pairs,with,language ratio,german ? english sentence pairs with language ratio,0.6305491924285889
translation,256,88,experimental-setup,language likelihood,greater than,0.8,language likelihood greater than 0.8,0.6681748628616333
translation,256,88,experimental-setup,language ratio,has,greater than 60 %,language ratio has greater than 60 %,0.5664764642715454
translation,256,88,experimental-setup,experimental setup,use,pycld3 1 library,experimental setup use pycld3 1 library,0.6168771386146545
translation,256,117,experimental-setup,implementations,in,"fairseq ( ott et al. , 2019 )","implementations in fairseq ( ott et al. , 2019 )",0.46843814849853516
translation,256,117,experimental-setup,experimental setup,use,implementations,experimental setup use implementations,0.6093301177024841
translation,256,118,experimental-setup,experimental setup,trained with,"adam optimizer ( kingma and ba , 2014 )","experimental setup trained with adam optimizer ( kingma and ba , 2014 )",0.7422086000442505
translation,256,119,experimental-setup,inverse sqrt learning rate scheduler,with,4000 warm - up steps,inverse sqrt learning rate scheduler with 4000 warm - up steps,0.6469454169273376
translation,256,119,experimental-setup,inverse sqrt learning rate scheduler,set,maximum learning rate,inverse sqrt learning rate scheduler set maximum learning rate,0.6708633303642273
translation,256,119,experimental-setup,experimental setup,use,inverse sqrt learning rate scheduler,experimental setup use inverse sqrt learning rate scheduler,0.6181721091270447
translation,256,121,experimental-setup,multiple gpus,during,training,multiple gpus during training,0.6865067481994629
translation,256,121,experimental-setup,multiple gpus,resulting in,approximate total effective batch size,multiple gpus resulting in approximate total effective batch size,0.640750527381897
translation,256,121,experimental-setup,approximate total effective batch size,of,128 k tokens,approximate total effective batch size of 128 k tokens,0.5797133445739746
translation,256,121,experimental-setup,experimental setup,use,multiple gpus,experimental setup use multiple gpus,0.5737535357475281
translation,256,122,experimental-setup,training,employ,label smoothing,training employ label smoothing,0.5876585841178894
translation,256,122,experimental-setup,training,employ,"set dropout rate ( srivastava et al. , 2014 )","training employ set dropout rate ( srivastava et al. , 2014 )",0.5064201951026917
translation,256,122,experimental-setup,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,256,122,experimental-setup,"al. , 2016 )",of,0.1,"al. , 2016 ) of 0.1",0.5144288539886475
translation,256,122,experimental-setup,"set dropout rate ( srivastava et al. , 2014 )",to,0.3,"set dropout rate ( srivastava et al. , 2014 ) to 0.3",0.543683648109436
translation,256,122,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,256,133,experimental-setup,learning rate,to,1e - 4,learning rate to 1e - 4,0.5727717280387878
translation,256,133,experimental-setup,1e - 4,without,learning rate scheduler,1e - 4 without learning rate scheduler,0.7436327338218689
translation,256,133,experimental-setup,max tokens per batch,as,4096,max tokens per batch as 4096,0.5830649733543396
translation,256,133,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,256,133,experimental-setup,experimental setup,set,max tokens per batch,experimental setup set max tokens per batch,0.6440370678901672
translation,256,162,experimental-setup,adam optimizer,with,decoupled weight decay,adam optimizer with decoupled weight decay,0.5940178036689758
translation,256,162,experimental-setup,experimental setup,trained with,adam optimizer,experimental setup trained with adam optimizer,0.7210889458656311
translation,256,163,experimental-setup,inverse sqrt learning rate scheduler,with,4000 warm - up steps,inverse sqrt learning rate scheduler with 4000 warm - up steps,0.6469454169273376
translation,256,163,experimental-setup,inverse sqrt learning rate scheduler,set,maximum learning rate,inverse sqrt learning rate scheduler set maximum learning rate,0.6708633303642273
translation,256,163,experimental-setup,experimental setup,use,inverse sqrt learning rate scheduler,experimental setup use inverse sqrt learning rate scheduler,0.6181721091270447
translation,256,181,experimental-setup,domain adaptation,perform,grid search,domain adaptation perform grid search,0.5896520018577576
translation,256,181,experimental-setup,grid search,on,four group of hyper-parameters,grid search on four group of hyper-parameters,0.5555758476257324
translation,256,181,experimental-setup,dropout,has,"0.0 , 0.1 , 0.3 )","dropout has 0.0 , 0.1 , 0.3 )",0.5584452152252197
translation,256,181,experimental-setup,experimental setup,For,domain adaptation,experimental setup For domain adaptation,0.5428394079208374
translation,256,196,experimental-setup,kbmira,to re-rank,hypotheses,kbmira to re-rank hypotheses,0.7924097776412964
translation,256,197,experimental-setup,candidates,via,various search algorithm,candidates via various search algorithm,0.6837481260299683
translation,256,197,experimental-setup,experimental setup,train,glat model variants,experimental setup train glat model variants,0.6765139698982239
translation,256,209,experimental-setup,our models,with,"fairseq ( ott et al. , 2019 )","our models with fairseq ( ott et al. , 2019 )",0.6154308915138245
translation,256,209,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,256,211,experimental-setup,number of tokens per batch,set to,256k,number of tokens per batch set to 256k,0.683854877948761
translation,256,211,experimental-setup,experimental setup,has,number of tokens per batch,experimental setup has number of tokens per batch,0.5598893761634827
translation,256,212,experimental-setup,dropout rate,set to,0.3,dropout rate set to 0.3,0.6678073406219482
translation,256,212,experimental-setup,0.3,for,first 100k steps,0.3 for first 100k steps,0.60997474193573
translation,256,212,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,256,220,experimental-setup,glat - base + ctc,set,max output length,glat - base + ctc set max output length,0.7152303457260132
translation,256,220,experimental-setup,max output length,to,twice,max output length to twice,0.5398691296577454
translation,256,220,experimental-setup,max output length,to,repeated tokens,max output length to repeated tokens,0.5231466293334961
translation,256,220,experimental-setup,max output length,remove,blanks,max output length remove blanks,0.7250062823295593
translation,256,220,experimental-setup,max output length,remove,repeated tokens,max output length remove repeated tokens,0.6884568333625793
translation,256,220,experimental-setup,repeated tokens,after,generation,repeated tokens after generation,0.6791129112243652
translation,256,220,experimental-setup,twice,has,source input length,twice has source input length,0.5696010589599609
translation,256,220,experimental-setup,experimental setup,For,glat - base + ctc,experimental setup For glat - base + ctc,0.6048200726509094
translation,256,90,experiments,"fast align ( dyer et al. , 2013 )",to automatically learn,german ? english word alignment,"fast align ( dyer et al. , 2013 ) to automatically learn german ? english word alignment",0.643197238445282
translation,256,90,experiments,german ? english word alignment,on,coarsely filtered corpus,german ? english word alignment on coarsely filtered corpus,0.510647177696228
translation,256,107,experiments,truecasing,use,moses truecaser,truecasing use moses truecaser,0.6118867993354797
translation,256,107,experiments,moses truecaser,to learn and apply,truecasing,moses truecaser to learn and apply truecasing,0.6834766864776611
translation,256,107,experiments,truecasing,on,all datasets,truecasing on all datasets,0.5378092527389526
translation,256,210,experiments,4 machines,with,8 nvidia v100 gpus,4 machines with 8 nvidia v100 gpus,0.5885886549949646
translation,256,5,model,"parallel ( i.e. , non-autoregressive ) translation system",using,glancing transformer,"parallel ( i.e. , non-autoregressive ) translation system using glancing transformer",0.6872544288635254
translation,256,5,model,"parallel ( i.e. , non-autoregressive ) translation system",enables,fast and accurate parallel decoding,"parallel ( i.e. , non-autoregressive ) translation system enables fast and accurate parallel decoding",0.6220356822013855
translation,256,5,model,model,build,"parallel ( i.e. , non-autoregressive ) translation system","model build parallel ( i.e. , non-autoregressive ) translation system",0.6828804016113281
translation,256,19,model,"dynamic linear combination of layers ( dlcl ,",for training,deep models,"dynamic linear combination of layers ( dlcl , for training deep models",0.7423967719078064
translation,256,20,model,data exploitation,filter,data,data exploitation filter data,0.7776284217834473
translation,256,20,model,data,with,multiple strategies,data with multiple strategies,0.6686840653419495
translation,256,20,model,model,For,data exploitation,model For data exploitation,0.6156125068664551
translation,256,69,model,glat - wide,expand,dimension,glat - wide expand dimension,0.6786508560180664
translation,256,69,model,dimension,of,feed -forward inner layer,dimension of feed -forward inner layer,0.5980232954025269
translation,256,69,model,feed -forward inner layer,to construct,glat - wide,feed -forward inner layer to construct glat - wide,0.6820122003555298
translation,256,69,model,model,expand,dimension,model expand dimension,0.6483131647109985
translation,256,69,model,model,has,glat - wide,model has glat - wide,0.5558593273162842
translation,256,91,model,model,has,fine-grained filtering,model has fine-grained filtering,0.5825544595718384
translation,256,108,model,subword segmentation,use,"proposed volt ( xu et al. , 2021 )","subword segmentation use proposed volt ( xu et al. , 2021 )",0.5320981740951538
translation,256,108,model,tokens,into,subwords,tokens into subwords,0.5855936408042908
translation,256,108,model,subwords,resulting in,joint vocabulary,subwords resulting in joint vocabulary,0.6542139053344727
translation,256,108,model,joint vocabulary,of,size,joint vocabulary of size,0.6093721985816956
translation,256,108,model,size,of,12 k subwords,size of 12 k subwords,0.6329366564750671
translation,256,108,model,model,has,subword segmentation,model has subword segmentation,0.5395278334617615
translation,256,116,model,at models,are,transformer models,at models are transformer models,0.6009248495101929
translation,256,116,model,transformer models,with,12 layers of encoder and decoder,transformer models with 12 layers of encoder and decoder,0.6281738877296448
translation,256,116,model,model,has,at models,model has at models,0.637887716293335
translation,256,160,model,model,on,large-scale distilled data,model on large-scale distilled data,0.5172619223594666
translation,256,160,model,model,on,small-scale in - domain data,model on small-scale in - domain data,0.5394812226295471
translation,256,160,model,model,until,convergence,model until convergence,0.7085715532302856
translation,256,160,model,model,on,small-scale in - domain data,model on small-scale in - domain data,0.5394812226295471
translation,256,160,model,convergence,finetune,model,convergence finetune model,0.7461626529693604
translation,256,160,model,model,on,small-scale in - domain data,model on small-scale in - domain data,0.5394812226295471
translation,256,160,model,model,After training,model,model After training model,0.6906313300132751
translation,256,222,model,glat,employ,three technologies,glat employ three technologies,0.6104072332382202
translation,256,222,model,three technologies,to improve,continuously training,three technologies to improve continuously training,0.6947139501571655
translation,256,222,model,continuously training,on,cycle kd data,continuously training on cycle kd data,0.5953128933906555
translation,256,222,model,continuously training,on,domain adaptation,continuously training on domain adaptation,0.572526752948761
translation,256,222,model,continuously training,on,reranking,continuously training on reranking,0.5723216533660889
translation,256,222,model,reranking,with,various features,reranking with various features,0.6333903670310974
translation,256,222,model,model,Based on,glat,model Based on glat,0.6466267704963684
translation,256,24,results,parallel translation system,surpasses,autoregressive models,parallel translation system surpasses autoregressive models,0.6346099376678467
translation,256,24,results,parallel translation system,achieves,highest bleu score ( 35.0 ),parallel translation system achieves highest bleu score ( 35.0 ),0.6420833468437195
translation,256,24,results,highest bleu score ( 35.0 ),in,german ? english translation task,highest bleu score ( 35.0 ) in german ? english translation task,0.4654058516025543
translation,256,120,results,betas,are,"( 0.9 , 0.98 )","betas are ( 0.9 , 0.98 )",0.5042093992233276
translation,256,120,results,results,has,betas,results has betas,0.5432360172271729
translation,256,184,results,dropout,set to,0,dropout set to 0,0.7171401977539062
translation,256,184,results,dropout,effectiveness of,over-fitting,dropout effectiveness of over-fitting,0.6772708296775818
translation,256,184,results,performance,is,surprisingly great,performance is surprisingly great,0.5993342399597168
translation,256,184,results,performance,effectiveness of,over-fitting,performance effectiveness of over-fitting,0.6874105930328369
translation,256,184,results,surprisingly great,effectiveness of,over-fitting,surprisingly great effectiveness of over-fitting,0.6332833766937256
translation,256,184,results,over-fitting,on,indomain dataset,over-fitting on indomain dataset,0.532749593257904
translation,256,184,results,dropout,has,performance,dropout has performance,0.5515318512916565
translation,256,184,results,0,has,performance,0 has performance,0.62660151720047
translation,256,184,results,results,when,dropout,results when dropout,0.6083375215530396
translation,256,217,results,usage,has,no positive effect,usage has no positive effect,0.5878819823265076
translation,256,217,results,raw data,has,no positive effect,raw data has no positive effect,0.5642099976539612
translation,256,217,results,results,experiment with,various utilization of raw data,results experiment with various utilization of raw data,0.6991804242134094
translation,256,217,results,results,show,usage,results show usage,0.5784894227981567
translation,256,224,results,distilled target monolingual data,further improve,performance,distilled target monolingual data further improve performance,0.6346523761749268
translation,256,224,results,performance,by,0.3 bleu scores,performance by 0.3 bleu scores,0.5349496603012085
translation,256,224,results,performance,about,0.3 bleu scores,performance about 0.3 bleu scores,0.5401179194450378
translation,256,224,results,results,Training on,distilled target monolingual data,results Training on distilled target monolingual data,0.6668232679367065
translation,257,80,ablation-analysis,effectiveness,of,aspect propagation ( ap ),effectiveness of aspect propagation ( ap ),0.568048894405365
translation,257,80,ablation-analysis,effectiveness,of,opinion propagation ( op ),effectiveness of opinion propagation ( op ),0.5655931234359741
translation,257,80,ablation-analysis,effectiveness,of,type-specific masked term discrimination ( tsmtd ),effectiveness of type-specific masked term discrimination ( tsmtd ),0.5946858525276184
translation,257,80,ablation-analysis,effectiveness,of,pairwise relations discrimination ( prd ),effectiveness of pairwise relations discrimination ( prd ),0.5791494250297546
translation,257,83,ablation-analysis,ap,more effective than,op,ap more effective than op,0.6057783365249634
translation,257,83,ablation-analysis,ap,scores,drop significantly,ap scores drop significantly,0.744132399559021
translation,257,83,ablation-analysis,drop significantly,not utilizing,ap and op,drop significantly not utilizing ap and op,0.7449182868003845
translation,257,8,model,deep contextualized relation - aware network ( dcran ),allows,interactive relations,deep contextualized relation - aware network ( dcran ) allows interactive relations,0.6636036038398743
translation,257,8,model,interactive relations,among,subtasks,interactive relations among subtasks,0.5775008797645569
translation,257,8,model,interactive relations,with,deep contextual information,interactive relations with deep contextual information,0.6151834726333618
translation,257,8,model,deep contextual information,based on,"two modules ( i.e. , aspect and opinion propagation","deep contextual information based on two modules ( i.e. , aspect and opinion propagation",0.6099827885627747
translation,257,8,model,model,propose,deep contextualized relation - aware network ( dcran ),model propose deep contextualized relation - aware network ( dcran ),0.6745883226394653
translation,257,9,model,novel self-supervised strategies,for,absa,novel self-supervised strategies for absa,0.667734682559967
translation,257,9,model,model,design,novel self-supervised strategies,model design novel self-supervised strategies,0.6161266565322876
translation,257,12,model,model,has,absa,model has absa,0.6399075984954834
translation,257,24,model,deep contextualized relation - aware network ( dcran ),for,absa,deep contextualized relation - aware network ( dcran ) for absa,0.6306443810462952
translation,257,24,model,model,propose,deep contextualized relation - aware network ( dcran ),model propose deep contextualized relation - aware network ( dcran ),0.6745883226394653
translation,257,25,model,dcran,allows,interactive relations,dcran allows interactive relations,0.6980405449867249
translation,257,25,model,dcran,explicitly considers,relations,dcran explicitly considers relations,0.7635502219200134
translation,257,25,model,interactive relations,among,subtasks,interactive relations among subtasks,0.5775008797645569
translation,257,25,model,subtasks,of,absa,subtasks of absa,0.6255590319633484
translation,257,25,model,relations,by using,contextual information,relations by using contextual information,0.5998806953430176
translation,257,25,model,model,has,dcran,model has dcran,0.6273421049118042
translation,257,74,results,dcran - bert base,shows,slightly lower absa - f1 scores,dcran - bert base shows slightly lower absa - f1 scores,0.6907391548156738
translation,257,74,results,slightly lower absa - f1 scores,than,previous state - of - the - art methods,slightly lower absa - f1 scores than previous state - of - the - art methods,0.5603659152984619
translation,257,74,results,results,observe that,dcran - bert base,results observe that dcran - bert base,0.6041905283927917
translation,257,76,results,previous state - of - the - art methods,in,all metrics,previous state - of - the - art methods in all metrics,0.4783291220664978
translation,257,76,results,dcran -bert large,has,significantly outperforms,dcran -bert large has significantly outperforms,0.625761866569519
translation,257,76,results,significantly outperforms,has,previous state - of - the - art methods,significantly outperforms has previous state - of - the - art methods,0.5471373200416565
translation,257,76,results,results,has,dcran -bert large,results has dcran -bert large,0.596450686454773
translation,257,77,results,electra based models,outperform,bert based models,electra based models outperform bert based models,0.687626838684082
translation,257,78,results,dcran -electra large,achieves,absolute gains,dcran -electra large achieves absolute gains,0.6985651850700378
translation,257,78,results,absolute gains,over,previous state - of - the - art results,absolute gains over previous state - of - the - art results,0.6286671757698059
translation,257,78,results,absolute gains,by,"5.5 % , 4.4 % , and 7.6 %","absolute gains by 5.5 % , 4.4 % , and 7.6 %",0.5478235483169556
translation,257,78,results,"5.5 % , 4.4 % , and 7.6 %",in,absa - f1,"5.5 % , 4.4 % , and 7.6 % in absa - f1",0.557400107383728
translation,257,78,results,absa - f1,on,"lap14 , rest14 , and rest15 datasets","absa - f1 on lap14 , rest14 , and rest15 datasets",0.5430054664611816
translation,257,78,results,results,has,dcran -electra large,results has dcran -electra large,0.588876485824585
translation,257,84,results,explicit self-supervised strategies,observe that,prd,explicit self-supervised strategies observe that prd,0.5961408019065857
translation,257,84,results,prd,is,more effective,prd is more effective,0.5989863872528076
translation,257,84,results,more effective,than,tsmtd,more effective than tsmtd,0.6243973970413208
translation,257,84,results,results,In the case of,explicit self-supervised strategies,results In the case of explicit self-supervised strategies,0.6387255191802979
translation,257,91,results,improve,not sufficient for inducing,performance improvement,improve not sufficient for inducing performance improvement,0.6511027812957764
translation,257,91,results,performance,not sufficient for inducing,performance improvement,performance not sufficient for inducing performance improvement,0.6563690900802612
translation,257,91,results,performance,for,multiple -aspect case,performance for multiple -aspect case,0.6192095279693604
translation,257,91,results,performance improvement,for,multiple -aspect case,performance improvement for multiple -aspect case,0.5957794189453125
translation,257,91,results,improve,has,performance,improve has performance,0.5578044652938843
translation,257,93,results,explicit self -supervised strategies,show,absolute absa - f1 improvements,explicit self -supervised strategies show absolute absa - f1 improvements,0.6385597586631775
translation,257,93,results,absolute absa - f1 improvements,of,0.97 % ( 80.22 % ? 81.19 % ),absolute absa - f1 improvements of 0.97 % ( 80.22 % ? 81.19 % ),0.523548424243927
translation,257,93,results,absolute absa - f1 improvements,of,3.04 % ( 65.16 % ? 68.20 ),absolute absa - f1 improvements of 3.04 % ( 65.16 % ? 68.20 ),0.5182569026947021
translation,257,93,results,3.04 % ( 65.16 % ? 68.20 ),on,rest14 and rest15 datasets,3.04 % ( 65.16 % ? 68.20 ) on rest14 and rest15 datasets,0.5366441607475281
translation,257,93,results,multiple -aspect,has,explicit self -supervised strategies,multiple -aspect has explicit self -supervised strategies,0.5765965580940247
translation,257,93,results,results,In the case of,multiple -aspect,results In the case of multiple -aspect,0.6638724207878113
translation,257,95,results,performance gain,by,explicit self -supervised strategies,performance gain by explicit self -supervised strategies,0.5860220789909363
translation,257,95,results,explicit self -supervised strategies,mostly derived from,multiple -aspect cases ( + 0.97 % ),explicit self -supervised strategies mostly derived from multiple -aspect cases ( + 0.97 % ),0.6767498850822449
translation,257,99,results,sentence - level accuracy,of,multiple - aspect,sentence - level accuracy of multiple - aspect,0.5613803863525391
translation,257,99,results,multiple - aspect,lower than,single -aspect,multiple - aspect lower than single -aspect,0.716924786567688
translation,257,99,results,absa - f1,has,sentence - level accuracy,absa - f1 has sentence - level accuracy,0.5401947498321533
translation,257,99,results,results,Unlike,absa - f1,results Unlike absa - f1,0.6254240274429321
translation,257,100,results,our explicit self -supervised strategies,leads,significant sentencelevel accuracy improvements,our explicit self -supervised strategies leads significant sentencelevel accuracy improvements,0.6324208974838257
translation,257,100,results,significant sentencelevel accuracy improvements,of,2.54 % ( 61.70 % ? 64.24 % ),significant sentencelevel accuracy improvements of 2.54 % ( 61.70 % ? 64.24 % ),0.5187636613845825
translation,257,100,results,significant sentencelevel accuracy improvements,of,3.74 % ( 48.60 % ? 52.34 % ),significant sentencelevel accuracy improvements of 3.74 % ( 48.60 % ? 52.34 % ),0.5204637050628662
translation,257,100,results,3.74 % ( 48.60 % ? 52.34 % ),on,rest14 and rest15 datasets,3.74 % ( 48.60 % ? 52.34 % ) on rest14 and rest15 datasets,0.5409592986106873
translation,257,100,results,multiple -aspect,has,our explicit self -supervised strategies,multiple -aspect has our explicit self -supervised strategies,0.6009963750839233
translation,257,100,results,results,In the case of,multiple -aspect,results In the case of multiple -aspect,0.6638724207878113
translation,257,101,results,only small improvements,in,sentence - level accuracy,only small improvements in sentence - level accuracy,0.46531233191490173
translation,257,101,results,sentence - level accuracy,on,both datasets,sentence - level accuracy on both datasets,0.41935089230537415
translation,257,101,results,sentence,contains,single-aspect,sentence contains single-aspect,0.6465445160865784
translation,257,101,results,results,observe,only small improvements,results observe only small improvements,0.5796152949333191
translation,258,105,baselines,two state - of- the - art nat models,that uses,"conditional mask lm ( devlin et al. , 2019 )","two state - of- the - art nat models that uses conditional mask lm ( devlin et al. , 2019 )",0.6160016655921936
translation,258,105,baselines,al. 2019 ),that uses,"conditional mask lm ( devlin et al. , 2019 )","al. 2019 ) that uses conditional mask lm ( devlin et al. , 2019 )",0.6589344143867493
translation,258,105,baselines,"conditional mask lm ( devlin et al. , 2019 )",to iteratively generate,target sequence,"conditional mask lm ( devlin et al. , 2019 ) to iteratively generate target sequence",0.7076724171638489
translation,258,105,baselines,target sequence,from,masked input,target sequence from masked input,0.597116231918335
translation,258,105,baselines,two state - of- the - art nat models,has,mask -predict,two state - of- the - art nat models has mask -predict,0.5912403464317322
translation,258,101,experimental-setup,all data,via,"bpe ( sennrich et al. , 2016 )","all data via bpe ( sennrich et al. , 2016 )",0.6019813418388367
translation,258,101,experimental-setup,"bpe ( sennrich et al. , 2016 )",with,32 k merge operations,"bpe ( sennrich et al. , 2016 ) with 32 k merge operations",0.5921318531036377
translation,258,101,experimental-setup,experimental setup,preprocess,all data,experimental setup preprocess all data,0.7040598392486572
translation,258,106,experimental-setup,optimal settings,to keep,iteration number,optimal settings to keep iteration number,0.6074515581130981
translation,258,106,experimental-setup,optimal settings,to keep,length beam,optimal settings to keep length beam,0.6021082997322083
translation,258,106,experimental-setup,iteration number,as,10,iteration number as 10,0.6255693435668945
translation,258,106,experimental-setup,length beam,as,5,length beam as 5,0.591061532497406
translation,258,106,experimental-setup,experimental setup,followed,optimal settings,experimental setup followed optimal settings,0.6572604179382324
translation,258,125,experiments,our method,improves,nat model,our method improves nat model,0.7110363841056824
translation,258,125,experiments,nat model,by,+ 0.7 bleu point,nat model by + 0.7 bleu point,0.5845829248428345
translation,258,125,experiments,nat model,on average,+ 0.7 bleu point,nat model on average + 0.7 bleu point,0.6764322519302368
translation,258,125,experiments,+ 0.7 bleu point,with,+ 2.2 alf,+ 0.7 bleu point with + 2.2 alf,0.6645680665969849
translation,258,125,experiments,+ 0.7 bleu point,on average,+ 2.2 alf,+ 0.7 bleu point on average + 2.2 alf,0.7247446775436401
translation,258,125,experiments,long- distance language pair ja-en,has,our method,long- distance language pair ja-en has our method,0.5473196506500244
translation,258,29,model,most,of,authentic and synthetic data,most of authentic and synthetic data,0.5753036141395569
translation,258,29,model,most,combine,three complementary approaches,most combine three complementary approaches,0.7154643535614014
translation,258,29,model,authentic and synthetic data,combine,three complementary approaches,authentic and synthetic data combine three complementary approaches,0.6892825961112976
translation,258,29,model,training strategy,for further boosting,nat performance,training strategy for further boosting nat performance,0.6749275922775269
translation,258,29,model,model,To make,most,model To make most,0.7583377361297607
translation,258,29,model,model,To make,authentic and synthetic data,model To make authentic and synthetic data,0.6498785018920898
translation,258,29,model,model,of,authentic and synthetic data,model of authentic and synthetic data,0.6058974266052246
translation,258,12,results,our approach,achieves,28.2 and 33.9 bleu points,our approach achieves 28.2 and 33.9 bleu points,0.6311517953872681
translation,258,12,results,28.2 and 33.9 bleu points,on,wmt14 english - german and wmt16 romanian - english datasets,28.2 and 33.9 bleu points on wmt14 english - german and wmt16 romanian - english datasets,0.5140636563301086
translation,258,12,results,results,has,our approach,results has our approach,0.6050099730491638
translation,258,87,results,more s ? t lfw links,with,more accurate alignment,more s ? t lfw links with more accurate alignment,0.636396050453186
translation,258,87,results,more s ? t lfw links,has,73.4 vs. 66.4 ),more s ? t lfw links has 73.4 vs. 66.4 ),0.5618553757667542
translation,258,87,results,more accurate alignment,has,89.2 vs. 73.3 ),more accurate alignment has 89.2 vs. 73.3 ),0.5197011232376099
translation,258,87,results,results,Compared with,raw,results Compared with raw,0.6119108200073242
translation,258,116,results,translation performance ( bleu ? ),across,different language pairs and nat models,translation performance ( bleu ? ) across different language pairs and nat models,0.6702710390090942
translation,258,116,results,significantly and consistently improves,has,translation performance ( bleu ? ),significantly and consistently improves has translation performance ( bleu ? ),0.6077939867973328
translation,258,116,results,results,Compared with,standard nat models,results Compared with standard nat models,0.6106742024421692
translation,258,117,results,improvements,on,translation performance,improvements on translation performance,0.5342718362808228
translation,258,117,results,translation performance,mainly due to,increase,translation performance mainly due to increase,0.6417319774627686
translation,258,117,results,increase,of,translation accuracy,increase of translation accuracy,0.5505918860435486
translation,258,117,results,translation accuracy,on,low-frequency words ( alf ? ),translation accuracy on low-frequency words ( alf ? ),0.5090406537055969
translation,258,117,results,results,has,improvements,results has improvements,0.615561842918396
translation,258,118,results,standard mask - predict model,by,+ 0.8 bleu score,standard mask - predict model by + 0.8 bleu score,0.5361027717590332
translation,258,118,results,+ 0.8 bleu score,with,substantial + 3.6 increase,+ 0.8 bleu score with substantial + 3.6 increase,0.665711522102356
translation,258,118,results,substantial + 3.6 increase,in,alf score,substantial + 3.6 increase in alf score,0.5304991602897644
translation,258,118,results,significantly improves,has,standard mask - predict model,significantly improves has standard mask - predict model,0.5902910828590393
translation,258,119,results,our approach,push,existing nat models,our approach push existing nat models,0.7824109196662903
translation,258,119,results,existing nat models,to achieve,new sota performances,existing nat models to achieve new sota performances,0.6774564385414124
translation,258,119,results,new sota performances,i.e.,28.2 and 33.9 bleu,new sota performances i.e. 28.2 and 33.9 bleu,0.6365812420845032
translation,258,119,results,28.2 and 33.9 bleu,on,en-de and ro-en,28.2 and 33.9 bleu on en-de and ro-en,0.6142928600311279
translation,258,119,results,results,has,our approach,results has our approach,0.6050099730491638
translation,258,123,results,translation quality,in,all cases,translation quality in all cases,0.5280786752700806
translation,258,123,results,baselines,has,our method,baselines has our method,0.5691280364990234
translation,258,123,results,our method,has,significantly and incrementally improves,our method has significantly and incrementally improves,0.5510399341583252
translation,258,123,results,significantly and incrementally improves,has,translation quality,significantly and incrementally improves has translation quality,0.5843620300292969
translation,258,123,results,results,Compared with,baselines,results Compared with baselines,0.6914018392562866
translation,258,124,results,lfr,achieves,+ 0.8 bleu improvement,lfr achieves + 0.8 bleu improvement,0.6561566591262817
translation,258,124,results,lfr,on average,+ 0.8 bleu improvement,lfr on average + 0.8 bleu improvement,0.6536937952041626
translation,258,124,results,+ 0.8 bleu improvement,over,traditional training,+ 0.8 bleu improvement over traditional training,0.6054494976997375
translation,258,124,results,on average + 3.0 % accuracy,on,low-frequency word translation,on average + 3.0 % accuracy on low-frequency word translation,0.4923549294471741
translation,258,124,results,zh-en,has,lfr,zh-en has lfr,0.6746771335601807
translation,258,124,results,increasing,has,on average + 3.0 % accuracy,increasing has on average + 3.0 % accuracy,0.6050114631652832
translation,258,124,results,results,For,zh-en,results For zh-en,0.6361060738563538
translation,258,126,results,nat models,with,proposed training strategy,nat models with proposed training strategy,0.6384390592575073
translation,258,126,results,nat models,perform,closely,nat models perform closely,0.6584376096725464
translation,258,126,results,proposed training strategy,perform,closely,proposed training strategy perform closely,0.6200680136680603
translation,258,126,results,closely,to,at teachers ( i.e. 0.2 ? bleu ),closely to at teachers ( i.e. 0.2 ? bleu ),0.6012136340141296
translation,258,126,results,results,has,nat models,results has nat models,0.47988003492355347
translation,258,136,results,translation quality,by rejuvenating,low-frequency words,translation quality by rejuvenating low-frequency words,0.6848295331001282
translation,258,136,results,low-frequency words,to,certain extent,low-frequency words to certain extent,0.5502916574478149
translation,258,162,results,at models,problem of,low-frequency words,at models problem of low-frequency words,0.7557754516601562
translation,258,162,results,low-frequency words,when using,knowledge distillation,low-frequency words when using knowledge distillation,0.7068473100662231
translation,258,162,results,results,has,at models,results has at models,0.5829153060913086
translation,259,145,ablation-analysis,source monotext loss,plays,more critical role,source monotext loss plays more critical role,0.7462726831436157
translation,259,145,ablation-analysis,more critical role,than,target monotext loss,more critical role than target monotext loss,0.5615754127502441
translation,259,145,ablation-analysis,ablation analysis,found that,source monotext loss,ablation analysis found that source monotext loss,0.6346020698547363
translation,259,96,baselines,english sentences,from,generic monolingual pool,english sentences from generic monolingual pool,0.5582829117774963
translation,259,96,baselines,english sentences,treat them as,in-domain sentences,english sentences treat them as in-domain sentences,0.5738657116889954
translation,259,5,experiments,cross-lingual data selection method,to extract,in- domain sentences,cross-lingual data selection method to extract in- domain sentences,0.6836869120597839
translation,259,5,experiments,in- domain sentences,in,missing language side,in- domain sentences in missing language side,0.5309805274009705
translation,259,5,experiments,missing language side,from,large generic monolingual corpus,missing language side from large generic monolingual corpus,0.5456274151802063
translation,259,89,experiments,10m english sentences,from,newcrawl 2007 - 2019,10m english sentences from newcrawl 2007 - 2019,0.6196339130401611
translation,259,89,experiments,10m english sentences,as,generic monolingual corpus,10m english sentences as generic monolingual corpus,0.5013238787651062
translation,259,89,experiments,newcrawl 2007 - 2019,as,generic monolingual corpus,newcrawl 2007 - 2019 as generic monolingual corpus,0.5147879719734192
translation,259,108,hyperparameters,adaptive layer,is,2 - layer feed - forward network,adaptive layer is 2 - layer feed - forward network,0.592726469039917
translation,259,108,hyperparameters,2 - layer feed - forward network,with,hidden size 128,2 - layer feed - forward network with hidden size 128,0.6325090527534485
translation,259,108,hyperparameters,hyperparameters,has,adaptive layer,hyperparameters has adaptive layer,0.5410261750221252
translation,259,109,hyperparameters,temperature parameter,in,contrastive loss,temperature parameter in contrastive loss,0.5400099158287048
translation,259,109,hyperparameters,contrastive loss,to,0.2,contrastive loss to 0.2,0.5595442652702332
translation,259,109,hyperparameters,hyperparameters,set,temperature parameter,hyperparameters set temperature parameter,0.6121401786804199
translation,259,110,hyperparameters,adaptive layer,using,adam optimiser,adaptive layer using adam optimiser,0.7095268964767456
translation,259,110,hyperparameters,adam optimiser,with,learning rate 1e - 5,adam optimiser with learning rate 1e - 5,0.636258602142334
translation,259,110,hyperparameters,adam optimiser,with,batch size,adam optimiser with batch size,0.6032729744911194
translation,259,110,hyperparameters,adam optimiser,with,up to 20 epochs,adam optimiser with up to 20 epochs,0.6254956722259521
translation,259,110,hyperparameters,batch size,of,64 sentences,batch size of 64 sentences,0.5814248919487
translation,259,110,hyperparameters,up to 20 epochs,with,early stopping,up to 20 epochs with early stopping,0.6291678547859192
translation,259,110,hyperparameters,early stopping,if,no improvement,early stopping if no improvement,0.5860337018966675
translation,259,110,hyperparameters,no improvement,for,5 epochs,no improvement for 5 epochs,0.6446237564086914
translation,259,110,hyperparameters,no improvement,loss of,dev set,no improvement loss of dev set,0.6910150051116943
translation,259,110,hyperparameters,dev set,in,old domain,dev set in old domain,0.5484442710876465
translation,259,110,hyperparameters,hyperparameters,train,adaptive layer,hyperparameters train adaptive layer,0.6829691529273987
translation,259,112,hyperparameters,transformer,as,nmt model,transformer as nmt model,0.5636608004570007
translation,259,112,hyperparameters,transformer,set,"mixing hyperparameters ? 1 , ? 2 , ? 3 to 1","transformer set mixing hyperparameters ? 1 , ? 2 , ? 3 to 1",0.6851033568382263
translation,259,112,hyperparameters,", 2017 )",as,nmt model,", 2017 ) as nmt model",0.5895870327949524
translation,259,112,hyperparameters,hyperparameters,use,transformer,hyperparameters use transformer,0.6335708498954773
translation,259,112,hyperparameters,hyperparameters,set,"mixing hyperparameters ? 1 , ? 2 , ? 3 to 1","hyperparameters set mixing hyperparameters ? 1 , ? 2 , ? 3 to 1",0.6935902833938599
translation,259,6,model,adaptive layer,on top of,multilingual bert,adaptive layer on top of multilingual bert,0.6943792700767517
translation,259,6,model,adaptive layer,by,contrastive learning,adaptive layer by contrastive learning,0.5961877703666687
translation,259,6,model,contrastive learning,to align,representation,contrastive learning to align representation,0.6978814005851746
translation,259,6,model,representation,between,source and target language,representation between source and target language,0.643229603767395
translation,259,18,model,missing language side,from,large monolingual generic corpus,missing language side from large monolingual generic corpus,0.5474336743354797
translation,259,18,model,model,propose,generalised approach,model propose generalised approach,0.666294515132904
translation,259,19,model,data selection method,trains,adaptive layer,data selection method trains adaptive layer,0.7530295252799988
translation,259,19,model,adaptive layer,on top of,multilingual bert,adaptive layer on top of multilingual bert,0.6943792700767517
translation,259,19,model,contrastive learning,such that,representations,contrastive learning such that representations,0.6132258176803589
translation,259,19,model,and target language,are,aligned,and target language are aligned,0.6151218414306641
translation,259,23,model,discriminative domain mixing method,for,supervised domain adaptation,discriminative domain mixing method for supervised domain adaptation,0.5781344771385193
translation,259,23,model,discriminative domain mixing method,jointly learns,domain discrimination and translation,discriminative domain mixing method jointly learns domain discrimination and translation,0.7582710385322571
translation,259,23,model,domain discrimination and translation,to,unsupervised setting,domain discrimination and translation to unsupervised setting,0.506714940071106
translation,259,23,model,model,extend,discriminative domain mixing method,model extend discriminative domain mixing method,0.6590256094932556
translation,259,24,model,nmt model,jointly learns to,translate,nmt model jointly learns to translate,0.7443162202835083
translation,259,24,model,nmt model,captures,characteris-tics,nmt model captures characteris-tics,0.7691165804862976
translation,259,24,model,translate,with,translation loss,translate with translation loss,0.6916019916534424
translation,259,24,model,translation loss,on,pseudo bitext,translation loss on pseudo bitext,0.5366793870925903
translation,259,24,model,characteris-tics,of,new domain,characteris-tics of new domain,0.5892176628112793
translation,259,24,model,characteris-tics,by,domain discrimination loss,characteris-tics by domain discrimination loss,0.5766098499298096
translation,259,24,model,new domain,by,domain discrimination loss,new domain by domain discrimination loss,0.5686740279197693
translation,259,24,model,domain discrimination loss,on,data,domain discrimination loss on data,0.5377261638641357
translation,259,24,model,data,from,old and new domains,data from old and new domains,0.5717869400978088
translation,259,24,model,model,has,nmt model,model has nmt model,0.5759021043777466
translation,259,111,model,domain discriminator,is,2 - layer feed - forward network,domain discriminator is 2 - layer feed - forward network,0.5490983724594116
translation,259,111,model,2 - layer feed - forward network,same hyperparameters as,adaptive layer,2 - layer feed - forward network same hyperparameters as adaptive layer,0.6841322779655457
translation,259,111,model,model,has,domain discriminator,model has domain discriminator,0.5597482323646545
translation,259,117,results,guda,able to,reduce,guda able to reduce,0.683592677116394
translation,259,117,results,in-domain data,selected,intelligently,in-domain data selected intelligently,0.6180024147033691
translation,259,117,results,reduce,has,gap,reduce has gap,0.5910917520523071
translation,259,117,results,results,seen that,guda,results seen that guda,0.7413055896759033
translation,259,118,results,selection method,has,consistently outperforms,selection method has consistently outperforms,0.616365373134613
translation,259,118,results,consistently outperforms,has,both,consistently outperforms has both,0.6366153359413147
translation,259,118,results,both,has,domain-finetune and ced strategy,both has domain-finetune and ced strategy,0.6189711093902588
translation,259,118,results,results,has,selection method,results has selection method,0.5398290157318115
translation,259,124,results,outperforms,up to,+ 1.2 bleu score,outperforms up to + 1.2 bleu score,0.6062475442886353
translation,259,124,results,other methods,up to,+ 1.2 bleu score,other methods up to + 1.2 bleu score,0.52734375
translation,259,124,results,method,has,outperforms,method has outperforms,0.6569275856018066
translation,259,124,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,259,132,results,nmt model,trained on,selected data,nmt model trained on selected data,0.7598176598548889
translation,259,132,results,selected data,with,clustering - based negative sampling k > 1,selected data with clustering - based negative sampling k > 1,0.6536774635314941
translation,259,132,results,clustering - based negative sampling k > 1,outperforms,one without clustering k = 1,clustering - based negative sampling k > 1 outperforms one without clustering k = 1,0.7832738161087036
translation,259,132,results,results,has,nmt model,results has nmt model,0.5247517228126526
translation,259,144,results,similar outcomes,in,domain adaption,similar outcomes in domain adaption,0.5413421988487244
translation,259,144,results,domain adaption,with,true bitext and the pseudo bitext,domain adaption with true bitext and the pseudo bitext,0.6595293879508972
translation,259,144,results,results,observe,similar outcomes,results observe similar outcomes,0.5732609629631042
translation,259,146,results,both monotext loss,achieves,best bleu score,both monotext loss achieves best bleu score,0.6102611422538757
translation,259,146,results,best bleu score,in,most of domain adaptation scenarios,best bleu score in most of domain adaptation scenarios,0.48669105768203735
translation,259,146,results,results,Combining,both monotext loss,results Combining both monotext loss,0.7170843482017517
translation,259,161,results,highest correct in - domain ngram rate,to,translation hypothesis,highest correct in - domain ngram rate to translation hypothesis,0.5391688942909241
translation,259,161,results,fully - supervised model,has,highest correct in - domain ngram rate,fully - supervised model has highest correct in - domain ngram rate,0.5553444027900696
translation,259,161,results,results,has,fully - supervised model,results has fully - supervised model,0.5531696677207947
translation,259,162,results,our proposed selection method,contributes,higher percentage,our proposed selection method contributes higher percentage,0.680905282497406
translation,259,162,results,higher percentage,of,in- domain ngrams,higher percentage of in- domain ngrams,0.6271767020225525
translation,259,162,results,in- domain ngrams,than,other selection methods,in- domain ngrams than other selection methods,0.5488077402114868
translation,259,162,results,other selection methods,in,all domains,other selection methods in all domains,0.4841412603855133
translation,259,162,results,results,has,our proposed selection method,results has our proposed selection method,0.5721777081489563
translation,260,59,experimental-setup,dataset,preprocessed and tokenized into,subwords,dataset preprocessed and tokenized into subwords,0.6635185480117798
translation,260,59,experimental-setup,subwords,with,"bpe ( sennrich et al. , 2016 )","subwords with bpe ( sennrich et al. , 2016 )",0.6050710082054138
translation,260,59,experimental-setup,"bpe ( sennrich et al. , 2016 )",using,scripts,"bpe ( sennrich et al. , 2016 ) using scripts",0.6672675013542175
translation,260,59,experimental-setup,experimental setup,has,dataset,experimental setup has dataset,0.5115352272987366
translation,260,60,experimental-setup,hyperparameters,has,4 attention heads,hyperparameters has 4 attention heads,0.5474745035171509
translation,260,61,experimental-setup,"adam ( kingma and ba , 2015 )",for,50 k steps,"adam ( kingma and ba , 2015 ) for 50 k steps",0.5607925653457642
translation,260,61,experimental-setup,"adam ( kingma and ba , 2015 )",with,early stopping,"adam ( kingma and ba , 2015 ) with early stopping",0.6337005496025085
translation,260,61,experimental-setup,50 k steps,with,early stopping,50 k steps with early stopping,0.6460955142974854
translation,260,61,experimental-setup,early stopping,using,4096 tokens per batch,early stopping using 4096 tokens per batch,0.6220731735229492
translation,260,61,experimental-setup,experimental setup,optimize with,"adam ( kingma and ba , 2015 )","experimental setup optimize with adam ( kingma and ba , 2015 )",0.686191737651825
translation,260,58,experiments,"fairseq ( ott et al. , 2019 )",to train,"transformer encoder-decoder ( vaswani et al. , 2017 )","fairseq ( ott et al. , 2019 ) to train transformer encoder-decoder ( vaswani et al. , 2017 )",0.6612298488616943
translation,260,58,experiments,"transformer encoder-decoder ( vaswani et al. , 2017 )",on,iwslt '14 de -en dataset,"transformer encoder-decoder ( vaswani et al. , 2017 ) on iwslt '14 de -en dataset",0.5225903987884521
translation,261,101,baselines,target side,of,synthetic data,target side of synthetic data,0.5867505669593811
translation,261,101,baselines,syntheticqe -mlm,has,target side,syntheticqe -mlm has target side,0.5970061421394348
translation,261,101,baselines,baselines,has,syntheticqe -mlm,baselines has syntheticqe -mlm,0.5787003636360168
translation,261,102,baselines,baselines,has,syntheticqe-mt+mlm,baselines has syntheticqe-mt+mlm,0.5555141568183899
translation,261,104,baselines,bertscore,based on,similarity scores,bertscore based on similarity scores,0.6862295866012573
translation,261,104,baselines,similarity scores,of,contextual bert embeddings,similarity scores of contextual bert embeddings,0.5754398703575134
translation,261,104,baselines,baselines,has,bertscore,baselines has bertscore,0.5821647047996521
translation,261,105,baselines,bertscore ++,uses,word alignments and mlms,bertscore ++ uses word alignments and mlms,0.6318368911743164
translation,261,105,baselines,baselines,has,bertscore ++,baselines has bertscore ++,0.5742865800857544
translation,261,106,baselines,baselines,has,"nmt -qe ( fomicheva et al. , 2020 )","baselines has nmt -qe ( fomicheva et al. , 2020 )",0.5448596477508545
translation,261,113,hyperparameters,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.999 and ? = 10 ?8","adam optimizer with ? 1 = 0.9 , ? 2 = 0.999 and ? = 10 ?8",0.6213843822479248
translation,261,113,hyperparameters,"? 1 = 0.9 , ? 2 = 0.999 and ? = 10 ?8",to optimize,model parameters,"? 1 = 0.9 , ? 2 = 0.999 and ? = 10 ?8 to optimize model parameters",0.6579741835594177
translation,261,113,hyperparameters,hyperparameters,used,adam optimizer,hyperparameters used adam optimizer,0.5959904789924622
translation,261,114,hyperparameters,training,set,batch size,training set batch size,0.6989448666572571
translation,261,114,hyperparameters,training,set,maximum sequence length,training set maximum sequence length,0.6940674185752869
translation,261,114,hyperparameters,training,set,number of training steps,training set number of training steps,0.6903215646743774
translation,261,114,hyperparameters,training,set,learning rate,training set learning rate,0.6812386512756348
translation,261,114,hyperparameters,training,set,dropout rate,training set dropout rate,0.6416471600532532
translation,261,114,hyperparameters,maximum sequence length,to,256,maximum sequence length to 256,0.5573639869689941
translation,261,114,hyperparameters,number of training steps,to,"100,000","number of training steps to 100,000",0.5989474654197693
translation,261,114,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,261,116,hyperparameters,mc dropout process,used,same dropout rate,mc dropout process used same dropout rate,0.616645097732544
translation,261,116,hyperparameters,same dropout rate,as during,training,same dropout rate as during training,0.6801708936691284
translation,261,116,hyperparameters,hyperparameters,For,mc dropout process,hyperparameters For mc dropout process,0.5516344904899597
translation,261,8,model,noises,propose,self-supervised method,noises propose self-supervised method,0.6837210655212402
translation,261,8,model,self-supervised method,for,sentence - and word- level qe,self-supervised method for sentence - and word- level qe,0.5839422345161438
translation,261,8,model,self-supervised method,performs,quality estimation,self-supervised method performs quality estimation,0.5806797742843628
translation,261,8,model,quality estimation,by recovering,masked target words,quality estimation by recovering masked target words,0.735336184501648
translation,261,31,model,model,propose,self-supervised qe method,model propose self-supervised qe method,0.6662935018539429
translation,261,124,results,baseline syntheticqe - mlm,except on,en- ru sentence - level task,baseline syntheticqe - mlm except on en- ru sentence - level task,0.5567957758903503
translation,261,124,results,single models,has,baseline syntheticqe - mt,single models has baseline syntheticqe - mt,0.548622190952301
translation,261,124,results,baseline syntheticqe - mt,has,outperforms,baseline syntheticqe - mt has outperforms,0.5950968265533447
translation,261,124,results,outperforms,has,baseline syntheticqe - mlm,outperforms has baseline syntheticqe - mlm,0.5741718411445618
translation,261,124,results,results,For,single models,results For single models,0.5855173468589783
translation,261,125,results,both baselines,on,sentence - and word - level tasks,both baselines on sentence - and word - level tasks,0.45856812596321106
translation,261,125,results,sentence - and word - level tasks,in,two different language pairs,sentence - and word - level tasks in two different language pairs,0.4708532989025116
translation,261,125,results,our single model,has,consistently outperforms,our single model has consistently outperforms,0.5949593186378479
translation,261,125,results,consistently outperforms,has,both baselines,consistently outperforms has both baselines,0.5949482917785645
translation,261,125,results,results,has,our single model,results has our single model,0.5605776309967041
translation,261,126,results,our single model,achieves,competitive or better performance,our single model achieves competitive or better performance,0.6752477288246155
translation,261,126,results,competitive or better performance,compared to,highly complex ensemble model,competitive or better performance compared to highly complex ensemble model,0.6545808911323547
translation,261,126,results,results,has,our single model,results has our single model,0.5605776309967041
translation,261,128,results,syntheticqe - mlm,on,sentence - and word - level tasks,syntheticqe - mlm on sentence - and word - level tasks,0.5099731683731079
translation,261,128,results,our single model,has,outperforms,our single model has outperforms,0.6204248070716858
translation,261,128,results,results,has,our single model,results has our single model,0.5605776309967041
translation,261,130,results,other unsupervised methods,on,sentence - level tasks,other unsupervised methods on sentence - level tasks,0.45269688963890076
translation,261,130,results,outperforms,has,other unsupervised methods,outperforms has other unsupervised methods,0.5777257084846497
translation,261,138,results,syntheticqe - mt,is,relatively low,syntheticqe - mt is relatively low,0.6258339881896973
translation,261,138,results,relatively low,when,recall,relatively low when recall,0.6572824716567993
translation,261,138,results,relatively low,when,recall,relatively low when recall,0.6572824716567993
translation,261,138,results,relatively low,when,recall,relatively low when recall,0.6572824716567993
translation,261,138,results,recall,above,0.2,recall above 0.2,0.703800618648529
translation,261,138,results,relatively low,when,recall,relatively low when recall,0.6572824716567993
translation,261,138,results,recall,above,0.2,recall above 0.2,0.703800618648529
translation,261,138,results,recall,has,is below 0.2,recall has is below 0.2,0.6052704453468323
translation,261,139,results,our method,reaches,relatively high precision,our method reaches relatively high precision,0.6745585799217224
translation,261,139,results,relatively high precision,whenever,recall,relatively high precision whenever recall,0.6417220830917358
translation,261,139,results,recall,is,low or high,recall is low or high,0.5831925868988037
translation,261,139,results,baselines,has,our method,baselines has our method,0.5691280364990234
translation,261,139,results,results,Compared with,baselines,results Compared with baselines,0.6914018392562866
translation,261,158,results,with wwm,outperforms,counterpart,with wwm outperforms counterpart,0.7407575249671936
translation,261,158,results,model,has,with wwm,model has with wwm,0.6074572801589966
translation,261,158,results,counterpart,has,without wwm,counterpart has without wwm,0.6410926580429077
translation,262,73,baselines,fairseq ( base ),is a,sequence modeling toolkit,fairseq ( base ) is a sequence modeling toolkit,0.6056126356124878
translation,262,73,baselines,blt -nmt,is,topic enhanced model,blt -nmt is topic enhanced model,0.56386399269104
translation,262,73,baselines,topic enhanced model,incorporated,bilingual topic knowledge,topic enhanced model incorporated bilingual topic knowledge,0.6683387756347656
translation,262,73,baselines,bilingual topic knowledge,into,nmt,bilingual topic knowledge into nmt,0.5924097895622253
translation,262,73,baselines,ltr - nmt,is,topic-based nmt model,ltr - nmt is topic-based nmt model,0.5529080033302307
translation,262,73,baselines,topic-based nmt model,using,cnn model,topic-based nmt model using cnn model,0.6057142615318298
translation,262,68,experimental-setup,corpus,for,all experiments,corpus for all experiments,0.6393430829048157
translation,262,68,experimental-setup,corpus,of,etm,corpus of etm,0.6048200726509094
translation,262,68,experimental-setup,all experiments,of,etm,all experiments of etm,0.5927672982215881
translation,262,68,experimental-setup,experimental setup,preprocess,corpus,experimental setup preprocess corpus,0.728046715259552
translation,262,69,experimental-setup,number of the topics,to,50,number of the topics to 50,0.6037893891334534
translation,262,69,experimental-setup,epoch number,to,500,epoch number to 500,0.6102805733680725
translation,262,69,experimental-setup,experimental setup,set,number of the topics,experimental setup set number of the topics,0.6058045029640198
translation,262,69,experimental-setup,experimental setup,set,epoch number,experimental setup set epoch number,0.626477062702179
translation,262,71,experimental-setup,nmt experiments,train,our models,nmt experiments train our models,0.7036195397377014
translation,262,71,experimental-setup,our models,on,one machine,our models on one machine,0.5781511664390564
translation,262,71,experimental-setup,one machine,with,4 nvidia v100 gpus,one machine with 4 nvidia v100 gpus,0.5585559606552124
translation,262,6,model,of embedding topic information,at,sentence level,of embedding topic information at sentence level,0.5343612432479858
translation,262,6,model,of embedding topic information,into,nmt model,of embedding topic information into nmt model,0.5364378690719604
translation,262,6,model,nmt model,to improve,translation performance,nmt model to improve translation performance,0.6268681287765503
translation,262,6,model,heterogeneous ways,has,of embedding topic information,heterogeneous ways has of embedding topic information,0.5560154318809509
translation,262,6,model,model,propose,heterogeneous ways,model propose heterogeneous ways,0.7118441462516785
translation,262,7,model,topic information,incorporated as,pre-encoder topic embedding,topic information incorporated as pre-encoder topic embedding,0.5686289668083191
translation,262,7,model,topic information,incorporated as,post-encoder topic embedding,topic information incorporated as post-encoder topic embedding,0.5791402459144592
translation,262,7,model,topic information,incorporated as,decoder topic embedding,topic information incorporated as decoder topic embedding,0.58863365650177
translation,262,7,model,decoder topic embedding,to increase,likelihood,decoder topic embedding to increase likelihood,0.6629149913787842
translation,262,7,model,of selecting target words,from,same topic,of selecting target words from same topic,0.5854259729385376
translation,262,7,model,same topic,of,source sentence,same topic of source sentence,0.5873889327049255
translation,262,7,model,likelihood,has,of selecting target words,likelihood has of selecting target words,0.602135956287384
translation,262,7,model,model,has,topic information,model has topic information,0.5398712158203125
translation,262,21,model,heterogeneous ways,of incorporating,topic information,heterogeneous ways of incorporating topic information,0.6486944556236267
translation,262,21,model,topic information,into,transformer architecture,topic information into transformer architecture,0.5778741836547852
translation,262,21,model,model,propose,heterogeneous ways,model propose heterogeneous ways,0.7118441462516785
translation,262,22,model,topic information,incorporated in,heterogeneous manner,topic information incorporated in heterogeneous manner,0.6278404593467712
translation,262,22,model,heterogeneous manner,namely as,pre-encoder topic embedding ( en c pre ),heterogeneous manner namely as pre-encoder topic embedding ( en c pre ),0.6031875610351562
translation,262,22,model,heterogeneous manner,namely as,post-encoder topic embedding ( en c post ),heterogeneous manner namely as post-encoder topic embedding ( en c post ),0.6093559861183167
translation,262,22,model,heterogeneous manner,namely as,decoder topic embedding ( dec ),heterogeneous manner namely as decoder topic embedding ( dec ),0.6413412690162659
translation,262,22,model,model,has,topic information,model has topic information,0.5398712158203125
translation,262,23,model,topic distribution,learned for,each word,topic distribution learned for each word,0.6497084498405457
translation,262,23,model,topic distribution,fed into,nmt model,topic distribution fed into nmt model,0.6431570649147034
translation,262,23,model,each word,summarized at,sentence level,each word summarized at sentence level,0.5995725393295288
translation,262,23,model,model,has,topic distribution,model has topic distribution,0.5086615085601807
translation,263,38,baselines,potential,of,large pre-trained multilingual model,potential of large pre-trained multilingual model,0.530600368976593
translation,263,38,baselines,large pre-trained multilingual model,trained with,mt objectives,large pre-trained multilingual model trained with mt objectives,0.7001253962516785
translation,263,76,baselines,ensemble of two variations,has,of the first model ( m1 - adapt and m1m - adapt ),ensemble of two variations has of the first model ( m1 - adapt and m1m - adapt ),0.5606018304824829
translation,263,62,experimental-setup,mc dropout,to account for,uncertainty of the qe model,mc dropout to account for uncertainty of the qe model,0.6398961544036865
translation,263,40,experiments,"mbart50 model ( tang et al. , 2020 )",trained with,multilingual finetuning,"mbart50 model ( tang et al. , 2020 ) trained with multilingual finetuning",0.7335469722747803
translation,263,40,experiments,multilingual finetuning,on,50 languages,multilingual finetuning on 50 languages,0.49636685848236084
translation,263,40,experiments,50 languages,including,all languages of interest,50 languages including all languages of interest,0.6899382472038269
translation,263,40,experiments,all languages of interest,for,qe 2021 task,all languages of interest for qe 2021 task,0.6152893900871277
translation,263,39,model,"mbart ( liu et al. , 2020 ) encoder-decoder architecture",to encode,source,"mbart ( liu et al. , 2020 ) encoder-decoder architecture to encode source",0.7302109003067017
translation,263,39,model,"mbart ( liu et al. , 2020 ) encoder-decoder architecture",force-decode,hypothesis,"mbart ( liu et al. , 2020 ) encoder-decoder architecture force-decode hypothesis",0.7656545639038086
translation,263,39,model,model,use,"mbart ( liu et al. , 2020 ) encoder-decoder architecture","model use mbart ( liu et al. , 2020 ) encoder-decoder architecture",0.6046075820922852
translation,263,75,results,trained xlm - roberta encoder,from,m1m model,trained xlm - roberta encoder from m1m model,0.5714834928512573
translation,263,75,results,prove beneficial,for,predictions,prove beneficial for predictions,0.6466620564460754
translation,263,75,results,predictions,on,post-edited data,predictions on post-edited data,0.567684531211853
translation,263,75,results,post-edited data,of,task 2,post-edited data of task 2,0.5697726011276245
translation,263,75,results,results,show,trained xlm - roberta encoder,results show trained xlm - roberta encoder,0.6261646151542664
translation,263,75,results,results,using,trained xlm - roberta encoder,results using trained xlm - roberta encoder,0.6538200974464417
translation,263,88,results,variations,outperform,m2 model,variations outperform m2 model,0.695644199848175
translation,263,88,results,performance,is,comparable,performance is comparable,0.5933461785316467
translation,263,88,results,m1m - adapt,for,specific language pairs,m1m - adapt for specific language pairs,0.6447966694831848
translation,263,88,results,m2-kl -g- mcd,has,outperform,m2-kl -g- mcd has outperform,0.6949991583824158
translation,263,88,results,outperform,has,m1m - adapt,outperform has m1m - adapt,0.6766034960746765
translation,263,88,results,results,observe,m2-kl -g- mcd,results observe m2-kl -g- mcd,0.578653872013092
translation,263,89,results,fine-tuning,results in,performance gains,fine-tuning results in performance gains,0.6494587063789368
translation,263,89,results,m1 model,on,metrics data,m1 model on metrics data,0.5567114353179932
translation,263,89,results,performance gains,for,majority of the language pairs,performance gains for majority of the language pairs,0.6050282120704651
translation,263,89,results,fine-tuning,has,m1 model,fine-tuning has m1 model,0.5289203524589539
translation,263,89,results,results,see that,fine-tuning,results see that fine-tuning,0.6683403253555298
translation,263,90,results,m1m,achieves,competitive performance,m1m achieves competitive performance,0.7067908048629761
translation,263,90,results,competitive performance,for,most pairs,competitive performance for most pairs,0.6176698207855225
translation,263,90,results,results,applying,m1m,results applying m1m,0.6974358558654785
translation,263,108,results,fine-tuning,on,task2 data,fine-tuning on task2 data,0.5280203223228455
translation,263,108,results,model,using,m1m encoder,model using m1m encoder,0.719538688659668
translation,263,108,results,m1m encoder,provides,performance boost,m1m encoder provides performance boost,0.6680358052253723
translation,263,108,results,performance boost,for,pearson correlation,performance boost for pearson correlation,0.6024354696273804
translation,263,108,results,pearson correlation,in,most language pairs,pearson correlation in most language pairs,0.5444302558898926
translation,263,108,results,competitive performance,for,rest,competitive performance for rest,0.6739254593849182
translation,263,108,results,fine-tuning,has,model,fine-tuning has model,0.5608916282653809
translation,263,108,results,task2 data,has,model,task2 data has model,0.5705038905143738
translation,263,108,results,results,when,fine-tuning,results when fine-tuning,0.6271789073944092
translation,263,116,results,uncertainty - related sources of information,improves,performance,uncertainty - related sources of information improves performance,0.6811711192131042
translation,263,116,results,performance,integrated into,qe system,performance integrated into qe system,0.7154187560081482
translation,263,116,results,results,demonstrated,uncertainty - related sources of information,results demonstrated uncertainty - related sources of information,0.6108623743057251
translation,263,116,results,results,handling,uncertainty - related sources of information,results handling uncertainty - related sources of information,0.7034899592399597
translation,264,127,ablation-analysis,performance,of,finetuned models,performance of finetuned models,0.6182485222816467
translation,264,127,ablation-analysis,finetuned models,on,general domain,finetuned models on general domain,0.5828119516372681
translation,264,127,ablation-analysis,drops significantly,due to,catastrophic forgetting problem,drops significantly due to catastrophic forgetting problem,0.7229341864585876
translation,264,127,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,264,142,ablation-analysis,more negative effect,to,translation performance,more negative effect to translation performance,0.5300673842430115
translation,264,142,ablation-analysis,more negative effect,than,source side noise,more negative effect than source side noise,0.588238000869751
translation,264,142,ablation-analysis,target side noise,has,more negative effect,target side noise has more negative effect,0.5877715349197388
translation,264,142,ablation-analysis,ablation analysis,has,target side noise,ablation analysis has target side noise,0.5181156992912292
translation,264,173,ablation-analysis,learnable kernel and learnable mixing weight,bring,improvement,learnable kernel and learnable mixing weight bring improvement,0.6083081364631653
translation,264,173,ablation-analysis,ablation analysis,has,learnable kernel and learnable mixing weight,ablation analysis has learnable kernel and learnable mixing weight,0.5136451125144958
translation,264,180,ablation-analysis,learnable mixing weight,more important than,learnable kernel function,learnable mixing weight more important than learnable kernel function,0.6404423713684082
translation,264,180,ablation-analysis,ablation analysis,has,learnable mixing weight,ablation analysis has learnable mixing weight,0.5089852809906006
translation,264,190,ablation-analysis,general domain and domain-specific translation,has,drops dramatically,general domain and domain-specific translation has drops dramatically,0.5669589638710022
translation,264,190,ablation-analysis,ablation analysis,Without,retrieval dropout,ablation analysis Without retrieval dropout,0.7551933526992798
translation,264,196,ablation-analysis,which types of word kernel - smoothing,influences,most,which types of word kernel - smoothing influences most,0.708795964717865
translation,264,196,ablation-analysis,ablation analysis,has,which types of word kernel - smoothing,ablation analysis has which types of word kernel - smoothing,0.5721334218978882
translation,264,203,ablation-analysis,kernel - smoothing,contributes most to,predictions,kernel - smoothing contributes most to predictions,0.7064064145088196
translation,264,203,ablation-analysis,predictions,of,"verbs , adverbs and nouns","predictions of verbs , adverbs and nouns",0.5732368230819702
translation,264,203,ablation-analysis,"verbs , adverbs and nouns",which are,morphologically complex word types,"verbs , adverbs and nouns which are morphologically complex word types",0.6455616354942322
translation,264,203,ablation-analysis,2 different domains,has,kernel - smoothing,2 different domains has kernel - smoothing,0.5126983523368835
translation,264,203,ablation-analysis,ablation analysis,across,2 different domains,ablation analysis across 2 different domains,0.7111974954605103
translation,264,15,baselines,knn - mt and our proposed kster,adapted for,domain-specific translation,knn - mt and our proposed kster adapted for domain-specific translation,0.6704388856887817
translation,264,15,baselines,domain-specific translation,with,in- domain database,domain-specific translation with in- domain database,0.6393792629241943
translation,264,15,baselines,baselines,has,knn - mt and our proposed kster,baselines has knn - mt and our proposed kster,0.5722545385360718
translation,264,121,baselines,baselines,has,knn,baselines has knn,0.5637263059616089
translation,264,101,experimental-setup,joint byte pair encoding ( bpe ),with,30 k merge operations,joint byte pair encoding ( bpe ) with 30 k merge operations,0.623333752155304
translation,264,101,experimental-setup,30 k merge operations,for,subword segmentation,30 k merge operations for subword segmentation,0.625044047832489
translation,264,101,experimental-setup,experimental setup,use,joint byte pair encoding ( bpe ),experimental setup use joint byte pair encoding ( bpe ),0.5917842388153076
translation,264,103,experimental-setup,"transformer base ( vaswani et al. , 2017 )",as,base model,"transformer base ( vaswani et al. , 2017 ) as base model",0.5158061385154724
translation,264,103,experimental-setup,experimental setup,employ,"transformer base ( vaswani et al. , 2017 )","experimental setup employ transformer base ( vaswani et al. , 2017 )",0.521295964717865
translation,264,108,experimental-setup,keys,of,examples,keys of examples,0.6098504066467285
translation,264,108,experimental-setup,keys,stored in,fp16 format,keys stored in fp16 format,0.6919416785240173
translation,264,108,experimental-setup,fp16 format,to reduce,memory demand,fp16 format to reduce memory demand,0.6158411502838135
translation,264,108,experimental-setup,experimental setup,has,keys,experimental setup has keys,0.5078819394111633
translation,264,109,experimental-setup,k = 16,to keep,balance,k = 16 to keep balance,0.6451020240783691
translation,264,109,experimental-setup,balance,between,translation quality,balance between translation quality,0.6592321395874023
translation,264,109,experimental-setup,balance,between,inference speed,balance between inference speed,0.6434128284454346
translation,264,109,experimental-setup,experimental setup,set,k = 16,experimental setup set k = 16,0.6840996146202087
translation,264,110,experimental-setup,base model,for,200k steps,base model for 200k steps,0.6630551815032959
translation,264,110,experimental-setup,experimental setup,train,base model,experimental setup train base model,0.6683900356292725
translation,264,112,experimental-setup,kster,for,30 k steps,kster for 30 k steps,0.6799280047416687
translation,264,112,experimental-setup,experimental setup,train,kster,experimental setup train kster,0.6959326267242432
translation,264,113,experimental-setup,each batch,contains,"32,768 tokens","each batch contains 32,768 tokens",0.6368131637573242
translation,264,113,experimental-setup,training procedures,has,each batch,training procedures has each batch,0.5738237500190735
translation,264,114,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,learning rates,"adam optimizer ( kingma and ba , 2015 ) with learning rates",0.6017654538154602
translation,264,114,experimental-setup,learning rates,set to,0.0002,learning rates set to 0.0002,0.6886215806007385
translation,264,114,experimental-setup,experimental setup,optimized by,"adam optimizer ( kingma and ba , 2015 )","experimental setup optimized by adam optimizer ( kingma and ba , 2015 )",0.7139168381690979
translation,264,115,experimental-setup,kster,introduced,526,kster introduced 526,0.6047441363334656
translation,264,115,experimental-setup,k trainable parameters,is,0.85 %,k trainable parameters is 0.85 %,0.5616663098335266
translation,264,115,experimental-setup,0.85 %,of,base model,0.85 % of base model,0.5906908512115479
translation,264,115,experimental-setup,526,has,k trainable parameters,526 has k trainable parameters,0.551213264465332
translation,264,115,experimental-setup,experimental setup,has,kster,experimental setup has kster,0.5892716646194458
translation,264,118,experimental-setup,individual database,for,each specific domain,individual database for each specific domain,0.5934314727783203
translation,264,118,experimental-setup,each specific domain,with,in- domain training data,each specific domain with in- domain training data,0.6080394983291626
translation,264,118,experimental-setup,in- domain training data,in,damt,in- domain training data in damt,0.5372738242149353
translation,264,118,experimental-setup,experimental setup,build,individual database,experimental setup build individual database,0.6943931579589844
translation,264,214,experimental-setup,source sentences,from,contrawsd,source sentences from contrawsd,0.5892930030822754
translation,264,214,experimental-setup,source sentences,from,training data,source sentences from training data,0.5090909004211426
translation,264,214,experimental-setup,training data,of,5 specific domains,training data of 5 specific domains,0.5734495520591736
translation,264,214,experimental-setup,5 specific domains,by,averaged bert embeddings,5 specific domains by averaged bert embeddings,0.5030228495597839
translation,264,214,experimental-setup,averaged bert embeddings,has,"devlin et al. , 2018 )","averaged bert embeddings has devlin et al. , 2018 )",0.6034811735153198
translation,264,214,experimental-setup,experimental setup,encode,source sentences,experimental setup encode source sentences,0.659492015838623
translation,264,8,experiments,kster,able to achieve,improvement,kster able to achieve improvement,0.6571530699729919
translation,264,8,experiments,improvement,of,1.1 to 1.5 bleu scores,improvement of 1.1 to 1.5 bleu scores,0.5645726919174194
translation,264,8,experiments,improvement,over,best existing online adaptation methods,improvement over best existing online adaptation methods,0.6455528140068054
translation,264,8,experiments,1.1 to 1.5 bleu scores,over,best existing online adaptation methods,1.1 to 1.5 bleu scores over best existing online adaptation methods,0.6080000400543213
translation,264,106,experiments,"faiss ( johnson et al. , 2017 ) index",for,nearest neighbour search,"faiss ( johnson et al. , 2017 ) index for nearest neighbour search",0.5673837661743164
translation,264,107,experiments,product quantization,to accelerate,retrieval,product quantization to accelerate retrieval,0.7047131061553955
translation,264,107,experiments,retrieval,in,large scale databases,retrieval in large scale databases,0.5505107641220093
translation,264,125,experiments,kster,outperforms,knn - mt,kster outperforms knn - mt,0.7508881092071533
translation,264,125,experiments,knn - mt,for,1.2 and 1.4 bleu scores,knn - mt for 1.2 and 1.4 bleu scores,0.5796821117401123
translation,264,125,experiments,1.2 and 1.4 bleu scores,on average,en - de and de -en translation,1.2 and 1.4 bleu scores on average en - de and de -en translation,0.6651307940483093
translation,264,125,experiments,1.2 and 1.4 bleu scores,in,en - de and de -en translation,1.2 and 1.4 bleu scores in en - de and de -en translation,0.5383507609367371
translation,264,125,experiments,domain-specific translation,has,kster,domain-specific translation has kster,0.5692765116691589
translation,264,154,experiments,kster,outperforms,knn - mt,kster outperforms knn - mt,0.7508881092071533
translation,264,154,experiments,knn - mt,for,3 and 6 bleu scores,knn - mt for 3 and 6 bleu scores,0.5877931714057922
translation,264,154,experiments,3 and 6 bleu scores,in,en - de and de -en direction,3 and 6 bleu scores in en - de and de -en direction,0.5601293444633484
translation,264,154,experiments,general domain sentence translation,has,kster,general domain sentence translation has kster,0.5553644895553589
translation,264,155,experiments,kster,outperforms,knn - mt,kster outperforms knn - mt,0.7508881092071533
translation,264,155,experiments,knn - mt,for,1.5 and 1.1 bleu scores,knn - mt for 1.5 and 1.1 bleu scores,0.5849896669387817
translation,264,155,experiments,1.5 and 1.1 bleu scores,in,en - de and de -en direction,1.5 and 1.1 bleu scores in en - de and de -en direction,0.5671189427375793
translation,264,155,experiments,domain-specific translation,has,kster,domain-specific translation has kster,0.5692765116691589
translation,264,184,experiments,kster,achieves,best performance,kster achieves best performance,0.7078772783279419
translation,264,184,experiments,best performance,with,k = 16,best performance with k = 16,0.6593172550201416
translation,264,184,experiments,damt,has,kster,damt has kster,0.6139139533042908
translation,264,7,model,kernel - smoothed translation,with,example retrieval ( kster ),kernel - smoothed translation with example retrieval ( kster ),0.6198325753211975
translation,264,7,model,model,propose to learn,kernel - smoothed translation,model propose to learn kernel - smoothed translation,0.7089418768882751
translation,264,28,model,kernel - smoothed translation with example retrieval ( kster ),to effectively learn and adapt,neural machine translation,kernel - smoothed translation with example retrieval ( kster ) to effectively learn and adapt neural machine translation,0.6637858748435974
translation,264,28,model,model,learn,kernel - smoothed translation with example retrieval ( kster ),model learn kernel - smoothed translation with example retrieval ( kster ),0.6921061277389526
translation,264,31,model,learnable kernel,to dynamically measure,relevance,learnable kernel to dynamically measure relevance,0.682909369468689
translation,264,31,model,relevance,of,retrieved examples,relevance of retrieved examples,0.5688528418540955
translation,264,31,model,relevance,based on,current context,relevance based on current context,0.6517466902732849
translation,264,31,model,retrieved examples,based on,current context,retrieved examples based on current context,0.6544918417930603
translation,264,31,model,model,introduce,learnable kernel,model introduce learnable kernel,0.6532516479492188
translation,264,32,model,exampled - based distribution,combined with,model - based distribution,exampled - based distribution combined with model - based distribution,0.6933252811431885
translation,264,32,model,model - based distribution,computed by,nmt,model - based distribution computed by nmt,0.7290093302726746
translation,264,32,model,nmt,with,adaptive mixing weight,nmt with adaptive mixing weight,0.6330236196517944
translation,264,32,model,adaptive mixing weight,for,next word prediction,adaptive mixing weight for next word prediction,0.6142222285270691
translation,264,32,model,model,has,exampled - based distribution,model has exampled - based distribution,0.5840919613838196
translation,264,174,model,kster,both,kernel and mixing weight,kster both kernel and mixing weight,0.6864460110664368
translation,264,174,model,model,In,kster,model In kster,0.612412691116333
translation,264,16,results,knn - mt and kster,achieve,improvements,knn - mt and kster achieve improvements,0.6622200012207031
translation,264,16,results,improvements,over,base,improvements over base,0.7617469429969788
translation,264,16,results,base,in,domain-specific translation performance,base in domain-specific translation performance,0.4961298108100891
translation,264,16,results,results,has,knn - mt and kster,results has knn - mt and kster,0.5415543913841248
translation,264,29,results,kster,retains,online adaptation,kster retains online adaptation,0.7470453381538391
translation,264,29,results,online adaptation,advantage of,non-parametric methods,online adaptation advantage of non-parametric methods,0.6590856313705444
translation,264,29,results,results,has,kster,results has kster,0.5826801061630249
translation,264,30,results,kster,improves,generalization ability,kster improves generalization ability,0.6948691010475159
translation,264,30,results,generalization ability,of,non-parametric methods,generalization ability of non-parametric methods,0.5576993227005005
translation,264,30,results,results,has,kster,results has kster,0.5826801061630249
translation,264,37,results,knn - mt,for,1.8 bleu scores,knn - mt for 1.8 bleu scores,0.5555956363677979
translation,264,37,results,1.8 bleu scores,in,unseen domains,1.8 bleu scores in unseen domains,0.484516978263855
translation,264,37,results,on average,in,unseen domains,on average in unseen domains,0.5637519359588623
translation,264,37,results,outperforms,has,knn - mt,outperforms has knn - mt,0.6100702881813049
translation,264,37,results,1.8 bleu scores,has,on average,1.8 bleu scores has on average,0.5633467435836792
translation,264,126,results,finetuning,achieves,best domain-specific performance,finetuning achieves best domain-specific performance,0.6851088404655457
translation,264,126,results,results,has,finetuning,results has finetuning,0.5693389773368835
translation,264,129,results,kster,performs,far better,kster performs far better,0.6322147250175476
translation,264,129,results,far better,than,finetuning,far better than finetuning,0.6229770183563232
translation,264,129,results,far better,than,knn - mt,far better than knn - mt,0.5906051397323608
translation,264,129,results,knn - mt,in,general domain,knn - mt in general domain,0.5450822710990906
translation,264,129,results,results,has,kster,results has kster,0.5826801061630249
translation,264,130,results,slightly better,than,gaussian kernel,slightly better than gaussian kernel,0.5571290850639343
translation,264,143,results,bleu scores,of,kster,bleu scores of kster,0.5676526427268982
translation,264,143,results,drop,in,all settings,drop in all settings,0.55153489112854
translation,264,143,results,drop,indicates,proposed method,drop indicates proposed method,0.6397119760513306
translation,264,143,results,less apparently,in,all settings,less apparently in all settings,0.6079162359237671
translation,264,143,results,proposed method,is,more robust,proposed method is more robust,0.5686815977096558
translation,264,143,results,more robust,with,low-quality database,more robust with low-quality database,0.6130485534667969
translation,264,143,results,kster,has,drop,kster has drop,0.6335368752479553
translation,264,143,results,drop,has,less apparently,drop has less apparently,0.6532593965530396
translation,264,143,results,results,has,bleu scores,results has bleu scores,0.5230661034584045
translation,264,156,results,joint-training,in,general domain performance,joint-training in general domain performance,0.5155180096626282
translation,264,156,results,joint-training,in,averaged domain-specific performance,joint-training in averaged domain-specific performance,0.5017188787460327
translation,264,156,results,outperforms,has,joint-training,outperforms has joint-training,0.586502194404602
translation,264,157,results,proposed method,achieves,advantages,proposed method achieves advantages,0.6166346669197083
translation,264,157,results,advantages,over,jointtraining,advantages over jointtraining,0.6856591701507568
translation,264,157,results,jointtraining,in,online adaptation,jointtraining in online adaptation,0.531284749507904
translation,264,157,results,jointtraining,in,translation performance,jointtraining in translation performance,0.49016979336738586
translation,264,157,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,264,160,results,knn - mt,performs,better,knn - mt performs better,0.6610379219055176
translation,264,160,results,better,in,domain-specific translation,better in domain-specific translation,0.553385317325592
translation,264,160,results,domain-specific translation,when,system prediction,domain-specific translation when system prediction,0.6080316305160522
translation,264,160,results,system prediction,relies more on,searched examples,system prediction relies more on searched examples,0.7196359038352966
translation,264,160,results,results,find that,knn - mt,results find that knn - mt,0.6310892105102539
translation,264,161,results,general domain translation performance,achieved when,system prediction,general domain translation performance achieved when system prediction,0.6370964646339417
translation,264,161,results,system prediction,relies more on,nmt prediction,system prediction relies more on nmt prediction,0.7147787809371948
translation,264,161,results,better,has,general domain translation performance,better has general domain translation performance,0.5426035523414612
translation,264,161,results,nmt prediction,has,high bandwidth and low mixing weight,nmt prediction has high bandwidth and low mixing weight,0.5655342936515808
translation,264,162,results,trade- off,between,general and specific domain performance,trade- off between general and specific domain performance,0.6740538477897644
translation,264,162,results,general and specific domain performance,in,knn - mt,general and specific domain performance in knn - mt,0.5124480128288269
translation,264,163,results,identical kernel and mixing weight,for,all test examples,identical kernel and mixing weight for all test examples,0.584183394908905
translation,264,163,results,identical kernel and mixing weight,can not achieve,best performance,identical kernel and mixing weight can not achieve best performance,0.710091769695282
translation,264,163,results,best performance,in,general domain and specific domains simultaneously,best performance in general domain and specific domains simultaneously,0.5441318154335022
translation,264,163,results,results,Applying,identical kernel and mixing weight,results Applying identical kernel and mixing weight,0.7006842494010925
translation,264,164,results,kster,with,gaussian kernel,kster with gaussian kernel,0.6423466205596924
translation,264,164,results,kster,achieves,better performance,kster achieves better performance,0.6995431780815125
translation,264,164,results,better performance,in,general domain and domain-specific translation,better performance in general domain and domain-specific translation,0.524982750415802
translation,264,164,results,results,has,kster,results has kster,0.5826801061630249
translation,264,178,results,bring improvement,in,general domain and domain-specific translation,bring improvement in general domain and domain-specific translation,0.5388914346694946
translation,264,178,results,learnable kernel and learnable mixing weight,has,bring improvement,learnable kernel and learnable mixing weight has bring improvement,0.5510181188583374
translation,264,178,results,results,has,learnable kernel and learnable mixing weight,results has learnable kernel and learnable mixing weight,0.5338041186332703
translation,264,179,results,two parts,brings,additional improvement,two parts brings additional improvement,0.6627251505851746
translation,264,179,results,two parts,has,learnable,two parts has learnable,0.6059111952781677
translation,264,179,results,learnable,has,simultaneously,learnable has simultaneously,0.62067049741745
translation,264,179,results,results,Keeping,two parts,results Keeping two parts,0.6587807536125183
translation,264,181,results,knn - mt,with,all k selections,knn - mt with all k selections,0.7040879726409912
translation,264,181,results,kster,has,outperforms,kster has outperforms,0.6674904227256775
translation,264,181,results,outperforms,has,knn - mt,outperforms has knn - mt,0.6100702881813049
translation,264,181,results,results,has,kster,results has kster,0.5826801061630249
translation,264,185,results,performance,of,our method,performance of our method,0.5712061524391174
translation,264,185,results,performance,increases with,k,performance increases with k,0.7610253691673279
translation,264,185,results,our method,increases with,k,our method increases with k,0.7178746461868286
translation,264,185,results,mdmt,has,performance,mdmt has performance,0.5604976415634155
translation,264,185,results,results,In,mdmt,results In mdmt,0.5383263826370239
translation,264,186,results,all k selections,has,kster,all k selections has kster,0.6354303956031799
translation,264,186,results,kster,has,outperforms,kster has outperforms,0.6674904227256775
translation,264,186,results,outperforms,has,knn - mt,outperforms has knn - mt,0.6100702881813049
translation,264,186,results,results,With,all k selections,results With all k selections,0.6458427906036377
translation,265,35,experiments,extended data set wibemt,to uncover,gender bias,extended data set wibemt to uncover gender bias,0.6273161768913269
translation,265,35,experiments,gender bias,in,neural machine translation systems,gender bias in neural machine translation systems,0.4949864149093628
translation,265,78,hyperparameters,two families of word embeddings,used,two pre-trained versions,two families of word embeddings used two pre-trained versions,0.5523926615715027
translation,265,78,hyperparameters,two families of word embeddings,two pre-trained versions of,fasttext,two families of word embeddings two pre-trained versions of fasttext,0.7097755670547485
translation,265,78,hyperparameters,two pre-trained versions,of,glove,two pre-trained versions of glove,0.6297905445098877
translation,265,78,hyperparameters,glove,from,pennington et al .,glove from pennington et al .,0.5854211449623108
translation,265,78,hyperparameters,hyperparameters,has,two families of word embeddings,hyperparameters has two families of word embeddings,0.49535179138183594
translation,265,79,hyperparameters,vector size,of,300 dimensions,vector size of 300 dimensions,0.6118810176849365
translation,265,79,hyperparameters,hyperparameters,has,four word embeddings,hyperparameters has four word embeddings,0.48664987087249756
translation,265,146,results,difference,to,percentage of translations,difference to percentage of translations,0.559798002243042
translation,265,146,results,percentage of translations,to,female inflection,percentage of translations to female inflection,0.547553300857544
translation,265,146,results,female inflection,of,original winobias sentences,female inflection of original winobias sentences,0.5424007177352905
translation,265,146,results,female inflection,becomes,significant,female inflection becomes significant,0.5972304344177246
translation,265,146,results,significant,for,all three nmt systems,significant for all three nmt systems,0.6933372616767883
translation,265,146,results,results,has,difference,results has difference,0.5636705756187439
translation,265,166,results,% t f g,in,sentences,% t f g in sentences,0.6199727654457092
translation,265,166,results,sentences,with,female occupations,sentences with female occupations,0.608285129070282
translation,265,166,results,female occupations,was,25.1 %,female occupations was 25.1 %,0.5138599276542664
translation,265,166,results,female occupations,was,19.0 %,female occupations was 19.0 %,0.5137553811073303
translation,265,166,results,female occupations,was,17.6 %,female occupations was 17.6 %,0.5171675086021423
translation,265,166,results,25.1 %,in,deepl,25.1 % in deepl,0.6083319783210754
translation,265,166,results,19.0 %,in,microsoft translator,19.0 % in microsoft translator,0.5125170350074768
translation,265,166,results,17.6 %,in,google translate,17.6 % in google translate,0.5096599459648132
translation,265,166,results,results,has,% t f g,results has % t f g,0.5769684910774231
translation,265,208,results,drastically improve,when it comes to,correct gender,drastically improve when it comes to correct gender,0.6582827568054199
translation,265,208,results,overall accuracy,of,all three nmt systems,overall accuracy of all three nmt systems,0.5403333902359009
translation,265,208,results,correct gender,in,translations,correct gender in translations,0.5064805150032043
translation,265,208,results,gender-adjectives,has,drastically improve,gender-adjectives has drastically improve,0.5754293203353882
translation,265,208,results,drastically improve,has,overall accuracy,drastically improve has overall accuracy,0.5793347358703613
translation,265,208,results,results,has,gender-adjectives,results has gender-adjectives,0.5050788521766663
translation,265,209,results,accuracy slightly drops,for,sentences,accuracy slightly drops for sentences,0.6442379951477051
translation,265,209,results,accuracy slightly drops,for,sentences,accuracy slightly drops for sentences,0.6442379951477051
translation,265,209,results,accuracy slightly drops,for,sentences,accuracy slightly drops for sentences,0.6442379951477051
translation,265,209,results,sentences,with,masculine pronouns,sentences with masculine pronouns,0.5775738954544067
translation,265,209,results,sentences,with,feminine pronouns,sentences with feminine pronouns,0.5842182040214539
translation,265,209,results,sentences,with,feminine pronouns,sentences with feminine pronouns,0.5842182040214539
translation,265,209,results,drastically improves,for,sentences,drastically improves for sentences,0.5766624808311462
translation,265,209,results,sentences,with,feminine pronouns,sentences with feminine pronouns,0.5842182040214539
translation,266,112,ablation-analysis,sparsity,of,predictions,sparsity of predictions,0.6100523471832275
translation,266,112,ablation-analysis,ablation analysis,has,sparsity,ablation analysis has sparsity,0.5322430729866028
translation,266,90,hyperparameters,efficient training,used,teacher forcing,efficient training used teacher forcing,0.6221104264259338
translation,266,90,hyperparameters,teacher forcing,in,all our models,teacher forcing in all our models,0.5502608418464661
translation,266,90,hyperparameters,hyperparameters,For,efficient training,hyperparameters For efficient training,0.5474828481674194
translation,266,91,hyperparameters,inference,used,beam search,inference used beam search,0.6094440817832947
translation,266,91,hyperparameters,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,266,91,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,266,91,hyperparameters,length penalty,has,1,length penalty has 1,0.5648252367973328
translation,266,91,hyperparameters,hyperparameters,During,inference,hyperparameters During inference,0.6701886057853699
translation,266,5,model,models,with,novel training objective,models with novel training objective,0.6165488958358765
translation,266,5,model,novel training objective,based on,recently - proposed bertscore evaluation metric,novel training objective based on recently - proposed bertscore evaluation metric,0.6252056360244751
translation,266,5,model,fine-tuning,has,models,fine-tuning has models,0.5599509477615356
translation,266,7,model,bertscore,propose,three approaches,bertscore propose three approaches,0.6753644347190857
translation,266,7,model,three approaches,for generating,soft predictions,three approaches for generating soft predictions,0.7031464576721191
translation,266,7,model,soft predictions,allowing,network,soft predictions allowing network,0.7535120248794556
translation,266,7,model,network,to remain,completely,network to remain completely,0.7085551023483276
translation,266,7,model,network,to remain,differentiable,network to remain differentiable,0.6920201778411865
translation,266,7,model,completely,has,differentiable,completely has differentiable,0.6247443556785583
translation,266,7,model,differentiable,has,end-to-end,differentiable has end-to-end,0.6115763187408447
translation,266,7,model,model,propose,three approaches,model propose three approaches,0.7077018022537231
translation,266,99,results,baseline,with,proposed approach,baseline with proposed approach,0.6966829299926758
translation,266,99,results,baseline,generally helped improve,f bert scores,baseline generally helped improve f bert scores,0.7065976858139038
translation,266,99,results,fine-tuning,has,baseline,fine-tuning has baseline,0.587217390537262
translation,266,99,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,266,101,results,best results,obtained with,gumbel -softmax ( gs ),best results obtained with gumbel -softmax ( gs ),0.6220056414604187
translation,266,101,results,best results,obtained with,more marked improvements,best results obtained with more marked improvements,0.6214931011199951
translation,266,101,results,gumbel -softmax ( gs ),with,more marked improvements,gumbel -softmax ( gs ) with more marked improvements,0.6397126913070679
translation,266,101,results,more marked improvements,for,de-en and en-tr,more marked improvements for de-en and en-tr,0.730046272277832
translation,266,101,results,+ 0.72 pp ms,for,de-en,+ 0.72 pp ms for de-en,0.714204728603363
translation,266,101,results,+ 0.03 pp ms,for,en-tr,+ 0.03 pp ms for en-tr,0.613463282585144
translation,266,101,results,de-en and en-tr,has,+ 0.36 pp bleu,de-en and en-tr has + 0.36 pp bleu,0.6571175456047058
translation,266,102,results,dense vectors ( dv ) and sparsemax ( sm ),not been,as effective,dense vectors ( dv ) and sparsemax ( sm ) not been as effective,0.6511080265045166
translation,266,102,results,dense vectors ( dv ) and sparsemax ( sm ),exception of,dense vectors,dense vectors ( dv ) and sparsemax ( sm ) exception of dense vectors,0.7362029552459717
translation,266,102,results,as effective,exception of,dense vectors,as effective exception of dense vectors,0.8130922317504883
translation,266,102,results,dense vectors,with,zh-en dataset,dense vectors with zh-en dataset,0.6236692667007446
translation,266,102,results,zh-en dataset,has,+ 0.25 pp bleu,zh-en dataset has + 0.25 pp bleu,0.5521610379219055
translation,266,102,results,results,has,dense vectors ( dv ) and sparsemax ( sm ),results has dense vectors ( dv ) and sparsemax ( sm ),0.5440983772277832
translation,266,104,results,none of the proposed approaches,obtained,significant improvements,none of the proposed approaches obtained significant improvements,0.6220788359642029
translation,266,104,results,significant improvements,with,en-es dataset,significant improvements with en-es dataset,0.6663542985916138
translation,266,106,results,embedding - based metrics,ranked,approaches,embedding - based metrics ranked approaches,0.6895116567611694
translation,266,106,results,approaches,in,same order,approaches in same order,0.5914716124534607
translation,266,106,results,embedding - based metrics,has,ms ),embedding - based metrics has ms ),0.5766160488128662
translation,266,106,results,results,both,embedding - based metrics,results both embedding - based metrics,0.6440146565437317
translation,267,52,ablation-analysis,computation of all transformations,of,set,computation of all transformations of set,0.6069151163101196
translation,267,52,ablation-analysis,computation of all transformations,accelerating,decoding,computation of all transformations accelerating decoding,0.8141145706176758
translation,267,52,ablation-analysis,parallelize,has,computation of all transformations,parallelize has computation of all transformations,0.5586689114570618
translation,267,67,ablation-analysis,both mechanisms,contribute to,performance,both mechanisms contribute to performance,0.7432972192764282
translation,267,67,ablation-analysis,ablation analysis,verifies,both mechanisms,ablation analysis verifies both mechanisms,0.6167221665382385
translation,267,9,model,capacity,for,multilingual nmt,capacity for multilingual nmt,0.6428254842758179
translation,267,9,model,capacity,by increasing,cardinality,capacity by increasing cardinality,0.7524163126945496
translation,267,9,model,model,efficiently increase,capacity,model efficiently increase capacity,0.7774001359939575
translation,267,10,model,multi-input-multi-output ( mimo ) architecture,allows,each transformation,multi-input-multi-output ( mimo ) architecture allows each transformation,0.6788703799247742
translation,267,10,model,each transformation,of,block,each transformation of block,0.5970968008041382
translation,267,10,model,block,to have,own input,block to have own input,0.6599708199501038
translation,267,10,model,model,present,multi-input-multi-output ( mimo ) architecture,model present multi-input-multi-output ( mimo ) architecture,0.6603184938430786
translation,267,11,model,task - aware attention mechanism,to learn to selectively utilize,individual transformations,task - aware attention mechanism to learn to selectively utilize individual transformations,0.7535425424575806
translation,267,11,model,individual transformations,from,set of transformations,individual transformations from set of transformations,0.5609710216522217
translation,267,11,model,set of transformations,for,different translation directions,set of transformations for different translation directions,0.5780562162399292
translation,267,11,model,model,present,task - aware attention mechanism,model present task - aware attention mechanism,0.590009868144989
translation,267,19,model,capacity,of,multilingual nmt model,capacity of multilingual nmt model,0.5634044408798218
translation,267,19,model,capacity,by increasing,cardinality,capacity by increasing cardinality,0.7524163126945496
translation,267,19,model,model,efficiently increase,capacity,model efficiently increase capacity,0.7774001359939575
translation,267,44,model,shared part,for,all language pairs,shared part for all language pairs,0.628984808921814
translation,267,44,model,shared part,trained on,full dataset,shared part trained on full dataset,0.6966698169708252
translation,267,44,model,language isolated part,activated in,corresponding translation task,language isolated part activated in corresponding translation task,0.6770023703575134
translation,267,44,model,language isolated part,trained on,part of the whole dataset,language isolated part trained on part of the whole dataset,0.7193747162818909
translation,267,44,model,2 parts,has,shared part,2 parts has shared part,0.6316459774971008
translation,267,63,results,our approach,achieves,better performance,our approach achieves better performance,0.6741553544998169
translation,267,63,results,better performance,in,all evaluations,better performance in all evaluations,0.48998183012008667
translation,267,63,results,1.31 times,has,as fast,1.31 times has as fast,0.5640604496002197
translation,267,63,results,results,shows,our approach,results shows our approach,0.7114341259002686
translation,267,70,results,6 layers,with,4 transformations,6 layers with 4 transformations,0.6727976202964783
translation,267,70,results,4 transformations,in,each block,4 transformations in each block,0.5468022227287292
translation,267,70,results,4 transformations,leads to,best perfor,4 transformations leads to best perfor,0.7016786336898804
translation,267,70,results,results,shows,6 layers,results shows 6 layers,0.6683597564697266
translation,267,70,results,results,using,6 layers,results using 6 layers,0.6403172612190247
translation,268,10,model,deep re-inforcement learning ( rl ) framework,in conjunction with,curriculum learning,deep re-inforcement learning ( rl ) framework in conjunction with curriculum learning,0.6959971189498901
translation,268,10,model,parameters,of,full-fledged nmt system,parameters of full-fledged nmt system,0.5750288367271423
translation,268,10,model,model,present,deep re-inforcement learning ( rl ) framework,model present deep re-inforcement learning ( rl ) framework,0.633101224899292
translation,268,23,model,deep-reinforcement - based framework,to adapt,parameters,deep-reinforcement - based framework to adapt parameters,0.6896055340766907
translation,268,23,model,parameters,of,pre-trained neural mt system,parameters of pre-trained neural mt system,0.5729342103004456
translation,268,23,model,pre-trained neural mt system,such that,generated translation,pre-trained neural mt system such that generated translation,0.5771297216415405
translation,268,23,model,generated translation,improves,performance,generated translation improves performance,0.6998278498649597
translation,268,23,model,performance,of,cross-lingual multi-class sentiment classifier,performance of cross-lingual multi-class sentiment classifier,0.5419000387191772
translation,268,23,model,model,propose,deep-reinforcement - based framework,model propose deep-reinforcement - based framework,0.6789509654045105
translation,268,24,model,deep actor-critic ( ac ) reinforcement learning framework,in the ambit of,curriculum learning ( cl ),deep actor-critic ( ac ) reinforcement learning framework in the ambit of curriculum learning ( cl ),0.7173588871955872
translation,268,24,model,quality of translation,in,cross-lingual setup,quality of translation in cross-lingual setup,0.5264204144477844
translation,268,24,model,model,propose,deep actor-critic ( ac ) reinforcement learning framework,model propose deep actor-critic ( ac ) reinforcement learning framework,0.6457930207252502
translation,268,25,model,actor-critic,to have,two neural networks,actor-critic to have two neural networks,0.5625266432762146
translation,268,25,model,actor,takes,action ( policy - based ),actor takes action ( policy - based ),0.641076385974884
translation,268,25,model,pre-trained nmt ),takes,action ( policy - based ),pre-trained nmt ) takes action ( policy - based ),0.6444798111915588
translation,268,34,model,harmonic-score- based reward function,in,our proposed rl - based framework,harmonic-score- based reward function in our proposed rl - based framework,0.5157146453857422
translation,268,34,model,harmonic-score- based reward function,preserve,sentiment and semantics,harmonic-score- based reward function preserve sentiment and semantics,0.7613294720649719
translation,268,34,model,model,optimization of,harmonic-score- based reward function,model optimization of harmonic-score- based reward function,0.6639922857284546
translation,268,38,results,fine-tuning method,in the ambit of,curriculum learning,fine-tuning method in the ambit of curriculum learning,0.6946253180503845
translation,268,38,results,fine-tuning method,achieves,additional performance gain,fine-tuning method achieves additional performance gain,0.667741596698761
translation,268,38,results,additional performance gain,of,mt system,additional performance gain of mt system,0.5792646408081055
translation,268,38,results,additional performance gain,setting where,curriculum based fine - tuning is not employed,additional performance gain setting where curriculum based fine - tuning is not employed,0.6784523725509644
translation,268,38,results,results,found that,fine-tuning method,results found that fine-tuning method,0.667992115020752
translation,269,247,ablation-analysis,performance,keeps,increasing,performance keeps increasing,0.7277666926383972
translation,269,247,ablation-analysis,increasing,with,more inferences,increasing with more inferences,0.6821798086166382
translation,269,247,ablation-analysis,score directly,has,more computation,score directly has more computation,0.5895980596542358
translation,269,247,ablation-analysis,score directly,has,better,score directly has better,0.6060382723808289
translation,269,247,ablation-analysis,score directly,has,performance,score directly has performance,0.5991214513778687
translation,269,247,ablation-analysis,ranking approaches,has,more computation,ranking approaches has more computation,0.5827593803405762
translation,269,247,ablation-analysis,ranking approaches,has,better,ranking approaches has better,0.6010066270828247
translation,269,247,ablation-analysis,more computation,has,better,more computation has better,0.5645479559898376
translation,269,247,ablation-analysis,ablation analysis,when accessing,score directly,ablation analysis when accessing score directly,0.6945294737815857
translation,269,373,ablation-analysis,three main aspects,are,important,three main aspects are important,0.583271861076355
translation,269,373,ablation-analysis,important,for,performance,important for performance,0.6339983344078064
translation,269,373,ablation-analysis,policy and the value terms,are,well - balanced,policy and the value terms are well - balanced,0.5630367398262024
translation,269,373,ablation-analysis,well - balanced,in,uct formula,well - balanced in uct formula,0.5496481657028198
translation,269,373,ablation-analysis,bleu and multilingual bertscore performance,when using,different value aggregation mechanisms,bleu and multilingual bertscore performance when using different value aggregation mechanisms,0.6732958555221558
translation,269,131,baselines,baselines,has,likelihood - based decoding greedy decoding ( gd ),baselines has likelihood - based decoding greedy decoding ( gd ),0.5388368964195251
translation,269,31,experiments,metric of interest,at,test time,metric of interest at test time,0.5556249618530273
translation,269,31,experiments,improved performance,compared to,likelihood - based approaches,improved performance compared to likelihood - based approaches,0.6607894897460938
translation,269,31,experiments,improved performance,scales with,amount of computation,improved performance scales with amount of computation,0.6829699873924255
translation,269,31,experiments,performance,scales with,amount of computation,performance scales with amount of computation,0.6734644174575806
translation,269,218,hyperparameters,our dictionary size,is,32 k,our dictionary size is 32 k,0.5800472497940063
translation,269,218,hyperparameters,our dictionary size,about,32 k,our dictionary size about 32 k,0.6256104111671448
translation,269,218,hyperparameters,hidden dimensionality,has,"512 , 16 heads","hidden dimensionality has 512 , 16 heads",0.5805035829544067
translation,269,218,hyperparameters,hyperparameters,has,encoder and decoder,hyperparameters has encoder and decoder,0.5516510009765625
translation,269,220,hyperparameters,incremental sampling,for,speed,incremental sampling for speed,0.6269477605819702
translation,269,220,hyperparameters,hyperparameters,use,incremental sampling,hyperparameters use incremental sampling,0.6205799579620361
translation,269,280,hyperparameters,hidden size,is,512,hidden size is 512,0.6110948920249939
translation,269,280,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,269,282,hyperparameters,unroll length,of,our models,unroll length of our models,0.5910211801528931
translation,269,282,hyperparameters,unroll length,is,128,unroll length is 128,0.585084855556488
translation,269,282,hyperparameters,our models,is,128,our models is 128,0.6231690049171448
translation,269,282,hyperparameters,hyperparameters,has,unroll length,hyperparameters has unroll length,0.5254996418952942
translation,269,283,hyperparameters,initial values,sampled from,gaussian distribution,initial values sampled from gaussian distribution,0.7052124738693237
translation,269,283,hyperparameters,gaussian distribution,with,mean 0,gaussian distribution with mean 0,0.6878401041030884
translation,269,283,hyperparameters,gaussian distribution,with,standard deviation 0.02,gaussian distribution with standard deviation 0.02,0.6287262439727783
translation,269,283,hyperparameters,normal gpt - 2,has,- style initialisers,normal gpt - 2 has - style initialisers,0.5950475335121155
translation,269,283,hyperparameters,hyperparameters,use,normal gpt - 2,hyperparameters use normal gpt - 2,0.6246193647384644
translation,269,298,hyperparameters,"adam ( kingma and ba , 2015 ) optimiser",with,learning rate 0.001,"adam ( kingma and ba , 2015 ) optimiser with learning rate 0.001",0.5973020792007446
translation,269,298,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 ) optimiser","hyperparameters use adam ( kingma and ba , 2015 ) optimiser",0.6264327168464661
translation,269,299,hyperparameters,batch size,is,4096,batch size is 4096,0.6176068782806396
translation,269,299,hyperparameters,batch size,train for,300000 steps,batch size train for 300000 steps,0.7286044359207153
translation,269,299,hyperparameters,100000 steps,for,ende dataset,100000 steps for ende dataset,0.6201082468032837
translation,269,299,hyperparameters,300000 steps,for,larger enfr dataset,300000 steps for larger enfr dataset,0.5912047624588013
translation,269,299,hyperparameters,hyperparameters,train for,100000 steps,hyperparameters train for 100000 steps,0.6405651569366455
translation,269,299,hyperparameters,hyperparameters,train for,300000 steps,hyperparameters train for 300000 steps,0.6608409881591797
translation,269,299,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,269,300,hyperparameters,dropout,with,weight 0.1,dropout with weight 0.1,0.6129825711250305
translation,269,300,hyperparameters,dropout,with,no weight decay,dropout with no weight decay,0.6295654773712158
translation,269,348,hyperparameters,beam size,tried,"2 , 4 , 6 , 8 , 10 and 20","beam size tried 2 , 4 , 6 , 8 , 10 and 20",0.6860083341598511
translation,269,348,hyperparameters,search,has,beam size,search has beam size,0.5585094690322876
translation,269,348,hyperparameters,hyperparameters,has,search,hyperparameters has search,0.551832914352417
translation,269,8,model,monte-carlo tree search ( mcts ),showcase,competitiveness,monte-carlo tree search ( mcts ) showcase competitiveness,0.7693461775779724
translation,269,8,model,model,introduce,monte-carlo tree search ( mcts ),model introduce monte-carlo tree search ( mcts ),0.6339717507362366
translation,269,8,model,model,showcase,competitiveness,model showcase competitiveness,0.8294786214828491
translation,269,217,model,joint policy / value model,based on,transformer encoder-decoder,joint policy / value model based on transformer encoder-decoder,0.6704887747764587
translation,269,217,model,model,has,joint policy / value model,model has joint policy / value model,0.5472864508628845
translation,269,279,model,both encoder and decoder,have,num_layers = 6 attention layers,both encoder and decoder have num_layers = 6 attention layers,0.5588932037353516
translation,269,279,model,model,has,both encoder and decoder,model has both encoder and decoder,0.5757463574409485
translation,269,287,model,linear projection,from,hidden dimensionality,linear projection from hidden dimensionality,0.5557418465614319
translation,269,287,model,linear projection,followed by,softmax operator,linear projection followed by softmax operator,0.6695499420166016
translation,269,287,model,hidden dimensionality,to,vocabulary size,hidden dimensionality to vocabulary size,0.5189033150672913
translation,269,287,model,softmax operator,to output,distribution,softmax operator to output distribution,0.7459376454353333
translation,269,287,model,distribution,over,whole vocabulary,distribution over whole vocabulary,0.6695743799209595
translation,269,287,model,model,consists in,linear projection,model consists in linear projection,0.6409651041030884
translation,269,296,model,feedforward layer,of,attention blocks,feedforward layer of attention blocks,0.5737295150756836
translation,269,296,model,bigger internal hidden dimensionality,of,3072,bigger internal hidden dimensionality of 3072,0.6280545592308044
translation,269,296,model,3072,instead of,2048,3072 instead of 2048,0.6349688172340393
translation,269,429,model,numpy - compatible version of mcts,run completely on,accelerator device,numpy - compatible version of mcts run completely on accelerator device,0.7496357560157776
translation,269,429,model,model,introduce,numpy - compatible version of mcts,model introduce numpy - compatible version of mcts,0.6734686493873596
translation,269,7,results,numerous decoding algorithms,rely on,value function,numerous decoding algorithms rely on value function,0.6733506917953491
translation,269,7,results,value function,parameterised by,neural network,value function parameterised by neural network,0.6913959980010986
translation,269,7,results,results,explore,numerous decoding algorithms,results explore numerous decoding algorithms,0.6159583926200867
translation,269,43,results,beam search,outperformed by,competitors,beam search outperformed by competitors,0.7605940103530884
translation,269,43,results,competitors,including,mcts,competitors including mcts,0.7569414377212524
translation,269,43,results,reference-less scores,has,beam search,reference-less scores has beam search,0.5959312915802002
translation,269,43,results,results,For,reference-less scores,results For reference-less scores,0.5621246695518494
translation,269,227,results,regular beam size,obtains,small but consistent improvement,regular beam size obtains small but consistent improvement,0.5781407952308655
translation,269,227,results,value-guided methods,perform,significantly better,value-guided methods perform significantly better,0.5927374362945557
translation,269,227,results,results,see that,regular beam size,results see that regular beam size,0.6871107220649719
translation,269,228,results,scales nicely,with,size of the dataset,scales nicely with size of the dataset,0.6402739882469177
translation,269,228,results,performance,has,scales nicely,performance has scales nicely,0.6042051911354065
translation,269,228,results,results,has,mcts,results has mcts,0.5076099038124084
translation,269,229,results,policies,of,our joint policy / value models,policies of our joint policy / value models,0.5755674242973328
translation,269,229,results,policies,perform,slightly worse,policies perform slightly worse,0.5727742314338684
translation,269,229,results,slightly worse,than,supervised counterparts,slightly worse than supervised counterparts,0.5763829946517944
translation,269,229,results,results,observe that,policies,results observe that policies,0.566399335861206
translation,269,230,results,initial supervised model policy,in conjunction with,multilingual value,initial supervised model policy in conjunction with multilingual value,0.6587816476821899
translation,269,230,results,initial supervised model policy,obtain,promising results,initial supervised model policy obtain promising results,0.5635298490524292
translation,269,230,results,promising results,notably,40.31 bleu,promising results notably 40.31 bleu,0.6052343249320984
translation,269,230,results,mlbertscore,with,mcts,mlbertscore with mcts,0.6685697436332703
translation,269,230,results,mcts,on,enfr dataset,mcts on enfr dataset,0.5584295988082886
translation,269,230,results,results,use,initial supervised model policy,results use initial supervised model policy,0.6638370156288147
translation,269,237,results,all the methods,that access,score,all the methods that access score,0.7143433094024658
translation,269,237,results,all the methods,perform,significantly better,all the methods perform significantly better,0.564978301525116
translation,269,237,results,score,perform,significantly better,score perform significantly better,0.611947238445282
translation,269,237,results,significantly better,than,value-guided counterparts,significantly better than value-guided counterparts,0.6036965250968933
translation,269,237,results,score,has,directly,score has directly,0.6373132467269897
translation,269,237,results,results,has,all the methods,results has all the methods,0.5179820656776428
translation,269,238,results,purely value- based v-mcts,is,competitive,purely value- based v-mcts is competitive,0.5919193625450134
translation,269,238,results,outperform,has,scorebased approach,outperform has scorebased approach,0.5961518883705139
translation,269,238,results,results,has,purely value- based v-mcts,results has purely value- based v-mcts,0.5518995523452759
translation,269,245,results,performance,start,degrading,performance start degrading,0.7265323400497437
translation,269,245,results,increasing,with,more computation,increasing with more computation,0.6928743124008179
translation,269,245,results,reference - based metrics,has,performance,reference - based metrics has performance,0.5526012778282166
translation,269,245,results,performance,has,quickly stops,performance has quickly stops,0.5955933332443237
translation,269,245,results,quickly stops,has,increasing,quickly stops has increasing,0.6373019814491272
translation,269,245,results,results,on,reference - based metrics,results on reference - based metrics,0.508480966091156
translation,269,246,results,more steadily,with,computation,more steadily with computation,0.7317011952400208
translation,269,246,results,higher quality value networks,has,performance,higher quality value networks has performance,0.5422452092170715
translation,269,246,results,performance,has,increases,performance has increases,0.5947421193122864
translation,269,246,results,increases,has,more steadily,increases has more steadily,0.6150848865509033
translation,269,251,results,optimising,via,value function,optimising via value function,0.6374824047088623
translation,269,251,results,reference - based metrics ( e.g. bleu ),via,value function,reference - based metrics ( e.g. bleu ) via value function,0.6405321955680847
translation,269,251,results,value function,is,surprisingly hard,value function is surprisingly hard,0.5477979183197021
translation,269,251,results,optimising,has,reference - based metrics ( e.g. bleu ),optimising has reference - based metrics ( e.g. bleu ),0.5727380514144897
translation,269,259,results,simple s+r method,performs,well,simple s+r method performs well,0.6377021074295044
translation,269,259,results,whenever access,has,simple s+r method,whenever access has simple s+r method,0.5958178639411926
translation,269,327,results,hybrid architecture,yields,perfor - mance improvements,hybrid architecture yields perfor - mance improvements,0.7299200892448425
translation,269,327,results,perfor - mance improvements,when using,value model,perfor - mance improvements when using value model,0.7063546180725098
translation,269,327,results,value model,with,mcts,value model with mcts,0.6549665927886963
translation,269,327,results,results,Using,hybrid architecture,results Using hybrid architecture,0.6897085309028625
translation,269,374,results,rely more heavily,on,value function,rely more heavily on value function,0.5624216794967651
translation,269,374,results,reference-less metric,has,best option,reference-less metric has best option,0.5756157040596008
translation,269,374,results,best option,has,rely more heavily,best option has rely more heavily,0.5664024353027344
translation,269,374,results,results,on,reference-less metric,results on reference-less metric,0.5492904782295227
translation,269,400,results,reference - based metrics,where,value functions,reference - based metrics where value functions,0.6179641485214233
translation,269,400,results,performance,of,value- based methods,performance of value- based methods,0.6075815558433533
translation,269,400,results,performance,reaches,peak quickly,performance reaches peak quickly,0.7824119329452515
translation,269,400,results,performance,start,degrading,performance start degrading,0.7265323400497437
translation,269,400,results,reference - based metrics,has,performance,reference - based metrics has performance,0.5526012778282166
translation,269,400,results,results,on,reference - based metrics,results on reference - based metrics,0.508480966091156
translation,269,407,results,s+rv,performs,worse,s+rv performs worse,0.6938548684120178
translation,269,407,results,worse,than,vgbs,worse than vgbs,0.6274159550666809
translation,269,407,results,worse,than,mcts,worse than mcts,0.632552444934845
translation,269,407,results,s+rv,has,value- based alternative,s+rv has value- based alternative,0.5883548259735107
translation,269,414,results,performance,of,policy models,performance of policy models,0.6007040143013
translation,269,414,results,policy models,trained on,distillation datasets,policy models trained on distillation datasets,0.7269119024276733
translation,269,414,results,slightly lower,than,plainly supervised counterparts,slightly lower than plainly supervised counterparts,0.5764299631118774
translation,269,417,results,larger dataset,see that,mcts decoding,larger dataset see that mcts decoding,0.6318533420562744
translation,269,417,results,mcts decoding,has,outperforms,mcts decoding has outperforms,0.6190559267997742
translation,269,417,results,outperforms,has,any other type of decoding,outperforms has any other type of decoding,0.607702910900116
translation,269,417,results,results,On,larger dataset,results On larger dataset,0.543695867061615
translation,269,420,results,low enough amount of simulations,trying to optimise,our reference -less metric,low enough amount of simulations trying to optimise our reference -less metric,0.6459261178970337
translation,269,420,results,our reference -less metric,yields,benefits,our reference -less metric yields benefits,0.7133871912956238
translation,269,486,results,beam search,performs,strongly,beam search performs strongly,0.7129167914390564
translation,269,486,results,strongly,for,reference - based metrics,strongly for reference - based metrics,0.6433281898498535
translation,269,486,results,value- based methods,prevail for,reference -less scores,value- based methods prevail for reference -less scores,0.7373360991477966
translation,269,486,results,results,has,beam search,results has beam search,0.5500618815422058
translation,270,6,baselines,data collection strategy,for,mt,data collection strategy for mt,0.5735123157501221
translation,270,6,baselines,data collection strategy,is,cheap and simple,data collection strategy is cheap and simple,0.46593648195266724
translation,270,6,baselines,mt,is,cheap and simple,mt is cheap and simple,0.5220332145690918
translation,270,122,baselines,mbart,is,transformer - based sequence - to-sequence model,mbart is transformer - based sequence - to-sequence model,0.5643345713615417
translation,270,122,baselines,transformer - based sequence - to-sequence model,pretrained on,25 monolingual raw text corpora,transformer - based sequence - to-sequence model pretrained on 25 monolingual raw text corpora,0.7150813341140747
translation,270,122,baselines,baselines,has,mbart,baselines has mbart,0.578681468963623
translation,270,128,experiments,hindi-to - english translation,see that,models,hindi-to - english translation see that models,0.5879777073860168
translation,270,128,experiments,hindi-to - english translation,on average,models,hindi-to - english translation on average models,0.6376190185546875
translation,270,128,experiments,models,trained on,sentences,models trained on sentences,0.7441256046295166
translation,270,128,experiments,sentences,collected via,gifs,sentences collected via gifs,0.6641969084739685
translation,270,128,experiments,sentences,from,images or m20,sentences from images or m20,0.6132879257202148
translation,270,128,experiments,images or m20,for,all training set sizes,images or m20 for all training set sizes,0.5870839953422546
translation,270,128,experiments,gifs,has,outperform,gifs has outperform,0.6847328543663025
translation,270,128,experiments,outperform,has,sentences,outperform has sentences,0.6297279596328735
translation,270,130,experiments,models,trained on,m20 data,models trained on m20 data,0.7544167041778564
translation,270,130,experiments,models,trained on,sentences,models trained on sentences,0.7441256046295166
translation,270,130,experiments,models,trained on,sentences,models trained on sentences,0.7441256046295166
translation,270,130,experiments,sentences,collected via,gifs,sentences collected via gifs,0.6641969084739685
translation,270,130,experiments,sentences,collected via,our images,sentences collected via our images,0.6659046411514282
translation,270,130,experiments,english-to -hindi translation,has,models,english-to -hindi translation has models,0.5535612106323242
translation,270,130,experiments,m20 data,has,outperform,m20 data has outperform,0.6539209485054016
translation,270,130,experiments,outperform,has,models,outperform has models,0.6446751952171326
translation,270,123,hyperparameters,learning rate,of,3e - 5,learning rate of 3e - 5,0.6390295624732971
translation,270,123,hyperparameters,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,270,123,hyperparameters,dropout,with,patience,dropout with patience,0.6697827577590942
translation,270,123,hyperparameters,0.3,for,up to 100 epochs,0.3 for up to 100 epochs,0.6044527888298035
translation,270,123,hyperparameters,0.3,with,patience,0.3 with patience,0.6003206968307495
translation,270,123,hyperparameters,patience,of,15,patience of 15,0.6757554411888123
translation,270,7,model,parallel sentences,from,monolingual annotators,parallel sentences from monolingual annotators,0.5288129448890686
translation,270,7,model,model,use,graphics interchange formats ( gifs ),model use graphics interchange formats ( gifs ),0.653915286064148
translation,270,17,model,new data collection strategy,is,"cheap , simple , effective","new data collection strategy is cheap , simple , effective",0.48297104239463806
translation,270,17,model,model,propose,new data collection strategy,model propose new data collection strategy,0.6842342019081116
translation,270,111,results,gif - as- pivot setting,rated,consistently higher,gif - as- pivot setting rated consistently higher,0.7115421891212463
translation,270,111,results,consistently higher,than,other two settings,consistently higher than other two settings,0.6057770252227783
translation,270,111,results,both language pairs,has,gif - as- pivot setting,both language pairs has gif - as- pivot setting,0.6186709403991699
translation,270,111,results,results,for,both language pairs,results for both language pairs,0.5250999927520752
translation,270,126,results,dataset size,mostly increases,performance,dataset size mostly increases performance,0.801110029220581
translation,270,126,results,performance,for,all data collection settings,performance for all data collection settings,0.586988091468811
translation,270,126,results,results,increasing,dataset size,results increasing dataset size,0.6884958744049072
translation,270,127,results,each model,performs,best,each model performs best,0.6614116430282593
translation,270,127,results,best,on,its own in - domain test set,best on its own in - domain test set,0.5077751278877258
translation,270,127,results,results,observe,each model,results observe each model,0.5967517495155334
translation,271,25,ablation-analysis,speech encoding sequence,via,adaptive feature selection,speech encoding sequence via adaptive feature selection,0.6341603994369507
translation,271,142,ablation-analysis,performance,of,our model,performance of our model,0.5847885608673096
translation,271,142,ablation-analysis,performance,in,bleu and apt,performance in bleu and apt,0.5486333966255188
translation,271,142,ablation-analysis,randomized context,has,hurts,randomized context has hurts,0.5820044875144958
translation,271,142,ablation-analysis,hurts,has,performance,hurts has performance,0.6043882966041565
translation,271,142,ablation-analysis,ablation analysis,show,randomized context,ablation analysis show randomized context,0.6423621773719788
translation,271,145,ablation-analysis,incorrect target context,acts,similarly,incorrect target context acts similarly,0.7612408995628357
translation,271,145,ablation-analysis,incorrect target context,combining,both contexts,incorrect target context combining both contexts,0.6542921662330627
translation,271,145,ablation-analysis,similarly,to,source counterpart,similarly to source counterpart,0.6113069653511047
translation,271,145,ablation-analysis,source counterpart,under,imed,source counterpart under imed,0.6912665963172913
translation,271,145,ablation-analysis,both contexts,leads to,slight but consistent performance degradation,both contexts leads to slight but consistent performance degradation,0.6742255687713623
translation,271,145,ablation-analysis,ablation analysis,observe,incorrect target context,ablation analysis observe incorrect target context,0.5908365249633789
translation,271,165,ablation-analysis,document- level decoding,has,imed (? = 1.0 ) performance,document- level decoding has imed (? = 1.0 ) performance,0.5545462369918823
translation,271,165,ablation-analysis,imed (? = 1.0 ) performance,has,drops greatly,imed (? = 1.0 ) performance has drops greatly,0.6035891175270081
translation,271,165,ablation-analysis,underperforming,has,baseline,underperforming has baseline,0.5998088121414185
translation,271,165,ablation-analysis,ablation analysis,After removing,document- level decoding,ablation analysis After removing document- level decoding,0.7059367895126343
translation,271,194,ablation-analysis,increase,of,sentence - level decoding ( ? ? 1.0 ),increase of sentence - level decoding ( ? ? 1.0 ),0.5772379040718079
translation,271,194,ablation-analysis,imed,produces,higher dal and ne,imed produces higher dal and ne,0.6591543555259705
translation,271,194,ablation-analysis,increase,has,imed,increase has imed,0.7228068113327026
translation,271,194,ablation-analysis,sentence - level decoding ( ? ? 1.0 ),has,imed,sentence - level decoding ( ? ? 1.0 ) has imed,0.626521646976471
translation,271,194,ablation-analysis,ablation analysis,With,increase,ablation analysis With increase,0.6709994673728943
translation,271,26,baselines,decoding methods,including,chunk - based decoding,decoding methods including chunk - based decoding,0.7109897136688232
translation,271,26,baselines,decoding methods,including,sliding - window based decoding,decoding methods including sliding - window based decoding,0.682131290435791
translation,271,94,hyperparameters,texts,are,tokenized and truecased,texts are tokenized and truecased,0.6150903701782227
translation,271,94,hyperparameters,texts,using,16 k merging operations,texts using 16 k merging operations,0.6980113387107849
translation,271,94,hyperparameters,tokenized and truecased,with,out - of- vocabulary words,tokenized and truecased with out - of- vocabulary words,0.6464093327522278
translation,271,94,hyperparameters,out - of- vocabulary words,handled by,bpe segmentation,out - of- vocabulary words handled by bpe segmentation,0.6692140698432922
translation,271,94,hyperparameters,out - of- vocabulary words,using,16 k merging operations,out - of- vocabulary words using 16 k merging operations,0.6430151462554932
translation,271,94,hyperparameters,hyperparameters,has,texts,hyperparameters has texts,0.5800265073776245
translation,271,95,hyperparameters,contextaware st,follows,"transformer base ( vaswani et al. , 2017 )","contextaware st follows transformer base ( vaswani et al. , 2017 )",0.6642069220542908
translation,271,95,hyperparameters,"transformer base ( vaswani et al. , 2017 )",has,6 layers,"transformer base ( vaswani et al. , 2017 ) has 6 layers",0.5742876529693604
translation,271,95,hyperparameters,hidden / feedforward size,has,512/2048,hidden / feedforward size has 512/2048,0.613482654094696
translation,271,96,hyperparameters,"adam ( ? 1 = 0.9 , ? 2 = 0.98 )",for,parameter updates,"adam ( ? 1 = 0.9 , ? 2 = 0.98 ) for parameter updates",0.592384397983551
translation,271,96,hyperparameters,parameter updates,with,label smoothing,parameter updates with label smoothing,0.5903198719024658
translation,271,96,hyperparameters,label smoothing,of,0.1,label smoothing of 0.1,0.599534809589386
translation,271,97,hyperparameters,same learning rate schedule,set,warmup step,same learning rate schedule set warmup step,0.7083396911621094
translation,271,97,hyperparameters,warmup step,to,4k,warmup step to 4k,0.5846807360649109
translation,271,97,hyperparameters,hyperparameters,use,same learning rate schedule,hyperparameters use same learning rate schedule,0.6440374255180359
translation,271,97,hyperparameters,hyperparameters,set,warmup step,hyperparameters set warmup step,0.6376988291740417
translation,271,98,hyperparameters,dropout,to,attention weights and residual connections,dropout to attention weights and residual connections,0.5242435336112976
translation,271,98,hyperparameters,dropout,with,rate,dropout with rate,0.5557408928871155
translation,271,98,hyperparameters,rate,of,0.2 and 0.5,rate of 0.2 and 0.5,0.6770293712615967
translation,271,98,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
translation,271,99,hyperparameters,hyperparameters,set,c = 2 and ? = 0.5,hyperparameters set c = 2 and ? = 0.5,0.6172747611999512
translation,271,100,hyperparameters,"afs ( = ?0.1 , ? = 2 /3 )",to,temporal and feature dimensions,"afs ( = ?0.1 , ? = 2 /3 ) to temporal and feature dimensions",0.5522265434265137
translation,271,100,hyperparameters,"afs ( = ?0.1 , ? = 2 /3 )",prunes out,?84 % speech encodings,"afs ( = ?0.1 , ? = 2 /3 ) prunes out ?84 % speech encodings",0.6710052490234375
translation,271,100,hyperparameters,temporal and feature dimensions,for,feature selection,temporal and feature dimensions for feature selection,0.6032755970954895
translation,271,101,hyperparameters,context- aware st,with,sentence - level baseline,context- aware st with sentence - level baseline,0.636757493019104
translation,271,101,hyperparameters,context- aware st,i.e.,st + afs,context- aware st i.e. st + afs,0.7110518217086792
translation,271,101,hyperparameters,context- aware st,finetune,model,context- aware st finetune model,0.7054194808006287
translation,271,101,hyperparameters,model,for,20 k steps,model for 20 k steps,0.6668345928192139
translation,271,101,hyperparameters,20 k steps,based on,concatenation method,20 k steps based on concatenation method,0.6835826635360718
translation,271,101,hyperparameters,concatenation method,with,batch size,concatenation method with batch size,0.6332710385322571
translation,271,101,hyperparameters,batch size,of,40 k subwords,batch size of 40 k subwords,0.5864140391349792
translation,271,101,hyperparameters,hyperparameters,initialize,context- aware st,hyperparameters initialize context- aware st,0.7649531960487366
translation,271,103,hyperparameters,beam search,for,decoding,beam search for decoding,0.6021772623062134
translation,271,103,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,271,103,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,271,103,hyperparameters,decoding,with,beam size,decoding with beam size,0.6321919560432434
translation,271,103,hyperparameters,decoding,with,length penalty,decoding with length penalty,0.6184776425361633
translation,271,103,hyperparameters,beam size,of,4,beam size of 4,0.6962505578994751
translation,271,103,hyperparameters,length penalty,of,0.6,length penalty of 0.6,0.5868892073631287
translation,271,103,hyperparameters,hyperparameters,adopt,beam search,hyperparameters adopt beam search,0.6588144898414612
translation,271,184,hyperparameters,training,finetune,each model,training finetune each model,0.6826397776603699
translation,271,184,hyperparameters,each model,for,extra 20 k steps,each model for extra 20 k steps,0.6377759575843811
translation,271,184,hyperparameters,extra 20 k steps,with,1:1 mix,extra 20 k steps with 1:1 mix,0.6628701686859131
translation,271,184,hyperparameters,1:1 mix,of,full-segment and prefix pairs,1:1 mix of full-segment and prefix pairs,0.5937248468399048
translation,271,184,hyperparameters,hyperparameters,For,training,hyperparameters For training,0.5661544799804688
translation,271,6,model,inmodel ensemble decoding,jointly performs,document - and sentence - level translation,inmodel ensemble decoding jointly performs document - and sentence - level translation,0.7911504507064819
translation,271,6,model,model,introduce,inmodel ensemble decoding,model introduce inmodel ensemble decoding,0.6221994161605835
translation,271,30,model,inmodel ensemble decoding ( imed ),to regularize,document- level translation,inmodel ensemble decoding ( imed ) to regularize document- level translation,0.691547155380249
translation,271,30,model,document- level translation,with,sentence - level counterpart,document- level translation with sentence - level counterpart,0.6439895033836365
translation,271,30,model,model,introduce,inmodel ensemble decoding ( imed ),model introduce inmodel ensemble decoding ( imed ),0.6226509809494019
translation,271,206,model,context,improves,simultaneous translation,context improves simultaneous translation,0.698306679725647
translation,271,206,model,simultaneous translation,by reducing,latency and erasure,simultaneous translation by reducing latency and erasure,0.6438383460044861
translation,271,206,model,model,has,context,model has context,0.5552915930747986
translation,271,5,results,extensive experiments,using,simple concatenationbased context - aware st model,extensive experiments using simple concatenationbased context - aware st model,0.6196686625480652
translation,271,5,results,simple concatenationbased context - aware st model,paired with,adaptive feature selection,simple concatenationbased context - aware st model paired with adaptive feature selection,0.684041440486908
translation,271,5,results,adaptive feature selection,on,speech encodings,adaptive feature selection on speech encodings,0.5774469971656799
translation,271,5,results,speech encodings,for,computational efficiency,speech encodings for computational efficiency,0.5964470505714417
translation,271,5,results,results,through,extensive experiments,results through extensive experiments,0.6231321692466736
translation,271,35,results,context,improves,overall translation quality,context improves overall translation quality,0.6540901064872742
translation,271,35,results,context,benefits,pronoun translation,context benefits pronoun translation,0.5635331869125366
translation,271,35,results,pronoun translation,across,different language pairs,pronoun translation across different language pairs,0.6961331963539124
translation,271,35,results,overall translation quality,has,+0.18 - 2.61 bleu,overall translation quality has +0.18 - 2.61 bleu,0.5278784036636353
translation,271,35,results,results,Incorporating,context,results Incorporating context,0.639300525188446
translation,271,36,results,results,has,context,results has context,0.5051900744438171
translation,271,37,results,st models,with,contexts,st models with contexts,0.6720697283744812
translation,271,37,results,contexts,suffer less from,( artificial ) audio segmentation errors,contexts suffer less from ( artificial ) audio segmentation errors,0.6823896169662476
translation,271,37,results,results,has,st models,results has st models,0.5162273049354553
translation,271,108,results,context,improve,translation,context improve translation,0.670136570930481
translation,271,110,results,our model with imed,outperforms,baseline,our model with imed outperforms baseline,0.797649085521698
translation,271,110,results,baseline,by,+ 0.48 bleu,baseline by + 0.48 bleu,0.5585227012634277
translation,271,110,results,results,has,our model with imed,results has our model with imed,0.6087421178817749
translation,271,116,results,swbd and imed,are,more stable,swbd and imed are more stable,0.6264070868492126
translation,271,116,results,swbd and imed,perform,best,swbd and imed perform best,0.6221120357513428
translation,271,116,results,swbd,surpasses,baseline,swbd surpasses baseline,0.6419503688812256
translation,271,116,results,baseline,by,2.06 apt ( 1?3 ),baseline by 2.06 apt ( 1?3 ),0.5263594388961792
translation,271,116,results,results,has,swbd and imed,results has swbd and imed,0.4974246323108673
translation,271,126,results,inspection,find that,contextaware st,inspection find that contextaware st,0.6178860068321228
translation,271,126,results,contextaware st,prefers to produce,longer translations,contextaware st prefers to produce longer translations,0.7579001188278198
translation,271,126,results,longer translations,than,baseline,longer translations than baseline,0.5828410983085632
translation,271,126,results,results,find that,contextaware st,results find that contextaware st,0.6144479513168335
translation,271,128,results,performance,in terms of,bleu,performance in terms of bleu,0.6349853277206421
translation,271,128,results,on par,with,context - aware st,on par with context - aware st,0.7050530910491943
translation,271,128,results,on par,in terms of,bleu,on par in terms of bleu,0.6658889055252075
translation,271,128,results,context - aware st,when,lp is 0.6,context - aware st when lp is 0.6,0.6323555707931519
translation,271,128,results,bleu,with,similar,bleu with similar,0.6849043965339661
translation,271,128,results,biasing,has,decoding,biasing has decoding,0.5807827115058899
translation,271,128,results,decoding,has,greatly improves,decoding has greatly improves,0.6089003086090088
translation,271,128,results,greatly improves,has,sentence - level st ( 1?8 ),greatly improves has sentence - level st ( 1?8 ),0.6085962653160095
translation,271,128,results,performance,has,on par,performance has on par,0.6100457906723022
translation,271,128,results,results,show,biasing,results show biasing,0.605239987373352
translation,271,129,results,context - aware st,benefits from,decoding,context - aware st benefits from decoding,0.6752153038978577
translation,271,129,results,decoding,with,larger length penalty,decoding with larger length penalty,0.667645275592804
translation,271,129,results,decoding,beating,all sentence - level st models,decoding beating all sentence - level st models,0.5600905418395996
translation,271,129,results,results,observe,context - aware st,results observe context - aware st,0.5690096616744995
translation,271,130,results,swbd,lp of,1.0,swbd lp of 1.0,0.6924301981925964
translation,271,130,results,best bleu,of,22.97,best bleu of 22.97,0.5074141025543213
translation,271,130,results,best bleu,of,63.51 ( 3?9 ),best bleu of 63.51 ( 3?9 ),0.538118839263916
translation,271,130,results,apt,of,63.51 ( 3?9 ),apt of 63.51 ( 3?9 ),0.5370915532112122
translation,271,130,results,results,has,swbd,results has swbd,0.5579026341438293
translation,271,143,results,more negative impact,on,imed,more negative impact on imed,0.5930836200714111
translation,271,143,results,more negative impact,resulting in,worse performance,more negative impact resulting in worse performance,0.684617280960083
translation,271,143,results,worse performance,than,baseline,worse performance than baseline,0.5445336699485779
translation,271,143,results,swbd,has,incorrect context,swbd has incorrect context,0.5525519847869873
translation,271,143,results,incorrect context,has,more negative impact,incorrect context has more negative impact,0.5705198049545288
translation,271,143,results,results,Compared to,swbd,results Compared to swbd,0.6660098433494568
translation,271,150,results,overall trend,of,impact of c,overall trend of impact of c,0.6532857418060303
translation,271,150,results,impact of c,on,bleu and apt,impact of c on bleu and apt,0.6331352591514587
translation,271,150,results,similar,for,different decoding methods,similar for different decoding methods,0.6424071788787842
translation,271,150,results,results,shows that,overall trend,results shows that overall trend,0.6569465398788452
translation,271,151,results,c to 1,delivers,best apt,c to 1 delivers best apt,0.7410294413566589
translation,271,151,results,context - aware st,achieves,best bleu,context - aware st achieves best bleu,0.6840994954109192
translation,271,151,results,best bleu,at,c = 2,best bleu at c = 2,0.5695416927337646
translation,271,151,results,results,Increasing,c to 1,results Increasing c to 1,0.7309171557426453
translation,271,151,results,results,Increasing,context - aware st,results Increasing context - aware st,0.6288143992424011
translation,271,164,results,context- aware st,outperforms,baseline,context- aware st outperforms baseline,0.7868734002113342
translation,271,164,results,baseline,by,> 0.73 acc hp,baseline by > 0.73 acc hp,0.5725956559181213
translation,271,164,results,swbd,performs,slightly better,swbd performs slightly better,0.6460997462272644
translation,271,164,results,slightly better,than,imed,slightly better than imed,0.662237286567688
translation,271,164,results,results,shows,context- aware st,results shows context- aware st,0.6375864148139954
translation,271,168,results,context,improves,robustness,context improves robustness,0.7216918468475342
translation,271,168,results,robustness,of,st models,robustness of st models,0.6110710501670837
translation,271,168,results,st models,to,audio segmentation errors,st models to audio segmentation errors,0.5219716429710388
translation,271,168,results,results,has,context,results has context,0.5051900744438171
translation,271,177,results,segmentation noise,deteriorates,translation quality,segmentation noise deteriorates translation quality,0.6937382221221924
translation,271,177,results,translation quality,for,all st models,translation quality for all st models,0.5718709230422974
translation,271,177,results,all st models,to,large degree ( > - 6 bleu ),all st models to large degree ( > - 6 bleu ),0.5591681003570557
translation,271,177,results,results,has,segmentation noise,results has segmentation noise,0.5967012047767639
translation,271,178,results,context - aware st,is,less sensitive,context - aware st is less sensitive,0.5962457060813904
translation,271,178,results,less sensitive,to,errors,less sensitive to errors,0.5870071053504944
translation,271,178,results,sentence - level st,has,context - aware st,sentence - level st has context - aware st,0.5715227723121643
translation,271,178,results,results,Compared to,sentence - level st,results Compared to sentence - level st,0.6024845242500305
translation,271,179,results,our model,with,imed,our model with imed,0.7534262537956238
translation,271,179,results,imed,yields,document - based bleu,imed yields document - based bleu,0.6831693053245544
translation,271,179,results,document - based bleu,of,22.03,document - based bleu of 22.03,0.5186970233917236
translation,271,179,results,substantially outperforming,by,1.63 bleu,substantially outperforming by 1.63 bleu,0.5737072825431824
translation,271,179,results,baseline,by,1.63 bleu,baseline by 1.63 bleu,0.503527820110321
translation,271,179,results,substantially outperforming,has,baseline,substantially outperforming has baseline,0.6217762231826782
translation,271,179,results,results,has,our model,results has our model,0.5871725678443909
translation,271,180,results,results,has,our results,results has our results,0.5639954209327698
translation,271,181,results,context,benefits,simultaneous translation,context benefits simultaneous translation,0.5960198044776917
translation,271,181,results,results,has,context,results has context,0.5051900744438171
translation,271,189,results,context- aware st,improves,translation quality,context- aware st improves translation quality,0.6383391618728638
translation,271,189,results,translation quality,has,> + 0.84 bleu,translation quality has > + 0.84 bleu,0.5714449882507324
translation,271,189,results,translation latency,has,> - 0.06 dal ),translation latency has > - 0.06 dal ),0.5938886404037476
translation,271,189,results,results,show,context- aware st,results show context- aware st,0.5912410020828247
translation,271,190,results,translation stability,when,target prefix constraint,translation stability when target prefix constraint,0.6304082274436951
translation,271,190,results,results,enhances,translation stability,results enhances translation stability,0.6247411370277405
translation,271,191,results,swbd,performs,worse,swbd performs worse,0.6733236908912659
translation,271,191,results,worse,in,ne,worse in ne,0.7105075120925903
translation,271,191,results,results,has,swbd,results has swbd,0.5579026341438293
translation,271,198,results,our model,obtains,improvements,our model obtains improvements,0.6487823128700256
translation,271,198,results,improvements,over,most metrics and language pairs,improvements over most metrics and language pairs,0.6396298408508301
translation,271,198,results,results,has,our model,results has our model,0.5871725678443909
translation,271,199,results,our model,performs,relatively worse,our model performs relatively worse,0.6055883169174194
translation,271,199,results,our model,with,smaller bleu gains,our model with smaller bleu gains,0.6328718662261963
translation,271,199,results,relatively worse,on,es,relatively worse on es,0.6420633792877197
translation,271,199,results,even negative results,in,acc hp,even negative results in acc hp,0.6188673377037048
translation,271,199,results,8 languages,has,our model,8 languages has our model,0.5781530141830444
translation,271,199,results,results,Out of,8 languages,results Out of 8 languages,0.5982212424278259
translation,271,200,results,our model,yields,largest improvement,our model yields largest improvement,0.6741145849227905
translation,271,200,results,largest improvement,on,ro,largest improvement on ro,0.5998342633247375
translation,271,200,results,results,has,our model,results has our model,0.5871725678443909
translation,271,201,results,our model,with,imed,our model with imed,0.7534262537956238
translation,271,201,results,imed,achieves,detokenized bleu,imed achieves detokenized bleu,0.722354531288147
translation,271,201,results,detokenized bleu,of,23.6,detokenized bleu of 23.6,0.5333542823791504
translation,271,201,results,detokenized bleu,surpassing,state - of - the - art result,detokenized bleu surpassing state - of - the - art result,0.6799086928367615
translation,271,201,results,23.6,on,en-ro,23.6 on en-ro,0.6606614589691162
translation,271,201,results,results,has,our model,results has our model,0.5871725678443909
translation,271,203,results,appropriate decoding method,observe,positive impact,appropriate decoding method observe positive impact,0.629167377948761
translation,271,203,results,positive impact,of,context,positive impact of context,0.5722101926803589
translation,271,203,results,context,on,translation,context on translation,0.5735799074172974
translation,271,203,results,results,With,concatenation - based contextual modeling,results With concatenation - based contextual modeling,0.6056703925132751
translation,271,204,results,context - aware st,improves,general translation quality,context - aware st improves general translation quality,0.6254923343658447
translation,271,204,results,context - aware st,helps,pronoun and homophone translation,context - aware st helps pronoun and homophone translation,0.5716798901557922
translation,271,204,results,general translation quality,in,bleu,general translation quality in bleu,0.45158734917640686
translation,271,204,results,results,has,context - aware st,results has context - aware st,0.5401754975318909
translation,272,153,ablation-analysis,slightly degrades,both,pronoun resolution accuracy,slightly degrades both pronoun resolution accuracy,0.6497963666915894
translation,272,153,ablation-analysis,slightly degrades,both,bleu,slightly degrades both bleu,0.6265466213226318
translation,272,153,ablation-analysis,one of the two strategies,has,slightly degrades,one of the two strategies has slightly degrades,0.5808820128440857
translation,272,153,ablation-analysis,ablation analysis,Removing,one of the two strategies,ablation analysis Removing one of the two strategies,0.7408571243286133
translation,272,154,ablation-analysis,more impact,on,accuracy,more impact on accuracy,0.5667194724082947
translation,272,154,ablation-analysis,word replacement,has,more impact,word replacement has more impact,0.5947131514549255
translation,272,154,ablation-analysis,ablation analysis,removing,word replacement,ablation analysis removing word replacement,0.738957405090332
translation,272,178,baselines,corefcl,introduces,max-margin contrastive learning loss,corefcl introduces max-margin contrastive learning loss,0.5791783332824707
translation,272,178,baselines,max-margin contrastive learning loss,to train,model,max-margin contrastive learning loss to train model,0.6240174174308777
translation,272,178,baselines,model,to explicitly discriminate,inconsistent contexts,model to explicitly discriminate inconsistent contexts,0.6926496624946594
translation,272,178,baselines,baselines,has,corefcl,baselines has corefcl,0.5637199282646179
translation,272,119,experimental-setup,tokenized,first,"moses ( koehn et al. , 2007 ) tokenizer","tokenized first moses ( koehn et al. , 2007 ) tokenizer",0.7257426381111145
translation,272,119,experimental-setup,tokenized,with,"moses ( koehn et al. , 2007 ) tokenizer","tokenized with moses ( koehn et al. , 2007 ) tokenizer",0.6072901487350464
translation,272,119,experimental-setup,tokenized,apply,"bpe ( sennrich et al. , 2016 b )","tokenized apply bpe ( sennrich et al. , 2016 b )",0.5707319378852844
translation,272,119,experimental-setup,"bpe ( sennrich et al. , 2016 b )",using,sentencepiece,"bpe ( sennrich et al. , 2016 b ) using sentencepiece",0.6398998498916626
translation,272,119,experimental-setup,size,of,merge operation,size of merge operation,0.5938488245010376
translation,272,119,experimental-setup,merge operation,is,16.5 k,merge operation is 16.5 k,0.5867736339569092
translation,272,119,experimental-setup,preprocessing,has,all english and german corpus,preprocessing has all english and german corpus,0.5362532138824463
translation,272,119,experimental-setup,experimental setup,For,preprocessing,experimental setup For preprocessing,0.5661813020706177
translation,272,123,experimental-setup,512,as,hidden dimension,512 as hidden dimension,0.5579418540000916
translation,272,123,experimental-setup,number of layers,is,6,number of layers is 6,0.591621994972229
translation,272,123,experimental-setup,number of attention heads,is,8,number of attention heads is 8,0.5953323841094971
translation,272,123,experimental-setup,dropout rate,set to,0.1,dropout rate set to 0.1,0.6670177578926086
translation,272,123,experimental-setup,experimental setup,set,512,experimental setup set 512,0.6774029731750488
translation,272,123,experimental-setup,experimental setup,set,number of attention heads,experimental setup set number of attention heads,0.6299266219139099
translation,272,123,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,272,124,experimental-setup,experimental setup,trained with,"adam ( kingma and ba , 2014 )","experimental setup trained with adam ( kingma and ba , 2014 )",0.7403237819671631
translation,272,125,experimental-setup,early stopping,of,training,early stopping of training,0.6120009422302246
translation,272,125,experimental-setup,training,when,mt loss,training when mt loss,0.614648163318634
translation,272,125,experimental-setup,mt loss,on,validation set,mt loss on validation set,0.5607064962387085
translation,272,125,experimental-setup,validation set,has,does not improve,validation set has does not improve,0.6253049373626709
translation,272,125,experimental-setup,experimental setup,employ,early stopping,experimental setup employ early stopping,0.5513901710510254
translation,272,7,model,contrastive learning scheme,based on,coreference,contrastive learning scheme based on coreference,0.6059576869010925
translation,272,7,model,coreference,between,source and contextual sentences,coreference between source and contextual sentences,0.5863975882530212
translation,272,7,model,corefcl,has,novel data augmentation,corefcl has novel data augmentation,0.6029946208000183
translation,272,7,model,model,propose,corefcl,model propose corefcl,0.7007261514663696
translation,272,20,model,coreference - based contrastive learning,for,context - aware nmt ( corefcl ),coreference - based contrastive learning for context - aware nmt ( corefcl ),0.6101919412612915
translation,272,20,model,coreference - based contrastive learning,has,novel data augmentation,coreference - based contrastive learning has novel data augmentation,0.5152198672294617
translation,272,20,model,context - aware nmt ( corefcl ),has,novel data augmentation,context - aware nmt ( corefcl ) has novel data augmentation,0.5711424350738525
translation,272,20,model,model,propose,coreference - based contrastive learning,model propose coreference - based contrastive learning,0.6516533493995667
translation,272,58,model,"transformer ( vaswani et al. , 2017 )",as,base model architecture,"transformer ( vaswani et al. , 2017 ) as base model architecture",0.5217770338058472
translation,272,58,model,model,consider,"transformer ( vaswani et al. , 2017 )","model consider transformer ( vaswani et al. , 2017 )",0.6662352681159973
translation,272,8,results,automatically detected coreference mentions,in,contextual sentence,automatically detected coreference mentions in contextual sentence,0.48417821526527405
translation,272,8,results,corefcl,train,model,corefcl train model,0.7059484720230103
translation,272,8,results,model,sensitive to,coreference inconsistency,model sensitive to coreference inconsistency,0.6837717890739441
translation,272,8,results,automatically detected coreference mentions,has,corefcl,automatically detected coreference mentions has corefcl,0.6044870018959045
translation,272,8,results,results,By corrupting,automatically detected coreference mentions,results By corrupting automatically detected coreference mentions,0.7442753911018372
translation,272,27,results,corefcl,consistently improves,overall bleu,corefcl consistently improves overall bleu,0.7447424530982971
translation,272,27,results,overall bleu,over,baseline models,overall bleu over baseline models,0.6034874320030212
translation,272,27,results,baseline models,without,corefcl,baseline models without corefcl,0.6717492938041687
translation,272,27,results,all translation tasks,has,corefcl,all translation tasks has corefcl,0.6151500344276428
translation,272,27,results,results,In,all translation tasks,results In all translation tasks,0.4590838849544525
translation,272,133,results,all context - aware models,show,moderate improvements,all context - aware models show moderate improvements,0.6222496628761292
translation,272,133,results,moderate improvements,over,sentence - level ( sent-level ) baseline,moderate improvements over sentence - level ( sent-level ) baseline,0.6419764161109924
translation,272,133,results,baseline systems,has,all context - aware models,baseline systems has all context - aware models,0.5344279408454895
translation,272,133,results,results,Among,baseline systems,results Among baseline systems,0.5861256718635559
translation,272,137,results,bleu gain,of,up to 1.4,bleu gain of up to 1.4,0.6158181428909302
translation,272,137,results,bleu gain,of,1.6/2.8 ( detokenized / char-level bleu ),bleu gain of 1.6/2.8 ( detokenized / char-level bleu ),0.5303949117660522
translation,272,137,results,up to 1.4,in,en- de tasks,up to 1.4 in en- de tasks,0.6040400266647339
translation,272,137,results,1.6/2.8 ( detokenized / char-level bleu ),in,en-ko subtitles task,1.6/2.8 ( detokenized / char-level bleu ) in en-ko subtitles task,0.4913063943386078
translation,272,137,results,corefcl,has,outperformed,corefcl has outperformed,0.6076923608779907
translation,272,137,results,outperformed,has,vanilla counterparts,outperformed has vanilla counterparts,0.5970774292945862
translation,272,138,results,corefcl,consistently improves,bleu,corefcl consistently improves bleu,0.7225595116615295
translation,272,138,results,corefcl,achieves,better results,corefcl achieves better results,0.6469972133636475
translation,272,138,results,bleu,on,all tasks,bleu on all tasks,0.4929596185684204
translation,272,138,results,better results,on,iwslt and en- ko subtitles tasks,better results on iwslt and en- ko subtitles tasks,0.522110641002655
translation,272,138,results,results,observed,corefcl,results observed corefcl,0.6347798705101013
translation,272,144,results,baselines,in,scoring accuracy,baselines in scoring accuracy,0.5005404353141785
translation,272,144,results,scoring accuracy,for,all models,scoring accuracy for all models,0.5528081655502319
translation,272,144,results,all models,by,up to 5.5 %,all models by up to 5.5 %,0.6131208539009094
translation,272,144,results,slight improvements,in,bleu scores,slight improvements in bleu scores,0.5239941477775574
translation,272,144,results,corefcl,has,significantly improves,corefcl has significantly improves,0.6193208694458008
translation,272,144,results,significantly improves,has,baselines,significantly improves has baselines,0.6309170722961426
translation,272,144,results,results,has,corefcl,results has corefcl,0.5511930584907532
translation,272,145,results,corefcl,achieved,substantial accuracy gain,corefcl achieved substantial accuracy gain,0.7119589447975159
translation,272,145,results,substantial accuracy gain,on,models,substantial accuracy gain on models,0.5732534527778625
translation,272,145,results,models,trained on,wmt,models trained on wmt,0.7463639974594116
translation,272,145,results,results,has,interesting finding,results has interesting finding,0.566483199596405
translation,272,152,results,both types of corruptions,results in,better performance,both types of corruptions results in better performance,0.6499149203300476
translation,272,152,results,results,using,both types of corruptions,results using both types of corruptions,0.6170620322227478
translation,272,165,results,coreference mentions,between,source and target sentence,coreference mentions between source and target sentence,0.5731558203697205
translation,272,165,results,corefcl,effectively generates,contrastive examples,corefcl effectively generates contrastive examples,0.6810139417648315
translation,272,165,results,contrastive examples,for applying,contrastive learning,contrastive examples for applying contrastive learning,0.6824844479560852
translation,272,165,results,contrastive learning,on,contextaware nmt models,contrastive learning on contextaware nmt models,0.54657381772995
translation,272,165,results,coreference mentions,has,corefcl,coreference mentions has corefcl,0.5924654603004456
translation,272,166,results,corefcl,consistently improves,translation quality,corefcl consistently improves translation quality,0.776257336139679
translation,272,166,results,corefcl,consistently improves,pronoun resolution accuracy,corefcl consistently improves pronoun resolution accuracy,0.7406848073005676
translation,273,89,hyperparameters,data imbalance,among,language pairs,data imbalance among language pairs,0.5898205637931824
translation,273,89,hyperparameters,data imbalance,use,sampling temperature,data imbalance use sampling temperature,0.6918148398399353
translation,273,89,hyperparameters,sampling temperature,of,5.0,sampling temperature of 5.0,0.5915728211402893
translation,273,89,hyperparameters,sampling temperature,upsamples,lowresource pairs,sampling temperature upsamples lowresource pairs,0.7864478826522827
translation,273,89,hyperparameters,5.0,upsamples,lowresource pairs,5.0 upsamples lowresource pairs,0.7558996081352234
translation,273,89,hyperparameters,hyperparameters,To counteract,data imbalance,hyperparameters To counteract data imbalance,0.6560487151145935
translation,273,89,hyperparameters,hyperparameters,use,sampling temperature,hyperparameters use sampling temperature,0.6309854984283447
translation,273,90,hyperparameters,similarity regularizer,use,weight,similarity regularizer use weight,0.6393327116966248
translation,273,90,hyperparameters,weight,of,0.1,weight of 0.1,0.6301047801971436
translation,273,90,hyperparameters,0.1,on,auxiliary loss,0.1 on auxiliary loss,0.5245068669319153
translation,273,92,hyperparameters,adapters,use,bottleneck dimension,adapters use bottleneck dimension,0.637446939945221
translation,273,92,hyperparameters,bottleneck dimension,of,256,bottleneck dimension of 256,0.6389319896697998
translation,273,92,hyperparameters,hyperparameters,For,adapters,hyperparameters For adapters,0.5596776008605957
translation,273,94,hyperparameters,decoding,use,beam size,decoding use beam size,0.66057288646698
translation,273,94,hyperparameters,decoding,limit,maximum output length,decoding limit maximum output length,0.6415003538131714
translation,273,94,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
translation,273,94,hyperparameters,maximum output length,to,1.3 * source length + 5,maximum output length to 1.3 * source length + 5,0.5501240491867065
translation,273,94,hyperparameters,hyperparameters,When,decoding,hyperparameters When decoding,0.6649425029754639
translation,273,94,hyperparameters,hyperparameters,limit,maximum output length,hyperparameters limit maximum output length,0.6677269339561462
translation,273,91,model,residual layer,skipped after,third encoder layer,residual layer skipped after third encoder layer,0.7335500121116638
translation,273,91,model,with residual removal,has,residual layer,with residual removal has residual layer,0.632906436920166
translation,273,91,model,model,For,with residual removal,model For with residual removal,0.61478191614151
translation,273,104,results,each language pair individually ( lang. dep. finetune ),see,large gains,each language pair individually ( lang. dep. finetune ) see large gains,0.5568097233772278
translation,273,104,results,large gains,with,average bleu score,large gains with average bleu score,0.6194488406181335
translation,273,104,results,average bleu score,increasing from,11.5 to 20.6,average bleu score increasing from 11.5 to 20.6,0.6477416753768921
translation,273,106,results,models,results in,low average bleu score,models results in low average bleu score,0.5606135725975037
translation,273,106,results,low average bleu score,of,5.7,low average bleu score of 5.7,0.5511942505836487
translation,273,106,results,results,averaging,models,results averaging models,0.6568225622177124
translation,273,108,results,fine- tuning,on,all 30 language directions together,fine- tuning on all 30 language directions together,0.4853299856185913
translation,273,108,results,fine- tuning,achieve,comparable gain,fine- tuning achieve comparable gain,0.7175818085670471
translation,273,108,results,all 30 language directions together,achieve,comparable gain,all 30 language directions together achieve comparable gain,0.678218424320221
translation,273,108,results,comparable gain,in,performance,comparable gain in performance,0.5660616159439087
translation,273,108,results,comparable gain,results in,bleu score,comparable gain results in bleu score,0.5595415234565735
translation,273,108,results,bleu score,of,19.6,bleu score of 19.6,0.5433894395828247
translation,273,110,results,fine- tuning,on,all the languages,fine- tuning on all the languages,0.5073578357696533
translation,273,110,results,average bleu score,of,big baseline model,average bleu score of big baseline model,0.5296708345413208
translation,273,110,results,average bleu score,from,15.4 to 27.4,average bleu score from 15.4 to 27.4,0.5484388470649719
translation,273,110,results,improve,has,average bleu score,improve has average bleu score,0.5387898087501526
translation,273,112,results,adapters,into,large baseline model,adapters into large baseline model,0.5884650349617004
translation,273,112,results,adapters,achieve,23.0 bleu,adapters achieve 23.0 bleu,0.6488025188446045
translation,273,121,results,clear improvement,of,baseline model,clear improvement of baseline model,0.5772154331207275
translation,273,121,results,baseline model,trained on,provided training data,baseline model trained on provided training data,0.7725515365600586
translation,273,121,results,results,see,clear improvement,results see clear improvement,0.6347724199295044
translation,273,122,results,225 k sentences,for,jv-ta and ta-jv each,225 k sentences for jv-ta and ta-jv each,0.701951265335083
translation,273,122,results,synthetic data,has,225 k sentences,synthetic data has 225 k sentences,0.5788988471031189
translation,273,122,results,improve,has,performance,improve has performance,0.5578044652938843
translation,273,122,results,results,Adding,synthetic data,results Adding synthetic data,0.6091068983078003
translation,273,123,results,data,generated from,ta-jv,data generated from ta-jv,0.6526572704315186
translation,273,123,results,performing better,than,other data,performing better than other data,0.6098266243934631
translation,273,123,results,both directions,has,data,both directions has data,0.6137474179267883
translation,273,123,results,results,For,both directions,results For both directions,0.5957531929016113
translation,273,125,results,additional gains,to,best performance,additional gains to best performance,0.5473192930221558
translation,273,125,results,best performance,of,16.0 and 11.7 bleu points,best performance of 16.0 and 11.7 bleu points,0.5385121703147888
translation,273,125,results,16.0 and 11.7 bleu points,for,both directions,16.0 and 11.7 bleu points for both directions,0.6390277743339539
translation,273,137,results,gains,when combining,similarity regularizer,gains when combining similarity regularizer,0.7503591775894165
translation,273,137,results,gains,when combining,adapters,gains when combining adapters,0.7645480036735535
translation,273,137,results,results,see,gains,results see gains,0.5693391561508179
translation,273,142,results,no improvement,in,overall average bleu score,no improvement in overall average bleu score,0.5118082761764526
translation,273,142,results,no improvement,observe,some gain,no improvement observe some gain,0.604188084602356
translation,273,142,results,some gain,in,lowest- resource direction,some gain in lowest- resource direction,0.5155562162399292
translation,273,142,results,lowest- resource direction,of,jv?ta,lowest- resource direction of jv?ta,0.6162547469139099
translation,273,142,results,jv?ta,which has,46 k parallel data,jv?ta which has 46 k parallel data,0.6266751289367676
translation,273,142,results,results,observe,some gain,results observe some gain,0.6738095879554749
translation,273,146,results,previous model,on,parallel data and the synthetic data,previous model on parallel data and the synthetic data,0.5651970505714417
translation,273,146,results,previous model,gave,additional improvement,previous model gave additional improvement,0.6428602933883667
translation,273,146,results,additional improvement,of,0.5 bleu,additional improvement of 0.5 bleu,0.5510326027870178
translation,273,146,results,results,Fine-tuning,previous model,results Fine-tuning previous model,0.6855446696281433
translation,273,147,results,our best system,uses,additional similarity regularization and adapters,our best system uses additional similarity regularization and adapters,0.6222773194313049
translation,273,147,results,our best system,improves,average bleu,our best system improves average bleu,0.6560862064361572
translation,273,147,results,additional similarity regularization and adapters,during,training,additional similarity regularization and adapters during training,0.6808913350105286
translation,273,147,results,average bleu,by,0.2 points,average bleu by 0.2 points,0.5757222175598145
translation,273,147,results,0.2 points,to,28.1,0.2 points to 28.1,0.5579290390014648
translation,273,147,results,previous improvements,has,our best system,previous improvements has our best system,0.5688782334327698
translation,273,147,results,results,on top of,previous improvements,results on top of previous improvements,0.6768240332603455
translation,273,148,results,submitted system,achieves,28.6 bleu,submitted system achieves 28.6 bleu,0.6470882296562195
translation,273,148,results,28.6 bleu,on average,blind test set,28.6 bleu on average blind test set,0.6415055394172668
translation,273,148,results,28.6 bleu,on,blind test set,28.6 bleu on blind test set,0.49505969882011414
translation,273,148,results,results,has,submitted system,results has submitted system,0.6237063407897949
translation,274,128,ablation-analysis,single-source monolingual models,observe,relatively smaller drop ( up to 13 percentage points ),single-source monolingual models observe relatively smaller drop ( up to 13 percentage points ),0.5738081336021423
translation,274,128,ablation-analysis,relatively smaller drop ( up to 13 percentage points ),in,performance,relatively smaller drop ( up to 13 percentage points ) in performance,0.5561637878417969
translation,274,128,ablation-analysis,performance,when testing,trg - src,performance when testing trg - src,0.7041894197463989
translation,274,128,ablation-analysis,trg - src,on,trg - all,trg - src on trg - all,0.5823729634284973
translation,274,128,ablation-analysis,up to 27 points ),when testing,trg - src 1,up to 27 points ) when testing trg - src 1,0.6911648511886597
translation,274,128,ablation-analysis,trg - src 1,on,trg - src 2,trg - src 1 on trg - src 2,0.6002298593521118
translation,274,128,ablation-analysis,larger drop (,has,up to 27 points ),larger drop ( has up to 27 points ),0.5884838104248047
translation,274,128,ablation-analysis,ablation analysis,For,single-source monolingual models,ablation analysis For single-source monolingual models,0.5497794151306152
translation,274,139,ablation-analysis,largest drop,for,neural models,largest drop for neural models,0.6251639723777771
translation,274,139,ablation-analysis,neural models,is,6.7 accuracy points,neural models is 6.7 accuracy points,0.5527151226997375
translation,274,139,ablation-analysis,smallest performance drop,for,handcr,smallest performance drop for handcr,0.6118091344833374
translation,274,139,ablation-analysis,smallest performance drop,is,12.7,smallest performance drop is 12.7,0.5702269077301025
translation,274,139,ablation-analysis,handcr,has,. +svm,handcr has . +svm,0.5899842977523804
translation,274,139,ablation-analysis,ablation analysis,has,largest drop,ablation analysis has largest drop,0.5771240592002869
translation,274,174,ablation-analysis,many of hand-crafted translationese features,are,statistically significant,many of hand-crafted translationese features are statistically significant,0.5636191368103027
translation,274,174,ablation-analysis,statistically significant,for predicting,predictions,statistically significant for predicting predictions,0.7579913139343262
translation,274,174,ablation-analysis,predictions,of,neural models,predictions of neural models,0.5559889078140259
translation,274,174,ablation-analysis,ablation analysis,conclude that,many of hand-crafted translationese features,ablation analysis conclude that many of hand-crafted translationese features,0.5731056332588196
translation,274,6,baselines,traditional feature - engineering - based approach,to,feature - learning - based one,traditional feature - engineering - based approach to feature - learning - based one,0.5455735921859741
translation,274,6,baselines,hand- crafted features,explain,variance,hand- crafted features explain variance,0.6689175963401794
translation,274,6,baselines,variance,in,neural models ' predictions,variance in neural models ' predictions,0.5268391370773315
translation,274,6,baselines,baselines,compare,traditional feature - engineering - based approach,baselines compare traditional feature - engineering - based approach,0.6648813486099243
translation,274,6,baselines,baselines,analyse,neural architectures,baselines analyse neural architectures,0.7060446739196777
translation,274,22,baselines,translationese classification,compare to,classical feature - engineering - based approaches,translationese classification compare to classical feature - engineering - based approaches,0.6527284979820251
translation,274,72,baselines,fasttext classifier ( ft ),is,efficient neural network model,fasttext classifier ( ft ) is efficient neural network model,0.5881984829902649
translation,274,72,baselines,efficient neural network model,with,single hidden layer,efficient neural network model with single hidden layer,0.6391416788101196
translation,274,72,baselines,baselines,has,fasttext classifier ( ft ),baselines has fasttext classifier ( ft ),0.5616289377212524
translation,274,80,baselines,baselines,has,long short - term memory network ( lstm ),baselines has long short - term memory network ( lstm ),0.552681565284729
translation,274,85,baselines,baselines,has,simplified transformer,baselines has simplified transformer,0.5889068245887756
translation,274,112,baselines,+svm,in,multi-source ( trg - all ) settings,+svm in multi-source ( trg - all ) settings,0.5187376737594604
translation,274,112,baselines,baselines,has,+svm,baselines has +svm,0.5966928005218506
translation,274,46,experimental-setup,support vector ma-chine classifier ( svm ),with,linear kernel,support vector ma-chine classifier ( svm ) with linear kernel,0.6221487522125244
translation,274,46,experimental-setup,support vector ma-chine classifier ( svm ),fit,hyperparameter c,support vector ma-chine classifier ( svm ) fit hyperparameter c,0.6921727061271667
translation,274,46,experimental-setup,hyperparameter c,on,validation set,hyperparameter c on validation set,0.5168677568435669
translation,274,46,experimental-setup,experimental setup,use,support vector ma-chine classifier ( svm ),experimental setup use support vector ma-chine classifier ( svm ),0.6303179860115051
translation,274,48,experimental-setup,surface features,has,syllable ratio,surface features has syllable ratio,0.5697816014289856
translation,274,48,experimental-setup,surface features,has,paragraph length,surface features has paragraph length,0.5197899341583252
translation,274,63,experimental-setup,publicly available language specific 300 - dimensional pre-trained wiki word vector models,trained on,wikipedia,publicly available language specific 300 - dimensional pre-trained wiki word vector models trained on wikipedia,0.7246326208114624
translation,274,63,experimental-setup,wikipedia,using,fasttext,wikipedia using fasttext,0.671398401260376
translation,274,63,experimental-setup,experimental setup,work with,publicly available language specific 300 - dimensional pre-trained wiki word vector models,experimental setup work with publicly available language specific 300 - dimensional pre-trained wiki word vector models,0.6279617547988892
translation,274,81,experimental-setup,single- layer uni-directional lstm,with,embedding and hidden layer,single- layer uni-directional lstm with embedding and hidden layer,0.6055207252502441
translation,274,81,experimental-setup,embedding and hidden layer,with,128 dimensions,embedding and hidden layer with 128 dimensions,0.6241577863693237
translation,274,81,experimental-setup,experimental setup,use,single- layer uni-directional lstm,experimental setup use single- layer uni-directional lstm,0.5711415410041809
translation,274,84,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,274,84,experimental-setup,learning rate,of,1?10 ?2,learning rate of 1?10 ?2,0.6394751071929932
translation,274,84,experimental-setup,adam optimiser,with,pytorch defaults,adam optimiser with pytorch defaults,0.6112436652183533
translation,274,94,experimental-setup,fine-tuning,done with,simpletransformers 3 library,fine-tuning done with simpletransformers 3 library,0.6403002738952637
translation,274,94,experimental-setup,experimental setup,has,fine-tuning,experimental setup has fine-tuning,0.5605509877204895
translation,274,97,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,274,97,experimental-setup,learning rate,of,4 ? 10 ?5,learning rate of 4 ? 10 ?5,0.6538532972335815
translation,274,97,experimental-setup,adam optimiser,with,epsilon 1 ? 10 ?8,adam optimiser with epsilon 1 ? 10 ?8,0.6561922430992126
translation,274,7,experiments,pre-trained neural word embeddings,as well as,several end-to - end neural architectures,pre-trained neural word embeddings as well as several end-to - end neural architectures,0.6034484505653381
translation,274,7,experiments,several end-to - end neural architectures,in,monolingual and multilingual settings,several end-to - end neural architectures in monolingual and multilingual settings,0.5117020606994629
translation,274,7,experiments,several end-to - end neural architectures,compare them to,feature - engineering - based svm classifiers,several end-to - end neural architectures compare them to feature - engineering - based svm classifiers,0.647308886051178
translation,274,117,experiments,en - all dataset,is,most difficult,en - all dataset is most difficult,0.5866740942001343
translation,274,117,experiments,most difficult,for,most of the models,most difficult for most of the models,0.609833300113678
translation,274,117,experiments,most of the models,among,trg - all datasets,most of the models among trg - all datasets,0.6210108995437622
translation,274,66,model,similarity - based classification,with,svms,similarity - based classification with svms,0.6331326961517334
translation,274,66,model,svms,where,kernel,svms where kernel,0.6301271915435791
translation,274,66,model,kernel,represents,similarities between pairs of texts,kernel represents similarities between pairs of texts,0.5495080947875977
translation,274,66,model,model,perform,similarity - based classification,model perform similarity - based classification,0.6229631304740906
translation,274,73,model,fast - text model,represents,texts,fast - text model represents texts,0.6514810919761658
translation,274,73,model,texts,as,bag of words,texts as bag of words,0.48831236362457275
translation,274,73,model,texts,as,bag of n-gram tokens,texts as bag of n-gram tokens,0.5399507284164429
translation,274,73,model,model,has,fast - text model,model has fast - text model,0.5587993860244751
translation,274,82,model,embedding layer,uses,wordpiece subunits,embedding layer uses wordpiece subunits,0.6180793046951294
translation,274,82,model,model,has,embedding layer,model has embedding layer,0.5496405363082886
translation,274,83,model,output,to,binary linear classifier,output to binary linear classifier,0.5663198232650757
translation,274,83,model,model,pool ( average ),all hidden states,model pool ( average ) all hidden states,0.7075594067573547
translation,274,89,model,model,introduce,simple cumulative sum-based contextualisation,model introduce simple cumulative sum-based contextualisation,0.623968780040741
translation,274,96,model,dropout,with,probability 0.1,dropout with probability 0.1,0.6266955733299255
translation,274,96,model,dropout,fed into,binary linear classifier,dropout fed into binary linear classifier,0.6854217052459717
translation,274,96,model,model,undergoes,dropout,model undergoes dropout,0.7485346794128418
translation,274,8,results,outperform,by,more than 20 accuracy points,outperform by more than 20 accuracy points,0.6007344126701355
translation,274,8,results,other approaches,by,more than 20 accuracy points,other approaches by more than 20 accuracy points,0.5547681450843811
translation,274,8,results,other approaches,with,bert - based model,other approaches with bert - based model,0.6099295616149902
translation,274,8,results,bert - based model,performing,best,bert - based model performing best,0.6771666407585144
translation,274,8,results,best,in,multilingual settings,best in multilingual settings,0.5256237983703613
translation,274,8,results,neural architectures,has,outperform,neural architectures has outperform,0.5967966914176941
translation,274,8,results,outperform,has,other approaches,outperform has other approaches,0.6127994656562805
translation,274,8,results,results,show,neural architectures,results show neural architectures,0.6120776534080505
translation,274,109,results,outperforms,followed closely by,other end-to - end neural architectures,outperforms followed closely by other end-to - end neural architectures,0.6639248728752136
translation,274,109,results,other architectures,followed closely by,other end-to - end neural architectures,other architectures followed closely by other end-to - end neural architectures,0.6268558502197266
translation,274,109,results,bert model,has,outperforms,bert model has outperforms,0.6637536287307739
translation,274,109,results,outperforms,has,other architectures,outperforms has other architectures,0.5945256352424622
translation,274,109,results,results,has,bert model,results has bert model,0.5506546497344971
translation,274,110,results,pre-trained wiki embeddings,improving,accuracy,pre-trained wiki embeddings improving accuracy,0.6112617254257202
translation,274,110,results,accuracy,of,fasttext method,accuracy of fasttext method,0.6088619828224182
translation,274,110,results,fasttext method,in,all cases,fasttext method in all cases,0.5453058481216431
translation,274,110,results,results,Using,pre-trained wiki embeddings,results Using pre-trained wiki embeddings,0.6020264625549316
translation,274,111,results,wiki +svm,performs,best,wiki +svm performs best,0.6197443008422852
translation,274,111,results,wiki +svm,shows,lower accuracy,wiki +svm shows lower accuracy,0.6608819365501404
translation,274,111,results,best,in,single-source settings,best in single-source settings,0.5226101875305176
translation,274,111,results,lower accuracy,than,handcr,lower accuracy than handcr,0.616152286529541
translation,274,111,results,svm classifier,has,wiki +svm,svm classifier has wiki +svm,0.5899085998535156
translation,274,114,results,monolingual single-source settings,observe that,accuracy,monolingual single-source settings observe that accuracy,0.5591719150543213
translation,274,114,results,accuracy,is,slightly lower,accuracy is slightly lower,0.5645228624343872
translation,274,114,results,slightly lower,when,source language,slightly lower when source language,0.6432368159294128
translation,274,114,results,source language,typologically closer to,text language,source language typologically closer to text language,0.6405375599861145
translation,274,114,results,results,In,monolingual single-source settings,results In monolingual single-source settings,0.48344239592552185
translation,274,118,results,all - all [ 3 ] setting,exhibits,comparable accuracy,all - all [ 3 ] setting exhibits comparable accuracy,0.6508976817131042
translation,274,118,results,comparable accuracy,to,trg - all setting,comparable accuracy to trg - all setting,0.5473600625991821
translation,274,118,results,comparable accuracy,for,neural models,comparable accuracy for neural models,0.628208577632904
translation,274,118,results,comparable accuracy,for,svm,comparable accuracy for svm,0.6768953204154968
translation,274,118,results,trg - all setting,for,neural models,trg - all setting for neural models,0.6798914670944214
translation,274,118,results,drop,around,9 points,drop around 9 points,0.7375380396842957
translation,274,118,results,results,has,all - all [ 3 ] setting,results has all - all [ 3 ] setting,0.5455302000045776
translation,274,134,results,multi-source monolingual models ( trg - all ),testing,trg - src 1 and trg - src 2 datasets,multi-source monolingual models ( trg - all ) testing trg - src 1 and trg - src 2 datasets,0.7169204354286194
translation,274,134,results,multi-source monolingual models ( trg - all ),on,- all,multi-source monolingual models ( trg - all ) on - all,0.5348554849624634
translation,274,134,results,trg - src 1 and trg - src 2 datasets,shows,slight increase,trg - src 1 and trg - src 2 datasets shows slight increase,0.6867935657501221
translation,274,134,results,trg - src 1 and trg - src 2 datasets,shows,slight decrease,trg - src 1 and trg - src 2 datasets shows slight decrease,0.681664764881134
translation,274,134,results,slight increase,in,performance,slight increase in performance,0.5485352873802185
translation,274,134,results,performance,for,source language,performance for source language,0.5995086431503296
translation,274,134,results,source language,more distant from,target,source language more distant from target,0.6944144368171692
translation,274,134,results,slight decrease,for,more closelyrelated source language,slight decrease for more closelyrelated source language,0.5799981355667114
translation,274,134,results,results,For,multi-source monolingual models ( trg - all ),results For multi-source monolingual models ( trg - all ),0.5557354688644409
translation,274,135,results,increases,for,neural models,increases for neural models,0.6901533007621765
translation,274,135,results,increases,not for,handcr,increases not for handcr,0.7126427292823792
translation,274,135,results,de -es set,has,performance,de -es set has performance,0.6044244170188904
translation,274,135,results,handcr,has,. +svm,handcr has . +svm,0.5899842977523804
translation,274,135,results,results,For,de -es set,results For de -es set,0.7247368693351746
translation,274,164,results,features,significant for,neural models,features significant for neural models,0.601420521736145
translation,274,164,results,features,significant for,gold labels,features significant for gold labels,0.6091210842132568
translation,274,164,results,features,significant for,gold labels,features significant for gold labels,0.6091210842132568
translation,274,164,results,neural models,largely overlap with,features,neural models largely overlap with features,0.7320979237556458
translation,274,164,results,features,significant for,gold labels,features significant for gold labels,0.6091210842132568
translation,274,164,results,f 1 - score,is,0.89,f 1 - score is 0.89,0.5587118864059448
translation,274,164,results,f 1 - score,is,0.75,f 1 - score is 0.75,0.5735815167427063
translation,274,164,results,f 1 - score,is,0.99,f 1 - score is 0.99,0.5676956176757812
translation,274,164,results,0.89,for,lstm,0.89 for lstm,0.5781234502792358
translation,274,164,results,0.75,for,simplified transformer,0.75 for simplified transformer,0.6348512172698975
translation,274,164,results,0.99,for,bert,0.99 for bert,0.6582204699516296
translation,274,164,results,results,observe,features,results observe features,0.5756442546844482
translation,274,209,results,svms,trained with,hand-crafted linguistically - inspired features,svms trained with hand-crafted linguistically - inspired features,0.7172350883483887
translation,274,209,results,classification accuracy,of,all trg - src and trg - all models,classification accuracy of all trg - src and trg - all models,0.5626621246337891
translation,274,209,results,classification accuracy,on,trg - src and trg - all test sets,classification accuracy on trg - src and trg - all test sets,0.5316386222839355
translation,274,209,results,all trg - src and trg - all models,on,trg - src and trg - all test sets,all trg - src and trg - all models on trg - src and trg - all test sets,0.5417761206626892
translation,274,209,results,neural - classifier -based models,has,substantially outperform,neural - classifier -based models has substantially outperform,0.5709025263786316
translation,274,209,results,substantially outperform,has,other architectures,substantially outperform has other architectures,0.5724412202835083
translation,274,209,results,other architectures,has,svms,other architectures has svms,0.5632339119911194
translation,274,209,results,results,has,neural - classifier -based models,results has neural - classifier -based models,0.53188556432724
translation,275,32,ablation-analysis,off-target translation,not only appears in,zero-shot directions,off-target translation not only appears in zero-shot directions,0.6414563655853271
translation,275,32,ablation-analysis,off-target translation,exists in,englishcentric pairs,off-target translation exists in englishcentric pairs,0.6723058223724365
translation,275,32,ablation-analysis,ablation analysis,show,off-target translation,ablation analysis show off-target translation,0.6382432579994202
translation,275,107,ablation-analysis,off-target rate,from,24.5 %,off-target rate from 24.5 %,0.5234201550483704
translation,275,107,ablation-analysis,24.5 %,down to,6.0 %,24.5 % down to 6.0 %,0.7112425565719604
translation,275,107,ablation-analysis,tlp,has,significantly reduces,tlp has significantly reduces,0.6033301949501038
translation,275,107,ablation-analysis,significantly reduces,has,off-target rate,significantly reduces has off-target rate,0.5969240665435791
translation,275,107,ablation-analysis,ablation analysis,has,tlp,ablation analysis has tlp,0.5419321060180664
translation,275,138,ablation-analysis,tgp,in,zero-shot setting,tgp in zero-shot setting,0.5592715740203857
translation,275,138,ablation-analysis,tgp,shown to,greatly improve,tgp shown to greatly improve,0.8004431128501892
translation,275,138,ablation-analysis,tgp,shown to,significantly reduces,tgp shown to significantly reduces,0.7127834558486938
translation,275,138,ablation-analysis,24.5 % ? 2.0 %,on,wmt,24.5 % ? 2.0 % on wmt,0.562422513961792
translation,275,138,ablation-analysis,65.8 % ? 31.1 %,on,opus,65.8 % ? 31.1 % on opus,0.5862252116203308
translation,275,138,ablation-analysis,greatly improve,has,translation performance,greatly improve has translation performance,0.5083110332489014
translation,275,138,ablation-analysis,significantly reduces,has,off-target occurrences,significantly reduces has off-target occurrences,0.5935268998146057
translation,275,138,ablation-analysis,significantly reduces,has,24.5 % ? 2.0 %,significantly reduces has 24.5 % ? 2.0 %,0.5945395827293396
translation,275,138,ablation-analysis,off-target occurrences,has,24.5 % ? 2.0 %,off-target occurrences has 24.5 % ? 2.0 %,0.5565140247344971
translation,275,138,ablation-analysis,ablation analysis,has,tgp,ablation analysis has tgp,0.5390928983688354
translation,275,6,experiments,off-target translation,is,dominant,off-target translation is dominant,0.5805360078811646
translation,275,6,experiments,dominant,even in,strong multilingual systems,dominant even in strong multilingual systems,0.6341270208358765
translation,275,6,experiments,dominant,trained on,massive multilingual corpora,dominant trained on massive multilingual corpora,0.6930815577507019
translation,275,38,hyperparameters,training batches,between,high- resource and low-resource language pairs,training batches between high- resource and low-resource language pairs,0.6406301856040955
translation,275,38,hyperparameters,training batches,adopt,temperature - based sampling,training batches adopt temperature - based sampling,0.6719846129417419
translation,275,38,hyperparameters,temperature - based sampling,to up / down - sample,bilingual data,temperature - based sampling to up / down - sample bilingual data,0.7967386245727539
translation,275,38,hyperparameters,hyperparameters,To balance,training batches,hyperparameters To balance training batches,0.5650628209114075
translation,275,38,hyperparameters,hyperparameters,adopt,temperature - based sampling,hyperparameters adopt temperature - based sampling,0.6435620784759521
translation,275,39,hyperparameters,hyperparameters,set,temperature,hyperparameters set temperature,0.6725029349327087
translation,275,70,hyperparameters,update frequency,=,200,update frequency = 200,0.7053821682929993
translation,275,70,hyperparameters,n,=,200,n = 200,0.700522780418396
translation,275,70,hyperparameters,update frequency,has,n,update frequency has n,0.5869150161743164
translation,275,70,hyperparameters,hyperparameters,set,update frequency,hyperparameters set update frequency,0.6293954253196716
translation,275,92,hyperparameters,beam search decoding,with,beam size,beam search decoding with beam size,0.6320339441299438
translation,275,92,hyperparameters,beam search decoding,with,length penalty,beam search decoding with length penalty,0.6329136490821838
translation,275,92,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
translation,275,92,hyperparameters,length penalty,of,1.0,length penalty of 1.0,0.572574257850647
translation,275,7,model,joint approach,to regularize,nmt models,joint approach to regularize nmt models,0.7343630790710449
translation,275,7,model,nmt models,at,representation - level and gradient-level,nmt models at representation - level and gradient-level,0.5568845272064209
translation,275,7,model,model,propose,joint approach,model propose joint approach,0.7219250202178955
translation,275,8,model,representation level,leverage,auxiliary target language prediction task,representation level leverage auxiliary target language prediction task,0.701815664768219
translation,275,8,model,auxiliary target language prediction task,to regularize,decoder outputs,auxiliary target language prediction task to regularize decoder outputs,0.6664860248565674
translation,275,8,model,model,At,representation level,model At representation level,0.581249475479126
translation,275,9,model,gradient level,leverage,small amount of direct data ( in thousands of sentence pairs ),gradient level leverage small amount of direct data ( in thousands of sentence pairs ),0.6979543566703796
translation,275,9,model,small amount of direct data ( in thousands of sentence pairs ),to regularize,model gradients,small amount of direct data ( in thousands of sentence pairs ) to regularize model gradients,0.7365629076957703
translation,275,9,model,model,At,gradient level,model At gradient level,0.5816574096679688
translation,275,21,model,model,reducing,offtarget translation,model reducing offtarget translation,0.6399698853492737
translation,275,27,model,joint representationlevel and gradient - level regularization,to directly address,multilingual system 's limitation,joint representationlevel and gradient - level regularization to directly address multilingual system 's limitation,0.6920447945594788
translation,275,27,model,multilingual system 's limitation,of modeling,different target languages,multilingual system 's limitation of modeling different target languages,0.6877480745315552
translation,275,27,model,model,propose,joint representationlevel and gradient - level regularization,model propose joint representationlevel and gradient - level regularization,0.6428316235542297
translation,275,28,model,representationlevel,regulate,nmt decoder states,representationlevel regulate nmt decoder states,0.6543599963188171
translation,275,28,model,nmt decoder states,by adding,auxiliary target language prediction ( tlp ) loss,nmt decoder states by adding auxiliary target language prediction ( tlp ) loss,0.7169089317321777
translation,275,28,model,auxiliary target language prediction ( tlp ) loss,such that,decoder outputs,auxiliary target language prediction ( tlp ) loss such that decoder outputs,0.582625687122345
translation,275,28,model,decoder outputs,retained with,target language information,decoder outputs retained with target language information,0.6680792570114136
translation,275,28,model,model,At,representationlevel,model At representationlevel,0.5473781824111938
translation,275,29,model,gradient - level,leverage,small amount of direct data ( in thousands of sentence pairs ),gradient - level leverage small amount of direct data ( in thousands of sentence pairs ),0.7113617658615112
translation,275,29,model,small amount of direct data ( in thousands of sentence pairs ),to project,model gradients,small amount of direct data ( in thousands of sentence pairs ) to project model gradients,0.7064999341964722
translation,275,29,model,model gradients,for,each target language,model gradients for each target language,0.5790905952453613
translation,275,29,model,tgp,for,target - gradient- projection,tgp for target - gradient- projection,0.6089884638786316
translation,275,29,model,model,At,gradient - level,model At gradient - level,0.5771315693855286
translation,275,47,model,sinusoidal positional embedding,for,position information,sinusoidal positional embedding for position information,0.6279935240745544
translation,275,47,model,model,decoder states,sinusoidal positional embedding,model decoder states sinusoidal positional embedding,0.748379647731781
translation,275,139,model,joint tlp+tgp tlp models,by replacing,original nmt loss,joint tlp+tgp tlp models by replacing original nmt loss,0.677632212638855
translation,275,139,model,original nmt loss,with,joint nmt + tlp loss,original nmt loss with joint nmt + tlp loss,0.61778324842453
translation,275,139,model,model,has,joint tlp+tgp tlp models,model has joint tlp+tgp tlp models,0.5788012146949768
translation,275,176,model,off-target translations,with,our proposed joint representation ( tlp ),off-target translations with our proposed joint representation ( tlp ),0.6496006846427917
translation,275,176,model,off-target translations,with,gradient ( tgp ) regularization,off-target translations with gradient ( tgp ) regularization,0.6188353300094604
translation,275,176,model,gradient ( tgp ) regularization,to guide,internal modeling,gradient ( tgp ) regularization to guide internal modeling,0.6847379207611084
translation,275,176,model,internal modeling,of,target languages,internal modeling of target languages,0.531676709651947
translation,275,176,model,model,reduce,off-target translations,model reduce off-target translations,0.6919310688972473
translation,275,31,results,our approaches,in,all language pairs,our approaches in all language pairs,0.4981291890144348
translation,275,31,results,our approaches,with,average + 5.59 and + 10.38 bleu gain,our approaches with average + 5.59 and + 10.38 bleu gain,0.6284534931182861
translation,275,31,results,our approaches,with,24.5 % ? 0.9 % and 65.8 % ? 4.7 % reduction,our approaches with 24.5 % ? 0.9 % and 65.8 % ? 4.7 % reduction,0.6296717524528503
translation,275,31,results,average + 5.59 and + 10.38 bleu gain,across,zero-shot pairs,average + 5.59 and + 10.38 bleu gain across zero-shot pairs,0.6842717528343201
translation,275,31,results,24.5 % ? 0.9 % and 65.8 % ? 4.7 % reduction,to,off-target rates,24.5 % ? 0.9 % and 65.8 % ? 4.7 % reduction to off-target rates,0.5551483035087585
translation,275,31,results,off-target rates,on,wmt - 10 and opus - 100,off-target rates on wmt - 10 and opus - 100,0.5963888168334961
translation,275,31,results,results,effectiveness of,our approaches,results effectiveness of our approaches,0.6471119523048401
translation,275,105,results,outperforms,in,most en-x and x - en directions,outperforms in most en-x and x - en directions,0.5594515800476074
translation,275,105,results,outperforms,in,all english -free directions,outperforms in all english -free directions,0.4955434203147888
translation,275,105,results,baselines,in,most en-x and x - en directions,baselines in most en-x and x - en directions,0.5273814797401428
translation,275,105,results,baselines,in,all english -free directions,baselines in all english -free directions,0.46508273482322693
translation,275,105,results,tlp,has,outperforms,tlp has outperforms,0.6465405821800232
translation,275,105,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,275,106,results,tlp,gains,+ 0.4 bleu,tlp gains + 0.4 bleu,0.7456763386726379
translation,275,106,results,tlp,gains,+0.28 bleu,tlp gains +0.28 bleu,0.7404586672782898
translation,275,106,results,tlp,gains,+ 2.12 bleu,tlp gains + 2.12 bleu,0.7319414615631104
translation,275,106,results,+ 0.4 bleu,on,en-x,+ 0.4 bleu on en-x,0.6410471200942993
translation,275,106,results,+ 0.4 bleu,on,x - en,+ 0.4 bleu on x - en,0.668423056602478
translation,275,106,results,+ 0.4 bleu,on,x - en,+ 0.4 bleu on x - en,0.668423056602478
translation,275,106,results,+0.28 bleu,on,x - en,+0.28 bleu on x - en,0.6494581699371338
translation,275,106,results,+ 2.12 bleu,on,english -free directions,+ 2.12 bleu on english -free directions,0.4915528893470764
translation,275,108,results,tlp,performs,similarly,tlp performs similarly,0.7487882971763611
translation,275,108,results,tlp,yielding,+ 0.77 bleu improvement,tlp yielding + 0.77 bleu improvement,0.6297285556793213
translation,275,108,results,similarly,in,english-centric directions,similarly in english-centric directions,0.5611472129821777
translation,275,108,results,+ 0.77 bleu improvement,on,english - free directions,+ 0.77 bleu improvement on english - free directions,0.4738493859767914
translation,275,108,results,65.8 % ? 60.5 % drop,in,off-target occurrences,65.8 % ? 60.5 % drop in off-target occurrences,0.5393710136413574
translation,275,108,results,opus - 100,has,tlp,opus - 100 has tlp,0.6507160067558289
translation,275,108,results,results,on,opus - 100,results on opus - 100,0.5803514122962952
translation,275,109,results,multilingual models,much better retain,information,multilingual models much better retain information,0.6064301133155823
translation,275,109,results,information,about,target language,information about target language,0.6191378235816956
translation,275,109,results,moderately improved,on,english - free pairs,moderately improved on english - free pairs,0.5308706760406494
translation,275,109,results,auxiliary tlp loss,has,multilingual models,auxiliary tlp loss has multilingual models,0.5513593554496765
translation,275,109,results,results,by adding,auxiliary tlp loss,results by adding auxiliary tlp loss,0.758260190486908
translation,275,116,results,tgp,gains,significant improvements,tgp gains significant improvements,0.8440117239952087
translation,275,116,results,tgp,reducing,off-target rates,tgp reducing off-target rates,0.7198144197463989
translation,275,116,results,significant improvements,on,all directions,significant improvements on all directions,0.5169074535369873
translation,275,116,results,significant improvements,averaging,+ 1.23 bleu,significant improvements averaging + 1.23 bleu,0.6215013861656189
translation,275,116,results,significant improvements,averaging,+ 1.38 bleu,significant improvements averaging + 1.38 bleu,0.6274661421775818
translation,275,116,results,significant improvements,averaging,+ 5.57 bleu,significant improvements averaging + 5.57 bleu,0.6197429895401001
translation,275,116,results,all directions,of,wmt - 10,all directions of wmt - 10,0.6413705348968506
translation,275,116,results,+ 1.23 bleu,on,en-x,+ 1.23 bleu on en-x,0.6336610317230225
translation,275,116,results,+ 1.38 bleu,on,x - en,+ 1.38 bleu on x - en,0.6407589316368103
translation,275,116,results,+ 5.57 bleu,on,englishfree directions,+ 5.57 bleu on englishfree directions,0.49218687415122986
translation,275,116,results,off-target rates,from,24.5 %,off-target rates from 24.5 %,0.5319842100143433
translation,275,116,results,24.5 %,down to,only 0.9 %,24.5 % down to only 0.9 %,0.7166063189506531
translation,275,117,results,+ 3.65 bleu,on,en-x,+ 3.65 bleu on en-x,0.6251164078712463
translation,275,117,results,+1.32 bleu,on,x - en,+1.32 bleu on x - en,0.6487897634506226
translation,275,117,results,+ 10.63 bleu,on,english -free,+ 10.63 bleu on english -free,0.46157172322273254
translation,275,117,results,whopping 65.8 % ? 4.8 % reduction,to,off-target occurrences,whopping 65.8 % ? 4.8 % reduction to off-target occurrences,0.5527585744857788
translation,275,117,results,opus - 100,has,+ 3.65 bleu,opus - 100 has + 3.65 bleu,0.6261940598487854
translation,275,118,results,overwhelming effectiveness,of,tgp,overwhelming effectiveness of tgp,0.6051849126815796
translation,275,118,results,tgp,on,all translation tasks,tgp on all translation tasks,0.523472249507904
translation,275,118,results,results,demonstrate,overwhelming effectiveness,results demonstrate overwhelming effectiveness,0.6080885529518127
translation,275,125,results,10 k steps,on,40k - step baseline,10 k steps on 40k - step baseline,0.5527133941650391
translation,275,125,results,noticeable improvement,on,english - free directions,noticeable improvement on english - free directions,0.503989040851593
translation,275,125,results,average,of,+ 1.62 bleu,average of + 1.62 bleu,0.5541641116142273
translation,275,125,results,average,of,+ 8.17 bleu,average of + 8.17 bleu,0.5600628852844238
translation,275,125,results,+ 1.62 bleu,on,wmt - 10,+ 1.62 bleu on wmt - 10,0.5616523623466492
translation,275,125,results,+ 8.17 bleu,on,opus - 100,+ 8.17 bleu on opus - 100,0.5964779853820801
translation,275,125,results,most reduction,on,off-target occurrences,most reduction on off-target occurrences,0.5998647809028625
translation,275,125,results,english - free directions,has,average,english - free directions has average,0.5713126063346863
translation,275,125,results,results,observe,noticeable improvement,results observe noticeable improvement,0.6206592917442322
translation,275,132,results,gain,of,+ 0.93 bleu,gain of + 0.93 bleu,0.5580984950065613
translation,275,132,results,gain,of,+ 1.19 bleu,gain of + 1.19 bleu,0.5445433259010315
translation,275,132,results,+ 0.93 bleu,on,en-x,+ 0.93 bleu on en-x,0.6265158653259277
translation,275,132,results,+ 1.19 bleu,on,x - en,+ 1.19 bleu on x - en,0.651898980140686
translation,275,132,results,+ 4.8,on,english - free,+ 4.8 on english - free,0.4844824969768524
translation,275,132,results,english - free,compared to,baseline,english - free compared to baseline,0.6690942645072937
translation,275,132,results,results,observe,gain,results observe gain,0.6004286408424377
translation,275,136,results,opus - 100,observe,consistent gain,opus - 100 observe consistent gain,0.6780737042427063
translation,275,136,results,opus - 100,observe,noticeable - 4.64 bleu drop,opus - 100 observe noticeable - 4.64 bleu drop,0.5882230401039124
translation,275,136,results,consistent gain,against,baseline,consistent gain against baseline,0.759465754032135
translation,275,136,results,noticeable - 4.64 bleu drop,on,english - free pairs,noticeable - 4.64 bleu drop on english - free pairs,0.5456922650337219
translation,275,136,results,english - free pairs,against,tgp,english - free pairs against tgp,0.682262122631073
translation,275,136,results,tgp,with,full oracle data,tgp with full oracle data,0.6314502954483032
translation,275,136,results,results,on,opus - 100,results on opus - 100,0.5803514122962952
translation,275,140,results,joint tlp +tgp approach,to,tgp -only,joint tlp +tgp approach to tgp -only,0.6008095145225525
translation,275,140,results,joint tlp +tgp approach,observe,no significant differences,joint tlp +tgp approach observe no significant differences,0.5965769290924072
translation,275,140,results,no significant differences,in,full oracle data scenario,no significant differences in full oracle data scenario,0.5452607274055481
translation,275,140,results,results,Comparing,joint tlp +tgp approach,results Comparing joint tlp +tgp approach,0.6477290987968445
translation,275,141,results,tgponly,by,+ 1.82 bleu,tgponly by + 1.82 bleu,0.609375536441803
translation,275,141,results,+ 1.82 bleu,on average,english - free pairs,+ 1.82 bleu on average english - free pairs,0.6584798097610474
translation,275,141,results,+ 1.82 bleu,in,english - free pairs,+ 1.82 bleu in english - free pairs,0.4979393482208252
translation,275,141,results,zero-shot setting,has,joint tlp + tgp approach,zero-shot setting has joint tlp + tgp approach,0.5822498202323914
translation,275,141,results,joint tlp + tgp approach,has,noticeably outperforms,joint tlp + tgp approach has noticeably outperforms,0.5943019986152649
translation,275,141,results,noticeably outperforms,has,tgponly,noticeably outperforms has tgponly,0.6179073452949524
translation,275,141,results,results,in,zero-shot setting,results in zero-shot setting,0.5531973242759705
translation,276,55,experimental-setup,experimental setup,tokenize,moses tokenizer,experimental setup tokenize moses tokenizer,0.7909220457077026
translation,276,56,experimental-setup,fastbpe,to learn,bpe ( byte pair encoding ),fastbpe to learn bpe ( byte pair encoding ),0.5876402258872986
translation,276,56,experimental-setup,bpe ( byte pair encoding ),with,32 k bpe codes,bpe ( byte pair encoding ) with 32 k bpe codes,0.6368511319160461
translation,276,56,experimental-setup,32 k bpe codes,over,combined tokenized data,32 k bpe codes over combined tokenized data,0.7223280072212219
translation,276,56,experimental-setup,combined tokenized data,of,both languages,combined tokenized data of both languages,0.5478520393371582
translation,276,15,experiments,shared task,on,unsupervised mt,shared task on unsupervised mt,0.5879539251327515
translation,276,15,experiments,very low resource supervised mt,at,wmt2021,very low resource supervised mt at wmt2021,0.5541373491287231
translation,276,8,hyperparameters,de ? dsb,use,final de ? hsb model,de ? dsb use final de ? hsb model,0.6526917815208435
translation,276,8,hyperparameters,final de ? hsb model,as,initialization,final de ? hsb model as initialization,0.5199135541915894
translation,276,8,hyperparameters,final de ? hsb model,train it further,iterative back - translation,final de ? hsb model train it further iterative back - translation,0.6654276847839355
translation,276,8,hyperparameters,initialization,of,de ? dsb model,initialization of de ? dsb model,0.6067009568214417
translation,276,8,hyperparameters,hyperparameters,For,de ? dsb,hyperparameters For de ? dsb,0.6205791234970093
translation,276,8,hyperparameters,hyperparameters,use,final de ? hsb model,hyperparameters use final de ? hsb model,0.6069350242614746
translation,276,59,hyperparameters,6 layers,in,encoder and decoder,6 layers in encoder and decoder,0.534135103225708
translation,276,59,hyperparameters,6 layers,with,1024 embedding dimension,6 layers with 1024 embedding dimension,0.6094373464584351
translation,276,59,hyperparameters,encoder and decoder,with,8 attention heads,encoder and decoder with 8 attention heads,0.6305298209190369
translation,276,59,hyperparameters,hyperparameters,use,6 layers,hyperparameters use 6 layers,0.5576580762863159
translation,276,61,hyperparameters,warm - up phase,of,4000 steps,warm - up phase of 4000 steps,0.5822820067405701
translation,276,61,hyperparameters,warm - up phase,with,initial learning rate,warm - up phase with initial learning rate,0.6372709274291992
translation,276,61,hyperparameters,4000 steps,with,initial learning rate,4000 steps with initial learning rate,0.6056479215621948
translation,276,61,hyperparameters,initial learning rate,starting from,1e ?7 to 1e ?4,initial learning rate starting from 1e ?7 to 1e ?4,0.7043888568878174
translation,276,61,hyperparameters,initial learning rate,in,warm - up phase,initial learning rate in warm - up phase,0.5286542177200317
translation,276,61,hyperparameters,initial learning rate,increased,linearly,initial learning rate increased linearly,0.6742278933525085
translation,276,61,hyperparameters,learning rate,increased,linearly,learning rate increased linearly,0.7153564691543579
translation,276,61,hyperparameters,learning rate,starts to,decrease,learning rate starts to decrease,0.6871997714042664
translation,276,61,hyperparameters,decrease,with,inverse square root learning rate schedule,decrease with inverse square root learning rate schedule,0.6703241467475891
translation,276,61,hyperparameters,warm - up phase,has,learning rate,warm - up phase has learning rate,0.5539109706878662
translation,276,62,hyperparameters,mini-batches,of size,2000 tokens,mini-batches of size 2000 tokens,0.7416920065879822
translation,276,62,hyperparameters,mini-batches,set,dropout,mini-batches set dropout,0.6493287682533264
translation,276,62,hyperparameters,dropout,to,0.1,dropout to 0.1,0.5452008843421936
translation,276,62,hyperparameters,hyperparameters,use,mini-batches,hyperparameters use mini-batches,0.6054610013961792
translation,276,62,hyperparameters,hyperparameters,set,dropout,hyperparameters set dropout,0.639107882976532
translation,276,63,hyperparameters,maximum sentence length,set to,100,maximum sentence length set to 100,0.7130685448646545
translation,276,63,hyperparameters,100,after applying,bpe,100 after applying bpe,0.6921521425247192
translation,276,63,hyperparameters,hyperparameters,has,maximum sentence length,hyperparameters has maximum sentence length,0.4956457316875458
translation,276,64,hyperparameters,decoding,set,beam size,decoding set beam size,0.6532772183418274
translation,276,64,hyperparameters,beam size,to,1,beam size to 1,0.6244003176689148
translation,276,66,hyperparameters,pretraining,performed for,100 epochs,pretraining performed for 100 epochs,0.5897111296653748
translation,276,66,hyperparameters,100 epochs,for,de ? hsb.,100 epochs for de ? hsb.,0.6309253573417664
translation,276,66,hyperparameters,further finetuned,using,iterative back -translation,further finetuned using iterative back -translation,0.6943545341491699
translation,276,66,hyperparameters,iterative back -translation,for,60 epochs,iterative back -translation for 60 epochs,0.5804240703582764
translation,276,66,hyperparameters,parallel data,for,60 epochs,parallel data for 60 epochs,0.6205323934555054
translation,276,66,hyperparameters,hyperparameters,has,pretraining,hyperparameters has pretraining,0.49708792567253113
translation,276,67,hyperparameters,further finetuned,for,iterative backtranslation,further finetuned for iterative backtranslation,0.6545534133911133
translation,276,67,hyperparameters,iterative backtranslation,using,final de ? hsb model,iterative backtranslation using final de ? hsb model,0.7008168697357178
translation,276,67,hyperparameters,final de ? hsb model,for,60 epochs,final de ? hsb model for 60 epochs,0.5712693929672241
translation,276,67,hyperparameters,hyperparameters,has,de ? dsb model,hyperparameters has de ? dsb model,0.5654706358909607
translation,276,72,results,bleu score,of,60.2 and 60.1,bleu score of 60.2 and 60.1,0.5817103981971741
translation,276,72,results,60.2 and 60.1,for,de ? hsb and hsb ? de,60.2 and 60.1 for de ? hsb and hsb ? de,0.6731815338134766
translation,276,72,results,results,achieve,bleu score,results achieve bleu score,0.594948410987854
translation,276,73,results,final model,of,de ? hsb,final model of de ? hsb,0.6254740953445435
translation,276,73,results,de ? hsb,as,initialization,de ? hsb as initialization,0.5634826421737671
translation,276,73,results,de ? hsb,achieve,bleu score,de ? hsb achieve bleu score,0.644268810749054
translation,276,73,results,initialization,of,model,initialization of model,0.599360466003418
translation,276,73,results,model,for,de ? dsb,model for de ? dsb,0.6894092559814453
translation,276,73,results,bleu score,of,6.4 and 5.9,bleu score of 6.4 and 5.9,0.5790955424308777
translation,276,73,results,6.4 and 5.9,for,de ? dsb and dsb ? de,6.4 and 5.9 for de ? dsb and dsb ? de,0.6709845066070557
translation,276,73,results,results,Using,final model,results Using final model,0.6475955247879028
translation,276,75,results,bleu scores,are,4.74 and 4.92,bleu scores are 4.74 and 4.92,0.5692393779754639
translation,276,75,results,4.74 and 4.92,for,de ? hsb and hsb ? de,4.74 and 4.92 for de ? hsb and hsb ? de,0.6605778932571411
translation,276,75,results,mass pretraining,has,bleu scores,mass pretraining has bleu scores,0.5828596353530884
translation,276,75,results,iterative back - translation,has,bleu scores,iterative back - translation has bleu scores,0.5470557808876038
translation,276,75,results,results,After,mass pretraining,results After mass pretraining,0.6650773286819458
translation,277,110,ablation-analysis,", m , p",produced,best result,", m , p produced best result",0.6394103765487671
translation,277,110,ablation-analysis,permutation trick,has,helped,permutation trick has helped,0.5893824100494385
translation,277,110,ablation-analysis,inv ),has,helped,inv ) has helped,0.6364286541938782
translation,277,110,ablation-analysis,did not solve,has,problem,did not solve has problem,0.6524942517280579
translation,277,110,ablation-analysis,ablation analysis,produced,best result,ablation analysis produced best result,0.6535293459892273
translation,277,110,ablation-analysis,ablation analysis,has,", m , p","ablation analysis has , m , p",0.5272102952003479
translation,277,113,ablation-analysis,performance,of,best additive model,performance of best additive model,0.6024747490882874
translation,277,113,ablation-analysis,best additive model,in,neutral mode,best additive model in neutral mode,0.49970492720603943
translation,277,113,ablation-analysis,best additive model,suffers,reduction,best additive model suffers reduction,0.6866455078125
translation,277,113,ablation-analysis,reduction,of,0.2 bleu,reduction of 0.2 bleu,0.5902528762817383
translation,277,113,ablation-analysis,0.2 bleu,in comparison to,baseline,0.2 bleu in comparison to baseline,0.5719510912895203
translation,277,113,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,277,168,ablation-analysis,smaller effect,than,model add,smaller effect than model add,0.5983612537384033
translation,277,168,ablation-analysis,model add,in improving,monotonicity,model add in improving monotonicity,0.6472914218902588
translation,277,168,ablation-analysis,model add,has,smaller effect,model add has smaller effect,0.5674014091491699
translation,277,168,ablation-analysis,ablation analysis,has,model add,ablation analysis has model add,0.5821951627731323
translation,277,198,experimental-setup,tpuv2 ( 16 cores ),with,batch size 256,tpuv2 ( 16 cores ) with batch size 256,0.6381025314331055
translation,277,198,experimental-setup,tpuv2 ( 16 cores ),used,"sentence packing ( shazeer et al. , 2018 )","tpuv2 ( 16 cores ) used sentence packing ( shazeer et al. , 2018 )",0.5367059111595154
translation,277,198,experimental-setup,"sentence packing ( shazeer et al. , 2018 )",to increase,efficiency,"sentence packing ( shazeer et al. , 2018 ) to increase efficiency",0.6590917110443115
translation,277,198,experimental-setup,efficiency,of,accelerator usage,efficiency of accelerator usage,0.5682989954948425
translation,277,198,experimental-setup,experimental setup,trained on,tpuv2 ( 16 cores ),experimental setup trained on tpuv2 ( 16 cores ),0.7046823501586914
translation,277,199,experimental-setup,learning rate,set to,0.0625,learning rate set to 0.0625,0.6971152424812317
translation,277,199,experimental-setup,0.0625,with,1 k steps,0.0625 with 1 k steps,0.6617748737335205
translation,277,199,experimental-setup,1 k steps,of,linear warm - up,1 k steps of linear warm - up,0.6130433678627014
translation,277,199,experimental-setup,1 k steps,of,square- root decay,1 k steps of square- root decay,0.6001350283622742
translation,277,199,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,277,200,experimental-setup,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,277,200,experimental-setup,experimental setup,used,default adam optimizer,experimental setup used default adam optimizer,0.6031295657157898
translation,277,200,experimental-setup,experimental setup,used,dropout rate,experimental setup used dropout rate,0.5948740243911743
translation,277,201,experimental-setup,en,trained for,minimum of 100k steps,en trained for minimum of 100k steps,0.7257885336875916
translation,277,201,experimental-setup,?,trained for,minimum of 100k steps,? trained for minimum of 100k steps,0.7664351463317871
translation,277,201,experimental-setup,de,trained for,minimum of 100k steps,de trained for minimum of 100k steps,0.7064813375473022
translation,277,201,experimental-setup,dev's set bleu score,with,patience,dev's set bleu score with patience,0.6489060521125793
translation,277,201,experimental-setup,patience,of,5,patience of 5,0.6949102878570557
translation,277,201,experimental-setup,en,has,?,en has ?,0.7122668027877808
translation,277,201,experimental-setup,en,has,de,en has de,0.5665467977523804
translation,277,201,experimental-setup,?,has,de,? has de,0.7920663952827454
translation,277,201,experimental-setup,experimental setup,For,en,experimental setup For en,0.6443126201629639
translation,277,201,experimental-setup,experimental setup,For,?,experimental setup For ?,0.6177961826324463
translation,277,201,experimental-setup,experimental setup,For,de,experimental setup For de,0.6747094988822937
translation,277,7,model,vector-valued interventions,allow for,fine- grained control,vector-valued interventions allow for fine- grained control,0.7187538743019104
translation,277,7,model,fine- grained control,over,multiple attributes simultaneously,fine- grained control over multiple attributes simultaneously,0.7161489725112915
translation,277,7,model,weighted linear combination,of,corresponding vectors,weighted linear combination of corresponding vectors,0.6023180484771729
translation,277,7,model,model,introducing,vector-valued interventions,model introducing vector-valued interventions,0.6887006163597107
translation,277,25,model,additive vector-valued interventions,"fine- grained , combinable and fine-tunable control of",translations,"additive vector-valued interventions fine- grained , combinable and fine-tunable control of translations",0.7353827953338623
translation,277,25,model,model,introduce,additive vector-valued interventions,model introduce additive vector-valued interventions,0.6700502634048462
translation,277,26,model,two implementations,of,vector-valued control,two implementations of vector-valued control,0.6039795279502869
translation,277,26,model,one attribute embedding vector,with,control direction and strength,one attribute embedding vector with control direction and strength,0.621964693069458
translation,277,26,model,control direction and strength,regulated by,multiplicative scalar factor,control direction and strength regulated by multiplicative scalar factor,0.6212162375450134
translation,277,26,model,multiplicative scalar factor,appropriate for,continuous attributes,multiplicative scalar factor appropriate for continuous attributes,0.7442176342010498
translation,277,26,model,separate embedding vectors,for,each discrete attribute value,separate embedding vectors for each discrete attribute value,0.6135321259498596
translation,277,26,model,model,propose,two implementations,model propose two implementations,0.6971590518951416
translation,277,27,model,attributes ' embeddings,additively combined with,encoder 's last layer representation,attributes ' embeddings additively combined with encoder 's last layer representation,0.6587045788764954
translation,277,27,model,attributes ' embeddings,used by,subset of the decoder layers,attributes ' embeddings used by subset of the decoder layers,0.7298803329467773
translation,277,27,model,subset of the decoder layers,through,source-attention mechanism,subset of the decoder layers through source-attention mechanism,0.6533575057983398
translation,277,27,model,model,has,attributes ' embeddings,model has attributes ' embeddings,0.584057092666626
translation,277,182,model,novel approach,for controlling,nmt system,novel approach for controlling nmt system,0.749916672706604
translation,277,182,model,nmt system,with respect to,multiple attributes,nmt system with respect to multiple attributes,0.7343238592147827
translation,277,182,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,277,33,results,improved,compared to,strong baselines,improved compared to strong baselines,0.6933850049972534
translation,277,33,results,+ 0.6 bleu points,for,german,+ 0.6 bleu points for german,0.61076420545578
translation,277,33,results,+ 2.5,for,japanese,+ 2.5 for japanese,0.6891002655029297
translation,277,33,results,explicit politeness information,has,evaluation scores,explicit politeness information has evaluation scores,0.5369164347648621
translation,277,33,results,evaluation scores,has,improved,evaluation scores has improved,0.5806885361671448
translation,277,33,results,strong baselines,has,+ 0.6 bleu points,strong baselines has + 0.6 bleu points,0.5519558191299438
translation,277,33,results,results,including,explicit politeness information,results including explicit politeness information,0.5616827011108398
translation,277,109,results,tagging,ordering,tags,tagging ordering tags,0.7949724197387695
translation,277,109,results,tagging,get,results,tagging get results,0.583857536315918
translation,277,109,results,tags,get,results,tags get results,0.6415941119194031
translation,277,109,results,results,between,26.58 and 27.32 points,results between 26.58 and 27.32 points,0.5839361548423767
translation,277,109,results,tags,has,differently,tags has differently,0.6543020009994507
translation,277,109,results,results,For,tagging,results For tagging,0.5845288038253784
translation,277,111,results,masking,to support,neutral mode,masking to support neutral mode,0.6817266345024109
translation,277,111,results,masking,works,well,masking works well,0.6335815787315369
translation,277,111,results,neutral mode,works,well,neutral mode works well,0.6028556227684021
translation,277,111,results,well,both,continuous and tagging models,well both continuous and tagging models,0.7448200583457947
translation,277,111,results,well,with,continuous and tagging models,well with continuous and tagging models,0.6862139105796814
translation,277,111,results,results,worth noting,masking,results worth noting masking,0.501814603805542
translation,277,111,results,results,using,masking,results using masking,0.5357542634010315
translation,277,114,results,2.9 bleu,over,baseline,2.9 bleu over baseline,0.5812510251998901
translation,277,114,results,2.9 bleu,is,better improvement,2.9 bleu is better improvement,0.5495254993438721
translation,277,114,results,results,moving to,oracle mode,results moving to oracle mode,0.6453499794006348
translation,277,146,results,oracle mode,leads to,substantial improvements,oracle mode leads to substantial improvements,0.6506566405296326
translation,277,146,results,substantial improvements,on,informal split,substantial improvements on informal split,0.5803180932998657
translation,277,146,results,informal split,of,test data,informal split of test data,0.5510004758834839
translation,277,146,results,all the additive models,has,oracle mode,all the additive models has oracle mode,0.5740042328834534
translation,277,146,results,results,in,all the additive models,results in all the additive models,0.46704286336898804
translation,277,153,results,controlling politeness,improves,bleu scores,controlling politeness improves bleu scores,0.6705510020256042
translation,277,153,results,bleu scores,on,every split,bleu scores on every split,0.5094916820526123
translation,277,153,results,every split,when,rule- based feature,every split when rule- based feature,0.6490306258201599
translation,277,167,results,all the models,produce,more monotone translations,all the models produce more monotone translations,0.6478781700134277
translation,277,167,results,more monotone translations,compared to,baseline,more monotone translations compared to baseline,0.6538918614387512
translation,277,167,results,results,has,all the models,results has all the models,0.5321599841117859
translation,277,250,results,wmt,see,good performance,wmt see good performance,0.6143810153007507
translation,277,250,results,good performance,with,similar scores,good performance with similar scores,0.6030375957489014
translation,277,250,results,similar scores,between,neutral and oracle mode,similar scores between neutral and oracle mode,0.6793290972709656
translation,277,250,results,results,On,wmt,results On wmt,0.5395960211753845
translation,278,48,baselines,baseline deepquest,use,bidirectional gated recurrent unit type recurrent neural networks,baseline deepquest use bidirectional gated recurrent unit type recurrent neural networks,0.5756047964096069
translation,278,48,baselines,bidirectional gated recurrent unit type recurrent neural networks,to model,qet,bidirectional gated recurrent unit type recurrent neural networks to model qet,0.7008659243583679
translation,278,48,baselines,baselines,has,baseline deepquest,baselines has baseline deepquest,0.6113662123680115
translation,278,15,experimental-setup,language models ( lms ),built using,kenlm,language models ( lms ) built using kenlm,0.6819413900375366
translation,278,15,experimental-setup,experimental setup,has,language models ( lms ),experimental setup has language models ( lms ),0.5518386960029602
translation,278,7,experiments,sentence - level direct assessment ( da ),has,in 11 language pairs,sentence - level direct assessment ( da ) has in 11 language pairs,0.5354137420654297
translation,278,47,results,super learner,improve,results,super learner improve results,0.6536607146263123
translation,278,47,results,results,has,super learner,results has super learner,0.6001785397529602
translation,279,16,model,question answering ( qa ),to,formulate and answer,question answering ( qa ) to formulate and answer,0.5834795832633972
translation,279,16,model,formulate and answer,based on,mt output,formulate and answer based on mt output,0.6328186988830566
translation,279,16,model,questions,based on,mt output,questions based on mt output,0.5995092391967773
translation,279,16,model,formulate and answer,has,questions,formulate and answer has questions,0.5902203321456909
translation,279,16,model,model,leverages,question answering ( qa ),model leverages question answering ( qa ),0.7157310843467712
translation,279,49,results,baseline,has,outperforms,baseline has outperforms,0.619843065738678
translation,279,49,results,outperforms,has,"traditional mt evaluation metrics ( sentbleu , bleu )","outperforms has traditional mt evaluation metrics ( sentbleu , bleu )",0.5818004012107849
translation,279,67,results,mteqa chrf keyphrase variant,yields,further improvements,mteqa chrf keyphrase variant yields further improvements,0.7380300164222717
translation,279,67,results,further improvements,at,system - and segmentlevel,further improvements at system - and segmentlevel,0.5110661387443542
translation,279,67,results,results,Using,mteqa chrf keyphrase variant,results Using mteqa chrf keyphrase variant,0.6598937511444092
translation,279,68,results,results,For,wmt21 metrics,results For wmt21 metrics,0.5690867900848389
translation,280,70,experimental-setup,number of merge operations,in,byte pair encoding ( bpe ),number of merge operations in byte pair encoding ( bpe ),0.5121118426322937
translation,280,70,experimental-setup,byte pair encoding ( bpe ),set to,32 k,byte pair encoding ( bpe ) set to 32 k,0.6772356033325195
translation,280,70,experimental-setup,32 k,for,source and target languages,32 k for source and target languages,0.60077303647995
translation,280,70,experimental-setup,experimental setup,has,number of merge operations,experimental setup has number of merge operations,0.5361546277999878
translation,280,78,experimental-setup,model,trained with,bmi - based adaptive objective,model trained with bmi - based adaptive objective,0.7191609740257263
translation,280,78,experimental-setup,bmi - based adaptive objective,for,100k steps,bmi - based adaptive objective for 100k steps,0.6215114593505859
translation,280,78,experimental-setup,experimental setup,trained with,bmi - based adaptive objective,experimental setup trained with bmi - based adaptive objective,0.6968590617179871
translation,280,78,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,280,6,model,novel bilingual mutual information ( bmi ) based adaptive objective,measures,learning difficulty,novel bilingual mutual information ( bmi ) based adaptive objective measures learning difficulty,0.4726239740848541
translation,280,6,model,novel bilingual mutual information ( bmi ) based adaptive objective,assigns,adaptive weight,novel bilingual mutual information ( bmi ) based adaptive objective assigns adaptive weight,0.6047705411911011
translation,280,6,model,learning difficulty,for,each target token,learning difficulty for each target token,0.5441893339157104
translation,280,6,model,each target token,perspective of,bilingualism,each target token perspective of bilingualism,0.6514344215393066
translation,280,6,model,adaptive weight,to improve,token - level adaptive training,adaptive weight to improve token - level adaptive training,0.6746439337730408
translation,280,6,model,model,propose,novel bilingual mutual information ( bmi ) based adaptive objective,model propose novel bilingual mutual information ( bmi ) based adaptive objective,0.6512168049812317
translation,280,7,model,larger training weights,to,tokens,larger training weights to tokens,0.5798295736312866
translation,280,7,model,tokens,with,higher bmi,tokens with higher bmi,0.6233997941017151
translation,280,7,model,difficult tokens,updated with,fine granularity,difficult tokens updated with fine granularity,0.6280896067619324
translation,280,88,results,outperforms,by,0.67 bleu points,outperforms by 0.67 bleu points,0.5690117478370667
translation,280,88,results,our transformer,has,outperforms,our transformer has outperforms,0.6413710713386536
translation,280,92,results,significant and consistent improvement,on,two large-scale dataset,significant and consistent improvement on two large-scale dataset,0.4969755709171295
translation,280,92,results,significant and consistent improvement,demonstrates,effectiveness of our method,significant and consistent improvement demonstrates effectiveness of our method,0.6502890586853027
translation,280,92,results,results,has,significant and consistent improvement,results has significant and consistent improvement,0.5533695816993713
translation,280,95,results,outperform,on,high subset,outperform on high subset,0.6141891479492188
translation,280,95,results,no obvious improvement,on,low subset,no obvious improvement on low subset,0.5996080040931702
translation,280,95,results,transformer,has,frequencybased methods,transformer has frequencybased methods,0.5627002716064453
translation,280,95,results,frequencybased methods,has,outperform,frequencybased methods has outperform,0.602222740650177
translation,280,102,results,lexical diversity,of,translation,lexical diversity of translation,0.6073046326637268
translation,280,102,results,our method,superior to,existing methods ( chi- square and exponential ),our method superior to existing methods ( chi- square and exponential ),0.7096120715141296
translation,280,102,results,existing methods ( chi- square and exponential ),based on,word frequency,existing methods ( chi- square and exponential ) based on word frequency,0.6215754747390747
translation,280,102,results,lexical diversity,has,our method,lexical diversity has our method,0.5615776181221008
translation,280,102,results,results,on improving,lexical diversity,results on improving lexical diversity,0.5927378535270691
translation,281,24,experiments,mt model,on,"wikimatrix ( schwenk et al. , 2021 )","mt model on wikimatrix ( schwenk et al. , 2021 )",0.5278271436691284
translation,281,24,experiments,further tuning,to,ape task,further tuning to ape task,0.6181928515434265
translation,281,24,experiments,ape task,with,post-editing samples,ape task with post-editing samples,0.6552060842514038
translation,281,66,results,both bleu and ter scores,on,development set,both bleu and ter scores on development set,0.536443293094635
translation,281,66,results,results,Using data from,previous years ' tasks,results Using data from previous years ' tasks,0.6867401599884033
translation,281,67,results,fine-tuning,on,wikimatrix data,fine-tuning on wikimatrix data,0.5464008450508118
translation,281,67,results,wikimatrix data,not led to,improvements,wikimatrix data not led to improvements,0.7093719840049744
translation,281,67,results,performance,when used in,ensemble,performance when used in ensemble,0.684299886226654
translation,281,67,results,ensemble,with,other model,ensemble with other model,0.6120874881744385
translation,281,67,results,helps improve,has,performance,helps improve has performance,0.582428514957428
translation,281,68,results,model a,beats,baseline,model a beats baseline,0.7216461300849915
translation,281,68,results,baseline,on,ter metric,baseline on ter metric,0.5185767412185669
translation,281,68,results,ter metric,by,0.31 points,ter metric by 0.31 points,0.5874985456466675
translation,281,68,results,0.31 points,on,test set,0.31 points on test set,0.5358439683914185
translation,281,68,results,manage to outperform,has,previous year 's best entry,manage to outperform has previous year 's best entry,0.6060593724250793
translation,281,68,results,results,has,model a,results has model a,0.5407454371452332
translation,282,105,experimental-setup,6 layers of encoder and decoder,with,model dimension,6 layers of encoder and decoder with model dimension,0.624589741230011
translation,282,105,experimental-setup,model dimension,of,512 and 8 attention heads,model dimension of 512 and 8 attention heads,0.5966449975967407
translation,282,105,experimental-setup,transformer base architecture,has,6 layers of encoder and decoder,transformer base architecture has 6 layers of encoder and decoder,0.5877724289894104
translation,282,105,experimental-setup,experimental setup,use,transformer base architecture,experimental setup use transformer base architecture,0.5776473879814148
translation,282,106,experimental-setup,bpe subword vocabularies,processed through,"sen-tencepiece ( kudo and richardson , 2018 ) bpe implementation","bpe subword vocabularies processed through sen-tencepiece ( kudo and richardson , 2018 ) bpe implementation",0.6926701664924622
translation,282,131,model,mbart,is,12 - layer transformer,mbart is 12 - layer transformer,0.6124674677848816
translation,282,131,model,12 - layer transformer,pretrained with,denoising objective,12 - layer transformer pretrained with denoising objective,0.6503841280937195
translation,282,131,model,denoising objective,in,self-supervised manner,denoising objective in self-supervised manner,0.5169535875320435
translation,282,131,model,self-supervised manner,using,span masking and sentence permutation noising functions,self-supervised manner using span masking and sentence permutation noising functions,0.7185128331184387
translation,282,131,model,model,has,mbart,model has mbart,0.6537178754806519
translation,282,6,results,fine-tuning,is,nearly as effective,fine-tuning is nearly as effective,0.5746404528617859
translation,282,6,results,only the cross-attention parameters,is,nearly as effective,only the cross-attention parameters is nearly as effective,0.5622200965881348
translation,282,6,results,nearly as effective,as,fine-tuning,nearly as effective as fine-tuning,0.5944486856460571
translation,282,6,results,fine-tuning,has,only the cross-attention parameters,fine-tuning has only the cross-attention parameters,0.5429465770721436
translation,282,6,results,fine-tuning,has,all parameters,fine-tuning has all parameters,0.5634962916374207
translation,282,6,results,fine-tuning,has,"i.e. , the entire translation model","fine-tuning has i.e. , the entire translation model",0.5411428213119507
translation,282,6,results,all parameters,has,"i.e. , the entire translation model","all parameters has i.e. , the entire translation model",0.5645973682403564
translation,282,31,results,our experiments and analyses,show,finetuning,our experiments and analyses show finetuning,0.6341463923454285
translation,282,31,results,cross-attention layers,while keeping,encoder and decoder fixed,cross-attention layers while keeping encoder and decoder fixed,0.6367834806442261
translation,282,31,results,encoder and decoder fixed,results,mt quality,encoder and decoder fixed results mt quality,0.4649560749530792
translation,282,31,results,encoder and decoder fixed,in,mt quality,encoder and decoder fixed in mt quality,0.5164259076118469
translation,282,31,results,finetuning,has,cross-attention layers,finetuning has cross-attention layers,0.5246565937995911
translation,282,31,results,finetuning,has,all parameters,finetuning has all parameters,0.5712763071060181
translation,282,115,results,"{ src , tgt } + xattn",substantially improves upon,"{ src , tgt }","{ src , tgt } + xattn substantially improves upon { src , tgt }",0.671122670173645
translation,282,115,results,"{ src , tgt } + xattn",substantially improves upon,all but one case ( ha - en ),"{ src , tgt } + xattn substantially improves upon all but one case ( ha - en )",0.7097669243812561
translation,282,115,results,"{ src , tgt } + xattn",substantially improves upon,competitive,"{ src , tgt } + xattn substantially improves upon competitive",0.6862744688987732
translation,282,115,results,"{ src , tgt } + xattn",in,all but one case ( ha - en ),"{ src , tgt } + xattn in all but one case ( ha - en )",0.5391760468482971
translation,282,115,results,"{ src , tgt }",in,all but one case ( ha - en ),"{ src , tgt } in all but one case ( ha - en )",0.5280145406723022
translation,282,115,results,results,shows,"{ src , tgt } + xattn","results shows { src , tgt } + xattn",0.673470675945282
translation,282,139,results,higher bleu,than,scratch baseline,higher bleu than scratch baseline,0.567560613155365
translation,282,139,results,scratch baseline,only in,ja- en case,scratch baseline only in ja- en case,0.6417189836502075
translation,282,139,results,embed fine-tuning,has,higher bleu,embed fine-tuning has higher bleu,0.590404212474823
translation,282,139,results,results,has,embed fine-tuning,results has embed fine-tuning,0.5867012739181519
translation,282,162,results,fine-tuning,only,cross-attention,fine-tuning only cross-attention,0.6290364265441895
translation,282,162,results,results,in,cross-lingual embeddings,results in cross-lingual embeddings,0.46993711590766907
translation,282,162,results,cross-lingual embeddings,with respect to,parent embeddings,cross-lingual embeddings with respect to parent embeddings,0.5808387398719788
translation,282,162,results,fine-tuning,has,cross-attention,fine-tuning has cross-attention,0.5231809616088867
translation,282,162,results,cross-attention,has,results,cross-attention has results,0.5052011013031006
translation,282,162,results,results,saw,fine-tuning,results saw fine-tuning,0.6023154854774475
translation,282,174,results,original fr- en model,has,"source-transferred models ( de- en , ro -en )","original fr- en model has source-transferred models ( de- en , ro -en )",0.5615047216415405
translation,282,174,results,original fr- en model,has,outperform,original fr- en model has outperform,0.599222719669342
translation,282,174,results,"source-transferred models ( de- en , ro -en )",has,outperform,"source-transferred models ( de- en , ro -en ) has outperform",0.5709269046783447
translation,282,174,results,outperform,has,targettransferred model ( fr - es ),outperform has targettransferred model ( fr - es ),0.5976199507713318
translation,282,174,results,results,Compared to,original fr- en model,results Compared to original fr- en model,0.6829943656921387
translation,282,175,results,tgt + xattn,much,more robust,tgt + xattn much more robust,0.7191244959831238
translation,282,175,results,more robust,against,forgetting,more robust against forgetting,0.6998617053031921
translation,282,175,results,forgetting,compared to,tgt + body,forgetting compared to tgt + body,0.6390507221221924
translation,282,175,results,results,has,tgt + xattn,results has tgt + xattn,0.5455797910690308
translation,283,92,ablation-analysis,special token,is,most helpful part,special token is most helpful part,0.5306086540222168
translation,283,92,ablation-analysis,most helpful part,for,imperceptibility subtask,most helpful part for imperceptibility subtask,0.5929420590400696
translation,283,92,ablation-analysis,ablation analysis,interesting that,special token,ablation analysis interesting that special token,0.7515413761138916
translation,283,106,ablation-analysis,special tokens,bring,plms,special tokens bring plms,0.6410489082336426
translation,283,106,ablation-analysis,plms,with,best improvements,plms with best improvements,0.6287030577659607
translation,283,106,ablation-analysis,best improvements,in,subtask - 1 and subtask - 2,best improvements in subtask - 1 and subtask - 2,0.5240512490272522
translation,283,106,ablation-analysis,ablation analysis,Note,special tokens,ablation analysis Note special tokens,0.6468311548233032
translation,283,116,ablation-analysis,each special token,helps,plms,each special token helps plms,0.6592771410942078
translation,283,116,ablation-analysis,plms,choose,right abstract concepts,plms choose right abstract concepts,0.6290580034255981
translation,283,116,ablation-analysis,right abstract concepts,submerged in,long sequential tokens,right abstract concepts submerged in long sequential tokens,0.6036548614501953
translation,283,116,ablation-analysis,ablation analysis,interesting that,each special token,ablation analysis interesting that each special token,0.7280288934707642
translation,283,132,baselines,approach,is,"final , stable and best model","approach is final , stable and best model",0.5699887871742249
translation,283,132,baselines,roberta large,with,special tokens,roberta large with special tokens,0.6547767519950867
translation,283,132,baselines,baselines,has,approach,baselines has approach,0.6269438862800598
translation,283,85,experimental-setup,system,implemented with,pytorch,system implemented with pytorch,0.7445048093795776
translation,283,85,experimental-setup,system,use,pytorch version,system use pytorch version,0.7057363986968994
translation,283,85,experimental-setup,pytorch version,of,pre-trained language models,pytorch version of pre-trained language models,0.5209768414497375
translation,283,85,experimental-setup,experimental setup,use,pytorch version,experimental setup use pytorch version,0.6437815427780151
translation,283,85,experimental-setup,experimental setup,has,system,experimental setup has system,0.535061240196228
translation,283,86,experimental-setup,"roberta ( liu et al. , 2019 ) large model",as,our plm encoder,"roberta ( liu et al. , 2019 ) large model as our plm encoder",0.48719707131385803
translation,283,86,experimental-setup,experimental setup,employ,"roberta ( liu et al. , 2019 ) large model","experimental setup employ roberta ( liu et al. , 2019 ) large model",0.538520872592926
translation,283,87,experimental-setup,adam optimizer,to,fine - tune,adam optimizer to fine - tune,0.5330520868301392
translation,283,87,experimental-setup,fine - tune,has,model,fine - tune has model,0.5780399441719055
translation,283,87,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,283,9,experiments,our system,achieves,eighth rank ( 87.51 % ),our system achieves eighth rank ( 87.51 % ),0.6545419096946716
translation,283,9,experiments,our system,achieves,tenth rank ( 89.64 % ),our system achieves tenth rank ( 89.64 % ),0.6633619070053101
translation,283,9,experiments,our system,on,official blind test set,our system on official blind test set,0.5396909713745117
translation,283,9,experiments,tenth rank ( 89.64 % ),on,official blind test set,tenth rank ( 89.64 % ) on official blind test set,0.5082604885101318
translation,283,9,experiments,official blind test set,of,subtask 1 and subtask 2,official blind test set of subtask 1 and subtask 2,0.5844025611877441
translation,283,95,experiments,model,with,special tokens,model with special tokens,0.6660438179969788
translation,283,95,experiments,label smoothing,performs,best,label smoothing performs best,0.58290034532547
translation,283,95,experiments,nonspecificility subtask,has,model,nonspecificility subtask has model,0.5470301508903503
translation,283,98,experiments,subtask - 3,of,re-cam,subtask - 3 of re-cam,0.6624146103858948
translation,283,98,experiments,re-cam,has,interaction,re-cam has interaction,0.5967264175415039
translation,283,7,model,many finetuning tricks,to improve,performance,many finetuning tricks to improve performance,0.688970685005188
translation,283,7,model,model,employ,many finetuning tricks,model employ many finetuning tricks,0.6091873645782471
translation,283,91,results,our methods,achieve,significant improvements,our methods achieve significant improvements,0.5663674473762512
translation,283,91,results,backbone model roberta large model,has,our methods,backbone model roberta large model has our methods,0.5172717571258545
translation,283,91,results,results,Compared to,backbone model roberta large model,results Compared to backbone model roberta large model,0.6633559465408325
translation,283,94,results,results,on,subtask - 2 of recam,results on subtask - 2 of recam,0.53376305103302
translation,283,96,results,all our methods,achieve,better performance,all our methods achieve better performance,0.6023985147476196
translation,283,96,results,backbone model roberta large,has,all our methods,backbone model roberta large has all our methods,0.521420955657959
translation,283,96,results,results,Compared to,backbone model roberta large,results Compared to backbone model roberta large,0.6722078919410706
translation,283,101,results,our model,is,relatively robust,our model is relatively robust,0.5570018291473389
translation,283,101,results,relatively robust,for,different abstract concepts,relatively robust for different abstract concepts,0.6287561655044556
translation,283,101,results,results,find that,our model,results find that our model,0.6804299354553223
translation,283,105,results,our proposed methods,help,backbone model,our proposed methods help backbone model,0.7000359296798706
translation,283,105,results,backbone model,better represent and understand,abstract concepts,backbone model better represent and understand abstract concepts,0.5631003975868225
translation,283,105,results,results,shows,our proposed methods,results shows our proposed methods,0.6718311309814453
translation,283,133,results,89.64 acc.,on,official blind test set,89.64 acc. on official blind test set,0.5016686916351318
translation,283,133,results,results,obtain,89.64 acc.,results obtain 89.64 acc.,0.55815589427948
translation,284,44,baselines,larger setting,with,12 - layer encoder,larger setting with 12 - layer encoder,0.6640341877937317
translation,284,44,baselines,larger setting,with,12 - layer decoder,larger setting with 12 - layer decoder,0.6672207117080688
translation,284,44,baselines,12 - layer decoder,to increase,model capacity,12 - layer decoder to increase model capacity,0.6581719517707825
translation,284,122,baselines,mrasp2 w/o aa,adopt,contrastive learning,mrasp2 w/o aa adopt contrastive learning,0.7044767141342163
translation,284,122,baselines,contrastive learning,on the basis of,m-transformer,contrastive learning on the basis of m-transformer,0.6591529846191406
translation,284,122,baselines,baselines,has,mrasp2 w/o aa,baselines has mrasp2 w/o aa,0.5602951049804688
translation,284,123,baselines,mrasp2 w/o mc24,excludes,monolingual data,mrasp2 w/o mc24 excludes monolingual data,0.7233810424804688
translation,284,123,baselines,monolingual data,from,mrasp2,monolingual data from mrasp2,0.5602566003799438
translation,284,123,baselines,baselines,has,mrasp2 w/o mc24,baselines has mrasp2 w/o mc24,0.5920860171318054
translation,284,45,experimental-setup,model dimension,is,1024,model dimension is 1024,0.6180194020271301
translation,284,45,experimental-setup,1024,on,16 heads,1024 on 16 heads,0.5823606848716736
translation,284,45,experimental-setup,experimental setup,has,model dimension,experimental setup has model dimension,0.5250306129455566
translation,284,46,experimental-setup,training,apply,layer normalization,training apply layer normalization,0.6632144451141357
translation,284,46,experimental-setup,training,apply,pre-norm residual connection,training apply pre-norm residual connection,0.6314491629600525
translation,284,46,experimental-setup,layer normalization,for,word embedding,layer normalization for word embedding,0.5588873028755188
translation,284,46,experimental-setup,pre-norm residual connection,for,both encoder and decoder,pre-norm residual connection for both encoder and decoder,0.6116756796836853
translation,284,46,experimental-setup,2019a ),for,both encoder and decoder,2019a ) for both encoder and decoder,0.6336947083473206
translation,284,46,experimental-setup,experimental setup,To ease,training,experimental setup To ease training,0.6276978850364685
translation,284,80,experimental-setup,aa,on,pc32,aa on pc32,0.6020009517669678
translation,284,80,experimental-setup,aa,by randomly replacing,words,aa by randomly replacing words,0.7365418076515198
translation,284,80,experimental-setup,words,in,source side sentences,words in source side sentences,0.49470043182373047
translation,284,80,experimental-setup,words,in,dictionaries,words in dictionaries,0.5022841691970825
translation,284,80,experimental-setup,source side sentences,with,synonyms,source side sentences with synonyms,0.5388856530189514
translation,284,80,experimental-setup,synonyms,from,arbitrary bilingual dictionary,synonyms from arbitrary bilingual dictionary,0.5228506922721863
translation,284,80,experimental-setup,words,in,dictionaries,words in dictionaries,0.5022841691970825
translation,284,80,experimental-setup,words,one of,synonyms,words one of synonyms,0.5839522480964661
translation,284,80,experimental-setup,synonyms,with,probability,synonyms with probability,0.6286377310752869
translation,284,80,experimental-setup,probability,of,90 %,probability of 90 %,0.6238237023353577
translation,284,80,experimental-setup,experimental setup,apply,aa,experimental setup apply aa,0.5845715403556824
translation,284,80,experimental-setup,experimental setup,apply,pc32,experimental setup apply pc32,0.643519401550293
translation,284,102,experimental-setup,transformer model,with,12 encoder layers,transformer model with 12 encoder layers,0.6378397345542908
translation,284,102,experimental-setup,transformer model,with,12 decoder layers,transformer model with 12 decoder layers,0.6155421733856201
translation,284,102,experimental-setup,experimental setup,use,transformer model,experimental setup use transformer model,0.5547628998756409
translation,284,103,experimental-setup,embedding size and ffn dimension,set to,1024,embedding size and ffn dimension set to 1024,0.6983323693275452
translation,284,103,experimental-setup,experimental setup,has,embedding size and ffn dimension,experimental setup has embedding size and ffn dimension,0.5097538828849792
translation,284,104,experimental-setup,learning rate,of,3e - 4,learning rate of 3e - 4,0.6384614109992981
translation,284,104,experimental-setup,learning rate,with,polynomial decay scheduling,learning rate with polynomial decay scheduling,0.6189050078392029
translation,284,104,experimental-setup,warm - up step,of,10000,warm - up step of 10000,0.6180870532989502
translation,284,105,experimental-setup,optimization,use,adam optimizer,optimization use adam optimizer,0.603814959526062
translation,284,105,experimental-setup,adam optimizer,with,= 1e - 6 and ? 2 = 0.98,adam optimizer with = 1e - 6 and ? 2 = 0.98,0.6146279573440552
translation,284,105,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,284,106,experimental-setup,training,set,threshold,training set threshold,0.7195834517478943
translation,284,106,experimental-setup,threshold,of,gradient norm,threshold of gradient norm,0.5797528624534607
translation,284,106,experimental-setup,gradient norm,to be,5.0,gradient norm to be 5.0,0.5552003383636475
translation,284,106,experimental-setup,gradient norm,clip,all gradients,gradient norm clip all gradients,0.7078388333320618
translation,284,106,experimental-setup,all gradients,with,larger norm,all gradients with larger norm,0.6709810495376587
translation,284,106,experimental-setup,experimental setup,To stabilize,training,experimental setup To stabilize training,0.6858928203582764
translation,284,7,model,training method,to obtain,single unified multilingual translation model,training method to obtain single unified multilingual translation model,0.5729398727416992
translation,284,7,model,mrasp2,has,training method,mrasp2 has training method,0.5492221713066101
translation,284,7,model,model,propose,mrasp2,model propose mrasp2,0.6931902170181274
translation,284,8,model,mrasp2,empowered by,two techniques,mrasp2 empowered by two techniques,0.643805205821991
translation,284,8,model,contrastive learning scheme,to close,gap,contrastive learning scheme to close gap,0.6722161769866943
translation,284,8,model,gap,among,representations,gap among representations,0.6813775897026062
translation,284,8,model,representations,of,different languages,representations of different languages,0.582328200340271
translation,284,8,model,data augmentation,on both,multiple parallel and monolingual data,data augmentation on both multiple parallel and monolingual data,0.5955790877342224
translation,284,8,model,data augmentation,to further align,token representations,data augmentation to further align token representations,0.6765785217285156
translation,284,8,model,multiple parallel and monolingual data,to further align,token representations,multiple parallel and monolingual data to further align token representations,0.6622244119644165
translation,284,8,model,two techniques,has,contrastive learning scheme,two techniques has contrastive learning scheme,0.5679042935371399
translation,284,8,model,model,has,mrasp2,model has mrasp2,0.5964649319648743
translation,284,25,model,unified many - to -many multilingual nmt,with,only english-centric parallel corpora,unified many - to -many multilingual nmt with only english-centric parallel corpora,0.6254706978797913
translation,284,25,model,unified many - to -many multilingual nmt,with,additional monolingual corpora,unified many - to -many multilingual nmt with additional monolingual corpora,0.6347064971923828
translation,284,29,model,objective,of,mrasp2,objective of mrasp2,0.5595231652259827
translation,284,29,model,mrasp2,ensures,model,mrasp2 ensures model,0.6539212465286255
translation,284,29,model,model,to represent,similar sentences,model to represent similar sentences,0.6728667616844177
translation,284,29,model,similar sentences,across,languages,similar sentences across languages,0.6716300845146179
translation,284,29,model,languages,in,shared space,languages in shared space,0.5187047123908997
translation,284,29,model,shared space,by training,encoder,shared space by training encoder,0.7955694794654846
translation,284,29,model,encoder,to minimize,representation distance,encoder to minimize representation distance,0.7281790375709534
translation,284,29,model,representation distance,of,similar sentences,representation distance of similar sentences,0.5704054832458496
translation,284,29,model,model,has,objective,model has objective,0.5278581380844116
translation,284,31,model,effective aligned augmentation technique,by extending,"ras ( lin et al. , 2020 )","effective aligned augmentation technique by extending ras ( lin et al. , 2020 )",0.7367310523986816
translation,284,31,model,"ras ( lin et al. , 2020 )",on,parallel and monolingual corpora,"ras ( lin et al. , 2020 ) on parallel and monolingual corpora",0.49289676547050476
translation,284,31,model,parallel and monolingual corpora,to create,pseudo-pairs,parallel and monolingual corpora to create pseudo-pairs,0.6463241577148438
translation,284,31,model,model,introduce,effective aligned augmentation technique,model introduce effective aligned augmentation technique,0.6711380481719971
translation,284,43,model,base architecture,of,mrasp2,base architecture of mrasp2,0.566499650478363
translation,284,43,model,mrasp2,is,state- of- theart transformer,mrasp2 is state- of- theart transformer,0.595039963722229
translation,284,43,model,model,has,base architecture,model has base architecture,0.5817302465438843
translation,284,52,model,mrasp2,introduces,contrastive loss,mrasp2 introduces contrastive loss,0.6808015704154968
translation,284,52,model,contrastive loss,to explicitly bring,different languages,contrastive loss to explicitly bring different languages,0.6270653605461121
translation,284,52,model,different languages,to map,shared semantic space,different languages to map shared semantic space,0.7020189166069031
translation,284,52,model,model,has,mrasp2,model has mrasp2,0.5964649319648743
translation,284,9,results,mrasp2,achieves,competitive or even better performance,mrasp2 achieves competitive or even better performance,0.6968730092048645
translation,284,9,results,competitive or even better performance,than,pre-trained and fine-tuned model mbart,competitive or even better performance than pre-trained and fine-tuned model mbart,0.5590165853500366
translation,284,9,results,pre-trained and fine-tuned model mbart,on,tens of wmt 's translation directions,pre-trained and fine-tuned model mbart on tens of wmt 's translation directions,0.510845422744751
translation,284,9,results,english-centric directions,has,mrasp2,english-centric directions has mrasp2,0.6233501434326172
translation,284,9,results,mrasp2,has,outperforms,mrasp2 has outperforms,0.6528735756874084
translation,284,9,results,outperforms,has,existing best unified model,outperforms has existing best unified model,0.5562112927436829
translation,284,9,results,results,For,english-centric directions,results For english-centric directions,0.5757575631141663
translation,284,10,results,mrasp2,achieves,improvement,mrasp2 achieves improvement,0.634596586227417
translation,284,10,results,improvement,of,average 10 + bleu,improvement of average 10 + bleu,0.5613264441490173
translation,284,10,results,average 10 + bleu,compared with,multilingual transformer baseline,average 10 + bleu compared with multilingual transformer baseline,0.6372690200805664
translation,284,10,results,non-english directions,has,mrasp2,non-english directions has mrasp2,0.6252502799034119
translation,284,10,results,results,For,non-english directions,results For non-english directions,0.5828667283058167
translation,284,30,results,mrasp2,by leveraging,monolingual data,mrasp2 by leveraging monolingual data,0.6801955699920654
translation,284,30,results,monolingual data,to further improve,multilingual translation quality,monolingual data to further improve multilingual translation quality,0.6457959413528442
translation,284,30,results,results,boost,mrasp2,results boost mrasp2,0.6331944465637207
translation,284,34,results,strong multilingual baseline,in,20 translation directions,strong multilingual baseline in 20 translation directions,0.4863826036453247
translation,284,34,results,strong multilingual baseline,on,wmt testsets,strong multilingual baseline on wmt testsets,0.5412172675132751
translation,284,34,results,englishcentric directions,has,mrasp2,englishcentric directions has mrasp2,0.5926414728164673
translation,284,34,results,mrasp2,has,outperforms,mrasp2 has outperforms,0.6528735756874084
translation,284,34,results,outperforms,has,strong multilingual baseline,outperforms has strong multilingual baseline,0.5968392491340637
translation,284,34,results,results,For,englishcentric directions,results For englishcentric directions,0.576035737991333
translation,284,35,results,mrasp2,obtains,better results,mrasp2 obtains better results,0.6330589056015015
translation,284,35,results,better results,than,strong bilingual mbart model,better results than strong bilingual mbart model,0.5486772656440735
translation,284,35,results,10 wmt translation benchmarks,has,mrasp2,10 wmt translation benchmarks has mrasp2,0.5807474851608276
translation,284,35,results,results,On,10 wmt translation benchmarks,results On 10 wmt translation benchmarks,0.4736131429672241
translation,284,125,results,gap,with,pivot-based model,gap with pivot-based model,0.6438171863555908
translation,284,125,results,mrasp2,has,significantly outperforms,mrasp2 has significantly outperforms,0.626102089881897
translation,284,125,results,significantly outperforms,has,m-transformer,significantly outperforms has m-transformer,0.6000994443893433
translation,284,125,results,substantially narrows,has,gap,substantially narrows has gap,0.5818485021591187
translation,284,125,results,results,find that,mrasp2,results find that mrasp2,0.6152262687683105
translation,284,129,results,mrasp2,improves,zero-shot translation,mrasp2 improves zero-shot translation,0.6733834743499756
translation,284,129,results,zero-shot translation,by,large margin,zero-shot translation by large margin,0.5399047136306763
translation,284,129,results,zero-shot translation,without losing,performance,zero-shot translation without losing performance,0.6256514191627502
translation,284,129,results,performance,on,english -centric directions,performance on english -centric directions,0.5432770252227783
translation,284,129,results,results,has,mrasp2,results has mrasp2,0.5210095643997192
translation,284,143,results,mrasp2,further improves,bleu,mrasp2 further improves bleu,0.7315087914466858
translation,284,143,results,bleu,in,all of the three scenarios,bleu in all of the three scenarios,0.520515501499176
translation,284,143,results,bleu,in,unsupervised directions,bleu in unsupervised directions,0.5205220580101013
translation,284,143,results,bleu,especially in,unsupervised directions,bleu especially in unsupervised directions,0.5996789932250977
translation,284,143,results,results,has,mrasp2,results has mrasp2,0.5210095643997192
translation,284,158,results,overall accuracy,follows,rule,overall accuracy follows rule,0.7452352046966553
translation,284,158,results,m-transformer,<,mrasp2 w/o aa,m-transformer < mrasp2 w/o aa,0.7175158858299255
translation,284,158,results,mrasp2 w/o aa,<,mrasp2,mrasp2 w/o aa < mrasp2,0.6154687404632568
translation,284,158,results,mrasp2 w/o aa,<,mrasp2,mrasp2 w/o aa < mrasp2,0.6154687404632568
translation,284,158,results,mrasp2,brings,more significant improvements,mrasp2 brings more significant improvements,0.6387675404548645
translation,284,158,results,more significant improvements,for,languages,more significant improvements for languages,0.6880561709403992
translation,284,158,results,languages,with,less data volume,languages with less data volume,0.6376392841339111
translation,284,158,results,less data volume,in,pc32,less data volume in pc32,0.5362696051597595
translation,284,158,results,rule,has,m-transformer,rule has m-transformer,0.6546406149864197
translation,284,158,results,m-transformer,has,mrasp2 w/o aa,m-transformer has mrasp2 w/o aa,0.6131837964057922
translation,284,159,results,mrasp2,increases,translation bleu score,mrasp2 increases translation bleu score,0.6217044591903687
translation,284,164,results,mrasp2,improves on,dutch ( nl ),mrasp2 improves on dutch ( nl ),0.692550778388977
translation,284,164,results,results,has,mrasp2,results has mrasp2,0.5210095643997192
translation,286,82,baselines,pivot,train,unsup( std ?tgt ) model,pivot train unsup( std ?tgt ) model,0.7381290793418884
translation,286,82,baselines,unsup( std ?tgt ) model,using,std and tgt monolingual corpora,unsup( std ?tgt ) model using std and tgt monolingual corpora,0.6318585872650146
translation,286,82,baselines,baselines,has,pivot,baselines has pivot,0.5686378479003906
translation,286,63,experimental-setup,raw text,using,byte pair encoding ( bpe,raw text using byte pair encoding ( bpe,0.6697158217430115
translation,286,63,experimental-setup,24 k merge operations,on,each src - std corpus,24 k merge operations on each src - std corpus,0.5375086665153503
translation,286,63,experimental-setup,24 k merge operations,trained separately on,src and std,24 k merge operations trained separately on src and std,0.6898577809333801
translation,286,63,experimental-setup,each src - std corpus,trained separately on,src and std,each src - std corpus trained separately on src and std,0.7186036705970764
translation,286,63,experimental-setup,experimental setup,preprocess,raw text,experimental setup preprocess raw text,0.7257814407348633
translation,286,71,experimental-setup,standard opennmt - py seq2seq models,of,py-torch,standard opennmt - py seq2seq models of py-torch,0.5615770816802979
translation,286,71,experimental-setup,standard opennmt - py seq2seq models,to train,our model,standard opennmt - py seq2seq models to train our model,0.6895874738693237
translation,286,71,experimental-setup,our model,with,vmf loss,our model with vmf loss,0.6527073979377747
translation,286,71,experimental-setup,vmf loss,has,"kumar and tsvetkov , 2019 )","vmf loss has kumar and tsvetkov , 2019 )",0.6081611514091492
translation,286,71,experimental-setup,experimental setup,modify,standard opennmt - py seq2seq models,experimental setup modify standard opennmt - py seq2seq models,0.6443830132484436
translation,286,51,experiments,english ( en ),as,src,english ( en ) as src,0.5746435523033142
translation,286,51,experiments,english ( en ),as,tgts,english ( en ) as tgts,0.5515503883361816
translation,286,51,experiments,english ( en ),as,tgts,english ( en ) as tgts,0.5515503883361816
translation,286,51,experiments,russian ( ru ),as,std,russian ( ru ) as std,0.5675112009048462
translation,286,51,experiments,belarusian ( be ),as,tgts,belarusian ( be ) as tgts,0.5572923421859741
translation,286,87,results,performance,of,lang - varmt,performance of lang - varmt,0.6174054145812988
translation,286,87,results,baselines,for,"ukrainian , belarusian , nynorsk ,","baselines for ukrainian , belarusian , nynorsk ,",0.631401002407074
translation,286,87,results,baselines,for,four arabic varieties,baselines for four arabic varieties,0.6133209466934204
translation,286,87,results,results,compares,performance,results compares performance,0.7928104996681213
translation,286,89,results,synthetic setup,Considering,std and tgt,synthetic setup Considering std and tgt,0.6886934638023376
translation,286,89,results,same language,is,sub-optimal,same language is sub-optimal,0.5900368094444275
translation,286,89,results,poor performance,of,non-adapted sup ( src ? std ) model,poor performance of non-adapted sup ( src ? std ) model,0.5786080956459045
translation,286,89,results,results,has,synthetic setup,results has synthetic setup,0.5307436585426331
translation,286,91,results,direct unsupervised translation,from,src to tgt,direct unsupervised translation from src to tgt,0.6414210796356201
translation,286,91,results,direct unsupervised translation,performs,poorly,direct unsupervised translation performs poorly,0.6533336043357849
translation,286,91,results,results,has,direct unsupervised translation,results has direct unsupervised translation,0.5510775446891785
translation,286,93,results,src,to,tgt,src to tgt,0.6809540390968323
translation,286,93,results,src,by pivoting through,std,src by pivoting through std,0.6700670123100281
translation,286,93,results,tgt,by pivoting through,std,tgt by pivoting through std,0.6759682297706604
translation,286,93,results,std,achieves,much better performance,std achieves much better performance,0.6945664286613464
translation,286,93,results,much better performance,owing to,strong unsup ( std ?tgt ) models,much better performance owing to strong unsup ( std ?tgt ) models,0.681786060333252
translation,286,93,results,results,Translating,src,results Translating src,0.5078273415565491
translation,286,97,results,langvarmt,has,consistently outperforms,langvarmt has consistently outperforms,0.6435971260070801
translation,286,97,results,consistently outperforms,has,all baselines,consistently outperforms has all baselines,0.5969254374504089
translation,286,97,results,results,has,langvarmt,results has langvarmt,0.5758264064788818
translation,286,98,results,1m uk sentences,achieves,similar performance,1m uk sentences achieves similar performance,0.6887152194976807
translation,286,98,results,similar performance,to,softmax ablation,similar performance to softmax ablation,0.5519956350326538
translation,286,98,results,similar performance,to,softmax,similar performance to softmax,0.559258222579956
translation,286,98,results,softmax ablation,of,our method,softmax ablation of our method,0.5984919667243958
translation,286,98,results,softmax ablation,of,softmax,softmax ablation of softmax,0.5740845203399658
translation,286,98,results,softmax ablation,of,small gains,softmax ablation of small gains,0.5800216197967529
translation,286,98,results,small gains,over,unsupervised methods,small gains over unsupervised methods,0.6762953400611877
translation,286,98,results,results,Using,1m uk sentences,results Using 1m uk sentences,0.6334646940231323
translation,286,99,results,approach,clearly better than,strongest baselines,approach clearly better than strongest baselines,0.7140068411827087
translation,286,99,results,strongest baselines,by,4 bleu points,strongest baselines by 4 bleu points,0.5648642778396606
translation,286,99,results,strongest baselines,over,4 bleu points,strongest baselines over 4 bleu points,0.6544587016105652
translation,286,99,results,strongest baselines,over,3.9 points,strongest baselines over 3.9 points,0.6316835284233093
translation,286,99,results,4 bleu points,for,uk ( 10k ),4 bleu points for uk ( 10k ),0.6234527230262756
translation,286,99,results,3.9 points,for,be ( 100k ),3.9 points for be ( 100k ),0.6608067154884338
translation,286,99,results,lower resource settings,has,approach,lower resource settings has approach,0.6059361696243286
translation,286,99,results,results,in,lower resource settings,results in lower resource settings,0.5320024490356445
translation,286,101,results,all data sizes,both,uk and be,all data sizes both uk and be,0.746954083442688
translation,286,101,results,all data sizes,achieve,substantial increase,all data sizes achieve substantial increase,0.6604759097099304
translation,286,101,results,uk and be,achieve,substantial increase,uk and be achieve substantial increase,0.664871096611023
translation,286,101,results,substantial increase,in,bleu,substantial increase in bleu,0.6059704422950745
translation,286,101,results,bleu,up to,6 bleu,bleu up to 6 bleu,0.60446697473526
translation,286,101,results,results,Across,all data sizes,results Across all data sizes,0.6572611927986145
translation,286,107,results,langvarmt,improves in,all other arabic varieties,langvarmt improves in all other arabic varieties,0.6422947645187378
translation,286,107,results,results,has,langvarmt,results has langvarmt,0.5758264064788818
translation,286,110,results,our approach,improves by,2.3 bleu points,our approach improves by 2.3 bleu points,0.6505166292190552
translation,286,110,results,2.3 bleu points,over,softmax - based baseline,2.3 bleu points over softmax - based baseline,0.5478650331497192
translation,286,110,results,results,has,our approach,results has our approach,0.6050099730491638
translation,286,114,results,sup ( en ?no ) model,performs,well,sup ( en ?no ) model performs well,0.6610370874404907
translation,286,114,results,well,on,nn,well on nn,0.7023167610168457
translation,286,114,results,well,with,11.3 bleu,well with 11.3 bleu,0.6574346423149109
translation,286,114,results,nn,with,11.3 bleu,nn with 11.3 bleu,0.630175769329071
translation,286,114,results,our method,yields,further gains,our method yields further gains,0.7283284664154053
translation,286,114,results,further gains,of,over 4 points,further gains of over 4 points,0.603888213634491
translation,286,114,results,over 4 points,over,baselines,over 4 points over baselines,0.7823309302330017
translation,287,78,baselines,auto-encoding,of,source sentences,auto-encoding of source sentences,0.5623422265052795
translation,287,78,baselines,ae src,has,auto-encoding,ae src has auto-encoding,0.5812236666679382
translation,287,79,baselines,auto-encoding,of,target sentences,auto-encoding of target sentences,0.5495232939720154
translation,287,79,baselines,ae trg,has,auto-encoding,ae trg has auto-encoding,0.5823467373847961
translation,287,79,baselines,baselines,has,ae trg,baselines has ae trg,0.5636865496635437
translation,287,80,baselines,"training shared - encoder , target - decoder , and attention",with,back - translated source sentences,"training shared - encoder , target - decoder , and attention with back - translated source sentences",0.5730334520339966
translation,287,80,baselines,"training shared - encoder , target - decoder , and attention",with,actual target sentences,"training shared - encoder , target - decoder , and attention with actual target sentences",0.5933486223220825
translation,287,80,baselines,back - translated source sentences,as,input,back - translated source sentences as input,0.5175342559814453
translation,287,80,baselines,actual target sentences,as,output,actual target sentences as output,0.5440360307693481
translation,287,80,baselines,bt src,has,"training shared - encoder , target - decoder , and attention","bt src has training shared - encoder , target - decoder , and attention",0.5253597497940063
translation,287,80,baselines,baselines,has,bt src,baselines has bt src,0.568756103515625
translation,287,81,baselines,baselines,has,bt trg,baselines has bt trg,0.5532791614532471
translation,287,104,baselines,xlm,employs,two -stage training,xlm employs two -stage training,0.6299667358398438
translation,287,104,baselines,two -stage training,of,unmt model,two -stage training of unmt model,0.5444020628929138
translation,287,104,baselines,baselines,has,xlm,baselines has xlm,0.6134690642356873
translation,287,95,experimental-setup,corpus,for,normalization,corpus for normalization,0.586790919303894
translation,287,95,experimental-setup,corpus,for,tokenization,corpus for tokenization,0.5950025320053101
translation,287,95,experimental-setup,corpus,for,lowercasing,corpus for lowercasing,0.6702778935432434
translation,287,95,experimental-setup,lowercasing,using,scripts,lowercasing using scripts,0.727073073387146
translation,287,95,experimental-setup,bpe segmentation,using,"subword - nmt ( sennrich et al. , 2016 )","bpe segmentation using subword - nmt ( sennrich et al. , 2016 )",0.6030526161193848
translation,287,95,experimental-setup,number of merge operations,set to,50k,number of merge operations set to 50k,0.6994954943656921
translation,287,95,experimental-setup,experimental setup,preprocessed,corpus,experimental setup preprocessed corpus,0.7140583395957947
translation,287,95,experimental-setup,experimental setup,for,bpe segmentation,experimental setup for bpe segmentation,0.5824622511863708
translation,287,96,experimental-setup,monolingual corpora,to independently train,embeddings,monolingual corpora to independently train embeddings,0.624082624912262
translation,287,96,experimental-setup,embeddings,for,each language,embeddings for each language,0.6194770932197571
translation,287,96,experimental-setup,each language,using,skip-gram model of word2vec,each language using skip-gram model of word2vec,0.62528395652771
translation,287,96,experimental-setup,experimental setup,use,monolingual corpora,experimental setup use monolingual corpora,0.5392030477523804
translation,287,97,experimental-setup,two languages,to,shared space,two languages to shared space,0.5670856237411499
translation,287,98,experimental-setup,undreamt 3 tool,to train,unmt system,undreamt 3 tool to train unmt system,0.6364989876747131
translation,287,105,model,pre-training stage,trains,encoder and decoder,pre-training stage trains encoder and decoder,0.779304027557373
translation,287,105,model,encoder and decoder,with,masked language modeling objective,encoder and decoder with masked language modeling objective,0.6080881953239441
translation,287,105,model,model,has,pre-training stage,model has pre-training stage,0.5330135226249695
translation,287,10,results,our proposed solution,achieves,significant performance improvement,our proposed solution achieves significant performance improvement,0.6647499799728394
translation,287,10,results,unmt models,train,conventionally,unmt models train conventionally,0.6950891613960266
translation,287,10,results,significant performance improvement,has,unmt models,significant performance improvement has unmt models,0.5768259167671204
translation,287,10,results,results,has,our proposed solution,results has our proposed solution,0.5817221403121948
translation,287,122,results,proposed re-training strategy,of,ae,proposed re-training strategy of ae,0.5941282510757446
translation,287,122,results,proposed re-training strategy,results in,statistically significant improvements ( p- value < 0.05 ),proposed re-training strategy results in statistically significant improvements ( p- value < 0.05 ),0.6618444323539734
translation,287,122,results,ae,used in conjunction with,bt,ae used in conjunction with bt,0.5787782073020935
translation,287,122,results,statistically significant improvements ( p- value < 0.05 ),across,all language pairs,statistically significant improvements ( p- value < 0.05 ) across all language pairs,0.6913146376609802
translation,287,122,results,statistically significant improvements ( p- value < 0.05 ),compared to,undreamt baseline approach,statistically significant improvements ( p- value < 0.05 ) compared to undreamt baseline approach,0.7060576677322388
translation,287,122,results,results,observe,proposed re-training strategy,results observe proposed re-training strategy,0.624706506729126
translation,287,142,results,baseline system,suffers from,drop,baseline system suffers from drop,0.7523043155670166
translation,287,142,results,drop,in,bleu score,drop in bleu score,0.5358816385269165
translation,287,142,results,denoising strategy,introducing,ambiguity,denoising strategy introducing ambiguity,0.7501356601715088
translation,287,142,results,ambiguity,into,model,ambiguity into model,0.6067454814910889
translation,287,142,results,results,has,baseline system,results has baseline system,0.605924129486084
translation,287,165,results,our simple retraining strategy,retraining,trained models,our simple retraining strategy retraining trained models,0.7494634389877319
translation,287,165,results,trained models,by removing,denoising component,trained models by removing denoising component,0.6864350438117981
translation,287,165,results,denoising component,from,auto-encoder objective ( ae ),denoising component from auto-encoder objective ( ae ),0.5421298146247864
translation,287,165,results,significant improvements,in,bleu scores,significant improvements in bleu scores,0.5356258749961853
translation,287,165,results,bleu scores,for,four language pairs,bleu scores for four language pairs,0.5603711605072021
translation,287,165,results,results,has,our simple retraining strategy,results has our simple retraining strategy,0.5424310564994812
translation,287,167,results,robustness,of,pre-trained language models,robustness of pre-trained language models,0.5517804622650146
translation,287,167,results,pre-trained language models,to,scrambled translation problem,pre-trained language models to scrambled translation problem,0.5188608169555664
translation,287,167,results,results,observe,robustness,results observe robustness,0.5889001488685608
translation,288,59,experimental-setup,sentences,with,"moses ( koehn et al. , 2007 ) tools","sentences with moses ( koehn et al. , 2007 ) tools",0.6557446122169495
translation,288,59,experimental-setup,"bpe ( sennrich et al. , 2016 )",with,30000 merging operations,"bpe ( sennrich et al. , 2016 ) with 30000 merging operations",0.6018778681755066
translation,288,59,experimental-setup,experimental setup,tokenize and truecase,sentences,experimental setup tokenize and truecase sentences,0.7160800695419312
translation,288,59,experimental-setup,experimental setup,applying,"bpe ( sennrich et al. , 2016 )","experimental setup applying bpe ( sennrich et al. , 2016 )",0.6210834383964539
translation,289,38,ablation-analysis,context - aware models,use,some information,context - aware models use some information,0.6731066703796387
translation,289,38,ablation-analysis,some information,from,context,some information from context,0.6215292811393738
translation,289,38,ablation-analysis,not increase uniformly,with,context size,not increase uniformly with context size,0.7017233967781067
translation,289,38,ablation-analysis,target context,seems to be used more by,models,target context seems to be used more by models,0.6561252474784851
translation,289,38,ablation-analysis,models,than,source context,models than source context,0.5260021686553955
translation,289,38,ablation-analysis,ablation analysis,find that,context - aware models,ablation analysis find that context - aware models,0.5850464701652527
translation,289,133,ablation-analysis,helpful,for,multi-encoder model,helpful for multi-encoder model,0.6599184274673462
translation,289,133,ablation-analysis,helpful,with,coword dropout,helpful with coword dropout,0.6666014194488525
translation,289,133,ablation-analysis,coword dropout,helping,significantly,coword dropout helping significantly,0.7004550695419312
translation,289,133,ablation-analysis,ablation analysis,shows,coword dropout,ablation analysis shows coword dropout,0.669511079788208
translation,289,41,hyperparameters,words,from,current source sentence,words from current source sentence,0.5189974904060364
translation,289,41,hyperparameters,hyperparameters,randomly drop,words,hyperparameters randomly drop words,0.7494609355926514
translation,289,80,hyperparameters,non-pretrained case,use,20 k vocabulary size,non-pretrained case use 20 k vocabulary size,0.6054770350456238
translation,289,80,hyperparameters,non-pretrained case,use,32 k vocabulary size,non-pretrained case use 32 k vocabulary size,0.6022449135780334
translation,289,80,hyperparameters,non-pretrained case,for,pretrained case,non-pretrained case for pretrained case,0.6052349805831909
translation,289,80,hyperparameters,20 k vocabulary size,shared across,source / target,20 k vocabulary size shared across source / target,0.7150139212608337
translation,289,80,hyperparameters,pretrained case,use,32 k vocabulary size,pretrained case use 32 k vocabulary size,0.5836333632469177
translation,289,80,hyperparameters,hyperparameters,For,non-pretrained case,hyperparameters For non-pretrained case,0.5531426668167114
translation,289,80,hyperparameters,hyperparameters,for,pretrained case,hyperparameters for pretrained case,0.5634602308273315
translation,289,83,hyperparameters,hidden size,of,512,hidden size of 512,0.640839695930481
translation,289,83,hyperparameters,feedforward size,of,1024,feedforward size of 1024,0.6507963538169861
translation,289,83,hyperparameters,transformer,has,hidden size,transformer has hidden size,0.6068261861801147
translation,289,83,hyperparameters,transformer,has,feedforward size,transformer has feedforward size,0.5561947822570801
translation,289,83,hyperparameters,transformer,has,6 layers,transformer has 6 layers,0.6229946613311768
translation,289,83,hyperparameters,transformer,has,8 attention heads,transformer has 8 attention heads,0.6171040534973145
translation,289,83,hyperparameters,hyperparameters,train,transformer,hyperparameters train transformer,0.653059720993042
translation,289,84,hyperparameters,hidden size,of,1024,hidden size of 1024,0.6436027884483337
translation,289,84,hyperparameters,feedforward size,of,4096,feedforward size of 4096,0.636847734451294
translation,289,84,hyperparameters,transformer large architecture,has,hidden size,transformer large architecture has hidden size,0.5921568274497986
translation,289,85,hyperparameters,adam optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adam optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6311911940574646
translation,289,85,hyperparameters,adam optimizer,with,linear warm - up,adam optimizer with linear warm - up,0.6248177886009216
translation,289,85,hyperparameters,adam optimizer,use,inverse square root learning rate scheduler,adam optimizer use inverse square root learning rate scheduler,0.5998950600624084
translation,289,85,hyperparameters,adam optimizer,with,linear warm - up,adam optimizer with linear warm - up,0.6248177886009216
translation,289,85,hyperparameters,inverse square root learning rate scheduler,with,initial value,inverse square root learning rate scheduler with initial value,0.6296774744987488
translation,289,85,hyperparameters,inverse square root learning rate scheduler,with,linear warm - up,inverse square root learning rate scheduler with linear warm - up,0.678528904914856
translation,289,85,hyperparameters,initial value,of,10 ?4 and 5 ? 10 ?4,initial value of 10 ?4 and 5 ? 10 ?4,0.6502028107643127
translation,289,85,hyperparameters,10 ?4 and 5 ? 10 ?4,for,pretrained and non-pretrained cases,10 ?4 and 5 ? 10 ?4 for pretrained and non-pretrained cases,0.6629376411437988
translation,289,85,hyperparameters,linear warm - up,in,first 4000 steps,linear warm - up in first 4000 steps,0.5578699111938477
translation,289,85,hyperparameters,hyperparameters,train using,adam optimizer,hyperparameters train using adam optimizer,0.7071405053138733
translation,289,86,hyperparameters,models,with,early stopping,models with early stopping,0.605107843875885
translation,289,86,hyperparameters,early stopping,on,validation perplexity,early stopping on validation perplexity,0.5686540603637695
translation,289,86,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,289,6,model,new metric,to quantify,usage of context,new metric to quantify usage of context,0.7223836779594421
translation,289,6,model,new metric,has,conditional cross-mutual information,new metric has conditional cross-mutual information,0.5667849183082581
translation,289,6,model,usage of context,has,by these models,usage of context has by these models,0.5944987535476685
translation,289,6,model,model,introduce,new metric,model introduce new metric,0.6943975687026978
translation,289,9,model,context,by,context - aware models,context by context - aware models,0.5905181765556335
translation,289,9,model,"new , simple training method",has,context - aware word dropout,"new , simple training method has context - aware word dropout",0.5247231721878052
translation,289,9,model,model,introduce,"new , simple training method","model introduce new , simple training method",0.653093695640564
translation,289,40,model,word dropout,for,context - aware machine translation,word dropout for context - aware machine translation,0.5287805199623108
translation,289,40,model,"al. , 2016a )",for,context - aware machine translation,"al. , 2016a ) for context - aware machine translation",0.5560024976730347
translation,289,40,model,model,simple but effective variation of,word dropout,model simple but effective variation of word dropout,0.6253538131713867
translation,289,92,results,non-pretrained case,for,both the source and target context,non-pretrained case for both the source and target context,0.5460030436515808
translation,289,92,results,biggest jump,in,context usage,biggest jump in context usage,0.4891473650932312
translation,289,92,results,context usage,increase,context size,context usage increase context size,0.6823811531066895
translation,289,92,results,context size,from,0 to 1,context size from 0 to 1,0.5784037113189697
translation,289,92,results,non-pretrained case,has,biggest jump,non-pretrained case has biggest jump,0.6187687516212463
translation,289,92,results,both the source and target context,has,biggest jump,both the source and target context has biggest jump,0.601330041885376
translation,289,92,results,results,For,non-pretrained case,results For non-pretrained case,0.629201352596283
translation,289,131,results,improvements,are,not as noticeable,improvements are not as noticeable,0.5790254473686218
translation,289,131,results,pretrained case,has,improvements,pretrained case has improvements,0.609232485294342
translation,289,131,results,models,has,dropout,models has dropout,0.5741817951202393
translation,289,131,results,results,For,pretrained case,results For pretrained case,0.6266536116600037
translation,289,141,results,coword dropout,leads to,improved performance,coword dropout leads to improved performance,0.6628694534301758
translation,289,141,results,improved performance,particularly for,non-pretrained case,improved performance particularly for non-pretrained case,0.6276010870933533
translation,289,141,results,results,increasing,coword dropout,results increasing coword dropout,0.6038511991500854
translation,289,144,results,coword dropout,improves,performance,coword dropout improves performance,0.6794116497039795
translation,289,144,results,performance,of,multi-encoder model,performance of multi-encoder model,0.5844447016716003
translation,289,144,results,multi-encoder model,across,all phenomena,multi-encoder model across all phenomena,0.7458317875862122
translation,289,144,results,benefits,for,multiple architectures,benefits for multiple architectures,0.6172462105751038
translation,289,144,results,multiple architectures,for,context - aware machine translation,multiple architectures for context - aware machine translation,0.5355284214019775
translation,289,144,results,results,shows that,coword dropout,results shows that coword dropout,0.6342406868934631
translation,290,65,hyperparameters,hyperparameters,trained for,"100,000 updates","hyperparameters trained for 100,000 updates",0.7334703207015991
translation,290,66,hyperparameters,"label smoothed cross entropy ( szegedy et al. , 2016 )",as,l t,"label smoothed cross entropy ( szegedy et al. , 2016 ) as l t",0.5310767292976379
translation,290,66,hyperparameters,"label smoothed cross entropy ( szegedy et al. , 2016 )",set,label smoothing ?,"label smoothed cross entropy ( szegedy et al. , 2016 ) set label smoothing ?",0.6201126575469971
translation,290,66,hyperparameters,l t,of,objective function,l t of objective function,0.5785714387893677
translation,290,66,hyperparameters,label smoothing ?,to,0.1,label smoothing ? to 0.1,0.5486994981765747
translation,290,66,hyperparameters,hyperparameters,used,"label smoothed cross entropy ( szegedy et al. , 2016 )","hyperparameters used label smoothed cross entropy ( szegedy et al. , 2016 )",0.5394911170005798
translation,290,66,hyperparameters,hyperparameters,set,label smoothing ?,hyperparameters set label smoothing ?,0.6376040577888489
translation,290,67,hyperparameters,hyperparameter ? sync,tuned for,each development set,hyperparameter ? sync tuned for each development set,0.7362775802612305
translation,290,67,hyperparameters,hyperparameter ? sync,set to,0.5,hyperparameter ? sync set to 0.5,0.7044844627380371
translation,290,67,hyperparameters,hyperparameter ? sync,set to,0.1,hyperparameter ? sync set to 0.1,0.6879498958587646
translation,290,67,hyperparameters,hyperparameter ? sync,set to,10.0,hyperparameter ? sync set to 10.0,0.6991238594055176
translation,290,67,hyperparameters,0.5,for,wmt14 en-de,0.5 for wmt14 en-de,0.6384344696998596
translation,290,67,hyperparameters,0.1,for,wmt16 en-ro,0.1 for wmt16 en-ro,0.565122663974762
translation,290,67,hyperparameters,10.0,for,as - pec ja-en,10.0 for as - pec ja-en,0.6549145579338074
translation,290,67,hyperparameters,hyperparameters,set to,0.5,hyperparameters set to 0.5,0.6732792854309082
translation,290,70,hyperparameters,decoding,used,beam search,decoding used beam search,0.5754576921463013
translation,290,70,hyperparameters,decoding,set,beam size,decoding set beam size,0.6532772183418274
translation,290,70,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,290,70,hyperparameters,beam search,set,beam size,beam search set beam size,0.6893483400344849
translation,290,70,hyperparameters,beam size,to,4,beam size to 4,0.6383150219917297
translation,290,70,hyperparameters,hyperparameters,In,decoding,hyperparameters In decoding,0.48789671063423157
translation,290,5,model,source-side and target-side syntactic self-attentions,by minimizing,difference,source-side and target-side syntactic self-attentions by minimizing difference,0.6406253576278687
translation,290,5,model,difference,between,target -side selfattentions,difference between target -side selfattentions,0.6278530955314636
translation,290,5,model,difference,between,source-side self-attentions,difference between source-side self-attentions,0.6850869655609131
translation,290,5,model,source-side self-attentions,mapped by,encoder-decoder attention matrix,source-side self-attentions mapped by encoder-decoder attention matrix,0.6936247944831848
translation,290,16,model,sentence structures,aligned across,two languages,sentence structures aligned across two languages,0.6778705716133118
translation,290,16,model,sentence structures,by,aligned self-attentions,sentence structures by aligned self-attentions,0.5729268193244934
translation,290,16,model,aligned self-attentions,on,source - and targetside,aligned self-attentions on source - and targetside,0.5771523714065552
translation,290,23,model,encoder layers and decoder layers,composed of,multiple sub-layers,encoder layers and decoder layers composed of multiple sub-layers,0.6627307534217834
translation,290,23,model,multiple sub-layers,includes,self-attention layer,multiple sub-layers includes self-attention layer,0.5922351479530334
translation,290,23,model,multiple sub-layers,includes,feed forward layer,multiple sub-layers includes feed forward layer,0.6452109813690186
translation,290,23,model,model,has,encoder layers and decoder layers,model has encoder layers and decoder layers,0.5435553193092346
translation,290,24,model,decoder layers,apply,encoder-decoder attention layer,decoder layers apply encoder-decoder attention layer,0.6480689644813538
translation,290,24,model,encoder-decoder attention layer,between,selfattention layer and the feed forward layer,encoder-decoder attention layer between selfattention layer and the feed forward layer,0.6042709946632385
translation,290,24,model,model,has,decoder layers,model has decoder layers,0.5338822603225708
translation,290,76,results,syncattn,improved by,"0.38 , 0.20 , and 0.27 bleu points","syncattn improved by 0.38 , 0.20 , and 0.27 bleu points",0.7098474502563477
translation,290,76,results,"0.38 , 0.20 , and 0.27 bleu points",in,"wmt14 en-de , wmt16 en-ro , aspec ja- en tasks","0.38 , 0.20 , and 0.27 bleu points in wmt14 en-de , wmt16 en-ro , aspec ja- en tasks",0.5292847752571106
translation,290,76,results,"0.38 , 0.20 , and 0.27 bleu points",compared to,dbsa,"0.38 , 0.20 , and 0.27 bleu points compared to dbsa",0.6206499934196472
translation,290,76,results,results,has,syncattn,results has syncattn,0.5844717025756836
translation,291,14,baselines,generative nmt,for,nmt,generative nmt for nmt,0.6636902689933777
translation,291,14,baselines,semantics,of,source and target sentences,semantics of source and target sentences,0.5443613529205322
translation,291,16,baselines,framework,generate,prototype,framework generate prototype,0.6719974875450134
translation,291,16,baselines,prototype,on,encoder side,prototype on encoder side,0.5486295819282532
translation,291,16,baselines,decoder,jointly use,encoding,decoder jointly use encoding,0.7519190907478333
translation,291,16,baselines,baselines,has,soft-prototype,baselines has soft-prototype,0.5959679484367371
translation,291,139,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,291,139,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,291,139,experimental-setup,beam size,has,4,beam size has 4,0.6405593156814575
translation,291,139,experimental-setup,length penalty,has,0.6,length penalty has 0.6,0.5379580855369568
translation,291,139,experimental-setup,experimental setup,employ,beam search,experimental setup employ beam search,0.4999246299266815
translation,291,143,experimental-setup,ffn filter,to,"1024 , 1024 , 16 , 6 , 6 and 4096","ffn filter to 1024 , 1024 , 16 , 6 , 6 and 4096",0.5879895091056824
translation,291,143,experimental-setup,model dimension,has,word embedding,model dimension has word embedding,0.5342845916748047
translation,291,143,experimental-setup,model dimension,has,decoder layer,model dimension has decoder layer,0.5357069373130798
translation,291,143,experimental-setup,experimental setup,set,model dimension,experimental setup set model dimension,0.6694767475128174
translation,291,144,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",employed with,"parameters ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9","adam optimizer ( kingma and ba , 2015 ) employed with parameters ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9",0.6313663721084595
translation,291,144,experimental-setup,experimental setup,has,"adam optimizer ( kingma and ba , 2015 )","experimental setup has adam optimizer ( kingma and ba , 2015 )",0.5367816686630249
translation,291,145,experimental-setup,dynamic learning rate,course of,nmt training,dynamic learning rate course of nmt training,0.6778926849365234
translation,291,145,experimental-setup,experimental setup,use,dynamic learning rate,experimental setup use dynamic learning rate,0.6229379773139954
translation,291,146,experimental-setup,dropout rate,set to,rate = 0.1,dropout rate set to rate = 0.1,0.7103398442268372
translation,291,146,experimental-setup,label smoothing,used with,gamma = 0.1,label smoothing used with gamma = 0.1,0.6495892405509949
translation,291,146,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,291,146,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,291,147,experiments,parallel corpora,for,one translation task ( e.g.,parallel corpora for one translation task ( e.g.,0.5509436726570129
translation,291,147,experiments,parallel corpora,concatenated to train,bpe,parallel corpora concatenated to train bpe,0.7857787609100342
translation,291,147,experiments,bpe,with,balance strategy,bpe with balance strategy,0.6941205263137817
translation,291,147,experiments,shared vocabulary,with,40,shared vocabulary with 40,0.6367957592010498
translation,291,147,experiments,shared vocabulary,with,000 subtokens,shared vocabulary with 000 subtokens,0.6664373874664307
translation,291,147,experiments,40,",",000 subtokens,"40 , 000 subtokens",0.635448694229126
translation,291,147,experiments,bpe,has,"sennrich et al. , 2016 b )","bpe has sennrich et al. , 2016 b )",0.5767214298248291
translation,291,6,model,global information,present,efficient method,global information present efficient method,0.6581426858901978
translation,291,6,model,global information,from,semantic space,global information from semantic space,0.525185227394104
translation,291,6,model,efficient method,consider,semantic draft,efficient method consider semantic draft,0.6628313660621643
translation,291,6,model,semantic draft,as,global information,semantic draft as global information,0.5333099961280823
translation,291,6,model,global information,from,semantic space,global information from semantic space,0.525185227394104
translation,291,6,model,decoding,with,almost free of cost,decoding with almost free of cost,0.6371928453445435
translation,291,6,model,model,to inject,global information,model to inject global information,0.7009645700454712
translation,291,6,model,model,present,efficient method,model present efficient method,0.6825156211853027
translation,291,15,model,deliberation,jointly considers,encoding,deliberation jointly considers encoding,0.7545392513275146
translation,291,15,model,framework,to predict,guess target sentence,framework to predict guess target sentence,0.6806329488754272
translation,291,15,model,framework,jointly considers,encoding,framework jointly considers encoding,0.7463889718055725
translation,291,15,model,guess target sentence,in,first- pass,guess target sentence in first- pass,0.5279328227043152
translation,291,15,model,guess target sentence,in,second - pass,guess target sentence in second - pass,0.5267086029052734
translation,291,15,model,guess target sentence,in,second - pass,guess target sentence in second - pass,0.5267086029052734
translation,291,15,model,target sentence,in,second - pass,target sentence in second - pass,0.494125097990036
translation,291,15,model,model,has,deliberation,model has deliberation,0.5613535046577454
translation,291,18,model,efficient method,consider,semantic draft,efficient method consider semantic draft,0.6628313660621643
translation,291,18,model,semantic draft,as,global information,semantic draft as global information,0.5333099961280823
translation,291,18,model,global information,for,decoding,global information for decoding,0.6407626271247864
translation,291,18,model,decoding,with,almost free of cost,decoding with almost free of cost,0.6371928453445435
translation,291,18,model,model,present,efficient method,model present efficient method,0.6825156211853027
translation,291,19,model,semantic draft,from,semantic space,semantic draft from semantic space,0.5762807130813599
translation,291,19,model,semantic draft,that is,gaussian inference model,semantic draft that is gaussian inference model,0.6384025812149048
translation,291,19,model,gaussian inference model,with,learnable parameters,gaussian inference model with learnable parameters,0.5830836296081543
translation,291,19,model,model,sample,semantic draft,model sample semantic draft,0.7180080413818359
translation,291,227,model,method / framework,to improve,performance,method / framework to improve performance,0.7356253266334534
translation,291,227,model,performance,of,nmt,performance of nmt,0.6209200620651245
translation,291,227,model,model,present,method / framework,model present method / framework,0.6265352964401245
translation,291,228,model,semantic draft,from,semantic space,semantic draft from semantic space,0.5762807130813599
translation,291,228,model,semantic draft,to obtain,required global information,semantic draft to obtain required global information,0.6283631324768066
translation,291,228,model,decoder,consider,semantic draft,decoder consider semantic draft,0.6897492408752441
translation,291,228,model,semantic draft,to obtain,required global information,semantic draft to obtain required global information,0.6283631324768066
translation,291,228,model,required global information,with,high efficiency,required global information with high efficiency,0.6377493739128113
translation,291,228,model,model,sample,semantic draft,model sample semantic draft,0.7180080413818359
translation,291,118,results,semantic space,built upon,multilingual encoder,semantic space built upon multilingual encoder,0.6172904968261719
translation,291,118,results,semantic space,built upon,cross-lingual generator,semantic space built upon cross-lingual generator,0.6669803261756897
translation,291,118,results,cross-lingual generator,in,our model,cross-lingual generator in our model,0.502291738986969
translation,291,118,results,semantic / global information,used in,decoder,semantic / global information used in decoder,0.6905168890953064
translation,291,118,results,comparison,has,semantic space,comparison has semantic space,0.5664882063865662
translation,291,118,results,results,has,comparison,results has comparison,0.5800483226776123
translation,291,166,results,baselines,of,big-transformer and gnmt,baselines of big-transformer and gnmt,0.5930925011634827
translation,291,166,results,big-transformer and gnmt,on,all the language pairs,big-transformer and gnmt on all the language pairs,0.5335408449172974
translation,291,166,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,291,166,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,291,166,results,results,has,our method,results has our method,0.5589964985847473
translation,291,167,results,our model,gains,competitive performance,our model gains competitive performance,0.748345673084259
translation,291,167,results,competitive performance,on,all the language pairs,competitive performance on all the language pairs,0.516522228717804
translation,291,167,results,state - of - the - art models,has,our model,state - of - the - art models has our model,0.5192977786064148
translation,291,167,results,results,Compared to,state - of - the - art models,results Compared to state - of - the - art models,0.6207705140113831
translation,291,183,results,some improvements,from employing,em - like process,some improvements from employing em - like process,0.6457578539848328
translation,291,183,results,best performance,on,all the language pairs,best performance on all the language pairs,0.5086081624031067
translation,291,183,results,results,observe,some improvements,results observe some improvements,0.627551794052124
translation,291,198,results,our model,achieves,competitive performance,our model achieves competitive performance,0.6867243051528931
translation,291,198,results,competitive performance,to,"mirror-gnmt ( zheng et al. , 2020 )","competitive performance to mirror-gnmt ( zheng et al. , 2020 )",0.542142927646637
translation,291,198,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,291,198,results,outperforms,has,gnmt,outperforms has gnmt,0.6416739225387573
translation,291,198,results,results,observe,our model,results observe our model,0.6353915333747864
translation,291,214,results,g,improves,general translation performance,g improves general translation performance,0.6890439391136169
translation,291,214,results,semantic space,improves,noisy translation,semantic space improves noisy translation,0.6365491151809692
translation,291,222,results,significantly improved,by simultaneously using,non-parallel data,significantly improved by simultaneously using non-parallel data,0.6914061903953552
translation,292,19,hyperparameters,transformers,trained with,early stopping,transformers trained with early stopping,0.7337927222251892
translation,292,19,hyperparameters,transformers,trained with,learning rate,transformers trained with learning rate,0.7238806486129761
translation,292,19,hyperparameters,learning rate,of,0.0002,learning rate of 0.0002,0.5917128324508667
translation,292,19,hyperparameters,0.0002,with,plateau - reduce schedule,0.0002 with plateau - reduce schedule,0.6174347996711731
translation,292,19,hyperparameters,plateau - reduce schedule,for,maximum of 350k updates,plateau - reduce schedule for maximum of 350k updates,0.6028576493263245
translation,292,19,hyperparameters,hyperparameters,has,transformers,hyperparameters has transformers,0.5240044593811035
translation,292,67,hyperparameters,12 layer model,using,"2 k bpe dataset subsets ( 100 % , 90 % , ... , 50 % )","12 layer model using 2 k bpe dataset subsets ( 100 % , 90 % , ... , 50 % )",0.6463426947593689
translation,292,67,hyperparameters,"2 k bpe dataset subsets ( 100 % , 90 % , ... , 50 % )",with,five different data shuffling seeds,"2 k bpe dataset subsets ( 100 % , 90 % , ... , 50 % ) with five different data shuffling seeds",0.6032067537307739
translation,292,67,hyperparameters,hyperparameters,train,12 layer model,hyperparameters train 12 layer model,0.6615753769874573
translation,294,186,experiments,promt submission promt.soft,is,clear winner,promt submission promt.soft is clear winner,0.5878702402114868
translation,294,186,experiments,single best system,according to,exact-match accuracy,single best system according to exact-match accuracy,0.6040352582931519
translation,294,186,experiments,one of the two best systems,according to,comet,one of the two best systems according to comet,0.7586414217948914
translation,294,186,experiments,english - russian,has,promt submission promt.soft,english - russian has promt submission promt.soft,0.5610141754150391
translation,294,188,experiments,submissions,by,kep,submissions by kep,0.6243840456008911
translation,294,188,experiments,kep,are,winning ones,kep are winning ones,0.6510006189346313
translation,294,188,experiments,winning ones,for,english -korean and czech - german,winning ones for english -korean and czech - german,0.6381872296333313
translation,294,130,results,exact match,with respect to,baselines,exact match with respect to baselines,0.6702260971069336
translation,294,130,results,pre-trained model,has,degrades,pre-trained model has degrades,0.5870735049247742
translation,294,130,results,improving,has,exact match,improving has exact match,0.5926094055175781
translation,294,130,results,degrades,has,all other metrics,degrades has all other metrics,0.5730441212654114
translation,294,130,results,results,has,pre-trained model,results has pre-trained model,0.5588445067405701
translation,294,182,results,two promt submissions,ranked,first,two promt submissions ranked first,0.6795248985290527
translation,294,182,results,first,according to,exact- match accuracy,first according to exact- match accuracy,0.6333149671554565
translation,294,182,results,exact- match accuracy,along with,cuni submission,exact- match accuracy along with cuni submission,0.6378064155578613
translation,294,182,results,promt.soft submission,is,statistically significantly better,promt.soft submission is statistically significantly better,0.5691685676574707
translation,294,182,results,statistically significantly better,than,other two,statistically significantly better than other two,0.6080661416053772
translation,294,182,results,other two,with respect to,comet,other two with respect to comet,0.7526284456253052
translation,294,182,results,results,has,two promt submissions,results has two promt submissions,0.520500898361206
translation,294,183,results,second winning submission,is,one by kep,second winning submission is one by kep,0.580849826335907
translation,294,183,results,second winning submission,ranks,first,second winning submission ranks first,0.7374528646469116
translation,294,183,results,one by kep,ranks,first,one by kep ranks first,0.675759494304657
translation,294,183,results,one by kep,according to,1 - ter,one by kep according to 1 - ter,0.7657049894332886
translation,294,183,results,first,according to,comet,first according to comet,0.7249186038970947
translation,294,183,results,results,has,second winning submission,results has second winning submission,0.5701647996902466
translation,294,185,results,another submission ( hw - tsc ),is,statistically significantly better,another submission ( hw - tsc ) is statistically significantly better,0.582472562789917
translation,294,185,results,statistically significantly better,than,all submissions,statistically significantly better than all submissions,0.5594029426574707
translation,294,185,results,all submissions,in,all metrics,all submissions in all metrics,0.4796942174434662
translation,294,185,results,all metrics,except for,1 - term,all metrics except for 1 - term,0.6538512110710144
translation,294,187,results,other system,ranks,first,other system ranks first,0.7907117009162903
translation,294,187,results,other system,ranks,last,other system ranks last,0.7965388894081116
translation,294,187,results,first,according to,comet ( promt.smartnd.v2 ),first according to comet ( promt.smartnd.v2 ),0.6949361562728882
translation,294,187,results,first,ranks,first,first ranks first,0.8011594414710999
translation,294,187,results,first,also,last,first also last,0.6178465485572815
translation,294,187,results,first,according to,1 - term score,first according to 1 - term score,0.6629682779312134
translation,294,187,results,last,according to,exact-match accuracy,last according to exact-match accuracy,0.6244462728500366
translation,294,187,results,results,has,other system,results has other system,0.6099205613136292
translation,294,193,results,terminology constrains,leads to,improvement,terminology constrains leads to improvement,0.709548830986023
translation,294,193,results,effect,would be,more substantial,effect would be more substantial,0.7229430675506592
translation,294,193,results,more substantial,if,training corpora,more substantial if training corpora,0.5815149545669556
translation,294,193,results,filtered,to exclude,sentences,filtered to exclude sentences,0.6999623775482178
translation,294,193,results,sentences,with,terms,sentences with terms,0.5648016929626465
translation,294,193,results,results,show,terminology constrains,results show terminology constrains,0.6091373562812805
translation,294,193,results,results,using,terminology constrains,results using terminology constrains,0.6509256362915039
translation,295,186,ablation-analysis,mto with weight,decreases,0.73 and 1.09 bleu scores,mto with weight decreases 0.73 and 1.09 bleu scores,0.6622957587242126
translation,295,186,ablation-analysis,0.73 and 1.09 bleu scores,on,validation set and test set,0.73 and 1.09 bleu scores on validation set and test set,0.5387137532234192
translation,295,186,ablation-analysis,ablation analysis,Compared with,mto with weight,ablation analysis Compared with mto with weight,0.6971962451934814
translation,295,214,baselines,language models,with,translation models,language models with translation models,0.5606253743171692
translation,295,214,baselines,translation models,for,data selection,translation models for data selection,0.5803898572921753
translation,295,214,baselines,data selection,during,training,data selection during training,0.7220062017440796
translation,295,113,experimental-setup,number of merge operations,in,byte pair encoding ( bpe ),number of merge operations in byte pair encoding ( bpe ),0.5121118426322937
translation,295,113,experimental-setup,number of merge operations,set to,32 k,number of merge operations set to 32 k,0.6968087553977966
translation,295,113,experimental-setup,32 k,for,source and target languages,32 k for source and target languages,0.60077303647995
translation,295,113,experimental-setup,zh?en,has,number of merge operations,zh?en has number of merge operations,0.6038536429405212
translation,295,113,experimental-setup,experimental setup,For,zh?en,experimental setup For zh?en,0.6451399326324463
translation,295,120,experimental-setup,hidden size,to,512,hidden size to 512,0.6113746166229248
translation,295,120,experimental-setup,encoder / decoder layers,to,6,encoder / decoder layers to 6,0.5847800374031067
translation,295,120,experimental-setup,experimental setup,set,hidden size,experimental setup set hidden size,0.6804612278938293
translation,295,120,experimental-setup,experimental setup,set,encoder / decoder layers,experimental setup set encoder / decoder layers,0.6149283051490784
translation,295,121,experimental-setup,tasks,trained with,8 nvidia v100 gpus,tasks trained with 8 nvidia v100 gpus,0.6931643486022949
translation,295,121,experimental-setup,experimental setup,has,tasks,experimental setup has tasks,0.5003909468650818
translation,295,122,experimental-setup,beam size,is,5,beam size is 5,0.657706618309021
translation,295,122,experimental-setup,length penalty,is,0.6,length penalty is 0.6,0.5568951964378357
translation,295,122,experimental-setup,experimental setup,has,beam size,experimental setup has beam size,0.46948209404945374
translation,295,122,experimental-setup,experimental setup,has,length penalty,experimental setup has length penalty,0.4614953398704529
translation,295,123,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",used in,all the models,"adam optimizer ( kingma and ba , 2014 ) used in all the models",0.6423674821853638
translation,295,123,experimental-setup,experimental setup,has,"adam optimizer ( kingma and ba , 2014 )","experimental setup has adam optimizer ( kingma and ba , 2014 )",0.535233736038208
translation,295,125,experimental-setup,number of training steps,is,150k,number of training steps is 150k,0.589794397354126
translation,295,125,experimental-setup,number of training steps,is,150 k,number of training steps is 150 k,0.6071942448616028
translation,295,125,experimental-setup,150k,for,jointly pretraining stage,150k for jointly pretraining stage,0.6284574270248413
translation,295,125,experimental-setup,150 k,for,finetuning,150 k for finetuning,0.6665137410163879
translation,295,125,experimental-setup,"en?de , zh?en , and en?fr",has,number of training steps,"en?de , zh?en , and en?fr has number of training steps",0.5434722900390625
translation,295,125,experimental-setup,experimental setup,For,"en?de , zh?en , and en?fr","experimental setup For en?de , zh?en , and en?fr",0.6393094062805176
translation,295,128,experimental-setup,threshold k,set to,30 %,threshold k set to 30 %,0.770234227180481
translation,295,128,experimental-setup,threshold k,set to,40 %,threshold k set to 40 %,0.7637792825698853
translation,295,128,experimental-setup,30 %,for,en?de and zh?en,30 % for en?de and zh?en,0.7349486947059631
translation,295,128,experimental-setup,40 %,for,en?fr,40 % for en?fr,0.6970908641815186
translation,295,128,experimental-setup,mso,has,threshold k,mso has threshold k,0.6026707291603088
translation,295,128,experimental-setup,experimental setup,For,mso,experimental setup For mso,0.61668461561203
translation,295,119,experiments,our approaches,with,opensource tooklit opennmt- py,our approaches with opensource tooklit opennmt- py,0.6806554794311523
translation,295,10,model,margin- based sentencelevel objective ( mso ),to maximize,margin,margin- based sentencelevel objective ( mso ) to maximize margin,0.6819939613342285
translation,295,10,model,margin,for preventing,lm from being overconfident,margin for preventing lm from being overconfident,0.6714463233947754
translation,295,10,model,model,propose,margin- based token - level objective ( mto ),model propose margin- based token - level objective ( mto ),0.6393219828605652
translation,295,10,model,model,propose,margin- based sentencelevel objective ( mso ),model propose margin- based sentencelevel objective ( mso ),0.6264590620994568
translation,295,28,model,margin- based sentence - level objective ( mso ),by adding,dynamic weight function,margin- based sentence - level objective ( mso ) by adding dynamic weight function,0.7286801934242249
translation,295,28,model,dynamic weight function,to alleviate,negative effect,dynamic weight function to alleviate negative effect,0.6940524578094482
translation,295,28,model,negative effect,of,dirty data,negative effect of dirty data,0.6008076667785645
translation,295,28,model,model,propose,margin- based sentence - level objective ( mso ),model propose margin- based sentence - level objective ( mso ),0.620974063873291
translation,295,124,model,lm architecture,is,pre-softmax linear transformation,lm architecture is pre-softmax linear transformation,0.5494643449783325
translation,295,124,model,lm architecture,decoder of,transformer,lm architecture decoder of transformer,0.7816867828369141
translation,295,124,model,lm architecture,sharing,embedding layer,lm architecture sharing embedding layer,0.6010487675666809
translation,295,124,model,lm architecture,sharing,pre-softmax linear transformation,lm architecture sharing pre-softmax linear transformation,0.6525167226791382
translation,295,124,model,transformer,excluding,cross-attention layers,transformer excluding cross-attention layers,0.67951500415802
translation,295,124,model,pre-softmax linear transformation,with,nmt model,pre-softmax linear transformation with nmt model,0.6260204911231995
translation,295,124,model,model,has,lm architecture,model has lm architecture,0.5766128301620483
translation,295,142,results,results,on,wmt14 english - to - french ( en? fr ),results on wmt14 english - to - french ( en? fr ),0.48944711685180664
translation,295,142,results,results,on,wmt19 chinese-to- english ( zh?en ),results on wmt19 chinese-to- english ( zh?en ),0.4938828945159912
translation,295,144,results,en?fr,has,our reimplemented result,en?fr has our reimplemented result,0.6812049746513367
translation,295,144,results,results,On,en?fr,results On en?fr,0.7138860821723938
translation,295,146,results,nmt + lm,yields,+ 0.07 and + 0.15 bleu,nmt + lm yields + 0.07 and + 0.15 bleu,0.7369163632392883
translation,295,146,results,improvements,on,en?fr and zh?en,improvements on en?fr and zh?en,0.6325318217277527
translation,295,146,results,baseline,has,nmt + lm,baseline has nmt + lm,0.5694509744644165
translation,295,146,results,+ 0.07 and + 0.15 bleu,has,improvements,+ 0.07 and + 0.15 bleu has improvements,0.5811504125595093
translation,295,146,results,results,Compared with,baseline,results Compared with baseline,0.695019543170929
translation,295,149,results,our mso,further yields,+ 0.14 and + 0.31 bleu improvements,our mso further yields + 0.14 and + 0.31 bleu improvements,0.5121480822563171
translation,295,149,results,mto,has,our mso,mto has our mso,0.6570559740066528
translation,295,149,results,results,based on,mto,results based on mto,0.7301239967346191
translation,295,150,results,improve,up to,+ 0.63 and + 1.50 bleu scores,improve up to + 0.63 and + 1.50 bleu scores,0.6185445785522461
translation,295,150,results,+ 0.63 and + 1.50 bleu scores,on,en?fr and zh? en,+ 0.63 and + 1.50 bleu scores on en?fr and zh? en,0.5549272298812866
translation,295,150,results,en?fr and zh? en,compared with,baselines,en?fr and zh? en compared with baselines,0.7296118140220642
translation,295,150,results,our approaches,has,improve,our approaches has improve,0.6158642768859863
translation,295,150,results,results,has,our approaches,results has our approaches,0.5885946750640869
translation,295,159,results,"nmt + lm , mto and mso",improve,adequacy,"nmt + lm , mto and mso improve adequacy",0.6546914577484131
translation,295,159,results,adequacy,with,"0.08 , 0.22 , and 0.37 scores","adequacy with 0.08 , 0.22 , and 0.37 scores",0.6574695110321045
translation,295,159,results,nmt baseline,has,"nmt + lm , mto and mso","nmt baseline has nmt + lm , mto and mso",0.5809441208839417
translation,295,159,results,results,Compared with,nmt baseline,results Compared with nmt baseline,0.6559051871299744
translation,295,160,results,most improvements,come from,our margin- based methods mto and mso,most improvements come from our margin- based methods mto and mso,0.617953360080719
translation,295,160,results,mso,performs,best,mso performs best,0.6522308588027954
translation,295,160,results,results,has,most improvements,results has most improvements,0.5330533981323242
translation,295,161,results,nmt +lm,achieves,0.2 improvement,nmt +lm achieves 0.2 improvement,0.6532328724861145
translation,295,161,results,0.2 improvement,compared with,nmt,0.2 improvement compared with nmt,0.6965597867965698
translation,295,161,results,fluency,has,nmt +lm,fluency has nmt +lm,0.6468732953071594
translation,295,161,results,results,For,fluency,results For fluency,0.5258268713951111
translation,295,162,results,mto and mso,yield,further improvements,mto and mso yield further improvements,0.7322484254837036
translation,295,162,results,further improvements,with,0.01 and 0.05 scores,further improvements with 0.01 and 0.05 scores,0.6705778241157532
translation,295,162,results,nmt +lm,has,mto and mso,nmt +lm has mto and mso,0.6317206025123596
translation,295,162,results,results,on,nmt +lm,results on nmt +lm,0.5314143300056458
translation,295,163,results,human evaluation,indicates,our mto and mso approaches,human evaluation indicates our mto and mso approaches,0.5920620560646057
translation,295,163,results,our mto and mso approaches,has,remarkably improve,our mto and mso approaches has remarkably improve,0.5621725916862488
translation,295,163,results,remarkably improve,has,translation adequacy,remarkably improve has translation adequacy,0.496393620967865
translation,295,163,results,slightly enhance,has,translation fluency,slightly enhance has translation fluency,0.5089753270149231
translation,295,163,results,results,has,human evaluation,results has human evaluation,0.5143810510635376
translation,295,178,results,all the four variations,bring,improvements,all the four variations bring improvements,0.6267105937004089
translation,295,178,results,improvements,over,nmt,improvements over nmt,0.6998006105422974
translation,295,178,results,improvements,over,nmt +lm,improvements over nmt +lm,0.6818805932998657
translation,295,178,results,results,has,all the four variations,results has all the four variations,0.5514050722122192
translation,296,111,ablation-analysis,ensemble,leads to,0.5 bleu increase,ensemble leads to 0.5 bleu increase,0.6344268918037415
translation,296,111,ablation-analysis,ablation analysis,has,ensemble,ablation analysis has ensemble,0.5272953510284424
translation,296,82,baselines,ensemble knowledge distillation ( ekd,has,ensemble knowledge distillation,ensemble knowledge distillation ( ekd has ensemble knowledge distillation,0.5794184803962708
translation,296,82,baselines,baselines,has,ensemble knowledge distillation ( ekd,baselines has ensemble knowledge distillation ( ekd,0.5733758807182312
translation,296,16,experimental-setup,each language pair,perform,multi-step data cleaning,each language pair perform multi-step data cleaning,0.6156058311462402
translation,296,16,experimental-setup,each language pair,keep,high-quality subset,each language pair keep high-quality subset,0.6197555065155029
translation,296,16,experimental-setup,multi-step data cleaning,on,provided dataset,multi-step data cleaning on provided dataset,0.5510594248771667
translation,296,16,experimental-setup,experimental setup,For,each language pair,experimental setup For each language pair,0.584132730960846
translation,296,40,experimental-setup,temperature sampling strategy,with,t=5,temperature sampling strategy with t=5,0.6908465623855591
translation,296,40,experimental-setup,t=5,to over-sample,low-resource language pairs,t=5 to over-sample low-resource language pairs,0.7675032019615173
translation,296,41,experimental-setup,all 30 directions,under,constrained condition,all 30 directions under constrained condition,0.6510758399963379
translation,296,41,experimental-setup,experimental setup,train,all 30 directions,experimental setup train all 30 directions,0.6671078205108643
translation,296,48,experimental-setup,batch size,of,4096,batch size of 4096,0.6386147141456604
translation,296,49,experimental-setup,adam optimizer,with,? 1 = 0.9 and ? 2 = 0.98,adam optimizer with ? 1 = 0.9 and ? 2 = 0.98,0.6311911940574646
translation,296,49,experimental-setup,learning rate,with,"4,000 warmup steps","learning rate with 4,000 warmup steps",0.639680802822113
translation,296,49,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,296,50,experimental-setup,training,employ,label smoothing,training employ label smoothing,0.5876585841178894
translation,296,50,experimental-setup,label smoothing,value of,0.1,label smoothing value of 0.1,0.6601623296737671
translation,296,50,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,296,51,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,296,51,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,296,51,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,296,51,experimental-setup,length penalty,? =,0.6,length penalty ? = 0.6,0.6168525218963623
translation,296,64,experimental-setup,"hybrid sentencepiece model ( kudo and richardson , 2018 )",in conjunction with,all 6 languages,"hybrid sentencepiece model ( kudo and richardson , 2018 ) in conjunction with all 6 languages",0.662320613861084
translation,296,64,experimental-setup,all 6 languages,as,shared word segmentation system,all 6 languages as shared word segmentation system,0.5245352983474731
translation,296,64,experimental-setup,shared word segmentation system,for,all language pairs,shared word segmentation system for all language pairs,0.6070140600204468
translation,296,64,experimental-setup,experimental setup,train,"hybrid sentencepiece model ( kudo and richardson , 2018 )","experimental setup train hybrid sentencepiece model ( kudo and richardson , 2018 )",0.6584610939025879
translation,296,7,experiments,single multilingual model,to,translate,single multilingual model to translate,0.5913153886795044
translation,296,7,experiments,translate,has,all the 30 directions,translate has all the 30 directions,0.5793179869651794
translation,296,6,results,transformer architecture,obtain,best performance,transformer architecture obtain best performance,0.6007760167121887
translation,296,6,results,best performance,via,multiple variants,best performance via multiple variants,0.6861577033996582
translation,296,6,results,multiple variants,with,larger parameter sizes,multiple variants with larger parameter sizes,0.6367418766021729
translation,296,6,results,results,use,transformer architecture,results use transformer architecture,0.6380276083946228
translation,296,108,results,first round ftst multi ?xx models,leads to,1.3 bleu,first round ftst multi ?xx models leads to 1.3 bleu,0.6476951241493225
translation,296,108,results,increase,for,30 directions,increase for 30 directions,0.6314007639884949
translation,296,108,results,baseline model,has,first round ftst multi ?xx models,baseline model has first round ftst multi ?xx models,0.5688725113868713
translation,296,108,results,1.3 bleu,has,increase,1.3 bleu has increase,0.5531812906265259
translation,296,108,results,results,Comparing with,baseline model,results Comparing with baseline model,0.6776164174079895
translation,296,109,results,second round ftst,achieves,1.2 bleu increase,second round ftst achieves 1.2 bleu increase,0.6500945091247559
translation,296,109,results,results,has,second round ftst,results has second round ftst,0.5762296319007874
translation,296,110,results,model,using,bilingual data,model using bilingual data,0.6819199919700623
translation,296,110,results,model,achieve,0.8 bleu increase,model achieve 0.8 bleu increase,0.6389057636260986
translation,296,110,results,bilingual data,with,adapter,bilingual data with adapter,0.6463907361030579
translation,296,110,results,results,fine- tune,model,results fine- tune model,0.6657537817955017
translation,296,114,results,final model,submitted,28.34 bleu,final model submitted 28.34 bleu,0.49127328395843506
translation,296,114,results,final model,achieves,28.64 bleu,final model achieves 28.64 bleu,0.6305190324783325
translation,296,114,results,final model,achieves,28.34 bleu,final model achieves 28.34 bleu,0.6332419514656067
translation,296,114,results,28.64 bleu,on,flores dev,28.64 bleu on flores dev,0.5958821177482605
translation,296,114,results,28.34 bleu,on,flores devtest,28.34 bleu on flores devtest,0.5562421679496765
translation,296,114,results,results,has,final model,results has final model,0.5473960638046265
translation,296,120,results,multi?en model,surpass,multi ? multi model,multi?en model surpass multi ? multi model,0.7608172297477722
translation,296,120,results,multi ? multi model,in,quality,multi ? multi model in quality,0.5591766834259033
translation,296,120,results,results,found that,multi?en model,results found that multi?en model,0.7253655195236206
translation,297,5,experiments,domain classification,for computing,sentence weights,domain classification for computing sentence weights,0.6701304912567139
translation,297,14,model,cnns and recursive neural tensor networks ( rntns ),to compute,domain scores,cnns and recursive neural tensor networks ( rntns ) to compute domain scores,0.7683950662612915
translation,297,14,model,domain scores,for,sentence weighting,domain scores for sentence weighting,0.5766051411628723
translation,297,14,model,sentence weighting,in,nmt,sentence weighting in nmt,0.5238586068153381
translation,297,14,model,model,apply,cnns and recursive neural tensor networks ( rntns ),model apply cnns and recursive neural tensor networks ( rntns ),0.6549587249755859
translation,297,28,results,transformed cnn scores,as,weights,transformed cnn scores as weights,0.5337040424346924
translation,297,28,results,weights,during,nmt training,weights during nmt training,0.6434677243232727
translation,297,28,results,better,than,cross-entropy based classifier,better than cross-entropy based classifier,0.5793737173080444
translation,297,213,results,sigmoid cnn scores,performed,best,sigmoid cnn scores performed best,0.27894213795661926
translation,297,213,results,results,has,sigmoid cnn scores,results has sigmoid cnn scores,0.5727117657661438
translation,298,195,ablation-analysis,qe systems,need,f1 score,qe systems need f1 score,0.6130190491676331
translation,298,195,ablation-analysis,f1 score,of,at least 80 %,f1 score of at least 80 %,0.539155900478363
translation,298,195,ablation-analysis,at least 80 %,to support,pe,at least 80 % to support pe,0.5981463193893433
translation,298,195,ablation-analysis,pe,in terms of,subjective helpfulness,pe in terms of subjective helpfulness,0.6489939093589783
translation,298,195,ablation-analysis,pe,in terms of,editing duration,pe in terms of editing duration,0.6823987364768982
translation,298,195,ablation-analysis,pe,in terms of,quality,pe in terms of quality,0.7390782237052917
translation,298,195,ablation-analysis,quality,of,final translations,quality of final translations,0.5778331756591797
translation,298,195,ablation-analysis,ablation analysis,agree,qe systems,ablation analysis agree qe systems,0.7002665400505066
translation,298,15,results,post-editing ( pe ),saves,time,post-editing ( pe ) saves time,0.6002667546272278
translation,298,15,results,post-editing ( pe ),reduces,errors,post-editing ( pe ) reduces errors,0.6351209282875061
translation,298,15,results,results,has,post-editing ( pe ),results has post-editing ( pe ),0.5485870242118835
translation,298,194,results,existing state - of - the - art wordlevel qe systems,are,not yet good enough,existing state - of - the - art wordlevel qe systems are not yet good enough,0.5473311543464661
translation,298,194,results,not yet good enough,to be helpful during,pe,not yet good enough to be helpful during pe,0.7663577198982239
translation,298,194,results,results,show,existing state - of - the - art wordlevel qe systems,results show existing state - of - the - art wordlevel qe systems,0.5760660171508789
translation,298,201,results,translation speed gains,obtained by,qe,translation speed gains obtained by qe,0.6421734690666199
translation,298,201,results,translation speed gains,obtained by,qe system,translation speed gains obtained by qe system,0.6170094013214111
translation,298,201,results,translation speed gains,help producing,higher quality translations,translation speed gains help producing higher quality translations,0.6791685819625854
translation,299,68,hyperparameters,"electra - base ( clark et al. , 2020 ) s",as,pre-trained monolingual lms,"electra - base ( clark et al. , 2020 ) s as pre-trained monolingual lms",0.48114272952079773
translation,299,68,hyperparameters,pre-trained monolingual lms,for,dual monolingual encoders,pre-trained monolingual lms for dual monolingual encoders,0.6108266115188599
translation,299,68,hyperparameters,hyperparameters,used,"electra - base ( clark et al. , 2020 ) s","hyperparameters used electra - base ( clark et al. , 2020 ) s",0.5846417546272278
translation,299,69,hyperparameters,qe pre-training,used,get_schedule_with _warmup,qe pre-training used get_schedule_with _warmup,0.6022777557373047
translation,299,69,hyperparameters,get_schedule_with _warmup,as,learning rate scheduler,get_schedule_with _warmup as learning rate scheduler,0.572692334651947
translation,299,69,hyperparameters,learning rate scheduler,with,"3,000 warm - up steps","learning rate scheduler with 3,000 warm - up steps",0.622697114944458
translation,299,69,hyperparameters,hyperparameters,In,qe pre-training,hyperparameters In qe pre-training,0.4620744585990906
translation,299,70,hyperparameters,"adamw ( loshchilov and hutter , 2018 ) optimizer",has,weight decay,"adamw ( loshchilov and hutter , 2018 ) optimizer has weight decay",0.505995512008667
translation,299,70,hyperparameters,weight decay,with,"?=0.5 , ? 1 =0.9 , ? 2 =0.999 , and = 1e - 8","weight decay with ?=0.5 , ? 1 =0.9 , ? 2 =0.999 , and = 1e - 8",0.6408722400665283
translation,299,70,hyperparameters,weight decay,with,gradient clipping,weight decay with gradient clipping,0.5931796431541443
translation,299,70,hyperparameters,hyperparameters,used,"adamw ( loshchilov and hutter , 2018 ) optimizer","hyperparameters used adamw ( loshchilov and hutter , 2018 ) optimizer",0.5678926110267639
translation,299,71,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,299,71,hyperparameters,64,for,qe pre-training and fine-tuning,64 for qe pre-training and fine-tuning,0.5644773840904236
translation,299,71,hyperparameters,64,for,fine-tuning,64 for fine-tuning,0.608843982219696
translation,299,71,hyperparameters,64,set,learning rate,64 set learning rate,0.667296826839447
translation,299,71,hyperparameters,64,set,"tuple of ( k s = 1 , k m = 1 , k g = 3 )","64 set tuple of ( k s = 1 , k m = 1 , k g = 3 )",0.6724515557289124
translation,299,71,hyperparameters,64,for,fine-tuning,64 for fine-tuning,0.608843982219696
translation,299,71,hyperparameters,learning rate,of,1e?5,learning rate of 1e?5,0.6333191394805908
translation,299,71,hyperparameters,learning rate,of,5e?5,learning rate of 5e?5,0.6546747088432312
translation,299,71,hyperparameters,"tuple of ( k s = 1 , k m = 1 , k g = 3 )",for,qe pre-training,"tuple of ( k s = 1 , k m = 1 , k g = 3 ) for qe pre-training",0.6340395212173462
translation,299,71,hyperparameters,tuple,for,fine-tuning,tuple for fine-tuning,0.6639778017997742
translation,299,71,hyperparameters,"s = 2 , k m = 2 , k g = 4 )",for,fine-tuning,"s = 2 , k m = 2 , k g = 4 ) for fine-tuning",0.6429852247238159
translation,299,71,hyperparameters,hyperparameters,Setting,batch size,hyperparameters Setting batch size,0.4602150022983551
translation,299,71,hyperparameters,hyperparameters,Setting,learning rate,hyperparameters Setting learning rate,0.4372222423553467
translation,299,71,hyperparameters,hyperparameters,Setting,tuple,hyperparameters Setting tuple,0.4684305191040039
translation,299,71,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,299,71,hyperparameters,hyperparameters,set,"tuple of ( k s = 1 , k m = 1 , k g = 3 )","hyperparameters set tuple of ( k s = 1 , k m = 1 , k g = 3 )",0.6478440165519714
translation,299,71,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,299,71,hyperparameters,hyperparameters,set,tuple,hyperparameters set tuple,0.6854996085166931
translation,299,5,model,stability,of,quality estimation models,stability of quality estimation models,0.5735558271408081
translation,299,5,model,model,improve,stability,model improve stability,0.6899270415306091
translation,299,6,model,output information,through,two additional cross attention networks,output information through two additional cross attention networks,0.6307867169380188
translation,299,6,model,model,first uses,two pre-trained monolingual encoders,model first uses two pre-trained monolingual encoders,0.6090145707130432
translation,299,16,model,each encoder,add,cross attention network,each encoder add cross attention network,0.6237552165985107
translation,299,16,model,cross attention network,for,learning,cross attention network for learning,0.6054262518882751
translation,299,16,model,learning,of,cross-lingual context,learning of cross-lingual context,0.492491751909256
translation,299,16,model,cross-lingual context,between,src and mt,cross-lingual context between src and mt,0.6062712073326111
translation,299,16,model,representations,for,qe,representations for qe,0.7284975051879883
translation,299,16,model,model,On top of,each encoder,model On top of each encoder,0.6756861805915833
translation,299,74,results,our ensembles,report,improved pcc,our ensembles report improved pcc,0.6409005522727966
translation,299,74,results,our ensembles,report,mt-side words mcc,our ensembles report mt-side words mcc,0.6323627829551697
translation,299,74,results,our ensembles,report,mt-side < gap >s mcc,our ensembles report mt-side < gap >s mcc,0.6596488356590271
translation,299,74,results,mt-side < gap >s mcc,of about,"0.5497 , 0.4296 , and 0.1225","mt-side < gap >s mcc of about 0.5497 , 0.4296 , and 0.1225",0.5882378220558167
translation,299,74,results,system,has,our ensembles,system has our ensembles,0.6220789551734924
translation,299,75,results,outperform,in terms of,sentence - level pcc,outperform in terms of sentence - level pcc,0.6537179350852966
translation,299,75,results,outperform,in terms of,mt-side words mcc,outperform in terms of mt-side words mcc,0.7040992975234985
translation,299,75,results,our systems,has,outperform,our systems has outperform,0.6241556406021118
translation,299,75,results,outperform,has,baseline systems,outperform has baseline systems,0.5964462757110596
translation,299,76,results,our systems,inferior to,baseline systems,our systems inferior to baseline systems,0.6054390668869019
translation,299,76,results,mt-side < gap >s mcc,by,narrow margin,mt-side < gap >s mcc by narrow margin,0.6392627954483032
translation,299,76,results,results,has,our systems,results has our systems,0.5982377529144287
translation,300,97,ablation-analysis,3.5x higher performance,at,encoder layers,3.5x higher performance at encoder layers,0.5221667885780334
translation,300,97,ablation-analysis,learned,to be very aware of,input domains,learned to be very aware of input domains,0.569113552570343
translation,300,97,ablation-analysis,ablation analysis,About,3.5x higher performance,ablation analysis About 3.5x higher performance,0.5882905721664429
translation,300,60,experiments,models,on,parallel data,models on parallel data,0.5492456555366516
translation,300,60,experiments,parallel data,covering,four corpora / text domains,parallel data covering four corpora / text domains,0.7060038447380066
translation,300,60,experiments,legal,has,"jrc - acquis , steinberger et al. , 2006 )","legal has jrc - acquis , steinberger et al. , 2006 )",0.5921269059181213
translation,300,192,experiments,document - level xlm -r automatic domains,have,low average score,document - level xlm -r automatic domains have low average score,0.5180628895759583
translation,300,192,experiments,low average score,due to,underperforming,low average score due to underperforming,0.6386316418647766
translation,300,192,experiments,underperforming,on,emea test set,underperforming on emea test set,0.5600205063819885
translation,300,59,hyperparameters,hyperparameters,train,"transformer - base ( vaswani et al. , 2017a ) nmt models","hyperparameters train transformer - base ( vaswani et al. , 2017a ) nmt models",0.6423702239990234
translation,300,62,hyperparameters,nmt models,trained for,60 epochs,nmt models trained for 60 epochs,0.7610111832618713
translation,300,62,hyperparameters,hyperparameters,has,nmt models,hyperparameters has nmt models,0.5213472843170166
translation,300,172,hyperparameters,baseline fine- tuning,performed for,50 epochs,baseline fine- tuning performed for 50 epochs,0.5806636214256287
translation,300,172,hyperparameters,baseline fine- tuning,in,single heterogenous corpus experiments,baseline fine- tuning in single heterogenous corpus experiments,0.48527225852012634
translation,300,172,hyperparameters,single heterogenous corpus experiments,for,25 epochs,single heterogenous corpus experiments for 25 epochs,0.5684032440185547
translation,300,172,hyperparameters,mixture of corpora experiments,has,baseline fine- tuning,mixture of corpora experiments has baseline fine- tuning,0.5625408291816711
translation,300,172,hyperparameters,hyperparameters,In,mixture of corpora experiments,hyperparameters In mixture of corpora experiments,0.5080697536468506
translation,300,23,results,document- level clustering,result of,k-means,document- level clustering result of k-means,0.6784942746162415
translation,300,23,results,k-means,matches,original corpora,k-means matches original corpora,0.765125572681427
translation,300,23,results,original corpora,has,almost perfectly,original corpora has almost perfectly,0.5705519914627075
translation,300,23,results,results,In the case of,document- level clustering,results In the case of document- level clustering,0.6726903915405273
translation,300,96,results,nmt,surpasses,language model,nmt surpasses language model,0.6192691922187805
translation,300,96,results,language model,in,ability to rediscover,language model in ability to rediscover,0.5140591263771057
translation,300,96,results,ability to rediscover,has,domains,ability to rediscover has domains,0.6067836880683899
translation,300,96,results,results,shows,nmt,results shows nmt,0.6447077393531799
translation,300,183,results,all corpora,except,europarl,all corpora except europarl,0.6715953350067139
translation,300,183,results,all corpora,except,europarl,all corpora except europarl,0.6715953350067139
translation,300,183,results,at least one model of the two,based on,document - level clustering,at least one model of the two based on document - level clustering,0.668173611164093
translation,300,183,results,at least one model of the two,manages to surpass,oracle performance,at least one model of the two manages to surpass oracle performance,0.7208799719810486
translation,300,183,results,oracle performance,obtained by,fine-tuning,oracle performance obtained by fine-tuning,0.6329907774925232
translation,300,183,results,fine-tuning,to,known domains,fine-tuning to known domains,0.5698254704475403
translation,300,183,results,document - level models,perform,comparably,document - level models perform comparably,0.5816799402236938
translation,300,183,results,comparably,to,oracle,comparably to oracle,0.5826736688613892
translation,300,183,results,all corpora,has,at least one model of the two,all corpora has at least one model of the two,0.6186752319335938
translation,300,183,results,all corpora,has,document - level models,all corpora has document - level models,0.5450496673583984
translation,300,183,results,europarl,has,at least one model of the two,europarl has at least one model of the two,0.6122546195983887
translation,300,183,results,europarl,has,document - level models,europarl has document - level models,0.5943164229393005
translation,300,183,results,europarl,has,document - level models,europarl has document - level models,0.5943164229393005
translation,300,183,results,results,see that,all corpora,results see that all corpora,0.5857969522476196
translation,300,183,results,results,for,all corpora,results for all corpora,0.5587616562843323
translation,300,184,results,document- level models,show,significantly better translation quality,document- level models show significantly better translation quality,0.6135335564613342
translation,300,184,results,significantly better translation quality,than,xlm -r sentence - level models,significantly better translation quality than xlm -r sentence - level models,0.5329372882843018
translation,300,184,results,significantly better translation quality,than,xlm,significantly better translation quality than xlm,0.607187032699585
translation,300,184,results,results,has,document- level models,results has document- level models,0.5007006525993347
translation,300,185,results,scores,averaged over,all four domains,scores averaged over all four domains,0.6462303996086121
translation,300,185,results,document clustering,obtained from,nmt encoder,document clustering obtained from nmt encoder,0.5603302717208862
translation,300,185,results,nmt encoder,is,overall winner,nmt encoder is overall winner,0.5610244870185852
translation,300,185,results,scores,has,document clustering,scores has document clustering,0.5722772479057312
translation,300,185,results,all four domains,has,document clustering,all four domains has document clustering,0.605741024017334
translation,300,186,results,results,for,en ?et language pair,results for en ?et language pair,0.6478239893913269
translation,300,187,results,fine-tuning,on,oracle domains,fine-tuning on oracle domains,0.5149737596511841
translation,300,187,results,fine-tuning,yields,average improvement,fine-tuning yields average improvement,0.7023935914039612
translation,300,187,results,oracle domains,yields,average improvement,oracle domains yields average improvement,0.6734417080879211
translation,300,187,results,average improvement,of,0.8 bleu points,average improvement of 0.8 bleu points,0.5373108983039856
translation,300,187,results,0.8 bleu points,over,baseline,0.8 bleu points over baseline,0.6468991041183472
translation,300,187,results,fine -tuning,on,unsupervised document clusters,fine -tuning on unsupervised document clusters,0.4580152928829193
translation,300,187,results,unsupervised document clusters,obtained from,nmt encoder,unsupervised document clusters obtained from nmt encoder,0.5660439133644104
translation,300,188,results,xlm -r sentence clusters,turn out to be,most successful approach,xlm -r sentence clusters turn out to be most successful approach,0.6079561710357666
translation,300,188,results,most successful approach,showing,significantly higher bleu scores,most successful approach showing significantly higher bleu scores,0.6397985816001892
translation,300,188,results,most successful approach,showing,outperforming,most successful approach showing outperforming,0.7283036112785339
translation,300,188,results,significantly higher bleu scores,than,all other automatic partitions,significantly higher bleu scores than all other automatic partitions,0.5645459294319153
translation,300,188,results,oracle,by,1.2 bleu points,oracle by 1.2 bleu points,0.5529268383979797
translation,300,188,results,document- level nmt clustering,manages to surpass,oracle performance,document- level nmt clustering manages to surpass oracle performance,0.6932859420776367
translation,300,188,results,emea test set,has,xlm -r sentence clusters,emea test set has xlm -r sentence clusters,0.5821483135223389
translation,300,188,results,outperforming,has,oracle,outperforming has oracle,0.6429049372673035
translation,300,188,results,results,for,emea test set,results for emea test set,0.6103003621101379
translation,300,189,results,oracle,shows,highest overall scores,oracle shows highest overall scores,0.685941755771637
translation,300,189,results,highest overall scores,with,document- level nmt clustering,highest overall scores with document- level nmt clustering,0.6289615035057068
translation,300,189,results,xlm -r sentence clustering,by,noticeable margin,xlm -r sentence clustering by noticeable margin,0.5878562927246094
translation,300,189,results,opensubtitles and jrc - acquis,has,oracle,opensubtitles and jrc - acquis has oracle,0.635198712348938
translation,300,189,results,document- level nmt clustering,has,close second,document- level nmt clustering has close second,0.5687054395675659
translation,300,189,results,outperforming,has,xlm -r sentence clustering,outperforming has xlm -r sentence clustering,0.5619183778762817
translation,300,189,results,results,For,opensubtitles and jrc - acquis,results For opensubtitles and jrc - acquis,0.6370327472686768
translation,300,190,results,opensubtitles,none of,automatic domain approaches,opensubtitles none of automatic domain approaches,0.511893093585968
translation,300,190,results,automatic domain approaches,manage to improve,baseline performance,automatic domain approaches manage to improve baseline performance,0.6370260715484619
translation,300,190,results,results,For,opensubtitles,results For opensubtitles,0.6415197849273682
translation,300,191,results,all automatic domain approaches,yield,comparable bleu scores,all automatic domain approaches yield comparable bleu scores,0.6717435717582703
translation,300,191,results,europarl,has,all automatic domain approaches,europarl has all automatic domain approaches,0.6311826109886169
translation,300,191,results,results,For,europarl,results For europarl,0.6374685168266296
translation,300,201,results,nmt sentence - level clustering,has,manages to outperform,nmt sentence - level clustering has manages to outperform,0.6180869936943054
translation,300,201,results,manages to outperform,has,baseline,manages to outperform has baseline,0.6200125217437744
translation,300,201,results,noticeably surpassing,has,all other automatic domain extraction methods,noticeably surpassing has all other automatic domain extraction methods,0.5788650512695312
translation,300,201,results,results,For,en - cs,results For en - cs,0.6415800452232361
translation,300,202,results,outperform,by,considerable margin,outperform by considerable margin,0.6616896986961365
translation,300,202,results,baseline model,by,considerable margin,baseline model by considerable margin,0.6105979681015015
translation,300,202,results,de - en,has,none of the approaches,de - en has none of the approaches,0.616607666015625
translation,300,202,results,none of the approaches,has,outperform,none of the approaches has outperform,0.6085806488990784
translation,300,202,results,outperform,has,baseline model,outperform has baseline model,0.6127602458000183
translation,300,202,results,results,For,de - en,results For de - en,0.7280613780021667
translation,300,203,results,sentence - level clustering,based on,xlm -r,sentence - level clustering based on xlm -r,0.6491307020187378
translation,300,203,results,sentence - level clustering,performs,comparably,sentence - level clustering performs comparably,0.5955811738967896
translation,300,203,results,comparably,to,baseline,comparably to baseline,0.6104291677474976
translation,300,203,results,results,has,sentence - level clustering,results has sentence - level clustering,0.5528534054756165
translation,300,204,results,document - level nmt clustering,shows,slightly lower score,document - level nmt clustering shows slightly lower score,0.6503213047981262
translation,300,204,results,results,has,document - level nmt clustering,results has document - level nmt clustering,0.5446966290473938
translation,300,205,results,document xlm -r and sentence nmt,perform,worse,document xlm -r and sentence nmt perform worse,0.6385785937309265
translation,300,205,results,worse,than,sentence xlm -r,worse than sentence xlm -r,0.5984117388725281
translation,300,205,results,results,has,document xlm -r and sentence nmt,results has document xlm -r and sentence nmt,0.5880879759788513
translation,300,212,results,any significant improvement,over,concat-cont baseline,any significant improvement over concat-cont baseline,0.7211306691169739
translation,300,212,results,concat-cont baseline,for,any of the methods,concat-cont baseline for any of the methods,0.6321746706962585
translation,300,212,results,results,observe,any significant improvement,results observe any significant improvement,0.6238667964935303
translation,300,213,results,data,separated into,12 clusters,data separated into 12 clusters,0.7114107012748718
translation,300,213,results,sentence - level nmt clustering,does not beat,continued training,sentence - level nmt clustering does not beat continued training,0.6613999009132385
translation,300,213,results,continued training,of,baseline,continued training of baseline,0.5951663851737976
translation,300,213,results,data,has,sentence - level nmt clustering,data has sentence - level nmt clustering,0.5552216172218323
translation,300,213,results,12 clusters,has,sentence - level nmt clustering,12 clusters has sentence - level nmt clustering,0.5783759951591492
translation,300,213,results,sentence - level nmt clustering,has,significantly outperforms,sentence - level nmt clustering has significantly outperforms,0.5338238477706909
translation,300,213,results,significantly outperforms,has,sentence - level xlm -r,significantly outperforms has sentence - level xlm -r,0.5876758098602295
translation,300,213,results,results,With,data,results With data,0.5543662309646606
translation,301,92,ablation-analysis,reduction,of,7 ? 22 %,reduction of 7 ? 22 %,0.5889198184013367
translation,301,92,ablation-analysis,7 ? 22 %,in,attribute f1,7 ? 22 % in attribute f1,0.5370901226997375
translation,301,92,ablation-analysis,attribute f1,by,simple back - translation,attribute f1 by simple back - translation,0.5904765725135803
translation,301,92,ablation-analysis,simple back - translation,with,chinese ( zh ),simple back - translation with chinese ( zh ),0.6245220899581909
translation,301,92,ablation-analysis,chinese ( zh ),preserving,more privacy,chinese ( zh ) preserving more privacy,0.7964248061180115
translation,301,92,ablation-analysis,chinese ( zh ),maintaining,95 %,chinese ( zh ) maintaining 95 %,0.6452595591545105
translation,301,92,ablation-analysis,95 %,of,original utility,95 % of original utility,0.5447497963905334
translation,301,92,ablation-analysis,highest score ( 81 % ),for,fluency,highest score ( 81 % ) for fluency,0.6240419745445251
translation,301,92,ablation-analysis,ablation analysis,observe,reduction,ablation analysis observe reduction,0.6358630061149597
translation,301,97,ablation-analysis,bt models,leads to,reduction,bt models leads to reduction,0.6880492568016052
translation,301,97,ablation-analysis,reduction,of,3.5 ? 9.7 %,reduction of 3.5 ? 9.7 %,0.5811249017715454
translation,301,97,ablation-analysis,reduction,maintaining,over 86 %,reduction maintaining over 86 %,0.7423918843269348
translation,301,97,ablation-analysis,3.5 ? 9.7 %,in,attribute f1,3.5 ? 9.7 % in attribute f1,0.5030555725097656
translation,301,97,ablation-analysis,over 86 %,of,original utility f1,over 86 % of original utility f1,0.556147038936615
translation,301,97,ablation-analysis,ablation analysis,has,bt models,ablation analysis has bt models,0.5279970765113831
translation,301,20,baselines,baselines,has,back - translation ( bt ),baselines has back - translation ( bt ),0.556398332118988
translation,301,57,baselines,bst,based on,latent representation disentanglement,bst based on latent representation disentanglement,0.6710265874862671
translation,301,57,baselines,latent representation disentanglement,through,adversarial training,latent representation disentanglement through adversarial training,0.6578855514526367
translation,301,57,baselines,bst,has,"prabhumoye et al. , 2018 )","bst has prabhumoye et al. , 2018 )",0.5546467304229736
translation,301,57,baselines,unmt,has,"lample et al. , 2019 )","unmt has lample et al. , 2019 )",0.609699547290802
translation,301,57,baselines,dls,has,"he et al. , 2020 )","dls has he et al. , 2020 )",0.5982482433319092
translation,301,50,experiments,6 high- resourced languages,as,pivot,6 high- resourced languages as pivot,0.5138776302337646
translation,301,50,experiments,decent quality,of,machine translation models,decent quality of machine translation models,0.5484725832939148
translation,301,93,experiments,german ( de ),has,better meteor score and utility,german ( de ) has better meteor score and utility,0.5899996161460876
translation,301,7,model,"simple , zero-shot way",to effectively lower,risk of author profiling,"simple , zero-shot way to effectively lower risk of author profiling",0.6919565200805664
translation,301,7,model,multilingual back - translation,using,off - the-shelf translation models,multilingual back - translation using off - the-shelf translation models,0.6249362826347351
translation,301,7,model,model,propose,"simple , zero-shot way","model propose simple , zero-shot way",0.6989650130271912
translation,301,95,results,style transfer methods,have,much better privacy preservation,style transfer methods have much better privacy preservation,0.4913111925125122
translation,301,95,results,much better privacy preservation,than,bt models,much better privacy preservation than bt models,0.5491576790809631
translation,301,95,results,45 ? 75 % reduction,in,attribute f1,45 ? 75 % reduction in attribute f1,0.522753894329071
translation,301,95,results,fluency,has,< 45 % gar,fluency has < 45 % gar,0.5976294875144958
translation,301,95,results,results,find,style transfer methods,results find style transfer methods,0.5233472585678101
translation,301,96,results,results,on,verbmobil dataset,results on verbmobil dataset,0.4936272203922272
translation,301,98,results,better performance,in,meteor and gar,better performance in meteor and gar,0.5504976511001587
translation,301,102,results,style transfer models,preserve,more gender privacy ( 19 ? 54 % ),style transfer models preserve more gender privacy ( 19 ? 54 % ),0.7393097281455994
translation,301,102,results,more gender privacy ( 19 ? 54 % ),than,bt models ( 5 ? 16 % ),more gender privacy ( 19 ? 54 % ) than bt models ( 5 ? 16 % ),0.5487476587295532
translation,301,102,results,results,has,style transfer models,results has style transfer models,0.4964692294597626
translation,301,104,results,bt models,is often better than,style transfer models,bt models is often better than style transfer models,0.6814368367195129
translation,301,104,results,style transfer models,for,all datasets,style transfer models for all datasets,0.5134100914001465
translation,302,75,baselines,xlm -roberta,used as,predictor,xlm -roberta used as predictor,0.7204664349555969
translation,302,75,baselines,predictor,for,feature generation,predictor for feature generation,0.5820877552032471
translation,302,75,baselines,baselines,has,baseline qe system,baselines has baseline qe system,0.605212926864624
translation,302,58,experimental-setup,gradient boosting and adaboost,use,implementation in scikit-learn,gradient boosting and adaboost use implementation in scikit-learn,0.5595282912254333
translation,302,58,experimental-setup,implementation in scikit-learn,with,10 - fold cross validation,implementation in scikit-learn with 10 - fold cross validation,0.6225661039352417
translation,302,58,experimental-setup,experimental setup,For,gradient boosting and adaboost,experimental setup For gradient boosting and adaboost,0.5616002082824707
translation,302,59,experimental-setup,settings,for,gradient boosting,settings for gradient boosting,0.5408266186714172
translation,302,59,experimental-setup,gradient boosting,are,number of estimators,gradient boosting are number of estimators,0.5369263887405396
translation,302,59,experimental-setup,gradient boosting,are,learning rate,gradient boosting are learning rate,0.503851592540741
translation,302,59,experimental-setup,gradient boosting,are,minimum number of samples 3,gradient boosting are minimum number of samples 3,0.570889413356781
translation,302,59,experimental-setup,number of estimators,has,600,number of estimators has 600,0.5728065371513367
translation,302,59,experimental-setup,learning rate,has,0.01,learning rate has 0.01,0.5422797799110413
translation,302,59,experimental-setup,experimental setup,has,settings,experimental setup has settings,0.4656887948513031
translation,302,60,experimental-setup,default settings,for,adaboost,default settings for adaboost,0.5772587060928345
translation,302,60,experimental-setup,experimental setup,use,default settings,experimental setup use default settings,0.6386933922767639
translation,302,7,model,system,is,ensemble of multilingual bert ( mbert ) - based regression models,system is ensemble of multilingual bert ( mbert ) - based regression models,0.579343855381012
translation,302,7,model,ensemble of multilingual bert ( mbert ) - based regression models,generated by,fine-tuning,ensemble of multilingual bert ( mbert ) - based regression models generated by fine-tuning,0.659937858581543
translation,302,7,model,fine-tuning,on,different input settings,fine-tuning on different input settings,0.5346648097038269
translation,302,20,model,fine-tuning capability,of,transformer - based encoder,fine-tuning capability of transformer - based encoder,0.5725670456886292
translation,302,20,model,fine-tuning capability,namely,"mbert ( devlin et al. , 2018 ) pre-trained model","fine-tuning capability namely mbert ( devlin et al. , 2018 ) pre-trained model",0.6275067329406738
translation,302,20,model,model,leverage,fine-tuning capability,model leverage fine-tuning capability,0.7755348086357117
translation,302,79,results,three input settings,seems that,mbert,three input settings seems that mbert,0.7299889922142029
translation,302,79,results,mbert,exhibits,competitive results,mbert exhibits competitive results,0.7146204113960266
translation,302,79,results,source -side text,in,m t and m t -m t settings,source -side text in m t and m t -m t settings,0.5540282726287842
translation,302,79,results,results,comparing,three input settings,results comparing three input settings,0.6422851085662842
translation,302,80,results,independent counterparts,for,all the language pairs,independent counterparts for all the language pairs,0.6289986968040466
translation,302,80,results,ensemble mbert model,has,ensbrt,ensemble mbert model has ensbrt,0.5590604543685913
translation,302,80,results,ensemble mbert model,has,outperforms,ensemble mbert model has outperforms,0.623087465763092
translation,302,80,results,ensbrt,has,outperforms,ensbrt has outperforms,0.6838694214820862
translation,302,80,results,outperforms,has,independent counterparts,outperforms has independent counterparts,0.5974702835083008
translation,302,85,results,ensbrt,demonstrates,comparable performance,ensbrt demonstrates comparable performance,0.6946606040000916
translation,302,85,results,ensbrt,outperforms it in,mae,ensbrt outperforms it in mae,0.6574488878250122
translation,302,85,results,ensbrt,outperforms it in,rmse,ensbrt outperforms it in rmse,0.7232896685600281
translation,302,85,results,comparable performance,to,baseline,comparable performance to baseline,0.5276662707328796
translation,302,85,results,comparable performance,for,pearson 's,comparable performance for pearson 's,0.6232141256332397
translation,302,85,results,baseline,for,pearson 's,baseline for pearson 's,0.5646787881851196
translation,302,85,results,rmse,for,language pairs,rmse for language pairs,0.6748132705688477
translation,302,85,results,results,has,ensbrt,results has ensbrt,0.6049898862838745
translation,303,219,ablation-analysis,beam search,of,generating stage,beam search of generating stage,0.5978155732154846
translation,303,219,ablation-analysis,beam search,has,greatly hurts,beam search has greatly hurts,0.5975905656814575
translation,303,219,ablation-analysis,generating stage,has,greatly hurts,generating stage has greatly hurts,0.5948498845100403
translation,303,219,ablation-analysis,greatly hurts,has,performance,greatly hurts has performance,0.5966358184814453
translation,303,233,ablation-analysis,ranking model,has,significantly hurts,ranking model has significantly hurts,0.5962715744972229
translation,303,233,ablation-analysis,significantly hurts,has,performance,significantly hurts has performance,0.6094658970832825
translation,303,233,ablation-analysis,ablation analysis,removing,ranking model,ablation analysis removing ranking model,0.7218108177185059
translation,303,9,experiments,factual probe,has,google - re and t-rex ),factual probe has google - re and t-rex ),0.63618004322052
translation,303,35,experiments,information extractions,as,text - to- triple problems,information extractions as text - to- triple problems,0.5549250245094299
translation,303,35,experiments,information extractions,by incorporating,task priors,information extractions by incorporating task priors,0.6901113986968994
translation,303,35,experiments,task priors,in,input text,task priors in input text,0.49206796288490295
translation,303,35,experiments,task priors,translating,input text,task priors translating input text,0.7193785905838013
translation,303,35,experiments,input text,to,triples,input text to triples,0.5304490923881531
translation,303,35,experiments,input text,as,output,input text as output,0.5616061091423035
translation,303,103,experiments,t-rex,reports,accuracy,t-rex reports accuracy,0.691447913646698
translation,303,103,experiments,accuracy,of,97.8 %,accuracy of 97.8 %,0.5595530867576599
translation,303,103,experiments,97.8 %,of,pairs,97.8 % of pairs,0.6064256429672241
translation,303,171,experiments,outperforming,has,fully supervised methods,outperforming has fully supervised methods,0.5480076670646667
translation,303,173,experiments,rnnoie,by,37.5,rnnoie by 37.5,0.5816255211830139
translation,303,173,experiments,rnnoie,by,44.6,rnnoie by 44.6,0.5780560970306396
translation,303,173,experiments,rnnoie,by,supervised oie system,rnnoie by supervised oie system,0.6370776295661926
translation,303,173,experiments,rnnoie,on average,37.5,rnnoie on average 37.5,0.6939737796783447
translation,303,173,experiments,rnnoie,on average,44.6,rnnoie on average 44.6,0.6988653540611267
translation,303,173,experiments,rnnoie,on average,supervised oie system,rnnoie on average supervised oie system,0.649507462978363
translation,303,173,experiments,37.5,in,f1,37.5 in f1,0.5704740881919861
translation,303,173,experiments,44.6,in,auc,44.6 in auc,0.5825537443161011
translation,303,173,experiments,zero-shot deepex,has,outperforms,zero-shot deepex has outperforms,0.6078876852989197
translation,303,173,experiments,outperforms,has,rnnoie,outperforms has rnnoie,0.7042987942695618
translation,303,194,experiments,google - re,perform,slightly worse,google - re perform slightly worse,0.5788045525550842
translation,303,194,experiments,slightly worse,compared to,lamas,slightly worse compared to lamas,0.6943396925926208
translation,303,194,experiments,other factual probing dataset,has,google - re,other factual probing dataset has google - re,0.5931095480918884
translation,303,23,model,unified framework,for,information extraction,unified framework for information extraction,0.5932693481445312
translation,303,23,model,model,propose,unified framework,model propose unified framework,0.6881625652313232
translation,303,24,model,every information extraction problem,as,text-totriple   problem,every information extraction problem as text-totriple   problem,0.5319166779518127
translation,303,24,model,model,treat,every information extraction problem,model treat every information extraction problem,0.58210289478302
translation,303,28,model,same translation process,on,all tasks,same translation process on all tasks,0.5538521409034729
translation,303,30,model,common translation module,for,all tasks,common translation module for all tasks,0.6135738492012024
translation,303,30,model,common translation module,is,important,common translation module is important,0.586031436920166
translation,303,30,model,zero-shot transfer,of,general knowledge,zero-shot transfer of general knowledge,0.5721526145935059
translation,303,30,model,general knowledge,that,pre-trained lm,general knowledge that pre-trained lm,0.5992209911346436
translation,303,30,model,model,design of,common translation module,model design of common translation module,0.5930231809616089
translation,303,34,model,information extraction tasks,in,zero-shot setting,information extraction tasks in zero-shot setting,0.5080841779708862
translation,303,34,model,deepex,has,unified framework,deepex has unified framework,0.5889455080032349
translation,303,34,model,model,introduce,deepex,model introduce deepex,0.6829400658607483
translation,303,36,model,our framework,to,open information extraction,our framework to open information extraction,0.5411714911460876
translation,303,36,model,our framework,to,relation classification,our framework to relation classification,0.5544400215148926
translation,303,36,model,our framework,to,factual probe,our framework to factual probe,0.6176469922065735
translation,303,36,model,model,apply,our framework,model apply our framework,0.6174347996711731
translation,303,31,results,predicting,relational triple goes with,text,predicting relational triple goes with text,0.713295042514801
translation,303,31,results,text,on,task - agnostic corpus,text on task - agnostic corpus,0.5323111414909363
translation,303,31,results,zero-shot capabilities,on,all tasks,zero-shot capabilities on all tasks,0.5055302977561951
translation,303,37,results,competitive zero-shot performance,to,current state - of - the - art,competitive zero-shot performance to current state - of - the - art,0.5442366600036621
translation,303,37,results,current state - of - the - art,including,fully supervised methods,current state - of - the - art including fully supervised methods,0.5733281970024109
translation,303,38,results,significantly outperforms,by averaging,37.5 points,significantly outperforms by averaging 37.5 points,0.7552520036697388
translation,303,38,results,supervised open information extraction,by averaging,37.5 points,supervised open information extraction by averaging 37.5 points,0.694265604019165
translation,303,38,results,zero-shot approach,has,significantly outperforms,zero-shot approach has significantly outperforms,0.6035423874855042
translation,303,38,results,significantly outperforms,has,supervised open information extraction,significantly outperforms has supervised open information extraction,0.5932162404060364
translation,303,38,results,37.5 points,has,in f1,37.5 points has in f1,0.5757607817649841
translation,303,38,results,results,has,zero-shot approach,results has zero-shot approach,0.5614597201347351
translation,303,39,results,our framework,delivers,more interpretable results,our framework delivers more interpretable results,0.6469526886940002
translation,303,39,results,our framework,achieving,comparable performance,our framework achieving comparable performance,0.6257165670394897
translation,303,39,results,comparable performance,on,all tasks,comparable performance on all tasks,0.5065321922302246
translation,303,172,results,significant,for,oie,significant for oie,0.7643029689788818
translation,303,172,results,results,has,improvements,results has improvements,0.615561842918396
translation,303,174,results,results,are,encouraging,results are encouraging,0.5960068106651306
translation,303,174,results,encouraging,suggesting,zero-shot transfer,encouraging suggesting zero-shot transfer,0.7074564099311829
translation,303,174,results,zero-shot transfer,of,latent knowledge,zero-shot transfer of latent knowledge,0.5581692457199097
translation,303,174,results,deepex,has,results,deepex has results,0.5611402988433838
translation,303,174,results,results,are,encouraging,results are encouraging,0.5960068106651306
translation,303,174,results,results,suggesting,zero-shot transfer,results suggesting zero-shot transfer,0.6370740532875061
translation,303,176,results,pr curves,for,all oie test sets,pr curves for all oie test sets,0.6349928379058838
translation,303,176,results,comparison systems,across,all datasets,comparison systems across all datasets,0.7463639974594116
translation,303,176,results,outperforms,has,comparison systems,outperforms has comparison systems,0.5870910286903381
translation,303,176,results,results,has,pr curves,results has pr curves,0.5173031687736511
translation,303,178,results,comparison methods,on,t-rex ( factual probe ),comparison methods on t-rex ( factual probe ),0.5351226329803467
translation,303,178,results,deepex,has,slightly outperforms,deepex has slightly outperforms,0.631794810295105
translation,303,178,results,slightly outperforms,has,comparison methods,slightly outperforms has comparison methods,0.5732525587081909
translation,303,178,results,results,has,deepex,results has deepex,0.5287911891937256
translation,303,188,results,fewrel,our,top - 10 zeroshot performance,fewrel our top - 10 zeroshot performance,0.6394515633583069
translation,303,188,results,top - 10 zeroshot performance,is,best,top - 10 zeroshot performance is best,0.565159261226654
translation,303,188,results,fewrel,has,top - 10 zeroshot performance,fewrel has top - 10 zeroshot performance,0.5771085023880005
translation,303,188,results,few-shot relation classification,has,top - 10 zeroshot performance,few-shot relation classification has top - 10 zeroshot performance,0.5398955941200256
translation,303,188,results,results,on,fewrel,results on fewrel,0.5891933441162109
translation,303,220,results,best supervised oie system,by,6.3,best supervised oie system by 6.3,0.5921106338500977
translation,303,220,results,best supervised oie system,by,7.5,best supervised oie system by 7.5,0.5976020693778992
translation,303,220,results,best supervised oie system,in,auc,best supervised oie system in auc,0.5197457671165466
translation,303,220,results,6.3,in,f1,6.3 in f1,0.5824856758117676
translation,303,220,results,7.5,in,auc,7.5 in auc,0.5964690446853638
translation,303,220,results,deepex,has,outperforms,deepex has outperforms,0.6407557129859924
translation,303,220,results,outperforms,has,best supervised oie system,outperforms has best supervised oie system,0.6094651222229004
translation,303,220,results,results,has,deepex,results has deepex,0.5287911891937256
translation,303,222,results,original rnnoie,performs,similarly,original rnnoie performs similarly,0.6885583996772766
translation,303,222,results,good coverage,of,triples,good coverage of triples,0.591293215751648
translation,303,222,results,triples,on,dev set,triples on dev set,0.5887068510055542
translation,303,222,results,results,has,original rnnoie,results has original rnnoie,0.5912659764289856
translation,303,230,results,bert base,performs,worse,bert base performs worse,0.6428277492523193
translation,303,230,results,worse,than,bert large,worse than bert large,0.6686754822731018
translation,303,230,results,results,find that,bert base,results find that bert base,0.6503897309303284
translation,303,247,results,factual probe,under,same framework,factual probe under same framework,0.6891000270843506
translation,303,247,results,same framework,in,zero-shot settings,same framework in zero-shot settings,0.5379140973091125
translation,304,173,ablation-analysis,target context and alignment,are,negative,target context and alignment are negative,0.6125254034996033
translation,304,173,ablation-analysis,negative,for,translation quality,negative for translation quality,0.6309762001037598
translation,304,173,ablation-analysis,source context,has,target context and alignment,source context has target context and alignment,0.5719455480575562
translation,304,173,ablation-analysis,ablation analysis,ablating,source context,ablation analysis ablating source context,0.6766811609268188
translation,304,110,baselines,dada,uses,gradient information,dada uses gradient information,0.6315581798553467
translation,304,110,baselines,gradient information,to generate,adversarial sequences,gradient information to generate adversarial sequences,0.6911889314651489
translation,304,110,baselines,adversarial sequences,for,more robust nmt,adversarial sequences for more robust nmt,0.6433624625205994
translation,304,110,baselines,baselines,has,dada,baselines has dada,0.5795818567276001
translation,304,111,baselines,"advaug ( cheng et al. , 2020 )",extends,dada,"advaug ( cheng et al. , 2020 ) extends dada",0.767458975315094
translation,304,111,baselines,dada,where,embeddings of virtual sequences,dada where embeddings of virtual sequences,0.6223312020301819
translation,304,111,baselines,embeddings of virtual sequences,sampled from,adversarial distribution,embeddings of virtual sequences sampled from adversarial distribution,0.7006629109382629
translation,304,111,baselines,adversarial distribution,for,augmentation,adversarial distribution for augmentation,0.6049918532371521
translation,304,111,baselines,baselines,has,"advaug ( cheng et al. , 2020 )","baselines has advaug ( cheng et al. , 2020 )",0.5503992438316345
translation,304,181,baselines,baselines,compare,tcwr,baselines compare tcwr,0.7081863284111023
translation,304,126,experimental-setup,encoder,"XLM ( Lample and Conneau , 2019 ) pre-trained with",masked language model,"encoder XLM ( Lample and Conneau , 2019 ) pre-trained with masked language model",0.6925926804542542
translation,304,126,experimental-setup,experimental setup,has,encoder,experimental setup has encoder,0.5011709332466125
translation,304,127,experimental-setup,tied,reducing,size of the model,tied reducing size of the model,0.7145348191261292
translation,304,127,experimental-setup,experimental setup,has,input-output embeddings,experimental setup has input-output embeddings,0.5544615983963013
translation,304,128,experimental-setup,faster convergence,apply,"prenorm ( nguyen and salazar , 2019 )","faster convergence apply prenorm ( nguyen and salazar , 2019 )",0.6665938496589661
translation,304,128,experimental-setup,"prenorm ( nguyen and salazar , 2019 )",for getting rid of,warm - up stage,"prenorm ( nguyen and salazar , 2019 ) for getting rid of warm - up stage",0.751625120639801
translation,304,128,experimental-setup,warm - up stage,of,transformer,warm - up stage of transformer,0.5766894817352295
translation,304,128,experimental-setup,experimental setup,To achieve,faster convergence,experimental setup To achieve faster convergence,0.6337150931358337
translation,304,129,experimental-setup,learning rate,set to,1e - 5,learning rate set to 1e - 5,0.7201114892959595
translation,304,129,experimental-setup,linearly decayed,with,more training steps,linearly decayed with more training steps,0.6598175764083862
translation,304,129,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,304,130,experimental-setup,hidden size o,set to,1024,hidden size o set to 1024,0.7120197415351868
translation,304,130,experimental-setup,experimental setup,has,hidden size o,experimental setup has hidden size o,0.5368027091026306
translation,304,131,experimental-setup,maximum sequence size,set to,512,maximum sequence size set to 512,0.7163803577423096
translation,304,131,experimental-setup,experimental setup,has,maximum sequence size,experimental setup has maximum sequence size,0.5203388333320618
translation,304,132,experimental-setup,"lamb ( you et al. , 2019 )",as,optimizer,"lamb ( you et al. , 2019 ) as optimizer",0.5419321060180664
translation,304,132,experimental-setup,experimental setup,use,"lamb ( you et al. , 2019 )","experimental setup use lamb ( you et al. , 2019 )",0.5681357383728027
translation,304,133,experimental-setup,"gelu ( hendrycks and gimpel , 2016 )",used as,activation function,"gelu ( hendrycks and gimpel , 2016 ) used as activation function",0.6163129210472107
translation,304,133,experimental-setup,experimental setup,has,"gelu ( hendrycks and gimpel , 2016 )","experimental setup has gelu ( hendrycks and gimpel , 2016 )",0.5322545766830444
translation,304,134,experimental-setup,16 sequences,used at,pre-training step,16 sequences used at pre-training step,0.6491157412528992
translation,304,134,experimental-setup,experimental setup,has,16 sequences,experimental setup has 16 sequences,0.5197470188140869
translation,304,135,experimental-setup,masked language model,for,50 %,masked language model for 50 %,0.6165099143981934
translation,304,135,experimental-setup,left time,used for,training,left time used for training,0.7111805081367493
translation,304,135,experimental-setup,training,has,translation language model,training has translation language model,0.517682671546936
translation,304,135,experimental-setup,experimental setup,train,masked language model,experimental setup train masked language model,0.6111559271812439
translation,304,137,experimental-setup,fairseq,to implement,nmt models,fairseq to implement nmt models,0.6829361915588379
translation,304,137,experimental-setup,experimental setup,use,fairseq,experimental setup use fairseq,0.6436086297035217
translation,304,138,experimental-setup,vocabularty size,is,37k,vocabularty size is 37k,0.5986522436141968
translation,304,138,experimental-setup,experimental setup,has,vocabularty size,experimental setup has vocabularty size,0.5033324956893921
translation,304,140,experimental-setup,hidden size,set to,1024,hidden size set to 1024,0.740225613117218
translation,304,140,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,304,141,experimental-setup,16,has,self-attention heads,16 has self-attention heads,0.5793914198875427
translation,304,141,experimental-setup,experimental setup,has,16,experimental setup has 16,0.5221939086914062
translation,304,141,experimental-setup,experimental setup,has,self-attention heads,experimental setup has self-attention heads,0.510919988155365
translation,304,142,experimental-setup,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,304,142,experimental-setup,experimental setup,use,adam,experimental setup use adam,0.5593148469924927
translation,304,143,experimental-setup,learning rate,initially set to,1e - 7,learning rate initially set to 1e - 7,0.6662266850471497
translation,304,143,experimental-setup,learning rate,gradually increased to,5e - 4,learning rate gradually increased to 5e - 4,0.6566877365112305
translation,304,143,experimental-setup,5e - 4,with,4 k warm - up steps,5e - 4 with 4 k warm - up steps,0.6559180021286011
translation,304,143,experimental-setup,4 k warm - up steps,before applying,linear decay,4 k warm - up steps before applying linear decay,0.7191362977027893
translation,304,143,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,304,144,experimental-setup,dropout,set to,0.3,dropout set to 0.3,0.6373292803764343
translation,304,144,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,304,145,experimental-setup,label smoothing,with,smoothing factor,label smoothing with smoothing factor,0.62163907289505
translation,304,145,experimental-setup,smoothing factor,has,0.1,smoothing factor has 0.1,0.514997661113739
translation,304,145,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,304,146,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,304,146,experimental-setup,beam size,set to,12,beam size set to 12,0.7525127530097961
translation,304,146,experimental-setup,decoding,has,beam size,decoding has beam size,0.5500069856643677
translation,304,146,experimental-setup,experimental setup,For,decoding,experimental setup For decoding,0.5741471648216248
translation,304,180,experimental-setup,en-vi,use,ted tst2012,en-vi use ted tst2012,0.6577754616737366
translation,304,180,experimental-setup,en-vi,use,ted tst2013,en-vi use ted tst2013,0.6551126837730408
translation,304,180,experimental-setup,ted tst2012,for,validation,ted tst2012 for validation,0.64203941822052
translation,304,180,experimental-setup,ted tst2012,for,testing,ted tst2012 for testing,0.6642841100692749
translation,304,180,experimental-setup,ted tst2012,for,testing,ted tst2012 for testing,0.6642841100692749
translation,304,180,experimental-setup,ted tst2013,for,testing,ted tst2013 for testing,0.6694458723068237
translation,304,180,experimental-setup,experimental setup,For,en-vi,experimental setup For en-vi,0.6530662775039673
translation,304,6,model,augmented parallel translation corpora,by generating,( path-specific ) counterfactual aligned phrases,augmented parallel translation corpora by generating ( path-specific ) counterfactual aligned phrases,0.665722131729126
translation,304,6,model,model,creates,augmented parallel translation corpora,model creates augmented parallel translation corpora,0.5928279161453247
translation,304,7,model,new source phrases,from,masked language model,new source phrases from masked language model,0.5290428996086121
translation,304,30,model,source phrase,is,removed,source phrase is removed,0.5667160749435425
translation,304,30,model,resampled,according to,trained masked language model,resampled according to trained masked language model,0.613234281539917
translation,304,30,model,( path-specific ) counterfactual inference,on,causal model,( path-specific ) counterfactual inference on causal model,0.5362809896469116
translation,304,30,model,causal model,given by,trained translation language model,causal model given by trained translation language model,0.5967313647270203
translation,304,30,model,trained translation language model,to resample,only the aligned target phrase,trained translation language model to resample only the aligned target phrase,0.6494895219802856
translation,304,30,model,only the aligned target phrase,given,changed source phrase,only the aligned target phrase given changed source phrase,0.6248988509178162
translation,304,30,model,model,perform,( path-specific ) counterfactual inference,model perform ( path-specific ) counterfactual inference,0.5614007711410522
translation,304,31,model,source / target context and alignment,for,data augmentation,source / target context and alignment for data augmentation,0.569832980632782
translation,304,125,model,encoder and decoder,composed of,six layers,encoder and decoder composed of six layers,0.6846191883087158
translation,304,125,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,304,139,model,model,has,six encoder and decoder layers,model has six encoder and decoder layers,0.5483944416046143
translation,304,153,results,bleu score,improves,more pre-training steps,bleu score improves more pre-training steps,0.7011569142341614
translation,304,153,results,results,has,bleu score,results has bleu score,0.5436024069786072
translation,304,157,results,model,with,xlm initialization,model with xlm initialization,0.6270930767059326
translation,304,157,results,model,with,random initialization,model with random initialization,0.6144117712974548
translation,304,157,results,model,converges,faster and better,model converges faster and better,0.7855817675590515
translation,304,157,results,model,with,random initialization,model with random initialization,0.6144117712974548
translation,304,157,results,faster and better,compared to,model,faster and better compared to model,0.7186187505722046
translation,304,157,results,model,with,random initialization,model with random initialization,0.6144117712974548
translation,304,158,results,xlm,trained using,masked language model objective,xlm trained using masked language model objective,0.5826752185821533
translation,304,158,results,masked language model objective,on,large-scale monolingual data,masked language model objective on large-scale monolingual data,0.4411734342575073
translation,304,158,results,large-scale pre-training,can improve,downstream language model pre-training tasks,large-scale pre-training can improve downstream language model pre-training tasks,0.667559802532196
translation,304,158,results,results,has,xlm,results has xlm,0.5113705992698669
translation,304,160,results,model,with,xlm initialization,model with xlm initialization,0.6270930767059326
translation,304,160,results,model,performs,better ( 17.49 bleu ),model performs better ( 17.49 bleu ),0.5868827104568481
translation,304,160,results,better ( 17.49 bleu ),compared to,counterpart ( 16.68 bleu ),better ( 17.49 bleu ) compared to counterpart ( 16.68 bleu ),0.6835033297538757
translation,304,160,results,results,has,model,results has model,0.5339115858078003
translation,304,187,results,tcwr,yields,improvements,tcwr yields improvements,0.7638034820556641
translation,304,187,results,improvements,of,"2.03 , 1.63 and 1.79 bleu","improvements of 2.03 , 1.63 and 1.79 bleu",0.5753517746925354
translation,304,187,results,"2.03 , 1.63 and 1.79 bleu",for,"en-tr , en-de and en-vi","2.03 , 1.63 and 1.79 bleu for en-tr , en-de and en-vi",0.6585933566093445
translation,304,187,results,baseline,has,with no data augmentation,baseline has with no data augmentation,0.5798344612121582
translation,304,187,results,baseline,has,tcwr,baseline has tcwr,0.5572033524513245
translation,304,187,results,with no data augmentation,has,tcwr,with no data augmentation has tcwr,0.6257330775260925
translation,304,187,results,results,on,three language pairs,results on three language pairs,0.46534067392349243
translation,304,187,results,results,Compared to,baseline,results Compared to baseline,0.6899874806404114
translation,304,188,results,alignment,for,nmt data augmentation,alignment for nmt data augmentation,0.5515581965446472
translation,304,188,results,tcwr,has,outperforms,tcwr has outperforms,0.6536661386489868
translation,304,188,results,outperforms,has,other augmentation methods,outperforms has other augmentation methods,0.5587957501411438
translation,304,188,results,results,has,tcwr,results has tcwr,0.5812371969223022
translation,304,189,results,tcwr,brings,consistent improvements,tcwr brings consistent improvements,0.6530731320381165
translation,304,189,results,consistent improvements,to,low-resource and high- resource language pairs,consistent improvements to low-resource and high- resource language pairs,0.5737503170967102
translation,304,189,results,results,demonstrate,tcwr,results demonstrate tcwr,0.620546281337738
translation,305,160,ablation-analysis,text,was,missing,text was missing,0.6459832191467285
translation,305,160,ablation-analysis,relatively large performance drop,implies,language modality,relatively large performance drop implies language modality,0.6104033589363098
translation,305,160,ablation-analysis,text,has,model,text has model,0.6128824353218079
translation,305,160,ablation-analysis,missing,has,model,missing has model,0.606469452381134
translation,305,160,ablation-analysis,model,has,relatively large performance drop,model has relatively large performance drop,0.5507715940475464
translation,305,160,ablation-analysis,ablation analysis,when,text,ablation analysis when text,0.6953706741333008
translation,305,141,baselines,sequence to sequence,for,sentiment ( seq2seq2sent ),sequence to sequence for sentiment ( seq2seq2sent ),0.6262989640235901
translation,305,141,baselines,multimodal sentiment analysis,with,transformer ( transmodality ),multimodal sentiment analysis with transformer ( transmodality ),0.6417288780212402
translation,305,141,baselines,translation - based,has,multimodal cyclic translation network ( mctn ),translation - based has multimodal cyclic translation network ( mctn ),0.5882948040962219
translation,305,141,baselines,translation - based,has,sequence to sequence,translation - based has sequence to sequence,0.6167308688163757
translation,305,141,baselines,translation - based,has,multimodal sentiment analysis,translation - based has multimodal sentiment analysis,0.5188969373703003
translation,305,141,baselines,multimodal cyclic translation network ( mctn ),has,sequence to sequence,multimodal cyclic translation network ( mctn ) has sequence to sequence,0.6179496645927429
translation,305,141,baselines,multimodal cyclic translation network ( mctn ),has,multimodal sentiment analysis,multimodal cyclic translation network ( mctn ) has multimodal sentiment analysis,0.5465080142021179
translation,305,141,baselines,baselines,has,translation - based,baselines has translation - based,0.582858681678772
translation,305,142,baselines,baselines,has,non-translation based,baselines has non-translation based,0.5552377104759216
translation,305,167,experiments,translation text ? audio,achieves,better performance,translation text ? audio achieves better performance,0.66877281665802
translation,305,167,experiments,better performance,than,audio ? text,better performance than audio ? text,0.601737916469574
translation,305,167,experiments,"( audio , text ) instance",has,translation text ? audio,"( audio , text ) instance has translation text ? audio",0.5981679558753967
translation,305,178,experiments,model,achieves,best performance,model achieves best performance,0.6728841066360474
translation,305,178,experiments,best performance,at,layer 1,best performance at layer 1,0.5768613219261169
translation,305,178,experiments,meld ( sentiment ),has,model,meld ( sentiment ) has model,0.601706862449646
translation,305,7,model,coupled - translation fusion network ( ctfn ),proposed to model,bi-direction interplay,coupled - translation fusion network ( ctfn ) proposed to model bi-direction interplay,0.7714049220085144
translation,305,7,model,bi-direction interplay,via,couple learning,bi-direction interplay via couple learning,0.7060958743095398
translation,305,7,model,couple learning,ensuring,robustness,couple learning ensuring robustness,0.745133101940155
translation,305,7,model,robustness,in respect to,missing modalities,robustness in respect to missing modalities,0.6480712294578552
translation,305,8,model,cyclic consistency constraint,presented to improve,translation performance,cyclic consistency constraint presented to improve translation performance,0.6786308884620667
translation,305,8,model,model,has,cyclic consistency constraint,model has cyclic consistency constraint,0.5726240873336792
translation,305,11,model,hierarchical architecture,to exploit,multiple bi-direction translations,hierarchical architecture to exploit multiple bi-direction translations,0.6765353679656982
translation,305,11,model,multiple bi-direction translations,leading to,double multimodal fusing embeddings,multiple bi-direction translations leading to double multimodal fusing embeddings,0.6555283069610596
translation,305,11,model,double multimodal fusing embeddings,compared with,traditional translation methods,double multimodal fusing embeddings compared with traditional translation methods,0.6318324208259583
translation,305,11,model,ctfn,has,hierarchical architecture,ctfn has hierarchical architecture,0.5749261379241943
translation,305,11,model,model,Based on,ctfn,model Based on ctfn,0.7165889739990234
translation,305,36,model,cyclic consistency constraint,proposed to improve,translation performance,cyclic consistency constraint proposed to improve translation performance,0.6906856298446655
translation,305,36,model,model,has,cyclic consistency constraint,model has cyclic consistency constraint,0.5726240873336792
translation,305,146,results,ctfn,exceeded,previous best transmodality,ctfn exceeded previous best transmodality,0.6864824891090393
translation,305,146,results,previous best transmodality,by,margin,previous best transmodality by margin,0.565902054309845
translation,305,146,results,margin,of,4.51,margin of 4.51,0.6084176301956177
translation,305,146,results,cmu - mosi dataset,has,ctfn,cmu - mosi dataset has ctfn,0.589803159236908
translation,305,146,results,previous best transmodality,has,"on ( video , audio )","previous best transmodality has on ( video , audio )",0.5938244462013245
translation,305,146,results,results,on,cmu - mosi dataset,results on cmu - mosi dataset,0.5326117873191833
translation,305,147,results,empirical improvement,of,ctfn,empirical improvement of ctfn,0.6039842963218689
translation,305,147,results,ctfn,was,0.78,ctfn was 0.78,0.6405874490737915
translation,305,147,results,meld ( sentiment ) dataset,has,empirical improvement,meld ( sentiment ) dataset has empirical improvement,0.5548974275588989
translation,305,147,results,results,on,meld ( sentiment ) dataset,results on meld ( sentiment ) dataset,0.5517246127128601
translation,305,148,results,"( video , audio )",is,more significant,"( video , audio ) is more significant",0.5701761245727539
translation,305,148,results,more significant,than,"( text , video )","more significant than ( text , video )",0.6010249853134155
translation,305,148,results,more significant,than,"( text , audio )","more significant than ( text , audio )",0.5982791185379028
translation,305,148,results,results,improvement of,"( video , audio )","results improvement of ( video , audio )",0.7238147258758545
translation,305,150,results,ctfn,exceeds,previous best transmodality,ctfn exceeds previous best transmodality,0.6430289149284363
translation,305,150,results,previous best transmodality,with,improvement,previous best transmodality with improvement,0.6132492423057556
translation,305,150,results,improvement,of,0.06,improvement of 0.06,0.5992418527603149
translation,305,150,results,0.06,leading to,comparable performance,0.06 leading to comparable performance,0.6506638526916504
translation,305,150,results,"text , audio , video )",has,ctfn,"text , audio , video ) has ctfn",0.6198099255561829
translation,305,153,results,tri-modality case,achieved,improvement,tri-modality case achieved improvement,0.7059566974639893
translation,305,153,results,improvement,of,0.61,improvement of 0.61,0.5649508833885193
translation,305,153,results,improvement,indicating,benefits,improvement indicating benefits,0.683861255645752
translation,305,153,results,bi-modality setting,has,tri-modality case,bi-modality setting has tri-modality case,0.6033535003662109
translation,305,153,results,results,compared to,bi-modality setting,results compared to bi-modality setting,0.6452452540397644
translation,305,159,results,"setting ( text , audio , video )",reach,comparable result,"setting ( text , audio , video ) reach comparable result",0.7443575263023376
translation,305,159,results,text - based settings,reach,comparable result,text - based settings reach comparable result,0.7325178384780884
translation,305,159,results,comparable result,with,relatively small performance drop,comparable result with relatively small performance drop,0.6714737415313721
translation,305,159,results,results,compared to,"setting ( text , audio , video )","results compared to setting ( text , audio , video )",0.6853514313697815
translation,305,161,results,"( audio , video , text )",demonstrates,hierarchical ctfn,"( audio , video , text ) demonstrates hierarchical ctfn",0.6466377973556519
translation,305,161,results,hierarchical ctfn,able to maintain,robustness and consistency,hierarchical ctfn able to maintain robustness and consistency,0.6710327863693237
translation,305,161,results,robustness and consistency,when considering,single input modality,robustness and consistency when considering single input modality,0.7174479365348816
translation,306,9,ablation-analysis,correct choice of subword model,shown to be,biggest driver,correct choice of subword model shown to be biggest driver,0.6202353835105896
translation,306,9,ablation-analysis,biggest driver,of,translation performance,biggest driver of translation performance,0.575056791305542
translation,306,9,ablation-analysis,ablation analysis,has,correct choice of subword model,ablation analysis has correct choice of subword model,0.5463996529579163
translation,306,140,ablation-analysis,dgt model,with,16 k bpe submodel,dgt model with 16 k bpe submodel,0.6281620860099792
translation,306,140,ablation-analysis,boosted validation accuracy,by,over 8 %,boosted validation accuracy by over 8 %,0.5934079885482788
translation,306,140,ablation-analysis,over 8 %,compared with,baseline,over 8 % compared with baseline,0.7699832320213318
translation,306,140,ablation-analysis,dgt model,has,boosted validation accuracy,dgt model has boosted validation accuracy,0.5610795021057129
translation,306,140,ablation-analysis,ablation analysis,training,dgt model,ablation analysis training dgt model,0.7257922291755676
translation,306,107,experimental-setup,nvidia geforce gtx 1080 ti.,has,"bisong , 2019 )","nvidia geforce gtx 1080 ti. has bisong , 2019 )",0.6114434003829956
translation,306,108,experimental-setup,mt models,trained using,pytorch implementation,mt models trained using pytorch implementation,0.692946195602417
translation,306,108,experimental-setup,pytorch implementation,has,of opennmt 2.0,pytorch implementation has of opennmt 2.0,0.5200501680374146
translation,306,108,experimental-setup,experimental setup,trained using,pytorch implementation,experimental setup trained using pytorch implementation,0.655376672744751
translation,306,108,experimental-setup,experimental setup,has,mt models,experimental setup has mt models,0.5342441201210022
translation,306,13,results,transformer optimized model,demonstrated,bleu score improvement,transformer optimized model demonstrated bleu score improvement,0.6505333781242371
translation,306,13,results,bleu score improvement,of,7.8 points,bleu score improvement of 7.8 points,0.5231402516365051
translation,306,13,results,7.8 points,compared with,baseline rnn model,7.8 points compared with baseline rnn model,0.6414651274681091
translation,306,13,results,results,has,transformer optimized model,results has transformer optimized model,0.5550001859664917
translation,306,15,results,our translation engines,demonstrated,significant improvements,our translation engines demonstrated significant improvements,0.6670358777046204
translation,306,15,results,google translate,has,our translation engines,google translate has our translation engines,0.5793423056602478
translation,306,117,results,both rnn and transformer architectures,incorporating,any submodel type,both rnn and transformer architectures incorporating any submodel type,0.7406483888626099
translation,306,117,results,any submodel type,led to,improvements,any submodel type led to improvements,0.6959484219551086
translation,306,117,results,improvements,in,model accuracy,improvements in model accuracy,0.498393714427948
translation,306,117,results,results,In training,both rnn and transformer architectures,results In training both rnn and transformer architectures,0.7375389933586121
translation,306,119,results,rnn architecture,on,dgt,rnn architecture on dgt,0.5945823788642883
translation,306,119,results,best performing model,with,32 k unigram submodel,best performing model with 32 k unigram submodel,0.5855891108512878
translation,306,119,results,best performing model,achieved,bleu score,best performing model achieved bleu score,0.6605521440505981
translation,306,119,results,higher,than,baseline,higher than baseline,0.5882788896560669
translation,306,119,results,rnn architecture,has,best performing model,rnn architecture has best performing model,0.5576742887496948
translation,306,119,results,dgt,has,best performing model,dgt has best performing model,0.5376182794570923
translation,306,119,results,bleu score,has,7.4 %,bleu score has 7.4 %,0.5784186720848083
translation,306,119,results,7.4 %,has,higher,7.4 % has higher,0.567992091178894
translation,306,119,results,results,Using,rnn architecture,results Using rnn architecture,0.6331961154937744
translation,306,122,results,maximum bleu score improvement,of,1.5 points ( 2.5 % ),maximum bleu score improvement of 1.5 points ( 2.5 % ),0.5330128073692322
translation,306,122,results,1.5 points ( 2.5 % ),is,quite modest,1.5 points ( 2.5 % ) is quite modest,0.5256993770599365
translation,306,122,results,quite modest,in the case of,public admin corpus,quite modest in the case of public admin corpus,0.6667360067367554
translation,306,122,results,results,has,maximum bleu score improvement,results has maximum bleu score improvement,0.5540392398834229
translation,306,129,results,transformer models,has,outperform,transformer models has outperform,0.6127658486366272
translation,306,129,results,outperform,has,all their rnn counterparts,outperform has all their rnn counterparts,0.5857754945755005
translation,306,129,results,results,has,transformer models,results has transformer models,0.5163614749908447
translation,306,131,results,16 k bpe submodels,on,transformer architectures,16 k bpe submodels on transformer architectures,0.5422049760818481
translation,306,131,results,16 k bpe submodels,leads to,better translation performance,16 k bpe submodels leads to better translation performance,0.6767518520355225
translation,306,131,results,results,using,16 k bpe submodels,results using 16 k bpe submodels,0.6460009217262268
translation,306,131,results,results,has,ter scores,results has ter scores,0.4983568489551544
translation,306,132,results,ter score,for,dgt transformer 16 k bpe model,ter score for dgt transformer 16 k bpe model,0.6405870318412781
translation,306,132,results,dgt transformer 16 k bpe model,is,significantly better ( 0.33 ),dgt transformer 16 k bpe model is significantly better ( 0.33 ),0.5557814836502075
translation,306,132,results,significantly better ( 0.33 ),compared with,baseline performance ( 0.41 ),significantly better ( 0.33 ) compared with baseline performance ( 0.41 ),0.6560122966766357
translation,306,132,results,results,has,ter score,results has ter score,0.5461512207984924
translation,306,139,results,subword model,led to,much slower convergence,subword model led to much slower convergence,0.6495627164840698
translation,306,139,results,only marginal gains,after,60 k steps,only marginal gains after 60 k steps,0.7090421319007874
translation,306,139,results,results,Including,subword model,results Including subword model,0.6273781061172485
translation,306,157,results,best performing model,on,55 k dgt corpus,best performing model on 55 k dgt corpus,0.4856094717979431
translation,306,157,results,best performing model,on,16 k bpe submodel,best performing model on 16 k bpe submodel,0.5335240960121155
translation,306,157,results,best performing model,achieved,bleu score,best performing model achieved bleu score,0.6605521440505981
translation,306,157,results,best performing model,achieved,ter score,best performing model achieved ter score,0.7264403700828552
translation,306,157,results,55 k dgt corpus,with,2 heads,55 k dgt corpus with 2 heads,0.6625998616218567
translation,306,157,results,bleu score,of,60.5,bleu score of 60.5,0.5577889680862427
translation,306,157,results,ter score,of,0.33,ter score of 0.33,0.5586562752723694
translation,306,157,results,results,has,best performing model,results has best performing model,0.5340396165847778
translation,306,159,results,all transformer models,using,8 heads,all transformer models using 8 heads,0.7051830291748047
translation,306,159,results,equivalent models,using,just 2 heads,equivalent models using just 2 heads,0.7060233354568481
translation,306,159,results,larger 88 k pa corpus,has,all transformer models,larger 88 k pa corpus has all transformer models,0.5771856307983398
translation,306,159,results,results,In the case of,larger 88 k pa corpus,results In the case of larger 88 k pa corpus,0.648459255695343
