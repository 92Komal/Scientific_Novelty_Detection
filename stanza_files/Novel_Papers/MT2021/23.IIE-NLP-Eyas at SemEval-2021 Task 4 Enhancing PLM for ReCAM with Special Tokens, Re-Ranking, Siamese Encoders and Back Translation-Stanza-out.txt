title
IIE-NLP - Eyas at SemEval - 2021 Task 4 : Enhancing PLM for ReCAM with Special Tokens , Re-Ranking , Siamese Encoders and Back Translation
abstract
This paper introduces our systems for all three subtasks of SemEval - 2021 Task 4 : Reading Comprehension of Abstract Meaning .
To help our model better represent and understand abstract concepts in natural language , we welldesign many simple and effective approaches adapted to the backbone model ( RoBERTa ) .
Specifically , we formalize the subtasks into the multiple -choice question answering format and add special tokens to abstract concepts , then , the final prediction of QA is considered as the result of subtasks .
Additionally , we employ many finetuning tricks to improve the performance .
Experimental results show that our approach gains significant performance compared with the baseline systems .
Our system ?
achieves eighth rank ( 87.51 % ) and tenth rank ( 89.64 % ) on the official blind test set of subtask 1 and subtask 2 respectively .
Introduction
The computer 's ability in understanding , representing , and expressing abstract meaning is a fundamental problem towards achieving true natural language understanding .
SemEval - 2021 Task 4 : Reading Comprehension of Abstract Meaning ( ReCAM ) provides a well - formed benchmark that aims to study the machine 's ability in representing and understanding abstract concepts ( Zheng et al. , 2021 ) .
The Reading Comprehension of Abstract Meaning ( ReCAM ) task is divided into three subtasks , including Imperceptibility , Nonspecificility , and Interaction .
Please refer to the task description paper ( Zheng et al. , 2021 ) for more details .
To address the above challenges in ReCAM , we first formalize all subtasks as a type of multiple -choice ?
Corresponding author . ?
Our Code is publicly available at https://github.
com/indexfziq/IIE-NLP-Eyas-SemEval2021 .
Question Answering ( QA ) task like ( Xing et al. , 2020 ) .
Recently , the large Pre-trained Language Models ( PLMs ) , such as GPT - 2 ( Radford et al. , 2019 ) , BERT ( Devlin et al. , 2019 ) , RoBERTa ( Liu et al. , 2019 ) , demonstrate their excellent ability in various natural language understanding tasks ( Wang et al. , 2018 ; Zellers et al. , 2018 Zellers et al. , , 2019 .
So , we employ the state - of- the- art PLM , RoBERTa , as our backbone model .
Moreover , we design many simple and effective approaches to improve the performance of the backbone model , such as adding special tokens , sentence re-ranking , label smoothing and back translation .
This paper describes approaches for all subtasks developed by the IIE-NLP - Eyas Team ( Natural Language Processing group of Institute of Information Engineering of the Chinese Academy of Sciences ) .
Our contributions are summarized as the followings : ?
We design many simple and effective approaches to improve the performance of the PLMs on all three subtasks , such as special tokens , sentence re-ranking , siamese encoders and back translation and label smoothing ; ?
Experiments demonstrate that our proposed methods achieve significant improvements compared with baselines and we obtain the 8th- place in subtask - 1 and the 10th - place in subtask - 2 on the final official evaluation .
Approaches
Since the format of the tasks in ReCAM is the same , we use the unified framework to address all tasks .
The following is the detail of our methods .
Task Definition
We first present the description of symbols which are used in this paper .
Formally , suppose there are seven key elements in all subtasks , i.e. { D , Q , A 1 , A 2 , A 3 , A 4 , A 5 }.
We sup-pose the D denotes the given article , the Q denotes the summary of the article with a placeholder , the A * denotes the candidate abstract concepts for all subtasks to fill in the placeholder .
Multi-Choice Based Model
The pre-trained language models have made a great contribution to MRC tasks .
Recently , a significant milestone is the BERT ( Devlin et al. , 2019 ) , which gets new state - of - the - art results on eleven natural language processing tasks .
In this section , we present the description of the multi-choice based model which we use in all subtasks .
Consider the BERT - style model RoBERTa 's ( Liu et al. , 2019 ) stronger performance than BERT , we utilize it as our backbone model , which introduces more data and bigger models for better performance .
A multiple -choice based QA model M consists of a PLM encoder and a task -specific classification layer which includes a feed-forward neural network f ( ? ) and a softmax operation .
For each pair of question - answer , the calculation of M is as follow : score i = exp ( f ( S i ) ) i exp ( f ( S i ) ) ( 1 ) S i = PLM ( [ Q ; A i ; D ] ) ( 2 ) where the [ ? ] is the input constructed according to the instruction of PLMs , and the S * is the final hidden state of the first token ( < s > ) .
For more details , we refer to the original work of PLMs ( Liu et al. , 2019 ) .
The candidate answer which owns a higher score will be identified as the final prediction .
The model M is trained end-to -end with the cross-entropy objective function .
Special Tokens Considering the great performance of special tokens in entity and relation extraction ( Zhong and Chen , 2021 ) , as well as of the prompt template on commonsense reasoning ( Xing et al. , 2020 ) , we attach special tokens to highlight the semantic representation of candidate abstract concepts in the input layer .
To help the PLMs represent and understand the abstract concept ( i.e. option word in ReCAM tasks ) in textual description ( i.e. summary of the article in ReCAM task ) , we use < e> and </e> to add on both ends of the abstract concept , i.e. < e> abstract concept </e >.
It is interesting that the special tokens are useful features contributing to most of the system 's boost , and we have tried many other useful special tokens which will be discussed in section 4 . Sentence Ranking
As the given passage is too long to be deal with the Pre-trained Language Models ( PLMs ) , we consider refining the passage input by rearranging the order of the sentences in the passage .
With this reorder process , the sentence , which is more critical to the question , can appear at the beginning of the passage .
Although the passage 's sequential information is sacrificed , we keep the more question - relevant information of the passage .
Supposing the passage D contains N sentences , i.e. , D = { W 1 , W 2 , ... , W N } , where each sentence W n = {t 1 , t 2 , ... , t M } including M tokens .
We denote the given cloze-style question as Q .
To rank the sentences in D , we resort BERT to compute the similarity score between each sentence , i.e .
W n , and Q following the algorithm in Zhang et al . ( 2020 ) .
After ranking , the sentences in D are sorted in descending order of similarity scores , and we can get a rearranged passage D as the passage input to the QA model .
In the implement progress , D will be truncated to fit into the PLM encoder with our setting max length .
Siamese Encoders
When exploring the dataset , we find that the complete question statement , representing the result statement after replacing the placeholder token with the candidate option , also contains the semantic information which can help to make the judgment about options .
Based on the observation , we propose a siamese encoders based architecture to inject the additional complete question statement information while not influence the input with passage .
On the other hand , it can be seen as introducing an auxiliary task to assist the main task .
Specifically , the training of siamese encoder based architecture is as following : l 1 i = P LM ( [ Qi ] ) [ 0 ] ( 3 ) l 2 i = P LM ( [ Q ; A i ; D ] ) [ 0 ] ( 4 ) P 1 ( A i | Q ) = softmax ( f ( l 1 i ) ) ( 5 ) P 2 ( A i |D , Q ) = softmax ( f ( l 2 i ) ) ( 6 ) where the P LM ( ? ) stands for PLM encoder , Qi is the complete question statement , i indicates the i-th candidate answer , f ( ? ) is the feed forward network .
To coordinate the two losses , we opt for an uncertainty loss ( Kendall et al. , 2018 ) to adjust it adaptively through ? { 1,2 } as : L ( ? , ? 1 , ? 2 ) = 1 2 ? 2 1 L 1 ( ? ) + 1 2 ? 2 2 L 2 ( ? ) + log ?
2 1 ? 2 2 , where L { 1,2 } are the cross-entropy loss between the model prediction P { 1,2 } and the ground truth label respectively .
Back Translation Generally speaking , more successful neural networks require a large number of parameters , often in the millions .
In order to make the neural network implements correctly , a lot of data is needed for training , but in actual situations , there is not as much data as we thought .
The role of data augmentation includes two aspects .
One is to increase the amount of training data and improve the generalization ability of the model .
The other is to increase the noise data and improve the robustness of the model .
A large number of the works ( Buslaev et al. , 2018 ; Bloice et al. , 2019 ; Chen et al. , 2020 ; Cubuk et al. , 2020 ; Sato et al. , 2018 ; Zhu et al. , 2020 ) consider the data augmentation to make better performances .
In the field of computer vision , a lot of work ( Buslaev et al. , 2018 ; Bloice et al. , 2019 ; Chen et al. , 2020 ; Cubuk et al. , 2020 ) uses existing data to perform operations , such as flipping , translation or rotation , to create more data , so that neural networks have better generalization effects .
Adding Gaussian distribution to text processing ( Sato et al. , 2018 ) can also achieve the effect of data augmentation .
Besides , some works ( Miyato et al. , 2017 ; Zhu et al. , 2020 ) utilize the adversarial training methods to do the data augmentation .
For convenience and simplicity , we adopt the back translation ( Sennrich et al. , 2016 ) to increase the amount of training data , which is used to construct pseudo parallel corpus in unsupervised machine translation ( Lample et al. , 2018 ) .
Specifically , we use the Google API ? to translate the passage into French , and then translate the translation into English in turn .
The pseudo parallel corpus can be obtained as : { D } = bkt ( { D} ) ( 7 ) where { D } means the translated English corpus that we used as data agument , bkt is back translation .
As for the question , given the existence of the special character placeholder , forced translation may result in grammatical errors and semantic gaps .
Therefore , the questions and options will be kept original .
After getting the pseudo parallel corpus , we train our model with the training data together with the cross-entropy loss function .
we consider training model with label smoothing ( Miller et al. , 1996 ; Pereyra et al. , 2017 ) .
Label smoothing can maintain uncertainty over the label space during training .
When training with label smoothing , for classification tasks , the hard one- hot label distribution is replaced with a softened label distribution through a smoothing value ? , which is a hyperparameter .
Specifically , for hard one- hot label distribution , the target category 's probability will be assigned to 1.0 and others are 0.0 .
Label smoothing will soften the label distribution by modifying the probability distribution with a discount .
Then , the target category 's probability will be 1 ? , and the probabilities of the rest categories are ? K?1 , where K is the number of task categories .
In our experiments , we set the smoothing value ? = 0.1 .
Label Smoothing
Experiments and Results
Experimental Setup
In all subtasks , the scale of each task is shown in Table 1 .
We train the model on training data and the related pseudo data generated by back translation , then select hyper-parameters based on the best performing model on the dev set , and then report results on the test set .
Our system is implemented with PyTorch and we use the PyTorch version of the pre-trained language models ? .
We employ RoBERTa ( Liu et al. , 2019 ) large model as our PLM encoder in Equation 2 .
The Adam optimizer ( Kingma and Ba , 2014 ) is used to fine - tune the model .
We introduce the detailed setup of the best model on the development dataset .
For subtask - 1 and subtask - 2 , the hyper-parameters are shown in
Evaluation Results Imperceptibility From Table 3 , we can see the results of our system on subtask - 1 of ReCAM .
Compared to the backbone model RoBERTa large model , our methods achieve significant improvements .
It is interesting that the special token is the most helpful part for the Imperceptibility subtask .
Nonspecificility
Table 4 summarizes the results of our approachs on subtask - 2 of ReCAM .
In Nonspecificility subtask , the model with special tokens and label smoothing performs best .
Compared to the backbone model ROBERTA LARGE , all our methods achieve better performance .
Interaction
We also perform subtask - 3 of Re-CAM , Interaction , which aims to provide more insights into the relationship of the two views on abstractness .
In this task , we test the performance of our system that is trained on one definition and evaluated on the other .
The results of our system 's performance on Imperceptibility and Nonspecificility subtasks which is shown in Table 5 .
We can find that our model is relatively robust for different abstract concepts .
Analysis and Discussion
Ablation Study
In this part , we perform an ablation study of our approaches ( special tokens , sentence re-ranking , label smoothing , siamese encoders and back translation ) .
Table 3 and 4 shows that our proposed methods help the backbone model better represent and understand the abstract concepts .
Note that the special tokens bring the PLMs with the best improvements in both subtask - 1 and subtask - 2 .
It is possible that the special tokens teach the model to focus on the abstract concept in a stronger manner .
Moreover , other common tricks bring with little improvements .
Discussion of Special Tokens
We also search for the best special tokens for Re - CAM on the dev set of subtask - 1 .
e stands for the word entity .
# and $ are common special tokens for NLP downstream applications .
As shown in Table 6 , <e> </e> enhance the representations of abstract concepts best of all .
# and $ work well .
In addition , the <> and </> could be helpful for PLMs to pay attention to the abstract concepts .
Moreover , it is interesting that each special token helps PLMs choose the right abstract concepts which are submerged in long sequential tokens ( including article and summary ) .
This result strengthen the point that special tokens can enhance the representation of abstract concepts in PLM based approaches .
In this paper , we design many simple and effective approaches to improve the performance of the PLMs on all three subtasks .
Experiments demonstrate that the proposed methods achieve significant improvement compared with the PLMs baseline and we obtain the eighth - place in subtask - 1 and tenth - place in subtask - 2 on the final official evaluation .
Moreover , we show that special tokens are useful features contributing to most of the system 's boost , which work well in enhancing PLMs for representating and understanding abstract concepts .
Table 1 : 1 Furthermore , for improving the generalization ability of the model trained on sole task and prevent the overconfidence of model , ?
The web page is available at https://translate.google.com
Data scale of each subtask .
Subtask Train Trail Dev Test Imperceptibility 3227 1000 837 2025 Nonspecificility 3318 1000 851 2017 Hyper-parameter Value LR { 1e- 5 , 2e- 5 } Batch size { 16 , 32 } Gradient norm 1.0 Warm - up { 0.1 , 1 , 2 } Max . input length ( # subwords ) 200 Epochs [ 3 , 10 ]
Table 2 : 2 Hyper-parameters of our approach .
Table 2 . 2 ? https://github.com/huggingface/transformers Models Trial Acc. Dev Acc. ROBERTA LARGE ( Liu et al. , 2019 ) 85.85 82.12 ( 1 ) w/ special tokens 87.81 87.69 ( 2 ) w/ sentence ranking 86.54 83.52 ( 3 ) w/ label smoothing 86.88 85.85 ( 4 ) w/ siamese encoders 86.62 83.22 ( 5 ) w/ back translation 87.23 84.32 Our Approach 87.81 87.69
Table 3 : 3
The results of our system on subtask - 1 .
Our approach is the final , stable and best model : ROBERTA LARGE with special tokens .
We finally obtain 87.51 Acc. on the official blind test set .
Models Trial Acc. Dev Acc. ROBERTA LARGE ( Liu et al. , 2019 ) 88.51 85.93 ( 1 ) w/ special tokens 87.47 88.98 ( 2 ) w/ sentence ranking 87.29 86.84 ( 3 ) w/ label smoothing 87.67 87.08 ( 4 ) w/ siamese encoders 87.34 86.18 ( 5 ) w/ back translation 88.41 87.54 Our Approach 87.10 89.54
Table 4 : 4 The results of our system on subtask - 2 .
Our approach is the final , stable and best model : ROBERTA LARGE with special tokens and label smoothing .
We finally obtain 89.64 Acc. on the official blind test set .
Table 5 : 5 The results of our approach on subtask - 3 .
Trained on Tested on Test Acc. Subtask - 1 Subtask - 1 87.51 Subtask - 1 Subtask - 2 84.13 Subtask - 2 Subtask -2 89.64 Subtask - 2 Subtask -1 81.09 Special Token Trial Acc. Dev Acc. <e> </e> 88.01 87.10 <#> </#> 88.63 86.93 <$> </$> 88.12 86.26 # /# 87.34 85.89 $ /$ 87.73 86.13 N/A 86.23 83.12
Table 6 : 6
The results of models with different special tokens on subtask - 1 .
