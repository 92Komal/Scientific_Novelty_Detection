title
Unsupervised Neural Machine Translation with Universal Grammar
abstract
Machine translation usually relies on parallel corpora to provide parallel signals for training .
The advent of unsupervised machine translation has brought machine translation away from this reliance , though performance still lags behind traditional supervised machine translation .
In unsupervised machine translation , the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation .
Chomsky 's Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints .
Therefore , in this paper , we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models .
Through experiments on multiple typical language pairs , we demonstrate the effectiveness of our proposed approaches .
Introduction Recently , Neural Machine Translation ( NMT ) ( Bahdanau et al. , 2014 ; Sutskever et al. , 2014 ) has been greatly developed and become the dominant paradigm in machine translation .
On the one hand , the development of deep neural networks such as Transformer ( Vaswani et al. , 2017 ; Li et al. , 2021a ) has played a significant role in NMT 's improvements .
On the other hand , large-scale parallel corpora like the UN corpus ( Ziemski et al. , 2016 ) have also played an important role .
Despite the recent success of NMT in standard benchmarks , the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs , especially in low-resource language pairs ( Koehn and Knowles , 2017 ) .
Unsupervised Neural Machine Translation ( UNMT ) ( Artetxe et al. , 2018 b ) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner , relying on nothing but monolingual corpora .
Unsupervised machine translation does not need the parallel information from parallel sentences ; rather , it generally uses embedding alignments , initializes parameters with pretrained language models , and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training ( Lample et al. , 2018 a , c ; Sun et al. , 2019 ; Conneau and Lample , 2019 ; .
The pseudo parallel data created by iterative back -translation is the key to the success of unsupervised NMT model training ( Kim et al. , 2020 ) .
It takes advantage of the equivalence of translation languages to bring supervision ( albeit weak supervision ) to model training .
Recent results in semi-supervised NMT have demonstrated that further training a UNMT model with true bilingual parallel sentences can lead to better translation performance ( He et al. , 2016 ; Kim et al. , 2020 ; Conneau and Lample , 2019 ; Song et al. , 2019a ) , which suggests that after training , UNMT models are still not optimized because of their lack of explicit supervision .
Universal grammar ( UG ) is a notion in linguistics and philosophy that goes back at least to Roger Bacon 's observation , " in its substance , grammar is one and the same in all languages , even if it accidentally varies " ( Bacon , 1902 ) . Chomsky ( 1965 a , b) developed a universal grammar theory .
The idea of a universal grammar states that all human languages are species of a common genus because they have all been shaped by a factor that is common to all human beings ( Lappin and Shieber , 2007 ; Nivre , 2015 ) .
Therefore , in this paper , we leverage this grammar commonality to derive additional supervision to enhance UNMT training .
In other words , our proposed method is built on the existence of universal grammar .
If there is no crosslingual commonality and definitional similarity in the syntactic structure , then we will not be able to obtain weakly supervised signals for UNMT .
Specifically , we choose the grammar representation framework of constituent syntax as the research object .
Unlike typical approaches to leveraging syntax information , rather than adopting a syntactic encoder to enhance representations , we focus on acquiring more supervision by finding commonalities between two languages ' syntaxes and demonstrate this supervision by training UNMT models .
Since different languages often share some of the same constituent types ( syntax categories ) , predicting these matching constituents in model training can be used for a weak alignment .
As shown in Figure 1 , although the two sentences are not parallel , during the training , the model is exposed to both NP and VP constituents , and a weak alignment between these constituents can be used to enhance the UNMT training , i.e. , the NP constituents in English and the NP constituents in German ( the same to VP , PP , ... , etc. ) are more likely to be parallel .
Notably , our method is only an application of Universal Grammar in UNMT , but far from all applications since we only leverage a very small part of Universal Grammar ( universal constituent and syntactic label definition ) .
Masked Language Modeling ( MLM ) is a com-monly used training approach for language modeling .
In MLM , some of the tokens in the sentence are masked , and then the model is required to predict these masked tokens at their placeholders .
Based on MLM , we propose a CONSTMLM approach that also draws from constituent syntax .
In our CONSTMLM , constituents are masked , and the model is tasked with predicting both the tokens in a constituent and the constituent 's syntactic category .
Masking large constituents will present too difficult a problem for the model , as there will be insufficient context , so we also propose BTLM , a method of leveraging back -translation to provide more context and alleviate this issue .
We then implement CONSTBTLM based on the CONSTMLM , which leverage our proposed BTLM .
To accommodate both UNMT and language modeling training , we have prepared both encoder-decoder models and encoder-only models for our CONSTBTLM , BTLM , and CONSTMLM approaches .
In our experiments , we demonstrated the effectiveness of leveraging universal grammar and of our proposed approaches on multiple unsupervised translation tasks .
Our proposed approaches show consistent improvements compared to the baselines in these tasks .
We also present a significantly boosted performance on several low-resource semisupervised tasks .
These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models .
The Proposed Approaches
Background
We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L 1 and L 2 .
Our UNMT model follows an encoder-decoder architecture as in standard NMT .
We use a joint subword ( Sennrich et al. , 2016 b ) vocabulary shared between languages and share parameters between source ?
target and target ?
source models to take advantage of multilingualism ( Edwards , 2002 ) .
In this framework , three training methods are indispensable for the feasibility of unsupervised machine translation : initialization , denoising generation , and iterative back - translation .
UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather than separately in multiple phases .
The model is given monolingual data { X i } in language L 1 and { Y j } in language L 2 . | X | and | Y | are the number of sentences in monolingual data { X i } and { X j } , respectively .
Initialization Initialization is a crucial step for bootstrapping UNMT models .
The initialization process injects non-randomized cross - or multilingual knowledge into a UNMT model .
In general , two types of initialization are usually adopted ( Lample et al. , 2018 c ) .
The first entails initializing the embedding layer of a UNMT model with pre-trained embeddings , while the second uses a pre-trained language model with the same structure as the UNMT encoder to initialize the embedding layer and most of the neural network parameters in the encoder and decoder ( Conneau and Lample , 2019 ) .
The experimental performance in ( Conneau and Lample , 2019 ) shows that using a pre-trained language model to initialize a UNMT model can produce better performance , so we choose this as our method of initialization .
Denoising Generation
Denoising generation training aims to help UNMT models learn to generate fluent texts .
Noise is introduced to input sentences via replace , delete , and shuffle functions , and then the UNMT model is tasked with encoding these noisy sentences and using the encoded noisy sentences to reconstruct the original sentences .
The UNMT model is optimized by loss L D during this training process : LD = | X | i=1 ? log PL 1 ?L 1 ( Xi|N ( Xi ) , ? ) + | Y | j=1 ? log PL 2 ?L 2 ( Yj |N ( Yj ) , ? ) , ( 1 ) where N ( ? ) refers to the noise functions and ? represents the UNMT model parameters .
P L 1 ?L 1 and P L 2 ?L
2 denote the reconstruction probabilities in the languages L 1 and L 2 , respectively .
Iterative Back-translation Back-translation ( Sennrich et al. , 2016a ) was first proposed to boost translation performance using target -side monolingual data .
By using symmetric models , it can boost translation in both directions .
In UNMT , back - translation is used to synthesize pseudo parallel data from monolingual text , which alleviates the scarcity of true parallel data .
This synthesis is performed repeatedly throughout the UNMT training .
The loss , L B , is defined as follows : LB = | X | i=1 ? log PL 2 ?L 1 ( Xi|SL 1 ?L 2 ( Xi , ? ) , ? ) + | Y | j=1 ? log PL 1 ?L 2 ( Yj|SL 2 ?L 1 ( Yj , ? ) , ? ) , ( 2 ) where S L 1 ?L 2 and S L 2 ?L
1 represent the translation processes from L 1 to L 2 and L 2 to L 1 , respectively .
P L 1 ?L 2 and P L 2 ?L
1 denote the translation probabilities between the two languages .
Bidirectional Encoder [ ? ] [? ] ? ! ? " ? # ? $ [ ? ] [? ] [ ? ] ? ? % ? & ? ' ( a) CONSTMLM enc-only Bidirectional Encoder [ ? ] [? ] ? ! ? " ? # ? $ [ ? ] [? ] [ ? ] ? Autoregressive Decoder ? % ? & ? ' [ ? ] ? % ? & ( b) CONSTMLM enc-dec Bidirectional Encoder [ ? ] [? ] ? ! ? " ? # ? $ [ ? ] [ ? ] [ ? ] ? ? % ? & ? ' " ? ! " ? " " ? % " ? & " ? ' [ ? ] ( c ) CONSTBTLM enc-only Bidirectional Encoder [ ? ] [ ? ] " ? ! " ? " " ? & " ? ' " ? % Autoregressive Encoder [ ? ] [? ] ? ! ? " ? # ? $ [ ? ] [? ] [ ? ] ? ? % ? & ? ' ? # ? $ [ ? ] ? ! ? " ( d) CONSTBTLM enc-dec
CONSTMLM
We propose Constituent Masked Language Modeling ( CONSTMLM ) in this section .
ConstMLM is a variant of MLM that is enhanced with constituent syntax information .
In traditional MLM , given a sentence X = {x 1 , x 2 , ... , x n } , length of tokens n , and set of masked positions M , the training loss L MLM for the MLM training is : LMLM = | M | i=1 ? log P ( xM i | X \ M , ? ) ( 3 ) where | M | is the size of set M , and X \ M indicates the sequence after masking .
The masked positions set M consists of randomly sampled discrete positions , that is , M = TopK ( [ rand i ( 0 , 1 ) ] n i= 1 ) .
Here , TopK is a function that selects positions by probability until the masking budget has been spent .
In span- based MLM like ( Joshi et al. , 2020 ) , a span of length is first sampled from a geometric distribution ? Geo ( p ) , and the start position of a span is sampled in the same manner as in MLM , giving final masked span set M S = {( M i , i ) } .
In another linguistically guided language modeling approach , Zhou et al . ( 2020 b ) proposed Syntactic / Semantic Phrase Masking ( SPM ) for their model LIMIT - BERT .
In SPM , the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set .
Only the span boundary information , however , is used in SPM ; the linguistic label is ignored , so we remedy this and propose CONSTMLM .
In CONSTMLM , we first extract and filter the constituent span set CS = {( s , e , c ) i } m i=1 , where s , e , and c represent the start position , end position , and syntactic category , respectively .
During filtering , constituent parse trees with a span ratio greater than ? = /n are removed .
Random sampling is also performed on this set to obtain the masked span set .
Unlike SpanBERT and LIMIT - BERT , we only sample one span at a time because CONSTMLM not only predicts the masked token in the sampled span but also predicts the syntactic category of the sampled span .
CONSTMLM sums the loss from both the span 's syntactic category and the regular masked language model objective for each token in the masked span : ( 4 ) Since the UNMT model architecture , which includes both an encoder and a decoder , is different from pre-trained language models in general , we provide two implementations of CONSTMLM : encoder-only and encoder-decoder .
In the encoderonly CONSTMLM , the masked span 's token and syntactic category prediction are both performed on the encoder side , which is no different from popular pre-trained language models such as BERT that only consist of encoders .
Both target prediction probabilities are calculated using the following process : where enc ( ? ) represents the encoding process , and Pooling ( ? ) is a pooling operation that uses a firsttoken pooling strategy .
In the encoder-only CONSTMLM , only the encoder is updated by the loss ; the decoder can not benefit from it .
Using the same training method on the decoder as on the encoder is not viable ; because the decoder uses incremental self-attention instead of full self-attention .
To mitigate this , we propose an encoder-decoder CONSTMLM , in which the masked token prediction probability is calculated as : P ( xi|X \ s:e , ? ) = Softmax ( MLP ( dec ( [ BOS , Xs:e?1 ] , enc ( X \ s:e ) ) ) ) , where dec ( ? ) represents the decoding process , and [ BOS , X s:e?1 ] is the operation of prepending a BOS token before sequence X s:e?1 .
In encoderdecoder CONSTMLM , the encoder still handles the incomplete sentence encoding , so the syntactic category prediction is consistent with that of the encoder-only version .
This means that the weak alignment information brought by the syntactic category still directly trains the encoder , while the decoder is optimized by the span generation process .
BTLM and CONSTBTLM
Whether in traditional MLM or span-based MLM , the number of tokens masked is limited to a certain ratio of the sentence .
In BERT 's implementation , at most 15 % of the tokens are put up for masking .
SpanBERT followed this practice and after obtaining span lengths by sampling a geometric distribution skewed towards shorter spans , removed spans with a length greater than max = 10 .
Skewing towards shorter spans is crucial because of an issue in MLM : if too many tokens are masked , it is difficult for the model to recover these tokens using the remaining incomplete sentences .
Limiting the number of masked tokens is especially important for span- based MLM , as spans can compose much larger parts of the sentence .
We call this the difficulty of reasoning with insufficient information .
This situation is still acceptable for language model pre-training , and limiting the maximum ratio of masked tokens in MLM and the span length in span- based MLM alleviates the issue , but for linguistically - guided span- based MLM , the length of the extracted span cannot be flexibly set because it contains specific grammatical information .
Making the maximum span width too small means too few spans or even no spans for some trees are extracted .
To combat the difficulty of reasoning with insufficient information , we first propose Back-translation Language Modeling ( BTLM ) , a training method that can use crosslingual translation as a source of information for inference .
It can be formally presented as : LBTLM = e:s ? log P ( xi|X \ s:e , SL 1 ?L 2 ( X ) , ? ) , ( 5 ) In BTLM , the sentence X in language L 1 is first translated into language L 2 by S L 1 ?L
2 for use as cross-lingual context .
Then , X is masked as in MLM .
Finally , the target prediction is performed by combining and considering the cross-lingual context and the MLM context .
Due to the existence of a complete ( albeit noisy ) cross-lingual context , the proportion of masked spans in a sentence can be significantly increased .
In addition , this training forces the model to infer with a cross-language context , which implicitly promotes bilingual alignment .
Based on BTLM , as CONSTMLM was built on MLM , we propose Constituent Back-translation Language Modeling ( CONSTBTLM ) .
The loss of CONSTBTLM is calculated similarly to that of CONSTMLM : LCONSTBTLM = e:s ? log P ( xi|X \ s:e , SL 1 ?L 2 ( X ) , ? ) + ? log P ( c| X \ s:e , SL 1 ?L 2 ( X ) , ? ) .
We also implemented encoder-only and encoderdecoder versions with CONSTBTLM for different purposes .
In encoder-only CONSTBTLM , the target prediction probability becomes : P ( xi|X \ s:e , ? , ? ) = Softmax ( MLP ( enc ( [ ? , X \s:e ] ) ) ) , P ( c| X \s:e , ? , ? ) = Softmax ( MLP ( Pooling ( enc ( [ ? , X \s:e ] ) ) ) ) , where ? = S L 1 ?L 2 ( X ) , and [ ? , X \s:e ] indicates that the translated sequence ? is prepended to the rest of the sequence .
Purely from an implementation perspective , the use of cross-lingual context here is consistent with the TLM proposed in ( Conneau and Lample , 2019 ) , but the difference is that we only mask the input monolingual sequence , while TLM masks both the input parallel sentences .
Correspondingly , in the encoder-decoder CON - STBTLM , the probabilities are calculated as : P ( xi|X \ s:e , ? , ? ) = Softmax ( MLP ( dec ( [ BOS , X , EOS ] , enc ( ? ) ) ) ) , P ( c| X \s:e , ? , ? ) = Softmax ( MLP ( Pooling ( dec ( [ BOS , X , EOS ] , enc ( ? ) ) ) ) ) , where X is the language sequence after being masked , which is equivalent to X \s:e in meaning but keeps the same length as the original sentence .
[ BOS , X , EOS ] means that a BOS is prepended to X , and an EOS is appended .
Due to the incremental attention adopted by the decoder , the Pooling here must choose the last - token pooling strategy .
In CONSTMLM , the encoder handles predicting syntactic categories .
Although the encoderdecoder version is designed so that the decoder can also be updated during CONSTMLM training , it is still only responsible for the masked span sequence generation .
In the encoder-only version with CONSTBTLM , the encoder also handles the prediction of syntactic categories , but cross-lingual context is adopted to support larger span masking .
As for the encoderdecoder version , the encoder handles the crosslingual context and the decoder predicts syntactic categories and generates masked span text .
In CON-STMLM and the encoder-only CONSTBTLM , the weak alignment training of the syntactic category is performed on the source side , while it is completed on the target side in the encoder-decoder CON - STBTLM .
For detailed training process , please refer to Appendix A.1 .
Empirical Evaluation
Setup Following the XLM codebase 1 and model structure setup ( 6 stacked Transformer layers with hidden dimension size of 1024 ) of ( Conneau and Lample , 2019 ) , we train the baseline UNMT model with an embedding - shared Transformer encoderdecoder architecture .
The UNMT model training is divided into two stages : pre-training and unsupervised training .
Our method is only used in the second stage for fast convergence .
In order to make the unsupervised training more sufficient , we used an epoch size of 400 K instead of the original recommended 200 K in XLM .
The ? in CONSTMLM is set to 0.3 , and 0.5 in CONSTBTLM .
As the source of monolingual corpus for training , we use the 2007 - 2018 News Crawl dataset for English ( En ) , French ( Fr ) , German ( De ) , Romanian ( Ro ) , and Chinese ( Zh ) .
Since the Chinese News Crawl data is relatively small , we extracted sentences from Wikipedia dumps and converted them from traditional Chinese to simplified Chinese for use .
Joint Byte-Pair Encodings ( BPE ) ( Sennrich et al. , 2016a ) with 60 K merge operations were used in the translation experiments for all language pairs .
We explored the role of UG at two different monolingual corpus sizes in UNMT .
All monolingual data from the newstest 2008 - 2018 is combined for use in the large-scale setting , while a subset of 5 M sentences per language was randomly sampled from this data in the smaller scale setting .
Our evaluations were mainly carried out under unsupervised and low-resource semi-supervised scenarios .
In the unsupervised translation scenario , we reported results on WMT newstest2014 for En-Fr and En-Ro , WMT newstest2016 for En-De , and WMT newstest2020 for En-Zh .
In the low-resource semi-supervised translation scenario , the IWSLT '14 En-Fr and En- De parallel sentences were used for training .
IWSLT14.TED .dev2010 , tst2010 , tst2011 , and tst2012 were merged to evaluate the En- Fr translation model and dev2010 , dev2012 , tst2010 , tst2011 , and tst2012 in IWSLT14 .
TED to evaluate the En- De model .
To acquire constituent parse trees for monolingual sentences , we adopted the current state - of- theart Berkeley Neural Parser ( Kitaev and Klein , 2018 ) as our parsing model and trained an En parser using PTB ( Marcus et al. , 1993 ) , Fr and De parsers using the SPMRL14 multilingual constituent treebank ( Seddah et al. , 2014 ) , and a Zh Parser using CTB ( Xue et al. , 2005 ) .
Since a constituent treebank is not available in Ro and for the consistency of the constituent trees used in En-Ro UNMT , we created En and Ro pseudo-constituent treebanks by converting their respective UD 2.7 treebanks using Head Feature Princinple ( HFP ) ( Pollard and Sag , 1994 ) , and trained En * and Ro * parsers using this .
The processing and training details of each parser are presented in Appendix A.2 .
For each language , 500K sentences are parsed with these trained parsers for UNMT and low-resource semisupervised NMT enhancement .
Results and Analysis
The results of the UNMT experiment are mainly shown in Table 1 .
When a large-scale monolingual corpus is used , our baseline model outperforms XLM 's reported results .
This may be due to the use of the larger epoch size , which makes for more adequate training .
Based on our strong baseline model , the four implementations of our CONSTMLM and CONSTBTLM approaches achieve consistent improvements in all language pairs , which demonstrates the effectiveness of universal grammar in UNMT .
Based on the large-scale monolingual corpus scenario , comparing the four implementations of CONSTMLM and CONSTBTLM , we find that enc-only is generally weaker than the enc-dec implementation .
This shows that training the model as a whole is better than training part of the model .
This conclusion also partially explains the source of improvement of other enc-dec pre-training methods in UNMT like MASS ( Song et al. , 2019 b ) and BART ( Lewis et al. , 2020 ) .
In the small-scale monolingual training data scenario , the performance of the baseline model has a large decline compared with the large-scale monolingual scenario , which shows that the size of monolingual data is still an important factor in the performance of the UNMT model .
Similar to the largescale monolingual scenario , our CONSTMLM and CONSTBTLM achieve improvements in translation performance , and the maximum increase is even greater than that in the large-scale monolingual scenario .
This shows that in the case of relatively scarce training data , the introduction of universal grammar as a prior knowledge can effectively alleviate the performance loss .
Comparing the improved results in our approaches of each language pair horizontally , we find the average improvement of each language pair is basically consistent with the overlap of constituent labels between languages ; that is , En-De , En-Zh , and En * - Ro * are more improved than is En- Fr ( refer to Appendix 4.4 for the detailed statistics ) .
This shows that the more grammatical commonalities two languages have , the greater their alignment 's supervision will be .
In addition , compared to the recent state - of - art work - MASS , due to their focus on pre-training , while ours concentrate on the NMT training with weak parallel information from universal grammar , our contribution is orthogonal to theirs .
In Table 2 , we report the evaluation results of the low-resource semi-supervised scenario .
We use a small-scale , monolingually trained UNMT model as the basis , so we also include the results of the UNMT model evaluated on the test datasets directly .
After using the parallel data , the performance of our baseline model greatly improved , which reinforces our claim that UNMT models do not receive enough supervision in BT training .
With the use of universal grammar for enhancement , the CONSTMLM and CONSTBTLM enconly methods only achieved a slight improvement , which maybe suggest the training enhancement on the encoder side does not significantly improve the performance of translation after the introduction of parallel data .
In the enc-dec approaches , the encoder and decoder are jointly optimized , and the performance improvement is greater , especially in CONSTBTLM enc-dec when larger and more spans can be leveraged .
Ablation Study
Constituent trees and parallel data size
To show that UG plays a similar role to the alignment information given by the parallel corpus , we compare the semi-supervised and UG - enhanced UNMT ( UGUNMT ) settings .
The experimental results are evaluated on IWSLT '14 En-Fr .
In the semi-supervised setting , we vary the amount of parallel data , while we vary the number of monolingual parse trees in UGUNMT .
The performance trend is shown in Figure 3 .
The trends in the figure demonstrate that the performance of the UNMT model steadily improved with the addition of parallel corpus .
The performance changes for UGUNMT also had a similar trend with the increase in the constituent parse data .
This suggests that UG information plays a role similar to that of parallel data ; that is , it brings supervision signals .
The demand for monolingual constituent parse data , however , is greater than that from parallel data , and the improvement of parallel data is greater than that from constituent parses , which shows that UG can only provide a weak signal of supervision .
While UG cannot achieve the same effect as parallel data , it is quite useful when there is a lack of parallel data .
Different Maximum Span Ratios
As in our approach description , we propose BTLM and its variant with the goal of mitigating the difficulty of reasoning with insufficient information in MLM .
Although this problem has been noted in the training of PrLMs such as SpanBERT , in order to verify this problem 's presence in the UNMT model and show that our proposed BTLM alleviates this issue , we explored the effects of different maximum span ratios ? in UNMT training .
The results are shown in Table 3 .
The comparison shows that the higher ? is , the greater the utilization proportion of the phrases in the constituent trees is .
In CONSTMLM and CON - STBTLM , when ? is small , the phrases for training are limited , and therefore , the performance gains are limited .
With increased ? , the utilization proportion increases , but CONSTMLM struggles with reasoning with insufficient data because too many spans are masked , and the performance even declines compared to baseline .
CONSTBTLM can adapt to larger ? and higher phrase utilization proportions , it achieves better results .
Cross-lingual Alignment Evaluation
In order to verify that better alignment in the UNMT model is obtained using UG and our proposed training approaches , we conducted an experimental exploration of embedding alignment according to the experimental settings of ( Conneau and Lample , 2019 ) and evaluated models on the SemEval ' 17 En - De cross-lingual semantic word similarity task ( Camacho - Collados et al. , 2017 ) .
We adopted the same vocabulary size for Concat Fasttext ( Bojanowski et al. , 2016 ) , MUSE ( Alaux et al. , 2018 ) , and XLM baselines , and our best En- De UNMT model and extracted the embeddings for comparison .
The results are shown in Table 4 .
As the results show , our method is not only better than pure embedding training methods , Concat Fasttest and MUSE , on the three evaluation metrics , but also surpasses our strong XLM baseline , which demonstrates that the alignment of the UGUNMT model is indeed improved with the weak alignment information from syntactic categories .
Universal Constituent Labels
To illustrate the universal nature of the phrase grammar , we calculate statistics on the labels of the constituents in the annotations of each language .
Specifically , the proportions of shared and differing labels are also calculated .
The statistics are shown in Table 7 .
The statistical data shows that most of the grammatical phenomena ( constituent labels ) of the three language pairs overlap , and distributions of these labels are also close across language .
The proportions of common labels in En-De and En- Zh are greater than that in En- Fr.
Although En , Fr , De , and Zh have their own unique grammatical phenomena , they have greater proportions of overlapping labels than differing labels .
Since En and Ro are pseudo-constituent labels transformed from UD , they cannot be directly compared with En- Fr , En-De , and En-Zh , but they do also have many similar labels and comparable common label proportions , indicating the UD annotation 's universality and the effectiveness of our conversion in preserving grammatical features .
This does not explain more complicated issues such as language similarity or commonality but rather indicates the overlap of grammatical phenomena and universal features in the annotations and parser predictions .
Effects of SpanBERT , LIMIT - BERT , and CONSTBTLM for UNMT
From the main experiments , the UNMT performance is improved , especially for the small -scale data setting .
To find out that if the improvements are caused by CONSTMLM / CONSTBTLM and the syntactic information is really necessary , we compare our approaches with LIMIT - BERT which apply a linguistically guided span based MLM objective during UNMT training , and SpanBERT which is with a non-syntax based span masking strategy .
Compared with SpanBERT and LIMIT -BERT in our UNMT framework , the implementation is relatively simple .
By removing the syntactic category prediction objective in the CONSTMLM enc-only variant , it is consistent with the objective of LIMIT - BERT , and further removes the use of the syntactic parse tree in the span sampling , the same objective of SpanBERT is achieved .
The results of the comparison are shown in Table 7 .
The use of SpanBERT and LIMIT - BERT training approaches has resulted in a performance improvement in translation over the XLM baseline , which indicates that additional span-based pretraining is helpful for UNMT .
SpanBERT outperforms LIMIT -BERT because syntactic annotation is costly , the fixed - size syntactic parse tree used severely limits the pre-training with span boundaries considered only , while SpanBERT with dynamic span mask can get sufficient training .
But in ConMLM , this disadvantage was mitigated by the introduction of additional syntactic label predictions , and when we used the enc-dec variant , which is more suitable for encoder-decoder structures , its performance exceeded SpanBERT .
This suggests that it is not that syntactic information is useless .
With the help of ConstBTLM , a stronger variant , the UNMT model achieves much better translation results .
This demonstrates that in UNMT training on the one hand additional pre-training is helpful , on the other hand , the use of effective means to integrate the weak alignment information provided by syntactic parse tress is also beneficial to improve translation performance .
Related Work UNMT has been greatly developed in recent years ( Artetxe et al. , 2018 b ; Sun et al. , 2019 ; Conneau and Lample , 2019 ; Ren et al. , 2019 ) .
Syntax has been used extensively explored in supervised MT research field ( Wu et al. , 2018
Conclusion and Future Work
In this paper , we mine weak alignment information from universal grammar annotations and use it to improve unsupervised machine translation .
Two specific training approaches , CONSTMLM and CONSTBTLM , are proposed to apply this weak supervision .
Via empirical exploration on unsupervised and semi-supervised machine translation benchmarks , we verify that universal grammar will boost cross-lingual alignment for UNMT .
Our analysis shows that using universal grammar , the reliance on parallel corpora can be reduced under the premise of achieving the same effect because the weak supervision signal based on universal grammar can play a similar role to the supervision signal of the parallel corpus .
In this work , we rely on the dependency syntax of 100 + languages provided by the universal dependency project for synthesizing pseudo-constituent syntax in some languages .
In the future , we intend to train a multilingual parser based on the multilingual language model - XLM - R ( with the training data as a combination of 10 + language constituent syntax ) , which has the ability to parse 100 + languages in a single model , further increasing the practicality of our method .
In addition , we will examine more low-resource languages to verify the method 's universality .
Akiko Eriguchi , Yoshimasa Tsuruoka , and Kyunghyun Cho. 2017
