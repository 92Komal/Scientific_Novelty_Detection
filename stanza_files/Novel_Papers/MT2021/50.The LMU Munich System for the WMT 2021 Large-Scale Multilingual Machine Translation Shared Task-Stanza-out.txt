title
The LMU Munich System for the WMT 2021 Large -Scale Multilingual Machine Translation Shared Task
abstract
This paper describes the submission of LMU Munich to the WMT 2021 multilingual machine translation task for small track # 1 , which studies translation between 6 languages ( Croatian , Hungarian , Estonian , Serbian , Macedonian , English ) in 30 directions .
We investigate the extent to which bilingual translation systems can influence multilingual translation systems .
More specifically , we trained 30 bilingual translation systems , covering all language pairs , and used data augmentation techniques such as back -translation and knowledge distillation to improve the multilingual translation systems .
Our best translation system scores 5 to 6 BLEU higher than a strong baseline system provided by the organizers .
As seen in the Dynalab leaderboard , our submission is the only fully constrained submission that uses only the corpus provided by the organizers and does not use any pretrained models .
Introduction Neural Machine Translation ( NMT ) ( Vaswani et al. , 2017 ) has been shown to be effective with rich and in-domain bilingual parallel corpora .
Although the NMT model obtained promising performances for high resource language pairs , it is hardly feasible to train translation models for all directions of the language pairs since the training progress is time - and resource -consuming .
Recent work has shown the effectiveness of multilingual neural machine translation ( MNMT ) , which aims to handle the translation from multiple source languages into multiple target languages with a single unified model ( Johnson et al. , 2017 ; Aharoni et al. , 2019 ; Arivazhagan et al. , 2019 ; . The MNMT model dramatically reduces training and serving costs .
It is faster to train a MNMT model than to train bilingual models for all language pairs in both directions , and MNMT signif-icantly simplifies deployment in production systems ( Johnson et al. , 2017 ; Arivazhagan et al. , 2019 ) .
Further , parameter sharing across different languages encourages knowledge transfer , which improves low-resource translation directions and potentially enables zero-shot translation ( i.e. , direct translation of a language pair not seen during training ) ( Ha et al. , 2017 ; Gu et al. , 2019 ; Ji et al. , 2020 ; . We participate in the WMT 2021 multilingual machine translation task for small track # 1 .
The task aims to train a multilingual model to translate 5 Central / East European languages ( Croatian , Hungarian , Estonian , Serbian , Macedonian ) and English in 30 directions .
The multilingual systems presented in this paper are based on the standard paradigm of MNMT proposed by Johnson et al . ( 2017 ) , which prefixes the source sentence with a special token to indicate the desired target language and does not change the target sentence at all .
Language tags are typically used in MNMT to identify the language to translate to .
A language code , in the form of a two - or three - character identification such as en for English , is the main constituent of a language tag and is provided by the ISO 639 standard Compared with the other three submissions to the task , our submissions have the following advantages : ?
Our submissions are fully constrained , which means we using the data only provided by the organizer , and do not use models pre-trained on extra data .
is smaller than the other submissions .
Data
The training data provided by the organizers come from the public available Opus repository ( Tiedemann , 2012 ) , which contains data of mixed quality from a variety of domains ( WMT - News , TED , QED , OpenSubtitles , etc. ) .
In addition to the bilingual parallel corpora , in- domain Wikipedia monolingual data for each language is provided .
The validation and test sets are obtained from the Flores 101 evaluation benchmark , which consists of 3001 sentences extracted from English Wikipedia covering a variety of different topics and domains .
See Table 1 for details on data used for training our systems .
Data Preprocessing
To prepare the data for training , we used the following steps to process all of the corpora : 1 . The datasets were truecased and the punctuation was normalized with standard scripts from the Moses toolkit 2 ( Koehn et al. , 2007 ) .
2 . Sentences containing 50 % punctuation are removed .
3 . Duplicate sentences are removed .
4 . We used a language detection tool 3 ( langid ) to filter out sentences with mixed language .
5 . SentencePiece 4 ( Kudo and Richardson , 2018 ) was used to produce subword units .
We trained a model with 0.9995 character coverage to have sufficient coverage of characterbased languages .
6 . The length filtering removes sentences that are too long ( more than 250 subwords after segmentation with Sentencepiece ) , sentences with a mismatched length ratio ( more than 3.0 ) between source and target language are removed .
Data Selection Data selection ( Moore and Lewis , 2010 ; Axelrod et al. , 2011 ; Gasc ? et al. , 2012 ) , aims to select the most relevant sentences from the out-of- domain corpora , which improved the in-domain translation performance .
The training data provided by the organizers is large scale and contains multiple domains .
Therefore , the data selection becomes a key factor affecting the performance of MNMT .
Preliminary experiments ( see in Table 1 model # 3 and model # 4 ) showed that the performance of using all corpora provided by the organizer was poor .
Following the original paper , we selected three data sources ( CCAligned , MultiCCAligned , WikiMatrix ) for further experimentation .
Method Description
We first trained bilingual translation models with 30 directions for all language pairs .
Next , we trained a single multilingual model that can translate all language pairs .
Finally , we use back -translation and knowledge distillation technologies to further improve the performance of the multilingual translation system .
The details of these components are outlined next .
Bilingual NMT Model
We use Transformer ( Vaswani et al. , 2017 ) architecture for all bilingual models .
To achieve the best BLEU score on the validation dataset , random search was used to select the hyperparameters since the datasets are in different sizes .
We segment the data into subword units using SentencePiece jointly learned for all languages .
The details of selected hyper-parameters are listed in Section 4.1 .
Multilingual NMT Model
The multilingual model architecture is identical to the bilingual NMT model .
To train multilingual models , we used a simple modification to the source sentence proposed by Johnson et al . ( 2017 ) which introduce an artificial token at the beginning of the source sentence indicating the target language ( Johnson et al. , 2017 ) .
For instance , for the English -Macedonian ( en?mk ) translation direction , we insert a token like < 2 m k > at the beginning of all English sentences and do not change the Macedonian sentences .
Back Translation Back-translation ( BT ) ( Sennrich et al. , 2016 ) is a simple and effective data augmentation technique , which makes use of monolingual corpora and has proven to be effective .
Back - translation first trains a target - to - source system that is used to translate monolingual target data into source sentences , resulting in a pseudo-parallel corpus .
Then we mix the pseudo-parallel corpus with the authentic parallel data and train the the desired source - to- target translation system .
has shown how BT can be useful for multilingual MT .
After generating the pseudo parallel corpus , we tag our BT data by adding an artificial token < BT > at the beginning of the source sentence ( Caswell et al. , 2019 ) , which indicates that the data is generated by back - translation .
Knowledge Distillation Knowledge Distillation ( KD ) is a commonly used technique to improve model performance .
The standard KD training ( Kim and Rush , 2016 ) derives a student model from a teacher model by training the student model to mimic the outputs of the teacher .
We follow a recent approach to KD proposed by Wang et al . ( 2021 ) , which uses selection at the batch level and at the global level to choose suitable samples for distillation .
Experiments
Training Details
We use the Transformer architecture ( Vaswani et al. , 2017 ) as implemented in fairseq 5 ( Ott et al. , 2019 ) .
For training NMT and MNMT systems , we use the Transformer - Big architecture ( hidden state 1024 , feed - forward layer 4096 , 16 attention heads , 6 encoder layers , 6 decoder layers ) .
For optimization , we follow the default settings from the original paper ( Vaswani et al. , 2017 ) and used the Adam optimizer with a learning rate of 0.0003 .
To prevent overfitting , we applied a dropout of 0.3 on all layers .
At the time of inference , a beam search of size 5 is used to balance the decoding time and accuracy of the search .
The number of warm - up steps was set to 4000 and the vocabulary size is 133k .
In addition , we set a length penalty factor of 1.7 to maintain a balance between long and short sentences .
The batch size is set to 128 during decoding .
We trained our models for approximately 3 weeks on one machine with 8 NVIDIA GTX 2080 Ti 11GB GPUs .
Because of the problems of the international tokenization in the standard BLEU score , the organizers used sentence - piece BLEU ( spBLEU ) 6 as the official evaluation metric which operates on strings segmented using a Sentence - Piece model .
Recently , the BLEU score was criticized as an unreliable automatic metric ( Mathur et al. , 2020 ; Kocmi et al. , 2021 ) .
Therefore , we also evaluate our models using chrF ( Popovi ? , 2015 ) and BERTScore ( Zhang et al. , 2019 ) .
Systems
All of our systems described in Section 3.2 are listed as follows : Flores .
As a baseline system , we use the pretrained models public available by Flores teams .
We use flores 101_mm100_615 M tested on the devtest datasets as our baseline .
Bilingual .
We trained the bilingual models using standard Transformer - Big architecture for 6 languages in 30 directions .
The hyperparameters used are discussed in Section 4.1 .
Multilingual .
We trained the multilingual translation model using standard Transformer - Big architecture and a specific language token to indicate the desired translation target language .
Tagged BT .
We augment the training data by exploring the monolingual corpus using backtranslation proposed by Caswell et al . ( 2019 ) , with tagged back - translated source sentences with an extra token < BT >.
Selective KD .
We focused on selective knowledge distillation proposed by Wang et al . ( 2021 ) , which uses batch - level and global - level selections to pick suitable samples for distillation .
Results
The results of our systems on the devtest dataset are presented in Table 2 . For models 1 - 4 , we observed Table 2 : The automatic evaluation metrics on devtest data .
spBLEU , chrF , BERTScore denotes the average scores of spBLEU , chrF and BERTScore respectively , BEST BLEU denotes the language pair with the best BLEU score .
Systems with subscript whole denote the use of all data provided by the organizers , and systems with subscript select denote the use of data selection .
Model # 6 is our primary system submitted to the Dynalab leaderboard .
Systems 7 * and 8 * were trained after the shared task and were not used for the final submission .
Our best systems were outperformed by two other shared task submissions , which however used models pre-trained on additional data sources .
The performance grid of our best system ( Model # 8 * ) is presented in Figure 1 .
We see from the results that the sr-en language pair produced the best results in terms of spBLEU score while the hu-hr language pair scored the lowest .
Conclusions
In this paper , we presented the LMU Munich system for the WMT 2021 Large-scale Multilingual Translation shared task for small track # 1 .
The task evaluates translation between five central / eastern European languages and English , in total 30 translation directions .
The system we submitted was fully constrained , using only the data provided by the organizers and not using any pre-trained model .
The experiments show that back -translation and knowledge distillation techniques are effective for training multilingual machine translation systems .
Figure 1 : 1 Figure 1 : spBLEU scores on devtest data in 30 directions
1 ( International Organization for Standardization , nd ) .
Following ISO 639 standard , en indicates English , mk indicates Macedonian , sr indicates Serbian , et indicates Estonian , hr indicates Croatian and hu indicates Hungarian in this paper .
https://github.com/moses-smt/ mosesdecoder/blob/master/scripts / tokenizer 3 https://fasttext.cc/docs/en/ language-identification.html 4 https://github.com/google/ sentencepiece
https://github.com/pytorch/fairseq
https://github.com/ngoyal2707/ sacrebleu/tree/adding_spm_tokenized_bleu
