title
Results of the WMT21 Metrics Shared Task : Evaluating Metrics with Expert-based Human Evaluations on TED and News Domain
abstract
This paper presents the results of the WMT21 Metrics Shared Task .
Participants were asked to score the outputs of the translation systems competing in the WMT21 News Translation Task with automatic metrics on two different domains : news and TED talks .
All metrics were evaluated on how well they correlate at the system - and segment - level with human ratings .
Contrary to previous years ' editions , this year we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics ( MQM ) .
This setup had several advantages : ( i ) expert-based evaluation has been shown to be more reliable , ( ii ) we were able to evaluate all metrics on two different domains using translations of the same MT systems , ( iii ) we added 5 additional translations coming from the same system during system development .
In addition , we designed three challenge sets that evaluate the robustness of all automatic metrics .
We present an extensive analysis on how well metrics perform on three language pairs : English ?
German , English ?
Russian and Chinese ?
English .
We further show the impact of different reference translations on reference - based metrics and compare our expert-based MQM annotation with the DA scores acquired by WMT .
Introduction
The metrics shared task 1 has been a key component of WMT since 2008 , serving as a way to validate the use of automatic MT evaluation metrics and driving the development of new metrics .
We evaluate reference - based automatic metrics that score MT output by comparing the MT with a reference translation generated by human translators , who are instructed to translate " from scratch " without post-editing from MT .
In addition , we also invited 1 http://www.statmt.org/wmt21/ metrics-task.html submissions of reference-free metrics ( quality estimation metrics or QE metrics ) that compare MT outputs directly with the source segments .
All metrics are evaluated based on their agreement with human rating when scoring MT systems and human translations at the system or sentence level .
This year , we implemented several changes to the methodology that was followed in previous years editions of the task : ?
Expert-based human evaluation
This year , we collected our own human ratings for selected language pairs ( en?de , en?ru , zh?en ) from professional translators via MQM ( Lommel et al. , 2014 ) .
As shown before ( Freitag et al. , 2021 ) , this produces more reliable 2 scores when compared to the DA - based human ratings acquired by the WMT News - Translation task .
This step was necessary as Freitag et al . ( 2021 ) suggested that some automatic metrics already outperform ( taking MQM as the golden standard ) the DAbased human ratings that were usually used in the past for the metrics task and thus the DA - based ground - truth may be of lower quality than some of our submissions .
? Additional Training Data
We encouraged the participants to further fine tune or test their metrics on the already existing MQM annotations for newstest2020 ( Freitag et al. , 2021 ) 3 . ? Additional domain
Since we collected our own human ratings , we were also able to expand the domain of the test sets beyond news and evaluate the performance of the metrics on translations of the same MT systems on TED talks , in order to test the generalization power of metrics .
?
Additional MT systems
One use case for automatic metrics is choosing the better among different model versions of the same MT system during system development .
To address this scenario , in addition to the WMT submissions and online systems , we added extra development systems to the set of MT systems on which we evaluated the metrics .
?
Additional challenge sets
We generated three challenge sets containing specific translation errors that are believed to be challenging for automatic MT evaluation metrics to identify .
These challenge sets test metrics robustness on several different phenomena such as sentiment polarity , antonym replacement , named entities , among others .
?
Designated primary metrics Participants had to designate a single metric as their primary submission for each track ( reference - free and unconstrained ) .
Other submissions were permitted , but only the primary metric is included in the official main results .
?
Accuracy for ranking system pairs
We calculate a joint score across all language pairs and adopt the pairwise accuracy score for ranking system pairs to generate the final metric ranking ( Kocmi et al. , 2021 ) .
Our main findings are : ? WMT direct assessment ( DA ) scores generally correlate poorly with MQM scores , and exhibit weaker preference for human translations compared to machine output .
In particular for English ?
German and Chinese ?
English , the two human evaluations methodologies produce very different rankings ( Tables 16 and 18 ) .
In both language pairs , DA ranks the human translations below many MT systems , demonstrating again that expert-based evaluation is needed to generate a reliable ground truth for metric development for high quality language pairs .
?
The majority of automatic metrics correlate better with MQM than the DA scores from WMT .
This confirms the findings of Freitag et al . ( 2021 ) that automatic metrics are already more reliable than non-expert human evaluations .
A metrics task with ground truth ratings acquired by non-experts would consequently not be very helpful .
?
The performance of many metrics largely varies depending on the underlying domain ( being either news or TED talks ) , resulting in distinct clusters of winning metrics for these two domains .
All metrics of the winning cluster on the news domain show lower correlation with human ratings when switching to the TED talks domain ( Table 8 ) .
Lower ranked metrics are more robust and can sometimes even improve the correlation to humans on the TED domain .
?
Trainable embedding - based metrics are typically better at rating and correctly ranking ( with respect to MQM golden truth ) humangenerated translations .
( Table 8 ) . ?
Reference -free metrics , in particular COMET -QE and OpenKiwi perform very well when human translations are included in the setup .
Nevertheless , once we focus on MT output only , reference -free metrics perform worse compared to reference - based metrics ( Table 8 ) . ?
Reference - based metrics performance is significantly worse when reference translations contain major errors ( Table 13 ) . ?
Some metrics are more robust than others when presented with alternate reference translations ( Table 14 ) .
It is unclear so far what characterizes a good reference translation in addition to the clear requirement of fidelity of the translation to the source .
?
When counting top performances across different language pairs , granularities , and test conditions ( Table 12 ) , three embeddingbased metrics -C- SPECPN , BLEURT - 20 4 , and COMET -MQM _2021 - emerge as distinctly better than the others , especially at the segment level and when rating human translations .
Reference - free metrics are also relatively good at rating human translations , but under-perform at segment-level .
Metric performance is distributed more evenly on system- level tasks , especially when the test set is out-of- domain .
?
Most metrics struggle to accurately penalize translations with errors in reversing negation or sentiment polarity ( Table 9 ) . ?
Of the 14 linguistically motivated categories represented in the challenge sets , highperforming metrics have lower correlations for Subordination and Named Entities and Terminology ( Tables 10 and 11 ) . ? MQM annotations on TED data , both between annotation setups ( Google and Unbabel ) and between annotators themselves , show relatively low levels of agreement .
However , we note that many of the system rankings remain relatively consistent ; critically we note that the human reference comes out on top in both setups and that resulting metrics ranking is not significantly affected .
This indicates that whilst MQM is an attractive framework for evaluation , the annotation task itself is still subject to human disagreement , especially on challenging content .
TED talks in particular are highly specialized and ambiguous , presenting a unique challenge for annotators and evaluation .
Data Similar to the previous years ' editions , the source , reference texts , and MT system outputs for the metrics task are mainly derived from the WMT21 News Translation Task .
This year , we expand the domain and evaluate the same MT systems on an additional out - of- domain data - TED talks , for our three primary language pairs : English ?
German , English ?
Russian and Chinese ?
English .
In addition to the MT system outputs from the WMT evaluation campaign , we added translations from five additional MT systems that represent different versions of the same system during system development .
WMT Test Sets
The Newstest2021 set contains between 1000 and 2000 segments for each translation direction .
All test sets are from the news domain .
The reference translations provided for Newstest2021 were created in the same translation direction as the MT systems .
We have two reference translations for English ?
Russian and Chinese ?
English and four reference translations for English ?
German .
For more details regarding the news test sets , we refer the reader to the WMT21 news translation task findings paper .
TED Talks Test Suite
A long standing question about automated MT evaluation metrics has been whether metrics generalize and perform well across domains .
In the past , metrics were mostly tested on news translation evaluation .
The WMT2016 metrics shared task ( Bojar et al. , 2016 ) experimented on the IT and medical domains but the number of MT systems involved were small ( 2 - 10 in each translation direction ) .
Thus , there was insufficient statistical evidence collected for a detailed analysis on how well metrics perform in different domains .
In an attempt to conduct a detailed analysis on the robustness of metrics when evaluating translations in a domain other than news , we generated and provided an additional test suite for translation by the MT systems participating in the news translation task , consisting of transcriptions of TED talks .
The TED domain is quite different from the news domain , particularly in its more informal and disfluent language style , yet it covers a wide variety of topics and vocabularies .
The TED talk transcripts translation test set was extracted from OPUS 5 based on the corpus released by Reimers and Gurevych ( 2020 ) .
The English TED talk transcripts were translated by volunteers into multiple languages .
To minimize the problem of translationese as the source for the Chinese ?
English part of the test suite , we had a first- language Chinese speaker select talks with Chinese translations that were judged to be naturalsounding in Chinese .
( Unfortunately , there are still some problems in the translation quality for the Chinese ?
English part of the test suite which we will further discuss in Section 8.1.1 . )
Then , the same talks were extracted from the corpus to create the English ?
German and English ?
Russian parts of the test suite , where the translation was already available in the corpus and the quality of the translation was approved by professional translators .
Additional MT Output
One major use case for automatic metrics is choosing among different versions of the same system during system development .
We translated all test sets for all language pairs with five different versions of the same system which we call metricsystem { 1 , .. , 5 } .
The underlying NMT models are trained on unconstrained training data and the model variations include baseline models , finetuned models and models considering document context .
As we will see , the quality performance of these systems and their relative rankings can be quite different depending on the language pair , as these were not trained to yield the highest performance on the news or TED domain .
MQM Human Evaluation Automatic metrics are usually evaluated by measuring correlations with human ratings .
The quality of the underlying human ratings is critical and recent findings ( Freitag et al. , 2021 ) have shown that crowd-sourced human ratings are not reliable for high quality MT output .
Furthermore , an evaluation schema based on MQM ( Lommel et al. , 2014 ) which requires explicit error annotation is preferable to an evaluation schema that only asks raters for a single scalar value per translation .
Contrarily to the previous versions of the WMT metrics task , for our primary evaluation this year , we decided not to use the crowd- sourced DA human ratings from the WMT News Translation task , and conducted our own MQM - based human evaluation on a subset of submissions and a subset of language pairs that are most interesting for evaluating current metrics .
This not only had the advantage of more reliable ratings for a subset of language pairs , but also gave us the opportunity to run the same human evaluation on a different domain ( TED talks ) on output generated by the same MT systems , in order to test the generalization capabilities of the metrics .
MQM is a general framework that provides a hierarchy of translation errors which can be tailored to specific applications .
Google and Unbabel sponsored the human evaluation for this year 's metrics task for a subset of language pairs using either professional translators ( English ?
German , Chinese ?
English ) or trusted and trained raters ( English?
Russian ) .
The error annotation typology and guidelines used by Google 's and Unbabel 's annotators differs slightly and is described in the following two sections .
English ?
German and Chinese ?
English
Annotations for English ?
German and Chinese ?
English were sponsored and executed by Google , using 23 professional translators ( 14 for English ?
German , 9 for Chinese ?
English ) with access to the full document context .
Instead of assign a scalar value to each translation , annotators were instructed to " just " label error spans within each segment in a document , paying particular attention to document context .
Each error was highlighted in the text , and labeled with an error category and a severity .
To temper the effect of long segments , we imposed a maximum of five errors per segment , instructing raters to choose the five most severe errors for segments containing more errors .
Segments that are too badly garbled to permit reliable identification of individual errors are assigned a special Non-translation error .
Error severities are assigned independent of category , and consist of Major , Minor , and Neutral levels , corresponding respectively to actual translation or grammatical errors , smaller imperfections , and purely subjective opinions about the translation .
Since we are ultimately interested in scoring segments , we adopt the weighting scheme shown in Table 2 , in which segment - level scores can range from 0 ( perfect ) to 25 ( worst ) .
The final segment - level score is an average over scores from all annotators .
For more details , exact annotator instructions and a list of error categories , we refer the reader to Freitag et al . ( 2021 ) as the exact same setup was used for the WMT21 metrics task .
English ?
Russian Annotation for English ?
Russian was performed by Unbabel who used a single professional native language annotator with several years of translation error experience based on variations of the MQM framework ( Lommel et al. , 2014 ) .
For this task , Unbabel provided a proprietary variant of MQM , specifically tailored for Russian language annotation .
In a manner similar to the Google annotation , the annotator was given full document context and instructed to highlight spans of errors according to the categories specified in the typology .
As with the Google annotation , the annotator was also instructed to indicate error severity .
The Unbabel severity options differ slightly from that of Google in that we also specify a ' critical ' error severity and do not specify a ' neutral ' category .
Additionally , in the Unbabel typology , all error categories are weighted equally within each severity level .
MQM scores at a segment level are calculated by summing the number of errors in the segment in each severity and applying a severity weight as described in Table 3 .
In contrast to the Google scheme , Unbabel does not impose a limit on the number of errors in a segment .
We do , however , apply a normalization of the score by segment length .
The full score calculation is shown in Equation 1 below : MQM = 100 ? ( 1 ? 10 ? # critical + 5 ? # major + # minor # tokens ) ( 1 )
The same type of MQM annotations were previously used in the WMT QE shared tasks for the document- level subtask ( Fonseca et al. , 2019 ; Specia et al. , 2020a )
Human Evaluation Results
As discussed in Section 1 , we decided to run our own human evaluation in order to generate our golden - truth ratings and come to stronger conclusions about the quality of each automatic metric across two domains .
Unfortunately , this also meant that we were only able to evaluate a subset of documents of newstest2021 and TED talks .
In The results of the MQM human evaluation can be seen in Table 5 .
Most of the reference translations are ranked first , surpassing all MT systems , except for ref-B for zh?en TED talks and ref-A for en?de newstest 2021 .
This confirms the findings in Freitag et al . ( 2021 ) that when human evaluation is conducted by professional translators and MQM , high-quality human translations typically still outperform MT .
We will discuss the impact of the identified low-quality reference translations in Section 8.1.1 in more detail .
We wish to highlight one more important observation : the ranking of the MT systems is sharply different when switching from the commonly used Newstest 2021 test sets to TED talks .
This is particularly interesting for the metrics task , as metrics need to assess MT quality purely on the basis of the translations themselves and cannot rely on features that are specific to any particular MT system .
We will analyse the differences between Google 's and Unbabel 's MQM approach in Section 8.2 and compare our MQM human evaluation with the DA assessment from WMT in more detail in Section 8.3 .
Metric Submissions and Baselines
Baselines SacreBLEU baselines
We use the following metrics from the SacreBLEU v1.5.0 ( Post , 2018 ) as baselines , with the default parameters : ? BLEU ( Papineni et al. , 2002 ) Table 5 : MQM human evaluations for Newstest2021 and TED .
Lower average error counts represent higher MT quality for En?De and Zh?En ( using Google 's formulation of MQM ) , while higher scores represent higher quality for En? Ru ( using Unbabel 's MQM definition ) .
? TER ( Snover et al. , 2006 ) measures the number of edits ( insertions , deletions , shifts and substitutions ) required to transform the MT output to the reference .
As in BLEU , for TER we used SacreBLEU sentence_ter and corpus_ter functions ( with default arguments 7 ) to obtain segment - level and systemlevel scores .
? CHRF ( Popovi ? , 2015 ) uses character n-grams instead of word n-grams to compare the MT output with the reference .
For CHRF we used the SacreBLEU sentence_chrf function ( with default arguments 8 ) for segment - level scores and we average those scores to obtain a corpus-level score .
BERTscore BERTSCORE ( Zhang et al. , 2020 ) leverages contextual embeddings from pre-trained transformers to create soft-alignments between words in candidate and reference sentences using a cosine similarity .
Based on the alignment matrix , BERTSCORE returns a precision , recall and F1 score .
We used F1 without TF - IDF weighting .
Prism PRISM ( Thompson and Post , 2020 ) is an automatic MT metric which uses a sequence - tosequence paraphraser to score MT system outputs conditioned on their respective human references .
7 TER +lang .LANGPAIR+tok.tercom-nonorm-punct noasian-uncased+version.1.5.0 8 chrF2+lang.LANGPAIR-+numchars.6+space.false-+version.1.5.0 .
We used the default parameters with version 0.1 and model m 39v1 .
Submissions
The rest of this section summarizes participating metrics .
COMET All COMET * metrics ( Rei et al. , 2021 ) were built using the Estimator architecture presented in Rei et al . ( 2020 a , b) .
The difference between all the submitted metrics stem from : the data used for training , the size of the encoder model and whether or not they take advantage of the reference translation .
? COMET - DA_2020 is the same model submitted for last year 's shared task ( Rei et al. , 2020 b ; Mathur et al. , 2020 b ) while COMET - DA_2021 is a retrained version of the previous model that includes the DA udgements collected in 2020 .
? COMET -MQM_2021 is an MQM adaptation of the COMET - DA_2021 model that further trains for 1 additional epoch on MQM z-scores extracted from the MQM ratings for newstest2020 provided for the task this year .
?
Finally , COMET -QE-MQM_2021 and COMET -QE-DA_2021 are the referencefree versions of COMET -MQM_2021 and COMET - DA_2021 respectively .
From all the submitted models , the authors identified COMET -MQM _2021 and COMET -QE-MQM _2021 as their primary submissions to this years shared task edition .
OPENKIWI -MQM OPENKIWI -MQM ( Kepler et al. , 2019 ; Rei et al. , 2021 ) is a multitask model that estimates a sentence - level MQM score along with word- level OK / BAD tags .
The model is trained on top of XLM - RoBERTa using proprietary MQM data from several customer support domains .
While word- level QE typically tags each word with an OK / BAD tag depending on post-edition information ( Specia et al. , 2020a ) , the OK / BAD tags used in OPENKIWI - MQM are derived directly from MQM annotation spans ignoring error types and / or severities .
YISI YISI ( Lo , 2019 ) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources .
? YISI - 1 is a reference - based MT evaluation metric .
It measures the semantic similarity between a machine translation and human references by aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from pretrained language models ( e.g. BERT , CamemBERT , RoBERTa , XLM , XLM -RoBERTa , etc. ) .
? YISI - 2 is the bilingual , reference-less version for MT quality estimation .
It uses bilingual mappings of the contextual embeddings extracted from multilingual pretrained language models ( e.g. XLM - RoBERTa ) to evaluate the crosslingual lexical semantic similarity between the input and MT output .
YISI is an untrained metric and the submissions this year are the same as those in WMT20 .
The metric settings are described in Lo ( 2020 ) and Lo and Larkin ( 2020 ) . MTEQA MTEQA ( Krubi?ski et al. , 2021 a , b ) is an MT evaluation metric that leverages automatically generated questions and answers to assess the quality of MT systems .
It builds upon the assumption that a good translation should preserve all of the key information that one can extract from the reference .
Based on syntactic structure and NER system , they extract potential answers from the reference , and for each of them generate a human readable question .
They then use a question - answering system to provide a new ( test ) answer given the question and the MT output as the context .
The test answer is then compared to the reference answer to obtain the numerical score .
REGEMT REGEMT ( Stefanik et al. , 2021 ) is a family of ensemble metrics trained on MQM labels .
? { SRC , TGT}- REGEMT :
This ensemble combine selected metrics of surface - , syntacticand semantic -level similarity as input features to a regression model that estimates a quality assessment .
Some of these features are newly introduced and some are based on related work .
The reference - free ensemble uses as input features :
Source length , Target length , Contextual SCM , Contextual WMD , BERTScore , Prism and Compositionality the reference - base ensemble uses : COMET , BLEURT , BLEU , METEOR , Noncontextual SCM and WMD .
? REGEMT - BASELINE : This ensemble uses only Source length and Target length of the given texts , in characters
The authors identified { SRC , TGT }-REGEMT as their primary submissions .
ROBLEURT ROBLEURT ( Wan et al. , 2021 ) , short for Robustly Optimizing the training of BLEURT , is a model - based metric based on powerful language model XLM - RoBERTa .
The ROB -LEURT metric is constructed by the following steps : 1 ) jointly leveraging the advantages of source-included and reference-only metric models , 2 ) continuously pre-training the model with massive synthetic data produced by the real- world machine translation engines , and 3 ) fine -tuning the model with a data denoising strategy .
BLEURT BLEURT - 20 and BLEURT -21 - BETA are obtained by fine-tuning Rebalanced mBERT ( Chung et al. , 2021 ) ( a multilingual variant of BERT ) on a combination of two datasets : previous ratings from the WMT shared , task and generated data .
The generated data consists of " perfect " sentence pairs , obtained by copying the reference into the hypothesis , as well as " catastrophic " sentence pairs , obtained by randomly sampling tokens for each language pair .
The fine-tuning methodology is similar to ( Sellam et al. , 2020 ) . BLEURT - 20 was trained on human ratings from WMT metrics 2015 to 2019 ( z-scores ) using WMT20 for test , and BLEURT -21 - BETA was trained on WMT 2015 to 2020 .
The suffixes " - 20 " and " - 21 " denote the year of the WMT Metrics ratings that were used to build the test sets .
The authors identified BLEURT - 20 as their primary submission .
hLEPOR and cushLEPOR ? HLEPOR ( Han et al. , 2013 ) is an augmented metric with factors including enhanced sentence length penalty , precision , recall , and positional difference penalty which captures word order .
? CUSHLEPOR ( LM ) ( Han et al. , 2021 ) is a customized hLEPOR metric that uses LABSE pre-trained language model to automatically optimise hLEPOR parameters towards better correlation to human judgement and lower cost .
? CUSHLEPOR ( PSQM ) ( Han et al. , 2021 ) is trained and validated on the MQM and pSQM annotations from human professionals ( Freitag et al. , 2021 ) .
The tuned cushLEPOR achieves very high agreement to LABSE pretrained language model in performance but uses much less computational cost as a distilled model .
The authors identified CUSHLEPOR ( LM ) as their primary submission .
C-SPEC C-SPEC ( Takahashi et al. , 2021 ) is designed for both segment- level and system-level translation evaluation .
The authors ' objective was to design a better metric by detecting significant translation errors that would not be ignored in real instances of human evaluation .
Thus , pseudonegative examples are generated in which selected words in the translation are replaced with alternatives based on a Word Attribute Transfer , and a metric model is built to handle such serious translation errors ( denoted as C-SPECPN ) .
A multilingual large pretrained model is fine-tuned on the provided corpus of past years ' metrics task and fine-tuned again further on the synthetic negative samples that is derived from the same fine -tuned corpus .
The authors identified C-SPECPN as their primary submission .
MEE ? MEE ( Mukherjee et al. , 2020 ) is an automatic evaluation metric that leverages the similarity between embeddings of words in candidate and reference sentences to assess translation quality focusing mainly on adequacy .
Unigrams are matched based on their surface forms , root forms and meanings which aids to capture lexical , morphological and semantic equivalence .
Semantic evaluation is achieved by using pretrained fasttext embeddings provided by Facebook to calculate the word similarity score between the candidate and the reference words .
MEE computes evaluation scores using three modules namely exact match , root match and synonym match .
In each module , fmean-score is calculated using harmonic mean of precision and recall by assigning more weight to recall .
A final translation score is obtained by taking the average of fmean-scores from the individual modules .
The authors identified MEE2 as their primary submission .
Main Results
Currently , the main use case of automatic metrics is to rank systems either during system development or by comparing your own output with the one from other research institutes or competitors .
Consequently , we present system-level correlations as our main metric in this year 's WMT21 metrics task .
To be in line with the main use case , we present pairwise accuracy numbers for each metric that calculate the accuracy scores on binary comparison of system outputs for each language pair .
We refer the reader to Section 7 for language pair specific results on both the segment and system level with more traditional correlation metrics .
System - Level
The system-level metric scores submitted by the participants pertained to the complete WMT test set , but we collected human MQM scores for only a subset of documents , as shown in Table 4 .
To correct for this discrepancy , we re-computed systemlevel scores as averages over the segments for which MQM scores were available , after first verifying with all participants that their system - level scores were computed in the same fashion .
To generate a single score combining the data from all 3 language pairs , we calculate pairwise accuracy ( Kocmi et al. , 2021 ) as our primary scoring metric .
Pairwise accuracy is defined as follows :
For each language pair and system pair , we calculate the difference of the metric scores ( metric ? ) and the difference in average human judgements ( human ? ) for each system pair .
We calculate accuracy for a given metric as the number of rank agreements between the metric and human deltas , divided by the total number of comparisons : Pairwise accuracy = | sign( metric ? ) = sign( human ? ) |
| all system pairs | ( 2 ) We present results for three different settings :
Looking at the news domain with and without human translations ( HT ) as additional systems : ( a) Newstest2021 w/ o HT , ( b) Newstest 2021 w/ HT , and ( c ) looking at TED talks w/ o HT .
In this section , we consider only the primary submissions of each metric team and the baseline metrics .
We have multiple reference translations for some settings .
Instead of reporting results with respect to all reference translations , we use here for reference - based metrics only the single reference that was judged best by the MQM raters for each language pair .
The remaining reference translations are used in the role of participating MT systems in the " w/ HT " evaluations .
Table 7 summarizes the use of reference translations for different language pairs and domains .
We will analyse the impact of using dif-ferent reference translations in Section 8.1 in more detail .
? Newstest2021 w/ HT
When considering the additional reference translations as system outputs ( ref - A for zh?en , ref-B for en?ru , ref-A and ref- D for en?de ) , the ranking of the metrics is sharply revised .
The QE metric COMET -QE-MQM_2021 and the reference - based metric C-SPECpn are the winners in this setup .
Overall , the embedding - based metrics that also rely on fine-tuning are much better in rating human translation higher than MT output and thus dominate this setting .
? TED talks w/ o HT
This year , we also measured the domain robustness of each metric on the TED talks domain .
In Table 8 , we can see that COMET - MQM _2021 and YiSi - 1 show the highest correlation with human ratings on the TED domain .
Interestingly , both metrics did not fall into the first significance cluster in the previous two settings of the news domain , leading to very different conclusion about the quality of metrics .
Significance Testing
We run PERM - BOTH hypothesis test ( Deutsch et al. , 2021 ) on the pairwise system- level accuracy of Table 8 to measure significance between metrics ' performance .
10 Results can be seen in Figure 1 .
By looking at the heat map of Newstest2021 without human translations ( Newstest 2021 w/ o HT ) , we observe that the top performing metrics are not significantly different .
This observation changes when we add human translations to the setup ( New-stest2021 w / HT ) .
The top 2 performing metrics , although different ones , are significantly better than all other metrics .
This setup gives us the clearest result of all our 3 different setups and highlights that embedding - based metrics that are fine- tuned on previous years ' human ratings rate human translations much better than all the other metrics and are good at distinguishing human-produced text .
Another different situation can be seen when looking at the TED talk setting ( TED talks w/o HT ) .
Even though we see more significant differences compared to Newstest 2021 w/ o HT , most pairs of metrics are not significantly different .
Challenge Sets
While the correlation analysis is testing the evaluation metrics on their ability to rank MT systems according to translation quality , we are also interested in understanding metrics ' performance on identifying certain types of translation errors .
We created three challenge sets containing translation errors that are believed to be challenging for automatic MT evaluation metrics to identify .
A good metric should not only rank candidate translations by their quality but also be sufficiently sensitive to these types of errors .
Each challenge set consists of two MT outputs ( and the corresponding source and reference ) where one of them contains the type of translation error of interest and the other does not .
Metrics are expected to give a lower score to the MT output containing the error .
We use Kendall 's tau -like correlation , typically used for DARR ( Bojar et al. , 2017 ; Ma et al. , 2018 Ma et al. , , 2019
Mathur et al. , 2020 b ) , for evaluating the primary submissions on the challenge sets .
Kendall 's tau -like correlation is defined as follows : ? = Concordant ? Discordant Concordant + Discordant ( 3 ) where Concordant is the number of times a metric assigns a higher score to the MT output without the error and Discordant is the number of times a metric assigns a higher score to the MT output containing the error of interest .
Negation and Sentiment Polarity Challenge Set
The goal of this challenge set is to test metrics ' ability to penalize translations when there is a catastrophic error in reversing of a negation or of sentiment polarity .
It is a common phenomenon that MT systems may either introduce or remove a negation ( with or without an explicit negation word ) , or may reverse the sentiment polarity of the sentence ( e.g. a negative sentence becomes positive or viceversa ) .
These types of errors could result in serious consequences of misleading users of MT .
The WMT2020 MT Robustness shared task ( Specia et al. , 2020 b ) collected Wikipedia Edit comments with toxic content that could lead to possible tgt-regEMT Prism cushLEPOR ( LM ) C-SPECpn bleurt - 20 MEE2 BERTScore chrF BLEU YiSi-1 COMET-QE-MQM_2021 COMET -MQM_2021 TER OpenKiwi-MQM YiSi - 2 src-regEMT tgt-regEMT Prism cushLEPOR ( LM ) C-SPECpn bleurt - 20 MEE2 BERTScore chrF BLEU YiSi-1 COMET-QE-MQM_2021 COMET -MQM_2021 TER OpenKiwi-MQM YiSi - 2 src-regEMT ( a ) newstest2021 w/o HT C-SPECpn COMET-QE-MQM_2021 bleurt -20 OpenKiwi-MQM tgt-regEMT COMET -MQM_2021 Prism MEE2 cushLEPOR ( LM ) BERTScore chrF BLEU YiSi-1 TER YiSi - 2 src-regEMT C-SPECpn COMET-QE-MQM_2021 bleurt -20 OpenKiwi-MQM tgt-regEMT COMET -MQM_2021 Prism MEE2 cushLEPOR ( LM ) BERTScore chrF BLEU YiSi-1 TER YiSi - 2 src-regEMT ( b ) newstest2021 w/ HT COMET -MQM_2021 YiSi - 1 bleurt - 20 BLEU Prism BERTScore cushLEPOR ( LM ) MEE2 chrF C-SPECpn OpenKiwi-MQM COMET-QE-MQM_2021 tgt-regEMT TER YiSi - 2 src-regEMT COMET -MQM_2021 YiSi - 1 bleurt - 20 BLEU Prism BERTScore cushLEPOR ( LM ) MEE2 chrF C-SPECpn OpenKiwi-MQM COMET-QE-MQM_2021 tgt-regEMT TER YiSi - 2 src-regEMT ( c) TED talks w/ o HT Figure 1 : The results of running PERM - BOTH hypothesis test to find a significant difference between metrics ' pairwise system-level accuracy .
Dark squares mean the row metric correlates significantly better than the column metric at ? = 0.05 .
catastrophic errors in the MT output .
After selecting segments of interest they created reference translations for the entire test set using professional translators .
Finally , they collected annotations of catastrophic errors on the translations performed by participating systems 11 .
To test metrics on sentiment polarity we looked for source sentences from the English ?
German data portion where we can find an MT output annotated with a sentiment polarity error and another MT output without the polarity error .
The resulting challenge set contains 177 source sentences ( not necessarily distinct ) , each equipped with two MT outputs , one with a catastrophic error and one without it .
We note that most of the sentences in this challenge set contain toxic language .
Table 9 shows the results for this challenge set .
We also show the actual number of concordant pairs here because this challenge set is rather small .
Despite the high severity of the translation error in reversing the sentiment polarity or negation , we see that both the baselines and the submissions struggle to accurately discriminate between translations with and without such errors .
TER and BERTSCORE are the only two metrics that are able to achieve a medium correlation ( i.e. greater than 0.4 ) with human annotators on ranking the translation with the catastrophic error as lower in translation quality .
Perhaps more importantly , embedding - based and semantic-oriented metrics , such as BERTSCORE , YISI - 1 , etc. , do not significantly outperform surface - form matching metrics , such as TER , CHRF and SENT - BLEU .
This may indicate that the pretrained language models used by the embedding - based metrics are weak at learning language representations that explicitly reflect differences in negation and sentiment polarity .
Corrupted Reference Challenge Set
The goal of this challenge set is to sanity check the behaviour of the submitted metrics and possibly identify some weaknesses in detecting specific anomalies in a corrupted reference translation .
In order to do this we used this years ' Chinese ?
English Newstest corpus , which contains two human systems ( referenceA and refer-enceB ) and we perturb one of these human systems while using the other as reference .
Given that , our final corpus is composed of 14 , 080 tuples with From Table 10 we can observe that for most embedding based metrics ( YISI , BERTSCORE , BLEURT -21 - BETA , ROBLEURT , PRISM ) correlations are close to 1.0 for all perturbation types .
The only exceptions are COMET -MQM _2021 and C-SPECPN that seem to struggle with sentence omission and punctuation removal .
This behaviour is even more unexpected if we take into consideration that they seem to be sensitive to word omission .
Regarding punctuation removal , since both metrics are fine-tuned on Google MQM annotations ( see Section 3.1 ) we hypothesize that they learn to be less sensitive to punctuation errors .
Regarding the lexical metrics , we can observe that SENT - BLEU , CHRF and CUSHLEPOR ( LM ) are not sensitive to tokenized text .
This is an expected behaviour for lexical metrics since they typically ignore whitespaces .
Also , CUSHLEPOR ( LM ) scores ?1.0 in lowercased text .
This seems to indicate that this metric does not encode casing information .
German?
English Challenge Set
The challenge set is based on the test suite by Macketanz et al . ( 2018a ) .
It is a test suite for German- English that consists of around 5,500 German test sentences covering 107 linguistically motivated phenomena ( listed in Avramidis et al . ( 2020 ) ) , organized in 14 categories .
These phenomena do not follow a linguistic theory but rather cover various grammatical aspects which are relevant for MT .
Each phenomenon is represented by at least 20 test sentences to guarantee a balanced test set .
The test suite is used to evaluate MT systems with regard to their performance on the test sentences .
The evaluation operates semi-automatically and is based on a set of handwritten rules which contain regular expressions and fixed strings .
The test suite has been used to evaluate the outputs of 40 German- English systems submitted at the translation task of the Conference of Machine Translation ( WMT ) for three consecutive years ( Macketanz et al. , 2018 b ; Avramidis et al. , 2019 Avramidis et al. , , 2020 and also this year ( Macketanz et al. , 2021 ) .
Across the past three years , this amounts to 40 system outputs .
We use these outputs to construct the challenge items for the metrics task , since the test suite contains only source sentences and handwritten rules for the outputs but no reference translations .
For every source sentence of the test suite we separate MT outputs into " correct " and " incorrect " ones using the handwritten rules of the test suite and create a tuple including ; ( 1 ) a set of " correct " MT outputs , to be given to the metrics as supposedly correct reference translations and , ( 2 ) a pair of one " incorrect " and one " correct " translation randomly sampled from the respective set .
Note that the " correct " candidate does appear among the references ( 1 ) .
The goal of the metric is to score the " incorrect " translation worse than the " correct " one .
The same source sentence may be appear more than once , if there is more than one WMT translation marked as wrong by the rules for this item .
The above process resulted in a metrics challenge set with 1,819 items with source , wrong hypothesis , correct hypothesis , and a pseudo-reference ( another MT that was deemed correct for that phenomenon ) .
The covered phenomena are : Function Words ( FW ) , Non-verbal Agreement ( NVA ) , Verb Tense / Aspect / Mood ( VT ) , Composition ( Comp. ) , Multi-Word Expressions Negation ( MWE N. ) , Punctuation ( Punct . ) , Verb Valency ( VV ) , Subordination ( Sub. ) , Coordination and Ellipsis ( CE ) , Named Entities and Terminology and Long Distance Dependencies and Interrogative ( LDD ) .
Overall , from Table 11 we observe that embedding - based metrics such as BLEURT - 20 and COMET - MQM _2021 seem to be less sensitive to Subordination , Named Entities and Terminology , and to Punctuation .
We can also observe a clear performance difference between referencefree and reference - based metrics .
Nonetheless most metrics have positive correlations in all covered phenomena .
Note that this corpus is composed of " pseudo- references " which can have a negative impact on metrics ' performance ( see Section 8.1 ) .
Results per Language Pair
We computed individual correlation results for each focus language pair ( English ?
German , English?
Russian , Chinese ?
English ) at both the system and segment level .
The system-level metric scores submitted by the participants pertained to the complete WMT test set , but we collected human MQM scores for only a subset of documents , as shown in Table 4 .
To correct for this discrepancy , we re-computed system-level scores as averages over the segments for which MQM scores were available , after first verifying with all participants that their system - level scores were computed in the same fashion .
13 Exceptions to this pattern are the baseline metrics BLEU and TER : the systemlevel versions of these metrics are not averages over segment - level scores , and we computed them only over the MQM segments .
Since we have multiple reference translations for the focus language pairs , we required participants to submit versions of their ( reference - based ) metrics for each reference .
We used only the scores corresponding to the reference that was judged best by the MQM raters for each language pair .
For the news domain , we evaluated metric performance both when using only MT outputs and using MT outputs augmented by human references , adding all remaining references in the latter condition except for English ?
German , where we excluded reference B since it was very similar to the best reference C.
Table 7 summarizes the use of reference translations for different language pairs and domains .
We measure correlation using the Pearson -r statistic at the system level and the Kendall - tau statistic at the segment level .
Pearson correlation is complementary to the pairwise accuracy used for our global results as discussed in Section 5 : it tests linear fit with MQM scores , a stringent but 4 3 3 COMET -MQM_2021 10 3 3 4 3 7 3 2 5 tgt- regEMT 4 1 1 2 3 1 2 1 1 COMET-QE-MQM_2021 3 1 1 1 3 3 OpenKiwi-MQM 3 2 1 3 1 2 RoBLEURT * 3 3 1 2 1 2 cushLEPOR ( LM ) 2 1 1 2 1 1 BERTScore 2 1 1 2 1 1 Prism 2 2 2 1 1 YiSi-1 2 2 2 1 1 MEE2 2 2 2 1 1 BLEU 1 1 1 1 hLEPOR 1 1 1 1 MTEQA * 1 1 1 1 TER 1 1 1 1 chrF 1 1 1 1 Table 12 : Summary of language-specific results .
Numbers give the count of times each primary metric occurred in the top cluster for the specified condition .
Metrics not being among the winners in any competition are not listed .
Reference -free metrics are indicated by italics .
All submissions labelled with * participated only in 1 or 2 language pairs .
reasonable criterion since we expect these scores to conform to a linear scale ( for example , a translation with two minor errors is twice as bad as one with only a single error ) .
Pearson has well -known drawbacks ( Mathur et al. , 2020a ) , notably sensitivity to outliers , which we avoided by choosing only relatively high - performing systems .
In preliminary tests , Pearson also yielded a larger number of pairwise significant differences among metrics than Kendall , an important property since our fairly small number of systems makes it difficult to reli-ably distinguish metrics at the system level .
Segment- level scores-metric or human- are naturally arranged as a system ? segment matrix ( rows ? columns ) .
There are several ways to extract vectors for input to correlation statistics .
Comparing metric and human row vectors corresponds to a use case of judging the relative quality of different segments output by a given MT system ( " where is my system making mistakes on this test set ? " ) ; comparing column vectors corresponds to judging the relative quality of outputs for a given source segment across different MT systems ( " which systems performed better or worse than mine on this segment ? " ) .
To avoid emphasizing either of these scenarios at the expense of the other , we flattened the metric and human score matrices into single vectors ( row1 , row2 , ... ) before comparing them .
This measures the metrics ' ability to assign independent scores to MT segments , abstracting away from system or source segment , and provides a large number of comparisons to boost statistical significance .
We used Kendall rather than Pearson correlation for robustness to segment- level noise .
14
The results for each language pair and granularity are shown in Tables 23 to 28 , with corresponding pairwise significance plots derived using the PERM - BOTH test in Figures 2 to 7 .
The tables contain results for all metrics ; the significance plots include only primary and baseline metrics .
In the tables , primary submissions are in bold , baseline metrics are underlined , and metrics that used only the source have " - src " appended to their name .
For each condition ( news without human translations , news with human translations , or TED ) , the scores of primary and baseline metrics in the top cluster are in bold .
The top cluster consists of primary or baseline metrics that are not significantly outperformed by other primary or baseline metrics nor outperformed by a primary or baseline metric not in the top cluster .
15
In the significance plots , this corresponds to the leftmost block of columns containing no dark squares .
Table 12 summarizes all results in this section by counting the number of times each metric occurs in the top cluster ( it got a " win " ) , summed across different ways of partitioning the results .
This synthesis is fairly crude , since it treats all conditions as equally important .
Also , membership in the top cluster is likely to be subject to high statistical variance , and metrics that fall outside this cluster are not accounted for ; in particular , those that sometimes perform very poorly are not penalized .
Nevertheless , the counts permit some general 14 Our use of Kendall differs in two major aspects from the " Kendall- like " statistic used for segment - level correlations in previous editions of the WMT metrics task : we do not threshold MQM score differences , as we consider them to be more reliable than DA scores ; and we compare all pairs of scores over complete flattened matrices rather than comparing pairs of scores in each column , and micro-averaging results across columns .
15 Note that this definition is different from the metric clustering used in previous metrics tasks , in which every metric in a cluster must be significantly better than all metrics in lower clusters .
observations .
In terms of total " wins " , three metrics stand out clearly : C-SPECPN , BLEURT - 20 , and COMET - MQM _2021 .
These have fairly evenlydistributed performance across languages , granularities , and data conditions , with the exception of BLEURT - 20 , which does relatively poorly on Chinese ?
English .
Their advantage over other metrics is most pronounced at the segment level and when human translations are included among the systems to be judged ( w / HT ) - both of which are more challenging tasks .
In contrast , the distribution of metrics that achieve top-level performance is much broader for system- level granularity , the outof-domain TED setting , and to a lesser extent the news w/o HT setting .
Two metrics that do not use a reference translation - COMET -QE-MQM_2021src and OpenKiwi-MQM - src-do surprisingly well overall , particularly in the w/ HT condition , but perform poorly at the segment level .
This could be explained by these metrics benefiting from their ability to distinguish human vs. machine produced text .
Finally , the surface- level baselines - BLEU , TER , and chrF - join the winners exclusively at the system level and almost exclusively in the out-ofdomain TED condition .
Additional Results
Impact of Reference Translation
The quality of the reference translation can have a higher impact on the correlation to human ratings than the actual choice of metric .
For all our different test sets and language pairs , we consequently included all reference translations in our human evaluation to ( a ) assure that we have reference translation with high quality and ( b ) to choose the best reference translation for our main results .
In this section , we present two interesting observations by looking into the Chinese ?
English TED talks and the English ?
German news setups .
zh?en TED
We started by having only one reference translation for all TED talks .
Unfortunately , the MQM evaluation revealed that the reference translation ref-A for Chinese ?
English was ranked last - lower than all the MT systems - and that it contained on average more than one major error ( = 5 MQM points ) per segment .
We spot checked the errors and agreed that the reference translation indeed contained many errors .
We then decided to acquire a new reference translation ( ref - B ) which turned out to be better than all MT systems after running a human evaluation .
The impact of using an excellent versus an inaccurate human reference translation can be seen in Table 13 .
All metrics achieve an accuracy score lower than 0.5 when using ref-A to calculate their scores .
This means that the metrics would perform worse than by chance .
By switching to ref-B , all but one metric ( tgt - regEMT ) greatly improve their correlation score .
This demonstrates once again that metrics become unreliable when they are provided with inaccurate reference translations .
en?de Newstest2021 For English ?
German Newstest2021 , we started with two reference translations ( ref-A and ref - B ) .
Both reference translations had issues : ref-A was ranked lower than two MT systems ( see Table 5 ) and we agreed with that assessment after spot checking the errors .
ref-B had high- levels of overlap with the Online -W MT system and is most likely a post-edited translation of Online -W .
Google vs. Unbabel MQM
Given that annotations were undertaken for English ?
Russian using a different setup and MQM scheme than those for English ?
German and English ?
Chinese we sought to provide some insight into the compatibility of the two schemes by repeating the annotation for English ?
German using Unbabel 's scheme and annotator pool :
For a subset of 5056 segments of the TED talk data for English ?
German from 10 MT systems , Unbabel had another expert annotator trained on MQM provide annotations using their proprietary typology .
MQM was calculated for each set of annotations ( using their respective scoring ) and the latter were then converted to a sequence of OK / BAD tags as a means of evaluating the level of agreement between the two annotations at a token level .
The Pearson 's r correlation score between the two sets of MQM annotations was found to be 0.212 , significant to p<0.05 .
Given the levels of correlation of metrics with Google 's MQM scores on the full set of English ?
German , this is surprisingly low .
Similarly , Cohen 's Kappa on the annotated tags was found to be 0.165 .
Not only do scores correlate poorly but agreement at the tag level is also fairly weak .
Equally , Cohen 's Kappa on the subset of annotations on which both sets of annotators found some error was found to be improved but still low ( 0.2 ) .
This indicates that even when limited to erroneous sentences , the annotators struggled to agree on where the errors were .
We note that the Google annotators left 59.5 % of the sample untouched ( i.e. error free ) , whereas the Unbabel annotator left only 46.9 % untouched .
It appears that the Unbabel annotator was on average more aggressive in their annotation which might partially explain low levels of agreement .
A number of the MT systems often produced the same translation of the same source text .
With this in mind , and given that Google used a pool of annotators , we were able to also compare annotations within the Google set .
For every source / target pair with more than two annotations we calculated and averaged the pairwise Cohen 's Kappa .
The mean Kappa across all of these segments was 0.21 , which suggests equally low levels of agreement between Google annotators .
Despite low segment level agreement we note that the ranking of systems remains fairly consistent between annotation schemes with a few outlying exceptions .
Table 15 details the rankings for our sample across annotation schemes .
In particular it is encouraging to note that the human reference ( albeit one of the worse ones , see Section 8.1.2 ) is ranked first in both cases ; at a high level both schemes are making meaningful quality judgements .
For the sake of completeness , we similarly examined the rankings of metrics at segment level ( measuring Pearson 's r correlation score and ranking the result ) against both sets of MQM scores for our sample .
Rankings in both cases were found to be sufficiently similar to official results reported in this paper and no metric moved more than three positions .
To rationalize these low segment - level agreement numbers , we asked an independent native language German speaker to look at a subset of annotations where we noticed the worst levels of segment - level agreement .
The independent rater provided some rudimentary annotation of the most obvious errors and some qualitative analysis of the segments themselves .
From this independent analysis , we were able to conclude at a high- level that the nature of TED talk text broken into segments is highly complex , context dependent and ambiguous even in the original language which resulted in equally ambiguous translation errors .
This serves as a harsh reminder of the complexity of the annotation task and that inevitably even human annotation using highly granular schemes like MQM is only as reliable as the simplicity of the underlying text .
The same reminder extends to human-generated references where highly specialized content will inevitably require specialized translators to ensure the most accurate translation .
We note that whilst we do not have human direct assessment ( DA ) scores on TED data in order to provide a direct comparison of the two annotation schemes in this setting , we observe in the following section that MQM appears to provide a more stable basis for evaluation in general .
Comparison to WMT Scoring
The WMT evaluation campaign ( Akhbardeh et al. , 2021 ) ran a human direct assessment ( DA ) evaluation for the primary submissions in the news domain for all language pairs .
Segment - level ratings with document context ( SR + DC ) on a 0 - 100 scale were collected either using source - based evaluation with a mix of researchers / translators ( for translations out of English ) or reference - based evaluation with crowd-workers ( for translations into English ) .
In general , for each MT system , only a subset of documents receive ratings , with the rated subset differing across systems .
System- level DA scores are averages over the available segment - level scores .
Both raw scores and per-rater z-normalized versions of the scores are provided .
Appendix C contains correlations to WMT Newstest DA scores for all metrics , at both segment and system level , for all 16 language pairs .
There is significant variation in metric performance and ranking across languages , although a general pattern is that correlations are substantially higher for out - of - English pairs than into-English .
Although the WMT correlations are not strictly comparable to the MQM results in previous sections , MQM scores tend to correlate somewhat better with metric scores for two of our three focus languages ( English ?
German and Chinese ?
English ) , and somewhat worse for English ?
Russian .
Tables 16 to 18 compare MQM and DA scores for our focus language pairs , on all systems where both sets of scores were available .
Notably , MQM scores rank human translations at or near the top more consistently than do DA scores .
The only reference ranked worse than MT by MQM is ref-A for English ?
German , which as discussed above is a low-quality translation .
In contrast , DA z-normalized scores rank all references below at least one MT system except for ref-A in English ?
Russian , which is ranked first , in agreement with MQM .
For English ?
German and English ?
Russian , MQM correlates better with raw DA scores than with z-normalized scores ; Pearson correlations are 0.508 versus 0.243 for the former and 0.911 versus 0.898 for the latter .
For Chinese ?
English the pattern reverses , with correlations of 0.216 versus 0.729 .
The correlations between MQM and WMT DA scores in the previous section motivated us to investigate how DA scores would fare in comparison to automatic metric scores when using MQM as gold scores .
We computed system-level Pearson correlations using z-normalized DA scores for MT outputs only and MT outputs augmented with human references for which DA , MQM , and metric scores were all available .
16 Tables 19 to 21 compare these to the performance of primary and baseline metrics using the references from Table 7 . 17
The performance of DA varies across languages : for English ?
German and English ?
Russian it ranks roughly among the bottom half of the automatic metrics ; while for Chinese ?
English it ranks third .
DA scores tend to perform better when judging human output , ranking 7th , 3rd , and 3rd for English ?
German , English ?
Russian , and Chinese ?
English , respectively .
WMT DA as a Metric
Conclusion
This paper summarized the results of the WMT21 shared task on automated machine translation eval -
Overall , metrics perform very differently based on domain , language pair or setting ( with or without human translations among candidate systems ) making it hard to declare a clear winner .
When counting top performances across all test conditions , three embeddingbased metrics -C- SPECPN , BLEURT - 20 , and COMET -MQM _2021 - emerge as distinctly better than the others , especially at the segment level and when rating human translations .
Nevertheless , it is unclear which test scenario and correlation metric is best to yield reliable results .
We would encourage the community to investigate different ways of how to evaluate automatic metrics .
We are very open to apply new suggestions in the next round of the Metrics Shared Task .
Another challenge is to define the overall ground truth ( i.e. the human ratings ) .
Even though , we are convinced that expert-based ratings via MQM are more reliable , we also found that the two MQM methodologies of Unbabel and Google disagree for some systems .
We would encourage the community to further work on establishing a reliable human evaluation setup .
The field would benefit from a reliable human evaluation standard that could be used by everyone .
