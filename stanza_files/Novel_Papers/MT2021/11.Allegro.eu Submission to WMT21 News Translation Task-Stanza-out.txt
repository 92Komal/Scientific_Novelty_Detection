title
Allegro.eu submission to WMT21 News Translation Task
abstract
This paper describes Allegro.eu submission for the WMT21 news translation shared task .
We focus on exploring data filtering and data augmenting methods .
We submitted two single-directional models , one for English ?
Icelandic direction and other for Icelandic ?
English direction .
Our news translation system is based on the transformer - big architecture , it makes use of corpora filtering , back - translation and forward translation applied to parallel and monolingual data alike .
Introduction
We participated in the WMT21 news translation shared task for English ?
Icelandic language pair .
It is a medium-resource regime with under 10 M parallel sentences .
In our experiments we focused on two approaches for improving translation system : data filtering methods inspired by work of ( J?nsson et al. , 2020 ) and data augmentation methods like back - translation or self-training ( Edunov et al. , 2018 ; Sennrich et al. , 2016 ; He et al. , 2019 ) .
We tried to use bi-directional translation models but single -directional proved to be better .
We also tried to make use of pretraining on monolingual corpora , but it also was unsuccessful .
Krubi?ski et al. ( 2020 ) showed in their ablation study that pretraining is the most successful for low-resource regimes under 1 M parallel sentences .
Data
Data Preprocessing
We removed malformed utf - 8 encodings , normalized text with NFKC Unicode normalization form , unescaped HTML , removed control characters and converted different whitespaces to a basic space character .
Data Filtering
We took part in a constrained track for the English ?
Icelandic language pair for the news translation task .
We used similar heuristic for filtering monolingual and parallel data .
A proper sentence pair should fulfil these criteria :
For each sentence separately : ? length in chars ? ( 10 , 500 ) ? length in words ? ( 2 , 100 ) ? average word length in chars < 12 ? max word length in chars < 28 ? digit ratio < 0.15 ? outside alphabet ratio < 0.015 ? language detection probability >
0.9 Criteria calculated on a sentence pair : ? no digit sequence mismatch ?
Levenshtein distance > 5 ? Poisson based length logprob > - 10 For language identification we used the CLD2 library .
We arrived at these threshold values by analyzing outliers of clean corpora : newsdev2021 development dataset and J?nsson 's cleaned ParIce corpus ( J?nsson et al. , 2020 ) .
Our filtering procedure is inspired by J?nsson 's and extracts 72 % of the same sentences they extracted from the raw ParIce corpus ( Barkarson and Steingr?msson , 2019 ) .
Each heuristic removes up to 5 % of lines from those clean corpora , when all thresholds would be applied they would remove around 9 % from the cleaned ParIce corpus .
For all available raw parallel corpora this procedure would remove 35 % of sentences .
Table 1 shows sizes of raw and filtered corpora available in the constrained track .
Poisson based length filtering
This section describes an improved method of filtering sentences based on their lengths .
A simple ratio of sentence lengths is a common method , but it is often too strict for short sentences and too loose for longer ones .
We are using a simple assumption , that the distribution of lengths of expected translation is given by the Poisson distribution with a mean equal to a length of the source sentence .
This type of length filtering is used by bicleaner framework ( S?nchez - Cartagena et al. , 2018 ) .
We use a correction factor scl = 1.04 , which is a ratio of chars in the English side to the Icelandic side for the whole parallel corpus .
We multiply source length by it or by its reciprocal before calculating probabilities , depending on the context .
Figure 1 compares this method with a ratio-based heuristic where the allowable ratio range is ( 0.5 , 2 ) .
For this language pair the correction factor is close to 1.0 , but for other language pairs it can deviate more , which can lead to bias when using a simple ratiobased heuristic .
Translation postprocessing
Our system has a tendency to generate the same quotation as in source text .
Therefore , before submitting our translations for evaluation , we applied simple regular expressions to fix quoting .
We made sure that only ( " " ) for English submission was used and for Icelandic we made sure that ( " " ) was used .
3 System overview
All of our models are based on the Transformer big architecture , as described in Vaswani et al . ( 2017 ) .
For training we used OpenNMT - py framework ( Klein et al. , 2017 ) together with sentencepiece tokenizer ( Kudo and Richardson , 2018 ) unigram model of size 32 k with full character coverage .
We trained models on A100 GPU for 210k steps with a batch of 8192 tokens which amounts to around 12h per model .
We used half -precision and tied embeddings .
For optimization we used Adam ( Kingma and Ba , 2014 ) , with a linear warmup for learning rate for 15 k steps up to 0.0005 and inverse square root decay afterwards .
Additionally , all of our models were randomly initialized .
Results Results are presented in Table 3 .
We trained a tokenizer on a cleaned ParIce corpus .
A baseline model we trained on all available parallel corpora and achieved 18.1 BLEU in English ?
Icelandic direction and 24.0 BLEU in Icelandic ?
English direction .
Data filtering impact
We ran 4 variants with the same parameters as described at the beginning of section 3 , but only for 100k steps .
We compared the translation quality of models trained with filtered training corpus and the impact of cleaning data used in training tokenizer .
We used the aforementioned cleaned ParIce corpus ( J?nsson et al. , 2020 ) to train the tokenizer .
Table 2 presents the results of this comparison .
Back - translation of monolingual corpora
We took 10 M monolingual sentences for each language and filtered them as described in section 2.2 .
For English we took only News Crawl from 2020 , for Icelandic we used News Crawl 2020 and also Icelandic Gigaword to obtain full 10 M sentences .
We translated the English source to Icelandic , then translated it back to English .
Back - translation of parallel corpora
We used this newly acquired model to translate the Icelandic side of clean parallel corpus to English and likewise filtered by GLEU score for the English side of the corpus , finally we extracted 75 % of most similar pairs .
It is interesting to note that 11 % of translations were the same as the English side of the parallel corpus .
We then created a corpus for training English ?
Icelandic model , this time with typical setup for back - translation where original sentences were used as a target : ? 4 M of clean parallel corpus
Then , analogously , we used this model to translate the other side of the clean parallel corpora and filter by GLEU score .
It is interesting to note that also 11 % of translations was the same as the Icelandic side of the parallel corpus .
We then created a corpus and trained Icelandic ?
English model which achieves 27.2 BLEU on the development set .
For this direction our final system was an ensemble of this new model and previous best .
Denoising
As it has been recently demonstrated by Raffel et al . ( 2020 ) , transfer learning can be successfully applied to sequence - to-sequences models .
Therefore , we tried doing unsupervised de-noising pre-training based on provided monolingual data .
We experimented with three different denoising schemes : ?
Token - based masked language modeling ( Devlin et al. , 2019 ) ?
Whole Word Masking objective inspired by BERT models released in May 2019 ? BART - like denoising with text infilling and sentence permutation ( Lewis et al. , 2020 )
We tried it in two regimes .
One where we pretrain model and then finetune it on translation downstream task .
The other where we train both denoising and translation objectives simultaneously .
However , we did n't observe any benefits from doing this .
The reason for this is unknown .
Conclusion
This paper describes Allegro.eu submission for the WMT21 news translation shared task .
We took part in constrained track for the English ?
Icelandic language pair only .
Participation in this task allowed us to deepen the understanding of filtering methods common in NMT .
The experiments demonstrated the importance of data filtering in medium -resource regime machine translation .
In this regime , less data but of higher quality can lead to superior results .
Figure 1 : 1 Figure 1 : Distribution of lengths of parallel corpora .
As depicted , Poisson - based heuristic allows more variation for shorter sentences and lower variation in length for longer ones .
