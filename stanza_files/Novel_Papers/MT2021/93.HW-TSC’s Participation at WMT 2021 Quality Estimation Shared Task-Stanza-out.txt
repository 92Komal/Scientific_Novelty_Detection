title
HW - TSC's Participation at WMT 2021 Quality Estimation Shared Task
abstract
This paper presents our work in WMT 2021 Quality Estimation ( QE ) Shared Task .
We participated in all of the three sub-tasks , including Sentence - Level Direct Assessment ( DA ) task , Word and Sentence - Level Post-editing Effort task and Critical Error Detection task , in all language pairs .
Our systems employ the framework of Predictor-Estimator , concretely with a pre-trained XLM - Roberta as Predictor and task -specific classifier or regressor as Estimator .
For all tasks , we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of multitask learning or encoding it with predictors directly .
Moreover , in zero-shot setting , our data augmentation strategy based on Monte- Carlo Dropout brings up significant improvement on DA sub-task .
Notably , our submissions achieve remarkable results over all tasks .
* Indicates equal contribution .
1 http://www.statmt.org/wmt21/quality-estimationtask.html
Introduction Quality Estimation ( QE ) focuses on estimating the quality of machine translation ( MT ) system output when no ground truth reference is available ( Specia et al. , 2018 ) .
QE covers wide range of tasks including word-level , sentence - level and document-level .
It has wide range of applications in MT quality check and post-editing effort estimation .
In WMT2021 Quality Estimation shared task 1 , there are three sub tasks - Sentence -Level Direct Assessment task , Word and Sentence -Level Postediting Effort task and Critical Error Detection task .
Each sub task involves several language pairs .
Our team participated in all the above three tasks over all language pairs .
We summarized our main contributions as follow : ?
We employ Predictor-Estimator architecture ( Kim et al. , 2017 b ; Kim and Lee , 2016 ) which is a two -stage model consisting of a word prediction model trained from large-scale parallel corpora , and a estimation model trained from quality - annotated QE data .
Different from the original Predictor-Estimator model in ( Kim et al. , 2017a ) , we use pre-trained XLM - Roberta large as predictor instead of RNN - based model to achieve better QE features , and use task -specific classifier or regressor as quality estimator .
?
We extend PE assisted QE ( PEAQE ) ( Kepler et al. , 2019 ; Wang et al. , 2020 ) by integrating real PE or addtional high-quality translation in the way of multitask learning or directly encoding it with predictor .
?
We explore data augmentation method based on Monte Carlo ( MC ) dropout ( Gal and Ghahramani , 2016 ) to enhance the performance of zero-shot language pairs in Direct Assessment ( DA ) task .
Our methods achieve impressive performance on both word and sentence level tasks .
Specifically , we peak the top - 1 on sentence - level DA over English - German and English - Japanese pairs .
For word and sentence - level post-editing effort task , our submissions of the majority language pairs obtain the best Pearson 's correlation or Matthews correlation coefficient .
We also win the first place in critical error detection task in English - Chinese and English - Japanese .
We will describe the tasks , datasets , and our methods for DA task , post-editing task , and critical error detection task in section 2 , section 3 , and section 4 respectively .
Section 5 presents details of our experimental setup and results , with a brief discussion and conclusion in the end .
2 Sentence -Level Direct Assessment Task
Task Description
The sentence- level Direct Assessment task focuses on estimating sentence - level translation quality scores which are annotated with Direct Assessment ( DA ) scores by professional translators .
The original DA scores are in scale of 0 - 100 .
The scores are then standardised using the z-score by rater .
The goal is to estimate a z-standardised DA score for each translation sentence .
Sentence - level DA task is evaluated by Pearson 's correlation between the predicted score and the gold human annotated z-standardised DA score .
The system is assessed from two aspects : single language pair and multilingual track which takes all languages into account , including zero-shot pairs , calculating the averaged Pearson correlation overall .
Dataset
For each language , 7000 , 1000 and 1000 sentence pairs are provided officially as training , development and test20 set before releasing another 1000 for the real blind test21 , including highresource English - German ( En - De ) and English - Chinese ( En - Zh ) , medium -resource Romanian - English ( Ro -En ) and Estonian - english ( Et- En ) , lowresource Sinhalese -English ( Si-En ) and Nepalese -English ( Ne-En ) , as well as Russian - English ( Ru-En ) .
Besides , 4 language pairs - English - Czech ( En- Cz ) , English - Japanese ( En-Ja ) , Pashto-English ( Ps- En ) and Khmer-English ( Km- En ) , are only offered blind test ( 1000 ) , without training data .
Implemented Systems
The systems for DA employ Predictor-Estimator architecture .
Following previous sota works ( Fomicheva et al. , 2020 ; Moura et al. , 2020 ; Rei et al. , 2020 ) , we use a pre-trained XLM - Roberta ( XLM - R ) ( Conneau et al. , 2019 ) model as a predictor due to its impressive performance on crosslingual downstream tasks .
Practically , we concatenate source ( SRC ) and target ( MT ) sentences in the format [ CLS ] SRC [ EOS ] [ SEP ] MT [ EOS ] following XLM -R usage , and take the embedding of pooled output of [ CLS ] token as features of a sentence pair .
For Estimator , we simply stack two -layer FFN , taking the [ CLS ] feature generated above as the input to predict sentence - level DA scores .
PE Assisted Sentence - Level DA Prediction Inspired by the Pseudo - PE techniques ( Kepler et al. , 2019 ; Wang et al. , 2020 ) , we take full use of postediting sentences provided in Post-editing Effort task through multitask learning .
The model jointly learns to score ( SRC , MT ) pair in a regression task , and distinguish between translations and postedited sentences - which is the better translation in a classification task .
In inference stage , the model only conducts regression task to predict DA score , as post-editing sentences are not available for blind test set .
The regression task applies loss function as : L reg = ( ?( E s , t ) ?
Y human ) 2 ( 1 ) where E s,t is the embedding of sentence pair ( source , mt ) , ? is the regressor taking them as input , through a two -layer FFN to compute DA score , and Y human is the Z-normalized DA score annotated by human .
The classification task forces the model to capture more expressive cross-lingual sentence representation which is paramount for DA score .
In implementation , we get the model to learn which is the pair with better translation between embedding of concatenated source and target E s,t and embedding of concatenated source and PE E s , p .
We splice two vectors in random order and apply two stacked FFN layers to compute classification result , in which 0 means the former pair is the better ( i.e. the former contains PE ) , 1 means the former is the worse and 2 means translation and post-edit are exactly the same .
Equation ( 2 ) gives the loss function for the classification task , where M is the number of classes ( M = 3 ) , Y is the binary indicator ( 0 , 1 , 2 ) if class label c is the correct classification for observations , P is the model predicted probabilities that the observation is of classes .
L cls = ?
M ?1 c=0 Y c log ( P c ) ( 2 )
Data Augmentation for Zero-shot Languages Instead of directly applying the multilingual DA model trained on other 7 language pairs to zeroshot languages , we exploit a data augmentation strategy based on MC dropout to improve the performance .
Specifically , we compute the expectation and variance for the set of estimated DA scores of zero-shot languages obtained by performing N ( N=30 ) stochastic forward passes through the welltrained but dropout-perturbed QE model .
In order to control the uncertainty introduced by the disturbance , we only retain dropout in estimator and last two layers in XLM -R .
We take variance as an indicator to detect observations with less uncertainty and use expectation as DA score label .
Then , we mix the generated zero-shot DA data with randomly selected non-zero-shot training set to fine - tune the model .
Experiments show that our data augmentation is effective to improve the performance , achieving better Pearson correlation .
3 Word and Sentence -Level Post-editing Effort Task Sentence - Level QE predicts the Human Translation Error Rate ( HTER ) .
HTER is the ratio between the number of edits ( insertions / deletions / replacements ) needed and the reference translation length .
The evaluation metrics of the sentence - level task is Pearson 's correlation metric .
Dataset
The dataset in these task provides the same source and translation as DA task , with an extra postedit sentence for each observation and task -specific token - level and sentence - level labels .
Besides , we generate addition -translation sentence ( AMT ) for each source sentence by using well - trained machine translation systems .
The motivation here is to add an additional criterion which is in the same language as the provided translation sentence .
We suppose that to detect the difference between two sentence in the same language is a simpler task for model .
There are some important label properties to highlight : ?
The number of BAD tags and OK tags is imbalanced , especially for GAP tags .
?
AMT 's BLEU score is significantly lower than MT taking post-edits as reference .
Its average HTER is higher than MT .
It indicates that the generated AMT is less closer to post-edits than MT .
Method
The systems for QE shared task2 also employ Predictor-Estimator architecture ( Kim et al. , 2017 b ) . Predictor .
Similar to Task1 , we use pre-trained XLM - Roberta ( XLM - R ) model as predictor after fine- tuning it with mask language modeling task ( Devlin et al. , 2018 ) using the provided source and PE sentences .
In order to improve the performance , refers to approach in ( Wang et al. , 2020 ) , we concatenate SRC , MT , AMT sentences together in the format of [ BOS ] SRC [ EOS ] [ SEP ] MT [ EOS ] [ SEP ] AMT [ EOS ] .
We notate the predictor as f ; SRC , MT and AMT text as X and Y and Z , corresponding features as H x , H y , H z respectively : H x , H y , H z = f ( X , Y , Z ) , ( 3 ) Estimator .
We utilise 4 independent 2 - layers FFN including binary three classification tasks to predict SRC word tags , MT / AMT word tags , MT / AMT gap tags respectively , and a regression task to predict HTER score of MT / AMT .
All predictions are obtained by performing specific transformations ?.
We define the predicted logits of SRC word , MT word , MT gap , AMT word , AMT gap as V xw , V yw , Vyg , V zw , Vzg ; and HTER predicted score of MT and AMT as V yh , V zh .
The estimator can be described as : V xw = ? xw ( H x ) , V yw = ? w ( H y ) , V zw = ? w ( H z ) , Vyg = ?
g ( f cat ( H y , V yw ) ) , Vzg = ? g ( f cat ( H z , V zw ) ) , V yh = ?
h ( f gap ( f cat ( H y , V yw , Vyg ) ) ) , V zh = ?
h ( f gap ( f cat ( H z , V zw , Vzg ) ) ) , ( 4 ) where f cat is the concatenate method in the last dimension , f gap is the global average pooling in the second dimension ignoring padding tokens in a batch just like ( Lin et al. , 2013 ) 3.2 . Loss .
We prepend and append two special < pad > labels to the original word label sequence , append a special < pad > label to the original gap label sequence during training , but loss of the padded labels is not computed .
For all classification tasks , to deal with the problem of imbalance between OK and BAD number , we use weighted cross entropy as the loss function , and the weight is calculated as w i = N C i , where w i is the inverse of the proportion of the instance with class C i .
For sentencelevel HTER score loss , we use mean squared error ( MSE ) as the loss function .
We define the tags of SRC word , MT word , MT gap , AMT word , AMT gap as V xw , V yw , V yg , V zw , V zg ; and HTER score of MT and AMT as V yh , V zh .
The model is trained under the multi-task learning framework by summing up the loss of all subtasks with specific weights : loss = ? ?{ xw , yw, yg , zw , zg} ? logP ( V? | X , Y , Z ) + ? ?{hy , hz} ? ( V? ? V? ) 2 , ( 5 ) where xw , yw , yg , zw , zg represents for classification tasks , hy , hz represents for regression tasks , ? is the weight of loss for a specific task .
The multi-task framework can improve the overall performance .
4 Critical Error Detection
Task Description
This is a new QE task focusing on predicting sentence - level binary scores indicating whether or not a translation contains ( at least one ) critical error .
The key point is to identify whether the translation will lead to misleading or more serious consequences , e.g. the translation involves critical mistranslation , hallucination or critical content deletion .
Only binary prediction ( whether or not any critical error contained ) is required .
The evaluation metrics of this task is also the MCC .
Dataset
The dataset contains 4 languages which are English - German , English - Chinese , English - least one catastrophic error in the translation .
It is noticed that the number of NOT and ERR tag is imbalanced .
Methods Similar as the above two tasks , our baseline system takes pre-trained XLM - R as predictor , stacked FFN layers as binary classifier .
We also experimented with replacing XLM -R by mBART ( Liu et al. , 2020 ) and replacing FFN layers with TextCNN , Bi-LSTM and other types of network .
Based on the intuition that the semantic difference between two monolingual sentences are easier to distinguish than that of two cross-lingual sentences , we propose to incorporate a " good " MT of the source sentence into ( src. mt ) pair during training , so that the auxiliary information provided by the " good " MT can help the model to directly compare mt with MT +src , instead of only depending on cross-lingual src .
With consideration of expensive overhead of manual translation , we assume that au- Voting - Based Ensemble .
Finally , we ensemble several models and take their majority voting as prediction results .
Experimental Results 5.1 Task1 : Sentence-level Direct Assessment Experimental Settings
Our system is implemented with hugging face transformers package .
The pre-trained xlm-roberta- large model which has approximately 550M parameters is taken as pre-dictor .
We train the predictor and the estimator together on the multilingual QE DA dataset using Adam ( Kingma and Ba , 2015 ) as optimizer with constant learning rate of 1e ?6 and training batch size of 16 .
The model is trained on a Nvidia Tesla V100 GPU .
Results
Table 1 shows the results on test20 set .
Our baseline is the system described in section 2.3 .
+ multitask method is introduced in section 2.3.1 .
To achieve more competitive scores while also maintain a relatively small number of parameters , we ensemble our result with MC dropout approach , that is to run N ( N=50 ) pass forwards with dropout and take the expectation of the N predictions as final answers .
Results
Table 3 shows the results on dev and test21 set .
Our baseline is the QE system without AMT data .
+ AMT method is the QE system with AMT data .
In the experiments , we generate 3 different kinds of AMT data with the machine translation system trained for the WMT2021 Machine Translation of News Shared Task , Baidu Fanyi 4 and Google Translate 5 . For each kind of AMT , we run N ( N=10 ) pass forward with dropout =0.1 using the a unified model trained with all AMT together .
The expectations of 3N predictions of score and token labels is taken as the final answers .
Task3 : Critical Error Detection
Table 4 shows the results of our system on development and blind test set .
Experiments show that the best results obtained when applying XLMR - Large and FFN layer on development set .
The involvement of AMT also brings significant improvement over all language pairs .
For ensemble settings , we ensemble multiple models with different pre-trained models and classification layers using voting - based method as introduced in section 4.3 .
In order to solve the problem of label imbalance , we also investigate different label weights when computing cross-entropy loss .
Due to the large gap between the number of NOT and ERR labels in the dataset , the weights ( NOT : ERR ) are clipped as 1:6 , 1:4 , 1:5 , 1:15 for enzh , ende , encs , enja .
Meanwhile , to better fit the data in the test set and avoid over-fitting , we utilise dropout with rate of 0.1 and weight decay of 1e ?5 .
Conclusion
We present our work on WMT 2021 QE shared task in this paper .
For all the three tasks to estimate sentence - level DA , token and sentence - level post-edit effort and sentence - level critical error , we employ predictor-estimator framework as our baseline .
To further boost performance , we investigate the usage of additional high-quality translations .
For task1 , we mainly focus on introducing postedits with multi-task learning .
Also , the effect of data augmentation method based on MC dropout is studied here to improve the result of zero-shot pairs .
For task 2 and 3 , we generate high-quality translations for each observation using multiple welltrained machine translation systems .
By directly concatenating AMT with the original source and target sentence then encoding it with pre-trained predictor , we achieved remarkable results over all language pairs and tasks .
In future , we will continue to invest time and effort on studying the effect of involving additional translations into QE tasks , for example , how the additional translation quality will affect QE performance , what the better ways are to incorporate additional translations in .
Table 2 : 2 Pearson correlation between prediction of our system and human DA judgement on test21 set .
Language Baseline + Multitask + Ensemble En-De 0.490 0.552 0.547 En-Zh 0.494 0.502 0.519 Ro- En 0.886 0.897 0.902 Et-En 0.798 0.805 0.814 Ne-En 0.776 0.789 0.801 Si-En 0.648 0.677 0.675 Ru-En 0.761 0.787 0.787 Average 0.693 0.716 0.721
Table 1 : Pearson correlation between prediction of our system and human DA judgement of non-zero-shot lan- guage pairs on test20 set .
Language Baseline + AugData + All En-De 0.481 / 0.584 En-Zh 0.523 / 0.583 Ro - En 0.878 / 0.901 Et- En 0.775 / 0.808 Ne-En 0.810 / 0.858 Si-En 0.564 / 0.581 Ru-En 0.753 / 0.787 En-Cz 0.546 0.557 0.573 En -Ja 0.297 0.349 0.364 Ps - En 0.592 0.622 0.622 Km -En 0.661 0.653 0.659 Multilingual 0.621 / 0.665 Czech , English - Japenese .
7000 training , 1000 validation , and 1000 blind test sentence pairs are available for each language .
Ground truth label has two classes , NOT means no catastrophic error , and ERR means at
