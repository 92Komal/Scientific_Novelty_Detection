title
G-Transformer for Document-level Machine Translation
abstract
Document - level MT models are still far from satisfactory .
Existing work extend translation unit from single sentence to multiple sentences .
However , study shows that when we further enlarge the translation unit to a whole document , supervised training of Transformer can fail .
In this paper , we find such failure is not caused by overfitting , but by sticking around local minima during training .
Our analysis shows that the increased complexity of target - to -source attention is a reason for the failure .
As a solution , we propose G-Transformer , introducing locality assumption as an inductive bias into Transformer , reducing the hypothesis space of the attention from target to source .
Experiments show that G-Transformer converges faster and more stably than Transformer , achieving new state - of - the - art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets .
Introduction Document - level machine translation ( MT ) has received increasing research attention ( Gong et al. , 2011 ; Hardmeier et al. , 2013 ; Garcia et al. , 2015 ; Miculicich et al. , 2018a ; Maruf et al. , 2019 ; . It is a more practically useful task compared to sentence - level MT because typical inputs in MT applications are text documents rather than individual sentences .
A salient difference between document- level MT and sentence - level MT is that for the former , much larger inter-sentential context should be considered when translating each sentence , which include discourse structures such as anaphora , lexical cohesion , etc .
Studies show that human translators consider such contexts when conducting document translation ( Hardmeier , 2014 ; L?ubli et al. , 2018 ) .
Despite that neural models achieve competitive performances on sentence - * * Corresponding author .
level MT , the performance of document- level MT is still far from satisfactory .
Existing methods can be mainly classified into two categories .
The first category translates a document sentence by sentence using a sequence - tosequence neural model ( Zhang et al. , 2018 ; Miculicich et al. , 2018 b ; Maruf et al. , 2019 ; Zheng et al. , 2020 ) .
Document - level context is integrated into sentence -translation by introducing additional context encoder .
The structure of such a model is shown in Figure 1 ( a ) .
These methods suffer from two limitations .
First , the context needs to be encoded separately for translating each sentence , which adds to the runtime complexity .
Second , more importantly , information exchange cannot be made between the current sentence and its document context in the same encoding module .
The second category extends the translation unit from a single sentence to multiple sentences ( Tiedemann and Scherrer , 2017 ; Agrawal et al. , 2018 ; and the whole document ( Junczys - Dowmunt , 2019 ; . Recently , it has been shown that when the translation unit increases from one sentence to four sentences , the performance improves Scherrer et al. , 2019 ) .
However , when the whole document is encoded as a single unit for sequence to sequence translation , direct supervised training has been shown to fail .
As a solution , either large-scale pre-training or data augmentation ( Junczys - Dowmunt , 2019 ) has been used as a solution , leading to improved performance .
These methods are shown in Figure 1 ( b ) .
One limitation of such methods is that they require much more training time due to the necessity of data augmentation .
Intuitively , encoding the whole input document as a single unit allows the best integration of context information when translating the current sentence .
However , little work has been done investigating the underlying reason why it is difficult to train such a document- level NMT model .
One remote clue is that as the input sequence grows larger , the input becomes more sparse ( Pouget - Abadie et al. , 2014 ; Koehn and Knowles , 2017 ) .
To gain more understanding , we make dedicated experiments on the influence of input length , data scale and model size for Transformer ( Section 3 ) , finding that a Transformer model can fail to converge when training with long sequences , small datasets , or big model size .
We further find that for the failed cases , the model gets stuck at local minima during training .
In such situation , the attention weights from the decoder to the encoder are flat , with large entropy values .
This can be because that larger input sequences increase the challenge for focusing on a local span to translate when generating each target word .
In other words , the hypothesis space for target - to -source attention is increased .
Given the above observations , we investigate a novel extension of Transformer , by restricting selfattention and target - to -source attention to a local context using a guidance mechanism .
As shown in Figure 1 ( c ) , while we still encode the input document as a single unit , group tags 1 2 3 are assigned to sentences to differentiate their positions .
Target - to -source attention is guided by matching the tag of target sentence to the tags of source sentences when translating each sentence , so that the hypothesis space of attention is reduced .
Intuitively , the group tags serve as a constraint on attention , which is useful for differentiating the current sentence and its context sentences .
Our model , named G-Transformer , can be thus viewed as a combination of the method in Figure 1 ( a ) and Figure 1 ( b ) , which fully separate and fully integrates a sentence being translated with its document level context , respectively .
We evaluate our model on three commonly used document- level MT datasets for English - German translation , covering domains of TED talks , News , and Europarl from small to large .
Experiments show that G-Transformer converges faster and more stably than Transformer on different settings , obtaining the state - of - the - art results under both non-pretraining and pre-training settings .
To our knowledge , we are the first to realize a truly document- by- document translation model .
We release our code and model at https://github.com/baoguangsheng/g-transformer.
Experimental Settings
We evaluate Transformer and G-Transformer on the widely adopted benchmark datasets ( Maruf et al. , 2019 ) , including three domains for English - German ( En - De ) translation .
TED .
The corpus is transcriptions of TED talks from IWSLT 2017 .
Each talk is used as a document , aligned at the sentence level .
tst2016-2017 is used for testing , and the rest for development .
News .
This corpus uses News Commentary v11 for training , which is document- delimited and sentence -aligned .
newstest2015 is used for development , and newstest 2016 for testing .
Europarl .
The corpus is extracted from Europarl v7 , where sentences are segmented and aligned using additional information .
The train , dev and test sets are randomly split from the corpus .
The detailed statistics of these corpora are shown in Table 1 .
We pre-process the documents by splitting them into instances with up - to 512 tokens , taking a sentence as one instance if its length exceeds 512 tokens .
We tokenize and truecase the sentences with MOSES ( Koehn et al. , 2007 ) tools , applying BPE ( Sennrich et al. , 2016 ) with 30000 merging operations .
We consider three standard model configurations .
Base Model .
Following the standard Transformer base model ( Vaswani et al. , 2017 ) dimension hidden vectors .
Big Model .
We follow the standard Transformer big model ( Vaswani et al. , 2017 ) , using 6 layers , 16 heads , 1024 dimension outputs , and 4096 dimension hidden vectors .
Large Model .
We use the same settings of BART large model , which involves 12 layers , 16 heads , 1024 dimension outputs , and 4096 dimension hidden vectors .
We use s-BLEU and d-BLEU ) as the metrics .
The detailed descriptions are in Appendix A .
Transformer and Long Inputs
We empirically study Transformer ( see Appendix B ) on the datasets .
We run each experiment five times using different random seeds , reporting the average score for comparison .
Failure Reproduction Input Length .
We use the Base model and fixed dataset for this comparison .
We split both the training and testing documents from Europarl dataset into instances with input length of 64 , 128 , 256 , 512 , and 1024 tokens , respectively .
For fair comparison , we remove the training documents with a length of less than 768 tokens , which may favour small input length .
The results are shown in former .
Data Scale .
We use the Base model and a fixed input length of 512 tokens .
For each setting , we randomly sample a training dataset of the expected size from the full dataset of Europarl .
The results are shown in Figure 2 b .
The performance increases sharply when the data scale increases from 20 K to 40K .
When data scale is equal or less than 20K , the BLEU scores are under 3 , which is unreasonably low , indicating that with a fixed model size and input length , the smaller dataset can also cause the failure of the training process .
For data scale more than 40K , the BLEU scores show a wide dynamic range , suggesting that the training process is unstable .
Model Size .
We test Transformer with different model sizes , using the full dataset of Europarl and a fixed input length of 512 tokens .
Transformer - Base can be trained successfully , giving a reasonable BLEU score .
However , the training of the Big and Large models failed , resulting in very low BLEU scores under 3 .
It demonstrates that the increased model size can also cause the failure with a fixed input length and data scale .
The results confirm the intuition that the performance will drop with longer inputs , smaller datasets , or bigger models .
However , the BLEU scores show a strong discontinuity with the change of input length , data scale , or model size , falling into two discrete clusters .
One is successfully trained cases with d-BLEU scores above 10 , and the other is failed cases with d-BLEU scores under 3 . Figure 5 : For the successful model , the attention distribution shrinks to narrow range ( low entropy ) and then expands to wider range ( high entropy ) .
Failure Analysis Training Convergence .
Looking into the failed models , we find that they have a similar pattern on loss curves .
As an example of the model trained on 20 K instances shown in Figure 3a , although the training loss continually decreases during training process , the validation loss sticks at the level of 7 , reaching a minimum value at around 9 K training steps .
In comparison , the successfully trained models share another pattern .
Taking the model trained on 40 K instances as an example , the loss curves demonstrate two stages , which is shown in Figure 3 b .
In the first stage , the validation loss similar to the failed cases has a converging trend to the level of 7 .
In the second stage , after 13 K training steps , the validation loss falls suddenly , indicating that the model may escape successfully from local minima .
From the two stages of the learning curve , we conclude that the real problem , contradicting our first intuition , is not about overfitting , but about local minima .
Attention Distribution .
We further look into the attention distribution of the failed models , observing that the attentions from target to source are widely spread over all tokens .
As Figure 4a shows , the distribution entropy is high for about 8.14 bits on validation .
In contrast , as shown in Figure 4 b , the successfully trained model has a much lower attention entropy of about 6.0 bits on validation .
Figure 5 shows the self-attention distributions of the successfully trained models .
The attention entropy of both the encoder and the decoder drops fast at the beginning , leading to a shrinkage of the attention range .
But then the attention entropy gradually increases , indicating an expansion of the attention range .
Such back - and - forth oscillation of the attention range may also result in unstable training and slow down the training process .
Conclusion
The above experiments show that training failure on Transformer can be caused by local minima .
Additionally , the oscillation of attention range may make it worse .
During training process , the attention module needs to identify relevant tokens from whole sequence to attend to .
Assuming that the sequence length is N , the complexity of the attention distribution increases when N grows from sentence - level to document-level .
We propose to use locality properties ( Rizzi , 2013 ; Hardmeier , 2014 ; Jawahar et al. , 2019 ) of both the language itself and the translation task as a constraint in Transformer , regulating the hypothesis space of the self-attention and target - to -source attention , using a simple group tag method .
