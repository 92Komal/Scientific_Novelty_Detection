title
The NiuTrans Machine Translation Systems for WMT21
abstract
This paper describes NiuTrans neural machine translation systems of the WMT 2021 news translation tasks .
We made submissions to 9 language directions , including English ? { Chinese , Japanese , Russian , Icelandic } and English ?
Hausa tasks .
Our primary systems are built on several effective variants of Transformer , e.g. , Transformer - DLCL , ODE - Transformer .
We also utilize back - translation , knowledge distillation , postensemble , and iterative fine-tuning techniques to enhance the model performance further .
Introduction Our NiuTrans team participated in the WMT 2021 news translation shared tasks , including English ?
Chinese ( EN?ZH ) , English ?
Japanese ( EN? JA ) , English ?
Russian ( EN? RU ) , English ?
Icelandic ( EN?IS ) and English ?
Hausa ( EN? HA ) , nine submissions in total .
All of our systems were built with constrained data sets .
We adopt some effective models and useful methods , which have been witnessed the success in previous papers ( Wang et al. , 2018 ; Zhang et al. , 2020 ; Meng et al. , 2020 ; Wu et al. , 2020 b ; Wu et al. , 2020a ; .
To enhance the performance of the single model , we choose pre-normalized Transformer - DLCL and ODE - Transformer ( Li et al. , 2021a ) as the backbone .
All systems are built upon the relative position representation ( Shaw et al. , 2018 ) due to its strong performance when models are deep .
For the system combination , we adopt the post-ensemble ( Kobayashi , 2018 ) to find the most similar hypothesis among several ensemble outputs , which could be regarded as a reranking technique without pre-training .
Previous works have emphasized the importance of diversity when building ensemble systems .
Besides the architecture diversity , we also adopt iterative ensemble knowledge distillation leveraging the source-side monolingual data to enlarge the diversity .
More details please refer to .
Our data preparation pipeline consists of threefold : ( i ) For the data filtering .
We use a stricter cleaning process than last year ( Zhang et al. , 2020 ) .
Details will be discussed in Section 2.1 . ( ii ) For the data augmentation , both iterative backtranslation ( Sennrich et al. , 2016a ) method , and iterative knowledge distillation ( Freitag et al. , 2017 ) method are employed to take the full advantage of monolingual data provided by the WMT organization .
In the back - translation stage , we leverage target -side monolingual sentences to generate source -side pseudo sentences and use a nucleus sampling ( Holtzman et al. , 2019 ) decoding strategy to improve the generalization ability .
Furthermore , we leverage in- domain source-side monolingual data by applying iterative knowledge distillation .
( iii )
For data selection , it 's hard to find massive in-domain data for low-resource languages to train a neural language model , so we use a statistical n-gram language model ( XenC toolkit3 1 ) instead .
Domain finetuning is quite essential to improve the translation system given a certain target domain .
We use domain adaptation to migrate the models from the general domain to the news domain by iterative finetuning .
After in- domain finetuning , we use multiple ensemble combinations by the postensemble method .
This paper is structured as follows :
In Section 2 , we introduce several effective techniques , including data preprocessing , deeper and wider Transformer models , iterative back - translation , itera - tive knowledge distillation , fine-tuning and postensemble .
In Section 3 , we show the experiment settings and report the experimental results of the validation set ( newstest 2020 ) .
Finally , we draw the conclusion in Section 4 .
2 System Overview
Data Preprocessing and Filtering For word segmentation , we use different tools in six languages .
English , Russian , Hausa and Icelandic sentences were segmented by Moses ( Koehn et al. , 2007 ) , while Chinese and Japanese used NiuTrans ( Xiao et al. , 2012 ) and MeCab 2 separately .
Then BPE ( Sennrich et al. , 2016 b ) with 32 K operations is used for five languages sides independently , except for 36 K operations in Russian .
The quality of the parallel training data is crucial to the performance of the models , so we use rigorous data filtering scheme as the suggestion in Zhang et al . ( 2020 ) 's work .
For most language pairs , rules are as follows : ?
Filter out sentences that contain long words over 40 characters or over 150 words .
?
The word ratio between the source word and the target word must not exceed 1:3 or 3:1 . ?
Use Unicode to filter sentences with more than 10 other characters .
?
Filter out the sentences which contain HTML tags or duplicated translations . ?
In monolingual data , some sentences contain two or more sentences .
We write a script to cut them into several sentences .
We use these rules to filter bilingual and monolingual data , detecting low-quality sentences with misalignment , translation errors , illegal characters , and missing translation .
2 https://github.com/taku910/mecab
Model Architectures
As shown in previous work Zhang et al. , 2020 ; Meng et al. , 2020 ) , deep Transformers bring significant improvements than the baseline on various machine translation benchmarks .
In their work , the performance of the model was significantly improved by increasing the encoder depth .
We keep the decoder depth unchanged as the brought benefit is marginal when the encoder is strong enough ( Li et al. , 2021 b ) .
Hence , we train two deep models in our experiment : Transformer DLCL and ODE Transformer ( Li et al. , 2021a ) with a larger filter size .
ODE
Transformer is designed from the ordinary differential equations ( ODE ) perspective .
Higher - order ODE solutions can gain fewer truncation errors , thus reducing the global error and improving the model performance .
The details of several models we mainly experimented with are summarized in Table 1 .
In addition , we incorporate relative position representation ( RPR ) into the self-attention mechanism on both the encoder and decoder sides .
Preliminary experiments demonstrate that only relative key information is enough , and we set the relative window size to 8 .
Large-scale Back -Translation Back-translation ( BT ) is an effective data augmentation technique to boost the performance of NMT models , which use monolingual data to generate pseudo-training parallel data .
Back - translation is divided into three stages : ?
Using bilingual parallel data to train a targetto-source intermediate ensemble of models .
?
Utilizing the ensemble of reverse direction models to translate the target monolingual corpus into the source corpus . ) ) .
Finally , we select the sentence of the smallest ALD .
?
Training models with the bilingual parallel corpus and the synthetic parallel corpus together .
Select in- domain monolingual data during backtranslation can significantly alleviate domain adaptation problems ( Zhang et al. , 2020 ) .
Our indomain data consist of the test sets released in recent years and the News Commentary high-quality monolingual data .
Due to insufficient data in the domain , we used a statistical method to select in - domain data , the XenC toolkit .
Furthermore , to avoid the high ranking of short sentences , we choose the in- domain source side sentences according to the distribution of sentence tokens number in the previous years ' test set .
For all tasks , we employ the beam search and Nucleus Sampling approaches to generate pseudo corpus and the scale of the pseudo corpus was about 1:1 to the real corpus .
Iterative Knowledge Distillation Knowledge distillation ( KD ) has been proven to be a powerful technique to improve the performance of the student model by transferring knowledge from the teacher model Zhang et al. , 2020 ) .
Here , we regard the ensemble models as a teacher model and single models as student models .
Specifically , we first use the ensemble model to generate synthetic corpus in the forward direction .
Then , we merge the synthetic parallel corpus with the bilingual parallel corpus to teach student models .
And by searching for better model ensemble combinations , we can provide stronger teacher models for the next round of knowledge distillation .
Our experiment found that the gap between the single model and the integrated model gradually narrowed as the iteration progressed .
So for the nine tasks we participated in , two iterations of knowledge distillation deliver the best performance .
Finetuning Domain adaptation plays an important role in improving the performance of the models .
A practical method of domain adaptation is to train models on large-scale out-domain corpus and then finetune the models with in- domain corpus ( Luong and Manning , 2015 ) .
For all tasks , we mainly reuse an iterative fine-tuning process ( Zhang et al. , 2020 ) and use the development sets and the test sets of previous years as in-domain corpus .
It is worth noting that , in order to be consistent with the composition of the test set , we select parallel sentences pair from the previous development sets and test sets in which the source side is real and the target side is manually translated .
Moreover , we found that iterative fine-tuning can better improve the translation quality of the names of news organizations in the news field .
Post-ensemble Ensemble learning is a technique widely used in several WMT shared tasks , which improves performance by using multiple single models .
In neural machine translation , a practical method of the model ensemble is to combine the probability distribution on the target vocabulary of different models in each step of sequence prediction .
Here , we adopted their method , which uses a greedy - based strategy to find a better combination of models on the development set .
However , enumerating all combinations of candidate models is an inefficient and cumbersome way .
In our ensemble experiments , we set the number of the ensemble to four and six .
We observed that simply expanding the scale of the ensemble does not necessarily improve translation performance .
Besides , brute force search for all models is costly and unrealistic .
As the number of models increases , the ensemble easily exceeds the computer capacity limit .
Therefore , for all tasks , we finally search for four single models as an ensemble .
In addition , we use a simple but effective unsupervised ensemble method , post-ensemble , which uses a clustering method to select a majority - like output from multiple ensembles .
As shown in the figure 1 , we first choose several ensemble combinations composed of different models to obtain more diversity .
Then we use these ensembles to generate multiple sentences , respectively .
Next , we calculate the Levinstein distance between each sentence , and finally , we select the sentence of the smallest average Levinstein distance with other sentences .
For more detailed content , please refer to the original paper ( Kobayashi , 2018 ) .
This technology can further improve the performance of the system based on ensemble learning .
Experiment
Experiment Settings
The implementation of our models is based on Fairseq ( Ott et al. , 2019 ) .
All models were trained on 8 RTX 2080 Ti GPUs .
We selected the pre-norm Transformer - base as the baseline for all tasks and enhanced our deep or wide models by enlarging the model depth and the hidden size , respectively .
We used Adam optimizer ( Kingma and Ba , 2014 ) with ?
1 = 0.9 , ? 2 = 0.997 during training .
As suggested in Ott et al . ( 2018 ) and 's work , models with larger capacities tend to perform much better within large batch size and learning rate .
Due to the high GPU memory consumption , accumulated gradients every two steps where each batch contains 2048 tokens .
Training for 15 epochs is sufficient for most tasks , and models have shown convergence in validation perplexity .
The max learning rate and warmup step were set to 0.002 and 8000 for deep models , and 0.0016 and 16000 for deep and wide models , e.g. , Transformer - DLCL , whose hidden dimension is 768 .
All the dropout probabilities were set to 0.1 , including the residual dropout , attention dropout , and the ReLu dropout .
We also used FP16 mix-precision training to accelerate further the training process with almost no loss in BLEU .
EN?ZH For EN ?ZH tasks , the training data consists of ParaCrawl , News Commentary v16 , WikiMatrix , UN Parallel Corpus V1.0 , and the CCMT Corpus .
We regarded the newstest2019 as the valid set and the newstest2020 as the test set to tune the hyperparameters .
After filtering the data , we sampled the top 12 and 20 million data according to the XenC score as the bilingual dataset .
For the ZH?EN task , we used 12 and 20 million data to train the baseline model , respectively , and found that the model trained by 12 million data is 1 and 1.2 BLEU point higher than the model trained by 20 million data in the valid and test set .
We found that the data quality of the bottom 8 million is lower and also selected the 12 million data as our training data .
During the first-step back - translation , we sampled 8 million monolingual data from the combination of News crawl , News Commentary , News discussions , and News crawl .
Then we used the baseline model to generate the hypotheses via the beam search strategy as the pseudo dataset .
In the second-step back - translation , we utilized the same amount of pseudo data while using nucleus sampling , whose p is 0.9 .
For ZH?EN and EN ?ZH , we got BLEU improvements of 1.8 and 2.9 in the first back -translation and further BLEU improvements of 0.5 and 0.8 in the second back - translation , respectively .
In addition , we implemented knowledge distillation twice to iteratively enhance the single model with the ensemble outputs .
The main goal is to make the single student mimic the behavior of the ensemble models , thus obtaining stronger ensemble teachers in the next step .
We used the test sets in previous years as in- domain data in EN ?ZH and EN ?
ZH directions respectively , and we used the XenC tool to sample 3 million from the large scale monolingual data based on in- domain data .
Then we used the best ensemble of models to construct pseudo data by decoding them and merge them to the original training data to continue training for each model .
We got BLEU improvements of 1.1 and 0.6 in the first knowledge distillation and further BLEU improvements of 0.6 and 0.3 in the second knowledge distillation in ZH?EN and EN?ZH .
After knowledge distillations , we used the newstest2017 - 2019 to fine - tune our models for five epochs with the 0.0001 learning rate and got 2 and 0 . directions , respectively .
In the final stage , we add newstest2020 to the fine-tuning data .
Finally , we searched for the best five combinations of 4 out of 12 models for post-ensemble to ensure the diversity of the models .
Based on the ensemble method , post-ensemble further brought us + 0.2 and + 0.3 BLEU in ZH?EN and EN ?
ZH directions .
Our main results showed in table 2 , we find that iterative back - translation , iterative knowledge distillation , and iterative fine - tune are effective methods to get significant improvements .
EN ?JA For EN ?JA tasks , we chose ParaCrawl v7.1 , News Commentary v16 , WikiMatrix , Japanese-English Subtitle Corpus ,
The Kyoto Free Translation Task Corpus , TED
Talks total of six parallel data corpora about 17.5 million .
For the ParaCrawl v7.1 , we only selected 8.5 million data according to the score of sentences provided by the dataset .
We chose all of News Crawl and News Commentary and 12 million data sampled from Common Crawl for the Japanese monolingual data .
After merging corpora into training data , we found that there were many - to - one situations in both the target side and the source side .
Therefore , we sorted sentences and calculated the Levenshtein ratio of two adjacent sentences to remove duplication sentences .
We applied this method to all version data before training models and removed 10 percent of the total data .
We randomly selected one out of many sentences in which Levenshtein ratios are greater than or equal to 0.9 .
We also implemented tagged back - translation , which brought us + 2.8 BLEU in the EN ?JA task .
In addition , beam search and nucleus sampling were used to generate two parts of translations to increase data diversity , and each part contains 12 million data .
An interesting phenomenon is that back - translation is useful for EN ?
JA task while knowledge distillation is helpful for JA ?EN task .
We suspect this is because the domain of Japanese monolingual more fits the field of the test set .
We also implemented knowledge distillation and fine-tuned iteratively .
During the knowledge distillation phase , we used FDA 3 and XenC to select monolingual data more like newstest2020 and generated pseudo data by using both post-ensemble and ensemble methods .
During the fine-tuning phase , we used the WMT 2020 valid set and opposite direction test set .
After performance stopped increasing at the second fine-tune , we utilized the best ensemble models to regenerate pseudo data by back -translation and knowledge distillation .
Then , we retrained multiple deep models .
Finally , we put all models together to greedy search for the best combination of 13 models .
And this method brought us + 0.7 BLEU in JA ?EN task .
Our main results are shown in table 2 .
EN?RU For EN ?
RU tasks , we used only two parallel datasets , including ParaCrawl v8 and News Commentary .
After the data filter , about 12 M sentence pairs were left to build our system .
Additionally , we set the merge operations of BPE to 36K .
We also used iterative back - translation , iterative knowledge distillation , and fine-tuned to enhance the model .
During the back- translation , English monolingual data is the same as the EN ?JA part , and Russian monolingual data sources consist of News Crawl and News Commentary .
During the knowledge distillation , we used FDA to select 4 million sentence pairs from the monolingual dataset according to the newstest2020 and newstest 2019 .
models for five epochs .
After KD , we used the newstest2019 and newstest2018 to fine - tune our models for five epochs with the 0.0001 learning rate and got 1.1 and 0.5 BLEU improvements in EN ?RU and RU?EN .
The detailed and full results can be described in Table 3 . Iterative BT , KD , and fine- tune are still very effective and improved 2.8 and 4.3 compared with the base model in EN ?
RU and RU?EN tasks , respectively .
EN?IS
The process of EN ?
IS tasks is similar to EN ?
HA task but more complicated .
Concretely , we used four parallel datasets , including ParaCrawl v7.1 , Wiki Titles v3 , WikiMatrix , and ParIce .
After the data filtering , about 5.5 million sentence pairs were left to build the baseline system .
The experimental results are listed in Table 3 .
We obtained significant improvements of 6.1 and 4.4 BLEU in EN ?IS and IS ?EN directions , respectively .
Then we implemented iterative KD two times and sampled 3 million in- domain source data according to WMT2021 development sets .
Table 3 shows that it 's a very effective method to get 2.2 and 1.1 improvements .
Furthermore , we fine-tuned models iteratively twice to transfer the knowledge into the target domain .
Due to implementing two ensemble combinations to decode sentences , the model ensemble still gained 0.7 and 0.8 improvements .
EN?HA
In the EN?HA direction , we used ParaCrawl v8 , Khamenei corpus , and English -Hausa Opus corpus three data sets , obtaining 1.43 M parallel data after cleaning .
We collected News crawl , Extended Common Crawl , and Common Crawl for the monolingual data , resulting in 5.7 M Hausa monolingual data .
Considering the insufficient scale of Hausa , we used all monolingual data in each round of backtranslation .
The implementation details of iterative knowledge distillation and back -translation are almost the same as the EN ?ZH tasks .
Table 2 summarized the results .
We can observe that the wide and deep models were still effective in low-resource language pairs .
Through the backtranslation and knowledge distillation techniques , we gain 4.6 and 1.7 BLEU improvements , respectively .
Submission Results
The results we finally submitted are shown in table 4 .
We participated in nine tasks this year .
On the whole , all of our systems performed competitively , especially in EH ?IS and RU?EN directions .
Through all the experimental results , we found that different methods perform differently on nine tasks .
Among them , iterative BT is effective for almost all tasks , except for the JA ?EN task .
Iterative KD performs better for EN?ZH , EN?JA and EH ?
IS tasks , while fine -tune is more suitable for ZN?EN and EN ?
RU tasks .
Conclusion
This paper introduced our submissions on WMT21 nine tasks .
Our main exploration is using a new effective architectures ODE Transformer and utilizing post-ensemble technology to enhance the system .
And we experimented with iterative backtranslation by different decoding strategies , iterative knowledge distillation , iterative fine-tuning , model ensembling and post-ensemble .
Figure 1 : 1 Figure1 : Process of the post-ensemble method .
Sn denotes the sentence generated by the n-th ensemble of models .
ALD denotes the average Levinstein distance of a sentence with other sentences .
For example , ALD 1 = 1 2 * ( Levinstein_distance ( S1 , S2 ) + Levinstein_distance ( S1 , S3 ) ) .
Finally , we select the sentence of the smallest ALD .
