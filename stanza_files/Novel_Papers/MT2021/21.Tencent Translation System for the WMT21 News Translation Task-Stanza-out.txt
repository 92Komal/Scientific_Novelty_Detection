title
Tencent Translation System for the WMT21 News Translation Task
abstract
This paper describes Tencent Translation systems for the WMT21 shared task .
We participate in the news translation task on three language pairs : Chinese ?
English , English ?
Chinese and German ?
English .
Our systems are built on various Transformer models with novel techniques adapted from our recent research work .
First , we combine different data augmentation methods including backtranslation , forward - translation and right- toleft training to enlarge the training data .
We also apply language coverage bias , data rejuvenation and uncertainty - based sampling approaches to select content-relevant and highquality data from large parallel and monolingual corpora .
Expect for in- domain finetuning , we also propose a fine- grained " one model one domain " approach to model characteristics of different news genres at finetuning and decoding stages .
Besides , we use greed - based ensemble algorithm and transductive ensemble method to further boost our systems .
Based on our success in the last WMT , we continuously employed advanced techniques such as large batch training , data selection and data filtering .
Finally , our constrained Chinese ?
English system achieves 33.4 case-sensitive BLEU score , which is the highest among all submissions .
The German ?
English system is ranked at second place accordingly .
Introduction
In this year 's news translation task , our translation team at Tencent AI Lab & Cloud Xiaowei participated in three shared tasks , including Chinese ?
English , English ?
Chinese and German ?
English .
We used the same data strategies , model architectures and corresponding techniques for all tasks .
We hypothesized that different models have their own strengths and characteristics , and they can complement each other .
Thus , we built various advanced NMT models which mainly differ in training data and model architectures .
These models ( i.e. DEEP , LARGE and LARGE - FFN ) are empirically designed based on Transformer - Deep which has proven more effective than the Transformer - Big models .
In addition to the original multi-head self-attention , we also proposed a mixed attention strategy by combining relative position with the original one , which extends the self-attention to efficiently consider representations of the relative positions .
We use a variation of relative position , the random attention ( RAN ) ( Zeng et al. , 2021 ) .
As a results , we combined these models at transductive fine -tuning stage .
In terms of data augmentation , we adapt backtranslation ( BT ) ( Sennrich et al. , 2016a ) , forwardtranslation ( FT ) ( Zhang and Zong , 2016 ) and rightto-left ( R2L ) techniques to generate large-scale synthetic training data .
Different from the standard back - translation , we add noise to the synthetic source sentence in order to take advantage of large-scale monolingual text .
In addition , we used tagged BT mechanism ( i.e. add a special token to the synthetic source sentence ) to help the model better distinguish the originality of data .
All the parallel data and a large amount of monolingual data are used in corresponding data augmentation methods , and finally we combine them together to build strong baseline models .
To enhance the domain-specific knowledge , we introduced approaches at both data and model levels .
First , we employed a hybrid data selection method ( Wang et al. ) to produce different finetuning datasets .
More specifically , we apply language coverage bias , data rejuvenation ( Jiao et al. , 2020 ) and uncertaintybased sampling ( Jiao et al. , 2021 ) to select content-relevant and high-quality data from parallel and monolingual corpora .
The news texts contain a number of sub-genres such as COVID -19 and government report .
Thus , we fine- tuned a domainspecific model translate each sub-genre of text in the test set ( i.e. " one domain one model " ) .
We take advantage of the combination methods to further improve the translation quality .
The " greedy search ensemble algorithm " is used to select the best combinations from single models .
Furthermore , we propose an multi-model & multi-iteration transductive ensemble ( m 2 TE ) method based on the translation results of the ensemble models .
First , we divided models into two parts .
Second , each part produced syntactic parallel testsets which is used to fine - tune another part of models .
We repeated this procedure for N times .
This paper is structured as follows : Section 2 describes our advanced model architectures .
We then present the data statistics and processing methods in Section 3 .
The methods and ablation study are detailed in Section 4 followed by final experimental results in Section 5 .
Finally , we conclude our work in Section 6 .
Model Architecture
In this section , we mainly introduced three model architectures , which are empirically adapted from Transformer ( Vaswani et al. , 2017 ) .
General Configurations
All models are implemented on top of the opensource toolkit Fairseq ( Ott et al. , 2019 ) .
Each single model is carried out on 8?16 NVIDIA V100 GPUs each of which have 32 GB memory .
We use the Adam optimizer with ?
1 = 0.9 and ? 2 = 0.98 .
The gradient accumulation is used due to the high GPU memory consumption .
We also employed large batching , which has significantly outperformed models with regular batch training .
To speed up the training process , we conduct training with half precision floating point ( FP16 ) .
We set max learning rate to 0.0007 and warmup-steps to 16000 .
All the dropout probabilities are set to 0.3 .
The detailed hyper-parameters of each model are summarized in Table 1 .
Deep Model Deep transformer has shown more effective performance than the TRANSFORMER - BIG models ( Dou et al. , 2018 ; .
We mainly modi- fied the TRANSFORMER - BASE model by using a 40 - layer encoder .
To stabilize the training of deep model , we use the Pre-Norm strategy , which is applied to the input of every sublayer .
The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as : normalize ?
Transform ? dropout ? residual- add .
The batch size is 5120 with 16 GPUs and " update-freq " is 1 .
We totally train models with 400K updates .
Large Model
The large model is empirically designed based on TRANSFORMER - BIG models ( Vaswani et al. , 2017 ; Yang et al. , 2020 ) with 24 encoder layers .
More specifically , the batch size is 4096 with 8 GPUs and the " update-freq " is 4 .
We totally train models with 400K updates .
Large-FFN Model
We train Larger
Transformers , the inner FFN dimension of which is twice as big as that of large Transformer .
Specifically , in this setting , the FFN dimension is set to 8192 .
The number of encoder and decoder layers are 20 and 6 respectively .
The number of head is 16 .
In addition to the original multi-head self-attention , we use a mixed attention strategy , where the random attention ( Zeng et al. , 2021 ) is combined with the original attention .
In this way , the self-attention mechanism can efficiently consider representations of the relative positions , or distances between sequence elements .
In training Large - FFN models , we set the batch size to 8192 toknes per GPU and the " update-freq " parameter is set to 8 .
The models are trained on 8 GPUs for about 3 days .
3 Data and Processing
Overview
Pre-Processing
To process raw data , we applied a series of opensource / in- house scripts , including non-character filter , punctuation normalization , and tokenization / segmentation .
The English and German languages are tokenized by Moses toolkit , 1 while the Chinese sentences are segmented by Jieba .
2 Furthermore , we generated subwords via BPE ( Sennrich et al. , 2016 b ) with 35 K merge operations .
The BPE models are trained on all the data in corresponding parallel and monolingual corpora instead of only parallel data .
The vocabulary sizes of Chinese ?
English are 59100 and 48772 , respectively .
The vocabulary sizes of English ?
German are 41812 and 40948 .
Filtering
To improve the quality of data , we filtered noisy sentences ( pairs ) according to their characteristics in terms of language identification , duplication , length , invalid string and traditional - simplified Chinese conversation .
First , we filtered sentences whose language identification is invalid especially for English ?
German .
Second , we removed similar sentences by comparing MD5 values of skelectons ( i.e. removing stop words from sentences ) .
About length , we filter out the sentences with length longer than 150 words .
For more noisy corpora ( e.g. ParaCrawl ) , we added hard filtering rules on special symbol , digital number , word length , punctuation number , HTML tags .
Regarding bingling data , we further considered source-target ratio .
For instance , the word ratio between the source and the target must not exceed 1:1.3 or 1.3:1 .
According to our observations , our method can significantly reduce noise issues including misalignment , translation error , illegal characters , over-translation and under-translation .
After filtering noisy training data , we used several data manipulation approaches to further improve the quality of the training data .
We first followed to identify the original languages of the bilingual sentence pairs , and explicitly distinguished between the source - and target-original training data using the bias-tagging strategy .
We also identified the inactive training examples which contribute less to the model performance , rejuvenated them with self-training ( Jiao et al. , 2020 ) .
For the data augmentation with backtranslation and forward - translation , we selected the most informative monolingual sentences by computing the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data ( Jiao et al. , 2021 ) .
Evaluation
We regarded the WMT2019 test set as the validation set , and WMT2020 test set as the test set for all experiments .
We ranked checkpoints according to either loss or BLEU on validation set .
We used sacreBLEU score 3 as our evaluation metrics which is officially recommended .
We also con - Table 3 : Effects of data augmentation methods on Chinese ?
English translation task .
We used generally the same amount of monolingual data with the bilingual corpus .
We used the DEEP model trained on the original bilingual data to construct the synthetic data , which is used together with the bilingual data to train the NMT models .
# Method WMT20 Data WMT21 Data Data WMT19 WMT20 Data WMT19 WMT20 ducted post-processing such as detokenizer .
perl on system output before sacreBLEU .
Method and Ablation Study
In this section , we conducted a comprehensive ablation study of the techniques used in this competition .
We reported results on the Chinese ?
English task using the constrained data .
Data Augmentation
In this evaluation , we used three commonlyused data augmentation methods , namely backtranslation ( BT ) , forward - translation ( FT ) and right - to - left training ( R2L ) , to exploit the useful monolingual data .
All the synthetic parallel data is used together with the original parallel data to train NMT models .
Back - Translation
This method first trains an intermediate target- to- source NMT system , which is used to translate monolingual target sentences into source language .
Then the synthetic parallel corpus is used to train models together the bilingual data .
In this work , we apply the noise backtranslations method as introduced in Lample et al . ( 2018 ) .
When translating monolingual data we use an ensemble of two models to get better source translations .
We follow to add noise to the synthetic source data .
Furthermore , we use a tag at the head of each synthetic source sentence as Caswell et al . ( 2019 ) does .
To filter the pseudo corpus , we translate the synthetic source into target and calculate a Round-Trip BLEU score , the synthetic pairs are dropped if the BLEU score is lower than 30 .
Forward-Translation
This method is similar to BT but performs in a reverse manner .
Recent stud-ies showed that back -translation harms the translation performance , while forward -translation improves the performance ( Edunov et al. , 2020 ; Marie et al. , 2020 ) .
Our preliminary experiments reconfirm their findings .
Accordingly , we use forwardtranslation to construct the synthetic parallel data by translating the monolingual source sentences by the source - to- target NMT model , which is trained on the original bilingual data .
Right- to- Left Training
The approach is proposed to address the error propagation problem in autoregressive generation task .
The main idea is to improve the agreement between translations generated by Right - to - Left ( R2L ) models and Left-to- Right ( L2R ) models .
Following this work , we translate the source-side sentences in both parallel and monolingual corpora with both a R2L model and a L2R model , and use the translated pseudo corpus to improve the L2R model .
For the right- to- left training , we trained another DEEP model on the bilingual data , whose target side is reversed .
We drop the pseudo parallel data if the BLEU score lower than 15 .
Experimental Results
As shown in Table 3 , we systematically investigated effects of 1 ) WMT20 / WMT21 training data and 2 ) individual / combined data augmentation methods on Chinese ?
English translation task .
For a comparison between the different training corpora of WMT20 and WMT21 , we also reported results on the WMT20 training data ( " WMT20 Data " ) used in last year we selected domain-relevant and high-quality sentences from all available monolingual data as listed in Table 2 .
To construct the new training data ( i.e. combining authentic and synthetic data ) , we selected the same amount of monolingual data with the bilingual corpus .
As seen , individually using FT and R2L can significantly improve the baseline model by around + 1 BLEU point .
About BT , we fount that it failed to outperform baseline in " WMT20 Data " while performs slightly better than baseline in " WMT21 Data " .
Finally , we trained the NMT models on the WMT21 training data augmented with the synthetic data generated by different data augmentation methods ( up to 99.8 M sentence pairs in total ) .
We can further improve the performance by combining them together , demonstrating complementarity of different methods .
Fine-Tuning
We use in- domain finetune to further improve the model performance , which has proven effective on the WMT19 ?
20 news translation tasks ( Sun et al. , 2019 ; Meng et al. , 2020 ; . We construct different types of finetune data with the following approaches .
dependent differences between sentence pairs originating from the source and target languages , because the target-original data 5 can not improve translation performance .
Accordingly , we select the source-original examples ( SO ) from the test sets as the finetune data .
Besides the WMT test sets , we also use the test sets from the CWMT competitions , which are available in the released data of WMT21 competition .
In the CWMT testsets , each source sentence has four references , therefore we construct four sentence pair for each instance in the CWMT test sets .
In -Domain Training Data
We employed data selection and data augmentation methods to select in - domain data from WMT / CWMT test sets and training corpus , respectively .
More specifically , we employed BM25 algorithm to select relevant sentence pairs by regarding source-side of WMT20 test set as queries .
As shown in Table 4 , the " Data Selection " is a subset of WMT test sets .
On the other hand , we extend the finetuning set by selecting in - domain data from the tranining corpus .
We further use the RT and R2L approaches in Section 4.1 to augment the finetune data with the TRANSFORMER - DEEP model .
Since the data augmentation approaches only require source-side sentences , we also construct the synthetic data for the WMT19 and WMT20 test sets .
6
We finetune the NMT model on the mixture of the additional synthetic corpus and the selected previous test sets .
One Domain One Model argued that low-frequency words contain more domain information than high- frequency words , since low-frequency words are mostly domain-specific nouns , etc. , which may indicate the topic directly .
Therefore , they adapt the TF - IDF algorithm to search and filter on the whole training set and then use them to train domain-specific models .
We automatically to assigned domain labels to each source -side document in the test set .
First , we used K-means clustering to obtain keywords of each document .
Then , we proposed a rule-based method to classify each document in three categories : COVID -19 , government report and other .
In this experiment , we only focused on two specific domains and thus we trained two domain-specific models to translate COVID -19 and government report documents , respectively .
The other documents are still dealt with a general- domain model .
Experimental Results
As shown in Table 5 , we investigated effects of different fine-tuning methods on Chinese ?
English translation task .
As seen , source-original data is more effective than combining non-source-original one into finetuning dataset ( 35.5 vs. 35.7 BLEU ) .
However , the CMWT dataset instead decrease the BLEU scores ( - 0.6 BLEU ) .
The data reduction ( " + DS " ) and expansion ( " + DA " ) methods can not further improve the performance of baseline model ( - 0.3 and + 0.1 BLEU ) .
Encouragingly , the " One Domain One Model " method can significantly improve the baseline model by + 0.4 BLEU point .
Model Ensemble Model ensemble is a widely used technique in previous WMT shared tasks , which can boost the performance by combining the predictions of several models at each decoding step Sun et al. , 2019 ; . In our work , we use two kinds of ensemble methods and finally the two are combined for further improvements .
Checkpoint Average
For one model ( same architecture and training data ) , we stored checkpoints according to their BLEU scores ( instead of PPL or training time ) on validation set .
Then we combined top -L checkpoints ( generate a final checkpoint ) by averaging their weights to avoid stochasticity .
To combine different models , we further ensembled the averaged checkpoint of each model .
In our empirical experiments , we find that this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness .
For more detail , please refer to the original paper .
We also train single models with different hyper parameters to ensure the diversity .
We refer to this method as Ensemble in the following .
Multi-Model & Multi-Iteration Transductive Ensemble Transductive ensemble ( TE ) is proposed by .
The key idea is that source input sentences from the validation and test sets ( in - domain seed ) are firstly translated to the target language space with multiple different well - trained NMT models , which results in a pre-translated synthetic dataset .
Then individual models are finetuned on the generated synthetic dataset .
We propose an variation of TE , namely Multi-Model & Multi-Iteration TE ( m 2 TE ) which is shown in Algorithm 1 .
The main difference from Iterative Transductive Ensemble is that E N can be different groups of ensembled models ( Deep , Large and Large - FFN models ) .
Final Results
In this section , we combined all the presented methods and techniques ( detailed in Section 4 ) together and showed the final results in Table 6 .
Chinese ?
English Translation Tasks
We train multiple single models in each settings .
We found that the R2L method can significantly improve the baseline by about 1 BLEU score .
It is surprising to find a gain of 2 BLEU improvement when combining all data augmentation meth -
In our experiments , the ensemble models consists of 5 single models : 1 DEEP , 2 LARGE , 2 LARGER - FNN models .
The simple ensembled model can outperform the best single model by 0.5? 2.0 BLEU scores .
We then apply transductive ensemble to each group of models and the performance achieves 36.8 BLEU on Chinese ?
English task .
Finally , we employed two fine- grained domain-specific models to translate COVID -19 and government report texts , respectively .
This can further improve the model by + 0.5 BLEU point .
We also find that the single models that applied TE cannot bring further improvement to ensemble results .
We do not apply re-ranking to this task , as we find that the improvement is insignificant .
German ?
English Translation Tasks
The baseline model are trained on bilingual data and R2L data .
This boosts the BLEU score from 41.6 to 42.1 .
After adding BT and FT , we further improve the BLEU score by 1.3 BLEU scores .
For finetuning English ?
German models , we select the document whose source side is originally in German from all previous development and test dataset as in-domain corpus D. Single models are trained with the above methods are then fine - tune on D for one epoch with a fixed learning rate of 1e - 4 .
In our final submission , the WMT2020 test set is added to D for better performance improvement .
The fine-tuning can further achieve 0.93 BLEU improvement on the DEEP model .
In this task , the ensemble models consists of 3 single models : 1 DEEP , 1 LARGE , 1 LARGER - FNN models .
The ensemble models outperform the best single model by 1.5 BLEU scores .
Furthermorem we apply a rule- based post-processing procedure on punctuation and this can improve the BLEU score on development set by 0.5 point .
Official Results
The official automatic results ( in terms of sacre - BLEU ) of our submissions for WMT 2021 are presented in Table 7 .
Among participated teams , our primary systems achieve the first and the second BLEU scores on Chinese ?
English and German ?
English , respectively .
The experimental results demonstrates that our models can achieve the state - of - the - art performance .
In the future , we will integrate these useful techniques in the Tencent TranSmart ( Huang et al. , 2021 ) , Mr. Translator ( https://fanyi.qq.com), and Tencent Simultaneous Translation systems .
Conclusion
This paper presents the Tencent Translation systems for WMT2021 news translation tasks .
We investigate various deep architectures to build strong baseline models .
methods such as BT , FT and R2L are combined to improve their performances .
We demonstrate that in- domain fine-tuning and fine- grained domain modelling are effective to further improve domain-specific quality .
Besides , our proposed greed - based ensemble algorithm and transductive ensemble method play key roles in our systems .
Among participated teams , our primary systems achieve the first and the second BLEU scores on Zh?En and De?En , respectively .
In the future , we will adopt useful methods to our advanced non-autoregressive translation models ( Ding et al. , 2021 b , a ) and investigate the effects of pre-training on NMT ( Liu et al. , 2021 a , b) .
It is worth mentioning that most advanced technologies reported in this paper are also adapted to our systems for biomedical translation task , which achieve three 1st ranks in German / French / Spanish ?
English tasks .
Table 2 2 lists statistics of parallel and monolingual data we used in training our systems .
The details
