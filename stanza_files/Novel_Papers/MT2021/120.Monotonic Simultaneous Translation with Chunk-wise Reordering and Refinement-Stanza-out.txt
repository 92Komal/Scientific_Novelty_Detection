title
Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement
abstract
Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora , leading to either excessive latency or necessity to anticipate asyet-unarrived words , when dealing with a language pair whose word orders significantly differ .
This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammaticality of a sentence being translated .
In this paper , we thus propose an algorithm to reorder and refine the target side of a full sentence translation corpus , so that the words / phrases between the source and target sentences are aligned largely monotonically , using word alignment and non-autoregressive neural machine translation .
We then train a widely used wait -k simultaneous translation model on this reorderedand - refined corpus .
The proposed approach improves BLEU scores and resulting translations exhibit enhanced monotonicity with source sentences .
Introduction Simultaneous interpretation is widely used in various scenarios such as cross-lingual communication between international speakers , international summits , and streaming translation of a live video .
Simultaneous interpretation has a latency advantage over conventional full-sentence translation , i.e. offline translation , as it requires only partial sequence to start translating .
However , as the source and target languages differ in word orders , there is a difficulty in simultaneous interpretation that does not exist in offline translation which translates only after the whole source sentence is received .
For example , when dealing with language pairs that significantly differ in word order ( e.g. , between SOV language and SVO language ) , an interpreter Figure 1 : An example illustration of monotonic reordering and refinement for simultaneous translation may not receive sufficient information with partial sequence to start generating a translation that respects the natural order of the target language .
One of the approaches to address this problem is to perform anticipation 1 . Note that the nature of anticipation relies on interpreters ' assumptions and the anticipation may provide incorrect translations .
Alternatively , human interpreters strategically resort to producing monotonic translations that follow the word order of the source sequence ( Cai et al. , 2020 ) .
To illustrate the differences between the two strategies , the example in Figure 1 may be referred to .
Of the two targets , offline target y respects the target language order and an online target ?
roughly follows the source word ordering .
Successful anticipation in Figure 1 's case would be to predict the initial words in y ( I was a " frog in a well " ) before receiving the full x .
This would pose difficulty even to professional translators as all the relevant information is in the latter part of the x ( ? ? ? ?I / " ?
? ? ? a well / ? ? ? in / ? ? ? ? a f rog " ? ? ? ? ? ? ?was . ) .
Bart?omiejczyk ( 2008 ) reports the success rate of human interpreters ' anticipation attempts to be as low as 38.1 % even though they make predictions based on pre-acquired domain knowledge .
On the other hand , a monotonic approach would be to gen-erate an ? style translation - the grammaticality in the resulting sequence is sacrificed to translate only the received information .
A similar case applies to Simultaneous Machine Translation ( SimulMT ) models , which start translating before a whole sentence is given .
Several studies ( Ma et al. , 2019 ; Arivazhagan et al. , 2019 ; often utilize offline full-sentence translation corpora to train SimulMT models .
Offline full-sentence parallel corpora are expected to follow the natural order of languages and mostly contain source to offline - target pairs .
Naturally , when SimulMT models are trained on these corpora , the models inevitably learn to perform anticipation .
Recent SimulMT studies are focused on reducing anticipation ( Zhang et al. , 2020a ) or performing better anticipation ( Zhang et al. , 2020 b ) .
On the contrary , studies on enabling monotonic translation in SimulMT are scarcely available .
Recently , Chen et al. ( 2020 ) suggest utilizing pseudoreferences for monotonic translation .
In this paper , we propose a paraphrasing method to generate a monotonic parallel corpus to allow a monotonic interpretation strategy in SimulMT .
Our method consists of two stages .
The method first chunks source and target sequences into segments and monotonically reorders the target segments based on source -target word alignment information ( Section 3.1 ) .
Then , the reordered targets are refined to enhance fluency and syntactic correctness ( Section 3.2 ) .
To show the effectiveness of our method , we train wait -k models ( Ma et al. , 2019 ) on the resulting monotonic parallel corpus of reordering - and- refinement .
Results show improvements in BLEU scores over baselines and models producing monotonic translations .
Our main contributions are as follows : ?
We propose a method to reorder and refine the target side in an offline corpus to build a monotonically aligned parallel corpus for SimulMT .
?
We investigate the monotonicity in different language pairs , and show monotonicity can be improved after the reordering - and- refinement process .
?
We train widely used wait -k models on generated monotonic parallel corpora in multiple language pairs .
The results show improvements over baselines in both translation quality and monotonicity .
Monotonicity Analysis
In this section , we analyze the degree of word order differences in multiple language pairs , i.e. , the monotonicity in different language pairs .
To measure the monotonicity , two rank correlation statistics are utilized : Kendall's ? and Spearman 's ?.
The analyzed language pairs are : English -{ Korean , Japanese , Chinese , German , French } .
According to Polinsky ( 2012 ) , English is a headinitial language and Korean and Japanese are rigid head-final languages ; Korean and Japanese are likely to exhibit extreme word order differences with English .
German and Chinese are considered a mixture of head-final and head-initial languages ; they are likely to have word differences with English , but not as severe as Korean or Japanese .
French is also head-initial , so English and French pair is likely to have similar word order .
Figure 2 show monotonicity measurements between English and five different languages which vary in monotonicity : English - German and English - French pairs show high monotonicity , while English - Japanese and English -Korean pairs show low monotonicity .
Lower monotonicity in language pairs presents higher difficulties for SimulMT tasks .
For example , wait -k algorithm only sees k + t source tokens to generate a target token at step t which could lead to unwanted anticipation .
To avoid such anticipation , as we mentioned in Section 1 , human interpreters often provide a monotonic translation .
In the same sense , we conjecture that promoting monotonicity in training corpora is beneficial for translation quality in SimulMT .
Monotonic Reordering and Refinement
In this section , we describe our proposed paraphrasing method of chunk - wise reordering and refinement to generate monotonic corpus for SimulMT .
Given source x = {x 1 , x 2 , ? ? ? , x | x| } and offline full sentence target y = {y 1 , y 2 , ? ? ? , y |y | } , an alignment a is defined as a set of position pairs of x and y. a = {( s , t ) : s ? { 1 , ? ? ? , | x|} , t ? { 1 , ? ? ? , |y |}} First , in chunk - wise reordering phase , we generate source chunk set C X C X = {( x 1:p 1 ) , ( x p 1 + 1: p 2 ) , ? ? ? , ( x p k?1 + 1 : p k ) } , where 0 < p 1 < p 2 < ? ? ? < p k = |x| and re- ordered target chunk set C Y C Y = {( y 1:q 1 ) , ( y q 1 + 1:q 2 ) , ? ? ? , ( y q k?1 + 1:q k ) } , where 0 < q 1 < q 2 < ? ? ? < q k = | y | , and y i ?
y is reordered target token from offline target y .
The elements of a reordered target chunk C Y i are corresponding target tokens of a source chunk C X i based on given alignment information a .
Also , we preserve the original target order within each C Y i .
For example , offline and reordered target in Figure 1 correspond to y and y respectively , and both sequences are only different in token orders .
The number of source chunks in one sentence is the same as the number of reordered target chunks ( | C X | = | C Y | ) , while the number of tokens in | C X i | and | C Y i | could vary .
We experiment two chunking methods ; fixed - size chunking and alignment - aware adaptive size chunking .
Given chunked sets C X and C Y , we refine reordered target tokens to generate more natural and fluent sentence with a Non-Autoregressive Translation ( NAT ) model .
In the refinement algorithm , final paraphrased sentence ? is generated from reordered sequence y .
Furthermore , we incorporate an Autoregressive Translation ( AT ) model into our refinement process .
The more detailed steps for each phase will be explained in the following subsections .
Chunk-wise Reordering
Fixed-size Chunk Reordering
In the fixed - size chunk reordering method , we simply chunk a sequence of tokens into fixed size segments .
The source chunk set C X in this chunking method is as follows : C X = {( x 1 :K ) , ( x K+1:2 K ) , ? ? ? , ( x [ | x | /K ] :|x| ) } , where K ? [ 1 , | x | ] is chunk size .
If k = 1 , C X is identical with x .
We conduct subword operation such as sentencepiece or BPE after chunking process in order to avoid subword separation .
Alignment - Aware Chunk Reordering
In the alignment - aware chunking method , we segment a sentence adaptively by leveraging alignment information a , as described in Algorithm 1 .
The left grid in Figure 3 presents the subword alignment between source and target sentence .
We run aligner on subword over word because the alignment performance is consistently better Zenkel et al . ( 2020 ) when using GIZA ++ ( Och and Ney , 2003 ) , which we use in our experiments .
Based on this alignment information , we initialize a list of chunks C .
As observable , there are some tokens which have no alignment information .
To avoid omission , we assign the same alignment as the previous token ; if a token is at the head , it follows the next token 's alignment .
To ensure subwords can be properly detokenized after reordering , we merge mid-splitted subwords .
The middle grid in Figure 3 presents the result of these initialization steps .
After initialization , we generate consistent chunks by merging all the inconsistent ones , following the definition of consistency in Zens et al . ( 2002 ) .
In a consistent chunk , tokens are only aligned to each other , not to tokens in other chunks .
If any chunk in C has size smaller than a minimum size threshold ? , we merge a chunk pair that are adjacent in both source and target side and have the shortest target distance between them .
If the distances are the same between multiple candidate pairs , we choose the pair of chunks that makes the smallest size after merging .
We additionally merge the chunks adjacent to the merged one if they are arranged monotonically .
Merging is repeated until all chunks meet the size requirements .
An example of final result is the right grid in Figure 3 .
Phrase extraction method used in statistical machine translation Koehn ( 2004 ) also makes phrase level alignments from word alignments using heuristics like ours , but it tends to choose shorter phrases since the number of co-occurrences decrease drastically as the phrase size grows , which makes it difficult to generate larger chunks to prevent hurting grammatical correctness while reordering phase .
Refinement Reordered target results from previous phase inevitably entail irregularities mainly for two rea -
One could be broken connectivity of collocations in segmentation process .
The other would be disfluently missing or containing words of endings and preposition as the position of chunk has been changed , thus requiring an addition of new words or clearing unnecessary words .
In this part , we focus on refining aforementioned anormalities in order to enhance fluency , while preserving the monotonicity at the same time .
Refinement with NAT
We iteratively decode partial source C X with pretrained translation model , given partial reordered target C Y as a guidance in order to generate corresponding online target ? .
More specific process is explained in Algorithm 2 .
As the model refines given [ ?i?1 ; C Y i ] , previous refined output ?i?1 could be altered as the model re-generates the entire sequence from scratch .
Similarly in re-Algorithm 2 : Chunk-wise Refinement Input : Source and target chunks C X , C Y Output : Paraphrased target ? 1 i = 1 2 ?0 = [ ]
3 while i ?
| C X | do 4 X i = C X 1:i and Y i = [ ?i?1 ; C Y i ] 5 ?i = arg max Y log p R ( Y | X i , Y i ) 6 i = i + 1 7 end Return : ?|C
X | translation ( Arivazhagan et al. , 2020 ; Han et al. , 2020 b ) , we set an option of fixed or alterable prefix to force the model whether to generate same target prefix of ?i?1 or to allow the model to modify the prefix .
As we limit the visibility of source information and iteratively generate target tokens with increasing source chunks , we expect the refinement model to generate monotonically aligned and paraphrased targets ? with enhanced fluency .
We use NAT architecture as the core refinement model R .
In NAT inference , the model 's decoder is first given source features and fed an empty target sequence .
Then the NAT decoder develops the empty sequence into a translation of the source sequence .
This development is often iterative .
Note that at every iteration step , the target sequence is refined - closer to the source sequence in meaning and become more fluent .
This motivates us to utilize NAT architecture in our refinement process for monotonic-yet- disfluent sequences .
In our approach , the NAT model starts refinement iteration with initialized tokens of previous output and reordered target chunk Y i , instead of an empty sequence .
This target initialization act as a weak supervision to generate monotonically aligned target , which allow model to focus only on the fluency the reordered targets .
Incorporation of AT Despite the aptness of NAT structure to our refinement phase , NAT model entails a performance degradation compared to AT model in the expense of speedup .
Also , there exists repetition problem in NAT ( Lee et al. , 2018 ; Gu and Kong , 2021 ) which is generated in the process of multiple chunkwise iterative refinement .
In order to complement the aforementioned weaknesses of NAT decoding , we incorporate AT into our refinement process with NAT model .
The final probability is computed jointly with the probability of AT and NAT model : p R ( Y | X i , Y i ) ? p AT ( Y | X i ) ? ? p N AT ( Y | X i , Y i ) ( 1 ? ) , ( 1 ) where ? ? [ 0 , 1 ] is hyper-parameter deciding the ratio between AT and NAT probability .
Experiments
Dataset
In this section , we describe the utilized datasets .
Detailed statistics are presented in Table 1 . Utilized EnKo trainset and devset are created using in-house translation corpora while test scores are reported on IWSLT17 ( Cettolo et al. , 2017 ) EnKo testset .
The DeEn trainset of WMT15 translation task ( Bojar et al. , 2015 ) is utilized .
newstest2013 is utilized as devset and newstest2015 is used as testset .
The EnJa trainset and validset are respectively the combination trainsets and validsets of KFTT ( Neubig , 2011 ) , JESC ( Pryzant et al. , 2018 ) , TED ( Cettolo et al. , 2012 ) .
The trainset and validset are used as preprocessed and provided by the MTNT authors 2 ( Michel and Neubig , 2018 ) .
Only the TED portion of testsets is used .
For EnZh training UN Corpus v1.0 ( Ziemski et al. , 2016 ) is used .
Trainset , devset , and testset follow the original splits .
Monotonicity of EnFr in Figure 2 is measured on the WMT14 ( Bojar et al. , 2014 ) trainset .
Additional details regarding utilized tokenization and vocabulary training are listed in Appendix A .
Metric
All the BLEU scores are cased - BLEU measured using sacreBLEU ( Post , 2018 ) .
Test scores are measured using models that report best BLEU on their respective devsets .
All references and translations of each Korean , Japanese , and Chinese languages are tokenized prior to BLEU evaluation .
Tokenizers utilized are mecab-ko 3 , KyTea 4 , and jieba for Korean , Japanese and Chinese respectively .
We report detokenized BLEU on DeEn results .
To measure monotoniticy , we use Kendal 's ? rank correlation coefficient .
Implementation Details
The default setting for NMT and SimulMT models follow the base configuration of transformer ( Vaswani et al. , 2017 )
Corpus Generation and Training
We demonstrate the effectiveness of our reorderingand - refinement method by training wait -k models on the resulting datasets .
The wait -k models are trained on the combination of the monotonically aligned training pairs and offline trainset .
AlignAw + NAT + AT denotes monotonically aligned corpora using alignment - aware reordering and refinement using joint probability of NAT and AT models .
And Offline refers to the offline full-sentence corpus .
Reordering Fixed :
For fixed - size reordering , we experiment with chunk sizes K ? { 4 , 6 , 8 , 10 , 12 } .
In waitk training , k and K are matched .
All fixed - size reordered - and - refined corpora have the same size as corresponding offline corpus .
AlignAw :
For each corpus , we generate four variations of alignment - aware reordering with source and target minimum chunk size of 2 , 3 .
Alignmentaware reordering is not applicable on the alreadymonotonic cases and the sentence pairs which are locally non-monotonic inside a chunk and globally monotonic among chunks within a single pair - typically , the reordering method is applicable to 20 % to 50 % of offline corpus .
We gather unique pairs from the created four variations to generate the final reordered pairs .
The statistics of reordered set for each translation direction is in
Refinement NAT : NAT models are utilized to refine the reordered pairs .
Both the fixed prefix and alterable prefix refinement is performed and combined .
BertScore ( Zhang et al. , 2020 c ) is measured and used to discard refinement results that show below average scores .
The size of the resulting set is the same as the corresponding offline corpus .
NAT + AT : NAT and AT models can both be utilized to jointly compute token probability in refinement ( Section 3.2.2 ) .
The AT models utilized are the baseline wait -k models trained on offline corpora .
We experiment with ? ? { 0.25 , 0.5 } .
The examples of reordered - and - refined sequences can be found in Appendix E.
Results and Analysis
Experimental Results on EnKo Table 2 shows BLEU scores and Kendal 's ?
s of wait -k models trained using original offline corpus and variations of reordered - and - refined corpus .
We observe that the models trained on monotonically reordered - and - refined corpora show higher BLEU scores and monotonicity .
Reordering :
Of the variations , corpora including AlignAw chunking process generally show better BLEU scores over Fixed + NAT when k ?
6 . This could be the benefit of the semantically plausible way to split sentences provided by AlignAw chunking .
On the other hand , models trained with Fixed + NAT corpora show higher BLEU when k ?
8 . Refinement : Experiments on utilizing AT probabilities show degraded BLEU scores in k ? 6 , 8 , 10 .
On the contrary , the models trained on AlignAw + NAT + AT corpora show enhanced monotonicity .
The ? value may be adjusted to make trade- off between promoting monotonicity in translation or enhancing translation quality in terms of BLEU .
Repetition Reduction with AT : Following ( Welleck et al. , 2020 ) , we report n-gram repetition rate , seq-rep-n , on each generated corpus in Table 3 .
We observe from seq-rep -n in all of the tested n values , that employing AT models in refinement help alleviating the repetition problem of posed by NAT models .
Language Pairs Comparison Figure 4 shows the difference in monotonicity between different language pairs : EnKo , EnJa , EnZh , and DeEn .
It is observable in Figure 4 that the overall monotonicity in EnKo and EnJa pairs is enhanced after paraphrasing , while monotonicity scores of DeEn remain almost the same , only showing slight improvement .
The extent of monotonicity enhancement in EnZh is between that of EnKo / Ja and DeEn .
In all language pairs , the enhancements are generally lower in long or very short sequences .
In the case of long sequence pairs , a pair may contain multiple sequences and be already aligned at the sequence level , thus resulting in marginal monotonicity enhancement .
In the case of shorter length sequences , the whole sentence may be merged into a single chunk , less benefiting from our process .
After the reordered sets are refined , monotonicity marginally decreases .
This is expected as forcibly aligned tokens are refined to augment the fluency in the resulting sentence .
To present the effectiveness of generated monotonic corpus in different language pairs , we train wait -k models on EnKo , EnJa , EnZh , and DeEn , and report BLEU and Kendal 's ? of the models in Figure 5 .
The horizontal dotted line presents the BLEU of unidirectional offline model .
The important observation we can find is that the monotonicity increment of wait -k model in Figure 5 is proportional to that of generated monotonic corpus in Figure 4 , suggesting that promoting monotonicity in training corpus is beneficial for SimulMT models to generate monotonic output , especially in language pairs with differing word orders .
Within two paraphrasing methods , Fixed + NAT and AlignAw + NAT , we can see that the monotonicity of Fixed + NAT is always in between that of Offline and AlignAw + NAT in Figure 5 , and the gap increases as the k value get higher .
While our methods are effective in EnKo , the performance of suggested method is similar or lower than that of baseline in EnJa .
We presume that the ineffectiveness in EnJa is due to its short average sentence length with highest emission rate , as shown in Table 1 .
A short sentence often cannot preserve semantic properties while being split into chunks and reordered .
For example , Fixedlength reordering always chunks all sentences ignores such feature and only increases disfluency in the chunked result .
Also , even though AlignAw enforces the consistency requirement on the chunks , adjustment of such requirement like changing minimum chunk size may be required considering the high emission rate .
Based on the highest performance at k = 8 , there is about 8 % BLEU improvement over Offline in EnKo whereas there is about 3 % improvement in EnZh and about 2 % improvement in DeEn .
It is roughly proportional to the monotonicity improvements shown in Figure 4 .
Evaluation on Online References
We test wait -k models on our in- house EnKo online and offline testsets of 150 lines .
We choose EnKo because the impact of reordering - and - refinement is the greatest in that pair .
The source sentences of both testsets are identical .
The online references are constructed by a professional interpreter under a simulated simultaneous interpretation scenario and the offline references are constructed by the same interpreter assuming a typical translation scenario .
In construction of online references interpreter was encouraged to perform monotonic interpretation rather than anticipation .
BLEU scores are computed with both online and offline references for each trained model .
Figure 6 plot the subtraction of BLEU scores on offline references from BLEU scores on online references .
It is noticeable that the wait -k models trained on offline corpus have negative value while all the models trained on generated corpus present positive values , which implies the effectiveness of our approach .
Overall , the substantial differences at k = 6 may suggest that the chunk size utilized by human interpreter has comparable value .
Related Work
Due to word order differences between languages , SimulMT training often face situations where anticipation is required .
Note that word order difference is observed to be problematic even for human interpreters ( Al - Rubai'i , 2004 ; Tohyama and Matsubara , 2006 ) .
Chen et al. ( 2020 ) suggest using pseudo-references which involve utilizing wait -k inference output to limit " future anticipation " in training .
Zhang et al. ( 2020 b ) utilize NMT teachers to implicitly embed future information in their SimulMT students for better anticipation performance .
Zhang et al. ( 2020a ) study adaptive policy to tackle this problem - authors suggest an adaptive SimulMT policy that dictate READ / WRITE actions based on whether " meaningful units " are fully formed with consumed input tokens .
Related work in the broader SimulMT and para-phrasing domain is presented in Appendix F.
Conclusion Most of SimulMT models are trained on offline translation corpora , which could lead to limitation in translation quality and achievable latency , especially in non-monotonic language pairs .
To address this problem , we propose a reordering - andrefinement algorithm to generate monotonically aligned online target with NAT model .
We then train widely used wait -k SimulMT models on this newly generated corpus .
Resulting models show BLEU score improvement and significant enhancement on monotonicity in multiple language pairs .
