title
TUDa at WMT21 : Sentence -Level Direct Assessment with Adapters
abstract
This paper presents our submissions to the WMT2021 Shared Task on Quality Estimation , Task 1 Sentence -Level Direct Assessment .
While top-performing approaches utilize massively multilingual Transformer - based language models which have been pre-trained on all target languages of the task , the resulting insights are limited , as it is unclear how well the approach performs on languages unseen during pre-training ; more problematically , these approaches do not provide any solutions for extending the model to new languages or unseen scripts - arguably one of the objectives of this shared task .
In this work , we thus focus on utilizing massively multilingual language models which only partly cover the target languages during their pre-training phase .
We extend the model to new languages and unseen scripts using recent adapter-based methods and achieve on par performance or even surpass models pre-trained on the respective languages .
Introduction In Machine Translation ( MT ) , the Quality Estimation ( QE ) task attempts to characterize the quality of a translation , without the availability of a ( goldlabel ) reference translation .
The introduction of a QE system would consequently allow for the automatic analysis of machine - translated sentences without costly human reference translation , with numerous applications , such as : the selection of candidate translations , the estimation of human editing effort , or the detection of low-quality or misleading translations ( Kepler et al. , 2019 ) .
However , in order to acquire training data , professional human translators are required to score the translation quality of many examples , making labeled data difficult to obtain , especially for low-resource languages .
This highlights the importance of crosslingual zero-shot transfer of QE systems , one of the objectives of the WMT21 shared task ( Specia et al. , 2021 ) , which introduces zero-shot evaluation sets of four new language pairs .
Previous approaches have predominantly focused on languages for which training data is available , such as the QE task at WMT20 .
The best results were obtained by fine-tuning massively multilingual Transformer - based language models ( Vaswani et al. , 2017 ) such as multilingual BERT ( mBERT ) ( Devlin et al. , 2019 ) or XLM -R ( Conneau et al. , 2020 ) Ranasinghe et al. , 2020 b ; Sun et al. , 2020a ; Nakamachi et al. , 2020 , inter alia ) , on the target QE tasks .
These supervised methods considerably outperform unsupervised methods ( Zhao et al. , 2020 ; Fomicheva et al. , 2020c ; Sun et al. , 2020a ; Song et al. , 2021 ) even in zero-shot settings ( Sun et al. , 2020a ) .
However , analyzing the applicability of fine-tuning multilingual models on the target language pairs that are covered during pre-training considerably limits the generated insights .
They are only applicable to the ?100 languages covered during pre-training , excluding the remaining majority of languages as the " curse-of-multilinguality " ( Conneau et al. , 2020 ) prohibits the over 7000 languages in the world ( Joshi et al. , 2020 ) to be represented within a single model
In this work , we thus aim to address these limitations by utilizing multilingual language models that only cover a subset of the target languages .
Here we focus on mBERT which - in contrast to XLM - R- has not seen the languages Sinhala , Pashto , and Khmer , all part of the WMT21 shared task .
As the script of Sinhala and Khmer are not included in the mBERT vocabulary , it is impossible for the corresponding tokenizer to correctly tokenize text in those languages .
Following Pfeiffer et al . ( 2020 b
Pfeiffer et al. ( , 2021 b we thus propose an adapter- based approach to extend mBERT to new languages and new scripts .
Our contributions are as follows : 1 ) we analyze adapter- based supervised approaches for QE and demonstrate their competitive performance compared to full model fine-tuning , both in supervised as well as zero-shot settings ; 2 ) we use recent adapter based methods to extend mBERT to unseen languages and scripts , achieving considerable performance gains over standard mBERT for unseen languages ; 3 ) we demonstrate competitive performance of our adapted mBERT approach compared to XLM -R , which has seen the respective languages during pre-training .
We release our code and adapters at https://github.com/ Aaronsom/wmt21-qe-tudarmstadt/.
Method
We describe our adapter- based approaches for supervised QE and the extension to unseen languages .
Task Formulation
We model QE as a regression task .
The Transformer receives as input both the source sentence and the translation hypothesis and is trained to predict the quality score for the sentence pair .
For this , we take the final contextualized representation of the special [ CLS ] - token produced by the Transformer and feed it into a multi-layer regression head to compute the predicted quality f ( s , t ) : f ( s , t ) = W 2 ? ( tanh ( W 1 ? r [ CLS ] ( s , t ) ) ) ( 1 ) with W 1 ? R h?h , W 2 ? R 1?h , tanh is the hyperbolic tangent , h is the hidden dimension of the Transformer , and r [ CLS ] ( s , t ) is the output representation of the [ CLS ] - token for the sourcetranslation input pair s , t.
We train the model using mean squared error .
Adapters
Adapters are randomly initialized weights , newly introduced at every layer of the pre-trained Transformer model .
During fine-tuning , only the adapter weights ( and the regression head ) are updated while the remaining model weights are kept frozen .
Houlsby et al. ( 2019 ) propose a feed-forward bottleneck adapter architecture consisting of a down- projection , a non-linearity , and finally an upprojection , both after the multi-attention as well as after the feed -forward component at every Transformer layer .
We use the adapter architecture proposed by Pfeiffer et al . ( 2021a ) which achieves on par results while reducing the number of trainable parameters of Houlsby et al . ( 2019 ) by only placing adapters after the feed-forward component ( see Figure 1a ) .
Adapters at layer l are defined as follows : a l ( h l , r l ) = U l ? ( ReLU ( D l ? h l ) ) + r l ( 2 ) where D l ?
R h r ?h , U l ?
R h? h r , ReLU is the rectified linear unit , h l is the hidden input representation , r l is the residual after the fully - connected layer , and r is the reduction factor -a hyperparameter that decides how much the adapter compresses the hidden representation .
Extending to Unseen Languages
While both XLM -R and mBERT have been pretrained on a large number of languages , XLM - R has seen all languages appearing in the WMT21 dataset , while mBERT has not been pre-trained on Sinhala , Khmer , and Pashto .
Further , the scripts of Sinhala and Khmer are not covered by mBERT 's vocabulary .
We thus follow Pfeiffer et al .
( 2020 b
Pfeiffer et al. ( , 2021 b to extend both the latent Transformer as well as input embedding representations to the respective languages , using adapter- based approaches .
Language Adapters .
Language adapters ( LAs ) ( Pfeiffer et al. , 2020 b ) are trained to encode idiosyncratic , language -specific information , and transform the underlying multilingual model 's latent representations to better align with the respective languages .
Correspondingly , they are trained monolingually using the masked language modeling ( MLM ) objective on unlabeled textual data in the target language .
Extending to unseen scripts .
Word piece tokenizers can ( arguably inadequately ( Rust et al. , 2021 ) ) tokenize unseen languages that are written in seen scripts , with a fall - back character - level tokenization .
Unfortunately , these tokenizers fail for unseen scripts , as even character - level tokens are not part of the vocabulary , leaving the tokenizer only with instantiating unknown placeholder tokens ( UNKs ) as alternatives .
Consequently , even by extending the overall capacity of the language model using language adapters , the model will not be able to adequately represent the respective languages .
To extend the model to unseen scripts , we learn a new language -specific tokenizer and train a new embedding matrix , initialized with lexically overlapping tokens of the original embedding matrix , and random initialization for the remaining unseen tokens ( Pfeiffer et al. , 2021 b ) .
Here , language adapters are trained together with the new embedding matrix , while the pre-trained Transformer weights are frozen .
Similar to standard LAs , these components are trained monolingually using the MLM objective on unlabeled textual data in the target language .
Task Adapters .
For target task fine-tuning we stack task -specific adapters on top of the pretrained LAs .
For most tasks , sentences of only one language are passed through the model , while for QE the original sentence in the source language and the translation of the target language are simultaneously passed through the model .
The tokens of the respective languages are thus passed through their respective LA .
The subsequent task adapter is shared between the two languages ( see Figure 1 b ) .
For cross-lingual transfer , the LAs of the training languages are replaced with the LAs of the evaluation languages .
For this reason , not only the transformer weights but also the LAs are frozen during training and only the task adapters are fine-tuned on the target task .
For languages with scripts not covered during pre-training , the new embedding matrix is used .
The embedding representations are subsequently concatenated ( see Figure 1 c ) .
Data
The sentence - level direct assessment task of WMT21 builds upon the data of WMT20 task 1 ( Fomicheva et al. , 2020a ) .
The WMT20 dataset consists of seven language pairs ranging from the high-resource English - German ( En- De ) and English - Chinese ( En - Zh ) , to the medium-resource Romanian - English ( Ro-En ) , Estonian - English ( Et- En ) and Russian - English ( Ru-En ) , and the lowresource Sinhalese -English ( Si-En ) and Nepalese -English ( Ne -En ) .
For each pair , sentences in the source language are sampled from Wikipedia ( or in the case of Russian , from Wikipedia and Reddit ) , translated with fairseq to the target language , and then annotated by at least three professional translators with Direct Assessment ( DA ) ( Guzm ? n et al. , 2019 ) .
The DA scores are z-normalized for each annotator and averaged to form the final score .
For each of the seven language pairs , the dataset contains 7000 training pairs and 1000 test and dev pairs .
The WMT21 dataset extends the WMT20 dataset by providing new test sets - with unpublished labels - consisting of 1000 sentences for each language pair of the WMT20 dataset .
In addition , they provide testsets for four new language pairs for zero-shot evaluation , each compris-ing of 1000 sentence pairs with unpublished labels : English - Czech ( En-Cs ) , English - Japanese ( En-Ja ) , Pashto-English ( Ps-En ) , Khmer-English ( Km-En ) .
Experiments
We describe our experimental setup along with the training and implementation details .
Training & Model Hyperparameters .
We initialize our models with mBERT and XLM -R ( both large and base-sized ) .
We use a reduction factor r of 8 for our task adapters .
Language adapters use r = 2 and have been trained on Wikipedia articles of the respective language .
The additional embeddings for Khmer and Sinhala contain 10 k tokens each and have been fine-tuned together with the respective LAs on the Wikipedia data .
We fine- tune our models using AdamW ( Loshchilov and Hutter , 2019 ) with a linear learning rate schedule without warm - up .
We simulate early stopping by storing the checkpoint with the best dev set performance - evaluating every 500 steps .
For all models , we use a learning rate of 1e - 4 and a batch size of 8 .
We train each model for 8 k steps .
Hyperparameters have been chosen based on the WMT20 dev set performance .
We have chosen the above hyperparameters from the following values ranges : learning rate { 5e - 5 , 1e - 4 , 2e - 4 , 5e - 4 } , batch size { 4 , 8 , 16 , 32 , 96 } , reduction factor r for the task adapters { 4 , 8 , 16 } , and training steps { 2 k , 3k , 5 k , 8 k , 10 k }.
Implementation Details .
To train adapters , use the AdapterHub framework ( Pfeiffer et al. , 2020a ) which builds upon the Hugging Face Transformers library ( Wolf et al. , 2020 ) .
In each batch we samples examples from only one language pair .
Experimental Setup .
We evaluate the performance of our QE models using Pearson correlation between the predicted quality and the actual label .
We evaluate our adapter approaches in an ALL and a leave- one - out zero-shot setup ( ZERO ) .
In the ALL setup , we train a model on all seven language pairs with training data available and then evaluate the model on all eleven language pairs - the seven pairs with training data and the four pairs without .
In the ZERO setting , for each of the seven language pairs which have a training set , we train a model with six of the pairs and then evaluate on the leftout seventh pair .
We evaluate both the large-sized XLM -R with adapters ( denoted A-XLMRLARGE ) and basesized mBERT and XLM -R with adapters ( denoted A-MBERT and A-XLMRBASE respectively ) .
For mBERT , we use both language adapters ( + LA ) and additional embeddings for Sinhala and Khmer ( + EMB ) .
We denote the setup with both as A+LA+EMB - MBERT .
We also consider adapter ensembles for XLM -R .
Here , we train five adapters in the ALL setup using different random seeds .
During the evaluation , we average the predictions of the five adapters for the final prediction .
Results & Discussion
We present the Pearson correlation results for our models on the WMT21 test set .
The reported values are obtained from the CodaLab competition .
1
Language Extension Results
We present our results on the WMT21 test set for our two setups .
The results for the ALL setup where we train with all seven pairs that have training data and then evaluate the model on all eleven pairs , i.e. the seven with training data and the four which are zero-shot , are found in Table 1 .
The leave- one- out ZERO results where we train on six of the seven pairs with training data and then evaluate in a zeroshot setup on the left-out pair are in Table 2 .
We consider how our language extension methods improve the results for the unseen languages Sinhala , Khmer , and Pashto .
We first evaluate how much we gain by representing input in the unseen script with extra embeddings instead of simply replacing all by the [ UNK ] - token .
For this , we compare A-MBERT with A+EMB - MBERT .
When we train with the Si-En data in ALL , the additional embeddings only give a relatively small performance boost of 0.04 points on top of already quite good results .
This is unexpected since half the input is not correctly represented .
We investigate this in more detail in ?5.2 .
In zero-shot ( Table 2 for Si-En and Table 1 for Km-En ) , the extra embeddings result in greatly improved results for Si- En by 0.25 points and by 0.05 points for Km- En .
Next , we compare models with and without language adapters in both setups .
For the languages seen by mBERT during pre-training , there is little difference between A ( + EMB ) - MBERT and A+LA ( + EMB ) -MBERT in both setups .
This Table 2 : Pearson correlation results of the leave- one- out ZERO setup for zero-shot results of the seven language pairs with training set .
We report the results for our adapters with mBERT and XLM -R ( base & large ) .
For mBERT , we extend the model with language adapters ( + LA ) and additional embeddings for Sinhala and Khmer ( + EMB ) aligns with the findings by Pfeiffer et al . ( 2020 b ) and suggests that language adapters are less helpful for seen languages .
For the three pairs with unseen languages , the language adapters can greatly improve the performance .
In zero-shot situations ( Table 2 for Si-En and Table 1 for the other two ) , we gain 0.18 points for Si-En , 0.07 for Km-En , and 0.28 points for Ps- En .
Similar to extra embeddings , when we train with the Si-En data in ALL , we only gain 0.03 points more with language adapters .
Ro-En
Ru-En
En-De
En-Zh
En - Cs En-Ja Finally , we compare mBERT with language adapters and additional embeddings ( A+LA +EMB - MBERT ) to a base-sized XLM -R A-XLMRBASE .
This comparison is not ideal due to the differences in pre-training between the Transformers - training set , selected languages , etc . - but we can assume that for the unseen languages , XLM -R serves as an estimated upper bound for the performance .
For seen language Table 3 : Pearson correlation for mBERT with adapters ( A- MB ) - without language extensions - on the WMT20 test set trained with all pairs where we use both source and translation ( S+T ) , only the translation ( T ) , or only the source ( S ) during training .
Evaluation is performed with both source and translation .
Ro-En
Ru-En
En-De
En-Zh
Analysis of Results on Trained Pairs
For the three unseen languages , we achieve large performance gains in zero-shot scenarios .
However , while we witness large performance gains in zero-shot scenarios of the adapter- based methods , the difference considerably smaller when training data in the target language is available .
Intuitively , we would expect a larger boost , considering half the input is in an unknown language and mostly not encoded .
However , these results align with previous findings .
formance of training with the complete pair .
We are able to reproduce these findings for WMT20 in Table 3 , and achieve similar results for Si- En when passing only the English translation as input to the model , compared to when training on both inputs .
However , when training with only the ( Sinhala ) source , we witness the expected drop in performance .
It is likely that in the zero-shot setup , the model cannot learn to exploit the statistical cues that allow it to function without the source sentence .
Hence , we obtain more appropriate representations with adapter-based methods where the languagespecific word-embedding representations result in considerable performance gains .
Ensembles
Ensembles have been used in previous work with great success ( Ranasinghe et al. , 2020a ; Fomicheva et al. , 2020 b ; Nakamachi et al. , 2020 ) .
With an adapter ensemble , the underlying Transformer weights are re-used resulting in a very parameterefficient setup-our ensemble with five adapters adds only 6.5 % more parameters on top of the large XLM -R Transformer .
However , our adapter ensemble A-XLMRLARGE ENSEMBLE only brings a slight performance boost , smaller than the reported boost by the ensembles of previous works .
More work is needed here to investigate why this is the case .
Comparison to Fully Fine-Tuned Models
We evaluate the general performance of adapters for the QE task in comparison to fully fine-tuned models .
For this , we compare our models on the WMT20 test set against the top submissions of the WMT20 shared task in Table 4 .
We find that they achieve competitive results with fully fine-tuned models that do not employ additional techniques like ensembles in both the ALL and ZERO setups .
Our highest - scoring submission , A-XLMRLARGE ENSEMBLE , places in the midfield for the WMT21 competition .
Parameter Count Adapters are considerably more parameter efficient with respect to the number of fine-tuned parameters , compared to fully fine-tuned models .
The number of adapter parameters is equivalent to only 1.3 % of the Transformer parameters for our models .
This makes adapters very lightweight for model sharing or for loading multiple adapters on the same GPU , e.g. , for language adapters or for multiple task adapters in a pipeline ( Nguyen et al. , 2021 ; .
The extension for the unseen languages for mBERT also adds only a small number of parameters : 2.4 % for each language adapter and 1.4 % for each monolingual embedding .
Conclusion
In this work , we proposed the use of adapters to fine-tune massively multilingual Transformers for the sentence - level QE task .
We demonstrated that adapters are able to achieve competitive results with fully fine-tuned models .
However , as fully fine-tuned approaches are limited to the languages seen during pre-training , we have employed recent language extension methods to integrate languages unseen by mBERT .
We extended mBERT with language adapters and monolingual embeddings for Sinhala , Khmer , and Pashto .
These methods greatly improved the zero-shot performance of the model and largely closed the gap to XLM - R which has been pre-trained on all languages appearing in WMT21 .
This demonstrates that our approach is applicable , not only to languages seen during pretraining , but also to unseen languages , even with unseen scripts .
This suggests that our method is able to extend multilingual models to a wider range of language not covered during pre-training .
We suggest that future shared tasks should consider disentangling languages which massively multilingual language models have been pretrained on , from those that are unseen during pretraining , to more closely reflect realistic scenarios , as the majority of languages cannot be represented within a single model ( Conneau et al. , 2020 ) . adapter architecture used by us .
Each adapter comprises of an down - and upprojection and is inserted after the feed -forward layer within each Transformer layer .
Task adapters for QE with multiple language adapters for the multilingual input .
The input parts are passed through the respective language adapter before the entire representation is passed to the task adapter .
Extra monolingual embeddings for scripts and languages not included in the multilingual embeddings alongside the multilingual embeddings .
The input embedding is chosen depending on the input language .
