title
Context-Interactive Pre-Training for Document Machine Translation
abstract
Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information .
However , it typically suffers from a lack of doc-level bilingual data .
To remedy this , here we propose a simple yet effective context- interactive pre-training approach , which targets benefiting from external largescale corpora .
The proposed model performs inter sentence generation to capture the crosssentence dependency within the target document , and cross sentence translation to make better use of valuable contextual information .
Comprehensive experiments illustrate that our approach can achieve state - of - the - art performance on three benchmark datasets , which significantly outperforms a variety of baselines .
Introduction Document machine translation ( Doc - MT ) aims at utilizing the surrounding contexts of the source sentence to tackle some linguistic consistency problems ( e.g. , deixis , ellipsis , and lexical cohesion ) in translation ( Tiedemann and Scherrer , 2017 ) .
However , due to the introduction of extra contexts , it also presents several intractable challenges : ( 1 ) Data scarcity of document- level bilingual corpora .
Since most bilingual corpora are preserved by sentence , well - aligned document - level data is relatively scarce , especially for low-resource languages or domains .
Such a data sparsity not only impairs the effective training of neural machine translation ( NMT ) models , but also tends to result in potential overfitting .
( 2 ) Effective utilization of valuable information contained in extra contexts .
Although some efforts ( Wang et al. , 2017 ; Tu et al. , 2018 ) have strived to incorporate contextual information via various architectures , they only observe minor performance gains compared with traditional sentence machine translation ( Sent - MT ) .
Recent work also reveals that contextual information cannot be fully leveraged by some existing approaches , where the source contexts tend to act as the data noise enriching the training signals .
( 3 ) Modeling of cross-sentence dependency within the target document .
Since the input of Doc - MT focuses on documents consisting of multiple sentences , the decoder should be able to deal with some discourse phenomena like coreference resolution , lexical cohesion , and lexical disambiguation .
( Voita et al. , 2019 b ) .
This goal requires the modeling of cross-sentence dependency within the target document .
To tackle the above three challenges , here we propose a simple yet effective context-interactive pre-training approach for Doc- MT .
The proposal consists of three pre-training tasks , whose sketch is presented in Figure 1 .
Specifically , the cross sentence translation task ( CST in Figure 1 ( A ) ) strives to generate the target sentence in the absence of the source sentence and only based on the source contexts .
With such a goal , the model is encouraged to maximize the utilization of extra contexts .
To capture interactions between multiple sentences in the target document so that the discourse phenomena can be modeled , we conduct inter sentence generation ( ISG in Figure 1 ( B ) ) that aims to predict the inter sentence based on the target surrounding contexts .
This task can be regarded as discourse language modeling that injects the cross-sentence dependency within the target document into the decoder of the translation model .
We also introduce parallel sentence translation ( PST in Figure 1 ( C ) ) to alleviate the lack of doc-level bilingual corpora and achieve knowledge transfer from abundant sentlevel parallel data to limited doc-level parallel data .
In order to avoid the catastrophic forgetting of pretrained model in downstream fine-tuning , elastic weight consolidation ( EWC ) regularization is introduced to further enhance the model performance .
We perform the evaluation on three benchmark datasets and results illustrate that our approach can achieve state - of - the - art performance , which is able to outperform a variety of baselines .
Related Work Document machine translation ( Doc - MT ) aims to translate the source sentence into another different language in the presence of additional contextual information .
The mainstream advances of this research field can be divided into three lines : uniencoder structure , dual-encoder structure , and pretrained models .
Uni-encoder structure .
This line of research aims at performing Doc - MT based on a universal Transformer , which takes the concatenation of the additional contexts and the source sentence as the input .
Tiedemann and Scherrer ( 2017 ) explores multiple different concatenation strategies and proves that the translation with extended source achieves the best performance .
Bawden et al. ( 2018 ) presents several new discourse testsets , which aims to evaluate the ability of the models to exploit previous source and target sentences .
utilizes dynamic or topic cache to model coherence for Doc - MT by capturing contextual information either from recently translated sentences or the entire document .
Going a step further , they presents an inter-sentence gate model to encode two adjacent sentences and controls the amount of information flowing from the preceding sentence to the translation of the current sentence with an inter-sentence gate .
Tu et al. ( 2018 ) augments translation model with a cache- like memory network that stores recent hidden representations as translation history .
Yang et al. ( 2019 ) introduce a query - guided capsule networks into document - level translation to capture high - level capsules related to the current source sentence .
proposes a unified encoder to process the concatenated source information that only attends to the source sentence at the top of encoder blocks .
Dual-encoder structure .
This line of work tends to adopt two encoders or another components to model the source sentences and the document- level contexts .
Wang et al. ( 2017 ) summarize the source history in a hierarchical way and then integrate the historical representation into translation model with multiple strategies .
Maruf and Haffari ( 2018 ) takes both source and target document context into account using memory networks , which modeling Doc - MT as a structured prediction problem with inter-dependencies among the observed and hidden variables .
introduces a light context encoder to represent source context and performs information fusion with the unidirectional multi-head attention .
Werlen et al. ( 2018 ) uses a hierarchical attention network ( HAN ) with two levels of abstraction : word level abstraction allows attention to words in previous sentences , and sentence level abstraction allows access to relevant previous sentences .
Source and target context both can be exploited .
Voita et al. ( 2019 b ) introduces a two -pass framework that first translates each sentence with a context-agnostic model , and then refines it using context of several previous sentences .
Furthermore , Voita et al. ( 2019a ) presents a monolingual Doc-Repair model that performs automatic post-editing on a sequence of sentencelevel translations to correct inconsistencies among them .
investigates multi-encoder approaches in Doc - MT and find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator .
Maruf et al. ( 2019 ) presents a hierarchical context - aware translation model , which selectively focus on relevant sentences in the document context and then attends to key words in those sentences .
Methodology
Following prior work , we translate the i-th source sentence x i into the i-th target sentence y i in the presence of extra source contexts c = ( x i?1 , x i + 1 ) , where x i?1 and x i+1 refer to the predecessor and successor of x i respectively .
We adopt Transformer as the model architecture of pre-training and machine translation .
The model is trained by minimizing the negative log-likelihood of target sequence y conditioned on the source sequence x , i.e. , L = ?logp ( y | x ) .
Readers can refer to Vaswani et al . ( 2017 ) for more details .
We introduce our approach based on EN?DE Doc-MT .
Pre-Training Tasks
Figure 1 shows the sketch of our context- interactive pre-training approach , elaborated on as follows .
Cross Sentence Translation ( CST )
When translating the i-th sentence x i and the source context c = ( x i?1 , x i + 1 ) into the i-th target sentence y i , prior approaches tend to pay most attention on x i ( Li et al. , 2019 ) , resulting in the neglect of c .
To maximize the use of the source context c , we propose cross sentence translation ( CST ) to encourage the model to more effectively utilize the valuable information contained in c .
We mask the whole source sentence x i in the model input , and enforce the model to generate the target sentence y i only based on c = ( x i?1 , x i + 1 ) .
To be specific , we pack both the source context c and the mask token [ mask ] as a continue span , and employ a special token </ s> to indict the end of each sentence .
To distinguish texts from different languages , we add language identifier ( e.g. , < en > for English and < de > for German ) to the ends of both the source input and target output .
Figure 1 ( A ) presents the illustration of this task on EN - DE translation , where the input of Transformer is the concatenation of ( x i?1 , < mask > , x i + 1 ) and the target output is y i .
Inter Sentence Generation ( ISG ) Voita et al. ( 2019 b ) has demonstrated that the cross-sentence dependency within the target document can effec-tively improve the translation quality .
Transformer decoder should be able to model the corresponding historical information to improve coherence or lexical cohesion and other aspects during translation .
Motivated by this , here we propose inter sentence generation ( ISG ) to capture the cross-sentence dependency among the target output .
The ISG task aims to predict the inter sentence y i based on its surrounding predecessor y i?1 and successor y i +1 .
In this way , the model is trained to capture the interactions between the sentences in the target document .
Besides , the training of ISG only requires the monolingual document corpora of the target language , which effectively alleviates the lack of doclevel parallel data in Doc- MT .
Figure 1 ( B ) presents the detailed illustration , where the model input is the concatenation of ( y i?1 , < mask > , y i +1 ) and the target output is y i .
Both source and target language identifiers are < de >. Parallel Sentence Translation ( PST )
In practice , the available sent-level parallel corpora usually present larger scale than doc-level parallel corpora .
Thus , here we introduce parallel sentence translation ( PST ) performing context-agnostic sentence translation , which only requires sent-level parallel data .
This further alleviates the lack of the doclevel parallel data in Doc-MT .
The illustration of PST is presented in Figure 1 ( C ) , where the input is the concatenation of ( < none > , x i , < none > ) and the target output is y i .
1 The source and target language identifiers are < en> and < de > , respectively .
EWC - Based Fine-Tuning .
After finishing the pre-training , the pre-trained transformer is used as the model initialization for subsequent finetuning on downstream datasets .
As shown in Figure 1 , the input of Transformer in this scenario is ( x i?1 , x i , x i + 1 ) , i.e. , the concatenation of the i-th source sentence x i and its surrounding context c = ( x i?1 , x i + 1 ) .
The desired output is the i-th target sentence y i .
The source and target language identifiers are same as PST .
However , obvious catastrophic forgetting has been observed during fine-tuning .
As fine-tuning continues , the model performance exhibits degradation .
Due to large-scale model capacity and limited downstream datasets , pre-trained models usually suffer from overfitting .
To remedy this , here we introduce Elastic Weight Consolidation ( EWC ) regularization ( Kirkpatrick et al. , 2016 ) . EWC regularizes Table 1 : The detailed illustration of different tasks .
" SLI " and " TLI " denotes the source and target language identifier , respectively .
" Use Mono- Doc " , " Use Bi-Doc " and " Use Bi-Sent " means that the corresponding task can use monolingual doc-level , bilingual doc-level , bilingual sent-level corpora , respectively .
the weights individually based on their importance to that task , which forces the model to remember the original language modeling tasks .
Formally , the EWC regularization is computed as : R = i ?
2 F i ( ?
i ? ? * i ) 2 ( 1 ) where ? is a hyperparameter weighting the importance of old LM tasks compared to new MT task , and i labels each parameter .
The final loss J for fine- tuning is the sum of negative log-likelihood in all pre-training tasks and newly introduced R , i.e. , J = L CST + L ISG + L PST + R .
We summarize the key information of our approach in Table 1 , which also shows the available data of different tasks .
Experiments
Settings
We train Transformer consisting of 12 encoder and 12 decoder layers with 1024 hidden size on 16 heads .
We adopt the public mBART.CC25 released by Liu et al . ( 2020 ) as the initialization .
For CST task , the pre-training data consists of : TED , Europarl , News Commentary and Rapid corpus .
The monolingual target documents used in ISG task are extracted from Wikipedia .
For PST task , we sample bilingual sentences in NewsCrawl utill 2018 .
We use sentence piece model ( Kudo and Richardson , 2018 ) to tokenize all data .
Gradient accumulation is used to simulate the batch size of 128 K tokens .
We use Adam optimizer with linear learning rate decay .
The learning rate and dropout is set to 3e?5 and 0.1 , respectively .
We set ? in Eq. 1 to 0.01 .
We evaluate on three EN - DE Doc-MT datasets provided by Maruf et al . ( 2019 ) : TED , News , and Europarl and perform limited grid-search of hyperparameter .
Baselines Unpretrained models .
Transformer ( Vaswani et al. , 2017 ) performs context-agnostic sent-level translation and HAN ( Werlen et al. , 2018 ) employs hierarchical attention to capture extra contexts .
SAN ( Maruf et al. , 2019 ) utilizes top-down attention to selectively focus on relevant sentences and QCN ( Yang et al. , 2019 ) uses query - guided capsule networks to capture the related capsulese .
Pretrained models .
Flat- Transformer apply BERT as the initialization of encoder .
We also implement the parallel sentence translationbased pre-training with mBART ( Liu et al. , 2020 ) initialization as the most comparable baseline .
To have a fair comparison , we adopt multi-BLEU as the evaluation metric .
We first conduct SPM - based detoken on the generated texts and then use Moses to re-tokenize all texts like the baselines .
Main Results
Table 2 shows the performance of different systems .
Results first confirm that large-scale pre-training can effectively accomplish model transferring and advance the performance of Doc- MT .
Besides , we can observe significant performance gain for our approach compared to the baselines .
For instance , it surpasses the mBART initialized model with PST by 0.72 BLEU .
With the proposed pre-training tasks , our approach succeeds in acquiring more effective knowledge from external large-scale corpora , leading to better translation quality .
Incremental Analysis
Here we perform further incremental analysis .
We treat Transformer with mBART initialization as the base model and cumulatively add each pre-training task until the full approach is rebuilt .
The results are shown in Table 3 .
We can observe that the removal of the parallel sentence translation ( PST ) task results in the largest performance degradation .
First , the scale of parallel sentences used for PST far exceeds that for the other two tasks , bringing the significant performance gains ;
In addition , PST closely resembles the downstream Doc - MT task , encouraging more effective knowledge transfer .
Besides ,
Table 3 also reveals that other CST and ISG tasks play an active role in improving translation quality .
By masking the whole source sentence in the input via CST , the model is encouraged to more effectively extract and utilize valuable information from extra contexts .
With the target doc-level language modeling , the cross-sentence dependency within the document is better captured .
Both contributes to improving the quality of Doc-MT .
Effectiveness of EWC Regularization
To avoid the catastrophic forgetting of pre-trained models in downstream fine-tuning , we introduce EWC regularization to force the model to remember the original language modeling task .
Table 4 presents the comparison of our approach with or without EWC regularization , demonstrating its effectiveness in improving model performance .
Results show that EWC regularization can achieve consistent improvements on various datasets , increasing the average BLEU score from 29. 24 to 29.54 .
By weighing the original LM task and newly introduced NMT task based on the importance of parameters , the overfitting of the pre-trained model on the limited downstream data is effectively alleviated , bringing consistent performance gains .
Conclusion
This work presents context-interactive pre-training to benefit document machine translation from external large-scale mono or bi-lingual corpora .
The proposed approach strives to capture the crosssentence dependency within the target document via inter sentence generation , and utilize valuable information contained in the source context via cross sentence translation .
Extensive experiments illustrate that our approach can consistently outperform extensive baselines , achieving state - of - the - art performance on various benchmark datasets .
Figure 1 : 1 Figure 1 : The sketch of our proposed context-interactive pre-training for Doc- MT .
The pre-training tasks consist of : ( A ) CST , ( B ) ISG , and ( C ) PST .
The lower - right sub-figure ( D ) shows the illustration of downstream fine-tuning .
