title
Contrastive Learning for Context-aware Neural Machine Translation Using Coreference Information
abstract
Context - aware neural machine translation ( NMT ) incorporates contextual information of surrounding texts , that can improve the translation quality of document - level machine translation .
Many existing works on context- aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results .
However , most existing works rely on crossentropy loss , resulting in limited use of contextual information .
In this paper , we propose CorefCL , a novel data augmentation and contrastive learning scheme based on coreference between the source and contextual sentences .
By corrupting automatically detected coreference mentions in the contextual sentence , CorefCL can train the model to be sensitive to coreference inconsistency .
We experimented with our method on common context - aware NMT models and two document - level translation tasks .
In the experiments , our method consistently improved BLEU of compared models on English - German and English -Korean tasks .
We also show that our method significantly improves coreference resolution in the English - German contrastive test suite .
Introduction Neural machine translation ( NMT ) has achieved impressive performances on translation quality , due to the introduction of novel deep neural network ( DNN ) architectures such as encoder-decoder model ( Cho et al. , 2014 ; Sutskever et al. , 2014 ) , and self-attentional networks like Transformer ( Vaswani et al. , 2017 ) .
The state- of- the- art NMT systems are now even comparable with human translators in sentence - level performance .
However , there are a number of issues on document- level translation ( L?ubli et al. , 2018 ) .
These include pronoun resolution across sentences ( Guillou et al. , 2018 ) , which needs cross-sentential contexts .
To incorporate such document- level con-textual information , several methods for contextaware NMT have been recently proposed .
Many of the works have focused on introducing new model architectures like multi-encoder models for encompassing contextual texts of the source language .
These works have shown significant improvement in addressing discourse phenomena such as anaphora resolution mentioned above , as well as moderate improvements in overall translation quality ( Lopes et al. , 2020 ) .
Despite some promising results , most of the existing works have trained the model by minimizing cross-entropy loss , making the model rather exploit contextual information implicitly such as a form of regularization ( Kim et al. , 2019 ; . Data augmentation for context- aware NMT is also an important issue , despite that recent works have focused on back - translation ( Huo et al. , 2020 ) .
In this paper , we propose a Coreference - based Contrastive Learning for context - aware NMT ( CorefCL ) , a novel data augmentation and contrastive learning scheme leveraging coreference information .
Cross-sentential coreference between the source and target sentence can be a good source of training signal for context - aware NMT since it occurs when one or more expressions refer to the same entity , thus reflects dependencies between the source and contextual sentences .
CorefCL starts by conducting automatic annotation of coreference between the source and contextual sentences .
Then , the referred mentions on contextual sentences are corrupted by removing and / or replacing tokens to generate contrastive examples .
With those contrastive examples , we introduce a contrastive learning scheme equipped with a max-margin loss which encourages the model to discriminate between the original examples and the contrastive ones .
By doing so , CorefCL makes the model more sensitive to cross-sentential contextual information .
We experimented with CorefCL on three English - German corpora and one English -Korean document- level corpus , including WMT , IWSLT TED talk , and OpenSubtitles ' 18 English - German subtitles translation task , and a web-crawled English -Korean subtitles translation .
In all translation tasks , CorefCL consistently improves overall BLEU over baseline models without CorefCL .
On experiments with three common context - aware model settings , we show that improvements by CorefCL are also model- agnostic .
Finally , we show that the proposed method significantly improved the performance on ContraPro , an English - German contrastive coreference benchmark .
Related Works
Context- aware NMT
Context - aware machine translation has been vigorously studied to exploit the crucial context information in surrounding sentences .
Recent works have shown that contextual information can help the model to generate not only more consistent but also more accurate translation ( Smith , 2017 ; Kim et al. , 2019 ) .
In particular , introduced a context- aware Transformer model which is able to induce anaphora relations , Miculicich et al . ( 2018 ) showed that a model using cross-sentential contextual information significantly outperforms in document- level translation tasks , and Yun et al . ( 2020 ) insisted that context - aware models record the best performance especially in spoken language translation tasks where mandatory information tend to be sparse over multiple sentences .
The simplest method for context - aware machine translation is to concatenate all surrounding sentences and treat the concatenated sequence as a single sentence ( Tiedemann and Scherrer , 2017 ) .
Although the concatenation strategy boosted Transformer architectures in multiple tasks ( Tiedemann and Scherrer , 2017 ; Yun et al. , 2020 ) , it lagged behind efficiency as the Transformer architecture has limited long- range dependency ( Tang et al. , 2018 ) .
To improve the efficiency , an additional encoder module is introduced to encode only the context sentences ( Libovick ?
and Helcl , 2017 ; Jean et al. , 2017 ; . Additionally , hierarchical structures also have been introduced because the context sentences do not have the same significance as the input sentences ( Miculicich et al. , 2018 ; Yun et al. , 2020 ) .
Coreference and NMT
The difference in coreference expressions among languages ( Zinsmeister et al. , 2017 ; Lapshinova-Koltunski et al. , 2020 ) gives MT systems a challenge on pronoun translation ( Bawden et al. , 2018 ) .
Several recent works have attempted to incorporate coreference information ( Ohtani et al. , 2019 ) .
The closest work to ours is ( Stojanovski and Fraser , 2018 ) which also adds noise on creating a coreference - augmented dataset , while we do not add oracle coreference information directly to the training data .
Data augmentation for NMT
One of the most common methods for data augmentation in NMT is back - translation that generates pseudo-parallel data from monolingual corpora using intermediate NMT models ( Sennrich et al. , 2016 a ) .
Generally , back - translation is conducted at sentence - level , however , several works have proposed document- level back - translation ( Sugiyama and Yoshinaga , 2019 ; Huo et al. , 2020 ) .
On the other hand , sentence corruption by removing or replacing word ( s ) has also been widely used for improving model performance and robustness ( Lample et al. , 2018 ; Voita et al. , 2019 ) .
Inspired by these works , we choose sentence corruption for contrastive learning .
Contrastive Learning Contrastive learning is to learn a representation by contrasting positive and negative ( contrastive ) examples .
It has succeed in various machine learning fields including computer vision ( Chen et al. , 2020 ) and natural language processing ( Mikolov et al. , 2013 ; Wu et al. , 2020 ; Lee et al. , 2021 ) .
Recently , several approaches to contrastive learning for NMT have also been studied .
Yang et al. ( 2019 ) proposed strategies for generating wordomitted contrastive examples and leveraging contrastive learning for reducing word omission errors in NMT .
Pan et al. ( 2021 ) applied contrastive learning for multilingual MT and employed data augmentation for obtaining both the positive and negative training examples .
While these works have been conducted in sentence - level NMT settings , we focus on extending contrastive learning in context - aware NMT .
Context - aware NMT models
In this section , we briefly overview context - aware NMT methods and describe our baseline models which are also commonly adopted in recent works .
Generally , a sentence- level ( context-agnostic ) NMT model takes an input sentence in a source language and returns an output sentence in a target language .
On the other hand , a context - aware NMT model is designed to handle surrounding contextual sentences of source and / or target sentences .
We focus on leveraging the contextual sentences of the source language .
Throughout this work , we consider the Transformer ( Vaswani et al. , 2017 ) as a base model architecture by following the majority of the recent works on context - aware NMT .
Transformer consists of a stack of self-attentional layers in which a self-attention module is followed by a feedforward module for each layer .
Here we list four Transformer - based configurations that we used in the experiments : ? sent-level :
As a baseline , we have experimented with the basic Transformer model which does not use any contextual sentences .
? concat :
This is a straightforward approach to incorporate contextual sentences without modifying the Transformer model ( Tiedemann and Scherrer , 2017 ) .
This concatenates all contextual sentences and an input sentence with special tokens between sentences .
? multi-enc :
This has an extra encoder for encoding contextual sentences separately .
We follow the model introduced in which obtain a hidden representation of contextual sentences by weight - shared Transformer encoder .
The model combines the encoded source and context representations using a source-to-context attention mechanism and a gated summation .
? multi-enc-hier :
To represent multiple contextual sentences effectively , hierarchical encoders for contextual sentences have been proposed ( Miculicich et al. , 2018 ; Yun et al. , 2020 ) .
In this configuration , the context representation is calculated in token - level first , then finally processed in sentence - level .
We experimented with the model of ( Yun et al. , 2020 ) in this paper .
All the model structures are described in Figure 1 .
Our Method : CorefCL
In this section , we explain the main idea of Core-fCL , a data augmentation and contrastive learning scheme leveraging coreference between the source and contextual sentences .
Data Augmentation Using Coreference Generally , constrastive learning encourages a model to discriminate ground - truth and contrastive ( negative ) examples .
In existing works , a number of approaches have been studied for obtaining contrastive examples : ?
Corrupting the sentence by randomly removing or replacing one or more tokens in the sentence .
( Yang et al. , 2019 ) ?
Choosing an irrelevant example in the batch or dataset .
( Pan et al. , 2021 ) ?
Perturbations on representation space .
Usually output vector of encoder or decoder is used .
( Lee et al. , 2021 )
CorefCL basically takes a similar approach to the first one , by the sentence corruption .
However , unlike previous works that modify the source sentence , CorefCL modifies the contextual sentences to form contrastive examples .
Specifically , we corrupt cross-sentential coreference mentions which occur between the source and its contextual sentences .
This is based on the intuition that coreference is one of the core components of coherent translation .
More formally , steps to forming contrastive examples in CorefCL are as follows ( see also Figure
In the experiments , we take both of the corruption strategies .
Precisely , the masked words are removed with a probability of 0.5 , or randomly replaced otherwise .
We found that this method is more effective compared to the methods using only one of the two corruption strategies .
Please refer to the ablation study in Section 5.5 for more details .
L M T = ( x , y , C ) ? D ?logP ( y|x , C ) .
Once the model is trained with MT loss , we fine - tune the model with a contrastive loss .
With a contrastive version of context C , our contrastive learning objective is minimizing a max-margin loss ( Huang et al. , 2018 ; Yang et al. , 2019 ) : L CL = ( x,y , C , C ) ?
D max {? + logP ( y|x , C ) ? logP ( y|x , C ) , 0 } .
Minimizing L CL encourages the log-likelihood of the ground - truth to be at least ? larger than that of the contrastive examples .
In our formulation , we want the model to be more sensitive to the subtle changes in the contextual sentences .
The contrastive loss is jointly optimized with MT loss since we empirically found that the joint optimization has yielded better performance than minimizing CL loss only as similar to ( Yu et al. , 2020 ) : L = ( 1 ? ?) L M T + ?L CL , where ? ? [ 0 , 1 ] is a weight for balancing between contrastive learning and MT loss .
For simplicity , we fixed ? during fine-tuning .
Experiments
Datasets
We experimented with CorefCL on various document- level parallel datasets : i ) 3 English - German datasets including WMT document- level news translation 2 ( Barrault et al. , 2019 ) , IWSLT TED talk 3 ( Cettolo et al. , 2017 ) , OpenSubtitles '18 4 ( Lison et al. , 2018 ) , and ii ) our webcrawled English - Korean subtitles corpus .
For all tasks , we take every 2 preceding sentences as contextual sentences and we only consider sentences within the same document ( article , talk , movie , one episode of TV programs , etc. ) of the source sentence .
If split of the validation and the test set is not presented in the data , we apply document - based split to ensure that training and validation / test data is well -separated .
Details of datasets are listed as follows : WMT
We use a set of parallel corpora annotated with document boundaries which is released in WMT '19 news translation task .
Specifically , we combine Europarl v9 , News Commentary v14 , and MODEL - RAPID to form a training set containing 3.7 M examples and 0.85 M with crosssentential coreferences .
For validation and test sets , we used newstest2013 and newstest2019 which contain 3.05 k and 2.14 k examples respectively .
IWSLT
The IWSLT dataset consists of transcriptions of TED talks in a variety of languages .
We used the 2017 version of the training set , a combination of dev2010 , tst2010 , tst2015 as a validation set , and tst2017 as a test set .
The resulting dataset consists of 232 k ( 50.3 k with cross-sentential coreferences ) , 3.5 k , 1.2 k examples of train , dev , test sets respectively .
OpenSubtitles
We also choose the English - German pair of OpenSubtitles 2018 corpora .
The raw corpus contains 24.4 M parallel sentences .
We follow the filtering methods in ( Voita et al. , 2019 ) by removing pairs that have a time overlap of subtitle frames less than 0.9 .
We also use separate documents for validation / test sets , resulting in 3.9M ( 1.01 M with cross-sentential coreferences ) , 40.7 k , 40.5 k examples for train / validation / test sets respectively .
En-Ko Subtitles For English -Korean experiments , we first crawled approximately 6.1 k bilingual subtitle files from websites such as Gom-Lab.com .
Since sentence pairs of these subtitles are already soft-aligned by the creators so we applied a simple time -code based heuristics to filter examples .
The final data contains 1.6 M ( 0.24 M with cross-sentential coreferences ) , 155.6 k , and 18.1 k examples of consecutive sentences in the training , validation , and test sets respectively .
For preprocessing , all English and German corpus is tokenized first with Moses ( Koehn et al. , 2007 ) tokenizer 5 . We then apply the BPE ( Sennrich et al. , 2016 b ) using SentencePiece 6 , and the size of the merge operation is approximately 16.5 k .
We also put a special token [ BOC ] at the beginning of contextual sentences to differentiate them from the source sentences .
Settings
We use model hyperparameters , such as the size of hidden dimensions and the number of hidden layers as same the transformer - base ( Vaswani et al. , 2017 ) , since all of the compared models are based on Transformer .
Specifically , we set 512 as the hidden dimension , the number of layers is 6 , the number of attention heads is 8 , and the dropout rate is set to 0.1 .
All models are trained with ADAM ( Kingma and Ba , 2014 ) with different learning rates for each dataset .
We employ early stopping of the training when the MT loss on the validation set does not improve .
We start training each baseline model from scratch with random initialization and documentlevel dataset .
Note that all the baseline models are not trained using iterative training as ( Zhang et al. , 2018 ; Huo et al. , 2020 ) which first trains the model from sentence - level task first , then document - level task .
All the evaluated models are implemented on top of the transformers 7 framework .
We measure the translation quality by the BLEU score ( Papineni et al. , 2002 ) we use the sacreBLEU ( Post , 2018 ) case-sensitive , detokenized scores for En- De , and case - insensitive scores with intl tokenizer for En- Ko task .
We also report case - insensitive char-level scores on En- Ko for comparison .
Overall BLEU Evaluation
We display the corpus-level test BLEU scores of all compared models on different tasks in Table 1 .
Among the baseline systems , all context - aware models show moderate improvements over the sentence - level ( sent-level ) baseline .
These results are comparable to that of Huo et al . ( 2020 ) on the IWSLT task except for multi-enc-hier , and Yun et al . ( 2020 ) on OpenSubtitles task .
One exception is a single-encoder model ( concat ) on WMT task , which seems due to the longer average sentence length .
We evaluated CorefCL by fine-tuning the context- aware models .
Results show that models with CorefCL outperformed their vanilla counterparts , with the BLEU gain of up to 1.4 in En- De tasks , and 1.6/2.8 ( detokenized / char-level BLEU ) in the En-Ko subtitles task .
We observed that while CorefCL consistently improves BLEU on all tasks , it achieves better results on IWSLT and En- Ko subtitles tasks .
Since improvements on much larger datasets like WMT and OpenSubtitles are smaller , we suggest that Core-fCL also works as a regularization .
Results on English -German Contrastive Evaluation Set
To assess how CorefCL improves the ability to deal with pronoun-related translations more in detail , we experiment our method with ContraPro .
We evaluate the models trained with WMT and OpenSubtitles tasks .
We also list BLEU scores of En- De translation using the English source text in ContraPro .
As shown in Table 2 , CorefCL significantly improves the baselines in scoring accuracy for all models by up to 5.5 % , as well as slight improvements in BLEU scores .
One interesting finding is that CorefCL also achieved substantial accuracy gain on the models trained on WMT .
Since the ContraPro is created from OpenSubtitles , WMT - trained models would yield lower performance because of domain shift between training and testing .
Table2 clearly shows the performance drop in BLEU , nevertheless , moderate improvements in accuracy can also be ob-served on WMT - trained models .
Ablation Study CorefCL uses the two corruption strategies for generating contrastive coreference mentions ; word omission and word replacement .
To make a better understanding of influence of these strategies , we evaluate CorefCL of different settings of these strategies .
Analysis
System
As shown in Table . 3 , using both types of corruptions results in better performance .
Removing one of the two strategies slightly degrades both the pronoun resolution accuracy and BLEU .
Although not being significant , removing the word replacement has more impact on accuracy .
This suggests that a standard context - aware model , at least for multi-enc-hier is less sensitive to word substitution .
The word replacement strategy can complement this behavior as resulted in better performance .
Qualitative Example
We display a sample from ContraPro corpus and its translations made by multi-enc-hier model trained with OpenSubtitle task .
In this example , since " coat " is translated as Mantel which is a masculine noun thus Er would be adequate translation of " It " instead of Sie which is feminine .
While multi-enc-hier incorrectly translated " It " as Sie , the model fine-tuned with CorefCL correctly resolved it as Er.
In practice , context - aware models that do not leverage target - side contexts struggle to maintain these kinds of coreference consistency Lapshinova - Koltunski et al. , 2019 ) because of the asymmetric nature of grammatical components and data distributions .
Results show that CorefCL can complement the limitation of source-only context - aware models .
Conclusions and Future Work
We have presented a data augmentation and contrastive learning scheme based on coreference for context - aware NMT .
By leveraging coreference mentions between the source and target sentence , CorefCL effectively generates contrastive examples for applying contrastive learning on contextaware NMT models .
In the experiments , CorefCL consistently improves the translation quality and pronoun resolution accuracy .
As future work , we plan to extend CorefCL to target contexts since maintaining coreference consistency needs both the source and the target contexts .
It would be also interesting that applying CorefCL for fine-tuning pre-trained big language models like BART ( Lewis et al. , 2020 ) or T5 ( Raffel et al. , 2020 ) for downstream document- level MT tasks .
Figure 1 : 1 Figure 1 : The structure of compared context- aware NMT models .
( a ) Figure 2 : a2 Figure 2 : Data augmentation process of CorefCL .
the source documents automatically .
We use NeuralCoref 1 to identify the coreference mentions between the source and its previous sentences as contextual sentences 2 . Filter the examples with cross-sentential coreference chain ( s ) between the source and contextual sentences .
Around 20 to 30 % of the training corpus is annotated in this way .
See Section 5.1 for details 3 . For each coreference chain , mask every word in the antecedents with a special token .
We also keep the original examples for training 4 .
Masked words are replaced randomly with other words in vocabulary ( word replacement ) , or omitted ( word omission )
1 https://github.com/huggingface/neuralcoref4.2 Contrastive Learning for Context- aware NMTContext - aware NMT models can implicitly capture dependencies between the source and contextual sentences .
CorefCL introduces a max-margin contrastive learning loss to train the model to explicitly discriminate inconsistent contexts .
This contrastive loss also encourages a model to be more sensitive to the contents of contextual sentences .
Formally , given the source x , target y , n contextual sentences C = [ c 1 , ? ? ? , c n ] in the data D , we first train the model by minimizing a negative log-likelihood loss , which is a common MT loss :
Figure 3 : 3 Figure 3 : Example translation with and without Core-fCL .
ContextShe opened her cupboard and gave me a petticoat .
She opened her cupboard and gave me a petticoat .
I should wear it .
She opened her cupboard and gave me a [ MASK ] .
I should wear it .
Source I should wear it .
Context '
She opened her cupboard and gave me a . ( omission )
She opened her cupboard and gave me a glass .
( replacement ) Source I should wear it .
Table 1 : 1 . For scoring BLEU , Corpus-level BLEU scores of compared models on different tasks .
For the En- Ko subtitles task , we list both detokenized ( detok . ) and character - level ( char. ) scores .
Improvements by CorefCL are denoted in ( ) .
Underlined score means that the model has the largest BLEU improvements among models in the same task .
System WMT OpenSubtitles IWSLT En-Ko Subtitles detok .
char. sent-level 22.7 27.6 29.3 8.6 19.2 concat 22.4 28.3 29.7 9.3 22.1 + CorefCL 23.5 ( + 1.1 ) 29.1 ( + 0.8 ) 30.9 ( + 1.3 ) 10.9 ( + 1.6 ) 24.9 ( + 2.8 ) multi-enc 23.1 28.6 29.8 9.2 21.7 + CorefCL 24.3 ( + 1.2 ) 29.8 ( + 1.4 ) 31.1 ( + 1.3 ) 10.8 ( + 1.6 ) 24.4 ( + 2.7 ) multi-enc-hier 24.4 29.1 30.0 10.3 23.1 + CorefCL 25.4 ( + 1.0 ) 30.2 ( + 1.1 ) 31.1 ( + 1.2 ) 11.7 ( + 1.4 ) 25.7 ( + 2.6 )
Table 3 : 3 Ablation study on coreference corruption strategy .
All systems are trained on OpenSubtitles English -German dataset and evaluated on ContraPro .
BLEU Accuracy multi-enc-hier 31.7 57.3 + CorefCL 33.6 60.5 - Word omission 32.4 59.4 - Word replacement 32.3 58.6
Context
What 'll I do with the coat ?
When you 're through with it , send it to the police .
Source It ...
It did n't belong to her .
multi-enc-hier Sie ... sie geh ?rte nicht zu ihr .
+ CorefCL Er? er ist nicht ihr geh?rte.
Reference Er ... er geh ?rte ihr nicht .
http://www.statmt.org/wmt19/translation-task.html
3 https://wit3.fbk.eu/home 4 https://opus.nlpl.eu/OpenSubtitles-v2018.php
https://github.com/moses-smt/mosesdecoder 6 https://github.com/google/sentencepiece 7 https://github.com/huggingface/transformers
