title
Neural Machine Translation with Synchronous Latent Phrase Structure
abstract
It is reported that grammatical information is useful for machine translation ( MT ) task .
However , the annotation of grammatical information requires the highly human resources .
Furthermore , it is not trivial to adapt grammatical information to MT since grammatical annotation usually adapts tokenization standards which might not be suitable to capture the relation of two languages , and the use of subword tokenization , e.g. , Byte - Pair- Encoding , to alleviate out - of- vocabulary problem might not be compatible with those annotations .
In this work , we propose two methods to explicitly incorporate grammatical information without supervising annotation ; first , latent phrase structure is induced in an unsupervised fashion from a multi-head attention mechanism ; second , the induced phrase structures in encoder and decoder are synchronized so that they are compatible with each other using constraints during training .
We demonstrate that our approach produces better performance and explainability in two tasks , translation and alignment tasks without extra resources .
Although we could not obtain the high quality phrase structure in constituency parsing when evaluated monolingually , we find that the induced phrase structures enhance the explainability of translation through the synchronization constraint .
Introduction
Although machine translation ( MT ) has achieved improved performance using neural machine translation ( NMT ) , the translation qualities for distant languages are still poor ( Johnson et al. , 2017 ) .
As a way to tackle the problem , statistical MT ( SMT ) incorporates synchronous grammar to achieve more linguistically accurate translations , in which complex structural relations between source and target languages are expressed using phrase structure ( Wong et al. , 2005 ) .
The synchronous grammar expresses the complex relationships between source and target languages and incorporates phrase structure to enable more linguistically accurate translation .
A similar idea could be employed for NMT to achieve improved performance on those distant language pairs .
However , grammatical information annotation demands high human resources .
In addition , such grammatical annotation is done on word- level granularities , which might not be the best tokenization for MT tasks due by language mismatch or out - of- vocabulary problem , and often sub-word tokenization , e.g. , Byte-Pair-Encoding ( BPE ) ( Sennrich et al. , 2016 ) , is employed to alleviate the problem .
As a result , it is difficult to incorporate grammatical information into NMT that handle multiple languages simultaneously .
Recently , there have been researches on unsupervised learning of phrase structure without relying on human annotations .
Although these phrase structures learned in an unsupervised fashion are very close to the human annotation ( Shen et al. , 2018 a , c ) , there exists no model which incorporates phrase structures as latent information to improve the performance and explainability of translation .
In this work , we introduce an approach to incorporate the phrase structure explicitly into Transformer ( Vaswani et al. , 2017 ) .
The approach can split into two steps ; first , latent phrase structures are induced in an unsupervised fashion for the source and target sides ( Shen et al. , 2018a ) ; second , the two induced latent phrase structures are synchronously agreed with each other through an attention mechanism ( Deguchi et al. , 2021 ) . Experiments on German-English and Japanese - English show that our synchronous latent structures have achieved better performance on translation and alignment tasks .
We also show that the induced phrase structures and synchronous structures can enhance the explainability of translation through our detailed analysis in word alignment task .
Related Work
NMT with Supervised Tree Structure
In the previous work , it is reported that supervised phrase structures ( Eriguchi et al. , 2017 ; Nguyen et al. , 2020 ) and dependency structures ( Ma et al. , 2019 ; Deguchi et al. , 2019 ) can help the performance of MT .
However , these approaches require an annotated corpus of syntactic structures .
In addition , such syntactic annotation is done on wordlevel granularities , which might not be the best tokenization for MT tasks due by language mismatch or out - of- vocabulary problem , and often BPE ( Sennrich et al. , 2016 ) , is employed to alleviate the problem .
However , the application of BPE to grammatical information might require a different approach for each language .
Latent Grammar Induction with Neural Machine Translation
Shen et al. ( 2018a ) introduce the concept called " syntactic distance " which represents the syntactic relation of word pairs .
Similarly ,
Shen et al. ( 2018 c ) introduce ordered neurons which allows to learn long-term or short-term information by a novel gating mechanism and activation function .
Kim et al. ( 2019 ) apply amortized variational inference for recurrent neural network grammar to learn the phrase structures in an unsupervised fashion .
Wang et al. ( 2019 ) add an extra constraint to the multi-head self-attention mechanism in order to encourage the attention heads to follow phrase structures .
Shen et al . ( 2020 ) introduce the constrained multi-head self-attention mechanism that allows to induct phrase and dependency structure at the same time .
These works successfully learn to induce phrase structure from language modeling task without extra linguistic resources .
It is described in ( Htut et al. , 2019 ) that translation task is a conditional language modeling task with many supervisory signals and is suitable for deriving phrase structure .
Unfortunately , despite grammatical information helps the understanding model work , previous work has not explicitly used induced phrase structures .
Transformer NMT
We employ the Transformer ( Vaswani et al. , 2017 ) as our base model , which is an encoder-decoder model that relies on an attention mechanism for computing the contextual representations of source and target text .
Both the encoder and decoder are composed of multiple layers , each of which includes a multi-head attention ( MHA ) and a feedforward sub-layer .
To compute the MHA output , three inputs , query Q , key K , and value V are projected into N different sub-spaces , namely heads , with each output computed in each subspace , then , projected back to the original space after aggregation : Q1 :N = QW Q , K1:N = KW K , V1:N = VW V ( 1 ) H n = A Vn = softmax Qn Kn ? d h Vn ( 2 ) MHA ( Q , K , V ) = concat H 1 , ... , H N W O ( 3 ) where W Q ?
R do ?d h , W K ? R do ?d h , W V ? R do ?d h , W O ?
R do ?d h are projection parameters .
d o is dimension of original space .
d h = d o / N is the dimension of subspace .
The value A denotes the attention probability for the jth target token overall the ith source token , computed by nth head .
In the translation task , Transformer is frequently used for its translation accuracy and efficiency .
Transformer decoder employs the autoregressive model which guesses the next token having read all the previous ones .
Also , since attention represents relationship the between source and target tokens , it is used in the alignment task ( Garg et al. , 2019 ) . et al. ( 2021 ) find that NMT performance can be improved by synchronizing the encoder attention to decoder attention , which is called " synchronous syntactic attention " .
The dependency information is embedded in these attention by supervised learning task .
The encoder- decoder attention can be viewed as a soft word alignment , which is a weight that can project the source vector into the target vector space without additional model parameters .
This work synchronize the source and target attentions that be embedded dependency information by supervision task .
To match the attention of encoder and decoder , they project the encoder attention to the target one , and incorporate constraints such that the source and target attention agree with each other .
Synchronous syntactic attention
Deguchi ? ? ? ? ? ? ? ? ? ? ? ?
Source distance : Target distance : Projected target distance :
Synchronous constraint Encoder-Decoder attention
Synchronous Latent Phrase Structure
In this section , we present the Synchronous Latent Phrase Structure .
This proposed method is split into two steps .
One is Latent Phrase Structure Induction ( LPSI ) and the other is Synchronous Constraint .
Figure 1 shows the flow of synchronizing Japanese source and English target syntactic distances .
Latent Phrase Structure Induction
We employ syntactic distance ( Shen et al. , 2018a ) as a way to induce phrase structure .
Each syntactic distance d i is associated with each span ( i , i + 1 ) which indicates the relative order of hierarchically splitting a sentence into smaller components .
For example , Figure 1 shows that the target syntactic distance between ' woman ' and ' with ' covers the phrase ' the woman with the telescope ' .
Mathematically , syntactic distance d i is computed through the convolution - based network : d i = tanh ( W D ? ? ? ? k i?M k i?M +1 ? ? ? k i ? ? ? ? + b D ) ( 4 ) where W D and b D are convolution kernel parameter , kernel size M represents a look - back range to calculate syntactic distance d. k i ?
Kn is same as key used in MHA .
The attention gate values are computed as follows : g i , t = P ( b t ? i ) = t?1 j=i+1 ? j, t ( 5 ) ? j, t = hardtanh ( ( d t ? d j ) ? ? ) + 1 2 where t is the current time step .
? j, t is a probability value that represents the syntactic relationship of distance d j and d t , and hardtanh ( x ) = max ( ? 1 , max ( 1 , x ) ) .
? is the temperature hyper parameter that controls the sensitivity of ?
j ,t to the differences between syntactic distances .
b t is a variable that indicates the position of break in the phrase structure .
This ? is sharper than softmax function , which allows to separate the constituents more easily .
The phrase structured MHA is defined based on the gates : ? i , t = g i , t ?
a i , t i g i , t ? a i , t ( 6 ) where a is an element of attention A .
The gate g i , t is a weight that constrains attention to only the same hierarchy in the phrase structure .
Here , ? is used in place of the elements of A in Equation 2 .
Synchronous Constraint
In the MT model , encoder and decoder learn separate phrase structures , which are not necessarily synchronized in that two structures may not be compatible with each other in terms of vector representations .
Therefore , synchronizing each phrase structure learned in encoder and decoder , inspired by synchronous grammar in SMT , may improve the performance of translation by the synchronous structure .
Inspired by synchronous syntactic attention ( Deguchi et al. , 2021 ) , we project the structure expressed by the encoder syntactic distance to the target one , and incorporate constraints such that the source and target syntactic distances agree with each other .
In Figure 1 , the source syntactic distance is projected to the target syntactic distance through the attention weight , and the syntactic correspondence between Japanese and English is learned from the target and projected syntactic distances of the phrase ' saw the woman with the telescope ' .
The synchronous constraint can be represented by using the Mean Squared Error ( MSE ) of the syntactic distance between the source and target languages : l ) is projected syntactic distance in lth decoder layer and computed as : L sync = L l i d ( l ) i ? d ( l ) i 2 ( 7 ) d ( d ( l ) = C ( l ) e ( l ) ( 8 ) where e ( l ) is syntactic distance in lth encoder layer .
C ( l ) ?
R J?I is the lth encoder-decoder attention weight , which represents the relationships of encoder and decoder representations , works just like MHA .
Here , I and J are length of source and target sentence .
The lth encoder-decoder attention weight is computed as : C ( l ) = softmax ( Q ( l ) dec K ( l ) enc ? ? h ) ( 9 ) where Q ( l ) dec and K ( l ) enc are lth decoder and encoder hidden weights .
The important element in phrase structure is the hierarchical positional relationship derived from syntactic distance .
However , MSE over- penalizes the models , because it results in the exact distance prediction task .
Therefore , we use the rank loss ( Burges et al. , 2005 ) as proposed by Shen et al . ( 2018 b ) , which takes hierarchical positioning into account .
Applying the rank loss to the synchronous constrict , we obtain the following : L sync = L l i , j> i hinge d ( l ) i ? d ( l ) j , d ( l ) i ? d ( l ) j ( 10 ) where hinge( x
1 , x 2 ) = max ( 0 , 1 ? sign( x 1 ) ? x 2 ) and sign ( x ) is sign function .
Therefore , the overall objective L is represented by : L = L trans + ?L sync ( 11 ) where L trans = ?
J i log p(y i |x , y < i ) where L trans is the objective of machine translation task and ? ?
0 is hyper parameter to control the degree of the synchronous constraint L sync .
x and y are source and target sentences , respectively .
Experiments
We train our proposed models using the training objective in Equation 11 and evaluate them on three tasks : translation , constituency parsing , and word alignment .
We implement models within the Fairseq sequence modeling toolkit ( Ott et al. , 2019 ) .
Training Details
We employ the transformer iwslt de en align fairseq configuration for German- English dataset and the transformer align fairseq configuration for Japanese - English dataset .
We use two MHA layers from the bottom to induct the phrase structures , and two encoder-decoder MHA layers from the top to synchronize the encoder and decoder syntactic distances 1 .
The hyper parameters are set as look back range M = 5 and temperature ? = 1.0 1 . The synchronous constrain hyper parameter is set by ? = 0.01 for MSE and Rank loss .
Tasks
Translation Task
We evaluated the effectiveness of the synchronous latent phrase structures for MT tasks on IWSLT '14 German-English and ASPEC Japanese - English datasets .
We train the translation models on the IWSLT '14 German-English and ASPEC Japanese -English ( Nakazawa et al. , 2016 ) datasets .
We use the prepare iwslt14.sh for IWSLT '14 German -English and follow the instruction of constructing the baseline system of WAT 2 , but KyTea ( Neubig et al. , 2011 ) is used as the tokenizer for Japanese sentences .
These datasets are applied BPE .
Table 1 shows the detailed data statistics .
To compare the effectiveness of synchronous latent phrase structure , we run additional baselines without latent phrase induction but with synchronous constraints applied to the attention weights .
We run inference with a beam size of 5 and report the quality of translation of our models with BLEU ( Papineni et al. , 2002 ) .
Constituency Parsing Task
In this experiment , we did not apply BPE and English data was parsed using Stanford CoreNLP version 4.1.0 3 , and thus the number of tokens in each sentence is preserved .
The latent phrase structure is obtained by force decoding ; we feed the gold target sentences from the test set into the word -wise trained MT models .
We report unlabeled F-measure ( UF ) as the quality of English latent phrase structures , inducted from the bottom syntactic distances , with scoring script Evalb 4 .
Here , UF is an F-measure that ignores constituency tags and evaluates only by bracketing .
Alignment Task
We also measure the impact of the alignment qualities represented by our synchronous grammar against other models including a statistical model FAST - ALIGN ( Dyer et al. , 2013 ) 5 .
We use the same experimental setup as described in ( Chen et al. , 2020 ) datasets , but we only use German-English Europarl v7 training data and the gold alignments 7 provided by ( Vilar et al. , 2006 ) .
Table 1 shows the detailed data statistics .
We report the alignment quality in the penultimate layer following ( Garg et al. , 2019 ) with Alignment Error Rate ( AER ) introduced in ( Vilar et al. , 2006 ) .
In this task , the trained model is BPE - wise , but the reported AER is word-wise .
Furthermore , we report the quality of symmetrized alignments that combined both unidirectional alignments .
The combination method is employed the grow-diagonal heuristic ( Koehn et al. , 2005 ) , in which alignments are greedily enlarged from the intersected alignments .
structure is not well inducted from the Japanese - English dataset and the advantage of Rank synchronous constraint is not utilized .
The difficulty of induction phrase structure in the Japanese - English dataset can also be read from the results of Transformer with LPSI .
Results
Translation Task
The synchronous syntactic attention model ( Deguchi et al. , 2021 ) also have good translation performance , but we can improve it further by incorporating the syntactic distance into the attention .
Although not shown in previous work ( Htut et al. , 2019 ) ,
Table 2 shows that the use of explicit latent phrase structure is useful for the MT task .
Interestingly , we found that the effective synchronous constrain differed between syntactically close , i.e. , German-English , and distant languages , i.e. , Japanese - English .
Constituency Parsing Task Table 3 compares the performance of our methods against baselines .
The results show that the synchronous constraint hurt the quality of latent phrase structures .
Especially , in MSE synchronous constraint , UF is drooped 17.01 points from the result of Transformer with latent phrase structure induction .
This is because the MSE synchronous constraints induct a synchronous grammar that is different from the phrase structure being evaluated .
In other words , synchronous constrain hinders the derivation of the latent phrase structures .
However , the decrease in UF by synchronous constrain by rank loss is small , whereas synchronous constrain by MSE greatly reduced UF .
It suggests that synchronous constrain by MSE derives an exact synchronization grammar and synchronous constrain by rank loss derives a minimal synchronization grammar .
As with prior study ( Htut et al. , 2019 ) , we did not find any correlation between the phrase structure qualities and translation qualities especially when two structures are synchronized in encoder and decoder .
This indicates that our induced grammatical structures using synchronous constraints might capture bilingual correspondence better than non-constrained models .
Figure 2 shows examples of parse tree from Stanford Parser and our Transformer with LSPI .
In the first example " a flash of the human spirit " , our model almost correctly inducts phrase structure in comparison with Stanford Parser .
The only mistake is grouping " the " and " human " first in the noun phrase " the human spirit " .
This mistake can be unique to concepts of syntactic distance , as it is the same as in the prior study ( Htut et al. , 2019 ) .
In the second example " have you ever seen a climate neutral tree ? " , our model correctly inducts the verb phrase " ever seen a climate neutral tree " , but fails to induct the phrase " have you ever " correctly .
30.8 ( 68.2 , 70.3 ) 32.4 ( 66.8 , 68.4 ) 27.7 ( 81.4 , 65 .0 ) -- Transformer 46.2 ( 51.0 , 57.1 ) 47.5 ( 49.5 , 56.1 ) 35.8 ( 84.9 , 51.3 ) 33.62 26.59 Transformer + LPSI 43.4 ( 53.5 , 60.1 ) 45.9 ( 51.1 , 57.6 ) 34.3 ( 84.6 , 53.4 ) 33 ( Cui et al. , 2019 ) .
Alignment Task Table 4 compares the performance of our methods against statistic and neural baseline approaches .
Compared with Transformer , the model with latent phrase structure show better translation performance and quality of alignments .
Furthermore , synchronizing source and target latent phrase structure decreases the AER , which indicates that synchronous constrain improves the interpretability of translation .
However , synchronous constrain by Rank loss resulted in a deterioration in AER , despite improving the translation performance BLEU .
Therefore , the relationship between BLEU and AER does not seem to be significantly correlated .
Table 5 shows that the effectiveness of synchronous latent phrase structure for two layers from the top in terms of AER .
In the penultimate layer , while synchronous constrain by MSE contributed to the improvement of AER , but synchronous constrain by rank loss conversely worsened AER .
However , rank loss resulted in a significant improvement AER in the third and fourth layers .
In the final layer , both synchronous constraints by MSE and rank loss result in the worse AER .
It suggests that the quality of the latent phrase structure derived from the second layer from the bottom is poor and this may have affected the results adversely .
Analysis
Effectiveness of Attention Gate
We realize that our gated multi-head attention ( GMHA ) , without synchronous constraint , is very similar to local attention within mixed multi-head attention ( MMHA ) ( Cui et al. , 2019 ) .
MMHA encourages each head to acquire different features by masking them differently and allows the model to be aware of the order of the sequence .
German-English and 14.08 BLEU point in ASPEC Japanese -English .
In the Transformer with latent phrase structure induction ( LPSI ) , the performance is only reduced by 0.89 BLEU point in IWSLT '14 German-English and 0.55 BLEU point in ASPEC Japanese - English without position embedding .
For a fair comparison , we employ local attention with 2 window in the two bottom layers of encoder .
Similarly , in the Transformer with local attention , the performance is only reduced by 0.93 BLEU point in IWSLT '14 German-English and 0.61 BLEU point in ASPEC Japanese - English without position embedding .
It indicates that local constraints on attention mechanisms help learning the order of the sequence rather than latent phrase structure induction .
Effectiveness of Synchronous Latent Phase Structure Figure 3 shows examples from the German-English alignment test set .
In the first example , we find that there are no false alignments in our models with synchronous constraints .
However , in rank loss , the alignment between ' Therefore ' and ' Daher ' , which was captured by MSE , is lost .
In the sec-ond example , duplicated our model correctly aligns them with ' um ' compared with FastAlign .
Therefore ,
The synchronous constraints by MSE and rank loss indicate that only alignments with high confidence are provided .
Furthermore , as can be seen from the precision values in this Table 4 , there are no false alignments in synchronous constrain by rank loss , and definite explainability of translation is achieved .
In other words , the synchronization constraint favors precision over recall , which may make the AER worse , but it can provide a reliable explanation for human .
The prior study ( Jain and Wallace , 2019 ; Serrano and Smith , 2019 ) conclude that the attentions have not explainability .
However , our attention is constrained by the syntactic distance , it can explain the relation between source and target sentence following the constituency tree .
We will work it as the future works .
Conclusion
This paper introduces the approach to improve the performance and explainability of MT .
In the MT task , our model improves the quality of translation even through distant language pairs .
In the alignment task , we demonstrate that synchronous constraint for syntactic distance can produce high precisional alignments to interpret MT hypothesis .
Currently , our approach induces the poor latent phrase structure constructed with the previous work .
To achieve the more high performance and explainability of MT , we would like to investigate other syntactic structures and a translation model which can induce better latent phrase structure .
Figure 1 : 1 Figure 1 : The example of relation between syntactic distances and synchronous constraint on Japanese to English translation task .
Starting from induction of source and target syntactic distances , we project the source distance to the target one through encoder-decoder attention weight .
By measuring the difference between the projected target syntactic distance and target one with the synchronous constraint .
It can embed the syntactic correspondences of source and target language into the encoder-decoder attention weight .
