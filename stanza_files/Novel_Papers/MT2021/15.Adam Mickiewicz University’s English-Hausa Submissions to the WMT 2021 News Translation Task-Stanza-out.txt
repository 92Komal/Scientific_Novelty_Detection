title
Adam Mickiewicz University 's English -Hausa Submissions to the WMT 2021 News Translation Task
abstract
This paper presents the Adam Mickiewicz University 's ( AMU ) submissions to the WMT 2021 News Translation Task .
The submissions focus on the English ?
Hausa translation directions , which is a low-resource translation scenario between distant languages .
Our approach involves thorough data cleaning , transfer learning using a high- resource language pair , iterative training , and utilization of monolingual data via back - translation .
We experiment with NMT and PB - SMT approaches alike , using the base Transformer architecture for all of the NMT models while utilizing PB - SMT systems as comparable baseline solutions .
Introduction
We describe the Adam Mickiewicz University 's submissions to the WMT 2021 News Translation Task .
We focused on translation between Hausa and English - a low-resource translation scenario between distant languages .
Our methods combine data cleaning with OpusFilter ( Aulamo et al. , 2020 ) and fastText ( Joulin et al. , 2016 ) , transfer learning ( Aji et al. , 2020 ; Zoph et al. , 2016 ) , iterative training , and back - translation ( Sennrich et al. , 2016 a ) .
All NMT models were trained with FAIRSEQ ( Ott et al. , 2019 ) , while the first iteration of the back -translation was generated with Moses ( Koehn et al. , 2007 ) .
The results presented in the paper are based on the first released development set ( " Dev - 1 " ) , which consists of 1000 sentences , the final development set ( " Dev-full " ) , which adds additional 1000 sentences to the first development set , and the released test set without additional test suites ( " Test " ) .
The test set consists of 1000 sentences in English ?
Hausa direction and 997 sentences in Hausa ?
English direction .
The final submissions significantly outperform the vanilla NMT baselines in terms of BLEU ( Pap-ineni et al. , 2002 ) metric results , as implemented in SACREBLEU ( Post , 2018 ) with default settings .
All systems were trained in a constrained scenario i.e. , using the data provided by the organizers of WMT 2021 only .
Data preparation
The quality of the training data has a great impact on the final performance of the NMT models ( Rikters , 2018 ) .
The data preparation consisted of data cleaning and filtering performed by using OpusFilter ( Aulamo et al. , 2020 ) pipelines .
We specified separate pipelines for monolingual and parallel data .
Data cleaning phase consisted of normalizing punctuation , removing non-printable characters , and decoding HTML entities by using Moses ( Koehn et al. , 2007 ) pre-processing scripts .
We applied subword segmentation on filtered data by using SentencePiece ( Kudo and Richardson , 2018 ) tool with byte-pair-encoding ( BPE ) ( Sennrich et al. , 2016 b ) algorithm .
The corpora we used for model training , along with the number of sentences before filtering , are specified in Table 1 . Number of sentences after filtering is presented in Table 2 . Monolingual data filtering For the monolingual data filtering , we defined an OpusFilter pipeline that consists of the following filters : ? deduplication filter , ? sentence length filter , ? word length filter , ?
Latin character score filter , ? language identification filter .
The sentence length filter requires that the sentence contain a minimum of 3 and a maximum Parallel data filtering
The filters used in the parallel data filtering pipeline are nearly identical to those used in the monolingual data filtering pipeline .
Filters are applied to both the source and target sentences in this scenario .
We also included a length ratio filter with a threshold of 2 , indicating that a sentence on the source side can be up to twice as long as a sentence on the target side and vice versa .
A similar pipeline was applied to the German- English data that was used for transfer learning .
We downsampled 3 M sentence pairs from ParaCrawl due to the imbalance in the German-English data .
Approach
Our models combine transfer learning from a high- resource language pair ( German- English ) , iterative training , and back - translation .
We used FAIRSEQ ( Ott et al. , 2019 ) toolkit in our experiments with NMT models , while we used Moses ( Koehn et al. , 2007 ) toolkit for our experiments with PB - SMT models .
All of our NMT models follow the base Transformer architecture ( Vaswani et al. , 2017 ) , using ReLU as the activation function and Adam ( Kingma and Ba , 2015 ) as the optimizer with the following parameters : ? 1 = 0.9 , ? 2 = 0.98 , = 1e?8 .
We set the inverse square root learning rate scheduling with a peak value of 1e ?3 .
We used learning rate warmup stage for 4000 updates with initial learning rate of 1e ?7 .
Dropout probability was set to 0.2 , while the attention dropout probability was set to 0.1 .
We also used label smoothing with a value of 0.1 .
In the case of baseline English -Hausa models , the joint vocabulary was based on both English and Hausa data .
In all cases , the vocabulary size was set to 32,000 .
The PB - SMT models were trained with default settings with Moses ( Koehn et al. , 2007 ) toolkit .
In addition , we trained a 5 - gram Operation Sequence Model ( Durrani et al. , 2013 ) .
All language models are 5 - gram models and were binarized with KenLM ( Heafield et al. , 2013 ) .
The models were trained on tokenized , word -level , lowercased sentences .
Re-casing was applied to the model outputs .
After training the base models , we also applied MERT ( Minimum Error Rate Training ) ( Och , 2003 ; tuning on the development set .
Baseline systems
We decided to train baseline models of two types : vanilla Transformer ( base ) and PB - SMT .
The experiments conducted on the first release of the development set showed that PB - SMT performs significantly better than NMT : we achieved + 1.8 BLEU score on Hausa ?
English and + 0.7 on English ?
Hausa .
Based on these results , we decided to use PB - SMT models to generate data for the first iteration of iterative training .
When the test set was published , we computed the scores for the baselines .
To our surprise , the scores obtained by NMT are much higher than PB - SMT , especially in the Hausa ?
English direction .
Transfer learning According to recent studies , transfer learning ( TL ) enhances translation quality in low-resource scenarios ( Zoph et al. , 2016 ; Aji et al. , 2020 ) .
We chose the German ?
English translation direction as a base .
In general , we followed ( Nguyen and Chiang , 2017 ) and trained a shared Hausa - German - English vocabulary ( BPE ) .
Then , we trained a German ?
English model using parallel data from the WMT 2021 Translation Task , which was filtered similarly to Hausa- English data .
Finally , we used the Hausa- English data to fine - tune the pretrained German ?
English model .
We obtained a BLEU score of 13.31 on the " Dev - 1 " development set ( + 1.1 BLEU compared to the NMT baseline ) , which was lower than the PB - SMT baseline .
Iterative back -translation Monolingual data has been widely employed in MT to enrich parallel corpora with synthetic data to improve the quality of MT systems , particularly in low-resource scenarios ( Bojar and Tamchyna , 2011 ; Bertoldi and Federico , 2009 ) .
We applied the back-translation technique ( Edunov et al. , 2018 ) iteratively ( Hoang et al. , 2018 ) to translate Hausa and English monolingual data into the other language , using intermediate models to generate incrementally better translations .
1 . First , we used the best baseline model ( PB - SMT based on Moses ) in English ?
Hausa direction to translate 5 M English sentences into Hausa .
2 . We used this additional data to train the Hausa ?
English model by applying transfer learning from the German ?
English model .
We upsampled the original parallel data 10 times to match the size of the back - translated data .
We used the resulting NMT model to translate all Hausa monolingual data into English via sampling .
3 . We combined the obtained back - translated data with the original parallel corpora to train the English ?
Hausa model in a manner similar to step 2 , with the exception that we did not upsample the parallel data in this scenario due to the fact that back - translated data was generated through sampling .
4 . This technique was applied iteratively , resulting in the systems shown in We notice a severe decrease in BLEU metric results on the test set as compared to the development set , particularly in the Hausa ?
English direction .
This could suggest a domain shift between the two sets .
Because our models are heavily based on the back - translated data , some vocabulary , especially proper names , may be missing from the training data .
Final results
Post-submission work
Due to a lack of computing power and time , our experiments and submissions were based on single model training .
After the submission deadline , we retrained the final models three times with different seeds .
Table 6 presents the results for the ensemble of four models in both directions .
We obtained slight improvements on both test sets , but the differences are insignificant .
On the other hand , the ensemble performed worse on the development set , especially on the first version .
Table 1 : 1 Corpora statistics before filtering .
of 100 words .
A maximum of 40 characters is required for the word length .
The required Latin character score for a sentence is set to 100 % .
Language identification filter is based on a fastText ( Joulin et al. , 2016 ) language identifier .
The open-source fastText language identification models do not identify Hausa , so we used the JW300 corpus from the English -Hausa Opus collection to train our custom language identifier .
A sentence must pass all filters to be included in the training data .
Data type Sentences Corpora Parallel en-ha 751,560 Khamenei , Opus , ParaCrawl Monolingual en 41,428,626
News crawl ( only 2020 ) Monolingual ha 2,311,959
News crawl , CommonCrawl Parallel de-en 8,600,361 Tilde Rapid , CommonCrawl , Europarl , News commentary , ParaCrawl Data type Sentences Monolingual en 39,812,834 Monolingual ha 1,227,921 Parallel ha-en 494,246
Table 2 : 2 Monolingual corpora statistics after filtering .
Table 3 : 3 Baseline results according to the automatic evaluation with BLEU metric .
System HA ? EN EN ? HA Dev-1 Test Dev-1 Test NMT baseline 12.21 11.44 10.28 11.05 PB -SMT baseline 14.00 6.59 11.02 9.36
Table 4 4 . In all
Table 4 : 4 Iterative back - translation results of the NMT systems on the " Dev - 1 " development set according to the automatic evaluation with BLEU metric .
Table 5 : 5 Final results according to the automatic evaluation with BLEU metric .
Table 5 presents the final results for both the English ?
Hausa and Hausa ?
English translation directions for both the development and test sets .
These results were produced by the final models from the iterative back -translation step described in section 3.3 .
Direction Dev-1 Dev-full Test EN ? HA 14.77 21.21 16.15 HA ? EN 22.85 25.23 14.13
Table 6 : 6 Post-submission models ensemble results according to the automatic evaluation with BLEU metric .
Direction Dev-1 Dev-full Test EN ? HA 14.68 21.00 16.34 HA ? EN 21.24 26.25 14.87
