title
Difficulty - Aware Machine Translation Evaluation
abstract
The high-quality translation results produced by machine translation ( MT ) systems still pose a huge challenge for automatic evaluation .
Current MT evaluation pays the same attention to each sentence component , while the questions of real-world examinations ( e.g. , university examinations ) have different difficulties and weightings .
In this paper , we propose a novel difficulty - aware MT evaluation metric , expanding the evaluation dimension by taking translation difficulty into consideration .
A translation that fails to be predicted by most MT systems will be treated as a difficult one and assigned a large weight in the final score function , and conversely .
Experimental results on the WMT19 English ?
German Metrics shared tasks show that our proposed method outperforms commonly - used MT metrics in terms of human correlation .
In particular , our proposed method performs well even when all the MT systems are very competitive , which is when most existing metrics fail to distinguish between them .
The source code is freely available at https://github.com/NLP2CT / Difficulty - Aware -MT-Evaluation .
Introduction
The human labor needed to evaluate machine translation ( MT ) evaluation is expensive .
To alleviate this , various automatic evaluation metrics are continuously being introduced to correlate with human judgements .
Unfortunately , cutting - edge MT systems are too close in performance and generation style for such metrics to rank systems .
Even for a metric whose correlation is reliable in most cases , empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems ( Ma et al. , 2019 ; Mathur et al. , 2020 ) , limiting the development of MT systems .
Current MT evaluation still faces the challenge of how to better evaluate the overlap between the reference and the model hypothesis taking into consideration adequacy and fluency , where all the evaluation units are treated the same , i.e. , all the matching scores have an equal weighting .
However , in real-world examinations , the questions vary in their difficulty .
Those questions which are easily answered by most subjects tend to have low weightings , while those which are hard to answer have high weightings .
A subject who is able to solve the more difficult questions can receive a high final score and gain a better ranking .
MT evaluation is also a kind of examination .
For bridging the gap between human examination and MT evaluation , it is advisable to incorporate a difficulty dimension into the MT evaluation metric .
In this paper , we take translation difficulty into account in MT evaluation and test the effectiveness on a representative MT metric BERTScore ( Zhang et al. , 2020 ) to verify the feasibility .
More specifically , the difficulty is first determined across the systems with the help of pairwise similarity , and then exploited as the weight in the final score function for distinguishing the contribution of different sub-units .
Experimental results on the WMT19 English ?
German evaluation task show that difficulty - aware BERTScore has a better correlation than do the existing metrics .
Moreover , it agrees very well with the human rankings when evaluating competitive systems .
Related Work
The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units : n-gram based ( Papineni et al. , 2002 ; Doddington , 2002 ; Lin and Och , 2004 ; Han et al. , 2012 ; Popovi ? , 2015 ) , edit-distance based ( Snover et al. , 2006 ; Leusch et al. , 2006 ) , alignment - based ( Banerjee and Lavie , 2005 ) , embedding - based ( Zhang et al. , 2020 ; Chow et al. , 2019 ; Lo , 2019 ) and end-to - end based ( Sellam et al. , 2020 ) . BLEU ( Papineni et al. , 2002 ) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age ( Shterionov et al. , 2018 ; Mathur et al. , 2020 ) .
Due to the fact that BLEU and its variants only assess surface linguistic features , some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation , which further improves the correlation with human judgement .
Among them , BERTScore ( Zhang et al. , 2020 ) has achieved a remarkable performance across MT evaluation benchmarks balancing speed and correlation .
In this paper , we choose BERTScore as our testbed .
3 Our Proposed Method
Motivation
In real-world examinations , the questions are empirically divided into various levels of difficulty .
Since the difficulty varies from question to question , the corresponding role a question plays in the evaluation does also .
Simple question , which can be answered by most of the subjects , usually receive of a low weighting .
But a difficult question , which has more discriminative power , can only be answered by a small number of good subjects , and thus receives a higher weighting .
Motivated by this evaluation mechanism , we measure difficulty of a translation by viewing the MT systems and sub-units of the sentence as the subjects and questions , respectively .
From this perspective , the impact of the sentence - level sub-units on the evaluation results supported a differentiation .
Those sub-units that may be incorrectly translated by most systems ( e.g. , polysemy ) should have a higher weight in the assessment , while easier - totranslate sub-units ( e.g. , the definite article ) should receive less weight .
Difficulty - Aware BERTScore
In this part , we aim to answer two questions : 1 ) how to automatically collect the translation difficulty from BERTScore ; and 2 ) how to integrate the difficulty into the score function .
Figure 1 presents an overall illustration .
Pairwise Similarity Traditional n-gram overlap cannot extract semantic similarity , word embedding provides a means of quantifying the degree of overlap , which allows obtaining more accurate difficulty information .
Since BERT is a strong language model , it can be utilized as a contextual embedding O BERT ( i.e. , the output of BERT ) for obtaining the representations of the reference t and the hypothesis h.
Given a specific hypothesis token h and reference token t , the similarity score sim(t , h ) is computed as follows : sim(t , h ) = O BERT ( t ) T O BERT ( h ) O BERT ( t ) ? O BERT ( h ) ( 1 ) Subsequently , a similarity matrix is constructed by pairwise calculating the token similarity .
Then the token - level matching score is obtained by greedily 1 : Absolute correlations with system-level human judgments on WMT19 metrics shared task .
For each metric , higher values are better .
Difficulty - aware BERTScore consistently outperforms vanilla BERTScore across different evaluation metrics and translation directions , especially when the evaluated systems are very competitive ( i.e. , evaluating on the top 30 % systems ) .
searching for the maximal similarity in the matrix , which will be further taken into account in sentencelevel score aggregation .
Difficulty Calculation
The calculation of difficulty can be tailored for different metrics based on the overlap matching score .
In this case , BERTScore evaluates the token - level overlap status by the pairwise semantic similarity , thus the token - level similarity is viewed as the bedrock of difficulty calculation .
For instance , if one token ( like " cat " ) in the reference may only find identical or synonymous substitutions in a few MT system outputs , then the corresponding translation difficulty weight ought to be larger than for other reference tokens , which further indicates that it is more valuable for evaluating the translation capability .
Combined with BERTScore mechanism , it is implemented by averaging the token similarities across systems .
Given K systems and their corresponding generated hypotheses h 1 , h 2 , ... , h K , the difficulty of a specific token t in the reference t is formulated as d( t ) = 1 ? K k=1 max h?h k sim(t , h ) K ( 2 ) An example is shown in Figure 1 : the entity " cat " is improperly translated to " monkey " and " puppy " , resulting in a lower pairwise similarity of the token " cat " , which indicates higher translation difficulty .
Therefore , by incorporating the translation difficulty into the evaluation process , the token " cat " is more contributive while the words like " cute " are less important in the overall score .
Score Function
Due to the fact that the translation generated by a current NMT model is fluent enough but not adequate yet , F - score which takes into account the Precision and Recall , is more appropriate to aggregate the matching scores , instead of only considering precision .
We thus follow vanilla BERTScore in using F-score as the final score .
The proposed method directly assigns difficulty weights to the counterpart of the similarity score without any hyperparameter : DA -R BERT = 1 | t| t?t d( t ) max h?h sim(t , h ) ( 3 ) DA -P BERT = 1 | h| h?h d( h ) max t?t sim(t , h ) ( 4 ) DA -F BERT = 2 ? DA-R BERT ? DA-P BERT DA-R BERT + DA-P BERT ( 5 ) For any h / ? t , we simply let d( h ) = 1 , i.e. , retaining the original calculation .
The motivation is that the human assessor keeps their initial matching judgement if the test taker produces a unique but reasonable alternative answer .
We regard DA -F BERT as the DA - BERTScore in the following part .
There are many variants of our proposed method : 1 ) designing more elaborate difficulty function ( Liu et al. , 2020 ; Zhan et al. , 2021 ) ; 2 ) applying a smoothing function to the difficulty distribution ; and 3 ) using other kinds of F - score , e.g. , F 0.5 - score .
The aim of this paper is not to explore this whole space but simply to show that a straightforward implementation works well for MT evaluation .
Experiments Data The WMT19 English ?
German ( En? De ) evaluation tasks are challenging due to the large discrepancy between human and automated assessments in terms of reporting the best system ( Bojar et al. , 2018 ; Barrault et al. , 2019 ; Freitag et al. , 2020 ) Metrics task . ?/?
denotes that the rank given by the evaluation metric is higher / lower than human judgement , and denotes that the given rank is equal to human ranking .
DA -BERTScore successfully ranks the best system that the other metrics failed .
Besides , it also shows the lowest rank difference .
our approach , we choose these tasks as our evaluation subjects .
There are 22 systems for En?De and 16 for De?En .
Each system has its corresponding human assessment results .
The experiments were centered on the correlation with system-level human ratings .
Comparing Metrics
In order to compare with the metrics that have different underlying evaluation mechanism , four representative metrics : BLEU ( Papineni et al. , 2002 ) , TER ( Snover et al. , 2006 ) , METEOR ( Banerjee and Lavie , 2005 ; Denkowski and Lavie , 2014 ) , BERTScore ( Zhang et al. , 2020 ) , which are correspondingly driven by n-gram , edit distance , word alignment and embedding similarity , are involved in the comparison experiments without losing popularity .
For ensuring reproducibility , the original 12 and widely used implementation 3 was used in the experiments .
Main Results
Following the correlation criterion adopted by the WMT official organization , Pearson 's correlation r is used for validating the system - 1 https://www.cs.cmu.edu/ alavie / METEOR/index.html 2 https://github.com/Tiiiger/bert score 3 https://github.com/mjpost/sacrebleu level correlation with human ratings .
In addition , two rank- correlations Spearman 's ? and original Kendall 's ?
are also used to examine the agreement with human ranking , as has been done in recent research ( Freitag et al. , 2020 ) .
Table 1 lists the results .
DA -BERTScore achieves competitive correlation results and further improves the correlation of BERTScore .
In addition to the results on all systems , we also present the results on the top 30 % systems where the calculated difficulty is more reliable and our approach should be more effective .
The result confirms our intuition that DA - BERTScore can significantly improve the correlations under the competitive scenario , e.g. , improving the | r| score from 0.204 to 0.974 on En?De and 0.271 to 0.693 on De?En.
Effect of Top-K Systems Figure 2 compares the Kendall 's correlation variation of the top -K systems .
Echoing previous research , the vast majority of metrics fail to correlate with human ranking and even perform negative correlation when K is lower than 6 , meaning that the current metrics are ineffective when facing competitive systems .
With the help of difficulty weights , the degradation in the correlation is alleviated , e.g. , improving ? score from 0.07 to 0.73 for BERTScore ( K = 6 ) .
These results indicate the effectiveness of our approach , establishing the necessity for adding difficulty .
Case Study of Ranking Table 2 presents a case study on the En?De task .
Existing metrics consistently select MSRA 's system as the best system , which shows a large divergence from human judgement .
DA -BERTScore ranks it the same as human ( 4th ) because most of its translations have low difficulty , thus lower weights are applied in the scores .
Encouragingly , DA-BERTScore ranks Facebook 's system as the best one , which implies that it overco - mes more challenging translation difficulties .
This testifies to the importance and effectiveness of considering translation difficulty in MT evaluation .
Case Study of Token -Level Difficulty
Table 3 presents two cases , illustrating that our proposed difficulty - aware method successfully identifies the omission errors ignored by BERTScore .
In the first case , the Facebook 's system correctly translates the token " right " , and in the second case , uses the substitute " Soldaten am Boden " which is lexically similar to the ground - truth token " Bodensoldaten " .
Although the MSRA 's system suffers word omissions in the two cases , its hypotheses receive the higher ranking given by BERTScore , which is inconsistent with human judgements .
The reason might be that the semantic of the hypothesis is highly close to the reference , thus the slight lexical difference is hard to be found when calculating the similarity score .
By distinguishing the difficulty of the reference tokens , DA - BERTScore successfully makes the evaluation focus on the difficult parts , and eventually correct the score of the Facebook 's system , thus giving the right rankings .
Distribution of Difficulty Weights
The difficulty weights can reflect the translation ability of a group of MT systems .
If the systems in a group are of higher translation ability , the calculated dif-ficulty weights will be smaller .
Starting from this intuition , we visualize the distribution of difficulty weights as shown in Figure 3 .
Clearly , we can see that the difficulty weights are centrally distributed at lower values , indicating that most of the tokens can be correctly translated by all the MT systems .
For the difficulty weights calculated on the top 30 % systems , the whole distribution skews to zero since these competitive systems have better translation ability and thus most of the translations are easy for them .
This confirms that the difficulty weight produced by our approach is reasonable .
Conclusion and Future Work
This paper introduces the conception of difficulty into machine translation evaluation , and verifies our assumption with a representative metric BERTScore .
Experimental results on the WMT19 English ?
German metric tasks show that our approach achieves a remarkable correlation with human assessment , especially for evaluating competitive systems , revealing the importance of incorporating difficulty into machine translation evaluation .
Further analyses show that our proposed difficultyaware BERTScore can strengthen the evaluation of word omission problems and generate reasonable distributions of difficulty weights .
Future works include : 1 ) optimizing the difficulty calculation ; 2 ) applying to other MT metrics ; and 3 ) testing on other generation tasks , e.g. , speech recognition and text summarization .
Figure 2 : 2 Figure2 : Effect of top -K systems in the En? De evaluation .
DA -BERTScore is highly correlated with human judgment for different values of K , especially when all the systems are competitive ( i.e. , K ?10 ) .
