title
TenTrans Large -Scale Multilingual Machine Translation System for WMT21
abstract
This paper describes TenTrans large-scale multilingual machine translation system for WMT 2021 .
We participate in the Small Track 2 in five South East Asian languages , thirty directions : Javanese , Indonesian , Malay , Tagalog , Tamil , English .
We mainly utilized forward / back- translation , in- domain data selection , knowledge distillation , and gradual fine-tuning from the pre-trained model FLORES - 101 .
We find that forward / backtranslation significantly improves the translation results , data selection and gradual fine-tuning are particularly effective during adapting domain , while knowledge distillation brings slight performance improvement .
Also , model averaging is used to further improve the translation performance based on these systems .
Our final system achieves an average BLEU score of 28.89 across thirty directions on the test set .
Introduction
We participate in the WMT 2021 large-scale multilingual machine translation task small track 2 in 6 languages : English , Indonesian , Javanese , Malay , Tamil , Tagalog ( briefly , En , Id , Jv , Ms , Ta , Tl ) .
Any two of these languages translated into each other produces a total of 30 directions , including English ?
Indonesian , English ?
Javanese , English ?
Malay , English ?
Tamil , English ?
Tagalog , Indonesian ?
Javanese , Indonesian ?
Malay , Indonesian ?
Tamil , Indonesian ?
Tagalog , Javanese ? Malay , Javanese ?
Tamil , Javanese ?
Tagalog , Malay ?
Tamil , Malay ?
Tagalog and Tamil ?
Tagalog .
To meet the requirements for data restrictions , our systems are all built with constrained data sets .
For all systems , we adopt a universal encoder-decoder architecture that shares parameters across all languages ( Johnson et al. , 2017 ) .
Our systems are based on several techniques and approaches .
We experiment with base and deeper Transformer ( Vaswani et al. , 2017 ) architectures to get reliable baselines , fine - tune the pre-training model FLORES - 101 ( Goyal et al. , 2021 ) to further improve the baseline system .
Moreover , we generate pseudo bilingual sentences from the large-scale monolingual data , apply sequence level knowledge distillation ( Kim and Rush , 2016 ) on partial language pairs , and try a more effectively fine-tuning strategy to domain adaptation .
Particularly in the language pairs with inferior translations , we specifically improve their performance .
All of these technologies have improved our systems , particularly data selection and gradual finetuning .
We carefully rethought this strategy and found the main gain may come from in -domain knowledge adaptation .
This paper was structured as follows : Section 2 describes the data set .
Then , we present a detailed overview of our systems in Section 3 .
The experiment settings and main results are shown in Section 4 .
Finally , we conclude our work in Section 5 .
Data Prepration
We use FLORES - 101 SentencePiece ( SPM ) 1 tokenizer model with 256 K tokens to tokenize bitext and monolingual sentences 2 . Since it is important to clean data strictly ( Wang et al. , 2018 ) , we follow m2m - 100 data preprocessing procedures 3 to filter bitext data .
The rules are as follows : ?
Remove sentences with more than 50 % punctuation .
?
Deduplicate training data . ?
Remove all instances of evaluation data from the training data .
?
Filter sentences that are longer than 250 tokens or length ratio upper than 3 .
For monolingual data , we still employ those rules except the length ratio filter .
See Table 1 for the statistics of bitext data sets and Table 2 for monolingual data sets .
System Overview
Base Systems
Our systems are based on the Transformer architecture ( Vaswani et al. , 2017 ) as implemented in TenTrans 4 , a unified end-to - end multilingual and multi-task training platform .
We first train a model following the Transformer base setup to jointly training all language pairs as our base system .
Then , inspired by , we experiment with raising network capacity by increasing encoder / decoder layers and feed-forward networks .
We found that using a deeper encode layer ( 24 ) and a larger feed - forward network size ( 4096 ) can provide reasonable performance improvements while maintaining manageable network size and not increasing inference time .
Because of the recent popularity of using largescale pre-training models to fine-tune specific languages and tasks ( Fan et al. , 2020 ; Liu et al. , 2020 ) , we use the pre-trained model FLORES - 101 released by the organizer to fine-tune on the bitext data .
This system has further improved our translation performance in all thirty translation directions .
Note that to fine- tune FLORES - 101 we train our models using FAIRSEQ ( Ott et al. , 2019 ) .
Forward -Translation and Back-Translation Back- translation is an effective and common way to boost translation quality by using monolingual data to produce pseudo training parallel data .
As opposed to back - translation , forward translation use source-side monolingual data to translate into the target language , and can be quite effective in some cases ( Bogoychev and Sennrich , 2019 ) . has shown that when monolingual data from source and target languages are used together to produce pseudo data , the translation quality is best , and the experimental performance will be improved with the increase of data .
In this work , considering the excellent performance of forward -translation and back - translation , we use both methods together .
For translation directions with more than 5 million bitext data , such as En?Id , En?Ms , En?Tl , we separately train an individual model for each direction and use it for the pseudo-corpus generation .
For other translation directions with less than 5 million bitext data , we use the baseline system of all language pairs jointly training for translating pseudo sentences .
Due to a large amount of English monolingual data , English monolingual sentence was randomly divided into 13.36 M , 25 M , 25 M , 25 M , and 25 M for En?Id , En?Jv , En?Ms , En?Ta , and En?Tl translation respectively .
All monolingual data of Id , Jv , Ms , Ta , and Tl are used in translation to all other directions .
In-domain Data Selection
The training data is provided by the publicly available Opus repository , which contains data of various quality from a variety of domains , while the hidden test set is the same domain as the provided dev and devtest datasets .
After fine- tuning on a mixture of authentic bitext and pseudo-data , we select domain-specific data from the bitext and continue to fine-tune to further improve translation quality .
Due to the scarcity of in- domain data , we utilize pre-trained language model multilingual BERT ( Devlin et al. , 2019 ) to train a domain classifier for extracting in-domain sentences from authentic bilingual sentences .
To train the domain classifier , we consider all available dev data as positive data , and randomly sample bilingual data as negative samples .
At the same domain test set , the domain classifier recognition accuracy is achieved at 93.97 % .
We select sentences predicted to be positive with a probability greater than threshold 0.7 to form an in-domain corpus .
Knowledge Distillation Knowledge distillation ( Hinton et al. , 2015 ) is a way to train a smaller network of students to perform better by learning from a larger teacher model .
On this basis , sequence - level knowledge distillation trains the student model on the new data generated by the teacher model to further improve the performance of the student ( Kim and Rush , 2016 ) .
A multilingual translation model that trains too many languages at the same time may degrade performance , especially involving 30 translation directions in this work .
It makes it harder for the model to accommodate all language pairs .
Based on this , we fine- tune the FLORES - 101 model on five language pairs with En?Ta , Id ?
Ta , Jv?Ta , Ms?Ta , Tl ?
Ta to produce an Any - to - Ta specific translation model ( Tan et al. , 2019 ) .
These five language pairs are chosen because they do not perform very well and have more room for improvement .
We used this model as the teacher model to translate the training data of the five language pairs .
The new data was then combined with data of other language pairs to train the student model .
Gradual Fine-tuning Fine-tuning can improve the machine translation model by adapting the initial model trained on abundant but less domain-specific examples to the data in the target domain .
This domain adaptation is usually accomplished with a phase of finetuning .
While Xu et al . ( 2021 ) prove that gradual fine-tuning over a multi-stage process can yield substantial further gains .
Intuitively , the model is iteratively trained to convergence on data whose distribution progressively approaches that of the in-domain data , similar to the curriculum learning strategy ( Bengio et al. , 2009 ; Kocmi and Bojar , 2017 ) .
In this work , we use gradual fine-tuning combined with in- domain data selection .
After training the domain classifier , authentic bilingual sentences with positive predictions and probabilities greater than the thresholds of 0.7 , 0.8 , 0.9 , and 0.99 are selected to form in - domain corpora with different similarity degrees .
Data statistics with different thresholds are shown in the Table 3 .
The higher the threshold , the more the selected data fits into the domain of the dev set and test set .
We started with a gradual fine-tuning on the domain-specific data selected at the 0.7 thresholds , followed by the 0.8 thresholds , and so on .
System Average
To further improve performance , we selected 12 language pairs that are significantly better than the baseline system .
We consider them BLEU - sensitive and performance -friendly language pairs , which include En?Ta , Id?
Ta , Jv?En , Jv?Ta , Jv?Tl , Ms?Ta , Ta?En , Ta?Id , Ta? Jv , Ta? Ms , Ta?Tl and Tl ? Ta .
After the gradual fine-tuning , we recover all the authentic bilingual sentences of these 12 language pairs , while the training sentences of other language pairs are still the training data when the threshold is 0.99 .
We continue to fine - tune the multilingual translation model .
We find that the results still improve on these 12 language pairs and the performance of other language pairs is almost unchanged .
Model Averaging Model averaging is typically used between 5 or 10 adjacent checkpoints on the same system .
It is almost impossible to average different systems because neurons or parameters at the same location in different systems may be responsible for completely different knowledge or responsibilities .
Our systems kept the random seeds consistent , and the training data did not differ too much , so we tried a variety of model averaging methods to see whether the performance was improved .
We finally chose average multiple checkpoints in a single system , and then averaged on different systems .
In this way , the translation result can be further improved .
Experiments
Experiment Settings Except for the FLORES - 101 fine-tuning experiments training on 48 NVIDIA P40 GPUs , the rest of our experiments are carried out with 16 NVIDIA P40 GPUs .
Our model apply Adam ( Kingma and Ba , 2015 ) as optimizer with ?
1 = 0.9 , ? 2 = 0.98 , and = 10 ?9 .
We set the label smoothing to 0.2 and the dropout rate to 0.3 .
The initial learning rate is set to 5e - 4 varied under a warm - up strategy with 4000 steps .
For training , the batch size is 4096 tokens per GPU .
For fine- tuning FLORES - 101 , we apply a temperature sampling strategy with sampling temperature T = 1.5 ( Arivazhagan et al. , 2019 ) .
During inference , we decode with beam search and set beam size to 4 for all language pairs .
The translation results we reported is detokenized and then the quality is evaluated using the 4 - gram case-sensitive BLEU ( Papineni et al. , 2002 ) with the SacreBLEU tool ( Post , 2018 ) . 5
Main Results
Results for all of our systems are shown in Table 4 .
For convenience , we only report the average BLEU for 30 language pairs .
The detailed BLEU scores for each language pair of systems implemented by TenTrans tool are shown in Table 5 , and the relevant systems for fine- tuning FLORES - 101 are shown in Table 6 .
As shown in Table 4 , we found that the baseline system with fine-tuning FLORES - 101 performed better than the baseline system with no pre-training model ( 24.23 vs. 22.25 ) .
Forward -translation and back-translation ( F&B ) greatly improved the translation performance in both TenTrans ( 25.05 vs. 22.25 ) and frameworks .
The results of individual models for forward -translation and back -translation are shown in Table 7 . Deep Transformer with 24 encoder layers further improves translation results , but still not as high as fine-tuning FLORES - 101 systems .
Given the excellent performance of the pre-trained model , our subsequent series of approaches are based on fine-tuning FLORES - 101 .
In -domain data selection is restricted to indomain data size ( threshold 0.7 ) , but we also obtain a solid improvement of 0.74 BLEU on average .
Gradual fine-tuning ( Gradual FT ) is also effective , which enables the model to potentially better fit 8 .
We guess that it may be because the translation quality of the teacher model is not excellent enough , which leads to the improvement of the student model is not satisfactory .
We then recovered bilingual sentences for 12 BLEU - sensitive language pairs .
As shown in Table 6 , the performance of these 12 language pairs improved significantly , while the results of the other language pairs barely changed , so our average BLEU improved further .
For model averaging , we tried different combinations and finally found that averaging the three best checkpoints in " + Gradual FT " and " + Recover 12 " will produce the best performance ( 28.94 ) .
Submitted Results
As shown in Table 9 , we ultimately chose the bestperforming model on devtest to submit to Dynabench 6 and achieve 28.89 in the hidden test set .
Conclusion
This paper introduced our TenTrans submissions on WMT21 large-scale multilingual machine translation small task 2 .
Our main exploration is using more diversified architectures and fine-tuning strategy , utilizing forward -translation and back translation and approaches including in-domain data selection , knowledge distillation , and gradual finetuning .
We experimented with these methods and continuously improve our system performance .
On the whole , all of our systems performed competitively and ranked 3rd on the leaderboard .
Table 1 : 1 Number of sentences in bitext data sets .
En Id Jv Ms Ta Tl No filter 126.44 M 5.46M 0.41M 1.87 M 2.06 M 0.41 M Filtered 113.36 M 5.26M 0.38N 1.85 M 2.03M 0.39M
