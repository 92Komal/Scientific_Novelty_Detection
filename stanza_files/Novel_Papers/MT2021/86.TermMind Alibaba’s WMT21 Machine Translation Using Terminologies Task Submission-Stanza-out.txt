title
TermMind : Alibaba 's WMT21 Machine Translation using Terminologies Task Submission
abstract
This paper describes our work in the WMT 2021 Machine Translation using Terminologies Shared Task .
We participate in the shared translation terminologies task in English to Chinese language pair .
To satisfy terminology constraints on translation , we use a terminology data augmentation strategy based on Transformer model .
We used tags to mark and add the term translations into the matched sentences .
We created synthetic terms using phrase tables extracted from bilingual corpus to increase the proportion of term translations in training data .
Detailed pre-processing and filtering on data , in - domain finetuning and ensemble method are used in our system .
Our submission obtains competitive results in the terminology - targeted evaluation .
Introduction Terminology is important for domain-specific machine translation .
Each domain has its own terminology , which represents the important and core concepts in the domain .
In the workflow of human translation , terminology is an effective method to integrate the knowledge of human translator into machine translations ( Wuebker et al. , 2016 ; Cheng et al. , 2016 ; ?lvaro Peris et al. , 2017 ) .
One line of approach is " hard constraint " .
The terminology is ensured to appear in the translation by adding constraints in beam search decoding ( Hokamp and Liu , 2017 ; Post and Vilar , 2018 ) .
However , the enforcement of terminology constraints tends to reduce the fluency of translation ( Hasler et al. , 2018 ) , especially when there are multiple constraints or the constraint is noisy ( Susanto et al. , 2020 ) .
Another line of approach is " soft constraint " .
Training data is augmented with placeholders or additional terminology translations ( Arthur et al. , 2016 ; Song et al. , 2019 ; Dinu et al. , 2019 ; Chen et al. , 2020 ; Ailem et al. , 2021a ) .
The above methods assume that the terminology translations are good ones .
However , in industry or real world the terminology translations may be noisy .
And in the human translation workflows the terminology constraints usually need to be applied hierarchically according to priority .
In these scenarios one source term will have more than one translation .
Therefore , we are happy to participate in this task and develop the method to deal with 1 - to - many term translations in neural machine translation systems .
The structure of the paper is as follows .
Section 2 describes the dataset , data pre-processing and selection .
We introduce details of our system in Section 3 .
The experiment settings , terminologies used in training and main results are introduced in Section 4 .
Finally , we conclude our work in Section 5 .
Data
Data Source
For this task , we utilize parallel data from English to Chinese language provided in WMT2021 : ParaCrawl v7.1 , News Commentary v16 , Wiki Titles v3 , UN Parallel Corpus V1.0 , CCMT Corpus and WikiMatrix .
In addition , we also require Chinese monolingual data from News crawl and News Commentary corpora for back translation .
Data Pre-processing
For all datasets , we tokenize English text with Moses 1 and the Chinese text with Jieba 2 tokenizer .
We create a joint source and target BPE vocab ( Sennrich et al. , 2016 ) with 40 k merge operations using filtered bilingual dataset as described in Section 2.3 , resulting in a vocabulary with size of 63 K words .
Data Selection
According to the previous works ( Li et al. , 2019 ; Sun et al. , 2019 ) the following schemes : ?
Remove the texts of over 120 tokens .
?
Remove bitexts with length ratios greater than 3 . ?
Remove texts with special HTML tags .
?
Remove duplicate bitexts .
?
Remove texts with fastText-langid ( Joulin et al. , 2016 b , a ) , which is an open-source tool for text - based language identification .
?
Remove Chinese sentences when the proportion of Chinese tokens is less than 0.8 .
System Overview
In this section , we will describe the details about the model and techniques of our work .
First , we will introduce the terminology data augmentation strategy to improve terminology translation accuracy .
Then , different transformer model architectures we adopted in the paper will be depicted .
Finally , we will introduce several strategies to train our models for performance improvement .
Terminology Learning
We use a terminology data augmentation strategy to encourage neural machine translation ( NMT ) to satisfy terminology constraints .
The key point of term translation idea is that when multiple possible terms are encountered , the NMT model is pre-ferred copying the correct terms , and the terms are correctly placed in the output sentence .
Encouraged by the work ( Chen et al. , 2020 ; Ailem et al. , 2021 b ) , we use tags to specify the term constraints in the source sentence .
We have given an example in the
Model Architecture
In our systems , we adopt three different model architectures with Transformer ( Vaswani et al. , 2017 ) : ? BIG Transformer is the Transformer - Base model ( Vaswani et al. , 2017 ) with 4096 feedforward network ( FFN ) width and 16 attention heads .
? DEEP Transformer ( Sun et al. , 2019 ) is Transformer - Base model with 20 encoder layers .
? LARGE Transformer is Transformer - Base model with 8192 FNN inner width .
We use 6 decoder layers for all models .
Our models are implemented with open-source toolkit Fairseq .
Optimization Strategies
To further improve the translation performance , several common strategies are used to train our models such as Back Translation , Finetuning and Ensemble .
The strategies are performed basically sequentially .
We use the terminology data augmentation on back translation and fine-tuning datasets to train models .
Back Translation
Back translation is a data augmentation technique to incorporate monolingual data into NMT model .
Similar to previous work ( Edunov et al. , 2018 ) , we use back translation to improve the model performance .
We first train a Chinese-to- English Transformer - Deep NMT model based on bilingual training dataset .
The NMT model is applied to translate Chinese monolingual corpus to English .
The pseudo parallel corpus is used to train models together with the bilingual training dataset .
Finetuning Previous study ( Sun et al. , 2019 ) demonstrate that fine-tuning a model on in-domain data effectively improve the model performance .
For the term translation task , two fine-tuning datasets are used in our works .
We use two kinds of finetuning datasets to train the model sequentially .
Base FT
We use all the previous English ?
Chinese development and test dataset as fine tuning corpus , including WMT2017 development data , WMT2017 test data , WMT2018 test data , WMT2019 test data and WMT2020 test data .
In-domain FT
To use in- domain dataset to fine tune the model , we perform data selection on out - of- domain corpus based on in- domain n-gram match .
The key idea is to select sentence pairs from the large out - of- domain corpus that are similar to the in-domain data .
We use the bilingual training data as the out-of- domain corpus and WMT2021 term development dataset as the indomain corpus .
We extract 1 - 3 grams from the indomain and out-of- domain dataset .
After exclude the ngrams from the out-of- domain data , the left in - domain ngrams are applied to match relevant sentence from the bilingual training .
In our work , we use source and target to select in - domain dataset respectively and finally the two sets are combined to train the model .
Ensemble Model ensemble is an effective strategy widely used in real-world tasks .
At each step of translation prediction , it combines the predicted probabilities of different models .
We use the log-avg strategy to ensemble the different NMT models .
The model diversity is an important factor for ensemble .
We have trained three Transformer models with different architectures including the variants of Transformer - BIG , Transformer -DEEP and Transformer - LARGE .
Experiments
Setups
Our models are implemented in Fairseq Library 3 . All the single models are trained based on 4 NVIDIA P100 - PCIe GPUs , each with 16 GB memory .
The models are optimized with Adam algorithm ( Kingma and Ba , 2015 ) with ?
1 = 0.9 and ? 2 = 0.98 .
We set max learning rate to 0.001 when training a single model from scratch and 0.0007 when fine-tuning the model .
The batch size is set to 2048 tokens per GPU .
The ' updatefreq ' parameter in Fairseq is set to 16 when training a single model from scratch and 4 when finetuning the model .
The dropout ( Gal and Ghahramani , 2016 ) probabilities are set to 0.1 in all experiments .
We select the checkpoint with the best BLEU score on development set as the final checkpoint in each training .
Evaluation of results focus on translation accuracy and term translation consistency .
We evaluate translation accuracy with SacreBLEU ( Post , 2018 ) case-sensitive detokenized BLEU .
Terminologytargeted metrics ( Anastasopoulos et al. , 2021 ) is used to term translation consistency , including exact-match accuracy , window overlap metric and terminology - biased Translation Edit Rate ( TERm ) 4 .
The exact-match accuracy is defined as the ratio between the number of matched source terms and the total number of source terms .
The window overlap metric is to evaluate the position accuracy of each target term in translation .
The TERm , a metric based on TER ( Snover et al. , 2006 ) , focuses on penalizing errors related to terminology tokens .
Terminologies
In order to increase the proportion of term translations in training data , we extract phrase tables from bilingual training corpus to create synthetic term translations .
First , we use FastAlign ( Dyer et al. , 2013 ) to generate word alignments .
Second , based on the word alignments we extract a phrase table by using moses ( Koehn et al. , 2007 ) with default settings .
We use count- based pruning ( Zens et al. , 2012 ) and fastText-langid ( Joulin et al. , 2016 b , a )
Results
Table 2 shows the English ?
Chinese translation results on WMT2021 terminologies development dataset , including BLEU , exact- match accuracy , window overlap accuracy ( 2/3 ) and 1- TERm Score .
We train multiple single models in each settings and report the best BLEU scores in Considering the effectiveness of fine-tuning , we use WMT2021 development data to fine tune the model after completing 100 steps .
In our final submission , we selected sentences with the higher probability from the translations of the ensemble Term model and the ensemble NMT model .
Conclusion
This paper presents the submissions by Alibaba for WMT 2021 English to Chinese translation terminologies task .
We have applied a terminology data augmentation method to integrate term translations into NMT systems .
We also used a series of data filtering strategies , fine -tuning and ensemble methods to improve the system performance .
Experimental results show the method can improve terminologies translation performance .
Table 1 : 1 , we selected data for training with Source
Those most at risk of COVID -19 infection and serious complications are the elderly and those with weakened immune systems or underlying health conditions like cardiovascular disease , diabetes , hypertension , chronic respiratory disease , and cancer .
Illustration of the terminology data augmentation .
Constraint diabetes infection ? ? | ? chronic respiratory disease ? | ?
Those most at risk of COVID -19 < term tgt= " ? | ? " > infection </ term > and serious complications are the elderly and those with weakened immune systems or Match underlying health conditions like cardiovascular disease , < term tgt= " ? " > diabetes </ term > , hypertension , < term tgt= " ? | ? ? " > chronic respiratory disease </ term > , and cancer .
Those most at risk of COVID -19 < S> [ MASK ] < C> ? [ SEP ] ? </ C> and serious complications are the elderly and those with weakened immune systems or Tag & Mask underlying health conditions like cardiovascular disease , < S> [ MASK ] < C> ? </ C> , hypertension , < S> [ MASK ] [ MASK ] [ MASK ] < C> ? [ SEP ] ? </ C> , and cancer .
Target COVID - 19 ? ? ? ? ?
