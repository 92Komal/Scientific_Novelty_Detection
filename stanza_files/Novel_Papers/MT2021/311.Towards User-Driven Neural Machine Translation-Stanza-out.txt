title
Towards User-Driven Neural Machine Translation
abstract
A good translation should not only translate the original content semantically , but also incarnate personal traits of the original text .
For a real-world neural machine translation ( NMT ) system , these user traits ( e.g. , topic preference , stylistic characteristics and expression habits ) can be preserved in user behavior ( e.g. , historical inputs ) .
However , current NMT systems marginally consider the user behavior due to : 1 ) the difficulty of modeling user portraits in zero-shot scenarios , and 2 ) the lack of userbehavior annotated parallel dataset .
To fill this gap , we introduce a novel framework called user-driven NMT .
Specifically , a cache- based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion .
Furthermore , we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT - Corpus .
Experimental results confirm that the proposed user-driven NMT can generate user-specific translations .
1
Introduction
In recent years , neural machine translation ( NMT ) models Luong et al. , 2015 ; Vaswani et al. , 2017 ) have shown promising quality and thus increasingly attracted users .
When drawing on a translation system , every user has his own traits , including topic preference , stylistic characteristics , and expression habits , which can be implicitly embodied in their behavior , e.g. , the historical inputs of these users .
A good translation should implicitly mirror user traits rather than * Jinsong Su is the corresponding author .
This work was done when Huan Lin was interning at DAMO Academy , Alibaba Group .
1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT .
merely translate the original content , as the example shown in Figure 1 .
However , current NMT models are mainly designed for the semantic transformation between the source and target sentences regardless of subtle traits with respect to user behavior .
It can be said that the effect of user behavior on translation modeling is still far from utilization , which , to some extent , limits the applicability of NMT models in real-world scenarios .
More recently , several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches , such as personality ( Mirkin et al. , 2015 ) , gender ( Rabinovich et al. , 2017 ) , and politeness ( Sennrich et al. , 2016 a ) .
However , previously explored signals characterize users from a single dimension , which insufficiently represent fine - grained user traits .
Furthermore , Michel and Neubig ( 2018 ) pay their attention to personalized TED talk translation , in which they train a speakerspecific bias to revise the prediction distribution .
In contrast with these studies , our work investigates a more realistic online scenario : a real- world MT system serves extensive users , where the user-behavior annotated data covering all users is unavailable .
Previous methods ( Mirkin et al. , 2015 ; Michel and Neubig , 2018 ) require the users in the training set and the test set to be consistent , therefore can not deal with this zero-shot issue .
Starting from this concern , we explore userdriven NMT that generates personalized translations for users unseen in the training dataset according to their behavior .
Specifically , we choose the historical inputs to represent user behavior since they can not only be easily obtained in the real-world scenarios , but also reflect the topic preference , stylistic characteristic , and context of user .
Moreover , compared with pre-defined or userspecific labels , historical inputs can be updated with current source sentences , which is also in line with realistic scenario .
In this work , we propose a novel framework for this task , where the NMT model is equipped with a cache module to restore and update historical inputs .
Besides , in order to further transfer the traits from the seen users to the unseen ones , we design a regularization framework based on contrastive learning ( Bose et al. , 2018 ; , which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users .
In order to further train and assess the proposed framework , we construct a new User-Driven Machine Translation dataset called UDT - Corpus .
This corpus consists of 6,550 users with totally 57,639 Chinese sentences collected from a realworld online MT system .
Among them , 17,099 Chinese sentences are annotated with their English translations by linguistic experts according to the user-specific historical inputs .
Experimental results demonstrate that the proposed framework facilitates the translation quality , and exactly generates diverse translations for different users .
To summarize , major contributions of our work are four-fold : ?
We introduce and explore user-driven NMT task that leverages user behavior to enhance translation model .
We hope our study can attract more attention to explore techniques on this topic . ?
We propose a novel framework for user-driven NMT based on cache module and contrastive learning , which is able to model user traits in zero-shot scenarios .
?
We collect UDT - Corpus and make it publicly available , which may contribute to the subsequent researches in the communities of NMT and user-driven models .
?
Extensive analyses indicate the effectiveness of our work and verify that NMT can profit from user behavior to generate diverse translations conforming to user traits .
Related Work
This section mainly includes the related studies of personalized machine translation , cache - based NMT and contrastive learning for NMT .
Personalized Machine Translation
Recently , some researchers have employed domain adaptation ( Zhang et al. , 2019 ; Gururangan et al. , 2020 ; Yao et al. , 2020 ) to generate personalized translations .
For example , Mirkin et al. ( 2015 ) show that the translation generated by the SMT model has an adverse effect on the prediction of author personalities , demonstrating the necessity of personalized machine translation .
Furthermore , Sennrich et al. ( 2016a ) control the politeness in the translation by adding a politeness label on the source side .
Rabinovich et al. ( 2017 ) explore a gender-personalized SMT system that retains the original gender traits .
These domain labels represent users in single dimension separately , which are insufficient to distinguish large-scale users in a fine- grained way .
The most correlated work to ours is Michel and Neubig ( 2018 ) which introduces a speaker -specific bias into the conventional NMT model .
However , these methods are unable to deal with users unseen at the training time .
Different from them , user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner .
Cache - Based Machine Translation
Inspired by the great success of cache on language modeling ( Kuhn and de Mori , 1990 ; Goodman , 2001 ; Federico et al. , 2008 ) , Nepveu et al. ( 2004 ) propose a cache- based adaptive SMT system .
Tiedemann ( 2010 ) explore a cache- based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document .
Bertoldi et al. ( 2013 ) use a cache mechanism to achieve online learning in phrase - based SMT .
Gong et al. ( 2011 ) , Kuang et al. ( 2018 ) , and Tu et al . ( 2018 ) further exploit cache- based approaches to leverage contextual information for document - level machine translation .
Contrast with the documentlevel NMT that learns to capture contextual information , our study aims at modeling user traits , such as , topic preference , stylistic characteristics , and expression habits .
Moreover , historical inputs of user has relatively fewer dependencies than the contexts used in document- level translation .
Contrastive Learning for NMT
Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning ( Vaswani et al. , 2013 ; Mnih and Kavukcuoglu , 2013 ; Liu and Sun , 2015 ; Bose et al. , 2018 ) .
Towards raising the ability of NMT in capturing global dependencies , Wiseman and Rush ( 2016 ) first introduce contrastive learning into NMT , where the ground -truth translation and the model output are considered as the positive and contrastive samples , respectively .
construct contrastive examples by deleting words from ground -truth translation to reduce word omission errors in NMT .
Contrast to these studies , we employ contrastive learning to create broader learning signals for our user-driven NMT model , where the prediction distribution of translations with respect to similar users and dissimilar users are considered as positive and contrastive samples , respectively .
Thus , our model can better transfer the knowledge of the seen users to the unseen ones .
User-Driven Translation Dataset
In order to build a user-driven NMT system , we construct a new dataset called UDT - Corpus containing 57,639 inputs of 6,550 users , 17,099 among them are Chinese-to - English translation examples .
Data Collection and Preprocessing
We collect raw examples from Alibaba Translate 2 which contain the user inputs and the translations given by the translation system .
For data preprocessing , we first anonymize data and perform data deduplication within each user .
Then , we utilize a pre-trained n-gram language model KenLM 3 to filter out translation examples with low-quality source data .
Moreover , we remove such pairs whose source sentence is shorter than 2 words or longer than 100 words .
Data Annotation
In the corpus , we represent each translation example as a triplet X ( u ) , Y ( u ) , H ( u ) , where H ( u ) is the historical inputs of the user u , X ( u ) is the current source sentence and Y ( u ) is the target translation sentence annotated with H ( u ) .
To obtain such a triplet , we first sequentially sample up to 10 source sentences which are the historical inputs of each user .
Then , for the given historical inputs , we collect their followed source input paired with the pseudo translation given by the translation system .
Afterwards , we assign these historical inputs and the current input pairs to two professional annotators and ask them to revise the pseudo translation according to the source sentence and historical inputs .
Specifically , we first ask one of them to annotate and the other to evaluate , and then resolve annotation disagreements by reviewing .
During annotation , 91.8 % of the original data are revised .
Moreover , annotators are asked to record whether their revision is affected by user history .
The result shows that 76.25 % of the sentences are impacted .
User-Driven NMT Framework
In this section , we first give a brief description about the problem formulation of user-driven NMT , and then introduce our proposed framework in detail .
We choose Transformer ( Vaswani et al. , 2017 ) as the basic NMT model due to its competitive performance .
In fact , our framework is transparent and applicable to other NMT models .
Figure 2 illustrates the basic framework of the proposed user-driven NMT .
Most typically , we equip the NMT model with two user-specific caches to exploit user behavior for better translation ( See Section ? 4.2 ) .
Besides , we augment the conventional NMT training objective with contrastive learning , which allows the model to learn translation diversity across users ( See Section ? 4.3 ) .
Problem Formulation
Given the source sentence X and the previously generated words Y < i = y 1 , ... , y i?1 , the conventional NMT model with parameter ? predicts the current target word y i by P ( y i |X , Y < i ; ? ) .
As a significant extension of conventional NMT , userdriven NMT with parameter ? aims to model P y ( u ) i | X ( u ) , Y ( u ) < i , u ; ? , that is , generates the translation that can reflect the traits of user u.
Unlike previous studies ( Mirkin et al. , 2015 ; Michel and Neubig , 2018 ) only caring for generating translations for users seen at the training time , our userdriven NMT mainly focuses on a more realistic online MT scenario , where the users for testing are unseen in the training dataset .
Moreover , the conventional domain adaptation methods can not be directly applied to this zero-shot scenario .
Topic Cache ? ! ( # ) Historical Inputs ? Context Cache ? % ( # ) ? ( # ) ? ( " ) ? ( " ! ) ? ( " " ) ? ( " ) ? ( " ) ?(? ! " |? " , ? #! " , ? " ) ?(? ! " |? " , ? #! " , ? " ! ) ?(? ! " |? " , ? #! " , ? " " ) ? $ % + ? & % '
User-Driven NMT Model Figure
2 : The architecture of our user-driven NMT model .
We use the topic cache and context cache to capture the long-term and short - term user traits for user u from corresponding historical inputs H ( u ) , respectively .
Then , we combine the representations of two caches to get a user behavior representation r ( u ) , which is fed into the NMT model for personalized translation .
Furthermore , we use contrastive learning involving similar user u + and dissimilar user u ? to increase the translation diversity among different users .
Cache- based User Behavior Modeling
Due to the advantages of cache mechanism on dynamic representations ( Gong et al. , 2011 ; Kuang et al. , 2018 ; Tu et al. , 2018 ) , we equip the conventional Transformer - based NMT model with two user-specific caches to leverage user behavior for NMT : 1 ) topic cache c ( u ) t that aims at capturing the global and long- term traits of user u ; and 2 ) context cache c ( u ) c , which is introduced to capture the short-term traits from the recent source inputs of user u .
During this process , we focus on the following three operations on cache : Cache Representation
In order to facilitate the efficient computation of the user behavior encoded by our caches , we define each cache as an embedding sequence of keywords .
We first calculate TF - IDF values of input words , and then extract words with TF - IDF weights higher than a predefined threshold to represent user behavior .
Note that the calculation of TF - IDF value of a word mainly depends on its frequency in the document and inverse document frequency in the corpus .
Since two caches play different roles in the userdriven NMT model , we identify keywords for two caches based on different definitions of " document " and " corpus " .
Specifically , when constructing topic cache c ( u ) t , we treat the historical inputs H ( u ) of the user u as the " document " and the historical inputs H ( u ) of all users U as the " corpus " , then define topic cache c ( u ) t as an embedding sequence of historical keywords .
Unlike the topic cache , for context cache c ( u ) c , we individually consider the current source sentence X ( u ) and historical inputs H ( u ) as the TF -IDF " document " and " corpus " , defining c ( u ) c as an embedding sequence of current keywords .
Besides , in the real-world MT scenario , there exists a large number of users without any historical input .
For these users , we find the most similar user according to the cosine similarity based on their TF - IDF bag- of - word representations of topic keywords , and initialize the corresponding topic cache with that of the most similar user .
Updating Caches
When using an online MT system , users often continuously input multiple sentences .
Thus , our caches should be dynamically updated to ensure the accurate encoding of user behavior .
To update topic cache , we first recalcualte the TF - IDF values of all historical input words , so as to redetermine the keywords stored in this cache .
As for context cache , we consider it as a filter window sliding across historical inputs , and apply first - infirst - out rule to replace its earliest keywords with the recently input ones .
Reading from Caches During the translation of the NMT model , we perform a gating operation on c ( u ) t and c ( u ) c , producing a vector r ( u ) that reflects user behavior as follows : r ( u ) = ?c ( u ) t + ( 1 ? ? ) c ( u ) c ( 1 ) ? = Sigmoid( W t c ( u ) t + W r c ( u ) c ) , ( 2 ) c ( u ) t = MeanPooling c ( u ) t , ( 3 ) c ( u ) c = MeanPooling c ( u ) c , ( 4 ) where both W t and W r are learnable parameter matrices .
Then , we directly add r ( u ) into the embedding sequence of original current source sentence X ( u ) , forming a source embedding sequence with user behavior as follows : X ( u ) = { x ( u ) i + r ( u ) } 1 ?i<| X ( u ) | . ( 5 ) Finally , the NMT model is fed with X ( u ) to generate the translation for u .
Due to the limitation of pages , we omit the detailed descriptions of the NMT model .
Please refer to Vaswani et al . ( 2017 ) for the details .
Model Training with a Contrastive Loss Given training instances X ( u ) , Y ( u ) , H ( u ) , we train the user-driven NMT model using the following objective function : L = L mle + L cl . ( 6 ) Here , L mle is the maximum likelihood translation loss extended from the conventional NMT training objective .
Formally , it is defined as : L mle = i ? log P ( y ( u ) i | X ( u ) , Y ( u ) < i , H ( u ) ; ? ) . ( 7 ) L cl is a triplet- margin- based constrastive loss , which allows the NMT model to learn the translation diversity across users .
Specifically , for an input sentence , an ideal userdriven NMT model should be able to generate translations with non-divergent user traits for similar users , while producing translations with diverse user traits for dissimilar users .
However , using only L mle cannot guarantee this since it separately considers each training instance during the model training .
To deal with this issue , for each training instance X ( u ) , Y ( u ) , H ( u ) , we first determine the most similar user u + according to the cosine similarity based on their bag-of-keyword representations , and randomly select a user without any same keyword as the dissimilar user u ? of u.
Finally , using historical inputs of u + and u ? , we construct several pseudo training instances to define L cl as follows : L cl = u?U max [ d ( X ( u ) , Y ( u ) , H ( u ) , H ( u + ) ) ( 8 ) ? d( X ( u ) , Y ( u ) , H ( u ) , H ( u ? ) ) + ? , 0 ] , where d X ( u ) , Y ( u ) , H ( u ) , H ( u + ) = || 1 | Y ( u ) | i log P y ( u ) i | X ( u ) , Y ( u ) < i , H ( u ) ?
1 | Y ( u ) | i log P y ( u ) i | X ( u ) , Y ( u ) < i , H ( u + ) || 2 ( 9 ) and ? is a predefined threshold , which is set to 2 in our experiments .
Here , we omit the definition of d X ( u ) , Y ( u ) , H ( u ) , H ( u ? ) , which is similar to d X ( u ) , Y ( u ) , H ( u ) , H ( u + ) .
Formally , L cl will encourage the NMT model to minimize the prediction difference between the training instances X ( u ) , Y ( u ) , H ( u ) and X ( u ) , Y ( u ) , H ( u + ) , and maximize the difference between the training instances X ( u ) , Y ( u ) , H ( u ) and X ( u ) , Y ( u ) , H ( u ? ) .
In this way , the NMT model can not only exploit pesudo training instances , but also produce more consistent translations with user traits .
Experiments
In this section , we carry out several groups of experiments to investigate the effectiveness of our proposed framework on UDT - Corpus .
Setup
We develop the user-driven NMT model based on Open-NMT Transformer ( Klein et al. , 2017 ) , and adopt a two -stage strategy to train this model : we first pre-train a Transformer - based NMT model on the WMT2017 Chinese - to - English dataset , and then fine -tune this model to our user-driven NMT model using UDT - Corpus .
Datasets
The WMT2017 Chinese-to- English dataset is composed of the News Commentary v12 , UN Parallel Corpus v1.0 , and CWMT corpora , with totally 25 M parallel sentences .
To fine- tune our model , we split UDT - Corpus into training , validation and test set , respectively .
Table 1 provides more detailed statistics of these datasets .
To improve the efficiency of model training , we train the model using only parallel sentences with no more than 100 words .
Following common practices , we employ byte pair encoding ( Sennrich et al. , 2016 b ) with 32 K merge operations to deal with all sentences .
Training Details Following Vaswani et al. ( 2017 ) , we use the following hyper-parameters : the word embedding dimension is set to 512 , the hidden layer dimension is 2048 , the layer numbers of both encoder and decoder are set to 6 , and the number of attention heads is set to 8 .
Besides , we use 4 GPUs for training .
At the pre-training stage , we employ the Adam optimizer with ? 2 = 0.998 .
We use the batch size of 16,384 tokens and pre-train the model for 200,000 steps .
Particularly , we adopt the dropout strategy ( Srivastava et al. , 2014 ) with rate 0.1 to enhance the robustness of our model .
When fine- tuning the model , we keep the other settings consistent with the pre-training stage , but reduce the batch size to 2048 tokens and fine - tune the model with early - stopping strategy .
Evaluation
We assess the translation quality with two metrics : one is case-insensitive BLEU ( mteval - v13a.pl , Papineni et al. , 2002 ) 4 and the other is METEOR 5 ( Denkowski and Lavie , 2011 ) .
Baselines
We represent our user-driven NMT model as UD - NMT and compare it with the following baselines : ? TF .
It is a Transformer - based NMT model pretrained on the WMT2017 corpus .
This model yields 24.61 BLEU score on WMT2017 Chinese- to - English translation task , which is comparable with reported results in , which makes our subsequent experiments convincing .
? TF - FT .
This model is also a Transformerbased NMT model that is further fine-tuned on the parallel sentences of UDT - Corpus .
? TF - FT + PesuData .
This model is a variant of TF - FT .
When constructing it , we pair historical inputs with their translations produced by our online translation system , forming additional data for fine-tuning TF - FT .
? TF - FT + ConcHist ( Tiedemann and Scherrer , 2017 ) .
In this model , we introduce user behavior into TF - FT by concatenating each input sentence with several historical inputs .
We mark all tokens in historical inputs with a special prefix to indicate that they are additional information .
? TF - FT + UserBias ( Michel and Neubig , 2018 ) . since ( Michel and Neubig , 2018 ) can not be directly applied to our scenario .
In particular , we replace the user ID in the test set with that of the most similar user in the training set .
Note that the first two baselines , e.g. , TF and TF - FT , are conventional NMT models without exploiting user behavior .
Effect of Cache Sizes
Since cache size directly determines the utility of user behavior , we investigate its effect on the performance of UD - NMT .
We denote the sizes of topic cache and context cache as s t and s c for simplicity .
Figure 3 lists the performance of our model with different s t and s c on validation set .
We observe that s t larger than 25 and s c larger than 35 do not lead to significant improvements .
For this result , we speculate that small cache sizes are unable to capture sufficient user behavior for NMT .
However , since the number of keywords are limited , larger cache sizes only bring limited information gain .
Therefore , we directly use s t = 25 and s c = 35 in the subsequent experiments .
Main Results From Table 2 , we observe that our UD - NMT model consistently outperforms all baselines in terms of two metrics .
Moreover , we draw several interesting conclusions : 3 : Ablation Study . ? : higher is better , ? : lower is better .
Since the user similarity is calculated based on the topic keywords , the model can not find similar user and dissimilar user without it .
Thus w/o topic cache does not have the s-BLEU , s- Sim. , d-BLEU and d-Sim .. ?/ ? : indicates the drop of translation quality is statistically significant comparing to " UD - NMT " ( p<0.01/0.05 ) .
Model BLEU ?
METEOR ?
s-BLEU ? d-BLEU ?
s-Sim .?
d- 1 ) All NMT models leveraging user behavior surpass vanilla models , including TF , TF - FT , showing that user behavior is useful for NMT .
2 ) UD - NMT exhibits better than TF - FT + Pesu-Data , which uses the same training data as ours .
The underlying reason is that UD - NMT can leverage user traits to generate better translations .
3 ) Although both TF - FT + UserBias and UD - NMT exploit user behavior for NMT , UD - NMT achieves better performance than TF - FT + User-Bias without introducing extra parameters .
This result demonstrates the advantage of cache on modeling user behavior than introducing user-specific biases into model parameters .
Ablation Study
To explore the effectiveness of different components in our model , we further compare UD - NMT with its several variants , as shown in Table 3 .
Particularly , we propose to evaluate translations using the following variant metrics : s- BLEU , s- Sim. , d-BLEU and d-Sim ..
When using s-BLEU , we replace the topic cache of current user with that of his most similar user .
Keeping the same current input , we calculate the BLEU score with ground -truth as reference and the translation for this similar user as hypothesis .
As for s-Sim. , we adopt the same strategy as s-BLEU , but use the translation for original user as reference to evaluate the BLEU score .
In other words , s-BLEU and d-BLEU assesses the translation quality given unsuitable user .
Therefore , higher s-BLEU and d-BLEU indicates better model robustness , while s-BLEU and d-BLEU measures how much the translation changes given different user .
Thus lower s-Sim. and d-Sim . show larger translation diversity .
Our conclusions are shown as follows : 1 ) w/o topic cache .
To build this variant , we remove topic cache from our model .
The result in Line 2 indicates that removing topic cache leads to a performance drop , suggesting that topic cache is useful for modeling user behavior .
2 ) w/o context cache .
Unlike the above variant , we only use topic cache to represent user traits in this variant .
According to the results shown in Line 3 , we observe that this change results in a significant performance decline of our model , demonstrating that context cache also effectively captures user behavior for NMT .
However , the translation diversity among users increases since the model will not be affected by the context cache in this variant , which is the same between different users when calculating s-Sim. and d-Sim ..
3 ) w/o similar user initialization .
In this variant , we do not initialize topic caches of the users without historical inputs using that of the most similar users .
From Line 4 , we observe that the performance of our model degrades without similar user initialization .
4 ) w/o contrastive learning .
In this variant , we remove the contrastive learning from the whole training objective to inspect the performance change of our model .
As shown in Line 4 , the performance of our model drops , proving that the contrastive learning is important for the training of our model .
Moreover , we can infer from Column 6 and 7 that our model can generate diverse translations .
Specifically , the translations of dissimilar users has larger diversity than that of similar ones .
Furthermore , we conclude that our model is robust , since it still performs well when we replace the topic cache of current user with those of other users ( See Column 4 and 5 ) .
Analysis of Contrastive Margin Inspired by , we argue that the contrastive learning may increase the prediction diversity of our model between users compared with using the MLE loss .
To confirm this , we randomly sample 300 examples from the training dataset , and compute the following margin : ? = d ( u + ) ( ? ) ? d ( u ? ) ( ? ) ? d ( u + ) mle ( ? ) ? d ( u ? ) mle ( ? ) , where d ( u + ) ( ? ) is defined in Equation 9 .
The definition of d ( u + ) mle ( ? ) is the same with d ( ? ) , the only difference lies in that the NMT model is only trained by the conventional MLE loss .
We find that d( ? ) has a larger margin than d mle ( ? ) on 88 % of sampled sentence pairs , with an average margin of 0.19 .
The results indicate again that the contrastive learning increases the translation diversity .
Qualitative Analysis
In order to intuitively understand how our cache module exactly affects the translations , we feed our model with the same current source sentence but different users , and display the 1 - best translations generated by our model .
As shown in the Figure 4 ( a ) , our model is able to produce correct but diverse translations according to different topic caches .
Moreover , it is interesting to observe that specific topic keywords such as " type b arr " , " negatively regulated " and " modulators " are translated to synonymous but " out - of- domain " phrases if the topic cache does not conform to input sentence .
On the contrary , the model conversely generates " indomain " translation if the topic cache comes from the same topic of input sentence .
Correlation Order Proportion UD-NMT > TF - FT + PesuData 86 % UD-NMT > TF - FT + UserBias 74 % Besides , to further reveal the effect of user behavior , we provide an example in Figure 4 ( b ) , which lists different translations by compared models for the same inputs .
The historical inputs indicate that this user may be an apparel seller , since his historical inputs contain the product titles and descriptions of clothing .
Thus , the keywords " Wear Resistant " in the source sentence are correlated with this user .
However , two baselines translate it to " Waterproof " and " Resistant " , respectively .
Moreover , TF - FT + UserBias generates a subject - verb-object structured sentence by adding the auxiliary verb " is " , which does not conform to the expression habit of the product title .
By contrast , with the hint of the keywords in historical inputs , our UD - NMT is able to produce suitable translation consistent with the topic preference of this user .
Manual Evaluation
To further find out weather the improvements of our model are contributed by user traits , we ran-domly sample 100 examples from the test dataset and ask the linguist experts to sort different systems according to the relevance between the generated translations and the historical input .
The results in Table 4 show that our model can generate translations more in line with history inputs than baseline models in most cases , proving that our method can make better use of user traits .
Conclusion
We propose user-driven NMT task , which aims to leverage user behavior to generate personalized translations .
With the help of cache module and contrastive estimation , we successfully build an end-to- end NMT model that is able to capture potential user traits from their historical inputs and generate diverse translations under a zero-shot learning fashion .
Furthermore , we contribute UDT - Corpus , which is the first Chinese -English parallel corpus annotated with user behavior .
We expect our study can attract more attention towards this topic .
It is a promising direction to explore other behavior in future , such as clickthrough and editing operations .
Moreover , following recent advancements in domain adaptation for NMT , we plan to further improve our model via adversial training based knowledge transfer ( Zeng et al. , 2018 ; Yao et al. , 2020 ; Su et al. , 2021 ) and dual knowledge transfer ( Zeng et al. , 2019 ) . Figure 1 : 1 Figure 1 : An example in which user traits leads to synonymous yet stylistically different translations .
